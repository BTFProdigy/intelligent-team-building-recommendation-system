Mining WordNet for Fuzzy Sentiment:
Sentiment Tag Extraction from WordNet Glosses
Alina Andreevskaia and Sabine Bergler
Concordia University
Montreal, Quebec, Canada
{andreev, bergler}@encs.concordia.ca
Abstract
Many of the tasks required for semantic
tagging of phrases and texts rely on a list
of words annotated with some semantic
features. We present a method for ex-
tracting sentiment-bearing adjectives from
WordNet using the Sentiment Tag Extrac-
tion Program (STEP). We did 58 STEP
runs on unique non-intersecting seed lists
drawn from manually annotated list of
positive and negative adjectives and evalu-
ated the results against other manually an-
notated lists. The 58 runs were then col-
lapsed into a single set of 7, 813 unique
words. For each word we computed a
Net Overlap Score by subtracting the total
number of runs assigning this word a neg-
ative sentiment from the total of the runs
that consider it positive. We demonstrate
that Net Overlap Score can be used as a
measure of the words degree of member-
ship in the fuzzy category of sentiment:
the core adjectives, which had the high-
est Net Overlap scores, were identified
most accurately both by STEP and by hu-
man annotators, while the words on the
periphery of the category had the lowest
scores and were associated with low rates
of inter-annotator agreement.
1 Introduction
Many of the tasks required for effective seman-
tic tagging of phrases and texts rely on a list of
words annotated with some lexical semantic fea-
tures. Traditional approaches to the development
of such lists are based on the implicit assumption
of classical truth-conditional theories of meaning
representation, which regard all members of a cat-
egory as equal: no element is more of a mem-
ber than any other (Edmonds, 1999). In this pa-
per, we challenge the applicability of this assump-
tion to the semantic category of sentiment, which
consists of positive, negative and neutral subcate-
gories, and present a dictionary-based Sentiment
Tag Extraction Program (STEP) that we use to
generate a fuzzy set of English sentiment-bearing
words for the use in sentiment tagging systems 1.
The proposed approach based on the fuzzy logic
(Zadeh, 1987) is used here to assign fuzzy sen-
timent tags to all words in WordNet (Fellbaum,
1998), that is it assigns sentiment tags and a degree
of centrality of the annotated words to the senti-
ment category. This assignment is based on Word-
Net glosses. The implications of this approach for
NLP and linguistic research are discussed.
2 The Category of Sentiment as a Fuzzy
Set
Some semantic categories have clear membership
(e.g., lexical fields (Lehrer, 1974) of color, body
parts or professions), while others are much more
difficult to define. This prompted the development
of approaches that regard the transition frommem-
bership to non-membership in a semantic category
as gradual rather than abrupt (Zadeh, 1987; Rosch,
1978). In this paper we approach the category of
sentiment as one of such fuzzy categories where
some words ? such as good, bad ? are very cen-
tral, prototypical members, while other, less cen-
tral words may be interpreted differently by differ-
ent people. Thus, as annotators proceed from the
core of the category to its periphery, word mem-
1Sentiment tagging is defined here as assigning positive,
negative and neutral labels to words according to the senti-
ment they express.
209
bership in this category becomes more ambiguous,
and hence, lower inter-annotator agreement can be
expected for more peripheral words. Under the
classical truth-conditional approach the disagree-
ment between annotators is invariably viewed as a
sign of poor reliability of coding and is eliminated
by ?training? annotators to code difficult and am-
biguous cases in some standard way. While this
procedure leads to high levels of inter-annotator
agreement on a list created by a coordinated team
of researchers, the naturally occurring differences
in the interpretation of words located on the pe-
riphery of the category can clearly be seen when
annotations by two independent teams are com-
pared. The Table 1 presents the comparison of GI-
H4 (General Inquirer Harvard IV-4 list, (Stone et
al., 1966)) 2 and HM (from (Hatzivassiloglou and
McKeown, 1997) study) lists of words manually
annotated with sentiment tags by two different re-
search teams.
GI-H4 HM
List composition nouns, verbs,
adj., adv.
adj. only
Total list size 8, 211 1, 336
Total adjectives 1, 904 1, 336
Tags assigned Positiv, Nega-
tiv or no tag
Positive
or Nega-
tive
Adj. with 1, 268 1, 336
non-neutral tags
Intersection 774 (55% 774 (58%
(% intersection) of GI-H4 adj) of HM)
Agreement on tags 78.7%
Table 1: Agreement between GI-H4 and HM an-
notations on sentiment tags.
The approach to sentiment as a category with
fuzzy boundaries suggests that the 21.3% dis-
agreement between the two manually annotated
lists reflects a natural variability in human an-
notators? judgment and that this variability is re-
lated to the degree of centrality and/or relative im-
portance of certain words to the category of sen-
timent. The attempts to address this difference
2The General Inquirer (GI) list used in this study was
manually cleaned to remove duplicate entries for words with
same part of speech and sentiment. Only the Harvard IV-4
list component of the whole GI was used in this study, since
other lists included in GI lack the sentiment annotation. Un-
less otherwise specified, we used the full GI-H4 list including
the Neutral words that were not assigned Positiv or Negativ
annotations.
in importance of various sentiment markers have
crystallized in two main approaches: automatic
assignment of weights based on some statistical
criterion ((Hatzivassiloglou and McKeown, 1997;
Turney and Littman, 2002; Kim and Hovy, 2004),
and others) or manual annotation (Subasic and
Huettner, 2001). The statistical approaches usu-
ally employ some quantitative criterion (e.g., mag-
nitude of pointwise mutual information in (Turney
and Littman, 2002), ?goodness-for-fit? measure in
(Hatzivassiloglou and McKeown, 1997), probabil-
ity of word?s sentiment given the sentiment if its
synonyms in (Kim and Hovy, 2004), etc.) to de-
fine the strength of the sentiment expressed by a
word or to establish a threshold for the member-
ship in the crisp sets 3 of positive, negative and
neutral words. Both approaches have their limi-
tations: the first approach produces coarse results
and requires large amounts of data to be reliable,
while the second approach is prohibitively expen-
sive in terms of annotator time and runs the risk of
introducing a substantial subjective bias in anno-
tations.
In this paper we seek to develop an approach
for semantic annotation of a fuzzy lexical cate-
gory and apply it to sentiment annotation of all
WordNet words. The sections that follow (1) de-
scribe the proposed approach used to extract sen-
timent information from WordNet entries using
STEP (Semantic Tag Extraction Program) algo-
rithm, (2) discuss the overall performance of STEP
on WordNet glosses, (3) outline the method for
defining centrality of a word to the sentiment cate-
gory, and (4) compare the results of both automatic
(STEP) and manual (HM) sentiment annotations
to the manually-annotated GI-H4 list, which was
used as a gold standard in this experiment. The
comparisons are performed separately for each of
the subsets of GI-H4 that are characterized by a
different distance from the core of the lexical cat-
egory of sentiment.
3 Sentiment Tag Extraction from
WordNet Entries
Word lists for sentiment tagging applications can
be compiled using different methods. Automatic
methods of sentiment annotation at the word level
can be grouped into two major categories: (1)
corpus-based approaches and (2) dictionary-based
3We use the term crisp set to refer to traditional, non-
fuzzy sets
210
approaches. The first group includes methods
that rely on syntactic or co-occurrence patterns
of words in large texts to determine their senti-
ment (e.g., (Turney and Littman, 2002; Hatzivas-
siloglou and McKeown, 1997; Yu and Hatzivas-
siloglou, 2003; Grefenstette et al, 2004) and oth-
ers). The majority of dictionary-based approaches
use WordNet information, especially, synsets and
hierarchies, to acquire sentiment-marked words
(Hu and Liu, 2004; Valitutti et al, 2004; Kim
and Hovy, 2004) or to measure the similarity
between candidate words and sentiment-bearing
words such as good and bad (Kamps et al, 2004).
In this paper, we propose an approach to senti-
ment annotation of WordNet entries that was im-
plemented and tested in the Semantic Tag Extrac-
tion Program (STEP). This approach relies both
on lexical relations (synonymy, antonymy and hy-
ponymy) provided in WordNet and on the Word-
Net glosses. It builds upon the properties of dic-
tionary entries as a special kind of structured text:
such lexicographical texts are built to establish se-
mantic equivalence between the left-hand and the
right-hand parts of the dictionary entry, and there-
fore are designed to match as close as possible the
components of meaning of the word. They have
relatively standard style, grammar and syntactic
structures, which removes a substantial source of
noise common to other types of text, and finally,
they have extensive coverage spanning the entire
lexicon of a natural language.
The STEP algorithm starts with a small set of
seed words of known sentiment value (positive
or negative). This list is augmented during the
first pass by adding synonyms, antonyms and hy-
ponyms of the seed words supplied in WordNet.
This step brings on average a 5-fold increase in
the size of the original list with the accuracy of the
resulting list comparable to manual annotations
(78%, similar to HM vs. GI-H4 accuracy). At the
second pass, the system goes through all WordNet
glosses and identifies the entries that contain in
their definitions the sentiment-bearing words from
the extended seed list and adds these head words
(or rather, lexemes) to the corresponding category
? positive, negative or neutral (the remainder). A
third, clean-up pass is then performed to partially
disambiguate the identified WordNet glosses with
Brill?s part-of-speech tagger (Brill, 1995), which
performs with up to 95% accuracy, and eliminates
errors introduced into the list by part-of-speech
ambiguity of some words acquired in pass 1 and
from the seed list. At this step, we also filter out
all those words that have been assigned contradict-
ing, positive and negative, sentiment values within
the same run.
The performance of STEP was evaluated using
GI-H4 as a gold standard, while the HM list was
used as a source of seed words fed into the sys-
tem. We evaluated the performance of our sys-
tem against the complete list of 1904 adjectives in
GI-H4 that included not only the words that were
marked as Positiv, Negativ, but also those that were
not considered sentiment-laden by GI-H4 annota-
tors, and hence were by default considered neutral
in our evaluation. For the purposes of the evalua-
tion we have partitioned the entire HM list into 58
non-intersecting seed lists of adjectives. The re-
sults of the 58 runs on these non-intersecting seed
lists are presented in Table 2. The Table 2 shows
that the performance of the system exhibits sub-
stantial variability depending on the composition
of the seed list, with accuracy ranging from 47.6%
to 87.5% percent (Mean = 71.2%, Standard Devi-
ation (St.Dev) = 11.0%).
Average Average
run size % correct
# of adj StDev % StDev
PASS 1 103 29 78.0% 10.5%
(WN Relations)
PASS 2 630 377 64.5% 10.8%
(WN Glosses)
PASS 3 435 291 71.2% 11.0%
(POS clean-up)
Table 2: Performance statistics on STEP runs.
The significant variability in accuracy of the
runs (Standard Deviation over 10%) is attributable
to the variability in the properties of the seed list
words in these runs. The HM list includes some
sentiment-marked words where not all meanings
are laden with sentiment, but also the words where
some meanings are neutral and even the words
where such neutral meanings are much more fre-
quent than the sentiment-laden ones. The runs
where seed lists included such ambiguous adjec-
tives were labeling a lot of neutral words as sen-
timent marked since such seed words were more
likely to be found in the WordNet glosses in their
more frequent neutral meaning. For example, run
# 53 had in its seed list two ambiguous adjectives
1
dim and plush, which are neutral in most of the
contexts. This resulted in only 52.6% accuracy
(18.6% below the average). Run # 48, on the
other hand, by a sheer chance, had only unam-
biguous sentiment-bearing words in its seed list,
and, thus, performed with a fairly high accuracy
(87.5%, 16.3% above the average).
In order to generate a comprehensive list cov-
ering the entire set of WordNet adjectives, the 58
runs were then collapsed into a single set of unique
words. Since many of the clearly sentiment-laden
adjectives that form the core of the category of
sentiment were identified by STEP in multiple
runs and had, therefore, multiple duplicates in the
list that were counted as one entry in the com-
bined list, the collapsing procedure resulted in
a lower-accuracy (66.5% - when GI-H4 neutrals
were included) but much larger list of English ad-
jectives marked as positive (n = 3, 908) or neg-
ative (n = 3, 905). The remainder of WordNet?s
22, 141 adjectives was not found in any STEP run
and hence was deemed neutral (n = 14, 328).
Overall, the system?s 66.5% accuracy on the
collapsed runs is comparable to the accuracy re-
ported in the literature for other systems run on
large corpora (Turney and Littman, 2002; Hatzi-
vassiloglou and McKeown, 1997). In order to
make a meaningful comparison with the results
reported in (Turney and Littman, 2002), we also
did an evaluation of STEP results on positives and
negatives only (i.e., the neutral adjectives from GI-
H4 list were excluded) and compared our labels to
the remaining 1266 GI-H4 adjectives. The accu-
racy on this subset was 73.4%, which is compara-
ble to the numbers reported by Turney and Littman
(2002) for experimental runs on 3, 596 sentiment-
marked GI words from different parts of speech
using a 2x109 corpus to compute point-wise mu-
tual information between the GI words and 14
manually selected positive and negative paradigm
words (76.06%).
The analysis of STEP system performance
vs. GI-H4 and of the disagreements between man-
ually annotated HM and GI-H4 showed that
the greatest challenge with sentiment tagging of
words lies at the boundary between sentiment-
marked (positive or negative) and sentiment-
neutral words. The 7% performance gain (from
66.5% to 73.4%) associated with the removal of
neutrals from the evaluation set emphasizes the
importance of neutral words as a major source of
sentiment extraction system errors 4. Moreover,
the boundary between sentiment-bearing (positive
or negative) and neutral words in GI-H4 accounts
for 93% of disagreements between the labels as-
signed to adjectives in GI-H4 and HM by two in-
dependent teams of human annotators. The view
taken here is that the vast majority of such inter-
annotator disagreements are not really errors but
a reflection of the natural ambiguity of the words
that are located on the periphery of the sentiment
category.
4 Establishing the degree of word?s
centrality to the semantic category
The approach to sentiment category as a fuzzy
set ascribes the category of sentiment some spe-
cific structural properties. First, as opposed to the
words located on the periphery, more central ele-
ments of the set usually have stronger and more
numerous semantic relations with other category
members 5. Second, the membership of these cen-
tral words in the category is less ambiguous than
the membership of more peripheral words. Thus,
we can estimate the centrality of a word in a given
category in two ways:
1. Through the density of the word?s relation-
ships with other words ? by enumerating its
semantic ties to other words within the field,
and calculating membership scores based on
the number of these ties; and
2. Through the degree of word membership am-
biguity ? by assessing the inter-annotator
agreement on the word membership in this
category.
Lexicographical entries in the dictionaries, such
as WordNet, seek to establish semantic equiva-
lence between the word and its definition and pro-
vide a rich source of human-annotated relation-
ships between the words. By using a bootstrap-
ping system, such as STEP, that follows the links
between the words in WordNet to find similar
words, we can identify the paths connecting mem-
bers of a given semantic category in the dictionary.
With multiple bootstrapping runs on different seed
4It is consistent with the observation by Kim and Hovy
(2004) who noticed that, when positives and neutrals were
collapsed into the same category opposed to negatives, the
agreement between human annotators rose by 12%.
5The operationalizations of centrality derived from the
number of connections between elements can be found in so-
cial network theory (Burt, 1980)
212
lists, we can then produce a measure of the den-
sity of such ties. The ambiguity measure de-
rived from inter-annotator disagreement can then
be used to validate the results obtained from the
density-based method of determining centrality.
In order to produce a centrality measure, we
conducted multiple runs with non-intersecting
seed lists drawn from HM. The lists of words
fetched by STEP on different runs partially over-
lapped, suggesting that the words identified by the
system many times as bearing positive or negative
sentiment are more central to the respective cate-
gories. The number of times the word has been
fetched by STEP runs is reflected in the Gross
Overlap Measure produced by the system. In
some cases, there was a disagreement between dif-
ferent runs on the sentiment assigned to the word.
Such disagreements were addressed by comput-
ing the Net Overlap Scores for each of the found
words: the total number of runs assigning the word
a negative sentiment was subtracted from the to-
tal of the runs that consider it positive. Thus, the
greater the number of runs fetching the word (i.e.,
Gross Overlap) and the greater the agreement be-
tween these runs on the assigned sentiment, the
higher the Net Overlap Score of this word.
The Net Overlap scores obtained for each iden-
tified word were then used to stratify these words
into groups that reflect positive or negative dis-
tance of these words from the zero score. The zero
score was assigned to (a) the WordNet adjectives
that were not identified by STEP as bearing posi-
tive or negative sentiment 6 and to (b) the words
with equal number of positive and negative hits
on several STEP runs. The performance measures
for each of the groups were then computed to al-
low the comparison of STEP and human annotator
performance on the words from the core and from
the periphery of the sentiment category. Thus, for
each of the Net Overlap Score groups, both auto-
matic (STEP) and manual (HM) sentiment annota-
tions were compared to human-annotated GI-H4,
which was used as a gold standard in this experi-
ment.
On 58 runs, the system has identified 3, 908
English adjectives as positive, 3, 905 as nega-
tive, while the remainder (14, 428) of WordNet?s
22, 141 adjectives was deemed neutral. Of these
14, 328 adjectives that STEP runs deemed neutral,
6The seed lists fed into STEP contained positive or neg-
ative, but no neutral words, since HM, which was used as a
source for these seed lists, does not include any neutrals.
Figure 1: Accuracy of word sentiment tagging.
884 were also found in GI-H4 and/or HM lists,
which allowed us to evaluate STEP performance
and HM-GI agreement on the subset of neutrals as
well. The graph in Figure 1 shows the distribution
of adjectives by Net Overlap scores and the aver-
age accuracy/agreement rate for each group.
Figure 1 shows that the greater the Net Over-
lap Score, and hence, the greater the distance of
the word from the neutral subcategory (i.e., from
zero), the more accurate are STEP results and the
greater is the agreement between two teams of hu-
man annotators (HM and GI-H4). On average,
for all categories, including neutrals, the accuracy
of STEP vs. GI-H4 was 66.5%, human-annotated
HM had 78.7% accuracy vs. GI-H4. For the words
with Net Overlap of ?7 and greater, both STEP
and HM had accuracy around 90%. The accu-
racy declined dramatically as Net Overlap scores
approached zero (= Neutrals). In this category,
human-annotated HM showed only 20% agree-
ment with GI-H4, while STEP, which deemed
these words neutral, rather than positive or neg-
ative, performed with 57% accuracy.
These results suggest that the two measures of
word centrality, Net Overlap Score based on mul-
tiple STEP runs and the inter-annotator agreement
(HM vs. GI-H4), are directly related 7. Thus, the
Net Overlap Score can serve as a useful tool in
the identification of core and peripheral members
of a fuzzy lexical category, as well as in predic-
7In our sample, the coefficient of correlation between the
two was 0.68. The Absolute Net Overlap Score on the sub-
groups 0 to 10 was used in calculation of the coefficient of
correlation.
213
tion of inter-annotator agreement and system per-
formance on a subgroup of words characterized by
a given Net Overlap Score value.
In order to make the Net Overlap Score measure
usable in sentiment tagging of texts and phrases,
the absolute values of this score should be nor-
malized and mapped onto a standard [0, 1] inter-
val. Since the values of the Net Overlap Score
may vary depending on the number of runs used in
the experiment, such mapping eliminates the vari-
ability in the score values introduced with changes
in the number of runs performed. In order to ac-
complish this normalization, we used the value of
the Net Overlap Score as a parameter in the stan-
dard fuzzy membership S-function (Zadeh, 1975;
Zadeh, 1987). This function maps the absolute
values of the Net Overlap Score onto the interval
from 0 to 1, where 0 corresponds to the absence of
membership in the category of sentiment (in our
case, these will be the neutral words) and 1 reflects
the highest degree of membership in this category.
The function can be defined as follows:
S(u;?, ?, ?) =
?
?
?
?
?
?
?
0 for u ? ?
2(u??
???
)2 for? ? u ? ?
1? 2(u??
???
)2 for ? ? u ? ?
1 for u ? ?
where u is the Net Overlap Score for the word
and ?, ?, ? are the three adjustable parameters: ?
is set to 1, ? is set to 15 and ?, which represents a
crossover point, is defined as ? = (? + ?)/2 = 8.
Defined this way, the S-function assigns highest
degree of membership (=1) to words that have the
the Net Overlap Score u ? 15. The accuracy vs.
GI-H4 on this subset is 100%. The accuracy goes
down as the degree of membership decreases and
reaches 59% for values with the lowest degrees of
membership.
5 Discussion and conclusions
This paper contributes to the development of NLP
and semantic tagging systems in several respects.
? The structure of the semantic category of
sentiment. The analysis of the category
of sentiment of English adjectives presented
here suggests that this category is structured
as a fuzzy set: the distance from the core
of the category, as measured by Net Over-
lap scores derived from multiple STEP runs,
is shown to affect both the level of inter-
annotator agreement and the system perfor-
mance vs. human-annotated gold standard.
? The list of sentiment-bearing adjectives. The
list produced and cross-validated by multiple
STEP runs contains 7, 814 positive and neg-
ative English adjectives, with an average ac-
curacy of 66.5%, while the human-annotated
list HM performed at 78.7% accuracy vs.
the gold standard (GI-H4) 8. The remaining
14, 328 adjectives were not identified as sen-
timent marked and therefore were considered
neutral.
The stratification of adjectives by their Net
Overlap Score can serve as an indicator
of their degree of membership in the cate-
gory of (positive/negative) sentiment. Since
low degrees of membership are associated
with greater ambiguity and inter-annotator
disagreement, the Net Overlap Score value
can provide researchers with a set of vol-
ume/accuracy trade-offs. For example, by
including only the adjectives with the Net
Overlap Score of 4 and more, the researcher
can obtain a list of 1, 828 positive and nega-
tive adjectives with accuracy of 81% vs. GI-
H4, or 3, 124 adjectives with 75% accuracy
if the threshold is set at 3. The normalization
of the Net Overlap Score values for the use in
phrase and text-level sentiment tagging sys-
tems was achieved using the fuzzy member-
ship function that we proposed here for the
category of sentiment of English adjectives.
Future work in the direction laid out by this
study will concentrate on two aspects of sys-
tem development. First further incremental
improvements to the precision of the STEP
algorithm will be made to increase the ac-
curacy of sentiment annotation through the
use of adjective-noun combinatorial patterns
within glosses. Second, the resulting list of
adjectives annotated with sentiment and with
the degree of word membership in the cate-
gory (as measured by the Net Overlap Score)
will be used in sentiment tagging of phrases
and texts. This will enable us to compute the
degree of importance of sentiment markers
found in phrases and texts. The availability
8GI-H4 contains 1268 and HM list has 1336 positive and
negative adjectives. The accuracy figures reported here in-
clude the errors produced at the boundary with neutrals.
214
of the information on the degree of central-
ity of words to the category of sentiment may
improve the performance of sentiment deter-
mination systems built to identify the senti-
ment of entire phrases or texts.
? System evaluation considerations. The con-
tribution of this paper to the development
of methodology of system evaluation is two-
fold. First, this research emphasizes the im-
portance of multiple runs on different seed
lists for a more accurate evaluation of senti-
ment tag extraction system performance. We
have shown how significantly the system re-
sults vary, depending on the composition of
the seed list.
Second, due to the high cost of manual an-
notation and other practical considerations,
most bootstrapping and other NLP systems
are evaluated on relatively small manually
annotated gold standards developed for a
given semantic category. The implied as-
sumption is that such a gold standard repre-
sents a random sample drawn from the pop-
ulation of all category members and hence,
system performance observed on this gold
standard can be projected to the whole se-
mantic category. Such extrapolation is not
justified if the category is structured as a lex-
ical field with fuzzy boundaries: in this case
the precision of both machine and human an-
notation is expected to fall when more pe-
ripheral members of the category are pro-
cessed. In this paper, the sentiment-bearing
words identified by the system were stratified
based on their Net Overlap Score and eval-
uated in terms of accuracy of sentiment an-
notation within each stratum. These strata,
derived from Net Overlap scores, reflect the
degree of centrality of a given word to the
semantic category, and, thus, provide greater
assurance that system performance on other
words with the same Net Overlap Score will
be similar to the performance observed on the
intersection of system results with the gold
standard.
? The role of the inter-annotator disagree-
ment. The results of the study presented in
this paper call for reconsideration of the role
of inter-annotator disagreement in the devel-
opment of lists of words manually annotated
with semantic tags. It has been shown here
that the inter-annotator agreement tends to
fall as we proceed from the core of a fuzzy
semantic category to its periphery. There-
fore, the disagreement between the annota-
tors does not necessarily reflect a quality
problem in human annotation, but rather a
structural property of the semantic category.
This suggests that inter-annotator disagree-
ment rates can serve as an important source
of empirical information about the structural
properties of the semantic category and can
help define and validate fuzzy sets of seman-
tic category members for a number of NLP
tasks and applications.
References
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
R.S. Burt. 1980. Models of network structure. Annual
Review of Sociology, 6:79?141.
Philip Edmonds. 1999. Semantic representations of
near-synonyms for automatic lexical choice. Ph.D.
thesis, University of Toronto.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Gregory Grefenstette, Yan Qu, David A. Evans, and
James G. Shanahan. 2004. Validating the Cover-
age of Lexical Resources for Affect Analysis and
Automatically Classifying New Words along Se-
mantic Axes. In Yan Qu, James Shanahan, and
Janyce Wiebe, editors, Exploring Attitude and Af-
fect in Text: Theories and Applications, AAAI-2004
Spring Symposium Series, pages 71?78.
Vasileios Hatzivassiloglou and Kathleen B. McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In 35th ACL, pages 174?181.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In KDD-04, pages 168?
177.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. UsingWordNet to measure
semantic orientation of adjectives. In LREC 2004,
volume IV, pages 1115?1118.
Soo-Min Kim and Edward Hovy. 2004. Determining
the sentiment of opinions. In COLING-2004, pages
1367?1373, Geneva, Switzerland.
215
Adrienne Lehrer. 1974. Semantic Fields and Lexi-
cal Structure. North Holland, Amsterdam and New
York.
Eleanor Rosch. 1978. Principles of Categorization. In
Eleanor Rosch and Barbara B. Lloyd, editors, Cog-
nition and Categorization, pages 28?49. Lawrence
Erlbaum Associates, Hillsdale, New Jersey.
P.J. Stone, D.C. Dumphy, M.S. Smith, and D.M.
Ogilvie. 1966. The General Inquirer: a computer
approach to content analysis. M.I.T. studies in com-
parative politics. M.I.T. Press, Cambridge, MA.
Pero Subasic and Alison Huettner. 2001. Affect Anal-
ysis of Text Using Fuzzy Typing. IEEE-FS, 9:483?
496.
Peter Turney and Michael Littman. 2002. Un-
supervised learning of semantic orientation from
a hundred-billion-word corpus. Technical Report
ERC-1094 (NRC 44929), National Research Coun-
cil of Canada.
Alessandro Valitutti, Carlo Strapparava, and Oliviero
Stock. 2004. Developing Affective Lexical Re-
sources. PsychNology Journal, 2(1):61?83.
Hong Yu and Vassileios Hatzivassiloglou. 2003. To-
wards Answering Opinion Questions: Separating
Facts from Opinions and Identifying the Polarity of
Opinion Sentences. In Conference on Empirical
Methods in Natural Language Processing (EMNLP-
03).
Lotfy A. Zadeh. 1975. Calculus of Fuzzy Restric-
tions. In L.A. Zadeh, K.-S. Fu, K. Tanaka, and
M. Shimura, editors, Fuzzy Sets and their Applica-
tions to cognitive and decision processes, pages 1?
40. Academic Press Inc., New-York.
Lotfy A. Zadeh. 1987. PRUF ? a Meaning Rep-
resentation Language for Natural Languages. In
R.R. Yager, S. Ovchinnikov, R.M. Tong, and H.T.
Nguyen, editors, Fuzzy Sets and Applications: Se-
lected Papers by L.A. Zadeh, pages 499?568. John
Wiley & Sons.
216
Proceedings of ACL-08: HLT, pages 290?298,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
When Specialists and Generalists Work Together: Overcoming Domain
Dependence in Sentiment Tagging
Alina Andreevskaia
Concordia University
Montreal, Quebec
andreev@cs.concordia.ca
Sabine Bergler
Concordia University
Montreal, Canada
bergler@cs.concordia.ca
Abstract
This study presents a novel approach to the
problem of system portability across differ-
ent domains: a sentiment annotation system
that integrates a corpus-based classifier trained
on a small set of annotated in-domain data
and a lexicon-based system trained on Word-
Net. The paper explores the challenges of sys-
tem portability across domains and text gen-
res (movie reviews, news, blogs, and product
reviews), highlights the factors affecting sys-
tem performance on out-of-domain and small-
set in-domain data, and presents a new sys-
tem consisting of the ensemble of two classi-
fiers with precision-based vote weighting, that
provides significant gains in accuracy and re-
call over the corpus-based classifier and the
lexicon-based system taken individually.
1 Introduction
One of the emerging directions in NLP is the de-
velopment of machine learning methods that per-
form well not only on the domain on which they
were trained, but also on other domains, for which
training data is not available or is not sufficient to
ensure adequate machine learning. Many applica-
tions require reliable processing of heterogeneous
corpora, such as the World Wide Web, where the
diversity of genres and domains present in the Inter-
net limits the feasibility of in-domain training. In
this paper, sentiment annotation is defined as the
assignment of positive, negative or neutral senti-
ment values to texts, sentences, and other linguistic
units. Recent experiments assessing system porta-
bility across different domains, conducted by Aue
and Gamon (2005), demonstrated that sentiment an-
notation classifiers trained in one domain do not per-
form well on other domains. A number of methods
has been proposed in order to overcome this system
portability limitation by using out-of-domain data,
unlabelled in-domain corpora or a combination of
in-domain and out-of-domain examples (Aue and
Gamon, 2005; Bai et al, 2005; Drezde et al, 2007;
Tan et al, 2007).
In this paper, we present a novel approach to the
problem of system portability across different do-
mains by developing a sentiment annotation sys-
tem that integrates a corpus-based classifier with
a lexicon-based system trained on WordNet. By
adopting this approach, we sought to develop a
system that relies on both general and domain-
specific knowledge, as humans do when analyzing
a text. The information contained in lexicographi-
cal sources, such as WordNet, reflects a lay person?s
general knowledge about the world, while domain-
specific knowledge can be acquired through classi-
fier training on a small set of in-domain data.
The first part of this paper reviews the extant lit-
erature on domain adaptation in sentiment analy-
sis and highlights promising directions for research.
The second part establishes a baseline for system
evaluation by drawing comparisons of system per-
formance across four different domains/genres -
movie reviews, news, blogs, and product reviews.
The final, third part of the paper presents our sys-
tem, composed of an ensemble of two classifiers ?
one trained on WordNet glosses and synsets and the
other trained on a small in-domain training set.
290
2 Domain Adaptation in Sentiment
Research
Most text-level sentiment classifiers use standard
machine learning techniques to learn and select fea-
tures from labeled corpora. Such approaches work
well in situations where large labeled corpora are
available for training and validation (e.g., movie re-
views), but they do not perform well when training
data is scarce or when it comes from a different do-
main (Aue and Gamon, 2005; Read, 2005), topic
(Read, 2005) or time period (Read, 2005). There are
two alternatives to supervised machine learning that
can be used to get around this problem: on the one
hand, general lists of sentiment clues/features can be
acquired from domain-independent sources such as
dictionaries or the Internet, on the other hand, unsu-
pervised and weakly-supervised approaches can be
used to take advantage of a small number of anno-
tated in-domain examples and/or of unlabelled in-
domain data.
The first approach, using general word lists au-
tomatically acquired from the Internet or from dic-
tionaries, outperforms corpus-based classifiers when
such classifiers use out-of-domain training data or
when the training corpus is not sufficiently large to
accumulate the necessary feature frequency infor-
mation. But such general word lists were shown to
perform worse than statistical models built on suf-
ficiently large in-domain training sets of movie re-
views (Pang et al, 2002). On other domains, such
as product reviews, the performance of systems that
use general word lists is comparable to the perfor-
mance of supervised machine learning approaches
(Gamon and Aue, 2005).
The recognition of major performance deficien-
cies of supervised machine learning methods with
insufficient or out-of-domain training brought about
an increased interest in unsupervised and weakly-
supervised approaches to feature learning. For in-
stance, Aue and Gamon (2005) proposed training
on a samll number of labeled examples and large
quantities of unlabelled in-domain data. This sys-
tem performed well even when compared to sys-
tems trained on a large set of in-domain examples:
on feedback messages from a web survey on knowl-
edge bases, Aue and Gamon report 73.86% accu-
racy using unlabelled data compared to 77.34% for
in-domain and 72.39% for the best out-of-domain
training on a large training set.
Drezde et al (2007) applied structural corre-
spondence learning (Drezde et al, 2007) to the task
of domain adaptation for sentiment classification of
product reviews. They showed that, depending on
the domain, a small number (e.g., 50) of labeled
examples allows to adapt the model learned on an-
other corpus to a new domain. However, they note
that the success of such adaptation and the num-
ber of necessary in-domain examples depends on
the similarity between the original domain and the
new one. Similarly, Tan et al (2007) suggested to
combine out-of-domain labeled examples with unla-
belled ones from the target domain in order to solve
the domain-transfer problem. They applied an out-
of-domain-trained SVM classifier to label examples
from the target domain and then retrained the classi-
fier using these new examples. In order to maximize
the utility of the examples from the target domain,
these examples were selected using Similarity Rank-
ing and Relative Similarity Ranking algorithms (Tan
et al, 2007). Depending on the similarity between
domains, this method brought up to 15% gain com-
pared to the baseline SVM.
Overall, the development of semi-supervised ap-
proaches to sentiment tagging is a promising direc-
tion of the research in this area but so far, based
on reported results, the performance of such meth-
ods is inferior to the supervised approaches with in-
domain training and to the methods that use general
word lists. It also strongly depends on the similarity
between the domains as has been shown by (Drezde
et al, 2007; Tan et al, 2007).
3 Factors Affecting System Performance
The comparison of system performance across dif-
ferent domains involves a number of factors that can
significantly affect system performance ? from train-
ing set size to level of analysis (sentence or entire
document), document domain/genre and many other
factors. In this section we present a series of experi-
ments conducted to assess the effects of different ex-
ternal factors (i.e., factors unrelated to the merits of
the system itself) on system performance in order to
establish the baseline for performance comparisons
across different domains/genres.
291
3.1 Level of Analysis
Research on sentiment annotation is usually con-
ducted at the text (Aue and Gamon, 2005; Pang et
al., 2002; Pang and Lee, 2004; Riloff et al, 2006;
Turney, 2002; Turney and Littman, 2003) or at the
sentence levels (Gamon and Aue, 2005; Hu and Liu,
2004; Kim and Hovy, 2005; Riloff et al, 2006). It
should be noted that each of these levels presents dif-
ferent challenges for sentiment annotation. For ex-
ample, it has been observed that texts often contain
multiple opinions on different topics (Turney, 2002;
Wiebe et al, 2001), which makes assignment of the
overall sentiment to the whole document problem-
atic. On the other hand, each individual sentence
contains a limited number of sentiment clues, which
often negatively affects the accuracy and recall if
that single sentiment clue encountered in the sen-
tence was not learned by the system.
Since the comparison of sentiment annotation
system performance on texts and on sentences
has not been attempted to date, we also sought
to close this gap in the literature by conducting
the first set of our comparative experiments on
data sets of 2,002 movie review texts and 10,662
movie review snippets (5331 with positive and
5331 with negative sentiment) provided by Bo Pang
(http://www.cs.cornell.edu/People/pabo/movie-
review-data/).
3.2 Domain Effects
The second set of our experiments explores system
performance on different domains at sentence level.
For this we used four different data sets of sentences
annotated with sentiment tags:
? A set of movie review snippets (further: movie)
from (Pang and Lee, 2005). This dataset of
10,662 snippets was collected automatically
from www.rottentomatoes.com website. All
sentences in reviews marked ?rotten? were con-
sidered negative and snippets from ?fresh? re-
views were deemed positive. In order to make
the results obtained on this dataset comparable
to other domains, a randomly selected subset of
1066 snippets was used in the experiments.
? A balanced corpus of 800 manually annotated
sentences extracted from 83 newspaper texts
(further, news). The full set of sentences
was annotated by one judge. 200 sentences
from this corpus (100 positive and 100 neg-
ative) were also randomly selected from the
corpus for an inter-annotator agreement study
and were manually annotated by two indepen-
dent annotators. The pairwise agreement be-
tween annotators was calculated as the percent
of same tags divided by the number of sen-
tences with this tag in the gold standard. The
pair-wise agreement between the three anno-
tators ranged from 92.5 to 95.9% (?=0.74 and
0.75 respectively) on positive vs. negative tags.
? A set of sentences taken from personal
weblogs (further, blogs) posted on Live-
Journal (http://www.livejournal.com) and on
http://www.cyberjournalist.com. This corpus
is composed of 800 sentences (400 sentences
with positive and 400 sentences with negative
sentiment). In order to establish the inter-
annotator agreement, two independent judges
were asked to annotate 200 sentences from this
corpus. The agreement between the two an-
notators on positive vs. negative tags reached
99% (?=0.97).
? A set of 1200 product review (PR) sentences
extracted from the annotated corpus made
available by Bing Liu (Hu and Liu, 2004)
(http://www.cs.uic.edu/ liub/FBS/FBS.html).
The data set sizes are summarized in Table 1.
Movies News Blogs PR
Text level 2002 texts n/a n/a n/a
Sentence level 10662 800 800 1200
snippets sent. sent. sent.
Table 1: Datasets
3.3 Establishing a Baseline for a Corpus-based
System (CBS)
Supervised statistical methods have been very suc-
cessful in sentiment tagging of texts: on movie re-
view texts they reach accuracies of 85-90% (Aue
and Gamon, 2005; Pang and Lee, 2004). These
methods perform particularly well when a large vol-
ume of labeled data from the same domain as the
292
test set is available for training (Aue and Gamon,
2005). For this reason, most of the research on senti-
ment tagging using statistical classifiers was limited
to product and movie reviews, where review authors
usually indicate their sentiment in a form of a stan-
dardized score that accompanies the texts of their re-
views.
The lack of sufficient data for training appears to
be the main reason for the virtual absence of exper-
iments with statistical classifiers in sentiment tag-
ging at the sentence level. To our knowledge, the
only work that describes the application of statis-
tical classifiers (SVM) to sentence-level sentiment
classification is (Gamon and Aue, 2005)1. The av-
erage performance of the system on ternary clas-
sification (positive, negative, and neutral) was be-
tween 0.50 and 0.52 for both average precision and
recall. The results reported by (Riloff et al, 2006)
for binary classification of sentences in a related
domain of subjectivity tagging (i.e., the separation
of sentiment-laden from neutral sentences) suggest
that statistical classifiers can perform well on this
task: the authors have reached 74.9% accuracy on
the MPQA corpus (Riloff et al, 2006).
In order to explore the performance of dif-
ferent approaches in sentiment annotation at the
text and sentence levels, we used a basic Na??ve
Bayes classifier. It has been shown that both
Na??ve Bayes and SVMs perform with similar ac-
curacy on different sentiment tagging tasks (Pang
and Lee, 2004). These observations were con-
firmed with our own experiments with SVMs and
Na??ve Bayes (Table 3). We used the Weka pack-
age (http://www.cs.waikato.ac.nz/ml/weka/) with
default settings.
In the sections that follow, we describe a set
of comparative experiments with SVMs and Na??ve
Bayes classifiers (1) on texts and sentences and (2)
on four different domains (movie reviews, news,
blogs, and product reviews). System runs with un-
igrams, bigrams, and trigrams as features and with
different training set sizes are presented.
1Recently, a similar task has been addressed by the Affective
Text Task at SemEval-1 where even shorter units ? headlines
? were classified into positive, negative and neutral categories
using a variety of techniques (Strapparava and Mihalcea, 2007).
4 Experiments
4.1 System Performance on Texts vs. Sentences
The experiments comparing in-domain trained sys-
tem performance on texts vs. sentences were con-
ducted on 2,002 movie review texts and on 10,662
movie review snippets. The results with 10-fold
cross-validation are reported in Table 22.
Trained on Texts Trained on Sent.
Tested on Tested on Tested on Tested on
Texts Sent. Texts Sent.
1gram 81.1 69.0 66.8 77.4
2gram 83.7 68.6 71.2 73.9
3gram 82.5 64.1 70.0 65.4
Table 2: Accuracy of Na??ve Bayes on movie reviews.
Consistent with findings in the literature (Cui et
al., 2006; Dave et al, 2003; Gamon and Aue, 2005),
on the large corpus of movie review texts, the in-
domain-trained system based solely on unigrams
had lower accuracy than the similar system trained
on bigrams. But the trigrams fared slightly worse
than bigrams. On sentences, however, we have ob-
served an inverse pattern: unigrams performed bet-
ter than bigrams and trigrams. These results high-
light a special property of sentence-level annota-
tion: greater sensitivity to sparseness of the model:
On texts, classifier error on one particular sentiment
marker is often compensated by a number of cor-
rectly identified other sentiment clues. Since sen-
tences usually contain a much smaller number of
sentiment clues than texts, sentence-level annota-
tion more readily yields errors when a single sen-
timent clue is incorrectly identified or missed by
the system. Due to lower frequency of higher-order
n-grams (as opposed to unigrams), higher-order n-
gram language models are more sparse, which in-
creases the probability of missing a particular sen-
timent marker in a sentence (Table 33). Very large
2All results are statistically significant at ? = 0.01 with two
exceptions: the difference between trigrams and bigrams for the
system trained and tested on texts is statistically significant at
alpha=0.1 and for the system trained on sentences and tested on
texts is not statistically significant at ? = 0.01.
3The results for movie reviews are lower than those reported
in Table 2 since the dataset is 10 times smaller, which results
in less accurate classification. The statistical significance of the
293
training sets are required to overcome this higher n-
gram sparseness in sentence-level annotation.
Dataset Movie News Blogs PRs
Dataset size 1066 800 800 1200
unigrams
SVM 68.5 61.5 63.85 76.9
NB 60.2 59.5 60.5 74.25
nb features 5410 4544 3615 2832
bigrams
SVM 59.9 63.2 61.5 75.9
NB 57.0 58.4 59.5 67.8
nb features 16286 14633 15182 12951
trigrams
SVM 54.3 55.4 52.7 64.4
NB 53.3 57.0 56.0 69.7
nb features 20837 18738 19847 19132
Table 3: Accuracy of unigram, bigram and trigram mod-
els across domains.
4.2 System Performance on Different Domains
In the second set of experiments we sought to com-
pare system results on sentences using in-domain
and out-of-domain training. Table 4 shows that in-
domain training, as expected, consistently yields su-
perior accuracy than out-of-domain training across
all four datasets: movie reviews (Movies), news,
blogs, and product reviews (PRs). The numbers for
in-domain trained runs are highlighted in bold.
Test Data
Training Data Movies News Blogs PRs
Movies 68.5 55.2 53.2 60.7
News 55.0 61.5 56.25 57.4
Blogs 53.7 49.9 63.85 58.8
PRs 55.8 55.9 56.25 76.9
Table 4: Accuracy of SVM with unigram model
results depends on the genre and size of the n-gram: on prod-
uct reviews, all results are statistically significant at ? = 0.025
level; on movie reviews, the difference between Nav?e Bayes
and SVM is statistically significant at ? = 0.01 but the signif-
icance diminishes as the size of the n-gram increases; on news,
only bi-grams produce a statistically significant (? = 0.01) dif-
ference between the two machine learning methods, while on
blogs the difference between SVMs and Nav?e Bayes is most
pronounced when unigrams are used (? = 0.025).
It is interesting to note that on sentences, regard-
less of the domain used in system training and re-
gardless of the domain used in system testing, un-
igrams tend to perform better than higher-order n-
grams. This observation suggests that, given the
constraints on the size of the available training sets,
unigram-based systems may be better suited for
sentence-level sentiment annotation.
5 Lexicon-Based Approach
The search for a base-learner that can produce great-
est synergies with a classifier trained on small-set
in-domain data has turned our attention to lexicon-
based systems. Since the benefits from combining
classifiers that always make similar decisions is min-
imal, the two (or more) base-learners should com-
plement each other (Alpaydin, 2004). Since a sys-
tem based on a fairly different learning approach
is more likely to produce a different decision un-
der a given set of circumstances, the diversity of
approaches integrated in the ensemble of classifiers
was expected to have a beneficial effect on the over-
all system performance.
A lexicon-based approach capitalizes on the
fact that dictionaries, such as WordNet (Fell-
baum, 1998), contain a comprehensive and domain-
independent set of sentiment clues that exist in
general English. A system trained on such gen-
eral data, therefore, should be less sensitive to do-
main changes. This robustness, however is expected
to come at some cost, since some domain-specific
sentiment clues may not be covered in the dictio-
nary. Our hypothesis was, therefore, that a lexicon-
based system will perform worse than an in-domain
trained classifier but possibly better than a classifier
trained on out-of domain data.
One of the limitations of general lexicons and
dictionaries, such as WordNet (Fellbaum, 1998), as
training sets for sentiment tagging systems is that
they contain only definitions of individual words
and, hence, only unigrams could be effectively
learned from dictionary entries. Since the struc-
ture of WordNet glosses is fairly different from
that of other types of corpora, we developed a sys-
tem that used the list of human-annotated adjec-
tives from (Hatzivassiloglou and McKeown, 1997)
as a seed list and then learned additional unigrams
294
from WordNet synsets and glosses with up to 88%
accuracy, when evaluated against General Inquirer
(Stone et al, 1966) (GI) on the intersection of our
automatically acquired list with GI. In order to ex-
pand the list coverage for our experiments at the text
and sentence levels, we then augmented the list by
adding to it all the words annotated with ?Positiv?
or ?Negativ? tags in GI, that were not picked up by
the system. The resulting list of features contained
11,000 unigrams with the degree of membership in
the category of positive or negative sentiment as-
signed to each of them.
In order to assign the membership score to each
word, we did 58 system runs on unique non-
intersecting seed lists drawn from manually anno-
tated list of positive and negative adjectives from
(Hatzivassiloglou and McKeown, 1997). The 58
runs were then collapsed into a single set of 7,813
unique words. For each word we computed a score
by subtracting the total number of runs assigning
this word a negative sentiment from the total of the
runs that consider it positive. The resulting measure,
termed Net Overlap Score (NOS), reflected the num-
ber of ties linking a given word with other sentiment-
laden words in WordNet, and hence, could be used
as a measure of the words? centrality in the fuzzy
category of sentiment. The NOSs were then normal-
ized into the interval from -1 to +1 using a sigmoid
fuzzy membership function (Zadeh, 1975)4. Only
words with fuzzy membership degree not equal to
zero were retained in the list. The resulting list
contained 10,809 sentiment-bearing words of differ-
ent parts of speech. The sentiment determination at
the sentence and text level was then done by sum-
ming up the scores of all identified positive unigrams
(NOS>0) and all negative unigrams (NOS<0) (An-
dreevskaia and Bergler, 2006).
5.1 Establishing a Baseline for the
Lexicon-Based System (LBS)
The baseline performance of the Lexicon-Based
System (LBS) described above is presented in Ta-
ble 5, along with the performance results of the in-
domain- and out-of-domain-trained SVM classifier.
Table 5 confirms the predicted pattern: the
LBS performs with lower accuracy than in-domain-
4With coefficients: ?=1, ?=15.
Movies News Blogs PRs
LBS 57.5 62.3 63.3 59.3
SVM in-dom. 68.5 61.5 63.85 76.9
SVM out-of-dom. 55.8 55.9 56.25 60.7
Table 5: System accuracy on best runs on sentences
trained corpus-based classifiers, and with similar
or better accuracy than the corpus-based classifiers
trained on out-of-domain data. Thus, the lexicon-
based approach is characterized by a bounded but
stable performance when the system is ported across
domains. These performance characteristics of
corpus-based and lexicon-based approaches prompt
further investigation into the possibility to combine
the portability of dictionary-trained systems with the
accuracy of in-domain trained systems.
6 Integrating the Corpus-based and
Dictionary-based Approaches
The strategy of integration of two or more sys-
tems in a single ensemble of classifiers has been
actively used on different tasks within NLP. In sen-
timent tagging and related areas, Aue and Gamon
(2005) demonstrated that combining classifiers can
be a valuable tool in domain adaptation for senti-
ment analysis. In the ensemble of classifiers, they
used a combination of nine SVM-based classifiers
deployed to learn unigrams, bigrams, and trigrams
on three different domains, while the fourth domain
was used as an evaluation set. Using then an SVM
meta-classifier trained on a small number of target
domain examples to combine the nine base clas-
sifiers, they obtained a statistically significant im-
provement on out-of-domain texts from book re-
views, knowledge-base feedback, and product sup-
port services survey data. No improvement occurred
on movie reviews.
Pang and Lee (2004) applied two different clas-
sifiers to perform sentiment annotation in two se-
quential steps: the first classifier separated subjec-
tive (sentiment-laden) texts from objective (neutral)
ones and then they used the second classifier to clas-
sify the subjective texts into positive and negative.
Das and Chen (2004) used five classifiers to deter-
mine market sentiment on Yahoo! postings. Simple
majority vote was applied to make decisions within
295
the ensemble of classifiers and achieved accuracy of
62% on ternary in-domain classification.
In this study we describe a system that attempts to
combine the portability of a dictionary-trained sys-
tem (LBS) with the accuracy of an in-domain trained
corpus-based system (CBS). The selection of these
two classifiers for this system, thus, was theory-
based. The section that follows describes the classi-
fier integration and presents the performance results
of the system consisting of an ensemble CBS and
LBS classifier and a precision-based vote weighting
procedure.
6.1 The Classifier Integration Procedure and
System Evaluation
The comparative analysis of the corpus-based and
lexicon-based systems described above revealed that
the errors produced by CBS and LBS were to a
great extent complementary (i.e., where one classi-
fier makes an error, the other tends to give the cor-
rect answer). This provided further justification to
the integration of corpus-based and lexicon-based
approaches in a single system.
Table 6 below illustrates the complementarity of
the performance CBS and LBS classifiers on the
positive and negative categories. In this experiment,
the corpus-based classifier was trained on 400 an-
notated product review sentences5. The two systems
were then evaluated on a test set of another 400 prod-
uct review sentences. The results reported in Table 6
are statistically significant at ? = 0.01.
CBS LBS
Precision positives 89.3% 69.3%
Precision negatives 55.5% 81.5%
Pos/Neg Precision 58.0% 72.1%
Table 6: Base-learners? precision and recall on product
reviews on test data.
Table 6 shows that the corpus-based system has a
very good precision on those sentences that it classi-
fies as positive but makes a lot of errors on those sen-
tences that it deems negative. At the same time, the
lexicon-based system has low precision on positives
5The small training set explains relatively low overall per-
formance of the CBS system.
and high precision on negatives6. Such complemen-
tary distribution of errors produced by the two sys-
tems was observed on different data sets from differ-
ent domains, which suggests that the observed dis-
tribution pattern reflects the properties of each of
the classifiers, rather than the specifics of the do-
main/genre.
In order to take advantage of the observed com-
plementarity of the two systems, the following pro-
cedure was used. First, a small set of in-domain
data was used to train the CBS system. Then both
CBS and LBS systems were run separately on the
same training set, and for each classifier, the preci-
sion measures were calculated separately for those
sentences that the classifier considered positive and
those it considered negative. The chance-level per-
formance (50%) was then subtracted from the pre-
cision figures to ensure that the final weights reflect
by how much the classifier?s precision exceeds the
chance level. The resulting chance-adjusted preci-
sion numbers of the two classifiers were then nor-
malized, so that the weights of CBS and LBS clas-
sifiers sum up to 100% on positive and to 100% on
negative sentences. These weights were then used
to adjust the contribution of each classifier to the de-
cision of the ensemble system. The choice of the
weight applied to the classifier decision, thus, varied
depending on whether the classifier scored a given
sentence as positive or as negative. The resulting
system was then tested on a separate test set of sen-
tences7. The small-set training and evaluation exper-
iments with the system were performed on different
domains using 3-fold validation.
The experiments conducted with the Ensemble
system were designed to explore system perfor-
mance under conditions of limited availability of an-
notated data for classifier training. For this reason,
the numbers reported for the corpus-based classifier
do not reflect the full potential of machine learn-
ing approaches when sufficient in-domain training
data is available. Table 7 presents the results of
these experiments by domain/genre. The results
6These results are consistent with an observation in
(Kennedy and Inkpen, 2006), where a lexicon-based system
performed with a better precision on negative than on positive
texts.
7The size of the test set varied in different experiments due
to the availability of annotated data for a particular domain.
296
are statistically significant at ? = 0.01, except the
runs on movie reviews where the difference between
the LBS and Ensemble classifiers was significant at
? = 0.05.
LBS CBS Ensemble
News Acc 67.8 53.2 73.3
F 0.82 0.71 0.85
Movies Acc 54.5 53.5 62.1
F 0.73 0.72 0.77
Blogs Acc 61.2 51.1 70.9
F 0.78 0.69 0.83
PRs Acc 59.5 58.9 78.0
F 0.77 0.75 0.88
Average Acc 60.7 54.2 71.1
F 0.77 0.72 0.83
Table 7: Performance of the ensemble classifier
Table 7 shows that the combination of two classi-
fiers into an ensemble using the weighting technique
described above leads to consistent improvement in
system performance across all domains/genres. In
the ensemble system, the average gain in accuracy
across the four domains was 16.9% relative to CBS
and 10.3% relative to LBS. Moreover, the gain in
accuracy and precision was not offset by decreases
in recall: the net gain in recall was 7.4% relative to
CBS and 13.5% vs. LBS. The ensemble system on
average reached 99.1% recall. The F-measure has
increased from 0.77 and 0.72 for LBS and CBS clas-
sifiers respectively to 0.83 for the whole ensemble
system.
7 Discussion
The development of domain-independent sentiment
determination systems poses a substantial challenge
for researchers in NLP and artificial intelligence.
The results presented in this study suggest that the
integration of two fairly different classifier learning
approaches in a single ensemble of classifiers can
yield substantial gains in system performance on all
measures. The most substantial gains occurred in
recall, accuracy, and F-measure.
This study permits to highlight a set of factors
that enable substantial performance gains with the
ensemble of classifiers approach. Such gains are
most likely when (1) the errors made by the clas-
sifiers are complementary, i.e., where one classifier
makes an error, the other tends to give the correct
answer, (2) the classifier errors are not fully random
and occur more often in a certain segment (or cate-
gory) of classifier results, and (3) there is a way for
a system to identify that low-precision segment and
reduce the weights of that classifier?s results on that
segment accordingly. The two classifiers used in this
study ? corpus-based and lexicon-based ? provided
an interesting illustration of potential performance
gains associated with these three conditions. The
use of precision of classifier results on the positives
and negatives proved to be an effective technique for
classifier vote weighting within the ensemble.
8 Conclusion
This study contributes to the research on sentiment
tagging, domain adaptation, and the development of
ensembles of classifiers (1) by proposing a novel ap-
proach for sentiment determination at sentence level
and delineating the conditions under which great-
est synergies among combined classifiers can be
achieved, (2) by describing a precision-based tech-
nique for assigning differential weights to classifier
results on different categories identified by the clas-
sifier (i.e., categories of positive vs. negative sen-
tences), and (3) by proposing a new method for sen-
timent annotation in situations where the annotated
in-domain data is scarce and insufficient to ensure
adequate performance of the corpus-based classifier,
which still remains the preferred choice when large
volumes of annotated data are available for system
training.
Among the most promising directions for future
research in the direction laid out in this paper is the
deployment of more advanced classifiers and fea-
ture selection techniques that can further enhance
the performance of the ensemble of classifiers. The
precision-based vote weighting technique may prove
to be effective also in situations, where more than
two classifiers are integrated into a single system.
We expect that these more advanced ensemble-of-
classifiers systems would inherit the benefits of mul-
tiple complementary approaches to sentiment anno-
tation and will be able to achieve better and more
stable accuracy on in-domain, as well as on out-of-
domain data.
297
References
Ethem Alpaydin. 2004. Introduction to Machine Learn-
ing. The MIT Press, Cambridge, MA.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
WordNet for a fuzzy sentiment: Sentiment tag extrac-
tion from WordNet glosses. In Proceedings the 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, Trento, IT.
Anthony Aue and Michael Gamon. 2005. Customizing
sentiment classifiers to new domains: a case study. In
Proccedings of the International Conference on Recent
Advances in Natural Language Processing, Borovets,
BG.
Xue Bai, Rema Padman, and Edoardo Airoldi. 2005. On
learning parsimonious models for extracting consumer
opinions. In Proceedings of the 38th Annual Hawaii
International Conference on System Sciences, Wash-
ington, DC.
Hang Cui, Vibhu Mittal, and Mayur Datar. 2006. Com-
parative experiments on sentiment classification for
online product reviews. In Proceedings of the 21st
International Conference on Artificial Intelligence,
Boston, MA.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the Peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW03, Budapest, HU.
Mark Drezde, John Blitzer, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, Prague, CZ.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Michael Gamon and Anthony Aue. 2005. Automatic
identification of sentiment vocabulary: exploiting low
association with known sentiment terms. In Proceed-
ings of the ACL-05 Workshop on Feature Engineering
for Machine Learning in Natural Language Process-
ing, Ann Arbor, US.
Vasileios Hatzivassiloglou and Kathleen B. McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the the 40th Annual Meeting
of the Association of Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD-04, pages 168?177.
Alistair Kennedy and Diana Inkpen. 2006. Senti-
ment Classification of Movie Reviews Using Con-
textual Valence Shifters. Computational Intelligence,
22(2):110?125.
Soo-Min Kim and Eduard Hovy. 2005. Automatic detec-
tion of opinion bearing words and sentences. In Pro-
ceedings of the Second International Joint Conference
on Natural Language Processing, Companion Volume,
Jeju Island, KR.
Bo Pang and Lilian Lee. 2004. A sentiment education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational Linguis-
tics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the 43nd
Meeting of the Association for Computational Linguis-
tics, Ann Arbor, US.
Bo Pang, Lilian Lee, and Shrivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Conference on Empiri-
cal Methods in Natural Language Processing.
Jonathon Read. 2005. Using emoticons to reduce depen-
dency in machine learning techniques for sentiment
classification. In Proceedings of the ACL-2005 Stu-
dent Research Workshop, Ann Arbor, MI.
Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe.
2006. Feature subsumption for opinion analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, Sydney, AUS.
P.J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie.
1966. The General Inquirer: a computer approach to
content analysis. M.I.T. studies in comparative poli-
tics. M.I.T. Press, Cambridge, MA.
Carlo Strapparava and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text. In Proceedings of the
4th International Workshop on Semantic Evaluations,
Prague, CZ.
Songbo Tan, Gaowei Wu, Huifeng Tang, and Zueqi
Cheng. 2007. A Novel Scheme for Domain-transfer
Problem in the context of Sentiment Analysis. In Pro-
ceedings of CIKM 2007.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: inference of semantic orientation
from association. ACM Transactions on Information
Systems (TOIS), 21:315?346.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
Meeting of the Association of Computational Linguis-
tics.
Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie
Martin, and Theresa Wilson. 2001. A corpus study of
Evaluative and Speculative Language. In Proceedings
of the 2nd ACL SIGDial Workshop on Discourse and
Dialogue, Aalberg, DK.
Lotfy A. Zadeh. 1975. Calculus of Fuzzy Restrictions.
In L.A. Zadeh et al, editor, Fuzzy Sets and their Ap-
plications to cognitive and decision processes, pages
1?40. Academic Press Inc., New-York.
298
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 82?89,
New York City, June 2006. c?2006 Association for Computational Linguistics
Postnominal Prepositional Phrase Attachment in Proteomics
Jonathan Schuman and Sabine Bergler
The CLaC Laboratory
Department of Computer Science and Software Engineering
Concordia University, Montreal, Canada
{j schuma,bergler}@cs.concordia.ca
Abstract
We present a small set of attachment
heuristics for postnominal PPs occurring
in full-text articles related to enzymes.
A detailed analysis of the results sug-
gests their utility for extraction of rela-
tions expressed by nominalizations (often
with several attached PPs). The system
achieves 82% accuracy on a manually an-
notated test corpus of over 3000 PPs from
varied biomedical texts.
1 Introduction
The biomedical sciences suffer from an overwhelm-
ing volume of information that is growing at explo-
sive rates. Most of this information is found only
in the form of published literature. Given the large
volume, it is becoming increasingly difficult for re-
searchers to find relevant information. Accordingly,
there is much to be gained from the development of
robust and reliable tools to automate this task.
Current systems in this domain focus primarily
on abstracts. Though the salient points of an article
are present in the abstract, much detailed informa-
tion is entirely absent and can be found only in the
full text (Shatkay and Feldman, 2003; Corney et al,
2004). Optimal conditions for enzymatic activity,
details of experimental procedures, and useful ob-
servations that are tangential to the main point of the
article are just a few examples of such information.
Full-text articles in enzymology are characterized
by many complex noun phrases (NPs), usually with
chains of several prepositional phrases (PPs). Nom-
inalized relations are particularly frequent, with ar-
guments and adjuncts mentioned in attached PPs.
Thus, the tasks of automated search, retrieval, and
extraction in this domain stand to benefit signifi-
cantly from efforts in semantic interpretation of NPs
and PPs.
There are currently no publicly available biomed-
ical corpora suitable for this task. (See (Cohen et al,
2005) for an overview of currently available biomed-
ical corpora.) Therefore, statistical approaches that
rely on extensive training data are essentially not
feasible. Instead, we approach the task through care-
ful analysis of the data and development of heuris-
tics. In this paper, we report on a rule-based post-
nominal PP attachment system developed as a first
step toward a more general NP semantics for pro-
teomics.
2 Background
Leroy et al (2002; 2003) note the importance of
noun phrases and prepositions in the capture of rela-
tional information in biomedical texts, citing the par-
ticular significance of the prepositions by, of, and in.
Their parser can extract many different relations us-
ing few rules by relying on closed-class words (e.g.
prepositions) instead of restricting patterns with spe-
cific predefined verbs and entities. This bottom-
up approach achieves high precision (90%) and a
claimed (though unquantified) high recall. However,
they side-step the issue of prepositional attachment
ambiguity altogether. Also, their system is targeted
specifically and only toward relations. While rela-
tions do cover a considerable portion of the most rel-
evant information in biomedical texts, there is also
much relevant lower frequency information (partic-
ularly in enzymology) such as the conditions under
which these relations are expressed.
82
Hahn et al (2002) point out that PPs are crucial
for semantic interpretation of biomedical texts due
to the wide variety of conceptual relations they in-
troduce. They note that this is reflected in their
training and test data, extracted from findings re-
ports in histopathology, where prepositions account
for about 10% of all words and more than 25% of
the text is contained in PPs. The coverage of PPs in
our development and test data, comprised of varied
texts in proteomics, is even higher with 26% of the
text occurring in postnominal PPs alone.
Little research in the biomedical domain ad-
dresses the problem of PP attachment proper. This
is partly due to the number of systems that pro-
cess text using named-entity-based templates, dis-
regarding PPs. In fact, the only recent BioNLP sys-
tem found in the literature that makes any mention
of PP attachment is Medstract (Pustejovsky et al,
2002), an automated information extraction system
for Medline abstracts. The shallow parsing module
used in Medstract performs ?limited? prepositional
attachment?only of prepositions are attached.
There are, of course, several PP attachment sys-
tems for other domains. Volk (2001) addresses PP
attachment using the frequency of co-occurrence of
a PP?s preposition, object NP, and possible attach-
ment points, calculated from query results of a web-
based search engine. This system was evaluated
on sentences from a weekly computer magazine,
scoring 74% accuracy for both VP and NP attach-
ment. Brill & Resnik (1994) put transformation-
based learning with added word-class information
from WordNet to the task of PP attachment. Their
system achieves 81.8% accuracy on sentences from
the Penn Treebank Wall Street Journal corpus.
The main concerns of both these systems differ
from the requirements for successful PP attachment
in proteomics. The main attachment ambiguity in
these general texts is between VP and NP attach-
ment, where there are few NPs to choose from for a
given PP. In contrast, proteomics texts, where NPs
are the main information carriers, contain many NPs
with long sequences of postnominal PPs. Conse-
quently, the possible attachment points for a given
PP are more numerous. By ?postnominal?, we de-
note PPs following an NP, where the attachment
point may be within the NP but may also precede
it. In focusing on postnominal PPs, we exclude here
PPs that trivially attach to the VP for lack of NP at-
tachment points and focus on the subset of PPs with
the highest degree of attachment ambiguity.
3 Approach
For this exploratory study we compiled two manu-
ally annotated corpora1 , a smaller, targeted devel-
opment corpus consisting of sentences referring to
enzymes in five articles, and a larger test corpus con-
sisting of the full text of nine articles drawn from a
wider set of topics. This bias in the data was set de-
liberately to test whether NPs referring to enzymes
follow a distinct pattern. Our results suggest that
the compiled heuristics are in fact not specific to en-
zymes, but work with comparable performance for a
much wider set of NPs.
As our goal is semantic interpretation of NPs,
only postnominal PPs were considered. A large
number of these follow a very simple attachment
principle?right association.
Right association (Kimball, 1973), or late clo-
sure, describes a preference for parses that result in
the parse tree with the most right branches. Sim-
ply stated, right association assumes that new con-
stituents are part of the closest possible constituent
that is under construction. In the case of postnomi-
nal PPs, right association attaches each PP to the NP
that immediately precedes it. An example where this
strategy does fairly well is given below.
The effect of hydrolysis of the hemicelluloses in the
milled wood lignin on the molecular mass distribu-
tion was then examined. . .
Notice that, except for the last PP, attachment to the
preceding NP is correct. The last PP, on the molecu-
lar mass distribution, modifies the head NP effect.
Another frequent pattern in our corpus is given
below with a corresponding text fragment. In this
pattern, the entire NP consists of one reaction fully
described by several PPs that all attach to a nominal-
ization in the head NP. Attachment according to this
pattern is in direct opposition to right association.
<ACTION> <PREPOSITION> <PRODUCT>
<PREPOSITION> <SUBSTRATE>
<PREPOSITION> <ENZYME>
<PREPOSITION> <MEASUREMENT>
1There was a single annotator for both corpora, who was
also the developer of the heuristics.
83
. . . the release of reducing sugars from car-
boxymethylcellulose by cellulase at 37 oC, pH
4.8. . .
In general, the attachment behavior of a large per-
centage of PPs in the examined literature can be
characterized by either right association or attach-
ment to a nominalization. The preposition of a PP
seems to be the main criterion for determining which
attachment principle to apply. A few prepositions
were observed to follow right association almost ex-
clusively, while others show a strong affinity toward
nominalizations, defaulting to right association only
when no nominalization is available.
These observations were implemented as attach-
ment heuristics for the most frequently occurring
PPs, as distinguished by their prepositions (see Ta-
ble 1 for frequency data). These rules, as outlined
below, account for 90% of all postnominal PPs in
the corpus. The remaining 10%, for which no clear
pattern could be found, are attached using right as-
sociation.
Devel. Corpus Test Corpus
Prep Freq Syst Base Freq Syst Base
of 50.0 99.0 99.0 53.4 98.2 98.2
in 11.9 74.8 55.6 11.7 67.0 54.6
from 8.3 87.0 87.0 3.67 71.8 71.8
for 4.5 81.1 81.0 5.1 56.1 56.0
with 4.5 83.8 75.7 4.7 70.8 65.2
between 4.2 68.6 68.6 1.2 84.2 84.2
at 3.3 81.5 18.5 4.0 68.3 40.7
on 3.1 84.6 57.7 2.1 80.0 53.9
by 2.5 95.2 23.8 2.4 76.7 45.2
to 2.3 63.2 63.2 5.0 51.6 51.6
as 1.8 66.7 46.7 0.7 40.9 36.4
Table 1: Frequency of prepositions with correspond-
ing PP attachment accuracy for the implemented
heuristics and the baseline (right association) on de-
velopment and test set.
Right Association (of, from, for)
PPs headed by of, from, and for attach almost exclu-
sively according to right association. In particular,
no violation of right association by of PPs has been
found. The system, therefore, attaches any PP from
this class to the NP immediately preceding it.
Strong Nominalization Affinity (by, at)
In contrast, by and at PPs attach almost exclusively
to nominalizations. Only rarely have they been ob-
served to attach to non-nominalization NPs. In most
cases where no nominalizations are present in the
NP, a PP of this class actually attaches to a preced-
ing VP. Typical nominalization and VP attachments
found in the corpus are exemplified in the following
two sentences.
. . . the formation of stalk cells by culB? pkaR?
cells decreased about threefold. . .
. . . xylooligosaccharides were not detected in hy-
drolytic products from corn cell walls by TLC
analysis.
This attachment preference is implemented in the
system as the heuristic for strong nominalization
affinity. Given a PP from this class, the system first
attempts attachment to the closest nominalization to
the left. If no such NP is found, the PP is assumed
to attach to a VP.
Weak Nominalization Affinity (in, with, as)
In, with, and as PPs show similar affinity toward
nominalizations. In fact, initially, these PPs were
attached with the strong affinity heuristic. How-
ever, after further observation it became apparent
that these PPs do often attach to non-nominalization
NPs. A typical example for each of these possibili-
ties is given as follows.
. . . incubation of the substrate pullulan with protein
fractions.
The major form of beta-amylase in Arabidopsis. . .
Here, the system first attempts nominalization at-
tachment. If no nominalizations are present in the
NP, instead of defaulting to VP attachment, the PP
is attached to the closest NP to its left that is not
the object of an of PP. This behavior is intuitively
consistent since in PPs are usually adjuncts to the
main NP (which is usually an entity if not a nom-
inalization) and are unlikely to modify any of the
NP?s modifiers.
?Effect on?
The final heuristic encodes the frequent attachment
of on PPs with NPs indicating effect, influence, im-
pact, etc. While this relationship seems intuitive and
likely to occur in varied texts, it may be dispropor-
tionally frequent in proteomics texts. Nonetheless,
the heuristic does have a strong basis in the exam-
ined literature. An example is provided below.
84
. . . the effects of reduced ?-amylase activity on seed
formation and germination. . .
The system checks NPs preceding an on PP for the
closest occurrence of an ?effect? NP. If no such NPs
are found, right association is used.
4 System Overview
There are three main phases of processing that must
occur before the PP attachment heuristics can be ap-
plied. These include preprocessing and two stages
of NP chunking. Upon completion of these three
phases, the PP attachment module is executed.
The preprocessing phase consists of standard to-
kenization and part-of-speech tagging, as well as
named entity recognition (and other term lookup)
using gazetteer lists and simple transducers. Recog-
nition is currently limited to enzymes, organisms,
chemicals, (enzymological) activities, and measure-
ments. A comprehensive enzyme list including syn-
onyms was compiled from BRENDA2 and some
limited organism lists3, including common abbrevi-
ations, were augmented based on organisms found
in the development corpus. For recognition of sub-
strates and products, some of the chemical entity
lists from BioRAT (Corney et al, 2004) are used.
Activity lists from BioRAT, with several enzyme-
specific additions, are also used.
The next phase of processing uses a chunker re-
ported in (Bergler et al, 2003) and further developed
for a related project. NP chunking is performed in
two stages, using two separate context-free gram-
mars and an Earley-type chart parser. No domain-
specific information is used in either of the gram-
mars; recognized entities and terms are used only for
improved tokenization. The first stage chunks base
NPs, without attachments. Here, the parser input
is segmented into smaller sentence fragments to re-
duce ambiguity and processing time. The fragments
are delimited by verbs, prepositions, and sentence
boundaries, since none of these can occur within a
base NP. In the second chunking stage, entire sen-
tences are parsed to extract NPs containing conjunc-
tions and PP attachments. At this stage, no attempt
is made to determine the proper attachment structure
of the PPs or to exclude postnominal PPs that should
2http://www.brenda.uni-koeln.de
3Compiled for a related project.
actually be attached to a preceding VP?any PP that
follows an NP has the potential to attach somewhere
in the NP.
The final phase of processing is performed by the
PP attachment module. Here, each postnominal PP
is examined and attached according to the rule for its
preposition. Only base NPs within the same NP are
considered as possible attachment points. For the
strong nominalization affinity heuristic, if no nomi-
nalization is found, the PP is assumed to attach to the
closest preceding VP. For both nominalization affin-
ity heuristics, the UMLS SPECIALIST Lexicon4 is
used to determine whether the head noun of each
possible attachment point is a nominalization.
5 Results & Analysis
The development corpus was compiled from five ar-
ticles retrieved from PubMed Central5 (PMC). The
articles were the top-ranked results returned from
five separate queries6 using BioKI:Enzymes, a lit-
erature navigation tool (Bergler et al, 2006). Sen-
tences containing enzymes were extracted and the
remaining sentences were discarded. In total, 476
sentences yielding 830 postnominal PPs were man-
ually annotated as the development corpus.
Attachment accuracy on the development corpus
is 88%. The accuracy and coverage of each rule is
summarized in Table 2 and discussed in the follow-
ing sections. Also, as a reference point for perfor-
mance comparison, the system was tested using only
the right association heuristic resulting in a baseline
accuracy of 80%. The system performance is con-
trasted with the baseline and summarized for each
preposition in Table 1.
Devel. Corpus Test Corpus
Heuristic Freq Accuracy Freq Accuracy
Right Association 62.8 96.2 62.1 93.3
Weak NA 18.2 76.2 17.1 67.0
Strong NA 5.8 87.5 6.4 71.4
?Effect on? 3.1 84.6 2.1 80.0
Default (RA) 10.1 60.7 12.3 49.5
Table 2: Coverage and accuracy of each heuristic.
4http://www.nlm.nih.gov/research/umls/
5http://www.pubmedcentral.com
6Amylase, CGTase, pullulanase, ferulic acid esterase, and
cellwallase were used as the PMC search terms and a list of
different enzymes was used for scoring.
85
To measure heuristic performance, the PP attach-
ment heuristics were scored on manual NP and PP
annotations. Thus all reported accuracy numbers re-
flect performance of the heuristics alone, isolated
from possible chunking errors. The PP attachment
module is, however, designed for input from the
chunker and does not handle constructs which the
chunker does not provide (e.g. PP conjunctions and
non-simple parenthetical NPs).
5.1 Right Association
The application of right association for PPs headed
by of, for, and from resulted in correct attachment in
96.2% of their occurrences in the development cor-
pus. Because this class of PPs is processed using
the baseline heuristic without any refinements, it has
no effect on overall system accuracy as compared to
overall baseline accuracy. However, it does provide
a clear delineation of the subset of PPs for which
right association is a sufficient and optimal solution
for attachment. Given the coverage of this class of
PPs (62.8% of the corpus), it also provides an expla-
nation for the relatively high baseline performance.
Of PPs are attached with 99% accuracy.
All errors involve attachment of PP conjunc-
tions, such as ?. . . a search of the literature
and of the GenBank database. . . ?, or attachment
to NPs containing non-simple parenthetical state-
ments, such as ?The synergy degree (the activi-
ties of XynA and cellulase cellulosome mixtures di-
vided by the corresponding theoretical activities)
of cellulase. . . ?. Sentences of these forms are not
accounted for in the NP chunker, around which the
PP attachment system was designed. Both scenarios
reflect shortcomings in the NP grammars, not in the
heuristic.
For and from PPs are attached with 81% and 87%
accuracy, respectively. The majority of the error
here corresponds to PPs that should be attached to a
VP. For example, attachment errors occurred both in
the sentence ?. . . this was followed by exoglucanases
liberating cellobiose from these nicks. . . ? and in the
sentence ?. . . the reactions were stopped by placing
the microtubes in boiling water for 2 to 3 min.?
5.2 Strong Nominalization Affinity
The heuristic for strong nominalization affinity deals
with only two types of PPs, those headed by the
prepositions by and at, both of which occur with
relatively low frequency in the development corpus.
Accordingly, the heuristic?s impact on the overall ac-
curacy of the system is rather small. However, it af-
fords the largest increase in accuracy for the PPs of
its class. The heuristic correctly determines attach-
ment with 87.5% accuracy.
While these PPs account for a small portion of
the corpus, they play a critical role in describing
enzymological information. Specifically, by PPs
are most often used in the description of relation-
ships between entities, as in the NP ?degradation
of xylan networks between cellulose microbrils
by xylanases?, while at PPs often quantitatively in-
dicate the condition under which observed behavior
or experiments take place, as in the NP ?Incubation
of the enzyme at 40 oC and pH 9.0?.
The heuristic provides a strong performance in-
crease over the baseline, correctly attaching 95.2%
of by PPs in contrast to 23.8% with the baseline. In
fact, only a single error occurred in attaching by PPs
in the development corpus and the sentence in ques-
tion, given below, appears to be ungrammatical in all
of its possible interpretations.
The TLC pattern of liberated cellooligosaccharides
by mixtures of XynA cellulosomes and cellulase cel-
lulosomes was similar to that caused by cellulase
cellulosomes alone.
A few other errors (e.g. typos, omission of words,
and grammatically incorrect or ambiguous con-
structs) were observed in the development corpus.
The extent of such errors and the degree to which
they affect the results (either negatively or posi-
tively) is unknown. However, such errors are in-
escapable and any automated system is susceptible
to their effects.
Although no errors in by PP attachment were
found in the development corpus, aside from the
given problematic sentence, one that would be pro-
cessed erroneously by the system was found manu-
ally in the GENIA Treebank7. It is given below to
demonstrate a boundary case for this heuristic.
. . . modulation of activity in B cells by human T-cell
leukemia virus type I tax gene. . .
Here, the system would attach the by PP to the clos-
est nominalization activity, when in fact, the cor-
7http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
86
rect attachment is to the nominalization modulation.
This error scenario is relevant to all of the PPs with
nominalization affinity. A possible solution is to
separate general nominalizations, such as activity
and action, from more specific ones, such as mod-
ulation, and to favor the latter type whenever possi-
ble. An experiment toward this end, with emphasis
on in PPs, was performed with promising results. It
is discussed in the following section.
For at PPs, 81.5% accuracy was achieved, as com-
pared to 18.5% with the baseline. The higher de-
gree of error with at PPs is indicative of their more
varied usage, requiring more contextual information
for correct attachment. An example of typical vari-
ation is given in the following two sentences, both
of which contain at PPs that the system incorrectly
attached to the nominalization activity.
The amylase exhibited maximal activity at pH 8.7
and 55 oC in the presence of 2.5 M NaCl.
. . . Bacillus sp. strain IMD370 produced alkaline
?-amylases with maxima for activity at pH 10.0.
While both sentences report observed conditions for
maximal enzyme activity using similar language, the
attachment of the at PPs differs between them. In the
first sentence, the activity was exhibited at the given
pH and temperature (VP attachment), but in the sec-
ond sentence, the enzyme was not necessarily pro-
duced at the given pH (NP attachment)?production
may have occurred under different conditions from
those reported for the activity maxima.
For errors of this nature, it seems that employing
semantic information about the preceding VP and
possibly also the head NP would lead to more ac-
curate attachment. There are, however, other similar
errors where even the addition of such information
does not immediately suggest the proper attachment.
5.3 Weak Nominalization Affinity
The weak nominalization affinity heuristic covers a
large portion of the development corpus (18.2%).
Overall system improvement over baseline attach-
ment accuracy can be achieved through successful
attachment of this class of PPs, particularly in and
with PPs, which are the second and fourth most fre-
quently used PPs in the development corpus, respec-
tively. Unfortunately, the usage of these PPs is also
perhaps the hardest to characterize. The heuristic
achieves only 76.2% accuracy. Though noticeably
better than right association alone, it is apparent that
the behavior of this class of PPs cannot be entirely
characterized by nominalization affinity.
Accuracy of in PP attachment increased by 19.2%
from the baseline with this heuristic. A significant
source of attachment error is the problem of mul-
tiple nominalizations in the same NP. As men-
tioned above, splitting nominalizations into general
and specific classes may solve this problem. To ex-
plore this conjecture, the most common (particularly
with in PPs) general nominalization, activity, was
ignored when searching for nominalization attach-
ment points. This resulted in a 3% increase in the
accuracy for in PPs with no adverse effects on any
of the other PPs with nominalization affinity.
Despite further anticipated improvements from
similar changes, attachment of in PPs stands to ben-
efit the most from additional semantic information in
the form of rules that encode containment semantics
(i.e. which types of things can be contained in other
types of things). Possible containment rules exist
for the few semantic categories that are already im-
plemented; enzymes, for instance, can be contained
in organisms, but organisms are rarely contained in
anything (though organisms can be said to be con-
tained in their species, the relationship is rarely ex-
pressed as containment). Further analysis and more
semantic categories are needed to formulate more
generally applicable rules.
With and as PPs are attached with 83.8% and
66.7% accuracy, respectively. All of the errors for
these PPs involve incorrect attachment to an NP
when the correct attachment is to a VP. Presented
below are two sentences that provide examples of
the particular difficulty of resolving these errors.
The xylanase A . . . was expressed by E. coli
with a C-terminal His tag from the vector pET-
29b. . .
The pullulanase-type activity was identified as
ZPU1 and the isoamylase-type activity as SU1.
In the first sentence, the with PP describes the
method by which xylanase A was expressed; it does
not restrict the organism in which the expression
occurred. This distinction requires understanding
the semantic relationship between C-terminal His
tags, protein (or enzyme) expression, and E. coli.
Namely, that His tags (polyhistidine-tags) are amino
87
acid motifs used for purification of proteins, specif-
ically proteins expressed in E. coli. Such informa-
tion could only be obtained from a highly domain-
specific knowledge source. In the second sentence,
the verb to which the as PP attaches is omitted. Ac-
cordingly, even if the semantics of verbs were used
to help determine attachment, the system would
need to recognize the ellipsis for correct attachment.
5.4 ?Effect on? Heuristic
The attachment accuracy for on PPs is 84.6% using
the ?effect on? heuristic, a noticeable improvement
over the 57.7% accuracy of the baseline. The few at-
tachment errors for on PPs were varied and revealed
no regularities suggesting future improvements.
5.5 Unclassified PPs
The remaining PPs, for which no heurisitics were
implemented, represent 10% of the development
corpus. The system attaches these PPs using right
association, with accuracy of 60.7%. Most frequent
are PPs headed by between, which are attached with
68.6% accuracy. A significant improvement is ex-
pected from a heuristic that attaches these PPs based
on observations of semantic features in the corpus.
Namely, that most of the NPs to which between PPs
attach can be categorized as binary relations (e.g.
bond, linkage, difference, synergy). This relational
feature can be expressed in the head noun or in a
prenominal modifier. In fact, more than 25% of be-
tween PPs in the development corpus attach to the
NP synergistic effects (or some similar alternative),
where between shows affinity toward the adjective
synergistic, not the head noun effects, which does
not attract between PP attachment on its own.
6 Evaluation on Varied Texts
To assess the general applicability of the heuristics
to varied texts, the system was evaluated on a test
corpus of an additional nine articles8 from PMC.
The entire text, except the abstract and introduc-
tion, of each article was manually annotated, result-
ing in 1603 sentences with 3079 postnominal PPs.
The system?s overall attachment accuracy on this
8PMC query terms: metabolism, biosynthesis, proteolysis,
peptidyltransferase, hexokinase, epimerase, laccase, ligase, de-
hydrogenase.
test data is 82%, comparable to that for the develop-
ment enzymology data. The accuracy and coverage
of each rule for the test data, as contrasted with the
development set, is given in Table 2. The baseline
heuristic achieved an accuracy of 77.5%. A com-
parative performance breakdown by preposition is
given in Table 1.
Overall, changes in the coverage and accuracy of
the heuristics are much less pronounced than ex-
pected from the increase in size and variance of both
subject matter and writing style between the devel-
opment and test data. The only significant change
in rule coverage is a slight increase in the number of
unclassified PPs to 12.3%. These PPs are also more
varied and the right-associative default heuristic is
less applicable (49.5% accuracy in the test data vs.
60.7% in the development data). The largest contri-
bution to this additional error stems from a doubling
of the frequency of to PPs in the test corpus. Prelim-
inary analysis of the corresponding errors suggests
that these PPs would be much better suited to the
strong nominalization affinity heuristic than the right
association default. The error incurred over all un-
classified PPs accounts for 1.4% of the accuracy dif-
ference between the development and test data. The
larger number of these PPs also explains the smaller
overall difference between the system and baseline
performance.
For PPs were observed to have more frequent VP
attachment in the test data. In particular, for PPs
with object NPs specifying a duration (or other mea-
surement), as exemplified below, attach almost ex-
clusively to VPs and nominalizations.
The sample was spun in a microfuge for 10 min. . .
This behavior is also apparent in the development
data, though in much smaller numbers. Applying the
strong nominalization affinity heuristic to these PPs
resulted in an increase of for PP attachment accuracy
in the test corpus to 75.8% and an overall increase in
accuracy of 1.0%.
A similar pattern was observed for at PPs, where
the pattern <CHEMICAL> at <CONCENTRATION> ac-
counts for 25.6% of all at PP attachment errors and
the majority of the performance decrease for the
strong nominalization affinity heuristic between the
two data sets. The remainder of the performance de-
crease for this heuristic is attributed to gaps in the
88
UMLS SPECIALIST Lexicon. For instance, the un-
derlined head nouns in the following examples are
not marked as nominalizations in the lexicon.
The double mutant inhibited misreading by paro-
momycin . . .
. . . the formation of stalk cells by culB? pkaR?
cells. . .
In our test corpus, these errors were only apparent
in by PP attachment, but can potentially affect all
nominalization-based attachment.
Aside from the cases mentioned in this section,
attachment trends in the test corpus are quite similar
to those observed in the development corpus. Given
the diversity in the test data, both in terms of subject
matter (between articles) and writing style (between
sections), the results suggest the suitability of our
heuristics to proteomics texts in general.
7 Conclusion
The next step for BioNLP is to process the full text
of scientific articles, where heavy NPs with poten-
tially long chains of PP attachments are frequent.
This study has investigated the attachment behav-
ior of postnominal PPs in enzyme-related texts and
evaluated a small set of simple attachment heuris-
tics on a test set of over 3000 PPs from a collec-
tion of more varied texts in proteomics. The heuris-
tics cover all prepositions, even infrequent ones,
that nonetheless convey important information. This
approach requires only NP chunked input and a
nominalization dictionary, all readily available from
on-line resources. The heuristics are thus useful
for shallow approaches and their accuracy of 82%
puts them in a position to reliably improve both,
proper recognition of entities and their properties
and bottom-up recognition of relationships between
entities expressed in nominalizations.
References
Sabine Bergler, Rene? Witte, Michelle Khalife?, Zhuoyan
Li, and Frank Rudzicz. 2003. Using knowledge-
poor coreference resolution for text summarization. In
On-line Proceedings of the Workshop on Text Summa-
rization, Document Understanding Conference (DUC
2003), Edmonton, Canada, May.
Sabine Bergler, Jonathan Schuman, Julien Dubuc, and
Alexandr Lebedev. 2006. BioKI:Enzymes - an
adaptable system to locate low-frequency informa-
tion in full-text proteomics articles. Poster abstract
in Proceedings of the HLT-NAACL Workshop on
Linking Natural Language Processing and Biology
(BioNLP?06), New York, NY, June.
Eric Brill and Philip Resnik. 1994. A rule-based ap-
proach to prepositional phrase attachment disambigua-
tion. In Proceedings of the 15th International Confer-
ence on Computational Linguistics (COLING-94).
Kevin Bretonnel Cohen, Lynne Fox, Philip V. Ogren, and
Lawrence Hunter. 2005. Corpus design for biomed-
ical natural language processing. In Proceedings of
the ACL-ISMB Workshop on Linking Biological Lit-
erature, Ontologies and Databases (BioLINK), pages
38?45, Detroit, MI, June. Association for Computa-
tional Linguistics.
David P.A. Corney, Bernard F. Buxton, William B. Lang-
don, and David T. Jones. 2004. BioRAT: extracting
biological information from full-length papers. Bioin-
formatics, 20(17):3206?3213.
Udo Hahn, Martin Romacker, and Stefan Schulz. 2002.
Creating knowledge repositories from biomedical re-
ports: the MEDSYNDIKATE text mining system. In
Proceedings of the 7th Pacific Symposium on Biocom-
puting, pages 338?49, Hawaii, USA.
John Kimball. 1973. Seven principles of surface struc-
ture parsing in natural language. Cognition, 2:15?47.
Gondy Leroy and Hsinchun Chen. 2002. Filling
preposition-based templates to capture information
from medical abstracts. In Proceedings of the 7th
Pacific Symposium on Biocomputing, pages 350?361,
Hawaii, USA.
Gondy Leroy, Hsinchun Chen, and Jesse D. Martinez.
2003. A shallow parser based on closed-class words
to capture relations in biomedical text. Journal of
Biomedical Informatics, 36:145?158, June.
James Pustejovsky, Jose? Castan?o, Roser Sauri, Anna
Rumshisky, Jason Zhang, and Wei Luo. 2002. Med-
stract: Creating large-scale information servers for
biomedical libraries. In ACL 2002 Workshop on Nat-
ural Language Processing in the Biomedical Domain,
Philadelphia, PA.
Hagit Shatkay and Ronen Feldman. 2003. Mining the
biomedical literature in the genomic era: An overview.
Journal of Computational Biology, 10(6):821?855.
Martin Volk. 2001. Exploiting the WWW as a corpus
to resolve PP attachment ambiguities. In Paul Rayson,
Andrew Wilson, Tony McEnery, Andrew Hardie, and
Shereen Khoja, editors, Proceedings of Corpus Lin-
guistics, pages 601?606, Lancaster, England, March.
89
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 91?92,
New York City, June 2006. c?2006 Association for Computational Linguistics
BioKI:Enzymes ? an adaptable system to locate low-frequency information
in full-text proteomics articles
Sabine Bergler, Jonathan Schuman, Julien Dubuc, Alexandr Lebedev
The CLaC Laboratory
Department of Computer Science and Software Engineering
Concordia University, 1455 de Maisonneuve Blvd West, Montreal, Quebec, H3G 1M8
bioki@cs.concordia.ca
1 Goals
BioKI:Enzymes is a literature navigation system that
uses a two-step process. First, full-text articles are
retrieved from PubMed Central (PMC). Then, for
each article, the most relevant passages are identified
according to a set of user selected keywords, and the
articles are ranked according to the pertinence of the
representative passages.
In contrast to most existing systems in informa-
tion retrieval (IR) and information extraction (IE) for
bioinformatics, BioKI:Enzymes processes full-text
articles, not abstracts. Full-text articles1 permit to
highlight low-frequency information?i.e. informa-
tion that is not redundant, that does not necessarily
occur in many articles, and within each article, may
be expressed only once (most likely in the body of
the article, not the abstract). It contrasts thus with
GoPubMed (Doms and Schroeder, 2005), a cluster-
ing system that retrieves abstracts using PMC search
and clusters them according to terms from the Gene
Ontology (GO).
Scientists face two major obstacles in using IR
and IE technology: how to select the best keywords
for an intended search and how to assess the validity
and relevance of the extracted information.
To address the latter problem, BioKI provides
convenient access to different degrees of context by
allowing the user to view the information in three
different formats. At the most abstract level, the
ranked list of articles provides the first five lines of
the most pertinent text segment selected by BioKI
(similar to the snippets provided by Google). Click-
ing on the article link will open a new window with a
1Only articles that are available in HTML format can cur-
rently be processed.
side-by-side view of the full-text article as retrieved
through PMC on the left and the different text seg-
ments2, ordered by their relevance to the user se-
lected keywords, on the right. The user has thus the
possibility to assess the information in the context of
the text segment first, and in the original, if desired.
2 Keyword-based Ranking
To address the problem of finding the best keywords,
BioKI:Enzymes explores different approaches. For
research in enzymology, our users specified a stan-
dard pattern of information retrieval, which is re-
flected in the user interface.
Enzymes are proteins that catalyze reactions dif-
ferently in different environments (pH and tem-
perature). Enzymes are characterized by the sub-
strate they act on and by the product of their catal-
ysis. Accordingly, a keyphrase pattern has enti-
ties (that tended to recur) prespecified for selection
in four categories: enzymes, their activities (such
as carbohydrate degrading), their qualities (such
as maximum activity), and measurements (such as
pH). The provided word lists are not exhaustive
and BioKI:Enzymes expects the user to specify new
terms (which are not required to conceptually fit the
category). The word lists are convenient for select-
ing alternate spellings that might be hard to enter (?-
amylase) and for setting up keyphrase templates in a
prole, which can be stored under a name and later
reused. Completion of the keyword lists is provided
through stemming and the equivalent treatment of
Greek characters and their different transliterations.
The interface presents the user with a search win-
dow, which has two distinct fields, one to specify
2We use TextTiler (Hearst, 1997) to segment the article.
91
the search terms for the PMC search, the other to
specify the (more fine-grained) keywords the sys-
tem uses to select the most relevant passages in the
texts and to rank the texts based on this choice. The
BioKI specific keywords can be chosen from the
four categories of keyword lists mentioned above or
entered. What distinguishes BioKI:Enzymes is the
direct control the user has over the weight of the key-
words in the ranking and the general mode of con-
sidering the keywords. Each of the four keyword
categories has a weight associated with it. In ad-
dition, bonus scores can be assigned for keywords
that co-occur at a distance less than a user-defined
threshold. The two modes of ranking are a basic
?and?, where the weight and threshold settings are
ignored and the text segment that has the most spec-
ified keywords closest together will be ranked high-
est. This is the mode of choice for a targeted search
for specific information, like ?pH optima? in a PMC
subcorpus for amylase.
The other mode is a basic ?or?, with additional
points for the co-occurrence of keywords within the
same text segment. Here, the co-occurrence bonus
is given for terms from the four different lists, not
for terms from the same list. While the search space
is much too big for a scientist to control all these de-
grees of freedom without support, our initial exper-
iments have shown that we could control the rank-
ing behavior with repeated refinements of the weight
settings, and even simulate the behavior of an ?and?
by judicious weight selection.
3 Assessment and Future Work
The evaluation of a ranking of full-text articles, for
which there are no Gold standards as of yet, is dif-
ficult and begins in the anecdotal. Our experts did
not explore the changes in ranking based on differ-
ent weight settings, but found the ?and? to be just
what they wanted from the system. We will ex-
periment with different weight distribution patterns
to see whether a small size of different weight set-
tings can be specified for predictable behavior and
whether this will have better acceptance.
The strength of BioKI lies in its adaptability to
user queries. In this it contrasts with template-based
IE systems like BioRAT (Corney et al, 2004), which
extracts information from full-length articles, but
uses handcoded templates to do so. Since BioKI
is not specific to an information need, but is meant
to give more control to the user and thus facilitate
access to any type of PMC search results, it is im-
portant that the same PMC search results can be re-
ordered by successively refining the selected BioKI
keywords until more desirable texts appear at the
top. This behavior is modeled after frequent behav-
ior using search engines such as Google, where of-
ten the first search serves to better select keywords
for a subsequent, better targeted search. This rerank-
ing based on keyword refinement can be done al-
most instantaneously (20 sec for 480 keyphrases on
161 articles), since the downloaded texts from PMC
are cached, and since the system spends most of its
runtime downloading and storing the articles from
PMC. This is currently a feasibility study, targeted to
eventually become a Web service. Performance still
needs to be improved (3:14 min for 1 keyphrase on
161 articles, including downloading), but the quality
of the ranking and variable context views might still
entice users to wait for them.
In conclusion, it is feasible to develop a highly
user-adaptable passage highlighting system over
full-text articles that focuses on low-frequency infor-
mation. This adaptability is provided both through
increased user control of the ranking parameters and
through presentation of results in different contexts
which at the same time justify the ranking and au-
thenticate keyword occurrences in their source text.
Acknowledgments
The rst prototype of BioKI was implemented by Evan Desai.
We thank our domain experts Justin Powlowski, Emma Masters,
and Regis-Olivier Benech. Work funded by Genome Quebec.
References
D. P. A. Corney, B.F. Buxton, W.B. Langdon, and D.T. Jones.
2004. BioRAT: Extracting biological information from full-
length papers. Bioinformatics, 20(17):3206?3213.
Andreas Doms and Michael Schroeder. 2005. GoPubMed: ex-
ploring PubMed with the Gene Ontology. Nucleic Acids Re-
search, 33:W783?W786. Web Server issue.
M.A. Hearst. 1997. Texttiling: Segmenting text into multi-
paragraph subtopic passages. Computational Linguistics,
23(1):34?64.
92
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 117?120,
Prague, June 2007. c?2007 Association for Computational Linguistics
CLaC and CLaC-NB: Knowledge-based and corpus-based approaches
to sentiment tagging
Alina Andreevskaia
Concordia University
1455 de Maisonneuve Blvd.
Montreal, Canada
andreev@cs.concordia.ca
Sabine Bergler
Concordia University
1455 de Maisonneuve Blvd.
Montreal, Canada
bergler@cs.concordia.ca
Abstract
For the Affective Text task at Semeval-
1/Senseval-4, the CLaC team compared a
knowledge-based, domain-independent ap-
proach and a standard, statistical machine
learning approach to ternary sentiment an-
notation of news headlines. In this paper
we describe the two systems submitted to
the competition and evaluate their results.
We show that the knowledge-based unsu-
pervised method achieves high accuracy and
precision but low recall, while supervised
statistical approach trained on small amount
of in-domain data provides relatively high
recall at the cost of low precision.
1 Introduction
Sentiment tagging of short text spans ? sentences,
headlines, or clauses ? poses considerable chal-
lenges for automatic systems due to the scarcity of
sentiment clues in these units: sometimes, the deci-
sion about the text span sentiment has to be based
on just a single sentiment clue and the cost of every
error is high. This is particularly true for headlines,
which are typically very short. Therefore, an ideal
system for sentiment tagging of headlines has to use
a large set of features with dependable sentiment an-
notations and to be able to reliably deduce the senti-
ment of the headline from the sentiment of its com-
ponents.
The valence labeling subtask of the Affective Text
task requires ternary ? positive vs. negative vs.
neutral ? classification of headlines. While such
categorization at the sentence level remains rela-
tively unexplored1 , the two related sentence-level,
binary classification tasks ? positive vs. negative
and subjective vs. objective ? have attracted con-
siderable attention in the recent years (Hu and Liu,
2004; Kim and Hovy, 2005; Riloff et al, 2006; Tur-
ney and Littman, 2003; Yu and Hatzivassiloglou,
2003). Unsupervised knowledge-based methods are
the preferred approach to classification of sentences
into positive and negative, mostly due to the lack of
adequate amounts of labeled training data (Gamon
and Aue, 2005). These approaches rely on presence
and scores of sentiment-bearing words that have
been acquired from dictionaries (Kim and Hovy,
2005) or corpora (Yu and Hatzivassiloglou, 2003).
Their accuracy on news sentences is between 65 and
68%.
Sentence-level subjectivity detection, where train-
ing data is easier to obtain than for positive vs. neg-
ative classification, has been successfully performed
using supervised statistical methods alone (Pang and
Lee, 2004) or in combination with a knowledge-
based approach (Riloff et al, 2006).
Since the extant literature does not provide clear
evidence for the choice between supervised machine
learning methods and unsupervised knowledge-
based approaches for the task of ternary sentiment
classification of sentences or headlines, we devel-
oped two systems for the Affective Text task at
SemEval-2007. The first system (CLaC) relies on
the knowledge-rich approach that takes into consid-
1To our knowledge, the only work that attempted such clas-
sification at the sentence level is (Gamon and Aue, 2005) that
classified product reviews.
117
eration multiple clues, such as a list of sentiment-
bearing unigrams and valence shifters, and makes
use of sentence structure in order to combine these
clues into an overall sentiment of the headline. The
second system (CLaC-NB) explores the potential of
a statistical method trained on a small amount of
manually labeled news headlines and sentences.
2 CLaC System: Syntax-Aware
Dictionary-Based Approach
The CLaC system relies on a knowledge-based,
domain-independent, unsupervised approach to
headline sentiment detection and scoring. The
system uses three main knowledge inputs: a list
of sentiment-bearing unigrams, a list of valence
shifters (Polanyi and Zaenen, 2006), and a set of
rules that define the scope and results of com-
bination of sentiment-bearing words with valence
shifters.
2.1 List of sentiment-bearing words
The unigrams used for sentence/headline classifica-
tion were learned from WordNet (Fellbaum, 1998)
dictionary entries using the STEP system described
in (Andreevskaia and Bergler, 2006b). In order to
take advantage of the special properties of WordNet
glosses and relations, we developed a system that
used the human-annotated adjectives from (Hatzi-
vassiloglou and McKeown, 1997) as a seed list and
learned additional unigrams from WordNet synsets
and glosses. The STEP algorithm starts with a
small set of manually annotated seed words that
is expanded using synonymy and antonymy rela-
tions in WordNet. Then the system searches all
WordNet glosses and selects the synsets that contain
sentiment-bearing words from the expanded seed
list in their glosses. In order to eliminate errors
produced by part-of-speech ambiguity of some of
the seed words, the glosses are processed by Brill?s
part-of-speech tagger (Brill, 1995) and only the seed
words with matching part-of-speech tags are consid-
ered. Headwords with sentiment-bearing seed words
in their definitions are then added to the positive or
negative categories depending on the seed-word sen-
timent. Finally, words that were assigned contra-
dicting ? positive and negative ? sentiment within
the same run were eliminated. The average accu-
racy of 60 runs with non-intersecting seed lists when
compared to General Inquirer (Stone et al, 1966)
was 74%. In order to improve the list coverage,
the words annotated as ?Positiv? or ?Negativ? in the
General Inquirer that were not picked up by STEP
were added to the final list.
Since sentiment-bearing words in English have
different degree of centrality to the category of sen-
timent, we have constructed a measure of word cen-
trality to the category of positive or negative sen-
timent described in our earlier work (Andreevskaia
and Bergler, 2006a). The measure, termed Net Over-
lap Score (NOS), is based on the number of ties that
connect a given word to other words in the category.
The number of such ties is reflected in the num-
ber of times each word was retrieved from Word-
Net by multiple independent STEP runs with non-
intersecting seed lists. This approach allowed us
to assign NOSs to each unigram captured by mul-
tiple STEP runs. Only words with fuzzy member-
ship score not equal to zero were retained in the
list. The resulting list contained 10,809 sentiment-
bearing words of different parts of speech.
2.2 Valence Shifters
The brevity of the headlines compared to typical
news sentences2 requires that the system is able to
make a correct decision based on very few sentiment
clues. Due to the scarcity of sentiment clues, the ad-
ditional factors, such as presence of valence shifters,
have a greater impact on the system performance on
headlines than on sentences or texts, where impact
of a single error can often be compensated by a num-
ber of other, correctly identified sentiment clues. For
this reason, we complemented the system based on
fuzzy score counts with the capability to discern and
take into account some relevant elements of syntac-
tic structure of sentences. We added to the system
two components in order to enable this capability:
(1) valence shifter handling rules and (2) parse tree
analysis.
Valence shifters can be defined as words that mod-
ify the sentiment expressed by a sentiment-bearing
word (Polanyi and Zaenen, 2006). The list of va-
lence shifters used in our experiments was a com-
2An average length of a sentence in a news corpus is over 20
words, while the average length of headlines in the test corpus
was only 7 words.
118
bination of (1) a list of common English nega-
tions, (2) a subset of the list of automatically ob-
tained words with increase/decrease semantics, and
(3) words picked up in manual annotation conducted
for other research projects by two trained linguists.
The full list consists of 490 words and expressions.
Each entry in the list of valence shifters has an action
and scope associated with it. The action and scope
tags are used by special handling rules that enable
our system to identify such words and phrases in the
text and take them into account in sentence senti-
ment determination. In order to correctly determine
the scope of valence shifters in a sentence, we intro-
duced into the system the analysis of the parse trees
produced by MiniPar (Lin, 1998).
As a result of this processing, every headline re-
ceived a score according to the combined fuzzy NOS
of its constituents. We then mapped this score,
which ranged between -1.2 and 0.99, into the
[-100, 100] scale as required by the competition or-
ganizers.
3 CLaC-NB System: Na??ve Bayes
Supervised statistical methods have been very suc-
cessful in sentiment tagging of texts and in subjec-
tivity detection at sentence level: on movie review
texts they reach an accuracy of 85-90% (Aue and
Gamon, 2005; Pang and Lee, 2004) and up to 92%
accuracy on classifying movie review snippets into
subjective and objective using both Nave Bayes and
SVM (Pang and Lee, 2004). These methods per-
form particularly well when a large volume of la-
beled data from the same domain as the test set is
available for training (Aue and Gamon, 2005). The
lack of sufficient data for training appears to be the
main reason for the virtual absence of experiments
with statistical classifiers in sentiment tagging at the
sentence level.
In order to explore the potential of statistical ap-
proaches on sentiment classification of headlines,
we implemented a basic Na??ve Bayes classifier with
smoothing using Lidstone?s law of succession (with
?=0.1). No feature selection was performed.
The development set for the Affective Text task
consisted of only 250 headlines, which is not suf-
ficient for training of a statistical classifier. In or-
der to increase the size of the training corpus, we
augmented it with a balanced set of 900 manually
annotated news sentences on a variety of topics ex-
tracted from the Canadian NewsStand database3 and
200 headlines from different domains collected from
Google News in January 20074.
The probabilities assigned by the classifier were
mapped to [-100, 100] as follows: all negative head-
lines received a score of -100, all positive headlines
+100, and neutral headlines 0.
4 Results and Discussion
Table 1 shows the results of the two CLaC systems
for valence labeling subtask of Affective Text task
compared to all participating systems average. The
best subtask scores are highlighted in bold.
System Pearson Acc. Prec. Rec. F1
correl.
CLaC 47.7 55.1 61.4 9.2 16
CLaC-NB 25.4 31.2 31.2 66.4 42
Task average 33.2 44.7 44.85 29.6 23.7
Table 1: System results
The comparison between the two CLaC systems
clearly demonstrates the relative advantages of the
two approaches. The knowledge-based unsuper-
vised system performed well above average on three
main measures: the Pearson correlation between
fine-grained sentiment assigned by CLaC system
and the human annotation; the accuracy for ternary
classification; and the precision of binary (positive
vs. negative) classification. These results demon-
strate that an accurately annotated list of sentiment-
bearing words combined with sophisticated valence
shifter handling produces acceptably accurate senti-
ment labels even for such difficult data as news head-
lines. This system, however, was not able to provide
good recall.
On the contrary, supervised machine learning has
very good recall, but low accuracy relative to the
results of the unsupervised knowledge-based ap-
proach. This shortcoming could be in part reduced
if more uniformly labeled headlines were available
3http://www.il.proquest.com/products pq/
descriptions/Canadian newsstand.shtml
4The interannotator agreement for this data, as measured by
Kappa, was 0.74.
119
for training. However, we can hardly expect large
amounts of such manually annotated data to be
handy in real-life situations.
5 Conclusions
The two CLaC systems that we submitted to the
Affective Text task have tested the applicability of
two main sentiment tagging approaches to news
headlines annotation. The results of the two sys-
tems indicate that the knowledge-based unsuper-
vised approach that relies on an automatically ac-
quired list of sentiment-bearing unigrams and takes
into account the combinatorial properties of valence
shifters, can produce high quality sentiment annota-
tions, but may miss many sentiment-laden headlines.
On the other hand, supervised machine learning has
good recall even with a relatively small training set,
but its precision and accuracy are low. In our future
work we will explore the potential of combining the
two approaches in a single system in order to im-
prove both recall and precision of sentiment annota-
tion.
References
Alina Andreevskaia and Sabine Bergler. 2006a. Mining
wordnet for a fuzzy sentiment: Sentiment tag extrac-
tion from wordnet glosses. In Proceedings EACL-06,
the 11rd Conference of the European Chapter of the
Association for Computational Linguistics, Trento, IT.
Alina Andreevskaia and Sabine Bergler. 2006b. Seman-
tic tag extraction from wordnet glosses. In Proceed-
ings of LREC-06, the 5th Conference on Language Re-
sources and Evaluation, Genova, IT.
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case study.
In RANLP-05, the International Conference on Recent
Advances in Natural Language Processing, Borovets,
Bulgaria.
Eric Brill. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing: A Case
Study in Part-of-Speech Tagging. Computational Lin-
guistics, 21(4).
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Michael Gamon and Anthony Aue. 2005. Automatic
identification of sentiment vocabulary: exploiting low
association with known sentiment terms. In Proceed-
ings of the ACL-05 Workshop on Feature Engineering
for Machine Learning in Natural Language Process-
ing, Ann Arbor, MI.
Vasileios Hatzivassiloglou and Kathleen B. McKeown.
1997. Predicting the Semantic Orientation of Ad-
jectives. In Proceedings of ACL-97, 35nd Meeting of
the Association for Computational Linguistics, pages
174?181, Madrid, Spain. ACL.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Tenth ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing (KDD-04), pages 168?177.
Soo-Min Kim and Eduard Hovy. 2005. Automatic de-
tection of opinion bearing words and sentences. In
Companion Volume to the Proceedings of IJCNLP-05,
the Second International Joint Conference on Natural
Language Processing, pages 61?66, Jeju Island, KR.
Dekang Lin. 1998. Dependency-based Evaluation
of MINIPAR. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, pages 768?774,
Granada, Spain.
Bo Pang and Lilian Lee. 2004. A sentiment education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of ACL-04,
42nd Meeting of the Association for Computational
Linguistics, pages 271?278.
Livia Polanyi and Annie Zaenen. 2006. Contextual Va-
lence Shifters. In James G. Shanahan, Yan Qu, and
Janyce Wiebe, editors, Computing Attitude and Affect
in Text: Theory and Application. Springer Verlag.
Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe.
2006. Feature subsumption for opinion analysis. In
Proceedings of EMNLP-06, the Conference on Empir-
ical Methods in Natural Language Processing, pages
440?448, Sydney, AUS.
P. J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie.
1966. The General Inquirer: a computer approach to
content analysis. M.I.T. studies in comparative poli-
tics. M.I.T. Press, Cambridge, MA.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: inference of semantic orientation
from association. ACM Transactions on Information
Systems (TOIS), 21:315?346.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Michael Collins and Mark Steedman, ed-
itors, Proceedings of EMNLP-03, 8th Conference on
Empirical Methods in Natural Language Processing,
pages 129?136, Sapporo, Japan.
120
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 46?53,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Recognizing Speculative Language in Biomedical Research Articles:
A Linguistically Motivated Perspective
Halil Kilicoglu and Sabine Bergler
Department of Computer Science and Software Engineering
Concordia University
Montreal, Quebec, Canada
{h_kilico, bergler}@cse.concordia.ca
Abstract
We explore a linguistically motivated ap-
proach to the problem of recognizing
speculative language (?hedging?) in bio-
medical research articles. We describe a
method, which draws on prior linguistic
work as well as existing lexical resources
and extends them by introducing syntactic
patterns and a simple weighting scheme to
estimate the speculation level of the sen-
tences. We show that speculative language
can be recognized successfully with such
an approach, discuss some shortcomings of
the method and point out future research
possibilities.
1 Introduction
Science involves making hypotheses, experiment-
ing, and reasoning to reach conclusions, which are
often tentative and provisional. Scientific writing,
particularly in biomedical research articles, reflects
this, as it is rich in speculative statements, also
known as hedges. Most text processing systems
ignore hedging and focus on factual language (as-
sertions). Although assertions, sometimes mere co-
occurrence of terms, are the focus of most infor-
mation extraction and text mining applications,
identifying hedged text is crucial, because hedging
alters, in some cases even reverses, factual state-
ments. For instance, the italicized fragment in ex-
ample (1) below implies a factual statement while
example (2) contains two hedging cues (indicate
and might), which render the factual proposition
speculative:
(1) Each empty cell indicates that the corre-
sponding TPase query was not used at the par-
ticular stage of PSI-BLAST analysis.
(2) These experiments indicated that the roX
genes might function as nuclear entry sites for
the assembly of the MSL proteins on the X
chromosome.
These examples not only illustrate the phe-
nomenon of hedging in the biomedical literature,
they also highlight some of the difficulties in rec-
ognizing hedges. The word indicate plays a differ-
ent role in each example, acting as a hedging cue
only in the second.
In recent years, there has been increasing inter-
est in the speculative aspect of biomedical lan-
guage (Light et al, 2004, Wilbur et al, 2006,
Medlock and Briscoe, 2007). In general, these
studies focus on issues regarding annotating
speculation and approach the problem of recog-
nizing speculation as a text classification problem,
using the well-known ?bag of words? method
(Light et al 2004, Medlock and Briscoe, 2007) or
simple substring matching (Light et al, 2004).
While both approaches perform reasonably well,
they do not take into account the more complex
and strategic ways hedging can occur in biomedi-
cal research articles. In example (3), hedging is
achieved with a combination of referring to ex-
perimental results (We ... show that ? indicating)
and the prepositional phrase to our knowledge:
(3) We further show that D-mib is specifically
required for Ser endocytosis and signaling
during wing development indicating for the
first time to our knowledge that endocytosis
regulates Ser signaling.
In this paper, we extend previous work through
linguistically motivated techniques. In particular,
we pay special attention to syntactic structures. We
46
address lexical hedges by drawing on a set of lexi-
cal hedging cues and expanding and refining it in a
semi-automatic manner to acquire a hedging dic-
tionary. To capture more complex strategic hedges,
we determine syntactic patterns that commonly act
as hedging indicators by analyzing a publicly
available hedge classification dataset.  Further-
more, recognizing that ?not all hedges are created
equal?, we use a weighting scheme, which also
takes into consideration the strengthening or weak-
ening effect of certain syntactic structures on lexi-
cal hedging cues. Our results demonstrate that
linguistic knowledge can be used effectively to
enhance the understanding of speculative language.
2 Related Work
The term hedging was first used in linguistic con-
text by Lakoff (1972). He proposed that natural
language sentences can be true or false to some
extent, contrary to the dominant truth-conditional
semantics paradigm of the era. He was mainly
concerned with how words and phrases, such as
mainly and rather, make sentences fuzzier or less
fuzzy.
Hyland (1998) provides one of the most com-
prehensive accounts of hedging in scientific arti-
cles in the linguistics literature. He views hedges
as polypragmatic devices with an array of purposes
such as weakening the force of statement, ex-
pressing deference to the reader and signaling un-
certainty. He proposes a fuzzy model, in which he
categorizes scientific hedges by their pragmatic
purpose, such as reliability hedges and reader-
oriented hedges. He also identifies the principal
syntactic realization devices for different types of
hedges, including epistemic verbs (verbs indicating
the speaker?s mode of knowing), adverbs and mo-
dal auxiliaries and presents the most frequently
used members of these types based on analysis of a
molecular biology article corpus.
Palmer (1986) identifies epistemic modality,
which expresses the speaker?s degree of commit-
ment to the truth of proposition and is closely
linked to hedging. He identifies three types of
epistemic modality: ?speculatives? express uncer-
tainty, ?deductives? indicate an inference from ob-
servable evidence, and ?assumptives? indicate
inference from what is generally known. He fo-
cuses mainly on the use of modal verbs in ex-
pressing various types of epistemic modality.
In their investigation of event recognition in
news text, Saur? et al (2006) address event modal-
ity at the lexical and syntactic level by means of
SLINKs (subordination links), some of which
(?modal?, ?evidential?) indicate hedges. They use
corpus-induced lexical knowledge from TimeBank
(Pustejovsky et al (2003)), standard linguistic
predicate classifications, and rely on a finite-state
syntactic module to identify subordinated events
based on the subcategorization properties of the
subordinating event.
DiMarco and Mercer (2004) study the intended
communicative purpose (dispute, confirmation, use
of materials, tools, etc.) of citations in scientific
text and show that hedging is used more frequently
in citation contexts.
In the medical field, Friedman et al (1994) dis-
cuss uncertainty in radiology reports and their
natural language processing system assigns one of
five levels of certainty to extracted findings.
Light et al (2004) explore issues with annotat-
ing speculative language in biomedicine and out-
line potential applications. They manually annotate
a corpus of approximately 2,000 sentences from
MEDLINE abstracts. Each sentence is annotated as
being definite, low speculative and highly specula-
tive. They experiment with simple substring
matching and a SVM classifier, which uses single
words as features. They obtain slightly better accu-
racy with simple substring matching suggesting
that more sophisticated linguistic knowledge may
play a significant role in identification of specula-
tive language. It is also worth noting that both
techniques yield better accuracy over full abstracts
than on the last two sentences of abstracts, in
which speculative language is found to be more
prevalent.
Medlock and Briscoe (2007) extend Light et
al.?s (2004) work, taking full-text articles into con-
sideration and applying a weakly supervised
learning model, which also uses single words as
features, to classify sentences as simply specula-
tive or non-speculative. They manually annotate a
test set and employ a probabilistic model for
training set acquisition using suggest and likely as
seed words. They use Light et al?s substring
matching as the baseline and improve to a re-
call/precision break-even point (BEP) of 0.76, us-
ing a SVM committee-based model from 0.60
recall/precision BEP of the baseline. They note that
their learning models are unsuccessful in identify-
47
ing assertive statements of knowledge paucity,
generally marked syntactically rather than lexi-
cally.
Wilbur et al (2006) suggest that factual infor-
mation mining is not sufficient and present an an-
notation scheme, in which they identify five
qualitative dimensions that characterize scientific
sentences: focus (generic, scientific, methodology),
evidence (E0-E3), certainty (0-3), polarity (posi-
tive, negative) and trend (+,-).  Certainty and evi-
dence dimensions, in particular, are interesting in
terms of hedging. They present this annotation
scheme as the basis for a corpus that will be used
to automatically classify biomedical text.
Discussion of hedging in Hyland (1998) pro-
vides the basic linguistic underpinnings of the
study presented here. Our goals are similar to those
outlined in the work of Light et al (2004) and
Medlock and Briscoe (2007); however, we propose
that a more linguistically oriented approach not
only could enhance recognizing speculation, but
would also bring us closer to characterizing the
semantics of speculative language. Some of the
work discussed above (in particular, Saur? et al
(2006) and Wilbur et al (2006)) will be relevant in
that regard.
3 Methods
To develop an automatic method to identify
speculative sentences, we first compiled a set of
core lexical surface realizations of hedging drawn
from Hyland (1998). Next, we augmented this set
by analyzing a corpus of 521 sentences, 213 of
which are speculative, and also noted certain syn-
tactic structures used for hedging. Furthermore, we
identified lexical cues and syntactic patterns that
strongly suggest non-speculative contexts (?un-
hedgers?). We then expanded and manually refined
the set of lexical hedging and ?unhedging? cues
using WordNet (Fellbaum, 1998) and the UMLS
SPECIALIST Lexicon (McCray et al, 1994).
Next, we quantified the strength of the hedging
cues and patterns through corpus analysis. Finally,
to recognize the syntactic patterns, we used the
Stanford Lexicalized Parser (Klein and Manning,
2003) and its dependency parse representation
(deMarneffe et al, 2006). We use weights assigned
to hedging cues to compute an overall hedging
score for each sentence.
To evaluate the effectiveness of our method, we
used basic information retrieval evaluation metrics:
precision, recall, accuracy and F
1
 score. In addi-
tion, we measure the recall/precision break-even
point (BEP), which indicates the point at which
precision and recall are equal, to provide a com-
parison to results previously reported. As baseline,
we use the substring matching method, described
in Light et al (2004) in addition to another sub-
string matching method, which uses terms ranked
in top 15 in Medlock and Briscoe (2007). To
measure the statistical significance of differences
between the performances of baseline and our
system, we used the binomial sign test.
4 Data Set
In our experiments, we use the publicly available
hedge classification dataset
1
, reported in Medlock
and Briscoe (2007). This dataset consists of a
manually annotated test set of 1537 sentences (380
speculative) extracted from six full-text articles on
Drosophila melanogaster (fruit-fly) and a training
set of 13,964 sentences (6423 speculative) auto-
matically induced using a probabilistic acquisition
model. A pool of 300,000 sentences randomly se-
lected from an archive of 5579 full-text articles
forms the basis for training data acquisition and
drives their weakly supervised hedge classification
approach.
While this probabilistic model for training data
acquisition is suitable for the type of weakly su-
pervised learning approach they describe, we find
that it may not be suitable as a fair data sample,
since the speculative instances overemphasize
certain hedging cues used as seed terms (suggest,
likely). On the other hand, the manually annotated
test set is valuable for our purposes. To train our
system, we (the first author) manually annotated a
separate training set of 521 sentences (213 specu-
lative) from the pool, using the annotation guide-
lines provided. Despite being admittedly small, the
training set seems to provide a good sample, as the
distribution of surface realization features (epis-
temic verbs (32%), adverbs (26%), adjectives
(19%), modal verbs (%21)) correspond roughly to
that presented in Hyland (1998).
5 Core Surface Realizations of Hedging
                                                           
1
 http://www.benmedlock.co.uk/hedgeclassif.html
48
Hyland (1998) provides the most comprehensive
account of surface realizations of hedging in sci-
entific articles, categorizing them into two classes:
lexical and non-lexical features. Lexical features
include modal auxiliaries (may and might being the
strongest indicators), epistemic verbs, adjectives,
adverbs and nouns. Some common examples of
these feature types are given in Table 1.
Feature Type Examples
Modal auxiliaries may, might, could, would,
should
Epistemic judgment
verbs
suggest, indicate, specu-
late, believe, assume
Epistemic evidential
verbs
appear, seem
Epistemic deductive
verbs
conclude, infer, deduce
Epistemic adjectives likely, probable, possible
Epistemic adverbs probably, possibly, per-
haps, generally
Epistemic nouns possibility, suggestion
Table 1. Lexical surface features of hedging
Non-lexical hedges usually include reference
to limiting experimental conditions, reference to a
model or theory or admission to a lack of knowl-
edge. Their surface realizations typically go be-
yond words and even phrases. An example is given
in sentence (4), with hedging cues italicized.
(4) Whereas much attention has focused on eluci-
dating basic mechanisms governing axon de-
velopment, relatively little is known about the
genetic programs required for the establish-
ment of dendrite arborization patterns that are
hallmarks of distinct neuronal types.
While lexical features can arguably be exploited
effectively by machine learning approaches, auto-
matic identification of non-lexical hedges auto-
matically seems to require syntactic and, in some
cases, semantic analysis of the text.
Our first step was to expand on the core lexical
surface realizations identified by Hyland (1998).
6 Expansion of Lexical Hedging Cues
Epistemic verbs, adjectives, adverbs and nouns
provide the bulk of the hedging cues. Although
epistemic features are commonly referred to and
analyzed in the linguistics literature and various
widely used lexicons exist that classify different
part-of-speech (e.g., VerbNet (Kipper Schuler,
2005) for verb classes), we are unaware of any
such comprehensive classification based on epis-
temological status of the words. We explore in-
ducing such a lexicon from the core lexical
examples identified in Hyland (1998) (a total of 63
hedging cues) and expanding it semi-automatically
using two lexicons: WordNet (Fellbaum, 1998)
and UMLS SPECIALIST Lexicon (McCray,
1994).
We first extracted synonyms for each epistemic
term in our list using WordNet synsets. We then
removed those synonyms that did not occur in our
pool of sentences, since they are likely to be very
uncommon words in scientific articles. Expanding
epistemic verbs is somewhat more involved than
expanding other epistemic types, as they tend to
have more synsets, indicating a greater degree of
word sense ambiguity (assume has 9 synsets).
Based on the observation that an epistemic verb
taking a clausal complement marked with that is a
very strong indication of hedging, we only consid-
ered verb senses which subcategorize for a that
complement. Expansion via WordNet resulted in
66 additional lexical features.
Next, we considered the case of nominaliza-
tions. Again, based on corpus analysis, we noted
that nominalizations of epistemic verbs and adjec-
tives are a common and effective means of hedging
in molecular biology articles. The UMLS
SPECIALIST Lexicon provides syntactic informa-
tion, including nominalizations, for biomedical as
well as general English terms. We extracted the
nominalizations of words in our expanded diction-
ary of epistemic verbs and adjectives from UMLS
SPECIALIST Lexicon and discarded those that do
not occur in our pool of sentences, resulting in an
additional 48 terms. Additional 5 lexical hedging
cues (e.g., tend, support) were identified via man-
ual corpus analysis and further expanded using the
methodology described above.
An interesting class of cues are terms expressing
strong certainty (?unhedgers?). Used within the
scope of negation, these terms suggest hedging,
while in the absence of negation they strongly sug-
gest a non-speculative context. Examples of these
include verbs indicating certainty, such as know,
demonstrate, prove and show, and adjectives, such
as clear. These features were also added to the
dictionary and used together with other surface
49
cues to recognize speculative sentences. The
hedging dictionary contains a total of 190 features.
7 Quantifying Hedging Strength
It is clear that not all hedging devices are equally
strong and that the choice of hedging device affects
the strength of the speculation. However, deter-
mining the strength of a hedging device is not
trivial. The fuzzy pragmatic model proposed by
Hyland (1998) employs general descriptive terms
such as ?strong? and ?weak? when discussing par-
ticular cases of hedging and avoids the need for
precise quantification. Light et al (2004) report
low inter-annotator agreement in distinguishing
low speculative sentences from highly speculative
ones. From a computational perspective, it would
be useful to quantify hedging strength to determine
the confidence of the author in his or her proposi-
tion.
As a first step in accommodating noticeable dif-
ferences in strengths of hedging features, we as-
signed weights (1 to 5, 1 representing the lowest
hedging strength and 5 the highest) to all hedging
features in our dictionary. Core features were as-
signed weights based on the discussion in Hyland
(1998). For instance, he identifies modal auxilia-
ries, may and might, as the prototypical hedging
devices, and they were given weights of 5. On the
other hand, modal auxiliaries commonly used in
non-epistemic contexts (would, could) were as-
signed a lower weight of 3. Though not as strong
as may and might, core epistemic verbs and ad-
verbs are generally good hedging cues and there-
fore were assigned weights of 4. Core epistemic
adjectives and nouns often co-occur with other
syntactic features to act as strong hedging cues and
were assigned weights of 3. Terms added to the
dictionary via expansion were assigned a weight
one less than their seed terms. For instance, the
nominalization supposition has weight 2, since it is
expanded from the verb suppose (weight 3), which
is further expanded from its synonym speculate
(weight 4), a core epistemic verb. The reduction in
weights of certain hedging cues reflects their pe-
ripheral nature in hedging.
Hyland (1998) notes that writers tend to com-
bine hedges (?harmonic combinations?) and sug-
gests the possibility of constructing scales of
certainty and tentativeness from these combina-
tions. In a similar vein, we accumulate the weights
of the hedging features found in a sentence and
assign an overall hedging score to each sentence.
8 The Role of Syntax
Corpus analysis shows that various syntactic de-
vices play a prominent role in hedging, both as
hedging cues and for strengthening or weakening
effects. For instance, while some epistemic verbs
do not act as hedging cues (or may be weak hedg-
ing cues) when used alone, together with a that
complement or an infinitival clause, they are good
indicators of hedging. A good example is appear,
which often occurs in molecular biology articles
with its ?come into sight? meaning (5) and be-
comes a good hedging cue when it takes an infini-
tival complement (6):
(5) The linearity of the ommatidial arrangement
was disrupted and numerous gaps appeared
between ommatidia arrow.
(6) In these data a substantial fraction of both si-
lent and replacement DNA mutations appear to
affect fitness.
On the other hand, as discussed above, words
expressing strong certainty (?unhedgers?) are good
indicators of hedging when negated, and strongly
non-speculative otherwise.
We examined the training set and identified the
most salient syntactic patterns that play a role in
hedging. A syntactic pattern, or lack thereof, af-
fects the overall score assigned to a hedging cue; a
strengthening syntactic pattern will increase the
overall score contributed by the cue, while a weak-
ening pattern will decrease it. For instance, in sen-
tence (5) above, the absence of the infinitival
complement will reduce the score contribution of
appear by 1, resulting in a score of 3 instead of 4.
On the other hand, that appear takes an infinitival
clause in example (6) will increase the score con-
tribution of appear by 1. All score contributions of
a sentence add up to its hedging score.
A purely syntactic case is that of whether (if).
Despite being a conjunction, it seems to act as a
hedging cue when it introduces a clausal comple-
ment regardless of existence of any other hedging
cue from the hedging dictionary. The basic syntac-
tic patterns we identified and implemented and
their effect on the overall hedging score are given
in Table 2.
50
To obtain the syntactic structures of sentences,
we used the statistical Stanford Lexicalized Parser
(Klein and Manning, 2003), which provides a full
parse tree, in addition to part-of-speech tagging
based on the Penn Treebank tagset. A particularly
useful feature of the Stanford Lexicalized Parser is
typed dependency parses extracted from phrase
structure parses (deMarneffe, et al (2006)). We
use these typed dependency parses to identify
clausal complements, infinitival clauses and nega-
tion. For instance, the following two dependency
relations indicate a clausal complement marked
with that and identify the second syntactic pattern
in Table 2.
ccomp(<EPISTEMIC VERB>,<VB>)
complm(<VB>,that)
In these relations, ccomp stands for clausal
complement with internal subject and complm
stands for complementizer. VB indicates any verb.
Syntactic Pattern Effect
on Score
+1
+2
<EPISTEMIC VERB> to(inf) VB
<EPISTEMIC VERB> that(comp) VB
Otherwise
-1
+2<EPISTEMIC NOUN> followed by
that(comp)
Otherwise
-1
not <UNHEDGING VERB> +1
no| not <UNHEDGING NOUN> +2
no| not immediately followed by
<UNHEDGING ADVERB>
+1
no| not immediately followed by
<UNHEDGING ADJECTIVE>
+1
whether| if in a clausal complement
context
3
Table 2. Syntactic patterns and their effect on the over-
all hedging score.
9 Baseline
For our experiments, we used two baselines. First,
we used the substring matching method reported in
Light et al (2004), which labels sentences con-
taining one of more of the following as specula-
tive: suggest, potential, likely, may, at least, in
part, possibl, further investigation, unlikely, puta-
tive, insights, point toward, promise and propose
(Baseline1). Secondly, we used the top 15 ranked
term features determined using P(spec|x
j
) in train-
ing and classification models (at smoothing pa-
rameter 
? 
?=5) reported in Medlock and Briscoe
(2007): suggest, likely, may, might, seems, Taken,
suggests, probably, Together, suggesting, possibly,
suggested, findings, observations, Given. Our sec-
ond baseline uses the substring matching method
with these features (Baseline2).
10 Results
The evaluation results obtained using the baseline
methods are given in Table 3.
Method Precision Recall Accuracy F
1
score
Baseline1 0.79 0.40 0.82 0.53
Baseline2 0.95 0.43 0.85 0.60
Table 3. Baseline evaluation results.
The evaluation results obtained from our system
by varying the overall hedging score and using it
as threshold are given in Table 4. It is worth noting
that the highest overall hedging score we obtained
was 16; however, we do not show the results for
every possible threshold here for brevity.
Hedging
Score
Threshold
Precision Recall Accuracy F
1
score
1 0.68 0.95 0.88 0.79
2 0.75 0.94 0.91 0.83
3 0.85 0.86 0.93 0.85
4 0.91 0.71 0.91 0.80
5 0.92 0.63 0.89 0.75
6 0.97 0.40 0.85 0.57
7 1 0.19 0.79 0.33
Table 4. Evaluation results from our system.
As seen from Table 3 and Table 4, our results
show improvement over both baseline methods in
terms of accuracy and F
1
 score. Increasing the
threshold (thereby requiring more or stronger
hedging devices to qualify a sentence as specula-
tive) improves the precision while lowering the
recall. The best accuracy and F
1
 score are achieved
at threshold t=3. At this threshold, the differences
between the results obtained with our method and
baseline methods are statistically significant at
0.01 level (p < 0.01).
51
Method Recall/Precision BEP
Baseline1 0.60
Baseline2 0.76
Our system 0.85
Table 5. Recall / precision break-even point (BEP) re-
sults
With the threshold providing the best accuracy
and F
1
 score, precision and recall are roughly the
same (0.85), indicating a recall/precision BEP of
approximately 0.85, also an improvement over
0.76 achieved with a weakly supervised classifier
(Medlock and Briscoe, 2007). Recall/precision
BEP scores are given in Table 5.
11 Discussion
Our results confirm that writers of scientific arti-
cles employ basic, predictable hedging strategies to
soften their claims or to indicate uncertainty and
demonstrate that these strategies can be captured
using a combination of lexical and syntactic
means. Furthermore, the results indicate that
hedging cues can be gainfully weighted to provide
a rough measure of tentativeness or uncertainty.
For instance, a sentence with the highest overall
hedging score is given below:
(7) In one study, Liquid facets was proposed to
target Dl to an endocytic recycling compart-
ment suggesting that recycling of Dl may be
required for signaling.
On the other hand, hedging is not strong in the
following sentence, which is assigned an overall
hedging score of 2:
(8) There is no apparent need for cytochrome c
release in C. elegans since CED-4 does not re-
quire it to activate CED-3.
Below, we discuss some of the common error
types we encountered. Our discussion is based on
evaluation at hedging score threshold of 0, where
existence of a hedging cue is sufficient to label a
sentence speculative.
Most of the false negatives produced by the
system are due to syntactic patterns not addressed
by our method. For instance, negation of ?unhedg-
ers? was used as a syntactic pattern; the pattern
was able to recognize know as an ?unhedger? in
the following sentence, but not the negative quanti-
fier (l i t t le), labeling the sentence as non-
speculative.
(9) Little was known however about the specific
role of the roX RNAs during the formation of
the DCC.
In fact, Hyland (1998) notes ?negation in scien-
tific research articles shows a preference for nega-
tive quantifiers (few, little) and lexical negation
(rarely, overlook).? However, we have not en-
countered this pattern while analyzing the training
set and have not addressed it. Nevertheless, our
approach lends itself to incremental development
and adding such a pattern to our rulebase is rela-
tively simple.
Another type of false negative is caused by cer-
tain derivational forms of epistemic words. In the
following example, the adjective suggestive is not
recognized as a hedging trigger, even though its
base form suggest is an epistemic verb.
(10) Phenotypic differences are suggestive of
distinct functions for some of these genes in
regulating dendrite arborization.
It seems that more sophisticated lexicon expan-
sion rules can be employed to handle such cases.
For example, WordNet?s ?derivationally related
form? feature may be used as the basis of these
expansion rules.
Regarding false positives, most of them are due
to word sense ambiguity concerning hedging cues.
For instance, the modal auxiliary could  is fre-
quently used as a past tense form of can in scien-
tific articles to express the role of enabling
conditions and external constraints on the occur-
rence of the proposition rather than uncertainty or
tentativeness regarding the proposition.  Currently,
our system is unable to recognize such cases. An
example is given below:
(10) Also we could not find any RAG-like se-
quences in the recently sequenced sea urchin
lancelet hydra and sea anemone genomes,
which encode RAG-like sequences.
The context around the hedging cue seems to
play a role in these cases. First person plural pro-
noun (we) and/or reference to objective enabling
conditions seem to be a common characteristic
among false positive cases of could.
In other cases, such as appear, in the absence of
strengthening syntactic cues (to, that), we lower
the hedging score; however, depending on the
threshold, this may not be sufficient to render the
sentence non-speculative.  Rather than lowering
the score equally for all epistemic verbs, a more
52
appropriate approach would be to consider verb
senses separately (e.g., appear should be effec-
tively unhedged without a strengthening cue, while
suggest should only be weakened).
Another type of false positives concern ?weak?
hedging cues, such as epistemic deductive verbs
(conclude, estimate) as well as adverbs (essen-
tially, usually) and nominalizations (implication,
assumption).
We have also seen a few instances, which seem
speculative on the surface, but were labeled non-
speculative. An example is given below:
(11) Caspases can also be activated with the aid
of Apaf-1, which in turn appears to be regu-
lated by cytochrome c and dATP.
12 Conclusion and Future Work
In this paper, we present preliminary experiments
we conducted in recognizing speculative sentences.
We draw on previous linguistic work and extend it
via semi-automatic methods of lexical acquisition.
Using a corpus specifically annotated for specula-
tion, we demonstrate that our linguistically ori-
ented approach improves on the previously
reported results.
Our next goal is to extend our work using a
larger, more comprehensive corpus. This will al-
low us to identify other commonly used hedging
strategies and refine and expand the hedging dic-
tionary.  We also aim to refine the weighting
scheme in a more principled way.
While recognizing that a sentence is speculative
is useful in and of itself, it seems more interesting
and clearly much more challenging to identify
speculative sentence fragments and the proposi-
tions that are being hedged. In the future, we will
move in this direction with the goal of character-
izing the semantics of speculative language.
Acknowledgements
We would like to thank Thomas C. Rindflesch for
his suggestions and comments on the first draft of
this paper.
References
deMarneffe, M. C., MacCartney B., Manning C.D.
2006. Generating Typed Dependency Parses from
Phrase Structure Parses. In Proc of 5th International
Conference on Language Resources and Evaluation,
pp. 449-54.
DiMarco C. and Mercer R.E. 2004. Hedging in Scien-
tific Articles as a Means of Classifying Citations. In
Exploring Attitude and Affect in Text: Theories and
Applications AAAI-EAAT 2004. pp.50-4.
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA.
Friedman C., Alderson P., Austin J., Cimino J.J., John-
son S.B. 1994. A general natural-language text proc-
essor for clinical radiology. Journal of the American
Medical Informatics Association, 1(2): 161-74.
Hyland K. 1998. Hedging in Scientific Research Arti-
cles. John Benjamins B.V., Amsterdam, Netherlands.
Kipper Schuler, K. 2005. VerbNet: A broad-coverage,
comprehensive verb lexicon. PhD thesis, University
of Pennsylvania.
Klein D. and Manning C. D. 2003. Accurate unlexical-
ized parsing. In Proc of 41st Meeting of the Associa-
tion for Computational Linguistics. pp. 423-30.
Lakoff  G. 1972. Hedges: A Study in Meaning Criteria
and the Logic of Fuzzy Concepts. Chicago Linguis-
tics Society Papers, 8, pp.183-228.
Light M., Qiu X.Y., Srinivasan P. 2004. The Language
of Bioscience: Facts, Speculations, and Statements in
between. In BioLINK 2004: Linking Biological Lit-
erature, Ontologies and Databases, pp. 17-24.
McCray A. T., Srinivasan S., Browne A. C. 1994. Lexi-
cal methods for managing variation in biomedical
terminologies.  In Proc of 18th Annual Symposium on
Computer Applications  in  Medical Care, pp. 235-9.
Medlock B. and Briscoe T. 2007. Weakly Supervised
Learning for Hedge Classification in Scientific Lit-
erature. In Proc of 45
th
 Meeting of the Association for
Computational Linguistics. pp.992-9.
Palmer F.R. 1986. Mood and Modality. Cambridge
University Press, Cambridge, UK.
Pustejovsky J., Hanks P., Saur? R., See A., Gaizauskas
R., Setzer A., Radev D., Sundheim B., Day D. Ferro
L., Lazo M. 2003. The TimeBank Corpus. In Proc of
Corpus Linguistics. pp. 647-56.
Saur? R., Verhagen M., Pustejovsky J. 2006. SlinkET: a
partial modal parser for events. In Proc of 5
th
 Inter-
national Conference on Language Resources and
Evaluation.
Wilbur W.J., Rzhetsky A., Shatkay H. 2006. New Di-
rections in Biomedical Text Annotations: Defini-
tions, Guidelines and Corpus Construction. BMC
Bioinformatics, 7:356.
53
Proceedings of the Workshop on BioNLP: Shared Task, pages 119?127,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Syntactic Dependency Based Heuristics for Biological Event Extraction
Halil Kilicoglu and Sabine Bergler
Department of Computer Science and Software Engineering
Concordia University
1455 de Maisonneuve Blvd. West
Montre?al, Canada
{h kilico,bergler}@cse.concordia.ca
Abstract
We explore a rule-based methodology for the
BioNLP?09 Shared Task on Event Extrac-
tion, using dependency parsing as the under-
lying principle for extracting and characteriz-
ing events. We approach the speculation and
negation detection task with the same princi-
ple. Evaluation results demonstrate the util-
ity of this syntax-based approach and point out
some shortcomings that need to be addressed
in future work.
1 Introduction
Exponential increase in the amount of genomic data
necessitates sophisticated approaches to accessing
knowledge in molecular biology literature, which re-
mains the primary medium for disseminating new
knowledge in molecular biology. Extracting rela-
tions and events directly from free text facilitates
such access. Advances made in foundational areas,
such as parsing and named entity recognition, boosts
the interest in biological event extraction (Zweigen-
baum et al, 2007). The BioNLP?09 Shared Task on
Event Extraction illustrates this shift and is likely to
inform future endeavors in the field.
The difficulty of extracting biological events from
scientific literature is due to several factors. First,
sentences are long and often have long-range depen-
dencies. In addition, the biological processes de-
scribed are generally complex, involving multiple
genes or proteins as well as other biological pro-
cesses. Furthermore, biological text is rich in higher
level phenomena, such as speculation and negation,
which need to be dealt with for correct interpreta-
tion of the text. Despite all this complexity, how-
ever, a closer look at various biological corpora also
suggests that beneath the complexity lie regularities,
which may potentially be exploited using relatively
simple heuristics.
We participated in Task 1 and Task 3 of the
Shared Task on Event Extraction. Our approach
draws primarily from dependency parse representa-
tion (Mel?c?uk, 1988; deMarneffe et al, 2006). This
representation, with its ability to reveal long-range
dependencies, is suitable for building event extrac-
tion systems. Dependencies typed with grammatical
relations, in particular, benefit such applications. To
detect and characterize biological events (Task 1),
we constructed a dictionary of event triggers based
on training corpus annotations. Syntactic depen-
dency paths between event triggers and event partic-
ipants in the training corpus served in developing a
grammar for participant identification. For specula-
tion and negation recognition (Task 3), we extended
and refined our prior work in speculative language
identification, which involved dependency relations
as well. Our results show that dependency relations,
despite their imperfections, provide a good founda-
tion, on which accurate and reliable event extraction
systems can be built and that the regularities of bio-
logical text can be adequately exploited with a lim-
ited set of syntactic patterns.
2 Related Work
Co-occurrence based approaches (Jenssen et al,
2001; Ding et al, 2002) to biological relation ex-
traction provide high recall at the expense of low
119
precision. Shallow parsing and syntactic templates
(Blaschke et al, 1999; Rindflesch et al, 2000; Fried-
man et al, 2001; Blaschke and Valencia, 2001;
Leroy et al, 2003; Ahlers et al, 2007), as well as
full parsing (Daraselia et al, 2004; Yakushiji et al,
2005), have also been explored as the basis for re-
lation extraction. In contrast to co-occurrence based
methods, these more sophisticated approaches pro-
vide higher precision at the expense of lower recall.
Approaches combining the strengths of complemen-
tary models have also been proposed (Bunescu et al,
2006) for high recall and precision.
More recently, dependency parse representation
has found considerable use in relation extraction,
particularly in extraction of protein-protein interac-
tions (PPI). Fundel et al (2007) use Stanford depen-
dency parses of Medline abstracts as the basis for
rules that extract gene/protein interactions. Rinaldi
et al (2007) extract relations combining a hand-
written grammar based on dependency parsing with
a statistical language model. Airola et al (2008) ex-
tract protein-protein interactions from scientific lit-
erature using supervised machine learning based on
an all-dependency-paths kernel.
The speculative aspect of the biomedical literature
(also referred to as hedging) has been the focus of
several recent studies. These studies primarily dealt
with distinguishing speculative sentences from non-
speculative ones. Supervised machine learning tech-
niques mostly dominate this area of research (Light
et al, 2004; Medlock and Briscoe, 2007; Szarvas,
2008). A more linguistically-based approach, rely-
ing on lexical and syntactic patterns, has been ex-
plored as well (Kilicoglu and Bergler, 2008). The
scope of speculative statements is annotated in the
BioScope corpus (Vincze et al, 2008); however, ex-
periments in detecting speculation scope have yet to
be reported.
Recognizing whether extracted events are negated
is crucial, as negation reverses the meaning of a
proposition. Most of the work on negation in
the biomedical domain focused on finding negated
terms or concepts. Some of these systems are
rule-based and rely on lexical or syntactic informa-
tion (Mutalik et al, 2001; Chapman et al, 2001;
Sanchez-Graillet and Poesio, 2007); while others
(Averbuch et al, 2004; Goldin and Chapman, 2003)
experiment with machine learning techniques. A re-
cent study (Morante et al, 2008) focuses on learn-
ing negation scope using memory-based classifiers
trained on the BioScope corpus.
Our approach to Task 1 is most similar to work
of Fundel et al (2007) as it builds on dependency-
based heuristics. However, we address a larger num-
ber of event classes, including regulatory events al-
lowing participation of other events. In addition,
event triggers are central to our approach, contrast-
ing with their system and most other PPI systems
that rely on finding dependency paths between enti-
ties. We extended prior work for Task 3 and obtained
state of the art results.
3 Event Detection and Characterization
As preparation for biological event extraction, we
combined the provided annotations, tokenized in-
put and dependency parses in an XML representa-
tion. Next, we determined good trigger words for
event classes and scored them. Finally, we devel-
oped a dependency-based grammar for event partici-
pant identification, which drives our event extraction
system.
3.1 Data Preprocessing
Our event detection and characterization pipeline re-
quires XML representation of a document as in-
put. Here, the XML representation of a document
contains sentences, their offset positions and de-
pendency parses as well as entities (Proteins) and
their offset positions in addition to word information
(tokens, part-of-speech tags, indexes and lemmas).
We used the Stanford Lexicalized Parser (Klein and
Manning, 2003) to extract word-related information,
as well as for dependency parsing.
3.2 Event Triggers
After parsing the training corpus and creating an en-
riched document representation, we proceeded with
constructing a dictionary of event triggers, draw-
ing from training corpus annotations of triggers and
making further refinements, as described below.
We view event triggers essentially as predicates
and thus restricted event triggers to words carrying
verb, noun or adjective part-of-speech tags. Our
analysis suggests that, in general, trigger words with
other POS tags are tenuously annotated event trig-
gers and in fact require more context to qualify as
120
event triggers. In Example (1), by is annotated
as trigger for a Positive regulation event;
however, it seems that the meaning of the entire
prepositional phrase introduced with by contributes
to trigger such an event:
(1) These data suggest a general role for Tax in-
duction of IL-1alpha gene transcription by the
NF-kappaB pathway.
We refined the event trigger list further through
limited term expansion and filtering, based on sev-
eral observations:
1. The event triggers with prefixes, such as
co, down and up, (e.g., coexpression, down-
regulate) were expanded to include both hy-
phenated and non-hyphenated forms.
2. For a trigger that has inflectional/derivational
forms acting as triggers in the development cor-
pus but not in the training corpus, we added
these forms as event triggers. Examples include
adding dimerization after dimerize and dimin-
ished(adj) after diminish, among others.
3. We removed several event triggers, which, we
considered, required more context to qualify
as event triggers for the corresponding event
classes. (e.g., absence, absent, follow, lack)
Finally, we did not consider multi-word event
triggers. We observed that core trigger meaning
generally came from a single word token (gener-
ally head of a noun phrase) in the fragment an-
notated as event trigger. For instance, for trigger
transcriptional activation, the annotated event class
is Positive regulation, which suggests that
the head activation carries the meaning in this in-
stance (since transcriptional is an event trigger for
the distinct Transcription event class). In an-
other instance, the trigger binding activity is anno-
tated as triggering a Binding event, indicating that
the head word activity is semantically empty. We
noted some exceptions to this constraint (e.g., neg-
atively regulate, positive regulation) and dealt with
them in the postprocessing step.
For the remaining event triggers, we computed a
?goodness score? via maximum likelihood estima-
tion. For a given event class C and event trigger t,
the ?goodness score? G(t,C) then is:
G(t,C) = w(C:t)/w(t)
where w(C:t) is the number of times t occurs as a
trigger for event class C and w(t) is the frequency
of trigger t in the training corpus. The newly added
event triggers were assigned the same scores as the
trigger they are derived from.
In the event extraction step, we do not consider
event triggers with a score below an empirically de-
termined threshold.
3.3 Dependency relations for event participant
identification
To identify the event participants Theme and Cause,
we developed a grammar based on the ?collapsed?
version of Stanford Parser dependency parses of
sentences. Grammar development was driven by
extraction and ranking of typed dependency rela-
tion paths connecting event triggers to correspond-
ing event participants in the training data. We then
analyzed these paths and implemented as rules those
deemed to be both correct and sufficiently general.
More than 2,000 dependency paths were ex-
tracted; however, their distribution was Zipfian, with
approximately 70% of them occurring only once.
We concentrated on the most frequent, therefore
general, dependency paths. Unsurprisingly, the most
frequent dependency path involved the dobj (direct
object) dependency between verbal event triggers
and Theme participants, occurring 826 times. Next
was the nn (nominal modifier) dependency between
nominal event triggers and their Theme participants.
The most frequent dependency for Cause partici-
pants was, again unsurprisingly, nsubj (nominal sub-
ject). The ranking of dependency paths indicated
that path length is inversely proportional to reliabil-
ity. We implemented a total of 27 dependency path
patterns.
Some of these patterns specifically address de-
ficiencies of the Stanford Parser. Prepositional
phrases are often attached incorrectly, causing prob-
lems in participant identification. Consider, for ex-
ample, one of the more frequent dependency paths,
dobj-prep on (direct object dependency followed by
prepositional modifier headed in on), occurring be-
tween the event trigger (effect) and participant (ex-
pression, itself a sub-event trigger):
(2) We have examined the effect of leukotriene B4
121
(LTB4), a potent lipid proinflammatory medi-
ator, on the expression of the proto-oncogenes
c-jun and c-fos.
dobj(examined,effect)
prep on(examined,expression)
This dependency path occurs almost exclusively
with PP attachment errors involving on, leading us
to stipulate a ?corrective? dependency path, imple-
mented for certain trigger words (e.g., effect, influ-
ence, impact in this case). Postnominal preposi-
tional attachment heuristics detailed in Schuman and
Bergler (2006) helped determine 6 such patterns.
Two common verbs (require and involve) deserve
special attention, as the semantic roles of their sub-
ject/object constituents differ from typical verbs.
The prototypical Cause dependency, nsubj, indicates
a Theme in the following sentence:
(3) Regulation of interleukin-1beta transcription
by Epstein-Barr virus involves a number of la-
tent proteins via their interaction with RBP.
nsubj(involves,Regulation)
For these two verbs, participant identification rules
are reversed.
An interesting phenomenon is NPs with hyphen-
ated adjectival modifiers, occurring frequently in
molecular biology texts (e.g., ?... LPS-mediated
TF expression...?). The majority of these cases in-
volve regulatory events. Such cases do not in-
volve a dependency path, as the participant (in
this case, LPS) and the event trigger (mediated)
form a single word. An additional rule ad-
dresses these cases, stipulating that the substring
preceding the hyphen is the Cause of the regu-
latory event triggered by the substring following
the hyphen. (Positive regulation (Trig-
ger=mediated,Theme=TF expression,Cause=LPS)).
Events allowing event participants (regulatory
events) are treated essentially the same way as
events taking entity participants. The main differ-
ence is that, when sub-events are considered, a de-
pendency path is found between the trigger of the
main event and the trigger of its sub-event, rather
than an annotated entity, as was shown above in Ex-
ample (2).
3.4 Extracting Events
The event detection and characterization pipeline
(Task 1) consists of three steps:
1. Determining whether a word is an event trigger.
2. If the word is an event trigger, identifying its
potential participant(s).
3. If the event trigger corresponds to a regula-
tory event and it has a potential sub-event
participant, determining in a recursive fashion
whether the sub-event is a valid event.
The first step is a simple dictionary lookup. Pro-
vided that a word is tagged as noun, verb or adjec-
tive, we check whether it is in our dictionary, and if
so, determine the event class for which it has a score
above the given threshold. This word is considered
the clue for an event.
We then apply our dependency-based rules to de-
termine whether any entity or event trigger (in the
case of regulatory events) in the sentence qualifies
as an argument of the event clue. Grammar rules are
applied in the order of simplicity; rules that involve
a direct dependency between the clue and any word
of the entity are considered first.
Once a list of potential participants is obtained
by consecutive application of the rules, one of
two things may happen: Provided that sub-events
are not involved and appropriate participants have
been identified (e.g., a Theme is found for a
Localization event), the event is simply added
to the extracted event list. Otherwise, we proceed
recursively to determine whether the sub-event par-
ticipant can be resolved to a simple event. If this
yields no such simple event in the end, the event in
question is rejected. In the following example, the
event triggered by inhibit is invalid even though its
Cause JunB is recognized, because its Theme, sub-
event triggered by activation, cannot be assigned a
Theme and therefore is considered invalid.
(4) ..., JunB, is shown to inhibit activation medi-
ated by JunD.
After events are extracted in this manner, two
postprocessing rules ensure increased accuracy.
One rule deals with a limited set of multi-
word event triggers. If a Regulation event
122
has been identified and the event trigger is
modified by positive or negative (or inflec-
tional forms positively, negatively), the event
class is updated to Positive regulation or
Negative regulation, respectively. The sec-
ond rule deals with the limitation of not allowing
multiple events on the same trigger and adds to
the extracted event list a Positive regulation
event, if a Gene expression event was recog-
nized for certain triggers, including overexpression
and several others related to transfection (e.g., trans-
fect, transfection, cotransfect).
Two grammatical constructions are crucial to de-
termining the event participants: coordination and
apposition. We summarize how they affect event ex-
traction below.
3.4.1 Coordination
Coordination plays two important roles in event
extraction:
1. When the event trigger is conjoined with an-
other word token, dependency relations con-
cerning the other conjunct are also considered
for participant identification.
2. When an event is detected and its participant
is found to be coordinated with other entities,
new events are created with the event trigger
and each of these entities. An exception are
Binding events, which may have multiple
Themes. In this case, we add conjunct entities
as the Themes of the base event.
Coordination between words is largely determined
by dependency relations. The participants of a de-
pendency with a type descending from conj (con-
junct) are considered coordinated (e.g., conj and,
conj or).
Recognizing that Stanford dependency parsing
misses some expressions of coordinated entities typ-
ical of biological text (in particular, those involving
parentheses), we implemented a few additional rules
to better resolve coordinated entities. These rules
stipulate that entities that have between them:
1. Only a comma (,) or a semi-colon (;)
2. A word with CC (coordinating conjunction)
part-of-speech tag
3. A complete parenthetical expression
4. Any combination of the above
are coordinated. For instance, in Example (5), we
recognize the coordination between interleukin-2
and IL-4, even though the parser does not:
(5) The activation of NFAT by TCR signals has
been well described for interleukin-2 (IL-2)
and IL-4 gene transcription in T cells.
conj and(interleukin-2,transcription)
3.4.2 Apposition
Words in an apposition construction are con-
sidered equivalent for event extraction purposes.
Therefore, if an appropriate dependency exists be-
tween a word and the trigger and the word is in ap-
position with an entity, that entity is marked as the
event participant. In Example 6, the appos (apposi-
tive) dependency shown serves to extract the event
Positive regulation (Trigger=upregulation,
Theme=intercellular adhesion molecule-1):
(6) ... upregulation of the lung vascular adhesion
molecule, intercellular adhesion molecule-1,
was greatly reduced by...
appos(molecule,molecule-1)
prep of(upregulation,molecule)
The dependencies that we consider to encode ap-
position constructions are: appos (appositive), ab-
brev (abbreviation), prep {including, such as, com-
pared to, compared with, versus} (prepositional
modifier marked with including, such as, compared
to, compared with or versus).
3.5 Speculation and Negation Detection
Once an event list is obtained for a sentence,
our speculation and negation module determines
whether these events are speculated and/or negated,
using additional dependency-based heuristics that
consider the dependencies between the event trigger
and speculation/negation cues.
3.5.1 Speculation Recognition
We refined an existing speculation detection mod-
ule in two ways for Task 3. First, we noted that
modal verbs (e.g., may) and epistemic adverbs (e.g.,
probably) rarely mark speculative contexts in the
123
training corpus, demonstrating the lack of a stan-
dardized notion of speculation among various cor-
pora. For Task 3, we ignored lexical cues in these
classes completely for increased accuracy. Sec-
ondly, corpus analysis revealed a new syntactic pat-
tern for speculation recognition. This pattern in-
volves the class of verbs that we called active cogni-
tion verbs (e.g., examine, evaluate, analyze, study,
investigate). We search for a Theme dependency
pattern between one of these verbs and an event trig-
ger and mark the event as speculated, if such a pat-
tern exists. Nominalizations of these verbs are also
considered. In Example (7), the event triggered by
effects is speculated, since effects is the direct object
(therefore, Theme) of studied:
(7) We have studied the effects of prednisone
(PDN), ... on the production of cytokines (IL-2,
IL-6, TNF-alpha, IL-10) by peripheral T lym-
phocytes...
3.5.2 Negation Detection
Negation detection is similar to speculation detec-
tion. Several classes of negation cues have been de-
termined based on corpus analysis and the negation
module negates events if there is an appropriate de-
pendency between one of these cues and the event
triggers. The lexical cues and the dependencies that
are sought are given in Table 1.
Negation Cue Dependency
lack, absence prep of(Cue,Trigger)
unable, <not> able, fail xcomp(Cue,Trigger)
inability, failure infmod(Cue, Trigger)
no, not, cannot det(Trigger, Cue)
Table 1: Negation cues and the corresponding depen-
dencies (xcomp: clausal complement, infmod: infinitival
modifier, det: determiner)
Additionally, participation of event triggers
in dependencies of certain types is sufficient
for negating the event it triggers. Such depen-
dency types are neg (negation) and conj negcc
(negated coordination). A neg dependency ap-
plies to event triggers only, while conj negcc is
sought between event participants, as well as
event triggers. Therefore, in Example (8), an event
(Positive regulation(Trigger=transactivate,
Theme: GM-CSF, Cause=ELF1)) is negated, based
on the dependencies below:
(8) Exogenous ETS1, but not ELF1, can transacti-
vate GM-CSF, ..., in a PMA/ionomycin depen-
dent manner.
conj negcc(ETS1, ELF1)
nsubj(transactivate, ETS1)
dobj(transactivate, GM-CSF)
Finally, if none of the above applies and the word
preceding the event trigger or one of the event partic-
ipants is a negation cue (no, not, cannot), the event
is negated.
4 Results and Discussion
Our event extraction system had one of the best per-
formances in the shared task. With the approxi-
mate span matching/approximate recursive match-
ing evaluation criteria, in Task 1, we were ranked
third, while our speculation and negation detection
module performed best among the six participating
systems in Task 3. Not surprisingly, our system fa-
vors precision, typical of rule-based systems. Full
results are given in Table 2.
The results reported are at goodness score thresh-
old of .08. Increasing the threshold increases preci-
sion, while lowering recall. The threshold was de-
termined empirically.
Our results confirm the usefulness of dependency
relations as foundation for event extraction systems.
There is much room for improvement, particularly in
terms of recall, and we believe that incremental na-
ture of our system development accommodates such
improvements fairly easily.
Our view of event triggers (?once a trigger, always
a trigger?), while simplistic, provides a good start-
ing point by greatly reducing the number of trigger
candidates in a sentence and typed dependencies to
consider. However, it also leads to errors. One such
example is given in Example (9):
(9) We show that ..., and that LPS treatment en-
hances the oligomerization of TLR2.
where we identify the event Binding (Trig-
ger=oligomerization,Theme=TLR2). We consider
oligomerization a reliable trigger, since it occurs
twice in the training corpus, both times as event trig-
gers. However, in this instance, it does not trigger
124
Event Class Recall Precis. F-score
Localization 35.63 92.54 51.45
Binding 20.46 40.57 27.20
Gene expression 55.68 79.45 65.47
Transcription 15.33 60.00 24.42
Protein catabolism 64.29 56.25 60.00
Phosphorylation 69.63 95.92 80.69
EVT-TOTAL 43.10 73.47 54.33
Regulation 24.05 45.75 31.53
Positive regulation 28.79 50.45 36.66
Negative regulation 26.65 51.53 35.13
REG-TOTAL 27.47 49.89 35.43
Negation 14.98 50.75 23.13
Speculation 16.83 50.72 25.27
MOD-TOTAL 15.86 50.74 24.17
ALL-TOTAL 32.68 60.83 42.52
Table 2: Evaluation results
an event. This narrow view also leads to recall er-
rors, in which we do not recognize an event trigger
as such, simply because we have not encountered it
in the training corpus, or it does not have an appro-
priate part-of-speech tag. A more sophisticated trig-
ger learning approach could aid in better detecting
event triggers.
We dealt with some deficiencies of Stanford de-
pendency parsing through additional rules, as de-
scribed in Section 3.3. However, many depen-
dency errors are still generated, due to the com-
plexity of biological text. For instance, in Exam-
ple (10), there is a coordination construction be-
tween NF-kappaB nuclear translocation and tran-
scription of E-selectin and IL-8. However, this con-
struction is missed and an erroneous prep of de-
pendency is found, leading to two false positive
errors: Localization (Trigger=translocation,
Theme=E-selectin) and Localization (Trig-
ger=translocation, Theme=IL-8).
(10) ... leading to NF-kappaB nuclear translocation
and transcription of E-selectin and IL-8, which
results in ...
conj and(transcription, translocation)
prep of(translocation, E-selectin)
conj and(E-selectin, IL-8)
These errors can be corrected via other ?corrective?
dependency paths; however, first, a closer examina-
tion of such error patterns is necessary.
In other instances, the required dependency is
completely missed by the parser, leading to recall
errors. For instance, in Example (11), we are un-
able to recognize two events (Regulation
(Trigger=regulation, Theme=4E-BP1) and
Regulation (Trigger=regulation, Theme=4E-
BP2)), due to lack of apposition dependencies
between repressors and 4E-BP1 or 4E-BP2:
(11) ... specific regulation of two repressors of
translation initiation, 4E-BP1 and 4E-BP2.
prep of(regulation,repressors)
prep of(repressors, initiation)
conj and(intiation, 4E-BP1)
conj and(initiation, 4E-BP2)
Typical of rule-base systems, we miss events ex-
pressed using less frequent patterns. Event partic-
ipants expressed as prepositional modifiers marked
with from is one such case. An example is given
below:
(12) Calcineurin activates transcription from the
GM-CSF promoter ...
In this case, the event Transcription (Trig-
ger=transcription, Theme=GM-CSF) is missed. It is
fairly easy to add a rule to address such occurrences.
We have not attempted to resolve anaphoric ex-
pressions for the shared task, which led to a fair
number of recall errors. In a similar vein, we ig-
nored events spanning multiple sentences. We ex-
pect that several studies addressing anaphora res-
olution in biomedical text (Castan?o et al, 2002;
Gasperin and Briscoe, 2008) will inform our near
future efforts in this area.
Evaluation results regarding Task 3 may seem
poor at first; however, most of the errors concern
misidentified or missed base events. Thus, in this
section, we focus on errors specifically triggered by
speculation and negation module. In the develop-
ment corpus, we identified 39 speculation instances,
4 of which were errors due to speculation process-
ing. Of 95 annotated speculation instances, 7 were
missed due to deficiencies in speculation processing.
125
Similarly, negation processing led to 5 false posi-
tives in 31 negation instances we identified and to 5
false negatives in 107 annotated negation instances.
We found that speculation false positive er-
rors are exclusively cases for which speculation
could be argued. For instance, in Example (13),
we recognize that appears to scopes over event
Negative regulation (Trigger=negatively
regulate, Theme=IL-2R), rendering it speculative.
However, it is not annotated as such. This is
further evidence for the difficulty of annotating such
phenomena correctly and consistently, since the
exact meaning is somewhat elusive.
(13) An unidentified Ets family protein binds to the
EBS overlapping the consensus GAS motif and
appears to negatively regulate the human IL-
2R alpha promoter.
Negation pattern that involves negation cues
(no,not,cannot) in the token preceding an event trig-
ger or participant, a pattern initially considered to
increase recall, caused most of negation false posi-
tive errors. An example is given in (14):
(14) The finding that HL-60/vinc/R cells respond to
TPA with induction of a monocytic phenotype,
but not c-jun expression, suggests that ...
Complex and less frequent patterns of expressing
speculation and negation were responsible for more
recall errors. Two such examples are given below:
(15) (a) These results ... and suggest a molecular
mechanism for the inhibition of TLR2 by
DN variants.
(b) Galectin-3 is ... and is expressed in many
leukocytes, with the notable exception of
B and T lymphocytes.
In (15a), speculation is detected; however, we are
unable to recognize that it scopes over the event
triggered by inhibition. In (15b), the prepositional
phrase, with the notable exception, is not considered
to indicate negation.
5 Conclusions and Future Work
We explored a rule-based approach to biological
event detection driven by typed dependency rela-
tions. This study marks our first foray into bio-event
extraction in a general way and, thus, we consider
the results very encouraging. In one area we investi-
gated before, speculation detection, our system per-
formed best and this confirms the portability and ex-
tensibility of our approach.
Modest recall figures point to areas of improve-
ment. We plan to address anaphora resolution and
multiple sentence spanning events in the near fu-
ture. Our na??ve approach to event triggers needs
refinement and we believe that sophisticated super-
vised machine learning techniques may be helpful.
In addition, biomedical lexical resources, includ-
ing UMLS SPECIALIST Lexicon (McCray et al,
1994), may be useful in improving event trigger
detection. Finally, dependency relations based on
the Stanford Parser provided better performance in
our case, in contrast to general consensus that those
based on Charniak Parser (Charniak and Johnson,
2005) are superior, and this, too, deserves further in-
vestigation.
References
C B Ahlers, M Fiszman, D Demner-Fushman, F M Lang,
and T C Rindflesch. 2007. Extracting semantic predi-
cations from Medline citations for pharmacogenomics.
Pac Symp Biocomput, pages 209?220.
A Airola, S Pyysalo, J Bjo?rne, T Pahikkala, F Ginter, and
T Salakoski. 2008. All-paths graph kernel for protein-
protein interaction extraction with evaluation of cross-
corpus learning. BMC Bioinformatics, 9 Suppl 11:s2.
M Averbuch, T Karson, B Ben-Ami, O Maimon, and
L Rokach. 2004. Context-sensitive medical informa-
tion retrieval. In Proc MEDINFO-2004, pages 1?8.
C Blaschke and A Valencia. 2001. The potential use
of SUISEKI as a protein interaction discovery tool.
Genome Inform, 12:123?134.
C Blaschke, M A Andrade, C Ouzounis, and A Valencia.
1999. Automatic extraction of biological information
from scientific text: protein-protein interactions. In
Proc Int Conf Intell Syst Mol Biol, pages 60?67.
R Bunescu, R Mooney, A Ramani, and E Marcotte. 2006.
Integrating co-occurrence statistics with information
extraction for robust retrieval of protein interactions
from Medline. In Proc BioNLP Workshop on Link-
ing Natural Language Processing and Biology, pages
49?56.
J Castan?o, J Zhang, and J Pustejovsky. 2002. Anaphora
resolution in biomedical literature. In Proc Interna-
tional Symposium on Reference Resolution for NLP.
126
W W Chapman, W Bridewell, P Hanbury, G F Cooper,
and B G Buchanan. 2001. A simple algorithm for
identifying negated findings and diseases in discharge
summaries. J Biomed Inform, 34(5):301?310.
E Charniak and M Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc 43rd Meeting of the Association for Computa-
tional Linguistics, pages 173?180.
N Daraselia, A. Yuryev, S Egorov, S Novichkova,
A Nikitin, and I Mazo. 2004. Extracting human pro-
tein interactions from MEDLINE using a full-sentence
parser. Bioinformatics, 20(5):604?611.
M C deMarneffe, B MacCartney, and C D Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc 5th International Con-
ference on Language Resources and Evaluation, pages
449?454.
J Ding, D Berleant, D Nettleton, and E Wurtele. 2002.
Mining MEDLINE: abstracts, sentences, or phrases?
Pac Symp Biocomput, 7:326?337.
C Friedman, P Kra, M Krauthammer, H Yu, and A Rzhet-
sky. 2001. GENIES: a natural-langauge processing
system for the extraction of molecular pathways from
journal articles. Bioinformatics, 17(1):74?82.
K Fundel, R Ku?ffner, and R Zimmer. 2007. RelEx re-
lation extraction using dependency parse trees. Bioin-
formatics, 23(3):365?371.
C Gasperin and T Briscoe. 2008. Statistical anaphora
resolution in biomedical texts. In Proc COLING 2008.
I M Goldin and W W Chapman. 2003. Learning to detect
negation with not in medical texts. In Proc Workshop
on Text Analysis and Search for Bioinformatics at the
26th ACM SIGIR Conference.
T K Jenssen, A Laegreid, J Komorowski, and E Hovig.
2001. A literature network of human genes for high-
throughput analysis of gene expression. Nat Genet,
28:21?28.
H Kilicoglu and S Bergler. 2008. Recognizing specu-
lative language in biomedical research articles: a lin-
guistically motivated perspective. BMC Bioinformat-
ics, 9 Suppl 11:s10.
D Klein and C D Manning. 2003. Accurate unlexicalized
parsing. In Proc 41th Meeting of the Association for
Computational Linguistics, pages 423?430.
G Leroy, H Chen, and J D Martinez. 2003. A shallow
parser based on closed-class words to capture relations
in biomedical text. Journal of Biomedical Informatics,
36:145?158.
M Light, X Y Qiu, and P Srinivasan. 2004. The language
of bioscience: facts, speculations, and statements in
between. In BioLINK 2004: Linking Biological Liter-
ature, Ontologies and Databases, pages 17?24.
A T McCray, S Srinivasan, and A C Browne. 1994. Lex-
ical methods for managing variation in biomedical ter-
minologies. In Proc 18th Annual Symposium on Com-
puter Applications in Medical Care, pages 235?239.
B Medlock and T Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature.
In Proc 45th Meeting of the Association for Computa-
tional Linguistics, pages 992?999.
I A Mel?c?uk. 1988. Dependency syntax: Theory and
Practice. State University Press of New York, NY.
R Morante, A Liekens, and W Daelemans. 2008. Learn-
ing the scope of negation in biomedical text. In Proc
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 715?724.
P G Mutalik, A Deshpande, and P M Nadkarni. 2001.
Use of general-purpose negation detection to augment
concept indexing of medical documents: A quantita-
tive study using the UMLS. J Am Med Inform Assoc,
8(6):598?609.
F Rinaldi, G Schneider, K Kaljurand, M Hess, C Andro-
nis, O Konstandi, and A Persidis. 2007. Mining of
relations between proteins over biomedical scientific
literature using a deep-linguistic approach. Artif. In-
tell. Med., 39(2):127?136.
T C Rindflesch, L Tanabe, J N Weinstein, and L Hunter.
2000. EDGAR: Extraction of drugs, genes, and rela-
tions from the biomedical literature. In Proc Pacific
Symposium on Biocomputing, pages 514?525.
O Sanchez-Graillet and M Poesio. 2007. Negation of
protein protein interactions: analysis and extraction.
Bioinformatics, 23(13):424?432.
J Schuman and S Bergler. 2006. Postnominal prepo-
sitional phrase attachment in proteomics. In Proc
BioNLP Workshop on Linking Natural Language Pro-
cessing and Biology, pages 82?89.
G Szarvas. 2008. Hedge classification in biomedical
texts with a weakly supervised selection of keywords.
In Proc 46th Meeting of the Association for Computa-
tional Linguistics, pages 281?289.
V Vincze, G Szarvas, R Farkas, G Mora, and J Csirik.
2008. The BioScope corpus: biomedical texts anno-
tated for uncertainty, negation and their scopes. BMC
Bioinformatics, 9 Suppl 11:S9.
A Yakushiji, Y Miyao, Y Tateisi, and J Tsujii. 2005.
Biomedical event extraction with predicate-argument
structure patterns. In Proc First International Sympo-
sium on Semantic Mining in Biomedicine, pages 60?
69.
P Zweigenbaum, D Demner-Fushman, H Yu, and K B
Cohen. 2007. Frontiers of biological text mining: cur-
rent progress. Briefings in Bioinformatics, 8(5):358?
375.
127
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 294?300,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UConcordia: CLaC Negation Focus Detection at *Sem 2012
Sabine Rosenberg and Sabine Bergler
CLaC Lab, Concordia University
1455 de Maisonneuve Blvd West, Montre?al, QC, Canada, H3W 2B3
sabin ro@cse.concordia.ca, bergler@cse.concordia.ca
Abstract
Simply detecting negation cues is not suffi-
cient to determine the semantics of negation,
scope and focus must be taken into account.
While scope detection has recently seen re-
peated attention, the linguistic notion of focus
is only now being introduced into computa-
tional work. The *Sem2012 Shared Task is
pioneering this effort by introducing a suitable
dataset and annotation guidelines. CLaC?s
NegFocus system is a solid baseline approach
to the task.
1 Introduction
Negation has attracted the attention of the NLP com-
munity and we have seen an increased advance in
sophistication of processing tools. In order to assess
factual information as asserted or not, it is important
to distinguish the difference between
(1) (a) Newt Gingrich Not Conceding Race
After Losing Florida Primary
(b) Newt Gingrich Conceding Race Af-
ter Losing Florida Primary
This distinction is important and cannot be properly
inferred from the surrounding context, not conced-
ing a race after losing is in fact contrary to expecta-
tion in the original headline (1a), and the constructed
(1b) is more likely in isolation.
Negation has been addressed as a task in itself,
rather than as a component of other tasks in recent
shared tasks and workshops. Detection of negation
cues and negation scope at CoNLL (Farkas et al,
2010), BioNLP (Kim et al, 2011) and the Negation
and Speculation in NLP Workshop (Morante and
Sporleder, 2010) laid the foundation for the *Sem
2012 Shared Task. While the scope detection has
been extended to fictional text in this task, an impor-
tant progression from the newspaper and biomedi-
cal genres, the newly defined Focus Detection for
Negation introduces the important question: what is
the intended opposition in (1a)? The negation trig-
ger is not, the scope of the negation is the entire
verb phrase, but which aspect of the verb phrase is
underscored as being at variance with reality, that
is, which of the following possible (for the sake of
linguistic argument only) continuations is the more
likely one:
(2) i . . . , Santorum does.
(?Newt Gingrich)
ii . . . , Doubling Efforts (?concede)
iii . . . , Demanding Recount (?race)
iv . . . , Texas redistricting at fault
(?Florida)
This notion of focus of negation is thus a prag-
matic one, chosen by the author and encoded with
various means. Usually, context is necessary to de-
termine focus. Often, different possible interpreta-
tions of focus do not change the factual meaning of
the overall text, but rather its coherence. In (1 a) the
imagined possible contexts (2 ii) and (2 iii) closely
correspond to a simple negation of (1 b), (2 i) and
(2 iv) do not feel properly represented by simply
negating (1 b). This level of interpretation is con-
tentious among people and it is the hallmark of well-
written, well-edited text to avoid unnecessary guess-
work while at the same time avoiding unnecessary
294
clarifying repetition. The potential for ambiguity is
demonstrated by Example (3) from (Partee, 1993),
where it is questionable whether the speaker in fact
has possession of the book in question.
(3) I didn?t get that book from Mary
Here, if the focus is from Mary, it would be likely
that the speaker has possion of the book, but received
it some other way. If the focus is that book, the
speaker does not have possession of it.
It is important to note hat this notion of focus is
not syntactically determined as shown in (3) (even
though we use syntactic heuristics here to approxi-
mate it) but pragmatically and it correlates with pro-
nunciation stress, as discussed in linguistics by (Han
and Romero, 2001). More recently, focus negation
has been identified as a special use (Poletto, 2008).
The difference of scope and focus of negation are
elaborated by (Partee, 1993), and have been used for
computational use by (Blanco and Moldovan, 2011).
The *Sem 2012 Task 2 on Focus Detection builds
on recent negation scope detection capabilities and
introduces a gold standard to identify the focus item.
Focus of negation is annotated over 3,993 sentences
in the WSJ section of the Penn TreeBank marked
with MNEG in PropBank. It accounts for verbal,
analytical and clausal relation to a negation trigger;
the role most likely to correspond to the focus was
selected as focus. All sentences of the training data
contain a negation. A sample annotation from the
gold standard is given in (4), where PropBank se-
mantic roles are labelled A1, M-NEG, and M-TMP
and focus is underlined (until June).
(4) ?AdecisionA1? is?n
?tM?NEG? expected
? until June M?TMP ?
2 Previous Work
A recent study in combining regular pattern ex-
traction with parse information for enhanced in-
dexing of radiology reports showed effective de-
tection of negated noun phrases for that corpus
(Huang and Lowe, 2007). NegFinder (Mutalik et
al., 2001) detects negated concepts in dictated med-
ical documents with a simple set of corpus spe-
cific context-free rules, and they observe that in
their corpus ?One of the words no, denies/denied,
not, or without was present in 92.5 percent of
all negations.? Interestingly, several of their rules
concern coordination (and, or) or prepositional
phrase attachment patterns (of, for). NegEx (Chap-
man et al, 2001) is publicly available and main-
tained and updated with community-enhanced trig-
ger lists (http://code.google.com/p/negex/
wiki/NegExTerms). NegEx ?locates trigger terms
indicating a clinical condition is negated or possi-
ble and determines which text falls within the scope
of the trigger terms.? NegEx uses a simple regular
expression algorithm with a small number of nega-
tion phrases and focuses on a wide variety of trig-
gers but limits them to domain relevant ones. Con-
sequently, the trigger terms and conditions are heav-
ily stacked with biomedical domain specific terms.
Outside the biomedical text community, sentiment
and opinion analysis research features negation de-
tection (Wilson, 2008). Current gold standard anno-
tations for explicit negation as well as related phe-
nomena include TIMEBANK (Pustejovsky et al,
2003), MPQA (Wiebe et al, 2005), and Bio-Scope
(Vincze et al, 2008).
(Wiegand et al, 2010) presents a flat feature com-
bination approach of features of different granularity
and analytic sophistication, since in opinion mining
the boundary between negation and negative expres-
sions is fluid.
3 CLaC?s NegFocus
CLaC Labs? general, lightweight negation mod-
ule is intended to be embedded in any process-
ing pipeline. The heuristics-based system is com-
posed of three modules for the GATE (Cunningham
et al, 2011) environment: the first component de-
tects and annotates explicit negation cues present in
the corpus, the second component detects and an-
notates the syntactic scope of the detected instances
of verbal negation, and the third component im-
plements focus heuristics for negation. The first
two steps were developed independently, drawing on
data from MPQA (Wiebe et al, 2005) and TIME-
BANK (Pustejovsky et al, 2003) with validation on
Bio-Scope (Vincze et al, 2008). The third step has
been added based on data for the *Sem 2012 chal-
lenge and is intended to validate both, the first two
?preprocessing? steps and the simple heuristic ap-
proximation of focus.
295
3.1 Data Preprocessing
Parser-based, our focus detection pipeline requires
as input entire sentences. Therefore, the first step
requires the extraction of each sentence utilizing the
supplied token numbers and save them in the correct
format. The system then performs standard prepro-
cessing: sentence splitting, tokenization, parsing us-
ing the Stanford Parser (Klein and Manning, 2003;
de Marneffe and Manning, 2006) and morphologi-
cal preprocessing. Note that NegFocus does not use
any PropBank annotations nor other provided train-
ing annotations, resulting in an independent, parser-
based stand-alone module.
3.2 Detection of Negation Triggers
The Focus Detection task only considers the explicit
negation cues not, nor, never. The first step in Neg-
Focus is thus to identify these triggers in the sen-
tences using an explicit negation trigger word list.
3.3 Syntactic Scope Detection
The Focus Detection task only considers negation of
verbs. Thus, NegFocus extracts the syntactic com-
plement of the verb to form the negated verb phrase
from the dependency graphs (de Marneffe and Man-
ning, 2006). We annotate this as the syntactic scope
of the negation. Note that while we use dependency
graphs, our syntactic scope is based on the parse tree
and differs from the notion of scope encoded in Bio-
Scope (Vincze et al, 2008) and the related format
used for the *Sem 2012 Negation Scope Annotation
task, which represent in our opinion the pragmatic
notion of scope for the logical negation operation.
Syntactic scope detection is thus considered to be
a basic stepping stone towards the pragmatic scope
and since the Focus Detection task does not provide
scope annotations, we use syntactic scope here to
validate this principle.
Our heuristics are inspired by (Kilicoglu and
Bergler, 2011). In the majority of cases the depen-
dency relation which identifies the syntactic scope
is the neg relation. Traditionally, parse trees iden-
tify scope as lower or to the right of the trigger term,
and our scope module assumes these grammatical
constraints, yet includes the verb itself for the pur-
poses of the shared task. Example 5, from the train-
ing dataset ?The Hound of the Baskervilles? by Co-
nan Doyle for the *Sem 2012 Negation Scope Anno-
tation task, demonstrates our syntactic scope of the
negation (underlined), in contrast with the gold stan-
dard scope annotation (in brackets). The gold anno-
tation guidelines follow the proposal of Morante et
al. (Morante et al, 2011)1.
(5) [We did] not [drive up to the door] but
got down near the gate of the avenue.
3.4 Focus Heuristics
The third and final step for NegFocus is to annotate
focus in sentences containing verbal negations. Us-
ing the verbal negation scope annotations of the pre-
vious step, four focus heuristics are invoked:
3.4.1 Baseline
The Baseline heuristic for this component is de-
fined according to notions discussed in (Huddle-
ston and Pullum, 2002), where the last constituent
in the verb phrase of a clause is commonly the de-
fault location to place the heaviest stress, which we
here equate with the focus. Example (6) depicts an
instance where both NegFocus results (underlined)
and the gold focus annotation (in brackets) match
exactly. The baseline heuristic achieves 47.4% re-
call and 49.4% precision on the training set and 47%
recall and 49.7% precision on the test set.
(6) NBC broadcast throughout the entire
night and did not go off the air
[until noon yesterday] .
As pointed out in Section 3.3, focus is not always
determined by scope (Partee, 1993). The training
data gave rise to three additional heuristics.
3.4.2 Adverb
When an adverb is directly preceding and con-
nected through an advmod dependency relation to
the negated verb, the adverb constituent is deter-
mined as the focus of the negation.
(7) Although it may not be [legally] obli-
gated to sell the company if the buy-
out group can?t revive its bid, it may
have to explore alternatives if the buyers
come back with a bid much lower than
the group ?s original $ 300-a-share pro-
posal.
1http://www.clips.ua.ac.be/sites/default/files/ctrs-n3.pdf
296
3.4.3 Noun Subject Passive
Passives are frequent in newspaper articles and
passive constructions front what would otherwise
be the verb complement. Thus the fronted mate-
rial should be eligible for focus assignment. Pas-
sives are flagged through the nsubjpass dependency,
and for cases where the negated verb participates in
an nsubjpass relation and has no other complement,
the nsubjpass is determined as the focus.
(8) [Billings] were n?t disclosed.
3.4.4 Negation Cue
The challenge data has cases where the negation
cue itself is its own focus. These cases seem to be
pragmatically determined. Error cases were reduced
when determining the negation cue to be its own fo-
cus in two cases. The first case occurs when the
negated verb has an empty complement (and is not a
passive construction), as in Example 9.
(9) Both said the new plan would [n?t] work.
The second case occurs when the negated verb
embeds a verb that we identify as an implicit nega-
tion. We have a list of implicit negation triggers
largely compiled from MPQA (Wiebe et al, 2005).
Implicit negations are verbs that lexically encode a
predicate and a negation, such as reject or fail.
(10) Black activist Walter Sisulu said the
African National Congress would [n?t]
reject violence as a way to pressure
the South African government into con-
cessions that might lead to negotiations
over apartheid . . .
4 Results
Ordering the heuristics impacts on recall. We place
the most specific heuristics before the more general
ones to avoid starvation effects. For example, the
adverb heuristic followed by the noun subject pas-
sive heuristic achieved better results at the begin-
ning, since they are more specific then the negation
cue heuristic.
Table 1 shows the performance of the heuristics
of NegFocus on the test set and on the development
set. We observe that the heuristics are stable across
the two sets with a 60% accuracy on the test set. The
worst performer is the baseline, which is very coarse
for such a semantically sophisticated task: assuming
that the last element of the negated verb phrase is the
focus is truly a baseline.
heuristic corr. incorr. acc.
Test Set
baseline 336 238 .59
adverb 26 4 .87
nsubjpass 10 8 .56
neg. cue 33 20 .62
Development Set
baseline 257 174 .6
adverb 15 6 .71
nsubjpass 10 6 .63
neg. cue 21 19 .53
Figure 1: Performance of NegFocus heuristics
The overall performance of the system is almost
balanced between precision and recall with an f-
measure of .58.
Test Set
Precision 60.00 [405/675]
Recall 56.88 [405/712]
F-score 58.40
Development Set
Precision 59.65 [303/508]
Recall 57.06 [303/531]
F-score 58.33
Figure 2: System Results
Our heuristics, albeit simplistic, are based on lin-
guistically sound observations. The heuristic nature
allows additional heuristics that are more tailored to
a corpus or a task to be added without incurring un-
manageable complexity, in fact each heuristic can
be tested on the development set and can report on
the test set to monitor its performance. The heuris-
tics will also provide excellent features for statistical
systems.
5 Error Analysis
We distinguish 11 classes of errors on the test set.
The classes of errors depicted in Table (3) indi-
cates that the classes of errors and their frequencies
are consistent across the different data sets. The
third error class in Table (3) is of particular inter-
297
Error Type Test Set Dev Set
1 Precision Errors: Verbal Negation Scope not found by NegFocus 37 23
2 Focus Mismatch: gold focus annotation is the neg. cue 138 112
3 Focus Mismatch: gold focus annotation is a constituent triggered
by the nsubj dependency to the negated verb
44 16
4 Focus Mismatch: gold focus annotation is the constituent trig-
gered by the nsubjpass dependency
7 12
5 Focus Mismatch: gold focus annotation is an adverb triggered by
the advmod dependency with the verb, but is not adjacent to the
verb
14 4
6 Partial Match: the spans of the gold focus annotation and NegFo-
cus annotation overlap
6 8
7 Focus Mismatch: gold focus annotation is not contained within
the NegFocus Syntactic Scope
4 5
8 NegFocus Syntactic Scope annotation error 10 9
9 Focus Mismatch: Miscellaneous errors 27 25
10 Focus Mismatch: gold focus annotation matches CLaC baseline
heuristic, however another CLaC focus heuristic was chosen
3 3
11 Focus Mismatch: gold focus annotation contains two discontinu-
ous focus annotation spans
17 11
TOTAL 307 228
Figure 3: System Errors
est to us, as it highlights the different interpretations
of verbal negation scope. NegFocus will not include
the noun subject in the syntactic negation scope, and
therefore the noun subject constituent is never a fo-
cus candidate as required in Example (11).
(11) In New York, [a spokesman for American
Brands] would n?t comment.
Similarly, the seventh error class in Table (3) con-
tains focus annotations that are not contained in
NegFocus negation scopes. Example (12) shows an
error where the sentence begins with a prepositional
phrase that is annotated as the gold focus.
(12) [On some days], the Nucor plant does n?t
produce anything.
We disagree with the gold annotations on this and
similar cases: the prepositional phrase on some days
is not negated, it provides a temporal specification
for the negated statement the Nucor plant produces
something and in our opinion, the negation negates
something, contrasting it with
(13) [On some days], the Nucor plant does n?t
produce a lot.
which allows for some production, which indi-
cates to us that without context information, low fo-
cus is warranted here.
NegFocus incorporates a focus heuristic for deter-
mining the passive noun subject constituent as the
focus of the negation, however only in cases where
the negated verb has an empty complement. The
fourth error class contains errors in focus determina-
tion where this heuristic fails and where the passive
subject is the gold focus despite the complement of
the negated verb not being empty, requiring further
analysis:
(14) To simplify the calculations , [com-
missions on the option and underlying
stock] are n?t included in the table.
NegFocus determines an adverb directly preced-
ing the verb trigger as the focus of the negation, but,
as described in the fifth error class, the gold focus
annotations in a few cases determine adverbs to be
the focus of the negation even when they don?t di-
rectly precede the verb, but are linked by the adv-
mod relation, as in Example (15). When we exper-
imented with relaxing the adjacency constraint, re-
298
Error Type Test Set Dev Set
1 NegFocus annotation is adverb 2 3
2 NegFocus annotation is passive noun subject 7 4
3 NegFocus Scope Error 7 14
4 NegFocus baseline heuristic at variance with gold annotation 122 91
TOTAL 138 112
Figure 4: Negation cue annotation misses
sults suffered. This, too, is an area where we wish
to investigate whether any general patterns are pos-
sible and what additional resources they require to
be reliable.
(15) ? The intervention has been friendly,
meaning that they [really] did n?t
have to do it, ? said Maria Fiorini
Ramirez, money-market economist at
Drexel Burnham Lambert Inc .
The majority of NegFocus errors occur in the sec-
ond error class. Table (4) further analyzes the second
error class, where the gold annotation puts the nega-
tion trigger in the focus but NegFocus finds another
focus (usually in the verb complement).
The gold standard annotations place the focus of
the negation of verb v on the negation trigger if it
cannot be inferred that an action v occurred (Blanco
and Moldovan, 2011). NegFocus will only make this
assumption when the verb complement constituent
is empty, otherwise the baseline focus heuristic will
be triggered, as depicted in Example (16).
(16) AMR declined to comment , and
Mr. Trump did [n?t] respond
to requests for interviews.
Furthermore, the CLaC system will choose to trigger
the subject passive focus heuristic in the case where
the verb complement constituent is empty, and the
passive noun subject is present. In contrast, the gold
standard annotations do not necessarily follow this
heuristic as seen in Example (17).
(17) That is n?t 51 %, and the claim is [n?t]
documented .
Lastly, the gold focus annotations include focus
spans which are discontinuous. NegFocus will only
detect one continuous focus span within one in-
stance of a verbal negation. The eleventh error class
includes those cases where NegFocus matches one
of the gold focus spans but not the other as seen in
Example (18).
(18) [The payments] aren?t expected
[to have an impact on coming operating
results], Linear added .
These error cases show that more analysis of the
data, but also of the very notion of focus, is neces-
sary.
6 Conclusion
We conclude that this experiment confirmed the hy-
pothesis that negation trigger detection, syntactic
scope determination, and focus determination are
usefully modelled as a pipeline of three simple mod-
ules that apply after standard text preprocessing and
dependency parsing. Approximating focus from a
principled, linguistic point of view proved to be a
quick and robust exercise. Performance on develop-
ment and test sets is nearly identical and in a range
around 58% f-measure. While the annotation stan-
dards as well as our heuristics warrant revisiting, we
believe that the value of the focus annotation will
prove its value beyond negation. The challenge data
provide a valuable resource in themselves, but we
believe that their true value will be shown by using
the derived notion of focus in downstream applica-
tions. For initial experiments, the simple NegFocus
pipeline is a stable prototype.
References
E. Blanco and D. Moldovan. 2011. Semantic represen-
tation of negation using focus detection. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies (ACL-HLT 2011), Portland, OR.
299
W. Chapman, W. Bridewell, P. Hanbury, G.F. Cooper, and
B. Buchanan. 2001. A simple algorithm for identi-
fying negated findings and diseases in discharge sum-
maries. Journal of Biomedical Informatics, 34(5):301-
310.
H. Cunningham, D. Maynard, K. Bontcheva, V. Tablan,
N. Aswani, I. Roberts, G. Gorrell, A. Funk,
A. Roberts, D. Damljanovic, T. Heitz, M.A. Green-
wood, H. Saggion, J. Petrak, Y. Li, and Wim P. 2011.
Text Processing with GATE (Version 6). GATE (April
15, 2011).
M. de Marneffe and C.D. Manning. 2006. Generating
typed dependency parses from phrase structure parses.
In LREC.
R. Farkas, V. Vincze, G.Mo?ra, J. Csirik, and G.Szarvas.
2010. The conll-2010 shared task: Learning to detect
hedges and their scope in natural language text. In
Proceedings of the Fourteenth Conference on Compu-
tational Natural Language Learning.
C-H. Han and M. Romero. 2001. Negation, focus and
alternative questions. In K. Megerdoomian and L.A.
Bar-el, editors, Proceedings of the West Coast Confer-
ence in Formal Linguistics XX, Somerville, MA. Cas-
cadilla Press.
Y. Huang and H.J. Lowe. 2007. A novel hybrid approach
to automated negation detection in clinical radiology
reports. Journal of the American Medical Informatics
Association : JAMIA, 14(3):304-311.
R.D. Huddleston and G.K. Pullum. 2002. The Cam-
bridge grammar of the English language. Cambridge
University Press, Cambridge, UK; New York.
H. Kilicoglu and S. Bergler. 2011. Effective bio-event
extraction using trigger words and syntactic dependen-
cies. Computational Intelligence, 27(4):583?609.
J.-D. Kim, Y. Wang, T. Takagi, and A. Yonezawa. 2011.
Overview of genia event task in bionlp shared task
2011. In Proceedings of BioNLP Shared Task 2011
Workshop at ACL-HLT.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics.
R. Morante and C. Sporleder, editors. 2010. NeSp-NLP
?10: Proceedings of the Workshop on Negation and
Speculation in Natural Language Processing, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
R. Morante, S. Schrauwen, and W. Daelemans. 2011.
Annotation of negation cues and their scope. guide-
lines v1.0. Technical report, CLiPS, University of
Antwerp.
P. G. Mutalik, A. Deshpande, and P. M. Nadkarni. 2001.
Use of general-purpose negation detection to augment
concept indexing of medical documents: a quantitative
study using the umls. Journal of the American Medi-
cal Informatics Association : JAMIA, 8(6):598-609.
B. Partee. 1993. On the ?scope of negation? and po-
larity sensitivity. In E. Hajicova, editor, Functional
Approaches to Language Description.
C. Poletto. 2008. The syntax of focus negation. Univer-
sity of Venice Working Papers in Linguistics, 18.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3).
M. Wiegand, B. Roth, D. Klakow, A. Balahur, and
A. Montoyo. 2010. A survey on the role of negation in
sentiment analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing (NeSp-NLP 2010).
Th. Wilson. 2008. Fine-Grained Subjectivity Analysis.
Ph.D. thesis, University of Pittsburgh. Intelligent Sys-
tems Program.
300
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 202?206, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
CLaC-CORE: Exhaustive Feature Combination for Measuring Textual
Similarity
Ehsan Shareghi
CLaC Laboratory
Concordia University
Montreal, QC H3G 1M8, CANADA
eh share@cse.concordia.ca
Sabine Bergler
CLaC Laboratory
Concordia University
Montreal, QC H3G 1M8, CANADA
bergler@cse.concordia.ca
Abstract
CLaC-CORE, an exhaustive feature combina-
tion system ranked 4th among 34 teams in the
Semantic Textual Similarity shared task STS
2013. Using a core set of 11 lexical features
of the most basic kind, it uses a support vector
regressor which uses a combination of these
lexical features to train a model for predicting
similarity between sentences in a two phase
method, which in turn uses all combinations
of the features in the feature space and trains
separate models based on each combination.
Then it creates a meta-feature space and trains
a final model based on that. This two step pro-
cess improves the results achieved by single-
layer standard learning methodology over the
same simple features. We analyze the correla-
tion of feature combinations with the data sets
over which they are effective.
1 Introduction
The Semantic Textual Similarity (STS) shared task
aims to find a unified way of measuring similarity
between sentences. In fact, sentence similarity is
a core element of tasks trying to establish how two
pieces of text are related, such as Textual Entailment
(RTE) (Dagan et al, 2006), and Paraphrase Recog-
nition (Dolan et al, 2004). The STS shared task was
introduced for SemEval-2012 and was selected as its
first shared task. Similar in spirit, STS differs from
the well-known RTE shared tasks in two important
points: it defines a graded similarity scale to mea-
sure similarity of two texts, instead of RTE?s binary
yes/no decision and the similarity relation is consid-
ered to be symmetrical, whereas the entailment rela-
tion of RTE is inherently unidirectional.
The leading systems in the 2012 competition used
a variety of very simple lexical features. Each sys-
tem combines a different set of related features.
CLaC Labs investigated the different combination
possibilities of these simple lexical features and
measured their performance on the different data
sets. Originally conceived to explore the space of
all possible feature combinations for ?feature com-
bination selection?, a two-step method emerged that
deliberately compiles and trains all feature combina-
tions exhaustively and then trains an SVM regressor
using all combination models as its input features.
It turns out that this technique is not nearly as pro-
hibitive as imagined and achieves statistically sig-
nificant improvements over the alternative of feature
selection or of using any one single combination in-
dividually.
We propose the method as a viable approach when
the characteristics of the data are not well under-
stood and no satisfactory training set is available.
2 Related Work
Recently, systems started to approach measuring
similarity by combining different resources and
methods. For example, the STS-2012 shared task?s
leading UKP (Ba?r et al, 2012) system uses n-grams,
string similarity, WordNet, and ESA, and a regres-
sor. In addition, they use MOSES, a statistical ma-
chine translation system (Koehn et al, 2007), to
translate each English sentence into Dutch, German,
and Spanish and back into English in an effort to in-
crease their training set of similar text pairs.
202
TakeLab (S?aric et al, 2012), in place two of the
2012 STS shared task, uses n-gram models, two
WordNet-based measures, LSA, and dependencies
to align subject-verb-object predicate structures. In-
cluding named-entities and number matching in the
feature space improved performance of their support
vector regressor.
(Shareghi and Bergler, 2013) illustrates two ex-
periments with STS-2012 training and test sets us-
ing the basic core features of these systems, outper-
forming the STS-2012 task?s highest ranking sys-
tems. The STS-2013 submission CLaC-CORE uses
the same two-step approach.
3 CLaC Methodology
Preprocessing consists of tokenizing, lemmatizing,
sentence splitting, and part of speech (POS) tagging.
We extract two main categories of lexical features:
explicit and implicit.
3.1 Explicit Lexical Features
Sentence similarity at the explicit level is based
solely on the input text and measures the similar-
ity between two sentences either by using an n-gram
model (ROUGE-1, ROUGE-2, ROUGE-SU4) or by
reverting to string similarity (longest common sub-
sequence, jaro, ROUGE-W):
Longest Common Subsequence (Allison and
Trevor, 1986) compare the length of the
longest sequence of characters, not necessarily
consecutive ones, in order to detect similarities
Jaro (Jaro, 1989) identifies spelling variation be-
tween two inputs based on the occurrence of
common characters between two text segments
at a certain distance
ROUGE-W (Lin et al, 2004a), a weighted version
of longest common subsequence, takes into ac-
count the number of the consecutive characters
in each match, giving higher score for those
matches that have larger number of consecu-
tive characters in common. This metric was de-
veloped to measure the similarity between ma-
chine generated text summaries and a manually
generated gold standard
ROUGE-1 unigrams (Lin et al, 2004a)
ROUGE-2 bigrams (Lin et al, 2004a)
ROUGE-SU4 4-Skip bigrams (including Uni-
grams) (Lin et al, 2004a)
3.2 Implicit Lexical Features
Sentence similarity at the implicit level uses exter-
nal resources to make up for the lexical gaps that
go otherwise undetected at the explicit level. The
synonymy of bag and suitcase is an example of an
implicit similarity. This type of implicit similarity
can be detected using knowledge sources such as
WordNet or Roget?s Thesaurus based on the Word-
Net::Similarity package (Pedersen et al, 2004) and
combination techniques (Mihalcea et al, 2006). For
the more semantically challenging non-ontologigal
relations, for example sanction and Iran, which lex-
ica do not provide, co-occurrence-based measures
like ESA are more robust. We use:
Lin (Lin, 1998) uses the Brown Corpus of Ameri-
can English to calculate information content of
two concepts? least common subsumer. Then
he scales it using the sum of the information
content of the compared concepts
Jiang-Conrath (Jiang and Conrath, 1997) uses the
conditional probability of encountering a con-
cept given an instance of its parent to calculate
the information content. Then they define the
distance between two concepts to be the sum
of the difference between the information con-
tent of each of the two given concepts and their
least common subsumer
Roget?s Thesaurus is another lexical resource and
is based on well-crafted concept classifica-
tion and was created by professional lexicogra-
phers. It has a nine-level ontology and doesn?t
have one of the major drawbacks of WordNet,
which is lack of links between part of speeches.
According to the schema proposed by (Jarmasz
and Szpakowicz, 2003) the distance of two
terms decreases within the interval of [0,16],
as the the common head that subsumes them
moves from top to the bottom and becomes
more specific. The electronic version of Ro-
get?s Thesaurus which was developed by (Jar-
masz and Szpakowicz, 2003) was used for ex-
tracting this score
203
Explicit Semantic Analyzer (Gabrilovich and
Markovitch, 2007) In order to have broader
coverage on word types not represented in
lexical resources, specifically for named enti-
ties, we add explicit semantic analyzer (ESA)
generated features to our feature space
3.3 CLaC-CORE
CLaC-CORE first generates all combinations of the
11 basic features (jaro, Lemma, lcsq, ROUGE-W,
ROUGE-1, ROUGE-2, ROUGE-SU4, roget, lin, jcn,
esa), that is 211 ? 1 = 2047 non-empty combina-
tions. The Two Phase Model Training step trains
a separate Support Vector Regressor (SVR) for
each combination creating 2047 Phase One Models.
These 2N ? 1 predicted scores per text data item
form a new feature vector called Phase Two Fea-
tures, which feed into a SVR to train our Phase Two
Model.
On a standard 2 core computer with ?100 GB
of RAM using multi-threading (thread pool of size
200, a training process per thread) it took roughly 15
hours to train the 2047 Phase One Models on 5342
text pairs and another 17 hours to build the Phase
Two Feature Space for the training data. Building
the Phase Two Feature Space for the test sets took
roughly 7.5 hours for 2250 test pairs.
For the current submissions we combine all train-
ing sets into one single training set used in all of our
submissions for the STS 2013 task.
4 Analysis of Results
Our three submission for STS-2013 compare a base-
line of Standard Learning (RUN-1)with two ver-
sions of our Two Phase Learning (RUN-2, RUN-
3). For the Standard Learning baseline, one regres-
sor was trained on the training set on all 11 Basic
Features and tested on the test sets. For the remain-
ing runs the Two Phase Learning method was used.
All our submissions use the same 11 Basic Features.
RUN-2 is our main contribution. RUN-3 is identical
to RUN-2 except for reducing the number of support
vectors and allowing larger training errors in an ef-
fort to assess the potential for speedup. This was
done by decreasing the value of ? (in the RBF ker-
nel) from 0.01 to 0.0001, and decreasing the value of
C (error weight) from 1 to 0.01. These parameters
resulted in a smoother and simpler decision surface
but negatively affected the performance for RUN-3
as shown in Table 1.
The STS shared task-2013 used the Pearson Cor-
relation Coefficient as the evaluation metric. The re-
sults of our experiments are presented in Table 1.
The results indicate that the proposed method, RUN-
rank headlines OnWN FNWN SMT
RUN-1 10 0.6774 0.7667 0.3793 0.3068
RUN-2 7 0.6921 0.7367 0.3793 0.3375
RUN-3 46 0.5276 0.6495 0.4158 0.3082
STS-bl 73 0.5399 0.2828 0.2146 0.2861
Table 1: CLaC-CORE runs and STS baseline perfor-
mance
2, was successful in improving the results achieved
by our baseline RUN-1 ever so slightly (the confi-
dence invervals at 5% differ to .016 at the upper end)
and far exceeds the reduced computation version of
RUN-3.
4.1 Successful Feature Combinations
Having trained separate models based on each sub-
set of features we can use the predicted scores gen-
erated by each of these models to calculate their cor-
relations to assess which of the feature combinations
were more effective in making predictions and how
this most successful combination varies bewteen the
different datasets.
best worst
headlines [ ROUGE-1 ROUGE-
SU4 esa lem]
[jcn lem lcsq]
0.7329 0.3375
OnWN [ROUGE-1 ROUGE-
SU4 esa lin jcn roget
lem lcsq ROUGE-W ]
[jaro]
0.7768 0.1425
FNWN [roget ROUGE-1
ROUGE-SU4]
[ROUGE-2 lem lcsq]
0.4464 -0.0386
SMT [lin jcn roget
ROUGE-1]
[esa lcsq]
0.3648 0.2305
Table 2: Best and worst feature combination performance
on test set
Table 2 lists the best and worst feature combina-
tions on each test set. ROUGE-1 (denoted by RO-
1), unigram overlap, is part of all four best perform-
ing subsets. The features ROUGE-SU4 and Roget?s
204
appear in three of the best four feature combina-
tions, making Roget?s the best performing lexicon-
based feature outperforming WordNet features on
this task. esa, lin, jcn are part of two of the best
subsets, where lin and jcn occur together both times,
suggesting synergy. Looking at the worst perform-
ing feature combinations is also instructive and sug-
gests that lcsq was not an effective feature (despite
being at the heart of the more successful ROUGE-W
measure).
We also analyze performance of individual fea-
tures over different datasets. Table 3 lists all the fea-
tures and, instead of looking at only the best com-
bination, takes the top three best combinations for
each test and compares how many times each fea-
ture has occurred in the resulting 12 combinations
(first column). Three clear classes of effectiveness
emerge, high (10-7), medium (6-4), and low (3-0).
Next, we observe that the test sets differ in the aver-
age length of the data: headlines and OnWN glosses
are very short, in contrast to the other two. Table 3
shows in fact contrastive feature behavior for these
two categories (denoted by short and long). The last
column reports the number of time a feature has oc-
curred in the best combinations (out of 4). Again,
ROUGE-1, ROUGE-SU4, and roget prove effective
across different test sets. esa and lem seem most re-
liable when we deal with short text fragments, while
roget and ROUGE-SU4 are most valuable on longer
texts. The individual most valuable features overall
are ROUGE-1, ROUGE-SU4, and roget.
Features total (/12) short (/6) long (/6) best (/4)
esa 6 6 0 2
lin 6 3 3 2
jcn 4 1 3 2
roget 9 3 6 3
lem 6 6 0 2
jaro 0 0 0 0
lcsq 3 3 0 1
ROUGE-W 7 4 3 1
ROUGE-1 10 6 4 4
ROUGE-2 3 1 2 0
ROUGE-SU4 10 5 5 3
Table 3: Feature contribution to the three best results over
four datasets
5 Conclusion
CLaC-CORE investigated the performance possibil-
ities of different feature combinations for 11 basic
lexical features that are frequently used in seman-
tic distance measures. By exhaustively training all
combinations in a two-phase regressor, we were able
to establish a few interesting observations.
First, our own baseline of simply training a SVM
regressor on all 11 basic features achieves rank 10
and outperforms the baseline used for the shared
task. It should probably become the new standard
baseline.
Second, our two-phase exhaustive model, while
resource intensive, is not at all prohibitive. If the
knowledge to pick appropriate features is not avail-
able and if not enough training data exists to per-
form feature selection, the exhaustive method can
produce results that outperform our baseline and one
that is competitive in the current field (rank 7 of 88
submissions). But more importantly, this method al-
lows us to forensically analyze feature combination
behavior contrastively. We were able to establish
that unigrams and 4-skip bigrams are most versatile,
but surprisingly that Roget?s Thesaurus outperforms
the two leading WordNet-based distance measures.
In addition, ROUGE-W, a weighted longest com-
mon subsequence algorithm that to our knowledge
has not previously been used for similarity mea-
surements shows to be a fairly reliable measure for
all data sets, in contrast to longest common subse-
quence, which is among the lowest performers.
We feel that the insight we gained well justified
the expense of our approach.
Acknowledgments
We are grateful to Michelle Khalife and Jona Schu-
man for their comments and feedback on this work.
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada (NSERC).
References
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. ITRI-04-08 The Sketch Engine. In-
formation Technology.
205
Alex J. Smola and Bernhard Scho?lkopf. 2004. A Tutorial
on Support Vector Regression. Statistics and Comput-
ing, 14(3).
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based Measures of Lexical Semantic
Relatedness. Computational Linguistics, 32(1).
Chin-Yew Lin. 2004a. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop.
Chin-Yew Lin and Franz Josef Och. 2004b. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statistics. In Proceedings of the 42nd Annual Meeting
on Association for Computational Linguistics. Associ-
ation for Computational Linguistics.
Christiane Fellbaum 2010. WordNet. Theory and
Applications of Ontology: Computer Applications.
Springer.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing Semantic Textual
Similarity by Combining Multiple Content Similarity
Measures. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012), in
conjunction with the First Joint Conference on Lexical
and Computational Semantics.
Dekang Lin. 1998. An Information-Theoretic Definition
of Similarity. In Proceedings of the 15th International
Conference on Machine Learning, volume 1.
Ehsan Shareghi, Sabine Bergler. 2013. Feature Combi-
nation for Sentence Similarity. To appear in Proceed-
ings of the 26st Conference of the Canadian Society
for Computational Studies of Intelligence (Canadian
AI?13). Advances in Artificial Intelligence, Regina,
SK, Canada. Springer-Verlag Berlin Heidelberg.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 Task 6: A
Pilot on Semantic Textual Similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational Se-
mantics.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence.
Frane S?aric, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic. 2012. TakeLab: Systems
for Measuring Semantic Text Similarity. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation (SemEval 2012), in conjunction with the
First Joint Conference on Lexical and Computational
Semantics.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The Pascal Recognising Textual Entailment
Challenge. Machine Learning Challenges. Evaluat-
ing Predictive Uncertainty, Visual Object Classifica-
tion, and Recognising Tectual Entailment.
Jay J. Jiang and David W. Conrath. 1997. Semantic Sim-
ilarity Based on Corpus Statistics and Lexical Taxon-
omy. Proceedings of the 10th International Confer-
ence on Research on Computational Linguistics.
Lloyd Allison and Trevor I. Dix. 1986. A Bit-String
Longest-Common-Subsequence Algorithm. Informa-
tion Processing Letters, 23(5).
Mario Jarmasz and Stan Szpakowicz. 2003. Rogets The-
saurus and Semantic Similarity. In Proceedings of the
Conference on Recent Advances in Natural Language
Processing.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: an Update.
ACM SIGKDD Explorations Newsletter, 11(1).
Matthew A. Jaro. 1989. Advances in Record-Linkage
Methodology as Applied to Matching the 1985 Census
of Tampa, Florida. Journal of the American Statistical
Association.
Philip Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcelo Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation. In
Proceedings of the 45th Annual Meeting of the ACL on
Interactive Poster and Demonstration Sessions. Asso-
ciation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the Na-
tional Conference on Artificial Intelligence.
Ted Pedersen, Siddharth Patwardhan and Jason Miche-
lizzi. 2004. WordNet:: Similarity: Measuring the
Relatedness of Concepts. In Demonstration Papers at
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies. Association for Computational Linguistics.
William B. Dolan, Chris Quirk, and Chris Brockett.
2004. Unsupervised Construction of Large Paraphrase
Corpora: Exploiting Massively Parallel News Sources.
In Proceedings of the 20th International Conference
on Computational Linguistics. Association for Com-
putational Linguistics.
206
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 70?77,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A High-Precision Approach to Detecting Hedges and Their Scopes
Halil Kilicoglu and Sabine Bergler
Department of Computer Science and Software Engineering
Concordia University
1455 de Maisonneuve Blvd. West
Montre?al, Canada
{h kilico,bergler}@cse.concordia.ca
Abstract
We extend our prior work on specula-
tive sentence recognition and speculation
scope detection in biomedical text to the
CoNLL-2010 Shared Task on Hedge De-
tection. In our participation, we sought
to assess the extensibility and portability
of our prior work, which relies on linguis-
tic categorization and weighting of hedg-
ing cues and on syntactic patterns in which
these cues play a role. For Task 1B,
we tuned our categorization and weight-
ing scheme to recognize hedging in bio-
logical text. By accommodating a small
number of vagueness quantifiers, we were
able to extend our methodology to de-
tecting vague sentences in Wikipedia arti-
cles. We exploited constituent parse trees
in addition to syntactic dependency rela-
tions in resolving hedging scope. Our re-
sults are competitive with those of closed-
domain trained systems and demonstrate
that our high-precision oriented methodol-
ogy is extensible and portable.
1 Introduction
Natural language is imbued with uncertainty,
vagueness, and subjectivity. However, informa-
tion extraction systems generally focus on ex-
tracting factual information, ignoring the wealth
of information expressed through such phenom-
ena. In recent years, the need for information ex-
traction and text mining systems to identify and
model such extra-factual information has increas-
ingly become clear. For example, online product
and movie reviews have provided a rich context
for analyzing sentiments and opinions in text (see
Pang and Lee (2008) for a recent survey), while
tentative, speculative nature of scientific writing,
particularly in biomedical literature, has provided
impetus for recent research in speculation detec-
tion (Light et al, 2004). The term hedging is often
used as an umbrella term to refer to an array of
extra-factual phenomena in natural language and
is the focus of the CoNLL-2010 Shared Task on
Hedge Detection.
The CoNLL-2010 Shared Task on Hedge De-
tection (Farkas et al, 2010) follows in the steps
of the recent BioNLP?09 Shared Task on Event
Extraction (Kim et al, 2009), in which one task
(speculation and negation detection) was con-
cerned with notions related to hedging in biomed-
ical abstracts. However, the CoNLL-2010 Shared
Task differs in several aspects. It sheds light on
the pervasiveness of hedging across genres and do-
mains: in addition to biomedical abstracts, it is
concerned with biomedical full text articles as well
as with Wikipedia articles. Both shared tasks have
been concerned with scope resolution; however,
their definitions of scope are fundamentally differ-
ent: the BioNLP?09 Shared Task takes the scope
of a speculation instance to be an abstract seman-
tic object (an event), thus a normalized logical
form. The CoNLL-2010 Shared Task, on the other
hand, defines it as a textual unit based on syntac-
tic considerations. It is also important to note that
hedging in scientific writing is a core aspect of the
genre (Hyland, 1998), while it is judged to be a
flaw which has to be eradicated in Wikipedia ar-
ticles. Therefore, hedge detection in these genres
serves different purposes: explicitly encoding the
factuality of a scientific claim (doubtful, probable,
etc.) versus flagging unreliable text.
We participated in both tasks of the CoNLL-
2010 Shared Task: namely, detection of sentences
with uncertainty (Task 1) and resolution of uncer-
tainty scope (Task 2). Since we pursued both of
these directions in prior work, one of our goals in
participating in the shared task was to assess how
our approach generalized to previously unseen
texts, even genres. Towards this goal, we adopted
70
an open-domain approach, where we aimed to use
previously developed techniques to the extent pos-
sible. Among all participating groups, we distin-
guished ourselves as the one that fully worked in
an open-domain setting. This approach worked
reasonably well for uncertainty detection (Task 1);
however, for the scope resolution task, we needed
to extend our work more substantially, since the
notion of scope was fundamentally different than
what we adopted previously. The performance
of our system was competitive; in terms of F-
measure, we were ranked near the middle in Task
1, while a more significant focus on scope reso-
lution resulted in fourth place ranking among fif-
teen systems. We obtained the highest precision
in tasks focusing on biological text. Considering
that we chose not to exploit the training data pro-
vided to the full extent, we believe that our system
is viable in terms of extensibility and portability.
2 Related Work
Several notions related to hedging have been pre-
viously explored in natural language processing.
In the news article genre, these have included
certainty, modality, and subjectivity. For ex-
ample, Rubin et al (2005) proposed a four di-
mensional model to categorize certainty in news
text: certainty level, focus, perspective and time.
In the context of TimeML (Pustejovsky et al,
2005), which focuses on temporal expressions
in news articles, event modality is encoded us-
ing subordination links (SLINKs), some of which
(MODAL,EVIDENTIAL) indicate hedging (Saur?? et
al., 2006). Saur?? (2008) exploits modality and
polarity to assess the factuality degree of events
(whether they correspond to facts, counter-facts or
possibilities), and reports on FactBank, a corpus
annotated for event factuality (Saur?? and Puste-
jovsky, 2009). Wiebe et al (2005) consider
subjectivity in news articles, and focus on the
notion of private states, encompassing specula-
tions, opinions, and evaluations in their subjectiv-
ity frames.
The importance of speculative language in
biomedical articles was first acknowledged by
Light et al (2004). Following work in this area
focused on detecting speculative sentences (Med-
lock and Briscoe, 2007; Szarvas, 2008; Kilicoglu
and Bergler, 2008). Similar to Rubin et al?s
(2005) work, Thompson et al (2008) proposed
a categorization scheme for epistemic modality in
biomedical text according to the type of infor-
mation expressed (e.g., certainty level, point of
view, knowledge type). With the availability of the
BioScope corpus (Vincze et al, 2008), in which
negation, hedging and their scopes are annotated,
studies in detecting speculation scope have also
been reported (Morante and Daelemans, 2009;
O?zgu?r and Radev, 2009). Negation and uncer-
tainty of bio-events are also annotated to some ex-
tent in the GENIA event corpus (Kim et al, 2008).
The BioNLP?09 Shared Task on Event Extraction
(Kim et al, 2009) dedicated a task to detecting
negation and speculation in biomedical abstracts,
based on the GENIA event corpus annotations.
Ganter and Strube (2009) elaborated on the link
between vagueness in Wikipedia articles indicated
by weasel words and hedging. They exploited
word frequency measures and shallow syntactic
patterns to detect weasel words in Wikipedia ar-
ticles.
3 Methods
Our methodology for hedge detection is essen-
tially rule-based and relies on a combination of
lexical and syntactic information. Lexical infor-
mation is encoded in a simple dictionary, and rel-
evant syntactic information is identified using the
Stanford Lexicalized Parser (Klein and Manning,
2003). We exploit constituent parse trees as well
as corresponding collapsed dependency represen-
tations (deMarneffe et al, 2006), provided by the
parser.
3.1 Detecting Uncertainty in Biological Text
For detecting uncertain sentences in biological text
(Task 1B), we built on the linguistically-inspired
system previously described in detail in Kilicoglu
and Bergler (2008). In summary, this system relies
on a dictionary of lexical speculation cues, derived
from a set of core surface realizations of hedging
identified by Hyland (1998) and expanded through
WordNet (Fellbaum, 1998) synsets and UMLS
SPECIALIST Lexicon (McCray et al, 1994) nom-
inalizations. A set of lexical certainty markers (un-
hedgers) are also included, as they indicate hedg-
ing when they are negated (e.g., know). These
hedging cues are categorized by their type (modal
auxiliaries, epistemic verbs, approximative adjec-
tives, etc.) and are weighted to reflect their cen-
tral/peripheral contribution to hedging, inspired by
the fuzzy model of Hyland (1998). We use a scale
71
of 1-5, where 5 is assigned to cues most central
to hedging and 1 to those that are most periph-
eral. For example, the modal auxiliary may has
a weight of 5, while a relatively weak hedging
cue, the epistemic adverb apparently, has a weight
of 2. The weight sum of cues in a sentence in
combination with a predetermined threshold de-
termines whether the sentence in question is un-
certain. Syntax, generally ignored in other stud-
ies on hedging, plays a prominent role in our ap-
proach. Certain syntactic constructions act as cues
(e.g., whether- and if -complements), while others
strengthen or weaken the effect of the cue associ-
ated with them. For example, a that-complement
taken by an epistemic verb increases the hedging
score contributed by the verb by 2, while lack of
any complement decreases the score by 1.
For the shared task, we tuned this categoriza-
tion and weighting scheme, based on an analy-
sis of the biomedical full text articles in training
data. We also adjusted the threshold. We elim-
inated some hedging cue categories completely
and adjusted the weights of a small number of
the remaining cues. The eliminated cue categories
included approximative adverbs (e.g., generally,
largely, partially) and approximative adjectives
(e.g., partial), often used to ?manipulate preci-
sion in quantification? (Hyland, 1998). The other
eliminated category included verbs of effort (e.g.,
try, attempt, seek), also referred to as rationalising
narrators (Hyland, 1998). The motivation behind
eliminating these categories was that cues belong-
ing to these categories were never annotated as
hedging cues in the training data. The elimination
process resulted in a total of 147 remaining hedg-
ing cues. Additionally, we adjusted the weights of
several other cues that were not consistently anno-
tated as cues in the training data, despite our view
that they were strong hedging cues. One example
is the epistemic verb predict, previously assigned a
weight of 4 based on Hyland?s analysis. We found
its annotation in the training data somewhat incon-
sistent, and lowered its weight to 3, thus requiring
a syntactic strengthening effect (an infinitival com-
plement, for example) for it to qualify as a hedging
cue in the current setting (threshold of 4).
3.2 Detecting Uncertainty in Wikipedia
Articles
Task 1W was concerned with detecting uncer-
tainty in Wikipedia articles. Uncertainty in this
context refers more or less to vagueness indicated
by weasel words, an undesirable feature accord-
ing to Wikipedia policy. Analysis of Wikipedia
training data provided by the organizers revealed
that there is overlap between weasel words and
hedging cues described in previous section. We,
therefore, sought to adapt our dictionary of hedg-
ing cues to the task of detecting vagueness in
Wikipedia articles. Similar to Task 1B, changes
involved eliminating cue categories and adjusting
cue weights. In addition, however, we also added
a previously unconsidered category of cues, due
to their prominence in Wikipedia data as weasel
words. This category (vagueness quantifiers (Lap-
pin, 2000)) includes words, such as some, several,
many and various, which introduce imprecision
when in modifier position. For instance, in the ex-
ample below, both some and certain contribute to
vagueness of the sentence.
(1) Even today, some cultures have certain in-
stances of their music intending to imitate
natural sounds.
For Wikipedia uncertainty detection, eliminated
categories included verbs and nouns concerning
tendencies (e.g., tend, inclination) in addition to
verbs of effort. The only modal auxiliary consis-
tently considered a weasel word was might; there-
fore, we only kept might in this category and elim-
inated the rest (e.g., may, would). Approxima-
tive adverbs, eliminated in detecting uncertainty
in biological text, not only were revived for this
task, but also their weights were increased as they
were more central to vagueness expressions. Be-
sides these changes in weighting and categoriza-
tion, the methodology for uncertainty detection in
Wikipedia articles was essentially the same as that
for biological text. The threshold we used in our
submission was, similarly, 4.
3.3 Scope Resolution for Uncertainty in
Biological Text
Task 2 of the shared task involved hedging scope
resolution in biological text. We previously tack-
led this problem within the context of biological
text in the BioNLP?09 Shared Task (Kilicoglu and
Bergler, 2009). That task defined the scope of
speculation instances as abstract, previously ex-
tracted bio-events. Our approach relied on find-
ing an appropriate syntactic dependency relation
between the bio-event trigger word identified in
72
earlier steps and the speculation cue. The cate-
gory of the hedging cue constrained the depen-
dency relations that are deemed appropriate. For
example, consider the sentence in (2a), where in-
volves is a bio-event trigger for a Regulation
event and suggest is a speculation cue of epis-
temic verb type. The first dependency relation
in (2b) indicates that the epistemic verb takes a
clausal complement headed by the bio-event trig-
ger. The second indicates that that is the comple-
mentizer. This cue category/dependency combi-
nation licenses the generation of a speculation in-
stance where the event indicated by the event trig-
ger represents the scope.
(2) (a) The results suggest that M-CSF induc-
tion of M-CSF involves G proteins, PKC
and NF kappa B.
(b) ccomp(suggest,involves)
complm(involves,that)
Several other cue category/dependency combi-
nations sought for speculation scope resolution are
given in Table 1. X represents a token that is nei-
ther a cue nor a trigger (aux: auxiliary, dobj: direct
object, neg: negation modifier).
Cue Category Dependency
Modal auxiliary (may) aux(Trigger,Cue)
Conditional (if ) complm(Trigger,Cue)
Unhedging noun dobj(X,Cue)
(evidence) ccomp(X,Trigger)
neg(Cue,no)
Table 1: Cue categories with examples and the de-
pendency relations to search
In contrast to this notion of scope being an ab-
stract semantic object, Task 2 (BioScope corpus,
in general) conceptualizes hedge scope as a con-
tinuous textual unit, including the hedging cue it-
self and the biggest syntactic unit the cue is in-
volved in (Vincze et al, 2008). This fundamen-
tal difference in conceptualization limits the di-
rect applicability of our prior approach to this
task. Nevertheless, we were able to use our work
as a building block in extending scope resolution
heuristics. We further augmented it by exploiting
constituent parse trees provided by Stanford Lex-
icalized Parser. These extensions are summarized
below.
3.3.1 Exploiting parse trees
The constituent parse trees contribute to scope
resolution uniformly across all hedging cue cate-
gories. We simply determine the phrasal node that
dominates the hedging cue and consider the tokens
within that phrase as being in the scope of the cue,
unless they meet one of the following exclusion
criteria:
1. Exclude tokens within post-cue sentential
complements (indicated by S and SBAR
nodes) introduced by a small number of
discourse markers (thus, whereas, because,
since, if, and despite).
2. Exclude punctuation marks at the right
boundary of the phrase
3. Exclude pre-cue determiners and adverbs at
the left boundary of the phrase
For example, in the sentence below, the verb
phrase that included the modal auxiliary may also
included the complement introduced by thereby.
Using the exclusion criteria 1 and 2, we excluded
the tokens following SPACER from the scope:
(3) (a) . . .motifs may be easily compared with
the results from BEAM, PRISM and
SPACER, thereby extending the SCOPE
ensemble to include a fourth class of
motifs.
(b) CUE: may
SCOPE: motifs may be easily compared
with the results from BEAM, PRISM
and SPACER
3.3.2 Extending dependency-based heuristics
The new scope definition was also accommodated
by extending the basic dependency-based heuris-
tics summarized earlier in this section. In addition
to finding the trigger word that satisfies the ap-
propriate dependency constraint with the hedging
cue (we refer to this trigger word as scope head,
henceforth), we also considered the other depen-
dency relations that the scope head was involved
in. These relations, then, were used in right ex-
pansion and left expansion of the scope. Right ex-
pansion involves finding the rightmost token that
is in a dependency relation with the scope head.
Consider the sentence below:
(4) The surprisingly low correlations between
Sig and accuracy may indicate that the ob-
jective functions employed by motif finding
73
programs are only a first approximation to bi-
ological significance.
The epistemic verb indicate has as its scope
head the token approximation, due to the existence
of a clausal complement dependency (ccomp) be-
tween them. On the other hand, the rightmost to-
ken of the sentence, significance, has a preposi-
tional modifier dependency (prep to) with approx-
imation. It is, therefore, included in the scope of
indicate. Two dependency types, adverbial clause
modifier (advcl) and conjunct (conj), were ex-
cluded from consideration when the rightmost to-
ken is sought, since they are likely to signal new
discourse units outside the scope.
In contrast to right expansion, which applies
to all hedging cue categories, left expansion ap-
plies only to a subset. Left expansion involves
searching for a subject dependency governed by
the scope head. The dependency types descend-
ing from the subject (subj) type in the Stanford de-
pendency hierarchy are considered: nsubj (nom-
inal subject), nsubjpass (passive nominal sub-
ject), csubj (clausal subject) and csubjpass (pas-
sive clausal subject). In the following example,
the first token, This, is added to the scope of likely
through left expansion (cop: copula).
(5) (a) This is most likely a conservative esti-
mate since a certain proportion of inter-
actions remain unknown . . .
(b) nsubj(likely,This)
cop(likely,is)
Left expansion was limited to the following cue
categories, with the additional constraints given:
1. Modal auxiliaries, only when their scope
head takes a passive subject (e.g., they is
added to the scope of may in they may be an-
notated as pseudogenes).
2. Cues in adjectival categories, when they are
in copular constructions (e.g., Example (5)).
3. Cues in several adjectival ad verbal cate-
gories, when they take infinitival comple-
ments (e.g., this is added to the scope of ap-
pears in However, this appears to add more
noise to the prediction without increasing the
accuracy).
After scope tokens are identified using the parse
tree as well as via left and right expansion, the al-
gorithm simply sets as scope the continuous tex-
tual unit that includes all the scope tokens and the
hedging cue. Since, likely is the hedging cue and
This and estimate are identified as scope tokens in
Example (5), the scope associated with likely be-
comes This is most likely a conservative estimate.
We found that citations, numbers and punc-
tuation marks occurring at the end of sentences
caused problems in scope resolution, specifically
in biomedical full text articles. Since they are
rarely within any scope, we implemented a simple
stripping algorithm to eliminate them from scopes
in such documents.
4 Results and Discussion
The official evaluation results regarding our sub-
mission are given in Table 2. These results were
achieved with the threshold 4, which was the opti-
mal threshold on the training data.
Prec. Recall F-score Rank
Task 1B 92.07 74.94 82.62 12/24
Task 1W 67.90 46.02 54.86 10/17
Task 2 62.47 49.47 55.21 4/15
Table 2: Evaluation results
In Task 1B, we achieved the highest precision.
However, our relatively low recall led to the place-
ment of our system in the middle. Our system al-
lows adjusting precision versus recall by setting
the threshold. In fact, setting the threshold to 3 af-
ter the shared task, we were able to obtain overall
better results (Precision=83.43, Recall=84.81, F-
score=84.12, Rank=8/24). However, we explicitly
targeted precision, and in that respect, our submis-
sion results were not surprising. In fact, we iden-
tified a new type of hedging signalled by coordi-
nation (either . . . or . . . as well as just or) in the
training data. An example is given below:
(6) (a) It will be either a sequencing error or a
pseudogene.
(b) CUE: either-or
SCOPE: either a seqeuncing error or a
pseudogene
By handling this class to some extent, we could
have increased our recall, and therefore, F-score
(65 out of 1,044 cues in the evaluation data for
biological text involved this class). However, we
decided against treating this class, as we believe
it requires a slightly different treatment due to its
special semantics.
74
In participating in Task 1W, our goal was to
test the ease of extensibility of our system. In
that regard, our results show that we were able
to exploit the overlap between our hedging cues
and the weasel words. The major difference we
noted between hedging in two genres was the
class of vagueness quantifiers, and, with little ef-
fort, we extended our system to consider them.
We also note that setting the threshold to 3 after
the shared task, our recall and F-score improved
significantly (Precision=63.21, Recall=53.67, F-
score=58.05, Rank=3/17).
Our more substantial effort for Task 2 resulted
in a better overall ranking, as well as the highest
precision in this task. In contrast to Task 1, chang-
ing the threshold in this task did not have a pos-
itive effect on the outcome. We also measured
the relative contribution of the enhancements to
scope resolution. The results are presented in Ta-
ble 3. Baseline is taken as the scope resolution al-
gorithm we developed in prior work. These results
show that: a) scope definition we adopted earlier
is essentially incompatible with the BioScope def-
inition b) simply taking the phrase that the hedg-
ing cue belongs to as the scope provides relatively
good results c) left and right expansion heuristics
are needed for increased precision and recall.
Prec. Recall F-score
Baseline 3.29 2.61 2.91
Baseline+ Left/
right expansion
25.18 20.03 22.31
Parse tree 49.20 39.10 43.58
Baseline+
Parse tree
50.66 40.27 44.87
All 62.47 49.47 55.21
Table 3: Effect of scope resolution enhancements
4.1 Error Analysis
In this section, we provide a short analysis of the
errors our system generated, focusing on biologi-
cal text.
Since our dictionary of hedging cues is incom-
plete and we did not attempt to expand it for Task
1B, we had a fair number of recall errors. As
we mentioned above, either-or constructions oc-
cur frequently in the training and evaluation data,
and we did not attempt to handle them. Addition-
ally, some lexical cues, such as feasible and im-
plicate, do not appear in our dictionary, causing
further recall errors. The weighting scheme also
affects recall. For example, the adjective appar-
ent has a weight of 2, which is not itself sufficient
to qualify a sentence as uncertain (with a thresh-
old of 4) (7a). On the other hand, when it takes
a clausal complement, the sentence is considered
uncertain (7b). The first sentence (7a) causes a re-
call error.
(7) (a) An apparent contradiction between the
previously reported number of cycling
genes . . .
(b) . . . it is apparent that the axonal termini
contain a significantly reduced number
of varicosities . . .
In some cases, syntactic constructions that play
a role in determining the certainty status of a sen-
tence cannot be correctly identified by the parser,
often leading to recall errors. For example, in the
sentence below, the clausal complement construc-
tion is missed by the parser. Since the verb indi-
cate has weight 3, this leads to a recall error in the
current setting.
(8) . . . indicating that dMyc overexpression can
substitute for PI3K activation . . .
Adjusting the weights of cues worked well gen-
erally, but also caused unexpected problems, due
to what seem like inconsistencies in annotation.
The examples below highlight the effect of low-
ering the weight of predict from 4 to 3. Exam-
ples (9a) and (9b) are almost identical on surface
and our system predicted both to be uncertain, due
to the fact that predicted took infinitival comple-
ments in both cases. However, only (9a) was an-
notated as uncertain, leading to a precision error in
(9b).
(9) (a) . . . include all protein pairs predicted to
have posterior odds ratio . . .
(b) Protein pairs predicted to have a poste-
rior odds ratio . . .
The error cases in scope resolution are more
varied. Syntax has a larger role in this task, and
therefore, parsing errors tend to affect the results
more directly. In the following example, dur-
ing left-expanding the scope of the modal auxil-
iary could, RNAi screens, rather than the full noun
phrase fruit fly RNAi screens, is identified as the
passive subject of the scope head (associated), be-
cause an appropriate modifier dependency cannot
75
be found between the noun phrase head screens
and either of the modifiers, fruit and fly.
(10) . . . was to investigate whether fruit fly RNAi
screens of conserved genes could be asso-
ciated with similar tick phenotypes and tick
gene function.
In general, the simple mechanism to exploit
constituent parse trees was useful in resolving
scope. However, it appears that a nuanced ap-
proach based on cue categories could enhance the
results further. In particular, the current mecha-
nism does not contribute much to resolving scopes
of adverbial cues. In the following example, parse
tree mechanism does not have any effect, leading
to both a precision and a recall error in scope res-
olution.
(11) (a) . . . we will consider tightening the defi-
nitions and possibly splitting them into
different roles.
(b) FP: possibly
FN: possibly splitting them into differ-
ent roles
Left/right expansion strategies were based on
the analysis of training data. However, we en-
countered errors caused by these strategies where
we found the annotations contradictory. In Exam-
ple (12a), the entire fragment is in the scope of
thought, while in (12b), the scope of suggested
does not include it was, even though on surface
both fragments are very similar.
(12) (a) . . . the kinesin-5 motor is thought to play
a key role.
(b) . . . it was suggested to enhance the nu-
clear translocation of NF-?B.
Post-processing in the form of citation stripping
was simplistic, and, therefore, was unable to han-
dle complex cases, as the one shown in the exam-
ple below. The algorithm is only able to remove
one reference at the end.
(13) (a) . . . it is possible that some other sig-
nalling system may operate with Semas
to confine dorsally projecting neurons to
dorsal neuropile [3],[40],[41].
(b) FP: may operate with Semas to con-
fine dorsally projecting neurons to dor-
sal neuropile [3],[40],
FN: may operate with Semas to con-
fine dorsally projecting neurons to dor-
sal neuropile
5 Conclusions
Rather than developing a dedicated methodology
that exclusively relies on the data provided by or-
ganizers, we chose to extend and refine our prior
work in hedge detection and used the training
data only in a limited manner: to tune our sys-
tem in a principled way. With little tuning, we
achieved the highest precision in Task 1B. We
were able to capitalize on the overlap between
hedging cues and weasel words for Task 1W and
achieved competitive results. Adapting our pre-
vious work in scope resolution to Task 2, how-
ever, was less straightforward, due to the incom-
patible definitions of scope. Nevertheless, by re-
fining the prior dependency-based heuristics with
left and right expansion strategies and utilizing a
simple mechanism for parse tree information, we
were able to accommodate the new definition of
scope to a large extent. With these results, we con-
clude that our methodology is portable and easily
extensible.
While the results show that using the parse tree
information for scope resolution benefited our per-
formance greatly, error analysis presented in the
previous sections also suggests that a finer-grained
approach based on cue categories could further
improve results, and we aim to explore this exten-
sion further.
References
Marie-Catherine deMarneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449?
454.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press, Cambridge, MA.
Viola Ganter and Michael Strube. 2009. Finding
Hedges by Chasing Weasels: Hedge Detection Us-
ingWikipedia Tags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 173?176.
76
Ken Hyland. 1998. Hedging in scientific research ar-
ticles. John Benjamins B.V., Amsterdam, Nether-
lands.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9 Suppl 11:s10.
Halil Kilicoglu and Sabine Bergler. 2009. Syntac-
tic dependency based heuristics for biological event
extraction. In Proceedings of Natural Language
Processing in Biomedicine (BioNLP) NAACL 2009
Workshop, pages 119?127.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9:10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing
in Biomedicine (BioNLP) NAACL 2009 Workshop,
pages 1?9.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41th Meeting of the Association for Computational
Linguistics, pages 423?430.
Shalom Lappin. 2000. An intensional parametric se-
mantics for vague quantifiers. Linguistics and Phi-
losophy, 23(6):599?620.
Marc Light, Xin Y. Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: facts, speculations, and
statements in between. In BioLINK 2004: Linking
Biological Literature, Ontologies and Databases,
pages 17?24.
Alexa T. McCray, Suresh Srinivasan, and Allen C.
Browne. 1994. Lexical methods for managing vari-
ation in biomedical terminologies. In Proceedings
of the 18th Annual Symposium on Computer Appli-
cations in Medical Care, pages 235?239.
Ben Medlock and Ted Briscoe. 2007. Weakly su-
pervised learning for hedge classification in scien-
tific literature. In Proceedings of the 45th Meet-
ing of the Association for Computational Linguis-
tics, pages 992?999.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. Detect-
ing speculations and their scopes in scientific text.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1398?1407.
Bo Pang and Lillian Lee. 2008. Sentiment Analysis
and Opinion Mining. Now Publishers Inc, Boston,
MA.
James Pustejovsky, Robert Knippen, Jessica Littman,
and Roser Saur??. 2005. Temporal and event in-
formation in natural language text. Language Re-
sources and Evaluation, 39(2):123?164.
Victoria L. Rubin, Elizabeth D. Liddy, and Noriko
Kando. 2005. Certainty identification in texts: Cat-
egorization model and manual tagging results. In
James G. Shanahan, Yan Qu, and Janyce Wiebe, ed-
itors, Computing Attitude and Affect in Text: The-
ories and Applications, volume 20, pages 61?76.
Springer Netherlands, Dordrecht.
Roser Saur?? and James Pustejovsky. 2009. FactBank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Roser Saur??, Marc Verhagen, and James Pustejovsky.
2006. Annotating and recognizing event modality in
text. In Proceedings of 19th International FLAIRS
Conference.
Roser Saur??. 2008. A Factuality Profiler for Eventual-
ities in Text. Ph.D. thesis, Brandeis University.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of the 46th Meeting
of the Association for Computational Linguistics,
pages 281?289.
Paul Thompson, Giulia Venturi, John McNaught,
Simonetta Montemagni, and Sophia Ananiadou.
2008. Categorising modality in biomedical texts. In
Proceedings of LREC 2008 Workshop on Building
and Evaluating Resources for Biomedical Text Min-
ing.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9 Suppl 11:S9.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2):165?210.
77
Proceedings of BioNLP Shared Task 2011 Workshop, pages 173?182,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Adapting a General Semantic Interpretation Approach to Biological Event
Extraction
Halil Kilicoglu and Sabine Bergler
Department of Computer Science and Software Engineering
Concordia University
1455 de Maisonneuve Blvd. West
Montre?al, Canada
{h kilico,bergler}@cse.concordia.ca
Abstract
The second BioNLP Shared Task on Event
Extraction (BioNLP-ST?11) follows up the
previous shared task competition with a focus
on generalization with respect to text types,
event types and subject domains. In this spirit,
we re-engineered and extended our event ex-
traction system, emphasizing linguistic gener-
alizations and avoiding domain-, event type-
or text type-specific optimizations. Similar
to our earlier system, syntactic dependencies
form the basis of our approach. However, di-
verging from that system?s more pragmatic na-
ture, we more clearly distinguish the shared
task concerns from a general semantic com-
position scheme, that is based on the no-
tion of embedding. We apply our methodol-
ogy to core bio-event extraction and specu-
lation/negation detection tasks in three main
tracks. Our results demonstrate that such a
general approach is viable and pinpoint some
of its shortcomings.
1 Introduction
In the past two years, largely due to the availabil-
ity of GENIA event corpus (Kim et al, 2008) and
the resulting shared task competition (BioNLP?09
Shared Task on Event Extraction (Kim et al,
2009)), event extraction in biological domain has
been attracting greater attention. One of the crit-
icisms towards this paradigm of corpus annota-
tion/competition has been that they are concerned
with narrow domains and specific representations,
and that they may not generalize well. For in-
stance, GENIA event corpus contains only Medline
abstracts on transcription factors in human blood
cells. Whether models trained on this corpus would
perform well on full-text articles or on text focusing
on other aspects of biomedicine (e.g., treatment or
etiology of disease) remains largely unclear. Since
annotated corpora are not available for every con-
ceivable domain, it is desirable for automatic event
extraction systems to be generally applicable to dif-
ferent types of text and domains without requiring
much training data or customization.
GENIA EPI ID BB BI
# core events 9 15 10 2 10
Triggers? Y Y Y N N
Full-text? Y N Y N N
Spec/Neg? Y Y Y N N
Table 1: An overview of BioNLP-ST?11 tracks
In the follow-up event to BioNLP?09 Shared
Task on Event Extraction, organizers of the second
BioNLP Shared Task on Event Extraction (BioNLP-
ST?11) (Kim et al, 2011a) address this challenge to
some extent. The theme of BioNLP-ST?11 is gen-
eralization and the net is cast much wider. There
are 4 event extraction tracks: in addition to the GE-
NIA track that again focuses on transcription fac-
tors (Kim et al, 2011b), the epigenetics and post-
translational modification track (EPI) focuses on
events relating to epigenetic change, such as DNA
methylation and histone modification, as well as
other common post-translational protein modifica-
tions (Ohta et al, 2011), whereas the infectious dis-
eases track (ID) focuses on bio-molecular mecha-
nisms of infectious diseases (Pyysalo et al, 2011a).
Both GENIA and ID tracks include data pertaining
to full-text articles, as well. The fourth track, Bacte-
ria, consists of two sub-tracks: Biotopes (BB) and
Interactions (BI) (Bossy et al (2011) and Jourde
173
et al (2011), respectively). A summary of the
BioNLP-ST?11 tracks is given in Table (1).
We participated in three tracks: GENIA, EPI, and
ID. In the spirit of the competition, our aim was to
demonstrate a methodology that was general and re-
quired little, if any, customization or training for in-
dividual tracks. For this purpose, we used a two-
phase approach: a syntax-driven composition phase
that exploits linguistic generalizations to create a
general semantic representation in a bottom-up man-
ner and a mapping phase, which relies on the shared
task event definitions and constraints to map rele-
vant parts of this semantic representation to event
instances. The composition phase takes as its input
simple entities and syntactic dependency relations
and is intended to be fully general. On the other
hand, the second phase is more task-specific even
though the kind of task-specific knowledge it re-
quires is largely limited to event definitions and trig-
ger expressions. In addition to extracting core bio-
logical events, our system also addresses speculation
and negation detection within the same framework.
Our results demonstrate the feasibility of a method-
ology that uses little training data or customization.
2 Methodology
In our general research, we are working towards
a linguistically-grounded, bottom-up discourse in-
terpretation scheme. In particular, we focus on
lower level discourse phenomena, such as causation,
modality, and negation, and investigate how they in-
teract with each other, as well as their effect on ba-
sic propositional semantic content (who did what to
who?) and higher discourse/pragmatics structure. In
our model, we distinguish three layers of proposi-
tions: atomic, embedding, and discourse. An atomic
proposition corresponds to the basic unit and low-
est level of meaning: in other words, a semantic re-
lation whose arguments correspond to ontologically
simple entities. Atomic propositions form the ba-
sis for embedding propositions, that is, propositions
taking as arguments other propositions (embedding
them). In turn, embedding and atomic propositions
act as arguments for discourse relations1. Our main
1Discourse relations, also referred to as coherence or rhetor-
ical relations (Mann and Thompson, 1988), are not relevant to
the shared task and, thus, we will not discuss them further in
motivation in casting the problem of discourse in-
terpretation in this structural manner is two-fold: a)
to explore the semantics of the embedding layer in
a systematic way b) to allow a bottom-up semantic
composition approach, which works its way from
atomic propositions towards discourse relations in
creating general semantic representations.
The first phase of our event extraction system
(composition) is essentially an implementation of
this semantic composition approach. Before delving
into further details regarding our implementation for
the shared task, however, it is necessary to briefly ex-
plain the embedding proposition categorization that
our interpretation scheme is based on. With this cat-
egorization, our goal is to make explicit the kind
of semantic information expressed at the embedding
layer. We distinguish three basic classes of embed-
ding propositions: MODAL, ATTRIBUTIVE, and RE-
LATIONAL. We provide a brief summary below.
2.1 MODAL type
The embedding propositions of MODAL type mod-
ify the status of the embedded proposition with re-
spect to its factuality, possibility, or necessity, and
so on. They typically involve a) judgement about
the status of the proposition, b) evidence for the
proposition, c) ability or willingness, and d) obli-
gations and permissions, corresponding roughly to
EPISTEMIC, EVIDENTIAL, DYNAMIC and DEONTIC
types (cf. Palmer (1986)), respectively. Further sub-
divisions are given in Figure (1). In the shared task
context, the MODAL class is mostly relevant to the
speculation and negation detection tasks.
2.2 ATTRIBUTIVE type
The ATTRIBUTIVE type of embedding serves to
specify an attribute of an embedded proposition (se-
mantic role of an argument). They typically involve
a verbal predicate (undergo in Example (1) below),
which takes a nominalized predicate (degradation)
as one of its syntactic arguments. The other syntac-
tic argument of the verbal predicate corresponds to
a semantic argument of the embedded predicate. In
Example (1), p105 is a semantic argument of PA-
TIENT type for the proposition indicated by degra-
dation.
this paper.
174
(1) . . . p105 undergoes degradation . . .
Verbs functioning in this way are plenty (e.g., per-
form for the AGENT role, experience for experiencer
role). With respect to the shared task, we found that
the usefulness of the ATTRIBUTIVE type of embed-
ding was largely limited to verbal predicates involve
and require and their nominal forms.
2.3 RELATIONAL type
The RELATIONAL type of embedding serves to se-
mantically link two propositions, providing a dis-
course/pragmatic function. It is characterized by
permeation of a limited set of discourse relations to
the clausal level, often signalled lexically by ?dis-
course verbs? (Danlos, 2006) (e.g., cause, mediate,
lead, correlate), their nominal forms or other ab-
stract nouns, such as role. We categorize the RELA-
TIONAL class into CAUSAL, TEMPORAL, CORREL-
ATIVE, COMPARATIVE, and SALIENCY types. In the
example below, the verbal predicate leads to indi-
cates a CAUSAL relation between the propositions
whose predicates are highlighted.
(2) Stimulation of cells leads to a rapid phospho-
rylation of I?B? . . .
While not all the subtypes of this class were relevant
to the shared task, we found that CAUSAL, CORREL-
ATIVE, and SALIENCY subtypes play a role, partic-
ularly in complex regulatory events. The portions of
the classification that pertain to the shared task are
given in Figure (1).
3 Implementation
In the shared task setting, embedding propositions
correspond to complex regulatory events (e.g., Reg-
ulation, Catalysis) as well as event modifications
(Negation and Speculation), whereas atomic propo-
sitions correspond to simple event types (e.g., Phos-
phorylation). While the treatment of these two types
differ in significant ways, they both require that sim-
ple entities are recognized, syntactic dependencies
are identified and a dictionary of trigger expressions
is available. We first briefly explain the construction
of the trigger dictionary.
3.1 Dictionary of Trigger Expressions
In the previous shared task, we relied on training
data and simple statistical measures to identify good
Figure 1: Embedding proposition categorization relevant
to the shared task
trigger expressions for events and used a list of trig-
gers that we manually compiled for speculation and
negation detection (see Kilicoglu and Bergler (2009)
for details). With respect to atomic propositions,
our method of constructing a dictionary of trigger
expressions remains essentially the same, including
the use of statistical measures to distinguish good
triggers. The only change we made was to consider
affixal negation and set polarity of several atomic
proposition triggers to negative (e.g., nonexpression,
unglycosylated). On the other hand, we have been
extending our manually compiled list of specula-
tion/negation triggers to include other types of em-
bedding triggers and to encode finer grained distinc-
tions in terms of their categorization and trigger be-
haviors. The training data provided for the shared
task also helped us expand this trigger dictionary,
particularly with respect to RELATIONAL trigger ex-
pressions. It is worth noting that we used the same
embedding trigger dictionary for all three tracks that
we participated in. Several entries from the embed-
ding trigger dictionary are summarized in Table (2).
Lexical polarity and strength values play a role
in the composition phase in associating a context-
dependent scalar value with propositions. Lexical
polarity values are largely derived from a polarity
lexicon (Wilson et al, 2005) and extended by us-
175
Trigger POS Semantic Type Lexical Polarity Strength
show VB DEMONSTRATIVE positive 1.0
unknown JJ EPISTEMIC negative 0.7
induce VB CAUSAL positive 1.0
fail VB SUCCESS negative 0.0
effect NN CAUSAL neutral 0.5
weakly RB HEDGE neutral -
absence NN REVERSE negative -
Table 2: Several entries from the embedding dictionary
ing heuristics involving the event types associated
with the trigger2. Some polarity values were as-
signed manually. Some strength values were based
on prior work (Kilicoglu and Bergler, 2008), oth-
ers were manually assigned. As Table (2) shows, in
some cases, the semantic type (e.g., DEMONSTRA-
TIVE, CAUSAL) is simply a mapping to the embed-
ding categorization. In other cases, such as weakly
or absence, the semantic type identifies the role that
the trigger plays in the composition phase. The em-
bedding trigger dictionary incorporates ambiguity;
however, for the shared task, we limit ourselves to
one semantic type per trigger to avoid the issue of
disambiguation. For ambiguous triggers extracted
from the training data, the semantic type with the
maximum likelihood is used. On the other hand, we
determined the semantic type to use manually for
triggers that we compiled independent of the train-
ing data. In this way, we use 466 triggers for atomic
propositions and 908 for embedding ones3.
3.2 Composition
As mentioned above, the composition phase as-
sumes simple entities, syntactic dependency rela-
tions and trigger expressions. Using these elements,
we construct a semantic embedding graph of the
document. To obtain syntactic dependency relations,
we segment documents into sentences, parse them
using the re-ranking parser of Charniak and John-
son (2005) adapted to the biomedical domain (Mc-
Closky and Charniak, 2008) and extract syntactic
2For example, if the most likely event type associated with
the trigger is Negative regulation, its polarity is considered neg-
ative.
3Note, however, that not all embedding propositions (or their
triggers) were directly relevant to the shared task.
dependencies from parse trees using the Stanford
dependency scheme (de Marneffe et al, 2006). In
addition to syntactic dependencies, we also require
information regarding individual tokens, including
lemma, part-of-speech, and positional information,
for which we also rely on Stanford parser tools. We
present a high level description of the composition
phase below.
3.2.1 From syntactic dependencies to
embedding graphs
As the first step in composition, we convert syn-
tactic dependencies into embedding relations. An
embedding relation, in our definition, is very simi-
lar to a syntactic dependency; it is typed and holds
between two textual elements. It diverges from a
syntactic dependency in two ways: its elements can
be multi-word expressions and it is aimed at better
reflecting the direction of the semantic dependency
between its elements. Take, for example, the sen-
tence fragment in Example (3a). Syntactic depen-
dencies are given in (3b) and the corresponding em-
bedding relations in (3c). The fact that the adjecti-
val predicate in modifier position (possible) semanti-
cally embeds its head (involvement) is captured with
the first embedding relation. The second syntactic
dependency already reflects the direction of the se-
mantic dependency between its elements accurately
and, thus, is unchanged as an embedding relation.
(3) (a) . . . possible involvement of HCMV . . .
(b) amod(involvement,possible)
prep of (involvement,HCMV)
(c) amod(possible,involvement)
prep of (involvement,HCMV)
To obtain the embedding relations in a sentence,
we apply a series of transformations to its syntactic
176
Figure 2: The embedding graph for the sentence Our previous results show that recombinant gp41 (aa565-647), the
extracellular domain of HIV-1 transmembrane glycoprotein, stimulates interleukin-10 (IL-10) production in human
monocytes. in the context of the document embedding graph for the Medline abstract with PMID 10089566.
dependencies. A transformation may not be neces-
sary, as with the prep of dependency in the exam-
ple above. It may result in collapsing several syn-
tactic dependencies into one, as well, or in splitting
one into several embedding relations. In addition
to capturing semantic dependency behavior explic-
itly, these transformations serve to incorporate se-
mantic information (entities and triggers) into the
embedding structure and to correct syntactic depen-
dencies that are systemically misidentified, such as
those that involve modifier coordination.
After these transformations, the resulting directed
acyclic embedding graph is, in the simplest case, a
tree, but more often a forest. An example graph is
given in Figure (2). The edges are associated with
the embedding relation types, and the nodes with
textual elements.
3.2.2 Composing Propositions
After constructing the embedding graph, we tra-
verse it in a bottom-up manner and compose se-
mantic propositions. Before this procedure can take
place, though, the embedding graph pertaining to
each sentence is further linked to the document em-
bedding graph in a way to reflect the proximity of
sentences, as illustrated in Figure (2). This is done
to enable discourse interpretation across sentences,
including coreference resolution.
Traversal of the embedding structure is guided by
argument identification rules, which apply to non-
leaf nodes in the embedding graph. An argument
identification rule is essentially a mapping from the
type of the embedding relation holding between a
parent node and its child node and part-of-speech of
the parent node to a logical argument type (logical
subject, logical object or adjunct). Constraints on
and exclusions from a rule can be defined, as shown
in Table (3). We currently use about 80 such rules,
mostly adapted from our previous shared task sys-
tem (Kilicoglu and Bergler, 2009).
After all the descendants of a non-leaf node are
recursively processed for arguments, a semantic
proposition can be composed. We define a seman-
tic proposition as consisting of a trigger, a collection
177
Relation Applies to Argument Constrained to Exclusions
prep on NN Object influence,impact,effect -
agent VB Subject - -
nsubjpass VB Object - -
whether comp VB Object INTERROGATIVE -
prep in NN Adjunct - effect, role, influence, importance
Table 3: Several argument identification rules. Note that constraints and exclusions may apply to trigger categories, as
well as to lemmas.
of core and adjunct arguments as well as a polarity
value and a scalar value. The polarity value can be
positive, negative or neutral. The scalar value is in
the (0,1) range. Atomic propositions are simply as-
signed polarity value of neutral4 and the scalar value
of 1.0. On the other hand, in the context of embed-
ding propositions, the computation of these values,
through which we attempt to capture some of the in-
teractions occurring at the embedding layer, is more
involved. For the sentence depicted in Figure (2),
the relevant resulting embedding and atomic propo-
sitions are given below.
(4) DEMONSTRATIVE(em1,Trigger=show,
Object=em2, Subject=Our previous results,
Polarity=positive, Value=1.0)
(5) CAUSAL(em2, Trigger=stimulates, Object=ap1,
Subject=recombinant gp41, Polarity=positive,
Value=1.0)
(6) Gene expression(ap1, Trigger= production,
Object= interleukin-10, Adjunct= human
monocytes, Polarity=neutral, Value=1.0)
The composition phase also deals with coordina-
tion of entities and propositions as well as with prop-
agation of arguments at the lower levels.
3.3 Mapping Propositions to Events
The goal of the mapping phase is to impose the
shared task constraints on the partial interpretation
achieved in the previous phase. We achieve this in
three steps.
The first step is to map embedding proposition
types to event (or event modification) types. We de-
fined constraints that guide this mapping. Some of
4Unless affixal negation is involved, in which case the as-
signed polarity value is negative.
these mappings are presented in Table (4). In this
way, Example (4) is pruned, since embedding propo-
sitions of DEMONSTRATIVE type satisfy the con-
straints only if they have negative polarity, as shown
in Table (4).
We then apply constraints concerned with the se-
mantic roles of the participants. For this step, we
define a small number of logical argument/semantic
role mappings. These are similar to argument identi-
fication rules, in that the mapping can be constrained
to certain event types or event types can be excluded
from it. We provide some of these mappings in Ta-
ble (5). With these mappings, the Object and Sub-
ject arguments of the proposition in Example (5) are
converted to Theme and Cause semantic roles, re-
spectively.
As the final step, we prune event participants that
do not conform to the event definition as well as the
propositions whose types could not be mapped to a
shared task event type. For example, a Cause par-
ticipant for a Gene expression event is pruned, since
only Theme participants are relevant for the shared
task. Further, a proposition with DEONTIC seman-
tic type is pruned, because it cannot be mapped to
a shared task type. The infectious diseases track
(ID) event type Process is interesting, because it may
take no participants at all, and we deal with this id-
iosyncrasy at this step, as well. This concludes the
progressive transformation of the graph to event and
event modification annotations.
4 Results and Discussion
With the two-phase methodology presented above,
we participated in three tracks: GENIA (Tasks 1 and
3), ID, and EPI. The official evaluation results we
obtained for the GENIA track are presented in Ta-
ble (6) and the results for the EPI and ID tracks in
178
Track Prop. Type Polarity Value Correspond. Event (Modification) Type
GENIA,ID CAUSAL neutral - Regulation
GENIA,ID,EPI SUCCESS negative - Negation
EPI CAUSAL positive - Catalysis
GENIA,ID,EPI SPECULATIVE - > 0.0 Speculation
GENIA,ID,EPI DEMONSTRATIVE negative - Speculation
Table 4: Several event (and event modification) mappings
Logical
Arg.
Semantic
Role
Constraint Exclusion
Object Theme - Process
Subject Cause - -
Subject Theme Binding -
Object Participant Process -
Object Scope Speculation,
Negation
-
Table 5: Logical argument to semantic role mappings
Table (7). With the official evaluation criteria, we
were ranked 5th in the GENIA track (5/15), 7th in
the EPI track (7/7) and 4th in the ID track (4/7).
There were only two submissions for the GENIA
speculation/negation task (Task 3) and our results
in this task were comparable to those of the other
participating group: our system performed slightly
better with speculation, and theirs with negation.
Our core module extracts adjunct arguments, us-
ing ABNER (Settles, 2005) as its source for addi-
tional named entities. We experimented with map-
ping these arguments to non-core event participants
(Site, Contextgene, etc.); however, we did not in-
clude them in our official submission, because they
seemed to require more work with respect to map-
ping to shared task specifications. Due to this short-
coming, the performance of our system suffered sig-
nificantly in the EPI track.
A particularly encouraging outcome for our sys-
tem is that our results on the GENIA development
set versus on the test set were very close (an F-
score of 51.03 vs. 50.32), indicating that our gen-
eral approach avoided overfitting, while capturing
the linguistic generalizations, as we intended. We
observe similar trends with the other tracks, as well.
In the EPI track, development/test F-score results
were 29.10 vs. 27.88; while, in the ID track, inter-
Event Class Recall Precis. F-score
Localization 39.27 90.36 54.74
Binding 29.33 49.66 36.88
Gene expression 65.87 86.84 74.91
Transcription 32.18 58.95 41.64
Protein catabolism 66.67 71.43 68.97
Phosphorylation 75.14 94.56 83.73
EVT-TOTAL 52.67 78.04 62.90
Regulation 33.77 42.48 37.63
Positive regulation 35.97 47.66 41.00
Negative regulation 36.43 43.88 39.81
REG-TOTAL 35.72 45.85 40.16
Negation 18.77 44.26 26.36
Speculation 21.10 38.46 27.25
MOD-TOTAL 19.97 40.89 26.83
ALL-TOTAL 43.55 59.58 50.32
Table 6: Official GENIA track results, with approximate
span matching/approximate recursive matching evalua-
tion criteria
estingly, our test set performance was better (39.64
vs. 44.21). We also obtained the highest recall in
the ID track, despite the fact that our system typi-
cally favors precision. We attribute this somewhat
idiosyncratic performance in the ID track partly to
the fact that we did not use a track-specific trigger
dictionary. Most of the ID track event types are
the same as those of GENIA track, which probably
led to identification of some ID events with GENIA-
only triggers5.
One of the interesting aspects of the shared task
was its inclusion of full-text articles in training and
evaluation. Cohen et al (2010) show that structure
and content of biomedical abstracts and article bod-
ies differ markedly and suggest that some of these
5This clearly also led to low precision particularly in com-
plex regulatory events.
179
Track-Eval. Type Recall Precis. F-score
EPI-FULL 20.83 42.14 27.88
EPI-CORE 40.28 76.71 52.83
ID-FULL 49.00 40.27 44.21
ID-CORE 50.77 43.25 46.71
Table 7: Official evaluation results for EPI and ID tracks.
Primary evaluation criteria underlined.
differences may pose problems in processing full-
text articles. Since one of our goals was to determine
the generality of our system across text types, we
did not perform any full text-specific optimization.
Our results on article bodies are notable: our system
had stable performance across text types (in fact, we
had a very slight F-score improvement on full-text
articles: 50.40 vs. 50.28). This contrasts with the
drop of a few points that seems to occur with other
well-performing systems. Taking only full-text arti-
cles into consideration, we would be ranked 4th in
the GENIA track. Furthermore, a preliminary error
analysis with full-text articles seems to indicate that
parsing-related errors are more prevalent in the full-
text article set than in the abstract set, consistent with
Cohen et al?s (2010) findings. At the same time, our
results confirm that we were able to abstract away
from this complexity to some degree with our ap-
proach.
We have a particular interest in speculation and
negation detection. Therefore, we examined our re-
sults on the GENIA development set with respect to
Task 3 more closely. Consistent with our previous
shared task results, we determined that the majority
of errors were due to misidentified or missed base
events (70% of the precision errors and 83% of the
recall errors)6. Task 3-specific precision errors in-
cluded cases in which speculation or negation was
debatable, as the examples below show. In Exam-
ple (7a), our system detected a Speculation instance,
due to the verbal predicate suggesting, which scopes
over the event indicated by role. In Example (7b),
our system detected a Negation instance, due to the
nominal predicate lack, which scopes over the events
indicated by expression. Neither were annotated as
6Even a bigger percentage of speculation/negation-related
errors in the EPI and ID tracks were due to the same problem,
as the overall accuracy in those tracks is lower.
such in the shared task corpus.
(7) (a) . . . suggesting a role of these 3? elements
in beta-globin gene expression.
(b) . . . DT40 B cell lines that lack expression
of either PKD1 or PKD3 . . .
Another class of precision errors was due to argu-
ment propagation up the embedding graph. It seems
the current algorithm may be too permissive in some
cases and a more refined approach to argument prop-
agation may be necessary. In the following example,
while suggest, an epistemic trigger, does not embed
induction directly (as shown in (8b)), the intermedi-
ate nodes simply propagate the proposition associ-
ated with the induction node up the graph, leading
us to conclude that the proposition triggered by in-
duction is speculated, leading to a precision error.
(8) (a) . . . these findings suggest that PWM is able
to initiate an intracytoplasmic signaling
cascade and EGR-1 induction . . .
(b) suggest ? able ? initiate ? induction
Among the recall errors, some of them were due
to shortcomings of the composition algorithm, as it
is currently implemented. One recall problem in-
volved the embedding status of and rules concern-
ing copular constructions, which we had not yet ad-
dressed. Therefore, we miss the relatively straight-
forward Speculation instances in the following ex-
amples.
(9) (a) . . . the A3G promoter appears constitu-
tively active.
(b) . . . the precise factors that mediate this in-
duction mechanism remain unknown.
Similarly, the lack of a trigger expression in our dic-
tionary may cause recall errors. The example below
shows an instance where this occurs, in addition to
lack of an appropriate argument identification rule:
(10) mRNA was quantified by real-time PCR for
FOXP3 and GATA3 expression.
Our system also missed an interesting, domain-
specific type of negation, in which the minus sign
indicates negation of the event that the entity partic-
ipates in.
(11) . . . CD14- surface Ag expression . . .
180
5 Conclusions and Future Work
We explored a two-phase approach to event ex-
traction, distinguishing general linguistic principles
from task-specific aspects, in accordance with the
generalization theme of the shared task. Our results
demonstrate the viability of this approach on both
abstracts and article bodies, while also pinpointing
some of its shortcomings. For example, our error
analysis shows that some aspects of semantic com-
position algorithm (argument propagation, in partic-
ular) requires more refinement. Furthermore, using
the same trigger expression dictionary for all tracks
seems to have negative effect on the overall perfor-
mance. The incremental nature of our system de-
velopment ensures that some of these shortcomings
will be addressed in future work.
We participated in three supporting tasks, two
of which (Co-reference (CO) and Entity Relations
(REL) tasks (Nguyen et al (2011) and Pyysalo et
al. (2011b), respectively) were relevant to the main
portion of the shared task; however, due to time con-
straints, we were not able to fully incorporate these
modules into our general framework, with the ex-
ception of the co-reference resolution of relative pro-
nouns. Since our goal is to move towards discourse
interpretation, we plan to incorporate these modules
(inter-sentential co-reference resolution, in particu-
lar) into our framework. After applying the lessons
we learned in the shared task and fully incorporating
these modules, we plan to make our system available
to the scientific community.
References
Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteen
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Meeting of the Associ-
ation for Computational Linguistics, pages 173?180.
K Bretonnel Cohen, Helen L Johnson, Karin Verspoor,
Christophe Roeder, and Lawrence E Hunter. 2010.
The structural and content aspects of abstracts versus
bodies of full text journal articles are different. BMC
Bioinformatics, 11:492.
Laurence Danlos. 2006. ?Discourse verbs? and dis-
course periphrastic links. In C Sidner, J Harpur,
A Benz, and P Ku?hnlein, editors, Second Workshop
on Constraints in Discourse (CID06), pages 59?65.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation, pages 449?454.
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP Shared Task 2011
- Bacteria Gene Interactions and Renaming. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2008. Recognizing
speculative language in biomedical research articles:
a linguistically motivated perspective. BMC Bioinfor-
matics, 9 Suppl 11:s10.
Halil Kilicoglu and Sabine Bergler. 2009. Syntactic de-
pendency based heuristics for biological event extrac-
tion. In Proceedings of Natural Language Process-
ing in Biomedicine (BioNLP) NAACL 2009 Workshop,
pages 119?127.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9:10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
the 46th Meeting of the Association for Computational
Linguistics, pages 101?104.
181
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
Overview of the Protein Coreference task in BioNLP
Shared Task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Frank R Palmer. 1986. Mood and modality. Cambridge
University Press, Cambridge, UK.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011a.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.
2011b. Overview of the Entity Relations (REL) sup-
porting task of BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Burr Settles. 2005. ABNER: An open source tool for
automatically tagging genes, proteins, and other entity
names in text. Bioinformatics, 21(14):3191?3192.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 347?354.
182
