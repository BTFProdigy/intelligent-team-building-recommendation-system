Examining the Role of Statistical and Linguistic Knowledge 
Sources in a General-Knowledge Question-Answering System 
Cla i re  Card ie  1 and V incent  Ng  1 and Dav id  P ie rce  1 and Chr i s  Buck ley  2 
Depar tment  of Computer  Science, Cornell  University, I thaca, NY 148531 
SaB IR  Research 2 
E-mail: cardie,yung,pierce@cs.cornel l .edu, chr isb@sabir .com 
Abstract 
We describe and evaluate an implemented system 
for general-knowledge question answering. The sys- 
tem combines techniques for standard ad-hoc infor- 
mation retrieval (IR), query-dependent text summa- 
rization, and shallow syntactic and semantic sen- 
tence analysis. In a series of experiments we examine 
the role of each statistical and linguistic knowledge 
source in the question-answering system. In con- 
trast to previous results, we find first that statisti- 
cal knowledge of word co-occurrences a computed 
by IR vector space methods can be used to quickly 
and accurately locate the relevant documents for 
each question. The use of query-dependent text 
summarization techniques, however, provides only 
small increases in performance and severely limits 
recall levels when inaccurate. Nevertheless, it is the 
text summarization component that allows subse- 
quent linguistic filters to focus on relevant passages. 
We find that even very weak linguistic knowledge 
can offer substantial improvements over purely IR- 
based techniques for question answering, especially 
when smoothly integrated with statistical prefer- 
ences computed by the IR subsystems. 
1 In t roduct ion  
In this paper, we describe and evaluate an imple- 
mented system for general-knowledge question an- 
swering. Open-ended question-answering systems 
that allow users to pose a question of any type, in 
any language, without domain restrictions, remain 
beyond the scope of today's text-processing systems. 
We investigate instead a restricted, but nevertheless 
useful variation of the problem (TREC-8, 2000): 
Given a large text collection and a set of 
questions specified in English, find answers 
to the questions in the collection. 
In addition, the restricted task guarantees that: 
? the answer exists in the collection, 
? all supporting information for the answer lies in 
a single document, and 
? the answer is short m less than 50 bytes in 
length. 
Consider, for example, the question Which country 
has the largest part of the Amazon rain forest?, taken 
from the TREC8 Question Answering development 
corpus. The answer (in document LA032590-0089) 
is Brazil 
Previous research as addressed similar question- 
answering (QA) scenarios using a variety of natu- 
ral language processing (NLP) and information re- 
trieval (IR) techniques. Lehnert (1978) tackles the 
difficult task of answering questions in the context of 
story understanding. Unlike our restricted QA task, 
questions to Lehnert's ystem often require answers 
that are not explicitly mentioned in the story. Her 
goal then is to answer questions by making infer- 
ences about actions and actors in the story using 
world knowledge in the form of scripts, plans, and 
goals (Schank and Abelson, 1977). More recently, 
Burke et al (1995; 1997) describe a system that an- 
swers natural anguage questions using a database of 
question-answer pairs built from existing frequently- 
asked question (FAQ) files. Their FAQFinder sys- 
tem uses IR techniques to match the given question 
to questions in the database. It then uses the Word- 
Net lexical semantic knowledge base (Miller et al, 
1990; Fellbaum, 1998) to improve the quality of the 
match. 
Kupiec (1993) investigates a closed-class QA task 
that is similar in many respects to the TREC8 
QA task that we address here: the system answers 
general-knowledge questions using an encyclopedia. 
In addition, Kupiec assumes that all answers are 
noun phrases. Although our task does not explic- 
itly include a "noun phrase" constraint, the answer 
length restriction effectively imposes the same bias 
toward noun phrase answers. Kupiec's MURAX sys- 
tem applies a combination of statistical (IR) and 
linguistic (NLP) techniques. A series of secondary 
boolean search queries with proximity constraints i
combined with shallow parsing methods to find rele- 
vant sections of the encyclopedia, to extract answer 
hypotheses, and to confirm phrase relations speci- 
fied in the question. In an evaluation on 70 "Trivial 
180 
question 
document 
collection 
Retrieval i documents, i 
i text passages i 
i .-i 
i 
IR Subsystems i 
Summarization 
Parsing 
Semantic 
Types 
Linguistic 
Relationships 
Linguistic Filters 
answer 
hypotheses 
! 
Figure 1: General Architecture of the Question-Answering System 
Pursuit" who and what questions, Kupiec concludes 
that robust natural language analysis can add to the 
quality of the information retrieval process. In addi- 
tion, he claims that, for their closed-class QA task, 
vector space IR methods (Salton et al, 1975) appear 
inadequate. 
We present here a new approach to the re- 
stricted question-answering task described above. 
Like MURAX, our system draws from both statisti- 
cal and linguistic sources to find answers to general- 
knowledge questions. The underlying architecture of 
the system, however, is very different: it combines 
vector space IR techniques for document retrieval, a
vector space approach to query-dependent text sum- 
marization, shallow corpus-based syntactic analysis, 
and knowledge-based semantic analysis. We eval- 
uate the system on the TREC8 QA development 
corpus as well as the TREC8 QA test corpus. In 
particular, all parameters for the final QA system 
are determined using the development corpus. Our 
current results are encouraging but not outstanding: 
the system is able to correctly answer 22 out of 38 of 
the development questions and 91 out of 200 of the 
test questions given five guesses for each question. 
Furthermore, the first guess is correct for 16 out of 
the 22 development questions and 53 out of 91 of the 
test questions. 
More importantly, we investigate the relative role 
of each statistical and linguistic knowledge source 
in the proposed IR/NLP question-answering system. 
In contrast o previous results, we find that sta- 
tistical knowledge of word co-occurrences a com- 
puted by vector space models of IR can be used to 
quickly and accurately ocate relevant documents in
the restricted QA task. When used in isolation, vec- 
tor space methods for query-dependent text summa- 
rization, however, provide relatively small increases 
in performance. In addition, we find that the text 
summarization component can severely limit recall 
levels. Nevertheless, it is the summarization compo- 
nent that allows the linguistic filters to focus on rele- 
vant passages. In particular, we find that very weak 
linguistic knowledge can offer substantial improve- 
ments over purely IR-based techniques for question 
answering, especially when smoothly integrated with 
the statistical preferences computed by the IR sub- 
systems. 
In the next section, we describe the general archi- 
tecture of the question-answering system. Section 3 
describes the baseline system and its information re- 
trieval component. Sections 4-7 describe and evalu- 
ate a series of variations to the baseline system that 
incorporate, in turn, query-dependent text summa- 
rization, a syntactic filter, a semantic filter, and an 
algorithm that allows syntactic knowledge to influ- 
ence the initial ordering of summary extracts. Sec- 
tion 8 compares our approach to some of those in 
the recent TREC8 QA evaluation (TREC-8, 2000) 
and describes directions for future work. 
2 System Arch i tec ture  
The basic architecture of the question-answering sys- 
tem is depicted in Figure 1. It contains two main 
components: the IR subsystems and the linguistic 
filters. As a preliminary, ofl\]ine step, the IR sub- 
system first indexes the text collection from which 
answers are to be extracted. Given a question, the 
goal of the IR component is then to return a ranked 
list of those text chunks (e.g. documents, entences, 
or paragraphs) from the indexed collection that are 
most relevant o the query and from which answer 
hypotheses can he extracted. Next, the QA system 
optionally applies one or more linguistic filters to 
the text chunks to extract an ordered list of answer 
hypotheses. The top hypotheses are concatenated to 
form five 50-byte guesses as allowed by the TREC8 
guidelines. Note that many of these guesses may 
be difficult to read and judged as incorrect by the 
181 
TREC8 assessors: we will also describe the results 
of generating single phrases as guesses wherever this 
is possible. 
In the sections below, we present and evaluate a
series of instantiations of this general architecture, 
each of which makes different assumptions regarding 
the type of information that will best support he 
QA task. The next section begins by describing the 
baseline QA system. 
3 The Vector Space Model for 
Document Retrieval 
It is clear that a successful QA system will need 
some way to find the documents hat are most rele- 
vant to the user's question. In a baseline system, we 
assume that standard IR techniques can be used for 
this task. In contrast to MURAX, however, we hy- 
pothesize that the vector space retrieval model will 
suffice. In the vector space model, both the ques- 
tion and the documents are represented asvectors 
with one entry for every unique word that appears 
in the collection. Each entry is the term weight, a 
real number that indicates the presence or absence 
of the word in the text. The similarity between a
question vector, Q = ql ,q2,. . .  ,qn, and a document 
vector, D = dl, d2,. . . ,  tin, is traditionally computed 
using a cosine similarity measure: 
n 
8im(Q,D)  = Z d, .q, 
i..~ l
Using this measure, the IR system returns a ranked 
list of those documents most similar to the question. 
The  Baseline QA System: The  Smart  Vec- 
tor  Space Model .  For the IR component of the 
baseline QA system, we use Smart (Salton, 1971), 
a sophisticated text-processing system based on the 
vector space model and employed as the retrieval 
engine for a number of the top-performing systems 
at recent Text REtrieval Conferences (e.g. Buckley 
et al, 1998a, 1998b). Given a question, Smart re- 
turns a ranked list of the documents most relevant 
to the question. For the baseline QA system and all 
subsequent variations, we use Smart with standard 
term-weighting strategies I and do not use automatic 
relevance f edback (Buckley, 1995). In addition, the 
baseline system applies no linguistic filters. To gen- 
erate answers for a particular question, the system 
starts at the beginning of the top-ranked ocument 
returned by Smart for the question and constructs 
five 50-byte chunks consisting of document text with 
stopwords removed. 
lWe use Lnu term weighting for documents and Itu term 
weighting for the question (Singhal et al, 1996). 
Evaluation. As noted above, we evaluate ach 
variation of our QA system on 38 TREC8 devel- 
opment questions and 200 TREC8 test questions. 
The indexed collection is TREC disks 4 and 5 (with- 
out Congressional Records). Results for the baseline 
Smart IR QA system are shown in the first row of 
Table 1. The system gets 3 out of 38 development 
questions and 29 out of 200 test questions correct. 
We judge the system correct if any of the five guesses 
contains each word of one of the answers. The final 
column of results hows the mean answer ank across 
all questions correctly answered. 
Smart is actually performing much better than its 
scores would suggest. For 18 of the 38 development 
questions, the answer appears in the top-ranked doc- 
ument; for 33 questions, the answer appears in one 
of the top seven documents. For only two questions 
does Smart fail to retrieve a good document in the 
top 25 documents. For the test corpus, over half 
of the 200 questions are answered in the top-ranked 
document (110); over 75% of the questions (155) are 
answered in top five documents. Only 19 questions 
were not answered in the top 20 documents. 
4 Query-Dependent Text 
Summar izat ion  fo r  Quest ion  
Answering 
We next hypothesize that query-dependent text 
summarization algorithms will improve the perfor- 
mance of the QA system by focusing the system 
on the most relevant portions of the retrieved oc- 
uments. The goal for query-dependent summariza- 
tion algorithms is to provide a short summary of 
a document with respect to a specific query. Al- 
though a number of methods for query-dependent 
text summarization are beginning to be developed 
and evaluated in a variety of realistic settings (Mani 
et al, 1999), we again propose the use of vector space 
methods from IR, which can be easily extended to 
the summarization task (Salton et al, 1994): 
1. Given a question and a document, divide the 
document into chunks (e.g. sentences, para- 
graphs, 200-word passages). 
2. Generate the vector epresentation forthe ques- 
tion and for each document chunk. 
3. Use the cosine similarity measure to determine 
the similarity of each chunk to the question. 
4. Return as the query-dependent summary the 
most similar chunks up to a predetermined sum- 
mary length (e.g. 10% or 20% of the original 
document). 
This approach to text summarization was shown 
to be quite successful in the recent SUMMAC eval- 
uation of text summarization systems (Mani et al, 
1999; Buckley et al, 1999). Our general assumption 
182 
here is that Ii~ approaches can be used to quickly 
and accurately find both relevant documents and 
relevant document portions. In related work, Chali 
et al (1999) also propose text summarization tech- 
niques as a primary component for their QA system. 
They employ a combination of vector-space meth- 
ods and lexical chaining to derive their sentence- 
based summaries. We hypothesize that  deeper anal- 
ysis of the summary extracts is better accomplished 
by methods from NLP that can determine syntac- 
tic and semantic relationships between relevant con- 
stituents. There is a risk in using query-dependent 
summaries to focus the search for answer hypothe- 
ses, however: if the summarization algorithm is inac- 
curate, the desired answers will occur outside of the 
summaries and will not be accessible to subsequent 
components of the QA system. 
The Query -Dependent  Text  Summar izat ion  
QA System.  In the next version of the QA sys- 
tem, we augment he baseline system to perform 
query-dependent text summarization for the top k 
retrieved ocuments. More specifically, the IR sub- 
system returns the summary extracts (sentences or 
paragraphs) for the top k documents after sort- 
ing them according to their cosine similarity scores 
w.r.t, the question. As before, no linguistic filters are 
applied, and answers are generated by constructing 
50-byte chunks from the ordered extracts after re- 
moving stopwords. In the experiments below, k = 7 
for the development questions and k = 6 for the test 
questions. 2 
Eva luat ion .  Results for the Text Summarization 
QA system using sentence-based summaries are 
shown in the second row of Table 1. Here we see 
a relatively small improvement: the system now 
answers four development and 45 test questions 
correctly. The mean answer rank, however, im- 
proves noticeably from 3.33 to 2.25 for the develop- 
ment corpus and from 3.07 to 2.67 for the test cor- 
pus. Paragraph-based summaries yield similar but 
slightly smaller improvements; as a result, sentence 
summaries are used exclusively in subsequent sec- 
tions. Unfortunately, the system's reliance on query- 
dependent text summarization actually limits its po- 
tential: in only 23 of the 38 development questions 
(61%), for example, does the correct answer appear 
in the summary for one of the top k -- 7 documents. 
The QA system cannot hope to answer correctly any 
of the remaining 15 questions. For only 135 of the 
200 questions in the test corpus (67.5%) does the 
correct answer appear in the summary for one of 
2The value for k was chosen so that at least 80% of the 
questions in the set had answers appearing in the retrieved 
documents ranked 1-k. We have not experimented exten- 
sively with many values of k and expect that better perfor- 
mance can be obtained by tuning k for each text collection. 
the top k -- 6 documents. 3 It is possible that au- 
tomatic relevance feedback or coreference r solution 
would improve performance. We are investigating 
these options in current work. 
The decision of whether or not to incorporate text 
summarization i the QA system depends, in part, 
on the ability of subsequent processing components 
(i.e. the linguistic filters) to locate answer hypothe- 
ses. If subsequent components are very good at 
discarding implausible answers, then summarization 
methods may limit system performance. Therefore, 
we investigate next the use of two linguistic filters in 
conjunction with the query-dependent text summa- 
rization methods evaluated here. 
5 Incorporat ing  the  Noun Phrase  
F i l te r  
The restricted QA task that we investigate requires 
answers to be short - -  no more than 50 bytes in 
length. This effectively eliminates how or why ques- 
tions from consideration. Almost all of the remain- 
ing question types are likely to have noun phrases as 
answers. In the TREC8 development corpus, for ex- 
ample, 36 of 38 questions have noun phrase answers. 
As a result, we next investigate the use of a 
very simple linguistic filter that considers only noun 
phrases as answer hypotheses. The filter operates on 
the ordered list of summary extracts for a particular 
question and produces a list of answer hypotheses, 
one for each noun phrase (NP) in the extracts in the 
left-to-right order in which they appeared. 
The  NP-based  QA System.  Our implementa- 
tion of the NP-based QA system uses the Empire 
noun phrase finder, which is described in detail in 
Cardie and Pierce (1998). Empire identifies base 
NPs - -  non-recursive noun phrases - -  using a very 
simple algorithm that matches part-of-speech tag se- 
quences based on a learned noun phrase grammar. 
The approach is able to achieve 94% precision and 
recall for base NPs derived from the Penn Treebank 
Wall Street Journal (Marcus et al, 1993). In the 
experiments below, the NP filter follows the applica- 
tion of the document retrieval and text summariza- 
tion components. Pronoun answer hypotheses are 
discarded, and the NPs are assembled into 50-byte 
chunks. 
Eva luat ion.  Results for the NP-based QA sys- 
tem are shown in the third row of Table 1. The 
noun phrase filter markedly improves system per- 
formance for the development corpus, nearly dou- 
3Paragraph-based summaries provide better coverage on 
the test corpus than sentence-based summaries: for 151 ques- 
tions, the correct answer appears in the summary for one of 
the top k documents. This suggests that paragraph sum- 
maries might be better suited for use with more sophisticated 
linguistic filters that are capable of discerning the answer in 
the larger summary. 
1~"~ 183
Development Corpus Test Corpus 
Smart Vector Space Model 
Query-Dependent Text Summarization 
Text Summarization + NPs 
Text Summarization + NPs + Semantic Type 
Text Summarization with Syntactic Ordering + 
NPs + Semantic Type 
Correct (%) MAR 
3/38 0.079 3.33 
4/38 0.105 2.25 
7/38 0.184 2.29 
21/38 0.553 1.38 
22/38 0.579 1.32 
Correct(%) MAR 
29/200 0.145 3.07 
45/200 0.225 2.67 
50/200 0.250 2.66 
86/200 0.430 1.90 
91/200 0.455 1.82 
Table 1: Evaluation of the Role of Statistical and Limited Linguistic Knowledge for the TREC8 Question 
Answering Task. Results for 38 development and 200 test questions are shown. The mean answer ank 
(MAR) is computed w.r.t, all questions correctly answered. 
bling the number of questions answered correctly. 
We found these results somewhat surprising since 
this linguistic filter is rather weak: we expected it
to work well only in combination with the semantic 
filter described below. The noun phrase filter has 
much less of an effect on the test corpus, improving 
performance on questions answered from 45 to 50. 
In a separate experiment, we applied the NP filter 
to the baseline system that includes no text summa? 
rization component. Here the NP filter does not 
improve performance - - the system gets only two 
questions correct. This indicates that the NP filter 
depends critically on the text summarization com- 
ponent. As a result, we will continue to use query- 
dependent text summarization i  the experiments 
below. 
The NP filter provides the first opportunity to 
look at single-phrase answers. The preceding QA 
systems produced answers that were rather unnat- 
urally chunked into 50-byte strings. When such 
chunking is disabled, only one development and 20 
test questions are answered. The difference in per- 
formance between the NP filter with chunking and 
the NP filter alone clearly indicates that the NP fil- 
ter is extracting ood guesses, but that subsequent 
linguistic processing is needed to promote the best 
guesses to the top of the ranked guess list. 
6 Incorporat ing  Semant ic  Type  
In fo rmat ion  
The NP filter does not explicitly consider the ques- 
tion in its search for noun phrase answers. It is clear, 
however, that a QA system must pay greater atten- 
tion to the syntactic and semantic onstraints spec- 
ified in the question. For example, a question like 
Who was president of the US in 19957 indicates 
that the answer is likely to be a person. In addition, 
there should be supporting evidence from the answer 
document that the person was president, and, more 
specifically, held this office in the US and in 1995. 
We introduce here a second linguistic filter that 
considers the primary semantic onstraint from the 
question. The filter begins by determining the ques- 
tion type, i.e. the semantic type requested in the 
question. It then takes the ordered set of summary 
extracts supplied by the IR subsytem, uses the syn- 
tactic filter from Section 5 to extract NPs, and gen- 
erates an answer hypothesis for every noun phrase 
that is semantically compatible with the question 
type. Our implementation of this semantic class fil- 
ter is described below. The filter currently makes no 
attempt to confirm other linguistic relations men- 
tioned in the question. 
The  Semantic Type  Checking QA System. 
For most questions, the question word itself deter- 
mines the semantic type of the answer. This is true 
for who, where, and when questions, for example, 
which request a person, place, and time expression 
as an answer. For many which and what questions, 
however, determining the question type requires ad- 
ditional syntactic analysis. For these, we currently 
extract the head noun in the question as the question 
type. For example, in Which country has the largest 
part o$ the Amazon rain :forest? we identify country 
as the question type. Our heuristics for determining 
question type were based on the development cor- 
pus and were designed to be general, but have not 
yet been directly evaluated on a separate question 
corpus. 
? Given the question type and an answer hypoth- 
esis, the Semantic Type Checking QA System then 
uses WordNet o check that an appropriate ancestor- 
descendent relationship holds. Given Brazil as an 
answer hypothesis for the above question, for exam- 
ple, Wordnet's type hierarchy confirms that Brazil 
is a subtype of country, allowing the system to con- 
clude that the semantic type of the answer hypoth- 
esis matches the question type. 
For words (mostly proper nouns) that do not ap- 
pear in WordNet, heuristics are used to determine 
semantic type. There are heuristics to recognize 
13 basic question types: Person, Location, Date, 
Month, Year, Time, Age, Weight, Area, Volume, 
Length, Amount, and Number. For Person ques- 
tions, for example, the system relies primarily on a 
rule that checks for capitalization and abbreviations 
' I IOA  184
in order to identify phrases that correspond to peo- 
ple. There are approximately 20 such rules that to- 
gether cover all 13 question types listed above. The 
rules effectively operate as a very simple named en- 
tity identifier. 
Eva luat ion .  Results for the Semantic Type 
Checking variation of the QA system are shown in 
the fourth row of Table 1. Here we see a dramatic 
increase in performance: the system answers three 
times as many development questions (21) correctly 
over the previous variation. This is especially en- 
couraging iven that the IR and text summarization 
components limit the maximum number correct o 
23. In addition, the mean answer rank improves 
from 2.29 to 1.38. A closer look at Table 1, however, 
indicates problems with the semantic type checking 
linguistic filter. While performance on the develop- 
ment corpus increases by 37 percentage points (from 
18.4% correct to 55.3% correct), relative gains for 
the test corpus are much smaller. There is only an 
improvement of 18 percentage points, from 25.0% 
correct (50/200) to 43.0% correct (86/200). This 
is a clear indication that the heuristics used in the 
semantic type checking component, which were de- 
signed based on the development corpus, do not gen- 
eralize well to different question sets. Replacing the 
current heuristics with a Named Entity identifica- 
tion component or learning the heuristics using stan- 
dard inductive learning techniques should help w i th  
the scalability of this linguistic filter. 
Nevertheless, it is somewhat surprising that very 
weak syntactic information (the NP filter) and weak 
semantic lass information (question type checking) 
can produce such improvements. In particular, it 
appears that it is reasonable to rely implicitly on 
the IR subsystems to enforce the other linguistic re- 
lationships pecified in the query (e.g. that Clinton 
is president, hat this office was held in the US and 
in 1995). 
Finally, when 50-byte chunking is disabled for 
the semantic type checking QA variation, there is 
a decrease in the number of questions correctly an- 
swered, to 19 and 57 for the development and test 
corpus, respectively. 
7 Syntact i c  P re ferences  fo r  Order ing  
Summary  Ext rac ts  
Syntactic and semantic linguistic knowledge has 
been used thus far as post-processing filters that lo- 
cate and confirm answer hypotheses from the statis- 
tically specified summary extracts. We hypothesized 
that further improvements might be made by allow- 
ing this linguistic knowledge to influence the initial 
ordering of text chunks for the linguistic filters. In a 
final system, we begin to investigate this claim. Our 
general approach is to define a new scoring mea- 
sure that operates on the summary extracts and can 
be used to reorder the extracts based on linguistic 
knowledge. 
The  QA System wi th  L inguist ic  Reorder ing  
o f  Summary  Ext racts .  As described above, our 
final version of the QA system ranks summary ex- 
tracts according to both their vector space similarity 
to the question as well as linguistic evidence that the 
answer lies within the extract. In particular, each 
summary extract E for question q is ranked accord- 
ing to a new score, Sq: 
sq(E) = w(E) . LRq(E) 
The intuition behind the new score is to prefer sum- 
mary extracts that exhibit the same linguistic rela- 
tionships as the question (as indicated by LRq) and 
to give more weight (as indicated by w) to linguistic 
relationship matches in extracts from higher-ranked 
documents. More specifically, LRq(E ) is the num- 
ber of linguistic relationships from the question that 
appear in E. In the experiments below, LRq(E) 
is just the number of base NPs from the question 
that appear in the summary extract. In future 
work, we plan to include other pairwise linguistic 
relationships (e.g. subject-verb relationships, verb- 
object relationships, pp-attachment relationships). 
The weight w(E) is a number between 0 and 1 that 
is based on the retrieval rank r of the document that 
contains E: 
w(E) = max(m, 1 - p. r) 
In our experiments, m = 0.5 and p = 0.1. Both 
values were selected manually based on the develop- 
ment corpus; an extensive search for the best such 
values was not done. 
The summary extracts are sorted according to the 
new scoring measure and the ranked list of sentences 
is provided to the linguistic filters as before. 
Eva luat ion .  Results for this final variation of the 
QA system are shown in the bottom row of Table 1. 
Here we see a fairly minor increase in performance 
over the use of linguistic filters alone: the system 
answers only one more question correctly than the 
previous variation for the development corpus and 
answers five additional questions for the test cor- 
pus. The mean answer rank improves only negligi- 
bly. Sixteen of the 22 correct answers (73%) appear 
as the top-ranked guess for the development corpus; 
only 53 out of 91 correct answers (58%) appear as 
the top-ranked guess for the test corpus. Unfortu- 
nately, when 50-byte chunking is disabled, system 
performance drops precipitously, by 5% (to 20 out 
of 38) for the development corpus and by 13% (to 
65 out of 200) for the test corpus. As noted above, 
this indicates that the filters are finding the answers, 
but more sophisticated linguistic sorting is needed 
to promote the best answers to the top. Through 
185 
its LRq term, the new scoring measure does pro- 
vide a mechanism for allowing other linguistic re- 
lationships to influence the initial ordering of sum- 
mary extracts. The current results, however, indi- 
cate that with only very weak syntactic information 
(i.e. base noun phrases), the new scoring measure 
is only marginally successful in reordering the sum- 
mary extracts based on syntactic information. 
As noted above, the final system (with the liberal 
50-byte answer chunker) correctly answers 22 out of 
38 questions for the development corpus. Of the 16 
errors, the text retrieval component is responsible for 
five (31.2%), the text summarization component for 
ten (62.5%), and the linguistic filters for one (6.3%). 
In this analysis we consider the linguistic filters re- 
sponsible for an error if they were unable to pro- 
mote an available answer hypothesis to one of the 
top five guesses. A slightly different situation arises 
for the test corpus: of the 109 errors, the text re- 
trieval component is responsible for 39 (35.8%), the 
text summarization component for 26 (23.9%), and 
the linguistic filters for 44 (40.4%). As discussed in 
Section 6, the heuristics that comprise the semantic 
type checking filter do not scale to the test corpus 
and are the primary reason for the larger percentage 
of errors attributed to the linguistic filters for that 
corpus. 
8 Re la ted  Work  and  Conc lus ions  
We have described and evaluated a series of 
question-answering systems, each of which incorpo- 
rates a different combination of statistical and lin- 
guistic knowledge sources. We find that even very 
weak linguistic knowledge can offer substantial im- 
provements over purely IR-based techniques espe- 
cially when smoothly integrated with the text pas- 
sage preferences computed by the IR subsystems. 
Although our primary goal was to investigate the 
use of statistical and linguistic knowledge sources, it 
is possible to compare our approach and our results 
to those for systems in the recent TREC8 QA evalu- 
ation. Scores on the TREC8 test corpus for systems 
participating in the QA evaluation ranged between 
3 and 146 correct. Discarding the top three scores 
and the worst three scores, the remaining eight sys- 
tems achieved between 52 and 91 correct. Using the 
liberal answer chunker, our final QA system equals 
the best of these systems (91 correct); without it, 
our score of 65 correct places our QA system near 
the middle of this group of eight. 
Like the work described here, virtually all of the 
top-ranked TREC8 systems use a combination of 
IR and shallow NLP for their QA systems. IBM's 
AnSel system (Prager et al, 2000), for example, 
employs finite-state patterns as its primary shallow 
NLP component. These are used to recognize a 
fairly broad set of about 20 named entities. The 
IR component indexes only text passages associ- 
ated with these entities. The AT&T QA system 
(Singhal et al, 2000), the Qanda system (Breck et 
al., 2000), and the SyncMatcher system (Oard et 
al., 2000) all employ vector-space methods from IR, 
named entity identifiers, and a fairly simple ques- 
tion type determiner. In addition, SyncMatcher 
uses a broad-coverage d pendency parser to enforce 
phrase relationship constraints. Instead of the vec- 
tor space model, the LASSO system (Moldovan et 
al., 2000) uses boolean search operators for para- 
graph retrieval. Recognition of answer hypotheses 
in their system relies on identifying named entities. 
Finally, the Cymphony QA system (Srihari and Li, 
2000) relies heavily on named entity identification; it 
also employs tandard IR techniques and a shallow 
parser. 
In terms of statistical and linguistic knowledge 
sources employed, the primary difference between 
these systems and ours is our lack of an adequate 
named entity tagger. Incorporation of such a tag- 
ger will be a focus of future work. In addition, we 
believe that the retrieval and summarization compo- 
nents can be improved by incorporating automatic 
relevance feedback (Buckley, 1995) and coreference 
resolution. Morton (1999), for example, shows that 
coreference r solution improves passage retrieval for 
their question-answering system. We also plan to 
reconsider paragraph-based summaries given their 
coverage on the test corpus. The most critical area 
for improvement, however, is the linguistic filters. 
The semantic type filter will be greatly improved by 
the addition of a named entity tagger, but we believe 
that additional gains can be attained by augmenting 
named entity identification with information from 
WordNet. Finally, we currently make no attempt to 
confirm any phrase relations from the query. With- 
out this, system performance will remain severely 
limited. 
9 Acknowledgments  
This work was supported in part by NSF Grants IRI- 
9624639 and GER-9454149. 
Re ferences  
E. Breck, J. Burger, L. Ferro, D. House, M. Light, 
and I. Mani. 2000. A Sys Called Qanda. In 
E. Voorhees, editor, Proceedings of the Eighth 
Text REtrieval Conference TREC 8. NIST Spe- 
cial Publication. In press. 
C. Buckley, M. Mitra, J. Walz, and C. Cardie. 
1998a. SMART high precision: TREC 7. In 
E. Voorhees, editor, Proceedings of the Seventh 
Text REtrieval Conference TREC 7, pages 285- 
298. NIST Special Publication 500-242. 
C. Buckley, M. Mitra, J. Walz, and C. Cardie. 
1998b. Using clustering and superconcepts within 
186 
SMART : TREC 6. In E. Voorhees, editor, Pro- 
ceedings of the Sixth Text REtrieval Conference 
TREC 6, pages 107-124. NIST Special Publica- 
tion 500-240. 
C. Buckley, C. Cardie, S. Mardis, M. Mitra, 
D. Pierce, K. Wagstaff, and J. Walz. 1999. The 
Smart/Empire TIPSTER IR System. In Proceed- 
ings, TIPSTER Text Program (Phase III). Mor- 
gan Kauhnann. To appear. 
Chris Buckley. 1995. Massive Query Expansion 
/or Relevance Feedback. Cornell University, Ph.D. 
Thesis, Ithaca, New York. 
R. Burke, K. Hammond, and J. Kozlovsky. 
1995. Knowledge-Based Information Retrieval 
from Semi-Structured Text. In Working Notes of 
the AAAI Fall Symposium on AI Applications in 
Knowledge Navigation and Retrieval, pages 19-24. 
AAAI Press. 
R. Burke, K. Hammond, V. Kulyukin, S. Lyti- 
hen, N. Tomuro, and S. Schoenberg. 1997. ques- 
tion answering from Frequently-Asked Question 
Files. Technical Report TR-97-05, University of 
Chicago. 
C. Cardie and D. Pierce. 1998. Error-Driven Prun- 
ing of Treebank Grammars for Base Noun Phrase 
Identification. In Proceedings of the 36th An- 
nual Meeting of the Association .for Computa- 
tional Linguistics and COLING-98, pages 218- 
224, University of Montreal, Montreal, Canada. 
Association for Computational Linguistics. 
Y. Chali, S. Matwin, and S. Szpakowicz. 1999. 
Query-Biased Text Summarization as a Question- 
Answering Technique. In Proceedings o.f the AAAI 
Fall Symposium on Question Answering Systems, 
pages 52-56. AAAI Press. AAAI TR FS-99-02. 
C. Fellbaum. 1998. WordNet: An Electronical Lex- 
iced Database. MIT Press, Cambridge, MA. 
J. Kupiec. 1993. MURAX: A Robust Linguistic ap- 
proach For Question Answering Using An On- 
Line Encyclopedia. In Proceedings of A CM SI- 
GIR, pages 181-190. 
W. Lehnert. 1978. The Process o/ Question Answer- 
ing. Lawrence Erlbaum Associates, Hillsdale, NJ. 
I. Mani, T. Firmin, D. House, G. Klein, B. Sund- 
heim, and L. Hirschman. 1999. The TIPSTER 
SUMMAC Text Summarization Evaluation. In 
Ninth Annual Meeting o.f the EACL, University 
of Bergen, Bergen, Norway. 
M. Marcus, M. Marcinkiewicz, and B. Santorini. 
1993. Building a Large Annotated Corpus of En- 
glish: The Penn Treebank. Computational Lin- 
guistics, 19(2):313-330. 
G. A. Miller, R. Beckwith, C. FeUbaum, D. Gross, 
and K. J. Miller. 1990. WordNet: an on-line lex- 
ical database. International Journal of Lexicogra- 
phy, 3(4):235-245. 
D. Moldovan, S. Harabagiu, M. Pa~ca, R. Mihal- 
cea, R. Goodrum, R. Girju, and V. Rus. 2000. 
LASSO: A Tool for Surfing the Answer Net. In 
E. Voorhees, editor, Proceedings of the Eighth 
Text REtrieval Conference TREC 8. NIST Spe- 
cial Publication. In press. 
T. S. Morton. 1999. Using Coreference to Im- 
prove Passage Retrieval for Question Answering. 
In Proceedings of the AAAI Fall Symposium on 
Question Answering Systems, pages 72-74. AAAI 
Press. AAAI TR FS-99-02. 
D. W. Oard, J. Wang, D. Lin, and I. Soboroff. 2000. 
TREC-8 Experiments at Maryland: CLIR, QA 
and Routing. In E. Voorhees, editor, Proceedings 
o.f the Eighth Text REtrieval Conference TREC 8. 
NIST Special Publication. In press. 
J. Prager, D. Radev, E. Brown, A. Coden, and 
V. Samn. 2000. The Use of Predictive Anno- 
tation for Question Answering in TRECS. In 
E. Voorhees, editor, Proceedings o/ the Eighth 
Text REtrieval Conference TREC 8. NIST Spe- 
cial Publication. In press. 
G. Salton, A. Wong, and C.S. Yang. 1975. A vector 
space model for information retrieval. Communi- 
cations o/the ACM, 18(11):613-620. 
G. Salton, J. Allan, C. Buckley, and M. Mitra. 1994. 
Automatic analysis, theme generation and sum- 
marization of machine-readable t xts. Science, 
264:1421-1426, June. 
Gerard Salton, editor. 1971. The SMART Re- 
trieval System--Experiments in Automatic Doc- 
ument Processing. Prentice Hall Inc., Englewood 
Cliffs, NJ. 
R. C. Schank and R. P. Abelson. 1977. Scripts, 
plans, goals, and understanding. Lawrence Erl- 
bantu Associates, Hillsdale, NJ. 
Amit Singhal, Chris Buckley, and Mandar Mitra. 
1996. Pivoted document length normalization. In 
H. Frei, D. Harman, P. Schauble, and R. Wilkin- 
son, editors, Proceedings o/the Nineteenth An- 
nual International ACM SIGIR Conference on 
Research and Development in Information Re- 
trieval, pages 21-29. Association for Computing 
Machinery. 
A. Singhal, S. Abney, M. Bacchiani, M. Collins, 
D. Hindle, and F. Pereira. 2000. AT&T at TREC- 
8. In E. Voorhees, editor, Proceedings of the 
Eighth Text REtrieval Conference TREC 8. NIST 
Special Publication. In press. 
R. Srihari and W. Li. 2000. Question Answer- 
ing Supported by Information Extraction. In 
E. Voorhees, editor, Proceedings of the Eighth 
Text REtrieval Conference TREC 8. NIST Spe- 
cial Publication. In press. 
TREC-8. 2000. Proceedings of the Eighth Text RE- 
trieval Conference TREC 8. NIST. In press. 
1Q'7  187
 
		Multidocument Summarization via Information Extraction 
Michael White and Tanya Korelsky 
CoGenTex, Inc. 
Ithaca, NY 
mike,tanya@cogentex.com 
Claire Cardie, Vincent Ng, David Pierce, and 
Kiri Wagstaff 
Department of Computer Science 
Cornell University, Ithaca, NY 
cardie,yung,pierce,wkiri@cs.cornell.edu 
 
ABSTRACT 
We present and evaluate the initial version of RIPTIDES, a 
system that combines information extraction, extraction-based 
summarization, and natural language generation to support user-
directed multidocument summarization. 
1. INTRODUCTION 
Although recent years has seen increased and successful research 
efforts in the areas of single-document summarization, 
multidocument summarization, and information extraction, very 
few investigations have explored the potential of merging 
summarization and information extraction techniques.  This paper 
presents and evaluates the initial version of RIPTIDES, a system 
that combines information extraction (IE), extraction-based 
summarization, and natural language generation to support user-
directed multidocument summarization.  (RIPTIDES stands for 
RapIdly Portable Translingual Information extraction and 
interactive multiDocumEnt Summarization.)  Following [10], we 
hypothesize that IE-supported summarization will enable the 
generation of more accurate and targeted summaries in specific 
domains than is possible with current domain-independent 
techniques.  
In the sections below, we describe the initial implementation and 
evaluation of the RIPTIDES IE-supported summarization system.  
We conclude with a brief discussion of related and ongoing work. 
2. SYSTEM DESIGN 
Figure 1 depicts the IE-supported summarization system. The 
system first requires that the user select (1) a set of documents in 
which to search for information, and (2) one or more scenario 
templates (extraction domains) to activate. The user optionally 
provides filters and preferences on the scenario template slots, 
specifying what information s/he wants to be reported in the 
summary. RIPTIDES next applies its Information Extraction 
subsystem to generate a database of extracted events for the 
selected domain and then invokes the Summarizer to generate a 
natural language summary of the extracted information subject to 
the user?s constraints. In the subsections below, we describe the 
IE system and the Summarizer in turn. 
2.1 IE System 
The domain for the initial IE-supported summarization system and 
its evaluation is natural disasters.  Very briefly, a top-level natural 
disasters scenario template contains: document-level information 
(e.g. docno, date-time); zero or more agent elements denoting 
each person, group, and organization in the text; and zero or 
more disaster elements.  Agent elements encode standard 
information for named entities (e.g. name, position, geo-political 
unit).  For the most part, disaster elements also contain standard 
event-related fields (e.g. type, number, date, time, location, 
damage sub-elements).  
The final product of the RIPTIDES system, however, is not a set 
of scenario templates, but a user-directed multidocument 
summary.  This difference in goals influences a number of 
template design issues.  First, disaster elements must distinguish 
different reports or views of the same event from multiple sources.  
As a result, the system creates a separate disaster event for each 
such account.  Disaster elements should also include the reporting 
agent, date, time, and location whenever possible.  In addition, 
damage elements (i.e. human and physical effects) are best 
grouped according to the reporting event.  Finally, a slight 
broadening of the IE task was necessary in that extracted text was 
not constrained to noun phrases.  In particular, adjectival and 
adverbial phrases that encode reporter confidence, and sentences 
and clauses denoting relief effort progress appear beneficial for 
creating informed summaries.  Figure 2 shows the scenario 
template for one of 25 texts tracking the 1998 earthquake in 
Afghanistan (TDT2 Topic 89).  The texts were also manually 
annotated for noun phrase coreference; any phrase involved in a 
coreference relation appears underlined in the running text. 
The RIPTIDES system for the most part employs a traditional IE 
architecture [4].  In addition, we use an in-house implementation 
of the TIPSTER architecture [8] to manage all linguistic 
annotations.  A preprocessor first finds sentences and tokens.  For 
syntactic analysis, we currently use the Charniak [5] parser, which 
creates Penn Treebank-style parses [9] rather than the partial 
parses used in most IE systems.  Output from the parser is 
converted automatically into TIPSTER parse and part-of-speech 
annotations, which are added to the set of linguistic annotations 
for the document.  The extraction phase of the system identifies 
domain-specific relations among relevant entities in the text.  It 
relies on Autoslog-XML, an XSLT implementation of the 
Autoslog-TS system [12], to acquire extraction patterns.  
Autoslog-XML is a weakly supervised learning system that 
requires two sets of texts for training ? one set comprises texts 
relevant to the domain of interest and the other, texts not relevant 
 
 
 
to the domain.  Based on these and a small set of extraction 
pattern templates, the system finds a ranked list of possible 
extraction patterns, which a user then annotates with the 
appropriate extraction label (e.g. victim). Once acquired, the 
patterns are applied to new documents to extract slot fillers for the 
domain.  Selectional restrictions on allowable slot fillers are 
implemented using WordNet [6] and BBN?s Identifinder [3] 
named entity component.  In the current version of the system, no 
coreference resolution is attempted; instead, we rely on a very 
simple set of heuristics to guide the creation of output templates.  
The disaster scenario templates extracted for each text are 
provided as input to the summarization component along with all 
linguistic annotations accrued in the IE phase.  No relief slots are 
included in the output at present, since there was insufficient 
annotated data to train a reliable sentence categorizer. 
2.2 The Summarizer 
In order to include relief and other potentially relevant 
information not currently found in the scenario templates, the 
Summarizer extracts selected sentences from the input articles and 
adds them to the summaries generated from the scenario 
templates.  The extracted sentences are listed under the heading 
Selected News Excerpts, as shown in the two sample summaries 
appearing in Figures 3 and 4, and discussed further in Section 
2.2.5 below. 
2.2.1 Summarization Stages 
The Summarizer produces each summary in three main stages.  In 
the first stage, the output templates are merged into an event-
oriented structure, while keeping track of source information.  The 
merge operation currently relies on simple heuristics to group 
extracted facts that are comparable; for example, during this phase 
damage reports are grouped according to whether they pertain to 
the event as a whole, or instead to damage in the same particular 
location.  Heuristics are also used in this stage to determine the 
most relevant damage reports, taking into account specificity, 
recency and news source.  Towards the same objective but using a 
more surface-oriented means, simple word-overlap clustering is 
used to group sentences from different documents into clusters 
that are likely to report similar content.  In the second stage, a 
base importance score is first assigned to each slot/sentence based 
on a combination of document position, document recency and 
group/cluster membership.  The base importance scores are then 
adjusted according to user-specified preferences and matching 
scenario
 templates
A powerful earthquake struck Afghanistan on May 
30 at 11:25? 
Damage 
VOA (06/02/1998) estimated that 5,000 were killed 
by the earthquake, whereas AP (APW, 06/02/1998) 
instead reported ? 
Relief Status 
CNN (06/02/1998): Food, water, medicine 
and other supplies have started to arrive.  
[?] 
NLG of
summary
content
selection
multi-document
template
merging
text collection
IE
System
user information
need
event-oriented
structure
event-oriented
structure with slot
importance scores
summary
Summarizer
slot   filler
slot   filler
slot   filler
slot   filler
...   
slot   filler
slot   filler
slot   filler
slot   filler
...   
slot   filler
slot   filler
slot   filler
slot   filler
...   
slot   filler
slot   filler
slot   filler
slot   filler
...   
Figure 1.  RIPTIDES System Design 
criteria.  The adjusted scores are used to select the most important 
slots/sentences to include in the summary, subject to the user-
specified word limit.  In the third and final stage, the summary is 
generated from the resulting content pool using a combination of 
top-down, schema-like text building rules and surface-oriented 
revisions.  The extracted sentences are simply listed in document 
order, grouped into blocks of adjacent sentences. 
2.2.2 Specificity of Numeric Estimates 
In order to intelligently merge and summarize scenario templates, 
we found it necessary to explicitly handle numeric estimates of 
varying specificity.  While we did find specific numbers (such as 
3,000) in some damage estimates, we also found cases with no 
number phrase at all (e.g. entire villages).  In between these 
extremes, we found vague estimates (thousands) and ranges of 
numbers (anywhere from 2,000 to 5,000).  We also found phrases 
that cannot be easily compared (more than half the region?s 
residents). 
To merge related damage information, we first calculate the 
numeric specificity of the estimate as one of the values NONE, 
VAGUE, RANGE, SPECIFIC, or INCOMPARABLE, based on the presence 
of a small set of trigger words and phrases (e.g. several, as many 
as, from ? to).  Next, we identify the most specific current 
estimates by news source, where a later estimate is considered to 
update an earlier estimate if it is at least as specific.  Finally, we 
determine two types of derived information units, namely (1) the 
minimum and maximum estimates across the news sources, and 
(2) any intermediate estimates that are lower than the maximum 
estimate.1   
In the content determination stage, scores are assigned to the 
derived information units based on the maximum score of the 
underlying units.  In the summary generation stage, a handful of 
text planning rules are used to organize the text for these derived 
units, highlighting agreement and disagreement across sources. 
2.2.3 Improving the Coherence of Extracted 
Sentences 
In our initial attempt to include extracted sentences, we simply 
chose the top ranking sentences that would fit within the word 
limit, subject to the constraint that no more than one sentence per 
cluster could be chosen, in order to help avoid redundancy.  We 
found that this approach often yielded summaries with very poor 
coherence, as many of the included sentences were difficult to 
make sense of in isolation.   
To improve the coherence of the extracted sentences, we have 
experimented with trying to boost coherence by favoring 
sentences in the context of the highest-ranking sentences over 
those with lower ranking scores, following the hypothesis that it is 
better to cover fewer topics in more depth than to change topics 
excessively.  In particular, we assign a score to a set of sentences 
by summing the base scores plus increasing coherence boosts for 
adjacent sentences, sentences that precede ones with an initial 
                                                                
1
 Less specific estimates such as ?hundreds? are considered lower 
than more specific numbers such as ?5000? when they are lower 
by more than a factor of 10. 
 Document no.: ABC19980530.1830.0342  
Date/time: 05/30/1998 18:35:42.49  
Disaster Type: earthquake  
?location: Afghanistan  
?date: today  
?magnitude: 6.9  
?magnitude-confidence: high  
?epicenter: a remote part of the country  
?damage:  
               human-effect:  
                   victim: Thousands of people  
                   number: Thousands  
                  outcome: dead  
                  confidence: medium  
                  confidence-marker: feared  
               physical-effect:  
                  object: entire villages  
                  outcome: damaged  
                  confidence: medium  
                  confidence-marker: Details now hard to 
                                                come by / reports say  
PAKISTAN MAY BE PREPARING 
FOR ANOTHER TEST  
Thousands of people are feared dead following... (voice-
over) ...a powerful earthquake that hit Afghanistan today. 
The quake registered 6.9 on the Richter scale, centered in 
a remote part of the country. (on camera) Details now 
hard to come by, but reports say entire villages were 
buried by the quake.  
 
Figure 2.  Example scenario template for the natural disasters domain 
Earthquake strikes quake-devastated villages in 
northern Afghanistan 
A earthquake struck quake-devastated villages in northern 
Afghanistan Saturday. The earthquake had a magnitude of 6.9 
on the Richter scale on the Richter scale. 
Damage 
Estimates of the death toll varied. CNN (06/02/1998) provided 
the highest estimate of 4,000 dead, whereas ABC 
(06/01/1998) gave the lowest estimate of 140 dead. 
In capital: Estimates of the number injured varied. 
Selected News Excerpts 
CNN (06/01/98):  
Thousands are dead and thousands more are still missing. Red 
cross officials say the first priority is the injured. Getting 
medicine to them is difficult due to the remoteness of the 
villages affected by the quake.  
PRI (06/01/98):  
We spoke to the head of the international red cross there, Bob 
McCaro on a satellite phone link. He says it?s difficult to 
know the full extent of the damage because the region is so 
remote. There?s very little infrastructure.  
PRI (06/01/98):  
Bob McCaro is the head of the international red cross in the 
neighboring country of Pakistan. He?s been speaking to us 
from there on the line.  
APW (06/02/98):  
The United Nations, the Red Cross and other agencies have 
three borrowed helicopters to deliver medical aid.  
Figure 4.  200 word summary of actual IE output, with 
emphasis on Red Cross 
pronoun, and sentences that preceded ones with strongly 
connecting discourse markers such as however, nevertheless, etc.  
We have also softened the constraint on multiple sampling from 
the same cluster, making use of a redundancy penalty in such 
cases.  We then perform a randomized local search for a good set 
of sentences according to these scoring criteria.  
2.2.4 Implementation 
The Summarizer is implemented using the Apache 
implementation of XSLT [1] and CoGenTex?s Exemplars 
Framework [13].  The Apache XSLT implementation has 
provided a convenient way to rapidly develop a prototype 
implementation of the first two processing stages using a series of 
XML transformations.  In the first step of the third summary 
generation stage, the text building component of the Exemplars 
Framework constructs a ?rough draft? of the summary text.  In 
this rough draft version, XML markup is used to partially encode 
the rhetorical, referential, semantic and morpho-syntactic structure 
of the text.  In the second generation step, the Exemplars text 
polishing component makes use of this markup to trigger surface-
Earthquake strikes Afghanistan 
A powerful earthquake struck Afghanistan last Saturday at 
11:25. The earthquake was centered in a remote part of the 
country and had a magnitude of 6.9 on the Richter scale. 
Damage 
Estimates of the death toll varied. VOA (06/02/1998) 
provided the highest estimate of 5,000 dead. CNN 
(05/31/1998) and CNN (06/02/1998) supplied lower estimates 
of 3,000 and up to 4,000 dead, whereas APW (06/02/1998) 
gave the lowest estimate of anywhere from 2,000 to 5,000 
dead. People were injured, while thousands more were 
missing. Thousands were homeless. 
Quake-devastated villages were damaged. Estimates of the 
number of villages destroyed varied. CNN (05/31/1998) 
provided the highest estimate of 50 destroyed, whereas VOA 
(06/04/1998) gave the lowest estimate of at least 25 destroyed. 
In Afghanistan, thousands of people were killed. 
Further Details 
Heavy after shocks shook northern afghanistan. More homes 
were destroyed. More villages were damaged. 
Landslides or mud slides hit the area. 
Another massive quake struck the same region three months 
earlier. Some 2,300 victims were injured. 
Selected News Excerpts 
ABC (05/30/98):  
PAKISTAN MAY BE PREPARING FOR ANOTHER TEST 
Thousands of people are feared dead following...  
ABC (06/01/98):  
RESCUE WORKERS CHALLENGED IN AFGHANISTAN 
There has been serious death and devastation overseas. In 
Afghanistan...  
CNN (06/02/98):  
Food, water, medicine and other supplies have started to 
arrive. But a U.N. relief coordinator says it?s a "scenario from 
hell".  
Figure 3.  200 word summary of simulated IE output, with 
emphasis on damage 
oriented revision rules that smooth the text into a more polished 
form.  A distinguishing feature of our text polishing approach is 
the use of a bootstrapping tool to partially automate the 
acquisition of application-specific revision rules from examples. 
2.2.5 Sample Summaries 
Figures 3 and 4 show two sample summaries that were included in 
our evaluation (see Section 3 for details).  The summary in Figure 
3 was generated from simulated output of the IE system, with 
preference given to damage information; the summary in Figure 4 
was generated from the actual output of the current IE system, 
with preference given to information including the words Red 
Cross.   
While the summary in Figure 3 does a reasonable job of reporting 
the various current estimates of the death toll, the estimates of the 
death toll shown in Figure 4 are less accurate, because the IE 
system failed to extract some reports, and the Summarizer failed 
to correctly merge others.  In particular, note that the lowest 
estimate of 140 dead attributed to ABC is actually a report about 
the number of school children killed in a particular town.  Since 
no location was given for this estimate by the IE system, the 
Summarizer?s simple heuristic for localized damaged reports ? 
namely, to consider a damage report to be localized if a location is 
given that is not in the same sentence as the initial disaster 
description ? did not work here.  The summary in Figure 3 also 
suffered from some problems with merging:  the inclusion of a 
paragraph about thousands killed in Afghanistan is due to an 
incorrect classification of this report as a localized one (owing to 
an error in sentence boundary detection), and the discussion of the 
number of villages damaged should have included a report of at 
least 80 towns or villages damaged. 
Besides the problems related to slot extraction and merging 
mentioned above, the summaries shown in Figures 3 and 4 suffer 
from relatively poor fluency.  In particular, the summaries could 
benefit from better use of descriptive terms from the original 
articles, as well as better methods of sentence combination and 
rhetorical structuring.  Nevertheless, as will be discussed further 
in Section 4, we suggest that the summaries show the potential for 
our techniques to intelligently combine information from many 
articles on the same natural disaster. 
3. EVALUATION AND INITIAL RESULTS 
To evaluate the initial version of the IE-supported summarization 
system, we used Topic 89 from the TDT2 collection ? 25 texts 
on the 1998 Afghanistan earthquake. Each document was 
annotated manually with the natural disaster scenario templates 
that comprise the desired output of the IE system. In addition, 
treebank-style syntactic structure annotations were added 
automatically using the Charniak parser.  Finally, MUC-style 
noun phrase coreference annotations were supplied manually.  All 
annotations are in XML.  The manual and automatic annotations 
were automatically merged, leading to inaccurate annotation 
extents in some cases.  
Next, the Topic 89 texts were split into a development corpus and 
a test corpus.  The development corpus was used to build the 
summarization system; the evaluation summaries were generated 
from the test corpus.  We report on three different variants of the 
RIPTIDES system here: in the first variant (RIPTIDES-SIM1), an 
earlier version of the Summarizer uses the simulated output of the 
IE system as its input, including the relief annotations; in the 
second variant (RIPTIDES-SIM2), the current version of the 
Summarizer uses the simulated output of the IE system, without 
the relief annotations; and in the third variant (RIPTIDES-IE), the 
Summarizer uses the actual output of the IE system as its input.2   
Summaries generated by the RIPTIDES variants were compared 
to a Baseline system consisting of a simple, sentence-extraction 
multidocument summarizer relying only on document position, 
recency, and word overlap clustering.  (As explained in the 
previous section, we have found that word overlap clustering 
provides a bare bones way to help determine what information is 
repeated in multiple articles, thereby indicating importance to the 
document set as a whole, as well as to help reduce redundancy in 
the resulting summaries.)  In addition, the RIPTIDES and 
Baseline system summaries were compared against the summaries 
of two human authors.  All of the summaries were graded with 
respect to content, organization, and readability on an A-F scale 
by three graduate students, all of whom were unfamiliar with this 
project.  Note that the grades for RIPTIDES-SIM1, the Baseline 
system, and the two human authors were assigned during a first 
evaluation in October, 2000, whereas the grades for RIPTIDES-
SIM2 and RIPTIDES-IE were assigned by the same graders in an 
update to this evaluation in April, 2001. 
Each system and author was asked to generate four summaries of 
different lengths and emphases: (1) a 100-word summary of the 
May 30 and May 31 articles; (2) a 400-word summary of all test 
articles, emphasizing specific, factual information; (3) a 200-word 
summary of all test articles, focusing on the damage caused by the 
quake, and excluding information about relief efforts, and (4) a 
200-word summary of all test articles, focusing on the relief 
efforts, and highlighting the Red Cross?s role in these efforts.  
The results are shown in Tables 1 and 2.  Table 1 provides the 
overall grade for each system or author averaged across all graders 
and summaries, where each assigned grade has first been 
converted to a number (with A=4.0 and F=0.0) and the average 
converted back to a letter grade.  Table 2 shows the mean and 
standard deviations of the overall, content, organization, and 
readability scores for the RIPTIDES and the Baseline systems 
averaged across all graders and summaries.  Where the differences 
vs. the Baseline system are significant according to the t-test, the 
p-values are shown. 
Given the amount of development effort that has gone into the 
system to date, we were not surprised that the RIPTIDES variants 
fared poorly when compared against the manually written 
summaries, with RIPTIDES-SIM2 receiving an average grade of 
C, vs. A- and B+ for the human authors.  Nevertheless, we were 
pleased to find that RIPTIDES-SIM2 scored a full grade ahead of 
the Baseline summarizer, which received a D, and that 
                                                                
2
 Note that since the summarizers for the second and third variants 
did not have access to the relief sentence categorizations, we 
decided to exclude from their input the two articles (one 
training, one test) classified by TDT2 Topic 89 as only 
containing brief mentions of the event of interest, as otherwise 
they would have no means of excluding the largely irrelevant 
material in these documents. 
RIPTIDES-IE managed a slightly higher grade of D+, despite the 
immature state of the IE system.  As Table 2 shows, the 
differences in the overall scores were significant for all three 
RIPTIDES variants, as were the scores for organization and 
readability, though not for content in the cases of RIPTIDES-
SIM1 and RIPTIDES-IE. 
4. RELATED AND ONGOING WORK 
The RIPTIDES system is most similar to the SUMMONS system 
of Radev and McKeown [10], which summarized the results of 
MUC-4 IE systems in the terrorism domain.  As a pioneering 
effort, the SUMMONS system was the first to suggest the 
potential of combining IE with NLG in a summarization system, 
though no evaluation was performed.  In comparison to 
SUMMONS, RIPTIDES appears to be designed to more 
completely summarize larger input document sets, since it focuses 
more on finding the most relevant current information, and since 
it includes extracted sentences to round out the summaries.  
Another important difference is that SUMMONS sidestepped the 
problem of comparing reported numbers of varying specificity 
(e.g. several thousand vs. anywhere from 2000 to 5000 vs. up to 
4000 vs. 5000), whereas we have implemented rules for doing so.  
Finally, we have begun to address some of the difficult issues that 
arise in merging information from multiple documents into a 
coherent event-oriented view, though considerable challenges 
remain to be addressed in this area. 
The sentence extraction part of the RIPTIDES system is similar to 
the domain-independent multidocument summarizers of Goldstein 
et al [7] and Radev et al [11] in the way it clusters sentences 
across documents to help determine which sentences are central to 
the collection, as well as to reduce redundancy amongst sentences 
included in the summary.  It is simpler than these systems insofar 
as it does not make use of comparisons to the centroid of the 
document set.  As pointed out in [2], it is difficult in general for 
multidocument summarizers to produce coherent summaries, 
since it is less straightforward to rely on the order of sentences in 
the underlying documents than in the case of single-document 
summarization.  Having also noted this problem, we have focused 
our efforts in this area on attempting to balance coherence and 
informativeness in selecting sets of sentences to include in the 
summary. 
In ongoing work, we are investigating techniques for improving 
merging accuracy and summary fluency in the context of 
summarizing the more than 150 news articles we have collected 
from the web about each of the recent earthquakes in Central 
America and India (January, 2001).  We also plan to investigate 
using tables and hypertext drill-down as a means to help the user 
verify the accuracy of the summarized information. 
By perusing the web collections mentioned above, we can see that 
trying to manually extricate the latest damage estimates from 150+ 
news articles from multiple sources on the same natural disaster 
would be very tedious.  Although estimates do usually converge, 
they often change rapidly at first, and then are gradually dropped 
from later articles, and thus simply looking at the latest article is 
not satisfactory.  While significant challenges remain, we suggest 
that our initial system development and evaluation shows that our 
approach has the potential to accurately summarize damage 
estimates, as well as identify other key story items using shallower 
techniques, and thereby help alleviate information overload in 
specific domains. 
5. ACKNOWLEDGMENTS 
We thank Daryl McCullough for implementing the coherence 
boosting randomized local search, and we thank Ted Caldwell, 
Daryl McCullough, Corien Bakermans, Elizabeth Conrey, 
Purnima Menon and Betsy Vick for their participation as authors 
and graders.  This work has been partially supported by DARPA 
TIDES contract no. N66001-00-C-8009. 
6. REFERENCES 
[1] The Apache XML Project.  2001.  ?Xalan Java.?  
http://xml.apache.org/. 
Table 1 
Baseline RIPTIDES-SIM1 RIPTIDES-SIM2 RIPTIDES-IE Person 1 Person 2 
D C/C- C D+ A- B+ 
 
 
Table 2 
 Baseline RIPTIDES-SIM1 RIPTIDES-SIM2 RIPTIDES-IE 
Overall 0.96 +/- 0.37 1.86 +/- 0.56 (p=.005) 2.1 +/- 0.59 (p=.005) 1.21 +/- 0.46 (p=.05) 
Content 1.44 +/- 1.0 1.78 +/- 0.68 2.2 +/- 0.65 (p=.005) 1.18 +/- 0.6 
Organization 0.64 +/- 0.46 2.48 +/- 0.56 (p=.005) 2.08 +/- 0.77 (p=.005) 1.08 +/- 0.65 (p=.05) 
Readability 0.75 +/- 0.6 1.58 +/- 0.61 (p=.005) 2.05 +/- 0.65 (p=.005) 1.18 +/- 0.62 (p=.05) 
 
[2] Barzilay, R., Elhadad, N. and McKeown, K.  2001.  
?Sentence Ordering in Multidocument Summarization.?  In 
Proceedings of HLT 2001. 
[3] Bikel, D., Schwartz, R. and Weischedel, R.  1999.  ?An 
Algorithm that Learns What's in a Name.?  Machine 
Learning 34:1-3, 211-231. 
[4] Cardie, C. 1997. ?Empirical Methods in Information 
Extraction.?  AI Magazine 18(4): 65-79. 
[5] Charniak, E.  1999.  ?A maximum-entropy-inspired parser.?  
Brown University Technical Report CS99-12. 
[6] Fellbaum, C.  1998.  WordNet: An Electronic Lexical 
Database.  MIT Press, Cambridge, MA. 
[7] Goldstein, J., Mittal, V., Carbonell, J. and Kantrowitz, M.  
2000.  ?Multi-document summarization by sentence 
extraction.?  In Proceedings of the ANLP/NAACL Workshop 
on Automatic Summarization, Seattle, WA. 
[8] Grishman, R.  1996.  ?TIPSTER Architecture Design 
Document Version 2.2.?  DARPA, available at 
http://www.tipster.org/. 
[9] Marcus, M., Marcinkiewicz, M. and Santorini, B.  1993.  
?Building a Large, Annotated Corpus of English: The Penn 
Treebank.?  Computational Linguistics 19:2, 313-330. 
[10] Radev, D. R. and McKeown, K. R.  1998. ?Generating 
natural language summaries from multiple on-line sources.?  
Computational Linguistics 24(3):469-500. 
[11] Radev, D. R., Jing, H. and Budzikowska, M.  2000.  
?Summarization of multiple documents: clustering, sentence 
extraction, and evaluation.?  In Proceedings of the 
ANLP/NAACL Workshop on Summarization, Seattle, WA. 
[12] Riloff, E.  1996.  ?Automatically Generating Extraction 
Patterns from Untagged Text.?  In Proceedings of the 
Thirteenth National Conference on Artificial Intelligence, 
Portland, OR, 1044-1049.  AAAI Press / MIT Press. 
[13] White, M. and Caldwell, T.  1998.  ?EXEMPLARS: A 
Practical, Extensible Framework for Dynamic Text 
Generation.?  In Proceedings of the Ninth International 
Workshop on Natural Language Generation, Niagara-on-
the-Lake, Canada, 266-275. 
 
 
Weakly Supervised Natural Language Learning Without Redundant Views
Vincent Ng and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
 
yung,cardie  @cs.cornell.edu
Abstract
We investigate single-view algorithms as an al-
ternative to multi-view algorithms for weakly
supervised learning for natural language pro-
cessing tasks without a natural feature split. In
particular, we apply co-training, self-training,
and EM to one such task and find that both self-
training and FS-EM, a new variation of EM that
incorporates feature selection, outperform co-
training and are comparatively less sensitive to
parameter changes.
1 Introduction
Multi-view weakly supervised learning paradigms such
as co-training (Blum and Mitchell, 1998) and co-EM
(Nigam and Ghani, 2000) learn a classification task from
a small set of labeled data and a large pool of unla-
beled data using separate, but redundant, views of the
data (i.e. using disjoint feature subsets to represent the
data). Multi-view learning has been successfully ap-
plied to a number of tasks in natural language processing
(NLP), including text classification (Blum and Mitchell,
1998; Nigam and Ghani, 2000), named entity classifica-
tion (Collins and Singer, 1999), base noun phrase brack-
eting (Pierce and Cardie, 2001), and statistical parsing
(Sarkar, 2001; Steedman et al, 2003).
The theoretical performance guarantees of multi-view
weakly supervised algorithms come with two fairly
strong assumptions on the views. First, each view must
be sufficient to learn the given concept. Second, the views
must be conditionally independent of each other given
the class label. When both conditions are met, Blum and
Mitchell prove that an initial weak learner can be boosted
using unlabeled data.
Unfortunately, finding a set of views that satisfies both
of these conditions is by no means an easy problem. In
addition, recent empirical results by Muslea et al (2002)
and Nigam and Ghani (2000) have shown that multi-view
algorithms are quite sensitive to the two underlying as-
sumptions on the views. Effective view factorization in
multi-view learning paradigms, therefore, remains an im-
portant issue for their successful application. In practice,
views are supplied by users or domain experts, who deter-
mine a natural feature split that is expected to be redun-
dant (i.e. each view is expected to be sufficient to learn
the target concept) and conditionally independent given
the class label.1
We investigate here the application of weakly super-
vised learning algorithms to problems for which no obvi-
ous natural feature split exists and hypothesize that, in
these cases, single-view weakly supervised algorithms
will perform better than their multi-view counterparts.
Motivated, in part, by the results in Mueller et al (2002),
we use the task of noun phrase coreference resolution
for illustration throughout the paper.2 In our experi-
ments, we compare the performance of the Blum and
Mitchell co-training algorithm with that of two com-
monly used single-view algorithms, namely, self-training
and Expectation-Maximization (EM). In comparison to
co-training, self-training achieves substantially superior
performance and is less sensitive to its input parameters.
EM, on the other hand, fails to boost performance, and
we attribute this phenomenon to the presence of redun-
dant features in the underlying generative model. Con-
sequently, we propose a wrapper-based feature selection
method (John et al, 1994) for EM that results in perfor-
mance improvements comparable to that observed with
self-training. Overall, our results suggest that single-view
1Abney (2002) argues that the conditional independence as-
sumption is remarkably strong and is rarely satisfied in real data
sets, showing that a weaker independence assumption suffices.
2Mueller et al (2002) explore a heuristic method for view
factorization for the related problem of anaphora resolution, but
find that co-training shows no performance improvements for
any type of German anaphor except pronouns over a baseline
classifier trained on a small set of labeled data.
                                                               Edmonton, May-June 2003
                                                              Main Papers , pp. 94-101
                                                         Proceedings of HLT-NAACL 2003
weakly supervised learning algorithms are a viable al-
ternative to multi-view algorithms for data sets where a
natural feature split into separate, redundant views is not
available.
The remainder of the paper is organized as follows.
Section 2 presents an overview of the three weakly su-
pervised learning algorithms mentioned previously. In
section 3, we introduce noun phrase coreference resolu-
tion and describe the machine learning framework for the
problem. In section 4, we evaluate the weakly supervised
learning algorithms on the task of coreference resolution.
Section 5 introduces a method for improving the perfor-
mance of weakly supervised EM via feature selection.
We conclude with future work in section 6.
2 Weakly Supervised Algorithms
In this section, we give a high-level description of our im-
plementation of the three weakly supervised algorithms
that we use in our comparison, namely, co-training, self-
training, and EM.
2.1 Co-Training
Co-training (Blum and Mitchell, 1998) is a multi-view
weakly supervised algorithm that trains two classifiers
that can help augment each other?s labeled data using two
separate but redundant views of the data. Each classifier
is trained using one view of the data and predicts the la-
bels for all instances in the data pool, which consists of
a randomly chosen subset of the unlabeled data. Each
then selects its most confident predictions from the pool
and adds the corresponding instances with their predicted
labels to the labeled data while maintaining the class dis-
tribution in the labeled data.
The number of instances to be added to the labeled
data by each classifier at each iteration is limited by a
pre-specified growth size to ensure that only the instances
that have a high probability of being assigned the correct
label are incorporated. The data pool is refilled with in-
stances drawn from the unlabeled data and the process is
repeated for several iterations. During testing, each clas-
sifier makes an independent decision for a test instance
and the decision associated with the higher confidence is
taken to be the final prediction for the instance.
2.2 Self-Training
Self-training is a single-view weakly supervised algo-
rithm that has appeared in various forms in the literature.
The version of the algorithm that we consider here is a
variation of the one presented in Banko and Brill (2001).
Initially, we use bagging (Breiman, 1996) to train a
committee of classifiers using the labeled data. Specifi-
cally, each classifier is trained on a bootstrap sample cre-
ated by randomly sampling instances with replacement
from the labeled data until the size of the bootstrap sam-
ple is equal to that of the labeled data. Then each member
of the committee (or bag) predicts the labels of all unla-
beled data. The algorithm selects an unlabeled instance
for adding to the labeled data if and only if all bags agree
upon its label. This ensures that only the unlabeled in-
stances that have a high probability of being assigned the
correct label will be incorporated into the labeled set. The
above steps are repeated until all unlabeled data is labeled
or a fixed point is reached. Following Breiman (1996),
we perform simple majority voting using the committee
to predict the label of a test instance.
2.3 EM
The use of EM as a single-view weakly supervised clas-
sification algorithm is introduced in Nigam et al (2000).
Like the classic unsupervised EM algorithm (Dempster
et al, 1977), weakly supervised EM assumes a paramet-
ric model of data generation. The labels of the unlabeled
data are treated as missing data. The goal is to find a
model such that the posterior probability of its parame-
ters is locally maximized given both the labeled data and
the unlabeled data.
Initially, the algorithm estimates the model parame-
ters by training a probabilistic classifier on the labeled
instances. Then, in the E-step, all unlabeled data is prob-
abilistically labeled by the classifier. In the M-step, the
parameters of the generative model are re-estimated us-
ing both the initially labeled data and the probabilistically
labeled data to obtain a maximum a posteriori (MAP) hy-
pothesis. The E-step and the M-step are repeated for sev-
eral iterations. The resulting model is then used to make
predictions for the test instances.
3 The Machine Learning Framework for
Coreference Resolution
Noun phrase coreference resolution refers to the problem
of determining which noun phrases (NPs) refer to each
real-world entity mentioned in a document. In this sec-
tion, we give an overview of the coreference resolution
system to which the weakly supervised algorithms de-
scribed in the previous section are applied.
The framework underlying the system is a standard
combination of classification and clustering employed
by supervised learning approaches (e.g. Ng and Cardie
(2002); Soon et al (2001)). Specifically, coreference res-
olution is recast as a classification task, in which a pair
of NPs is classified as co-referring or not based on con-
straints that are learned from an annotated corpus. Train-
ing instances are generated by pairing each NP with each
of its preceding NPs in the document. The classification
associated with a training instance is one of COREFER-
ENT or NOT COREFERENT depending on whether the NPs
Feature Type Feature Description
Lexical PRO STR C if both NPs are pronominal and are the same string; else I.
PN STR C if both NPs are proper names and are the same string; else I.
SOON STR NONPRO C if both NPs are non-pronominal and the string of NP matches that of NP ; else I.
Grammatical PRONOUN 1 Y if NP is a pronoun; else N.
PRONOUN 2 Y if NP is a pronoun; else N.
DEMONSTRATIVE 2 Y if NP starts with a demonstrative such as ?this,? ?that,? ?these,? or ?those;? else N.
BOTH PROPER NOUNS C if both NPs are proper names; NA if exactly one NP is a proper name; else I.
NUMBER C if the NP pair agree in number; I if they disagree; NA if number information for one
or both NPs cannot be determined.
GENDER C if the NP pair agree in gender; I if they disagree; NA if gender information for one or
both NPs cannot be determined.
ANIMACY C if the NPs match in animacy; else I.
APPOSITIVE C if the NPs are in an appositive relationship; else I.
PREDNOM C if the NPs form a predicate nominal construction; else I.
BINDING I if the NPs violate conditions B or C of the Binding Theory; else C.
CONTRAINDICES I if the NPs cannot be co-indexed based on simple heuristics; else C. For instance, two
non-pronominal NPs separated by a preposition cannot be co-indexed.
SPAN I if one NP spans the other; else C.
MAXIMALNP I if both NPs have the same maximal NP projection; else C.
SYNTAX I if the NPs have incompatible values for the BINDING, CONTRAINDICES, SPAN or
MAXIMALNP constraints; else C.
INDEFINITE I if NP is an indefinite and not appositive; else C.
PRONOUN I if NP is a pronoun and NP is not; else C.
EMBEDDED 1 Y if NP is an embedded noun; else N.
TITLE I if one or both of the NPs is a title; else C.
Semantic WNCLASS C if the NPs have the same WordNet semantic class; I if they don?t; NA if the semantic
class information for one or both NPs cannot be determined.
ALIAS C if one NP is an alias of the other; else I.
Positional SENTNUM Distance between the NPs in terms of the number of sentences.
Others PRO RESOLVE C if NP is a pronoun and NP is its antecedent according to a naive pronoun resolution
algorithm; else I.
Table 1: Feature set for the coreference system. The feature set contains relational and non-relational features that are used to
generate an instance representing two NPs, NP and NP , in document 	 , where NP precedes NP . Non-relational features test
some property P of one of the NPs under consideration and take on a value of YES or NO depending on whether P holds. Relational
features test whether some property P holds for the NP pair under consideration and indicate whether the NPs are COMPATIBLE or
INCOMPATIBLE w.r.t. P; a value of NOT APPLICABLE is used when property P does not apply.
co-refer in the text. A separate clustering mechanism then
coordinates the possibly contradictory pairwise classifi-
cations and constructs a partition on the set of NPs.
We perform the experiments in this paper
using our coreference resolution system (see
Ng and Cardie (2002)). For the sake of complete-
ness, we include the descriptions of the 25 features
employed by the system in Table 1. Linguistically,
the features can be divided into five groups: lexical,
grammatical, semantic, positional, and others. However,
we use naive Bayes rather than decision tree induction as
the underlying learning algorithm to train a coreference
classifier, simply because (1) it provides a generative
model assumed by EM and hence facilitates comparison
between different approaches and (2) it is more robust
to the skewed class distributions inherent in coreference
data sets than decision tree learners. When the corefer-
ence system is used within the weakly supervised setting,
a weakly supervised algorithm bootstraps the corefer-
ence classifier from the given labeled and unlabeled data
rather than from a much larger set of labeled instances.
We conclude this section by noting that view factor-
ization is a non-trivial task for coreference resolution.
For many lexical tagging problems such as part-of-speech
tagging, views can be drawn naturally from the left-hand
and right-hand context. For other tasks such as named en-
tity classification, views can be derived from features in-
side and outside the phrase under consideration (Collins
and Singer, 1999). Unfortunately, neither of these op-
tions is possible for coreference resolution. We will ex-
plore several heuristic methods for view factorization in
the next section.
4 Evaluation
In this section, we empirically test our hypothesis that
single-view weakly supervised algorithms can potentially
outperform their multi-view counterparts for problems
without a natural feature split.
4.1 Experimental Setup
To ensure a fair comparison of the weakly supervised
algorithms, the experiments are designed to determine
the best parameter setting of each algorithm (in terms
of its effectiveness to improve performance) for the data
sets we investigate. Specifically, we keep the parame-
ters common to all three weakly supervised algorithms
(i.e. the labeled and unlabeled data) constant and vary the
algorithm-specific parameters, as described below.
Evaluation. We use the MUC-6 (1995) and MUC-7
(1998) coreference data sets for evaluation. The training
set is composed of 30 ?dry run? texts, 1 of which is se-
lected to be the annotated text and the remaining 29 texts
are used as unannotated data. For MUC-6, 3486 training
instances are generated from 84 NPs in the annotated text.
For MUC-7, 3741 training instances are generated from
87 NPs. The unlabeled data is composed of 488173 in-
stances and 478384 instances for the MUC-6 and MUC-7
data sets, respectively. Testing is performed by applying
the bootstrapped coreference classifier and the clustering
algorithm described in section 3 on the 20?30 ?formal
evaluation? texts for each of the MUC-6 and MUC-7 data
sets.
Co-training parameters. The co-training parameters
are set as follows.
Views. We tested three pairs of views. Table 2 re-
produces the 25 features of the coreference system and
shows the views we employ. Specifically, the three view
pairs are generated by the following methods.

 Mueller et al?s heuristic method. Starting from two
empty views, the iterative algorithm selects for each
view the feature whose addition maximizes the per-
formance of the respective view on the labeled data
at each iteration. 3 This method produces the view
pair V1 and V2 in Table 2 for the MUC-6 data set.
A different view pair is produced for MUC-7.

 Random splitting of features into views. Starting
from two empty views, an iterative algorithm that
randomly chooses a feature for each view at each
step is used to split the feature set. The resulting
view pair V3 and V4 is used for both the MUC-6
and MUC-7 data sets.

 Splitting of features according to the feature
type. Specifically, one view comprises the lexico-
syntactic features and the other the remaining ones.
This approach produces the view pair V5 and V6,
which is used for both data sets.
Pool size. We tested pool sizes of 500, 1000, 5000.
Growth size. We tested values of 10, 50, 100, 200, 250.
3Space limitation precludes a detailed description of this
method. See Mueller et al (2002) for details.
Feature V1 V2 V3 V4 V5 V6
PRO STR X X X
PN STR X X X
SOON STR NONPRO X X X
PRONOUN 1 X X X
PRONOUN 2 X X X
DEMONSTRATIVE 2 X X X
BOTH PROPER NOUNS X X X
NUMBER X X X
GENDER X X X
ANIMACY X X X
APPOSITIVE X X X
PREDNOM X X X
BINDING X X X
CONTRAINDICES X X X
SPAN X X X
MAXIMALNP X X X
SYNTAX X X X
INDEFINITE X X X
PRONOUN X X X
EMBEDDED 1 X X X
TITLE X X X
WNCLASS X X X
ALIAS X X X
SENTNUM X X X
PRO RESOLVE X X X
Table 2: Co-training view pairs employed by the corefer-
ence system. Column 1 lists the 25 features shown in Table 1.
Columns 2-7 show three different pairs of views that we have
attempted for co-training coreference classifiers.
Number of co-training iterations. We monitored per-
formance on the test data at every 10 iterations of co-
training and ran the algorithm until performance stabi-
lized.
Self-training parameters. Given the labeled and unla-
beled data, self-training requires only the specification of
the number of bags. We tested all odd number of bags
between 1 and 25.
EM parameters. Given the labeled and unlabeled data,
EM has only one parameter ? the number of iterations.
We ran EM to convergence and kept track of its test set
performance at every iteration.
4.2 Results and Discussion
Results are shown in Table 3, where performance is re-
ported in terms of recall, precision, and F-measure using
the model-theoretic MUC scoring program (Vilain et al,
1995). The baseline coreference system, which is trained
only on the labeled document using naive Bayes, achieves
an F-measure of 55.5 and 43.8 on the MUC-6 and MUC-
7 data sets, respectively.
The results shown in row 2 of Table 3 correspond to
the best F-measure scores achieved by co-training for the
two data sets based on co-training runs that comprise all
of the parameter combinations described in the previous
subsection. The parameter settings with which the best
Experiments MUC-6 MUC-7
Best Parameter Setting R P F Best Parameter Setting R P F
Baseline ? 58.3 52.9 55.5 ? 52.8 37.4 43.8
Co-Training v=V5/V6,g=50,p=5000,i=220 47.5 81.9 60.1 v=V5/V6,g=100,p=500,i=260 40.6 77.6 53.3
Self-Training b=7 54.1 78.6 64.1 b=9 54.6 62.6 58.3
EM i=20 64.8 51.8 57.6 i=2 54.1 40.7 46.4
FS-EM ? 64.2 66.6 65.4 ? 53.3 70.3 60.5
Table 3: Comparative results of co-training, self-training, EM, and FS-EM (to be described in section 5). Recall,
Precision, and F-measure are provided. For co-training, self-training, and EM, the best results (F-measure) achieved by the algo-
rithms and the corresponding parameter settings (with views v, growth size g, pool size p, number of iterations i, and number of
bags b) are shown.
0 100 200 300 400 500 600 700 800 900 1000
30
40
50
60
70
80
90
100
Number of Co?Training Iterations
Sc
or
e
Baseline
Recall
Precision
F?measure
Figure 1: Learning curve for co-training (pool size =
5000, growth size = 50) for the MUC-6 data set.
results are obtained are also shown in the table. To get a
better picture of the behavior of co-training, we present
the learning curve for the co-training run that gives rise
to the best F-measure for the MUC-6 data set in Figure 1.
The horizontal (dotted) line shows the performance of the
baseline system, which achieves an F-measure of 55.5, as
described above. As co-training progresses, F-measure
peaks at iteration 220 and then gradually drops below that
of the baseline after iteration 570.
Although co-training produces substantial improve-
ments over the baseline at its best parameter settings, a
closer examination of our results reveals that they cor-
roborate previous findings: the algorithm is sensitive not
only to the number of iterations, but to other input pa-
rameters such as the pool size and the growth size as well
(Nigam and Ghani, 2000; Pierce and Cardie, 2001). The
lack of a principled method for determining these param-
eters in a weakly supervised setting where labeled data is
scarce remains a serious disadvantage for co-training.
Self-training results are shown in row 3 of Table 3:
self-training performs substantially better than both the
baseline and co-training for both data sets. In contrast
to co-training, however, self-training is relatively insensi-
1 3 5 7 9 11 13 15 17 19 21 23 25
50
55
60
65
70
75
80
Number of Bags
Sc
or
e
Baseline
Recall
Precision
F?measure
Figure 2: Effect of the number of bags on the perfor-
mance of self-training for the MUC-6 data set.
tive to its input parameter. Figure 2 shows the fairly con-
sistent performance of self-training with seven or more
bags for the MUC-6 data set. We observe similar trends
for the MUC-7 data set. These results are consistent with
empirical studies of bagging across a variety of classifi-
cation tasks where seven to 25 bags are deemed sufficient
(Breiman, 1996).
To gain a deeper insight into the behavior of self-
training, we plot the learning curve for self-training using
7 bags in Figure 3, again for the MUC-6 data set. At itera-
tion 0 (i.e. before any unlabeled data is incorporated), the
F-measure score achieved by self-training is higher than
that of the baseline system (58.5 vs. 55.5). The observed
difference is due to voting within the self-training algo-
rithm. Voting has proved to be an effective technique for
improving the accuracy of a classifier when training data
is scarce by reducing the variance of a particular training
corpus (Breiman, 1996). After the first iteration, there
is a rapid increase in F-measure, which is accompanied
by large gains in precision and smaller drops in recall.
These results are consistent with our intuition regarding
self-training: at each iteration the algorithm incorporates
only instances whose label it is most confident about into
0 1 2 3
50
55
60
65
70
75
80
Number of Self?Training Iterations
Sc
or
e
Baseline
Recall
Precision
F?measure
Figure 3: Learning curve for self-training using 7 bags
for the MUC-6 data set.
the labeled data, thereby ensuring that precision will in-
crease. 4
As we can see from Table 3, the recall level achieved
by co-training is much lower than that of self-training.
This is an indication that each co-training view is insuf-
ficient to learn the concept: the feature split limits any
interaction of features in different views that might pro-
duce better recall. Overall, these results provide evidence
that self-training is a better alternative to co-training for
weakly supervised learning for problems such as corefer-
ence resolution where no natural feature split exists.
On the other hand, EM only gives rise to modest per-
formance gains over the baseline system, as we can see
from row 4 of Table 3. The performance of EM depends
in part on the correctness of the underlying generative
model (Nigam et al, 2000), which in our case is naive
Bayes. In this model, an instance with  feature values

,  ,

and class  is created by first choosing
the class with prior probability ffImproving Machine Learning Approaches to Coreference Resolution
Vincent Ng and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
 
yung,cardie  @cs.cornell.edu
Abstract
We present a noun phrase coreference sys-
tem that extends the work of Soon et
al. (2001) and, to our knowledge, pro-
duces the best results to date on the MUC-
6 and MUC-7 coreference resolution data
sets ? F-measures of 70.4 and 63.4, re-
spectively. Improvements arise from two
sources: extra-linguistic changes to the
learning framework and a large-scale ex-
pansion of the feature set to include more
sophisticated linguistic knowledge.
1 Introduction
Noun phrase coreference resolution refers to the
problem of determining which noun phrases (NPs)
refer to each real-world entity mentioned in a doc-
ument. Machine learning approaches to this prob-
lem have been reasonably successful, operating pri-
marily by recasting the problem as a classification
task (e.g. Aone and Bennett (1995), McCarthy and
Lehnert (1995)). Specifically, a pair of NPs is clas-
sified as co-referring or not based on constraints that
are learned from an annotated corpus. A separate
clustering mechanism then coordinates the possibly
contradictory pairwise classifications and constructs
a partition on the set of NPs. Soon et al (2001),
for example, apply an NP coreference system based
on decision tree induction to two standard coref-
erence resolution data sets (MUC-6, 1995; MUC-
7, 1998), achieving performance comparable to the
best-performing knowledge-based coreference en-
gines. Perhaps surprisingly, this was accomplished
in a decidedly knowledge-lean manner ? the learn-
ing algorithm has access to just 12 surface-level fea-
tures.
This paper presents an NP coreference system that
investigates two types of extensions to the Soon et
al. corpus-based approach. First, we propose and
evaluate three extra-linguistic modifications to the
machine learning framework, which together pro-
vide substantial and statistically significant gains
in coreference resolution precision. Second, in an
attempt to understand whether incorporating addi-
tional knowledge can improve the performance of
a corpus-based coreference resolution system, we
expand the Soon et al feature set from 12 features
to an arguably deeper set of 53. We propose addi-
tional lexical, semantic, and knowledge-based fea-
tures; most notably, however, we propose 26 addi-
tional grammatical features that include a variety of
linguistic constraints and preferences. Although the
use of similar knowledge sources has been explored
in the context of both pronoun resolution (e.g. Lap-
pin and Leass (1994)) and NP coreference resolution
(e.g. Grishman (1995), Lin (1995)), most previous
work treats linguistic constraints as broadly and un-
conditionally applicable hard constraints. Because
sources of linguistic information in a learning-based
system are represented as features, we can, in con-
trast, incorporate them selectively rather than as uni-
versal hard constraints.
Our results using an expanded feature set are
mixed. First, we find that performance drops signifi-
cantly when using the full feature set, even though
the learning algorithms investigated have built-in
feature selection mechanisms. We demonstrate em-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 104-111.
                         Proceedings of the 40th Annual Meeting of the Association for
pirically that the degradation in performance can be
attributed, at least in part, to poor performance on
common noun resolution. A manually selected sub-
set of 22?26 features, however, is shown to pro-
vide significant gains in performance when chosen
specifically to improve precision on common noun
resolution. Overall, the learning framework and lin-
guistic knowledge source modifications boost per-
formance of Soon?s learning-based coreference res-
olution approach from an F-measure of 62.6 to 70.4,
and from 60.4 to 63.4 for the MUC-6 and MUC-7
data sets, respectively. To our knowledge, these are
the best results reported to date on these data sets for
the full NP coreference problem.1
The rest of the paper is organized as follows. In
sections 2 and 3, we present the baseline corefer-
ence system and explore extra-linguistic modifica-
tions to the machine learning framework. Section 4
describes and evaluates the expanded feature set. We
conclude with related and future work in Section 5.
2 The Baseline Coreference System
Our baseline coreference system attempts to dupli-
cate both the approach and the knowledge sources
employed in Soon et al (2001). More specifically, it
employs the standard combination of classification
and clustering described above.
Building an NP coreference classifier. We use
the C4.5 decision tree induction system (Quinlan,
1993) to train a classifier that, given a description
of two NPs in a document, NP and NP , decides
whether or not they are coreferent. Each training
instance represents the two NPs under consideration
and consists of the 12 Soon et al features, which
are described in Table 1. Linguistically, the features
can be divided into four groups: lexical, grammati-
cal, semantic, and positional.2 The classification as-
sociated with a training instance is one of COREF-
ERENT or NOT COREFERENT depending on whether
the NPs co-refer in the associated training text. We
follow the procedure employed in Soon et al to cre-
1Results presented in Harabagiu et al (2001) are higher
than those reported here, but assume that all and only the noun
phrases involved in coreference relationships are provided for
analysis by the coreference resolution system. We presume no
preprocessing of the training and test documents.
2In all of the work presented here, NPs are identified, and
features values computed entirely automatically.
ate the training data: we rely on coreference chains
from the MUC answer keys to create (1) a positive
instance for each anaphoric noun phrase, NP , and its
closest preceding antecedent, NP ; and (2) a negative
instance for NP paired with each of the intervening
NPs, NP , NP	 , 


 , NP . This method of neg-
ative instance selection is further described in Soon
et al (2001); it is designed to operate in conjunction
with their method for creating coreference chains,
which is explained next.
Applying the classifier to create coreference
chains. After training, the decision tree is used by
a clustering algorithm to impose a partitioning on all
NPs in the test texts, creating one cluster for each set
of coreferent NPs. As in Soon et al, texts are pro-
cessed from left to right. Each NP encountered, NP ,
is compared in turn to each preceding NP, NP , from
right to left. For each pair, a test instance is created
as during training and is presented to the corefer-
ence classifier, which returns a number between 0
and 1 that indicates the likelihood that the two NPs
are coreferent.3 NP pairs with class values above 0.5
are considered COREFERENT; otherwise the pair is
considered NOT COREFERENT. The process termi-
nates as soon as an antecedent is found for NP or the
beginning of the text is reached.
2.1 Baseline Experiments
We evaluate the Duplicated Soon Baseline sys-
tem using the standard MUC-6 (1995) and MUC-
7 (1998) coreference corpora, training the corefer-
ence classifier on the 30 ?dry run? texts, and ap-
plying the coreference resolution algorithm on the
20?30 ?formal evaluation? texts. The MUC-6 cor-
pus produces a training set of 26455 instances (5.4%
positive) from 4381 NPs and a test set of 28443
instances (5.2% positive) from 4565 NPs. For the
MUC-7 corpus, we obtain a training set of 35895 in-
stances (4.4% positive) from 5270 NPs and a test set
of 22699 instances (3.9% positive) from 3558 NPs.
Results are shown in Table 2 (Duplicated Soon
Baseline) where performance is reported in terms
of recall, precision, and F-measure using the model-
theoretic MUC scoring program (Vilain et al, 1995).
3We convert the binary class value using the smoothed ratio



, where p is the number of positive instances and t is the
total number of instances contained in the corresponding leaf
node.
Feature Type Feature Description
Lexical SOON STR C if, after discarding determiners, the string denoting NP matches that of
NP ; else I.
Grammatical PRONOUN 1* Y if NP is a pronoun; else N.
PRONOUN 2* Y if NP is a pronoun; else N.
DEFINITE 2 Y if NP starts with the word ?the;? else N.
DEMONSTRATIVE 2 Y if NP starts with a demonstrative such as ?this,? ?that,? ?these,? or
?those;? else N.
NUMBER* C if the NP pair agree in number; I if they disagree; NA if number informa-
tion for one or both NPs cannot be determined.
GENDER* C if the NP pair agree in gender; I if they disagree; NA if gender information
for one or both NPs cannot be determined.
BOTH PROPER NOUNS* C if both NPs are proper names; NA if exactly one NP is a proper name;
else I.
APPOSITIVE* C if the NPs are in an appositive relationship; else I.
Semantic WNCLASS* C if the NPs have the same WordNet semantic class; I if they don?t; NA if
the semantic class information for one or both NPs cannot be determined.
ALIAS* C if one NP is an alias of the other; else I.
Positional SENTNUM* Distance between the NPs in terms of the number of sentences.
Table 1: Feature Set for the Duplicated Soon Baseline system. The feature set contains relational and non-relational
features. Non-relational features test some property P of one of the NPs under consideration and take on a value of YES or NO
depending on whether P holds. Relational features test whether some property P holds for the NP pair under consideration and
indicate whether the NPs are COMPATIBLE or INCOMPATIBLE w.r.t. P; a value of NOT APPLICABLE is used when property P does
not apply. *?d features are in the hand-selected feature set (see Section 4) for at least one classifier/data set combination.
The system achieves an F-measure of 66.3 and
61.2 on the MUC-6 and MUC-7 data sets, respec-
tively. Similar, but slightly worse performance
was obtained using RIPPER (Cohen, 1995), an
information-gain-based rule learning system. Both
sets of results are at least as strong as the original
Soon results (row one of Table 2), indicating indi-
rectly that our Baseline system is a reasonable du-
plication of that system.4 In addition, the trees pro-
duced by Soon and by our Duplicated Soon Baseline
are essentially the same, differing only in two places
where the Baseline system imposes additional con-
ditions on coreference.
The primary reason for improvements over the
original Soon system for the MUC-6 data set ap-
pears to be our higher upper bound on recall (93.8%
vs. 89.9%), due to better identification of NPs. For
MUC-7, our improvement stems from increases in
precision, presumably due to more accurate feature
value computation.
4In all of the experiments described in this paper, default
settings for all C4.5 parameters are used. Similarly, all RIPPER
parameters are set to their default value except that classification
rules are induced for both the positive and negative instances.
3 Modifications to the Machine Learning
Framework
This section studies the effect of three changes to
the general machine learning framework employed
by Soon et al with the goal of improving precision
in the resulting coreference resolution systems.
Best-first clustering. Rather than a right-to-left
search from each anaphoric NP for the first coref-
erent NP, we hypothesized that a right-to-left search
for a highly likely antecedent might offer more pre-
cise, if not generally better coreference chains. As
a result, we modify the coreference clustering algo-
rithm to select as the antecedent of NP the NP with
the highest coreference likelihood value from among
preceding NPs with coreference class values above
0.5.
Training set creation. For the proposed best-first
clustering to be successful, however, a different
method for training instance selection would be
needed: rather than generate a positive training ex-
ample for each anaphoric NP and its closest an-
tecedent, we instead generate a positive training ex-
amples for its most confident antecedent. More
specifically, for a non-pronominal NP, we assume
that the most confident antecedent is the closest non-
C4.5 RIPPER
MUC-6 MUC-7 MUC-6 MUC-7
System Variation R P F R P F R P F R P F
Original Soon et al 58.6 67.3 62.6 56.1 65.5 60.4 - - - - - -
Duplicated Soon Baseline 62.4 70.7 66.3 55.2 68.5 61.2 60.8 68.4 64.3 54.0 69.5 60.8
Learning Framework 62.4 73.5 67.5 56.3 71.5 63.0 60.8 75.3 67.2 55.3 73.8 63.2
String Match 60.4 74.4 66.7 54.3 72.1 62.0 58.5 74.9 65.7 48.9 73.2 58.6
Training Instance Selection 61.9 70.3 65.8 55.2 68.3 61.1 61.3 70.4 65.5 54.2 68.8 60.6
Clustering 62.4 70.8 66.3 56.5 69.6 62.3 60.5 68.4 64.2 55.6 70.7 62.2
All Features 70.3 58.3 63.8 65.5 58.2 61.6 67.0 62.2 64.5 61.9 60.6 61.2
Pronouns only ? 66.3 ? ? 62.1 ? ? 71.3 ? ? 62.0 ?
Proper Nouns only ? 84.2 ? ? 77.7 ? ? 85.5 ? ? 75.9 ?
Common Nouns only ? 40.1 ? ? 45.2 ? ? 43.7 ? ? 48.0 ?
Hand-selected Features 64.1 74.9 69.1 57.4 70.8 63.4 64.2 78.0 70.4 55.7 72.8 63.1
Pronouns only ? 67.4 ? ? 54.4 ? ? 77.0 ? ? 60.8 ?
Proper Nouns only ? 93.3 ? ? 86.6 ? ? 95.2 ? ? 88.7 ?
Common Nouns only ? 63.0 ? ? 64.8 ? ? 62.8 ? ? 63.5 ?
Table 2: Results for the MUC-6 and MUC-7 data sets using C4.5 and RIPPER. Recall, Precision, and F-measure
are provided. Results in boldface indicate the best results obtained for a particular data set and classifier combination.
pronominal preceding antecedent. For pronouns,
we assume that the most confident antecedent is sim-
ply its closest preceding antecedent. Negative exam-
ples are generated as in the Baseline system.5
String match feature. Soon?s string match feature
(SOON STR) tests whether the two NPs under con-
sideration are the same string after removing deter-
miners from each. We hypothesized, however, that
splitting this feature into several primitive features,
depending on the type of NP, might give the learn-
ing algorithm additional flexibility in creating coref-
erence rules. Exact string match is likely to be a
better coreference predictor for proper names than
it is for pronouns, for example. Specifically, we
replace the SOON STR feature with three features
? PRO STR, PN STR, and WORDS STR ? which
restrict the application of string matching to pro-
nouns, proper names, and non-pronominal NPs, re-
spectively. (See the first entries in Table 3.) Al-
though similar feature splits might have been con-
sidered for other features (e.g. GENDER and NUM-
BER), only the string match feature was tested here.
Results and discussion. Results on the learning
framework modifications are shown in Table 2 (third
block of results). When used in combination, the
modifications consistently provide statistically sig-
nificant gains in precision over the Baseline system
5This new method of training set creation slightly alters the
class value distribution in the training data: for the MUC-6 cor-
pus, there are now 27654 training instances of which 5.2% are
positive; for the MUC-7 corpus, there are now 37870 training
instances of which 4.2% are positive.
without any loss in recall.6 As a result, we observe
reasonable increases in F-measure for both classi-
fiers and both data sets. When using RIPPER, for
example, performance increases from 64.3 to 67.2
for the MUC-6 data set and from 60.8 to 63.2 for
MUC-7. Similar, but weaker, effects occur when ap-
plying each of the learning framework modifications
to the Baseline system in isolation. (See the indented
Learning Framework results in Table 2.)
Our results provide direct evidence for the claim
(Mitkov, 1997) that the extra-linguistic strategies
employed to combine the available linguistic knowl-
edge sources play an important role in computa-
tional approaches to coreference resolution. In par-
ticular, our results suggest that additional perfor-
mance gains might be obtained by further investi-
gating the interaction between training instance se-
lection, feature selection, and the coreference clus-
tering algorithm.
4 NP Coreference Using Many Features
This section describes the second major extension
to the Soon approach investigated here: we explore
the effect of including 41 additional, potentially use-
ful knowledge sources for the coreference resolu-
tion classifier (Table 3). The features were not de-
rived empirically from the corpus, but were based on
common-sense knowledge and linguistic intuitions
6Chi-square statistical significance tests are applied to
changes in recall and precision throughout the paper. Unless
otherwise noted, reported differences are at the 0.05 level or
higher. The chi-square test is not applicable to F-measure.
regarding coreference. Specifically, we increase the
number of lexical features to nine to allow more
complex NP string matching operations. In addi-
tion, we include four new semantic features to al-
low finer-grained semantic compatibility tests. We
test for ancestor-descendent relationships in Word-
Net (SUBCLASS), for example, and also measure
the WordNet graph-traversal distance (WNDIST) be-
tween NP and NP . Furthermore, we add a new posi-
tional feature that measures the distance in terms of
the number of paragraphs (PARANUM) between the
two NPs.
The most substantial changes to the feature set,
however, occur for grammatical features: we add 26
new features to allow the acquisition of more sophis-
ticated syntactic coreference resolution rules. Four
features simply determine NP type, e.g. are both
NPs definite, or pronouns, or part of a quoted string?
These features allow other tests to be conditioned on
the types of NPs being compared. Similarly, three
new features determine the grammatical role of one
or both of the NPs. Currently, only tests for clausal
subjects are made. Next, eight features encode tra-
ditional linguistic (hard) constraints on coreference.
For example, coreferent NPs must agree both in gen-
der and number (AGREEMENT); cannot SPAN one
another (e.g. ?government? and ?government offi-
cials?); and cannot violate the BINDING constraints.
Still other grammatical features encode general lin-
guistic preferences either for or against coreference.
For example, an indefinite NP (that is not in appo-
sition to an anaphoric NP) is not likely to be coref-
erent with any NP that precedes it (ARTICLE). The
last subset of grammatical features encodes slightly
more complex, but generally non-linguistic heuris-
tics. For instance, the CONTAINS PN feature ef-
fectively disallows coreference between NPs that
contain distinct proper names but are not them-
selves proper names (e.g. ?IBM executives? and
?Microsoft executives?).
Two final features make use of an in-house
naive pronoun resolution algorithm (PRO RESOLVE)
and a rule-based coreference resolution system
(RULE RESOLVE), each of which relies on the origi-
nal and expanded feature sets described above.
Results and discussion. Results using the ex-
panded feature set are shown in the All Features
block of Table 2. These and all subsequent results
also incorporate the learning framework changes
from Section 3. In comparison, we see statistically
significant increases in recall, but much larger de-
creases in precision. As a result, F-measure drops
precipitously for both learning algorithms and both
data sets. A closer examination of the results indi-
cates very poor precision on common nouns in com-
parison to that of pronouns and proper nouns. (See
the indented All Features results in Table 2.7) In
particular, the classifiers acquire a number of low-
precision rules for common noun resolution, pre-
sumably because the current feature set is insuffi-
cient. For instance, a rule induced by RIPPER clas-
sifies two NPs as coreferent if the first NP is a proper
name, the second NP is a definite NP in the subject
position, and the two NPs have the same seman-
tic class and are at most one sentence apart from
each other. This rule covers 38 examples, but has
18 exceptions. In comparison, the Baseline sys-
tem obtains much better precision on common nouns
(i.e. 53.3 for MUC-6/RIPPER and 61.0 for MUC-
7/RIPPER with lower recall in both cases) where the
primary mechanism employed by the classifiers for
common noun resolution is its high-precision string
matching facility. Our results also suggest that data
fragmentation is likely to have contributed to the
drop in performance (i.e. we increased the number
of features without increasing the size of the training
set). For example, the decision tree induced from the
MUC-6 data set using the Soon feature set (Learn-
ing Framework results) has 16 leaves, each of which
contains 1728 instances on average; the tree induced
from the same data set using all of the 53 features,
on the other hand, has 86 leaves with an average of
322 instances per leaf.
Hand-selected feature sets. As a result, we next
evaluate a version of the system that employs man-
ual feature selection: for each classifier/data set
combination, we discard features used primarily to
induce low-precision rules for common noun res-
olution and re-train the coreference classifier using
the reduced feature set. Here, feature selection does
not depend on a separate development corpus and
7For each of the NP-type-specific runs, we measure overall
coreference performance, but restrict NP to be of the specified
type. As a result, recall and F-measure for these runs are not
particularly informative.
L PRO STR* C if both NPs are pronominal and are the same string; else I.
e PN STR* C if both NPs are proper names and are the same string; else I.
x WORDS STR C if both NPs are non-pronominal and are the same string; else I.
i
c
SOON STR NONPRO* C if both NPs are non-pronominal and the string of NP matches that of NP ; else I.
a
l
WORD OVERLAP C if the intersection between the content words in NP and NP is not empty; else I.
MODIFIER C if the prenominal modifiers of one NP are a subset of the prenominal modifiers of the
other; else I.
PN SUBSTR C if both NPs are proper names and one NP is a proper substring (w.r.t. content words
only) of the other; else I.
WORDS SUBSTR C if both NPs are non-pronominal and one NP is a proper substring (w.r.t. content words
only) of the other; else I.
G NP BOTH DEFINITES C if both NPs start with ?the;? I if neither start with ?the;? else NA.
r
a
type BOTH EMBEDDED C if both NPs are prenominal modifiers ; I if neither are prenominal modifiers; else NA.
m
m
BOTH IN QUOTES C if both NPs are part of a quoted string; I if neither are part of a quoted string; else NA.
a BOTH PRONOUNS* C if both NPs are pronouns; I if neither are pronouns, else NA.
t role BOTH SUBJECTS C if both NPs are grammatical subjects; I if neither are subjects; else NA.
i SUBJECT 1* Y if NP is a subject; else N.
c SUBJECT 2 Y if NP is a subject; else N.
a
l
lin-
gui-
AGREEMENT* C if the NPs agree in both gender and number; I if they disagree in both gender and
number; else NA.
stic ANIMACY* C if the NPs match in animacy; else I.
MAXIMALNP* I if both NPs have the same maximal NP projection; else C.
con- PREDNOM* C if the NPs form a predicate nominal construction; else I.
stra- SPAN* I if one NP spans the other; else C.
ints BINDING* I if the NPs violate conditions B or C of the Binding Theory; else C.
CONTRAINDICES* I if the NPs cannot be co-indexed based on simple heuristics; else C. For instance, two
non-pronominal NPs separated by a preposition cannot be co-indexed.
SYNTAX* I if the NPs have incompatible values for the BINDING, CONTRAINDICES, SPAN or
MAXIMALNP constraints; else C.
ling. INDEFINITE* I if NP is an indefinite and not appositive; else C.
prefs PRONOUN I if NP is a pronoun and NP is not; else C.
heur-
istics
CONSTRAINTS* C if the NPs agree in GENDER and NUMBER and do not have incompatible values for
CONTRAINDICES, SPAN, ANIMACY, PRONOUN, and CONTAINS PN; I if the NPs have
incompatible values for any of the above features; else NA.
CONTAINS PN I if both NPs are not proper names but contain proper names that mismatch on every
word; else C.
DEFINITE 1 Y if NP starts with ?the;? else N.
EMBEDDED 1* Y if NP is an embedded noun; else N.
EMBEDDED 2 Y if NP is an embedded noun; else N.
IN QUOTE 1 Y if NP is part of a quoted string; else N.
IN QUOTE 2 Y if NP is part of a quoted string; else N.
PROPER NOUN I if both NPs are proper names, but mismatch on every word; else C.
TITLE* I if one or both of the NPs is a title; else C.
S
e
CLOSEST COMP C if NP is the closest NP preceding NP that has the same semantic class as NP and the
two NPs do not violate any of the linguistic constraints; else I.
m
a
SUBCLASS C if the NPs have different head nouns but have an ancestor-descendent relationship in
WordNet; else I.
n
t
i
WNDIST Distance between NP and NP in WordNet (using the first sense only) when they have
an ancestor-descendent relationship but have different heads; else infinity.
c WNSENSE Sense number in WordNet for which there exists an ancestor-descendent relationship
between the two NPs when they have different heads; else infinity.
P
os
PARANUM Distance between the NPs in terms of the number of paragraphs.
O
t
PRO RESOLVE* C if NP is a pronoun and NP is its antecedent according to a naive pronoun resolution
algorithm; else I.
h
er
RULE RESOLVE C if the NPs are coreferent according to a rule-based coreference resolution algorithm;
else I.
Table 3: Additional features for NP coreference. As before, *?d features are in the hand-selected feature set for at least
one classifier/data set combination.
is guided solely by inspection of the features associ-
ated with low-precision rules induced from the train-
ing data. In current work, we are automating this
feature selection process, which currently employs
a fair amount of user discretion, e.g. to determine a
precision cut-off. Features in the hand-selected set
for at least one of the tested system variations are
*?d in Tables 1 and 3.
In general, we hypothesized that the hand-
selected features would reclaim precision, hopefully
without losing recall. For the most part, the ex-
perimental results support this hypothesis. (See the
Hand-selected Features block in Table 2.) In com-
parison to the All Features version, we see statisti-
cally significant gains in precision and statistically
significant, but much smaller, drops in recall, pro-
ducing systems with better F-measure scores. In
addition, precision on common nouns rises substan-
tially, as expected. Unfortunately, the hand-selected
features precipitate a large drop in precision for pro-
noun resolution for the MUC-7/C4.5 data set. Ad-
ditional analysis is required to determine the reason
for this.
Moreover, the Hand-selected Features produce
the highest scores posted to date for both the MUC-
6 and MUC-7 data sets: F-measure increases w.r.t.
the Baseline system from 64.3 to 70.4 for MUC-
6/RIPPER, and from 61.2 to 63.4 for MUC-7/C4.5.
In one variation (MUC-7/RIPPER), however, the
Hand-selected Features slightly underperforms the
Learning Framework modifications (F-measure of
63.1 vs. 63.2) although changes in recall and pre-
cision are not statistically significant. Overall, our
results indicate that pronoun and especially com-
mon noun resolution remain important challenges
for coreference resolution systems. Somewhat dis-
appointingly, only four of the new grammatical
features corresponding to linguistic constraints and
preferences are selected by the symbolic learning
algorithms investigated: AGREEMENT, ANIMACY,
BINDING, and MAXIMALNP.
Discussion. In an attempt to gain additional in-
sight into the difference in performance between our
system and the original Soon system, we compare
the decision tree induced by each for the MUC-6
ALIAS = C: + (347.0/23.8)
ALIAS = I:
|  SOON_STR_NONPRO = C:
|  |  ANIMACY = NA: - (4.0/2.2)
|  |  ANIMACY = I: + (0.0)
|  |  ANIMACY = C: + (259.0/45.8)
|  SOON_STR_NONPRO = I:
|  |  PRO_STR = C: + (39.0/2.6)
|  |  PRO_STR = I:
|  |  |  PRO_RESOLVE = C:
|  |  |  |  EMBEDDED_1 = Y: - (7.0/3.4)
|  |  |  |  EMBEDDED_1 = N:
|  |  |  |  |  PRONOUN_1 = Y:
|  |  |  |  |  |  ANIMACY = NA: - (6.0/2.3)
|  |  |  |  |  |  ANIMACY = I: - (1.0/0.8)
|  |  |  |  |  |  ANIMACY = C: + (10.0/3.5)
|  |  |  |  |  PRONOUN_1 = N:
|  |  |  |  |  |  MAXIMALNP = C: + (108.0/18.2)
|  |  |  |  |  |  MAXIMALNP = I:
|  |  |  |  |  |  |  WNCLASS = NA: - (5.0/1.2)
|  |  |  |  |  |  |  WNCLASS = I: + (0.0)
|  |  |  |  |  |  |  WNCLASS = C: + (12.0/3.6)
|  |  |  PRO_RESOLVE = I:
|  |  |  |  APPOSITIVE = I: - (26806.0/713.8)
|  |  |  |  APPOSITIVE = C:
|  |  |  |  |  GENDER = NA: + (28.0/2.6)
|  |  |  |  |  GENDER = I: + (5.0/3.2)
|  |  |  |  |  GENDER = C: - (17.0/3.7)
Figure 1: Decision Tree using the Hand-selected
feature set on the MUC-6 data set.
data set.8 For our system, we use the tree induced on
the hand-selected features (Figure 1). The two trees
are fairly different. In particular, our tree makes
use of many of the features that are not present in
the original Soon feature set. The root feature for
Soon, for example, is the general string match fea-
ture (SOON STR); splitting the SOON STR feature
into three primitive features promotes the ALIAS fea-
ture to the root of our tree, on the other hand. In
addition, given two non-pronominal, matching NPs
(SOON STR NONPRO=C), our tree requires an addi-
tional test on ANIMACY before considering the two
NPs coreferent; the Soon tree instead determines
two NPs to be coreferent as long as they are the same
string. Pronoun resolution is also performed quite
differently by the two trees, although both consider
two pronouns coreferent when their strings match.
Finally, intersentential and intrasentential pronomi-
nal references are possible in our system while inter-
sentential pronominal references are largely prohib-
ited by the Soon system.
5 Conclusions
We investigate two methods to improve existing
machine learning approaches to the problem of
8Soon et al (2001) present only the tree learned for the
MUC-6 data set.
noun phrase coreference resolution. First, we pro-
pose three extra-linguistic modifications to the ma-
chine learning framework, which together consis-
tently produce statistically significant gains in pre-
cision and corresponding increases in F-measure.
Our results indicate that coreference resolution sys-
tems can improve by effectively exploiting the in-
teraction between the classification algorithm, train-
ing instance selection, and the clustering algorithm.
We plan to continue investigations along these lines,
developing, for example, a true best-first clustering
coreference framework and exploring a ?supervised
clustering? approach to the problem. In addition,
we provide the learning algorithms with many addi-
tional linguistic knowledge sources for coreference
resolution. Unfortunately, we find that performance
drops significantly when using the full feature set;
we attribute this, at least in part, to the system?s poor
performance on common noun resolution and to data
fragmentation problems that arise with the larger
feature set. Manual feature selection, with an eye
toward eliminating low-precision rules for common
noun resolution, is shown to reliably improve per-
formance over the full feature set and produces the
best results to date on the MUC-6 and MUC-7 coref-
erence data sets ? F-measures of 70.4 and 63.4, re-
spectively. Nevertheless, there is substantial room
for improvement. As noted above, for example, it is
important to automate the precision-oriented feature
selection procedure as well as to investigate other
methods for feature selection. We also plan to in-
vestigate previous work on common noun phrase
interpretation (e.g. Sidner (1979), Harabagiu et al
(2001)) as a means of improving common noun
phrase resolution, which remains a challenge for
state-of-the-art coreference resolution systems.
Acknowledgments
Thanks to three anonymous reviewers for their comments and,
in particular, for suggesting that we investigate data fragmen-
tation issues. This work was supported in part by DARPA
TIDES contract N66001-00-C-8009, and NSF Grants 0081334
and 0074896.
References
C. Aone and S. W. Bennett. 1995. Evaluating Auto-
mated and Manual Acquisition of Anaphora Resolu-
tion Strategies. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 122?129.
W. Cohen. 1995. Fast Effective Rule Induction. In Pro-
ceedings of the Twelfth International Conference on
Machine Learning.
R. Grishman. 1995. The NYU System for MUC-6 or
Where?s the Syntax? In Proceedings of the Sixth Mes-
sage Understanding Conference (MUC-6).
S. Harabagiu, R. Bunescu, and S. Maiorano. 2001. Text
and Knowledge Mining for Coreference Resolution.
In Proceedings of the Second Meeting of the North
America Chapter of the Association for Computational
Linguistics (NAACL-2001), pages 55?62.
S. Lappin and H. Leass. 1994. An Algorithm for
Pronominal Anaphora Resolution. Computational
Linguistics, 20(4):535?562.
D. Lin. 1995. University of Manitoba: Description of the
PIE System as Used for MUC-6. In Proceedings of the
Sixth Message Understanding Conference (MUC-6).
J. McCarthy and W. Lehnert. 1995. Using Decision
Trees for Coreference Resolution. In Proceedings of
the Fourteenth International Conference on Artificial
Intelligence, pages 1050?1055.
R. Mitkov. 1997. Factors in anaphora resolution: they
are not the only things that matter. A case study based
on two different approaches. In Proceedings of the
ACL?97/EACL?97 Workshop on Operational Factors
in Practical, Robust Anaphora Resolution.
MUC-6. 1995. Proceedings of the Sixth Message Under-
standing Conference (MUC-6). Morgan Kaufmann,
San Francisco, CA.
MUC-7. 1998. Proceedings of the Seventh Message
Understanding Conference (MUC-7). Morgan Kauf-
mann, San Francisco, CA.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
C. Sidner. 1979. Towards a Computational Theory
of Definite Anaphora Comprehension in English Dis-
course. PhD Thesis, Massachusetts Institute of Tech-
nology.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A
Machine Learning Approach to Coreference Resolu-
tion of Noun Phrases. Computational Linguistics,
27(4):521?544.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth Mes-
sage Understanding Conference (MUC-6), pages 45?
52, San Francisco, CA. Morgan Kaufmann.
Learning Noun Phrase Anaphoricity to Improve Coreference Resolution:
Issues in Representation and Optimization
Vincent Ng
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
yung@cs.cornell.edu
Abstract
Knowledge of the anaphoricity of a noun phrase
might be profitably exploited by a coreference sys-
tem to bypass the resolution of non-anaphoric noun
phrases. Perhaps surprisingly, recent attempts to
incorporate automatically acquired anaphoricity in-
formation into coreference systems, however, have
led to the degradation in resolution performance.
This paper examines several key issues in com-
puting and using anaphoricity information to im-
prove learning-based coreference systems. In par-
ticular, we present a new corpus-based approach to
anaphoricity determination. Experiments on three
standard coreference data sets demonstrate the ef-
fectiveness of our approach.
1 Introduction
Noun phrase coreference resolution, the task of de-
termining which noun phrases (NPs) in a text refer
to the same real-world entity, has long been con-
sidered an important and difficult problem in nat-
ural language processing. Identifying the linguis-
tic constraints on when two NPs can co-refer re-
mains an active area of research in the commu-
nity. One significant constraint on coreference, the
non-anaphoricity constraint, specifies that a non-
anaphoric NP cannot be coreferent with any of its
preceding NPs in a given text.
Given the potential usefulness of knowledge
of (non-)anaphoricity for coreference resolution,
anaphoricity determination has been studied fairly
extensively. One common approach involves the
design of heuristic rules to identify specific types
of (non-)anaphoric NPs such as pleonastic pro-
nouns (e.g., Paice and Husk (1987), Lappin and Le-
ass (1994), Kennedy and Boguraev (1996), Den-
ber (1998)) and definite descriptions (e.g., Vieira
and Poesio (2000)). More recently, the problem
has been tackled using unsupervised (e.g., Bean and
Riloff (1999)) and supervised (e.g., Evans (2001),
Ng and Cardie (2002a)) approaches.
Interestingly, existing machine learning ap-
proaches to coreference resolution have performed
reasonably well without anaphoricity determination
(e.g., Soon et al (2001), Ng and Cardie (2002b),
Strube and Mu?ller (2003), Yang et al (2003)). Nev-
ertheless, there is empirical evidence that resolution
systems might further be improved with anaphoric-
ity information. For instance, our coreference sys-
tem mistakenly identifies an antecedent for many
non-anaphoric common nouns in the absence of
anaphoricity information (Ng and Cardie, 2002a).
Our goal in this paper is to improve learning-
based coreference systems using automatically
computed anaphoricity information. In particular,
we examine two important, yet largely unexplored,
issues in anaphoricity determination for coreference
resolution: representation and optimization.
Constraint-based vs. feature-based representa-
tion. How should the computed anaphoricity
information be used by a coreference system?
From a linguistic perspective, knowledge of non-
anaphoricity is most naturally represented as ?by-
passing? constraints, with which the coreference
system bypasses the resolution of NPs that are deter-
mined to be non-anaphoric. But for learning-based
coreference systems, anaphoricity information can
be simply and naturally accommodated into the ma-
chine learning framework by including it as a fea-
ture in the instance representation.
Local vs. global optimization. Should the
anaphoricity determination procedure be developed
independently of the coreference system that uses
the computed anaphoricity information (local opti-
mization), or should it be optimized with respect
to coreference performance (global optimization)?
The principle of software modularity calls for local
optimization. However, if the primary goal is to im-
prove coreference performance, global optimization
appears to be the preferred choice.
Existing work on anaphoricity determination
for anaphora/coreference resolution can be char-
acterized along these two dimensions. Inter-
estingly, most existing work employs constraint-
based, locally-optimized methods (e.g., Mitkov et
al. (2002) and Ng and Cardie (2002a)), leaving
the remaining three possibilities largely unexplored.
In particular, to our knowledge, there have been
no attempts to (1) globally optimize an anaphoric-
ity determination procedure for coreference perfor-
mance and (2) incorporate anaphoricity into corefer-
ence systems as a feature. Consequently, as part of
our investigation, we propose a new corpus-based
method for achieving global optimization and ex-
periment with representing anaphoricity as a feature
in the coreference system.
In particular, we systematically evaluate all four
combinations of local vs. global optimization and
constraint-based vs. feature-based representation of
anaphoricity information in terms of their effec-
tiveness in improving a learning-based coreference
system. Results on three standard coreference
data sets are somewhat surprising: our proposed
globally-optimized method, when used in conjunc-
tion with the constraint-based representation, out-
performs not only the commonly-adopted locally-
optimized approach but also its seemingly more nat-
ural feature-based counterparts.
The rest of the paper is structured as follows.
Section 2 focuses on optimization issues, dis-
cussing locally- and globally-optimized approaches
to anaphoricity determination. In Section 3, we
give an overview of the standard machine learning
framework for coreference resolution. Sections 4
and 5 present the experimental setup and evaluation
results, respectively. We examine the features that
are important to anaphoricity determination in Sec-
tion 6 and conclude in Section 7.
2 The Anaphoricity Determination
System: Local vs. Global Optimization
In this section, we will show how to build a model
of anaphoricity determination. We will first present
the standard, locally-optimized approach and then
introduce our globally-optimized approach.
2.1 The Locally-Optimized Approach
In this approach, the anaphoricity model is sim-
ply a classifier that is trained and optimized inde-
pendently of the coreference system (e.g., Evans
(2001), Ng and Cardie (2002a)).
Building a classifier for anaphoricity determina-
tion. A learning algorithm is used to train a classi-
fier that, given a description of an NP in a document,
decides whether or not the NP is anaphoric. Each
training instance represents a single NP and consists
of a set of features that are potentially useful for dis-
tinguishing anaphoric and non-anaphoric NPs. The
classification associated with a training instance ?
one of ANAPHORIC or NOT ANAPHORIC ? is de-
rived from coreference chains in the training doc-
uments. Specifically, a positive instance is created
for each NP that is involved in a coreference chain
but is not the head of the chain. A negative instance
is created for each of the remaining NPs.
Applying the classifier. To determine the
anaphoricity of an NP in a test document, an
instance is created for it as during training and pre-
sented to the anaphoricity classifier, which returns
a value of ANAPHORIC or NOT ANAPHORIC.
2.2 The Globally-Optimized Approach
To achieve global optimization, we construct a para-
metric anaphoricity model with which we optimize
the parameter1 for coreference accuracy on held-
out development data. In other words, we tighten
the connection between anaphoricity determination
and coreference resolution by using the parameter
to generate a set of anaphoricity models from which
we select the one that yields the best coreference
performance on held-out data.
Global optimization for a constraint-based rep-
resentation. We view anaphoricity determination
as a problem of determining how conservative an
anaphoricity model should be in classifying an NP
as (non-)anaphoric. Given a constraint-based repre-
sentation of anaphoricity information for the coref-
erence system, if the model is too liberal in classi-
fying an NP as non-anaphoric, then many anaphoric
NPs will be misclassified, ultimately leading to a de-
terioration of recall and of the overall performance
of the coreference system. On the other hand, if the
model is too conservative, then only a small fraction
of the truly non-anaphoric NPs will be identified,
and so the resulting anaphoricity information may
not be effective in improving the coreference sys-
tem. The challenge then is to determine a ?good?
degree of conservativeness. As a result, we can de-
sign a parametric anaphoricity model whose con-
servativeness can be adjusted via a conservativeness
parameter. To achieve global optimization, we can
simply tune this parameter to optimize for corefer-
ence performance on held-out development data.
Now, to implement this conservativeness-based
anaphoricity determination model, we propose two
methods, each of which is built upon a different def-
inition of conservativeness.
Method 1: Varying the Cost Ratio
Our first method exploits a parameter present in
many off-the-shelf machine learning algorithms for
1We can introduce multiple parameters for this purpose,
but to simply the optimization process, we will only consider
single-parameter models in this paper.
training a classifier ? the cost ratio (cr), which is
defined as follows.
cr := cost of misclassifying a positive instancecost of misclassifying a negative instance
Inspection of this definition shows that cr provides
a means of adjusting the relative misclassification
penalties placed on training instances of different
classes. In particular, the larger cr is, the more con-
servative the classifier is in classifying an instance
as negative (i.e., non-anaphoric). Given this obser-
vation, we can naturally define the conservativeness
of an anaphoricity classifier as follows. We say that
classifier A is more conservative than classifier B in
determining an NP as non-anaphoric if A is trained
with a higher cost ratio than B.
Based on this definition of conservativeness, we
can construct an anaphoricity model parameterized
by cr. Specifically, the parametric model maps
a given value of cr to the anaphoricity classifier
trained with this cost ratio. (For the purpose of train-
ing anaphoricity classifiers with different values of
cr, we use RIPPER (Cohen, 1995), a propositional
rule learning algorithm.) It should be easy to see
that increasing cr makes the model more conserva-
tive in classifying an NP as non-anaphoric. With
this parametric model, we can tune cr to optimize
for coreference performance on held-out data.
Method 2: Varying the Classification Threshold
We can also define conservativeness in terms of the
number of NPs classified as non-anaphoric for a
given set of NPs. Specifically, given two anaphoric-
ity models A and B and a set of instances I to be
classified, we say that A is more conservative than
B in determining an NP as non-anaphoric if A clas-
sifies fewer instances in I as non-anaphoric than B.
Again, this definition is consistent with our intuition
regarding conservativeness.
We can now design a parametric anaphoricity
model based on this definition. First, we train
in a supervised fashion a probablistic model of
anaphoricity PA(c | i), where i is an instance rep-
resenting an NP and c is one of the two possible
anaphoricity values. (In our experiments, we use
maximum entropy classification (MaxEnt) (Berger
et al, 1996) to train this probability model.) Then,
we can construct a parametric model making bi-
nary anaphoricity decisions from PA by introduc-
ing a threshold parameter t as follows. Given a
specific t (0 ? t ? 1) and a new instance i, we
define an anaphoricity model M tA in which M tA(i)
= NOT ANAPHORIC if and only if PA(c = NOT
ANAPHORIC | i) ? t. It should be easy to see that
increasing t yields progressively more conservative
anaphoricity models. Again, t can be tuned using
held-out development data.
Global optimization for a feature-based repre-
sentation. We can similarly optimize our pro-
posed conservativeness-based anaphoricity model
for coreference performance when anaphoricity in-
formation is represented as a feature for the corefer-
ence system. Unlike in a constraint-based represen-
tation, however, we cannot expect that the recall of
the coreference system would increase with the con-
servativeness parameter. The reason is that we have
no control over whether or how the anaphoricity
feature is used by the coreference learner. In other
words, the behavior of the coreference system is less
predictable in comparison to a constraint-based rep-
resentation. Other than that, the conservativeness-
based anaphoricity model is as good to use for
global optimization with a feature-based represen-
tation as with a constraint-based representation.
We conclude this section by pointing out that the
locally-optimized approach to anaphoricity deter-
mination is indeed a special case of the global one.
Unlike the global approach in which the conserva-
tiveness parameter values are tuned based on la-
beled data, the local approach uses ?default? param-
eter values. For instance, when RIPPER is used to
train an anaphoricity classifier in the local approach,
cr is set to the default value of one. Similarly, when
probabilistic anaphoricity decisions generated via a
MaxEnt model are converted to binary anaphoricity
decisions for subsequent use by a coreference sys-
tem, t is set to the default value of 0.5.
3 The Machine Learning Framework for
Coreference Resolution
The coreference system to which our automatically
computed anaphoricity information will be applied
implements the standard machine learning approach
to coreference resolution combining classification
and clustering. Below we will give a brief overview
of this standard approach. Details can be found in
Soon et al (2001) or Ng and Cardie (2002b).
Training an NP coreference classifier. After a
pre-processing step in which the NPs in a document
are automatically identified, a learning algorithm is
used to train a classifier that, given a description of
two NPs in the document, decides whether they are
COREFERENT or NOT COREFERENT.
Applying the classifier to create coreference
chains. Test texts are processed from left to right.
Each NP encountered, NPj , is compared in turn to
each preceding NP, NPi. For each pair, a test in-
stance is created as during training and is presented
to the learned coreference classifier, which returns
a number between 0 and 1 that indicates the likeli-
hood that the two NPs are coreferent. The NP with
the highest coreference likelihood value among the
preceding NPs with coreference class values above
0.5 is selected as the antecedent of NPj ; otherwise,
no antecedent is selected for NPj .
4 Experimental Setup
In Section 2, we examined how to construct locally-
and globally-optimized anaphoricity models. Re-
call that, for each of these two types of models,
the resulting (non-)anaphoricity information can be
used by a learning-based coreference system either
as hard bypassing constraints or as a feature. Hence,
given a coreference system that implements the two-
step learning approach shown above, we will be able
to evaluate the four different combinations of com-
puting and using anaphoricity information for im-
proving the coreference system described in the in-
troduction. Before presenting evaluation details, we
will describe the experimental setup.
Coreference system. In all of our experiments,
we use our learning-based coreference system (Ng
and Cardie, 2002b).
Features for anaphoricity determination. In
both the locally-optimized and the globally-
optimized approaches to anaphoricity determination
described in Section 2, an instance is represented by
37 features that are specifically designed for distin-
guishing anaphoric and non-anaphoric NPs. Space
limitations preclude a description of these features;
see Ng and Cardie (2002a) for details.
Learning algorithms. For training coreference
classifiers and locally-optimized anaphoricity mod-
els, we use both RIPPER and MaxEnt as the un-
derlying learning algorithms. However, for training
globally-optimized anaphoricity models, RIPPER is
always used in conjunction with Method 1 and Max-
Ent with Method 2, as described in Section 2.2.
In terms of setting learner-specific parameters,
we use default values for all RIPPER parameters
unless otherwise stated. For MaxEnt, we always
train the feature-weight parameters with 100 iter-
ations of the improved iterative scaling algorithm
(Della Pietra et al, 1997), using a Gaussian prior
to prevent overfitting (Chen and Rosenfeld, 2000).
Data sets. We use the Automatic Content Ex-
traction (ACE) Phase II data sets.2 We choose
ACE rather than the more widely-used MUC cor-
pus (MUC-6, 1995; MUC-7, 1998) simply because
2See http://www.itl.nist.gov/iad/894.01/
tests/ace for details on the ACE research program.
BNEWS NPAPER NWIRE
Number of training texts 216 76 130
Number of test texts 51 17 29
Number of training insts
(for anaphoricity)
20567 21970 27338
Number of training insts
(for coreference)
97036 148850 122168
Table 1: Statistics of the three ACE data sets
ACE provides much more labeled data for both
training and testing. However, our system was set
up to perform coreference resolution according to
the MUC rules, which are fairly different from the
ACE guidelines in terms of the identification of
markables as well as evaluation schemes. Since our
goal is to evaluate the effect of anaphoricity infor-
mation on coreference resolution, we make no at-
tempt to modify our system to adhere to the rules
specifically designed for ACE.
The coreference corpus is composed of three data
sets made up of three different news sources: Broad-
cast News (BNEWS), Newspaper (NPAPER), and
Newswire (NWIRE). Statistics collected from these
data sets are shown in Table 1. For each data set,
we train an anaphoricity classifier and a coreference
classifier on the (same) set of training texts and eval-
uate the coreference system on the test texts.
5 Evaluation
In this section, we will compare the effectiveness of
four approaches to anaphoricity determination (see
the introduction) in improving our baseline corefer-
ence system.
5.1 Coreference Without Anaphoricity
As mentioned above, we use our coreference system
as the baseline system where no explicit anaphoric-
ity determination system is employed. Results us-
ing RIPPER and MaxEnt as the underlying learners
are shown in rows 1 and 2 of Table 2 where perfor-
mance is reported in terms of recall, precision, and
F-measure using the model-theoretic MUC scoring
program (Vilain et al, 1995). With RIPPER, the
system achieves an F-measure of 56.3 for BNEWS,
61.8 for NPAPER, and 51.7 for NWIRE. The per-
formance of MaxEnt is comparable to that of RIP-
PER for the BNEWS and NPAPER data sets but
slightly worse for the NWIRE data set.
5.2 Coreference With Anaphoricity
The Constraint-Based, Locally-Optimized
(CBLO) Approach. As mentioned before, in
constraint-based approaches, the automatically
computed non-anaphoricity information is used as
System Variation BNEWS NPAPER NWIRE
Experiments L R P F C R P F C R P F C
1 No RIP 57.4 55.3 56.3 - 60.0 63.6 61.8 - 53.2 50.3 51.7 -
2 Anaphoricity ME 60.9 52.1 56.2 - 65.4 58.6 61.8 - 54.9 46.7 50.4 -
3 Constraint- RIP 42.5 77.2 54.8 cr=1 46.7 79.3 58.8? cr=1 42.1 64.2 50.9 cr=1
4 Based, RIP 45.4 72.8 55.9 t=0.5 52.2 75.9 61.9 t=0.5 36.9 61.5 46.1? t=0.5
5 Locally- ME 44.4 76.9 56.3 cr=1 50.1 75.7 60.3 cr=1 43.9 63.0 51.7 cr=1
6 Optimized ME 47.3 70.8 56.7 t=0.5 57.1 70.6 63.1? t=0.5 38.1 60.0 46.6? t=0.5
7 Feature- RIP 53.5 61.3 57.2 cr=1 58.7 69.7 63.7? cr=1 54.2 46.8 50.2? cr=1
8 Based, RIP 58.3 58.3 58.3? t=0.5 63.5 57.0 60.1? t=0.5 63.4 35.3 45.3? t=0.5
9 Locally- ME 59.6 51.6 55.3? cr=1 65.6 57.9 61.5 cr=1 55.1 46.2 50.3 cr=1
10 Optimized ME 59.6 51.6 55.3? t=0.5 66.0 57.7 61.6 t=0.5 54.9 46.7 50.4 t=0.5
11 Constraint- RIP 54.5 68.6 60.8? cr=5 58.4 68.8 63.2? cr=4 50.5 56.7 53.4? cr=3
12 Based, RIP 54.1 67.1 59.9? t=0.7 56.5 68.1 61.7 t=0.65 50.3 53.8 52.0 t=0.7
13 Globally- ME 54.8 62.9 58.5? cr=5 62.4 65.6 64.0? cr=3 52.2 57.0 54.5? cr=3
14 Optimized ME 54.1 60.6 57.2 t=0.7 61.7 64.0 62.8? t=0.7 52.0 52.8 52.4? t=0.7
15 Feature- RIP 60.8 56.1 58.4? cr=8 62.2 61.3 61.7 cr=6 54.6 49.4 51.9 cr=8
16 Based, RIP 59.7 57.0 58.3? t=0.6 63.6 59.1 61.3 t=0.8 56.7 48.4 52.3 t=0.7
17 Globally- ME 59.9 51.0 55.1? cr=9 66.5 57.1 61.4 cr=1 56.3 46.9 51.2? cr=10
18 Optimized ME 59.6 51.6 55.3? t=0.95 65.9 57.5 61.4 t=0.95 56.5 46.7 51.1? t=0.5
Table 2: Results of the coreference systems using different approaches to anaphoricity determination on the
three ACE test data sets. Information on which Learner (RIPPER or MaxEnt) is used to train the coreference clas-
sifier, as well as performance results in terms of Recall, Precision, F-measure and the corresponding Conservativeness
parameter are provided whenever appropriate. The strongest result obtained for each data set is boldfaced. In addition,
results that represent statistically significant gains and drops with respect to the baseline are marked with an asterisk
(*) and a dagger (?), respectively.
hard bypassing constraints, with which the corefer-
ence system attempts to resolve only NPs that the
anaphoricity classifier determines to be anaphoric.
As a result, we hypothesized that precision would
increase in comparison to the baseline system. In
addition, we expect that recall will drop owing to
the anaphoricity classifier?s misclassifications of
truly anaphoric NPs. Consequently, overall per-
formance is not easily predictable: F-measure will
improve only if gains in precision can compensate
for the loss in recall.
Results are shown in rows 3-6 of Table 2. Each
row corresponds to a different combination of
learners employed in training the coreference and
anaphoricity classifiers.3 As mentioned in Section
2.2, locally-optimized approaches are a special case
of their globally-optimized counterparts, with the
conservativeness parameter set to the default value
of one for RIPPER and 0.5 for MaxEnt.
In comparison to the baseline, we see large gains
in precision at the expense of recall. Moreover,
CBLO does not seem to be very effective in improv-
ing the baseline, in part due to the dramatic loss in
recall. In particular, although we see improvements
in F-measure in five of the 12 experiments in this
group, only one of them is statistically significant.4
3Bear in mind that different learners employed in train-
ing anaphoricity classifiers correspond to different parametric
methods. For ease of exposition, however, we will refer to the
method simply by the learner it employs.
4The Approximate Randomization test described in Noreen
Worse still, F-measure drops significantly in three
cases.
The Feature-Based, Locally-Optimized (FBLO)
Approach. The experimental setting employed
here is essentially the same as that in CBLO, ex-
cept that anaphoricity information is incorporated
into the coreference system as a feature rather than
as constraints. Specifically, each training/test coref-
erence instance i(NPi,NPj) (created from NPj and
a preceding NP NPi) is augmented with a feature
whose value is the anaphoricity of NPj as computed
by the anaphoricity classifier.
In general, we hypothesized that FBLO would
perform better than the baseline: the addition of an
anaphoricity feature to the coreference instance rep-
resentation might give the learner additional flexi-
bility in creating coreference rules. Similarly, we
expect FBLO to outperform its constraint-based
counterpart: since anaphoricity information is rep-
resented as a feature in FBLO, the coreference
learner can incorporate the information selectively
rather than as universal hard constraints.
Results using the FBLO approach are shown in
rows 7-10 of Table 2. Somewhat unexpectedly, this
approach is not effective in improving the baseline:
F-measure increases significantly in only two of the
12 cases. Perhaps more surprisingly, we see signif-
icant drops in F-measure in five cases. To get a bet-
(1989) is applied to determine if the differences in the F-
measure scores between two coreference systems are statisti-
cally significant at the 0.05 level or higher.
System Variation BNEWS (dev) NPAPER (dev) NWIRE (dev)
Experiments L R P F C R P F C R P F C
1 Constraint- RIP 62.6 76.3 68.8 cr=5 65.5 73.0 69.1 cr=4 56.1 58.9 57.4 cr=3
2 Based, RIP 62.5 75.5 68.4 t=0.7 63.0 71.7 67.1 t=0.65 56.7 54.8 55.7 t=0.7
3 Globally- ME 63.1 71.3 66.9 cr=5 66.2 71.8 68.9 cr=3 57.9 59.7 58.8 cr=3
4 Optimized ME 62.9 70.8 66.6 t=0.7 61.4 74.3 67.3 t=0.65 58.4 55.3 56.8 t=0.7
Table 3: Results of the coreference systems using a constraint-based, globally-optimized approach to
anaphoricity determination on the three ACE held-out development data sets. Information on which Learner
(RIPPER or MaxEnt) is used to train the coreference classifier as well as performance results in terms of Recall,
Precision, F-measure and the corresponding Conservativeness parameter are provided whenever appropriate. The
strongest result obtained for each data set is boldfaced.
ter idea of why F-measure decreases, we examine
the relevant coreference classifiers induced by RIP-
PER. We find that the anaphoricity feature is used in
a somewhat counter-intuitive manner: some of the
induced rules posit a coreference relationship be-
tween NPj and a preceding NP NPi even though NPj
is classified as non-anaphoric. These results seem to
suggest that the anaphoricity feature is an irrelevant
feature from a machine learning point of view.
In comparison to CBLO, the results are mixed:
there does not appear to be a clear winner in any of
the three data sets. Nevertheless, it is worth noticing
that the CBLO systems can be characterized as hav-
ing high precision/low recall, whereas the reverse is
true for FBLO systems in general. As a result, even
though CBLO and FBLO systems achieve similar
performance, the former is the preferred choice in
applications where precision is critical.
Finally, we note that there are other ways to
encode anaphoricity information in a coreference
system. For instance, it is possible to represent
anaphoricity as a real-valued feature indicating the
probability of an NP being anaphoric rather than as
a binary-valued feature. Future work will examine
alternative encodings of anaphoricity.
The Constraint-Based, Globally-Optimized
(CBGO) Approach. As discussed above, we
optimize the anaphoricity model for coreference
performance via the conservativeness parameter. In
particular, we will use this parameter to maximize
the F-measure score for a particular data set and
learner combination using held-out development
data. To ensure a fair comparison between global
and local approaches, we do not rely on additional
development data in the former; instead we use
2
3 of the original training texts for acquiring the
anaphoricity and coreference classifiers and the
remaining 13 for development for each of the data
sets. As far as parameter tuning is concerned,
we tested values of 1, 2, . . . , 10 as well as their
reciprocals for cr and 0.05, 0.1, . . . , 1.0 for t.
In general, we hypothesized that CBGO would
outperform both the baseline and the locally-
optimized approaches, since coreference perfor-
mance is being explicitly maximized. Results using
CBGO, which are shown in rows 11-14 of Table 2,
are largely consistent with our hypothesis. The best
results on all of the three data sets are achieved us-
ing this approach. In comparison to the baseline,
we see statistically significant gains in F-measure in
nine of the 12 experiments in this group. Improve-
ments stem primarily from large gains in precision
accompanied by smaller drops in recall. Perhaps
more importantly, CBGO never produces results
that are significantly worse than those of the base-
line systems on these data sets, unlike CBLO and
FBLO. Overall, these results suggest that CBGO is
more robust than the locally-optimized approaches
in improving the baseline system.
As can be seen, CBGO fails to produce statisti-
cally significant improvements over the baseline in
three cases. The relatively poorer performance in
these cases can potentially be attributed to the un-
derlying learner combination. Fortunately, we can
use the development data not only for parameter
tuning but also in predicting the best learner com-
bination. Table 3 shows the performance of the
coreference system using CBGO on the develop-
ment data, along with the value of the conservative-
ness parameter used to achieve the results in each
case. Using the notation Learner1/Learner2 to
denote the fact that Learner1 and Learner2 are
used to train the underlying coreference classifier
and anaphoricity classifier respectively, we can see
that the RIPPER/RIPPER combination achieves the
best performance on the BNEWS development set,
whereas MaxEnt/RIPPER works best for the other
two. Hence, if we rely on the development data to
pick the best learner combination for use in testing,
the resulting coreference system will outperform the
baseline in all three data sets and yield the best-
performing system on all but the NPAPER data sets,
achieving an F-measure of 60.8 (row 11), 63.2 (row
11), and 54.5 (row 13) for the BNEWS, NPAPER,
1 2 3 4 5 6 7 8 9 10
50
55
60
65
70
75
80
85
cr
Sc
or
e
Recall
Precision
F?measure
Figure 1: Effect of cr on the performance of the
coreference system for the NPAPER development
data using RIPPER/RIPPER
and NWIRE data sets, respectively. Moreover, the
high correlation between the relative coreference
performance achieved by different learner combina-
tions on the development data and that on the test
data also reflects the stability of CBGO.
In comparison to the locally-optimized ap-
proaches, CBGO achieves better F-measure scores
in almost all cases. Moreover, the learned conser-
vativeness parameter in CBGO always has a larger
value than the default value employed by CBLO.
This provides empirical evidence that the CBLO
anaphoricity classifiers are too liberal in classifying
NPs as non-anaphoric.
To examine the effect of the conservativeness pa-
rameter on the performance of the coreference sys-
tem, we plot in Figure 1 the recall, precision, F-
measure curves against cr for the NPAPER develop-
ment data using the RIPPER/RIPPER learner com-
bination. As cr increases, recall rises and precision
drops. This should not be surprising, since (1) in-
creasing cr causes fewer anaphoric NPs to be mis-
classified and allows the coreference system to find
a correct antecedent for some of them, and (2) de-
creasing cr causes more truly non-anaphoric NPs to
be correctly classified and prevents the coreference
system from attempting to resolve them. The best
F-measure in this case is achieved when cr=4.
The Feature-Based, Globally-Optimized
(FBGO) Approach. The experimental set-
ting employed here is essentially the same as that
in the CBGO setting, except that anaphoricity
information is incorporated into the coreference
system as a feature rather than as constraints.
Specifically, each training/test instance i(NPi,NPj)
is augmented with a feature whose value is the
computed anaphoricity of NPj . The development
data is used to select the anaphoricity model
(and hence the parameter value) that yields the
best-performing coreference system. This model
is then used to compute the anaphoricity value for
the test instances. As mentioned before, we use the
same parametric anaphoricity model as in CBGO
for achieving global optimization.
Since the parametric model is designed with a
constraint-based representation in mind, we hypoth-
esized that global optimization in this case would
not be as effective as in CBGO. Nevertheless, we
expect that this approach is still more effective in
improving the baseline than the locally-optimized
approaches.
Results using FBGO are shown in rows 15-18
of Table 2. As expected, FBGO is less effective
than CBGO in improving the baseline, underper-
forming its constraint-based counterpart in 11 of the
12 cases. In fact, FBGO is able to significantly im-
prove the corresponding baseline in only four cases.
Somewhat surprisingly, FBGO is by no means su-
perior to the locally-optimized approaches with re-
spect to improving the baseline. These results seem
to suggest that global optimization is effective only
if we have a ?good? parameterization that is able to
take into account how anaphoricity information will
be exploited by the coreference system. Neverthe-
less, as discussed before, effective global optimiza-
tion with a feature-based representation is not easy
to accomplish.
6 Analyzing Anaphoricity Features
So far we have focused on computing and us-
ing anaphoricity information to improve the perfor-
mance of a coreference system. In this section, we
examine which anaphoricity features are important
in order to gain linguistic insights into the problem.
Specifically, we measure the informativeness of
a feature by computing its information gain (see
p.22 of Quinlan (1993) for details) on our three
data sets for training anaphoricity classifiers. Over-
all, the most informative features are HEAD MATCH
(whether the NP under consideration has the same
head as one of its preceding NPs), STR MATCH
(whether the NP under consideration is the same
string as one of its preceding NPs), and PRONOUN
(whether the NP under consideration is a pronoun).
The high discriminating power of HEAD MATCH
and STR MATCH is a probable consequence of the
fact that an NP is likely to be anaphoric if there is
a lexically similar noun phrase preceding it in the
text. The informativeness of PRONOUN can also be
expected: most pronominal NPs are anaphoric.
Features that determine whether the NP under
consideration is a PROPER NOUN, whether it is a
BARE SINGULAR or a BARE PLURAL, and whether
it begins with an ?a? or a ?the? (ARTICLE) are also
highly informative. This is consistent with our in-
tuition that the (in)definiteness of an NP plays an
important role in determining its anaphoricity.
7 Conclusions
We have examined two largely unexplored issues
in computing and using anaphoricity information
for improving learning-based coreference systems:
representation and optimization. In particular, we
have systematically evaluated all four combinations
of local vs. global optimization and constraint-based
vs. feature-based representation of anaphoricity in-
formation in terms of their effectiveness in improv-
ing a learning-based coreference system.
Extensive experiments on the three ACE corefer-
ence data sets using a symbolic learner (RIPPER)
and a statistical learner (MaxEnt) for training coref-
erence classifiers demonstrate the effectiveness of
the constraint-based, globally-optimized approach
to anaphoricity determination, which employs our
conservativeness-based anaphoricity model. Not
only does this approach improve a ?no anaphoric-
ity? baseline coreference system, it is more effec-
tive than the commonly-adopted locally-optimized
approach without relying on additional labeled data.
Acknowledgments
We thank Regina Barzilay, Claire Cardie, Bo Pang,
and the anonymous reviewers for their invaluable
comments on earlier drafts of the paper. This work
was supported in part by NSF Grant IIS?0208028.
References
David Bean and Ellen Riloff. 1999. Corpus-based iden-
tification of non-anaphoric noun phrases. In Proceed-
ings of the ACL, pages 373?380.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39?71.
Stanley Chen and Ronald Rosenfeld. 2000. A survey of
smoothing techniques for ME models. IEEE Transac-
tions on Speech on Audio Processing, 8(1):37?50.
William Cohen. 1995. Fast effective rule induction. In
Proceedings of ICML.
Stephen Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine Intel-
ligence, 19(4):380?393.
Michel Denber. 1998. Automatic resolution of anaphora
in English. Technical report, Eastman Kodak Co.
Richard Evans. 2001. Applying machine learning to-
ward an automatic classification of it. Literary and
Linguistic Computing, 16(1):45?57.
Christopher Kennedy and Branimir Boguraev. 1996.
Anaphor for everyone: Pronominal anaphora resolu-
tion without a parser. In Proceedings of COLING,
pages 113?118.
Shalom Lappin and Herbert Leass. 1994. An algorithm
for pronominal anaphora resolution. Computational
Linguistics, 20(4):535?562.
Ruslan Mitkov, Richard Evans, and Constantin Orasan.
2002. A new, fully automatic version of Mitkov?s
knowledge-poor pronoun resolution method. In Al.
Gelbukh, editor, Computational Linguistics and Intel-
ligent Text Processing, pages 169?187.
MUC-6. 1995. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6).
MUC-7. 1998. Proceedings of the Seventh Message Un-
derstanding Conference (MUC-7).
Vincent Ng and Claire Cardie. 2002a. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In Proceedings of COLING,
pages 730?736.
Vincent Ng and Claire Cardie. 2002b. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the ACL, pages 104?111.
Eric W. Noreen. 1989. Computer Intensive Methods for
Testing Hypothesis: An Introduction. John Wiley &
Sons.
Chris Paice and Gareth Husk. 1987. Towards the au-
tomatic recognition of anaphoric features in English
text: the impersonal pronoun ?it?. Computer Speech
and Language, 2.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. San Mateo, CA: Morgan Kaufmann.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the ACL, pages 168?175.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message Understanding Conference
(MUC-6), pages 45?52.
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim
Tan. 2003. Coreference resolution using competitive
learning approach. In Proceedings of the ACL, pages
176?183.
Combining Sample Selection and Error-Driven Pruning for
Machine Learning of Coreference Rules
Vincent Ng and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
 
yung,cardie  @cs.cornell.edu
Abstract
Most machine learning solutions to noun
phrase coreference resolution recast the
problem as a classification task. We ex-
amine three potential problems with this
reformulation, namely, skewed class dis-
tributions, the inclusion of ?hard? training
instances, and the loss of transitivity in-
herent in the original coreference relation.
We show how these problems can be han-
dled via intelligent sample selection and
error-driven pruning of classification rule-
sets. The resulting system achieves an F-
measure of 69.5 and 63.4 on the MUC-
6 and MUC-7 coreference resolution data
sets, respectively, surpassing the perfor-
mance of the best MUC-6 and MUC-7
coreference systems. In particular, the
system outperforms the best-performing
learning-based coreference system to date.
1 Introduction
Noun phrase coreference resolution refers to the
problem of determining which noun phrases (NPs)
refer to each real-world entity mentioned in a doc-
ument. Machine learning approaches to this prob-
lem have been reasonably successful, operating pri-
marily by recasting the problem as a classification
task (e.g. Aone and Bennett (1995), McCarthy and
Lehnert (1995), Soon et al (2001)). Specifically, an
inductive learning algorithm is used to train a classi-
fier that decides whether or not two NPs in a docu-
ment are coreferent. Training data are typically cre-
ated by relying on coreference chains from the train-
ing documents: training instances are generated by
pairing each NP with each of its preceding NPs; in-
stances are labeled as positive if the two NPs are in
the same coreference chain, and labeled as negative
otherwise.1
A separate clustering mechanism then coordinates
the possibly contradictory pairwise coreference clas-
sification decisions and constructs a partition on the
set of NPs with one cluster for each set of corefer-
ent NPs. Although, in principle, any clustering algo-
rithm can be used, most previous work uses a single-
link clustering algorithm to impose coreference par-
titions.2 An implicit assumption in the choice of the
single-link clustering algorithm is that coreference
resolution is viewed as anaphora resolution, i.e. the
goal during clustering is to find an antecedent for
each anaphoric NP in a document.3
Three intrinsic properties of coreference4, how-
ever, make the formulation of the problem as a
classification-based single-link clustering task po-
tentially undesirable:
Coreference is a rare relation. That is, most
NP pairs in a document are not coreferent. Con-
1Two NPs are in the same coreference chain if and only if
they are coreferent.
2One exception is Kehler?s work on probabilistic corefer-
ence (Kehler, 1997), in which he applies Dempster?s Rule of
Combination (Dempster, 1968) to combine all pairwise proba-
bilities of coreference to form a partition.
3In this paper, we consider an NP anaphoric if it is part of a
coreference chain but is not the head of the chain.
4Here, we use the term coreference loosely to refer to either
the problem or the binary relation defined on a set of NPs. The
particular choice should be clear from the context.
                                            Association for Computational Linguistics.
                      Language Processing (EMNLP), Philadelphia, July 2002, pp. 55-62.
                         Proceedings of the Conference on Empirical Methods in Natural
sequently, generating training instances by pairing
each NP with each of its preceding NPs creates
highly skewed class distributions, in which the num-
ber of positive instances is overwhelmed by the
number of negative instances. For example, the stan-
dard MUC-6 and MUC-7 (1995; 1998) coreference
data sets contain only 2% positive instances. Un-
fortunately, learning in the presence of such skewed
class distributions remains an open area of research
in the machine learning community (e.g. Pazzani et
al. (1994), Fawcett (1996), Cardie and Howe (1997),
Kubat and Matwin (1997)).
Coreference is a discourse-level problem with dif-
ferent solutions for different types of NPs. The
interpretation of a pronoun, for example, may be de-
pendent only on its closest antecedent and not on the
rest of the members of the same coreference chain.
Proper name resolution, on the other hand, may be
better served by ignoring locality constraints alto-
gether and relying on string-matching or more so-
phisticated aliasing techniques. Consequently, gen-
erating positive instances from all pairs of NPs from
the same coreference chain can potentially make the
learning task harder: all but a few coreference links
derived from any chain might be hard to identify
based on the available contextual cues.
Coreference is an equivalence relation. Recast-
ing the problem as a classification task precludes en-
forcement of the transitivity constraint. After train-
ing, for example, the classifier might determine that
A is coreferent with B, and B with C, but that A and
C are not coreferent. Hence, the clustering mecha-
nism is needed to coordinate these possibly contra-
dictory pairwise classifications. In addition, because
the coreference classifiers are trained independent of
the clustering algorithm to be used, improvements in
classification accuracy do not guarantee correspond-
ing improvements in clustering-level accuracy, i.e.
overall performance on the coreference resolution
task might not improve.
This paper examines each of the above issues.
First, to address the problem of skewed class dis-
tributions, we apply a technique for negative in-
stance selection similar to that proposed in Soon et
al. (2001). In contrast to results reported there, how-
ever, we show empirically that system performance
increases noticeably in response to negative example
selection, with increases in F-measure of 3-5%.
Second, in an attempt to avoid the inclusion of
?hard? training instances, we present a corpus-based
method for implicit selection of positive instances.
The approach is a fully automated variant of the ex-
ample selection algorithm introduced in Harabagiu
et al (2001). With positive example selection, sys-
tem performance (F-measure) again increases, by
12-14%.
Finally, to more tightly tie the classification- and
clustering-level coreference decisions, we propose
an error-driven rule pruning algorithm that opti-
mizes the coreference classifier ruleset with respect
to the clustering-level coreference scoring function.
Overall, the use of pruning boosts system perfor-
mance from an F-measure of 69.3 to 69.5, and from
57.2 to 63.4 for the MUC-6 and MUC-7 data sets,
respectively, enabling the system to achieve perfor-
mance that surpasses that of the best MUC corefer-
ence systems by 4.6% and 1.6%. In particular, the
system outperforms the best-performing learning-
based coreference system (Soon et al, 2001) by
6.9% and 3.0%.
The remainder of the paper is organized as fol-
lows. In sections 2 and 3, we present the machine
learning framework underlying the baseline corefer-
ence system and examine the effect of negative sam-
ple selection. Section 4 presents our corpus-based
algorithm for selection of positive instances. Section
5 describes and evaluates the error-driven pruning
algorithm. We conclude with future work in section
6.
2 The Machine Learning Framework for
Coreference Resolution
Our machine learning framework for coreference
resolution is a standard combination of classification
and clustering, as described above.
Creating an instance. An instance in our machine
learning framework is a description of two NPs in a
document. More formally, let NP be the  th NP in
document  . An instance formed from NP  and NP 
is denoted by 	
 ffBootstrapping Coreference Classifiers with
Multiple Machine Learning Algorithms
Vincent Ng and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
{yung,cardie}@cs.cornell.edu
Abstract
Successful application of multi-view co-
training algorithms relies on the ability to
factor the available features into views that
are compatible and uncorrelated. This can
potentially preclude their use on problems
such as coreference resolution that lack an
obvious feature split. To bootstrap coref-
erence classifiers, we propose and eval-
uate a single-view weakly supervised al-
gorithm that relies on two different learn-
ing algorithms in lieu of the two different
views required by co-training. In addition,
we investigate a method for ranking un-
labeled instances to be fed back into the
bootstrapping loop as labeled data, aiming
to alleviate the problem of performance
deterioration that is commonly observed
in the course of bootstrapping.
1 Introduction
Co-training (Blum and Mitchell, 1998) is a weakly
supervised paradigm that learns a task from a small
set of labeled data and a large pool of unlabeled
data using separate, but redundant views of the data
(i.e. using disjoint feature subsets to represent the
data). To ensure provable performance guaran-
tees, the co-training algorithm assumes as input a
set of views that satisfies two fairly strict condi-
tions. First, each view must be sufficient for learn-
ing the target concept. Second, the views must be
conditionally independent of each other given the
class. Empirical results on artificial data sets by
Muslea et al (2002) and Nigam and Ghani (2000)
confirm that co-training is sensitive to these assump-
tions. Indeed, although the algorithm has been ap-
plied successfully to natural language processing
(NLP) tasks that have a natural view factorization
(e.g. web page classification (Blum and Mitchell,
1998) and named entity classification (Collins and
Singer, 1999)), there has been little success, and a
number of reported problems, when applying co-
training to NLP data sets for which no natural fea-
ture split has been found (e.g. anaphora resolution
(Mueller et al, 2002)).
As a result, researchers have begun to investigate
co-training procedures that do not require explicit
view factorization. Goldman and Zhou (2000) and
Steedman et al (2003b) use two different learning
algorithms in lieu of the multiple views required by
standard co-training.1 The intuition is that the two
learning algorithms can potentially substitute for the
two views: different learners have different rep-
resentation and search biases and can complement
each other by inducing different hypotheses from the
data. Despite their similarities, the principles under-
lying the Goldman and Zhou and Steedman et al
co-training algorithms are fundamentally different.
In particular, Goldman and Zhou rely on hypothesis
testing to select new instances to add to the labeled
data. On the other hand, Steedman et al use two
learning algorithms that correspond to coarsely dif-
ferent features, thus retaining in spirit the advantages
1Steedman et al (2003b) bootstrap two parsers that use dif-
ferent statistical models via co-training. Hence, the two parsers
can effectively be viewed as two different learning algorithms.
provided by conditionally independent feature splits
in the Blum and Mitchell algorithm.
The goal of this paper is two-fold. First, we
propose a single-view algorithm for bootstrapping
coreference classifiers. Like anaphora resolution,
noun phrase coreference resolution is a problem for
which a natural feature split is not readily available.
In related work (Ng and Cardie, 2003), we com-
pare the performance of the Blum and Mitchell co-
training algorithm with that of two existing single-
view bootstrapping algorithms ? self-training with
bagging (Banko and Brill, 2001) and EM (Nigam et
al., 2000) ? on coreference resolution, and show
that single-view weakly supervised learners are a vi-
able alternative to co-training for the task. This pa-
per instead focuses on developing a single-view al-
gorithm that combines aspects of each of the Gold-
man and Zhou and Steedman et al algorithms.
Second, we investigate a new method that, in-
spired by Steedman et al (2003a), ranks unlabeled
instances to be added to the labeled data in an at-
tempt to alleviate a problem commonly observed in
bootstrapping experiments ? performance deterio-
ration due to the degradation in the quality of the
labeled data as bootstrapping progresses (Pierce and
Cardie, 2001; Riloff and Jones, 1999).
In a set of baseline experiments, we first demon-
strate that multi-view co-training fails to boost the
performance of the coreference system under var-
ious parameter settings. We then show that our
single-view weakly supervised algorithm success-
fully bootstraps the coreference classifiers, boost-
ing the F-measure score by 9-12% on two standard
coreference data sets. Finally, we present experi-
mental results that suggest that our method for rank-
ing instances is more resistant to performance dete-
rioration in the bootstrapping process than Blum and
Mitchell?s ?rank-by-confidence? method.
2 Noun Phrase Coreference Resolution
Noun phrase coreference resolution refers to the
problem of determining which noun phrases (NPs)
refer to each real-world entity mentioned in a doc-
ument.2 In this section, we give an overview of
the coreference resolution system to which the boot-
2Concrete examples of the coreference task can be found in
MUC-6 (1995) and MUC-7 (1998).
strapping algorithms will be applied.
The framework underlying the coreference sys-
tem is a standard combination of classification and
clustering (see Ng and Cardie (2002) for details).
Coreference resolution is first recast as a classifica-
tion task, in which a pair of NPs is classified as co-
referring or not based on constraints that are learned
from an annotated corpus. A separate clustering
mechanism then coordinates the possibly contradic-
tory pairwise classifications and constructs a parti-
tion on the set of NPs. When the system operates
within the weakly supervised setting, a weakly su-
pervised algorithm bootstraps the coreference classi-
fier from the given labeled and unlabeled data rather
than from a much larger set of labeled instances. The
clustering algorithm, however, is not manipulated by
the bootstrapping procedure.
3 Learning Algorithms
We employ naive Bayes and decision list learners
in our single-view, multiple-learner framework for
bootstrapping coreference classifiers. This section
gives an overview of the two learners.
3.1 Naive Bayes
A naive Bayes (NB) classifier is a generative classi-
fier that assigns to a test instance i with feature val-
ues <x
1
, . . ., x
m
> the maximum a posteriori (MAP)
label y?, which is determined as follows:
y? = arg max
y
P (y | i)
= arg max
y
P (y)P (i | y)
= arg max
y
P (y)
m
?
i = 1
P (x
i
| y)
The first equality above follows from the definition
of MAP, the second one from Bayes rule, and the last
one from the conditional independence assumption
of the feature values. We determine the class priors
P(y) and the class densities P(x
i
| y) directly from
the training data using add-one smoothing.
3.2 Decision Lists
Our decision list (DL) algorithm is based on that de-
scribed in Collins and Singer (1999). For each avail-
able feature f
i
and each possible value v
j
of f
i
in the
training data, the learner induces an element of the
Observations Justifications
Many feature-value pairs alone can de-
termine the class value.3 For example,
two NPs cannot be coreferent if they differ
in gender or semantic class.
Decision lists draw a decision boundary based on a single feature-value pair
and can take advantage of this observation directly. On the other hand, naive
Bayes classifiers make a decision based on a combination of features and
thus cannot take advantage of this observation directly.
The class distributions in coreference
data sets are skewed. Specifically, the
fact that most NP pairs in a document are
not coreferent implies that the negative in-
stances grossly outnumber the positives.
Naive Bayes classifiers are fairly resistant to class skewness, which can
only exert its influence on classifier prediction via the class priors. On the
other hand, decision lists suffer from skewed class distributions. Elements
corresponding to the negative class tend to aggregate towards the beginning
of the list, causing the classifier to perform poorly on the minority class.
Many instances contain redundant in-
formation as far as classification is con-
cerned. For example, two NPs may dif-
fer in both gender and semantic class, but
knowing one of these two differences is suf-
ficient for determining the class value.
Both naive Bayes classifiers and decision lists can take advantage of data
redundancy. Frequency counts of feature-value pairs in these classifiers are
updated independently, and thus a single instance can possibly contribute to
the discovery of more than one useful feature-value pair. On the other hand,
some classifiers such as decision trees are not able to take advantage of this
redundancy because of their intrinsic nature of recursive data partitioning.
Table 1: The justifications (shown in the right column) for using naive Bayes and decision list learner as
the underlying learning algorithms for bootstrapping coreference classifiers are based on the corresponding
observations on the coreference task and the features used by the coreference system in the left column.
decision list for each class y. The elements in the list
are sorted in decreasing order of the strength associ-
ated with each element, which is defined as the con-
ditional probability P(y | f
i
= v
j
) and is estimated
based on the training data as follows:
P (y | f
i
= v
j
) =
N (f
i
= v
j
, y) + ?
N (f
i
= v
j
) + k?
N (x) is the frequency of event x in the training
data, ? a smoothing parameter, and k the number
of classes. In this paper, k = 2 and we set ? to 0.01.
A test instance is assigned the class associated with
the first element of the list whose predicate is satis-
fied by the description of the instance.
While generative classifiers estimate class densi-
ties, discriminative classifiers like decision lists fo-
cus on approximating class boundaries. Table 1 pro-
vides the justifications for choosing these two learn-
ers as components in our single-view, multi-learner
bootstrapping algorithm. Based on observations of
the coreference task and the features employed by
our coreference system, the justifications suggest
that the two learners can potentially compensate for
each other?s weaknesses.
4 Multi-View Co-Training
In this section, we describe the Blum and Mitchell
(B&M) multi-view co-training algorithm and apply
it to coreference resolution.
3This justifies the use of a decision list as a potential classi-
fier for bootstrapping. See Yarowsky (1995) for details.
4.1 The Multi-View Co-Training Algorithm
The intuition behind the B&M co-training algorithm
is to train two classifiers that can help augment each
other?s labeled data by exploiting two separate but
redundant views of the data. Specifically, each clas-
sifier is trained using one view of the labeled data
and predicts labels for all instances in the data pool,
which consists of a randomly chosen subset of the
unlabeled data. Each then selects its most confident
predictions, and adds the corresponding instances
with their predicted labels to the labeled data while
maintaining the class distribution in the labeled data.
The number of instances to be added to the la-
beled data by each classifier at each iteration is lim-
ited by a pre-specified growth size to ensure that
only the instances that have a high probability of be-
ing assigned the correct label are incorporated. The
data pool is replenished with instances from the un-
labeled data and the process is repeated.
During testing, each classifier makes an indepen-
dent decision for a test instance. In this paper, the
decision associated with the higher confidence is
taken to be the final prediction for the instance.
4.2 Experimental Setup
One of the goals of the experiments is to enable a
fair comparison of the multi-view algorithm with
our single-view bootstrapping algorithm. Since the
B&M co-training algorithm is sensitive not only to
the views employed but also to other input parame-
MUC-6 MUC-7
Naive Bayes Decision List Naive Bayes Decision List
Experiments R P F R P F R P F R P F
Baseline 50.7 52.6 51.6 17.9 72.0 28.7 40.1 40.2 40.1 32.4 78.3 45.8
Multi-view Co-Training 33.3 90.7 48.7 19.5 71.2 30.6 32.9 76.3 46.0 32.4 78.3 45.8
Single-view Bootstrapping 53.6 79.0 63.9 40.1 83.1 54.1 43.5 73.2 54.6 38.3 75.4 50.8
Self-Training 48.3 63.5 54.9 18.7 70.8 29.6 40.1 40.2 40.1 32.9 78.1 46.3
Table 2: Results of multi-view co-training, single-view bootstrapping, and self-training. Recall, Precision, and
F-measure are provided. Except for the baselines, the best results (F-measure) achieved by the algorithms are shown.
ters such as the pool size and the growth size (Pierce
and Cardie, 2001), we evaluate the algorithm under
different parameter settings, as described below.
Evaluation. We use the MUC-6 (1995) and MUC-
7 (1998) coreference data sets for evaluation. The
training set is composed of 30 ?dry run? texts, from
which 491659 and 482125 NP pair instances are
generated for the MUC-6 and MUC-7 data sets, re-
spectively. Unlike Ng and Cardie (2003) where we
choose one of the dryrun texts (contributing ap-
proximately 3500?3700 instances) form the labeled
data set, however, here we randomly select 1000 in-
stances. The remaining instances are used as un-
labeled data. Testing is performed by applying the
bootstrapped coreference classifier and the cluster-
ing algorithm described in section 2 on the 20?30
?formal evaluation? texts for each of the MUC-6 and
MUC-7 data sets.
Two sets of experiments are conducted, one using
naive Bayes as the underlying supervised learning
algorithm and the other the decision list learner. All
results reported are averages across five runs.
Co-training parameters. The co-training param-
eters are set as follows.
Views. We used three methods to generate the
views from the 25 features used by the coreference
system: Mueller et al?s (2002) greedy method, ran-
dom splitting of features into views, and splitting
of features according to the feature type (i.e. lexico-
syntactic vs. non-lexico-syntactic features).4
Pool size. We tested values of 500, 1000, 5000.
Growth size. We tested values of 10, 50, 100, 200.
4.3 Results and Discussion
Results are shown in Table 2, where performance is
reported in terms of recall, precision, and F-measure
4Space limitation precludes a detailed description of these
methods. See Ng and Cardie (2003) for details.
0 50 100 150 200 250 300 350 400 450 500
20
30
40
50
60
70
80
90
100
Number of Co?Training Iterations
Sc
or
e
Baseline
Recall
Precision
F?measure
Figure 1: Learning curve for co-training (pool size
= 500, growth size = 50, views formed by randomly
splitting the features) for MUC-6.
using the model-theoretic MUC scoring program
(Vilain et al, 1995). The baseline coreference sys-
tem, which is trained only on the initially labeled
data using all of the features, achieves an F-measure
of 51.6 (NB) and 28.7 (DL) on the MUC-6 data set
and 40.1 (NB) and 45.8 (DL) on MUC-7.
The results shown in row 2 of Table 2 correspond
to the best F-measure scores achieved by co-training
across all of the parameter combinations described
in the previous subsection. In comparison to the
baseline, co-training is able to improve system per-
formance in only two of the four classifier/data set
combinations: F-measure increases by 2% and 6%
for MUC-6/DL and MUC-7/NB, respectively. Nev-
ertheless, co-training produces high-precision clas-
sifiers in all four cases (at the expense of recall). In
practical applications in which precision is critical,
the co-training classifiers may be preferable to the
baseline classifiers despite the fact that they achieve
similar F-measure scores.
Figure 1 depicts the learning curve for the co-
training run that gives rise to the best F-measure for
the MUC-6 data set using naive Bayes. The hor-
izontal (dotted) line shows the performance of the
baseline system, as described above. As co-training
progresses, F-measure rises to 48.7 at iteration ten
and gradually drops to and stabilizes at 42.9. We ob-
serve similar performance trends for the other clas-
sifier/data set combinations. The drop in F-measure
is potentially due to the pollution of the labeled data
by mislabeled instances (Pierce and Cardie, 2001).
5 Single-View Bootstrapping
In this section, we describe and evaluate our single-
view, multi-learner bootstrapping algorithm, which
combines ideas from Goldman and Zhou (2000) and
Steedman et al (2003b). We will start by giving an
overview of these two co-training algorithms.
5.1 Related Work
The Goldman and Zhou (G&Z) Algorithm.
This single-view algorithm begins by training two
classifiers on the initially labeled data using two
different learning algorithms; it requires that each
classifier partition the instance space into a set of
equivalence classes (e.g. in a decision tree, each leaf
node defines an equivalence class). Each classi-
fier then considers each equivalence class and uses
hypothesis testing to determine if adding all unla-
beled instances within the equivalence class to the
other classifier?s labeled data will improve the per-
formance of its counterparts. The process is then
repeated until no more instances can be labeled.
The Steedman et al (Ste) Algorithm. This algo-
rithm is a variation of B&M applied to two diverse
statistical parsers. Initially, each parser is trained on
the labeled data. Each then parses and scores all
sentences in the data pool, and then adds the most
confidently parsed sentences to the training data of
the other parser. The parsers are retrained, and the
process is repeated for several iterations.
The algorithm differs from B&M in three main
respects. First, the training data of the two parsers
diverge after the first co-training iteration. Second,
the data pool is flushed and refilled entirely with in-
stances from the unlabeled data after each iteration.
This reduces the possibility of having unreliably la-
beled sentences accumulating in the pool. Finally,
the two parsers, each of which is assumed to hold a
unique ?view? of the data, are effectively two differ-
ent learning algorithms.
5.2 Our Single-View Bootstrapping Algorithm
As mentioned before, our algorithm uses two dif-
ferent learning algorithms to train two classifiers on
the same set of features (i.e. the full feature set).
At each bootstrapping iteration, each classifier la-
bels and scores all instances in the data pool. The
highest scored instances labeled by one classifier are
added to the training data of the other classifier and
vice versa. Since the two classifiers are trained on
the same view, it is important to maintain a separate
training set for each classifier: this reduces the prob-
ability that the two classifiers converge to the same
hypothesis at an early stage and hence implicitly in-
creases the ability to bootstrap. Like Ste, the entire
data pool is replenished with instances drawn from
the unlabeled data after each iteration, and the pro-
cess is repeated. So our algorithm is effectively Ste
applied to coreference resolution ? instead of two
parsing algorithms that correspond to different fea-
tures, we use two learning algorithms, each of which
relies on the same set of features as in G&Z. The
similarities and differences among B&M, G&Z, Ste,
and our algorithm are summarized in Table 3.
5.3 Results and Discussion
We tested different pool sizes and growth sizes as
specified in section 4.2 to determine the best pa-
rameter setting for our algorithm. For both data
sets, the best F-measure score is achieved using a
pool size of 5000 and a growth size of 50. The re-
sults under this parameter setting are given in row
3 of Table 2. In comparison to the baseline, we see
dramatic improvement in F-measure for both clas-
sifiers and both data sets. In addition, we see si-
multaneous gains in recall and precision in all cases
except MUC-7/DL. Furthermore, single-view boot-
strapping beats co-training (in terms of F-measure
scores) by a large margin in all four cases. These
results provide suggestive evidence that single-view,
multi-learner bootstrapping might be a better alter-
native to its multi-view, single-learner counterparts
for coreference resolution.
The bootstrapping run that corresponds to this pa-
rameter setting for the MUC-6 data set using naive
Bayes is shown in Figure 2. Again, we see a ?typi-
Blum and Mitchell Goldman and Zhou Steedman et al Ours
Bootstrapping basis Use different views Use different learners Use different parsers Use different learners
Number of instances
added per iteration
Fixed Variable Fixed Fixed
Training sets for the
two learners/parsers
Same Different Different Different
Data pool flushed af-
ter each iteration
No N/A (No data pool is
used)
Yes Yes
Example selection
method
Highest scored in-
stances
Instances in all
equivalance classes
that are expected to
improve a classifier
Highest scored sen-
tences
Highest scored in-
stances
Table 3: Summary of the major similarities and differences among four bootstrapping schemes: Blum and
Mitchell, Goldman and Zhou, Steedman et al, and ours. Only the relevant dimensions are discussed here.
0 500 1000 1500 2000 2500 3000 3500 4000
40
45
50
55
60
65
70
75
80
85
Number of Co?Training Iterations
Sc
or
e
Baseline
Recall
Precision
F?measure
Figure 2: Learning curve for our single-view boot-
strapping algorithm (pool size = 5000, growth size =
50) for MUC-6.
cal? bootstrapping curve: an initial rise in F-measure
followed by a gradual deterioration. In comparison
to Figure 1, the recall level achieved by co-training is
much lower than that of single-view bootstrapping.
This appears to indicate that each co-training view is
insufficient for learning the target concept: the fea-
ture split limits any interaction of features that can
produce better recall.
Finally, Figure 2 shows that performance in-
creases most rapidly in the first 200 iterations. This
provides indirect evidence that the two classifiers
have acquired different hypotheses from the ini-
tial data and are exchanging information with each
other. To ensure that the classifiers are indeed bene-
fiting from each other, we conducted a self-training
experiment for each classifier separately: at each
self-training iteration, each classifier labels all 5000
instances in the data pool using all available features
and selects the most confidently labeled 50 instances
for addition to its labeled data.5 The best F-measure
scores achieved by self-training are shown in the last
row of Table 2. Overall, self-training only yields
marginal performance gains over the baseline.
Nevertheless, self-training outperforms co-
training in both cases where naive Bayes is used.
While these results seem to suggest that co-training
is inherently handicapped for coreference resolu-
tion, there are two plausible explanations against
this conclusion. First, the fact that self-training has
access to all of the available features may account
for its superior performance to co-training. This is
again partially supported by the fact that the recall
level achieved by co-training is lower than that of
self-training in both cases in which self-training
outperforms co-training. Second, 1000 instances
may simply not be sufficient for co-training to be
effective for this task: in related work (Ng and
Cardie, 2003), we find that starting with 3500?3700
labeled instances instead of 1000 allows co-training
to improve the baseline by 4.6% and 9.5% in
F-measure using naive Bayes classifiers for the
MUC-6 and MUC-7 data sets, respectively.
6 An Alternative Ranking Method
As we have seen before, F-measure scores ulti-
mately decrease as bootstrapping progresses. If the
drop were caused by the degradation in the quality
of the bootstrapped data, then a more ?conservative?
instance selection method than that of B&M would
help alleviate this problem. Our hypothesis is that
selection methods that are based solely on the con-
fidence assigned to an instance by a single classifier
5Note that this is self-training without bagging, unlike the
self-training algorithm discussed in Ng and Cardie (2003).
i1
> i
2
if any of the following is true:
[?(C
1
(i
1
)) = ?(C
2
(i
1
))] ? [?(C
1
(i
2
)) = ?(C
2
(i
2
))]
[?(C
1
(i
1
)) = ?(C
2
(i
1
))] ? [?(C
1
(i
2
)) = ?(C
2
(i
2
))] ? [|C
1
(i
1
) ? C
2
(i
1
)| > |C
1
(i
2
) ? C
2
(i
2
)|]
[?(C
1
(i
1
)) = ?(C
2
(i
1
))] ? [?(C
1
(i
2
)) = ?(C
2
(i
2
))] ? [max(C
1
(i
1
), 1 ? C
1
(i
1
)) > max(C
1
(i
2
), 1 ? C
1
(i
2
))]
Figure 3: The ranking method that a binary classifier C
1
uses to impose a partial ordering on the instances
to be selected and added to the training set of binary classifier C
2
. i
1
and i
2
are arbitrary instances, and ? is
a function that rounds a number to its closest integer.
may be too liberal. In particular, these methods al-
low the addition of instances with opposing labels
to the labeled data; this can potentially result in in-
creased incompatibility between the classifiers.
Consequently, we develop a new procedure for
ranking instances in the data pool. The bootstrap-
ping algorithm then selects the highest ranked in-
stances to add to the labeled data in each iteration.
The method favors instances whose label is agreed
upon by both classifiers (Preference 1). However,
incorporating instances that are confidently labeled
by both classifiers may reduce the probability of
acquiring new information from the data. There-
fore, the method imposes an additional preference
for instances that are confidently labeled by one but
not both (Preference 2). If none of the instances
receives the same label from the classifiers, the
method resorts to the ?rank-by-confidence? method
used by B&M (Preference 3).
More formally, define a binary classifier as a func-
tion that maps an instance to a value that indicates
the probability that it is labeled as positive. Now,
let ? be a function that rounds a number to its near-
est integer. Given two binary classifiers C
1
and C
2
and instances i
1
and i
2
, the ranking method shown in
Figure 3 uses the three preferences described above
to impose a partial ordering on the given instances
for incorporation into C
2
?s labeled data. The method
similarly ranks instances to be added to C
1
?s labeled
data, with the roles of C
1
and C
2
reversed.
Steedman et al (2003a) also investigate instance
selection methods for co-training, but their goal is
primarily to use selection methods as a means to
explore the trade-off between maximizing coverage
and maximizing accuracy.6 In contrast, our focus
6McCallum and Nigam (1998) tackle this idea of balancing
0 500 1000 1500 2000 2500 3000 3500 4000
44
46
48
50
52
54
56
58
60
62
64
Number of Co?Training Iterations
F?
m
ea
su
re
Baseline
Using the B&M ranking method
Using our ranking method
Figure 4: F-measure curves for our single-view
bootstrapping algorithm with different ranking
methods (pool size = 5000, growth size = 50) for
MUC-6.
here is on examining whether a more conservative
ranking method can alleviate the problem of perfor-
mance deterioration. Nevertheless, Preference 2 is
inspired by their Sint-n selection method, which se-
lects an instance if it belongs to the intersection of
the set of the n percent highest scoring instances
of one classifier and the set of the n percent lowest
scoring instances of the other. To our knowledge, no
previous work has examined a ranking method that
combines the three preferences described above.
To compare our ranking procedure with B&M?s
rank-by-confidence method, we repeat the boot-
strapping experiment shown in Figure 2 except that
we replace B&M?s ranking method with ours. The
learning curves generated using the two ranking
methods with naive Bayes for the MUC-6 data set
are shown in Figure 4. The results are consistent
with our intuition regarding the two ranking meth-
accuracy and coverage by combining EM and active learning.
ods. The B&M ranking method is more liberal.
In particular, each classifier always selects the most
confidently labeled instances to add to the other?s la-
beled data at each iteration. If the underlying learn-
ers have indeed induced two different hypotheses
from the data, then each classifier can potentially ac-
quire informative instances from the other and yield
performance improvements very rapidly.
In contrast, our ranking method is more conserva-
tive in that it places more emphasis on maintaining
labeled data accuracy than the B&M method. As
a result, the classifier learns at a slower rate when
compared to that in the B&M case: it is not until iter-
ation 600 that we see a sharp rise in F-measure. Due
to the ?liberal? nature of the B&M method, however,
its performance drops dramatically as bootstrapping
progresses, whereas ours just dips temporarily. This
can potentially be attributed to the more rapid injec-
tion of mislabeled instances into the labeled data in
the B&M case. At iteration 2800, our method starts
to outperform B&M?s. Overall, our ranking method
does not exhibit the performance trend observed
with the B&M method: except for the spike between
iterations 0 and 100, F-measure does not deteriorate
as bootstrapping progresses. Since it is hard to deter-
mine a ?good? stopping point for bootstrapping due
to the paucity of labeled data in a weakly supervised
setting, our ranking method can potentially serve as
an alternative to the B&M method.
7 Conclusions
We have proposed a single-view, multi-learner boot-
strapping algorithm for coreference resolution and
shown empirically that the algorithm is a better al-
ternative to the Blum and Mitchell co-training al-
gorithm for this task for which no natural feature
split has been found. In addition, we have investi-
gated an example ranking method for bootstrapping
that, unlike Blum and Mitchell?s rank-by-confidence
method, can potentially alleviate the problem of per-
formance deterioration due to the pollution of the la-
beled data in the course of bootstrapping.
Acknowledgments
We thank the anonymous reviewers for their invalu-
able and insightful comments. This work was sup-
ported in part by NSF Grant IIS?0208028.
References
Michele Banko and Eric Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Proceed-
ings of the ACL/EACL, pages 26?33.
Avrim Blum and Tom Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In Proceedings of COLT,
pages 92?100.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings of
EMNLP/VLC, pages 100?110.
Sally Goldman and Yan Zhou. 2000. Enhancing supervised
learning with unlabeled data. In Proceedings of ICML, pages
327?334.
Andrew McCallum and Kamal Nigam. 1998. Employing EM
and pool-based active learning for text classification. In Pro-
ceedings of ICML, pages 359?367.
MUC-6. 1995. Proceedings of the Sixth Message Understand-
ing Conference (MUC-6).
MUC-7. 1998. Proceedings of the Seventh Message Under-
standing Conference (MUC-7).
Christoph Mueller, Stefan Rapp, and Michael Strube. 2002.
Applying co-training to reference resolution. In Proceedings
of the ACL, pages 352?359.
Ion Muslea, Steven Minton, and Craig Knoblock. 2002. Active
+ Semi-Supervised Learning = Robust Multi-View Learning.
In Proceedings of ICML.
Vincent Ng and Claire Cardie. 2002. Combining sample selec-
tion and error-driven pruning for machine learning of coref-
erence rules. In Proceedings of EMNLP, pages 55?62.
Vincent Ng and Claire Cardie. 2003. Weakly supervised natu-
ral language learning without redundant views. In Proceed-
ings of HLT-NAACL.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the effec-
tiveness and applicability of co-training. In Proceedings of
CIKM, pages 86?93.
Kamal Nigam, Andrew McCallum, Sabastian Thrun, and
Tom Mitchell. 2000. Text classification from labeled
and unlabeled documents using EM. Machine Learning,
39(2/3):103?134.
David Pierce and Claire Cardie. 2001. Limitations of co-
training for natural language learning from large datasets. In
Proceedings of EMNLP, pages 1?9.
Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping. In Pro-
ceedings of AAAI, pages 474?479.
M. Steedman, R. Hwa, S. Clark, M. Osborne, A. Sarkar,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003a.
Example selection for bootstrapping statistical parsers. In
Proceedings of HLT-NAACL.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003b.
Bootstrapping statistical parsers from small datasets. In Pro-
ceedings of the EACL.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scoring
scheme. In Proceedings of the Sixth MessageUnderstanding
Conference (MUC-6), pages 45?52.
David Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Proceedingsof the ACL,
pages 189?196.
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 218?227, Prague, June 2007. c?2007 Association for Computational Linguistics
Unsupervised Part-of-Speech Acquisition for Resource-Scarce Languages 
Sajib Dasgupta and Vincent Ng 
Human Language Technology Research Institute 
University of Texas at Dallas 
Richardson, TX 75083-0688 
{sajib,vince}@hlt.utdallas.edu 
 
 
 
Abstract 
This paper proposes a new bootstrapping 
approach to unsupervised part-of-speech 
induction. In comparison to previous 
bootstrapping algorithms developed for this 
problem, our  approach aims to improve 
the quality of the seed clusters by 
employing seed words that are both 
distributionally and morphologically 
reliable. In particular, we present a novel 
method for combining morphological and 
distributional information for seed 
selection. Experimental results demonstrate 
that our approach works well for English 
and Bengali, thus providing suggestive 
evidence that it is applicable to both 
morphologically impoverished languages 
and highly inflectional languages. 
1 Introduction 
The availability of a high-quality lexicon is crucial 
to the development of fundamental text-processing 
components such as part-of-speech (POS) taggers 
and syntactic parsers. While hand-crafted lexicons 
are readily available for resource-rich languages 
such as English, the same is not true for resource-
scarce languages. Unfortunately, manually 
constructing a lexicon requires a lot of linguistic 
expertise, and is practically infeasible for highly 
inflectional and agglutinative languages, which 
contain a very large number of lexical items. Given 
the scarcity of annotated data for acquiring the 
lexicon in a supervised manner, researchers have 
instead investigated unsupervised POS induction 
techniques for automating the lexicon construction 
process. In essence, the goal of unsupervised POS 
induction is to learn the set of possible POS tags 
for each lexical item from an unannotated corpus. 
 The most common approach to unsupervised 
POS induction to date has been motivated by Har-
ris?s (1954) distributional hypothesis: words with 
similar co-occurrence patterns should have similar 
syntactic behavior. More specifically, unsupervised 
POS induction algorithms typically operate by (1) 
representing each target word (i.e., a word to be 
tagged with its POS) as a context vector that en-
codes its left and right context, (2) clustering dis-
tributionally similar words, and (3) manually label-
ing each cluster with a POS tag by inspecting the 
members of the cluster. 
This distributional approach works under the as-
sumption that the context vector of each word en-
codes sufficient information for enabling accurate 
word clustering. However, many words are dis-
tributionally unreliable: due to data sparseness, 
they occur infrequently and hence their context 
vectors do not capture reliable statistical informa-
tion. To overcome this problem, Clark (2000) pro-
poses a bootstrapping approach, in which he (1) 
clusters the most distributionally reliable words, 
and then (2) incrementally augments each cluster 
with words that are distributionally similar to those 
already in the cluster. 
The goal of this paper is to propose a new boot-
strapping approach to unsupervised POS induction 
that can operate in a resource-scarce setting. Most 
notably, our approach aims to improve the quality 
of the seed clusters by employing seed words that 
are both distributionally and morphologically reli-
able. In particular, we present a novel method for 
combining morphological and distributional infor-
mation for seed selection. Furthermore, given our 
218
emphasis on resource-scarce languages, our ap-
proach does not rely on any language resources. In 
particular, the morphological information that it 
exploits is provided by an unsupervised morpho-
logical analyzer.  
It is perhaps not immediately clear why morpho-
logical information would play a crucial role in the 
induction process, especially since the distribu-
tional approach has achieved considerable success 
for English POS induction (see Lamb (1961), 
Sch?tze (1995) and Clark (2000)). To understand 
the role and significance of morphology, it is im-
portant to first understand why the distributional 
approach works well for English. Recall from the 
above that the distributional approach assumes that 
the information encoded in the context vector of 
each word, which typically consists of the 250 
most frequent words of a given language, is suffi-
cient for accurately clustering the words. This ap-
proach works well for English because the most 
frequent English words are composed primarily of 
closed-class words such as ?to? and ?is?, which 
provide strong clues to the POS of the target word. 
However, this assumption is not necessarily valid 
for fairly free word order and highly inflectional 
languages such as Bengali. The reason is that (1) 
co-occurrence statistics collected from free word 
order languages are not as reliable as those from 
fixed word order languages; and (2) many of the 
closed-class words that appear in the context vec-
tor for English words are realized as inflections in 
Bengali. The absence of these highly informative 
words implies that the context vectors may no 
longer capture sufficient information for accurately 
clustering Bengali words, and hence the use of 
morphological information becomes particularly 
important for unsupervised POS induction for 
these inflectional languages.  
We will focus primarily on labeling open-class 
words with their POS tags. Our decision is moti-
vated by the fact that closed-class words generally 
comprise a small percentage of the lexical items of 
a language. In Bengali, the percentage of closed-
class words is even smaller than that in English: as 
mentioned before, many closed-class words in 
English are realized as suffixes in Bengali. 
Although our attempt to incorporate morpho-
logical information into the distributional POS in-
duction framework was originally motivated by 
inflectional languages, experimental results show 
that our approach works well for both English and 
Bengali, suggesting its applicability to both mor-
phologically impoverished languages and highly 
inflectional languages. Owing to the lack of pub-
licly available resources for Bengali, we manually 
created a 5000-word Bengali lexicon for evaluation 
purposes. Hence, one contribution of our work lies 
in the creation of an annotated dataset for Bengali. 
By making this dataset publicly available 1 , we 
hope to facilitate the comparison of different unsu-
pervised POS induction algorithms and to stimu-
late interest in Bengali language processing.  
The rest of the paper is organized as follows. 
Section 2 discusses related work on unsupervised 
POS induction. Section 3 describes our tagsets for 
English and Bengali. The next three sections de-
scribe the three steps of our bootstrapping ap-
proach: cluster the words using morphological in-
formation (Section 4), remove potentially misla-
beled words from each cluster (Section 5), and 
bootstrap each cluster using a weakly supervised 
learner (Section 6). Finally, we present evaluation 
results in Section 7 and conclusions in Section 8.  
2 Related Work 
Several unsupervised POS induction algorithms 
have also attempted to incorporate morphological 
information into the distributional framework, but 
our work differs from these in two respects.  
Computing morphological information. Previous 
POS induction algorithms have attempted to derive 
morphological information from dictionaries (Ha-
ji, 2000) and knowledge-based morphological 
analyzers (Duh and Kirchhoff, 2006). However, 
these resources are generally not available for re-
source-scarce languages. Consequently, research-
ers have attempted to derive morphological infor-
mation heuristically (e.g., Cucerzan and Yarowsky 
(2000), Clark (2003), Freitag (2004)). For instance, 
Cucerzan and Yarowsky (2000) posit a character 
sequence x as a suffix if there exists a sufficient 
number of distinct words w in the vocabulary such 
that the concatentations wx are also in the vocabu-
lary.  It is conceivable that such heuristically com-
puted morphological information can be inaccurate, 
thus rendering the usefulness of a more accurate 
morphological analyzer. To address this problem, 
we exploit morphological information provided by 
an unsupervised word segmentation algorithm.   
                                                 
1
 See http://www.utdallas.edu/~sajib/posDatasets.html. 
219
Tag Description Treebank tags 
JJ Adjective JJ 
JJR Adjective, comparative JJR 
JJS Adjective, superlative JJS 
NN Singular noun NN, NNP 
NNS Plural noun NNS, NNPS 
RB Adverb RB 
VB Verb, non-3rd ps. sing. present VB, VBP 
VBD Verb, past tense or past participle VBD, VBN 
VBG Verb, gerund/present participle VBG 
VBZ Verb, 3rd ps. sing. present VBZ 
Table 1: The English tagset 
 
Using morphological information. Perhaps due to 
the overly simplistic methods employed to com-
pute morphological information, morphology has 
only been used as what Biemann (2006) called 
add-on?s in existing POS induction algorithms, 
which remain primarily distributional in nature. In 
contrast, our approach more tightly integrates mor-
phology into the distributional framework. As we 
will see, we train SVM classifiers using both mor-
phological and distributional features to select seed 
words for our bootstrapping algorithm, effectively 
letting SVM combine these two sources of infor-
mation and perform automatic feature weighting. 
Another appealing feature of our approach is that 
when labeling each unlabeled word with its POS 
tag, an SVM classifier also returns a numeric value 
that indicates how confident the word is labeled. 
This opens up the possibility of having a human 
improve our automatically constructed lexicon by 
manually checking those entries that are tagged 
with low confidence by an SVM classifier. 
Recently, there have been attempts to perform 
(mostly) unsupervised POS tagging without rely-
ing on a POS lexicon. Haghighi and Klein?s (2006) 
prototype-driven approach requires just a few pro-
totype examples for each POS tag, exploiting these 
labeled words to constrain the labels of their dis-
tributionally similar words when training a genera-
tive log-linear model for POS tagging. Smith and 
Eisner (2005) train a log-linear model for POS tag-
ging in an unsupervised manner using contrastive 
estimation, which seeks to move probability mass 
to a positive example e from its neighbors (i.e., 
negative examples created by perturbing e). 
3 The English and Bengali Tagsets 
Given our focus on automatically labeling open 
class words, our English and Bengali tagsets are 
designed  to essentially  cover  all of the open-class 
Tag Description Examples 
JJ Adjective vhalo, garam, kharap 
NN Singular noun kanna, ridoy, shoshon 
NN2 2nd order inflectional noun dhopake, kalamtike 
NN6 6th order inflectional noun gharer, manusher 
NN7 7th order inflectional noun dhakai, barite, graame 
NNP Proper noun arjun, ahmmad 
NNS Plural noun manushgulo, pakhider 
NNSH Noun ending with ?sh? barish, jatrish 
VB Finite verb kheyechi, krlam, krI 
VBN Non-finite verb kre, giye, jete, kadte 
Table 2: The Bengali tagset 
 
words. Our English tagset, which is composed of 
ten tags, is shown in Table 1. As we can see, a tag 
in our tagset can be mapped to more than one Penn 
Treebank tags. For instance, we use the tag ?NN? 
for both singular and plural common nouns. Our 
decision of which Penn Treebank tags to group 
together is based on that of Sch?tze (1995).  
Our Bengali tagset, which also consists of ten 
tags, is adapted from the one proposed by Saha et 
al. (2004) (see Table 2). It is worth noting that 
unlike English, we assign different tags to Bengali 
proper nouns and common nouns. The reason is 
that for English, it is not particularly crucial to dis-
tinguish the two types of nouns during POS induc-
tion, since they can be distinguished fairly easily 
using heuristics such as initial capitalization. For 
Bengali, such simple heuristics do not exist, as the 
Bengali alphabet does not have any upper and 
lower case letters. Hence, it is important to distin-
guish Bengali proper nouns and common nouns 
during POS induction. 
4 Clustering the Morphologically Similar 
Words 
As mentioned before, our approach aims to more 
tightly integrate morphological information into 
the distributional POS induction framework. In 
fact, our POS induction algorithm begins by clus-
tering the morphologically similar words (i.e., 
words that combine with the same set of suffixes). 
The motivation for clustering morphologically 
similar words can be attributed to our hypothesis 
that words having similar POS should combine 
with a similar set of suffixes. For instance, verbs in 
English combine with suffixes like ?ing?, ?ed? and 
?s?, whereas adjectives combine with suffixes like 
?er? and ?est?. Note, however, that the suffix ?s? 
can attach to both verbs and nouns in English, and 
so it is not likely to be a useful feature for identify-
220
ing the POS of a word. The question, then, is how 
to determine which suffixes are useful for the POS 
identification task in an unsupervised setting where 
we do not have any prior knowledge of language-
specific grammatical constraints. This section pro-
poses a method for identifying the ?useful? suf-
fixes and employing them to cluster the morpho-
logically similar words. As we will see, our clus-
tering algorithm not only produces soft clusters, 
but it also automatically determines the number of 
clusters for a particular language.   
Before we describe how to identify the useful 
suffixes, we need to (1) induce all of the suffixes 
and (2) morphologically segment the words in our 
vocabulary. 2  However, neither of these tasks is 
simple for a truly resource-scarce language for 
which we do not have a dictionary or a knowledge-
based morphological analyzer. As mentioned in the 
introduction, our proposed solution to both tasks is 
to use an unsupervised morphological analyzer that 
can be built just from an unannotated corpus. In 
particular, we have implemented an unsupervised 
morphological analyzer that outperforms Gold-
smith?s (2001) Linguistica and Creutz and Lagus?s 
(2005) Morfessor for our English and Bengali 
datasets and compares favorably to the best-
performing morphological parsers in MorphoChal-
lenge 20053 (see Dasgupta and Ng (2007)).  
Given the segmentation of each word and the 
most frequent 30 suffixes4 provided by our mor-
phological analyzer, our clustering algorithm oper-
ates by (1) clustering the similar suffixes and then 
(2) assigning words to each cluster based on the 
suffixes a word combines with. To cluster similar 
suffixes, we need to define the similarity between 
two suffixes. Informally, we say that two suffixes x 
and y are similar if a word that combines with x 
also combines with y and vice versa. In practice, 
we will rarely posit two suffixes as similar under 
this definition unless we assume access to a com-
plete vocabulary ? an assumption that is especially 
unrealistic for resource-scarce languages. As a re-
sult, we relax this definition and consider two suf-
fixes x and y similar if P(x | y) > t and P(y | x) > t, 
where P(x | y) is the probability of a word combin-
ing with suffix x given that it combines with suffix 
                                                 
2
 A vocabulary is simply a set of (distinct) words extracted 
from an unannotated corpus. We extracted our English and 
Bengali vocabulary from WSJ and Prothom Alo, respectively.  
3
 http://www.cis.hut.fi/morphochallenge2005/ 
4
 We found that 30 suffixes are sufficient to cluster the words. 
y, and t is a threshold that we set to 0.4 in all of our 
experiments. Note that both probabilities can be 
estimated from an unannotated corpus.5 Given this 
definition of similarity, we can cluster the similar 
suffixes using the following steps: 
Creating the initial clusters.  First, we create a 
suffix graph, in which we have (1) one node for 
each of the 30 suffixes, and (2) a directed edge 
from suffix x to suffix y if P(y | x) > 0.4. We then 
identify the strongly connected components of this 
graph using depth-first search. These strongly con-
nected components define our initial partitioning of 
the 30 suffixes. We denote the suffixes assigned to 
a cluster the primary keys of the cluster.   
Improving the initial clusters. Recall that we 
ultimately want to cluster the words by assigning 
each word w to the cluster in which w combines 
with all of its primary keys. Given this goal, it is 
conceivable that singleton clusters are not 
desirable. For instance, a cluster that has ?s? as its 
only primary key is not useful, because although a 
lot of words combine with ?s?, they do not 
necessarily have the same POS. As a result, we 
improve each initial cluster by adding more 
suffixes to the cluster, in hopes of improving the 
resulting clustering of the words by placing 
additional constraints on each cluster. More 
specifically, we add a suffix y to a cluster c if, for 
each primary key x of c, P(y | x) > 0.4. If this 
condition is satisfied, then y becomes a secondary 
key of c. For each initial cluster c?, we perform this 
check using each of the suffixes x? not in c? to see 
if x? can be added to c?. If, after this expansion 
step, we still have a cluster c* defined by a single 
primary key x that also serves as a secondary key 
in other clusters, then x is probably ambiguous 
(i.e., x can probably attach to words belonging to 
different POSs); and consequently, we remove c*. 
We denote the resulting set of clusters by C. 
Populating the clusters with words. Next, for 
each word w in our vocabulary, we check whether 
w can be assigned to any of the clusters in C. Spe-
cifically, we assign w to a cluster c if w can com-
bine with each of its primary keys and at least half 
of its secondary keys.  
Labeling and merging the clusters. After popu-
lating each cluster with words, we manually label 
                                                 
5
 For instance, we compute P(x | y) as the ratio of the number 
of distinct words that combines with both x and y to the num-
ber of distinct words that combine with y only. 
221
each of them with a POS tag from the tagset. We 
found that all of the clusters are labeled as NN, 
VB, or JJ. The reason is that the clustered words 
are mostly root words. We then merge all the clus-
ters labeled with the same POS tag, yielding only 
three ?big? clusters. Note that these ?big? clusters 
are soft clusters, since a word can belong to more 
than one of them. For instance, ?cool? can combine 
with ?s? or ?ing? to form a VB, and it can also 
combine with ?er? or ?est? to form a JJ. 
Generating sub-clusters. Recall that each ?big? 
cluster contains a set of suffixes and also a set of 
words that combines with those suffixes. Now, for 
each ?big? cluster c, we create one sub-cluster cx 
for each suffix x that appears in c. Then, for each 
word w in c, we use our unsupervised morphologi-
cal analyzer to generate w+x and add the surface 
form to the corresponding sub-cluster. 
Labeling the sub-clusters. Finally, we manually 
label each sub-cluster with a POS tag from our 
tagset. For example, all the words ending in ?ing? 
will be labeled as VBG. As before, we merge two 
clusters if they are labeled with the same POS tag. 
The resulting clusters are our morphologically 
formed clusters. 
5 Purifying the Seed Set 
The clusters formed thus far cannot be expected to 
be perfectly accurate, since (1) our unsupervised 
morphological analyzer is not perfect, and (2) 
morphology alone is not always sufficient for de-
termining the POS of a word. In fact, we found that 
many adjectives are mislabeled as nouns for both 
languages. For instance, ?historic? is labeled as a 
noun, since it combines with suffixes like ?al? and 
?ally? that ?accident? combines with. In addition, 
many words are labeled with the POS that does not 
correspond to their most common word sense. For 
instance, while words like ?chair?, ?crowd? and 
?cycle? are more commonly used as nouns than 
verbs, they are labeled as verbs by our clustering 
algorithm. The reason is that suffixes that typically 
attach to verbs (e.g., ?s?, ?ed?, ?ing?) also attach to 
these words. Such labelings, though not incorrect, 
are undesirable, considering the fact that these 
words are to be used as seeds to bootstrap our mor-
phologically formed clusters in a distributional 
manner. For instance, since ?chair? and ?crowd? 
are distributionally similar to nouns, their presence 
in the verb clusters can potentially contaminate the 
clusters with nouns during the bootstrapping proc-
ess. Hence, for the purpose of effective bootstrap-
ping, we also consider these words ?mislabeled?.  
To identify the words that are potentially misla-
beled, we rely on the following assumption: words 
that are morphologically similar should also be 
distributionally similar and vice versa. Based on 
this assumption, we propose a purification method 
that posits a word w as potentially mislabeled (and 
therefore should be removed or relabeled) if the 
POS of w as predicted using distributional infor-
mation differs from that as determined by mor-
phology. 
The question, then, is how to predict the POS 
tag of a word using distributional information? Our 
idea is to use ?supervised? learning, where we train 
and test on the seed set. Conceptually, we (1) train 
a multi-class classifier on the morphologically la-
beled words, each of which is represented by its 
context vector, and (2) apply the classifier to rela-
bel the same set of words. If the new label of a 
word w differs from its original label, then mor-
phology and context disagree upon the POS of w; 
and as mentioned above, our method then deter-
mines that the word is potentially misclassified. 
Note, however, that (1) the training instances are 
not perfectly labeled and (2) it does not make sense 
to train a classifier on data that is seriously misla-
beled. Hence, we make the assumption that a large 
percentage (> 70%) of the training instances is cor-
rectly labeled6, and that our method would work 
with a training set labeled at this level of accuracy. 
In addition, since we are training a classifier based 
on distributional features, we train and test on only 
distributionally reliable words, which we define to 
be words that appear at least five times in our cor-
pus. Distributionally unreliable words will all be 
removed from the morphologically formed clus-
ters, since we cannot predict their POS using dis-
tributional information.  
In our implementation of this method, rather 
than train a multi-class classifier, we train a set of 
binary classifiers using SVMlight (Joachims, 1999) 
together with the distributional features for deter-
mining the POS tag of a given word.7 More spe-
cifically, we train one classifier for each pair of 
                                                 
6
 An inspection of the morphologically formed clusters reveals 
that this assumption is satisfied for both languages. 
7
 In this and all subsequent uses of SVMlight, we set al the 
training parameters to their default values. 
222
POS tags. For instance, since we have ten POS 
tags for English, we will train 45 binary classifi-
ers.8 To determine the POS tag of a given English 
word w, we will use these 45 pairwise classifiers to 
independently assign a label to w. For instance, the 
NN-JJ classifier will assign either NN or JJ to w. 
We then count how many times w is tagged with 
each of the ten POS tags. If there is a POS tag t 
whose count is nine, it means that all the nine clas-
sifiers associated with t have classified w as t, and 
so our method will label w as t. Otherwise, we re-
move w from our seed set, since we cannot confi-
dently label it using our classifier ensemble. 
To create the training set for the NN-JJ classi-
fier, for instance, we can possibly use all of the 
words labeled with NN and JJ as positive and 
negative instances, respectively. However, to en-
sure that we do not have a skewed class distribu-
tion, we use the same number of instances from 
each class to train the classifier. More formally, let 
INN be the set of instances labeled with NN, and IJJ 
be the set of instances labeled with JJ. Without loss 
of generality, assume that |INN| < |IJJ|, where |X| de-
notes the size of the set X. To avoid class skew-
ness, we have to sample from IJJ, since it is the lar-
ger set. Our sampling method is motivated by bag-
ging (Breiman, 1996). More specifically, we create 
10 training sets from IJJ, each of which has size |INN| and is formed by sampling with replacement 
from IJJ. We then combine each of these 10 train-
ing sets separately with INN, and train 10 SVM 
classifiers from the 10 resulting training sets. 
Given a test instance i, we first apply the 10 classi-
fiers independently to i and obtain the signed con-
fidence values9 of the predictions provided by the 
classifiers. We then take the average of the 10 con-
fidence values, assigning i the positive class if the 
average is at least 0, and negative otherwise.   
As mentioned above, we use distributional fea-
tures to represent an instance created from a word 
w. The distributional features are created based on 
Sch?tze?s (1995) method. Specifically, the left 
context and the right context of w are each encoded 
using the most frequent 500 words from the vo-
cabulary. A feature in the left (right) context has 
                                                 
8
 We could have trained just one 10-class classifier, but the 
fairly large number of classes leads us to speculate that this 
multi-class classifier will not achieve a high accuracy. 
9
 Here, a large positive number indicates that the classifier 
confidently labels the instance as NN, and a large negative 
number represents confident prediction for JJ. 
the value 1 if the corresponding word appears to 
the left (right) of w in our corpus, and 0 otherwise. 
However, we found that using distributional fea-
tures alone would erroneously classify words like 
?car? and ?cars? as having the same POS because 
the two words are distributionally similar. In gen-
eral, it is difficult to distinguish words in NN from 
those in NNS by distributional means. The same 
problem occurs for words in VB and VBD. To ad-
dress this problem, we augment the feature set with 
suffixal features. Specifically, we create one binary 
feature for each of the 30 most frequent suffixes 
that we employed in the previous section. The fea-
ture corresponding to suffix x has the value 1 if x is 
the suffix of w. Moreover, we create an additional 
suffixal feature whose value is 1 if none of the 30 
most frequent suffixes is the suffix of w.  
6 Augmenting the Seed Set 
After purification, we have a set of clusters filled 
with distributionally and morphologically reliable 
seed words that receive the same POS tag when 
predicted independently by morphological features 
and distributional features. Our goal in this section 
is to augment this seed set. Since we have a small 
seed set (5K words for English and 8K words for 
Bengali) and a large number of unlabeled words, 
we believe that it is most natural to apply a weakly 
supervised learning algorithm to bootstrap the clus-
ters. Specifically, we employ a version of self-
training together with SVM as the underlying 
learning algorithm. 10  Below we first present the 
high-level idea of our self-training algorithm and 
then discuss the implementation details. 
Conceptually, our self-training algorithm works 
as follows. We first train a multi-class SVM classi-
fier on the seed set for determining the POS tag of 
a word using the morphological and distributional 
features described in the previous section, and then 
apply it to label the unlabeled (i.e., unclustered) 
words. Words that are labeled with a confidence 
value that exceeds the current threshold (which is 
initially set to 1 and -1 for positively and nega-
tively labeled instances, respectively) will be 
                                                 
10
 As a related note, Clark?s (2001) bootstrapping algorithm 
uses KL-divergence to measure the distributional similarity 
between an unlabeled word and a labeled word, adding to a 
cluster the words that are most similar to its current member. 
For us, SVM is a more appealing option because it automati-
cally combines the morphological and distributional features. 
223
added to the seed set.  In the next iteration, we re-
train the classifier on the augmented labeled data, 
apply it to the unlabeled data, and add to the la-
beled data those instances whose predicted confi-
dence is above the current threshold. If none of the 
instances has a predicted confidence above the cur-
rent threshold, we reduce the threshold by 0.1. (For 
instance, if the original thresholds are 1 and -1, 
they will be changed to 0.9 and -0.9.) We then re-
peat the above procedure until the thresholds reach 
0.5 and -0.5. 11  Finally, we apply the resulting 
bootstrapped classifier to label all of the unlabeled 
words that have a corpus frequency of at least five, 
using a threshold of 0. 
In our implementation of the self-training algo-
rithm, rather than train a multi-class classifier in 
each bootstrapping iteration, we train pairwise 
classifiers (recall that for English, 45 classifiers are 
formed from 10 POS tags) using the morphological 
and distributional features described in the previ-
ous section. Again, since we employ distributional 
features, we apply the 45 pairwise classifiers only 
to the distributionally reliable words (i.e., words 
with corpus frequency at least 5). To classify an 
unlabeled word w, we apply the 45 pairwise classi-
fiers to independently assign a label to w.12  We 
then count how many times w is tagged with each 
of the ten POS tags. If there is a POS tag whose 
count is nine and all of these nine votes are associ-
ated with confidence that exceeds the current 
threshold, then we add w to the labeled data to-
gether with its assigned tag.  
7 Evaluation 
7.1 Experimental Setup 
Corpora. Recall that our bootstrapping algorithm 
assumes as input an unannotated corpus from 
which we (1) extract our vocabulary (i.e., the set of 
words to be labeled) and (2) collect the statistics 
needed in morphological and distributional cluster-
                                                 
11
 We decided to stop the bootstrapping procedure at thresh-
olds of 0.5 and -0.5, because the more bootstrapping iterations 
we use, the lower are the quality of the bootstrapped data as 
well as the accuracy of the bootstrapped classifier.  
12
 As in purification, each pairwise classifier is implemented 
as a set of 10 classifiers, each of which is trained on an equal 
number of instances from both classes. Testing also proceeds 
as before: the label of an instance is derived from the average 
of the confidence values returned by the 10 classifiers, and the 
confidence value associated with the label is just the average 
of the 10 confidence values. 
ing. We use as our English corpus the Wall Street 
Journal (WSJ) portion of the Penn Treebank (Mar-
cus et al, 1993). Our Bengali corpus is composed 
of five years of articles taken from the Bengali 
newspaper Prothom Alo.  
Vocabulary creation. To extract our English vo-
cabulary, we pre-processed each document in the 
WSJ corpus by first tokenizing them and then re-
moving the most frequent 500 words (as they are 
mostly closed class words), capitalized words, 
punctuations, numbers, and unwanted character 
sequences (e.g., ?***?). The resulting English vo-
cabulary consists of approximately 35K words. We 
applied similar pre-processing steps to the Prothom 
Alo articles to generate our Bengali vocabulary, 
which consists of 80K words. 
Test set preparation. Our English test set is com-
posed of the 25K words in the vocabulary that ap-
pear at least five times in the WSJ corpus.  The 
gold-standard POS tags for each word w are de-
rived automatically from the parse trees in which w 
appears. To create the Bengali test set, we ran-
domly chose 5K words from the vocabulary that 
appear at least five times in Prothom Alo. Each 
word in the test set was then labeled with its POS 
tags by two of our linguists. 
Evaluation metric. Following Sch?tze (1995), we 
report performance in terms of recall, precision, 
and F1. Recall is the percentage of POS tags cor-
rectly proposed, precision is the percentage of POS 
tags proposed that are correct, and F1 is simply the 
harmonic mean of recall and precision. To exem-
plify, suppose the correct tagset for ?crowd? is 
{NN, VB}; if our system outputs {VB, JJ, RB}, 
then recall is 50%, precision is 33%, and F1 is 
40%.  Importantly, all of our results will be re-
ported on word types. This prevents the frequently 
occurring words from having a higher influence on 
the results than their infrequent counterparts. 
7.2 Results and Discussion 
The baseline system. We use as our baseline sys-
tem one of the best existing unsupervised POS in-
duction algorithms (Clark, 2003). More specifi-
cally, we downloaded from Clark?s website13 the 
code that implements a set of POS induction algo-
rithms he proposed. Among these implementa-
tions, we chose cluster_neyessenmorph, which 
combines morphological and distributional infor-
                                                 
13
 http://www.cs.rhul.ac.uk/home/alexc/ 
224
mation and achieves the best performance in his 
paper. When running his program, we use WSJ and 
Prothom Alo as the input corpora. In addition, we 
set the number of clusters produced to be 128, 
since this setting yields the best result in his paper. 
Results of the baseline system for the English and 
Bengali test sets are shown under the ?After Boot-
strapping? column in row 1 of Tables 3 and 4. As 
we can see, the baseline achieves F1-scores of 59% 
and 45% for English and Bengali, respectively. 
The other results in row 1 will be discussed below. 
Our induction system. Recall that our unsuper-
vised POS induction algorithm operates in three 
steps. To better understand the performance con-
tribution of each of these steps, we show in row 2 
of Tables 3 and 4 the results of our system after we 
(1) morphologically cluster the words, (2) purify 
the seed set, and (3) augment the seed set. Impor-
tantly, the numbers shown for each step are com-
puted over the set of words in the test set that are 
labeled at the end of that step. For instance, the 
morphological clustering algorithm labeled 11K 
English words and 25K Bengali words, and so re-
call, precision and F1-score are computed over the 
subset of these labeled words that appear in the test 
set. Similarly, after bootstrapping, all the words 
that appear at least five times in our corpus are la-
beled; since our labeled data is now a superset of 
our test data, the numbers in the last column are 
the results of our algorithm for the entire test set.  
As we can see, after morphological clustering, 
our system achieves F1-scores of 79% and 78% for 
English and Bengali, respectively. When measured 
on exactly the same set of words, the baseline only 
achieves F-scores of 59% and 56%. In fact, com-
paring rows 1 and 2, we outperform the baseline in 
each of the three steps of our algorithm. In particu-
lar, our system yields F1-scores of 73% and 77% 
for the entire English and Bengali test sets, thus 
outperforming the baseline by 14% and 18% for 
English and Bengali, respectively.  
Two additional points deserve mentioning. First, 
for both languages, the highest F1-score is 
achieved after the purification step. A closer analy-
sis of the labeled words reveals the reason. For 
English, many of the nouns incorrectly labeled as 
verbs by the morphological clustering algorithm 
were subsequently removed during the purification 
step when distributional similarity was used on top 
of morphological similarity. For Bengali, many 
proper nouns were assigned by the morphological 
clustering algorithm to the clusters dominated by 
common nouns (because the two types of Bengali 
nouns are morphologically similar), and many of 
these mislabeled proper nouns were subsequently 
removed during purification. Second, as expected, 
precision drops after the seed augmentation step, 
since the quality of the labeled data deteriorates as 
bootstrapping progresses. Nevertheless, with a lot 
more words labeled in the bootstrapping step, we 
still achieve F1-scores of 73% for English and 76% 
for Bengali.  
The remaining rows of the Tables 3 and 4 show 
the performance of our algorithm for each tag in 
our two POS tagsets. Different observations can be 
made for the two languages. For English, the poor 
results for VBZ and NNS can be attributed to the 
fact that it is not easy to distinguish between these 
two tags: ?s? is a typical suffix for words that are 
NNS and words that are the third person singular 
of a verb. In addition, results for verbs are better 
than those for nouns, since verbs are easier to iden-
tify using only morphological knowledge. 
For Bengali, results for adjectives are not good, 
since (1) adjectives and nouns have very similar 
distributional property in Bengali and (2) there are 
not enough suffixes to induce the adjectives mor-
phologically. Moreover, we achieve high precision 
but low recall for proper nouns. This implies that 
most of the words that our algorithm labels as 
proper nouns are indeed correct, but there are also 
many proper nouns that are mislabeled. A closer 
examination of the clusters reveals that many of 
these proper nouns are mislabeled as common 
nouns, presumably because these two types of 
Bengali nouns are morphologically and distribu-
tionally similar and therefore it is difficult to sepa-
rate them. We will leave the identification of Ben-
gali proper nouns as a topic for future research.   
7.3 Additional Experiments 
Labeling rare words with morphological infor-
mation. Although our discussion thus far has fo-
cused on words whose corpus frequency is at least 
five, it would be informative to examine how well 
our algorithm performs on rare, distributionally 
unreliable words (i.e., words with corpus fre-
quency less than five). Recall that our morphologi-
cal clustering algorithm also clusters rare words. In 
fact, these rare words comprise 15% of the English 
words and 18% of the Bengali words in our mor-
phological formed clusters. Perhaps more impor-
225
After Morphological Clustering After Purification After Bootstrapping  
P R F1 P R F1 P R F1 
Baseline 84.1 45.3 58.9 84.9 51.4 64.1 75.6 48.0 59.0 
Ours 85.9 74.0 79.4 89.3 74.4 81.7 80.4 66.8 73.1 
JJ 88.7 49.1 63.2 91.4 51.9 66.1 57.7 62.9 60.2 
JJR 91.1 86.2 88.6 92.1 92.0 92.0 62.1 83.1 71.0 
JJS 100 98.3 99.1 100 100 100 81.3 86.9 83.9 
NN 91.6 43.7 59.2 94.8 42.8 58.8 95.2 47.1 62.8 
NNS 90.6 39.2 53.5 93.5 41.3 57.2 96.6 44.7 60.9 
RB 100 76.1 86.4 100 82.2 90.6 98.8 63.5 77.3 
VB 74.0 97.7 84.1 79.8 96.0 87.1 65.7 92.8 76.9 
VBD 96.6 98.9 97.7 97.6 100 98.8 96.7 91.9 93.3 
VBG 89.9 100 94.7 91.1 100 95.7 90.8 93.5 92.1 
VBZ 60.9 99.9 74.7 65.1 96.8 77.7 52.8 92.6 67.3 
Table 3: POS induction results for English based on word type 
 
After Morphological Clustering After Purification After Bootstrapping  
P R F1 P R F1 P R F1 
Baseline 82.1 42.3 55.5 83.1 45.3 58.3 78.1 43.3 49.3 
Ours 74.1 81.3 77.5 83.4 78.0 80.7 74.1 79.2 76.6 
JJ 50.0 51.8 50.9 56.1 55.0 55.5 57.5 51.4 54.3 
NN 63.0 96.8 76.4 67.0 96.0 78.9 62.2 92.2 74.3 
NN2 96.3 100 98.1 99.0 100 99.5 99.0 99.0 99.0 
NN6 95.5 89.2 92.2 97.2 90.0 93.9 97.1 91.0 93.9 
NN7 88.4 94.1 89.7 92.1 99.2 93.1 90.1 78.7 84.1 
NNP 87.2 37.3 52.3 92.8 43.8 59.4 92.7 51.5 66.1 
NNS 62.7 93.1 75.0 66.8 93.5 77.9 65.2 94.1 77.1 
NNSH 91.0 100 95.6 91.0 100 95.7 91.0 100 95.7 
VB 68.9 93.0 79.2 77.0 94.6 84.9 73.9 91.8 81.9 
VBN 84.3 49.1 62.1 82.4 50.1 62.9 56.1 46.7 50.1 
Table 4: POS induction results for Bengali based on word type
 
tantly, when measuring performance on just these 
morphologically clustered rare words, our algo-
rithm achieves F1-scores of 81% and 79% for Eng-
lish and Bengali, respectively. These results pro-
vide empirical support for the claim that morpho-
logical information can be usefully employed to 
label rare words (Clark, 2003). 
Soft clustering. Many words have more than one 
POS tag. For instance, ?received? can be labeled as 
VBD and JJ. Although our morphological cluster-
ing algorithm can predict some of these ambigui-
ties, those are at the ?big? cluster level. At the sub-
cluster level, the algorithm imposes a hard cluster-
ing on the words. In other words, no word appears 
in more than one sub-cluster. 
Ideally, a POS induction algorithm should pro-
duce soft clusters due to lexical ambiguity. In fact, 
Jardino and Adda (1994), Sch?tze (1997) and 
Clark (2000) have attempted to address the ambi-
guity problem to a certain extent. We have also 
experimented with a very simple method for han-
dling ambiguity in our bootstrapping algorithm: 
when augmenting the seed set, instead of labeling a  
 
word with a tag that receives 9 votes from the 45 
pairwise classifiers, we label a word with any tag 
that receives at least 8 votes, effectively allowing 
the assignment of more than one label to a word. 
However, our experimental results (not shown due 
to space limitations) indicate that the incorporation 
of this method does not yield better overall per-
formance, since many of the additional labels are 
erroneous and hence their presence deteriorates the 
quality of the bootstrapped data.  
8 Conclusions 
We have proposed a new bootstrapping algorithm 
for unsupervised POS induction. In contrast to ex-
isting algorithms developed for this problem, our 
algorithm is designed to (1) operate under a re-
source-scarce setting in which no language-
specific tools or resources are available and (2) 
more tightly integrate morphological information 
with the distributional POS induction framework. 
In particular, our algorithm (1) improves the qual-
ity of the seed clusters by employing seed words 
226
that are distributionally and morphologically reli-
able and (2) uses support vector learning to com-
bine morphological and distributional information. 
Our results show that it outperforms Clark?s algo-
rithm for English and Bengali, suggesting that it is 
applicable to both morphologically impoverished 
and highly inflectional languages.  
Acknowledgements 
We thank the five anonymous EMNLP-CoNLL 
referees for their valuable comments. We also 
thank Zeeshan Abedin and Mahbubur Rahman 
Haque for creating the Bengali lexicon. 
References 
Chris Biemann. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In Pro-
ceedings of the COLING/ACL 2006 Student Research 
Workshop.  
Leo Breiman. 1996. Bagging predictors. Machine 
Learning 24(2):123-140. 
Alexander Clark. 2000. Inducing syntactic categories by 
context distributional clustering. In Proceedings of 
CoNLL, pages 91-94. 
Alexander Clark. 2003. Combining distributional and 
morphological information for part of speech induc-
tion. In Proceedings of the EACL.  
Mathias Creutz and Krista Lagus. 2005. Unsupervised 
morpheme segmentation and morphology induction 
from text corpora using Morfessor 1.0. In Computer 
and Information Science, Report A81, Helsinki Uni-
versity of Technology. 
Silviu Cucerzan and David Yarowsky. 2000. Language 
independent, minimally supervised induction of lexi-
cal probabilities. In Proceedings of the ACL, pages 
270-277. 
Sajib Dasgupta and Vincent Ng. 2007. High-
performance, language-independent morphological 
segmentation. In Proceedings of NAACL-HLT, pages 
155-163. 
Kevin Duh and Katrin Kirchhoff. 2006. Lexicon acqui-
sition for dialectal Arabic using transductive learn-
ing. In Proceedings of EMNLP, pages 399-407. 
Dayne Freitag. 2004. Toward unsupervised whole-
corpus tagging. In Proceedings of COLING, pages 
357-363. 
John Goldsmith. 2001. Unsupervised learning of the 
morphology of a natural language. In Computational 
Linguistics 27(2):153-198. 
Aria Haghighi and Dan Klein. 2006. Prototype-driven 
learning for sequence models. In Proceedings of 
HLT-NAACL, pages 320-327. 
Jan Haji. 2000. Morphological tagging: Data vs. dic-
tionaries. In Proceedings of the NAACL, pages 94-
101. 
Zellig Harris. 1954. Distributional structure. In Word, 
10(2/3):146-162. 
Michele Jardino and Gilles Adda. 1994. Automatic de-
termination of a stochastic bi-gram class language 
model. In Proceedings of Grammatical Inference and 
Applications, Second International Colloquium, 
ICGI-94, pages 57-65. 
Thorsten Joachims. 1999. Making large-scale SVM 
learning practical. In Advances in Kernel Methods ? 
Support Vector Learning, pages 44-56. MIT Press. 
Sydney Lamb. 1961. On the mechanization of syntactic 
analysis. In Proceedings of the 1961 Conference on 
Machine Translation of Languages and Applied Lan-
guage Analysis, Volume 2, pages 674-685. HMSO, 
London. 
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Bea-
trice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational 
Linguistics, 19(2):313-330. 
Andrei Mikheev. 1997. Automatic rule induction for 
unknown word-guessing. Computational Linguistics, 
23(3):405-423. 
Goutam Kumar Saha, Amiya Baran Saha, and Sudipto 
Debnath. 2004. Computer assisted Bangla words 
POS tagging. In Proceedings of the International 
Symposium on Machine Translation NLP and TSS 
(iTRANS, 2004). 
Hinrich Sch?tze. 1995. Distributional part-of-speech 
tagging. In Proceedings of the EACL, pages 141-148. 
Hinrich Sch?tze. 1997. Ambiguity Resolution in Lan-
guage Learning. CSLI Publications.  
Noah Smith and Jason Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data. In 
Proceedings of the ACL, pages 354-362. 
 
227
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 640?649,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Unsupervised Models for Coreference Resolution
Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
vince@hlt.utdallas.edu
Abstract
We present a generative model for unsuper-
vised coreference resolution that views coref-
erence as an EM clustering process. For
comparison purposes, we revisit Haghighi
and Klein?s (2007) fully-generative Bayesian
model for unsupervised coreference resolu-
tion, discuss its potential weaknesses and con-
sequently propose three modifications to their
model. Experimental results on the ACE data
sets show that our model outperforms their
original model by a large margin and com-
pares favorably to the modified model.
1 Introduction
Coreference resolution is the problem of identifying
which mentions (i.e., noun phrases) refer to which
real-world entities. The availability of annotated
coreference corpora produced as a result of the MUC
conferences and the ACE evaluations has prompted
the development of a variety of supervised machine
learning approaches to coreference resolution in re-
cent years. The focus of learning-based coreference
research has also shifted from the acquisition of a
pairwise model that determines whether two men-
tions are co-referring (e.g., Soon et al (2001), Ng
and Cardie (2002), Yang et al (2003)) to the de-
velopment of rich linguistic features (e.g., Ji et al
(2005), Ponzetto and Strube (2006)) and the ex-
ploitation of advanced techniques that involve joint
learning (e.g., Daume? III and Marcu (2005)) and
joint inference (e.g., Denis and Baldridge (2007))
for coreference resolution and a related extraction
task. The rich features, coupled with the increased
complexity of coreference models, have made these
supervised approaches more dependent on labeled
data and less applicable to languages for which lit-
tle or no annotated data exists. Given the growing
importance of multi-lingual processing in the NLP
community, however, the development of unsuper-
vised and weakly supervised approaches for the au-
tomatic processing of resource-scarce languages has
become more important than ever.
In fact, several popular weakly supervised learn-
ing algorithms such as self-training, co-training
(Blum and Mitchell, 1998), and EM (Dempster et
al., 1977) have been applied to coreference resolu-
tion (Ng and Cardie, 2003) and the related task of
pronoun resolution (Mu?ller et al, 2002; Kehler et
al., 2004; Cherry and Bergsma, 2005). Given a small
number of coreference-annotated documents and a
large number of unlabeled documents, these weakly
supervised learners aim to incrementally augment
the labeled data by iteratively training a classifier1
on the labeled data and using it to label mention
pairs randomly drawn from the unlabeled documents
as COREFERENT or NOT COREFERENT. However,
classifying mention pairs using such iterative ap-
proaches is undesirable for coreference resolution:
since the non-coreferent mention pairs significantly
outnumber their coreferent counterparts, the result-
ing classifiers generally have an increasing tendency
to (mis)label a pair as non-coreferent as bootstrap-
ping progresses (see Ng and Cardie (2003)).
Motivated in part by these results, we present a
generative, unsupervised model for probabilistically
1For co-training, a pair of view classifiers are trained; and
for EM, a generative model is trained instead.
640
inducing coreference partitions on unlabeled doc-
uments, rather than classifying mention pairs, via
EM clustering (Section 2). In fact, our model com-
bines the best of two worlds: it operates at the
document level, while exploiting essential linguistic
constraints on coreferent mentions (e.g., gender and
number agreement) provided by traditional pairwise
classification models.
For comparison purposes, we revisit a fully-
generative Bayesian model for unsupervised coref-
erence resolution recently introduced by Haghighi
and Klein (2007), discuss its potential weaknesses
and consequently propose three modifications to
their model (Section 3). Experimental results on the
ACE data sets show that our model outperforms their
original model by a large margin and compares fa-
vorably to the modified model (Section 4).
2 Coreference as EM Clustering
In this section, we will explain how we recast un-
supervised coreference resolution as EM clustering.
We begin by introducing some of the definitions and
notations that we will use in this paper.
2.1 Definitions and Notations
A mention can be a pronoun, a name (i.e., a proper
noun), or a nominal (i.e., a common noun). An en-
tity is a set of coreferent mentions. Given a docu-
ment D consisting of n mentions, m1, . . . ,mn, we
use Pairs(D) to denote the set of
(n
2
)
mention pairs,
{mij | 1 ? i < j ? n}, where mij is formed
from mentions mi and mj . The pairwise probabil-
ity formed from mi and mj refers to the probabil-
ity that the pair mij is coreferent and is denoted as
Pcoref (mij). A clustering of n mentions is an n x
n Boolean matrix C , where Cij (the (i,j)-th entry of
C) is 1 if and only if mentions mi and mj are coref-
erent. An entry in C is relevant if it corresponds
to a mention pair in Pairs(D). A valid clustering
is a clustering in which the relevant entries satisfy
the transitivity constraint. In other words, C is valid
if and only if (Cij = 1 ? Cjk = 1) =? Cik = 1
? 1 ? i < j < k ? n. Hence, a valid clustering
corresponds to a partition of a given set of mentions,
and the goal of coreference resolution is to produce
a valid clustering in which each cluster corresponds
to a distinct entity.
2.2 The Model
As mentioned previously, our generative model op-
erates at the document level, inducing a valid clus-
tering on a given document D. More specifically,
our model consists of two steps. It first chooses a
clustering C based on some clustering distribution
P (C), and then generates D given C:
P (D,C) = P (C)P (D | C).
To facilitate the incorporation of linguistic con-
straints defined on a pair of mentions, we represent
D by its mention pairs, Pairs(D). Now, assuming
that these mention pairs are generated conditionally
independently of each other given Cij ,
P (D | C) =
?
mij?Pairs(D)
P (mij | Cij).
Next, we represent mij as a set of seven features
that is potentially useful for determining whether mi
and mj are coreferent (see Table 1).2 Hence, we can
rewrite P (D | C) as
?
mij?Pairs(D)
P (m1ij , . . . ,m7ij | Cij),
where mkij is the value of the kth feature of mij .
To reduce data sparseness and improve the es-
timation of the above probabilities, we make con-
ditional independence assumptions about the gen-
eration of these feature values. Specifically, as
shown in the first column of Table 1, we di-
vide the seven features into three groups (namely,
strong coreference indicators, linguistic constraints,
and mention types), assuming that two feature
values are conditionally independent if and only
if the corresponding features belong to differ-
ent groups. With this assumption, we can de-
compose P (m1ij , . . .m7ij | Cij) into a product
of three probabilities: P (m1ij,m2ij ,m3ij | Cij),
P (m4ij ,m5ij,m6ij | Cij), and P (m7ij | Cij). Each of
these distributions represents a pair of multinomial
distributions, one for the coreferent mention pairs
(Cij = 1) and the other for the non-coreferent men-
tion pairs (Cij = 0). Hence, the set of parameters
of our model, ?, consists of P (m1,m2,m3 | c),
P (m4,m5,m6 | c), and P (m7 | c).
2See Soon et al (2001) for details on feature value compu-
tations. Note that all feature values are computed automatically.
641
Feature Type Feature ID Feature Description
Strong 1 STR MATCH T if neither of the two mentions is a pronoun and after discarding determiners,
Coreference the string denoting mention mi is identical to that of mention mj ; else F.
Indicators 2 ALIAS T if one mention is an acronym, an abbreviation, or a name variant of the
other; else F. For instance, Bill Clinton and President Clinton are aliases, so
are MIT and Massachusetts Institute of Technology.
3 APPOSITIVE T if the mentions are in an appositive relationship; else F.
Linguistic 4 GENDER T if the mentions agree in gender; F if they disagree; NA if gender information
Constraints for one or both mentions cannot be determined.
5 NUMBER T if the mentions agree in number; F if they disagree; NA if number informa-
tion for one or both mentions cannot be determined.
6 SEM CLASS T if the mentions have the same semantic class; F if they don?t; NA if the
semantic class information for one or both mentions cannot be determined.
Mention Types 7 NPTYPE the feature value is the concatenation of the mention type of the two mentions,
titj , where ti, tj ? { PRONOUN, NAME, NOMINAL }.
Table 1: Feature set for representing a mention pair. The first six features are relational features that test whether some
property P holds for the mention pair under consideration and indicate whether the mention pair is TRUE or FALSE
w.r.t. P; a value of NOT APPLICABLE is used when property P does not apply.
2.3 The Induction Algorithm
To induce a clustering C on a document D, we run
EM on our model, treating D as observed data and
C as hidden data. Specifically, we use EM to itera-
tively estimate the model parameters, ?, from doc-
uments that are probabilistically labeled (with clus-
terings) and apply the resulting model to probabilis-
tically re-label a document (with clusterings). More
formally, we employ the following EM algorithm:
E-step: Compute the posterior probabilities of the
clusterings, P (C|D,?), based on the current ?.
M-step: Using P (C|D,?) computed in the E-step,
find the ?? that maximizes the expected complete
log likelihood,
?
C P (C|D,?) log P (D,C|??).
We begin the induction process at the M-step.3 To
find the ? that maximizes the expected complete log
likelihood, we use maximum likelihood estimation
with add-one smoothing. Since P (C|D,?) is not
available in the first EM iteration, we instead use
an initial distribution over clusterings, P (C). The
question, then, is: which P (C) should we use? One
possibility is the uniform distribution over all (possi-
bly invalid) clusterings. Another, presumably better,
choice is a distribution that assigns non-zero prob-
ability mass to only the valid clusterings. Yet an-
other possibility is to set P (C) based on a docu-
ment labeled with coreference information. In our
experiments, we employ this last method, assigning
3Another possibility, of course, is to begin at the E-step by
making an initial guess at ?.
a probability of one to the correct clustering of the
labeled document (see Section 4.1 for details).
After (re-)estimating ? in the M-step, we proceed
to the E-step, where the goal is to find the condi-
tional clustering probabilities. Given a document
D, the number of coreference clusterings is expo-
nential in the number of mentions in D, even if
we limit our attention to those that are valid. To
cope with this computational complexity, we ap-
proximate the E-step by computing only the condi-
tional probabilities that correspond to the N most
probable coreference clusterings given the current
?. We identify the N most probable clusterings and
compute their probabilities as follows. First, using
the current ?, we reverse the generative model and
compute Pcoref (mij) for each mention pair mij in
Pairs(D). Next, using these pairwise probabilities,
we apply Luo et al?s (2004) Bell tree approach to
coreference resolution to compute the N -best clus-
terings and their probabilities (see Section 2.4 for
details). Finally, to obtain the required conditional
clustering probabilities for the E-step, we normalize
the probabilities assigned to the N -best clusterings
so that they sum to one.
2.4 Computing the N-Best Partitions
As described above, given the pairwise probabilities,
we use Luo et al?s (2004) algorithm to heuristically
compute the N -best clusterings (or, more precisely,
N -best partitions4) and their probabilities based on
4Note that Luo et al?s search algorithm only produces valid
clusterings, implying that the resulting N -best clusterings are
642
Input: M = {m1, ..., mn}: mentions, N : no. of best partitions
Output: N -best partitions
1: // initialize the data structures that store partial partitions
2: H1 := {PP := {[m1]}}, S(PP ) = 1
3: H2, ..., Hn = ?
4: for i = 2 to n
5: // process each partial partition
6: foreach PP ? Hi?1
7: // process each cluster in PP
8: foreach C ? PP
9: Extend PP to PP ? by linking mi to C
10: Compute S(PP ?)
11: Hi := Hi? {PP ?}
12: Extend PP to PP ? by putting mi into a new cluster
13: Compute S(PP ?)
14: Hi := Hi? {PP ?}
15: return N most probable partitions in Hn
Figure 1: Our implementation of Luo et al?s algorithm
the Bell tree. Informally, each node in a Bell tree
corresponds to an ith-order partial partition (i.e., a
partition of the first i mentions of the given docu-
ment), and the ith level of the tree contains all possi-
ble ith-order partial partitions. Hence, the set of leaf
nodes constitutes all possible partitions of all of the
mentions. The search for the N most probable parti-
tions starts at the root, and a partitioning of the men-
tions is incrementally constructed as we move down
the tree. Since an exhaustive search is computation-
ally infeasible, Luo et al employ a beam search pro-
cedure to explore only the most probable paths at
each step of the search process. Figure 1 shows our
implementation of this heuristic search algorithm.
The algorithm takes as input a set of n mentions
(and their pairwise probabilities), and returns the N
most probable partitionings of the mentions. It uses
data structures S and the Hi?s to store intermediate
results. Specifically, S(PP ) stores the score of the
partial partition PP . Hi is associated with the ith
level of the Bell tree, and is used to store the most
probable ith-order partial partitions. Each Hi has a
maximum size of 2N : if more than 2N partitions
are inserted into a given Hi, then only the 2N most
probable ones will be stored. This amounts to prun-
ing the search space by employing a beam size of
2N (i.e., expanding only the 2N most probable par-
tial partitions) at each step of the search.
The algorithm begins by initializing H1 with the
only partial partition of order one, {[m1]}, which
indeed partitions. This is desirable, as there is no reason for us
to put non-zero probability mass on invalid clusterings.
has a score of one (line 2). Then it processes the
mentions sequentially, starting with m2 (line 4).
When processing mi, it takes each partial partition
PP in Hi?1 and creates a set of ith-order parti-
tions by extending PP with mi in all possible ways.
Specifically, for each cluster C (formed by a subset
of the first i?1 mentions) in PP , the algorithm gen-
erates a new ith-order partition, PP ?, by linking mi
to C (line 9), and stores PP ? in Hi (line 11). The
score of PP ?, S(PP ?), is computed by using the
pairwise coreference probabilities as follows:
S(PP ?) = S(PP ) ? max
mk?C
Pcoref (mki).
Of course, PP can also be extended by putting mi
into a new cluster (line 12). This yields PP ?, an-
other partition to be inserted into Hi (line 14), and
S(PP ?) = ??S(PP )?(1? max
k?{1,...,i?1}
Pcoref (mki)),
where ? (the start penalty) is a positive constant (<
1) used to penalize partitions that start a new clus-
ter. After processing each of the n mentions using
the above steps, the algorithm returns the N most
probable partitions in Hn (line 15).
Our implementation of Luo et al?s search algo-
rithm differs from their original algorithm only in
terms of the number of pruning strategies adopted.
Specifically, Luo et al introduce a number of heuris-
tics to prune the search space in order to speed up the
search. We employ only the beam search heuristic,
with a beam size that is five times larger than theirs.
Our larger beam size, together with the fact that we
do not use other pruning strategies, implies that we
are searching through a larger part of the space than
them, thus potentially yielding better partitions.
3 Haghighi and Klein?s Coreference Model
To gauge the performance of our model, we com-
pare it with a Bayesian model for unsupervised
coreference resolution that was recently proposed by
Haghighi and Klein (2007). In this section, we will
give an overview of their model, discuss its weak-
nesses and propose three modifications to the model.
3.1 Notations
For consistency, we follow Haghighi and Klein?s
(H&K) notations. Z is the set of random variables
643
that refer to (indices of) entities. ?z is the set of
parameters associated with entity z. ? is the entire
set of model parameters, which includes all the ?z?s.
Finally, X is the set of observed variables (e.g., the
head of a mention). Given a document, the goal is
to find the most probable assignment of entity in-
dices to its mentions given the observed values. In
other words, we want to maximize P (Z|X). In a
Bayesian approach, we compute this probability by
integrating out all the parameters. Specifically,
P (Z|X) =
?
P (Z|X, ?)P (?|X)d?.
3.2 The Original H&K Model
The original H&K model is composed of a set of
models: the basic model and two other models
(namely, the pronoun head model and the salience
model) that aim to improve the basic model.5
3.2.1 Basic Model
The basic model generates a mention in a two-step
process. First, an entity index is chosen according to
an entity distribution, and then the head of the men-
tion is generated given the entity index based on an
entity-specific head distribution. Here, we assume
that (1) all heads H are observed and (2) a mention
is represented solely by its head noun, so nothing
other than the head is generated. Furthermore, we
assume that the head distribution is drawn from a
symmetric Dirichlet with concentration ?H . Hence,
P (Hi,j = h|Z,H?i,j) ? nh,z + ?H
where Hi,j is the head of mention j in document
i, and nh,z is the number of times head h is emit-
ted by entity index z in (Z,H?i,j).6 On the other
hand, since the number of entities in a document is
not known a priori, we draw the entity distribution
from a Dirichlet process with concentration ?, ef-
fectively yielding a model with an infinite number
of mixture components. Using the Chinese restau-
rant process representation (see Teh et al (2006)),
P (Zij = z|Z?i,j) ?
{
? , if z = znew
nz , otherwise
5H&K also present a cross-document coreference model,
but since it focuses primarily on cross-document coreference
and improves within-document coreference performance by
only 1.5% in F-score, we will not consider this model here.
6H?i,j is used as a shorthand for H ? {Hi,j}.
where nz is the number of mentions in Z?i,j labeled
with entity index z, and znew is a new entity index
not already in Z?i,j. To perform inference, we use
Gibbs sampling (Geman and Geman, 1984) to gen-
erate samples from this conditional distribution:
P (Zi,j |Z?i,j,H) ? P (Zi,j|Z?i,j)P (Hi,j |Z,H?i,j)
where the two distributions on the right are defined
as above. Starting with a random assignment of en-
tity indices to mentions, the Gibbs sampler itera-
tively re-samples an entity index according to this
posterior distribution given the current assignment.
3.2.2 Pronoun Head Model
Head generation in the basic model is too simplis-
tic: it has a strong tendency to assign the same en-
tity index to mentions having the same head. This is
particularly inappropriate for pronouns. Hence, we
need a different model for generating pronouns.
Before introducing this pronoun head model, we
need to augment the set of entity-specific param-
eters, which currently contains only a distribution
over heads (?hZ ). Specifically, we add distributions
?tZ , ?
g
Z , and ?nZ over entity properties: ?tZ is a
distribution over semantic types (PER, ORG, LOC,
MISC), ?gZ over gender (MALE, FEMALE, EITHER,
NEUTER), and ?nZ over number (SG, PL). We assume
that each of these distributions is drawn from a sym-
metric Dirichlet. A small concentration parameter
is used, since each entity should have a dominating
value for each of these properties.
Now, to estimate ?tZ , ?
g
Z , and ?nZ , we need to
know the gender, number, and semantic type of each
mention. For some mentions (e.g., ?he?), these
properties are easy to compute; for others (e.g., ?it?),
they are not. Whenever a mention has unobserved
properties, we need to fill in the missing values. We
could resort to sampling, but sampling these prop-
erties is fairly inefficient. So, following H&K, we
keep soft counts for each of these properties and use
them rather than perform hard sampling.
When an entity z generates a pronoun h using the
pronoun head model,7 it first generates a gender g, a
number n, and a semantic type t independently from
the distributions ?gz , ?nz , and ?tz; and then generates
h using the distribution P (H = h|G = g,N =
7While pronouns are generated by this pronoun head model,
names and nominals continue to be handled by the basic model.
644
n, T = t, ?). Note that this last distribution is a
global distribution that is independent of the chosen
entity index. ? is a parameter drawn from a symmet-
ric Dirichlet (with concentration ?P ) that encodes
our prior knowledge of the relationship between a
semantic type and a pronoun. For instance, given the
type PERSON, there is a higher probability of gener-
ating ?he? than ?it?. As a result, we maintain a list
of compatible semantic types for each pronoun, and
give a pronoun a count of (1 + ?P ) if it is compatible
with the drawn semantic type; otherwise, we give it
a count of ?P . In essence, we use this prior to prefer
the generation of pronouns that are compatible with
the chosen semantic type.
3.2.3 Salience Model
Pronouns typically refer to salient entities, so the
basic model could be improved by incorporating
salience. We start by assuming that each entity has
an activity score that is initially set to zero. Given
a set of mentions and an assignment of entity in-
dices to mentions, Z, we process the mentions in a
left-to-right manner. When a mention, m, is encoun-
tered, we multiply the activity score of each entity by
0.5 and add one to the activity score of the entity to
which m belongs. This captures the intuitive notion
that frequency and recency both play a role in deter-
mining salience. Next, we rank the entities based on
their activity scores and discretize the ranks into five
?salience? buckets S: TOP (1), HIGH (2?3), MID (4?
6), LOW (7+), and NONE. Finally, this salience in-
formation is used to modify the entity distribution:8
P (Zij = z|Z?i,j) ? nz ? P (Mi,j |Si,j,Z)
where Si,j is the salience value of the jth mention
in document i, and Mi,j is its mention type, which
can take on one of three values: pronoun, name, and
nominal. P (Mi,j |Si,j,Z), the distribution of men-
tion type given salience, was computed from H&K?s
development corpus (see Table 2). According to
the table, pronouns are preferred for salient entities,
whereas names and nominals are preferred for enti-
ties that are less active.
8Rather than having just one probability term on the right
hand side of the sampling equation, H&K actually have a prod-
uct of probability terms, one for each mention that appears later
than mention j in the given document. However, they acknowl-
edge that having the product makes sampling inefficient, and
decided to simplify the equation to this form in their evaluation.
Salience Feature Pronoun Name Nominal
TOP 0.75 0.17 0.08
HIGH 0.55 0.28 0.17
MID 0.39 0.40 0.21
LOW 0.20 0.45 0.35
NONE 0.00 0.88 0.12
Table 2: Posterior distribution of mention type given
salience (taken from Haghighi and Klein (2007))
3.3 Modifications to the H&K Model
Next, we discuss the potential weaknesses of H&K?s
model and propose three modifications to it.
Relaxed head generation. The basic model fo-
cuses on head matching, and is therefore likely to
(incorrectly) posit the large airport and the small
airport as coreferent, for instance. In fact, head
matching is a relatively inaccurate indicator of coref-
erence, in comparison to the ?strong coreference in-
dicators? shown in the first three rows of Table 1. To
improve H&K?s model, we replace head matching
with these three strong indicators as follows. Given
a document, we assign each of its mentions a head
index, such that two mentions have the same head
index if and only if at least one of the three strong
indicators returns a value of True. Now, instead of
generating a head, the head model generates a head
index, thus increasing the likelihood that aliases are
assigned the same entity index, for instance. Note
that this modification is applied only to the basic
model. In particular, pronoun generation continues
to be handled by the pronoun head model and will
not be affected. We hypothesize that this modifica-
tion would improve precision, as the strong indica-
tors are presumably more precise than head match.
Agreement constraints. While the pronoun head
model naturally prefers that a pronoun be generated
by an entity whose gender and number are compati-
ble with those of the pronoun, the entity (index) that
is re-sampled for a pronoun according to the sam-
pling equation for P (Zi,j |Z?i,j,H) may still not be
compatible with the pronoun with respect to gen-
der and number. The reason is that an entity in-
dex is assigned based not only on the head distri-
bution but also on the entity distribution. Since enti-
ties with many mentions are preferable to those with
few mentions, it is possible for the model to favor
the assignment of a grammatically incompatible en-
tity (index) to a pronoun if the entity is sufficiently
645
large. To eliminate this possibility, we enforce the
agreement constraints at the global level. Specifi-
cally, we sample an entity index for a given mention
with a non-zero probability if and only if the corre-
sponding entity and the head of the mention agree in
gender and number. We hypothesize that this modi-
fication would improve precision.
Pronoun-only salience. In Section 3.2.3, we mo-
tivate the need for salience using pronouns only,
since proper names can to a large extent be resolved
using string-matching facilities and are not particu-
larly sensitive to salience. Nominals (especially def-
inite descriptions), though more sensitive to salience
than names, can also be resolved by simple string-
matching heuristics in many cases (Vieira and Poe-
sio, 2000; Strube et al, 2002). Hence, we hypothe-
size that the use of salience for names and nominals
would adversely affect their resolution performance,
as incorporating salience could diminish the role of
string match in the resolution process, according to
the sampling equations. Consequently, we modify
H&K?s model by limiting the application of salience
to the resolution of pronouns only. We hypothesize
that this change would improve precision.
4 Evaluation
4.1 Experimental Setup
To evaluate our EM-based model and H&K?s model,
we use the ACE 2003 coreference corpus, which
is composed of three sections: Broadcast News
(BNEWS), Newswire (NWIRE), and Newspaper
(NPAPER). Each section is in turn composed of a
training set and a test set. Due to space limitations,
we will present evaluation results only for the test
sets of BNEWS and NWIRE, but verified that the
same performance trends can be observed on NPA-
PER as well. Unlike H&K, who report results us-
ing only true mentions (extracted from the answer
keys), we show results for true mentions as well as
system mentions that were extracted by an in-house
noun phrase chunker. The relevant statistics of the
BNEWS and NWIRE test sets are shown in Table 3.
Scoring programs. To score the output of the
coreference models, we employ the commonly-used
MUC scoring program (Vilain et al, 1995) and the
recently-developed CEAF scoring program (Luo,
2005). In the MUC scorer, recall is computed as
BNEWS NWIRE
Number of documents 51 29
Number of true mentions 2608 2630
Number of system mentions 5424 5197
Table 3: Statistics of the BNEWS and NWIRE test sets
the percentage of coreference links in the reference
partition that appear in the system partition; preci-
sion is computed in the same fashion as recall, ex-
cept that the roles of the reference partition and the
system partition are reversed. As a link-based scor-
ing program, the MUC scorer (1) does not reward
successful identification of singleton entities and (2)
tends to under-penalize partitions that have too few
entities. The entity-based CEAF scorer was pro-
posed in response to these two weaknesses. Specif-
ically, it operates by computing the optimal align-
ment between the set of reference entities and the
set of system entities. CEAF precision and recall
are both positively correlated with the score of this
optimal alignment, which is computed by summing
over each aligned entity pair the number of mentions
that appear in both entities of that pair. As a conse-
quence, a system that proposes too many entities or
too few entities will have low precision and recall.
Parameter initialization. We use a small amount
of labeled data for parameter initialization for the
two models. Specifically, for evaluations on the
BNEWS test data, we use as labeled data one
randomly-chosen document from the BNEWS train-
ing set, which has 58 true mentions and 102 system
mentions. Similarly for NWIRE, where the chosen
document has 42 true mentions and 72 system men-
tions. For our model, we use the labeled document
to initialize the parameters. Also, we set N (the
number of most probable partitions) to 50 and ? (the
start penalty used in the Bell tree) to 0.8, the latter
being recommended by Luo et al (2004).
For H&K?s model, we use the labeled data to tune
the concentration parameter ?. While H&K set ? to
0.4 without much explanation, a moment?s thought
reveals that the choice of ? should reflect the frac-
tion of mentions that appear in a singleton cluster.
We therefore estimate this value from the labeled
document, yielding 0.4 for true mentions (which is
consistent with H&K?s choice) and 0.7 for system
mentions. The remaining parameters, the ??s, are all
646
set to e?4, following H&K. In addition, as is com-
monly done in Bayesian approaches, we do not sam-
ple entities directly from the conditional distribution
P (Z|X); rather, we sample from this distribution
raised to the power exp cik?1 , where c=1.5, i is the
current iteration number that starts at 0, and k (the
number of sampling iterations) is set to 20. Finally,
due to sampling and the fact that the initial assign-
ment of entity indices to mentions is random, all the
reported results for H&K?s model are averaged over
five runs.
4.2 Results and Discussions
The Heuristic baseline. As our first baseline, we
employ a simple rule-based system that posits two
mentions as coreferent if and only if at least one of
the three strong coreference indicators listed in Ta-
ble 1 returns True. Results of this baseline, reported
in terms of recall (R), precision (P), and F-score (F)
using the MUC scorer and the CEAF scorer, are
shown in row 1 of Tables 4 and 5, respectively. Each
row in these tables shows performance using true
mentions and system mentions for the BNEWS and
NWIRE data sets. As we can see, (1) recall is gen-
erally low, since this simple heuristic can only iden-
tify a small fraction of the coreference relations; (2)
CEAF recall is consistently higher than MUC recall,
since CEAF also rewards successful identification of
non-coreference relations; and (3) precision for true
mentions is higher than that for system mentions,
since the number of non-coreferent pairs that satisfy
the heuristic is larger for system mentions.
The Degenerate EM baseline. Our second base-
line is obtained by running only one iteration of our
EM-based coreference model. Specifically, it starts
with the M-step by initializing the model parame-
ters using the labeled document, and ends with the
E-step by applying the resulting model (in combi-
nation with the Bell tree search algorithm) to ob-
tain the most probable coreference partition for each
test document. Since there is no parameter re-
estimation, this baseline is effectively a purely su-
pervised system trained on one (labeled) document.
Results are shown in row 2 of Tables 4 and 5.
As we can see, recall is consistently much higher
than precision, suggesting that the model has pro-
duced fewer entities than it should. Perhaps more
interestingly, in comparison to the Heuristic base-
line, Degenerate EM performs consistently worse
according to CEAF but generally better according to
MUC. This discrepancy stems from the aforemen-
tioned properties that MUC under-penalizes parti-
tions with too few entities, whereas CEAF lowers
both recall and precision when given such partitions.
Our EM-based coreference model. Our model
operates in the same way as the Degenerate EM
baseline, except that EM is run until convergence,
with the test set being used as unlabeled data for pa-
rameter re-estimation. Any performance difference
between our model and Degenerate EM can thus be
attributed to EM?s exploitation of the unlabeled data.
Results of our model are shown in row 3 of Tables
4 and 5. In comparison to Degenerate EM, MUC
F-score increases by 4-5% for BNEWS and 4-21%
for NWIRE; CEAF F-score increases even more dra-
matically, by 10-17% for BNEWS and 16-27% for
NWIRE. Improvements stem primarily from large
gains in precision and comparatively smaller loss in
recall. Such improvements suggest that our model
has effectively exploited the unlabeled data.
In comparison to the Heuristic baseline, we gener-
ally see increases in both recall and precision when
system mentions are used, and as a result, F-score
improves substantially by 7-15%. When true men-
tions are used, we still see gains in recall, but these
gains are accompanied by loss in precision. F-score
generally increases (by 2-22%), except for the case
with NWIRE where we see a 0.5% drop in CEAF
F-score as a result of a larger decrease in precision.
The Original H&K model. We use as our third
baseline the Original H&K model (see Section 3.2).
Results of this model are shown in row 4 of Tables
4 and 5.9 Overall, it underperforms our model by 6-
16% in MUC F-score and 6-14% in CEAF F-score,
due primarily to considerable drop in both recall and
precision in almost all cases.
The Modified H&K model. Next, we incorporate
our three modifications into the Original H&K base-
line one after the other. Results are shown in rows
5-7 of Tables 4 and 5. Several points deserve men-
tioning. First, the addition of each modification im-
proves the F-score for both true and system mentions
9The H&K results shown here are not directly comparable
with those reported in Haghighi and Klein (2007), since H&K
evaluated their system on the ACE 2004 coreference corpus.
647
Broadcast News (BNEWS) Newswire (NWIRE)
True Mentions System Mentions True Mentions System Mentions
Experiments R P F R P F R P F R P F
1 Heuristic Baseline 27.8 72.0 40.1 30.9 44.3 36.4 31.2 70.3 43.3 36.3 53.4 43.2
2 Degenerate EM Baseline 63.6 53.1 57.9 70.8 36.3 48.0 64.5 42.6 51.3 69.0 25.1 36.8
3 Our EM-based Model 56.1 71.4 62.8 42.4 66.0 51.6 47.0 68.3 55.7 55.2 60.6 57.8
4 Haghighi and Klein Baseline 49.4 60.2 54.3 50.8 40.7 45.2 44.7 55.5 49.5 43.0 40.9 41.9
5 + Relaxed Head Generation 53.0 65.4 58.6 48.3 45.7 47.0 45.1 62.5 52.4 40.9 50.0 45.0
6 + Agreement Constraints 53.6 68.7 60.2 50.4 47.5 48.9 44.6 63.7 52.5 41.7 51.2 46.0
7 + Pronoun-only Salience 56.8 68.3 62.0 52.2 53.0 52.6 46.8 66.2 54.8 44.3 57.3 50.0
8 Fully Supervised Model 53.7 70.8 61.1 53.0 70.3 60.4 52.0 69.6 59.6 53.1 70.5 60.6
Table 4: Results obtained using the MUC scoring program for the Broadcast News and Newswire data sets
Broadcast News (BNEWS) Newswire (NWIRE)
True Mentions System Mentions True Mentions System Mentions
Experiments R P F R P F R P F R P F
1 Heuristic Baseline 42.1 75.8 54.1 44.2 48.7 46.3 43.9 73.4 54.9 47.5 53.4 50.3
2 Degenerate EM Baseline 51.2 43.1 46.8 53.7 26.8 35.8 51.0 30.5 38.2 45.1 18.6 26.3
3 Our EM-based Model 53.3 60.5 56.7 47.5 59.6 52.9 49.2 60.7 54.4 53.5 52.1 52.8
4 Haghighi and Klein Baseline 43.7 48.8 46.1 46.0 33.9 39.0 45.5 51.7 48.4 44.6 39.2 41.7
5 + Relaxed Head Generation 45.8 52.4 48.9 45.4 39.6 42.3 46.0 57.0 50.9 44.5 48.3 46.3
6 + Agreement Constraints 51.8 60.5 55.8 50.6 43.8 47.0 47.8 60.1 53.2 46.5 50.4 48.4
7 + Pronoun-only Salience 53.9 59.9 56.7 52.3 49.9 51.1 49.6 62.8 55.4 47.4 55.7 51.2
8 Fully Supervised Model 55.0 63.3 58.8 56.2 64.2 59.9 54.7 64.7 59.3 56.5 65.4 60.6
Table 5: Results obtained using the CEAF scoring program for the Broadcast News and Newswire data sets
in both data sets using both scorers. These results
provide suggestive evidence that our modifications
are highly beneficial. The three modifications, when
applied in combination, improve Original H&K sub-
stantially by 5-8% in MUC F-score and 7-12% in
CEAF F-score, yielding results that compare favor-
ably to those of our model in almost all cases.
Second, the use of agreement constraints yields
larger improvements with CEAF than with MUC.
This discrepancy can be attributed to the fact that
CEAF rewards the correct identification of non-
coreference relations, whereas MUC does not. Since
agreement constraints are intended primarily for dis-
allowing coreference, they contribute to the success-
ful identification of non-coreference relations and as
a result yield gains in CEAF recall and precision.
Third, the results are largely consistent with our
hypothesis that these modifications enhance preci-
sion. Together, they improve the precision of the
Original H&K baseline by 8-16% (MUC) and 11-
16% (CEAF), yielding a coreference model that
compares favorably with our EM-based approach.
Comparison with a supervised model. Finally,
we compare our EM-based model with a fully super-
vised coreference resolver. Inspired by state-of-the-
art resolvers, we create our supervised classification
model by training a discriminative learner (the C4.5
decision tree induction system (Quinlan, 1993)) with
a diverse set of features (the 34 features described in
Ng (2007)) on a large training set (the entire ACE
2003 coreference training corpus), and cluster using
the Bell tree search algorithm. The fully supervised
results shown in row 8 of Tables 4 and 5 suggest that
our EM-based model has room for improvements,
especially when system mentions are used.
5 Conclusions
We have presented a generative model for unsuper-
vised coreference resolution that views coreference
as an EM clustering process. Experimental results
indicate that our model outperforms Haghighi and
Klein?s (2007) coreference model by a large margin
on the ACE data sets and compares favorably to a
modified version of their model. Despite these im-
provements, its performance is still not comparable
to that of a fully supervised coreference resolver.
A natural way to extend these unsupervised coref-
erence models is to incorporate additional linguis-
tic knowledge sources, such as those employed by
our fully supervised resolver. However, feature en-
gineering is in general more difficult for generative
models than for discriminative models, as the former
typically require non-overlapping features. We plan
to explore this possibility in future work.
648
Acknowledgments
We thank the three anonymous reviewers for their
comments on an earlier draft of the paper. This work
was supported in part by NSF Grant 0812261.
References
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of COLT, pages 92?100.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution. In
Proceedings of CoNLL, pages 88?95.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
HLT/EMNLP, pages 97?104.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39(1):1?38.
Pascal Denis and Jason Baldridge. 2007. Global, joint
determination of anaphoricity and coreference resolu-
tion using integer programming. In Proceedings of
NAACL/HLT, pages 236?243.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721?741.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of the ACL, pages 848?855.
Heng Ji, David Westbrook, and Ralph Grishman. 2005.
Using semantic relations to refine coreference deci-
sions. In Proceedings of HLT/EMNLP, pages 17?24.
Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-
sandr Simma. 2004. Competitive self-trained pronoun
interpretation In Proceedings of HLT-NAACL 2004:
Short Papers, pages 33?36.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the ACL, pages
135?142.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT/EMNLP, pages
25?32.
Christoph Mu?ller, Stefan Rapp, and Michael Strube.
2002. Applying co-training to reference resolution. In
Proceedings of the ACL, pages 352?359.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of IJCAI, pages 1689?
1694.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the ACL, pages 104?111.
Vincent Ng and Claire Cardie. 2003. Weakly supervised
natural language learning without redundant views. In
HLT-NAACL: Main Proceedings, pages 173?180.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of HLT/NAACL, pages 192?199.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Michael Strube, Stefan Rapp, and Christoph Mu?ller.
2002. The influence of minimum edit distance on ref-
erence resolution. In Proceedings of EMNLP, pages
312?319.
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1527?1554.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45?52.
Xiaofeng Yang, GuoDong Zhou, Jian Su, and Chew Lim
Tan. 2003. Coreference resolution using competitive
learning approach. In Proceedings of the ACL, pages
176?183.
649
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 580?589,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Topic-wise, Sentiment-wise, or Otherwise?
Identifying the Hidden Dimension for Unsupervised Text Classification
Sajib Dasgupta and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{sajib,vince}@hlt.utdallas.edu
Abstract
While traditional work on text clustering
has largely focused on grouping docu-
ments by topic, it is conceivable that a user
may want to cluster documents along other
dimensions, such as the author?s mood,
gender, age, or sentiment. Without know-
ing the user?s intention, a clustering al-
gorithm will only group documents along
the most prominent dimension, which may
not be the one the user desires. To ad-
dress this problem, we propose a novel
way of incorporating user feedback into
a clustering algorithm, which allows a
user to easily specify the dimension along
which she wants the data points to be clus-
tered via inspecting only a small number
of words. This distinguishes our method
from existing ones, which typically re-
quire a large amount of effort on the part
of humans in the form of document an-
notation or interactive construction of the
feature space. We demonstrate the viabil-
ity of our method on several challenging
sentiment datasets.
1 Introduction
Text clustering is one of the most important appli-
cations in Natural Language Processing (NLP). A
common approach to this problem consists of (1)
computing the similarity between each pair of doc-
uments, each of which is typically represented as a
bag of words; and (2) using an unsupervised clus-
tering algorithm to partition the documents. The
majority of existing work on text clustering has
focused on topic-based clustering, where high ac-
curacies can be achieved even for datasets with a
large number of classes (e.g., 20 Newsgroups).
On the other hand, there has been relatively lit-
tle work on sentiment-based clustering and the re-
lated task of unsupervised polarity classification,
where the goal is to cluster (or classify) a set of
documents (e.g., reviews) according to the po-
larity (e.g., ?thumbs up? or ?thumbs down?) ex-
pressed by the author in an unsupervised man-
ner. Despite the large amount of recent work on
sentiment analysis and opinion mining, much of
it has focused on supervised methods (e.g., Pang
et al (2002), Kim and Hovy (2004), Mullen and
Collier (2004)). One weakness of these existing
supervised polarity classification systems is that
they are typically domain- and language-specific.
Hence, when given a new domain or language,
one needs to go through the expensive process of
collecting a large amount of annotated data in or-
der to train a high-performance polarity classifier.
Some recent attempts have been made to leverage
existing sentiment corpora or lexica to automati-
cally create annotated resources for new domains
or languages. However, such methods require
the existence of either a parallel corpus/machine
translation engine for projecting/translating anno-
tations/lexica from a resource-rich language to the
target language (Banea et al, 2008; Wan, 2008),
or a domain that is ?similar? enough to the target
domain (Blitzer et al, 2007). When the target do-
main or language fails to meet this requirement,
sentiment-based clustering or unsupervised polar-
ity classification become appealing alternatives.
Unfortunately, to our knowledge, these tasks are
largely under-investigated in the NLP community.
Turney?s (2002) work is perhaps one of the most
notable examples of unsupervised polarity classi-
fication. However, while his system learns the se-
mantic orientation of the phrases in a review in an
unsupervised manner, this information is used to
predict the polarity of a review heuristically.
Despite its practical significance, sentiment-
based clustering is a challenging task. To illus-
trate its difficulty, consider the task of clustering
a set of movie reviews. Since each review may
contain a description of the plot and the author?s
580
sentiment, a clustering algorithm may cluster re-
views along either the plot dimension or the senti-
ment dimension; and without knowing the user?s
intention, they will be clustered along the most
prominent dimension. Assuming the usual bag-
of-words representation, the most prominent di-
mension will more likely be plot, as it is not un-
common for a review to be devoted almost exclu-
sively to the plot, with the author briefly express-
ing her sentiment only at the end of the review.
Even if the reviews contain mostly subjective ma-
terial, the most prominent dimension may still not
be sentiment, due to the fact that many reviews are
sentimentally ambiguous. Specifically, a reviewer
may have negative opinions on the actors but at the
same time talk enthusiastically about how much
she enjoyed the plot. The presence of both posi-
tive and negative sentiment-bearing words in these
reviews renders the sentiment dimension hidden
(i.e., less prominent) as far as clustering is con-
cerned. Therefore, there is no guarantee that the
clustering algorithm will automatically produce a
sentiment-based clustering of the reviews.
Hence, it is important for a user to provide feed-
back on the clustering process to ensure that the
reviews are clustered along the sentiment dimen-
sion, possibly in an interactive manner. One way
to do this would be to ask the user to annotate
a small number of reviews with polarity infor-
mation, possibly through an active learning pro-
cedure to minimize human intervention (Dredze
and Crammer, 2008). Another way would be to
have the user explicitly identify the relevant fea-
tures (in our case, the sentiment-bearing words) at
the beginning of the clustering process (Liu et al,
2004), or incrementally construct the set of rele-
vant features in an interactive fashion (Bekkerman
et al, 2007; Raghavan and Allan, 2007; Roth and
Small, 2009). In addition, the user may supply
constraints on which pairs of documents must or
must not appear in the same cluster (Wagstaff et
al., 2001), or simply tell the algorithm whether
two clusters should be merged or split during the
clustering process (Balcan and Blum, 2008). It is
worth noting that many of these feedback mech-
anisms were developed by machine learning re-
searchers for general clustering tasks and not for
sentiment-based clustering.
Our goal in this paper is to propose a novel
mechanism allowing a user to cluster a set of docu-
ments along the desired dimension, which may be
a hidden dimension, with very limited user feed-
back. In comparison to the aforementioned feed-
back mechanisms, ours is arguably much simpler:
we only require that the user select a dimension
by examining a small number of features for each
dimension, as opposed to having the user gener-
ate the feature space in an interactive manner or
identify clusters that need to be merged or split. In
particular, identifying clusters for merging or split-
ting in Balcan and Blum?s algorithm may not be as
easy as it appears: for each MERGE or SPLIT de-
cision the user makes, she has to sample a large
number of documents from the cluster(s), read
through the documents, and base her decision on
the extent to which the documents are (dis)similar
to each other. Perhaps more importantly, our hu-
man experiments involving five users indicate that
all of them can easily identify the sentiment di-
mension based on the features, thus providing sug-
gestive evidence that our method is viable.
In sum, our contributions in this paper are three-
fold. First, we propose a novel feedback mecha-
nism for clustering allowing a user to easily spec-
ify the dimension along which she wants data
points to be clustered and apply the mechanism
to the challenging, yet under-investigated problem
of sentiment-based clustering. Second, spectral
learning, which is the core of our method, has not
been applied extensively to NLP problems, and we
hope that our work can increase the awareness of
this powerful machine learning technique in the
NLP community. Finally, we demonstrate the via-
bility of our method not only by evaluating its per-
formance on sentiment datasets, but also via a set
of human experiments, which is typically absent
in papers that involve algorithms for incorporating
user feedback.
The rest of the paper is organized as follows.
Section 2 presents the basics of spectral clustering,
which will facilitate the discussion of our feedback
mechanism in Section 3. We describe our human
experiments and evaluation results on several sen-
timent datasets in Section 4, and present our con-
clusions in Section 5.
2 Spectral Clustering
When given a clustering task, an important ques-
tion to ask is: which clustering algorithm should
we use? A popular choice is k-means. Neverthe-
less, it is well-known that k-means has the major
drawback of not being able to separate data points
581
that are not linearly separable in the given feature
space (e.g., see Dhillon et al (2004) and Cai et al
(2005)). Spectral clustering algorithms were de-
veloped in response to this problem with k-means.
The central idea behind spectral clustering is to
(1) construct a low-dimensional space from the
original (typically high-dimensional) space while
retaining as much information about the original
space as possible, and (2) cluster the data points in
this low-dimensional space. The rest of this sec-
tion provides the details of spectral clustering.
2.1 Algorithm
Although there are several well-known spectral
clustering algorithms in the literature (e.g., Weiss
(1999), Shi and Malik (2000), Kannan et al
(2004)), we adopt the one proposed by Ng et al
(2002), as it is arguably the most widely-used. The
algorithm takes as input a similarity matrix S cre-
ated by applying a user-defined similarity function
to each pair of data points. Below are the main
steps of the algorithm:
1. Create the diagonal matrix D whose (i,i)-
th entry is the sum of the i-th row of S,
and then construct the Laplacian matrix L =
D
?1/2
SD
?1/2
.
2. Find the eigenvalues and eigenvectors of L.
3. Create a new matrix from the m eigenvectors
that correspond to the m largest eigenvalues.1
4. Each data point is now rank-reduced to a
point in the m-dimensional space. Normal-
ize each point to unit length (while retaining
the sign of each value).
5. Cluster the resulting data points using k-
means.
In essence, each dimension in the reduced space
is defined by exactly one eigenvector. The reason
why eigenvectors with large eigenvalues are used
is that they capture the largest variance in the data.
As a result, each of them can be thought of as re-
vealing an important dimension of the data.
2.2 Clustering with Eigenvectors
As Ng et al (2002) point out, ?different authors
still disagree on which eigenvectors to use, and
how to derive clusters from them?. There are two
common methods for deriving clusters using the
eigenvectors. These methods will serve as our
baselines in our evaluation.
1For brevity, we will refer to the eigenvector with the n-th
largest eigenvalue simply as the n-th eigenvector.
Method 1: Using the second eigenvector only
The first method is to use only the second eigen-
vector, e
2
, to partition the points. Besides reveal-
ing one of the most important dimensions of the
data, this eigenvector induces an intuitively ideal
partition of the data ? the partition induced by the
minimum normalized cut of the similarity graph2,
where the nodes are the data points and the edge
weights are the pairwise similarity values of the
points (Shi and Malik, 2000). Clustering in a one-
dimensional space is trivial: since we have a lin-
earization of the points, all we need to do is to
determine a threshold for partitioning the points.
However, we follow Ng et al (2002) and cluster
using 2-means in this one-dimensional space.
Method 2: Using m eigenvectors
Recall from Section 2.1 that after eigen-
decomposing the Laplacian matrix, each data
point is represented by m co-ordinates. In the
second method, we simply use 2-means to cluster
the data points in this m-dimensional space,
effectively exploiting all of the m eigenvectors.
3 Our Approach
As mentioned before, sentiment-based clustering
is challenging, in part due to the fact that the re-
views can be clustered along more than one di-
mension. In this section, we propose and incor-
porate a user feedback mechanism into a spec-
tral clustering algorithm, which makes it easy for
a user to specify the dimension along which she
wants to cluster the data points.
Recall that our method first applies spectral
clustering to reveal the most important dimensions
of the data, and then lets the user select the de-
sired dimension. To motivate the importance of
user feedback, it helps to understand why the two
baseline clustering algorithms described in Sec-
tion 2.2, which are also based on spectral meth-
ods but do not rely on user feedback, may not al-
ways yield a sentiment-based clustering. To be-
gin with, consider the first method, where only
the second eigenvector is used to induce the par-
tition. Recall that the second eigenvector reveals
the most prominent dimension of the data. Hence,
if sentiment is not the most prominent dimension
(which can happen if the non-sentiment-bearing
2Using the normalized cut (as opposed to the usual cut)
ensures that the size of the two clusters are relatively bal-
anced, avoiding trivial cuts where one cluster is empty and
the other is full. See Shi and Malik (2000) for details.
582
words outnumber the sentiment-bearing words in
the bag-of-words representation of a review), then
the resulting clustering of the reviews may not be
sentiment-oriented. A similar line of reasoning
can be used to explain why the second baseline
clustering algorithm, which clusters based on all
of the eigenvectors in the low-dimensional space,
may not always work well. Since each eigenvector
corresponds to a different dimension (and, in par-
ticular, some of them correspond to non-sentiment
dimensions), using all of them to represent a re-
view may hamper the accurate computation of the
similarity of two reviews as far as clustering along
the sentiment dimension is concerned. In the rest
of this section, we discuss the major steps of our
user-feedback mechanism in detail.
Step 1: Identify the important dimensions
To identify the important dimensions of the given
reviews, we take the top eigenvectors computed
from the eigen-decomposition of the Laplacian
matrix, which is in turn formed from the input sim-
ilarity matrix. We compute the similarity between
two reviews by taking the dot product of their fea-
ture vectors (see Section 4.1 for details on feature
vector generation). Following Ng et al, we set the
diagonal entries of the similarity matrix to 0.
Step 2: Identify the relevant features
Given the eigen-decomposition from Step 1, we
first obtain the second through the fifth eigenvec-
tors3, which as mentioned above, correspond to
the most important dimensions of the data. Then,
we ask the user to select one of the four dimen-
sions defined by these eigenvectors according to
their relevance to sentiment. One way to do this
is to (1) induce one partition of the reviews from
each of the four eigenvectors, using a procedure
identical to Method 1 in Section 2.2, and (2) have
the user inspect the four partitions and decide
which corresponds most closely to a sentiment-
based clustering. The main drawback associated
with this kind of user feedback is that the user may
have to read a large number of reviews in order to
make a decision. Hence, to reduce human effort,
we employ an alternative procedure: we (1) iden-
tify the most informative features for characteriz-
ing each partition, and (2) have the user inspect
just the features rather than the reviews.
While traditional feature selection techniques
such as log-likelihood ratio and information
3The first eigenvector is not used because it is a constant
vector, meaning that it cannot be used to partition the data.
gain can be applied to identify these informa-
tive features (see Yang and Pedersen (1997)
for an overview), we employ a more sophisti-
cated feature-ranking method that we call max-
imum margin feature ranking (MMFR). Recall
that a maximum margin classifier (e.g., a support
vector machine) separates data points from two
classes while maximizing the margin of separa-
tion. Specifically, a maximum margin hyperplane
is defined by w ? x ? b = 0, where x is a fea-
ture vector representing an arbitrary data point,
and w (a weight vector) and b (a scalar) are pa-
rameters that are learned by solving the following
constrained optimization problem:
argmin
1
2
?w?
2
+ C
?
i
?
i
subject to
c
i
(w ? x
i
? b) ? 1? ?
i
, 1 ? i ? n,
where c
i
? {+1,?1} is the class of the i-th train-
ing point x
i
, ?
i
is the degree of misclassification
of x
i
, and C is a regularization parameter that bal-
ances training error and model complexity.
We use w to identify the most informative fea-
tures for a partition. Note that a feature with a
large positive weight is strongly indicative of the
positive class, whereas a feature with a large neg-
ative weight is strongly indicative of the negative
class. In other words, the most informative fea-
tures are those with large absolute weight values.
We exploit this observation and identify the most
informative features for a partition by (1) training
an SVM classifier4 on the partition, where data
points in the same cluster belong to the same class;
(2) sorting the features according to the SVM-
learned feature weights; and (3) generating two
ranked lists of informative features using the top
and bottom 100 features, respectively.
Given the ranked lists generated for each of the
four partitions, the user will select one of the parti-
tions/dimensions as most relevant to sentiment by
inspecting as many features in the ranked lists as
needed. After picking the most relevant dimen-
sion, the user will label one of the two feature lists
associated with this dimension as POSITIVE and
the other as NEGATIVE. Since each feature list
represents one of the clusters, the cluster associ-
ated with the positive list is labeled POSITIVE and
4All the SVM classifiers in this paper are trained using
the SVMlight package (Joachims, 1999), with the learning
parameters set to their default values.
583
the cluster associated with the negative list is la-
beled NEGATIVE.
In comparison to existing user feedback mech-
anisms for assisting a clustering algorithm, ours
requires comparatively little human intervention:
we only require that the user select a dimension by
examining a small number of features, as opposed
to having the user construct the feature space or
identify clusters that need to be merged or split as
is required with other methods.
Step 3: Identify the unambiguous reviews
There is a caveat, however. As mentioned in the
introduction, many reviews contain both positive
and negative sentiment-bearing words. These am-
biguous reviews are more likely to be clustered
incorrectly than their unambiguous counterparts.
Now, since the ranked lists of features are derived
from the partition, the presence of these ambigu-
ous reviews can adversely affect the identification
of informative features using MMFR. As a result,
we remove the ambiguous reviews before deriving
informative features from a partition.
We employ a simple method for identifying un-
ambiguous reviews. In the computation of eigen-
values, each data point factors out the orthogo-
nal projections of each of the other data points
with which they have an affinity. Ambiguous data
points receive the orthogonal projections from
both the positive and negative data points, and
hence they have near zero values in the pivot
eigenvectors. We exploit this important informa-
tion. The basic idea is that the data points with
near zero values in the eigenvectors are more am-
biguous than those with large absolute values. As
a result, we posit 250 reviews from each cluster
whose corresponding values in the eigenvector are
farthest away from zero as unambiguous, and in-
duce the ranked list of features only from the re-
sulting 500 unambiguous reviews.5
Step 4: Cluster along the selected dimension
Finally, we employ the 2-means algorithm to clus-
ter all the reviews along the dimension (i.e., the
eigenvector) selected by the user, regardless of
whether a review is ambiguous or not.
5Note that 500 is a somewhat arbitrary choice. Under-
lying this choice is our assumption that a fraction of the re-
views is unambiguous. As we will see in the evaluation sec-
tion, these 500 reviews can be classified with a high accuracy;
consequently, the features induced from the resulting clus-
ters are also of high quality. Additional experiments reveal
that the list of top-ranking features does not change signifi-
cantly when induced from a smaller number of unambiguous
reviews.
4 Evaluation
4.1 Experimental Setup
Datasets. We use five sentiment classification
datasets, including the widely-used movie review
dataset [MOV] (Pang et al, 2002) as well as four
datasets containing reviews of four different types
of products from Amazon [books (BOO), DVDs
(DVD), electronics (ELE), and kitchen appliances
(KIT)] (Blitzer et al, 2007). Each dataset has
2000 labeled reviews (1000 positives and 1000
negatives). To illustrate the difference between
topic-based clustering and sentiment-based clus-
tering, we will also show topic-based clustering
results on POL, a dataset created by taking all the
documents from two sections of 20 Newsgroups,
namely, sci.crypt and talks.politics.
To preprocess a document, we first tokenize and
downcase it, and then represent it as a vector of
unigrams, using frequency as presence. In ad-
dition, we remove from the vector punctuation,
numbers, words of length one, and words that oc-
cur in only a single review. Following the common
practice in the information retrieval community,
we also exclude words with high document fre-
quency, many of which are stopwords or domain-
specific general-purpose words (e.g., ?movies? in
the movie domain). A preliminary examination
of our evaluation datasets reveals that these words
typically comprise 1?2% of a vocabulary. The de-
cision of exactly how many terms to remove from
each dataset is subjective: a large corpus typically
requires more removals than a small corpus. To be
consistent, we simply sort the vocabulary by doc-
ument frequency and remove the top 1.5%.
Evaluation metrics. We employ two evaluation
metrics. First, we report results in terms of the ac-
curacy achieved on the 2000 labeled reviews for
each dataset. Second, following Kamvar et al
(2003), we evaluate the clusters produced by our
approach against the gold-standard clusters using
the Adjusted Rand Index (ARI). ARI ranges from
?1 to 1; better clusterings have higher ARI values.
4.2 Baseline Systems
Clustering using the second eigenvector only.
As our first baseline, we adopt Shi and Malik?s ap-
proach and cluster the reviews using only the sec-
ond eigenvector, e
2
, as described in Section 2.2.
Results on POL and the five sentiment datasets are
584
Accuracy Adjusted Rand Index
System Variation POL MOV KIT BOO DVD ELE POL MOV KIT BOO DVD ELE
Baseline: 2nd eigenvector 93.7 70.9 69.7 58.9 55.3 50.8 0.76 0.17 0.15 0.03 0.01 0.01
Baseline: m eigenvectors 95.9 59.3 63.2 60.1 62.5 63.8 0.84 0.03 0.07 0.04 0.06 0.08
Our approach 93.7 70.9 69.7 69.5 70.8 65.8 0.76 0.17 0.15 0.15 0.17 0.10
Table 1: Results in terms of accuracy and Adjusted Rand Index for the six datasets.
shown in row 1 of Table 1.6 As we can see, this
baseline achieves an accuracy of 90% on POL, but
a much lower accuracy (of 50?70%) on the sen-
timent datasets. The same performance trend can
be observed with ARI. These results provide sup-
port for the claim that sentiment-based clustering
is more difficult than topic-based clustering.
In addition, it is worth noting that the base-
line achieves much lower accuracies and ARI val-
ues on BOO, DVD, and ELE than on the re-
maining two sentiment datasets. Since e
2
cap-
tures the most prominent dimension, these results
suggest that sentiment dimension is not the most
prominent dimension in these three datasets. In
fact, this is intuitively plausible. For instance,
in the book domain, positive book reviews typ-
ically contain a short description of the content,
with the reviewer only briefly expressing her sen-
timent somewhere in the review. Similarly for the
electronics domain: electronic product reviews are
typically aspect-oriented, with the reviewer talk-
ing about the pros and cons of each aspect of the
product (e.g., battery, durability). Since the re-
views are likely to contain both positive and nega-
tive sentiment-bearing words, the sentiment-based
clustering is unlikely to be captured by e
2
.
Clustering using top five eigenvectors. As our
second baseline, we represent each data point
using the top five eigenvectors (i.e., e
1
through
e
5
), and cluster them using 2-means in this 5-
dimensional space, as described in Section 2.2.
Hence, this can be thought of as an ?ensemble?
approach, where the clustering decision is collec-
tively made by the five eigenvectors.
Results are shown in row 2 of Table 1. In
comparison to the first baseline, we see improve-
ments in accuracy and ARI for the three datasets
on which the first baseline performs poorly (i.e.,
BOO, DVD, and ELE), with the most drastic
improvement observed on ELE. On the other
hand, performance on the remaining two senti-
6Owing to the randomness in the choice of seeds for 2-
means, these and all other experimental results involving 2-
means are averaged over ten independent runs.
ment datasets deteriorates. These results can be
attributed to the fact that for BOO, DVD, and
ELE, e
2
does not capture the sentiment dimension,
but since some other eigenvector in the ensemble
does, we see improvements. On the other hand, e
2
has already captured the sentiment dimension in
MOV and KIT; as a result, employing additional
dimensions, which may not be sentiment-related,
may only introduce noise into the computation of
the similarities between the reviews.
4.3 Our Approach
Human experiments. Unlike the two baselines,
our approach requires users to specify which of the
four dimensions (defined by the second through
fifth eigenvectors) are most closely related to sen-
timent by inspecting a set of features derived from
the unambiguous reviews for each dimension us-
ing MMFR. To better understand how easy it is
for a human to select the desired dimension given
the features, we performed the experiment inde-
pendently with five humans (all of whom are com-
puter science graduate students not affiliated with
this research) and computed the agreement rate.
More specifically, for each dataset, we showed
each human judge the top 100 features for each
cluster according to MMFR (see Tables 4?6 for
a snippet). In addition, we informed them of the
intended dimension: for example, for POL, the
judge was told that the intended clustering is Poli-
tics vs. Science. Also, if she determined that more
than one dimension was relevant to the intended
clustering, she was instructed to rank these dimen-
sions in terms of their degree of relevance, where
the most relevant one would appear first in the list.
The dimensions (expressed in terms of the IDs
of the eigenvectors) selected by each of the five
judges for each dataset are shown in Table 2. The
agreement rate (shown in the last row of the ta-
ble) was computed based on only the highest-
ranked dimension selected by each judge. As we
can see, perfect agreement is achieved for four of
the five sentiment datasets, and for the remaining
two datasets, near-perfect agreement is achieved.
585
Judge POL MOV KIT BOO DVD ELE
1 2,3,4 2 2 4 3 3
2 2,4 2 2 4 3 3
3 4 2,4 4 4 3 3
4 2,3 2 2 4 3 3,4
5 2 2 2 4 3 3
Agr 80% 100% 80% 100% 100% 100%
Table 2: Human agreement rate.
POL MOV KIT BOO DVD ELE
Acc 99.8 87.0 87.6 86.2 87.4 77.6
Table 3: Accuracies on unambiguous documents.
These results together with the fact that it took 5?
6 minutes to identify the relevant dimension, indi-
cate that asking a human to determine the intended
dimension based on solely the ?informative? fea-
tures is a viable task.
Clustering results. Next, we cluster all 2000
documents for each dataset using the dimension
selected by the majority of the human judges. The
clustering results are shown in row 3 of Table 1. In
comparison to the better baseline for each dataset,
we see that our approach performs substantially
better on BOO, DVD and ELE, at almost the same
level on MOV and KIT, but slightly worse on POL.
Note that the improvements observed for BOO,
DVD and ELE can be attributed to the failure of e
2
to capture the sentiment dimension. Perhaps most
importantly, by exploiting human feedback, our
approach has achieved more stable performance
across the datasets than the baselines, with accura-
cies ranging from 65.8% to 93.7% and ARI rang-
ing from 0.10 to 0.76.
Role of unambiguous documents. Recall that
the features with the largest MMFR were com-
puted from the unambiguous documents only. To
get an intuitive understanding of the role of unam-
biguous documents in our approach, we show in
Table 3 the accuracy when the unambiguous doc-
uments in each dataset were clustered using the
eigenvector selected by the majority of the judges.
As we can see, the accuracy of each dataset is
higher than the corresponding accuracy shown in
row 3 of Table 1. In fact, an accuracy of more than
85% was achieved on all but one dataset. This sug-
gests that our method of identifying unambiguous
documents is useful.
Note that it is crucial to be able to achieve a high
accuracy on the unambiguous documents: if clus-
tering accuracy is low, the features induced from
the clusters may not be an accurate representation
of the corresponding dimension, and the human
judge may have a difficult time identifying the in-
tended dimension. In fact, some human judges re-
ported difficulty in identifying the correct dimen-
sion for the ELE dataset, and this can be attributed
in part to the low accuracy achieved on the unam-
biguous documents.
Features as summary. Recall that the method
we proposed represents each dimension with a
small number of features and asks a user to se-
lect the desired dimension by inspecting the corre-
sponding feature lists. In other words, each feature
list serves as a ?summary? of its corresponding di-
mension, and inspecting the features induced for
each dimension can give us insights into the dif-
ferent dimensions of a dataset. Hence, if a user is
not sure how she wants the data points to be clus-
tered (due to lack of knowledge of the data, for
instance), our automatically induced features may
serve as an overview of the different dimensions
of the data. To better understand whether these
features can indeed provide a user with additional
useful information about a dataset, we show in Ta-
bles 4?6 the top ten features induced for each clus-
ter and each dimension for the six datasets. As an
example, consider the MOV dataset. Inspecting
the induced features, we can determine that it has
a sentiment dimension (e
2
), as well as a humor vs.
thriller dimension (e
4
). In other words, if we clus-
ter along e
2
, we get a sentiment-based clustering;
and if we cluster along e
4
, we obtain a genre-based
(humor vs. thriller) clustering.
User feedback vs. labeled data. Recall that our
two baselines are unsupervised, whereas our ap-
proach can be characterized as semi-supervised, as
it relies on user feedback to select the intended di-
mension. Hence, it should not be surprising to see
that the average clustering performance of our ap-
proach is better than that of the baselines.
To do a fairer comparison, we conduct another
experiment in which we compare our approach
against a semi-supervised sentiment classification
system, which uses transductive SVM as the un-
derlying semi-supervised learner. More specifi-
cally, the goal of this experiment is to determine
how many labeled documents are needed in or-
der for the transductive learner to achieve the same
level of performance as our approach. To answer
this question, we first give the transductive learner
access to the 2000 documents for each dataset as
586
POL MOV
e
2
e
3
e
4
e
5
e
2
e
3
e
4
e
5
C
1
C
1
C
1
C
1
C
1
C
1
C
1
C
1
serder beyer serbs escrow relationship production jokes starts
armenian arabs palestinians serial son earth kids person
turkey andi muslims algorithm tale sequences live saw
armenians research wrong chips husband aliens animation feeling
muslims israelis department ensure perfect war disney lives
sdpa tim bosnia care drama crew animated told
argic uci live strong focus alien laughs happen
davidian ab matter police strong planet production am
dbd@ura z@virginia freedom omissions beautiful horror voice felt
troops holocaust politics excepted nature evil hilarious happened
C
2
C
2
C
2
C
2
C
2
C
2
C
2
C
2
sternlight escrow standard internet worst sex thriller comic
wouldn sternlight sternlight uucp stupid romantic killer sequences
pgp algorithm des uk waste school murder michael
crypto access escrow net bunch relationship crime supporting
algorithm net employer quote wasn friends police career
isn des net ac video jokes car production
likely privacy york co worse laughs dead peter
access uk jake didn boring sexual killed style
idea systems code ai guess cute starts latest
cryptograph pgp algorithm mit anyway mother violence entertaining
Table 4: Top ten features induced for each dimension for the POL and MOV domains. The shaded columns
correspond to the dimensions selected by the human judges. e
2
, . . ., e
5
are the top eigenvectors; C
1
and C
2
are the clusters.
BOO ELE
e
2
e
3
e
4
e
5
e
2
e
3
e
4
e
5
C
1
C
1
C
1
C
1
C
1
C
1
C
1
C
1
history series loved must mouse music easy amazon
must man highly wonderful cable really used cable
modern history easy old cables ipod card card
important character enjoyed feel case too fine recommend
text death children away red little using dvd
reference between again children monster headphones problems camera
excellent war although year picture hard fine fast
provides seems excellent someone kit excellent drive far
business political understand man overall need computer printer
both american three made paid fit install picture
C
2
C
2
C
2
C
2
C
2
C
2
C
2
C
2
plot buy money boring working worked money phone
didn bought bad series never problem worth off
thought information nothing history before never amazon worked
boring easy waste pages phone item over power
got money buy information days amazon return battery
character recipes anything between headset working years unit
couldn pictures doesn highly money support much set
ll look already page months months headphones phones
ending waste instead excellent return returned sony range
fan copy seems couldn second another received little
Table 5: Top ten features induced for each dimension for the BOO and ELE domains. The shaded columns
correspond to the dimensions selected by the human judges. e
2
, . . ., e
5
are the top eigenvectors; C
1
and C
2
are the clusters.
unlabeled data. Next, we randomly sample 50 un-
labeled documents and assign them the true label.
We then re-train the classifier and compute its ac-
curacy on the 2000 documents. We keep adding
more labeled data (50 in each iteration) until it
reaches the accuracy achieved by our system. Re-
sults of this experiment are shown in Table 7. Ow-
ing in the randomness involved in the selection of
unlabeled documents, these results are averaged
over ten independent runs. As we can see, our
587
KIT DVD
e
2
e
3
e
4
e
5
e
2
e
3
e
4
e
5
C
1
C
1
C
1
C
1
C
1
C
1
C
1
C
1
love works really pan worth music video money
clean water nice oven bought collection music quality
nice clean works cooking series excellent found video
size work too made money wonderful feel worth
set ice quality pans season must bought found
kitchen makes small better fan loved workout version
easily thing sturdy heat collection perfect daughter picture
sturdy need little cook music highly recommend waste
recommend keep think using tv makes our special
price best item clean thought special disappointed sound
C
2
C
2
C
2
C
2
C
2
C
2
C
2
C
2
months price ve love young worst series saw
still item years coffee between money cast watched
back set love too actors thought fan loved
never ordered never recommend men boring stars enjoy
worked amazon clean makes cast nothing original whole
money gift months over seems minutes comedy got
did got over size job waste actors family
amazon quality pan little beautiful saw worth series
return received been maker around pretty classic season
machine knives pans cup director reviews action liked
Table 6: Top ten features induced for each dimension for the KIT and DVD domains. The shaded columns
correspond to the dimensions selected by the human judges. e
2
, . . ., e
5
are the top eigenvectors; C
1
and C
2
are the clusters.
POL MOV KIT BOO DVD ELE
# labels 400 150 200 350 350 200
Table 7: Transductive SVM results.
user feedback is equivalent to the effort of hand-
annotating 275 documents per dataset on average.
Multiple relevant dimensions. As seen from
Table 2, some human judges selected more than
one dimension for some datasets (e.g., 2,3,4 for
POL; 2,4 for MOV; and 3,4 for ELE). However,
we never took into account these ?extra? dimen-
sions in our previous experiments. To better un-
derstand whether these extra dimensions can help
improve accuracy and ARI, we conduct another
experiment in which we apply 2-means to clus-
ter the documents in a space that is defined by
all of the selected dimensions. The final accu-
racy turns out to be 95.9%, 70.9%, and 67.5% for
POL, MOV, and ELE respectively, which is con-
siderably better than using only the optimal di-
mension and suggests that the extra dimensions
contain useful information.
5 Conclusions
Unsupervised clustering algorithms typically
group objects along the most prominent di-
mension, in part owing to their objective of
simultaneously maximizing inter-cluster similar-
ity and intra-cluster dissimilarity. Hence, if the
user?s intended clustering dimension is not the
most prominent dimension, these unsupervised
clustering algorithms will fail miserably. To
address this problem, we proposed to integrate a
novel user feedback mechanism into a spectral
clustering algorithm, which allows us to mine
the intended, possibly hidden, dimension of the
data and produce the desired clustering. This
mechanism differs from competing methods in
that it requires very limited feedback: to select the
intended dimension, the user only needs to inspect
a small number of features. We demonstrated its
viability via a set of human and automatic experi-
ments with unsupervised sentiment classification,
obtaining promising results.
In future work, we plan to explore several ex-
tensions to our proposed method. First, we plan to
use our user-feedback method in combination with
existing methods (e.g., Bekkerman et al (2007))
for improving its performance. For instance, in-
stead of having the user construct a relevant fea-
ture space from scratch, she can simply extend
the set of informative features identified for the
user-selected dimension. Second, since none of
the steps in our method is specifically designed
for sentiment classification, we plan to apply it to
other non-topic-based text classification tasks.
588
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on an earlier draft of the pa-
per. This work was supported in part by NSF
Grant IIS-0812261.
References
Maria-Florina Balcan and Avrim Blum. 2008. Clus-
tering with interactive feedback. In Proceedings of
ALT, pages 316?328.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Proceedings
of EMNLP, pages 127?135.
Ron Bekkerman, Hema Raghavan, James Allan, and
Koji Eguchi. 2007. Interactive clustering of text
collections according to a user-specified criterion.
In Proceedings of IJCAI, pages 684?689.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the ACL, pages 440?447.
Deng Cai, Xiaofei He, and Jiawei Han. 2005. Doc-
ument clustering using locality preserving indexing.
IEEE Transactions on Knowledge and Data Engi-
neering, 17(12):1624?1637.
Inderjit Dhillon, Yuqiang Guan, and Brian Kulis. 2004.
Kernel k-means, spectral clustering and normalized
cuts. In Proceedings of KDD, pages 551?556.
Mark Dredze and Koby Crammer. 2008. Active learn-
ing with confidence. In Proceedings of ACL-08:HLT
Short Papers (Companion Volume), pages 233?236.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and
Alexander Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 44?56. MIT
Press.
Sepandar Kamvar, Dan Klein, and Chris Manning.
2003. Spectral learning. In Proceedings of IJCAI,
pages 561?566.
Ravi Kannan, Santosh Vempala, and Adrian Vetta.
2004. On clusterings: Good, bad and spectral. Jour-
nal of the ACM, 51(3):497?515.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of COL-
ING, pages 1367?1373.
Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip S. Yu.
2004. Text classification by labeling words. In Pro-
ceedings of AAAI, pages 425?430.
Tony Mullen and Nigel Collier. 2004. Sentiment
analysis using support vector machines with diverse
information sources. In Proceedings of EMNLP,
pages 412?418.
Andrew Ng, Michael Jordan, and Yair Weiss. 2002.
On spectral clustering: Analysis and an algorithm.
In Advances in NIPS 14.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86.
Hema Raghavan and James Allan. 2007. An interac-
tive algorithm for asking and incorporating feature
feedback into support vector machines. In Proceed-
ings of SIGIR, pages 79?86.
Dan Roth and Kevin Small. 2009. Interactive feature
space construction using semantic information. In
Proceedings of CoNLL, pages 66?74.
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 22(8):888?
905.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the ACL, pages
417?424.
Kiri Wagstaff, Claire Cardie, Seth Rogers, and Ste-
fan Schro?dl. 2001. Constrained k-means cluster-
ing with background knowledge. In Proceedings of
ICML, pages 577?584.
Xiaojun Wan. 2008. Using bilingual knowledge and
ensemble techniques for unsupervised Chinese sen-
timent analysis. In Proceedings of EMNLP, pages
553?561.
Yair Weiss. 1999. Segmentation using eigenvectors: A
unifying view. In Proceedings of ICCV, pages 975?
982.
Yiming Yang and Jan Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings of ICML, pages 412?420.
589
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 968?977,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Supervised Models for Coreference Resolution
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{altaf,vince}@hlt.utdallas.edu
Abstract
Traditional learning-based coreference re-
solvers operate by training a mention-
pair classifier for determining whether two
mentions are coreferent or not. Two in-
dependent lines of recent research have
attempted to improve these mention-pair
classifiers, one by learning a mention-
ranking model to rank preceding men-
tions for a given anaphor, and the other
by training an entity-mention classifier
to determine whether a preceding clus-
ter is coreferent with a given mention.
We propose a cluster-ranking approach to
coreference resolution that combines the
strengths of mention rankers and entity-
mention models. We additionally show
how our cluster-ranking framework natu-
rally allows discourse-new entity detection
to be learned jointly with coreference res-
olution. Experimental results on the ACE
data sets demonstrate its superior perfor-
mance to competing approaches.
1 Introduction
Noun phrase (NP) coreference resolution is the
task of identifying which NPs (or mentions) re-
fer to the same real-world entity or concept. Tra-
ditional learning-based coreference resolvers op-
erate by training a model for classifying whether
two mentions are co-referring or not (e.g., Soon
et al (2001), Ng and Cardie (2002b), Kehler et al
(2004), Ponzetto and Strube (2006)). Despite their
initial successes, these mention-pair models have
at least two major weaknesses. First, since each
candidate antecedent for a mention to be resolved
(henceforth an active mention) is considered inde-
pendently of the others, these models only deter-
mine how good a candidate antecedent is relative
to the active mention, but not how good a candi-
date antecedent is relative to other candidates. In
other words, they fail to answer the critical ques-
tion of which candidate antecedent is most prob-
able. Second, they have limitations in their ex-
pressiveness: the information extracted from the
two mentions alone may not be sufficient for mak-
ing an informed coreference decision, especially if
the candidate antecedent is a pronoun (which is se-
mantically empty) or a mention that lacks descrip-
tive information such as gender (e.g., Clinton).
To address the first weakness, researchers have
attempted to train a mention-ranking model for
determining which candidate antecedent is most
probable given an active mention (e.g., Denis and
Baldridge (2008)). Ranking is arguably a more
natural reformulation of coreference resolution
than classification, as a ranker allows all candidate
antecedents to be considered simultaneously and
therefore directly captures the competition among
them. Another desirable consequence is that there
exists a natural resolution strategy for a ranking
approach: a mention is resolved to the candidate
antecedent that has the highest rank. This con-
trasts with classification-based approaches, where
many clustering algorithms have been employed
to co-ordinate the pairwise coreference decisions
(because it is unclear which one is the best).
To address the second weakness, researchers
have investigated the acquisition of entity-mention
coreference models (e.g., Luo et al (2004), Yang
et al (2004)). Unlike mention-pair models, these
entity-mention models are trained to determine
whether an active mention belongs to a preced-
ing, possibly partially-formed, coreference cluster.
Hence, they can employ cluster-level features (i.e.,
features that are defined over any subset of men-
tions in a preceding cluster), which makes them
more expressive than mention-pair models.
Motivated in part by these recently developed
models, we propose in this paper a cluster-
ranking approach to coreference resolution that
combines the strengths of mention-ranking mod-
968
els and entity-mention models. Specifically, we
recast coreference as the problem of determining
which of a set of preceding coreference clusters
is the best to link to an active mention using a
learned cluster ranker. In addition, we show how
discourse-new detection (i.e., the task of determin-
ing whether a mention introduces a new entity in
a discourse) can be learned jointly with corefer-
ence resolution in our cluster-ranking framework.
It is worth noting that researchers typically adopt
a pipeline coreference architecture, performing
discourse-new detection prior to coreference res-
olution and using the resulting information to pre-
vent a coreference system from resolving men-
tions that are determined to be discourse-new (see
Poesio et al (2004) for an overview). As a re-
sult, errors in discourse-new detection could be
propagated to the resolver, possibly leading to a
deterioration of coreference performance (see Ng
and Cardie (2002a)). Jointly learning discourse-
new detection and coreference resolution can po-
tentially address this error-propagation problem.
In sum, we believe our work makes three main
contributions to coreference resolution:
Proposing a simple, yet effective coreference
model. Our work advances the state-of-the-art
in coreference resolution by bringing learning-
based coreference systems to the next level of
performance. When evaluated on the ACE 2005
coreference data sets, cluster rankers outperform
three competing models ? mention-pair, entity-
mention, and mention-ranking models ? by a
large margin. Also, our joint-learning approach
to discourse-new detection and coreference reso-
lution consistently yields cluster rankers that out-
perform those adopting the pipeline architecture.
Equally importantly, cluster rankers are conceptu-
ally simple and easy to implement and do not rely
on sophisticated training and inference procedures
to make coreference decisions in dependent rela-
tion to each other, unlike relational coreference
models (see McCallum and Wellner (2004)).
Bridging the gap between machine-learning
approaches and linguistically-motivated ap-
proaches to coreference resolution. While ma-
chine learning approaches to coreference resolu-
tion have received a lot of attention since the mid-
90s, popular learning-based coreference frame-
works such as the mention-pair model are ar-
guably rather unsatisfactory from a linguistic point
of view. In particular, they have not leveraged
advances in discourse-based anaphora resolution
research in the 70s and 80s. Our work bridges
this gap by realizing in a new machine learn-
ing framework ideas rooted in Lappin and Leass?s
(1994) heuristic-based pronoun resolver, which in
turn was motivated by classic salience-based ap-
proaches to anaphora resolution.
Revealing the importance of adopting the right
model. While entity-mention models have pre-
viously been shown to be worse or at best
marginally better than their mention-pair counter-
parts (Luo et al, 2004; Yang et al, 2008), our
cluster-ranking models, which are a natural exten-
sion of entity-mention models, significantly out-
performed all competing approaches. This sug-
gests that the use of an appropriate learning frame-
work can bring us a long way towards high-
performance coreference resolution.
The rest of the paper is structured as follows.
Section 2 discusses related work. Section 3 de-
scribes our baseline coreference models: mention-
pair, entity-mention, and mention-ranking. We
discuss our cluster-ranking approach in Section 4,
evaluate it in Section 5, and conclude in Section 6.
2 Related Work
Heuristic-based cluster ranking. As men-
tioned previously, the work most related to ours is
Lappin and Leass (1994), whose goal is to perform
pronoun resolution by assigning an anaphoric pro-
noun to the highest-scored preceding cluster. Nev-
ertheless, Lappin and Leass?s work differs from
ours in several respects. First, they only tackle
pronoun resolution rather than the full coreference
task. Second, their algorithm is heuristic-based; in
particular, the score assigned to a preceding clus-
ter is computed by summing over the weights as-
sociated with the factors applicable to the cluster,
where the weights are determined heuristically,
rather than learned, unlike ours.
Like many heuristic-based pronoun resolvers
(e.g., Mitkov (1998)), they first apply a set of con-
straints to filter grammatically incompatible can-
didate antecedents and then rank the remaining
ones using salience factors. As a result, their
cluster-ranking model employs only factors that
capture the salience of a cluster, and can therefore
be viewed as a simple model of attentional state
(see Grosz and Sidner (1986)) realized by coref-
erence clusters. By contrast, our resolution strat-
egy is learned without applying hand-coded con-
969
straints in a separate filtering step. In particular,
we attempt to determine the compatibility between
a cluster and an active mention, using factors that
determine not only salience (e.g., the distance be-
tween the cluster and the mention) but also lexical
and grammatical compatibility, for instance.
Entity-mention coreference models. Luo et al
(2004) represent one of the earliest attempts to
investigate learning-based entity-mention models.
They use the ANY predicate to generate cluster-
level features as follows: given a binary-valued
feature X defined over a pair of mentions, they
introduce an ANY-X cluster-level feature, which
has the value TRUE if X is true between the active
mention and any mention in the preceding clus-
ter under consideration. Contrary to common wis-
dom, this entity-mention model underperforms its
mention-pair counterpart in spite of the general-
ization from mention-pair to cluster-level features.
In Yang et al?s (2004) entity-mention model, a
training instance is composed of an active men-
tion m
k
, a preceding cluster C, and a mention
m
j
in C that is closest in distance to m
k
in the
associated text. The feature set used to repre-
sent the instance is primarily composed of fea-
tures that describe the relationship between m
j
and m
k
, as well as a few cluster-level features.
In other words, the model still relies heavily on
features used in a mention-pair model. In par-
ticular, the inclusion of m
j
in the feature vector
representation to some extent reflects the authors?
lack of confidence that a strong entity-mention
model can be trained without mention-pair-based
features. Our ranking model, on the other hand, is
trained without such features. More recently, Yang
et al (2008) have proposed another entity-mention
model trained by inductive logic programming.
Like their previous work, the scarcity of cluster-
level predicates (only two are used) under-exploits
the expressiveness of entity-mention models.
Mention ranking. The notion of ranking can-
didate antecedents can be traced back to center-
ing algorithms, many of which use grammatical
roles to rank forward-looking centers (see Grosz
et al (1995), Walker et al (1998), and Mitkov
(2002)). However, mention ranking has been
employed in learning-based coreference resolvers
only recently. As mentioned before, Denis and
Baldridge (2008) train a mention-ranking model.
Their work can be viewed as an extension of Yang
et al?s (2003) twin-candidate coreference model,
which ranks only two candidate antecedents at a
time. Unlike ours, however, their model ranks
mentions rather than clusters, and relies on an
independently-trained discourse-new detector.
Discourse-new detection. Discourse-new de-
tection is often tackled independently of coref-
erence resolution. Pleonastic its have been de-
tected using heuristics (e.g., Kennedy and Bogu-
raev (1996)) and learning-based techniques such
as rule learning (e.g., Mu?ller (2006)), kernels (e.g.,
Versley et al (2008)), and distributional methods
(e.g., Bergsma et al (2008)). Non-anaphoric defi-
nite descriptions have been detected using heuris-
tics (e.g., Vieira and Poesio (2000)) and unsu-
pervised methods (e.g., Bean and Riloff (1999)).
General discourse-new detectors that are applica-
ble to different types of NPs have been built using
heuristics (e.g., Byron and Gegg-Harrison (2004))
and modeled generatively (e.g., Elsner and Char-
niak (2007)) and discriminatively (e.g., Uryupina
(2003)). There have also been attempts to perform
joint inference for discourse-new detection and
coreference resolution using integer linear pro-
gramming (ILP), where a discourse-new classifier
and a coreference classifier are trained indepen-
dently of each other, and then ILP is applied as a
post-processing step to jointly infer discourse-new
and coreference decisions so that they are consis-
tent with each other (e.g., Denis and Baldridge
(2007)). Joint inference is different from our joint-
learning approach, which allows the two tasks to
be learned jointly and not independently.
3 Baseline Coreference Models
In this section, we describe three coreference mod-
els that will serve as our baselines: the mention-
pair model, the entity-mention model, and the
mention-ranking model. For illustrative purposes,
we will use the text segment shown in Figure 1.
Each mention m in the segment is annotated as
[m]
cid
mid
, where mid is the mention id and cid is
the id of the cluster to which m belongs. As we
can see, the mentions are partitioned into four sets,
with Barack Obama, his, and he in one cluster, and
each of the remaining mentions in its own cluster.
3.1 Mention-Pair Model
As noted before, a mention-pair model is a clas-
sifier that decides whether or not an active men-
tion m
k
is coreferent with a candidate antecedent
m
j
. Each instance i(m
j
,m
k
) represents m
j
and
970
[Barack Obama]1
1
nominated [Hillary Rodham Clinton]2
2
as
[[his]1
3
secretary of state]3
4
on [Monday]4
5
. [He]1
6
...
Figure 1: An illustrative example
m
k
and consists of the 39 features shown in Ta-
ble 1. These features have largely been employed
by state-of-the-art learning-based coreference sys-
tems (e.g., Soon et al (2001), Ng and Cardie
(2002b), Bengtson and Roth (2008)), and are com-
puted automatically. As can be seen, the features
are divided into four blocks. The first two blocks
consist of features that describe the properties of
m
j
and m
k
, respectively, and the last two blocks
of features describe the relationship between m
j
and m
k
. The classification associated with a train-
ing instance is either positive or negative, depend-
ing on whether m
j
and m
k
are coreferent.
If one training instance were created from each
pair of mentions, the negative instances would
significantly outnumber the positives, yielding
a skewed class distribution that will typically
have an adverse effect on model training. As
a result, only a subset of mention pairs will
be generated for training. Following Soon et
al. (2001), we create (1) a positive instance for
each discourse-old mention m
k
and its closest
antecedent m
j
; and (2) a negative instance for
m
k
paired with each of the intervening mentions,
m
j+1
,m
j+2
, . . . ,m
k?1
. In our running example
shown in Figure 1, three training instances will
be generated for He: i(Monday, He), i(secretary
of state, He), and i(his, He). The first two of
these instances will be labeled as negative, and
the last one will be labeled as positive. To train a
mention-pair classifier, we use the SVM learning
algorithm from the SVMlight package (Joachims,
2002), converting all multi-valued features into an
equivalent set of binary-valued features.
After training, the resulting SVM classifier is
used to identify an antecedent for a mention in a
test text. Specifically, an active mention m
k
se-
lects as its antecedent the closest preceding men-
tion that is classified as coreferent with m
k
. If m
k
is not classified as coreferent with any preceding
mention, it will be considered discourse-new (i.e.,
no antecedent will be selected for m
k
).
3.2 Entity-Mention Model
Unlike a mention-pair model, an entity-mention
model is a classifier that decides whether or not
an active mention m
k
is coreferent with a par-
tial cluster c
j
that precedes m
k
. Each training
instance, i(c
j
,m
k
), represents c
j
and m
k
. The
features for an instance can be divided into two
types: (1) features that describe m
k
(i.e, those
shown in the second block of Table 1), and (2)
cluster-level features, which describe the relation-
ship between c
j
and m
k
. Motivated by previ-
ous work (Luo et al, 2004; Culotta et al, 2007;
Yang et al, 2008), we create cluster-level fea-
tures from mention-pair features using four pred-
icates: NONE, MOST-FALSE, MOST-TRUE, and
ALL. Specifically, for each feature X shown in
the last two blocks in Table 1, we first convert X
into an equivalent set of binary-valued features if
it is multi-valued. Then, for each resulting binary-
valued feature X
b
, we create four binary-valued
cluster-level features: (1) NONE-X
b
is true when
X
b
is false between m
k
and each mention in c
j
; (2)
MOST-FALSE-X
b
is true when X
b
is true between
m
k
and less than half (but at least one) of the men-
tions in c
j
; (3) MOST-TRUE-X
b
is true when X
b
is
true between m
k
and at least half (but not all) of
the mentions in c
j
; and (4) ALL-X
b
is true when X
b
is true between m
k
and each mention in c
j
. Hence,
for each X
b
, exactly one of these four cluster-level
features evaluates to true.
Following Yang et al (2008), we create (1) a
positive instance for each discourse-old mention
m
k
and the preceding cluster c
j
to which it be-
longs; and (2) a negative instance for m
k
paired
with each partial cluster whose last mention ap-
pears between m
k
and its closest antecedent (i.e.,
the last mention of c
j
). Consider again our run-
ning example. Three training instances will be
generated for He: i({Monday}, He), i({secretary
of state}, He), and i({Barack Obama, his}, He).
The first two of these instances will be labeled as
negative, and the last one will be labeled as pos-
itive. As in the mention-pair model, we train an
entity-mention classifier using the SVM learner.
After training, the resulting classifier is used to
identify a preceding cluster for a mention in a test
text. Specifically, the mentions are processed in
a left-to-right manner. For each active mention
m
k
, a test instance is created between m
k
and
each of the preceding clusters formed so far. All
the test instances are then presented to the classi-
fier. Finally, m
k
will be linked to the closest pre-
ceding cluster that is classified as coreferent with
m
k
. If m
k
is not classified as coreferent with any
971
Features describing m
j
, a candidate antecedent
1 PRONOUN 1 Y if m
j
is a pronoun; else N
2 SUBJECT 1 Y if m
j
is a subject; else N
3 NESTED 1 Y if m
j
is a nested NP; else N
Features describing m
k
, the mention to be resolved
4 NUMBER 2 SINGULAR or PLURAL, determined using a lexicon
5 GENDER 2 MALE, FEMALE, NEUTER, or UNKNOWN, determined using a list of common first names
6 PRONOUN 2 Y if m
k
is a pronoun; else N
7 NESTED 2 Y if m
k
is a nested NP; else N
8 SEMCLASS 2 the semantic class of m
k
; can be one of PERSON, LOCATION, ORGANIZATION, DATE, TIME,
MONEY, PERCENT, OBJECT, OTHERS, determined using WordNet and an NE recognizer
9 ANIMACY 2 Y if m
k
is determined as HUMAN or ANIMAL by WordNet and an NE recognizer; else N
10 PRO TYPE 2 the nominative case of m
k
if it is a pronoun; else NA. E.g., the feature value for him is HE
Features describing the relationship between m
j
, a candidate antecedent and m
k
, the mention to be resolved
11 HEAD MATCH C if the mentions have the same head noun; else I
12 STR MATCH C if the mentions are the same string; else I
13 SUBSTR MATCH C if one mention is a substring of the other; else I
14 PRO STR MATCH C if both mentions are pronominal and are the same string; else I
15 PN STR MATCH C if both mentions are proper names and are the same string; else I
16 NONPRO STR MATCH C if the two mentions are both non-pronominal and are the same string; else I
17 MODIFIER MATCH C if the mentions have the same modifiers; NA if one of both of them don?t have a modifier;
else I
18 PRO TYPE MATCH C if both mentions are pronominal and are either the same pronoun or different only w.r.t.
case; NA if at least one of them is not pronominal; else I
19 NUMBER C if the mentions agree in number; I if they disagree; NA if the number for one or both
mentions cannot be determined
20 GENDER C if the mentions agree in gender; I if they disagree; NA if the gender for one or both mentions
cannot be determined
21 AGREEMENT C if the mentions agree in both gender and number; I if they disagree in both number and
gender; else NA
22 ANIMACY C if the mentions match in animacy; I if they don?t; NA if the animacy for one or both mentions
cannot be determined
23 BOTH PRONOUNS C if both mentions are pronouns; I if neither are pronouns; else NA
24 BOTH PROPER NOUNS C if both mentions are proper nouns; I if neither are proper nouns; else NA
25 MAXIMALNP C if the two mentions does not have the same maximial NP projection; else I
26 SPAN C if neither mention spans the other; else I
27 INDEFINITE C if m
k
is an indefinite NP and is not in an appositive relationship; else I
28 APPOSITIVE C if the mentions are in an appositive relationship; else I
29 COPULAR C if the mentions are in a copular construction; else I
30 SEMCLASS C if the mentions have the same semantic class; I if they don?t; NA if the semantic class
information for one or both mentions cannot be determined
31 ALIAS C if one mention is an abbreviation or an acronym of the other; else I
32 DISTANCE binned values for sentence distance between the mentions
Additional features describing the relationship between m
j
, a candidate antecedent and m
k
, the mention to be resolved
33 NUMBER? the concatenation of the NUMBER 2 feature values of m
j
and m
k
. E.g., if m
j
is Clinton and
m
k
is they, the feature value is SINGULAR-PLURAL, since m
j
is singular and m
k
is plural
34 GENDER? the concatenation of the GENDER 2 feature values of m
j
and m
k
35 PRONOUN? the concatenation of the PRONOUN 2 feature values of m
j
and m
k
36 NESTED? the concatenation of the NESTED 2 feature values of m
j
and m
k
37 SEMCLASS? the concatenation of the SEMCLASS 2 feature values of m
j
and m
k
38 ANIMACY? the concatenation of the ANIMACY 2 feature values of m
j
and m
k
39 PRO TYPE? the concatenation of the PRO TYPE 2 feature values of m
j
and m
k
Table 1: The feature set for coreference resolution. Non-relational features describe a mention and in
most cases take on a value of YES or NO. Relational features describe the relationship between the two
mentions and indicate whether they are COMPATIBLE, INCOMPATIBLE or NOT APPLICABLE.
preceding cluster, it will be considered discourse-
new. Note that all partial clusters preceding m
k
are formed incrementally based on the predictions
of the classifier for the first k ? 1 mentions.
3.3 Mention-Ranking Model
As noted before, a ranking model imposes a
ranking on all the candidate antecedents of an
active mention m
k
. To train a ranker, we
use the SVM ranker-learning algorithm from the
SVMlight package. Like the mention-pair model,
each training instance i(m
j
,m
k
) represents m
k
and a preceding mention m
j
. In fact, the fea-
tures that represent the instance as well as the
method for creating training instances are identi-
cal to those employed by the mention-pair model.
972
The only difference lies in the assignment of
class values to training instances. Assuming that
S
k
is the set of training instances created for
anaphoric mention m
k
, the class value for an in-
stance i(m
j
,m
k
) in S
k
is the rank of m
j
among
competing candidate antecedents, which is 2 if
m
j
is the closest antecedent of m
k
, and 1 other-
wise.1 To exemplify, consider our running exam-
ple. As in the mention-pair model, three training
instances will be generated for He: i(Monday, He),
i(secretary of state, He), i(his, He). The third in-
stance will have a class value of 2, and the remain-
ing two will have a class value of 1.
After training, the mention-ranking model is ap-
plied to rank the candidate antecedents for an ac-
tive mention in a test text as follows. Given an ac-
tive mention m
k
, we follow Denis and Baldridge
(2008) and use an independently-trained classifier
to determine whether m
k
is discourse-new. If so,
m
k
will not be resolved. Otherwise, we create test
instances for m
k
by pairing it with each of its pre-
ceding mentions. The test instances are then pre-
sented to the ranker, and the preceding mention
that is assigned the largest value by the ranker is
selected as the antecedent of m
k
.
The discourse-new classifier used in the resolu-
tion step is trained with 26 of the 37 features2 de-
scribed in Ng and Cardie (2002a) that are deemed
useful for distinguishing between anaphoric and
non-anaphoric mentions. These features can be
broadly divided into two types: (1) features that
encode the form of the mention (e.g., NP type,
number, definiteness), and (2) features that com-
pare the mention to one of its preceding mentions.
4 Coreference as Cluster Ranking
In this section, we describe our cluster-ranking ap-
proach to NP coreference. As noted before, our
approach aims to combine the strengths of entity-
mention models and mention-ranking models.
4.1 Training and Applying a Cluster Ranker
For ease of exposition, we will describe in this
subsection how to train and apply a cluster ranker
when it is used in a pipeline architecture, where
discourse-new detection is performed prior to
coreference resolution. In the next subsection, we
will show how the two tasks can be learned jointly.
1A larger class value implies a better rank in SVMlight.
2The 11 features that we did not employ are CONJ,
POSSESSIVE, MODIFIER, POSTMODIFIED, SPECIAL NOUNS,
POST, SUBCLASS, TITLE, and the positional features.
Recall that a cluster ranker ranks a set of pre-
ceding clusters for an active mention m
k
. Since
a cluster ranker is a hybrid of a mention-ranking
model and an entity-mention model, the way it is
trained and applied is also a hybrid of the two.
In particular, the instance representation employed
by a cluster ranker is identical to that used by
an entity-mention model, where each training in-
stance i(c
j
, m
k
) represents a preceding cluster c
j
and a discourse-old mention m
k
and consists of
cluster-level features formed from predicates. Un-
like in an entity-mention model, however, in a
cluster ranker, (1) a training instance is created be-
tween each discourse-old mention m
k
and each of
its preceding clusters; and (2) since we are train-
ing a model for ranking clusters, the assignment of
class values to training instances is similar to that
of a mention ranker. Specifically, the class value of
a training instance i(c
j
, m
k
) created for m
k
is the
rank of c
j
among the competing clusters, which is
2 if m
k
belongs to c
j
, and 1 otherwise.
Applying the learned cluster ranker to a test text
is similar to applying a mention ranker. Specifi-
cally, the mentions are processed in a left-to-right
manner. For each active mention m
k
, we first
apply an independently-trained classifier to deter-
mine if m
k
is discourse-new. If so, m
k
will not be
resolved. Otherwise, we create test instances for
m
k
by pairing it with each of its preceding clus-
ters. The test instances are then presented to the
ranker, and m
k
is linked to the cluster that is as-
signed the highest value by the ranker. Note that
these partial clusters preceding m
k
are formed in-
crementally based on the predictions of the ranker
for the first k?1 mentions; no gold-standard coref-
erence information is used in their formation.
4.2 Joint Discourse-New Detection and
Coreference Resolution
The cluster ranker described above can be used
to determine which preceding cluster a discourse-
old mention should be linked to, but it cannot be
used to determine whether a mention is discourse-
new or not. The reason is simple: all the training
instances are generated from discourse-old men-
tions. Hence, to jointly learn discourse-new de-
tection and coreference resolution, we must train
the ranker using instances generated from both
discourse-old and discourse-new mentions.
Specifically, when training the ranker, we pro-
vide each active mention with the option to start
973
a new cluster by creating an additional instance
that (1) contains features that solely describe the
active mention (i.e., the features shown in the sec-
ond block of Table 1), and (2) has the highest rank
value among competing clusters (i.e., 2) if it is
discourse-new and the lowest rank value (i.e., 1)
otherwise. The main advantage of jointly learning
the two tasks is that it allows the ranking model
to evaluate all possible options for an active men-
tion (i.e., whether to resolve it, and if so, which
preceding cluster is the best) simultaneously.
After training, the resulting cluster ranker pro-
cesses the mentions in a test text in a left-to-right
manner. For each active mention m
k
, we create
test instances for it by pairing it with each of its
preceding clusters. To allow for the possibility that
m
k
is discourse-new, we create an additional test
instance that contains features that solely describe
the active mention (similar to what we did in the
training step above). All these test instances are
then presented to the ranker. If the additional test
instance is assigned the highest rank value by the
ranker, then m
k
is classified as discourse-new and
will not be resolved. Otherwise, m
k
is linked to
the cluster that has the highest rank. As before,
all partial clusters preceding m
k
are formed incre-
mentally based on the predictions of the ranker for
the first k ? 1 mentions.
5 Evaluation
5.1 Experimental Setup
Corpus. We use the ACE 2005 coreference cor-
pus as released by the LDC, which consists of the
599 training documents used in the official ACE
evaluation.3 To ensure diversity, the corpus was
created by selecting documents from six different
sources: Broadcast News (bn), Broadcast Con-
versations (bc), Newswire (nw), Webblog (wb),
Usenet (un), and conversational telephone speech
(cts). The number of documents belonging to each
source is shown in Table 2. For evaluation, we par-
tition the 599 documents into a training set and a
test set following a 80/20 ratio, ensuring that the
two sets have the same proportion of documents
from the six sources.
Mention extractor. We evaluate each corefer-
ence model using both true mentions (i.e., gold
standard mentions4) and system mentions (i.e., au-
3Since we did not participate in ACE 2005, we do not
have access to the official test set.
4Note that only mention boundaries are used.
Dataset bn bc nw wl un cts
# of documents 60 226 106 119 49 39
Table 2: Statistics for the ACE 2005 corpus
tomatically identified mentions). To extract sys-
tem mentions from a test text, we trained a men-
tion extractor on the training texts. Following Flo-
rian et al (2004), we recast mention extraction as
a sequence labeling task, where we assign to each
token in a test text a label that indicates whether it
begins a mention, is inside a mention, or is outside
a mention. Hence, to learn the extractor, we create
one training instance for each token in a training
text and derive its class value (one of b, i, and o)
from the annotated data. Each instance represents
w
i
, the token under consideration, and consists of
29 linguistic features, many of which are modeled
after the systems of Bikel et al (1999) and Florian
et al (2004), as described below.
Lexical (7): Tokens in a window of 7:
{w
i?3
, . . . , w
i+3
}.
Capitalization (4): Determine whether w
i
IsAllCap, IsInitCap, IsCapPeriod, and
IsAllLower (see Bikel et al (1999)).
Morphological (8): w
i
?s prefixes and suffixes of
length one, two, three, and four.
Grammatical (1): The part-of-speech (POS)
tag of w
i
obtained using the Stanford log-linear
POS tagger (Toutanova et al, 2003).
Semantic (1): The named entity (NE) tag of w
i
obtained using the Stanford CRF-based NE recog-
nizer (Finkel et al, 2005).
Gazetteers (8): Eight dictionaries containing
pronouns (77 entries), common words and words
that are not names (399.6k), person names (83.6k),
person titles and honorifics (761), vehicle words
(226), location names (1.8k), company names
(77.6k), and nouns extracted from WordNet that
are hyponyms of PERSON (6.3k).
We employ CRF++5, a C++ implementation of
conditional random fields, for training the mention
detector, which achieves an F-score of 86.7 (86.1
recall, 87.2 precision) on the test set. These ex-
tracted mentions are to be used as system mentions
in our coreference experiments.
Scoring programs. To score the output of a
coreference model, we employ three scoring pro-
grams: MUC (Vilain et al, 1995), B3 (Bagga and
Baldwin, 1998), and ?
3
-CEAF (Luo, 2005).
5Available from http://crfpp.sourceforge.net
974
There is a complication, however. When scor-
ing a response (i.e., system-generated) partition
against a key (i.e., gold-standard) partition, a scor-
ing program needs to construct a mapping between
the mentions in the response and those in the key.
If the response is generated using true mentions,
then every mention in the response is mapped to
some mention in the key and vice versa; in other
words, there are no twinless (i.e., unmapped) men-
tions (Stoyanov et al, 2009). However, this is
not the case when system mentions are used. The
aforementioned complication does not arise from
the construction of the mapping, but from the fact
that Bagga and Baldwin (1998) and Luo (2005) do
not specify how to apply B3 and CEAF to score
partitions generated from system mentions.
We propose a simple solution to this problem:
we remove all and only those twinless system
mentions that are singletons before applying B3
and CEAF. The reason is simple: since the coref-
erence resolver has successfully identified these
mentions as singletons, it should not be penal-
ized, and removing them allows us to avoid such
penalty. Note that we only remove twinless (as op-
posed to all) system mentions that are singletons:
this allows us to reward a resolver for success-
ful identification of singleton mentions that have
twins, thus overcoming a major weakness of and
common criticism against the MUC scorer. Also,
we retain twinless system mentions that are non-
singletons, as the resolver should be penalized for
identifying spurious coreference relations. On the
other hand, we do not remove twinless mentions
in the key partition, as we want to ensure that the
resolver makes the correct (non-)coreference de-
cisions for them. We believe that our proposal ad-
dresses Stoyanov et al?s (2009) problem of hav-
ing very low precision when applying the CEAF
scorer to score partitions of system mentions.
5.2 Results and Discussions
The mention-pair baseline. We train our first
baseline, the mention-pair coreference classifier,
using the SVM learning algorithm as implemented
in the SVMlight package (Joachims, 2002).6 Re-
sults of this baseline using true mentions and sys-
tem mentions, shown in row 1 of Tables 3 and 4,
are reported in terms of recall (R), precision (P),
and F-score (F) provided by the three scoring pro-
6For this and subsequent uses of the SVM learner in our
experiments, we set al parameters to their default values.
grams. As we can see, this baseline achieves F-
scores of 54.3?70.0 and 53.4?62.5 for true men-
tions and system mentions, respectively.
The entity-mention baseline. Next, we train
our second baseline, the entity-mention corefer-
ence classifier, using the SVM learner. Results of
this baseline are shown in row 2 of Tables 3 and
4. For true mentions, this baseline achieves an F-
score of 54.8?70.7. In comparison to the mention-
pair baseline, F-score rises insignificantly accord-
ing to all three scorers.7 Similar trends can be ob-
served for system mentions, where the F-scores
between the two models are statistically indistin-
guishable across the board. While the insignifi-
cant performance difference is somewhat surpris-
ing given the improved expressiveness of entity-
mention models over mention-pair models, similar
trends have been reported by Luo et al (2004).
The mention-ranking baseline. Our third base-
line is the mention-ranking coreference model,
trained using the ranker-learning algorithm in
SVMlight. To identify discourse-new mentions,
we employ two methods. In the first method, we
adopt a pipeline architecture, where we train an
SVM classifier for discourse-new detection inde-
pendently of the mention ranker on the training set
using the 26 features described in Section 3.3. We
then apply the resulting classifier to each test text
to filter discourse-new mentions prior to corefer-
ence resolution. Results of the mention ranker are
shown in row 3 of Tables 3 and 4. As we can
see, the ranker achieves F-scores of 57.8?71.2 and
54.1?65.4 for true mentions and system mentions,
respectively, yielding a significant improvement
over the entity-mention baseline in all but one case
(MUC/true mentions).
In the second method, we perform discourse-
new detection jointly with coreference resolution
using the method described in Section 4.2. While
we discussed this joint learning method in the con-
text of cluster ranking, it should be easy to see
that the method is equally applicable to a men-
tion ranker. Results of the mention ranker using
this joint architecture are shown in row 4 of Ta-
bles 3 and 4. As we can see, the ranker achieves
F-scores of 61.6?73.4 and 55.6?67.1 for true men-
tions and system mentions, respectively. For both
types of mentions, the improvements over the cor-
responding results for the entity-mention baseline
7We use Approximate Randomization (Noreen, 1989) for
testing statistical significance, with p set to 0.05.
975
MUC CEAF B3
Coreference Model R P F R P F R P F
1 Mention-pair model 71.7 69.2 70.4 54.3 54.3 54.3 53.3 63.6 58.0
2 Entity-mention model 71.7 69.7 70.7 54.8 54.8 54.8 53.2 65.1 58.5
3 Mention-ranking model (Pipeline) 68.7 73.9 71.2 57.8 57.8 57.8 55.8 63.9 59.6
4 Mention-ranking model (Joint) 69.4 77.8 73.4 61.6 61.6 61.6 57.0 70.1 62.9
5 Cluster-ranking model (Pipeline) 71.7 78.2 74.8 61.8 61.8 61.8 58.2 69.1 63.2
6 Cluster-ranking model (Joint) 69.9 83.3 76.0 63.3 63.3 63.3 56.0 74.6 64.0
Table 3: MUC, CEAF, and B3 coreference results using true mentions.
MUC CEAF B3
Coreference Model R P F R P F R P F
1 Mention-pair model 70.0 56.4 62.5 56.1 51.0 53.4 50.8 57.9 54.1
2 Entity-mention model 68.5 57.2 62.3 56.3 50.2 53.1 51.2 57.8 54.3
3 Mention-ranking model (Pipeline) 62.2 68.9 65.4 51.6 56.7 54.1 52.3 61.8 56.6
4 Mention-ranking model (Joint) 62.1 73.0 67.1 53.0 58.5 55.6 50.4 65.5 56.9
5 Cluster-ranking model (Pipeline) 65.3 72.3 68.7 54.1 59.3 56.6 55.3 63.7 59.2
6 Cluster-ranking model (Joint) 64.1 75.4 69.3 56.7 62.6 59.5 54.4 70.5 61.4
Table 4: MUC, CEAF, and B3 coreference results using system mentions.
are significant, and suggest that mention ranking is
a precision-enhancing device. Moreover, in com-
parison to the pipeline architecture in row 3, we
see that F-score rises significantly by 2.2?3.8% for
true mentions, and improves by a smaller margin
of 0.3?1.7% for system mentions. These results
demonstrate the benefits of joint modeling.
Our cluster-ranking model. Finally, we evalu-
ate our cluster-ranking model. As in the mention-
ranking baseline, we employ both the pipeline ar-
chitecture and the joint architecture for discourse-
new detection. Results are shown in rows 5 and
6 of Tables 3 and 4, respectively, for the two ar-
chitectures. When true mentions are used, the
pipeline architecture yields an F-score of 61.8?
74.8, which represents a significant improvement
over the mention ranker adopting the pipeline ar-
chitecture. With the joint architecture, the clus-
ter ranker achieves an F-score of 63.3?76.0. This
also represents a significant improvement over the
mention ranker adopting the joint architecture, the
best of the baselines, and suggests that cluster
ranking is a better precision-enhancing model than
mention ranking. Moreover, comparing the re-
sults in these two rows reveals the superiority of
the joint architecture over the pipeline architec-
ture, particularly in terms of its ability to enhance
system precision. Similar performance trends can
be observed when system mentions are used.
6 Conclusions
We have presented a cluster-ranking approach that
recasts the mention resolution process as the prob-
lem of finding the best preceding cluster to link an
active mention to. Crucially, our approach com-
bines the strengths of entity-mention models and
mention-ranking models. Experimental results on
the ACE 2005 corpus show that (1) jointly learn-
ing coreference resolution and discourse-new de-
tection allows the cluster ranker to achieve bet-
ter performance than adopting a pipeline corefer-
ence architecture; and (2) our cluster ranker signif-
icantly outperforms the mention ranker, the best of
the three baseline coreference models, under both
the pipeline architecture and the joint architecture.
Overall, we believe that our cluster-ranking ap-
proach advances the state-of-the-art in coreference
resolution both theoretically and empirically.
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on the paper. This work was
supported in part by NSF Grant IIS-0812261.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. In Proc. of COLING-ACL, pages 79?85.
D. Bean and E. Riloff. 1999. Corpus-based identifica-
tion of non-anaphoric noun phrases. In Proc. of the
ACL, pages 373?380.
E. Bengtson and D. Roth. 2008. Understanding the
values of features for coreference resolution. In
Proc. of EMNLP, pages 294?303.
S. Bergsma, D. Lin, and R. Goebel. 2008. Distribu-
tional identification of non-referential pronouns. In
Proc. of ACL-08:HLT, pages 10?18.
976
D. Bikel, R. Schwartz, and R. Weischedel. 1999. An
algorithm that learns what?s in a name. Machine
Learning, 34(1?3):211?231.
D. Byron and W. Gegg-Harrison. 2004. Eliminating
non-referring noun phrases from coreference resolu-
tion. In Proc. of DAARC, pages 21?26.
A. Culotta, M. Wick, and A. McCallum. 2007. First-
order probabilistic models for coreference resolu-
tion. In Proc. of NAACL-HLT, pages 81?88.
P. Denis and J. Baldridge. 2007. Global, joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proc. of NAACL-HLT,
pages 236?243.
P. Denis and J. Baldridge. 2008. Specialized models
and ranking for coreference resolution. In Proc. of
EMNLP, pages 660?669.
M. Elsner and E. Charniak. 2007. A generative
discourse-new model for text coherence. Technical
Report CS-07-04, Brown University.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Proc. of
the ACL, pages 363?370.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N. Nicolov, and I. Zitouni.
2004. A statistical model for multilingual entity de-
tection and tracking. In Proc. of HLT/NAACL.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995.
Centering: A framework for modeling the local co-
herence of discourse. Computational Linguistics,
21(2):203?226.
B. J. Grosz and C. L. Sidner. 1986. Attention, inten-
tions, and the structure of discourse. Computational
Linguistics, 12(3):175?204.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. of KDD, pages 133?142.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004.
The (non)utility of predicate-argument frequencies
for pronoun interpretation. In Proc. of HLT/NAACL.
C. Kennedy and B. Boguraev. 1996. Anaphor for ev-
eryone: Pronominal anaphora resolution without a
parser. In Proc. of COLING, pages 113?118.
S. Lappin and H. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational
Linguistics, 20(4):535?562.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous coref-
erence resolution algorithm based on the Bell tree.
In Proc. of the ACL, pages 135?142.
X. Luo. 2005. On coreference resolution performance
metrics. In Proc. of HLT/EMNLP, pages 25?32.
A. McCallum and B. Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun
coreference. In Advances in NIPS.
R. Mitkov. 2002. Anaphora Resolution. Longman.
R. Mitkov. 1998. Robust pronoun resolution with lim-
ited knowledge. In Proc. of COLING/ACL, pages
869?875.
C. Mu?ller. 2006. Automatic detection of nonrefer-
ential it in spoken multi-party dialog. In Proc. of
EACL, pages 49?56.
V. Ng and C. Cardie. 2002a. Identifying anaphoric and
non-anaphoric noun phrases to improve coreference
resolution. In Proc. of COLING, pages 730?736.
V. Ng and C. Cardie. 2002b. Improving machine learn-
ing approaches to coreference resolution. In Proc. of
the ACL, pages 104?111.
E. W. Noreen. 1989. Computer Intensive Methods for
Testing Hypothesis: An Introduction. John Wiley &
Sons.
M. Poesio, O. Uryupina, R. Vieira, M. Alexandrov-
Kabadjov, and R. Goulart. 2004. Discourse-new
detectors for definite description resolution: A sur-
vey and a preliminary proposal. In Proc. of the ACL
Workshop on Reference Resolution.
S. P. Ponzetto and M. Strube. 2006. Exploiting seman-
tic role labeling, WordNet and Wikipedia for coref-
erence resolution. In Proc. of HLT/NAACL, pages
192?199.
W. M. Soon, H. T. Ng, and D. Lim. 2001. A
machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics,
27(4):521?544.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009.
Conundrums in noun phrase coreference resolution:
Making sense of the state-of-the-art. In Proc. of the
ACL.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with
a cyclic dependency network. In Proc. of HLT-
NAACL, pages 252?259.
O. Uryupina. 2003. High-precision identification of
discourse new and unique noun phrases. In Proc. of
the ACL Student Research Workshop.
Y. Versley, A. Moschitti, M. Poesio, and X. Yang.
2008. Coreference systems based on kernel meth-
ods. In Proc. of COLING, pages 961?968.
R. Vieira and M. Poesio. 2000. Processing definite de-
scriptions in corpora. In Corpus-based and Compu-
tational Approaches to Discourse Anaphora, pages
189?212. UCL Press.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proc. of MUC-6, pages 45?52.
M. Walker, A. Joshi, and E. Prince, editors. 1998.
Centering Theory in Discourse. Oxford University
Press.
X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003.
Coreference resolution using competitive learning
approach. In Proc. of the ACL, pages 176?183.
X. Yang, J. Su, G. Zhou, and C. L. Tan. 2004. An NP-
cluster based approach to coreference resolution. In
Proc. of COLING, pages 226?232.
X. Yang, J. Su, J. Lang, C. L. Tan, and S. Li. 2008.
An entity-mention model for coreference resolution
with inductive logic programming. In Proc. of the
ACL, pages 843?851.
977
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 354?362,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Learning-Based Named Entity Recognition for Morphologically-Rich,
Resource-Scarce Languages
Kazi Saidul Hasan and Md. Altaf ur Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{saidul,altaf,vince}@hlt.utdallas.edu
Abstract
Named entity recognition for morpholog-
ically rich, case-insensitive languages, in-
cluding the majority of semitic languages,
Iranian languages, and Indian languages,
is inherently more difficult than its English
counterpart. Worse still, progress on ma-
chine learning approaches to named entity
recognition for many of these languages
is currently hampered by the scarcity of
annotated data and the lack of an accu-
rate part-of-speech tagger. While it is
possible to rely on manually-constructed
gazetteers to combat data scarcity, this
gazetteer-centric approach has the poten-
tial weakness of creating irreproducible
results, since these name lists are not
publicly available in general. Motivated
in part by this concern, we present a
learning-based named entity recognizer
that does not rely on manually-constructed
gazetteers, using Bengali as our represen-
tative resource-scarce, morphologically-
rich language. Our recognizer achieves
a relative improvement of 7.5% in F-
measure over a baseline recognizer. Im-
provements arise from (1) using in-
duced affixes, (2) extracting information
from online lexical databases, and (3)
jointly modeling part-of-speech tagging
and named entity recognition.
1 Introduction
While research in natural language processing has
gained a lot of momentum in the past several
decades, much of this research effort has been fo-
cusing on only a handful of politically-important
languages such as English, Chinese, and Arabic.
On the other hand, being the fifth most spoken lan-
guage1 with more than 200 million native speakers
residing mostly in Bangladesh and the Indian state
of West Bengal, Bengali has far less electronic
resources than the aforementioned languages. In
fact, a major obstacle to the automatic processing
of Bengali is the scarcity of annotated corpora.
One potential solution to the problem of data
scarcity is to hand-annotate a small amount of
data with the desired linguistic information and
then develop bootstrapping algorithms for com-
bining this small amount of labeled data with
a large amount of unlabeled data. In fact, co-
training (Blum and Mitchell, 1998) has been suc-
cessfully applied to English named entity recog-
nition (NER) (Collins & Singer [henceforth C&S]
(1999)). In C&S?s approach, consecutive words
tagged as proper nouns are first identified as poten-
tial NEs, and each such NE is then labeled by com-
bining the outputs of two co-trained classifiers.
Unfortunately, there are practical difficulties in ap-
plying this technique to Bengali NER. First, one
of C&S?s co-trained classifiers uses features based
on capitalization, but Bengali is case-insensitive.
Second, C&S identify potential NEs based on
proper nouns, but unlike English, (1) proper noun
identification for Bengali is non-trivial, due to the
lack of capitalization; and (2) there does not ex-
ist an accurate Bengali part-of-speech (POS) tag-
ger for providing such information, owing to the
scarcity of annotated data for training the tagger.
In other words, Bengali NER is complicated not
only by the scarcity of annotated data, but also by
the lack of an accurate POS tagger. One could
imagine building a Bengali POS tagger using un-
1See http://en.wikipedia.org/wiki/Bengali language.
354
supervised induction techniques that have been
successfully developed for English (e.g., Schu?tze
(1995), Clark (2003)), including the recently-
proposed prototype-driven approach (Haghighi
and Klein, 2006) and Bayesian approach (Gold-
water and Griffiths, 2007). The majority of these
approaches operate by clustering distributionally
similar words, but they are unlikely to work well
for Bengali for two reasons. First, Bengali is a
relatively free word order language, and hence
the distributional information collected for Ben-
gali words may not be as reliable as that for En-
glish words. Second, many closed-class words
that typically appear in the distributional repre-
sentation of an English word (e.g., prepositions
and particles such as ?in? and ?to?) are realized
as inflections in Bengali, and the absence of these
informative words implies that the context vector
may no longer capture sufficient information for
accurately clustering the Bengali words.
In view of the above problems, many learning-
based Bengali NE recognizers have relied heavily
on manually-constructed name lists for identify-
ing persons, organizations, and locations. There
are at least two weaknesses associated with this
gazetteer-centric approach. First, these name lists
are typically not publicly available, making it dif-
ficult to reproduce the results of these NE recog-
nizers. Second, it is not clear how comprehen-
sive these lists are. Relying on comprehensive lists
that comprise a large portion of the names in the
test set essentially reduces the NER problem to a
dictionary-lookup problem, which is arguably not
very interesting from a research perspective.
In addition, many existing learning-based Ben-
gali NE recognizers have several common weak-
nesses. First, they use as features pseudo-affixes,
which are created by extracting the first n and the
last n characters of a word (where 1 ? n ? 4)
(e.g., Dandapat et al (2007)). While affixes en-
code essential grammatical information in Ben-
gali due to its morphological richness, this extrac-
tion method is arguably too ad-hoc and does not
cover many useful affixes. Second, they typically
adopt a pipelined NER architecture, performing
POS tagging prior to NER and encoding the result-
ing not-so-accurate POS information as a feature.
In other words, errors in POS tagging are propa-
gated to the NE recognizer via the POS feature,
thus limiting its performance.
Motivated in part by these weaknesses, we in-
vestigate how to improve a learning-based NE rec-
ognizer that does not rely on manually-constructed
gazetteers. Specifically, we investigate two learn-
ing architectures for our NER system. The first
one is the aforementioned pipelined architecture
in which the NE recognizer uses as features the
output of a POS tagger that is trained indepen-
dently of the recognizer. Unlike existing Bengali
POS and NE taggers, however, we examine two
new knowledge sources for training these taggers:
(1) affixes induced from an unannotated corpus
and (2) semantic class information extracted from
Wikipedia. In the second architecture, we jointly
learn the POS tagging and the NER tasks, allow-
ing features for one task to be accessible to the
other task during learning. The goal is to exam-
ine whether any benefits can be obtained via joint
modeling, which could address the error propaga-
tion problem with the pipelined architecture.
While we focus on Bengali NER in this pa-
per, none of the proposed techniques are language-
specific. In fact, we believe that these techniques
are of relevance and interest to the EACL com-
munity because they can be equally applicable to
the numerous resource-scarce European and Mid-
dle Eastern languages that share similar linguis-
tic and extra-linguistic properties as Bengali. For
instance, the majority of semitic languages and
Iranian languages are, like Bengali, morpholog-
ically productive; and many East European lan-
guages such as Czech and Polish resemble Bengali
in terms of not only their morphological richness,
but also their relatively free word order.
The rest of the paper is organized as follows.
In Section 2, we briefly describe the related work.
Sections 3 and 4 show how we induce affixes from
an unannotated corpus and extract semantic class
information from Wikipedia. In Sections 5 and
6, we train and evaluate a POS tagger and an NE
recognizer independently, augmenting the feature
set typically used for these two tasks with our new
knowledge sources. Finally, we describe and eval-
uate our joint model in Section 7.
2 Related Work
Cucerzan and Yarowsky (1999) exploit morpho-
logical and contextual patterns to propose a
language-independent solution to NER. They use
affixes based on the paradigm that named enti-
ties corresponding to a particular class have sim-
ilar morphological structure. Their bootstrapping
355
approach is tested on Romanian, English, Greek,
Turkish, and Hindi. The recall for Hindi is the
lowest (27.84%) among the five languages, sug-
gesting that the lack of case information can sig-
nificantly complicate the NER task.
To investigate the role of gazetteers in NER,
Mikheev et al (1999) combine grammar rules with
maximum entropy models and vary the gazetteer
size. Experimental results show that (1) the F-
scores for NE classes like person and organiza-
tion are still high without gazetteers, ranging from
85% to 92%; and (2) a small list of country names
can improve the low F-score for locations substan-
tially. It is worth noting that their recognizer re-
quires that the input data contain POS tags and
simple semantic tags, whereas ours automatically
acquires such linguistic information. In addition,
their approach uses part of the dataset to extend the
gazetteer. Therefore, the resulting gazetteer list is
specific to a particular domain; on the other hand,
our approach does not generate a domain-specific
list, since it makes use of Wikipedia articles.
Kozareva (2006) generates gazetteer lists for
person and location names from unlabeled data
using common patterns and a graph exploration
algorithm. The location pattern is essentially
a preposition followed by capitalized context
words. However, this approach is inadequate for a
morphologically-rich language like Bengali, since
prepositions are often realized as inflections.
3 Affix Induction
Since Bengali is morphologically productive, a lot
of grammatical information about Bengali words
is expressed via affixes. Hence, these affixes could
serve as useful features for training POS and NE
taggers. In this section, we show how to induce
affixes from an unannotated corpus.
We rely on a simple idea proposed by Keshava
and Pitler (2006) for inducing affixes. Assume that
(1) V is a vocabulary (i.e., a set of distinct words)
extracted from a large, unannotated corpus, (2) ?
and ? are two character sequences, and (3) ?? is
the concatenation of ? and ?. If ?? and ? are
found in V , we extract ? as a suffix. Similarly, if
?? and ? are found in V , we extract ? as a prefix.
In principle, we can use all of the induced af-
fixes as features for training a POS tagger and an
NE recognizer. However, we choose to use only
those features that survive our feature selection
process (to be described below), for the follow-
ing reasons. First, the number of induced affixes
is large, and using only a subset of them as fea-
tures could make the training process more effi-
cient. Second, the above affix induction method is
arguably overly simplistic and hence many of the
induced affixes could be spurious.
Our feature selection process is fairly simple:
we (1) score each affix by multiplying its fre-
quency (i.e., the number of distinct words in V to
which each affix attaches) and its length2, and (2)
select only those whose score is above a certain
threshold. In our experiments, we set this thresh-
old to 50, and generate our vocabulary of 140K
words from five years of articles taken from the
Bengali newspaper Prothom Alo. This enables us
to induce 979 prefixes and 975 suffixes.
4 Semantic Class Induction from
Wikipedia
Wikipedia has recently been used as a knowl-
edge source for various language processing tasks,
including taxonomy construction (Ponzetto and
Strube, 2007a), coreference resolution (Ponzetto
and Strube, 2007b), and English NER (e.g.,
Bunescu and Pas?ca (2006), Cucerzan (2007),
Kazama and Torisawa (2007), Watanabe et al
(2007)). Unlike previous work on using Wikipedia
for NER, our goal here is to (1) generate a list
of phrases and tokens that are potentially named
entities from the 16914 articles in the Bengali
Wikipedia3 and (2) heuristically annotate each of
them with one of four classes, namely, PER (per-
son), ORG (organization), LOC (location), or OTH-
ERS (i.e., anything other than PER, ORG and LOC).
4.1 Generating an Annotated List of Phrases
We employ the steps below to generate our anno-
tated list.
Generating and annotating the titles Recall
that each Wikipedia article has been optionally as-
signed to one or more categories by its creator
and/or editors. We use these categories to help an-
notate the title of an article. Specifically, if an ar-
ticle has a category whose name starts with ?Born
on? or ?Death on,? we label the corresponding ti-
tle with PER. Similarly, if it has a category whose
name starts with ?Cities of? or ?Countries of,? we
2The dependence on frequency and length is motivated by
the observation that less frequent and shorter affixes are more
likely to be erroneous (see Goldsmith (2001)).
3See http://bn.wikipedia.org. In our experiments, we used
the Bengali Wikipedia dump obtained on October 22, 2007.
356
NE Class Keywords
PER ?born,? ?died,? ?one,? ?famous?
LOC ?city,? ?area,? ?population,? ?located,? ?part of?
ORG ?establish,? ?situate,? ?publish?
Table 1: Keywords for each named entity class
label the title as LOC. If an article does not be-
long to one of the four categories above, we label
its title with the help of a small set of seed key-
words shown in Table 1. Specifically, for each of
the three NE classes shown on the left of Table
1, we compute a weighted sum of its keywords:
a keyword that appears in the first paragraph has
a weight of 3, a keyword that appears elsewhere
in the article has a weight of 1, and a keyword
that does not appear in the article has a weight of
0. The rationale behind using different weights is
simple: the first paragraph is typically a brief ex-
position of the title, so it should in principle con-
tain words that correlate more closely with the ti-
tle than words appearing in the rest of the article.
We then label the title with the class that has the
largest weighted sum. Note, however, that we ig-
nore any article that contains fewer than two key-
words, since we do not have reliable evidence for
labeling its title as one of the NE classes. We put
all these annotated titles into a title list.
Getting more location names To get more loca-
tion names, we search for the character sequences
?birth place:? and ?death place:? in each article,
extracting the phrase following any of these se-
quences and label it as LOC. We put all such la-
beled locations into the title list.
Generating and annotating the tokens in the ti-
tles Next, we extract the word tokens from each
title in the title list and label each token with an
NE class. The reason for doing this is to improve
generalization: if ?Dhaka University? is labeled as
ORG in the title list, then it is desirable to also label
the token ?University? as ORG, because this could
help identify an unseen phrase that contains the
term ?University? as an organization. Our token
labeling method is fairly simple. First, we gener-
ate the tokens from each title in the title list, as-
signing to each token the same NE label as that
of the title from which it is generated. For in-
stance, from the title ?Anna Frank,? ?Anna? will
be labeled as PER; and from ?Anna University,? ?
Anna? will be labeled as LOC. To resolve such
ambiguities (i.e., assigning different labels to the
same token), we keep a count of how many times
?Anna? is labeled with each NE class, and set its
final label to be the most frequent NE class. We
put all these annotated tokens into a token list. If
the title list and the token list have an element in
common, we remove the element from the token
list, since we have a higher confidence in the la-
bels of the titles.
Merging the lists Finally, we append the token
list to the title list. The resulting title list contains
4885 PERs, 15176 LOCs, and 188 ORGs.
4.2 Applying the Annotated List to a Text
We can now use the title list to annotate a text.
Specifically, we process each word w in the text in
a left-to-right manner, using the following steps:
1. Check whether w has been labeled. If so, we
skip this word and process the next one.
2. Check whether w appears in the Samsad
Bengali-English Dictionary4. If so, we as-
sume that w is more likely to be used as a
non-named entity, thus leaving the word un-
labeled and processing the next word instead.
3. Find the longest unlabeled word sequence5
that begins with w and appears in the title
list. If no such sequence exists, we leave w
unlabeled and process the next word. Oth-
erwise, we label it with the NE tag given
by the title list. To exemplify, consider a
text that starts with the sentence ?Smith Col-
lege is in Massachusetts.? When processing
?Smith,? ?Smith College? is the longest se-
quence that starts with ?Smith? and appears
in the title list (as an ORG). As a result, we
label all occurrences of ?Smith College? in
the text as an ORG. (Note that without using
the longest match heuristic, ?Smith? would
likely be mislabeled as PER.) In addition, we
take the last word of the ORG (which in this
case is ?College?) and annotate each of its oc-
currence in the rest of the text as ORG.6
These automatic annotations will then be used
to derive a set of WIKI features for training our
POS tagger and NE recognizer. Hence, unlike
existing Bengali NE recognizers, our ?gazetteers?
are induced rather than manually created.
4See http://dsal.uchicago.edu/dictionaries/biswasbengali/.
5This is a sequence in which each word is unlabeled.
6However, if we have a PER match (e.g., ?Anna Frank?)
or a LOC match (e.g., ?Las Vegas?), we take each word in the
matched phrase and label each of its occurrence in the rest of
the text with the same NE tag.
357
Current word wi
Previous word wi?1
2nd previous word wi?2
Next word wi+1
2nd next word wi+2
Current pseudo-affixes pfi (prefix), sfi (suffix)
Current induced affixes pii (prefix), sii (suffix)
Previous induced affixes pii?1 (prefix), sii?1 (suffix)
Induced affix bigrams pii?1pii (prefix), sii?1sii (suffix)
Current Wiki tag wikii
Previous Wiki tag wikii?1
Wiki bigram wikii?1wikii
Word bigrams wi?2wi?1, wi?1wi, wiwi+1,
wi+1wi+2
Word trigrams wi?2wi?1wi
Current number qi
Table 2: Feature templates for the POS tagging
experiments
5 Part-of-Speech Tagging
In this section, we will show how we train and
evaluate our POS tagger. As mentioned before, we
hypothesize that introducing our two knowledge
sources into the feature set for the tagger could
improve its performance: using the induced affixes
could improve the extraction of grammatical infor-
mation from the words, and using the Wikipedia-
induced list, which in principle should comprise
mostly of names, could help improve the identifi-
cation of proper nouns.
Corpus Our corpus is composed of 77942 words
and is annotated with one of 26 POS tags in the
tagset defined by IIIT Hyderabad7. Using this cor-
pus, we perform 5-fold cross-validation (CV) ex-
periments in our evaluation. It is worth noting that
this dataset has a high unknown word rate of 15%
(averaged over the five folds), which is due to the
small size of the dataset. While this rate is compa-
rable to another Bengali POS dataset described in
Dandapat et al (2007), it is much higher than the
2.6% unknown word rate in the test set for Ratna-
parkhi?s (1996) English POS tagging experiments.
Creating training instances Following previ-
ous work on POS tagging, we create one train-
ing instance for each word in the training set. The
class value of an instance is the POS tag of the cor-
responding word. Each instance is represented by
a set of linguistic features, as described next.
7A detailed description of these POS tags can be found in
http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf,
and are omitted here due to space limitations. This tagset
and the Penn Treebank tagset differ in that (1) nouns do not
have a number feature; (2) verbs do not have a tense feature;
and (3) adjectives and adverbs are not subcategorized.
Features Our feature set consists of (1) base-
line features motivated by those used in Danda-
pat et al?s (2007) Bengali POS tagger and Singh
et al?s (2006) Hindi POS tagger, as well as (2)
features derived from our induced affixes and the
Wikipedia-induced list. More specifically, the
baseline feature set has (1) word unigrams, bi-
grams and trigrams; (2) pseudo-affix features that
are created by taking the first three characters and
the last three characters of the current word; and
(3) a binary feature that determines whether the
current word is a number. As far as our new fea-
tures are concerned, we create one induced prefix
feature and one induced suffix feature from both
the current word and the previous word, as well
as two bigrams involving induced prefixes and in-
duced suffixes. We also create three WIKI features,
including the Wikipedia-induced NE tag of the
current word and that of the previous word, as well
as the combination of these two tags. Note that
the Wikipedia-induced tag of a word can be ob-
tained by annotating the test sentence under con-
sideration using the list generated from the Ben-
gali Wikipedia (see Section 4). To make the de-
scription of these features more concrete, we show
the feature templates in Table 2.
Learning algorithm We used CRF++8, a C++
implementation of conditional random fields (Laf-
ferty et al, 2001), as our learning algorithm for
training a POS tagging model.
Evaluating the model To evaluate the resulting
POS tagger, we generate test instances in the same
way as the training instances. 5-fold CV results of
the POS tagger are shown in Table 3. Each row
consists of three numbers: the overall accuracy,
as well as the accuracies on the seen and the un-
seen words. Row 1 shows the accuracy when the
baseline feature set is used; row 2 shows the ac-
curacy when the baseline feature set is augmented
with our two induced affix features; and the last
row shows the results when both the induced af-
fix and the WIKI features are incorporated into
the baseline feature set. Perhaps not surprisingly,
(1) adding more features improves performance,
and (2) accuracies on the seen words are substan-
tially better than those on the unseen words. In
fact, adding the induced affixes to the baseline fea-
ture set yields a 7.8% reduction in relative error
in overall accuracy. We also applied a two-tailed
paired t-test (p < 0.01), first to the overall accura-
8Available from http://crfpp.sourceforge.net
358
Experiment Overall Seen Unseen
Baseline 89.83 92.96 72.08
Baseline+Induced Affixes 90.57 93.39 74.64
Baseline+Induced Affixes+Wiki 90.80 93.50 75.58
Table 3: 5-fold cross-validation accuracies for
POS tagging
Predicted Tag Correct Tag % of Error
NN NNP 22.7
NN JJ 9.6
JJ NN 7.4
NNP NN 5.0
NN VM 4.9
Table 4: Most frequent errors for POS tagging
cies in rows 1 and 2, and then to the overall accu-
racies in rows 2 and 3. Both pairs of numbers are
statistically significantly different from each other,
meaning that incorporating the two induced affix
features and then the WIKI features both yields sig-
nificant improvements.
Error analysis To better understand the results,
we examined the errors made by the tagger. The
most frequent errors are shown in Table 4. From
the table, we see that the largest source of errors
arises from mislabeling proper nouns as common
nouns. This should be expected, as proper noun
identification is difficult due to the lack of capital-
ization information. Unfortunately, failure to iden-
tify proper nouns could severely limit the recall of
an NE recognizer. Also, adjectives and common
nouns are difficult to distinguish, since these two
syntactic categories are morphologically and dis-
tributionally similar to each other. Finally, many
errors appear to involve mislabeling a word as a
common noun. The reason is that there is a larger
percentage of common nouns (almost 30%) in the
training set than other POS tags, thus causing the
model to prefer tagging a word as a common noun.
6 Named Entity Recognition
In this section, we show how to train and evaluate
our NE recognizer. The recognizer adopts a tradi-
tional architecture, assuming that POS tagging is
performed prior to NER. In other words, the NE
recognizer will use the POS acquired in Section 5
as one of its features. As in Section 5, we will fo-
cus on examining how our knowledge sources (the
induced affixes and the WIKI features) impact the
performance of our recognizer.
Corpus The corpus we used for NER evaluation
is the same as the one described in the previous
POS of current word ti
POS of previous word ti?1
POS of 2nd previous word ti?2
POS of next word ti+1
POS of 2nd next word ti+2
POS bigrams ti?2ti?1, ti?1ti, titi+1, ti+1ti+2
First word fwi
Table 5: Additional feature templates for the NER
experiments
section. Specifically, in addition to POS infor-
mation, each sentence in the corpus is annotated
with NE information. We focus on recognizing the
three major NE types in this paper, namely persons
(PER), organizations (ORG), and locations (LOC).
There are 1721 PERs, 104 ORGs, and 686 LOCs in
the corpus. As far as evaluation is concerned, we
conduct 5-fold CV experiments, dividing the cor-
pus into the same five folds as in POS tagging.
Creating training instances We view NE
recognition as a sequence labeling problem. In
other words, we combine NE identification and
classification into one step, labeling each word in
a test text with its NE tag. Any word that does not
belong to one of our three NE tags will be labeled
as OTHERS. We adopt the IOB convention, pre-
ceding an NE tag with a B if the word is the first
word of an NE and an I otherwise. Now, to train
the NE recognizer, we create one training instance
from each word in a training text. The class value
of an instance is the NE tag of the corresponding
word, or OTHERS if the word is not part of an NE.
Each instance is represented by a set of linguistic
features, as described next.
Features Our feature set consists of (1) base-
line features motivated by those used in Ekbal
et al?s (2008) Bengali NE recognizer, as well as
(2) features derived from our induced affixes and
the Wikipedia-induced list. More specifically, the
baseline feature set has (1) word unigrams; (2)
pseudo-affix features that are created by taking the
first three characters and the last three characters
of the current word; (3) a binary feature that deter-
mines whether the current word is the first word of
a sentence; and (4) a set of POS-related features,
including the POS of the current word and its sur-
rounding words, as well as POS bigrams formed
from the current and surrounding words. Our in-
duced affixes and WIKI features are incorporated
into the baseline NE feature set in the same man-
ner as in POS tagging. In essence, the feature tem-
359
Experiment R P F
Baseline 60.97 74.46 67.05
Person 66.18 74.06 69.90
Organization 29.81 44.93 35.84
Location 52.62 80.40 63.61
Baseline+Induced Affixes 60.45 73.30 66.26
Person 65.70 72.61 69.02
Organization 31.73 46.48 37.71
Location 51.46 80.05 62.64
Baseline+Induced Affixes+Wiki 63.24 75.19 68.70
Person 66.47 75.16 70.55
Organization 30.77 43.84 36.16
Location 60.06 79.69 68.50
Table 6: 5-fold cross-validation results for NER
plates employed by the NE recognizer are the top
12 templates in Table 2 and those in Table 5.
Learning algorithm We again use CRF++ as
our sequence learner for acquiring the recognizer.
Evaluating the model To evaluate the resulting
NE tagger, we generate test instances in the same
way as the training instances. To score the output
of the recognizer, we use the CoNLL-2000 scor-
ing program9, which reports performance in terms
of recall (R), precision (P), and F-measure (F). All
NE results shown in Table 6 are averages of the
5-fold CV experiments. The first block of the Ta-
ble 6 shows the overall results when the baseline
feature set is used; in addition, we also show re-
sults for each of the three NE tags. As we can see,
the baseline achieves an F-measure of 67.05. The
second block shows the results when the baseline
feature set is augmented with our two induced af-
fix features. Somewhat unexpectedly, F-measure
drops by 0.8% in comparison to the baseline. Ad-
ditional experiments are needed to determine the
reason. Finally, when the WIKI features are in-
corporated into the augmented feature set, the sys-
tem achieves an F-measure of 68.70 (see the third
block), representing a statistically significant in-
crease of 1.6% in F-measure over the baseline.
As we can see, improvements stem primarily from
dramatic gains in recall for locations.
Discussions Several points deserve mentioning.
First, the model performs poorly on the ORGs, ow-
ing to the small number of organization names
in the corpus. Worse still, the recall drops after
adding the WIKI features. We examined the list
of induced ORG names and found that it is fairly
noisy. This can be attributed in part to the diffi-
culty in forming a set of seed words that can ex-
tract ORGs with high precision (e.g., the ORG seed
?situate? extracted many LOCs). Second, using the
9http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt
WIKI features does not help recalling the PERs. A
closer examination of the corpus reveals the rea-
son: many sentences describe fictitious characters,
whereas Wikipedia would be most useful for arti-
cles that describe famous people. Overall, while
the WIKI features provide our recognizer with a
small, but significant, improvement, the useful-
ness of the Bengali Wikipedia is currently lim-
ited by its small size. Nevertheless, we believe the
Bengali Wikipedia will become a useful resource
for language processing as its size increases.
7 A Joint Model for POS Tagging and
NER
The NE recognizer described thus far has adopted
a pipelined architecture, and hence its perfor-
mance could be limited by the errors of the POS
tagger. In fact, as discussed before, the major
source of errors made by our POS tagger concerns
the confusion between proper nouns and common
nouns, and this type of error, when propagated
to the NE recognizer, could severely limit its re-
call. Also, there is strong empirical support for
this argument: the NE recognizers, when given ac-
cess to the correct POS tags, have F-scores rang-
ing from 76-79%, which are 10% higher on aver-
age than those with POS tags that were automat-
ically computed. Consequently, we hypothesize
that modeling POS tagging and NER jointly would
yield better performance than learning the two
tasks separately. In fact, many approaches have
been developed to jointly model POS tagging and
noun phrase chunking, including transformation-
based learning (Ngai and Florian, 2001), factorial
HMMs (Duh, 2005), and dynamic CRFs (Sutton
et al, 2007). Some of these approaches are fairly
sophisticated and also require intensive computa-
tions during inference. For instance, when jointly
modeling POS tagging and chunking, Sutton et al
(2007) reduce the number of POS tags from 45
to 5 when training a factorial dynamic CRF on a
small dataset (with only 209 sentences) in order to
reduce training and inference time.
In contrast, we propose a relatively simple
model for jointly learning Bengali POS tagging
and NER, by exploiting the limited dependencies
between the two tasks. Specifically, we make the
observation that most of the Bengali words that are
part of an NE are also proper nouns. In fact, based
on statistics collected from our evaluation corpus
(see Sections 5 and 6), this observation is correct
360
Experiment R P F
Baseline 54.76 81.70 65.57
Baseline+Induced Affixes 56.79 88.96 69.32
Baseline+Induced Affixes+Wiki 61.73 86.35 71.99
Table 7: 5-fold cross-validation joint modeling re-
sults for NER
97.3% of the time. Note, however, that this ob-
servation does not hold for English, since many
prepositions and determiners are part of an NE.
On the other hand, this observation largely holds
for Bengali because prepositions and determiners
are typically realized as noun suffixes.
This limited dependency between the POS tags
and the NE tags allows us to develop a simple
model for jointly learning the two tasks. More
specifically, we will use CRF++ to learn the joint
model. Training and test instances are generated
as described in the previous two subsections (i.e.,
one instance per word). The feature set will con-
sist of the union of the features that were used to
train the POS tagger and the NE tagger indepen-
dently, minus the POS-related features that were
used in the NE tagger. The class value of an in-
stance is computed as follows. If a word is not a
proper noun, its class is simply its POS tag. Oth-
erwise, its class is its NE tag, which can be PER,
ORG, LOC, or OTHERS. In other words, our joint
model exploits the observation that we made ear-
lier in the section by assuming that only proper
nouns can be part of a named entity. This allows
us to train a joint model without substantially in-
creasing the number of classes.
We again evaluate our joint model using 5-fold
CV experiments. The NE results of the model are
shown in Table 7. The rows here can be interpreted
in the same manner as those in Table 6. Compar-
ing these three experiments with their counterparts
in Table 6, we can see that, except for the base-
line, jointly modeling offers a significant improve-
ment of 3.3% in overall F-measure.10 In particu-
lar, the joint model benefits significantly from our
10The POS tagging results are not shown due to space lim-
itations. Overall, the POS accuracies drop insignificantly as
a result of joint modeling, for the following reason. Recall
from Section 5 that the major source of POS tagging errors
arises from the mislabeling of many proper nouns as com-
mon nouns, due primarily to the large number of common
nouns in the corpus. The joint model aggravates this prob-
lem by subcategorizing the proper nouns into different NE
classes, causing the tagger to have an even stronger bias to-
wards labeling a proper noun as a common noun than before.
Nevertheless, as seen from the results in Tables 6 and 7, such
a bias has yielded an increase in NER precision.
two knowledge sources, achieving an F-measure
of 71.99% when both of them are incorporated.
Finally, to better understand the value of the in-
duced affix features in the joint model as well as
the pipelined model described in Section 6, we
conducted an ablation experiment, in which we in-
corporated only the WIKI features into the base-
line feature set. With pipelined modeling, the F-
measure for NER is 68.87%, which is similar to
the case where both induced affixes and the WIKI
features are used. With joint modeling, however,
the F-measure for NER is 70.87%, which is 1%
lower than the best joint modeling score. These
results provide suggestive evidence that the in-
duced affix features play a significant role in the
improved performance of the joint model.
8 Conclusions
We have explored two types of linguistic fea-
tures, namely the induced affix features and the
Wikipedia-related features, to improve a Bengali
POS tagger and NE recognizer. Our experimen-
tal results have demonstrated that (1) both types of
features significantly improve a baseline POS tag-
ger and (2) the Wikipedia-related features signif-
icantly improve a baseline NE recognizer. More-
over, by exploiting the limited dependencies be-
tween Bengali POS tags and NE tags, we pro-
posed a new model for jointly learning the two
tasks, which not only avoids the error-propagation
problem present in the pipelined system architec-
ture, but also yields statistically significant im-
provements over the NE recognizer that is trained
independently of the POS tagger. When applied in
combination, our three extensions contributed to a
relative improvement of 7.5% in F-measure over
the baseline NE recognizer. Most importantly, we
believe that these extensions are of relevance and
interest to the EACL community because many
European and Middle Eastern languages resemble
Bengali in terms of not only their morphological
richness but also their scarcity of annotated cor-
pora. We plan to empirically verify our belief in
future work.
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on the paper. We also thank
CRBLP, BRAC University, Bangladesh, for pro-
viding us with Bengali resources. This work was
supported in part by NSF Grant IIS-0812261.
361
References
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of COLT, pages 92?100.
Razvan Bunescu and Marius Pas?ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proceedings of EACL, pages 9?16.
Alexander Clark. 2003. Combining distributional and
morphological information for part-of-speech induc-
tion. In Proceedings of EACL, pages 59?66.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In Pro-
ceedings of EMNLP/VLC, pages 100?110.
Silviu Cucerzan and David Yarowsky. 1999. Lan-
guage independent named entity recognition com-
bining morphological and contextual evidence. In
Proceedings of EMNLP/VLC, pages 90?99.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceed-
ings of EMNLP-CoNLL, pages 708?716.
Sandipan Dandapat, Sudeshna Sarkar, and Anupam
Basu. 2007. Automatic part-of-speech tagging for
Bengali: An approach for morphologically rich lan-
guages in a poor resource scenario. In Proceedings
of the ACL Companion Volume, pages 221?224.
Kevin Duh. 2005. Jointly labeling multiple sequences:
A factorial HMM approach. In Proceedings of the
ACL Student Research Workshop, pages 19?24.
Asif Ekbal, Rejwanul Haque, and Sivaji Bandyopad-
hyay. 2008. Named entity recognition in Bengali:
A conditional random field approach. In Proceed-
ings of IJCNLP, pages 589?594.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153?198.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744?751.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT-NAACL, pages 320?327.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of EMNLP-
CoNLL, pages 698?707.
Samarth Keshava and Emily Pitler. 2006. A simpler,
intuitive approach to morpheme induction. In PAS-
CAL Challenge Workshop on Unsupervised Segmen-
tation of Words into Morphemes.
Zornitsa Kozareva. 2006. Bootstrapping named entity
recognition with automatically generated gazetteer
lists. In Proceedings of the EACL Student Research
Workshop, pages 15?22.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Andrei Mikheev, Marc Moens, and Claire Grover.
1999. Named entity recognition without gazetteers.
In Proceedings of EACL, pages 1?8.
Grace Ngai and Radu Florian. 2001. Transformation
based learning in the fast lane. In Proceedings of
NAACL, pages 40?47.
Simone Paolo Ponzetto and Michael Strube. 2007a.
Deriving a large scale taxonomy from wikipedia. In
Proceedings of AAAI, pages 1440?1445.
Simone Paolo Ponzetto and Michael Strube. 2007b.
Knowledge derived from Wikipedia for computing
semantic relatedness. Journal of Artificial Intelli-
gence Research, 30:181?212.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP, pages 133?142.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of EACL, pages 141?148.
Smriti Singh, Kuhoo Gupta, Manish Shrivastava, and
Pushpak Bhattacharyya. 2006. Morphological rich-
ness offsets resource demand ? Experiences in con-
structing a POS tagger for Hindi. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 779?786.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. Journal of Machine
Learning Research, 8:693?723.
Yotaro Watanabe, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. A graph-based approach to named
entity categorization in Wikipedia using conditional
random fields. In Proceedings of EMNLP-CoNLL,
pages 649?657.
362
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 363?371,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Weakly Supervised Part-of-Speech Tagging for Morphologically-Rich,
Resource-Scarce Languages
Kazi Saidul Hasan and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{saidul,vince}@hlt.utdallas.edu
Abstract
This paper examines unsupervised ap-
proaches to part-of-speech (POS) tagging
for morphologically-rich, resource-scarce
languages, with an emphasis on Goldwa-
ter and Griffiths?s (2007) fully-Bayesian
approach originally developed for En-
glish POS tagging. We argue that ex-
isting unsupervised POS taggers unreal-
istically assume as input a perfect POS
lexicon, and consequently, we propose
a weakly supervised fully-Bayesian ap-
proach to POS tagging, which relaxes the
unrealistic assumption by automatically
acquiring the lexicon from a small amount
of POS-tagged data. Since such relaxation
comes at the expense of a drop in tag-
ging accuracy, we propose two extensions
to the Bayesian framework and demon-
strate that they are effective in improv-
ing a fully-Bayesian POS tagger for Ben-
gali, our representative morphologically-
rich, resource-scarce language.
1 Introduction
Unsupervised POS tagging requires neither man-
ual encoding of tagging heuristics nor the avail-
ability of data labeled with POS information.
Rather, an unsupervised POS tagger operates by
only assuming as input a POS lexicon, which con-
sists of a list of possible POS tags for each word.
As we can see from the partial POS lexicon for
English in Figure 1, ?the? is unambiguous with re-
spect to POS tagging, since it can only be a deter-
miner (DT), whereas ?sting? is ambiguous, since
it can be a common noun (NN), a proper noun
(NNP) or a verb (VB). In other words, the lexi-
con imposes constraints on the possible POS tags
Word POS tag(s)
... ...
running NN, JJ
sting NN, NNP, VB
the DT
... ...
Figure 1: A partial lexicon for English
of each word, and such constraints are then used
by an unsupervised tagger to label a new sentence.
Conceivably, tagging accuracy decreases with the
increase in ambiguity: unambiguous words such
as ?the? will always be tagged correctly; on the
other hand, unseen words (or words not present
in the POS lexicon) are among the most ambigu-
ous words, since they are not constrained at all
and therefore can receive any of the POS tags.
Hence, unsupervised POS tagging can present sig-
nificant challenges to natural language processing
researchers, especially when a large fraction of
the words are ambiguous. Nevertheless, the de-
velopment of unsupervised taggers potentially al-
lows POS tagging technologies to be applied to a
substantially larger number of natural languages,
most of which are resource-scarce and, in particu-
lar, have little or no POS-tagged data.
The most common approach to unsupervised
POS tagging to date has been to train a hidden
Markov model (HMM) in an unsupervised man-
ner to maximize the likelihood of an unannotated
corpus, using a special instance of the expectation-
maximization (EM) algorithm (Dempster et al,
1977) known as Baum-Welch (Baum, 1972).
More recently, a fully-Bayesian approach to un-
supervised POS tagging has been developed by
Goldwater and Griffiths (2007) [henceforth G&G]
as a viable alternative to the traditional maximum-
likelihood-based HMM approach. While unsuper-
vised POS taggers adopting both approaches have
363
demonstrated promising results, it is important to
note that they are typically evaluated by assuming
the availability of a perfect POS lexicon. This as-
sumption, however, is fairly unrealistic in practice,
as a perfect POS lexicon can only be constructed
by having a linguist manually label each word in
a language with its possible POS tags.1 In other
words, the labor-intensive POS lexicon construc-
tion process renders unsupervised POS taggers a
lot less unsupervised than they appear. To make
these unsupervised taggers practical, one could at-
tempt to automatically construct a POS lexicon, a
task commonly known as POS induction. How-
ever, POS induction is by no means an easy task,
and it is not clear how well unsupervised POS tag-
gers work when used in combination with an au-
tomatically constructed POS lexicon.
The goals of this paper are three-fold. First,
motivated by the successes of unsupervised ap-
proaches to English POS tagging, we aim to inves-
tigate whether such approaches, especially G&G?s
fully-Bayesian approach, can deliver similar per-
formance for Bengali, our representative resource-
scarce language. Second, to relax the unrealis-
tic assumption of employing a perfect lexicon as
in existing unsupervised POS taggers, we propose
a weakly supervised fully-Bayesian approach to
POS tagging, where we automatically construct a
POS lexicon from a small amount of POS-tagged
data. Hence, unlike a perfect POS lexicon, our au-
tomatically constructed lexicon is necessarily in-
complete, yielding a large number of words that
are completely ambiguous. The high ambiguity
rate inherent in our weakly supervised approach
substantially complicates the POS tagging pro-
cess. Consequently, our third goal of this paper is
to propose two potentially performance-enhancing
extensions to G&G?s Bayesian POS tagging ap-
proach, which exploit morphology and techniques
successfully used in supervised POS tagging.
The rest of the paper is organized as follows.
Section 2 presents related work on unsupervised
approaches to POS tagging. Section 3 gives an
introduction to G&G?s fully-Bayesian approach
to unsupervised POS tagging. In Section 4, we
describe our two extensions to G&G?s approach.
Section 5 presents experimental results on Bengali
POS tagging, focusing on evaluating the effective-
1When evaluating an unsupervised POS tagger, re-
searchers typically construct a pseudo-perfect POS lexicon
by collecting the possible POS tags of a word directly from
the corpus on which the tagger is to be evaluated.
ness of our two extensions in improving G&G?s
approach. Finally, we conclude in Section 6.
2 Related Work
With the notable exception of Synder et
al.?s (2008; 2009) recent work on unsupervised
multilingual POS tagging, existing approaches to
unsupervised POS tagging have been developed
and tested primarily on English data. For instance,
Merialdo (1994) uses maximum likelihood esti-
mation to train a trigram HMM. Schu?tze (1995)
and Clark (2000) apply syntactic clustering and
dimensionality reduction in a knowledge-free
setting to obtain meaningful clusters. Haghighi
and Klein (2006) develop a prototype-driven
approach, which requires just a few prototype
examples for each POS tag and exploits these
labeled words to constrain the labels of their
distributionally similar words. Smith and Eisner
(2005) train an unsupervised POS tagger using
contrastive estimation, which seeks to move
probability mass to a positive example e from
its neighbors (i.e., negative examples are created
by perturbing e). Wang and Schuurmans (2005)
improve an unsupervised HMM-based tagger by
constraining the learned structure to maintain
appropriate marginal tag probabilities and using
word similarities to smooth the lexical parameters.
As mentioned before, Goldwater and Griffiths
(2007) have recently proposed an unsupervised
fully-Bayesian POS tagging framework that op-
erates by integrating over the possible parameter
values instead of fixing a set of parameter values
for unsupervised sequence learning. Importantly,
this Bayesian approach facilitates the incorpora-
tion of sparse priors that result in a more practical
distribution of tokens to lexical categories (John-
son, 2007). Similar to Goldwater and Griffiths
(2007) and Johnson (2007), Toutanova and John-
son (2007) also use Bayesian inference for POS
tagging. However, their work departs from exist-
ing Bayesian approaches to POS tagging in that
they (1) introduce a new sparse prior on the dis-
tribution over tags for each word, (2) extend the
Latent Dirichlet Allocation model, and (3) explic-
itly model ambiguity class. While their tagging
model, like Goldwater and Griffiths?s, assumes as
input an incomplete POS lexicon and a large unla-
beled corpus, they consider their approach ?semi-
supervised? simply because of the human knowl-
edge involved in constructing the POS lexicon.
364
3 A Fully Bayesian Approach
3.1 Motivation
As mentioned in the introduction, the most com-
mon approach to unsupervised POS tagging is to
train an HMM on an unannotated corpus using the
Baum-Welch algorithm so that the likelihood of
the corpus is maximized. To understand what the
HMM parameters are, let us revisit how an HMM
simultaneously generates an output sequence w
= (w0, w1, ..., wn) and the associated hidden state
sequence t = (t0, t1, ..., tn). In the context of POS
tagging, each state of the HMM corresponds to a
POS tag, the output sequence w is the given word
sequence, and the hidden state sequence t is the
associated POS tag sequence. To generate w and
t, the HMM begins by guessing a state t0 and then
emitting w0 from t0 according to a state-specific
output distribution over word tokens. After that,
we move to the next state t1, the choice of which
is based on t0?s transition distribution, and emit
w1 according to t1?s output distribution. This gen-
eration process repeats until the end of the word
sequence is reached. In other words, the parame-
ters of an HMM, ?, are composed of a set of state-
specific (1) output distributions (over word tokens)
and (2) transition distributions, both of which can
be learned using the EM algorithm. Once learning
is complete, we can use the resulting set of param-
eters to find the most likely hidden state sequence
given a word sequence using the Viterbi algorithm.
Nevertheless, EM sometimes fails to find good
parameter values.2 The reason is that EM tries to
assign roughly the same number of word tokens to
each of the hidden states (Johnson, 2007). In prac-
tice, however, the distribution of word tokens to
POS tags is highly skewed (i.e., some POS cate-
gories are more populated with tokens than oth-
ers). This motivates a fully-Bayesian approach,
which, rather than committing to a particular set
of parameter values as in an EM-based approach,
integrates over all possible values of ? and, most
importantly, allows the use of priors to favor the
learning of the skewed distributions, through the
use of the term P (?|w) in the following equation:
P (t|w) =
?
P (t|w, ?)P (?|w)d? (1)
The question, then, is: which priors on ? would
allow the acquisition of skewed distributions? To
2When given good parameter initializations, however, EM
can find good parameter values for an HMM-based POS tag-
ger. See Goldberg et al (2008) for details.
answer this question, recall that in POS tagging, ?
is composed of a set of tag transition distributions
and output distributions. Each such distribution is
a multinomial (i.e., each trial produces exactly one
of some finite number of possible outcomes). For
a multinomial with K outcomes, a K-dimensional
Dirichlet distribution, which is conjugate to the
multinomial, is a natural choice of prior. For sim-
plicity, we assume that a distribution in ? is drawn
from a symmetric Dirichlet with a certain hyper-
parameter (see Teh et al (2006) for details).
The value of a hyperparameter, ?, affects the
skewness of the resulting distribution, as it as-
signs different probabilities to different distribu-
tions. For instance, when ? < 1, higher proba-
bilities are assigned to sparse multinomials (i.e.,
multinomials in which only a few entries are non-
zero). Intuitively, the tag transition distributions
and the output distributions in an HMM-based
POS tagger are sparse multinomials. As a re-
sult, it is logical to choose a Dirichlet prior with
? < 1. By integrating over all possible param-
eter values, the probability that i-th outcome, yi,
takes the value k, given the previous i ? 1 out-
comes y?i= (y1, y2, ..., yi?1), is
P (k|y?i, ?) =
?
P (k|?)P (?|y?i, ?)d? (2)
= nk + ?i? 1 + K? (3)
where nk is the frequency of k in y?i. See
MacKay and Peto (1995) for the derivation.
3.2 Model
Our baseline POS tagging model is a standard tri-
gram HMM with tag transition distributions and
output distributions, each of which is a sparse
multinomial that is learned by applying a symmet-
ric Dirichlet prior:
ti | ti?1, ti?2, ? (ti?1,ti?2) ? Mult(? (ti?1,ti?2))
wi | ti, ?(ti) ? Mult(?(ti))
? (ti?1,ti?2) | ? ? Dirichlet(?)
?(ti) | ? ? Dirichlet(?)
where wi and ti denote the i-th word and tag. With
a tagset of size T (including a special tag used as
sentence delimiter), each of the tag transition dis-
tributions has T components. For the output sym-
bols, each of the ?(ti) has Wti components, where
Wti denotes the number of word types that can be
emitted from the state corresponding to ti.
365
From the closed form in Equation 3, given pre-
vious outcomes, we can compute the tag transition
and output probabilities of the model as follows:
P (ti|t?i, ?) =
n(ti?2,ti?1,ti) + ?
n(ti?2,ti?1) + T?
(4)
P (wi|ti, t?i,w?i, ?) =
n(ti,wi) + ?
nti + Wti?
(5)
where n(ti?2,ti?1,ti) and n(ti,wi) are the frequen-
cies of observing the tag trigram (ti?2, ti?1, ti)
and the tag-word pair (ti, wi), respectively. These
counts are taken from the i ? 1 tags and words
generated previously. The inference procedure de-
scribed next exploits the property that trigrams
(and outputs) are exchangeable; that is, the prob-
ability of a set of trigrams (and outputs) does not
depend on the order in which it was generated.
3.3 Inference Procedure
We perform inference using Gibbs sampling (Ge-
man and Geman, 1984), using the following pos-
terior distribution to generate samples:
P (t|w, ?, ?) ? P (w|t, ?)P (t|?)
Starting with a random assignment of a POS tag
to each word (subject to the constraints in the POS
lexicon), we resample each POS tag, ti, accord-
ing to the conditional distribution shown in Figure
2. Note that the current counts of other trigrams
and outputs can be used as ?previous? observa-
tions due to the property of exchangeability.
Following G&G, we use simulated annealing to
find the MAP tag sequence. The temperature de-
creases by a factor of exp(
log( ?2?1 )
N?1 ) after each iter-
ation, where ?1 is the initial temperature and ?2 is
the temperature after N sampling iterations.
4 Two Extensions
In this section, we present two extensions to
G&G?s fully-Bayesian framework to unsupervised
POS tagging, namely, induced suffix emission and
discriminative prediction.
4.1 Induced Suffix Emission
For morphologically-rich languages like Bengali,
a lot of grammatical information (e.g., POS) is ex-
pressed via suffixes. In fact, several approaches to
unsupervised POS induction for morphologically-
rich languages have exploited the observation that
some suffixes can only be associated with a small
number of POS tags (e.g., Clark (2003), Dasgupta
and Ng (2007)). To exploit suffixes in HMM-
based POS tagging, one can (1) convert the word-
based POS lexicon to a suffix-based POS lexicon,
which lists the possible POS tags for each suffix;
and then (2) have the HMM emit suffixes rather
than words, subject to the constraints in the suffix-
based POS lexicon. Such a suffix-based HMM,
however, may suffer from over-generalization. To
prevent over-generalization and at the same time
exploit suffixes, we propose as our first exten-
sion to G&G?s framework a hybrid approach to
word/suffix emission: a word is emitted if it is
present in the word-based POS lexicon; otherwise,
its suffix is emitted. In other words, our approach
imposes suffix-based constraints on the tagging of
words that are unseen w.r.t. the word-based POS
lexicon. Below we show how to induce the suffix
of a word and create the suffix-based POS lexicon.
Inducing suffixes To induce suffixes, we rely on
Keshava and Pitler?s (2006) method. Assume that
(1) V is a vocabulary (i.e., a set of distinct words)
extracted from a large, unannotated corpus, (2) C1
and C2 are two character sequences, and (3) C1C2
is the concatenation of C1 and C2. If C1C2 and
C1 are found in V , we extract C2 as a suffix.
However, this unsupervised suffix induction
method is arguably overly simplistic and hence
many of the induced affixes could be spurious. To
identify suffixes that are likely to be correct, we
employ a simple procedure: we (1) score each suf-
fix by multiplying its frequency (i.e., the number
of distinct words in V to which each suffix at-
taches) and its length3, and (2) select only those
whose score is above a certain threshold. In our
experiments, we set this threshold to 50, and gen-
erate our vocabulary from five years of articles
taken from the Bengali newspaper Prothom Alo.
This enables us to induce 975 suffixes.
Constructing a suffix-based POS lexicon
Next, we construct a suffix-based POS lexicon.
For each word w in the original word-based
POS lexicon, we (1) use the induced suffix list
obtained in the previous step to identify the
longest-matching suffix of w, and then (2) assign
all the POS tags associated with w to this suffix.
Incorporating suffix-based output distributions
Finally, we extend our trigram model by introduc-
3The dependence on frequency and length is motivated by
the observation that less frequent and shorter affixes are more
likely to be erroneous (see Goldsmith (2001)).
366
P (ti|t?i,w, ?, ?) ?
n(ti,wi) + ?
nti + Wti?
.
n(ti?2,ti?1,ti) + ?
n(ti?2,ti?1) + T?
.
n(ti?1,ti,ti+1) + I(ti?2 = ti?1 = ti = ti+1) + ?
n(ti?1,ti) + I(ti?2 = ti?1 = ti) + T?
.
n(ti,ti+1,ti+2) + I(ti?2 = ti = ti+2, ti?1 = ti+1) + I(ti?1 = ti = ti+1 = ti+2) + ?
n(ti,ti+1) + I(ti?2 = ti, ti?1 = ti+1) + I(ti?1 = ti = ti+1) + T?
Figure 2: The sampling distribution for ti (taken directly from Goldwater and Griffiths (2007)). All nx
values are computed from the current values of all tags except for ti. Here, I(arg) is a function that
returns 1 if arg is true and 0 otherwise, and t?i refers to the current values of all tags except for ti.
ing a state-specific probability distribution over in-
duced suffixes. Specifically, if the current word is
present in the word-based POS lexicon, or if we
cannot find any suffix for the word using the in-
duced suffix list, then we emit the word. Other-
wise, we emit its suffix according to a suffix-based
output distribution, which is drawn from a sym-
metric Dirichlet with hyperparameter ?:
si | ti, ?(ti) ? Mult(?(ti))
?(ti) | ? ? Dirichlet(?)
where si denotes the induced suffix of the i-th
word. The distribution, ?(ti), has Sti components,
where Sti denotes the number of induced suffixes
that can be emitted from the state corresponding to
ti. We compute the induced suffix emission prob-
abilities of the model as follows:
P (si|ti, t?i, s?i, ?) =
n(ti,si) + ?
nti + Sti?
(6)
where n(ti,si) is the frequency of observing the
tag-suffix pair (ti, si).
This extension requires that we slightly modify
the inference procedure. Specifically, if the cur-
rent word is unseen (w.r.t. the word-based POS
lexicon) and has a suffix (according to the induced
suffix list), then we sample from a distribution that
is almost identical to the one shown in Figure 2,
except that we replace the first fraction (i.e., the
fraction involving the emission counts) with the
one shown in Equation (6). Otherwise, we simply
sample from the distribution in Figure 2.
4.2 Discriminative Prediction
As mentioned in the introduction, the (word-
based) POS lexicons used in existing approaches
to unsupervised POS tagging were created some-
what unrealistically by collecting the possible
POS tags of a word directly from the corpus on
which the tagger is to be evaluated. To make the
lexicon formation process more realistic, we pro-
pose a weakly supervised approach to Bayesian
POS tagging, in which we automatically create the
word-based POS lexicon from a small set of POS-
tagged sentences that is disjoint from the test data.
Adopting a weakly supervised approach has an ad-
ditional advantage: the presence of POS-tagged
sentences makes it possible to exploit techniques
developed for supervised POS tagging, which is
the idea behind discriminative prediction, our sec-
ond extension to G&G?s framework.
Given a small set of POS-tagged sentences L,
discriminative prediction uses the statistics col-
lected from L to predict the POS of a word in a
discriminative fashion whenever possible. More
specifically, discriminative prediction relies on
two simple ideas typically exploited by supervised
POS tagging algorithms: (1) if the target word
(i.e., the word whose POS tag is to be predicted)
appears in L, we can label the word with its POS
tag in L; and (2) if the target word does not appear
in L but its context does, we can use its context to
predict its POS tag. In bigram and trigram POS
taggers, the context of a word is represented us-
ing the preceding one or two words. Nevertheless,
since L is typically small in a weakly supervised
setting, it is common for a target word not to sat-
isfy any of the two conditions above. Hence, if it is
not possible to predict a target word in a discrim-
inative fashion (due to the limited size of L), we
resort to the sampling equation in Figure 2.
To incorporate the above discriminative deci-
sion steps into G&G?s fully-Bayesian framework
for POS tagging, the algorithm estimates three
types of probability distributions from L. First,
to capture context, it computes (1) a distribu-
tion over the POS tags following a word bi-
gram, (wi?2, wi?1), that appears in L [henceforth
D1(wi?2, wi?1)] and (2) a distribution over the
POS tags following a word unigram, wi?1, that ap-
pears in L [henceforth D2(wi?1)]. Then, to cap-
367
Algorithm 1 Algorithm for incorporating discrim-
inative prediction
Input: wi: current word
wi?1: previous word
wi?2: second previous word
L: a set of POS-tagged sentences
Output: Predicted tag, ti
1: if wi ? L then
2: ti ? Tag drawn from the distribution of wi?s candi-
date tags
3: else if (wi?2, wi?1) ? L then
4: ti ? Tag drawn from the distribution of the POS tags
following the word bigram (wi?2, wi?1)
5: else if wi?1 ? L then
6: ti ? Tag drawn from the distribution of the POS tags
following the word unigram wi?1
7: else
8: ti ? Tag obtained using the sampling equation
9: end if
ture the fact that a word can have more than one
POS tag, it also estimates a distribution over POS
tags for each word wi that appears in L [hence-
forth D3(wi)].
Implemented as a set of if-else clauses, the al-
gorithm uses these three types of distributions to
tag a target word, wi, in a discriminative manner.
First, it checks whether wi appears in L (line 1). If
so, it tags wi according to D3(wi). Otherwise, it
attempts to label wi based on its context. Specifi-
cally, if (wi?2, wi?1), the word bigram preceding
wi, appears in L (line 3), then wi is tagged accord-
ing to D1(wi?2, wi?1). Otherwise, it backs off to
a unigram distribution: if wi?1, the word preced-
ing wi, appears in L (line 5), then wi is tagged
according to D2(wi?1). Finally, if it is not possi-
ble to tag the word discriminatively (i.e., if all the
above cases fail), it resorts to the sampling equa-
tion (lines 7?8). We apply simulated annealing to
all four cases in this iterative tagging procedure.
5 Evaluation
5.1 Experimental Setup
Corpus Our evaluation corpus is the one used
in the shared task of the IJCNLP-08 Workshop on
NER for South and South East Asian Languages.4
Specifically, we use the portion of the Bengali
dataset that is manually POS-tagged. IIIT Hy-
derabad?s POS tagset5, which consists of 26 tags
specifically developed for Indian languages, has
been used to annotate the data. The corpus is com-
posed of a training set and a test set with approxi-
4The corpus is available from http://ltrc.iiit.ac.in/ner-ssea-
08/index.cgi?topic=5.
5http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf
mately 50K and 30K tokens, respectively. Impor-
tantly, all our POS tagging results will be reported
using only the test set; the training set will be used
for lexicon construction, as we will see shortly.
Tagset We collapse the set of 26 POS tags into
15 tags. Specifically, while we retain the tags cor-
responding to the major POS categories, we merge
some of the infrequent tags designed to capture
Indian language specific structure (e.g., reduplica-
tion, echo words) into a category called OTHERS.
Hyperparameter settings Recall that our tag-
ger consists of three types of distributions ? tag
transition distributions, word-based output distri-
butions, and suffix-based output distributions ?
drawn from a symmetric Dirichlet with ?, ?,
and ? as the underlying hyperparameters, respec-
tively. We automatically determine the values of
these hyperparameters by (1) randomly initializ-
ing them and (2) resampling their values by using
a Metropolis-Hastings update (Gilks et al, 1996)
at the end of each sampling iteration. Details of
this update process can be found in G&G.
Inference Inference is performed by running a
Gibbs sampler for 5000 iterations. The initial tem-
perature is set to 2.0, which is gradually lowered
to 0.08 over the iterations. Owing to the random-
ness involved in hyperparameter initialization, all
reported results are averaged over three runs.
Lexicon construction methods To better under-
stand the role of a POS lexicon in tagging perfor-
mance, we evaluate each POS tagging model by
employing lexicons constructed by three methods.
The first lexicon construction method, arguably
the most unrealistic among the three, follows that
of G&G: for each word, w, in the test set, we (1)
collect from each occurrence of w in the training
set and the test set its POS tag, and then (2) insert
w and all the POS tags collected for w into the
POS lexicon. This method is unrealistic because
(1) in practice, a human needs to list all possible
POS tags for each word in order to construct this
lexicon, thus rendering the resulting tagger con-
siderably less unsupervised than it appears; and
(2) constructing the lexicon using the dataset on
which the tagger is to be evaluated implies that
there is no unseen word w.r.t. the lexicon, thus un-
realistically simplifies the POS tagging task. To
make the method more realistic, G&G also create
a set of relaxed lexicons. Each of these lexicons
includes the tags for only the words that appear
at least d times in the test corpus, where d ranges
368
1 2 3 4 5 6 7 8 9 10
30
40
50
60
70
80
90
d
Ac
cu
ra
cy
 (%
)
(a) Lexicon 1
 
 
MLHMM
BHMM
BHMM+IS
1 2 3 4 5 6 7 8 9 10
30
35
40
45
50
55
60
65
70
75
d
Ac
cu
ra
cy
 (%
)
(b) Lexicon 2
 
 
MLHMM
BHMM
BHMM+IS
Figure 3: Accuracies of POS tagging models using (a) Lexicon 1 and (b) Lexicon 2
from 1 to 10 in our experiments. Any unseen (i.e.,
out-of-dictionary) word is ambiguous among the
15 possible tags. Not surprisingly, both ambigu-
ity and the unseen word rate increase with d. For
instance, the ambiguous token rate increases from
40.0% with 1.7 tags/token (d=1) to 77.7% with 8.1
tags/token (d=10). Similarly, the unseen word rate
increases from 16% (d=2) to 46% (d=10). We will
refer to this set of tag dictionaries as Lexicon 1.
The second method generates a set of relaxed
lexicons, Lexicon 2, in essentially the same way
as the first method, except that these lexicons in-
clude only the words that appear at least d times
in the training data. Importantly, the words that
appear solely in the test data are not included in
any of these relaxed POS lexicons. This makes
Lexicon 2 a bit more realistic than Lexicon 1 in
terms of the way they are constructed. As a result,
in comparison to Lexicon 1, Lexicon 2 has a con-
siderably higher ambiguous token rate and unseen
word rate: its ambiguous token rate ranges from
64.3% with 5.3 tags/token (d=1) to 80.5% with 8.6
tags/token (d=10), and its unseen word rate ranges
from 25% (d=1) to 50% (d=10).
The third method, arguably the most realistic
among the three, is motivated by our proposed
weakly supervised approach. In this method, we
(1) form ten different datasets from the (labeled)
training data of sizes 5K words, 10K words, . . .,
50K words, and then (2) create one POS lexicon
from each dataset L by listing, for each word w in
L, all the tags associated with w in L. This set of
tag dictionaries, which we will refer to as Lexicon
3, has an ambiguous token rate that ranges from
57.7% with 5.1 tags/token (50K) to 61.5% with
8.1 tags/token (5K), and an unseen word rate that
ranges from 25% (50K) to 50% (5K).
5.2 Results and Discussion
5.2.1 Baseline Systems
We use as our first baseline system G&G?s
Bayesian POS tagging model, as our goal is to
evaluate the effectiveness of our two extensions
in improving their model. To further gauge the
performance of G&G?s model, we employ another
baseline commonly used in POS tagging exper-
iments, which is an unsupervised trigram HMM
trained by running EM to convergence.
As mentioned previously, we evaluate each tag-
ging model by employing the three POS lexicons
described in the previous subsection. Figure 3(a)
shows how the tagging accuracy varies with d
when Lexicon 1 is used. Perhaps not surpris-
ingly, the trigram HMM (MLHMM) and G&G?s
Bayesian model (BHMM) achieve almost identi-
cal accuracies when d=1 (i.e., the complete lexi-
con with a zero unseen word rate). As d increases,
both ambiguity and the unseen word rate increase;
as a result, the tagging accuracy decreases. Also,
consistent with G&G?s results, BHMM outper-
forms MLHMM by a large margin (4?7%).
Similar performance trends can be observed
when Lexicon 2 is used (see Figure 3(b)). How-
ever, both baselines achieve comparatively lower
tagging accuracies, as a result of the higher unseen
word rate associated with Lexicon 2.
369
5 10 15 20 25 30 35 40 45 50
45
50
55
60
65
70
75
80
Training data (K)
Ac
cu
ra
cy
 (%
)
Lexicon 3
 
 
SHMM
BHMM
BHMM+IS
BHMM+IS+DP
Figure 4: Accuracies of the POS tagging models
using Lexicon 3
Results using Lexicon 3 are shown in Figure
4. Owing to the availability of POS-tagged sen-
tences, we replace MLHMM with its supervised
counterpart that is trained on the available labeled
data, yielding the SHMM baseline. The accuracies
of SHMM range from 48% to 67%, outperforming
BHMM as the amount of labeled data increases.
5.2.2 Adding Induced Suffix Emission
Next, we augment BHMM with our first
extension, induced suffix emission, yielding
BHMM+IS. For Lexicon 1, BHMM+IS achieves
the same accuracy as the two baselines when d=1.
The reason is simple: as all the test words are
in the POS lexicon, the tagger never emits an in-
duced suffix. More importantly, BHMM+IS beats
BHMM and MLHMM by 4?9% and 10?14%, re-
spectively. Similar trends are observed for Lex-
icon 2, where BHMM+IS outperforms BHMM
and MLHMM by a larger margin of 5?10% and
12?16%, respectively. For Lexicon 3, BHMM+IS
outperforms SHMM, the stronger baseline, by 6?
11%. Overall, these results suggest that induced
suffix emission is a strong performance-enhancing
extension to G&G?s approach.
5.2.3 Adding Discriminative Prediction
Finally, we augment BHMM+IS with discrimi-
native prediction, yielding BHMM+IS+DP. Since
this extension requires labeled data, it can only be
applied in combination with Lexicon 3. As seen
in Figure 4, BHMM+IS+DP outperforms SHMM
by 10?14%. Its discriminative nature proves to be
Predicted Tag Correct Tag % of Error
NN NNP 8.4
NN JJ 6.9
VM VAUX 5.9
Table 1: Most frequent POS tagging errors for
BHMM+IS+DP on the 50K-word training set
strong as it even beats BHMM+IS by 3?4%.
5.2.4 Error Analysis
Table 1 lists the most common types of er-
rors made by the best-performing tagging model,
BHMM+IS+DP (50K-word labeled data). As we
can see, common nouns and proper nouns (row
1) are difficult to distinguish, due in part to the
case insensitivity of Bengali. Also, it is difficult
to distinguish Bengali common nouns and adjec-
tives (row 2), as they are distributionally similar
to each other. The confusion between main verbs
[VM] and auxiliary verbs [VAUX] (row 3) arises
from the fact that certain Bengali verbs can serve
as both a main verb and an auxiliary verb, depend-
ing on the role the verb plays in the verb sequence.
6 Conclusions
While Goldwater and Griffiths?s fully-Bayesian
approach and the traditional maximum-likelihood
parameter-based approach to unsupervised POS
tagging have offered promising results for English,
we argued in this paper that such results were ob-
tained under the unrealistic assumption that a per-
fect POS lexicon is available, which renders these
taggers less unsupervised than they appear. As a
result, we investigated a weakly supervised fully-
Bayesian approach to POS tagging, which relaxes
the unrealistic assumption by automatically ac-
quiring the lexicon from a small amount of POS-
tagged data. Since such relaxation comes at the
expense of a drop in tagging accuracy, we pro-
posed two performance-enhancing extensions to
the Bayesian framework, namely, induced suffix
emission and discriminative prediction, which ef-
fectively exploit morphology and techniques from
supervised POS tagging, respectively.
Acknowledgments
We thank the three anonymous reviewers and
Sajib Dasgupta for their comments. We also thank
CRBLP, BRAC University, Bangladesh, for pro-
viding us with Bengali resources and Taufiq Hasan
Al Banna for his MATLAB code. This work was
supported in part by NSF Grant IIS-0812261.
370
References
Leonard E. Baum. 1972. An equality and associ-
ated maximization technique in statistical estimation
for probabilistic functions of Markov processes. In-
equalities, 3:1?8.
Alexander Clark. 2000. Inducing syntactic categories
by context distribution clustering. In Proceedings of
CoNLL: Short Papers, pages 91?94.
Alexander Clark. 2003. Combining distributional and
morphological information for part-of-speech induc-
tion. In Proceedings of the EACL, pages 59?66.
Sajib Dasgupta and Vincent Ng. 2007. Unsupervised
part-of-speech acquisition for resource-scarce lan-
guages. In Proceedings of EMNLP-CoNLL, pages
218?227.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39:1?38.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 6:721?741.
Walter R. Gilks, Sylvia Richardson, and David
J. Spiegelhalter (editors). 1996. Markov Chain
Monte Carlo in Practice. Chapman & Hall, Suffolk.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings of ACL-
08:HLT, pages 746?754.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153?198.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744?751.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT-NAACL, pages 320?327.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of EMNLP-
CoNLL, pages 296?305.
Samarth Keshava and Emily Pitler. 2006. A simpler,
intuitive approach to morpheme induction. In PAS-
CAL Challenge Workshop on Unsupervised Segmen-
tation of Words into Morphemes.
David J. C. MacKay and Linda C. Bauman Peto. 1995.
A hierarchical Dirichlet language model. Natural
Language Engineering, 1:289?307.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?172.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of EACL, pages 141?148.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the ACL, pages 354?362.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay. 2008. Unsupervised multi-
lingual learning for POS tagging. In Proceedings of
EMNLP, pages 1041?1050.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay. 2009. Adding more lan-
guages improves unsupervised multilingual tagging.
In Proceedings of NAACL-HLT.
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1527?1554.
Kristina Toutanova and Mark Johnson. 2007. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of NIPS.
Qin Iris Wang and Dale Schuurmans. 2005. Im-
proved estimation for unsupervised part-of-speech
tagging. In Proceedings of the 2005 IEEE Interna-
tional Conference on Natural Language Processing
and Knowledge Engineering (IEEE NLP-KE), pages
219?224.
371
Book Reviews
Semisupervised Learning for Computational Linguistics
Steven Abney
(University of Michigan)
Boca Raton, FL: Chapman & Hall / CRC (Computer science and data analysis series,
edited by David Madigan et al), 2007, xi+308 pp; hardbound, ISBN 978-1-58488-559-7,
$79.95, ?44.99
Reviewed by
Vincent Ng
University of Texas at Dallas
Semi-supervised learning is by no means an unfamiliar concept to natural language
processing researchers. Labeled data has been used to improve unsupervised parameter
estimation procedures such as the EM algorithm and its variants since the beginning
of the statistical revolution in NLP (e.g., Pereira and Schabes 1992). Unlabeled data
has also been used to improve supervised learning procedures, the most notable ex-
amples being the successful applications of self-training and co-training to word sense
disambiguation (Yarowsky 1995) and named entity classification (Collins and Singer
1999).
Despite its increasing importance, semi-supervised learning is not a topic that is
typically discussed in introductorymachine learning texts (e.g., Mitchell 1997; Alpaydin
2004) or NLP texts (e.g., Manning and Schu?tze 1999; Jurafsky and Martin 2000).1
Consequently, to learn about semi-supervised learning research, one has to consult the
machine-learning literature. This can be a daunting task for NLP researchers who have
little background in machine learning. Steven Abney?s book Semisupervised Learning for
Computational Linguistics is targeted precisely at such researchers, aiming to provide
them with a ?broad and accessible presentation? of topics in semi-supervised learning.
According to the preamble, the reader is assumed to have taken only an introductory
course in NLP ?that include[s] statistical methods?concretely the material contained
in Jurafsky and Martin (2000) and Manning and Schu?tze (1999).? Nonetheless, I agree
with the author that any NLP researcher who has a solid background in machine
learning is ready to ?tackle the primary literature on semisupervised learning, and will
probably not find this book particularly useful? (page 11).
As the author promises, the book is self-contained and quite accessible to those
who have little background in machine learning. In particular, of the 12 chapters
in the book, three are devoted to preparatory material, including: a brief introduc-
tion to machine learning, basic unconstrained and constrained optimization tech-
niques (e.g., gradient descent and the method of Lagrange multipliers), and relevant
linear-algebra concepts (e.g., eigenvalues, eigenvectors, matrix and vector norms,
1 Although Manning and Schu?tze (1999) and Jurafsky and Martin (2000) do discuss self-training, they do
so only in the context of Yarowsky?s word sense disambiguation algorithm.
Computational Linguistics Volume 34, Number 3
diagonalization). The remaining chapters focus roughly on six types of semi-supervised
learning methods:2
 Self-training. After introducing the self-training algorithm and its variants,
the author discusses its applications in NLP and its relationship to other
semi-supervised learning algorithms.
 Agreement-based methods. The co-training algorithm, along with a
theoretical analysis of its conditional independence assumption and its
applications in NLP, are presented. Additionally, a random field, which
penalizes disagreement among neighboring nodes, is introduced as an
alternative way of enforcing agreement.
 Clustering algorithms. Basic hard clustering algorithms (e.g., k-means,
graph mincuts, hierarchical clustering), EM (as a soft clustering
algorithm), and their role in semi-supervised learning are discussed.
 Boundary-oriented methods. Two discriminative learning algorithms,
boosting and support vector machines, are introduced as a means to
facilitate the discussion of their semi-supervised counterparts: co-boosting
and transductive SVMs.
 Label propagation in graphs. In graph-based approaches to semi-supervised
learning, the labels of the labeled nodes are propagated to the unlabeled
nodes, with the goal of maximizing the agreement of the labels of
proximate nodes. The author shows that this goal is equivalent to finding a
harmonic function given the labeled nodes, and presents several algorithms,
including the method of relaxation, for computing this function.
 Spectral methods. Spectral methods for semi-supervised learning can be
viewed as interpolation across a partially labeled graph as described
previously using a ?standing wave.? The author explains the connection
between such a wave and the spectrum of a matrix, and establishes the
relationship of spectral clustering algorithms to other semi-supervised
learners, including graph mincuts, random walks, and label propagation.
The book is rich in theory and algorithms, and although it is targeted at those who
lack relevant mathematical background, each theory and algorithm is presented in a
rigorous manner.
Another nice feature of the book is that it reveals the connection among seemingly
disparate ideas. As mentioned earlier, it shows that many semi-supervised learners
can in fact be viewed as self-training; also, the description of the connection between
spectral clustering and other semi-supervised learners is insightful.
In addition, I like the organization of the book. One reason is the presentation of
co-training: Although the algorithm is presented in Chapter 2, its theoretical underpin-
nings are not described until Chapter 9. This enables the reader to see its applications
in NLP (in Chapter 3) before going through the mathematics, which could be important
for researchers who are linguistically but not mathematically oriented. Another reason
2 The presentation of the methods here does not reflect the order in which they are introduced in the
book; rather, it is motivated by the book?s Section 1.3, which gives an overview of the ?leading ideas?
of the book.
450
Book Reviews
is that the preparatory material is presented on a need-to-know basis. This allows the
discussion of algorithmic ideas as soon as the reader grasps the relevant fundamentals.
For instance, function optimization and basic linear algebra concepts are presented
in separate chapters, with the latter being deferred to Chapter 11, right before the
discussion of spectral clustering in Chapter 12.
Whereas the discussion of self-training and co-training is complemented by their
application to NLP problems, the same is not true for the remaining semi-supervised
learners described in the book. The reader is often left to imagine the potential NLP
applications of these learners, and as a consequence is unable to gain an understanding
of the state of the art of semi-supervised learning for NLP. In fact, given its scarcity of
NLP applications, the book perhaps does not merit its current title. It does have a rich
bibliography on semi-supervised learning for NLP, but most of the references are not
cited in the text.
The book also lacks a discussion of the practical issues in applying the semi-
supervised learners. For instance, the author does not mention that in practice it is
not easy to choose k in k-means clustering, merely describing k as a parameter of the
clustering algorithm. As another example, when introducing the EM algorithm, the
author applies it to a generative model that can be expressed in exponential form,
without acknowledging that one of themost difficult issues surrounding the application
of EM concerns the design of the right generative model given the data. The lack of
NLP applications in the book has unfortunately enabled the author to sidestep these
practical issues. On a related note, one can hardly find any discussions of the strengths
and weaknesses of the semi-supervised learners in the book. This could leave the reader
without the ability to choose the best learner for a given NLP problem, and is probably
another undesirable consequence of the book?s reluctance to discuss NLP applications.
The author?s decision to focus exclusively on semi-supervised classification prob-
lems effectively limits the scope of the book. One consequence of this decision is that the
reader may not be able to apply the EM algorithm to train a hidden Markov model for
solving sequence-learning problems as basic as part-of-speech tagging upon completion
of this book. Given the recent surge of interest in structure prediction in the NLP
community, and the fact that co-training and semi-supervised EM have been applied
to structure-prediction problems such as statistical parsing and part-of-speech tagging,
the book?s sole focus on classification problem is perhaps one of its weaknesses.
There are a few occasions on which the reader might not get a complete picture
of the capability of an algorithm. For instance, the reader might think that spectral
methods can be applied only to binary classification tasks, owing to the book?s exclusive
focus on such tasks in its discussion of spectral clustering. Similarly for the treatment
of support vector machines: The reader may get the impression that SVMs cannot be
used to learn non-linear functions, as the discussion of kernels is deliberately omitted
due to their irrelevance to transductive learning. Although it is important to keep the
presentation focused, I believe that the author could easily have removed potential con-
fusions by explicitly stating the full capability of an algorithm and referring the reader
to the relevant papers for details.
Given the rapid growth of semi-supervised learning research in the past decade,
there is currently a need for a broad and accessible reference to this area of research.
Abney?s book serves this purpose in spite of the aforementioned weaknesses, and I
believe that it is a useful starting point for any non?machine-learning experts who
intend to apply semi-supervised learning techniques to their research. As someone who
has some prior knowledge of semi-supervised learning, I still find this book insightful:
It reveals deep connections among apparently disparate ideas. If I were to teach a course
451
Computational Linguistics Volume 34, Number 3
on semi-supervised learning for NLP, I would undoubtedly use this book as a primary
reference.
References
Alpaydin, Ethem. 2004. Introduction to
Machine Learning. The MIT Press,
Cambridge, MA.
Collins, Michael and Yoram Singer. 1999.
Unsupervised models for named entity
classification. In Proceedings of the 1999
Joint Conference on Empirical Methods in
Natural Language Processing and Very
Large Corpora, pages 100?110, College
Park, MD.
Jurafsky, Daniel and James H. Martin. 2000.
Speech and Language Processing. Prentice
Hall, Upper Saddle River, NJ.
Manning, Christopher D. and Hinrich
Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. The MIT
Press, Cambridge, MA.
Mitchell, Tom M.?1997. Machine Learning.
McGraw Hill, Columbus, OH.
Pereira, Fernando and Yves Schabes. 1992.
Inside-outside reestimation from partially
bracketed corpora. In Proceedings of the
30th Annual Meeting of the Association for
Computational Linguistics, pages 128?135,
Newark, DE.
Yarowsky, David. 1995. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd
Annual Meeting of the Association for
Computational Linguistics, pages 189?196,
Cambridge, MA.
Vincent Ng is an assistant professor in the Department of Computer Science at the University of
Texas at Dallas. He is also affiliated with the university?s Human Language Technology Research
Institute, where he conducts research on statistical natural language processing and teaches un-
dergraduate and graduate courses inmachine learning. Ng?s address is: Department of Computer
Science, University of Texas at Dallas, Richardson, TX 75080-0688; e-mail: vince@hlt.utdallas.edu.
452
Proceedings of NAACL HLT 2007, pages 155?163,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
High-Performance, Language-Independent Morphological Segmentation 
 
 
Sajib Dasgupta and Vincent Ng 
Human Language Technology Research Institute 
University of Texas at Dallas 
Richardson, TX 75083-0688 
{sajib,vince}@hlt.utdallas.edu 
 
 
 
Abstract 
This paper introduces an unsupervised 
morphological segmentation algorithm 
that shows robust performance for four 
languages with different levels of mor-
phological complexity. In particular, our 
algorithm outperforms Goldsmith?s Lin-
guistica and Creutz and Lagus?s Mor-
phessor for English and Bengali, and 
achieves performance that is comparable 
to the best results for all three PASCAL 
evaluation datasets. Improvements arise 
from (1) the use of relative corpus fre-
quency and suffix level similarity for de-
tecting incorrect morpheme attachments 
and (2) the induction of orthographic rules 
and allomorphs for segmenting words 
where roots exhibit spelling changes dur-
ing morpheme attachments. 
1 Introduction 
Morphological analysis is the task of segmenting a 
word into morphemes, the smallest meaning-
bearing elements of natural languages. Though 
very successful, knowledge-based morphological 
analyzers operate by relying on manually designed 
segmentation heuristics (e.g. Koskenniemi (1983)), 
which require a lot of linguistic expertise and are 
time-consuming to construct. As a result, research 
in morphological analysis has exhibited a shift to 
unsupervised approaches, in which a word is typi-
cally segmented based on morphemes that are 
automatically induced from an unannotated corpus. 
Unsupervised approaches have achieved consider-
able success for English and many European lan-
guages (e.g. Goldsmith (2001), Schone and Juraf-
sky (2001), Freitag (2005)). The recent PASCAL 
Challenge on Unsupervised Segmentation of 
Words into Morphemes1  has further intensified 
interest in this problem, selecting as target lan-
guages English as well as two highly agglutinative 
languages, Turkish and Finnish. However, the 
evaluation of the Challenge reveals that (1) the 
success of existing unsupervised morphological 
parsers does not carry over to the two agglutinative 
languages, and (2) no segmentation algorithm 
achieves good performance for all three languages. 
Motivated by these state-of-the-art results, our 
goal in this paper is to develop an unsupervised  
morphological segmentation algorithm that can 
work well across different languages. With this 
goal in mind, we evaluate our algorithm on four 
languages with different levels of morphological 
complexity, namely English, Turkish, Finnish and 
Bengali. It is worth noting that Bengali is an under-
investigated Indo-Aryan language that is highly 
inflectional and lies between English and Turk-
ish/Finnish in terms of morphological complexity. 
Experimental results demonstrate the robustness of 
our algorithm across languages: it not only outper-
forms Goldsmith?s (2001) Linguistica and Creutz 
and Lagus?s (2005) Morphessor for English and 
Bengali, but also compares favorably to the best-
performing PASCAL morphological parsers when 
evaluated on all three datasets in the Challenge.  
The performance improvements of our segmen-
tation algorithm over existing morphological ana-
lyzers can be attributed to our extending Keshava 
and Pitler?s (2006) segmentation method, the best 
performer for English in the aforementioned 
                                                          
1
 http://www.cis.hut.fi/morphochallenge2005/ 
155
PASCAL Challenge, with the capability of han-
dling two under-investigated problems: 
Detecting incorrect attachments. Many existing 
morphological parsers incorrectly segment ?candi-
date? as ?candid?+?ate?, since they fail to identify 
that the morpheme ?ate? should not attach to the 
word ?candid?. Schone and Jurafsky?s (2001) work 
represents one of the few attempts to address this 
inappropriate morpheme attachment problem, in-
troducing a method that exploits the semantic re-
latedness between word pairs. In contrast, we 
propose two arguably simpler, yet effective tech-
niques that rely on relative corpus frequency and 
suffix level similarity to solve the problem. 
Inducing orthographic rules and allomorphs.  
One problem with Keshava and Pitler?s algorithm 
is that it fails to segment words where the roots 
exhibit spelling changes during attachment to mor-
phemes (e.g. ?denial? = ?deny?+?al?). To address 
this problem, we automatically acquire allomorphs 
and orthographic change rules from an unannotated 
corpus. These rules also allow us to output the ac-
tual segmentation of the words that exhibit spelling 
changes during morpheme attachment, thus avoid-
ing the segmentation of ?denial? as ?deni?+?al?, as 
is typically done in existing morphological parsers. 
    In addition to addressing the aforementioned 
problems, our segmentation algorithm has two ap-
pealing features. First, it can segment words with 
any number of morphemes, whereas many analyz-
ers can only be applied to words with one root and 
one suffix (e.g. D?Jean (1998), Snover and Brent 
(2001)). Second, it exhibits robust performance 
even when inducing morphemes from a very large 
vocabulary, whereas Goldsmith?s (2001) and 
Freitag?s (2005) morphological analyzers perform 
well only when a small vocabulary is employed, 
showing deteriorating performance as the vocabu-
lary size increases.  
The rest of this paper is organized as follows. 
Section 2 presents related work on unsupervised 
morphological analysis. In Section 3, we describe 
our basic morpheme induction algorithm. We then 
show how to exploit the induced morphemes to (1) 
detect incorrect attachments by using relative cor-
pus frequency (Section 4) and suffix level similar-
ity (Section 5) and (2) induce orthographic rules 
and allomorphs (Section 6). Section 7 describes 
our algorithm for segmenting a word using the in-
duced morphemes. We present evaluation results 
in Section 8 and conclude in Section 9. 
2 Related Work 
As mentioned in the introduction, the problem of 
unsupervised morphological learning has been ex-
tensively studied for English and many other 
European languages. In this section, we will give 
an overview of the related work on this problem. 
Harris (1955) develops a strategy for identifying 
morpheme boundaries that checks whether the 
number of different letters following a sequence of 
letters exceeds some given threshold. D?Jean 
(1998) improves Harris?s segmentation algorithm 
by first inducing a list of 100 most frequent mor-
phemes and then using those morphemes for word 
segmentation. The aforementioned PASCAL Chal-
lenge on Unsupervised Word Segmentation has 
undoubtedly intensified interest in this problem. 
Among the participating groups, Keshava and Pit-
ler?s (2006) segmentation algorithm combines the 
ideas of D?Jean and Harris and achieves the best 
result for the English dataset, but it only offers me-
diocre performance for Finnish and Turkish.   
There is another class of unsupervised morpho-
logical learning algorithms whose design is driven 
by the Minimum Description Length (MDL) prin-
ciple. Specifically, EM is used to iteratively seg-
ment a list of words using some predefined 
heuristics until the length of the morphological 
grammar converges to a minimum. Brent et al 
(1995) are the first to introduce an information-
theoretic notion of compression to represent the 
MDL framework. Goldsmith (2001) also adopts 
the MDL approach, providing a new compression 
system that incorporates signatures when measur-
ing the length of the morphological grammar. 
Creutz (2003) proposes a probabilistic maximum a 
posteriori formulation that uses prior distributions 
of morpheme length and frequency to measure the 
goodness of an induced morpheme, achieving bet-
ter results for Finnish but worse results for English 
in comparison to Goldsmith?s Linguistica. 
3 The Basic Morpheme Induction Algo-
rithm 
Our unsupervised segmentation algorithm is com-
posed of two steps: (1) inducing prefixes, suffixes 
and roots from a vocabulary that consists of words 
taken from a large corpus, and (2) segmenting a 
word using these induced morphemes. This section 
describes our basic morpheme induction method. 
156
3.1 Extracting a List of Candidate Affixes 
The first step of our morpheme induction method 
involves extracting a list of candidate prefixes and 
suffixes. We rely on a fairly simple idea originally 
proposed by Keshava and Pitler (2006) for extract-
ing candidate affixes. Assume that    and 

 are two 
character sequences and  

 is the concatenation of 
 
 and 

. If  

 and    are both found in the vocabu-
lary, then we extract 

 as a candidate suffix. Simi-
larly, if  

 and 

 are both found in the vocabulary, 
then we extract    as a candidate prefix.  
The above affix induction method is arguably 
overly simplistic and therefore can generate many 
spurious affixes. To filter spurious affixes, we (1) 
score each affix by multiplying its frequency (i.e. 
the number of distinct words to which each affix 
attaches) and its length2, and then (2) retain only 
the K top-scoring affixes, where K is set differently 
for prefixes and suffixes. The value of K is some-
what dependent on the vocabulary size, as the af-
fixes in a larger vocabulary system are generated 
from a larger number of words.  For example, we 
set the thresholds to 70 for prefixes and 50 for suf-
fixes for English; on the other hand, since the Fin-
nish vocabulary is almost six times larger than that 
of English, we set the corresponding thresholds to 
be approximately six times larger (400 and 300 for 
prefixes and suffixes respectively).3  
3.2 Detecting Composite Suffixes 
Next, we detect and remove composite suffixes (i.e. 
suffixes that are formed by combining multiple 
suffixes [e.g. ?ers? = ?er?+?s?]) from our induced 
suffix list, because their presence can lead to un-
der-segmentation of words (e.g. ?walkers?, whose 
correct segmentation is ?walk?+?er?+?s?, will be 
erroneously segmented as ?walk?+?ers?). Compos-
ite suffix detection is a particularly important prob-
lem for languages like Bengali in which composite 
suffixes are abundant (see Dasgupta and Ng 
(2007)). Note, however, that simple concatenation 
of multiple suffixes does not always produce a 
composite suffix. For example, ?ent?, ?en? and ?t? 
all are valid suffixes in English, but ?ent? is not a 
                                                          
2
 The dependence on frequency and length is motivated by the observation that 
less-frequent and shorter affixes (especially those of length 1) are more likely to 
be erroneous (see Goldsmith (2001)). 
3
 Since this method for setting our vocabulary-dependent thresholds is fairly 
simple, the use of these thresholds should not be viewed as rendering our seg-
mentation algorithm language-dependent.    
composite suffix. Hence, we need a more sophisti-
cated method for composite suffix detection.  
 Our detection method is motivated by the fol-
lowing observation: if xy is a composite suffix and 
a word w combines with xy, then it is highly likely 
that w will also combine with its first component 
suffix x. Note that this property does not hold for 
non-composite suffixes. For instance, words that 
combine with the non-composite suffix ?ent? (e.g. 
?absorb?) do not combine with its first component 
suffix ?en?. Consequently, given two suffixes x 
and y, our method posits xy as a composite suffix if 
xy and x are similar in terms of the words to which 
they attach. Specifically, we consider xy and x to 
be similar if their similarity value as computed by 
the formula below is greater than 0.6: 
||
|?|)|(),(
W
W
xyxPxxySimilarity == , 
where |W  | is the number of distinct words that 
combine with both xy and x, and |W| is the number 
of  distinct words that combine with xy. 
3.3 Extracting a List of Candidate Roots 
Finally, we extract a list of candidate roots using 
the induced list of affixes as follows. For each 
word, w, in the vocabulary, we check whether w is 
divisible, i.e. whether w can be segmented as r+x 
or p+r, where p is an induced prefix, x is an in-
duced suffix, and r is a word in the vocabulary. We 
then add w to the root list if it is not divisible. 
Note, however, that the resulting root list may con-
tain compound words (i.e. words with multiple 
roots). Hence, we make another pass over our root 
list to remove any word that is a concatenation of 
multiple words in the vocabulary. 
4 Detecting Incorrect Attachments Using 
Relative Frequency 
Our induced root list is not perfect: many correct 
roots are missing due to over-segmentation. For 
example, since ?candidate? and ?candid? are in the 
vocabulary and ?ate? is an induced suffix, our root 
induction method will incorrectly segment ?candi-
date? as ?candid?+?ate?; as a result, it does not 
consider ?candidate? as a root. So, to improve the 
root induction method, we need to determine that 
the attachment of the morpheme ?ate? to the root 
word ?candid? is incorrect. In this section, we pro-
pose a simple yet novel idea of using relative cor-
157
pus frequency to determine whether the attachment 
of a morpheme to a root word is plausible or not.  
Consider again the two words ?candidate? and 
?candid?. While ?candidate? occurs 6380 times in 
our corpus, ?candid? occurs only 119 times. This 
frequency disparity can be an important clue to 
determining that there is no morphological relation 
between ?candidate? and ?candid?. Similar obser-
vation is also made by Yarowsky and Wicentowski 
(2000), who successfully employ relative fre-
quency similarity or disparity to rank candidate 
VBD/VB pairs (e.g. ?sang?/?sing?) that are irregu-
lar in nature. Unlike Yarowsky and Wicentowski, 
however, our goal is to detect incorrect affix at-
tachments and improve morphological analysis.  
Our incorrect attachment detection algorithm, 
which exploits frequency disparity, is based on the 
following hypothesis: if a word w is formed by 
attaching an affix m to a root word r, then the cor-
pus frequency of w is likely to be less than that of r 
(i.e. the frequency ratio of w to r is less than one). 
In other words, we hypothesize that the inflectional 
or derivational forms of a root word occur less fre-
quently in a corpus than the root itself.  
To illustrate this hypothesis, Table 1 shows 
some randomly chosen English words together 
with their word-root frequency ratios (WRFRs).  
The <word, root> pairs in the left side of the table 
are examples of correct attachments, whereas those 
in the right side are not. Note that only those words 
that represent correct attachments have a WRFR 
less than 1. 
The question, then, is: to what extent does our 
hypothesis hold? To investigate this question, we 
generated examples of correct attachments by ran-
domly selecting 400 words from our English vo-
cabulary and then removing those that are root 
words, proper nouns, or compound words. We then 
manually segmented each of the remaining 378 
words as Prefix+Root or Root+Suffix with the aid 
of the CELEX lexical database (Baayean et al, 
1996). Somewhat surprisingly, we found that the 
WRFR is less than 1 in only 71.7% of these at-
tachments. When the same experiment was re-
peated on 287 hand-segmented Bengali words, the 
hypothesis achieves a higher accuracy of 83.6%.  
To better understand why our hypothesis does 
not work well for English, we measured its accu-
racy separately for the Root+Suffix words and the 
Prefix+Root words, and found that the hypothesis 
fails mostly on the suffixal attachments (see Table 
2).  Though surprising at first glance, the relatively 
poor accuracy on suffixal attachments can be at-
tributed to the fact that many words in English 
(e.g. ?amusement?, ?winner?) appear more fre-
quently in our corpus than their corresponding root 
forms (e.g. ?amuse?, ?win?). For Bengali, our hy-
pothesis fails mainly on verbs, whose inflected 
forms occur more often in our corpus than their 
roots. This violation of the hypothesis can be at-
tributed to the grammatical rule that the main verb 
of a Bengali sentence has to be inflected according 
to the subject in order to maintain sentence order. 
To improve the accuracy of our hypothesis on 
detecting correct attachments, we relax our initial 
hypothesis as follows: if an attachment is correct, 
then the corresponding WRFR is less than some 
predefined threshold t, where t > 1. However, we 
do not want t to be too large, since our algorithm 
may then determine many incorrect attachments as 
correct. In addition, since our hypothesis has a high 
accuracy for prefixal attachments than suffixal at-
tachments, the threshold we employ for prefixes 
can be smaller than that for suffixes. Taking into 
account these considerations, we use a threshold of 
10 for suffixes and 2 for prefixes for all the lan-
guages we consider in this paper. 
 
Correct Parses Incorrect Parses 
Word Root WRFR Word Root WRFR 
bear-able bear 0.01 candid-ate candid 53.6 
attend-ance attend 0.24 medic-al medic 483.9 
arrest-ing arrest 0.06 prim-ary prim 327.4 
sub-group group 0.0002 ac-cord cord 24.0 
re-cycle cycle 0.028 ad-diction diction 52.7 
un-settle settle 0.018 de-crease crease 20.7 
Table 1: Word-root frequency ratios 
 
 Root+Suffix Prefix+Root Overall 
 # of words 344 34 378 
WRFR < 1 70.1% 88.2% 71.7% 
Table 2: Hypothesis validation for English 
 
Now we can employ our hypothesis to detect in-
correct attachments and improve root induction as 
follows. For each word, w, in the vocabulary, we 
check whether (1) w can be segmented as r+x or 
p+r, where p and x are valid prefixes and suffixes 
respectively and r is another word in the vocabu-
lary, and (2) the WRFR for w and r is less than our 
predefined thresholds (10 for suffixes and 2 for 
prefixes). If both conditions are satisfied, it means 
that w is divisible. Hence, we add w into the list of 
roots if at least one of the conditions is violated. 
158
5 Suffix Level Similarity 
Many of the incorrect suffixal attachments have a 
WRFR between 1 and 10, but the detection algo-
rithm described in the previous section will deter-
mine all of them as correct attachments. Hence, in 
this section, we propose another technique, which 
we call suffix level similarity, to identify some of 
these incorrect attachments. 
Suffix level similarity is motivated by the fol-
lowing observation: if a word w combines with a 
suffix x, then w should also combine with the suf-
fixes that are ?morphologically similar? to x. To 
exemplify, consider the suffix ?ate? and the root 
word ?candid?. The words that combine with the 
suffix ?ate? (e.g. ?alien?, ?fabric?, ?origin?) also 
combine with suffixes like ?ated?, ?ation? and ?s?. 
Given this observation, the question of whether 
?candid? combines with the suffix ?ate? then lies 
in whether or not ?candid? combines with ?ated?, 
?s? and ?ation?. The fact that ?candid? does not 
combine with many of the above suffixes provides 
suggestive evidence that ?candidate? cannot be 
derived from ?candid?.  
More specifically, to check whether a word w 
combines with a suffix x using suffix level simial-
rity, we (1) find the set of words Wx that can com-
bine with x; (2) find the set of suffixes Sx that 
attach to all of the words in Wx under the constraint 
that Sx does not contain x; and (3) find the 10 suf-
fixes in Sx that are most ?similar? to x. The ques-
tion, then, is how to define similarity. Intuitively, a 
good similarity metric should reflect, for instance, 
the fact that ?ated? is a better suffix to consider in 
the attachment decision for ?ate? than ?s? (i.e. 
?ated? is more similar to ?ate? than ?s?), since ?s? 
attaches to most nouns and verbs in English and 
hence should not be a distinguishing feature for 
incorrect attachment detection.  
We employ a probabilistic measure (PM) that 
computes the similarity between suffixes x and y as 
the product of (1) the probability of a word com-
bining with y given that it combines with x and (2) 
the probability of a word combining with x given 
that it combines with y. More specifically, 
,*)|(*)|(),(
21 n
n
n
nyxPxyPyxPM ==  
where n1 is the number of distinct words that com-
bine with x, n2 is the number of distinct words that 
combine with y, and n is the number of distinct 
words that combine with both x and y.4  
After getting the 10 suffixes that are most simi-
lar to x, we employ them as features and use the 
associated similarity values (we scale them linearly 
between 1 and 10) as the weights of these 10 fea-
tures. The decision of whether a suffix x can attach 
to a word w depends on whether the following ine-
quality is satisfied: 
,
10
1
twf ii >  
where fi is a boolean variable that has the value 1 if 
w combines with xi, where xi is one of the 10 suf-
fixes that are most similar to x; wi is the scaled 
similarity between x and xi; and t is a predefined 
threshold that is greater than 0. 
One potential problem with suffix level similar-
ity is that it is an overly strict condition for those 
words that combine with only one or two suffixes 
in the vocabulary. For example, if the word ?char-
acter? has just one variant in the vocabulary (e.g. 
?characters?), suffix level similarity will determine 
the attachment of ?s? to ?character? as incorrect, 
since the weighted sum in the above inequality will 
be 0. To address this sparseness problem, we rely 
on both relative corpus frequency and suffix level 
similarity to identify incorrect attachments. Spe-
cifically, if the WRFR of a <word, root> pair is 
between 1 and 10, we determine that an attachment 
to the root is incorrect if 
 
-WRFR +    * (suffix level similarity) < 0, 
 
where    is set to 0.15. 
Finally, since long words have a higher chance 
of getting segmented, we do not apply suffix level 
similarity to words whose length is greater than 10. 
6 Inducing Orthographic Rules and Al-
lomorphs 
The biggest drawback of the system, described 
thus far, is its failure to segment words where the 
roots exhibit spelling changes during attachment to 
morphemes (e.g. ?denial? = ?deny?+?al?). The 
reasons are (1) the system does not have any 
knowledge of language-specific orthographic rules 
(e.g. in English, the character ?y? at the morpheme 
boundary is changed to ?i? when the root combines 
                                                          
4
 Note that this metric has the desirable property of returning a low similarity 
value for ?s?: while n is likely to be large, it will be offset by a large n2. 
159
with the suffix ?al?), and (2) the vocabulary we 
employ for morpheme induction does not normally 
contain the allomorphic variations of the roots 
(e.g. ?deni? is allomorphic variation of ?deny?). To 
segment these words correctly, we need to generate 
the allomorphs and orthographic rules automati-
cally given a set of induced roots and affixes.  
Before giving the details of the generation 
method, we note that the induction of orthographic 
rules is a challenging problem, since different lan-
guages exhibit orthographic changes in different 
ways. For some languages (e.g. English) rules are 
mostly predictable, whereas for others (e.g. Fin-
nish) rules are highly irregular. It is hard to obtain 
a generalized mapping function that aligns every 
<root, allomorph> pair, considering the fact that 
our system is unsupervised. An additional chal-
lenge is to ensure that the incorporation of these 
orthographic rules will not adversely affect system 
performance (i.e. they will not be applied to regu-
lar words and thus segment them incorrectly). 
Yarowsky and Wicentowski (2000) propose an 
interesting algorithm that employs four similarity 
measures to successfully identify the most prob-
able root of a highly irregular word. Unlike them, 
however, our goal is to (1) check whether the 
learned rules can actually improve an unsupervised 
morphological system, not just to align <root, al-
lomorph> pair, and (2) examine whether our sys-
tem is extendable to different languages.  
Taking into consideration the aforementioned 
challenges, our induction algorithm will (1) handle 
orthographic character changes that occur only at 
morpheme boundaries; (2) generate allomorphs for 
suffixal attachments only5, assuming that roots ex-
hibit the character changes during attachment, not 
suffixes; and (3) learn rules that aligns <root, allo-
morph> pairs of edit distance 1 (which may in-
volve 1-character replacement, deletion or 
insertion). Despite these limitations, we will see 
that the incorporation of the induced rules im-
proves segmentation accuracy significantly. 
 Let us first discuss how we learn a replacement 
rule, which identifies <allomorph, root> pairs 
where the last character of the root is replaced by 
another character. The steps are as follows: 
(1) Inducing candidate allomorphs 
If   A

 is a word in the vocabulary (e.g. ?denial?, 
where   =?den?, A=?i?, and 

=?al?),   is an in-
                                                          
5
 We only learn rules for suffixes of length greater than 1, since most suffixes of 
length 1 do not participate in orthographic changes.  
duced suffix,   B is an induced root (e.g. ?deny?, 
where B=?y?), and the attachment of   to   B is 
correct according to relative corpus frequency (see 
Section 4), then we hypothesize that   A is an allo-
morph of   B. For each induced suffix, we use this 
hypothesis to generate the allomorphs and identify 
those that are generated from at least two suffixes 
as candidate allomorphs. We denote the list of 
<candidate allomorph, root, suffix> tuples by L. 
(2) Learning orthographic rules 
Every <candidate allomorph, root, suffix> tuple as 
learned above is associated with an orthographic 
rule. For example, from the words ?denial?, ?deny? 
and suffix ?al?, we learn the rule ?y:i / _ + al?6; 
from ?social?, ?sock? and ?al?, we learn the rule 
?k:i / _ + al?, which, however, is erroneous. So, we 
check whether each of the learned rules occurs fre-
quently enough for all the <allomorph, root> pairs 
associated with a suffix, with the goal of filtering 
the low-frequency orthographic rules. Specifically, 
for each suffix 

, we repeat the following steps: 
(a) Counting the frequency of rules. Let L   be the 
list of <candidate allomorph, root> pairs in L that 
are associated with the suffix 

. For each pair p in 
L  , we first check whether its candidate allomorph 
appears in any other <candidate allomorph, root> 
pairs in L  . If not, we increment the frequency of 
the orthographic rule associated with p by 1. For 
example, the pair <?deni?, ?deny?> increases the 
frequency of the rule ?y:i? by 1 on condition that 
?deni? does not appear in any other pairs.  
(b) Filtering the rules. We first remove the infre-
quent rules, specifically those that are induced by 
less than 15% of the tuples in L  . Then we check 
whether there exists two rules of the form A:B and 
A:C in the induced rule list. If so, then we have a 
morphologically undesirable situation where the 
character A changes to B and C under the same 
environment (i.e.  ). To address this problem, we 
first calculate the strength of a rule as follows: 
=
@
@):(
):():(
Afrequency
BAfrequencyBAstrength  
We then retain only those rules whose fre-
quency*strength is greater than some predefined 
threshold. We denote the list of rules that satisfy 
the above constraints by R  . 
(c) Identifying valid allomorphs. For each rule in 
R  , we identify the associated <candidate allo-
                                                          
6
 This is the Chomsky and Halle notation for representing orthographic rules. a:b 
/ c _ d means a changes to b when the left context is c and the right context is d. 
160
morph, root> pairs in L  . We refer to the candidate 
allomorphs in each of those pairs as valid allo-
morphs and add them to the list of roots. We also 
remove from the original root list the words that 
can be segmented by the induced allomorphs and 
the associated rules (e.g. ?denial?). 
(d) Identifying composite suffixes. For each rule 
in R  , we also check whether it can identify com-
posite suffixes where the first component suffix?s 
last character is replaced during attachment to the 
second component suffix (e.g.  ?liness? = 
?ly?+?ness?). Specifically, if (1) A:B / _   is a rule 
in R  , (2)   A  (say ?liness?),   (say ?ness?) and   B 
(say ?ly?) are induced suffixes, and (3)   A  satis-
fies the requirements of a composite suffix (see 
Section 3.2), then we determine that   A  is a com-
posite suffix composed of   B and 

. 
We employ the same procedure for learning in-
sertion and deletion rules, except that strength is 
always set to 1 for these two types of rules. The 
threshold we set at step (b) is somewhat dependent 
on the vocabulary size, since the frequency count 
of each rule will naturally be larger when a larger 
vocabulary is used. Following our method for set-
ting vocabulary-dependent thresholds (see Section 
3.1), we set the threshold to 4 for English and 25 
for Finnish, for instance. 
Finally, we adapt our candidate allomorph de-
tection method described above to induce allo-
morphs that are generated through orthographic 
changes of edit distance greater than 1. Specifi-
cally, if  

 is a word in the induced root list (e.g. 
?stability?7, where   =?stabil? and 

=?ity?),   is an 
induced suffix, and the attachment of 

 to    is cor-
rect according to suffix level similarity, then we 
hypothesize that    (?stabil?) is a candidate allo-
morph. For each induced suffix, we use this hy-
pothesis to generate candidate allomorphs and 
consider as valid allomorphs only those that are 
generated from at least three different suffixes.8 
7 Word Segmentation 
After inducing the morphemes, we can use them to 
segment a word w in the test set. Specifically, we 
                                                          
7
 The correct segmentation of ?stability? is ?stable?+?ity?.  The ?stabil?-?stable? 
allomorph-root pair is an example of an orthographic change of edit distance 2. 
8
 This technique can also be used to induce out-of-vocabulary (OOV) roots. For 
example, the presence of ?perplexity?, ?perplexed? and ?perplexing? in a vo-
cabulary allows us to induce the root ?perplex?. OOV root induction is particu-
larly important for languages like Bengali, where verb roots mostly take the 
imperative form and hence are absent in a vocabulary created from a newspaper 
corpus, which normally comprises only the first and third person verb forms. 
(1) generate all possible segmentations of w using 
only the induced affixes and roots, and (2) apply a 
sequence of tests to remove candidate segmenta-
tions until we are left with only one candidate, 
which we take to be the final segmentation of w. 
Our first test involves removing any candidate 
segmentation m1m2 ? mn that violates any of the 
linguistic constraints below: 
? At least one of m1, m2, ?, mn is a root. 
? For 1 ? i < n, if mi is a prefix, then mi+1 must 
be a root or a prefix. 
? For 1 < i ? n, if mi is a suffix, then mi-1 must 
be a root or a suffix. 
? m1 can?t be a suffix and mn can?t be a prefix.  
Next, we apply our second test, in which we re-
tain only those candidate segmentations that have 
the smallest number of morphemes. For example, 
if ?friendly? has two candidate segmentations 
?friend?+?ly? and ?fri?+?end?+?ly?, we will select 
the first one to be the segmentation of w. 
If more than one candidate segmentation still ex-
ists, we score each remaining candidate using the 
heuristic below, selecting the highest-scoring can-
didate to be the final segmentation of w.  Basically, 
we score each candidate segmentation by adding 
the strength of each morpheme in the segmenta-
tion, where (1) the strength of an affix is the num-
ber of distinct words in the vocabulary to which 
the affix attaches, multiplied by the length of the 
affix, and (2) the strength of a root is the number of 
distinct morphemes with which the root combines, 
again multiplied by the length of the root. 
8 Evaluation 
In this section, we will first evaluate our segmenta-
tion algorithm for English and Bengali, and then 
examine its performance on the PASCAL datasets. 
8.1 Experimental Setup 
Vocabulary creation. We extracted our English 
vocabulary from the Wall Street Journal corpus of 
the Penn Treebank and the BLLIP corpus, preproc-
essing the documents by first tokenizing them and 
then removing capitalized words, punctuations and 
numbers. In addition, we removed words of fre-
quency 1 from BLLIP, because many of them are 
proper nouns and misspelled words. The final Eng-
lish vocabulary consists of approximately 60K dis-
tinct words. We applied the same pre-processing 
161
steps to five years of articles taken from the Ben-
gali newspaper Prothom Alo to generate our Ben-
gali vocabulary, which consists of 140K words. 
Test set preparation. To create our English test 
set, we randomly chose 000 words from our vo-
cabulary that are at least 4-character long9 and also 
appear in CELEX. Although 95% of the time we 
adopted the segmentation proposed by CELEX, in 
some cases the CELEX segmentations are errone-
ous (e.g. ?rolling? and ?lodging? remain unseg-
mented in CELEX). As a result, we cross-check 
with the online version of Merriam-Webster to 
make the necessary changes. To create the Bengali 
test set, we randomly chose 5000 words from our 
vocabulary and manually removed proper nouns 
and misspelled words from the set before giving it 
to two of our linguists for hand-segmentation. The 
final test set contains 4191 words. 
Evaluation metrics.  We use two standard metrics 
-- exact accuracy and F-score -- to evaluate the 
performance of our segmentation algorithm on the 
test sets. Exact accuracy is the percentage of the 
words whose proposed segmentation is identical to 
the correct segmentation. F-score is the harmonic 
mean of recall and precision, which are computed 
based on the placement of morpheme boundaries.10   
8.2 Results for English and Bengali 
The baseline systems. We use two publicly avail-
able and widely used unsupervised morphological 
learning systems -- Goldsmith?s (2001) Linguis-
tica11 and Creutz and Lagus?s (2005) Morphessor 
1.012 -- as our baseline systems. The first two rows 
of Table 3 show the results of these systems for our 
test sets (with all the training parameters set to 
their default values). As we can see, Linguistica 
performs substantially better for English in terms 
of both exact accuracy and F-score, whereas Mor-
phessor outperforms Linguistica for Bengali.   
Our segmentation algorithm. Results of our 
segmentation algorithm are shown in rows 3-6 of 
Table 3. Specifically, row 3 shows the results of 
our basic segmentation system as described in Sec-
tion 3. Rows 4-6 show the results where our three 
techniques (i.e. relative frequency, suffix level 
                                                          
9
 Words of length less than 4 do not have any morphological segmentation in 
English. Hence, by imposing this length restriction on the words in our test set, 
we effectively make the evaluation more challenging. This is also the reason for 
our using words that are at least 3-character long in the Bengali test set.  
10
 See http://www.cis.hut.fi/morphochallenge2005/evaluation.shtml for details. 
11
 http://humanities.uchicago.edu/faculty/goldsmith/Linguistica2000/ 
12
 http://www.cis.hut.fi/projects/morpho/ 
similarity and allomorph detection) are incorpo-
rated into the basic system one after the other. It is 
worth mentioning that (1) our basic algorithm al-
ready outperforms the baseline systems in terms of 
both exact accuracy and F-score; and (2) while 
each of our additions to the basic algorithm boosts 
system performance, relative corpus frequency and 
allomorph detection contribute to performance im-
provements particularly significantly. As we can 
see, the best segmentation performance is achieved 
when all of our three additions are applied to the 
basic algorithm.  
 
 English 
 
Bengali 
 A P R F A P R F 
Linguistica 68.9 
 
84.8 75.7 80.0 36.3 58.2 63.3 60.6 
Morphessor 64.9 
 
69.6 85.3 76.6 56.5 89.7 67.4 76.9 
Basic in-
duction 
68.1 79.4 82.8 81.1 57.7 79.6 81.2 80.4 
Relative 
frequency 
74.0 86.4 82.5 84.4 63.2 85.6 79.9 82.7 
Suffix level 
similarity 
74.9 88.6 82.3 85.3 66.1 89.7 78.8 83.9 
Allomorph 
detection 
78.3 88.3 86.4 87.4 68.3 89.3 81.3 85.1 
Table 3: Results (reported in terms of exact accu-
racy (A), precision (P), recall (R) and F-score (F)) 
8.3 PASCAL Challenge Results 
To get an idea of how our algorithm performs in 
comparison to the PASCAL participants, we con-
ducted evaluations on the PASCAL datasets for 
English, Finnish and Turkish. Table 4 shows the F-
scores of four segmentation algorithms for these 
three datasets: the best-performing PASCAL sys-
tem (Winner), Morphessor, our system that uses 
the basic morpheme induction algorithm (Basic), 
and our system with all three extensions incorpo-
rated (Complete). Below we discuss these results. 
English. There are 533 test cases in this dataset. 
Using the vocabulary created as described in Sec-
tion 8.1, our Complete algorithm achieves an F-
score of 79.4%, which outperforms the winner 
(Keshava and Pitler, 2006) by 2.6%. Although our 
basic morpheme induction algorithm is similar to 
that of Keshava and Pitler, a closer examination of 
the results reveals that F-score increases signifi-
cantly with the incorporation of relative frequency 
and allomorph detection. 
Finnish and Turkish. The real challenge in the 
PASCAL Challenge is the evaluation on Finnish 
162
and Turkish due to their morphological richness. 
We use the 400K and 300K most frequent words 
from the Finnish and Turkish datasets provided by 
the organizers as our vocabulary. When tested on 
the gold standard of 661 Finnish and 775 Turkish 
words, our Complete system achieves F-scores of 
65.2% and 66.2%, which are better than the win-
ner?s scores (Bernhard (2006)). In addition, Com-
plete outperforms Basic by 3-6% in F-score; these 
results suggest that the new techniques proposed in 
this paper (especially allomorph detection) are also 
very effective for Finnish and Turkish. 
 
 English Finnish Turkish 
Winner 76.8 64.7 65.3 
Morphessor 66.2 66.4 70.1 
Basic 75.8 59.2 63.4 
Complete 79.4 65.2 66.2 
Table 4: F-scores for the PASCAL gold standards 
 
   As mentioned in the introduction, none of the 
participating PASCAL systems offers robust per-
formance across different languages. For instance, 
Keshava and Pitler?s algorithm, the winner for 
English, has F-scores of only 47% and 54% for 
Finnish and Turkish respectively, whereas Bern-
hard?s algorithm, the winner for Finnish and Turk-
ish, achieves an F-score of only 66% for English. 
On the other hand, our algorithm outperforms the 
winners for all the languages in the competition, 
demonstrating its robustness across languages.  
Finally, although Morphessor achieves better re-
sults for Turkish and Finnish than our Complete 
system, it performs poorly for English, having an 
F-score of only 66.2%. On the other hand, our re-
sults for Finnish and Turkish are not significantly 
poorer than those of Morphessor. 
9 Conclusions 
We have presented an unsupervised word segmen-
tation algorithm that offers robust performance 
across languages with different levels of morpho-
logical complexity. Our algorithm not only outper-
forms Linguistica and Morphessor for English and 
Bengali, but also compares favorably to the best-
performing PASCAL morphological parsers when 
evaluated against all three target languages --
English, Turkish, and Finnish -- in the Challenge. 
Experimental results indicate that the use of rela-
tive corpus frequency for incorrect attachment de-
tection and the induction of orthographic rules and 
allomorphs have contributed to the performance of 
our algorithm particularly significantly. 
References  
R. H. Baayen, R. Piepenbrock and L. Gulikers. 1996. The 
CELEX2 lexical database (CD-ROM), LDC, Univ of 
Pennsylvania, Philadephia, PA. 
D. Bernhard. 2006. Unsupervised morphological segmentation 
based on segment predictability and word segment align-
ment. In PASCAL Challenge Workshop on Unsupervised 
Segmentation of Words into Morphemes. 
M. R. Brent, S. K. Murthy and A. Lundberg. 1995. Discover-
ing morphemic suffixes: A case study in minimum descrip-
tion length induction. In Proceedings of the Fifth 
International Workshop on AI and Statistics. 
M. Creutz. 2003. Unsupervised segmentation of words using 
prior distributions of morph length and frequency. In Pro-
ceedings of the ACL, pages 280-287. 
M. Creutz and K. Lagus. 2005. Unsupervised morpheme seg-
mentation and morphology induction from text corpora us-
ing Morfessor 1.0. In Computer and Information Science, 
Report A81, Helsinki University of Technology. 
S. Dasgupta and V. Ng. 2007. Unsupervised word segmenta-
tion for Bangla. In Proceedings of ICON, pages 15-24. 
H. D?Jean. 1998. Morphemes as necessary concepts for struc-
tures discovery from untagged corpora. In Workshop on 
Paradigms and Grounding in Natural Language Learning, 
pages 295-299. 
D. Freitag. 2005. Morphology induction from term clusters. In 
Proceedings of CoNLL, pages 128-135. 
J. Goldsmith. 2001. Unsupervised learning of the morphology 
of a natural language. In Computational Linguistics 27(2), 
pages 153-198. 
Z. Harris. 1955. From phoneme to morpheme. In Language, 
31(2): 190-222.  
S. Keshava and E. Pitler. 2006. A simpler, intuitive approach 
to morpheme induction. In PASCAL Challenge Workshop 
on Unsupervised Segmentation of Words into Morphemes. 
K. Koskenniemi. 1983. Two-level morphology: a general 
computational model for word-form recognition and pro-
duction. Publication No. 11. Helsinki: University of Hel-
sinki Department of General Linguistics. 
P. Schone and D. Jurafsky. 2001. Knowledge-free induction of 
inflectional morphologies. In Proceedings of the NAACL, 
pages 183-191. 
M. G. Snover and M. R. Brent. 2001. A Bayesian model for 
morpheme and paradigm identification. In Proceedings of 
the ACL, pages 482-490. 
D. Yarowsky and R. Wicentowski. 2000. Minimally super-
vised morphological analysis by multimodal alignment. In 
Proceedings of the ACL, pages 207-216. 
163
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 575?583,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Graph-Cut-Based Anaphoricity Determination for Coreference Resolution
Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
vince@hlt.utdallas.edu
Abstract
Recent work has shown that explicitly iden-
tifying and filtering non-anaphoric mentions
prior to coreference resolution can improve
the performance of a coreference system.
We present a novel approach to this task of
anaphoricity determination based on graph
cuts, and demonstrate its superiority to com-
peting approaches by comparing their effec-
tiveness in improving a learning-based coref-
erence system on the ACE data sets.
1 Introduction
Coreference resolution is the problem of identifying
which noun phrases (NPs, or mentions) refer to the
same real-world entity in a text or dialogue. Accord-
ing to Webber (1979), coreference resolution can
be decomposed into two complementary tasks: ?(1)
identifying what a text potentially makes available
for anaphoric reference and (2) constraining the can-
didate set of a given anaphoric expression down to
one possible choice.? The first task is nowadays typ-
ically formulated as an anaphoricity determination
task, which aims to classify whether a given men-
tion is anaphoric or not. Knowledge of anaphoric-
ity could improve the precision of a coreference sys-
tem, since non-anaphoric mentions do not have an
antecedent and therefore do not need to be resolved.
Previous work on anaphoricity determination can
be broadly divided into two categories (see Poe-
sio et al (2004) for an overview). Research in the
first category aims to identify specific types of non-
anaphoric phrases, with some identifying pleonas-
tic it (using heuristics [e.g., Paice and Husk (1987),
Lappin and Leass (1994), Kennedy and Boguraev
(1996)], supervised approaches [e.g., Evans (2001),
Mu?ller (2006), Versley et al (2008)], and distribu-
tional methods [e.g., Bergsma et al (2008)]), and
others identifying non-anaphoric definite descrip-
tions (using rule-based techniques [e.g., Vieira and
Poesio (2000)] and unsupervised techniques [e.g.,
Bean and Riloff (1999)]).
On the other hand, research in the second cat-
egory focuses on (1) determining the anaphoricity
of all types of mentions, and (2) using the result-
ing anaphoricity information to improve coreference
resolution. For instance, Ng and Cardie (2002a)
train an anaphoricity classifier to determine whether
a mention is anaphoric, and let an independently-
trained coreference system resolve only those men-
tions that are classified as anaphoric. Somewhat sur-
prisingly, they report that using anaphoricity infor-
mation adversely affects the performance of their
coreference system, as a result of an overly conser-
vative anaphoricity classifier that misclassifies many
anaphoric mentions as non-anaphoric. One solu-
tion to this problem is to use anaphoricity infor-
mation as soft constraints rather than as hard con-
straints for coreference resolution. For instance,
when searching for the best partition of a set of
mentions, Luo (2007) combines the probabilities re-
turned by an anaphoricity model and a coreference
model to score a coreference partition, such that a
partition is penalized whenever an anaphoric men-
tion is resolved. Another, arguably more popular,
solution is to ?improve? the output of the anaphoric-
ity classifier by exploiting the dependency between
anaphoricity determination and coreference resolu-
575
tion. For instance, noting that Ng and Cardie?s
anaphoricity classifier is too conservative, Ng (2004)
first parameterizes their classifier such that its con-
servativeness can be varied, and then tunes this pa-
rameter so that the performance of the coreference
system is maximized. As another example, De-
nis and Baldridge (2007) and Finkel and Manning
(2008) perform joint inference for anaphoricity de-
termination and coreference resolution, by using In-
teger Linear Programming (ILP) to enforce the con-
sistency between the output of the anaphoricity clas-
sifier and that of the coreference classifier.
While this ILP approach and Ng?s (2004) ap-
proach to improving the output of an anaphoricity
classifier both result in increased coreference per-
formance, they have complementary strengths and
weaknesses. Specifically, Ng?s approach can di-
rectly optimize the desired coreference evaluation
metric, but by treating the coreference system as a
black box during the optimization process, it does
not exploit the potentially useful pairwise probabil-
ities provided by the coreference classifier. On the
other hand, the ILP approach does exploit such pair-
wise probabilities, but optimizes an objective func-
tion that does not necessarily have any correlation
with the desired evaluation metric.
Our goals in this paper are two-fold. First, moti-
vated in part by previous work, we propose a graph-
cut-based approach to anaphoricity determination
that combines the strengths of Ng?s approach and
the ILP approach, by exploiting pairwise corefer-
ence probabilities when co-ordinating anaphoricity
and coreference decisions, and at the same time al-
lowing direct optimization of the desired corefer-
ence evaluation metric. Second, we compare our
cut-based approach with the five aforementioned ap-
proaches to anaphoricity determination (namely, Ng
and Cardie (2002a), Ng (2004), Luo (2007), De-
nis and Baldridge (2007), and Finkel and Manning
(2008)) in terms of their effectiveness in improv-
ing a learning-based coreference system. To our
knowledge, there has been no attempt to perform
a comparative evaluation of existing approaches to
anaphoricity determination. It is worth noting, in
particular, that Luo (2007), Denis and Baldridge
(2007), and Finkel and Manning (2008) evaluate
their approaches on true mentions extracted from
the answer keys. Since true mentions are com-
posed of all the NPs involved in coreference rela-
tions but only a subset of the singleton NPs (i.e.,
NPs that are not coreferent with any other NPs) in
a text, evaluating the utility of anaphoricity deter-
mination on true mentions to some extent defeats
the purpose of performing anaphoricity determina-
tion, which precisely aims to identify non-anaphoric
mentions. Hence, we hope that our evaluation on
mentions extracted using an NP chunker can reveal
their comparative strengths and weaknesses.
We perform our evaluation on three ACE coref-
erence data sets using two commonly-used scor-
ing programs. Experimental results show that (1)
employing our cut-based approach to anaphoric-
ity determination yields a coreference system that
achieves the best performance for all six data-
set/scoring-program combinations, and (2) among
the five existing approaches, none performs consis-
tently better than the others.
The rest of the paper is organized as follows. Sec-
tion 2 describes our learning-based coreference sys-
tem. In Section 3, we give an overview of the five
baseline approaches to anaphoricity determination.
Section 4 provides the details of our graph-cut-based
approach. Finally, we present evaluation results in
Section 5 and conclude in Section 6.
2 Baseline Coreference Resolution System
Our baseline coreference system implements the
standard machine learning approach to coreference
resolution (see Ng and Cardie (2002b), Ponzetto and
Strube (2006), Yang and Su (2007), for instance),
which consists of probabilistic classification and
clustering, as described below.
2.1 The Standard Machine Learning Approach
We use maximum entropy (MaxEnt) classification
(Berger et al, 1996) in conjunction with the 33 fea-
tures described in Ng (2007) to acquire a model, PC ,
for determining the probability that two mentions,
mi and mj , are coreferent. Hence,
PC(mi,mj) = P (COREFERENT | mi,mj).
In the rest of the paper, we will refer to PC(mi,mj)
as the pairwise coreference probability between mi
and mj . To generate training instances, we em-
ploy Soon et al?s (2001) procedure, relying on the
training texts to create (1) a positive instance for
576
each anaphoric mention, mj , and its closest an-
tecedent, mi; and (2) a negative instance for mj
paired with each of the intervening mentions, mi+1,
mi+2,. . ., mj?1. When training the feature-weight
parameters of the MaxEnt model, we use 100 it-
erations of the improved iterative scaling (IIS) al-
gorithm (Della Pietra et al, 1997) together with a
Gaussian prior to prevent overfitting.
After training, the coreference model is used to
select an antecedent for each mention in a test text.
Following Soon et al (2001), we select as the an-
tecedent of each mention, mj , the closest preced-
ing mention that is classified as coreferent with mj ,
where mention pairs with pairwise probabilities of at
least 0.5 are considered coreferent. If no such men-
tion exists, no antecedent will be selected for mj . In
essence, we use a closest-first clustering algorithm
to impose a partitioning on the mentions.
3 Baseline Approaches to Anaphoricity
Determination
As mentioned previously, we will use five existing
approaches to anaphoricity determination as base-
lines in our evaluation. Common to all five ap-
proaches is the acquisition of an anaphoricity model,
PA, for determining the probability that a mention,
mj , is anaphoric. Hence,
PA(mj) = P (ANAPHORIC | mj)
To train PA, we again employ MaxEnt modeling,
and create one training instance from each mention
in a training text. Hence, each instance represents a
single mention and consists of 37 features that are
potentially useful for distinguishing anaphoric and
non-anaphoric mentions (see Ng and Cardie (2002a)
for a detailed description of these features).1
The classification of a training instance ? one
of ANAPHORIC or NOT ANAPHORIC ? is derived
directly from the coreference chains in the associ-
ated training text. Like the coreference model, the
anaphoricity model is trained by running 100 iter-
ations of IIS with a Guassian prior. The resulting
model is then applied to a test text to determine the
1While we train the anaphoricity model using the Ng and
Cardie (2002a) feature set, it should be clear that any features
that are useful for distinguishing anaphoric and non-anaphoric
mentions can be used (e.g., those proposed by Uryupina (2003)
and Elsner and Charniak (2007)).
probability that a mention is anaphoric.
In the rest of this section, we provide an overview
of the five baseline approaches to anaphoricity deter-
mination. We will characterize each approach along
two dimensions: (1) whether it attempts to improve
PA, and if so, how; and (2) whether the resulting
anaphoricity information is used as hard constraints
or soft constraints by the coreference system.
3.1 Ng and Cardie (2002a)
Ng and Cardie (N&C) do not attempt to improve PA,
simply using the anaphoricity information it pro-
vides as hard constraints for coreference resolution.
Specifically, the coreference system resolves only
those mentions that are determined as anaphoric by
PA, where a mention is classified as anaphoric if the
classification threshold is at least 0.5.
3.2 Ng (2004)
PA may not be ?sufficiently? accurate, however,
as N&C report a significant drop in the perfor-
mance of their coreference system after incorpo-
rating anaphoricity information, owing in part to
their overly conservative anaphoricity model that
misclassifies many anaphoric mentions as non-
anaphoric. To address this problem, Ng (2004) at-
tempts to improve PA by introducing a threshold
parameter c to adjust the conservativeness of PA
as follows. Given a specific c (0 ? c ? 1), a
mention mj is classified as anaphoric by PA if and
only if PA(mj) ? c. It should be easy to see
that decreasing c yields progressively less conserva-
tive anaphoricity models (i.e., more mentions will
be classified as anaphoric). The parameter c is tuned
using held-out development data to optimize the per-
formance of the coreference system that employs
anaphoricity information (as hard constraints).
In essence, Ng?s approach to improving PA treats
the coreference system as a black box, merely se-
lecting the value for c that yields the best score ac-
cording to the desired coreference evaluation met-
ric on the held-out data. In particular, unlike some
of the anaphoricity determination approaches dis-
cussed later on, this approach does not attempt to co-
ordinate the anaphoricity decisions and the pairwise
coreference decisions. Nevertheless, as mentioned
before, a unique strength of this approach lies in its
ability to optimize directly the desired coreference
577
evaluation metric.
3.3 Luo (2007)
Among the five anaphoricity determination ap-
proaches, Luo?s (2007) is the only one where
anaphoricity information is exploited as soft con-
straints by the coreference model, PC .
Specifically, Luo?s algorithm attempts to find the
most probable coreference partition of a given set
of mentions. To do so, it scores a partition us-
ing the probabilities provided by PA and PC . Let
us illustrate how this can be done via the follow-
ing example. Given a document with four men-
tions, m1, . . . ,m4, and a partition of the mentions,
{[m1,m3,m4], [m2]}, automatically produced by
some coreference system, Luo?s algorithm scores
the partition by considering the mentions in the
document in a left-to-right manner. As the first
mention in the document, m1 is not anaphoric,
and the probability that it is non-anaphoric is 1 ?
PA(m1). Then, the algorithm processes m2, which
according to the partition is non-anaphoric, and
the probability of its being non-anaphoric is 1 ?
PA(m2). Next, it processes m3, which is coref-
erent with m1 with probability PC(m1,m3). Fi-
nally, it processes m4, which is coreferent with m1
and m3. The probability that m4 is coreferent with
the cluster consisting of m1 and m3 is defined to
be max(PC(m1,m4), PC (m3,m4)), according to
Luo?s algorithm. The score of this partition is the
product of these four probabilities, two provided by
PA and two by PC . As can be seen, a partition
is penalized whenever a mention that is unlikely to
be anaphoric (according to PA) is being resolved to
some antecedent according to the partition.
Nevertheless, it is computationally infeasible to
score all possible partitions given a set of mentions,
as the number of partitions is exponential in the
number of mentions. To cope with this computa-
tional complexity, Luo employs the algorithm pro-
posed in Luo et al (2004) to heuristically search for
the most probable partition by performing a beam
search through a Bell tree. In essence, only the most
promising nodes in the tree are expanded at each
step of the search process, where the ?promise? of
a node is defined in terms of the probabilities pro-
vided by PA and PC , as described above. Details of
this process can be found in Luo et al (2004).
3.4 Denis and Baldridge (2007)
As mentioned before, Denis and Baldridge (D&B)
aim to improve the outputs of PA and PC by em-
ploying Integer Linear Programming (ILP) to per-
form joint inference for anaphoricity determination
and coreference resolution. The ILP approach is mo-
tivated by the observation that the outputs of these
two models have to satisfy certain constraints. For
instance, if PC determines that a mention, mj , is
not coreferent with any other mentions in the as-
sociated text, then PA should determine that mj is
non-anaphoric. In practice, however, since PA and
PC are trained independently of each other, this and
other constraints cannot be enforced.
ILP provides a framework for jointly determining
anaphoricity and coreference decisions for a given
set of mentions based on the probabilities provided
by PA and PC , such that the resulting joint deci-
sions satisfy the desired constraints while respecting
as much as possible the probabilistic decisions made
by the independently-trained PA and PC . Specifi-
cally, an ILP program is composed of an objective
function to be optimized subject to a set of linear
constraints, and is created for each test text D as fol-
lows. Let M be the set of mentions in D, and P be
the set of mention pairs formed from M (i.e., P =
{(mi,mj) | mi,mj ? M, i < j}). Each ILP pro-
gram has a set of indicator variables. In our case, we
have one binary-valued variable for each anaphoric-
ity decision and coreference decision to be made by
an ILP solver. Following D&B?s notation, we use yj
to denote the anaphoricity decision for mention mj ,
and x?i,j? to denote the coreference decision involv-
ing mentions mi and mj . In addition, each variable
is associated with an assignment cost. Specifically,
let cC?i,j? = ? log(PC(mi,mj)) be the cost of setting
x?i,j? to 1, and c?C?i,j? = ? log(1 ? PC(mi,mj)) be
the complementary cost of setting x?i,j? to 0. We can
similarly define the cost associated with each yj , let-
ting cAj = ? log(PA(mj)) be the cost of setting yj to
1, and c?Aj = ? log(1 ? PA(mj)) be the complemen-
tary cost of setting yj to 0. Given these costs, we
aim to optimize the following objective function:
min ?
(mi,mj)?P
cC?i,j? ? x?i,j? + c?C?i,j? ? (1 ? x?i,j?)
+ ?
mj?M
cAj ? yj + c?Aj ? (1 ? yj)
578
subject to a set of manually-specified linear con-
straints. D&B specify four types of constraints: (1)
each indicator variable can take on a value of 0 or 1;
(2) if mi and mj are coreferent (x?i,j?=1), then mj is
anaphoric (yj=1); (3) if mj is anaphoric (yj=1), then
it must be coreferent with some preceding mention
mi; and (4) if mj is non-anaphoric, then it cannot be
coreferent with any mention. Note that we are mini-
mizing the objective function, since each assignment
cost is expressed as a negative logarithm value. We
use lp solve2, an ILP solver, to solve this program.
It is easy to see that enforcing consistency using
ILP amounts to employing anaphoricity informa-
tion as hard constraints for the coreference system.
Since transitivity is not guaranteed by the above con-
straints, we follow D&B and use the aggressive-
merge clustering algorithm to put any two mentions
that are posited as coreferent into the same cluster.
3.5 Finkel and Manning (2008)
Finkel and Manning (F&M) present one simple ex-
tension to D&B?s ILP approach: augmenting the
set of linear constraints with the transitivity con-
straint. This ensures that if x?i,j?=1 and x?j,k?=1,
then x?i,k?=1. As a result, the coreference decisions
do not need to be co-ordinated by a separate cluster-
ing mechanism.
4 Cut-Based Anaphoricity Determination
As mentioned in the introduction, our graph-cut-
based approach to anaphoricity determination is mo-
tivated by Ng?s (2004) and the ILP approach, aim-
ing to combine the strengths of the two approaches.
Specifically, like Ng (2004), our approach allows di-
rect optimization of the desired coreference evalua-
tion metric; and like the ILP approach, our approach
co-ordinates anaphoricity decisions and coreference
decisions by exploiting the pairwise probabilities
provided by a coreference model. In this section,
we will introduce our cut-based approach, starting
by reviewing concepts related to minimum cuts.
4.1 The Minimum Cut Problem Setting
Assume that we want to partition a set of n objects,
{x1, x2, . . . , xn}, into two sets, Y1 and Y2. We have
two types of scores concerning the x?s and the Y ?s:
2Available from http://lpsolve.sourceforge.net/
membership scores and similarity scores. The mem-
bership score, memYi(xj), is a non-negative quan-
tity that approximates the ?affinity? of xj to Yi. On
the other hand, the similarity score, sim(xj , xk), is
a non-negative quantity that provides an estimate of
the similarity between xj and xk.
Informally, our goal is to maximize each object?s
net happiness, which is computed by subtracting its
membership score of the class it is not assigned to
from its membership score of the class it is assigned
to. However, at the same time, we want to avoid
assigning similar objects to different classes. More
formally, we seek to minimize the partition cost:?
xj?Y1,xk?Y2
sim(xj, xk)+
?
x?Y1
memY2(x)+
?
x?Y2
memY1(x)
There exists an efficient algorithm for solving this
seemingly intractable problem when it is recast as
a graph problem. So, let us construct a graph, G,
based on the available scores as follows. First, we
create two nodes, s and t (called the source and
the sink, respectively), to represent the two classes.
Then, we create one ?object? node for each of the
n objects. For each object, xj , we add two directed
edges, one from s to xj (with weight memY1(xj))
and the other from xj to t (with weight memY2(xj)).
Moreover, for each pair of object nodes, xj and xk,
we add two directed edges (one from xj to xk and
another from xk to xj), both of which have weight
sim(xj , xk). A cut in G is defined as a partition of
the nodes into two sets, S and T , such that s ? S,
t ? T ; and the cost of the cut, cost(S, T ), is the
sum of the weights of the edges going from S to
T . A minimum cut is a cut that has the lowest cost
among all the cuts of G. It can be proved that find-
ing a minimum cut of G is equivalent to minimizing
the partition cost defined as above. The main advan-
tage of recasting the above minimization problem as
a graph-cut problem is that there exist polynomial-
time maxflow algorithms for finding a minimum cut.
4.2 Graph Construction
Next, we show how to construct the graph to which
the mincut-finding algorithm will be applied. The
ultimate goal is to use the mincut finder to parti-
tion a given set of mentions into two subsets, so that
our coreference system will attempt to resolve only
those mentions that are in the subset correspond-
ing to ANAPHORIC. In other words, the resulting
579
anaphoricity information will be used to identify and
filter non-anaphoric mentions prior to coreference
resolution. The graph construction process, which
takes as input a set of mentions in a test text, is com-
posed of three steps, as described below.
Step 1: Mimicking Ng and Cardie (2002a)
To construct the desired graph, G, we first create
the source, s, and the sink, t, that represent the
classes ANAPHORIC and NOT ANAPHORIC, respec-
tively. Then, for each mention mn in the input text,
we create one node, n, and two edges, sn and nt,
connecting n to s and t. Next, we compute wsn
and wnt, the weights associated with sn and nt.
A natural choice would be to use PA(mn) as the
weight of sn and (1?wsn) as the weight of nt. (We
will assume throughout that wnt is always equal to
1 ? wsn.) If we apply the mincut finder to the cur-
rent G, it should be easy to see that (1) any node
n where wsn > 0.5 will be assigned to s, (2) any
node n where wsn < 0.5 will be assigned to t,
and (3) any remaining node will be assigned to one
of them. (Without loss of generality, we assume
that such nodes are assigned to s.) Hence, the set
of mentions determined as anaphoric by the mincut
finder is identical to the set of mentions classified as
anaphoric by PA, thus yielding a coreference system
that is functionally equivalent to N&C?s. This also
implies that G shares the same potential weakness
as PA: being overly conservative in determining a
mention as anaphoric.
Step 2: Mimicking Ng (2004)
One way to ?improve? G is to make it functionally
equivalent to Ng?s (2004) approach. Specifically,
our goal in Step 2 is to modify the edge weights in
G (without adding new edges or nodes) such that the
mincut finder classifies a node n as anaphoric if and
only if PA(mn) ? c for some c ? [0, 1]. Now, recall
from Step 1 that the mincut finder classifies a node
n as anaphoric if and only if wsn ? 0.5. Hence,
to achieve the aforementioned goal, we just need to
ensure the property that wsn ? 0.5 if and only if
PA(mn) ? c. Consequently, we compute wsn using
a sigmoid function:
wsn = 11 + e???(PA(mn)?c)
where ? is a constant that controls the ?steepness?
of the sigmoid.3 It should be easy to verify that the
sigmoid satisfies the aforementioned property. As
noted before, wnt = 1 ? wsn for each node n. In-
spired by Ng (2004), the value of the parameter c
will be tuned based on held-out development data to
maximize coreference performance.
Step 3: Incorporating coreference probabilities
Like Ng?s (2004) approach, the current G suffers
from the weakness of not exploiting the pairwise
probabilities provided by PC . Fortunately, these
probabilities can be naturally incorporated into G as
similarity scores. To see why these pairwise prob-
abilities are potentially useful, consider two men-
tions, mi and mj , in a text D that are coreferent and
are both anaphoric. Assume that the graph G con-
structed from D has these edge weights: wsi = 0.8,
wsj = 0.3, and wij = wji = 0.8. Without the sim-
ilarity scores, the mincut finder will correctly deter-
mine mi as anaphoric but incorrectly classify mj as
non-anaphoric. On the other hand, if the similarity
scores are taken into account, the mincut finder will
correctly determine both mentions as anaphoric.
The above discussion suggests that it is desirable
to incorporate edges between two nodes, i and j,
when mi and mj are likely to be coreferent (i.e.,
PC(mi,mj) ? c2 for some constant c2). In our im-
plementation, we tune this new parameter, c2, jointly
with c (see Step 2) on development data to maxi-
mize coreference performance. While it is possible
to imagine scenarios where incorporating pairwise
probabilities is not beneficial, we believe that these
probabilities represent a source of information that
could be profitably exploited via learning appropri-
ate values for c and c2.4
3One of the main reasons why we use a sigmoid function
(rather than a linear function) is that the weights will still fall
within the [0, 1] interval after the transformation, a property that
will turn out to be convenient when the pairwise coreference
probabilities are incorporated (see Step 3). ? is chosen so that
the difference between two weights after the transformation is
as close as possible to their difference before the transformation.
With this criterion in mind, we set ? to 0.42 in our experiments.
4Incorporating the coreference probabilities can potentially
identify some of the anaphoric mentions that would be misclas-
sified otherwise. However, note that the minimum cut algorithm
does not maintain the notion of directionality that would allow
one to determine that a discourse-new mention (i.e., the first
mention of a coreference chain) is not anaphoric. In particu-
lar, the algorithm tends to classify all members of a coreference
chain, including the first mention, as anaphoric. We did not ex-
580
5 Evaluation
5.1 Experimental Setup
For evaluation, we use the ACE Phase II coreference
corpus, which is composed of three sections: Broad-
cast News (BNEWS), Newspaper (NPAPER), and
Newswire (NWIRE). Each section is in turn com-
posed of a training set and a test set. For each
section, we train an anaphoricity model, PA, and
a coreference model, PC , on the training set, and
evaluate PC (when used in combination with differ-
ent approaches to anaphoricity determination) on the
test set. As noted before, the mentions used are ex-
tracted automatically using an in-house NP chunker.
Results are reported in terms of recall (R), precision
(P), and F-measure (F), obtained using two corefer-
ence scoring programs: the MUC scorer (Vilain et
al., 1995) and the CEAF scorer (Luo, 2005).
5.2 Results and Discussions
?No Anaphoricity? baseline. Our first baseline is
the learning-based coreference system described in
Section 2, which does not employ any anaphoric-
ity determination algorithm. Results using the MUC
scorer and the CEAF scorer are shown in row 1 of
Tables 1 and 2, respectively. As we can see, MUC
F-score ranges from 55.0 to 61.7 and CEAF F-score
ranges from 55.3 to 61.2.
Duplicated Ng and Cardie (2002a) baseline.
Next, we evaluate our second baseline, which is
N&C?s coreference system. As seen from row 2 of
Tables 1 and 2, MUC F-score ranges from 50.5 to
60.0 and CEAF F-score ranges from 54.5 to 59.4.
In comparison to the first baseline, we see drops in
F-score in all cases as a result of considerable pre-
cipitation in recall, which can in turn be attributed
to the misclassification of many anaphoric mentions
by the anaphoricity model. More specifically, MUC
F-score decreases by 1.7?5.5%, whereas CEAF F-
score decreases by 0.5?1.8%. These trends are con-
sistent with those reported in N&C?s paper.
Duplicated Ng (2004) baseline. Our third base-
line is Ng?s (2004) coreference system. Recall that
this resolver requires the tuning of the conservative-
ness parameter, c, on held-out data. To ensure a fair
comparison between different resolvers, we do not
plicitly address this issue, simply letting the coreference clus-
tering algorithm discover that first mentions are non-anaphoric.
rely on additional data for parameter tuning. Rather,
we reserve 13 of the available training data for tuning
c, for which we tested values from 0 to 1 in steps of
0.01, and use the remaining 23 of the data for training
PA and PC . Results are shown in row 3 of Tables
1 and 2, where MUC F-score ranges from 57.0 to
61.9 and CEAF F-score ranges from 55.5 to 60.6. In
comparison to the first baseline, we obtain mixed re-
sults: MUC F-score increases by 2.0% and 0.2% for
BNEWS and NPAPER, respectively, but drops by
0.1% for NWIRE; CEAF F-score increases by 0.2%
and 1.1% for BNEWS and NPAPER, respectively,
but drops by 0.6% for NWIRE.
Duplicated Luo (2007) baseline. Results of our
fourth baseline, in which the anaphoricity and pair-
wise coreference probabilities are combined to score
a partition using Luo?s system, are shown in row 4
of Tables 1 and 2. Here, we see that MUC F-score
ranges from 55.8 to 62.1 and CEAF F-score ranges
from 56.3 to 61.5. In comparison to the first base-
line, performance improves, though insignificantly,5
in all cases: MUC F-score increases by 0.2?0.8%,
whereas CEAF F-score increases by 0.3?1.0%.
Duplicated Denis and Baldridge (2007) base-
line. Our fifth baseline performs joint inference
for anaphoricity determination and coreference res-
olution using D&B?s ILP approach. Results are
shown in row 5 of Tables 1 and 2, where MUC
F-score ranges from 56.2 to 63.8 and CEAF F-
score ranges from 56.9 to 61.5. In comparison to
the first baseline, MUC F-score always increases,
with improvements ranging from 1.2% to 2.1%.
CEAF results are mixed: F-score increases signifi-
cantly for BNEWS, drops insignificantly for NPA-
PER, and rises insignificantly for NWIRE. The dif-
ference in performance trends between the two scor-
ers can be attributed to the fact that the MUC
scorer typically under-penalizes errors due to over-
merging, which occurs as a result of D&B?s using
the aggressive-merge clustering algorithm. In addi-
tion, we can see that D&B?s approach performs at
least as good as Luo?s approach in all but one case
(NPAPER/CEAF).
Duplicated Finkel and Manning (2008) baseline.
Our sixth baseline is F&M?s coreference system,
5Like the MUC organizers, we use Approximate Random-
ization (Noreen, 1989) for significance testing, with p=0.05.
581
Broadcast News Newspaper Newswire
Approach to Anaphoricity Determination R P F R P F R P F
1 No Anaphoricity 57.7 52.6 55.0 60.8 62.6 61.7 59.1 58.1 58.6
2 Duplicated Ng and Cardie (2002a) 40.3 67.7 50.5? 52.1 70.6 60.0 43.0 69.3 53.1?
3 Duplicated Ng (2004) 51.9 63.2 57.0 60.0 63.8 61.9 59.3 57.7 58.5
4 Duplicated Luo (2007) 55.4 56.1 55.8 60.6 63.7 62.1 58.4 59.2 58.8
5 Duplicated Denis and Baldridge (2007) 57.3 55.1 56.2? 63.8 63.7 63.8? 60.4 59.3 59.8?
6 Duplicated Finkel and Manning (2008) 56.4 55.3 55.8 63.8 63.7 63.8? 59.7 59.2 59.5
7 Graph Minimum Cut 53.1 67.5 59.4? 57.9 71.2 63.9? 54.1 69.0 60.6?
Table 1: MUC scores for the three ACE data sets. F-scores that represent statistically significant gains and drops with
respect to the ?No Anaphoricity? baseline are marked with an asterisk (*) and a dagger (?), respectively.
Broadcast News Newspaper Newswire
Approach to Anaphoricity Determination R P F R P F R P F
1 No Anaphoricity 63.2 49.2 55.3 64.5 54.3 59.0 67.3 56.1 61.2
2 Duplicated Ng and Cardie (2002a) 55.9 53.3 54.5 60.7 56.3 58.5 60.6 58.2 59.4
3 Duplicated Ng (2004) 62.5 49.9 55.5 63.5 57.0 60.1 65.6 56.3 60.6
4 Duplicated Luo (2007) 62.7 51.1 56.3 64.6 55.4 59.6 67.0 56.8 61.5
5 Duplicated Denis and Baldridge (2007) 63.8 51.4 56.9? 62.6 53.6 57.8 67.0 56.8 61.5
6 Duplicated Finkel and Manning (2008) 63.2 51.3 56.7? 62.6 53.6 57.8 66.7 56.7 61.3
7 Graph Minimum Cut 61.4 57.6 59.4? 64.1 59.4 61.7? 65.7 61.9 63.8?
Table 2: CEAF scores for the three ACE data sets. F-scores that represent statistically significant gains and drops with
respect to the ?No Anaphoricity? baseline are marked with an asterisk (*) and a dagger (?), respectively.
which is essentially D&B?s approach augmented
with transitivity constraints. Results are shown in
row 6 of Tables 1 and 2, where MUC F-score ranges
from 55.8 to 63.8 and CEAF F-score ranges from
56.7 to 61.3. In comparison to the D&B baseline, we
see that F-score never improves, regardless of which
scoring program is used. In fact, recall slightly de-
teriorates, and this can be attributed to F&M?s ob-
servation that transitivity constraints tend to produce
smaller clusters. Overall, these results suggest that
enforcing transitivity for coreference resolution is
not useful for improving coreference performance.
Our graph-cut-based approach. Finally, we
evaluate the coreference system using the anaphoric-
ity information provided by our cut-based approach.
As before, we reserve 13 of the training data for
jointly tuning the two parameters, c and c2, and use
the remaining 23 for training PA and PC . For tun-
ing, we tested values from 0 to 1 in steps of 0.1 for
both c and c2. Results are shown in row 7 of Ta-
bles 1 and 2. As we can see, MUC F-score ranges
from 59.4 to 63.9 and CEAF F-score ranges from
59.4 to 63.8, representing a significant improvement
over the first baseline in all six cases: MUC F-score
rises by 2.0?4.4% and CEAF F-score rises by 2.6?
4.1%. Such an improvement can be attributed to a
large gain in precision and a smaller drop in recall.
This implies that our mincut algorithm has success-
fully identified many non-anaphoric mentions, but
in comparison to N&C?s approach, it misclassifies
a smaller number of anaphoric mentions. Moreover,
our approach achieves the best F-score for each data-
set/scoring-program combination, and significantly
outperforms the best baseline (D&B) in all but two
cases, NPAPER/MUC and NWIRE/MUC.
6 Conclusions
We have presented a graph-cut-based approach to
anaphoricity determination that (1) directly opti-
mizes the desired coreference evaluation metric
through parameterization and (2) exploits the proba-
bilities provided by the coreference model when co-
ordinating anaphoricity and coreference decisions.
Another major contribution of our work is the em-
pirical comparison of our approach against five ex-
isting approaches to anaphoricity determination in
terms of their effectiveness in improving a coref-
erence system using automatically extracted men-
tions. Our approach demonstrates effectiveness and
robustness by achieving the best result on all three
ACE data sets according to both the MUC scorer
and the CEAF scorer. We believe that our cut-based
approach provides a flexible mechanism for co-
ordinating anaphoricity and coreference decisions.
582
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments, Kazi Saidul Hasan for his
help on using lp solve, and NSF for its gracious sup-
port of this work under Grant IIS-0812261. The de-
scription of the minimum cut framework in Section
4.1 was inspired by Pang and Lee (2004).
References
D. Bean and E. Riloff. 1999. Corpus-based identification
of non-anaphoric noun phrases. In Proc. of the ACL,
pages 373?380.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
S. Bergsma, D. Lin, and R. Goebel. 2008. Distributional
identification of non-referential pronouns. In Proc. of
ACL-08:HLT, pages 10?18.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19(4):380?393.
P. Denis and J. Baldridge. 2007. Global, joint determina-
tion of anaphoricity and coreference resolution using
integer programming. In Proc. of NAACL/HLT, pages
236?243.
M. Elsner and E. Charniak. 2007. A generative
discourse-new model for text coherence. Technical
Report CS-07-04, Brown University.
R. Evans. 2001. Applying machine learning toward an
automatic classification of it. Literary and Linguistic
Computing, 16(1):45?57.
J. R. Finkel and C. Manning. 2008. Enforcing transitiv-
ity in coreference resolution. In Proc. of ACL-08:HLT
Short Papers (Companion Volume), pages 45?48.
C. Kennedy and B. Boguraev. 1996. Anaphor for every-
one: Pronominal anaphora resolution without a parser.
In Proc. of COLING, pages 113?118.
S. Lappin and H. Leass. 1994. An algorithm for pronom-
inal anaphora resolution. Computational Linguistics,
20(4):535?562.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the Bell tree. In
Proc. of the ACL, pages 135?142.
X. Luo. 2005. On coreference resolution performance
metrics. In Proc. of HLT/EMNLP, pages 25?32.
X. Luo. 2007. Coreference or not: A twin model
for coreference resolution. In Proc. of NAACL-HLT,
pages 73?80.
C. Mu?ller. 2006. Automatic detection of nonreferential it
in spoken multi-party dialog. In Proc. of EACL, pages
49?56.
V. Ng. 2007. Shallow semantics for coreference resolu-
tion. In Proceedings of IJCAI, pages 1689?1694.
V. Ng and C. Cardie. 2002a. Identifying anaphoric and
non-anaphoric noun phrases to improve coreference
resolution. In Proc. of COLING, pages 730?736.
V. Ng and C. Cardie. 2002b. Improving machine learn-
ing approaches to coreference resolution. In Proc. of
the ACL, pages 104?111.
V. Ng. 2004. Learning noun phrase anaphoricity to im-
prove conference resolution: Issues in representation
and optimization. In Proc. of the ACL, pages 151?158.
E. W. Noreen. 1989. Computer Intensive Methods for
Testing Hypothesis: An Introduction. John Wiley &
Sons.
C. Paice and G. Husk. 1987. Towards the automatic
recognition of anaphoric features in English text: the
impersonal pronoun ?it?. Computer Speech and Lan-
guage, 2.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of the ACL, pages
271?278.
M. Poesio, O. Uryupina, R. Vieira, M. Alexandrov-
Kabadjov, and R. Goulart. 2004. Discourse-new de-
tectors for definite description resolution: A survey
and a preliminary proposal. In Proc. of the ACL Work-
shop on Reference Resolution.
S. P. Ponzetto and M. Strube. 2006. Exploiting semantic
role labeling, WordNet and Wikipedia for coreference
resolution. In Proc. of HLT/NAACL, pages 192?199.
W. M. Soon, H. T. Ng, and D. Lim. 2001. A machine
learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
O. Uryupina. 2003. High-precision identification of dis-
course new and unique noun phrases. In Proc. of the
ACL Student Research Workshop.
Y. Versley, A. Moschitti, M. Poesio, and X. Yang. 2008.
Coreference systems based on kernels methods. In
Proc. of COLING, pages 961?968.
R. Vieira and M. Poesio. 2000. Processing definite de-
scriptions in corpora. In S. Botley and A. McEnery,
editors, Corpus-based and Computational Approaches
to Discourse Anaphora, pages 189?212. UCL Press.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proc. of MUC-6, pages 45?52.
B. L. Webber. 1979. A Formal Approach to Discourse
Anaphora. Garland Publishing, Inc.
X. Yang and J. Su. 2007. Coreference resolution using
semantic relatedness information from automatically
discovered patterns. In Proc. of ACL, pages 528?535.
583
Proceedings of the 43rd Annual Meeting of the ACL, pages 157?164,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Machine Learning for Coreference Resolution:
From Local Classification to Global Ranking
Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
vince@hlt.utdallas.edu
Abstract
In this paper, we view coreference reso-
lution as a problem of ranking candidate
partitions generated by different coref-
erence systems. We propose a set of
partition-based features to learn a rank-
ing model for distinguishing good and bad
partitions. Our approach compares fa-
vorably to two state-of-the-art coreference
systems when evaluated on three standard
coreference data sets.
1 Introduction
Recent research in coreference resolution ? the
problem of determining which noun phrases (NPs)
in a text or dialogue refer to which real-world
entity ? has exhibited a shift from knowledge-
based approaches to data-driven approaches, yield-
ing learning-based coreference systems that rival
their hand-crafted counterparts in performance (e.g.,
Soon et al (2001), Ng and Cardie (2002b), Strube et
al. (2002), Yang et al (2003), Luo et al (2004)). The
central idea behind the majority of these learning-
based approaches is to recast coreference resolution
as a binary classification task. Specifically, a clas-
sifier is first trained to determine whether two NPs
in a document are co-referring or not. A separate
clustering mechanism then coordinates the possibly
contradictory pairwise coreference classification de-
cisions and constructs a partition on the given set of
NPs, with one cluster for each set of coreferent NPs.
Though reasonably successful, this ?standard? ap-
proach is not as robust as one may think. First, de-
sign decisions such as the choice of the learning al-
gorithm and the clustering procedure are apparently
critical to system performance, but are often made
in an ad-hoc and unprincipled manner that may be
suboptimal from an empirical point of view.
Second, this approach makes no attempt to search
through the space of possible partitions when given
a set of NPs to be clustered, employing instead a
greedy clustering procedure to construct a partition
that may be far from optimal.
Another potential weakness of this approach con-
cerns its inability to directly optimize for clustering-
level accuracy: the coreference classifier is trained
and optimized independently of the clustering pro-
cedure to be used, and hence improvements in clas-
sification accuracy do not guarantee corresponding
improvements in clustering-level accuracy.
Our goal in this paper is to improve the robustness
of the standard approach by addressing the above
weaknesses. Specifically, we propose the following
procedure for coreference resolution: given a set of
NPs to be clustered, (1) use   pre-selected learning-
based coreference systems to generate   candidate
partitions of the NPs, and then (2) apply an auto-
matically acquired ranking model to rank these can-
didate hypotheses, selecting the best one to be the fi-
nal partition. The key features of this approach are:
Minimal human decision making. In contrast to
the standard approach, our method obviates, to a
large extent, the need to make tough or potentially
suboptimal design decisions.1 For instance, if we
1We still need to determine the  coreference systems to be
employed in our framework, however. Fortunately, the choice
of  is flexible, and can be as large as we want subject to the
157
cannot decide whether learner
 
is better to use than
learner  in a coreference system, we can simply
create two copies of the system with one employing
 
and the other  , and then add both into our pre-
selected set of coreference systems.
Generation of multiple candidate partitions. Al-
though an exhaustive search for the best partition is
not computationally feasible even for a document
with a moderate number of NPs, our approach ex-
plores a larger portion of the search space than the
standard approach via generating multiple hypothe-
ses, making it possible to find a potentially better
partition of the NPs under consideration.
Optimization for clustering-level accuracy via
ranking. As mentioned above, the standard ap-
proach trains and optimizes a coreference classifier
without necessarily optimizing for clustering-level
accuracy. In contrast, we attempt to optimize our
ranking model with respect to the target coreference
scoring function, essentially by training it in such
a way that a higher scored candidate partition (ac-
cording to the scoring function) would be assigned a
higher rank (see Section 3.2 for details).
Perhaps even more importantly, our approach pro-
vides a general framework for coreference resolu-
tion. Instead of committing ourselves to a partic-
ular resolution method as in previous approaches,
our framework makes it possible to leverage the
strengths of different methods by allowing them to
participate in the generation of candidate partitions.
We evaluate our approach on three standard coref-
erence data sets using two different scoring met-
rics. In our experiments, our approach compares fa-
vorably to two state-of-the-art coreference systems
adopting the standard machine learning approach,
outperforming them by as much as 4?7% on the
three data sets for one of the performance metrics.
2 Related Work
As mentioned before, our approach differs from the
standard approach primarily by (1) explicitly learn-
ing a ranker and (2) optimizing for clustering-level
accuracy. In this section we will focus on discussing
related work along these two dimensions.
Ranking candidate partitions. Although we are
not aware of any previous attempt on training a
available computing resources.
ranking model using global features of an NP par-
tition, there is some related work on partition rank-
ing where the score of a partition is computed via
a heuristic function of the probabilities of its NP
pairs being coreferent.2 For instance, Harabagiu et
al. (2001) introduce a greedy algorithm for finding
the highest-scored partition by performing a beam
search in the space of possible partitions. At each
step of this search process, candidate partitions are
ranked based on their heuristically computed scores.
Optimizing for clustering-level accuracy. Ng
and Cardie (2002a) attempt to optimize their rule-
based coreference classifier for clustering-level ac-
curacy, essentially by finding a subset of the learned
rules that performs the best on held-out data with
respect to the target coreference scoring program.
Strube and Mu?ller (2003) propose a similar idea, but
aim instead at finding a subset of the available fea-
tures with which the resulting coreference classifier
yields the best clustering-level accuracy on held-out
data. To our knowledge, our work is the first attempt
to optimize a ranker for clustering-level accuracy.
3 A Ranking Approach to Coreference
Our ranking approach operates by first dividing the
available training texts into two disjoint subsets: a
training subset and a held-out subset. More specifi-
cally, we first train each of our   pre-selected coref-
erence systems on the documents in the training sub-
set, and then use these resolvers to generate   can-
didate partitions for each text in the held-out subset
from which a ranking model will be learned. Given
a test text, we use our   coreference systems to cre-
ate   candidate partitions as in training, and select
the highest-ranked partition according to the ranking
model to be the final partition.3 The rest of this sec-
tion describes how we select these   learning-based
coreference systems and acquire the ranking model.
3.1 Selecting Coreference Systems
A learning-based coreference system can be defined
by four elements: the learning algorithm used to
train the coreference classifier, the method of creat-
ing training instances for the learner, the feature set
2Examples of such scoring functions include the Dempster-
Shafer rule (see Kehler (1997) and Bean and Riloff (2004)) and
its variants (see Harabagiu et al (2001) and Luo et al (2004)).
3The ranking model breaks ties randomly.
158
used to represent a training or test instance, and the
clustering algorithm used to coordinate the coref-
erence classification decisions. Selecting a corefer-
ence system, then, is a matter of instantiating these
elements with specific values.
Now we need to define the set of allowable values
for each of these elements. In particular, we want to
define them in such a way that the resulting coref-
erence systems can potentially generate good can-
didate partitions. Given that machine learning ap-
proaches to the problem have been promising, our
choices will be guided by previous learning-based
coreference systems, as described below.
Training instance creation methods. A training
instance represents two NPs, NP  and NP , having a
class value of COREFERENT or NOT COREFERENT
depending on whether the NPs co-refer in the asso-
ciated text. We consider three previously-proposed
methods of creating training instances.
In McCarthy and Lehnert?s method, a positive
instance is created for each anaphoric NP paired
with each of its antecedents, and a negative instance
is created by pairing each NP with each of its preced-
ing non-coreferent noun phrases. Hence, the number
of instances created by this method is quadratic in
the number of NPs in the associated text. The large
number of instances can potentially make the train-
ing process inefficient.
In an attempt to reduce the training time, Soon et
al.?s method creates a smaller number of training in-
stances than McCarthy and Lehnert?s. Specifically,
a positive instance is created for each anaphoric NP,
NP , and its closest antecedent, NP  ; and a negative
instance is created for NP paired with each of the
intervening NPs, NP  , NP  , 	
	
	 , NP .
Unlike Soon et al, Ng and Cardie?s method gen-
erates a positive instance for each anaphoric NP and
its most confident antecedent. For a non-pronominal
NP, the most confident antecedent is assumed to
be its closest non-pronominal antecedent. For pro-
nouns, the most confident antecedent is simply its
closest preceding antecedent. Negative instances are
generated as in Soon et al?s method.
Feature sets. We employ two feature sets for rep-
resenting an instance, as described below.
Soon et al?s feature set consists of 12 surface-
level features, each of which is computed based on
one or both NPs involved in the instance. The fea-
tures can be divided into four groups: lexical, gram-
matical, semantic, and positional. Space limitations
preclude a description of these features. Details can
be found in Soon et al (2001).
Ng and Cardie expand Soon et al?s feature set
from 12 features to a deeper set of 53 to allow more
complex NP string matching operations as well as
finer-grained syntactic and semantic compatibility
tests. See Ng and Cardie (2002b) for details.
Learning algorithms. We consider three learning
algorithms, namely, the C4.5 decision tree induction
system (Quinlan, 1993), the RIPPER rule learning
algorithm (Cohen, 1995), and maximum entropy
classification (Berger et al, 1996). The classifica-
tion model induced by each of these learners returns
a number between 0 and 1 that indicates the likeli-
hood that the two NPs under consideration are coref-
erent. In this work, NP pairs with class values above
0.5 are considered COREFERENT; otherwise the pair
is considered NOT COREFERENT.
Clustering algorithms. We employ three cluster-
ing algorithms, as described below.
The closest-first clustering algorithm selects as
the antecedent of NP its closest preceding coreferent
NP. If no such NP exists, then NP is assumed to be
non-anaphoric (i.e., no antecedent is selected).
On the other hand, the best-first clustering al-
gorithm selects as the antecedent of NP the clos-
est NP with the highest coreference likelihood value
from its set of preceding coreferent NPs. If this
set is empty, then no antecedent is selected for NP .
Since the most likely antecedent is chosen for each
NP, best-first clustering may produce partitions with
higher precision than closest-first clustering.
Finally, in aggressive-merge clustering, each NP
is merged with all of its preceding coreferent NPs.
Since more merging occurs in comparison to the pre-
vious two algorithms, aggressive-merge clustering
may yield partitions with higher recall.
Table 1 summarizes the previous work on coref-
erence resolution that employs the learning algo-
rithms, clustering algorithms, feature sets, and in-
stance creation methods discussed above. With three
learners, three training instance creation methods,
two feature sets, and three clustering algorithms, we
can produce 54 coreference systems in total.
159
Decision tree learners Aone and Bennett (1995), McCarthy and Lehnert (1995), Soon et al (2001),
Learning (C4.5/C5/CART) Strube et al (2002), Strube and Mu?ller (2003), Yang et al (2003)
algorithm RIPPER Ng and Cardie (2002b)
Maximum entropy Kehler (1997), Morton (2000), Luo et al (2004)
Instance McCarthy and Lehnert?s McCarthy and Lehnert (1995), Aone and Bennett (1995)
creation Soon et al?s Soon et al (2001), Strube et al (2002), Iida et al (2003)
method Ng and Cardie?s Ng and Cardie (2002b)
Feature Soon et al?s Soon et al (2001)
set Ng and Cardie?s Ng and Cardie (2002b)
Clustering Closest-first Soon et al (2001), Strube et al (2002)
algorithm Best-first Aone and Bennett (1995), Ng and Cardie (2002b), Iida et al (2003)
Aggressive-merge McCarthy and Lehnert (1995)
Table 1: Summary of the previous work on coreference resolution that employs the learning algorithms, the
clustering algorithms, the feature sets, and the training instance creation methods discussed in Section 3.1.
3.2 Learning to Rank Candidate Partitions
We train an SVM-based ranker for ranking candidate
partitions by means of Joachims? (2002) SVM
 
 
package, with all the parameters set to their default
values. To create training data, we first generate 54
candidate partitions for each text in the held-out sub-
set as described above and then convert each parti-
tion into a training instance consisting of a set of
partition-based features and method-based features.
Partition-based features are used to characterize a
candidate partition and can be derived directly from
the partition itself. Following previous work on us-
ing global features of candidate structures to learn
a ranking model (Collins, 2002), the global (i.e.,
partition-based) features we consider here are sim-
ple functions of the local features that capture the
relationship between NP pairs.
Specifically, we define our partition-based fea-
tures in terms of the features in the Ng and Cardie
(N&C) feature set (see Section 3.1) as follows. First,
let us assume that

  is the  -th nominal feature in
N&C?s feature set and 	    is the 
 -th possible value
of

  . Next, for each  and 
 , we create two partition-
based features,     and    .     is computed over
the set of coreferent NP pairs (with respect to the
candidate partition), denoting the probability of en-
countering

 	    in this set when the pairs are
represented as attribute-value vectors using N&C?s
features. On the other hand,     is computed over
the set of non-coreferent NP pairs (with respect to
the candidate partition), denoting the probability of
encountering

 	    in this set when the pairs are
represented as attribute-value vectors using N&C?s
features. One partition-based feature, for instance,
would denote the probability that two NPs residing
in the same cluster have incompatible gender values.
Intuitively, a good NP partition would have a low
probability value for this feature. So, having these
partition-based features can potentially help us dis-
tinguish good and bad candidate partitions.
Method-based features, on the other hand, are
used to encode the identity of the coreference sys-
tem that generated the candidate partition under con-
sideration. Specifically, we have one method-based
feature representing each pre-selected coreference
system. The feature value is 1 if the corresponding
coreference system generated the candidate partition
and 0 otherwise. These features enable the learner
to learn how to distinguish good and bad partitions
based on the systems that generated them, and are
particularly useful when some coreference systems
perform consistently better than the others.
Now, we need to compute the ?class value? for
each training instance, which is a positive integer de-
noting the rank of the corresponding partition among
the 54 candidates generated for the training docu-
ment under consideration. Recall from the intro-
duction that we want to train our ranking model so
that higher scored partitions according to the target
coreference scoring program are ranked higher. To
this end, we compute the rank of each candidate par-
tition as follows. First, we apply the target scoring
program to score each candidate partition against the
correct partition derived from the training text. We
then assign rank  to the  -th lowest scored parti-
tion.4 Effectively, the learning algorithm learns what
a good partition is from the scoring program.
4Two partitions with the same score will have the same rank.
160
Training Corpus Test Corpus
# Docs # Tokens # Docs # Tokens
BNEWS 216 67470 51 18357
NPAPER 76 71944 17 18174
NWIRE 130 85688 29 20528
Table 2: Statistics for the ACE corpus.
4 Evaluation
4.1 Experimental Setup
For evaluation purposes, we use the ACE (Au-
tomatic Content Extraction) coreference corpus,
which is composed of three data sets created
from three different news sources, namely, broad-
cast news (BNEWS), newspaper (NPAPER), and
newswire (NWIRE).5 Statistics of these data sets are
shown in Table 2. In our experiments, we use the
training texts to acquire coreference classifiers and
evaluate the resulting systems on the test texts with
respect to two commonly-used coreference scoring
programs: the MUC scorer (Vilain et al, 1995) and
the B-CUBED scorer (Bagga and Baldwin, 1998).
4.2 Results Using the MUC Scorer
Baseline systems. We employ as our baseline sys-
tems two existing coreference resolvers: our dupli-
cation of the Soon et al (2001) system and the Ng
and Cardie (2002b) system. Both resolvers adopt
the standard machine learning approach and there-
fore can be characterized using the four elements
discussed in Section 3.1. Specifically, Soon et al?s
system employs a decision tree learner to train a
coreference classifier on instances created by Soon?s
method and represented by Soon?s feature set, coor-
dinating the classification decisions via closest-first
clustering. Ng and Cardie?s system, on the other
hand, employs RIPPER to train a coreference classi-
fier on instances created by N&C?s method and rep-
resented by N&C?s feature set, inducing a partition
on the given NPs via best-first clustering.
The baseline results are shown in rows 1 and 2
of Table 3, where performance is reported in terms
of recall, precision, and F-measure. As we can see,
the N&C system outperforms the Duplicated Soon
system by about 2-6% on the three ACE data sets.
5See http://www.itl.nist.gov/iad/894.01/
tests/ace for details on the ACE research program.
Our approach. Recall that our approach uses la-
beled data to train both the coreference classifiers
and the ranking model. To ensure a fair comparison
of our approach with the baselines, we do not rely
on additional labeled data for learning the ranker;
instead, we use half of the training texts for training
classifiers and the other half for ranking purposes.
Results using our approach are shown in row 3 of
Table 3. Our ranking model, when trained to opti-
mize for F-measure using both partition-based fea-
tures and method-based features, consistently pro-
vides substantial gains in F-measure over both base-
lines. In comparison to the stronger baseline (i.e.,
N&C), F-measure increases by 7.4, 7.2, and 4.6 for
the BNEWS, NPAPER, and NWIRE data sets, re-
spectively. Perhaps more encouragingly, gains in F-
measure are accompanied by simultaneous increase
in recall and precision for all three data sets.
Feature contribution. In an attempt to gain addi-
tional insight into the contribution of partition-based
features and method-based features, we train our
ranking model using each type of features in iso-
lation. Results are shown in rows 4 and 5 of Ta-
ble 3. For the NPAPER and NWIRE data sets, we
still see gains in F-measure over both baseline sys-
tems when the model is trained using either type of
features. The gains, however, are smaller than those
observed when the two types of features are applied
in combination. Perhaps surprisingly, the results for
BNEWS do not exhibit the same trend as those for
the other two data sets. Here, the method-based fea-
tures alone are strongly predictive of good candidate
partitions, yielding even slightly better performance
than when both types of features are applied. Over-
all, however, these results seem to suggest that both
partition-based and method-based features are im-
portant to learning a good ranking model.
Random ranking. An interesting question is:
how much does supervised ranking help? If all of
our candidate partitions are of very high quality, then
ranking will not be particularly important because
choosing any of these partitions may yield good re-
sults. To investigate this question, we apply a ran-
dom ranking model, which randomly selects a can-
didate partition for each test text. Row 6 of Table 3
shows the results (averaged over five runs) when the
random ranker is used in place of the supervised
161
BNEWS NPAPER NWIRE
System Variation R P F R P F R P F
1 Duplicated Soon et al baseline 52.7 47.5 50.0 63.3 56.7 59.8 48.7 40.9 44.5
2 Ng and Cardie baseline 56.5 58.6 57.5 57.1 68.0 62.1 43.1 59.9 50.1
3 Ranking framework 62.2 67.9 64.9 67.4 71.4 69.3 50.1 60.3 54.7
4 Partition-based features only 54.5 55.5 55.0 66.3 63.0 64.7 50.7 51.2 51.0
5 Method-based features only 62.0 68.5 65.1 67.5 61.2 64.2 51.1 49.9 50.5
6 Random ranking model 48.6 54.8 51.5 57.4 63.3 60.2 40.3 44.3 42.2
7 Perfect ranking model 66.0 69.3 67.6 70.4 71.2 70.8 56.6 59.7 58.1
Table 3: Results for the three ACE data sets obtained via the MUC scoring program.
ranker. In comparison to the results in row 3, we
see that the supervised ranker surpasses its random
counterpart by about 9-13% in F-measure, implying
that ranking plays an important role in our approach.
Perfect ranking. It would be informative to see
whether our ranking model is performing at its up-
per limit, because further performance improvement
beyond this point would require enlarging our set of
candidate partitions. So, we apply a perfect ranking
model, which uses an oracle to choose the best can-
didate partition for each test text. Results in row 7 of
Table 3 indicate that our ranking model performs at
about 1-3% below the perfect ranker, suggesting that
we can further improve coreference performance by
improving the ranking model.
4.3 Results Using the B-CUBED Scorer
Baseline systems. In contrast to the MUC results,
the B-CUBED results for the two baseline systems
are mixed (see rows 1 and 2 of Table 4). Specifically,
while there is no clear winner for the NWIRE data
set, N&C performs better on BNEWS but worse on
NPAPER than the Duplicated Soon system.
Our approach. From row 3 of Table 4, we see that
our approach achieves small but consistent improve-
ments in F-measure over both baseline systems. In
comparison to the better baseline, F-measure in-
creases by 0.1, 1.1, and 2.0 for the BNEWS, NPA-
PER, and NWIRE data sets, respectively.
Feature contribution. Unlike the MUC results,
using more features to train the ranking model does
not always yield better performance with respect to
the B-CUBED scorer (see rows 3-5 of Table 4). In
particular, the best result for BNEWS is achieved
using only method-based features, whereas the best
result for NPAPER is obtained using only partition-
based features. Nevertheless, since neither type of
features offers consistently better performance than
the other, it still seems desirable to apply the two
types of features in combination to train the ranker.
Random ranking. Comparing rows 3 and 6 of Ta-
ble 4, we see that the supervised ranker yields a non-
trivial improvement of 2-3% in F-measure over the
random ranker for the three data sets. Hence, rank-
ing still plays an important role in our approach with
respect to the B-CUBED scorer despite its modest
performance gains over the two baseline systems.
Perfect ranking. Results in rows 3 and 7 of Ta-
ble 4 indicate that the supervised ranker underper-
forms the perfect ranker by about 5% for BNEWS
and 3% for both NPAPER and NWIRE in terms
of F-measure, suggesting that the supervised ranker
still has room for improvement. Moreover, by com-
paring rows 1-2 and 7 of Table 4, we can see that
the perfect ranker outperforms the baselines by less
than 5%. This is essentially an upper limit on how
much our approach can improve upon the baselines
given the current set of candidate partitions. In other
words, the performance of our approach is limited in
part by the quality of the candidate partitions, more
so with B-CUBED than with the MUC scorer.
5 Discussion
Two questions naturally arise after examining the
above results. First, which of the 54 coreference sys-
tems generally yield superior results? Second, why
is the same set of candidate partitions scored so dif-
ferently by the two scoring programs?
To address the first question, we take the 54 coref-
erence systems that were trained on half of the avail-
able training texts (see Section 4) and apply them to
the three ACE test data sets. Table 5 shows the best-
performing resolver for each test set and scoring pro-
gram combination. Interestingly, with respect to the
162
BNEWS NPAPER NWIRE
System Variation R P F R P F R P F
1 Duplicated Soon et al baseline 53.4 78.4 63.5 58.0 75.4 65.6 56.0 75.3 64.2
2 Ng and Cardie baseline 59.9 72.3 65.5 61.8 64.9 63.3 62.3 66.7 64.4
3 Ranking framework 57.0 77.1 65.6 62.8 71.2 66.7 59.3 75.4 66.4
4 Partition-based features only 55.0 79.1 64.9 61.3 74.7 67.4 57.1 76.8 65.5
5 Method-based features only 63.1 69.8 65.8 58.4 75.2 65.8 58.9 75.5 66.1
6 Random ranking model 52.5 79.9 63.4 58.4 69.2 63.3 54.3 77.4 63.8
7 Perfect ranking model 64.5 76.7 70.0 61.3 79.1 69.1 63.2 76.2 69.1
Table 4: Results for the three ACE data sets obtained via the B-CUBED scoring program.
MUC scorer, the best performance on the three data
sets is achieved by the same resolver. The results
with respect to B-CUBED are mixed, however.
For each resolver shown in Table 5, we also com-
pute the average rank of the partitions generated
by the resolver for the corresponding test texts.6
Intuitively, a resolver that consistently produces
good partitions (relative to other candidate parti-
tions) would achieve a low average rank. Hence, we
can infer from the fairly high rank associated with
the top B-CUBED resolvers that they do not perform
consistently better than their counterparts.
Regarding our second question of why the same
set of candidate partitions is scored differently by the
two scoring programs, the reason can be attributed
to two key algorithmic differences between these
scorers. First, while the MUC scorer only rewards
correct identification of coreferent links, B-CUBED
additionally rewards successful recognition of non-
coreference relationships. Second, the MUC scorer
applies the same penalty to each erroneous merging
decision, whereas B-CUBED penalizes erroneous
merging decisions involving two large clusters more
heavily than those involving two small clusters.
Both of the above differences can potentially
cause B-CUBED to assign a narrower range of F-
measure scores to each set of 54 candidate partitions
than the MUC scorer, for the following reasons.
First, our candidate partitions in general agree more
on singleton clusters than on non-singleton clusters.
Second, by employing a non-uniform penalty func-
tion B-CUBED effectively removes a bias inherent
in the MUC scorer that leads to under-penalization
of partitions in which entities are over-clustered.
Nevertheless, our B-CUBED results suggest that
6The rank of a partition is computed in the same way as in
Section 3.2, except that we now adopt the common convention
of assigning rank   to the   -th highest scored partition.
(1) despite its modest improvement over the base-
lines, our approach offers robust performance across
the data sets; and (2) we could obtain better scores
by improving the ranking model and expanding our
set of candidate partitions, as elaborated below.
To improve the ranking model, we can potentially
(1) design new features that better characterize a
candidate partition (e.g., features that measure the
size and the internal cohesion of a cluster), and (2)
reserve more labeled data for training the model. In
the latter case we may have less data for training
coreference classifiers, but at the same time we can
employ weakly supervised techniques to bootstrap
the classifiers. Previous attempts on bootstrapping
coreference classifiers have only been mildly suc-
cessful (e.g., Mu?ller et al (2002)), and this is also
an area that deserves further research.
To expand our set of candidate partitions, we can
potentially incorporate more high-performing coref-
erence systems into our framework, which is flex-
ible enough to accommodate even those that adopt
knowledge-based (e.g., Harabagiu et al (2001)) and
unsupervised approaches (e.g., Cardie and Wagstaff
(1999), Bean and Riloff (2004)). Of course, we
can also expand our pre-selected set of corefer-
ence systems via incorporating additional learning
algorithms, clustering algorithms, and feature sets.
Once again, we may use previous work to guide our
choices. For instance, Iida et al (2003) and Ze-
lenko et al (2004) have explored the use of SVM,
voted perceptron, and logistic regression for train-
ing coreference classifiers. McCallum and Well-
ner (2003) and Zelenko et al (2004) have employed
graph-based partitioning algorithms such as corre-
lation clustering (Bansal et al, 2002). Finally,
Strube et al (2002) and Iida et al (2003) have pro-
posed new edit-distance-based string-matching fea-
tures and centering-based features, respectively.
163
Scoring Average Coreference System
Test Set Program Rank Instance Creation Method Feature Set Learner Clustering Algorithm
BNEWS MUC 7.2549 McCarthy and Lehnert?s Ng and Cardie?s C4.5 aggressive-merge
BCUBED 16.9020 McCarthy and Lehnert?s Ng and Cardie?s C4.5 aggressive-merge
NPAPER MUC 1.4706 McCarthy and Lehnert?s Ng and Cardie?s C4.5 aggressive-merge
B-CUBED 9.3529 Soon et al?s Soon et al?s RIPPER closest-first
NWIRE MUC 7.7241 McCarthy and Lehnert?s Ng and Cardie?s C4.5 aggressive-merge
B-CUBED 13.1379 Ng and Cardie?s Ng and Cardie?s MaxEnt closest-first
Table 5: The coreference systems that achieved the highest F-measure scores for each test set and scorer
combination. The average rank of the candidate partitions produced by each system for the corresponding test set is also shown.
Acknowledgments
We thank the three anonymous reviewers for their
valuable comments on an earlier draft of the paper.
References
C. Aone and S. W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strate-
gies. In Proc. of the ACL, pages 122?129.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proc. of COLING-ACL, pages 79?85.
N. Bansal, A. Blum, and S. Chawla. 2002. Correlation
clustering. In Proc. of FOCS, pages 238?247.
D. Bean and E. Riloff. 2004. Unsupervised learning of
contextual role knowledge for coreference resolution.
In Proc. of HLT/NAACL, pages 297?304.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
C. Cardie and K. Wagstaff. 1999. Noun phrase coref-
erence as clustering. In Proc. of EMNLP/VLC, pages
82?89.
W. Cohen. 1995. Fast effective rule induction. In Proc.
of ICML, pages 115?123.
M. Collins. 2002. Discriminative training methods for
Hidden Markov Models: Theory and experiments with
perceptron algorithms. In Proc. of EMNLP, pages 1?8.
S. Harabagiu, R. Bunescu, and S. Maiorano. 2001. Text
and knowledge mining for coreference resolution. In
Proc. of NAACL, pages 55?62.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003.
Incorporating contextual cues in trainable models for
coreference resolution. In Proc. of the EACL Work-
shop on The Computational Treatment of Anaphora.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. of KDD, pages 133?142.
A. Kehler. 1997. Probabilistic coreference in informa-
tion extraction. In Proc. of EMNLP, pages 163?173.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S.
Roukos. 2004. A mention-synchronous coreference
resolution algorithm based on the Bell tree. In Proc.
of the ACL, pages 136?143.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In Proc. of the IJCAI
Workshop on Information Integration on the Web.
J. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proc. of the IJCAI,
pages 1050?1055.
T. Morton. 2000. Coreference for NLP applications. In
Proc. of the ACL.
C. M u?ller, S. Rapp, and M. Strube. 2002. Applying co-
training to reference resolution. In Proc. of the ACL,
pages 352?359.
V. Ng and C. Cardie. 2002a. Combining sample selec-
tion and error-driven pruning for machine learning of
coreference rules. In Proc. of EMNLP, pages 55?62.
V. Ng and C. Cardie. 2002b. Improving machine learn-
ing approaches to coreference resolution. In Proc. of
the ACL, pages 104?111.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
W. M. Soon, H. T. Ng, and D. Lim. 2001. A machine
learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
M. Strube and C. M u?ller. 2003. A machine learning ap-
proach to pronoun resolution in spoken dialogue. In
Proc. of the ACL, pages 168?175.
M. Strube, S. Rapp, and C. M u?ller. 2002. The influence
of minimum edit distance on reference resolution. In
Proc. of EMNLP, pages 312?319.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.
Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proc. of the Sixth Message Un-
derstanding Conference (MUC-6), pages 45?52.
X. Yang, G. D. Zhou, J. Su, and C. L. Tan. 2003. Coref-
erence resolution using competitive learning approach.
In Proc. of the ACL, pages 176?183.
D. Zelenko, C. Aone, and J. Tibbetts. 2004. Coreference
resolution for information extraction. In Proc. of the
ACL Workshop on Reference Resolution and its Appli-
cations, pages 9?16.
164
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 611?618,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Examining the Role of Linguistic Knowledge Sources in the Automatic
Identification and Classification of Reviews
Vincent Ng and Sajib Dasgupta and S. M. Niaz Arifin
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{vince,sajib,arif}@hlt.utdallas.edu
Abstract
This paper examines two problems in
document-level sentiment analysis: (1) de-
termining whether a given document is a
review or not, and (2) classifying the po-
larity of a review as positive or negative.
We first demonstrate that review identifi-
cation can be performed with high accu-
racy using only unigrams as features. We
then examine the role of four types of sim-
ple linguistic knowledge sources in a po-
larity classification system.
1 Introduction
Sentiment analysis involves the identification of
positive and negative opinions from a text seg-
ment. The task has recently received a lot of
attention, with applications ranging from multi-
perspective question-answering (e.g., Cardie et al
(2004)) to opinion-oriented information extraction
(e.g., Riloff et al (2005)) and summarization (e.g.,
Hu and Liu (2004)). Research in sentiment analy-
sis has generally proceeded at three levels, aim-
ing to identify and classify opinions from doc-
uments, sentences, and phrases. This paper ex-
amines two problems in document-level sentiment
analysis, focusing on analyzing a particular type
of opinionated documents: reviews.
The first problem, polarity classification, has
the goal of determining a review?s polarity ? pos-
itive (?thumbs up?) or negative (?thumbs down?).
Recent work has expanded the polarity classifi-
cation task to additionally handle documents ex-
pressing a neutral sentiment. Although studied
fairly extensively, polarity classification remains a
challenge to natural language processing systems.
We will focus on an important linguistic aspect
of polarity classification: examining the role of a
variety of simple, yet under-investigated, linguis-
tic knowledge sources in a learning-based polarity
classification system. Specifically, we will show
how to build a high-performing polarity classifier
by exploiting information provided by (1) high or-
der n-grams, (2) a lexicon composed of adjectives
manually annotated with their polarity information
(e.g., happy is annotated as positive and terrible as
negative), (3) dependency relations derived from
dependency parses, and (4) objective terms and
phrases extracted from neutral documents.
As mentioned above, the majority of work on
document-level sentiment analysis to date has fo-
cused on polarity classification, assuming as in-
put a set of reviews to be classified. A relevant
question is: what if we don?t know that an input
document is a review in the first place? The sec-
ond task we will examine in this paper ? review
identification ? attempts to address this question.
Specifically, review identification seeks to deter-
mine whether a given document is a review or not.
We view both review identification and polar-
ity classification as a classification task. For re-
view identification, we train a classifier to dis-
tinguish movie reviews and movie-related non-
reviews (e.g., movie ads, plot summaries) using
only unigrams as features, obtaining an accuracy
of over 99% via 10-fold cross-validation. Simi-
lar experiments using documents from the book
domain also yield an accuracy as high as 97%.
An analysis of the results reveals that the high ac-
curacy can be attributed to the difference in the
vocabulary employed in reviews and non-reviews:
while reviews can be composed of a mixture of
subjective and objective language, our non-review
documents rarely contain subjective expressions.
Next, we learn our polarity classifier using pos-
itive and negative reviews taken from two movie
611
review datasets, one assembled by Pang and Lee
(2004) and the other by ourselves. The result-
ing classifier, when trained on a feature set de-
rived from the four types of linguistic knowl-
edge sources mentioned above, achieves a 10-fold
cross-validation accuracy of 90.5% and 86.1% on
Pang et al?s dataset and ours, respectively. To our
knowledge, our result on Pang et al?s dataset is
one of the best reported to date. Perhaps more im-
portantly, an analysis of these results show that the
various types of features interact in an interesting
manner, allowing us to draw conclusions that pro-
vide new insights into polarity classification.
2 Related Work
2.1 Review Identification
As noted in the introduction, while a review can
contain both subjective and objective phrases, our
non-reviews are essentially factual documents in
which subjective expressions can rarely be found.
Hence, review identification can be viewed as an
instance of the broader task of classifying whether
a document is mostly factual/objective or mostly
opinionated/subjective. There have been attempts
on tackling this so-called document-level subjec-
tivity classification task, with very encouraging
results (see Yu and Hatzivassiloglou (2003) and
Wiebe et al (2004) for details).
2.2 Polarity Classification
There is a large body of work on classifying the
polarity of a document (e.g., Pang et al (2002),
Turney (2002)), a sentence (e.g., Liu et al (2003),
Yu and Hatzivassiloglou (2003), Kim and Hovy
(2004), Gamon et al (2005)), a phrase (e.g., Wil-
son et al (2005)), and a specific object (such as a
product) mentioned in a document (e.g., Morinaga
et al (2002), Yi et al (2003), Popescu and Etzioni
(2005)). Below we will center our discussion of
related work around the four types of features we
will explore for polarity classification.
Higher-order n-grams. While n-grams offer a
simple way of capturing context, previous work
has rarely explored the use of n-grams as fea-
tures in a polarity classification system beyond un-
igrams. Two notable exceptions are the work of
Dave et al (2003) and Pang et al (2002). Interest-
ingly, while Dave et al report good performance
on classifying reviews using bigrams or trigrams
alone, Pang et al show that bigrams are not use-
ful features for the task, whether they are used in
isolation or in conjunction with unigrams. This
motivates us to take a closer look at the utility of
higher-order n-grams in polarity classification.
Manually-tagged term polarity. Much work has
been performed on learning to identify and clas-
sify polarity terms (i.e., terms expressing a pos-
itive sentiment (e.g., happy) or a negative senti-
ment (e.g., terrible)) and exploiting them to do
polarity classification (e.g., Hatzivassiloglou and
McKeown (1997), Turney (2002), Kim and Hovy
(2004), Whitelaw et al (2005), Esuli and Se-
bastiani (2005)). Though reasonably successful,
these (semi-)automatic techniques often yield lex-
icons that have either high coverage/low precision
or low coverage/high precision. While manually
constructed positive and negative word lists exist
(e.g., General Inquirer1), they too suffer from the
problem of having low coverage. This prompts us
to manually construct our own polarity word lists2
and study their use in polarity classification.
Dependency relations. There have been several
attempts at extracting features for polarity classi-
fication from dependency parses, but most focus
on extracting specific types of information such as
adjective-noun relations (e.g., Dave et al (2003),
Yi et al (2003)) or nouns that enjoy a dependency
relation with a polarity term (e.g., Popescu and Et-
zioni (2005)). Wilson et al (2005) extract a larger
variety of features from dependency parses, but
unlike us, their goal is to determine the polarity of
a phrase, not a document. In comparison to previ-
ous work, we investigate the use of a larger set of
dependency relations for classifying reviews.
Objective information. The objective portions
of a review do not contain the author?s opinion;
hence features extracted from objective sentences
and phrases are irrelevant with respect to the po-
larity classification task and their presence may
complicate the learning task. Indeed, recent work
has shown that benefits can be made by first sepa-
rating facts from opinions in a document (e.g, Yu
and Hatzivassiloglou (2003)) and classifying the
polarity based solely on the subjective portions of
the document (e.g., Pang and Lee (2004)). Moti-
vated by the work of Koppel and Schler (2005), we
identify and extract objective material from non-
reviews and show how to exploit such information
in polarity classification.
1http://www.wjh.harvard.edu/?inquirer/
spreadsheet guid.htm
2Wilson et al (2005) have also manually tagged a list of
terms with their polarity, but this list is not publicly available.
612
Finally, previous work has also investigated fea-
tures that do not fall into any of the above cate-
gories. For instance, instead of representing the
polarity of a term using a binary value, Mullen
and Collier (2004) use Turney?s (2002) method to
assign a real value to represent term polarity and
introduce a variety of numerical features that are
aggregate measures of the polarity values of terms
selected from the document under consideration.
3 Review Identification
Recall that the goal of review identification is
to determine whether a given document is a re-
view or not. Given this definition, two immediate
questions come to mind. First, should this prob-
lem be addressed in a domain-specific or domain-
independent manner? In other words, should a re-
view identification system take as input documents
coming from the same domain or not?
Apparently this is a design question with no
definite answer, but our decision is to perform
domain-specific review identification. The reason
is that the primary motivation of review identifi-
cation is the need to identify reviews for further
analysis by a polarity classification system. Since
polarity classification has almost exclusively been
addressed in a domain-specific fashion, it seems
natural that its immediate upstream component ?
review identification ? should also assume do-
main specificity. Note, however, that assuming
domain specificity is not a self-imposed limita-
tion. In fact, we envision that the review identifica-
tion system will have as its upstream component a
text classification system, which will classify doc-
uments by topic and pass to the review identifier
only those documents that fall within its domain.
Given our choice of domain specificity, the next
question is: which documents are non-reviews?
Here, we adopt a simple and natural definition:
a non-review is any document that belongs to the
given domain but is not a review.
Dataset. Now, recall from the introduction that
we cast review identification as a classification
task. To train and test our review identifier, we
use 2000 reviews and 2000 non-reviews from the
movie domain. The 2000 reviews are taken from
Pang et al?s polarity dataset (version 2.0)3, which
consists of an equal number of positive and neg-
ative reviews. We collect the non-reviews for the
3Available from http://www.cs.cornell.edu/
people/pabo/movie-review-data.
movie domain from the Internet Movie Database
website4, randomly selecting any documents from
this site that are on the movie topic but are not re-
views themselves. With this criterion in mind, the
2000 non-review documents we end up with are
either movie ads or plot summaries.
Training and testing the review identifier. We
perform 10-fold cross-validation (CV) experi-
ments on the above dataset, using Joachims?
(1999) SVMlight package5 to train an SVM clas-
sifier for distinguishing reviews and non-reviews.
All learning parameters are set to their default
values.6 Each document is first tokenized and
downcased, and then represented as a vector of
unigrams with length normalization.7 Following
Pang et al (2002), we use frequency as presence.
In other words, the ith element of the document
vector is 1 if the corresponding unigram is present
in the document and 0 otherwise. The resulting
classifier achieves an accuracy of 99.8%.
Classifying neutral reviews and non-reviews.
Admittedly, the high accuracy achieved using such
a simple set of features is somewhat surpris-
ing, although it is consistent with previous re-
sults on document-level subjectivity classification
in which accuracies of 94-97% were obtained (Yu
and Hatzivassiloglou, 2003; Wiebe et al, 2004).
Before concluding that review classification is an
easy task, we conduct an additional experiment:
we train a review identifier on a new dataset where
we keep the same 2000 non-reviews but replace
the positive/negative reviews with 2000 neutral re-
views (i.e., reviews with a mediocre rating). In-
tuitively, a neutral review contains fewer terms
with strong polarity than a positive/negative re-
view. Hence, this additional experiment would al-
low us to investigate whether the lack of strong
polarized terms in neutral reviews would increase
the difficulty of the learning task.
Our neutral reviews are randomly chosen from
Pang et al?s pool of 27886 unprocessed movie re-
views8 that have either a rating of 2 (on a 4-point
scale) or 2.5 (on a 5-point scale). Each review then
undergoes a semi-automatic preprocessing stage
4See http://www.imdb.com.
5Available from svmlight.joachims.org.
6We tried polynomial and RBF kernels, but none yields
better performance than the default linear kernel.
7We observed that not performing length normalization
hurts performance slightly.
8Also available from Pang?s website. See Footnote 3.
613
where (1) HTML tags and any header and trailer
information (such as date and author identity) are
removed; (2) the document is tokenized and down-
cased; (3) the rating information extracted by reg-
ular expressions is removed; and (4) the document
is manually checked to ensure that the rating infor-
mation is successfully removed. When trained on
this new dataset, the review identifier also achieves
an accuracy of 99.8%, suggesting that this learning
task isn?t any harder in comparison to the previous
one.
Discussion. We hypothesized that the high accu-
racies are attributable to the different vocabulary
used in reviews and non-reviews. As part of our
verification of this hypothesis, we plot the learn-
ing curve for each of the above experiments.9 We
observe that a 99% accuracy was achieved in all
cases even when only 200 training instances are
used to acquire the review identifier. The abil-
ity to separate the two classes with such a small
amount of training data seems to imply that fea-
tures strongly indicative of one or both classes are
present. To test this hypothesis, we examine the
?informative? features for both classes. To get
these informative features, we rank the features by
their weighted log-likelihood ratio (WLLR)10:
P (wt|cj) log
P (wt|cj)
P (wt|?cj)
,
where wt and cj denote the tth word in the vocab-ulary and the jth class, respectively. Informally,
a feature (in our case a unigram) w will have a
high rank with respect to a class c if it appears fre-
quently in c and infrequently in other classes. This
correlates reasonably well with what we think an
informative feature should be. A closer examina-
tion of the feature lists sorted by WLLR confirms
our hypothesis that each of the two classes has its
own set of distinguishing features.
Experiments with the book domain. To under-
stand whether these good review identification re-
sults only hold true for the movie domain, we
conduct similar experiments with book reviews
and non-reviews. Specifically, we collect 1000
book reviews (consisting of a mixture of positive,
negative, and neutral reviews) from the Barnes
9The curves are not shown due to space limitations.
10Nigam et al (2000) show that this metric is effec-
tive at selecting good features for text classification. Other
commonly-used feature selection metrics are discussed in
Yang and Pedersen (1997).
and Noble website11, and 1000 non-reviews that
are on the book topic (mostly book summaries)
from Amazon.12 We then perform 10-fold CV ex-
periments using these 2000 documents as before,
achieving a high accuracy of 96.8%. These results
seem to suggest that automatic review identifica-
tion can be achieved with high accuracy.
4 Polarity Classification
Compared to review identification, polarity classi-
fication appears to be a much harder task. This
section examines the role of various linguistic
knowledge sources in our learning-based polarity
classification system.
4.1 Experimental Setup
Like several previous work (e.g., Mullen and Col-
lier (2004), Pang and Lee (2004), Whitelaw et al
(2005)), we view polarity classification as a super-
vised learning task. As in review identification,
we use SVMlight with default parameter settings
to train polarity classifiers13 , reporting all results
as 10-fold CV accuracy.
We evaluate our polarity classifiers on two
movie review datasets, each of which consists of
1000 positive reviews and 1000 negative reviews.
The first one, which we will refer to as Dataset A,
is the Pang et al polarity dataset (version 2.0). The
second one (Dataset B) was created by us, with the
sole purpose of providing additional experimental
results. Reviews in Dataset B were randomly cho-
sen from Pang et al?s pool of 27886 unprocessed
movie reviews (see Section 3) that have either a
positive or a negative rating. We followed exactly
Pang et al?s guideline when determining whether
a review is positive or negative.14 Also, we took
care to ensure that reviews included in Dataset B
do not appear in Dataset A. We applied to these re-
views the same four pre-processing steps that we
did to the neutral reviews in the previous section.
4.2 Results
The baseline classifier. We can now train our
baseline polarity classifier on each of the two
11www.barnesandnoble.com
12www.amazon.com
13We also experimented with polynomial and RBF kernels
when training polarity classifiers, but neither yields better re-
sults than linear kernels.
14The guidelines come with their polarity dataset. Briefly,
a positive review has a rating of ? 3.5 (out of 5) or ? 3 (out
of 4), whereas a negative review has a rating of ? 2 (out of 5)
or ? 1.5 (out of 4).
614
System Variation Dataset A Dataset B
Baseline 87.1 82.7
Adding bigrams 89.2 84.7
and trigrams
Adding dependency 89.0 84.5
relations
Adding polarity 90.4 86.2
info of adjectives
Discarding objective 90.5 86.1
materials
Table 1: Polarity classification accuracies.
datasets. Our baseline classifier employs as fea-
tures the k highest-ranking unigrams according to
WLLR, with k/2 features selected from each class.
Results with k = 10000 are shown in row 1 of Ta-
ble 1.15 As we can see, the baseline achieves an
accuracy of 87.1% and 82.7% on Datasets A and
B, respectively. Note that our result on Dataset
A is as strong as that obtained by Pang and Lee
(2004) via their subjectivity summarization algo-
rithm, which retains only the subjective portions
of a document.
As a sanity check, we duplicated Pang et al?s
(2002) baseline in which all unigrams that appear
four or more times in the training documents are
used as features. The resulting classifier achieves
an accuracy of 87.2% and 82.7% for Datasets A
and B, respectively. Neither of these results are
significantly different from our baseline results.16
Adding higher-order n-grams. The negative
results that Pang et al (2002) obtained when us-
ing bigrams as features for their polarity classi-
fier seem to suggest that high-order n-grams are
not useful for polarity classification. However, re-
cent research in the related (but arguably simpler)
task of text classification shows that a bigram-
based text classifier outperforms its unigram-
based counterpart (Peng et al, 2003). This
prompts us to re-examine the utility of high-order
n-grams in polarity classification.
In our experiments we consider adding bigrams
and trigrams to our baseline feature set. However,
since these higher-order n-grams significantly out-
number the unigrams, adding all of them to the
feature set will dramatically increase the dimen-
15We experimented with several values of k and obtained
the best result with k = 10000.
16We use two-tailed paired t-tests when performing signif-
icance testing, with p set to 0.05 unless otherwise stated.
sionality of the feature space and may undermine
the impact of the unigrams in the resulting clas-
sifier. To avoid this potential problem, we keep
the number of unigrams and higher-order n-grams
equal. Specifically, we augment the baseline fea-
ture set (consisting of 10000 unigrams) with 5000
bigrams and 5000 trigrams. The bigrams and tri-
grams are selected based on their WLLR com-
puted over the positive reviews and negative re-
views in the training set for each CV run.
Results using this augmented feature set are
shown in row 2 of Table 1. We see that accu-
racy rises significantly from 87.1% to 89.2% for
Dataset A and from 82.7% to 84.7% for Dataset B.
This provides evidence that polarity classification
can indeed benefit from higher-order n-grams.
Adding dependency relations. While bigrams
and trigrams are good at capturing local dependen-
cies, dependency relations can be used to capture
non-local dependencies among the constituents of
a sentence. Hence, we hypothesized that our n-
gram-based polarity classifier would benefit from
the addition of dependency-based features.
Unlike most previous work on polarity classi-
fication, which has largely focused on exploiting
adjective-noun (AN) relations (e.g., Dave et al
(2003), Popescu and Etzioni (2005)), we hypothe-
sized that subject-verb (SV) and verb-object (VO)
relations would also be useful for the task. The
following (one-sentence) review illustrates why.
While I really like the actors, the plot is
rather uninteresting.
A unigram-based polarity classifier could be con-
fused by the simultaneous presence of the posi-
tive term like and the negative term uninteresting
when classifying this review. However, incorpo-
rating the VO relation (like, actors) as a feature
may allow the learner to learn that the author likes
the actors and not necessarily the movie.
In our experiments, the SV, VO and AN re-
lations are extracted from each document by the
MINIPAR dependency parser (Lin, 1998). As
with n-grams, instead of using all the SV, VO and
AN relations as features, we select among them
the best 5000 according to their WLLR and re-
train the polarity classifier with our n-gram-based
feature set augmented by these 5000 dependency-
based features. Results in row 3 of Table 1 are
somewhat surprising: the addition of dependency-
based features does not offer any improvements
over the simple n-gram-based classifier.
615
Incorporating manually tagged term polarity.
Next, we consider incorporating a set of features
that are computed based on the polarity of adjec-
tives. As noted before, we desire a high-precision,
high-coverage lexicon. So, instead of exploiting a
learned lexicon, we manually develop one.
To construct the lexicon, we take Pang et al?s
pool of unprocessed documents (see Section 3),
remove those that appear in either Dataset A or
Dataset B17, and compile a list of adjectives from
the remaining documents. Then, based on heuris-
tics proposed in psycholinguistics18 , we hand-
annotate each adjective with its prior polarity (i.e.,
polarity in the absence of context). Out of the
45592 adjectives we collected, 3599 were labeled
as positive, 3204 as negative, and 38789 as neu-
tral. A closer look at these adjectives reveals that
they are by no means domain-dependent despite
the fact that they were taken from movie reviews.
Now let us consider a simple procedure P for
deriving a feature set that incorporates information
from our lexicon: (1) collect all the bigrams from
the training set; (2) for each bigram that contains at
least one adjective labeled as positive or negative
according to our lexicon, create a new feature that
is identical to the bigram except that each adjec-
tive is replaced with its polarity label19; (3) merge
the list of newly generated features with the list
of bigrams20 and select the top 5000 features from
the merged list according to their WLLR.
We then repeat procedure P for the trigrams
and also the dependency features, resulting in a
total of 15000 features. Our new feature set com-
prises these 15000 features as well as the 10000
unigrams we used in the previous experiments.
Results of the polarity classifier that incorpo-
rates term polarity information are encouraging
(see row 4 of Table 1). In comparison to the classi-
fier that uses only n-grams and dependency-based
features (row 3), accuracy increases significantly
(p = .1) from 89.2% to 90.4% for Dataset A, and
from 84.7% to 86.2% for Dataset B. These results
suggest that the classifier has benefited from the
17We treat the test documents as unseen data that should
not be accessed for any purpose during system development.
18http://www.sci.sdsu.edu/CAL/wordlist
19Neutral adjectives are not replaced.
20A newly generated feature could be misleading for the
learner if the contextual polarity (i.e., polarity in the presence
of context) of the adjective involved differs from its prior po-
larity (see Wilson et al (2005)). The motivation behind merg-
ing with the bigrams is to create a feature set that is more
robust in the face of potentially misleading generalizations.
use of features that are less sparse than n-grams.
Using objective information. Some of the
25000 features we generated above correspond to
n-grams or dependency relations that do not con-
tain subjective information. We hypothesized that
not employing these ?objective? features in the
feature set would improve system performance.
More specifically, our goal is to use procedure P
again to generate 25000 ?subjective? features by
ensuring that the objective ones are not selected
for incorporation into our feature set.
To achieve this goal, we first use the following
rote-learning procedure to identify objective ma-
terial: (1) extract all unigrams that appear in ob-
jective documents, which in our case are the 2000
non-reviews used in review identification [see Sec-
tion 3]; (2) from these ?objective? unigrams, we
take the best 20000 according to their WLLR com-
puted over the non-reviews and the reviews in the
training set for each CV run; (3) repeat steps 1 and
2 separately for bigrams, trigrams and dependency
relations; (4) merge these four lists to create our
80000-element list of objective material.
Now, we can employ procedure P to get a list of
25000 ?subjective? features by ensuring that those
that appear in our 80000-element list are not se-
lected for incorporation into our feature set.
Results of our classifier trained using these sub-
jective features are shown in row 5 of Table 1.
Somewhat surprisingly, in comparison to row 4,
we see that our method for filtering objective fea-
tures does not help improve performance on the
two datasets. We will examine the reasons in the
following subsection.
4.3 Discussion and Further Analysis
Using the four types of knowledge sources pre-
viously described, our polarity classifier signifi-
cantly outperforms a unigram-based baseline clas-
sifier. In this subsection, we analyze some of these
results and conduct additional experiments in an
attempt to gain further insight into the polarity
classification task. Due to space limitations, we
will simply present results on Dataset A below,
and show results on Dataset B only in cases where
a different trend is observed.
The role of feature selection. In all of our ex-
periments we used the best k features obtained via
WLLR. An interesting question is: how will these
results change if we do not perform feature selec-
tion? To investigate this question, we conduct two
616
experiments. First, we train a polarity classifier us-
ing all unigrams from the training set. Second, we
train another polarity classifier using all unigrams,
bigrams, and trigrams. We obtain an accuracy of
87.2% and 79.5% for the first and second experi-
ments, respectively.
In comparison to our baseline classifier, which
achieves an accuracy of 87.1%, we can see that
using all unigrams does not hurt performance, but
performance drops abruptly with the addition of
all bigrams and trigrams. These results suggest
that feature selection is critical when bigrams and
trigrams are used in conjunction with unigrams for
training a polarity classifier.
The role of bigrams and trigrams. So far we
have seen that training a polarity classifier using
only unigrams gives us reasonably good, though
not outstanding, results. Our question, then, is:
would bigrams alone do a better job at capturing
the sentiment of a document than unigrams? To
answer this question, we train a classifier using all
bigrams (without feature selection) and obtain an
accuracy of 83.6%, which is significantly worse
than that of a unigram-only classifier. Similar re-
sults were also obtained by Pang et al (2002).
It is possible that the worse result is due to the
presence of a large number of irrelevant bigrams.
To test this hypothesis, we repeat the above exper-
iment except that we only use the best 10000 bi-
grams selected according to WLLR. Interestingly,
the resulting classifier gives us a lower accuracy
of 82.3%, suggesting that the poor accuracy is not
due to the presence of irrelevant bigrams.
To understand why using bigrams alone does
not yield a good classification model, we examine
a number of test documents and find that the fea-
ture vectors corresponding to some of these docu-
ments (particularly the short ones) have all zeroes
in them. In other words, none of the bigrams from
the training set appears in these reviews. This sug-
gests that the main problem with the bigram model
is likely to be data sparseness. Additional experi-
ments show that the trigram-only classifier yields
even worse results than the bigram-only classifier,
probably because of the same reason.
Nevertheless, these higher-order n-grams play a
non-trivial role in polarity classification: we have
shown that the addition of bigrams and trigrams
selected via WLLR to a unigram-based classifier
significantly improves its performance.
The role of dependency relations. In the previ-
ous subsection we see that dependency relations
do not contribute to overall performance on top
of bigrams and trigrams. There are two plausi-
ble reasons. First, dependency relations are simply
not useful for polarity classification. Second, the
higher-order n-grams and the dependency-based
features capture essentially the same information
and so using either of them would be sufficient.
To test the first hypothesis, we train a clas-
sifier using only 10000 unigrams and 10000
dependency-based features (both selected accord-
ing to WLLR). For Dataset A, the classifier
achieves an accuracy of 87.1%, which is statis-
tically indistinguishable from our baseline result.
On the other hand, the accuracy for Dataset B is
83.5%, which is significantly better than the cor-
responding baseline (82.7%) at the p = .1 level.
These results indicate that dependency informa-
tion is somewhat useful for the task when bigrams
and trigrams are not used. So the first hypothesis
is not entirely true.
So, it seems to be the case that the dependency
relations do not provide useful knowledge for po-
larity classification only in the presence of bigrams
and trigrams. This is somewhat surprising, since
these n-grams do not capture the non-local depen-
dencies (such as those that may be present in cer-
tain SV or VO relations) that should intuitively be
useful for polarity classification.
To better understand this issue, we again exam-
ine a number of test documents. Our initial in-
vestigation suggests that the problem might have
stemmed from the fact that MINIPAR returns de-
pendency relations in which all the verb inflections
are removed. For instance, given the sentence My
cousin Paul really likes this long movie, MINIPAR
will return the VO relation (like, movie). To see
why this can be a problem, consider another sen-
tence I like this long movie. From this sentence,
MINIPAR will also extract the VO relation (like,
movie). Hence, this same VO relation is cap-
turing two different situations, one in which the
author himself likes the movie, and in the other,
the author?s cousin likes the movie. The over-
generalization resulting from these ?stemmed? re-
lations renders dependency information not useful
for polarity classification. Additional experiments
are needed to determine the role of dependency re-
lations when stemming in MINIPAR is disabled.
617
The role of objective information. Results
from the previous subsection suggest that our
method for extracting objective materials and re-
moving them from the reviews is not effective in
terms of improving performance. To determine the
reason, we examine the n-grams and the depen-
dency relations that are extracted from the non-
reviews. We find that only in a few cases do these
extracted objective materials appear in our set of
25000 features obtained in Section 4.2. This ex-
plains why our method is not as effective as we
originally thought. We conjecture that more so-
phisticated methods would be needed in order to
take advantage of objective information in polar-
ity classification (e.g., Koppel and Schler (2005)).
5 Conclusions
We have examined two problems in document-
level sentiment analysis, namely, review identifi-
cation and polarity classification. We first found
that review identification can be achieved with
very high accuracies (97-99%) simply by training
an SVM classifier using unigrams as features. We
then examined the role of several linguistic knowl-
edge sources in polarity classification. Our re-
sults suggested that bigrams and trigrams selected
according to the weighted log-likelihood ratio as
well as manually tagged term polarity informa-
tion are very useful features for the task. On the
other hand, no further performance gains are ob-
tained by incorporating dependency-based infor-
mation or filtering objective materials from the re-
views using our proposed method. Nevertheless,
the resulting polarity classifier compares favorably
to state-of-the-art sentiment classification systems.
References
C. Cardie, J. Wiebe, T. Wilson, and D. Litman. 2004. Low-
level annotations and summary representations of opin-
ions for multi-perspective question answering. In New Di-
rections in Question Answering. AAAI Press/MIT Press.
K. Dave, S. Lawrence, and D. M. Pennock. 2003. Mining
the peanut gallery: Opinion extraction and semantic clas-
sification of product reviews. In Proc. of WWW, pages
519?528.
A. Esuli and F. Sebastiani. 2005. Determining the semantic
orientation of terms through gloss classification. In Proc.
of CIKM, pages 617?624.
M. Gamon, A. Aue, S. Corston-Oliver, and E. K. Ringger.
2005. Pulse: Mining customer opinions from free text.
In Proc. of the 6th International Symposium on Intelligent
Data Analysis, pages 121?132.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting
the semantic orientation of adjectives. In Proc. of the
ACL/EACL, pages 174?181.
M. Hu and B. Liu. 2004. Mining and summarizing customer
reviews. In Proc. of KDD, pages 168?177.
T. Joachims. 1999. Making large-scale SVM learning prac-
tical. In Advances in Kernel Methods - Support Vector
Learning, pages 44?56. MIT Press.
S.-M. Kim and E. Hovy. 2004. Determining the sentiment of
opinions. In Proc. of COLING, pages 1367?1373.
M. Koppel and J. Schler. 2005. Using neutral examples for
learning polarity. In Proc. of IJCAI (poster).
D. Lin. 1998. Dependency-based evaluation of MINIPAR.
In Proc. of the LREC Workshop on the Evaluation of Pars-
ing Systems, pages 48?56.
H. Liu, H. Lieberman, and T. Selker. 2003. A model of tex-
tual affect sensing using real-world knowledge. In Proc.
of Intelligent User Interfaces (IUI), pages 125?132.
S. Morinaga, K. Yamanishi, K. Tateishi, and T. Fukushima.
2002. Mining product reputations on the web. In Proc. of
KDD, pages 341?349.
T. Mullen and N. Collier. 2004. Sentiment analysis using
support vector machines with diverse information sources.
In Proc. of EMNLP, pages 412?418.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000.
Text classification from labeled and unlabeled documents
using EM. Machine Learning, 39(2/3):103?134.
B. Pang and L. Lee. 2004. A sentimental education: Senti-
ment analysis using subjectivity summarization based on
minimum cuts. In Proc. of the ACL, pages 271?278.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning tech-
niques. In Proc. of EMNLP, pages 79?86.
F. Peng, D. Schuurmans, and S. Wang. 2003. Language and
task independent text categorization with simple language
models. In HLT/NAACL: Main Proc. , pages 189?196.
A.-M. Popescu and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proc. of HLT-
EMNLP, pages 339?346.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction.
In Proc. of AAAI, pages 1106?1111.
P. Turney. 2002. Thumbs up or thumbs down? Semantic ori-
entation applied to unsupervised classification of reviews.
In Proc. of the ACL, pages 417?424.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Using ap-
praisal groups for sentiment analysis. In Proc. of CIKM,
pages 625?631.
J. M. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin.
2004. Learning subjective language. Computational Lin-
guistics, 30(3):277?308.
T. Wilson, J. M. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment analysis.
In Proc. of EMNLP, pages 347?354.
Y. Yang and J. O. Pedersen. 1997. A comparative study on
feature selection in text categorization. In Proc. of ICML,
pages 412?420.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a given
topic using natural language processing techniques. In
Proc. of the IEEE International Conference on Data Min-
ing (ICDM).
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Proc. of
EMNLP, pages 129?136.
618
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 536?543,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Semantic Class Induction and Coreference Resolution
Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
vince@hlt.utdallas.edu
Abstract
This paper examines whether a learning-
based coreference resolver can be improved
using semantic class knowledge that is au-
tomatically acquired from a version of the
Penn Treebank in which the noun phrases
are labeled with their semantic classes. Ex-
periments on the ACE test data show that a
resolver that employs such induced semantic
class knowledge yields a statistically signif-
icant improvement of 2% in F-measure over
one that exploits heuristically computed se-
mantic class knowledge. In addition, the in-
duced knowledge improves the accuracy of
common noun resolution by 2-6%.
1 Introduction
In the past decade, knowledge-lean approaches have
significantly influenced research in noun phrase
(NP) coreference resolution ? the problem of deter-
mining which NPs refer to the same real-world en-
tity in a document. In knowledge-lean approaches,
coreference resolvers employ only morpho-syntactic
cues as knowledge sources in the resolution process
(e.g., Mitkov (1998), Tetreault (2001)). While these
approaches have been reasonably successful (see
Mitkov (2002)), Kehler et al (2004) speculate that
deeper linguistic knowledge needs to be made avail-
able to resolvers in order to reach the next level of
performance. In fact, semantics plays a crucially im-
portant role in the resolution of common NPs, allow-
ing us to identify the coreference relation between
two lexically dissimilar common nouns (e.g., talks
and negotiations) and to eliminate George W. Bush
from the list of candidate antecedents of the city, for
instance. As a result, researchers have re-adopted
the once-popular knowledge-rich approach, investi-
gating a variety of semantic knowledge sources for
common noun resolution, such as the semantic rela-
tions between two NPs (e.g., Ji et al (2005)), their
semantic similarity as computed using WordNet
(e.g., Poesio et al (2004)) or Wikipedia (Ponzetto
and Strube, 2006), and the contextual role played by
an NP (see Bean and Riloff (2004)).
Another type of semantic knowledge that has
been employed by coreference resolvers is the se-
mantic class (SC) of an NP, which can be used to dis-
allow coreference between semantically incompat-
ible NPs. However, learning-based resolvers have
not been able to benefit from having an SC agree-
ment feature, presumably because the method used
to compute the SC of an NP is too simplistic: while
the SC of a proper name is computed fairly accu-
rately using a named entity (NE) recognizer, many
resolvers simply assign to a common noun the first
(i.e., most frequent) WordNet sense as its SC (e.g.,
Soon et al (2001), Markert and Nissim (2005)). It
is not easy to measure the accuracy of this heuristic,
but the fact that the SC agreement feature is not used
by Soon et al?s decision tree coreference classifier
seems to suggest that the SC values of the NPs are
not computed accurately by this first-sense heuristic.
Motivated in part by this observation, we exam-
ine whether automatically induced semantic class
knowledge can improve the performance of a
learning-based coreference resolver, reporting eval-
uation results on the commonly-used ACE corefer-
536
ence corpus. Our investigation proceeds as follows.
Train a classifier for labeling the SC of an NP.
In ACE, we are primarily concerned with classify-
ing an NP as belonging to one of the ACE seman-
tic classes. For instance, part of the ACE Phase 2
evaluation involves classifying an NP as PERSON,
ORGANIZATION, GPE (a geographical-political re-
gion), FACILITY, LOCATION, or OTHERS. We adopt
a corpus-based approach to SC determination, re-
casting the problem as a six-class classification task.
Derive two knowledge sources for coreference
resolution from the induced SCs. The first
knowledge source (KS) is semantic class agreement
(SCA). Following Soon et al (2001), we represent
SCA as a binary value that indicates whether the in-
duced SCs of the two NPs involved are the same or
not. The second KS is mention, which is represented
as a binary value that indicates whether an NP be-
longs to one of the five ACE SCs mentioned above.
Hence, the mention value of an NP can be readily
derived from its induced SC: the value is NO if its
SC is OTHERS, and YES otherwise. This KS could
be useful for ACE coreference, since ACE is con-
cerned with resolving only NPs that are mentions.
Incorporate the two knowledge sources in a
coreference resolver. Next, we investigate whether
these two KSs can improve a learning-based base-
line resolver that employs a fairly standard feature
set. Since (1) the two KSs can each be repre-
sented in the resolver as a constraint (for filtering
non-mentions or disallowing coreference between
semantically incompatible NPs) or as a feature, and
(2) they can be applied to the resolver in isolation or
in combination, we have eight ways of incorporating
these KSs into the baseline resolver.
In our experiments on the ACE Phase 2 coref-
erence corpus, we found that (1) our SC induc-
tion method yields a significant improvement of 2%
in accuracy over Soon et al?s first-sense heuristic
method as described above; (2) the coreference re-
solver that incorporates our induced SC knowledge
by means of the two KSs mentioned above yields
a significant improvement of 2% in F-measure over
the resolver that exploits the SC knowledge com-
puted by Soon et al?s method; (3) the mention KS,
when used in the baseline resolver as a constraint,
improves the resolver by approximately 5-7% in F-
measure; and (4) SCA, when employed as a feature
by the baseline resolver, improves the accuracy of
common noun resolution by about 5-8%.
2 Related Work
Mention detection. Many ACE participants have
also adopted a corpus-based approach to SC deter-
mination that is investigated as part of the mention
detection (MD) task (e.g., Florian et al (2006)).
Briefly, the goal of MD is to identify the boundary
of a mention, its mention type (e.g., pronoun, name),
and its semantic type (e.g., person, location). Un-
like them, (1) we do not perform the full MD task,
as our goal is to investigate the role of SC knowl-
edge in coreference resolution; and (2) we do not
use the ACE training data for acquiring our SC clas-
sifier; instead, we use the BBN Entity Type Corpus
(Weischedel and Brunstein, 2005), which consists of
all the Penn Treebank Wall Street Journal articles
with the ACE mentions manually identified and an-
notated with their SCs. This provides us with a train-
ing set that is approximately five times bigger than
that of ACE. More importantly, the ACE participants
do not evaluate the role of induced SC knowledge
in coreference resolution: many of them evaluate
coreference performance on perfect mentions (e.g.,
Luo et al (2004)); and for those that do report per-
formance on automatically extracted mentions, they
do not explain whether or how the induced SC infor-
mation is used in their coreference algorithms.
Joint probabilistic models of coreference. Re-
cently, there has been a surge of interest in im-
proving coreference resolution by jointly modeling
coreference with a related task such as MD (e.g.,
Daume? and Marcu (2005)). However, joint models
typically need to be trained on data that is simulta-
neously annotated with information required by all
of the underlying models. For instance, Daume? and
Marcu?s model assumes as input a corpus annotated
with both MD and coreference information. On the
other hand, we tackle coreference and SC induction
separately (rather than jointly), since we train our SC
determination model on the BBN Entity Type Cor-
pus, where coreference information is absent.
3 Semantic Class Induction
This section describes how we train and evaluate a
classifier for determining the SC of an NP.
537
3.1 Training the Classifier
Training corpus. As mentioned before, we use
the BBN Entity Type Corpus for training the SC
classifier. This corpus was originally developed to
support the ACE and AQUAINT programs and con-
sists of annotations of 12 named entity types and
nine nominal entity types. Nevertheless, we will
only make use of the annotations of the five ACE
semantic types that are present in our ACE Phase 2
coreference corpus, namely, PERSON, ORGANIZA-
TION, GPE, FACILITY, and LOCATION.
Training instance creation. We create one train-
ing instance for each proper or common NP (ex-
tracted using an NP chunker and an NE recognizer)
in each training text. Each instance is represented
by a set of lexical, syntactic, and semantic features,
as described below. If the NP under consideration is
annotated as one of the five ACE SCs in the corpus,
then the classification of the associated training in-
stance is simply the ACE SC value of the NP. Other-
wise, the instance is labeled as OTHERS. This results
in 310063 instances in the training set.
Features. We represent the training instance for a
noun phrase, NPi, using seven types of features:
(1) WORD: For each word w in NPi, we create a
WORD feature whose value is equal to w. No fea-
tures are created from stopwords, however.
(2) SUBJ VERB: If NPi is involved in a subject-verb relation, we create a SUBJ VERB feature whose
value is the verb participating in the relation. We
use Lin?s (1998b) MINIPAR dependency parser to
extract grammatical relations. Our motivation here
is to coarsely model subcategorization.
(3) VERB OBJ: A VERB OBJ feature is created in
a similar fashion as SUBJ VERB if NPi participatesin a verb-object relation. Again, this represents our
attempt to coarsely model subcategorization.
(4) NE: We use BBN?s IdentiFinder (Bikel et al,
1999), a MUC-style NE recognizer to determine the
NE type of NPi. If NPi is determined to be a PERSONor ORGANIZATION, we create an NE feature whose
value is simply its MUC NE type. However, if NPiis determined to be a LOCATION, we create a feature
with value GPE (because most of the MUC LOCA-
TION NEs are ACE GPE NEs). Otherwise, no NE
feature will be created (because we are not interested
in the other MUC NE types).
ACE SC Keywords
PERSON person
ORGANIZATION social group
FACILITY establishment, construction, building, facil-
ity, workplace
GPE country, province, government, town, city,
administration, society, island, community
LOCATION dry land, region, landmass, body of water,
geographical area, geological formation
Table 1: List of keywords used in WordNet search
for generating WN CLASS features.
(5) WN CLASS: For each keyword w shown in the
right column of Table 1, we determine whether the
head noun of NPi is a hyponym of w in WordNet,using only the first WordNet sense of NPi.1 If so,we create a WN CLASS feature with w as its value.
These keywords are potentially useful features be-
cause some of them are subclasses of the ACE SCs
shown in the left column of Table 1, while others
appear to be correlated with these ACE SCs.2
(6) INDUCED CLASS: Since the first-sense heuris-
tic used in the previous feature may not be accurate
in capturing the SC of an NP, we employ a corpus-
based method for inducing SCs that is motivated by
research in lexical semantics (e.g., Hearst (1992)).
Given a large, unannotated corpus3 , we use Identi-
Finder to label each NE with its NE type and MINI-
PAR to extract all the appositive relations. An ex-
ample extraction would be <Eastern Airlines, the
carrier>, where the first entry is a proper noun la-
beled with either one of the seven MUC-style NE
types4 or OTHERS5 and the second entry is a com-
mon noun. We then infer the SC of a common
noun as follows: (1) we compute the probability
that the common noun co-occurs with each of the
eight NE types6 based on the extracted appositive
relations, and (2) if the most likely NE type has a
co-occurrence probability above a certain threshold
(we set it to 0.7), we create a INDUCED CLASS fea-
1This is motivated by Lin?s (1998c) observation that a coref-
erence resolver that employs only the first WordNet sense per-
forms slightly better than one that employs more than one sense.
2The keywords are obtained via our experimentation with
WordNet and the ACE SCs of the NPs in the ACE training data.
3We used (1) the BLLIP corpus (30M words), which con-
sists of WSJ articles from 1987 to 1989, and (2) the Reuters
Corpus (3.7GB data), which has 806,791 Reuters articles.
4Person, organization, location, date, time, money, percent.
5This indicates the proper noun is not a MUC NE.
6For simplicity, OTHERS is viewed as an NE type here.
538
ture for NPi whose value is the most likely NE type.
(7) NEIGHBOR: Research in lexical semantics sug-
gests that the SC of an NP can be inferred from its
distributionally similar NPs (see Lin (1998a)). Mo-
tivated by this observation, we create for each of
NPi?s ten most semantically similar NPs a NEIGH-
BOR feature whose value is the surface string of
the NP. To determine the ten nearest neighbors, we
use the semantic similarity values provided by Lin?s
dependency-based thesaurus, which is constructed
using a distributional approach combined with an
information-theoretic definition of similarity.
Learning algorithms. We experiment with four
learners commonly employed in language learning:
Decision List (DL): We use the DL learner as de-
scribed in Collins and Singer (1999), motivated by
its success in the related tasks of word sense dis-
ambiguation (Yarowsky, 1995) and NE classifica-
tion (Collins and Singer, 1999). We apply add-one
smoothing to smooth the class posteriors.
1-Nearest Neighbor (1-NN): We use the 1-NN clas-
sifier as implemented in TiMBL (Daelemans et al,
2004), employing dot product as the similarity func-
tion (which defines similarity as the number of com-
mon feature-value pairs between two instances). All
other parameters are set to their default values.
Maximum Entropy (ME): We employ Lin?s ME
implementation7 , using a Gaussian prior for smooth-
ing and running the algorithm until convergence.
Naive Bayes (NB): We use an in-house implementa-
tion of NB, using add-one smoothing to smooth the
class priors and the class-conditional probabilities.
In addition, we train an SVM classifier for SC
determination by combining the output of five clas-
sification methods: DL, 1-NN, ME, NB, and Soon
et al?s method as described in the introduction,8
with the goal of examining whether SC classifica-
tion accuracy can be improved by combining the
output of individual classifiers in a supervised man-
ner. Specifically, we (1) use 80% of the instances
generated from the BBN Entity Type Corpus to train
the four classifiers; (2) apply the four classifiers and
Soon et al?s method to independently make predic-
7See http://www.cs.ualberta.ca/?lindek/downloads.htm
8In our implementation of Soon?s method, we label an in-
stance as OTHERS if no NE or WN CLASS feature is generated;
otherwise its label is the value of the NE feature or the ACE SC
that has the WN CLASS features as its keywords (see Table 1).
PER ORG GPE FAC LOC OTH
Training 19.8 9.6 11.4 1.6 1.2 56.3
Test 19.5 9.0 9.6 1.8 1.1 59.0
Table 2: Distribution of SCs in the ACE corpus.
tions for the remaining 20% of the instances; and (3)
train an SVM classifier (using the LIBSVM pack-
age (Chang and Lin, 2001)) on these 20% of the in-
stances, where each instance, i, is represented by a
set of 31 binary features. More specifically, let Li =
{li1, li2, li3, li4, li5} be the set of predictions that weobtained for i in step (2). To represent i, we generate
one feature from each non-empty subset of Li.
3.2 Evaluating the Classifiers
For evaluation, we use the ACE Phase 2 coreference
corpus, which comprises 422 training texts and 97
test texts. Each text has its mentions annotated with
their ACE SCs. We create our test instances from
the ACE texts in the same way as the training in-
stances described in Section 3.1. Table 2 shows the
percentages of instances corresponding to each SC.
Table 3 shows the accuracy of each classifier (see
row 1) for the ACE training set (54641 NPs, with
16414 proper NPs and 38227 common NPs) and the
ACE test set (13444 NPs, with 3713 proper NPs and
9731 common NPs), as well as their performance on
the proper NPs (row 2) and the common NPs (row
3). We employ as our baseline system the Soon et al
method (see Footnote 8), whose accuracy is shown
under the Soon column. As we can see, DL, 1-NN,
and SVM show a statistically significant improve-
ment over the baseline for both data sets, whereas
ME and NB perform significantly worse.9 Addi-
tional experiments are needed to determine the rea-
son for ME and NB?s poor performance.
In an attempt to gain additional insight into the
performance contribution of each type of features,
we conduct feature ablation experiments using the
DL classifier (DL is chosen simply because it is the
best performer on the ACE training set). Results are
shown in Table 4, where each row shows the accu-
racy of the DL trained on all types of features except
for the one shown in that row (All), as well as accu-
racies on the proper NPs (PN) and the common NPs
(CN). For easy reference, the accuracy of the DL
9We use Noreen?s (1989) Approximate Randomization test
for significance testing, with p set to .05 unless otherwise stated.
539
Training Set Test Set
Soon DL 1-NN ME NB SVM Soon DL 1-NN ME NB SVM
1 Overall 83.1 85.0 84.0 54.5 71.3 84.2 81.1 82.9 83.1 53.0 70.3 83.3
2 Proper NPs 83.1 84.1 81.0 54.2 65.5 82.2 79.6 82.0 79.8 55.8 64.4 80.4
3 Common NPs 83.1 85.4 85.2 54.6 73.8 85.1 81.6 83.3 84.3 51.9 72.6 84.4
Table 3: SC classification accuracies of different methods for the ACE training set and test set.
Training Set Test Set
Feature Type PN CN All PN CN All
All features 84.1 85.4 85.0 82.0 83.3 82.9
- WORD 84.2 85.4 85.0 82.0 83.1 82.8
- SUBJ VERB 84.1 85.4 85.0 82.0 83.3 82.9
- VERB OBJ 84.1 85.4 85.0 82.0 83.3 82.9
- NE 72.9 85.3 81.6 74.1 83.2 80.7
- WN CLASS 84.1 85.9 85.3 81.9 84.1 83.5
- INDUCED C 84.0 85.6 85.1 82.0 83.6 83.2
- NEIGHBOR 82.8 84.9 84.3 80.2 82.9 82.1
Table 4: Results for feature ablation experiments.
Training Set Test Set
Feature Type PN CN All PN CN All
WORD 64.0 83.9 77.9 66.5 82.4 78.0
SUBJ VERB 24.0 70.2 56.3 28.8 70.5 59.0
VERB OBJ 24.0 70.2 56.3 28.8 70.5 59.0
NE 81.1 72.1 74.8 78.4 71.4 73.3
WN CLASS 25.6 78.8 62.8 30.4 78.9 65.5
INDUCED C 25.8 81.1 64.5 30.0 80.3 66.3
NEIGHBOR 67.7 85.8 80.4 68.0 84.4 79.8
Table 5: Accuracies of single-feature classifiers.
classifier trained on all types of features is shown
in row 1 of the table. As we can see, accuracy drops
significantly with the removal of NE and NEIGHBOR.
As expected, removing NE precipitates a large drop
in proper NP accuracy; somewhat surprisingly, re-
moving NEIGHBOR also causes proper NP accuracy
to drop significantly. To our knowledge, there are no
prior results on using distributionally similar neigh-
bors as features for supervised SC induction.
Note, however, that these results do not imply
that the remaining feature types are not useful for
SC classification; they simply suggest, for instance,
that WORD is not important in the presence of other
feature types. To get a better idea of the utility of
each feature type, we conduct another experiment in
which we train seven classifiers, each of which em-
ploys exactly one type of features. The accuracies
of these classifiers are shown in Table 5. As we can
see, NEIGHBOR has the largest contribution. This
again demonstrates the effectiveness of a distribu-
tional approach to semantic similarity. Its superior
performance to WORD, the second largest contribu-
tor, could be attributed to its ability to combat data
sparseness. The NE feature, as expected, is crucial
to the classification of proper NPs.
4 Application to Coreference Resolution
We can now derive from the induced SC informa-
tion two KSs ? semantic class agreement and men-
tion ? and incorporate them into our learning-based
coreference resolver in eight different ways, as de-
scribed in the introduction. This section examines
whether our coreference resolver can benefit from
any of the eight ways of incorporating these KSs.
4.1 Experimental Setup
As in SC induction, we use the ACE Phase 2 coref-
erence corpus for evaluation purposes, acquiring the
coreference classifiers on the 422 training texts and
evaluating their output on the 97 test texts. We re-
port performance in terms of two metrics: (1) the F-
measure score as computed by the commonly-used
MUC scorer (Vilain et al, 1995), and (2) the accu-
racy on the anaphoric references, computed as the
fraction of anaphoric references correctly resolved.
Following Ponzetto and Strube (2006), we consider
an anaphoric reference, NPi, correctly resolved if NPiand its closest antecedent are in the same corefer-
ence chain in the resulting partition. In all of our
experiments, we use NPs automatically extracted by
an in-house NP chunker and IdentiFinder.
4.2 The Baseline Coreference System
Our baseline coreference system uses the C4.5 deci-
sion tree learner (Quinlan, 1993) to acquire a classi-
fier on the training texts for determining whether two
NPs are coreferent. Following previous work (e.g.,
Soon et al (2001) and Ponzetto and Strube (2006)),
we generate training instances as follows: a positive
instance is created for each anaphoric NP, NPj , andits closest antecedent, NPi; and a negative instance iscreated for NPj paired with each of the interveningNPs, NPi+1, NPi+2, . . ., NPj?1. Each instance is rep-resented by 33 lexical, grammatical, semantic, and
540
positional features that have been employed by high-
performing resolvers such as Ng and Cardie (2002)
and Yang et al (2003), as described below.
Lexical features. Nine features allow different
types of string matching operations to be performed
on the given pair of NPs, NPx and NPy10, including(1) exact string match for pronouns, proper nouns,
and non-pronominal NPs (both before and after de-
terminers are removed); (2) substring match for
proper nouns and non-pronominal NPs; and (3) head
noun match. In addition, one feature tests whether
all the words that appear in one NP also appear in
the other NP. Finally, a nationality matching feature
is used to match, for instance, British with Britain.
Grammatical features. 22 features test the gram-
matical properties of one or both of the NPs. These
include ten features that test whether each of the two
NPs is a pronoun, a definite NP, an indefinite NP, a
nested NP, and a clausal subject. A similar set of
five features is used to test whether both NPs are
pronouns, definite NPs, nested NPs, proper nouns,
and clausal subjects. In addition, five features deter-
mine whether the two NPs are compatible with re-
spect to gender, number, animacy, and grammatical
role. Furthermore, two features test whether the two
NPs are in apposition or participate in a predicate
nominal construction (i.e., the IS-A relation).
Semantic features. Motivated by Soon et al
(2001), we have a semantic feature that tests whether
one NP is a name alias or acronym of the other.
Positional feature. We have a feature that com-
putes the distance between the two NPs in sentences.
After training, the decision tree classifier is used
to select an antecedent for each NP in a test text.
Following Soon et al (2001), we select as the an-
tecedent of each NP, NPj , the closest preceding NPthat is classified as coreferent with NPj . If no suchNP exists, no antecedent is selected for NPj .
Row 1 of Table 6 and Table 7 shows the results
of the baseline system in terms of F-measure (F)
and accuracy in resolving 4599 anaphoric references
(All), respectively. For further analysis, we also re-
port the corresponding recall (R) and precision (P)
in Table 6, as well as the accuracies of the system in
resolving 1769 pronouns (PRO), 1675 proper NPs
(PN), and 1155 common NPs (CN) in Table 7. As
10We assume that NPx precedes NPy in the associated text.
we can see, the baseline achieves an F-measure of
57.0 and a resolution accuracy of 48.4.
To get a better sense of how strong our baseline
is, we re-implement the Soon et al (2001) corefer-
ence resolver. This simply amounts to replacing the
33 features in the baseline resolver with the 12 fea-
tures employed by Soon et al?s system. Results of
our Duplicated Soon et al system are shown in row
2 of Tables 6 and 7. In comparison to our baseline,
the Duplicated Soon et al system performs worse
according to both metrics, and although the drop in
F-measure seems moderate, the performance differ-
ence is in fact highly significant (p=0.002).11
4.3 Coreference with Induced SC Knowledge
Recall from the introduction that our investigation of
the role of induced SC knowledge in learning-based
coreference resolution proceeds in three steps:
Label the SC of each NP in each ACE document.
If a noun phrase, NPi, is a proper or common NP,then its SC value is determined using an SC classi-
fier that we acquired in Section 3. On the other hand,
if NPi is a pronoun, then we will be conservative andposit its SC value as UNCONSTRAINED (i.e., it is se-
mantically compatible with all other NPs).12
Derive two KSs from the induced SCs. Recall that
our first KS, Mention, is defined on an NP; its value
is YES if the induced SC of the NP is not OTHERS,
and NO otherwise. On the other hand, our second
KS, SCA, is defined on a pair of NPs; its value is
YES if the two NPs have the same induced SC that
is not OTHERS, and NO otherwise.
Incorporate the two KSs into the baseline re-
solver. Recall that there are eight ways of incor-
porating these two KSs into our resolver: they can
each be represented as a constraint or as a feature,
and they can be applied to the resolver in isolation
and in combination. Constraints are applied dur-
ing the antecedent selection step. Specifically, when
employed as a constraint, the Mention KS disallows
coreference between two NPs if at least one of them
has a Mention value of NO, whereas the SCA KS dis-
allows coreference if the SCA value of the two NPs
involved is NO. When encoded as a feature for the
resolver, the Mention feature for an NP pair has the
11Again, we use Approximate Randomization with p=.05.
12The only exception is pronouns whose SC value can be eas-
ily determined to be PERSON (e.g., he, him, his, himself).
541
System Variation R P F R P F R P F R P F
1 Baseline system 60.9 53.6 57.0 ? ? ? ? ? ? ? ? ?
2 Duplicated Soon et al 56.1 54.4 55.3 ? ? ? ? ? ? ? ? ?
Add to the Baseline Soon?s SC Method Decision List SVM Perfect Information
3 Mention(C) only 56.9 69.7 62.6 59.5 70.6 64.6 59.5 70.7 64.6 61.2 83.1 70.5
4 Mention(F) only 60.9 54.0 57.2 61.2 52.9 56.7 60.9 53.6 57.0 62.3 33.7 43.8
5 SCA(C) only 56.4 70.0 62.5 57.7 71.2 63.7 58.9 70.7 64.3 61.3 86.1 71.6
6 SCA(F) only 62.0 52.8 57.0 62.5 53.5 57.6 63.0 53.3 57.7 71.1 33.0 45.1
7 Mention(C) + SCA(C) 56.4 70.0 62.5 57.7 71.2 63.7 58.9 70.8 64.3 61.3 86.1 71.6
8 Mention(C) + SCA(F) 58.2 66.4 62.0 60.9 66.8 63.7 61.4 66.5 63.8 71.1 76.7 73.8
9 Mention(F) + SCA(C) 56.4 69.8 62.4 57.7 71.3 63.8 58.9 70.6 64.3 62.7 85.3 72.3
10 Mention(F) + SCA(F) 62.0 52.7 57.0 62.6 52.8 57.3 63.2 52.6 57.4 71.8 30.3 42.6
Table 6: Coreference results obtained via the MUC scoring program for the ACE test set.
System Variation PRO PN CN All PRO PN CN All PRO PN CN All
1 Baseline system 59.2 54.8 22.5 48.4 ? ? ? ? ? ? ? ?
2 Duplicated Soon et al 53.4 45.7 16.9 41.4 ? ? ? ? ? ? ? ?
Add to the Baseline Soon?s SC Method Decision List SVM
3 Mention(C) only 58.5 51.3 16.5 45.3 59.1 54.1 20.2 47.5 59.1 53.9 20.6 47.5
4 Mention(F) only 59.2 55.0 22.5 48.5 59.2 56.1 22.4 48.8 59.4 55.2 22.6 48.6
5 SCA(C) only 58.1 50.1 16.4 44.7 58.1 51.8 17.1 45.5 58.5 52.0 19.6 46.3
6 SCA(F) only 59.2 54.9 27.8 49.7 60.4 56.7 30.1 51.5 60.8 56.4 29.4 51.3
7 Mention(C) + SCA(C) 58.1 50.1 16.4 44.7 58.1 51.8 17.1 45.5 58.5 51.9 19.5 46.3
8 Mention(C) + SCA(F) 58.9 52.0 22.3 47.2 60.2 55.9 28.1 50.6 60.7 55.3 27.4 50.4
9 Mention(F) + SCA(C) 58.1 50.3 16.3 44.8 58.1 52.4 16.7 45.6 58.6 52.4 19.7 46.6
10 Mention(F) + SCA(F) 59.2 55.0 27.6 49.7 60.4 56.8 30.1 51.5 60.8 56.5 29.5 51.4
Table 7: Resolution accuracies for the ACE test set.
value YES if and only if the Mention value for both
NPs is YES, whereas the SCA feature for an NP pair
has its value taken from the SCA KS.
Now, we can evaluate the impact of the two KSs
on the performance of our baseline resolver. Specifi-
cally, rows 3-6 of Tables 6 and 7 show the F-measure
and the resolution accuracy, respectively, when ex-
actly one of the two KSs is employed by the baseline
as either a constraint (C) or a feature (F), and rows
7-10 of the two tables show the results when both
KSs are applied to the baseline. Furthermore, each
row of Table 6 contains four sets of results, each of
which corresponds to a different method for deter-
mining the SC value of an NP. For instance, the first
set is obtained by using Soon et al?s method as de-
scribed in Footnote 8 to compute SC values, serving
as sort of a baseline for our results using induced SC
values. The second and third sets are obtained based
on the SC values computed by the DL and the SVM
classifier, respectively.13 The last set corresponds to
an oracle experiment in which the resolver has ac-
cess to perfect SC information. Rows 3-10 of Table
13Results using other learners are not shown due to space lim-
itations. DL and SVM are chosen simply because they achieve
the highest SC classification accuracies on the ACE training set.
7 can be interpreted in a similar manner.
From Table 6, we can see that (1) in comparison to
the baseline, F-measure increases significantly in the
five cases where at least one of the KSs is employed
as a constraint by the resolver, and such improve-
ments stem mainly from significant gains in preci-
sion; (2) in these five cases, the resolvers that use
SCs induced by DL and SVM achieve significantly
higher F-measure scores than their counterparts that
rely on Soon?s method for SC determination; and (3)
none of the resolvers appears to benefit from SCA in-
formation whenever mention is used as a constraint.
Moreover, note that even with perfectly computed
SC information, the performance of the baseline sys-
tem does not improve when neither MD nor SCA is
employed as a constraint. These results provide fur-
ther evidence that the decision tree learner is not ex-
ploiting these two semantic KSs in an optimal man-
ner, whether they are computed automatically or per-
fectly. Hence, in machine learning for coreference
resolution, it is important to determine not only what
linguistic KSs to use, but also how to use them.
While the coreference results in Table 6 seem to
suggest that SCA and mention should be employed
as constraints, the resolution results in Table 7 sug-
542
gest that SCA is better encoded as a feature. Specifi-
cally, (1) in comparison to the baseline, the accuracy
of common NP resolution improves by about 5-8%
when SCA is encoded as a feature; and (2) whenever
SCA is employed as a feature, the overall resolution
accuracy is significantly higher for resolvers that use
SCs induced by DL and SVM than those that rely on
Soon?s method for SC determination, with improve-
ments in resolution observed on all three NP types.
Overall, these results provide suggestive evidence
that both KSs are useful for learning-based corefer-
ence resolution. In particular, mention should be em-
ployed as a constraint, whereas SCA should be used
as a feature. Interestingly, this is consistent with the
results that we obtained when the resolver has access
to perfect SC information (see Table 6), where the
highest F-measure is achieved by employing men-
tion as a constraint and SCA as a feature.
5 Conclusions
We have shown that (1) both mention and SCA can
be usefully employed to improve the performance
of a learning-based coreference system, and (2) em-
ploying SC knowledge induced in a supervised man-
ner enables a resolver to achieve better performance
than employing SC knowledge computed by Soon
et al?s simple method. In addition, we found that
the MUC scoring program is unable to reveal the
usefulness of the SCA KS, which, when encoded
as a feature, substantially improves the accuracy of
common NP resolution. This underscores the im-
portance of reporting both resolution accuracy and
clustering-level accuracy when analyzing the perfor-
mance of a coreference resolver.
References
D. Bean and E. Riloff. 2004. Unsupervised learning of contex-
tual role knowledge for coreference resolution. In Proc. of
HLT/NAACL, pages 297?304.
D. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999. An
algorithm that learns what?s in a name. Machine Learning
34(1?3):211?231.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proc. of EMNLP/VLC.
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2004. TiMBL: Tilburg Memory Based Learner, ver-
sion 5.1, Reference Guide. ILK Technical Report.
H. Daume? III and D. Marcu. 2005. A large-scale exploration
of effective global features for a joint entity detection and
tracking model. In Proc. of HLT/EMNLP, pages 97?104.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006. Fac-
torizing complex models: A case study in mention detection.
In Proc. of COLING/ACL, pages 473?480.
M. Hearst. 1992. Automatic acquisition of hyponyms from
large text corpora. In Proc. of COLING.
H. Ji, D. Westbrook, and R. Grishman. 2005. Using seman-
tic relations to refine coreference decisions. In Proc. of
HLT/EMNLP, pages 17?24.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The
(non)utility of predicate-argument frequencies for pronoun
interpretation. In Proc. of NAACL, pages 289?296.
D. Lin. 1998a. Automatic retrieval and clustering of similar
words. In Proc. of COLING/ACL, pages 768?774.
D. Lin. 1998b. Dependency-based evaluation of MINIPAR. In
Proc. of the LREC Workshop on the Evaluation of Parsing
Systems, pages 48?56.
D. Lin. 1998c. Using collocation statistics in information ex-
traction. In Proc. of MUC-7.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.
2004. A mention-synchronous coreference resolution algo-
rithm based on the Bell tree. In Proc. of the ACL.
K. Markert and M. Nissim. 2005. Comparing knowledge
sources for nominal anaphora resolution. Computational
Linguistics, 31(3):367?402.
R. Mitkov. 2002. Anaphora Resolution. Longman.
R. Mitkov. 1998. Robust pronoun resolution with limited
knowledge. In Proc. of COLING/ACL, pages 869?875.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proc. of the ACL.
E. W. Noreen. 1989. Computer Intensive Methods for Testing
Hypothesis: An Introduction. John Wiley & Sons.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.
Learning to resolve bridging references. In Proc. of the ACL.
S. P. Ponzetto and M. Strube. 2006. Exploiting semantic role
labeling, WordNet and Wikipedia for coreference resolution.
In Proc. of HLT/NAACL, pages 192?199.
J. R. Quinlan. 1993. C4.5: Programs for Machine Learning.
Morgan Kaufmann, San Mateo, CA.
W. M. Soon, H. T. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases. Compu-
tational Linguistics, 27(4):521?544.
J. Tetreault. 2001. A corpus-based evaluation of centering and
pronoun resolution. Computational Linguistics, 27(4).
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scor-
ing scheme. In Proc. of MUC-6, pages 45?52.
R. Weischedel and A. Brunstein. 2005. BBN pronoun corefer-
ence and entity type corpus. Linguistica Data Consortium.
X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003. Coreference
resolution using competitive learning approach. In Proc. of
the ACL, pages 176?183.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In Proc. of the ACL.
543
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 701?709,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Mine the Easy, Classify the Hard:
A Semi-Supervised Approach to Automatic Sentiment Classification
Sajib Dasgupta and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{sajib,vince}@hlt.utdallas.edu
Abstract
Supervised polarity classification systems
are typically domain-specific. Building
these systems involves the expensive pro-
cess of annotating a large amount of data
for each domain. A potential solution
to this corpus annotation bottleneck is to
build unsupervised polarity classification
systems. However, unsupervised learning
of polarity is difficult, owing in part to the
prevalence of sentimentally ambiguous re-
views, where reviewers discuss both the
positive and negative aspects of a prod-
uct. To address this problem, we pro-
pose a semi-supervised approach to senti-
ment classification where we first mine the
unambiguous reviews using spectral tech-
niques and then exploit them to classify
the ambiguous reviews via a novel com-
bination of active learning, transductive
learning, and ensemble learning.
1 Introduction
Sentiment analysis has recently received a lot
of attention in the Natural Language Processing
(NLP) community. Polarity classification, whose
goal is to determine whether the sentiment ex-
pressed in a document is ?thumbs up? or ?thumbs
down?, is arguably one of the most popular tasks
in document-level sentiment analysis. Unlike
topic-based text classification, where a high accu-
racy can be achieved even for datasets with a large
number of classes (e.g., 20 Newsgroups), polarity
classification appears to be a more difficult task.
One reason topic-based text classification is easier
than polarity classification is that topic clusters are
typically well-separated from each other, result-
ing from the fact that word usage differs consid-
erably between two topically-different documents.
On the other hand, many reviews are sentimentally
ambiguous for a variety of reasons. For instance,
an author of a movie review may have negative
opinions of the actors but at the same time talk
enthusiastically about how much she enjoyed the
plot. Here, the review is ambiguous because she
discussed both the positive and negative aspects of
the movie, which is not uncommon in reviews. As
another example, a large portion of a movie re-
view may be devoted exclusively to the plot, with
the author only briefly expressing her sentiment at
the end of the review. In this case, the review is
ambiguous because the objective material in the
review, which bears no sentiment orientation, sig-
nificantly outnumbers its subjective counterpart.
Realizing the challenges posed by ambiguous
reviews, researchers have explored a number of
techniques to improve supervised polarity classi-
fiers. For instance, Pang and Lee (2004) train an
independent subjectivity classifier to identify and
remove objective sentences from a review prior to
polarity classification. Koppel and Schler (2006)
use neutral reviews to help improve the classi-
fication of positive and negative reviews. More
recently, McDonald et al (2007) have investi-
gated a model for jointly performing sentence- and
document-level sentiment analysis, allowing the
relationship between the two tasks to be captured
and exploited. However, the increased sophistica-
tion of supervised polarity classifiers has also re-
sulted in their increased dependence on annotated
data. For instance, Koppel and Schler needed to
manually identify neutral reviews to train their po-
larity classifier, and McDonald et al?s joint model
requires that each sentence in a review be labeled
with polarity information.
Given the difficulties of supervised polarity
classification, it is conceivable that unsupervised
polarity classification is a very challenging task.
Nevertheless, a solution to unsupervised polarity
classification is of practical significance. One rea-
son is that the vast majority of supervised polarity
701
classification systems are domain-specific. Hence,
when given a new domain, a large amount of an-
notated data from the domain typically needs to be
collected in order to train a high-performance po-
larity classification system. As Blitzer et al (2007)
point out, this data collection process can be ?pro-
hibitively expensive, especially since product fea-
tures can change over time?. Unfortunately, to
our knowledge, unsupervised polarity classifica-
tion is largely an under-investigated task in NLP.
Turney?s (2002) work is perhaps one of the most
notable examples of unsupervised polarity clas-
sification. However, while his system learns the
semantic orientation of phrases in a review in an
unsupervised manner, such information is used to
heuristically predict the polarity of a review.
At first glance, it may seem plausible to apply
an unsupervised clustering algorithm such as k-
means to cluster the reviews according to their po-
larity. However, there is reason to believe that such
a clustering approach is doomed to fail: in the ab-
sence of annotated data, an unsupervised learner
is unable to identify which features are relevant
for polarity classification. The situation is further
complicated by the prevalence of ambiguous re-
views, which may contain a large amount of irrel-
evant and/or contradictory information.
In light of the difficulties posed by ambiguous
reviews, we differentiate between ambiguous and
unambiguous reviews in our classification process
by addressing the task of semi-supervised polar-
ity classification via a ?mine the easy, classify the
hard? approach. Specifically, we propose a novel
system architecture where we first automatically
identify and label the unambiguous (i.e., ?easy?)
reviews, then handle the ambiguous (i.e., ?hard?)
reviews using a discriminative learner to bootstrap
from the automatically labeled unambiguous re-
views and a small number of manually labeled re-
views that are identified by an active learner.
It is worth noting that our system differs from
existing work on unsupervised/active learning in
two aspects. First, while existing unsupervised
approaches typically rely on clustering or learn-
ing via a generative model, our approach distin-
guishes between easy and hard instances and ex-
ploits the strengths of discriminative models to
classify the hard instances. Second, while exist-
ing active learners typically start with manually la-
beled seeds, our active learner relies only on seeds
that are automatically extracted from the data. Ex-
perimental results on five sentiment classification
datasets demonstrate that our system can gener-
ate high-quality labeled data from unambiguous
reviews, which, together with a small number of
manually labeled reviews selected by the active
learner, can be used to effectively classify ambigu-
ous reviews in a discriminative fashion.
The rest of the paper is organized as follows.
Section 2 gives an overview of spectral cluster-
ing, which will facilitate the presentation of our
approach to unsupervised sentiment classification
in Section 3. We evaluate our approach in Section
4 and present our conclusions in Section 5.
2 Spectral Clustering
In this section, we give an overview of spectral
clustering, which is at the core of our algorithm
for identifying ambiguous reviews.
2.1 Motivation
When given a clustering task, an important ques-
tion to ask is: which clustering algorithm should
be used? A popular choice is k-means. Neverthe-
less, it is well-known that k-means has the major
drawback of not being able to separate data points
that are not linearly separable in the given feature
space (e.g, see Dhillon et al (2004)). Spectral
clustering algorithms were developed in response
to this problem with k-means clustering. The cen-
tral idea behind spectral clustering is to (1) con-
struct a low-dimensional space from the original
(typically high-dimensional) space while retaining
as much information about the original space as
possible, and (2) cluster the data points in this low-
dimensional space.
2.2 Algorithm
Although there are several well-known spectral
clustering algorithms in the literature (e.g., Weiss
(1999), Meila? and Shi (2001), Kannan et al
(2004)), we adopt the one proposed by Ng et al
(2002), as it is arguably the most widely used. The
algorithm takes as input a similarity matrix S cre-
ated by applying a user-defined similarity function
to each pair of data points. Below are the main
steps of the algorithm:
1. Create the diagonal matrix G whose (i,i)-
th entry is the sum of the i-th row of S,
and then construct the Laplacian matrix L =
G?1/2SG?1/2.
2. Find the eigenvalues and eigenvectors of L.
702
3. Create a new matrix from the m eigenvectors
that correspond to the m largest eigenvalues.1
4. Each data point is now rank-reduced to a
point in the m-dimensional space. Normal-
ize each point to unit length (while retaining
the sign of each value).
5. Cluster the resulting data points using k-
means.
In essence, each dimension in the reduced space
is defined by exactly one eigenvector. The rea-
son why eigenvectors with large eigenvalues are
retained is that they capture the largest variance in
the data. Therefore, each of them can be thought
of as revealing an important dimension of the data.
3 Our Approach
While spectral clustering addresses a major draw-
back of k-means clustering, it still cannot be ex-
pected to accurately partition the reviews due to
the presence of ambiguous reviews. Motivated by
this observation, rather than attempting to cluster
all the reviews at the same time, we handle them in
different stages. As mentioned in the introduction,
we employ a ?mine the easy, classify the hard?
approach to polarity classification, where we (1)
identify and classify the ?easy? (i.e., unambigu-
ous) reviews with the help of a spectral cluster-
ing algorithm; (2) manually label a small number
of ?hard? (i.e., ambiguous) reviews selected by an
active learner; and (3) using the reviews labeled
thus far, apply a transductive learner to label the
remaining (ambiguous) reviews. In this section,
we discuss each of these steps in detail.
3.1 Identifying Unambiguous Reviews
We begin by preprocessing the reviews to be clas-
sified. Specifically, we tokenize and downcase
each review and represent it as a vector of uni-
grams, using frequency as presence. In addition,
we remove from the vector punctuation, numbers,
words of length one, and words that occur in a
single review only. Finally, following the com-
mon practice in the information retrieval commu-
nity, we remove words with high document fre-
quency, many of which are stopwords or domain-
specific general-purpose words (e.g., ?movies? in
the movie domain). A preliminary examination
of our evaluation datasets reveals that these words
1For brevity, we will refer to the eigenvector with the n-th
largest eigenvalue simply as the n-th eigenvector.
typically comprise 1?2% of a vocabulary. The de-
cision of exactly how many terms to remove from
each dataset is subjective: a large corpus typically
requires more removals than a small corpus. To be
consistent, we simply sort the vocabulary by doc-
ument frequency and remove the top 1.5%.
Recall that in this step we use spectral clustering
to identify unambiguous reviews. To make use of
spectral clustering, we first create a similarity ma-
trix, defining the similarity between two reviews
as the dot product of their feature vectors, but fol-
lowing Ng et al (2002), we set its diagonal entries
to 0. We then perform an eigen-decomposition of
this matrix, as described in Section 2.2. Finally,
using the resulting eigenvectors, we partition the
length-normalized reviews into two sets.
As Ng et al point out, ?different authors still
disagree on which eigenvectors to use, and how to
derive clusters from them?. To create two clusters,
the most common way is to use only the second
eigenvector, as Shi and Malik (2000) proved that
this eigenvector induces an intuitively ideal par-
tition of the data ? the partition induced by the
minimum normalized cut of the similarity graph2,
where the nodes are the data points and the edge
weights are the pairwise similarity values of the
points. Clustering in a one-dimensional space is
trivial: since we have a linearization of the points,
all we need to do is to determine a threshold for
partitioning the points. A common approach is to
set the threshold to zero. In other words, all points
whose value in the second eigenvector is positive
are classified as positive, and the remaining points
are classified as negative. However, we found that
the second eigenvector does not always induce a
partition of the nodes that corresponds to the min-
imum normalized cut. One possible reason is that
Shi and Malik?s proof assumes the use of a Lapla-
cian matrix that is different from the one used by
Ng et al To address this problem, we use the first
five eigenvectors: for each eigenvector, we (1) use
each of its n elements as a threshold to indepen-
dently generate n partitions, (2) compute the nor-
malized cut value for each partition, and (3) find
the minimum of the n cut values. We then select
the eigenvector that corresponds to the smallest of
the five minimum cut values.
Next, we identify the ambiguous reviews from
2Using the normalized cut (as opposed to the usual cut)
ensures that the size of the two clusters are relatively bal-
anced, avoiding trivial cuts where one cluster is empty and
the other is full. See Shi and Malik (2000) for details.
703
the resulting partition. To see how this is done,
consider the example in Figure 1, where the goal
is to produce two clusters from five data points.
( 1 1 1 0 0
1 1 1 0 0
0 0 1 1 0
0 0 0 1 1
0 0 0 1 1
) (?0.6983 0.7158
?0.6983 0.7158
?0.9869 ?0.1616
?0.6224 ?0.7827
?0.6224 ?0.7827
)
Figure 1: Sample data and the top two eigenvec-
tors of its Laplacian
In the matrix on the left, each row is the feature
vector generated for Di, the i-th data point. By in-
spection, one can identify two clusters, {D1,D2}
and {D4,D5}. D3 is ambiguous, as it bears re-
semblance to the points in both clusters and there-
fore can be assigned to any of them. In the ma-
trix on the right, the two columns correspond to
the top two eigenvectors obtained via an eigen-
decomposition of the Laplacian matrix formed
from the five data points. As we can see, the sec-
ond eigenvector gives us a natural cluster assign-
ment: all the points whose corresponding values
in the second eigenvector are strongly positive will
be in one cluster, and the strongly negative points
will be in another cluster. Being ambiguous, D3 is
weakly negative and will be assigned to the ?neg-
ative? cluster. Before describing our algorithm for
identifying ambiguous data points, we make two
additional observations regarding D3.
First, if we removed D3, we could easily clus-
ter the remaining (unambiguous) points, since the
similarity graph becomes more disconnected as
we remove more ambiguous data points. The
question then is: why is it important to produce
a good clustering of the unambiguous points? Re-
call that the goal of this step is not only to iden-
tify the unambiguous reviews, but also to annotate
them as POSITIVE or NEGATIVE, so that they can
serve as seeds for semi-supervised learning in a
later step. If we have a good 2-way clustering of
the seeds, we can simply annotate each cluster (by
sampling a handful of its reviews) rather than each
seed. To reiterate, removing the ambiguous data
points can help produce a good clustering of their
unambiguous counterparts.
Second, as an ambiguous data point, D3 can in
principle be assigned to any of the two clusters.
According to the second eigenvector, it should be
assigned to the ?negative? cluster; but if feature
#4 were irrelevant, it should be assigned to the
?positive? cluster. In other words, the ability to
determine the relevance of each feature is crucial
to the accurate clustering of the ambiguous data
points. However, in the absence of labeled data,
it is not easy to assess feature relevance. Even if
labeled data were present, the ambiguous points
might be better handled by a discriminative learn-
ing system than a clustering algorithm, as discrim-
inative learners are more sophisticated, and can
handle ambiguous feature space more effectively.
Taking into account these two observations, we
aim to (1) remove the ambiguous data points while
clustering their unambiguous counterparts, and
then (2) employ a discriminative learner to label
the ambiguous points in a later step.
The question is: how can we identify the
ambiguous data points? To do this, we ex-
ploit an important observation regarding eigen-
decomposition. In the computation of eigenvalues,
each data point factors out the orthogonal projec-
tions of each of the other data points with which
they have an affinity. Ambiguous data points re-
ceive the orthogonal projections from both the
positive and negative data points, and hence they
have near-zero values in the pivot eigenvectors.
Given this observation, our algorithm uses the
eight steps below to remove the ambiguous points
in an iterative fashion and produce a clustering of
the unambiguous points.
1. Create a similarity matrix S from the data
points D.
2. Form the Laplacian matrix L from S.
3. Find the top five eigenvectors of L.
4. Row-normalize the five eigenvectors.
5. Pick the eigenvector e for which we get the
minimum normalized cut.
6. Sort D according to e and remove ? points in
the middle of D (i.e., the points indexed from
|D|
2 ? ?2 + 1 to
|D|
2 +
?
2 ).
7. If |D| = ?, goto Step 8; else goto Step 1.
8. Run 2-means on e to cluster the points in D.
This algorithm can be thought of as the oppo-
site of self-training. In self-training, we iteratively
train a classifier on the data labeled so far, use it
to classify the unlabeled instances, and augment
the labeled data with the most confidently labeled
instances. In our algorithm, we start with an ini-
tial clustering of all of the data points, and then
iteratively remove the ? most ambiguous points
from the dataset and cluster the remaining points.
Given this analogy, it should not be difficult to see
the advantage of removing the data points in an it-
erative fashion (as opposed to removing them in a
704
single iteration): the clusters produced in a given
iteration are supposed to be better than those in
the previous iterations, as subsequent clusterings
are generated from less ambiguous points. In our
experiments, we set ? to 50 and ? to 500.3
Finally, we label the two clusters. To do this,
we first randomly sample 10 reviews from each
cluster and manually label each of them as POS-
ITIVE or NEGATIVE. Then, we label a cluster as
POSITIVE if more than half of the 10 reviews from
the cluster are POSITIVE; otherwise, it is labeled
as NEGATIVE. For each of our evaluation datasets,
this labeling scheme always produces one POSI-
TIVE cluster and one NEGATIVE cluster. In the rest
of the paper, we will refer to these 500 automati-
cally labeled reviews as seeds.
A natural question is: can this algorithm pro-
duce high-quality seeds? To answer this question,
we show in the middle column of Table 1 the label-
ing accuracy of the 500 reviews produced by our
iterative algorithm for our five evaluation datasets
(see Section 4.1 for details on these datasets). To
better understand whether it is indeed beneficial
to remove the ambiguous points in an iterative
fashion, we also show the results of a version of
this algorithm in which we remove all but the 500
least ambiguous points in just one iteration (see
the rightmost column). As we can see, for three
datasets (Movie, Kitchen, and Electronics), the
accuracy is above 80%. For the remaining two
(Book and DVD), the accuracy is not particularly
good. One plausible reason is that the ambiguous
reviews in Book and DVD are relatively tougher
to identify. Another reason can be attributed to
the failure of the chosen eigenvector to capture the
sentiment dimension. Recall that each eigenvector
captures an important dimension of the data, and
if the eigenvector that corresponds to the minimum
normalized cut (i.e., the eigenvector that we chose)
does not reveal the sentiment dimension, the re-
sulting clustering (and hence the seed accuracy)
will be poor. However, even with imperfectly la-
beled seeds, we will show in the next section how
we exploit these seeds to learn a better classifier.
3.2 Incorporating Active Learning
Spectral clustering allows us to focus on a small
number of dimensions that are relevant as far as
creating well-separated clusters is concerned, but
3Additional experiments indicate that the accuracy of our
approach is not sensitive to small changes to these values.
Dataset Iterative Single Step
Movie 89.3 86.5
Kitchen 87.9 87.1
Electronics 80.4 77.6
Book 68.5 70.3
DVD 66.3 65.4
Table 1: Seed accuracies on five datasets.
they are not necessarily relevant for creating po-
larity clusters. In fact, owing to the absence of la-
beled data, unsupervised clustering algorithms are
unable to distinguish between useful and irrelevant
features for polarity classification. Nevertheless,
being able to distinguish between relevant and ir-
relevant information is important for polarity clas-
sification, as discussed before. Now that we have
a small, high-quality seed set, we can potentially
make better use of the available features by train-
ing a discriminative classifier on the seed set and
having it identify the relevant and irrelevant fea-
tures for polarity classification.
Despite the high quality of the seed set, the re-
sulting classifier may not perform well when ap-
plied to the remaining (unlabeled) points, as there
is no reason to believe that a classifier trained
solely on unambiguous reviews can achieve a
high accuracy when classifying ambiguous re-
views. We hypothesize that a high accuracy can
be achieved only if the classifier is trained on both
ambiguous and unambiguous reviews.
As a result, we apply active learning (Cohn
et al, 1994) to identify the ambiguous reviews.
Specifically, we train a discriminative classifier us-
ing the support vector machine (SVM) learning al-
gorithm (Joachims, 1999) on the set of unambigu-
ous reviews, and then apply the resulting classifier
to all the reviews in the training folds4 that are not
seeds. Since this classifier is trained solely on the
unambiguous reviews, it is reasonable to assume
that the reviews whose labels the classifier is most
uncertain about (and therefore are most informa-
tive to the classifier) are those that are ambigu-
ous. Following previous work on active learning
for SVMs (e.g., Campbell et al (2000), Schohn
and Cohn (2000), Tong and Koller (2002)), we de-
fine the uncertainty of a data point as its distance
from the separating hyperplane. In other words,
4Following Dredze and Crammer (2008), we perform
cross-validation experiments on the 2000 labeled reviews in
each evaluation dataset, choosing the active learning points
from the training folds. Note that the seeds obtained in the
previous step were also acquired using the training folds only.
705
points that are closer to the hyperplane are more
uncertain than those that are farther away.
We perform active learning for five iterations.
In each iteration, we select the 10 most uncertain
points from each side of the hyperplane for human
annotation, and then re-train a classifier on all of
the points annotated so far. This yields a total of
100 manually labeled reviews.
3.3 Applying Transductive Learning
Given that we now have a labeled set (composed
of 100 manually labeled points selected by active
learning and 500 unambiguous points) as well as
a larger set of points that are yet to be labeled
(i.e., the remaining unlabeled points in the train-
ing folds and those in the test fold), we aim to
train a better classifier by using a weakly super-
vised learner to learn from both the labeled and
unlabeled data. As our weakly supervised learner,
we employ a transductive SVM.
To begin with, note that the automatically ac-
quired 500 unambiguous data points are not per-
fectly labeled (see Section 3.1). Since these unam-
biguous points significantly outnumber the manu-
ally labeled points, they could undesirably domi-
nate the acquisition of the hyperplane and dimin-
ish the benefits that we could have obtained from
the more informative and perfectly labeled active
learning points otherwise. We desire a system that
can use the active learning points effectively and at
the same time is noise-tolerant to the imperfectly
labeled unambiguous data points. Hence, instead
of training just one SVM classifier, we aim to re-
duce classification errors by training an ensemble
of five classifiers, each of which uses all 100 man-
ually labeled reviews and a different subset of the
500 automatically labeled reviews.
Specifically, we partition the 500 automatically
labeled reviews into five equal-sized sets as fol-
lows. First, we sort the 500 reviews in ascending
order of their corresponding values in the eigen-
vector selected in the last iteration of our algorithm
for removing ambiguous points (see Section 3.1).
We then put point i into set Li mod 5. This ensures
that each set consists of not only an equal number
of positive and negative points, but also a mix of
very confidently labeled points and comparatively
less confidently labeled points. Each classifier Ci
will then be trained transductively, using the 100
manually labeled points and the points in Li as la-
beled data, and the remaining points (including all
points in Lj , where i 6= j) as unlabeled data.
After training the ensemble, we classify each
unlabeled point as follows: we sum the (signed)
confidence values assigned to it by the five ensem-
ble classifiers, labeling it as POSITIVE if the sum
is greater than zero (and NEGATIVE otherwise).
Since the points in the test fold are included in the
unlabeled data, they are all classified in this step.
4 Evaluation
4.1 Experimental Setup
For evaluation, we use five sentiment classifica-
tion datasets, including the widely-used movie re-
view dataset [MOV] (Pang et al, 2002) as well as
four datasets that contain reviews of four differ-
ent types of product from Amazon [books (BOO),
DVDs (DVD), electronics (ELE), and kitchen ap-
pliances (KIT)] (Blitzer et al, 2007). Each dataset
has 2000 labeled reviews (1000 positives and 1000
negatives). We divide the 2000 reviews into 10
equal-sized folds for cross-validation purposes,
maintaining balanced class distributions in each
fold. It is important to note that while the test fold
is accessible to the transductive learner (Step 3),
only the reviews in training folds (but not their la-
bels) are used for the acquisition of seeds (Step 1)
and the selection of active learning points (Step 2).
We report averaged 10-fold cross-validation re-
sults in terms of accuracy. Following Kamvar et al
(2003), we also evaluate the clusters produced by
our approach against the gold-standard clusters us-
ing Adjusted Rand Index (ARI). ARI ranges from
?1 to 1; better clusterings have higher ARI values.
4.2 Baseline Systems
Recall that our approach uses 100 hand-labeled re-
views chosen by active learning. To ensure a fair
comparison, each of our three baselines has ac-
cess to 100 labeled points chosen from the train-
ing folds. Owing to the randomness involved in
the choice of labeled data, all baseline results are
averaged over ten independent runs for each fold.
Semi-supervised spectral clustering. We im-
plemented Kamvar et al?s (2003) semi-supervised
spectral clustering algorithm, which incorporates
labeled data into the clustering framework in the
form of must-link and cannot-link constraints. In-
stead of computing the similarity between each
pair of points, the algorithm computes the similar-
ity between a point and its k most similar points
only. Since its performance is highly sensitive to
706
Accuracy Adjusted Rand Index
System Variation MOV KIT ELE BOO DVD MOV KIT ELE BOO DVD
1 Semi-supervised spectral learning 67.3 63.7 57.7 55.8 56.2 0.12 0.08 0.01 0.02 0.02
2 Transductive SVM 68.7 65.5 62.9 58.7 57.3 0.14 0.09 0.07 0.03 0.02
3 Active learning 68.9 68.1 63.3 58.6 58.0 0.14 0.14 0.08 0.03 0.03
4 Our approach (after 1st step) 69.8 70.8 65.7 58.6 55.8 0.15 0.17 0.10 0.03 0.01
5 Our approach (after 2nd step) 73.5 73.0 69.9 60.6 59.8 0.22 0.21 0.16 0.04 0.04
6 Our approach (after 3rd step) 76.2 74.1 70.6 62.1 62.7 0.27 0.23 0.17 0.06 0.06
Table 2: Results in terms of accuracy and Adjusted Rand Index for the five datasets.
k, we tested values of 10, 15, . . ., 50 for k and re-
ported in row 1 of Table 2 the best results. As we
can see, accuracy ranges from 56.2% to 67.3%,
whereas ARI ranges from 0.02 to 0.12.
Transductive SVM. We employ as our second
baseline a transductive SVM5 trained using 100
points randomly sampled from the training folds
as labeled data and the remaining 1900 points as
unlabeled data. Results of this baseline are shown
in row 2 of Table 3. As we can see, accuracy
ranges from 57.3% to 68.7% and ARI ranges from
0.02 to 0.14, which are significantly better than
those of semi-supervised spectral learning.
Active learning. Our last baseline implements
the active learning procedure as described in Tong
and Koller (2002). Specifically, we begin by train-
ing an inductive SVM on one labeled example
from each class, iteratively labeling the most un-
certain unlabeled point on each side of the hyper-
plane and re-training the SVM until 100 points are
labeled. Finally, we train a transductive SVM on
the 100 labeled points and the remaining 1900 un-
labeled points, obtaining the results in row 3 of Ta-
ble 1. As we can see, accuracy ranges from 58%
to 68.9%, whereas ARI ranges from 0.03 to 0.14.
Active learning is the best of the three baselines,
presumably because it has the ability to choose the
labeled data more intelligently than the other two.
4.3 Our Approach
Results of our approach are shown in rows 4?6 of
Table 2. Specifically, rows 4 and 5 show the re-
sults of the SVM classifier when it is trained on
the labeled data obtained after the first step (unsu-
pervised extraction of unambiguous reviews) and
the second step (active learning), respectively. Af-
ter the first step, our approach can already achieve
5All the SVM classifiers in this paper are trained using
the SVMlight package (Joachims, 1999). All SVM-related
learning parameters are set to their default values, except in
transductive learning, where we set p (the fraction of unla-
beled examples to be classified as positive) to 0.5 so that the
system does not have any bias towards any class.
comparable results to the best baseline. Per-
formance increases substantially after the second
step, indicating the benefits of active learning.
Row 6 shows the results of transductive learn-
ing with ensemble. Comparing rows 5 and 6,
we see that performance rises by 0.7%-2.9% for
all five datasets after ?ensembled? transduction.
This could be attributed to (1) the unlabeled data,
which may have provided the transductive learner
with useful information that are not accessible to
the other learners, and (2) the ensemble, which is
more noise-tolerant to the imperfect seeds.
4.4 Additional Experiments
To gain insight into how the design decisions we
made in our approach impact performance, we
conducted the following additional experiments.
Importance of seeds. Table 1 showed that for
all but one dataset, the seeds obtained through
multiple iterations are more accurate than those
obtained in a single iteration. To envisage the im-
portance of seeds, we conducted an experiment
where we repeated our approach using the seeds
learned in a single iteration. Results are shown in
the first row of Table 3. In comparison to row 6 of
Table 2, we can see that results are indeed better
when we bootstrap from higher-quality seeds.
To further understand the role of seeds, we ex-
perimented with a version of our approach that
bootstraps from no seeds. Specifically, we used
the 500 seeds to guide the selection of active learn-
ing points, but trained a transductive SVM using
only the active learning points as labeled data (and
the rest as unlabeled data). As can be seen in row
2 of Table 3, the results are poor, suggesting that
our approach yields better performance than the
baselines not only because of the way the active
learning points were chosen, but also because of
contributions from the imperfectly labeled seeds.
We also experimented with training a transduc-
tive SVM using only the 100 least ambiguous
seeds (i.e., the points with the largest unsigned
707
Accuracy Adjusted Rand Index
System Variation MOV KIT ELE BOO DVD MOV KIT ELE BOO DVD
1 Single-step cluster purification 74.9 72.7 70.1 66.9 60.7 0.25 0.21 0.16 0.11 0.05
2 Using no seeds 58.3 55.6 59.7 54.0 56.1 0.04 0.04 0.02 0.01 0.01
3 Using the least ambiguous seeds 74.6 69.7 69.1 60.9 63.3 0.24 0.16 0.14 0.05 0.07
4 No Ensemble 74.1 72.7 68.8 61.5 59.9 0.23 0.21 0.14 0.05 0.04
5 Passive learning 74.1 72.4 68.0 63.7 58.6 0.23 0.20 0.13 0.07 0.03
6 Using 500 active learning points 82.5 78.4 77.5 73.5 73.4 0.42 0.32 0.30 0.22 0.22
7 Fully supervised results 86.1 81.7 79.3 77.6 80.6 0.53 0.41 0.34 0.30 0.38
Table 3: Additional results in terms of accuracy and Adjusted Rand Index for the five datasets.
second eigenvector values) in combination with
the active learning points as labeled data (and the
rest as unlabeled data). Note that the accuracy of
these 100 least ambiguous seeds is 4?5% higher
than that of the 500 least ambiguous seeds shown
in Table 1. Results are shown in row 3 of Table 3.
As we can see, using only 100 seeds turns out to be
less beneficial than using all of them via an ensem-
ble. One reason is that since these 100 seeds are
the most unambiguous, they may also be the least
informative as far as learning is concerned. Re-
member that SVM uses only the support vectors to
acquire the hyperplane, and since an unambiguous
seed is likely to be far away from the hyperplane,
it is less likely to be a support vector.
Role of ensemble learning To get a better idea
of the role of the ensemble in the transductive
learning step, we used all 500 seeds in combina-
tion with the 100 active learning points to train a
single transductive SVM. Results of this experi-
ment (shown in row 4 of Table 3) are worse than
those in row 6 of Table 2, meaning that the en-
semble has contributed positively to performance.
This should not be surprising: as noted before,
since the seeds are not perfectly labeled, using all
of them without an ensemble might overwhelm the
more informative active learning points.
Passive learning. To better understand the role
of active learning in our approach, we replaced it
with passive learning, where we randomly picked
100 data points from the training folds and used
them as labeled data. Results, shown in row 5 of
Table 3, are averaged over ten independent runs
for each fold. In comparison to row 6 of Table 2,
we see that employing points chosen by an active
learner yields significantly better results than em-
ploying randomly chosen points, which suggests
that the way the points are chosen is important.
Using more active learning points. An interest-
ing question is: how much improvement can we
obtain if we employ more active learning points?
In row 6 of Table 3, we show the results when the
experiment in row 6 of Table 2 was repeated using
500 active learning points. Perhaps not surpris-
ingly, the 400 additional labeled points yield a 4?
11% increase in accuracy. For further comparison,
we trained a fully supervised SVM classifier using
all of the training data. Results are shown in row
7 of Table 3. As we can see, employing only 500
active learning points enables us to almost reach
fully-supervised performance for three datasets.
5 Conclusions
We have proposed a novel semi-supervised ap-
proach to polarity classification. Our key idea
is to distinguish between unambiguous, easy-to-
mine reviews and ambiguous, hard-to-classify re-
views. Specifically, given a set of reviews, we
applied (1) an unsupervised algorithm to identify
and classify those that are unambiguous, (2) an
active learner that is trained solely on automati-
cally labeled unambiguous reviews to identify a
small number of prototypical ambiguous reviews
for manual labeling, and (3) an ensembled trans-
ductive learner to train a sophisticated classifier
on the reviews labeled so far to handle the am-
biguous reviews. Experimental results on five sen-
timent datasets demonstrate that our ?mine the
easy, classify the hard? approach, which only re-
quires manual labeling of a small number of am-
biguous reviews, can be employed to train a high-
performance polarity classification system.
We plan to extend our approach by exploring
two of its appealing features. First, none of the
steps in our approach is designed specifically for
sentiment classification. This makes it applica-
ble to other text classification tasks. Second, our
approach is easily extensible. Since the semi-
supervised learner is discriminative, our approach
can adopt a richer representation that makes use
of more sophisticated features such as bigrams or
manually labeled sentiment-oriented words.
708
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on an earlier draft of the pa-
per. This work was supported in part by NSF
Grant IIS-0812261.
References
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the ACL, pages 440?447.
Colin Campbell, Nello Cristianini, , and Alex J. Smola.
2000. Query learning with large margin classifiers.
In Proceedings of ICML, pages 111?118.
David Cohn, Les Atlas, and Richard Ladner. 1994.
Improving generalization with active learning. Ma-
chine Learning, 15(2):201?221.
Inderjit Dhillon, Yuqiang Guan, and Brian Kulis. 2004.
Kernel k-means, spectral clustering and normalized
cuts. In Proceedings of KDD, pages 551?556.
Mark Dredze and Koby Crammer. 2008. Active learn-
ing with confidence. In Proceedings of ACL-08:HLT
Short Papers (Companion Volume), pages 233?236.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and
Alexander Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 44?56. MIT
Press.
Sepandar Kamvar, Dan Klein, and Chris Manning.
2003. Spectral learning. In Proceedings of IJCAI,
pages 561?566.
Ravi Kannan, Santosh Vempala, and Adrian Vetta.
2004. On clusterings: Good, bad and spectral. Jour-
nal of the ACM, 51(3):497?515.
Moshe Koppel and Jonathan Schler. 2006. The im-
portance of neutral examples for learning sentiment.
Computational Intelligence, 22(2):100?109.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Proceedings of
the ACL, pages 432?439.
Marina Meila? and Jianbo Shi. 2001. A random walks
view of spectral segmentation. In Proceedings of
AISTATS.
Andrew Ng, Michael Jordan, and Yair Weiss. 2002.
On spectral clustering: Analysis and an algorithm.
In Advances in NIPS 14.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the ACL, pages 271?278.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86.
Greg Schohn and David Cohn. 2000. Less is more:
Active learning with support vector machines. In
Proceedings of ICML, pages 839?846.
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 22(8):888?
905.
Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Re-
search, 2:45?66.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the ACL, pages
417?424.
Yair Weiss. 1999. Segmentation using eigenvectors: A
unifying view. In Proceedings of ICCV, pages 975?
982.
709
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 843?851,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Semi-Supervised Cause Identification from Aviation Safety Reports
Isaac Persing and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{persingq,vince}@hlt.utdallas.edu
Abstract
We introduce cause identification, a new
problem involving classification of in-
cident reports in the aviation domain.
Specifically, given a set of pre-defined
causes, a cause identification system seeks
to identify all and only those causes that
can explain why the aviation incident de-
scribed in a given report occurred. The dif-
ficulty of cause identification stems in part
from the fact that it is a multi-class, multi-
label categorization task, and in part from
the skewness of the class distributions and
the scarcity of annotated reports. To im-
prove the performance of a cause identi-
fication system for the minority classes,
we present a bootstrapping algorithm that
automatically augments a training set by
learning from a small amount of labeled
data and a large amount of unlabeled data.
Experimental results show that our algo-
rithm yields a relative error reduction of
6.3% in F-measure for the minority classes
in comparison to a baseline that learns
solely from the labeled data.
1 Introduction
Automatic text classification is one of the most im-
portant applications in natural language process-
ing (NLP). The difficulty of a text classification
task depends on various factors, but typically, the
task can be difficult if (1) the amount of labeled
data available for learning the task is small; (2)
it involves multiple classes; (3) it involves multi-
label categorization, where more than one label
can be assigned to each document; (4) the class
distributions are skewed, with some categories
significantly outnumbering the others; and (5) the
documents belong to the same domain (e.g., movie
review classification). In particular, when the doc-
uments to be classified are from the same domain,
they tend to be more similar to each other with
respect to word usage, thus making the classes
less easily separable. This is one of the reasons
why topic-based classification, even with multiple
classes as in the 20 Newsgroups dataset1, tends to
be easier than review classification, where reviews
from the same domain are to be classified accord-
ing to the sentiment expressed2 .
In this paper, we introduce a new text classifi-
cation problem involving the Aviation Safety Re-
porting System (ASRS) that can be viewed as a
difficult task along each of the five dimensions dis-
cussed above. Established in 1967, ASRS collects
voluntarily submitted reports about aviation safety
incidents written by flight crews, attendants, con-
trollers, and other related parties. These incident
reports are made publicly available to researchers
for automatic analysis, with the ultimate goal of
improving the aviation safety situation. One cen-
tral task in the automatic analysis of these reports
is cause identification, or the identification of why
an incident happened. Aviation safety experts at
NASA have identified 14 causes (or shaping fac-
tors in NASA terminology) that could explain why
an incident occurred. Hence, cause identification
can be naturally recast as a text classification task:
given an incident report, determine which of a set
of 14 shapers contributed to the occurrence of the
incident described in the report.
As mentioned above, cause identification is
considered challenging along each of the five
aforementioned dimensions. First, there is a
scarcity of incident reports labeled with the
shapers. This can be attributed to the fact that
there has been very little work on this task. While
the NASA researchers have applied a heuristic
method for labeling a report with shapers (Posse
1http://kdd.ics.uci.edu/databases/20newsgroups/
2Of course, the fact that sentiment classification requires
a deeper understanding of a text also makes it more difficult
than topic-based text classification (Pang et al, 2002).
843
et al, 2005), the method was evaluated on only
20 manually labeled reports, which are not made
publicly available. Second, the fact that this is
a 14-class classification problem makes it more
challenging than a binary classification problem.
Third, a report can be labeled with more than one
category, as several shapers can contribute to the
occurrence of an aviation incident. Fourth, the
class distribution is very skewed: based on an
analysis of our 1,333 annotated reports, 10 of the
14 categories can be considered minority classes,
which account for only 26% of the total num-
ber of labels associated with the reports. Finally,
our cause identification task is domain-specific,
involving the classification of documents that all
belong to the aviation domain.
This paper focuses on improving the accuracy
of minority class prediction for cause identifica-
tion. Not surprisingly, when trained on a dataset
with a skewed class distribution, most supervised
machine learning algorithms will exhibit good per-
formance on the majority classes, but relatively
poor performance on the minority classes. Unfor-
tunately, achieving good accuracies on the minor-
ity classes is very important in our task of identify-
ing shapers from aviation safety reports, where 10
out of the 14 shapers are minority classes, as men-
tioned above. Minority class prediction has been
tackled extensively in the machine learning liter-
ature, using methods that typically involve sam-
pling and re-weighting of training instances, with
the goal of creating a less skewed class distribution
(e.g., Pazzani et al (1994), Fawcett (1996), Ku-
bat and Matwin (1997)). Such methods, however,
are unlikely to perform equally well for our cause
identification task given our small labeled set, as
the minority class prediction problem is compli-
cated by the scarcity of labeled data. More specif-
ically, given the scarcity of labeled data, many
words that are potentially correlated with a shaper
(especially a minority shaper) may not appear in
the training set, and the lack of such useful indi-
cators could hamper the acquisition of an accurate
classifier via supervised learning techniques.
We propose to address the problem of minority
class prediction in the presence of a small training
set by means of a bootstrapping approach, where
we introduce an iterative algorithm to (1) use a
small set of labeled reports and a large set of unla-
beled reports to automatically identify words that
are most relevant to the minority shaper under con-
sideration, and (2) augment the labeled data by us-
ing the resulting words to annotate those unlabeled
reports that can be confidently labeled. We evalu-
ate our approach using cross-validation on 1,333
manually annotated reports. In comparison to a
supervised baseline approach where a classifier is
acquired solely based on the training set, our boot-
strapping approach yields a relative error reduc-
tion of 6.3% in F-measure for the minority classes.
In sum, the contributions of our work are three-
fold. First, we introduce a new, challenging
text classification problem, cause identification
from aviation safety reports, to the NLP commu-
nity. Second, we created an annotated dataset for
cause identification that is made publicly available
for stimulating further research on this problem3.
Third, we introduce a bootstrapping algorithm for
improving the prediction of minority classes in the
presence of a small training set.
The rest of the paper is organized as follows. In
Section 2, we present the 14 shapers. Section 3 ex-
plains how we preprocess and annotate the reports.
Sections 4 and 5 describe the baseline approaches
and our bootstrapping algorithm, respectively. We
present results in Section 6, discuss related work
in Section 7, and conclude in Section 8.
2 Shaping Factors
As mentioned in the introduction, the task of cause
identification involves labeling an incident report
with all the shaping factors that contributed to the
occurrence of the incident. Table 1 lists the 14
shaping factors, as well as a description of each
shaper taken verbatim from Posse et al (2005).
As we can see, the 14 classes are not mutually ex-
clusive. For instance, a lack of familiarity with
equipment often implies a deficit in proficiency in
its use, so the two shapers frequently co-occur. In
addition, while some classes cover a specific and
well-defined set of issues (e.g., Illusion), some en-
compass a relatively large range of situations. For
instance, resource deficiency can include prob-
lems with equipment, charts, or even aviation per-
sonnel. Furthermore, ten shaping factors can be
considered minority classes, as each of them ac-
count for less than 10% of the labels. Accurately
predicting minority classes is important in this do-
main because, for example, the physical factors
minority shaper is frequently associated with in-
cidents involving near-misses between aircraft.
3http://www.hlt.utdallas.edu/?persingq/ASRSdataset.html
844
Id Shaping Factor Description %
1 Attitude Any indication of unprofessional or antagonistic attitude by a controller or flight crew mem-
ber, e.g., complacency or get-homeitis (in a hurry to get home).
2.4
2 Communication
Environment
Interferences with communications in the cockpit such as noise, auditory interference, radio
frequency congestion, or language barrier.
5.5
3 Duty Cycle A strong indication of an unusual working period, e.g., a long day, flying very late at night,
exceeding duty time regulations, having short and inadequate rest periods.
1.8
4 Familiarity A lack of factual knowledge, such as new to or unfamiliar with company, airport, or aircraft. 3.2
5 Illusion Bright lights that cause something to blend in, black hole, white out, sloping terrain, etc. 0.1
6 Other Anything else that could be a shaper, such as shift change, passenger discomfort, or disori-
entation.
13.3
7 Physical
Environment
Unusual physical conditions that could impair flying or make things difficult. 16.0
8 Physical
Factors
Pilot ailment that could impair flying or make things more difficult, such as being tired,
drugged, incapacitated, suffering from vertigo, illness, dizziness, hypoxia, nausea, loss of
sight or hearing.
2.2
9 Preoccupation A preoccupation, distraction, or division of attention that creates a deficit in performance,
such as being preoccupied, busy (doing something else), or distracted.
6.7
10 Pressure Psychological pressure, such as feeling intimidated, pressured, or being low on fuel. 1.8
11 Proficiency A general deficit in capabilities, such as inexperience, lack of training, not qualified, or not
current.
14.4
12 Resource
Deficiency
Absence, insufficient number, or poor quality of a resource, such as overworked or unavail-
able controller, insufficient or out-of-date chart, malfunctioning or inoperative or missing
equipment.
30.0
13 Taskload Indicators of a heavy workload or many tasks at once, such as short-handed crew. 1.9
14 Unexpected Something sudden and surprising that is not expected. 0.6
Table 1: Descriptions of shaping factor classes. The ?%? column shows the percent of labels the shapers account for.
3 Dataset
We downloaded our corpus from the ASRS web-
site4. The corpus consists of 140,599 incident
reports collected during the period from January
1998 to December 2007. Each report is a free
text narrative that describes not only why an in-
cident happened, but also what happened, where it
happened, how the reporter felt about the incident,
the reporter?s opinions of other people involved in
the incident, and any other comments the reporter
cared to include. In other words, a lot of informa-
tion in the report is irrelevant to (and thus compli-
cates) the task of cause identification.
3.1 Preprocessing
Unlike newswire articles, at which many topic-
based text classification tasks are targeted, the
ASRS reports are informally written using various
domain-specific abbreviations and acronyms, tend
to contain poor grammar, and have capitalization
information removed, as illustrated in the follow-
ing sentence taken from one of the reports.
HAD BEEN CLRED FOR APCH BY
ZOA AND HAD BEEN HANDED OFF
TO SANTA ROSA TWR.
4http://asrs.arc.nasa.gov/
This sentence is grammatically incorrect (due to
the lack of a subject), and contains abbrevia-
tions such as CLRED, APCH, and TWR. This
makes it difficult for a non-aviation expert to un-
derstand. To improve readability (and hence fa-
cilitate the annotation process), we preprocess
each report as follows. First, we expand the ab-
breviations/acronyms with the help of an official
list of acronyms/abbreviations and their expanded
forms5. Second, though not as crucial as the first
step, we heuristically restore the case of the words
by relying on an English lexicon: if a word ap-
pears in the lexicon, we assume that it is not a
proper name, and therefore convert it into lower-
case. After preprocessing, the example sentence
appears as
had been cleared for approach by ZOA
and had been handed off to santa rosa
tower.
Finally, to facilitate automatic analysis, we stem
each word in the narratives.
3.2 Human Annotation
Next, we randomly picked 1,333 preprocessed re-
ports and had two graduate students not affiliated
5See http://akama.arc.nasa.gov/ASRSDBOnline/pdf/
ASRS Decode.pdf. In the very infrequently-occurring case
where the same abbreviation or acronym may have more
than expansion, we arbitrarily chose one of the possibilities.
845
Id Total (%) F1 F2 F3 F4 F5
1 52 (3.9) 11 7 7 17 10
2 119 (8.9) 29 29 22 16 23
3 38 (2.9) 10 5 6 9 8
4 70 (5.3) 11 12 9 14 24
5 3 (0.2) 0 0 0 1 2
6 289 (21.7) 76 44 60 42 67
7 348 (26.1) 73 63 82 59 71
8 48 (3.6) 11 14 8 11 4
9 145 (10.9) 29 25 38 28 25
10 38 (2.9) 12 10 4 7 5
11 313 (23.5) 65 50 74 46 78
12 652 (48.9) 149 144 125 123 111
13 42 (3.2) 7 8 8 6 13
14 14 (1.1) 3 3 3 3 2
Table 2: Number of occurrences of each shaping
factor in the dataset. The ?Total? column shows the num-
ber of narratives labeled with each shaper and the percentage
of narratives tagged with each shaper in the 1,333 labeled
narrative set. The ?F? columns show the number narratives
associated with each shaper in folds F1 ? F5.
x (# Shapers) 1 2 3 4 5 6
Percentage 53.6 33.2 10.3 2.7 0.2 0.1
Table 3: Percentage of documents with x labels.
with this research independently annotate them
with shaping factors, based solely on the defi-
nitions presented in Table 1. To measure inter-
annotator agreement, we compute Cohen?s Kappa
(Carletta, 1996) from the two sets of annotations,
obtaining a Kappa value of only 0.43. This not
only suggests the difficulty of the cause identifica-
tion task, but also reveals the vagueness inherent
in the definition of the 14 shapers. As a result,
we had the two annotators re-examine each report
for which there was a disagreement and reach an
agreement on its final set of labels. Statistics of the
annotated dataset can be found in Table 2, where
the ?Total? column shows the size of each of the
14 classes, expressed both as the number of re-
ports that are labeled with a particular shaper and
as a percent (in parenthesis). Since we will per-
form 5-fold cross validation in our experiments,
we also show the number of reports labeled with
each shaper under the ?F? columns for each fold.
To get a better idea of how many reports have mul-
tiple labels, we categorize the reports according to
the number of labels they contain in Table 3.
4 Baseline Approaches
In this section, we describe two baseline ap-
proaches to cause identification. Since our ulti-
mate goal is to evaluate the effectiveness of our
bootstrapping algorithm, the baseline approaches
only make use of small amounts of labeled data for
acquiring classifiers. More specifically, both base-
lines recast the cause identification problem as a
set of 14 binary classification problems, one for
predicting each shaper. In the binary classification
problem for predicting shaper si, we create one
training instance from each document in the train-
ing set, labeling the instance as positive if the doc-
ument has si as one of its labels, and negative oth-
erwise. After creating training instances, we train
a binary classifier, ci, for predicting si, employing
as features the top 50 unigrams that are selected
according to information gain computed over the
training data (see Yang and Pedersen (1997)). The
SVM learning algorithm as implemented in the
LIBSVM software package (Chang and Lin, 2001)
is used for classifier training, owing to its robust
performance on many text classification tasks.
In our first baseline, we set al the learning pa-
rameters to their default values. As noted before,
we divide the 1,333 annotated reports into five
folds of roughly equal size, training the classifiers
on four folds and applying them separately to the
remaining fold. Results are reported in terms of
precision (P), recall (R), and F-measure (F), which
are computed by aggregating over the 14 shapers
as follows. Let tpi be the number of test reports
correctly labeled as positive by ci; pi be the total
number of test reports labeled as positive by ci;
and ni be the total number of test reports that be-
long to si according to the gold standard. Then,
P =
?
i tpi
?
i pi
,R =
?
i tpi
?
i ni
, and F = 2PRP + R.
Our second baseline is similar to the first, ex-
cept that we tune the classification threshold (CT)
to optimize F-measure. More specifically, recall
that LIBSVM trains a classifier that by default em-
ploys a CT of 0.5, thus classifying an instance as
positive if and only if the probability that it be-
longs to the positive class is at least 0.5. How-
ever, this may not be the optimal threshold to use
as far as performance is concerned, especially for
the minority classes, where the class distribution
is skewed. This is the motivation behind tuning
the CT of each classifier. To ensure a fair compar-
ison with the first baseline, we do not employ ad-
ditional labeled data for parameter tuning; rather,
we reserve 25% of the available training data for
tuning, and use the remaining 75% for classifier
846
acquisition. This amounts to using three folds
for training and one fold for development in each
cross validation experiment. Using the develop-
ment data, we tune the 14 CTs jointly to optimize
overall F-measure. However, an exact solution to
this optimization problem is computationally ex-
pensive. Consequently, we find a local maximum
by employing a local search algorithm, which al-
ters one parameter at a time to optimize F-measure
by holding the remaining parameters fixed.
5 Our Bootstrapping Algorithm
One of the potential weaknesses of the two base-
lines described in the previous section is that the
classifiers are trained on only a small amount of
labeled data. This could have an adverse effect
on the accuracy of the resulting classifiers, espe-
cially those for the minority classes. The situation
is somewhat aggravated by the fact that we are
adopting a one-versus-all scheme for generating
training instances for a particular shaper, which,
together with the small amount of labeled data, im-
plies that only a couple of positive instances may
be available for training the classifier for a minor-
ity class. To alleviate the data scarcity problem
and improve the accuracy of the classifiers, we
propose in this section a bootstrapping algorithm
that automatically augments a training set by ex-
ploiting a large amount of unlabeled data. The ba-
sic idea behind the algorithm is to iteratively iden-
tify words that are high-quality indicators of the
positive or negative examples, and then automati-
cally label unlabeled documents that contain a suf-
ficient number of such indicators.
Our bootstrapping algorithm, shown in Figure
1, aims to augment the set of positive and neg-
ative training instances for a given shaper. The
main function, Train, takes as input four argu-
ments. The first two arguments, P and N , are the
positive and negative instances, respectively, gen-
erated by the one-versus-one scheme from the ini-
tial training set, as described in the previous sec-
tion. The third argument, U , is the unlabeled set
of documents, which consists of all but the doc-
uments in the training set. In particular, U con-
tains the documents in the development and test
sets. Hence, we are essentially assuming access
to the test documents (but not their labels) dur-
ing the training process, as in a transductive learn-
ing setting. The last argument, k, is the number
of bootstrapping iterations. In addition, the algo-
Train(P,N, U, k)
Inputs:
P : positively labeled training examples of shaper x
N : negatively labeled training examples of shaper x
U : set of unlabeled narratives in corpus
k: number of bootstrapping iterations
PW ? ?
NW ? ?
for i = 0 to k ? 1 do
if |P | > |N | then
[P, PW ]? ExpandTrainingSet(P,N, U, PW )
else
[N, NW ]?ExpandTrainingSet(N,P, U, NW )
end if
end for
ExpandTrainingSet(A,B, U, W )
Inputs:
A, B, U : narrative sets
W : unigram feature set
for j = 1 to 4 do
t? arg maxt/?W
(
log( C(t,A)C(t,B)+1 )
)
// C(t, X): number of narratives in X containing t
W ?W ? {t}
end for
return [A ? S(W, U), W ]
// S(W,U): narratives in U containing ? 3 words in W
Figure 1: Our bootstrapping algorithm.
rithm uses two variables, PW and NW , to store
the sets of high-quality indicators for the positive
instances and the negative instances, respectively,
that are found during the bootstrapping process.
Next, we begin our k bootstrapping iterations.
In each iteration, we expand either P or N , de-
pending on their relative sizes. In order to keep
the two sets as close in size as possible, we choose
to expand the smaller of the two sets.6 After that,
we execute the function ExpandTrainingSet to ex-
pand the selected set. Without loss of general-
ity, assume that P is chosen for expansion. To
do this, ExpandTrainingSet selects four words that
seem much more likely to appear in P than in
N from the set of candidate words7. To select
these words, we calculate the log likelihood ratio
log( C(t,P )C(t,N)+1 ) for each candidate word t, where
C(t, P ) is the number of narratives in P that con-
tain t, and C(t,N) similarly is the number of nar-
ratives in N that contain t. If this ratio is large,
6It may seem from the way P and N are constructed that
N is almost always larger than P and therefore is unlikely to
be selected for expansion. However, the ample size of the un-
labeled set means that the algorithm still adds large numbers
of narratives to the training data. Hence, even for minority
classes, P often grows larger than N by iteration 3.
7A candidate word is a word that appears in the training
set (P ?N ) at least four times.
847
we posit that t is a good indicator of P . Note that
incrementing the count in the denominator by one
has a smoothing effect: it avoids selecting words
that appears infrequently in P and not at all in N .
There is a reason for selecting multiple words
(rather than just one word) in each bootstrap-
ping iteration: we want to prevent the algorithm
from selecting words that are too specific to one
subcategory of a shaping factor. For example,
shaping factor 7 (Physical Environment) is com-
posed largely of incidents influenced by weather
phenomena. In one experiment, we tried select-
ing only one word per bootstrapping iteration.
For shaper 7, the first word added to PW was
?snow?. Upon the next iteration, the algorithm
added ?plow? to PW. While ?plow? may itself be
indicative of shaper 7, we believe its selection was
due to the recent addition to P of a large number of
narratives containing ?snow?. Hence, by selecting
four words per iteration, we are forcing the algo-
rithm to ?branch out? among these subcategories.
After adding the selected words to PW , we
augment P with all the unlabeled documents con-
taining at least three words from PW . The rea-
son we impose the ?at least three? requirement
is precision: we want to ensure, with a reason-
able level of confidence, that the unlabeled doc-
uments chosen to augment P should indeed be
labeled with the shaper under consideration, as
incorrectly labeled documents would contaminate
the labeled data, thus accelerating the deterioration
of the quality of the automatically labeled data in
subsequent bootstrapping iterations and adversely
affecting the accuracy of the classifier trained on it
(Pierce and Cardie, 2001).
The above procedure is repeated in each boot-
strapping iteration. As mentioned above, if N
is smaller in size than P , we will expand N in-
stead, adding to NW the four words that are the
strongest indicators of a narrative being a negative
example of the shaper under consideration, and
augmenting N with those unlabeled narratives that
contain at least three words from NW .
The number of bootstrapping iterations is con-
trolled by the input parameter k. As we will see
in the next section, we run the bootstrapping algo-
rithm for up to five iterations only, as the quality
of the bootstrapped data deteriorates fairly rapidly.
The exact value of k will be determined automati-
cally using development data, as discussed below.
After bootstrapping, the augmented training
data can be used in combination with any of the
two baseline approaches to acquire a classifier for
identifying a particular shaper. Whichever base-
line is used, we need to reserve one of the five
folds to tune the parameter k in our cross vali-
dation experiments. In particular, if the second
baseline is used, we will tune CT and k jointly
on the development data using the local search al-
gorithm described previously, where we adjust the
values of both CT and k for one of the 14 classi-
fiers in each step of the search process to optimize
the overall F-measure score.
6 Evaluation
6.1 Baseline Systems
Since our evaluation centers on the question of
how effective our bootstrapping algorithm is in ex-
ploiting unlabeled documents to improve classifier
performance, our two baselines only employ the
available labeled documents to train the classifiers.
Recall that our first baseline, which we call
B0.5 (due to its being a baseline with a CT of
0.5), employs default values for all of the learn-
ing parameters. Micro-averaged 5-fold cross val-
idation results of this baseline for all 14 shapers
and for just 10 minority classes (due to our focus
on improving minority class prediction) are ex-
pressed as percentages in terms of precision (P),
recall (R), and F-measure (F) in the first row of
Table 4. As we can see, the baseline achieves
an F-measure of 45.4 (14 shapers) and 35.4 (10
shapers). Comparing these two results, the higher
F-measure achieved using all 14 shapers can be at-
tributed primarily to improvements in recall. This
should not be surprising: as mentioned above, the
number of positive instances of a minority class
may be small, thus causing the resulting classi-
fier to be biased towards classifying a document
as negative.
Instead of employing a CT value of 0.5, our
second baseline, Bct, tunes CT using one of the
training folds and simply trains a classifier on the
remaining three folds. For parameter tuning, we
tested CTs of 0.0, 0.05, . . ., 1.0. Results of this
baseline are shown in row 2 of Table 4. In com-
parison to the first baseline, we see that F-measure
improves considerably by 7.4% and 4.5% for 14
shapers and 10 shapers respectively8 , which illus-
8It is important to note that the parameters are optimized
separately for each pair of 14-shaper and 10-shaper exper-
iments in this paper, and that the 10-shaper results are not
848
All 14 Classes 10 Minority Classes
System P R F P R F
B0.5 67.0 34.4 45.4 68.3 23.9 35.4
Bct 47.4 59.2 52.7 47.8 34.3 39.9
E0.5 60.9 40.4 48.6 53.2 35.3 42.4
Ect 50.5 54.9 52.6 49.1 39.4 43.7
Table 4: 5-fold cross validation results.
trates the importance of employing the right CT
for the cause identification task.
6.2 Our Approach
Next, we evaluate the effectiveness of our boot-
strapping algorithm in improving classifier per-
formance. More specifically, we apply the two
baselines separately to the augmented training set
produced by our bootstrapping algorithm. When
combining our bootstrapping algorithm with the
first baseline, we produce a system that we call
E0.5 (due to its being trained on the expanded
training set with a CT of 0.5). E0.5 has only one
tunable parameter, k (i.e., the number of boot-
strapping iterations), whose allowable values are
0, 1, . . ., 5. When our algorithm is used in com-
bination with the second baseline, we produce an-
other system, Ect, which has both k and the CT
as its parameters. The allowable values of these
parameters, which are to be tuned jointly, are the
same as those employed by Bct and E0.5.
Results of E0.5 are shown in row 3 of Table
4. In comparison to B0.5, we see that F-measure
increases by 3.2% and 7.0% for 14 shapers and
10 shapers, respectively. Such increases can be
attributed to less imbalanced recall and precision
values, as a result of a large gain in recall accom-
panied by a roughly equal drop in precision. These
results are consistent with our intuition: recall can
be improved with a larger training set, but preci-
sion can be hampered when learning from nois-
ily labeled data. Overall, these results suggest that
learning from the augmented training set is useful,
especially for the minority classes.
Results of Ect are shown in row 4 of Table 4.
In comparison to Bct, we see mixed results: F-
measure increases by 3.8% for 10 shapers (which
represents a relative error reduction of 6.3%, but
drops by 0.1% for 14 shapers. Overall, these re-
sults suggest that when the CT is tunable, train-
ing set expansion helps the minority classes but
hurts the remaining classes. A closer look at the
results reveals that the 0.1% F-measure drop is due
simply extracted from the 14-shaper experiments.
to a large drop in recall accompanied by a smaller
gain in precision. In other words, for the four
non-minority classes, the benefits obtained from
using the bootstrapped documents can also be ob-
tained by simply adjusting the CT. This could be
attributed to the fact that a decent classifier can be
trained using only the hand-labeled training exam-
ples for these four shapers, and as a result, the au-
tomatically labeled examples either provide very
little new knowledge or are too noisy to be useful.
On the other hand, for the 10 minority classes, the
3.8% gain in F-measure can be attributed to a si-
multaneous rise in recall and precision. Note that
such gain cannot possibly be obtained by simply
adjusting the CT, since adjusting the CT always
results in higher recall and lower precision or vice
versa. Overall, the simultaneous rise in recall and
precision implies that the bootstrapped documents
have provided useful knowledge, particularly in
the form of positive examples, for the classifiers.
Even though the bootstrapped documents are nois-
ily labeled, they can still be used to improve the
classifiers, as the set of initially labeled positive
examples for the minority classes is too small.
6.3 Additional Analyses
Quality of the bootstrapped data. Since the
bootstrapped documents are noisily labeled, a nat-
ural question is: How noisy are they? To get a
sense of the accuracy of the bootstrapped docu-
ments without further manual labeling, recall that
our experimental setup resembles a transductive
setting where the test documents are part of the
unlabeled data, and consequently, some of them
may have been automatically labeled by the boot-
strapping algorithm. In fact, 137 documents in the
five test folds were automatically labeled in the
14-shaper Ect experiments, and 69 automatically
labeled documents were similarity obtained from
the 10-shaper Ect experiments. For 14 shapers, the
accuracies of the positively and negatively labeled
documents are 74.6% and 97.1%, respectively,
and the corresponding numbers for 10 shapers are
43.2% and 81.3%. These numbers suggest that
negative examples can be acquired with high ac-
curacies, but the same is not true for positive ex-
amples. Nevertheless, learning the 10 shapers
from the not-so-accurately-labeled positive exam-
ples still allows us to outperform the correspond-
ing baseline.
849
Shaping Factor Positive Expanders Negative Expanders
Familiarity unfamiliar, layout, unfamilarity, rely
Physical Environment cloud, snow, ice, wind
Physical Factors fatigue, tire, night, rest, hotel, awake, sleep, sick declare, emergency, advisory, separation
Preoccupation distract, preoccupied, awareness, situational,
task, interrupt, focus, eye, configure, sleep
declare, ice snow, crash, fire, rescue, anti,
smoke
Pressure bad, decision, extend, fuel, calculate, reserve,
diversion, alternate
Table 5: Example positive and negative expansion words collected by Ect for selected shaping factors.
Analysis of the expanders. To get an idea of
whether the words acquired during the bootstrap-
ping process (henceforth expanders) make intu-
itive sense, we show in Table 5 example positive
and negative expanders obtained for five shaping
factors from the Ect experiments. As we can see,
many of the positive expanders are intuitively ob-
vious. We might, however, wonder about the con-
nection between, for example, the shaper Famil-
iarity and the word ?rely?, or between the shaper
Pressure and the word ?extend?. We suspect that
the bootstrapping algorithm is likely to make poor
word selections particularly in the cases of the mi-
nority classes, where the positively labeled train-
ing data used to select expansion words is more
sparse. As suggested earlier, poor word choice
early in the algorithm is likely to cause even poorer
word choice later on.
On the other hand, while none of the negative
expanders seem directly meaningful in relation to
the shaper for which they were selected, some of
them do appear to be related to other phenomena
that may be negatively correlated with the shaper.
For instance, the words ?snow? and ?ice? were
selected as negative expanders for Preoccupation
and also as positive expanders for Physical Envi-
ronment. While these two shapers are only slightly
negatively correlated, it is possible that Preoccu-
pation may be strongly negatively correlated with
the subset of Physical Environment incidents in-
volving cold weather.
7 Related Work
Since we recast cause identification as a text clas-
sification task and proposed a bootstrapping ap-
proach that targets at improving minority class
prediction, the work most related to ours involves
one or both of these topics.
Guzma?n-Cabrera et al (2007) address the
problem of class skewness in text classification.
Specifically, they first under-sample the majority
classes, and then bootstrap the classifier trained
on the under-sampled data using unlabeled doc-
uments collected from the Web.
Minority classes can be expanded without the
availability of unlabeled data as well. For ex-
ample, Chawla et al (2002) describe a method
by which synthetic training examples of minor-
ity classes can be generated from other labeled
training examples to address the problem of im-
balanced data in a variety of domains.
Nigam et al (2000) propose an iterative semi-
supervised method that employs the EM algorithm
in combination with the naive Bayes generative
model to combine a small set of labeled docu-
ments and a large set of unlabeled documents. Mc-
Callum and Nigam (1999) suggest that the ini-
tial labeled examples can be obtained using a list
of keywords rather than through annotated data,
yielding an unsupervised algorithm.
Similar bootstrapping methods are applicable
outside text classification as well. One of the
most notable examples is Yarowsky?s (1995) boot-
strapping algorithm for word sense disambigua-
tion. Beginning with a list of unlabeled contexts
surrounding a word to be disambiguated and a list
of seed words for each possible sense, the algo-
rithm iteratively uses the seeds to label a training
set from the unlabeled contexts, and then uses the
training set to identify more seed words.
8 Conclusions
We have introduced a new problem, cause identi-
fication from aviation safety reports, to the NLP
community. We recast it as a multi-class, multi-
label text classification task, and presented a boot-
strapping algorithm for improving the prediction
of minority classes in the presence of a small train-
ing set. Experimental results show that our algo-
rithm yields a relative error reduction of 6.3% in
F-measure over a purely supervised baseline when
applied to the minority classes. By making our
annotated dataset publicly available, we hope to
stimulate research in this challenging problem.
850
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on an earlier draft of the
paper. We are indebted to Muhammad Arshad
Ul Abedin, who provided us with a preprocessed
version of the ASRS corpus and, together with
Marzia Murshed, annotated the 1,333 documents.
This work was supported in part by NASA Grant
NNX08AC35A and NSF Grant IIS-0812261.
References
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The Kappa statistic. Computational
Linguistics, 22(2):249?254.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: A library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.
Hall, and W. Philip Kegelmeyer. 2002. SMOTE:
Synthetic minority over-sampling technique. Jour-
nal of Artificial Intelligence Research, 16:321?357.
Tom Fawcett. 1996. Learning with skewed class distri-
butions ? summary of responses. Machine Learn-
ing List: Vol. 8, No. 20.
Rafael Guzma?n-Cabrera, Manuel Montes-y-Go?mez,
Paolo Rosso, and Luis Villasen?or Pineda. 2007.
Taking advantage of the Web for text classification
with imbalanced classes. In Proceedings of MICAI,
pages 831?838.
Miroslav Kubat and Stan Matwin. 1997. Addressing
the curse of imbalanced training sets: One-sided se-
lection. In Proceedings of ICML, pages 179?186.
Andrew McCallum and Kamal Nigam. 1999. Text
classification by bootstrapping with keywords, EM
and shrinkage. In Proceedings of the ACL Work-
shop for Unsupervised Learning in Natural Lan-
guage Processing, pages 52?58.
Kamal Nigam, Andrew McCallum, Sebastian Thrun,
and Tom Mitchell. 2000. Text classification from
labeled and unlabeled documents using EM. Ma-
chine Learning, 39(2/3):103?134.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86.
Michael Pazzani, Christopher Merz, Patrick Murphy,
Kamal Ali, Timothy Hume, and Clifford Brunk.
1994. Reducing misclassification costs. In Proceed-
ings of ICML, pages 217?225.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of EMNLP, pages 1?9.
Christian Posse, Brett Matzke, Catherine Anderson,
Alan Brothers, Melissa Matzke, and Thomas Ferry-
man. 2005. Extracting information from narratives:
An application to aviation safety reports. In Pro-
ceedings of the Aerospace Conference 2005, pages
3678?3690.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of ICML, pages 412?420.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the ACL, pages 189?196.
851
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 84?85,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Discriminative Models for Semi-Supervised Natural Language Learning
Sajib Dasgupta and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{sajib,vince}@hlt.utdallas.edu
1 Discriminative vs. Generative Models
An interesting question surrounding semi-
supervised learning for NLP is: should we use
discriminative models or generative models? De-
spite the fact that generative models have been
frequently employed in a semi-supervised setting
since the early days of the statistical revolution
in NLP, we advocate the use of discriminative
models. The ability of discriminative models to
handle complex, high-dimensional feature spaces
and their strong theoretical guarantees have made
them a very appealing alternative to their gen-
erative counterparts. Perhaps more importantly,
discriminative models have been shown to offer
competitive performance on a variety of sequential
and structured learning tasks in NLP that are
traditionally tackled via generative models , such
as letter-to-phoneme conversion (Jiampojamarn
et al, 2008), semantic role labeling (Toutanova
et al, 2005), syntactic parsing (Taskar et al,
2004), language modeling (Roark et al, 2004), and
machine translation (Liang et al, 2006). While
generative models allow the seamless integration
of prior knowledge, discriminative models seem
to outperform generative models in a ?no prior?,
agnostic learning setting. See Ng and Jordan (2002)
and Toutanova (2006) for insightful comparisons of
generative and discriminative models.
2 Discriminative EM?
A number of semi-supervised learning systems can
bootstrap from small amounts of labeled data using
discriminative learners, including self-training, co-
training (Blum and Mitchell, 1998), and transduc-
tive SVM (Joachims, 1999). However, none of them
seems to outperform the others across different do-
mains, and each has its pros and cons. Self-training
can be used in combination with any discriminative
learning model, but it does not take into account the
confidence associated with the label of each data
point, for instance, by placing more weight on the
(perfectly labeled) seeds than on the (presumably
noisily labeled) bootstrapped data during the learn-
ing process. Co-training is a natural choice if the
data possesses two independent, redundant feature
splits. However, this conditional independence as-
sumption is a fairly strict assumption and can rarely
be satisfied in practice; worse still, it is typically not
easy to determine the extent to which a dataset sat-
isfies this assumption. Transductive SVM tends to
learn better max-margin hyperplanes with the use
of unlabeled data, but its optimization procedure is
non-trivial and its performance tends to deteriorate if
a sufficiently large amount of unlabeled data is used.
Recently, Brefeld and Scheffer (2004) have pro-
posed a new semi-supervised learning technique,
EM-SVM, which is interesting in that it incorpo-
rates a discriminative model in an EM setting. Un-
like self-training, EM-SVM takes into account the
confidence of the new labels, ensuring that the in-
stances that are labeled with less confidence by the
SVM have less impact on the training process than
the confidently-labeled instances. So far, EM-SVM
has been tested on text classification problems, out-
performing transductive SVM. It would be interest-
ing to see whether EM-SVM can beat existing semi-
supervised learners for other NLP tasks.
84
3 Effectiveness of Bootstrapping
How effective are the aforementioned semi-
supervised learning systems in bootstrapping from
small amounts of labeled data? While there are quite
a few success stories reporting considerable perfor-
mance gains over an inductive baseline (e.g., parsing
(McClosky et al, 2008), coreference resolution (Ng
and Cardie, 2003), and machine translation (Ueff-
ing et al, 2007)), there are negative results too (see
Pierce and Cardie (2001), He and Gildea (2006),
Duh and Kirchhoff (2006)). Bootstrapping perfor-
mance can be sensitive to the setting of the param-
eters of these semi-supervised learners (e.g., when
to stop, how many instances to be added to the la-
beled data in each iteration). To date, however, re-
searchers have relied on various heuristics for pa-
rameter selection, but what we need is a principled
method for addressing this problem. Recently, Mc-
Closky et al (2008) have characterized the condi-
tions under which self-training would be effective
for semi-supervised syntactic parsing. We believe
that the NLP community needs to perform more re-
search of this kind, which focuses on identifying the
algorithm(s) that achieve good performance under a
given setting (e.g., few initial seeds, large amounts
of unlabeled data, complex feature space, skewed
class distributions).
4 Domain Adaptation
Domain adaptation has recently become a popular
research topic in the NLP community. Labeled data
for one domain might be used to train a initial classi-
fier for another (possibly related) domain, and then
bootstrapping can be employed to learn new knowl-
edge from the new domain (Blitzer et al, 2007). It
would be interesting to see if we can come up with
a similar semi-supervised learning model for pro-
jecting resources from a resource-rich language to
a resource-scarce language.
References
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the ACL.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of COLT.
Ulf Brefeld and Tobias Scheffer. 2004. Co-EM support
vector learning. In Proceedings of ICML.
Kevin Duh and Katrin Kirchhoff. 2006. Lexicon acqui-
sition for dialectal Arabic using transductive learning.
In Proceedings of EMNLP.
Shan He and Daniel Gildea. 2006. Self-training and co-
training for semantic role labeling.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proceed-
ings of ACL-08:HLT.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of ICML.
Percy Liang, Alexandre Bouchard, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach
to machine translation. In Proceedings of the ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. When is self-training effective for parsing? In
Proceedings of COLING.
Vincent Ng and Claire Cardie. 2003. Weakly supervised
natural language learning without redundant views. In
Proceedings of HLT-NAACL.
Andrew Ng and Michael Jordan. 2002. On discrimina-
tive vs.generative classifiers: A comparison of logistic
regression and Naive Bayes. In Advances in NIPS.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of EMNLP.
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language modeling
with conditional random fields and the perceptron al-
gorithm. In Proceedings of the ACL.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher Manning. 2004. Max-margin pars-
ing. In Proceedings of EMNLP.
Kristina Toutanova, Aria Haghighi, , and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of the ACL.
Kristina Toutanova. 2006. Competitive generative mod-
els with structure learning for NLP classification tasks.
In Proceedings of EMNLP.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the ACL.
85
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 931?939,
Beijing, August 2010
Inducing Fine-Grained Semantic Classes via
Hierarchical and Collective Classification
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
{altaf,vince}@hlt.utdallas.edu
Abstract
Research in named entity recognition and
mention detection has typically involved a
fairly small number of semantic classes,
which may not be adequate if seman-
tic class information is intended to sup-
port natural language applications. Moti-
vated by this observation, we examine the
under-studied problem of semantic sub-
type induction, where the goal is to au-
tomatically determine which of a set of
92 fine-grained semantic classes a noun
phrase belongs to. We seek to improve the
standard supervised approach to this prob-
lem using two techniques: hierarchical
classification and collective classification.
Experimental results demonstrate the ef-
fectiveness of these techniques, whether
or not they are applied in isolation or in
combination with the standard approach.
1 Introduction
Semantic class determination refers to the task
of classifying a noun phrase (NP), be it a name
or a nominal, as one of a set of pre-defined se-
mantic classes. A semantic class classifier is a
basic text-processing component in many high-
level natural language processing (NLP) applica-
tions, including information-extraction (IE) sys-
tems and question-answering (QA) systems. In
recent years, supervised semantic class determi-
nation has been tackled primarily in the context of
(1) coreference resolution (e.g., Ng (2007), Huang
et al (2009)), where semantic classes are induced
and subsequently used to disallow coreference be-
tween semantically incompatible NPs, and (2) the
mention detection task in the ACE evaluations
(e.g., Florian et al (2004; 2006)), where the goal
is to identify the boundary of a mention (i.e., a
noun phrase that belongs to one of the pre-defined
ACE semantic classes), its mention type (e.g., pro-
noun, name), and its semantic class. The output
of a mention detector is then used by downstream
IE components, which typically include a coref-
erence resolution system and a relation extraction
system. Owing in part to its potentially large in-
fluence on downstream IE components, accurate
semantic class determination is crucial.
Over the years, NLP researchers have focused
on a relatively small number of semantic classes in
both NE recognition and mention detection: seven
classes in the MUC-6 and MUC-7 NE recognition
task, four classes in the CoNLL 2002 and 2003
NE recognition shared task, and seven classes in
the ACE 2005 mention detection task. Given that
one of the uses of semantic class information is
to support NLP applications, it is questionable
whether this purpose can be adequately served by
such a small number of semantic classes. For ex-
ample, given the question ?Which city was the
first Olympic Games held in??, it would be help-
ful for a QA system to know which NEs are cities.
However, virtually all of the existing NE recog-
nizers and mention detectors can only determine
whether an NE is a location or not.
Our goal in this paper is to tackle the under-
studied problem of determining fine-grained se-
mantic classes (henceforth semantic subtypes).
More specifically, we aim to classify an NP as
one of the 92 fine-grained, domain-independent
semantic classes that are determined to be use-
ful for supporting the development of QA and
931
IE systems in the ACE and AQUAINT programs.
These 92 semantic subtypes have been used to
manually annotate the NPs in the BBN Entity Type
Corpus (Weischedel and Brunstein, 2005). Given
the availability of this semantic subtype-annotated
corpus, we adopt a supervised machine learn-
ing approach to semantic subtype determination.
Specifically, given (the boundary of) an NP, we
train a classification model to determine which of
the 92 semantic subtypes it belongs to.
More importantly, we seek to improve the stan-
dard approach to semantic subtype induction de-
scribed above by proposing two techniques. The
first technique, collective classification, aims to
address a common weakness in the standard su-
pervised learning paradigm, where a classifier
classifies each instance independently of the oth-
ers and is unable to exploit any relational informa-
tion between a pair (or a subset) of the instances
that may be helpful for classification. The sec-
ond technique, hierarchical classification, exploits
the observation that these 92 semantic subtypes
can be grouped into a smaller number of coarse-
grained semantic types (henceforth semantic su-
pertypes). With this two-level hierarchy, learning
can proceed in a sequential fashion: given an NP,
we first determine its semantic supertype and then
classify it as one of the semantic subtypes that
fall under the predicted supertype in the hierar-
chy. Empirical results show that these two tech-
niques, when applied in isolation to the standard
learning approach to subtype induction, can sig-
nificantly improve its accuracy, and the best result
is achieved when they are applied in combination.
The rest of the paper is organized as follows.
Section 2 provides an overview of the 92 seman-
tic subtypes and the evaluation corpus. In Sec-
tion 3, we present our baseline semantic subtype
classification system. Sections 4 and 5 introduce
collective classification and hierarchical classifi-
cation respectively, and describe how these two
techniques can be used to improve the baseline
semantic subtype classifier. We show evaluation
results in Section 6 and conclude in Section 7.
2 Semantic Subtypes
As noted before, each name and nominal in the
BBN Entity Type Corpus is annotated with one of
the 92 semantic subtypes. In our experiments, we
use all the 200 Penn Treebank Wall Street Journal
articles in the corpus, yielding 17,292 NPs that are
annotated with their semantic subtypes.
Table 1 presents an overview of these subtypes.
Since they have been manually grouped into 29
supertypes, we also show the supertypes in the ta-
ble. More specifically, the first column shows the
supertypes, the second column contains a brief de-
scription of a supertype, and the last column lists
the subtypes that correspond to the supertype in
the first column. In cases where a supertype con-
tains only one subtype (e.g., PERSON), the super-
type is not further partitioned into different sub-
types; for classification purposes, we simply treat
the subtype as identical to its supertype (and hence
the two always have the same name). A detailed
description of these supertypes and subtypes can
be found in Weischedel and Brunstein (2005). Fi-
nally, we show the class distribution: the paren-
thesized number after each subtype is the percent-
age of the 17,292 NPs annotated with the subtype.
3 Baseline Classification Model
We adopt a supervised machine learning approach
to train our baseline classifier for determining the
semantic subtype of an NP. This section describes
the details of the training process.
Training corpus. As mentioned before, we use
the Wall Street Journal articles in the BBN Entity
Type Corpus for training the classifier.
Training instance creation. We create one
training instance for each annotated NP, NPi,
which is either a name or a nominal, in each train-
ing text. The classification of an instance is its an-
notated semantic subtype value, which is one of
the 92 semantic subtypes. Each instance is repre-
sented by a set of 33 features1, as described below.
1. Mention String (3): Three features are de-
rived from the string of NPi. Specifically, we em-
ploy the NP string as a feature. If NPi contains
more than one token, we create one feature for
each of its constituent tokens. Finally, to distin-
guish the different senses of a nominal, we create
1As we will see, since we employ an exponential model,
an instance may be represented by fewer than 33 features.
932
Supertype Brief Description Subtypes
PERSON Proper names of people. Person (9.2).
PERSON DESC Any head word of a common noun Person Desc (16.8).
referring to a person or group of people.
NORP This type is named after its subtypes: Nationality (2.9), Religion (0.1), Political (0.6),
nationality, religion, political, etc. Other (0.1).
FACILITY Names of man-made structures, including Building (0.1), Bridge (0.02), Airport (0.01),
infrastructure, buildings, monuments, Attraction (0.01), Highway Street (0.05),
camps, farms, mines, ports, etc. Other (0.1).
FACILITY DESC Head noun of a noun phrase describing Building (0.5), Bridge (0.05), Airport (0.01),
buildings, bridges, airports, etc. Highway Street (0.2), Attraction (0.02), Other (0.5).
ORGANIZATION Names of companies, government Government (3.6), Corporation (8.3), Political (0.5),
agencies, educational institutions and Educational (0.3), Hotel (0.04), City (0.01),
other institutions. Hospital (0.01), Religious (0.1), Other (0.7).
ORG DESC Heads of descriptors of companies, Government (2.1), Corporation (4.3), Political (0.2),
educational institutions and other Educational (0.1), Religious (0.1), Hotel (0.1),
governments, government agencies, etc. City (0.01), Hospital (0.02), Other (0.7).
GPE Names of countries, cities, states, Country (4.2), City (3.2), State Province (1.4),
provinces, municipalities, boroughs. Other (0.1).
GPE DESC Heads of descriptors of countries, cities, Country (0.8), City (0.3), State Province (0.3),
states, provinces, municipalities. Other (0.1).
LOCATION Names of locations other than GPEs. River (0.03), Lake Sea Ocean (0.05), Region (0.2),
E.g., mountain ranges, coasts, borders, Continent (0.1), Other (0.2).
planets, geo-coordinates, bodies of water.
PRODUCT Name of any product. It does not Food (0.01), Weapon (0.02), Vehicle (0.2),
include the manufacturer). Other (0.2).
PRODUCT DESC Descriptions of weapons and vehicles Food (0.01), Weapon (0.2), Vehicle (0.97),
only. Cars, buses, machine guns, missiles, Other (0.02).
bombs, bullets, etc.
DATE Classify a reference to a date or period. Date (7.99), Duration (1.9), Age (0.5), Other (0.4).
TIME Any time ending with A.M. or P.M. Time (0.5).
PERCENT Percent symbol or the actual word percent. Percent (2.07).
MONEY Any monetary value. Money (2.9).
QUANTITY Used to classify measurements. E.g., 4 1D (0.11), 2D (0.08), 3D (0.1), Energy (0.01),
miles, 4 grams, 4 degrees, 4 pounds, etc. Speed (0.01), Weight (0.1), Other (0.04).
ORDINAL All ordinal numbers. E.g., First, fourth. Ordinal (0.6).
CARDINAL Numerals that provide a count or quantity. Cardinal (5.1).
EVENT Named hurricanes, battles, wars, sports War (0.03), Hurricane (0.1), Other (0.24).
events, and other named events.
PLANT Any plant, flower, tree, etc. Plant (0.2).
ANIMAL Any animal class or proper name of an Animal (0.7).
animal, real or fictional.
SUBSTANCE Any chemicals, elements, drugs, and Food (1.1), Drug (0.46), Chemical (0.23), Other (0.9).
foods. E.g., boron, penicillin, plutonium.
DISEASE Any disease or medical condition. Disease (0.6).
LAW Any document that has been made into Law (0.5).
a law. E.g., Bill of Rights, Equal Rights.
LANGUAGE Any named language. Language (0.2).
CONTACT INFO Address, phone. Address (0.01), Phone (0.04).
GAME Any named game. Game (0.1).
WORK OF ART Titles of books, songs and other creations. Book (0.16), Play (0.04), Song (0.03), Painting (0.01),
Other (0.4).
Table 1: The 92 semantic subtypes and their corresponding supertypes.
a feature whose value is the concatenation of the
head of NPi and its WordNet sense number.2
2We employ the sense number that is manually annotated
for each NP in the WSJ corpus as part of the OntoNotes
project (Hovy et al, 2006).
2. Verb String (3): If NPi is governed by a verb,
the following three features are derived from the
governing verb. First, we employ the string of the
governing verb as a feature. Second, we create
a feature whose value is the semantic role of the
933
governing verb.3 Finally, to distinguish the differ-
ent senses of the governing verb, we create a fea-
ture whose value is the concatenation of the verb
and its WordNet sense number.
3. Semantic (5): We employ five semantic fea-
tures. First, if NPi is an NE, we create a feature
whose value is the NE label of NPi, as determined
by the Stanford CRF-based NE recognizer (Finkel
et al, 2005). However, if NPi is a nominal, we cre-
ate a feature that encodes the WordNet semantic
class of which it is a hyponym, using the manu-
ally determined sense of NPi.4 Moreover, to im-
prove generalization, we employ a feature whose
value is the WordNet synset number of the head
noun of a nominal. If NPi has a governing verb,
we also create a feature whose value is the Word-
Net synset number of the verb. Finally, if NPi is a
nominal, we create a feature based on its WordNet
equivalent concept. Specifically, for each entity
type defined in ACE 20055, we create a list con-
taining all the word-sense pairs in WordNet (i.e.,
synsets) whose glosses are compatible with that
entity type.6 Then, given NPi and its sense, we use
these lists to determine if it belongs to any ACE
2005 entity type. If so, we create a feature whose
value is the corresponding entity type.
4. Morphological (8). If NPi is a nominal, we
create eight features: prefixes and suffixes of
length one, two, three, and four.
5. Capitalization (4): We create four cap-
italization features to determine whether NPi
IsAllCap, IsInitCap, IsCapPeriod, and
IsAllLower (see Bikel et al (1999)).
6. Gazetteers (8): We compute eight gazetteer-
based features, each of which checks whether NPi
is in a particular gazetteer. The eight dictionaries
contain pronouns (77 entries), common words and
words that are not names (399.6k), person names
(83.6k), person titles and honorifics (761), vehi-
3We also employ the semantic role that is manually anno-
tated for each NP in the WSJ corpus in OntoNotes.
4The semantic classes we considered are person, location,
organization, date, time, money, percent, and object.
5The ACE 2005 entity types include person, organization,
GPE, facility, location, weapon, and vehicle.
6Details of how these lists are constructed can be found
in Nicolae and Nicolae (2006).
cle words (226), location names (1.8k), company
names (77.6k), and nouns extracted from Word-
Net that are hyponyms of PERSON (6.3k).
7. Grammatical (2): We create a feature that
encodes the part-of-speech (POS) sequence of NPi
obtained via the Stanford POS tagger (Toutanova
et al, 2003). In addition, we have a feature that
determines whether NPi is a nominal or not.
We employ maximum entropy (MaxEnt) mod-
eling7 for training the baseline semantic subtype
classifier. MaxEnt is chosen because it provides
a probabilistic classification for each instance,
which we will need to perform collective classi-
fication, as described in the next section.
4 Collective Classification
One weakness of the baseline classification model
is that it classifies each instance independently. In
particular, the model cannot take into account re-
lationships between them that may be helpful for
improving classification accuracy. For example,
if two NPs are the same string in a given doc-
ument, then it is more likely than not that they
have the same semantic subtype according to the
?one sense per discourse? hypothesis (Gale et al,
1992). Incorporating this kind of relational infor-
mation into the feature set employed by the base-
line system is not an easy task, since each feature
characterizes only a single NP.
To make use of the relational information, one
possibility is to design a new learning procedure.
Here, we adopt a different approach: we perform
collective classification, or joint probabilistic in-
ference, on the output of the baseline model. The
idea is to treat the output for each NP, which is
a probability distribution over the semantic sub-
types, as its prior label/class distribution, and con-
vert it into a posterior label/class distribution by
exploiting the available relational information as
an additional piece of evidence. For this purpose,
we will make use of factor graphs. In this section,
we first give a brief overview of factor graphs8,
and show how they can be used to perform joint
7We use the MaxEnt implementation available at
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
8See Bunescu and Mooney (2004) and Loeliger (2004)
for a detailed introduction to factor graphs.
934
inference for semantic subtype determination.
4.1 Factor Graphs
Factor graphs model optimization problems of
an objective function g, which is a real-valued
function of n random variables X1, ..., Xn. We
assume that g can be decomposed into a product
of m factors. In other words, g (X1, ..., Xn) =
f1 (s1 (X1, ..., Xn)) ...fm (sm (X1, ..., Xn)),
where each factor fk is a real-valued function
of some subset of X1, ... , Xn, denoted as
sk (X1, ..., Xn). Each fk can be thought of as a
feature function that computes the compatibility
of an assignment of values to the variables in
sk (X1, ..., Xn) with respect to a user-defined
feature. Hence, a larger function value is more
desirable, as it corresponds to a more compatible
assignment of values to the variables involved.
A factor graph consists of two types of nodes:
variable nodes and factor nodes. Each random
variable Xi is represented by a variable node, and
each factor fk is represented by a factor node.
Each factor node fk is connected only to the nodes
corresponding to sk. This results in a bipartite
graph, where edges exist only between a variable
node and a factor node.
Given this graph, there are several methods for
finding an optimal assignment of the random vari-
ables X1, ..., Xn such that the objective function
g is maximized. Exact inference using the sum-
product algorithm (Kschischang et al, 2001) is
possible if there are no cycles in the graph; other-
wise a belief propagation algorithm, such as loopy
belief propagation (Murphy et al, 1999), can be
applied. Although there are no cycles in our factor
graphs, we choose to use loopy belief propagation
as our inferencer, since it performs approximate
inference and is therefore computationally more
efficient than an exact inferencer.
4.2 Application to Subtype Inference
To apply joint inference to semantic subtype in-
duction, we create one factor graph for each test
document, where each variable node is random
variable Xi over the set of semantic subtype la-
bels L and represents an NP, NPi, in the docu-
ment. To retain the prior probabilities over the
semantic subtype labels lq ? L obtained from the
baseline classification model, each variable node
is given a factor f (Xi) = P (Xi = lq). If no
additional factors that model the relation between
two nodes/instances are introduced, maximizing
the objective function for this graph (by maximiz-
ing the product of factors) will find an assignment
identical to the one obtained by taking the most
probable semantic subtype label assigned to each
instance by the baseline classifier.
Next, we exploit the relationship between two
random variables. Specifically, we want to en-
courage the inference algorithm to assign the
same label to two variables if there exists a rela-
tion between the corresponding NPs that can pro-
vide strong evidence that they should receive the
same label. To do so, we create a pairwise fac-
tor node that connects two variable nodes if the
aforementioned relation between the underlying
NPs is satisfied. However, to implement this idea,
we need to address two questions.
First, which relation between two NPs can pro-
vide strong evidence that they have the same se-
mantic subtype? We exploit the coreference re-
lation. Intuitively, the coreference relation is a
reasonable choice, as coreferent entities are likely
to have the same semantic subtype. Here, we
naively posit two NPs as coreferent if at least one
of the following conditions is satisfied: (1) they
are the same string after determiners are removed;
(2) they are aliases (i.e., one is an acronym or
abbreviation of the other); and (3) they are both
proper names and have at least one word in com-
mon (e.g., ?Delta? and ?Delta Airlines?).9
Second, how can we define a pairwise factor,
fpair, so that it encourages the inference algo-
rithm to assign the same label to two nodes? One
possibility is to employ the following definition:
fpair(Xi, Xj)
= P (Xi = lp, Xj = lq),where lp, lq ? L
=
{
1 if lp = lq
0 otherwise
In essence, fpair prohibits the assignment of dif-
ferent labels to the two nodes it connects. In our
9The third condition can potentially introduce many false
positives, positing ?Bill Clinton? and ?Hillary Clinton? as
coreferent, for instance. However, this kind of false positives
does not pose any problem for us, since the two NPs involved
belong to the same semantic subtype (i.e., PERSON).
935
experiments, however, we ?improve? fpair by in-
corporating semantic supertype information into
its definition, as shown below:
fpair(Xi, Xj)
= P (Xi = lp, Xj = lq),where lp, lq ? L
=
{
Psup(sup(lp)|NPi)Psup(sup(lq)|NPj) if lp = lq
0 otherwise
In this definition, sup(lq) is the supertype of lq
according to the semantic type hierarchy shown
in Section 2, and Psup(sup(lq)|NPj) is the proba-
bility that NPj belongs to sup(lq) according to the
semantic supertype classification model Psup (see
Section 5 for details on how this model can be
trained). In essence, we estimate the joint proba-
bility by (1) assuming that the two events are inde-
pendent, and then (2) computing each event using
supertype information. Intuitively, this definition
allows fpair to favor those label assignments that
are more compatible with the predictions of Psup.
After graph construction, we apply an infer-
encer to compute a marginal probability distribu-
tion over the labels for each node/instance in the
graph by maximizing the objective function g, and
output the most probable label for each instance
according to its marginal distribution.
5 Hierarchical Classification
The pairwise factor fpair defined above exploits
supertype information in a soft manner, meaning
that the most probable label assigned to an NP by
an inferencer is not necessarily consistent with its
predicted supertype (e.g., an NP may receive Ho-
tel as its subtype even if its supertype is PERSON).
In this section, we discuss how to use supertype
information for semantic subtype classification in
a hard manner so that the predicted subtype is
consistent with its supertype.
To exploit supertype information, we first train
a model, Psup, for determining the semantic su-
pertype of an NP using MaxEnt. This model is
trained in essentially the same way as the base-
line model described in Section 3. In particular,
it is trained on the same set of instances using the
same feature set as the baseline model. The only
difference is that the class value of each training
instance is the semantic supertype of the associ-
ated NP rather than its semantic subtype.
Next, we train 29 supertype-specific classifi-
cation models for determining the semantic sub-
type of an NP. For instance, the ORGANIZATION-
specific classification model will be used to clas-
sify an NP as belonging to one of its subtypes
(e.g., Government, Corporation, Political agen-
cies). A supertype-specific classification model is
trained much like the baseline model. Each in-
stance is represented using the same set of fea-
tures as in the baseline, and its class label is its
semantic subtype. The only difference is that the
model is only trained only on the subset of the
instances for which it is intended. For instance,
the ORGANIZATION-specific classification model
is trained only on instances whose class is a sub-
type of ORGANIZATION.
After training, we can apply the supertype clas-
sification model and the supertype-specific sub-
type classification model to determine the se-
mantic subtype of an NP in a hierarchical fash-
ion. Specifically, we first employ the supertype
model to determine its semantic supertype. Then,
depending on this predicted semantic supertype,
we use the corresponding subtype classification
model to determine its subtype.
6 Evaluation
For evaluation, we partition the 200 Wall Street
Journal Articles in the BBN Entity Type corpus
into a training set and a test set following a 80/20
ratio. As mentioned before, each text in the Entity
Type corpus has its NPs annotated with their se-
mantic subtypes. Test instances are created from
these texts in the same way as the training in-
stances described in Section 3. To investigate
whether we can benefit from hierarchical and col-
lective classifications, we apply these two tech-
niques to the Baseline classification model in iso-
lation and in combination, resulting in the four
sets of results in Tables 2 and 3.
The Baseline results are shown in the second
column of Table 2. Due to space limitations, it is
not possible to show the result for each semantic
subtype. Rather, we present semantic supertype
results, which are obtained by micro-averaging
the corresponding semantic subtype results and
are expressed in terms of recall (R), precision (P),
and F-measure (F). Note that only those semantic
936
Baseline only Baseline+Hierarchical
Semantic Supertype R P F R P F
1 PERSON 91.9 89.7 90.8 88.8 91.1 89.9
2 PERSON DESC 91.3 87.8 89.5 92.1 89.8 91.0
3 SUBSTANCE 60.0 66.7 63.2 70.0 58.3 63.6
4 NORP 87.8 90.3 89.0 91.9 90.7 91.3
5 FACILITY DESC 72.7 88.9 80.0 68.2 93.8 79.0
6 ORGANIZATION 76.6 73.8 75.2 78.5 73.2 75.8
7 ORG DESC 75.0 70.7 72.8 75.8 75.2 75.5
8 GPE 75.6 73.9 74.7 77.0 75.4 76.2
9 GPE DESC 60.0 75.0 66.7 70.0 70.0 70.0
10 PRODUCT DESC 53.3 88.9 66.7 53.3 88.9 66.7
11 DATE 85.0 85.0 85.0 84.5 85.4 85.0
12 PERCENT 100.0 100.0 100.0 100.0 100.0 100.0
13 MONEY 83.9 86.7 85.3 88.7 96.5 92.4
14 QUANTITY 22.2 100.0 36.4 66.7 66.7 66.7
15 ORDINAL 100.0 100.0 100.0 100.0 100.0 100.0
16 CARDINAL 96.0 77.4 85.7 94.0 81.0 87.0
Accuracy 81.56 82.60
Table 2: Results for Baseline only and Baseline with hierarchical classification.
Baseline+Collective Baseline+Both
Semantic Supertype R P F R P F
1 PERSON 93.8 98.1 95.9 91.9 100.0 95.8
2 PERSON DESC 93.9 88.5 91.1 92.6 89.5 91.0
3 SUBSTANCE 60.0 85.7 70.6 70.0 63.6 66.7
4 NORP 89.2 93.0 91.0 90.5 94.4 92.4
5 FACILITY DESC 63.6 87.5 73.7 68.2 93.8 79.0
6 ORGANIZATION 85.8 76.2 80.7 87.4 76.3 81.3
7 ORG DESC 75.8 74.1 74.9 75.8 74.6 75.2
8 GPE 74.1 75.8 74.9 81.5 81.5 81.5
9 GPE DESC 60.0 60.0 60.0 70.0 77.8 73.7
10 PRODUCT DESC 53.3 88.9 66.7 53.3 88.9 66.7
11 DATE 85.0 85.4 85.2 85.0 86.3 85.6
12 PERCENT 100.0 100.0 100.0 100.0 100.0 100.0
13 MONEY 83.9 86.7 85.3 90.3 96.6 93.3
14 QUANTITY 22.2 100.0 36.4 66.7 66.7 66.7
15 ORDINAL 100.0 100.0 100.0 100.0 100.0 100.0
16 CARDINAL 96.0 78.7 86.5 94.0 83.9 88.7
Accuracy 83.70 85.08
Table 3: Results for Baseline with collective classification and Baseline with both techniques.
supertypes with non-zero scores are shown. As we
can see, only 16 of the 29 supertypes have non-
zero scores.10 Among the ?traditional? seman-
tic types, the Baseline yields good performance
for PERSON, but only mediocre performance for
ORGANIZATION and GPE. While additional ex-
periments are needed to determine the reason, we
speculate that this can be attributed to the fact that
PERSON and PERSON DESC have only one seman-
tic subtype (which is the supertype itself), whereas
10The 13 supertypes that have zero scores are all under-
represented classes, each of which accounts for less than one
percent of the instances in the dataset.
ORGANIZATION and GPE have nine and four sub-
types, respectively. The classification accuracy is
shown in the last row of the table. As we can see,
the Baseline achieves an accuracy of 81.56.
Results obtained when hierarchical classifica-
tion is applied to the Baseline are shown in the
third column of Table 2. In comparison to the
Baseline, accuracy rises from 81.56 to 82.60. This
represents an error reduction of 5.6%, and the dif-
ference between these two accuracies is statisti-
cally significant at the p = 0.04 level.11
11All significance test results in this paper are obtained us-
ing Approximate Randomization (Noreen, 1989).
937
Results obtained when collective classification
alone is applied to the Baseline are shown in
the second column of Table 3. In this case, the
prior probability distribution over the semantic
subtypes that is needed to create the factor asso-
ciated with each node in the factor graph is sim-
ply the probabilistic classification of the test in-
stance that the node corresponds to. In compar-
ison to the Baseline, accuracy rises from 81.56
to 83.70. This represents an error reduction of
11.6%, and the difference is significant at the
p = 0.01 level. Also, applying collective clas-
sification to the Baseline yields slightly better re-
sults than applying hierarchical classification to
the Baseline, and the difference in their results is
significant at the p = 0.002 level.
Finally, results obtained when both hierarchi-
cal and collective classification are applied to the
Baseline are shown in the third column of Table
3. In this case, the prior distribution needed to
create the factor associated with each node in the
factor graph is provided by the supertype-specific
classification model that is used to classify the test
instance in hierarchical classification. In compar-
ison to the Baseline, accuracy rises from 81.56
to 85.08. This represents an error reduction of
19.1%, and the difference is highly significant
(p < 0.001). Also, applying both techniques to
the Baseline yields slightly better results than ap-
plying only collective classification to the Base-
line, and the difference in their results is signifi-
cant at the p = 0.003 level.
6.1 Feature Analysis
Next, we analyze the effects of the seven feature
types described in Section 3 on classification ac-
curacy. To measure feature performance, we take
the best-performing system (i.e., Baseline com-
bined with both techniques), begin with all seven
feature types, and iteratively remove them one by
one so that we get the best accuracy. The re-
sults are shown in Table 4. Across the top line,
we list the numbers representing the seven feature
classes. The feature class that corresponds to each
number can be found in Section 3, where they are
introduced. For instance, ?2? refers to the fea-
tures computed based on the governing verb. The
first row of results shows the system performance
1 3 7 4 2 5 6
81.4 75.8 83.3 83.7 84.1 85.2 85.6
80.4 74.9 84.3 85.3 85.3 86.1
80.4 78.3 83.9 86.5 86.7
81.8 76.2 85.2 87.6
75.4 83.4 84.6
66.2 80.9
Table 4: Results of feature analysis.
after removing just one feature class. In this
case, removing the sixth feature class (Gazetteers)
improves accuracy to 85.6, while removing the
mention string features reduces accuracy to 81.4.
The second row repeats this, after removing the
gazetteer features.
Somewhat surprisingly, using only mention
string, semantic, and grammatical features yields
the best accuracy (87.6). This indicates that
gazetteers, morphological features, capitalization,
and features computed based on the governing
verb are not useful. Removing the grammati-
cal features yields a 3% drop in accuracy. After
that, accuracy drops by 4% when semantic fea-
tures are removed, whereas a 18% drop in accu-
racy is observed when the mention string features
are removed. Hence, our analysis suggests that
the mention string features are the most useful fea-
tures for semantic subtype prediction.
7 Conclusions
We examined the under-studied problem of se-
mantic subtype induction, which involves clas-
sifying an NP as one of 92 semantic classes,
and showed that two techniques ? hierarchi-
cal classification and collective classification ?
can significantly improve a baseline classification
model trained using an off-the-shelf learning al-
gorithm on the BBN Entity Type Corpus. In par-
ticular, collective classification addresses a ma-
jor weakness of the standard feature-based learn-
ing paradigm, where a classification model classi-
fies each instance independently, failing to capture
the relationships among subsets of instances that
might improve classification accuracy. However,
collective classification has not been extensively
applied in the NLP community, and we hope that
our work can increase the awareness of this pow-
erful technique among NLP researchers.
938
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on an earlier draft of the pa-
per. This work was supported in part by NSF
Grant IIS-0812261.
References
Bikel, Daniel M., Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning: Special Issue on
Natural Language Learning, 34(1?3):211?231.
Bunescu, Razvan and Raymond J. Mooney. 2004.
Collective information extraction with relational
markov networks. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics, pages 483?445.
Finkel, Jenny Rose, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 363?370.
Florian, Radu, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A sta-
tistical model for multilingual entity detection and
tracking. In HLT-NAACL 2004: Main Proceedings,
pages 1?8.
Florian, Radu, Hongyan Jing, Nanda Kambhatla, and
Imed Zitouni. 2006. Factorizing complex mod-
els: A case study in mention detection. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 473?480.
Gale, William, Ken Church, and David Yarowsky.
1992. One sense per discourse. In Proceedings
of the 4th DARPA Speech and Natural Language
Workshop, pages 233?237.
Hovy, Eduard, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
Ontonotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages
57?60.
Huang, Zhiheng, Guangping Zeng, Weiqun Xu, and
Asli Celikyilmaz. 2009. Accurate semantic class
classifier for coreference resolution. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1232?1240.
Kschischang, Frank, Brendan J. Frey, and Hans-
Andrea Loeliger. 2001. Factor graphs and the sum-
product algorithm. IEEE Transactions on Informa-
tion Theory, 47:498?519.
Loeliger, Hans-Andrea. 2004. An introduction to
factor graphs. IEEE Signal Processing Magazine,
21(1):28?41.
Murphy, Kevin P., Yair Weiss, and Michael I. Jordan.
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In Proceedings of the
Fifteenth Conference on Uncertainty in Artificial In-
telligence, pages 467?475.
Ng, Vincent. 2007. Semantic class induction and
coreference resolution. In Proceedings of the 45th
Annual Meeting of the Association of Computa-
tional Linguistics, pages 536?543.
Nicolae, Cristina and Gabriel Nicolae. 2006. Best-
Cut: A graph algorithm for coreference resolution.
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
275?283.
Noreen, Eric W. 1989. Computer Intensive Methods
for Testing Hypothesis: An Introduction. John Wi-
ley & Sons.
Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency net-
work. In HLT-NAACL 2003: Proceedings of the
Main Conference, pages 173?180.
Weischedel, Ralph and Ada Brunstein. 2005. BBN
pronoun coreference and entity type corpus. In Lin-
guistic Data Consortium, Philadelphia.
939
Coling 2010: Poster Volume, pages 365?373,
Beijing, August 2010
Conundrums in Unsupervised Keyphrase Extraction:
Making Sense of the State-of-the-Art
Kazi Saidul Hasan and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
{saidul,vince}@hlt.utdallas.edu
Abstract
State-of-the-art approaches for unsuper-
vised keyphrase extraction are typically
evaluated on a single dataset with a single
parameter setting. Consequently, it is un-
clear how effective these approaches are
on a new dataset from a different domain,
and how sensitive they are to changes in
parameter settings. To gain a better under-
standing of state-of-the-art unsupervised
keyphrase extraction algorithms, we con-
duct a systematic evaluation and analysis
of these algorithms on a variety of stan-
dard evaluation datasets.
1 Introduction
The keyphrases for a given document refer to a
group of phrases that represent the document. Al-
though we often come across texts from different
domains such as scientific papers, news articles
and blogs, which are labeled with keyphrases by
the authors, a large portion of the Web content re-
mains untagged. While keyphrases are excellent
means for providing a concise summary of a doc-
ument, recent research results have suggested that
the task of automatically identifying keyphrases
from a document is by no means trivial. Re-
searchers have explored both supervised and un-
supervised techniques to address the problem of
automatic keyphrase extraction. Supervised meth-
ods typically recast this problem as a binary clas-
sification task, where a model is trained on anno-
tated data to determine whether a given phrase is
a keyphrase or not (e.g., Frank et al (1999), Tur-
ney (2000; 2003), Hulth (2003), Medelyan et al
(2009)). A disadvantage of supervised approaches
is that they require a lot of training data and yet
show bias towards the domain on which they are
trained, undermining their ability to generalize
well to new domains. Unsupervised approaches
could be a viable alternative in this regard.
The unsupervised approaches for keyphrase ex-
traction proposed so far have involved a number
of techniques, including language modeling (e.g.,
Tomokiyo and Hurst (2003)), graph-based rank-
ing (e.g., Zha (2002), Mihalcea and Tarau (2004),
Wan et al (2007), Wan and Xiao (2008), Liu
et al (2009a)), and clustering (e.g., Matsuo and
Ishizuka (2004), Liu et al (2009b)). While these
methods have been shown to work well on a par-
ticular domain of text such as short paper abstracts
and news articles, their effectiveness and portabil-
ity across different domains have remained an un-
explored issue. Worse still, each of them is based
on a set of assumptions, which may only hold for
the dataset on which they are evaluated.
Consequently, we have little understanding of
how effective the state-of the-art systems would be
on a completely new dataset from a different do-
main. A few questions arise naturally. How would
these systems perform on a different dataset with
their original configuration? What could be the
underlying reasons in case they perform poorly?
Is there any system that can generalize fairly well
across various domains?
We seek to gain a better understanding of the
state of the art in unsupervised keyphrase ex-
traction by examining the aforementioned ques-
tions. More specifically, we compare five unsu-
pervised keyphrase extraction algorithms on four
corpora with varying domains and statistical char-
acteristics. These algorithms represent the ma-
365
jor directions in this research area, including Tf-
Idf and four recently proposed systems, namely,
TextRank (Mihalcea and Tarau, 2004), SingleR-
ank (Wan and Xiao, 2008), ExpandRank (Wan
and Xiao, 2008), and a clustering-based approach
(Liu et al, 2009b). Since none of these systems
(except TextRank) are publicly available, we re-
implement all of them and make them freely avail-
able for research purposes.1 To our knowledge,
this is the first attempt to compare the perfor-
mance of state-of-the-art unsupervised keyphrase
extraction systems on multiple datasets.
2 Corpora
Our four evaluation corpora belong to different
domains with varying document properties. Ta-
ble 1 provides an overview of each corpus.
The DUC-2001 dataset (Over, 2001), which is
a collection of 308 news articles, is annotated by
Wan and Xiao (2008). We report results on all 308
articles in our evaluation.
The Inspec dataset is a collection of 2,000 ab-
stracts from journal papers including the paper ti-
tle. Each document has two sets of keyphrases as-
signed by the indexers: the controlled keyphrases,
which are keyphrases that appear in the In-
spec thesaurus; and the uncontrolled keyphrases,
which do not necessarily appear in the thesaurus.
This is a relatively popular dataset for automatic
keyphrase extraction, as it was first used by Hulth
(2003) and later by Mihalcea and Tarau (2004)
and Liu et al (2009b). In our evaluation, we use
the set of 500 abstracts designated by these previ-
ous approaches as the test set and its set of uncon-
trolled keyphrases. Note that the average docu-
ment length for this dataset is the smallest among
all our datasets.
The NUS Keyphrase Corpus (Nguyen and
Kan, 2007) includes 211 scientific conference pa-
pers with lengths between 4 to 12 pages. Each
paper has one or more sets of keyphrases assigned
by its authors and other annotators. We use all the
211 papers in our evaluation. Since the number
of annotators can be different for different docu-
ments and the annotators are not specified along
with the annotations, we decide to take the union
1See http://www.hlt.utdallas.edu/
?
saidul/code.html for details.
of all the gold standard keyphrases from all the
sets to construct one single set of annotation for
each paper. As Table 1 shows, each NUS pa-
per, both in terms of the average number of to-
kens (8291) and candidate phrases (2027) per pa-
per, is more than five times larger than any doc-
ument from any other corpus. Hence, the num-
ber of candidate keyphrases that can be extracted
is potentially large, making this corpus the most
challenging of the four.
Finally, the ICSI meeting corpus (Janin et al,
2003), which is annotated by Liu et al (2009a),
includes 161 meeting transcriptions. Following
Liu et al, we remove topic segments marked as
?chitchat? and ?digit? from the dataset and use all
the remaining segments for evaluation. Each tran-
script contains three sets of keyphrases produced
by the same three human annotators. Since it is
possible to associate each set of keyphrases with
its annotator, we evaluate each system on this
dataset three times, once for each annotator, and
report the average score. Unlike the other three
datasets, the gold standard keys for the ICSI cor-
pus are mostly unigrams.
3 Unsupervised Keyphrase Extractors
A generic unsupervised keyphrase extraction sys-
tem typically operates in three steps (Section 3.1),
which will help understand the unsupervised sys-
tems explained in Section 3.2.
3.1 Generic Keyphrase Extractor
Step 1: Candidate lexical unit selection The
first step is to filter out unnecessary word to-
kens from the input document and generate a list
of potential keywords using heuristics. Com-
monly used heuristics include (1) using a stop
word list to remove non-keywords (e.g., Liu et al
(2009b)) and (2) allowing words with certain part-
of-speech tags (e.g., nouns, adjectives, verbs) to
be considered candidate keywords (Mihalcea and
Tarau (2004), Liu et al (2009a), Wan and Xiao
(2008)). In all of our experiments, we follow Wan
and Xiao (2008) and select as candidates words
with the following Penn Treebank tags: NN, NNS,
NNP, NNPS, and JJ, which are obtained using the
Stanford POS tagger (Toutanova and Manning,
2000).
366
Corpora
DUC-2001 Inspec NUS ICSI
Type News articles Paper abstracts Full papers Meeting transcripts
# Documents 308 500 211 161
# Tokens/Document 876 134 8291 1611
# Candidate words/Document 312 57 3271 453
# Candidate phrases/Document 207 34 2027 296
# Tokens/Candidate phrase 1.5 1.7 1.6 1.5
# Gold keyphrases 2484 4913 2327 582
# Gold keyphrases/Document 8.1 9.8 11.0 3.6
U/B/T/O distribution (%) 17/61/18/4 13/53/25/9 27/50/16/7 68/29/2/1
# Tokens/Gold keyphrase 2.1 2.3 2.1 1.3
Table 1: Corpus statistics for the four datasets used in this paper. A candidate word/phrase, typically a sequence
of one or more adjectives and nouns, is extracted from the document initially and considered a potential keyphrase. The
U/B/T/O distribution indicates how the gold standard keys are distributed among unigrams, bigrams, trigrams, and other
higher order n-grams.
Step 2: Lexical unit ranking Once the can-
didate list is generated, the next task is to rank
these lexical units. To accomplish this, it is nec-
essary to build a representation of the input text
for the ranking algorithm. Depending on the un-
derlying approach, each candidate word is repre-
sented by its syntactic and/or semantic relation-
ship with other candidate words. The relationship
can be defined using co-occurrence statistics, ex-
ternal resources (e.g., neighborhood documents,
Wikipedia), or other syntactic clues.
Step 3: Keyphrase formation In the final step,
the ranked list of candidate words is used to form
keyphrases. A candidate phrase, typically a se-
quence of nouns and adjectives, is selected as a
keyphrase if (1) it includes one or more of the
top-ranked candidate words (Mihalcea and Tarau
(2004), Liu et al (2009b)), or (2) the sum of the
ranking scores of its constituent words makes it a
top scoring phrase (Wan and Xiao, 2008).
3.2 The Five Keyphrase Extractors
As mentioned above, we re-implement five unsu-
pervised approaches for keyphrase extraction. Be-
low we provide a brief overview of each system.
3.2.1 Tf-Idf
Tf-Idf assigns a score to each term t in a doc-
ument d based on t?s frequency in d (term fre-
quency) and how many other documents include
t (inverse document frequency) and is defined as:
tfidft = tft ? log(D/Dt) (1)
where D is the total number of documents and Dt
is the number of documents containing t.
Given a document, we first compute the Tf-
Idf score of each candidate word (see Step 1 of
the generic algorithm). Then, we extract all the
longest n-grams consisting of candidate words
and score each n-gram by summing the Tf-Idf
scores of its constituent unigrams. Finally, we out-
put the top N n-grams as keyphrases.
3.2.2 TextRank
In the TextRank algorithm (Mihalcea and Ta-
rau, 2004), a text is represented by a graph. Each
vertex corresponds to a word type. A weight,
wij , is assigned to the edge connecting the two
vertices, vi and vj , and its value is the number
of times the corresponding word types co-occur
within a window of W words in the associated
text. The goal is to (1) compute the score of each
vertex, which reflects its importance, and then (2)
use the word types that correspond to the highest-
scored vertices to form keyphrases for the text.
The score for vi, S(vi), is initialized with a de-
fault value and is computed in an iterative manner
until convergence using this recursive formula:
S(vi) = (1? d) + d?
?
vj?Adj(vi)
wji?
vk?Adj(vj)
wjk
S(vj)
(2)
where Adj(vi) denotes vi?s neighbors and d is the
damping factor set to 0.85 (Brin and Page, 1998).
Intuitively, a vertex will receive a high score if it
has many high-scored neighbors. As noted before,
after convergence, the T% top-scored vertices are
367
selected as keywords. Adjacent keywords are then
collapsed and output as a keyphrase.
According to Mihalcea and Tarau (2004), Tex-
tRank?s best score on the Inspec dataset is
achieved when only nouns and adjectives are used
to create a uniformly weighted graph for the text
under consideration, where an edge connects two
word types only if they co-occur within a window
of two words. Hence, our implementation of Tex-
tRank follows this configuration.
3.2.3 SingleRank
SingleRank (Wan and Xiao, 2008) is essen-
tially a TextRank approach with three major dif-
ferences. First, while each edge in a TextRank
graph (in Mihalcea and Tarau?s implementation)
has the same weight, each edge in a SingleRank
graph has a weight equal to the number of times
the two corresponding word types co-occur. Sec-
ond, while in TextRank only the word types that
correspond to the top-ranked vertices can be used
to form keyphrases, in SingleRank, we do not fil-
ter out any low-scored vertices. Rather, we (1)
score each candidate keyphrase, which can be any
longest-matching sequence of nouns and adjec-
tives in the text under consideration, by summing
the scores of its constituent word types obtained
from the SingleRank graph, and (2) output the N
highest-scored candidates as the keyphrases for
the text. Finally, SingleRank employs a window
size of 10 rather than 2.
3.2.4 ExpandRank
ExpandRank (Wan and Xiao, 2008) is a
TextRank extension that exploits neighborhood
knowledge for keyphrase extraction. For a given
document d, the approach first finds its k near-
est neighboring documents from the accompany-
ing document collection using a similarity mea-
sure (e.g., cosine similarity). Then, the graph for
d is built using the co-occurrence statistics of the
candidate words collected from the document it-
self and its k nearest neighbors.
Specifically, each document is represented by
a term vector where each vector dimension cor-
responds to a word type present in the document
and its value is the Tf-Idf score of that word type
for that document. For a given document d0, its k
nearest neighbors are identified, and together they
form a larger document set of k+1 documents,
D = {d0, d1, d2, ..., dk}. Given this document
set, a graph is constructed, where each vertex cor-
responds to a candidate word type in D, and each
edge connects two vertices vi and vj if the corre-
sponding word types co-occur within a window of
W words in the document set. The weight of an
edge, w(vi, vj), is computed as follows:
w(vi, vj) =
?
dk?D
sim(d0, dk)? freqdk(vi, vj) (3)
where sim(d0, dk) is the cosine similarity be-
tween d0 and dk, and freqdk(vi, vj) is the co-
occurrence frequency of vi and vj in document dk.
Once the graph is constructed, the rest of the pro-
cedure is identical to SingleRank.
3.2.5 Clustering-based Approach
Liu et al (2009b) propose to cluster candidate
words based on their semantic relationship to en-
sure that the extracted keyphrases cover the en-
tire document. The objective is to have each clus-
ter represent a unique aspect of the document and
take a representative word from each cluster so
that the document is covered from all aspects.
More specifically, their algorithm (henceforth
referred to as KeyCluster) first filters out the stop
words from a given document and treats the re-
maining unigrams as candidate words. Second,
for each candidate, its relatedness with another
candidate is computed by (1) counting how many
times they co-occur within a window of size W
in the document and (2) using Wikipedia-based
statistics. Third, candidate words are clustered
based on their relatedness with other candidates.
Three clustering algorithms are used of which
spectral clustering yields the best score. Once
the clusters are formed, one representative word,
called an exemplar term, is picked from each clus-
ter. Finally, KeyCluster extracts from the docu-
ment all the longest n-grams starting with zero
or more adjectives and ending with one or more
nouns, and if such an n-gram includes one or more
exemplar words, it is selected as a keyphrase. As
a post-processing step, a frequent word list gener-
ated from Wikipedia is used to filter out the fre-
quent unigrams that are selected as keyphrases.
368
4 Evaluation
4.1 Experimental Setup
TextRank and SingleRank setup Following
Mihalcea and Tarau (2004) and Wan and Xiao
(2008), we set the co-occurrence window size for
TextRank and SingleRank to 2 and 10, respec-
tively, as these parameter values have yielded the
best results for their evaluation datasets.
ExpandRank setup Following Wan and Xiao
(2008), we find the 5 nearest neighbors for each
document from the remaining documents in the
same corpus. The other parameters are set in the
same way as in SingleRank.
KeyCluster setup As argued by Liu et al
(2009b), Wikipedia-based relatedness is computa-
tionally expensive to compute. As a result, we fol-
low them by computing the co-occurrence-based
relatedness instead, using a window of size 10.
Then, we cluster the candidate words using spec-
tral clustering, and use the frequent word list that
they generously provided us to post-process the
resulting keyphrases by filtering out those that are
frequent unigrams.
4.2 Results and Discussion
In an attempt to gain a better insight into the
five unsupervised systems, we report their perfor-
mance in terms of precision-recall curves for each
of the four datasets (see Figure 1). This contrasts
with essentially all previous work, where the per-
formance of a keyphrase extraction system is re-
ported in terms of an F-score obtained via a par-
ticular parameter setting on a particular dataset.
We generate the curves for each system as fol-
lows. For Tf-Idf, SingleRank, and ExpandRank,
we vary the number of keyphrases, N , predicted
by each system. For TextRank, instead of vary-
ing the number of predicted keyphrases, we vary
T , the percentage of top-scored vertices (i.e., un-
igrams) that are selected as keywords at the end
of the ranking step. The reason is that TextRank
only imposes a ranking on the unigrams but not
on the keyphrases generated from the high-ranked
unigrams. For KeyCluster, we vary the number
of clusters produced by spectral clustering rather
than the number of predicted keyphrases, again
because KeyCluster does not impose a ranking on
the resulting keyphrases. In addition, to give an
estimate of how each system performs in terms of
F-score, we also plot curves corresponding to dif-
ferent F-scores in these graphs.
Tf-Idf Consistent with our intuition, the preci-
sion of Tf-Idf drops as recall increases. Although
it is the simplest of the five approaches, Tf-Idf is
the best performing system on all but the Inspec
dataset, where TextRank and KeyCluster beat Tf-
Idf on just a few cases. It clearly outperforms all
other systems for NUS and ICSI.
TextRank The TextRank curves show a differ-
ent progression than Tf-Idf: precision does not
drop as much when recall increases. For instance,
in case of DUC and ICSI, precision is not sensi-
tive to changes in recall. Perhaps somewhat sur-
prisingly, its precision increases with recall for In-
spec, allowing it to even reach a point (towards
the end of the curve) where it beats Tf-Idf. While
additional experiments are needed to determine
the reason for this somewhat counter-intuitive re-
sult, we speculate that this may be attributed to
the fact that the TextRank curves are generated
by progressively increasing T (i.e., the percent-
age of top-ranked vertices/unigrams that are used
to generate keyphrases) rather than the number of
predicted keyphrases, as mentioned before. In-
creasing T does not necessarily imply an increase
in the number of predicted keyphrases, however.
To see the reason, consider an example in which
we want TextRank to extract the keyphrase ?ad-
vanced machine learning? for a given document.
Assume that TextRank ranks the unigrams ?ad-
vanced?, ?learning?, and ?machine? first, second,
and third, respectively in its ranking step. When
T = 2n , where n denotes the total number of
candidate unigrams, only the two highest-ranked
unigrams (i.e., ?advanced? and ?learning?) can
be used to form keyphrases. This implies that
?advanced? and ?learning? will each be predicted
as a keyphrase, but ?advanced machine learning?
will not. However, when T = 3n , all three un-
igrams can be used to form a keyphrase, and
since TextRank collapses unigrams adjacent to
each other in the text to form a keyphrase, it will
correctly predict ?advanced machine learning? as
a keyphrase. Note that as we increase T from 2n
to 3n , recall increases, and so does precision, since
369
 0
 10
 20
 30
 40
 50
 0  20  40  60  80  100
Pr
ec
isi
on
 (%
)
Recall (%)
F=10
F=20
F=30
F=40
Tf-Idf
TextRank
SingleRank
ExpandRank
KeyCluster
(a) DUC
 0
 10
 20
 30
 40
 50
 0  20  40  60  80  100
Pr
ec
isi
on
 (%
)
Recall (%)
F=10
F=20
F=30
F=40
Tf-Idf
TextRank
SingleRank
ExpandRank
KeyCluster
(b) Inspec
 0
 2
 4
 6
 8
 10
 0  20  40  60  80  100
Pr
ec
isi
on
 (%
)
Recall (%)
F=10
Tf-Idf
TextRank
SingleRank
ExpandRank
KeyCluster
(c) NUS
 0
 2
 4
 6
 8
 10
 0  20  40  60  80  100
Pr
ec
isi
on
 (%
)
Recall (%)
F=10
Tf-Idf
TextRank
SingleRank
ExpandRank
KeyCluster
(d) ICSI
Figure 1: Precision-recall curves for all four datasets
?advanced? and ?learning? are now combined to
form one keyphrase (and hence the number of pre-
dicted keyphrases decreases). In other words, it
is possible to see a simultaneous rise in precision
and recall in a TextRank curve. A natural ques-
tion is: why does is happen only for Inspec but
not the other datasets? The reason could be at-
tributed to the fact that Inspec is composed of ab-
stracts: since the number of keyphrases that can be
generated from these short documents is relatively
small, precision may not drop as severely as with
the other datasets even when all of the unigrams
are used to form keyphrases.
On average, TextRank performs much worse
compared to Tf-Idf. The curves also prove Tex-
tRank?s sensitivity to T on Inspec, but not on the
other datasets. This certainly gives more insight
into TextRank since it was evaluated on Inspec
only for T=33% by Mihalcea and Tarau (2004).
SingleRank SingleRank, which is supposed to
be a simple variant of TextRank, surprisingly ex-
hibits very different performance. First, it shows
a more intuitive nature: precision drops as recall
increases. Second, SingleRank outperforms Tex-
tRank by big margins on all the datasets. Later,
we will examine which of the differences between
them is responsible for the differing performance.
370
DUC Inspec NUS ICSI
Parameter F Parameter F Parameter F Parameter F
Tf-Idf N = 14 27.0 N = 14 36.3 N = 60 6.6 N = 9 12.1
TextRank T = 100% 9.7 T = 100% 33.0 T = 5% 3.2 T = 25% 2.7
SingleRank N = 16 25.6 N = 15 35.3 N = 190 3.8 N = 50 4.4
ExpandRank N = 13 26.9 N = 15 35.3 N = 177 3.8 N = 51 4.3
KeyCluster m = 0.9n 14.0 m = 0.9n 40.6 m = 0.25n 1.7 m = 0.9n 3.2
Table 2: Best parameter settings. N is the number of predicted keyphrases, T is the percentage of vertices selected as
keywords in TextRank, m is the number of clusters in KeyCluster, expressed in terms of n, the fraction of candidate words.
ExpandRank Consistent with Wan and Xiao
(2008), ExpandRank beats SingleRank on DUC
when a small number of phrases are predicted, but
their difference diminishes as more phrases are
predicted. On the other hand, their performance
is indistinguishable from each other on the other
three datasets. A natural question is: why does
ExpandRank improve over SingleRank only for
DUC but not for the other datasets? To answer
this question, we look at the DUC articles and
find that in many cases, the 5-nearest neighbors
of a document are on the same topic involving the
same entities as the document itself, presumably
because many of these news articles are simply
updated versions of an evolving event. Conse-
quently, the graph built from the neighboring doc-
uments is helpful for predicting the keyphrases of
the given document. Such topic-wise similarity
among the nearest documents does not exist in the
other datasets, however.
KeyCluster As in TextRank, KeyCluster does
not always yield a drop in precision as recall im-
proves. This, again, may be attributed to the fact
that the KeyCluster curves are generated by vary-
ing the number of clusters rather than the num-
ber of predicted keyphrases, as well as the way
keyphrases are formed from the exemplars. An-
other reason is that the frequent Wikipedia uni-
grams are excluded during post-processing, mak-
ing KeyCluster more resistant to precision drops.
Overall, KeyCluster performs slightly better than
TextRank on DUC and ICSI, yields the worst per-
formance on NUS, and scores the best on Inspec
when the number of clusters is high. These results
seem to suggest that KeyCluster works better if
more clusters are used.
Best parameter settings Table 2 shows for each
system the parameter values yielding the best F-
score on each dataset. Two points deserve men-
tion. First, in comparison to SingleRank and
ExpandRank, Tf-Idf outputs fewer keyphrases to
achieve its best F-score on most datasets. Second,
the systems output more keyphrases on NUS than
on other datasets to achieve their best F-scores
(e.g., 60 for Tf-Idf, 190 for SingleRank, and 177
for ExpandRank). This can be attributed in part to
the fact that the F-scores on NUS are low for all
the systems and exhibit only slight changes as we
output more phrases.
Our re-implementations Do our duplicated
systems yield scores that match the original
scores? Table 3 sheds light on this question.
First, consider KeyCluster, where our score
lags behind the original score by approximately
5%. An examination of Liu et al?s (2009b) re-
sults reveals a subtle caveat in keyphrase extrac-
tion evaluations. In Inspec, not all gold-standard
keyphrases appear in their associated document,
and as a result, none of the five systems we con-
sider in this paper can achieve a recall of 100.
While Mihalcea and Tarau (2004) and our re-
implementations use all of these gold-standard
keyphrases in our evaluation, Hulth (2003) and
Liu et al address this issue by using as gold-
standard keyphrases only those that appear in the
corresponding document when computing recall.2
This explains why our KeyCluster score (38.9) is
lower than the original score (43.6). If we fol-
low Liu et al?s way of computing recall, our re-
implementation score goes up to 42.4, which lags
behind their score by only 1.2.
Next, consider TextRank, where our score lags
behind Mihalcea and Tarau?s original score by
more than 25 points. We verified our implemen-
tation against a publicly available implementation
2As a result, Liu et al and Mihalcea and Tarau?s scores
are not directly comparable, but Liu et al did not point this
out while comparing scores in their paper.
371
Dataset F-scoreOriginal Ours
Tf-Idf DUC 25.4 25.7
TextRank Inspec 36.2 10.0
SingleRank DUC 27.2 24.9
ExpandRank DUC 31.7 26.4
KeyCluster Inspec 43.6 38.9
Table 3: Original vs. re-implementation scores
of TextRank3, and are confident that our imple-
mentation is correct. It is also worth mentioning
that using our re-implementation of SingleRank,
we are able to match the best scores reported by
Mihalcea and Tarau (2004) on Inspec.
We score 2 and 5 points less than Wan and
Xiao?s (2008) implementations of SingleRank and
ExpandRank, respectively. We speculate that doc-
ument pre-processing (e.g., stemming) has con-
tributed to the discrepancy, but additional exper-
iments are needed to determine the reason.
SingleRank vs. TextRank Figure 1 shows that
SingleRank behaves very differently from Tex-
tRank. As mentioned in Section 3.2.3, the two
algorithms differ in three major aspects. To de-
termine which aspect is chiefly responsible for the
large difference in their performance, we conduct
three ?ablation? experiments. Each experiment
modifies exactly one of these aspects in SingleR-
ank so that it behaves like TextRank, effectively
ensuring that the two algorithms differ only in the
remaining two aspects. More specifically, in the
three experiments, we (1) change SingleRank?s
window size to 2, (2) build an unweighted graph
for SingleRank, and (3) incorporate TextRank?s
way of forming keyphrases into SingleRank, re-
spectively. Figure 2 shows the resultant curves
along with the SingleRank and TextRank curves
on Inspec taken from Figure 1b. As we can see,
the way of forming phrases, rather than the win-
dow size or the weight assignment method, has
the largest impact on the scores. In fact, after in-
corporating TextRank?s way of forming phrases,
SingleRank exhibits a remarkable drop in perfor-
mance, yielding a curve that resembles the Tex-
tRank curve. Also note that SingleRank achieves
better recall values than TextRank. To see the rea-
son, recall that TextRank requires that every word
of a gold keyphrase must appear among the top-
3http://github.com/sharethis/textrank
 0
 10
 20
 30
 40
 50
 0  20  40  60  80  100
Pr
ec
isi
on
 (%
)
Recall (%)
F=10
F=20
F=30
F=40
SingleRank
SingleRank+Window size=2
SingleRank+Unweighted
SingleRank+TextRank phrases
TextRank
Figure 2: Ablation results for SingleRank on In-
spec
ranked unigrams. This is a fairly strict criterion,
especially in comparison to SingleRank, which
does not require all unigrams of a gold keyphrase
to be present in the top-ranked list. We observe
similar trends for the other datasets.
5 Conclusions
We have conducted a systematic evaluation of five
state-of-the-art unsupervised keyphrase extraction
algorithms on datasets from four different do-
mains. Several conclusions can be drawn from
our experimental results. First, to fully under-
stand the strengths and weaknesses of a keyphrase
extractor, it is essential to evaluate it on multi-
ple datasets. In particular, evaluating it on a sin-
gle dataset has proven inadequate, as good per-
formance can sometimes be achieved due to cer-
tain statistical characteristics of a dataset. Sec-
ond, as demonstrated by our experiments with
TextRank and SingleRank, post-processing steps
such as the way of forming keyphrases can have
a large impact on the performance of a keyphrase
extractor. Hence, it may be worthwhile to investi-
gate alternative methods for extracting candidate
keyphrases (e.g., Kumar and Srinathan (2008),
You et al (2009)). Finally, despite the large
amount of recent work on unsupervised keyphrase
extractor, our results indicated that Tf-Idf remains
a strong baseline, offering very robust perfor-
mance across different datasets.
372
Acknowledgments
We thank the three anonymous reviewers for their
comments. Many thanks to Anette Hulth and
Yang Liu for providing us with the Inspec and
ICSI datasets; Rada Mihalcea, Paco Nathan, and
Xiaojun Wan for helping us understand their al-
gorithms/implementations; and Peng Li for pro-
viding us with the frequent word list that he and
his co-authors used in their paper. This work was
supported in part by NSF Grant IIS-0812261.
References
Brin, Sergey and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks, 30(1?7):107?117.
Frank, Eibe, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence, pages 668?673.
Hulth, Anette. 2003. Improved automatic keyword
extraction given more linguistic knowledge. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, pages
216?223.
Janin, Adam, Don Baron, Jane Edwards, Dan El-
lis, David Gelbart, Nelson Morgan, Barbara Pe-
skin, Thilo Pfau, Elizabeth Shriberg, Andreas Stol-
cke, and Chuck Wooters. 2003. The ICSI meeting
corpus. In Proceedings of 2003 IEEE Conference
on Acoustics, Speech, and Signal Processing, pages
364?367.
Kumar, Niraj and Kannan Srinathan. 2008. Automatic
keyphrase extraction from scientific documents us-
ing n-gram filtration technique. In Proceedings of
the Eighth ACM Symposium on Document Engi-
neering, pages 199?208.
Liu, Feifan, Deana Pennell, Fei Liu, and Yang Liu.
2009a. Unsupervised approaches for automatic
keyword extraction using meeting transcripts. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 620?628.
Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong
Sun. 2009b. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 257?266.
Matsuo, Yutaka and Mitsuru Ishizuka. 2004. Key-
word extraction from a single document using word
co-occurrence statistical information. International
Journal on Artificial Intelligence Tools, 13(1):157?
169.
Medelyan, Olena, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1318?1327.
Mihalcea, Rada and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 404?411.
Nguyen, Thuy Dung and Min-Yen Kan. 2007.
Keyphrase extraction in scientific publications. In
Proceedings of the International Conference on
Asian Digital Libraries, pages 317?326.
Over, Paul. 2001. Introduction to DUC-2001: An in-
trinsic evaluation of generic news text summariza-
tion systems. In Proceedings of the 2001 Document
Understanding Conference.
Tomokiyo, Takashi and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL Workshop on Multiword Ex-
pressions.
Toutanova, Kristina and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Pro-
ceedings of the 2000 Joint SIGDAT Conference on
Empirical Methods in Natural Language processing
and Very Large Corpora, pages 63?70.
Turney, Peter. 2000. Learning algorithms for
keyphrase extraction. Information Retrieval,
2:303?336.
Turney, Peter. 2003. Coherent keyphrase extraction
via web mining. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence,
pages 434?439.
Wan, Xiaojun and Jianguo Xiao. 2008. Single
document keyphrase extraction using neighborhood
knowledge. In Proceedings of the 23rd AAAI Con-
ference on Artificial Intelligence, pages 855?860.
Wan, Xiaojun, Jianwu Yang, and Jianguo Xiao. 2007.
Towards an iterative reinforcement approach for si-
multaneous document summarization and keyword
extraction. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 552?559.
You, Wei, Dominique Fontaine, and Jean-Paul Barthe`s.
2009. Automatic keyphrase extraction with a re-
fined candidate set. In Proceedings of the 2009
IEEE/WIC/ACM International Joint Conference on
Web Intelligence and Intelligent Agent Technology,
pages 576?579.
Zha, Hongyuan. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement
principle and sentence clustering. In Proceedings of
the 25th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 113?120.
373
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1682?1693, Dublin, Ireland, August 23-29 2014.
Ensemble-Based Medical Relation Classification
Jennifer D?Souza and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{jld082000,vince}@hlt.utdallas.edu
Abstract
Despite the successes of distant supervision approaches to relation extraction in the news do-
main, the lack of a comprehensive ontology of medical relations makes it difficult to apply such
approaches to relation classification in the medical domain. In light of this difficulty, we propose
an ensemble approach to this task where we exploit human-supplied knowledge to guide the de-
sign of members of the ensemble. Results on the 2010 i2b2/VA Challenge corpus show that our
ensemble approach yields a 19.8% relative error reduction over a state-of-the-art baseline.
1 Introduction
Medical relation (MR) classification, an information extraction task in the clinical domain that was re-
cently defined in the 2010 i2b2/VA Challenge (Uzuner et al., 2011), involves determining the relation
between a pair of medical concepts (problems, treatments, or tests). The ability to classify MRs is indis-
pensable to sound automatic analysis of patient health records.
While MR classification is a relatively new task, there has been a lot of work on extracting semantic
relations from news articles. Supervised approaches train classifiers on data annotated with the target
relation types, typically using a rich feature set (Zhou et al., 2005; Surdeanu and Ciaramita, 2007; Zhou et
al., 2007). Since obtaining annotated data is a time-consuming and labor-intensive process, researchers
have considered unsupervised approaches (Shinyama and Sekine, 2006; Banko et al., 2007). While
unsupervised approaches can use a large amount of unannotated data and extract a large number of
relations, it may not be easy to map the resulting relations to those needed for a given knowledge base.
One way to mitigate this problem is semi-supervised learning: starting from a given set of seed instances,
a bootstrapping algorithm is used to iteratively learn extraction patterns and extract instances (Brin,
1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et
al., 2005; Pantel and Pennacchiotti, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).
However, the resulting patterns often suffer from semantic drift and low precision. Recent years have
seen a surge of interest in distant supervision for relation extraction (Mintz et al., 2009; Nguyen and
Moschitti, 2011; Krause et al., 2012; Min et al., 2013). The idea is to automatically create annotated
relation instances by extracting their labels from relation instances in a knowledge base such as Freebase
(Bollacker et al., 2008) and YAGO (Suchanek et al., 2007).
Our goal in this paper is to advance the state of the art in MR classification. One of the major chal-
lenges in MR classification is the scarcity of labeled data. At first glance, we can mitigate this problem
using distant supervision approaches. However, there is difficulty in applying these approaches to MR
classification: only one of the relation types defined in the 2010 i2b2 Challenge is represented in the
Unified Medical Language System
1
, the most comprehensive medical ontology available to date.
In light of this difficulty, we propose an ensemble approach to MR classification, where we exploit
human-supplied knowledge to guide the design of different members of the ensemble. Unlike state-
of-the-art supervised approaches to this task, which represent contextual information largely as flat (i.e.,
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
www.nlm.nih.gov/research/umls/
1682
discrete- or real-valued) features (de Bruijn et al., 2011; Rink et al., 2011) or structured tree features (Zhu
et al., 2013), we represent contexts as sequences, specifically word sequences and dependency sequences,
and use them to derive lexical and dependency patterns. Our ensemble approach exploits human-supplied
knowledge in three ways. First, while existing approaches employ similarity functions already defined
in off-the-shelf learning algorithms (e.g., linear kernel (Rink et al., 2011), tree kernel (Zhu et al., 2013))
to compute the similarity between two relation instances, we define functions to compare the similarity
between two patterns. Second, to complement the automatically induced patterns, we hand-craft patterns
based on manual observations made on the training set, specifically by having a human identify the
contexts of two concepts that are strongly indicative of a medical relation class. Finally, we employ
human knowledge to identify the constraints on the classification of different relation instances, and
enforce the resulting constraints in an integer linear programming (ILP) framework. Evaluation results
on the 2010 i2b2/VA Challenge corpus (henceforth the i2b2 corpus) show that our ensemble approach
yields a 19.8% relative error reduction over a state-of-the-art system.
The rest of the paper is organized as follows. Section 2 provides an overview of the i2b2 corpus. Sec-
tion 3 describes the baseline systems. Sections 4 and 5 describe our new components and our ensemble
approach. Section 6 discusses our constraints for enforcing global consistency. We present evaluation
results in Section 7, conduct an error analysis in Section 8, and conclude in Section 9.
2 Corpus
For evaluation, we use the i2b2 corpus, which comprises 426 de-identified discharge summaries. We
adopt the i2b2 organizers? partition of the 426 summaries into a training set (170 summaries) and a test
set (256 summaries). As many of the algorithms in our approach require parameter tuning, we reserve
30 of the 170 summaries in the training set for development purposes.
In each discharge summary, the concepts and the medical relation between each pair of concepts
are marked up. Each concept is annotated with a type attribute that indicates whether it is a TEST,
a PROBLEM, or a TREATMENT. In addition, a PROBLEM concept has an assertion attribute, which
specifies whether the problem was present, absent, or possible in the patient, conditionally present in
the patient under certain circumstances, hypothetically present in the patient at some future point, or
mentioned in the patient report but associated with someone other than the patient.
Eleven types of intra-sentential pairwise relations are annotated. A brief description of these relation
types and the relevant statistics are provided in Table 1. As we can see from the table, each medical
relation has a type and is defined only on intra-sentence TREATMENT-PROBLEM, TEST-PROBLEM, and
PROBLEM-PROBLEM concept pairs. Also, while there are 11 relation types, three of them, namely
Relations 6, 9, and 11, denote the absence of a medical relation between the corresponding concepts.
The purpose of having ?no relation? classes is to ensure that every pair of TEST/PROBLEM/TREATMENT
concepts is annotated, whether or not a medical relation exists between them.
3 Baseline MR Classification Systems
We employ two supervised MR classification systems as baselines. The first baseline is a state-of-the-art
system that achieved the best performance in the official 2010 i2b2 evaluation. The second baseline is a
tree kernel-based system, motivated by the fact that tree kernels are frequently used in relation extraction
(e.g., Zhou et al. (2007), Zhu et al. (2013)).
3.1 SVM with Flat Features
Our first baseline, Rink et al.?s (2011) system, employs an SVM classifier trained on a set of flat features
(i.e., features that are discrete- or real-valued).
Following Rink et al., we create training instances as follows. First, we form training instances be-
tween every pair of (PROBLEM, TEST, and TREATMENT) concepts in the training documents, labeling
an instance with its relation type. Since the instances belonging to the three ?no relation? classes signifi-
cantly outnumber those belonging to the remaining eight classes, we reduce data skew by downsampling
1683
Id Relation Example Total (%)
1 TrIP: Treatment improves medical problem Her pain resolved after surgery 203 (0.6)
2 TrWP: Treatment worsens medical problem treated with Zofran with no relief 133 (0.4)
3 TrCP: Treatment causes medical problem Transdermal nitroglycerin caused headache 526 (1.8)
4 TrAP: Treatment is administered for medical
problem
start on Decadron 4 mg q6 to prevent swelling 2613 (8.9)
5 TrNAP: Treatment is not administered be-
cause of medical problem
His Avandia was discontinued secondary to the
side effect profile
174 (0.6)
6 NTrP: No relation between treatment and
problem
with sutures intact and no erythema or puru-
lence noted .
4462 (15.2)
7 TeRP: Test reveals medical problem A postoperative MRI revealed no remarkable
findings
3051 (10.4)
8 TeCP: Test conducted to investigate medical
problem
An ultrasound was done to rule out cholestasis 504 (1.7)
9 NTeP: No relation between test and problem Throughout the stay his labs remained normal
and his pain controlled .
2964 (10.1)
10 PIP: Medical problem indicates medical prob-
lem
with a moderate-sized , dense , fixed inferior
defect indicative of scar
2202 (7.5)
11 NPP: No relation between paired medical
problems
He is somewhat cantankerous and demanding
of the nurses .
12503 (42.6)
Table 1: The 11 relation types for medical relation classification. Each relation type is defined on an
ordered pair where concepts in the pair are as specified by the relation. The ?Total? and ?%? columns
show the number and percentage of instances annotated with the corresponding relation type over all 426
discharge summaries, respectively.
instances belonging to the three ?no relation? classes.
2
Specifically, we downsample the instances be-
longing to the three ?no relation? classes (i.e., NTrP, NTeP, and NPP) by ensuring that (1) the ratio of
the number of NTrP instances to the number of TREATMENT-PROBLEM instances is 0.06; (2) the ratio
of the number of NTeP instances to the number of TEST-PROBLEM instances is 0.03; and (3) the ratio
of the number of NPP instances to the number of PROBLEM-PROBLEM instances is 0.5. These ratios are
selected using our 30-summary development set, as described in Section 2. As mentioned above, each
instance corresponds to a pair of concepts, c
1
and c
2
, and is represented using 37 groups of features that
can be divided into five categories:
3
Context (13 groups). The words, the POS tags, the bigrams, the string of words, the sequence of
phrase chunk types, and the concept types used between c
1
and c
2
; the word preceding c
1
/c
2
; any of the
three words succeeding c
1
/c
2
; the predicates associated with c
1
/c
2
; the predicates associated with both
concepts; and a feature that indicates whether a conjunction regular expression matched the string of
words between c
1
and c
2
.
Similarity (5 groups). We find the concept pairs in the training set that are most similar to the (c
1
,c
2
)
pair (i.e., its nearest neighbors), and create features that encode the statistics collected from these nearest
neighbors. To find the nearest neighbors, we (1) represent each pair in the training set as a sequence; (2)
define the number of nearest neighbors to use; and (3) define a similarity metric to compute the similarity
of two sequences.
Following Rink et al. (2011), we employ five methods to represent a pair. The five methods are: (1) as a
sequence of POS tags for the entire sentence containing the pair; (2) as a phrase chunk sequence between
the two concepts; (3) as a word lemma sequence beginning the two words before the first concept, up to
and including the second word following the second concept in the pair; (4) as a concept type sequence
for all the concepts found in the sentence containing the pair; and (5) as a shortest dependency path
sequence connecting the two concepts. Table 2 shows an example of these five methods of generating
sequences from the TEST concept her exam and the PROBLEM concept her hyperreflexia in the sentence
2
Other methods for addressing class imbalance, such as over-sampling (Chawla et al., 2002) and cost-sensitive learning
(Turney, 1995), can also be employed.
3
To compute the features, we use (1) the Stanford CoreNLP tool (Manning et al., 2014) to obtain POS tags, word lemmas,
and dependency structures; (2) GENIA (http://www.nactem.ac.uk/tsujii/GENIA/tagger) to obtain phrase
chunks; and (3) SENNA (Collobert et al., 2011) to obtain predicate-argument structures.
1684
Generation Method Sequence
(1) RB VB , test
c1
RB VBD RB IN problem
c2
.
(2) ADVP VP ADVP PP
(3) postop , test
c1
only improve slightly in problem
c2
.
(4) test
c1
problem
c2
(5) test
c1
?nsubj?> prep <?pobj?problem
c2
Table 2: Examples of the five methods of sequence generation.
Postop, her exam only improved slightly in her hyperreflexia . Note that for better generalization, the
two concepts are replaced with their concept type (i.e., her exam and her hyperreflexia are replaced
with test
c1
and problem
c2
respectively) before sequence generation. Like Rink et al., we seek different
numbers of nearest neighbors for the five methods of generating sequences. For the first method, we use
100 nearest neighbors; for the second method, 15 neighbors; for the third method, 20 neighbors; for the
fourth method, 100 neighbors; and for the fifth method, 20 neighbors. We use the Levenshtein distance
(Levenshtein, 1966) as the similarity metric.
After finding the nearest neighbors for each of the five methods of sequence representation, we create
features as follows. For each method, we compute the percentage of nearest neighbors belonging to each
of the 11 relation types, and then create 11 features whose values are these 11 numbers.
Single concept (11 groups). Any word lemma from c
1
/c
2
; any word used to describe c
1
/c
2
; the concept
type for c
1
/c
2
; the string of words in c
1
/c
2
; the concatenation of assertion types for both concepts; and the
sentiment category (i.e., positive or negative) of c
1
/c
2
obtained from the General Inquirer lexicon (Stone
et al., 1968).
Wikipedia (6 groups). Six features are computed based on the Wikipedia articles, their categories, and
the links between them. The first feature encodes whether neither c
1
nor c
2
contains any substring that
may be matched against the title of an article. The second feature encodes whether the links between
the articles retrieved based on the two concepts are absent. The next two features encode whether a link
exists from the article pertaining to c
1
(c
2
) to the article pertaining to c
2
(c
1
). The fifth feature encodes
whether there are links between the articles pertaining to both concepts. The last feature encodes whether
both concepts have the same concept type according to their Wikipedia categories.
Vicinity (2 groups). The concatenation of the type of c
1
and the type of the closest concept preceding
c
1
; and the concatenation of the type of c
2
and the type of the closest concept succeeding c
2
.
After creating the training instances, we train a 11-class classifier on them using SVM
multiclass
(Tsochantaridis et al., 2004). We set C, the regularization parameter, to 10,000, since preliminary exper-
iments indicate that preferring generalization to overfitting (by setting C to a small value) tends to yield
poorer classification performance. The remaining learning parameters are set to their default values. Af-
ter training, we use the resulting classifier to make predictions on the test instances, which are generated
in the same way as the training instances.
3.2 SVM with Structured Feature
In this framework, each instance is represented using a single structured feature computed from the
parse tree of the sentence containing the concept pair. Since publicly available SVM learners capable
of handling structured features can only make binary predictions, we train 11 SVM classifiers, one for
representing each medical relation. In each classifier?s training data, a positive instance is one whose
class value matches the medical relation class value of the classifier, and a negative instance is one with
other class values applicable to the given concept pair. Since the negative instances significantly out-
number the positive instances in each of these binary classifiers, we reduce data skew by downsampling
the negative instances. Following the order of the 11 relations listed in Table 1, the optimal ratios of
negative-to-positive instances according to our 30-summary development set are 0.2, 0.2, 0.06, 0.2, 0.5,
1, 1, 0.3, 0.06, 0.06, and 0.09, respectively. We set C to 100 based on the development data.
While we want to use a parse tree directly as a feature for representing an instance, we do not want
to use the entire parse tree as a feature. Specifically, while using the entire parse tree enables a richer
1685
representation of the syntactic context of the two concepts than using a partial parse tree, the increased
complexity of the tree also makes it more difficult for the SVM learner to make generalizations.
To strike a better balance between having a rich representation of the context and improving the
learner?s ability to generalize, we extract a subtree from a parse tree and use it as the value of the struc-
tured feature of an instance. Specifically, given two concepts in an instance and the associated syntactic
parse tree T , we retain as our subtree the portion of T that covers (1) all the nodes lying on the shortest
path between the two entities, and (2) all the immediate children of these nodes that are not the leaves of
T . This subtree is known as a simple expansion tree.
After training the 11 tree kernel-based relation classifiers, we can apply them to classify a test instance.
The class value of an instance is determined based on the classifier with the maximum classification
confidence, where the confidence value of an instance is its signed distance from the SVM hyperplane.
4 Exploiting Sequences for MR Classification
Unlike the two baselines, which exploit flat features and parse-based structured features for MR classifi-
cation, in this section we describe three MR classification systems that exploit sequences.
4.1 Dependency-Based Sequences
The first system is based on sequences of dependency relations. To see why dependency relations could
be useful for MR classification, consider the sentences in Table 3:
(1)
(2)
Table 3: Example dependency paths.
In sentences (1) and (2), the PROBLEM concepts His pain and The patient?s pain occur as the subject
of the verb controlled and the TREATMENT concepts oral medication and Motrin occur as objects of the
prepositional with modifier of the same verb controlled. In other words, intuitively, the verb controlled
cues that the PROBLEM concept is being controlled, and together with the preposition with it cues that the
TREATMENT concept is doing the controlling. Note that in each case the relation between the PROBLEM
and TREATMENT is TrIP, which can now be easily inferred given the dependency relations of the concept
pairs with the verb controlled. These examples suggest that the verb closest to each of the two concepts
is an important word as it cues the relation.
Given the usefulness of dependency structures and the verb closest to each concept for MR classifica-
tion, we represent each training/test instance as a paired dependency sequence with separate dependency
paths traced from each concept in the pair to its closest verb. To reduce data sparsity, for the argument
words found in a dependency path, we replace them with their POS tags. For example, given sentence (1),
the path extracted from His pain is ?nsubjpass ( controlled NN )? and from oral medication is ?prep (
controlled with ) pobj ( with NN )?.
4
Next, we describe how to classify a test instance inst. First, we identify the set of training instances T
that satisfy two conditions: (a) the ancestor verb pair in the training instance is the same as that in inst,
and (b) each of the two dependency sequences in the training instance either is the same as, or contains,
or is contained in the corresponding dependency sequences in inst. Second, we find for inst its nearest
neighbor in T by employing the following similarity function:
Similarity(train, test) = cosine(path
train
c
1
, path
test
c
1
) ? cosine(path
train
c
2
, path
test
c
2
) (1)
4
Note that sometimes a dependency path cannot be traced (e.g., a verb does not exist, which is not uncommon in a discharge
report) for a given concept pair. If this happens, no instance will be generated from the concept pair.
1686
where cosine(x, y) is a function that computes the cosine similarity of x and y.
5
Finally, if the similarity
between inst and its nearest neighbor in T is greater than a threshold, we classify inst using the class
value of its nearest neighbor.
6
Otherwise, this system will leave inst unclassified. In other words, this
system is precision-oriented, classifying only those instances it can classify confidently.
4.2 Lexical Patterns
In our second system, we represent each concept pair as a lexical pattern. Specifically, we employ
Generation Method 3 as described in the Similarity features in Section 3.1 to generate a lexical pattern
from a concept pair. To classify a test instance inst, we employ the one-nearest-neighbor algorithm. To
identify the nearest neighbor of inst, we employ the Levenshtein distance as the similarity metric.
Two questions naturally arise. First, since these lexical patterns have already been used to generate
features in the flat-feature baseline, why do we still employ them in a separate system? To answer
this question, note that although these lexical patterns were used to generate features for training the
flat-feature baseline classifier, we have no control over whether these features are deemed useful by the
learning algorithm and are subsequently used by the resulting classifier. Having a separate system that
employs these patterns ensures that they will be used when making the final classification decision.
Second, given that we described five methods to generate sequences in Section 3.1, why do we employ
Generation Method 3 but not the remaining methods? In principle, we can employ the remaining four
generation methods for generating lexical patterns as well: all we need to do is to create four additional
systems, each of which makes use of the patterns created by exactly one of the four methods. In prac-
tice, however, not all generation methods are equally good: if a method does not generate patterns that
adequately capture context, then employing the resulting patterns may yield poor-performing systems.
Consequently, we employ only the system corresponding to the generation method that yields the best
performance on the development data, which turns out to be the system corresponding to Method 3.
4.3 Rules
In the previous subsection, we employ automatically induced patterns. In contrast, our third system em-
ploys patterns that are hand-crafted based on manual observations made on the training set. Specifically,
we ask a human to identify the contexts of two concepts that are strongly indicative of a relation class.
Like the automatically induced patterns, each hand-crafted pattern is composed of the types of the two
concepts involved and the context in which they occur. For example, in the pattern due to PROBLEM by
TREATMENT, the TREATMENT is likely to cause the PROBLEM and therefore it will be labeled as TrCP.
As another example, in the pattern attributed to PROBLEM as a result of PROBLEM, the two PROBLEMs
are likely to have an indicative relation and therefore it will be labeled as PIP. At the end of this process,
we end up with 136 manually labeled patterns, which we will subsequently refer to as a ruleset.
Next, we order these rules in decreasing order of accuracy, where the accuracy of a rule is defined
as the number of times it yields the correct MR type divided by the number of times it is applied, as
measured on the training set.
Given this ruleset, we can classify a test instance using the first applicable rule in it. If no rules are
applicable, the test instance will remain unclassified.
7
5 The Ensemble
In the previous section, we described three systems for MR classification. Together with the two baseline
systems, we have five systems for MR classification. A natural way to make use of all of them for MR
classification is to include them in an ensemble. The question, then, is: how do we classify a test instance
using this ensemble? The simplest approach is perhaps majority voting, but that presumes that each
5
To apply cosine similarity, we represent each path as a frequency-count vector, where each dimension in the vector corre-
sponds to a dependency type or an argument word appearing in the path.
6
Based on development set experiments, the similarity threshold values for each concept pair type are: T
Treatment?Problem
= 0.85; T
Test?Problem
= 0.75; and T
Problem?Problem
= 0.75.
7
Space limitations preclude a complete listing of these rules. See our website at https://www.hlt.utdallas.edu/
?
jld082000/medical-relations/ for the complete list of rules.
1687
member of the ensemble is equally important. In practice, however, some members are more important
than the others, so the votes cast by these members should have higher weights.
To model this observation, we combine the (probabilistic) votes of the members in a weighted fashion
using the following formula:
P
combined
(c) = w
1
? P
tree
(c) + w
2
? P
flat
(c) + w
3
? P
dependency
(c) + w
4
? P
word
(c) + w
5
? P
rules
(c)
(2)
where w
i
(i = 1, . . . , 5) is a combination weight, and P
x
(c) is the probability that the test instance
belongs to class c according to system x.
Two questions naturally arise. First, how can the combination weights be determined? We perform an
exhaustive search on held-out development data to find the combination of weights that jointly maximizes
overall accuracy on the development set. We allow each weight to vary between 0 and 1 in steps of 0.1,
subject to the constraint that the five weights sum to 1.
Second, how can P
x
(c) be computed? In other words, how can each system compute the probability
that a given test instance belongs to a certain class? To answer this question, we have to convert the
output of each system for each test instance into a 11-element probability vector, which is used to encode
the probability that the given test instance belongs to each of the 11 relation types.
We perform the conversion as follows. For the two baseline systems, the SVM outputs a confidence
value for each class. Hence, to obtain the probability vector, we first normalize the confidence value
associated with each class so that it falls within the [0,1] range, and then normalize the resulting values
so that they sum to 1. For the systems employing lexical patterns and dependency-based sequences, the
class chosen by each system receives a probability of 0.6, and each of the other classes applicable to
the test instance under consideration receives an equal share of the remaining probability mass. For the
rule-based system, we take the rule that is used to classify the test instance and apply this rule to each
instance in the training set to estimate the probability that the rule is correct with respect to each of the
11 classes. We can then use the resulting 11 probabilities to create the 11-element probability vector.
Finally, recall that some of these systems are not applicable to all of the test instances. If this happens,
the corresponding system(s) will return a vector in which all of its elements are set to 0.
6 Enforcing Global Consistency
So far we have had an ensemble that, given a test instance, returns the probability that it belongs to each
of the 11 classes. Since the test instances are classified independently of each other, there is no guarantee
that the resulting classifications are globally consistent. To enforce global consistency, we employ global
constraints implemented in the Integer Linear Programming (ILP) framework (Roth and Yih, 2004).
Since our constraints are intra-sentential, we formulate one ILP program for each sentence s in each
training summary. Each ILP program contains 11?N
s
variables, whereN
s
is the number of test instances
formed from the concept pairs in s. In other words, there is one binary indicator variable x
i,j,r
for each
relation class r of each test instance inst formed from concept i and concept j, which will be set to 1 by
the ILP solver if and only if it thinks inst should belong to class r.
Our objective is to maximize the linear combination of these variables and their corresponding proba-
bilities given by the ensemble (see (3) below) subject to two types of constraints, the integrity constraints
and the consistency constraints. The integrity constraints ensure that each concept pair is assigned ex-
actly one relation type (see the equality constraint in (4)). The consistency constraints ensure consistency
between the predictions made for different instances in the same sentence.
Maximize:
?
(i,j)?R
?
r?L
p
i,j,r
x
i,j,r
(3)
subject to:
?
r?L
x
i,j,r
= 1 ? (i, j) ? R (4)
1688
Relation Relations in Conflict
TrIP(tr
i
,p
j
) TrWP(tr
i
,p
k
),TrCP(tr
i
,p
m
),TrNAP(tr
i
,p
n
)
TrWP(tr
i
,p
j
) TrIP(tr
i
,p
k
),TrCP(tr
i
,p
m
),TrNAP(tr
i
,p
n
)
TrCP(tr
i
,p
j
) TrIP(tr
i
,p
k
),TrWP(tr
i
,p
m
),TrNAP(tr
i
,p
n
)
TrAP(tr
i
,p
j
) TrNAP(tr
i
,p
k
)
TrNAP(tr
i
,p
j
) TrAP(tr
i
,p
k
),TrIP(tr
i
,p
m
),TrWP(tr
i
,p
n
),TrCP(tr
i
,p
o
)
TeRP(te
i
,p
j
) TeCP(te
i
,p
k
)
TeCP(te
i
,p
j
) TeRP(te
i
,p
k
)
Table 4: Constraints on relation types.
and consistency constraints.
Note that (1) p
i,j,r
is the probability that the instance formed from concept i and concept j belongs to
relation type r according to the ensemble; (2) L denotes the set of unique relation types; and (3) R is the
set of instances in the sentence under consideration.
The consistency constraints are listed in Table 4. Each row of the table represents a constraint and can
be interpreted as follows. If the relation in the first column holds, then none of the relations in the second
column can hold. Consider, for instance, the constraint in the first row of the table, which says that if
TREATMENT tr
i
improves PROBLEM p
j
, then tr
i
cannot worsen, cause, or be administered for any other
PROBLEM. At first glance, it may not seem intuitive that a treatment that improves one problem cannot
also worsen or cause other problems. This can be attributed to the way a patient discharge summary
is written: while the constraint can be violated for concept pairs in different sentences, there is no case
in which the constraint is violated for concept pairs in the same sentence in the training set. These
constraints can be implemented as linear constraints in ILP. For example, the constraint ?if TREATMENT
tr
i
improves PROBLEM p
j
, then tr
i
cannot worsen PROBLEM p
k
? can be implemented as follows.
x
i,j,TrIP
? 1? x
i,k,TrWP
(5)
7 Evaluation
7.1 Experimental Setup
Following the 2010 i2b2/VA evaluation scheme, we assume that (1) gold concepts and their types are
given, and (2) a medical relation classification system is evaluated on all but the ?no relation? types. In
other words, a system will not be directly rewarded if it correctly identifies a ?no relation? instance, but
will be penalized if it misclassifies a ?no relation? instance as one of the eight relation types.
As mentioned before, we use 170 training summaries from the 2010 i2b2/VA corpus for classifier
training and reserve 256 test summaries for evaluating system performance. Thirty training summaries
are used for development purposes in all experiments that require parameter tuning.
7.2 Results and Discussion
Table 5 shows the 8-class classification results for our MR classification task, where results are expressed
in terms of recall (R), precision (P), and micro F-score (F).
Row 1 and row 2 show the results of the flat-feature baseline and the structured-feature baseline,
respectively. As we can see, the flat-feature baseline performs significantly better than the structured-
feature baseline.
8
It is worth mentioning that since the dataset available to the research community which
we are using contains a subset of the summaries from the dataset that was available to the shared task
participants, we were unable to directly compare our system?s performance with theirs. Nevertheless, we
believe that the results of our reimplementation of Rink et al.?s (2011) system in row 1 can be taken to be
roughly the state of the art results on this dataset.
Rows 3?5 show the results of the three systems we introduced. As we can see from row 3, by using
simple lexical patterns in combination with the Levenshtein similarity metric, we achieve an F-score that
is significantly better than that of the structured-feature baseline but significantly worse (at p < 0.01) than
8
All statistical significance tests are paired t-tests with p < 0.05 unless otherwise stated.
1689
Individual System R P F
1 Flat 66.7 58.1 62.1
2 Tree 64.3 55.6 59.6
3 Lexical Patterns 63.9 59.2 61.4
4 Dependencies 4.3 82.9 8.2
5 Rules 11.9 84.4 9.1
Ensemble System R P F
6 Ensemble
(1+2)
69.2 61.3 65.0
7 Ensemble
(1+2+3)
70.4 63.1 66.6
8 Ensemble
(1+2+3+4)
70.0 64.7 67.2
9 Ensemble
(1+2+3+4+5)
71.1 64.8 67.8
10 Ensemble
(1+2+3+4+5)
+ ILP 72.9 66.7 69.6
Single Classifier R P F
11 Single
(1+2)
53.0 73.6 61.7
12 Single
(1+2+3)
54.4 74.7 63.0
13 Single
(1+2+3+4)
56.4 73.7 63.9
14 Single
(1+2+3+4+5)
56.3 74.5 64.1
15 Single
(1+2+3+4+5)
+ ILP 58.9 75.0 66.0
Bagged System R P F
16 Bagging
(1+2)
54.6 73.6 62.7
17 Bagging
(1+2+3)
54.5 73.8 62.7
18 Bagging
(1+2+3+4)
56.9 73.2 64.0
19 Bagging
(1+2+3+4+5)
56.7 73.9 64.2
20 Bagging
(1+2+3+4+5)
+ ILP 59.2 75.5 66.4
Table 5: Medical relation classification results.
that of the flat-feature baseline. On the other hand, the remaining two systems are precision-oriented:
they classify an instance only if they can do so confidently, thus resulting in poor recall.
Rows 6?10 show the results of our ensemble approach when the individual MR classification systems
are added incrementally to the flat-feature baseline. Except for the addition of the dependency-based
system and the hand-crafted rules, which yielded insignificant improvements in F-score, the addition of
all other components yielded significant improvements. In fact, every significant improvement in F-score
is accompanied by a simultaneous rise in recall and precision. The best-performing system is the one
that comprises all of our components, achieving an F-score of 69.6. This translates to a relative error
reduction of 19.8% and a highly significant improvement (p < 0.001) over our reimplementation of
Rink et al.?s (2011) state-of-the-art baseline. The weights learned for the members of the ensemble are
indeed different: both baselines have a weight of 0.3, the rule-based system and the lexical patterns have
a weight of 0.1, and the remaining weight goes to the dependency-based component.
7.3 Additional Comparisons
Given the above results, a natural question is: is an ensemble approach ever needed to combine the
knowledge sources exploited by different systems in order to obtain these improvements? In other words,
can we achieve similar performance by training a single classifier using a feature set containing all the
features currently exploited by different members of the ensemble?
To answer this question, we repeat the experiments in rows 6?10 of Table 5, except that in each ex-
periment we train a single classifier on a feature set formed from the union of those features employed
by all the members of the corresponding ensemble. Results are shown in rows 11?15 of Table 5. In each
of these five experiments the F-score obtained by our ensemble approach is significantly better than that
achieved by the corresponding single-classifier approach. In addition, although we see improvements
in F-score as we add the individual extensions (including ILP) incrementally to the flat-feature base-
line, none of these improvements is statistically significant. Nevertheless, when applied in combination,
these extensions yield a system that is significantly better than the flat-feature baseline. Overall, these
results provide suggestive evidence that to achieve the same level of performance we cannot replace our
ensemble approach with a simpler setup that relies on a single classifier.
Given that our ensemble approach performs better than a single-classifier approach, a relevant question
is: do we have to use our ensemble approach, or can we still achieve similar performance by replacing it
with a generic ensemble learning method such as bagging (Breiman, 1996)?
To answer this question, we repeat the experiments in rows 6-10 of Table 5, except that we train a
committee of classifiers using bagging. Recall that in bagging each classifier in the committee is trained
on a bootstrap sample created by randomly sampling instances with replacement from the training data
until the size of the bootstrap sample is equal to that of the training data. In our implementation, we
train 20 multi-class SVM classifiers using SVM
multiclass
. Given a test instance, each member of the
committee will independently cast a probabilistic vote, and the class that receives the largest number
of probabilistic votes from the committee members will be assigned to the test instance. Results are
shown in rows 16?20 of Table 5. In each of these five experiments, the F-score obtained by bagging
1690
is significantly worse that that achieved by our ensemble approach. In fact, comparing bagging and the
single-classifier approach, their results are statistically indistinguishable in all but one case (row 11 vs.
row 16), where bagging achieves significantly better performance. Like in the single-classifier experi-
ments, in the bagging experiments we see improvements in F-score as we add the individual extensions
(including ILP) incrementally to the flat-feature baseline, although the improvements are significant only
with the addition of ILP and the dependency-based system. Nevertheless, when applied in combination,
these extensions yield a system that is significantly better than the flat-feature baseline. Overall, these
results provide suggestive evidence that to achieve the same level of performance we cannot replace our
ensemble approach with bagging.
8 Error Analysis
To gain additional insights into our ensemble approach and to provide directions for future work, we
conduct an error analysis of our best-performing system.
NTeP confused as TeRP. This is a frequent type of confusion where 34% of the TEST?PROBLEM pairs
that do not have a relation are misclassified as having a ?Test Reveals Problem? relation. Below are two
subcategories of errors commonly made by the system in this confusion category.
? TEST with numeric results followed by PROBLEM concepts in written text
The following example illustrates this confusion:
. . . [
test
mean gradient] 33 mm , [
problem
decreased disc motion] , [
problem
mobile mass in LVOT] ,
[
problem
mild AI] , [
problem
mild to moderate MR] . . .
In sentences like the one above where a TEST concept has a numeric result (result of TEST mean
gradient is 33 mm), since the TEST concept is already associated with its result, it has no relation with
any other concepts in the sentence. While in some cases the system is able to correctly classify the
relation between the TEST concept and the first following PROBLEM concept, in almost all cases, it fails
to propagate this no relation class down through the other PROBLEMs listed in a series following the
TEST concept. For the sentence above, it incorrectly classifies the relation between TEST concept mean
gradient and each of the PROBLEM concepts mobile mass in LVOT, mild AI, and mild to moderate MR
as TeRP instead of NTeP.
? TEST reveals PROBLEM that is consistent with other PROBLEM
This is a common error where a TEST concept is classified as revealing two consistent PROBLEM
concepts when in actuality it only reveals one of the PROBLEMs. Consider the following sentence:
[
test
Radiograph] revealed [
problem
bilateral diffuse granular pattern] consistent with [
problem
surfactant
deficiency] .
In this sentence, PROBLEM concept bilateral diffuse granular pattern is described as being consistent
with another PROBLEM concept surfactant deficiency. While the system correctly classifies the pair (Ra-
diograph, bilateral diffuse granular pattern) as TeRP, it misclassifies the pair (Radiograph, surfactant
deficiency) as TeRP. In case of the second pair, the TEST concept has no relation with the PROBLEM
concept. From this common error type, an insight one can derive is that the system is currently missing
knowledge of the association of the two PROBLEMs w.r.t. each other, and thus in turn cannot make an
informed decision of which of the two PROBLEMs the TEST concept actually reveals.
PIP confused as NPP. The second major confusion in the system?s output concerns misclassifying
PROBLEM concept pairs that are indicative of each other as having ?no relation?. We observe that 39.5%
of the PIP instances get classified as NPP.
? PROBLEM without another PROBLEM
In a sentence, if a PROBLEM concept is actually said to be without another PROBLEM, then such a pair is
commonly misclassified by the classifier into the no-relation class NPP instead of the has-relation class
PIP. An example of this can be found in the sentence ?[
problem
Angio site] was clean , dry , and intact
without [
problem
bleeding] or [
problem
drainage] .?, where PROBLEM Angio site is classified as NPP with
1691
both concepts bleeding and drainage, respectively. Such cases call for domain-specific knowledge that
can aid in identifying attributes of PROBLEMs, like that the PROBLEM concepts bleeding and drainage
are commonly associated attributes of the PROBLEM concept Angio site. With this information the
system is better equipped to recognize that PROBLEM Angio site is related to its attributes.
9 Conclusion
We investigated a new approach to the medical relation classification task, where we employed human-
supplied knowledge to assist the construction of relation classification systems based on sequences, com-
bined them via an ensemble, and then enforced global consistency using constraints in an ILP framework.
Experimental results on the i2b2 corpus show a significant relative error reduction of 19.8% over a state-
of-the-art baseline.
Acknowledgments
We thank the three anonymous reviewers for their detailed and insightful comments on an earlier draft
of this paper. This work was supported in part by NSF Grants IIS-1147644 and IIS-1219142.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In
Proceedings of the Fifth ACM Conference on Digital Libraries, pages 85?94.
Michele Banko, Michael Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction for the Web. In Proceedings of the 20th International Joint Conference on Artificial
Intelligence, pages 2670?2676.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collabora-
tively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD
International Conference on Management of Data, pages 1247?1250.
Leo Breiman. 1996. Bagging predictors. Machine Learning, 24:123?140.
Sergey Brin. 1999. Extracting patterns and relations from the World Wide Web. In The World Wide Web and
Databases, pages 172?183. Springer.
Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the Web using minimal su-
pervision. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages
576?583.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. 2002. SMOTE: Synthetic
minority over-sampling technique. Journal of Artificial Intelligence Research, 16:321?357.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011.
Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493?2537.
Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko, Joel Martin, and Xiaodan Zhu. 2011. Machine-learned
solutions for three stages of clinical information extraction: The state of the art at i2b2 2010. Journal of the
American Medical Informatics Association, 18(5):557?562.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S
Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the Web: An experimental
study. Artificial Intelligence, 165(1):91?134.
Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu Xu. 2012. Large-scale learning of relation-extraction rules
with distant supervision from the Web. In Proceedings of the International Semantic Web Conference, pages
263?278.
Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707?710.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation
extraction with an incomplete knowledge base. In Proceedings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 777?782.
1692
Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics: System Demonstrations, pages 55?60.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP, pages 1003?1011.
Truc Vien T. Nguyen and Alessandro Moschitti. 2011. End-to-end relation extraction using distant supervision
from external semantic repositories. In Proceedings of the 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies, pages 277?282.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting
semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Computational Linguistics, pages 113?120.
Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In
Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 41?47.
Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping.
In Proceedings of the Sixteenth National Conference on Artificial Intelligence, pages 474?479.
Bryan Rink, Sanda Harabagiu, and Kirk Roberts. 2011. Automatic extraction of relations between medical
concepts in clinical texts. Journal of the American Medical Informatics Association, 18(5):594?600.
Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language
tasks. In Proceedings of the Eighth Conference on Computational Natural Language Learning, pages 1?8.
Benjamin Rozenfeld and Ronen Feldman. 2008. Self-supervised relation extraction from the Web. Knowledge
and Information Systems, 17(1):17?33.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Technology Conference of the North American Chapter of the
Association for Computational Linguistics, pages 304?311.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A
Computer Approach to Content Analysis. MIT Press, Cambridge, Massachusetts.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: A core of semantic knowledge. In
Proceedings of the 16th International World Wide Web Conference, pages 697?706.
Mihai Surdeanu and Massimiliano Ciaramita. 2007. Robust information extraction with perceptrons. In Proceed-
ings of the NIST 2007 Automatic Content Extraction Workshop.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine
learning for interdependent and structured output spaces. In Proceedings of the 21st International Conference
on Machine Learning, pages 104?112.
Peter Turney. 1995. Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction
algorithm. Journal of Artificial Intelligence Research, 2:369?409.
?
Ozlem Uzuner, Brett R South, Shuying Shen, and Scott L DuVall. 2011. 2010 i2b2/VA Challenge on concepts,
assertions, and relations in clinical text. Journal of the American Medical Informatics Association, 18(5):552?
556.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction.
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 427?434.
GuoDong Zhou, Min Zhang, DongHong Ji, and QiaoMing Zhu. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In Proceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language Learning, pages 728?736.
Xiaodan Zhu, Colin Cherry, Svetlana Kiritchenko, Joel Martin, and Berry de Bruijn. 2013. Detecting concept
relations in clinical text: Insights from a state-of-the-art model. Journal of Biomedical Informatics, 46(2):275?
285.
1693
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 229?239,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Modeling Organization in Student Essays
Isaac Persing and Alan Davis and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{persingq,alan,vince}@hlt.utdallas.edu
Abstract
Automated essay scoring is one of the most
important educational applications of natural
language processing. Recently, researchers
have begun exploring methods of scoring es-
says with respect to particular dimensions of
quality such as coherence, technical errors,
and relevance to prompt, but there is rela-
tively little work on modeling organization.
We present a new annotated corpus and pro-
pose heuristic-based and learning-based ap-
proaches to scoring essays along the organi-
zation dimension, utilizing techniques that in-
volve sequence alignment, alignment kernels,
and string kernels.
1 Introduction
Automated essay scoring, the task of employing
computer technology to evaluate and score writ-
ten text, is one of the most important educational
applications of natural language processing (NLP)
(see Shermis and Burstein (2003) and Shermis et al
(2010) for an overview of the state of the art in this
task). Recent years have seen a surge of interest in
this and other educational applications in the NLP
community, as evidenced by the panel discussion
on ?Emerging Application Areas in Computational
Linguistics? at NAACL 2009, as well as increased
participation in the series of workshops on ?Innova-
tive Use of NLP for Building Educational Applica-
tions?. Besides its potential commercial value, au-
tomated essay scoring brings about a number of rel-
atively less-studied but arguably rather challenging
discourse-level problems that involve the computa-
tional modeling of different facets of text structure,
such as content, coherence, and organization.
A major weakness of many existing essay scor-
ing engines such as IntelliMetric (Elliot, 2001) and
Intelligent Essay Assessor (Landauer et al, 2003)
is that they adopt a holistic scoring scheme, which
summarizes the quality of an essay with a single
score and thus provides very limited feedback to
the writer. In particular, it is not clear which di-
mension of an essay (e.g., coherence, relevance)
a score should be attributed to. Recent work ad-
dresses this problem by scoring a particular dimen-
sion of essay quality such as coherence (Miltsakaki
and Kukich, 2004), technical errors, and relevance
to prompt (Higgins et al, 2004). Automated sys-
tems that provide instructional feedback along mul-
tiple dimensions of essay quality such as Criterion
(Burstein et al, 2004) have also begun to emerge.
Nevertheless, there is an essay scoring dimension
for which few computational models have been de-
veloped ? organization. Organization refers to the
structure of an essay. A high score on organization
means that writers introduce a topic, state their po-
sition on that topic, support their position, and con-
clude, often by restating their position (Silva, 1993).
A well-organized essay is structured in a way that
logically develops an argument. Note that organi-
zation is a different facet of text structure than co-
herence, which is concerned with the transition of
ideas at both the global (e.g., paragraph) and local
(e.g., sentence) levels. While organization is an im-
portant dimension of essay quality, state-of-the-art
essay scoring software such as e-rater V.2 (Attali
and Burstein, 2006) employs rather simple heuristic-
based methods for computing the score of an essay
along this particular dimension.
Our goal in this paper is to develop a compu-
tational model for the organization of student es-
229
says. While many models of text coherence have
been developed in recent years (e.g., Barzilay and
Lee (2004), Barzilay and Lapata (2005), Soricut and
Marcu (2006), Elsner et al (2007)), the same is not
true for text organization. One reason is the avail-
ability of training and test data for coherence mod-
eling. Coherence models are typically evaluated on
the sentence ordering task, and hence training and
test data can be generated simply by scrambling the
order of the sentences in a text. On the other hand, it
is not particularly easy to find poorly organized texts
for training and evaluating organization models. We
believe that student essays are an ideal source of
well- and poorly-organized texts. We evaluate our
organization model on a data set of 1003 essays an-
notated with organization scores.
In sum, our contributions in this paper are two-
fold. First, we address a less-studied discourse-level
task ? predicting the organization score of an essay
? by developing a computational model of organi-
zation, thus establishing a baseline against which fu-
ture work on this task can be compared. Second, we
annotate a subset of our student essay corpus with
organization scores and make this data set publicly
available. Since progress in organization modeling
is hindered in part by the lack of a publicly anno-
tated corpus, we believe that our data set will be a
valuable resource to the NLP community.
2 Corpus Information
We use as our corpus the 4.5 million word Interna-
tional Corpus of Learner English (ICLE) (Granger
et al, 2009), which consists of more than 6000 es-
says written by university undergraduates from 16
countries and 16 native languages who are learners
of English as a Foreign Language. 91% of the ICLE
texts are argumentative. The essays we used vary
greatly in length, containing an average of 31.1 sen-
tences in 7.5 paragraphs, averaging 4.1 sentences per
paragraph. About one quarter of the essays had five
or fewer paragraphs, and another quarter contained
nine or more paragraphs. Similarly, about one quar-
ter of essays contained 24 or fewer sentences and the
longest quarter contained 36 or more sentences
We selected a subset consisting of 1003 essays
from the ICLE to annotate and use for training and
testing of our model of essay organization. While
Topic Languages Essays
Most university degrees are
theoretical and do not prepare
students for the real world.
They are therefore of very lit-
tle value.
13 147
The prison system is out-
dated. No civilized society
should punish its criminals: it
should rehabilitate them.
11 103
In his novel Animal Farm,
George Orwell wrote ?All
men are equal but some are
more equal than others.? How
true is this today?
10 82
Table 1: Some examples of writing topics.
narrative writing asks students to compose descrip-
tive stories, argumentative (also known as persua-
sive) writing requires students to state their opinion
on a topic and to validate that opinion with convinc-
ing arguments. For this reason, we selected only ar-
gumentative essays rather than narrative pieces, be-
cause they contain the discourse structures and kind
of organization we are interested in modeling.
To ensure representation across native languages
of the authors, we selected mostly essays written
in response to topics which are well-represented in
multiple languages. This avoids many issues that
may arise when certain vocabulary is used in re-
sponse to a particular topic for which essays written
by authors from only a few languages are available.
Table 1 shows three of the twelve topics selected for
annotation. Fifteen native languages are represented
in the set of essays selected for annotation.
3 Corpus Annotation
To develop our essay organization model, human an-
notators scored 1003 essays using guidelines in an
essay annotation rubric. Annotators evaluated the
organization of each essay using a numerical score
from 1 to 4 at half-point increments. This contrasts
with previous work on essay scoring, where the cor-
pus is annotated with a binary decision (i.e., good or
bad) for a given scoring dimension (e.g., Higgins et
al. (2004)). Hence, our annotation scheme not only
provides a finer-grained distinction of organization
quality (which can be important in practice), but also
230
makes the prediction task more challenging.
The meaning of each integer score was described
and discussed in detail. Table 2 shows the descrip-
tion of each score for the organization dimension.
Score Description of Essay Organization
4 essay is well structured and is organized in
a way that logically develops an argument
3 essay is fairly well structured but could
somewhat benefit from reorganization
2 essay is poorly structured and would
greatly benefit from reorganization
1 essay is completely unstructured and re-
quires major reorganization
Table 2: Descriptions of the meaning of each score.
Our annotators were selected from over 30 appli-
cants who were familiarized with the scoring rubric
and given sample essays to score. The six who were
most consistent with the expected scores were given
additional essays to annotate. To ensure consistency
in scoring, we randomly selected a large subset of
our corpus (846 essays) to have graded by two differ-
ent annotators. Analysis of these doubly annotated
essays reveals that, though annotators only exactly
agree on the organization score of an essay 29% of
the time, the scores they apply are within 0.5 points
in 71% of essays and within 1.0 point in 93% of es-
says. Additionally, if we treat one annotator?s scores
as a gold standard and the other annotator?s scores
as predictions, the predicted scores have a mean er-
ror of 0.54 and a mean squared error of 0.50. Table 3
shows the number of essays that received each of the
seven scores for organization.
score 1.0 1.5 2.0 2.5 3.0 3.5 4.0
essays 24 14 35 146 416 289 79
Table 3: Distribution of organization scores.
4 Function Labeling
As mentioned before, a high score on organization
means that writers introduce a topic, support their
position, and conclude. If one or more of these ele-
ments are missing or if they appear out of order (e.g.,
the conclusion appears before the introduction), the
resulting essay will typically be considered poorly
organized. Hence, knowing the discourse function
label of each paragraph in an essay would be help-
ful for predicting its organization score.
Two questions naturally arise. First, how can we
obtain the discourse function label of each para-
graph? One way is to automatically acquire such
labels from a corpus of student essays where each
paragraph is annotated with its discourse function
label. To our knowledge, however, there is no pub-
licly available corpus that is annotated with such in-
formation. As a result, we will resort to labeling a
paragraph with its function label heuristically.
Second, which paragraph function labels would
be most useful for scoring the organization of an es-
say? Based on our linguistic intuition, we identify
four potentially useful paragraph function labels: In-
troduction, Body, Rebuttal, and Conclusion. Table 4
gives the descriptions of these labels.
Label Name Paragraph Type
I Introduction introduces essay topic and
states author?s position and
main ideas
B Body provides reasons, evidence,
and examples to support main
ideas
C Conclusion summarizes and concludes ar-
guments made in body para-
graphs
R Rebuttal considers counter-arguments
to thesis or main ideas
Table 4: Descriptions of paragraph function labels.
Setting aside for the moment the problem of ex-
actly how to predict an essay?s organization score
given its paragraph sequence, the problem of ob-
taining paragraph labels to use for this task still re-
mains. As mentioned above, we adopt a heuristic ap-
proach to paragraph function labeling. The question,
then, is: what kind of knowledge sources should our
heuristics be based on? We have identified two types
of knowledge sources that are potentially useful for
paragraph labeling. The first of these are positional,
dealing with where in the essay a paragraph appears.
So for example, the first paragraph in an essay is
likely to be an Introduction, while the last is likely
to be a Conclusion. A paragraph in any other posi-
tion, on the other hand, is more likely to be a Body
or Rebuttal paragraph.
231
Label Name Sentence Function
P Prompt restates the prompt given to the author and contains no new material or opinions
T Transition shifts the focus to new topics but contains no meaningful information
H Thesis states the author?s position on the topic for which he/she is arguing
M Main Idea asserts reasons and foundational arguments that support the thesis
E Elaboration further explains reasons and ideas but contains no evidence or examples
S Support provides evidence and examples to support the claims made in other statements
C Conclusion summarizes and concludes the entire argument or one of the main ideas
R Rebuttal considers counter-arguments that contrast with the thesis or main ideas
O Solution puts to rest the questions and problems brought up by counter-arguments
U Suggestion proposes solutions the problems brought up by the argument
Table 5: Descriptions of sentence function labels.
A second potentially useful knowledge source in-
volves the types of sentences appearing in a para-
graph. This idea presupposes that, like paragraphs,
sentences too can have discourse function labels in-
dicating the logical role they play in an argument.
The sentence label schema we propose, which is de-
scribed in Table 5, is based on work in discourse
structure by Burstein et al (2003), but features addi-
tional sentence labels.
To illustrate why these sentence function labels
may be useful for paragraph labeling, consider a
paragraph containing a Thesis sentence. The pres-
ence of a Thesis sentence is a strong indicator that
the paragraph containing it is either an Introduction
or Conclusion. Similarly, a paragraph containing
Rebuttal or Solution sentences is more likely to be
a Body or Rebuttal paragraph.
Hence, to obtain a paragraph?s function label,
we need to first label its sentences. However, we
are faced with the same problem: how can we ob-
tain the sentence function labels? One way is to
learn them from a corpus where each sentence is
manually annotated with its sentence function la-
bel, which is the approach adopted by Burstein et
al. (2003). However, this annotated corpus is not
publicly available. In fact, to our knowledge, there
is no publicly-available corpus that is annotated with
sentence function labels. Consequently, we adopt a
heuristic approach to sentence function labeling.
Overall, we created a knowledge-lean set of
heuristic rules labeling paragraphs and sentences.
Because many of the paragraph labeling heuristics
depend on the availability of sentence labels, we will
describe the sentence labeling heuristics first. For
each sentence function label x, we identify several
features whose presence increases our confidence
that a given sentence is an example of x. So for
example, the presence of any of the words ?agree?,
?think?, or ?opinion? increases our confidence that
the sentence they occur in is a Thesis. If the sentence
instead contains words such as ?however?, ?but?,
or ?argue?, these increase our confidence that the
sentence is a Rebuttal. The features we examine
for sentence labeling are not limited to words, how-
ever. Each content word the sentence shares with
the essay prompt gives us evidence that the sentence
is a restatement of the prompt. Having searched a
sentence for all these clues, we finally assign the
sentence the function label having the most support
among the clues found.
The heuristic rules for paragraph labeling are sim-
ilar in nature, though they depend heavily on the
labels of a paragraph?s component sentences. If a
paragraph contains Thesis, Prompt, or Background
sentences, the paragraph is likely to be an Introduc-
tion. However, if a paragraph contains Main Idea,
Support, or Conclusion sentences, it is likely to be
a Body paragraph. Finally, as mentioned previously,
some positional information is used in labeling para-
graphs. For example, a paragraph that is the first
paragraph in an essay is likely to be an Introduction,
but a paragraph that is neither the first nor the last
is likely to be either a Rebuttal or Body paragraph.
After searching a paragraph for all these features,
we gather the pieces of evidence in support of each
paragraph label and assign the paragraph the label
having the most support.1
1Space limitations preclude a complete listing of these para-
232
5 Heuristic-Based Organization Scoring
Having applied labels to each paragraph in an es-
say, how can we use these labels to predict the es-
say?s score? Recall that the importance of each para-
graph label stems not from the label itself, but from
the sequence of labels it appears in. Motivated by
this observation, we exploit a technique that is com-
monly used in bioinformatics ? sequence align-
ment. While sequence alignment has also been used
in text and paraphrase generation (e.g., Barzilay and
Lee (2002; 2003)), it has not been extensively ap-
plied to other areas of language processing, includ-
ing essay scoring. In this section, we will present
two heuristic approaches to organization scoring,
one based on aligning paragraph sequences and the
other on aligning sentence sequences.
5.1 Aligning Paragraph Sequences
As mentioned above, our first approach to heuristic
organization scoring involves aligning paragraph se-
quences. Specifically, this approach operates in two
steps. Given an essay e in the test set, we (1) find the
k essays in the training set that are most similar to e
via paragraph sequence alignment, and then (2) pre-
dict the organization score of e by aggregating the
scores of its k nearest neighbors obtained in the first
step. Below we describe these two steps in detail.
First, to obtain the k nearest neighbors of e,
we employ the Needleman-Wunsch alignment algo-
rithm (Needleman and Wunsch, 1970), which com-
putes a similarity score for any pair of essays by
finding an optimal alignment between their para-
graph sequences. To illustrate why we believe se-
quence alignment can help us determine which es-
says are most similar, consider two example es-
says. One essay, which we will call IBBBC, begins
with an Introductory paragraph, follows it with three
Body paragraphs, and finally ends with a Conclud-
ing paragraph. Another essay CRRRI begins with
a paragraph stating its Conclusion, follows it with
three Rebuttal paragraphs, and ends with a para-
graph Introducing the essay?s topic. We can tell by
a casual glance at the sequences that any reasonable
similarity function should tell us that they are not
graph and sentence labeling heuristics. See our website at
http://www.hlt.utdallas.edu/
?
alan/ICLE/ for
the complete list of heuristics.
very similar. The Needleman-Wunsch alignment al-
gorithm has this effect since the score of the align-
ment it produces would be hurt by the facts that (1)
there is not much overlap in the sets of paragraph
labels each contains, and (2) the paragraph labels
they do share (I and C) do not occur in the same
order. The resulting alignment would therefore con-
tain many mismatches or indels.2
If we now consider a third essay whose para-
graph sequence could be represented as IBRBC, a
good similarity function should tell us that IBBBC
and IBRBC are very similar. The Needleman-
Wunsch alignment score between the two paragraph
sequences has this property, as the alignment al-
gorithm would discover that the two sequences are
identical except for the third paragraph label, which
could be mismatched for a small penalty. We would
therefore conclude that the IBBBC and IBRBC es-
says should receive similar organization scores.
To fully specify how to find the k nearest neigh-
bors of an essay, we need to define a similarity func-
tion between paragraph labels. In sequence align-
ment, similarity function S(i, j) tells us how likely
it is that symbol i (in our case, a paragraph label)
will be substituted with another symbol j. While
we expect that in an alignment between high-scoring
essays, an Introduction paragraph is most likely to
be aligned with another Introduction paragraph, how
much worse should the alignment score be if an In-
troduction paragraph needs to be mismatched with
a Rebuttal paragraph or replaced with an indel? We
solve this problem by heuristically defining the sim-
ilarity function as follows: S(i, j) = 1 when i = j,
S(i, j) = ?1 when i 6= j, and also S(i,?) =
S(?, i) = ?1, where ??? is an indel. In other
words, the similarity function encourages the align-
ment between two identical function labels and dis-
courages the alignment between two different func-
tion labels, regardless of the type of function labels.
After obtaining the k nearest neighbors of e, the
next step is to predict the organization score of e
by aggregating the scores of its k nearest neighbors
into one number. (Note that we know the organiza-
2In pairwise sequence alignment, a mismatch occurs when
one symbol has to be substituted for another to make two se-
quences match. An indel indicates that in order to transform
one sequence to match another, we must either insert a symbol
into one sequence or delete a symbol from the other sequence.
233
tion score of each nearest neighbor, since they are
all taken from the training set.) One natural way to
do this would be to take the mean, median, or mode
of its k nearest neighboring essays from the training
set. Hence, our first heuristic method Hp for scoring
organization has three variants.
5.2 Aligning Sentence Sequences
An essay?s paragraph sequence captures information
about its organization at a high level, but ignores
much of its lower level structure. Since we have also
heuristically labeled sentences, it now makes sense
to examine the sequences of sentence function labels
within an essay?s paragraphs. The intuition is that at
least some portion of an essay?s organization score
can be attributed to the organization of the sentence
sequences of its component paragraphs.
To address this concern, we propose a second
heuristic approach to organization scoring. Given
a test essay e, we first find for each paragraph in
e the k paragraphs in the training set that are most
similar to it. Specifically, each paragraph is repre-
sented by its sequence of sentence function labels.
Given this paragraph representation, we can find the
k nearest neighbors of a paragraph by applying the
Needleman-Wunsch algorithm described in the pre-
vious subsection to align sentence sequences, using
the same similarity function we defined above.
Next, we score each paragraph pi by aggregating
the scores of its k nearest neighbors obtained in the
first step, assuming the score of a nearest neighbor
paragraph is the same as the organization score of
the training set essay containing it. As before, we
can employ the mean, median, or mode to aggregate
the scores of the nearest neighbors of pi.
Finally, we predict the organization score of e by
aggregating the scores of its paragraphs obtained in
the second step. Again, we can employ mean, me-
dian, or mode to aggregate the scores. Since we have
three ways of aggregating the scores of a paragraph?s
nearest neighbors and three ways of aggregating the
resulting paragraph scores, this second method Hs
for scoring organization has nine variants.
6 Learning-Based Organization Scoring
In the previous section, we proposed two heuris-
tic approaches to organization scoring, one based
on aligning paragraph label sequences and the other
based on aligning sentence label sequences. In the
process of constructing these two systems, however,
we created a lot of information about the essays
which might also be useful for organization scoring,
but which the heuristic systems are unable to exploit.
To remedy the problem, we introduce three learning-
based systems which abstract the additional infor-
mation we produced in three different ways. In each
system, we use the SVMlight (Joachims, 1999) im-
plementation of regression support vector machines
(SVMs) (Cortes and Vapnik, 1995) to train a regres-
sor because SVMs have been frequently and suc-
cessfully applied to a variety of NLP problems.
6.1 Linear Kernel
Owing to the different ways we presented of com-
bining the scores of an essay?s nearest neighbors,
the paragraph label sequence alignment approach
has three variants, and its sentence label sequence
alignment counterpart has nine. Unfortunately, these
heuristic approaches suffer from two major weak-
nesses. First, it is not intuitively clear which of
these 12 ways for predicting an essay?s organiza-
tion score is clearly better than the others. Second,
it is not clear that the k nearest neighbors of an es-
say will always be similar to it with respect to or-
ganization score. While we do expect the alignment
scores between good essays with reasonable para-
graph sequences to be high, poorly organized es-
says by their nature have more random paragraph
sequences. Hence, we have no intuition about the k
nearest neighbors of a poor essay, as it may have as
high an alignment score with another poorly orga-
nized essay as with a good essay.
Our solution to these problems is to use the orga-
nization scores obtained by the 12 heuristic variants
as features in a linear kernel SVM learner. We be-
lieve that using the estimates given by all the 12 vari-
ants of the two heuristic approaches rather than only
one of them addresses the first weakness mentioned
above. The second weakness, on the other hand, is
addressed by treating the organization score predic-
tions obtained by the nearest neighbor methods as
features for an SVM learner rather than as estimates
of an essay?s organization score.
The approach we have just described, however,
does not exploit the full power of linear kernel
234
SVMs. One strength of linear kernels is that they
make it easy to incorporate a wide variety of dif-
ferent types of features. In an attempt to further
enhance the prediction capability of the SVM re-
gressor, we will provide it with not only the 12 fea-
tures derived from the heuristic-based approaches,
but also with two additional types of features.
First, to give our learner more direct access to
the information we used to heuristically predict es-
say scores, we can extract paragraph label subse-
quences3 from each essay and use them as features.
To illustrate the intuition behind these features, con-
sider two paragraph subsequences: Introduction?
Body and Rebuttal?Introduction. It is fairly typi-
cal to see the first subsequence, I?B, at the begin-
ning of a good essay, so its occurrence should give
us a small amount of evidence that the essay it oc-
curs in is well-organized. The presence of the sec-
ond subsequence, R?I, however, should indicate that
its essay?s organization is poor because, in general, a
good essay should not give a Rebuttal before an In-
troduction. Because we can envision subsequences
of various lengths being useful, we create a binary
presence or absence feature in the linear kernel for
each paragraph subsequence of length 1, 2, 3, 4, or
5 appearing in the training set.
Second, we employ sentence label subsequences
as features in the linear kernel. Recall that when
describing our alignment-based nearest neighbor
organization score prediction methods, we noted
that an essay?s organization score may be partially
attributable to how well the sentences within its
paragraphs are organized. For example, if one
of an essay?s paragraphs contains the sentence la-
bel subsequence Main Idea?Elaboration?Support?
Conclusion this gives us some evidence that the es-
say is overall well-organized since one of its compo-
nent paragraphs contains this reasonably-organized
subsequence. An essay with a paragraph contain-
ing the subsequence Conclusion?Support?Thesis?
Rebuttal, however, is likely to be poorly orga-
nized because this is a poorly-organized subse-
quence. Since sentence label subsequences of dif-
fering lengths may be useful for score prediction, we
create a binary presence or absence feature for each
sentence label subsequence of length 1, 2, 3, 4, or 5
3Note that a subsequence is not necessarily contiguous.
in the training set.
While the number of nearest neighbor features is
manageable, the presence of a large number of fea-
tures can sometimes confuse a learner. For that rea-
son, we do feature selection on the two types of
subsequence features, selecting only 100 features
for each type that has the highest information gain
(see Yang and Pedersen (1997) for details). We
call the system resulting from the use of these three
types of features Rlnps because it uses Regression
with linear kernel to predict essay scores, and it
uses nearest neighbor, paragraph subsequence, and
sentence subsequence features.
6.2 String Kernel
In a traditional learning setting, the feature set em-
ployed by an off-the-shelf learning algorithm typ-
ically consists of flat features (i.e., features whose
values are discrete- or real-valued, as the ones de-
scribed in the Linear Kernel subsection). Advanced
machine learning algorithms such as SVMs, on the
other hand, have enabled the use of structured fea-
tures (i.e., features whose values are structures such
as parse trees and sequences), owing to their ability
to employ kernels to efficiently compute the similar-
ity between two potentially complex structures.
Perhaps the most obvious advantage of employ-
ing structured features is simplicity. To understand
this advantage, consider learning in a traditional set-
ting. Recall that we can only employ flat features in
this setting, as we did with the linear kernel. Hence,
if we want to use information from a parse tree as
features, we will need to design heuristics to extract
the desired parse-based features from parse trees.
For certain tasks, designing a good set of heuris-
tics can be time-consuming and sometimes difficult.
On the other hand, SVMs enable a parse tree to
be employed directly as a structured feature, obvi-
ating the need to design heuristics to extract infor-
mation from potentially complex structures. How-
ever, structured features have only been applied to a
handful of NLP tasks such as semantic role labeling
(Moschitti, 2004), syntactic parsing and named en-
tity identification (Collins and Duffy, 2002), relation
extraction (Bunescu and Mooney, 2005), and coref-
erence resolution (Versley et al, 2008). Our goal
here is to explore this rarely-exploited capability of
SVMs for the task of essay scoring.
235
While the vast majority of previous NLP work
on using structured features have involved tree ker-
nels, we employ a kernel that is rarely investigated in
NLP: string kernels (Lodhi et al, 2002). Informally,
a string kernel aims to efficiently compute the sim-
ilarity between two strings (or sequences) of sym-
bols based on the similarity of their subsequences.
We apply string kernels to essay scoring as follows:
we represent each essay using its paragraph function
label sequence, and employ a string kernel to com-
pute the similarity between two essays based on this
representation. Typically, a string kernel takes as in-
put two parameters: K (which specifies the length
of the subsequences in the two strings to compare)
and ? (which is a value between 0 and 1 that spec-
ifies whether matches between non-contiguous sub-
sequences in the two strings should be considered
as important as matches between contiguous subse-
quences). In our experiments, we select values for
these parameters in a somewhat arbitrary manner. In
particular, since ? ranges between 0 and 1, we sim-
ply set it to 0.5. For K , since in the flat features we
considered all paragraph label sequences of lengths
from 1 to 5, we again take the middle value, setting
it to 3. We call the system using this kernel Rs be-
cause it uses a Regression SVM with a string kernel
to predict essay scores.
6.3 Alignment Kernel
In general, the purpose of a kernel function is to
measure the similarity between two examples. The
string kernel we described in the previous subsec-
tion is just one way of measuring the similarity of
two essays given their paragraph sequences. While
this may be the most obvious way to use paragraph
sequence information from a machine learning per-
spective, our earlier use of the Needleman-Wunsch
algorithm suggests a more direct way of extracting
structured information from paragraph sequences.
More specifically, recall that the Needleman-
Wunsch algorithm finds an optimal alignment be-
tween two paragraph sequences, where an opti-
mal alignment is defined as an alignment having
the highest possible alignment score. The optimal
alignment score can be viewed as another similar-
ity measure between two essays. As such, with
some slight modifications, the alignment score be-
tween two paragraph sequences can be used as the
kernel value for an Alignment Kernel.4 We call
the system using this kernel Ra because it uses a
Regression SVM with an alignment kernel to pre-
dict essay scores.
6.4 Combining Kernels
Recall that the flat features are computed using a lin-
ear kernel, while the two types of structured features
are computed using string and alignment kernels. If
we want our learner to make use of more than one of
these types of features, we need to employ a compos-
ite kernel to combine them. Specifically, we define
and employ the following composite kernel:
Kc(F1, F2) =
1
n
n
?
i=1
Ki(F1, F2),
where F1 and F2 are the full set of features (contain-
ing both flat and structured features) that represent
the two essays under consideration, Ki is the ith ker-
nel we are combining, and n is the number of kernels
we are combining. To ensure that each kernel under
consideration contributes equally to the composite
kernel, each kernel value Ki(F1, F2) is normalized
so that its value falls between 0 and 1.
7 Evaluation
7.1 Evaluation Metrics
We designed three evaluation metrics to measure the
error of our organization scoring system. The sim-
plest metric, S1, is perhaps the most intuitive. It
measures the frequency at which a system predicts
the wrong score out of the seven possible scores.
Hence, a system that predicts the right score only
25% of the time would receive an S1 score of 0.75.
The S2 metric is slightly less intuitive than S1,
but no less reasonable. It measures the average
distance between the system?s score and the actual
score. This metric reflects the idea that a system
that estimates scores close to the annotator-assigned
scores should be preferred over a system whose esti-
mations are further off, even if both systems estimate
the correct score at the same frequency.
Finally, the S3 evaluation metric measures the
average square of the distance between a system?s
4In particular, we note that for theoretical reasons, a kernel
function must always return a non-negative value. The align-
ment score function does not have this property, so we increase
all alignment scores until their theoretical minimum value is 0.
236
organization score estimations and the annotator-
assigned scores. The intuition behind this system
is that not only should we prefer a system whose es-
timations are close to the annotator scores, but we
should also prefer one whose estimations are not too
frequently very far away from the annotator scores.
These three scores are given by:
1
N
?
Ai 6=Ei
1, 1
N
N
?
i=1
|Ai ? Ei|,
1
N
N
?
i=1
(Ai ? Ei)2,
where Ai and Ei are the annotator assigned and sys-
tem estimated scores respectively for essay i, and N
is the number of essays. Since many of the systems
we have described assign test essays real-valued or-
ganization scores, to obtain Ei for system S1 we
round the outputs of each system to the nearest of
the seven scores the human annotators were permit-
ted to assign (1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0).
To test our system, we performed 5-fold cross val-
idation on our 1003 essay set, micro-averaging our
results into three scores corresponding to the three
scoring metrics described above.
7.2 Results and Discussion
The average baseline. As mentioned before, there
is no standard baseline for organization modeling
against which we can compare our systems. To start
with, we employ a simple ?average? baseline. Avg
computes the average organization score of essays
in the training set and assigns this score to each test
set essay. Results of this baseline are shown in row
1 of Table 6. Though simple, this baseline is by no
means easy-to-beat, since 41% of the essays have a
score of 3, and 96% of the essays have a score that
is within one point of 3.
Heuristic baselines. Recall that we have 12 ver-
sions of the two heuristic approaches to organization
prediction. Space limitations preclude a discussion
of the results of all these versions, so instead, to ob-
tain the strongest baseline results, we show only the
best results achieved by the three versions based on
aligning paragraph label sequences in row 2 (Hp)
and the best results achieved by the nine versions
based on aligning sentence label sequences in row
3 (Hs) of Table 6. It is clear from the results that
the Hp systems yielded the best baseline predictions
under all three scoring metrics, performing signif-
icantly better than both the Avg and Hs systems
System S1 S2 S3
1 Avg .585 .412 .348
2 Hp .548 .339 .198
3 Hs .575 .397 .329
4 Rlnps .520 .331 .186
5 Rs .577 .369 .222
6 Ra .686 .519 .429
7 Rlsnps .534 .332 .187
8 Rlanps .541 .332 .178
9 Rsa .517 .325 .177
10 Rlsanps .517 .323 .175
Table 6: System Performance
(p < 0.01) with respect to the S2 and S3 metrics,
but its S1 performance is less significant with re-
spect to Avg (p < 0.1) and is indistinguishable at
even the p < 0.1 level from Hs.5 In general, how-
ever, it appears to be the case that systems based
on aligning paragraph label sequences achieve better
results than systems that attempt to align sentence
label sequences.
Learning-based approaches. Rows 4?6 of Table
6 show the results we obtained using each of the
three single-kernel systems. When compared to the
best baseline, these results suggest that Hp is a pretty
good heuristic approach to organization scoring. In
fact, only one of these three learning-based sys-
tems (Rlnps) performs better than Hp under the three
scoring metrics, and in each case, the difference be-
tween the two is not significant even at p < 0.1. This
suggests that, even though Rlnps performs slightly
better than Hp, the only major benefit we have ob-
tained by using the linear kernel is that it has made
it unnecessary for us to choose between the 12 pro-
posed heuristic systems.
Considering that the second best one-kernel sys-
tem, Rs, does not have access to any of the near-
est neighbor features, which have already proven
useful, its performance seems reasonably good in
that its performance is at least better than the Avg
system. This suggests that, even though Rs does
not perform exceptionally, it is extracting some use-
ful information for organization scoring from the
heuristically assigned paragraph label sequences.
The best one-kernel system, Rlnps, however, is sig-
5All significance tests are two-tailed paired t-tests.
237
nificantly better than Rs with respect to all three
scoring metrics, with p < 0.1 for S1 and p < 0.05
for S2 and S3. By contrast, it initially appears that
the alignment kernel is not extracting any useful
information from these paragraph sequences at all,
since its S1, S2, and S3 scores are all much worse
than all of the baseline systems. The second best
one-kernel system Rs performs significantly better
than Ra at p < 0.01 for all three scoring metrics.
Next, we explore the impact of composite kernels,
which allow our learners to make use of multiple
types of flat and structured features. Specifically, the
results shown in rows 7?9 are obtained by combin-
ing two kernels at a time. These experiments reveal
the surprising result that the two worst performing
single-kernel systems, Rs and Ra, when combined
into Rsa, yield the best two-kernel system results,
which are significant with respect to the best one-
kernel system results under S3 at p < 0.1. This re-
sult suggests that these two different methods of ex-
tracting information from paragraph sequences pro-
vide us with different kinds of evidence useful for
organization scoring, although neither method by it-
self was exceptionally useful. Though Rsa does
not have any access to nearest neighbor informa-
tion, it still performs significantly better than Hp at
p < 0.05 under S1 and S3.
While we have already pointed out that Rsa is
the best composite two-kernel system, it is not clear
which of Rlsnps and Rlanps is second-best. Neither
system consistently performs better than the other
under all three scoring metrics, and the differences
between them are not significant even at p < 0.1. It
is clear only that Rsa is better than both, as its scores
are statistically significantly better at p < 0.01 with
respect to Rlsnps and Rlanps under at least one of
the three scoring metrics in each case.
Finally, in the last row of Table 6, we combine
all three kernels into one SVM learner. The most
important lesson we learn from this experiment is
that each of the three kernels provides the learner
with a different kind of useful information, so that
a composite kernel using all three sources of in-
formation performs better than any system using
fewer kernels. Although the improvements over the
best two-kernel system (Rsa) and one-kernel sys-
tem (Rlnps) are small, they are still statistically sig-
nificant at p < 0.1 under one of the scoring metrics,
S3. When we compare this combined system to the
best baseline (Hp), we discover the improvements
derived from the three-kernel system are significant
improvements over it at p < 0.05 and p < 0.01 with
respect to S1 and S3 respectively.
Feature analysis. To better understand which of
the three flat features (nearest neighbors, paragraph
label sequences, or sentence label sequences) con-
tributes the most to the linear kernel portion of the
systems? performances, we analyze the three fea-
ture types on Rlnps using the backward elimination
feature selection algorithm. First, we remove each
of the three feature groups independently from the
Rlnps?s feature set and determine which of the three
removals yields the best performance according to
each scoring metric. Next, among the remaining
two feature groups, we repeat the same step, remov-
ing each of the two groups independently from the
feature set to determine which of the two removals
yields the best performance.
While space limitations preclude showing the ac-
tual numbers, the trend is consistent among all three
scoring metrics: the first feature type to remove
is paragraph sequences (meaning that they are the
least important) and the last to remove is the near-
est neighbor features. Nevertheless, performance al-
ways drops when a feature type is removed, indicat-
ing that all three feature types contribute positively
to overall performance. The fact that flat paragraph
sequence features proved to be least useful high-
lights the importance of the structured methods we
presented for using paragraph sequence information.
8 Conclusions
We have investigated the relatively less-studied
problem of modeling the organization in student es-
says. The contributions of our work include the
novel application of two techniques from bioinfor-
matics and machine learning ? sequence align-
ment and string kernels, as well as the introduc-
tion of alignment kernels ? to essay scoring. We
showed that each technique makes a significant con-
tribution to a scoring system, and we hope that this
work will increase awareness of these powerful tech-
niques among NLP researchers. Finally, to stimulate
work on this problem, we make our corpus of anno-
tated essays available to other researchers.
238
Acknowledgments
We thank the three reviewers for their comments.
Our six annotators, Andrew Hubbs, Karin Khoo,
Jayne Koath, Christopher Maier, Andrew Mallon,
and Cory Thornton, all deserve numerous thanks,
because without the countless hours they each spent
annotating hundreds of essays, none of the research
described in this paper would have been possible.
References
Yigal Attali and Jill Burstein. 2006. Automated es-
say scoring with e-rater V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the ACL, pages 141?148.
Regina Barzilay and Lillian Lee. 2002. Bootstrapping
lexical choice via multiple-sequence alignment. In
Proceedings of EMNLP, pages 164?171.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In HLT-NAACL 2003: Main Pro-
ceedings, pages 16?23.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Main Proceedings, pages 113?120.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT/EMNLP, pages 724?731.
Jill Burstein, Martin Chodorow, and Claudia Leacock.
2004. Automated essay evaluation: The Criterion on-
line writing evaluation service. AI Magazine, 25(3),
27?36.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.
Finding the write stuff: Automatic identification of
discourse structure in student essays. IEEE Intelligent
Systems, 18(1):32?39.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Proceed-
ings of the ACL, pages 263?270.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
Scott Elliot. 2001. IntelliMetric: From here to validity.
Paper presented at the annual meeting of the American
Educational Research Association, Seattle, WA.
Micha Elsner, Joseph Austerweil, and Eugene Charniak.
2007. A unified local and global model for discourse
coherence. In NAACL HLT 2007: Proceedings of the
Main Conference, pages 436?443.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses universitaires de
Louvain.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In HLT-NAACL 2004: Main
Proceedings, pages 185?192.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and Alexan-
der Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning, pages 44?56. MIT Press.
Thomas K. Landauer, Darrell Laham, and Peter W.
Foltz. 2003. Automated scoring and annotation of
essays with the Intelligent Essay AssessorTM. In Auto-
mated Essay Scoring: A Cross-Disciplinary Perspec-
tive, pages 87?112.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Christopher J. C. H. Watkins. 2002.
Text classification using string kernels. Journal of Ma-
chine Learning Research, 2:419?444.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring systems.
Natural Language Engineering, 10(1):25?55.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. In Proceedings of the
ACL, pages 335?342.
Saul Ben Needleman and Christian Dennis Wunsch.
1970. A general method applicable to the search for
similarities in the amino acid sequence of two proteins.
Journal of Molecular Biology, 48(3):443?453, March.
Mark Shermis and Jill Burstein. 2003. Automated Essay
Scoring: A Cross-Disciplinary Perspective. Lawrence
Erlbaum Associates, Inc., Mahwah, NJ.
Mark Shermis, Jill Burstein, Derrick Higgins, and Klaus
Zechner. 2010. Automated essay scoring: Writing
assessment and instruction. In International Encyclo-
pedia of Education (3rd edition), pages 20?26.
Tony Silva. 1993. Toward an understanding of the dis-
tinct nature of L2 writing: The ESL research and its
implications. 27(4):657?677.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 803?810.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference systems
based on kernels methods. In Proceedings of COL-
ING, pages 961?968.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings of ICML, pages 412?420.
239
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1069?1080,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning the Information Status of Noun Phrases in Spoken Dialogues
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{altaf,vince}@hlt.utdallas.edu
Abstract
An entity in a dialogue may be old, new,
or mediated/inferrable with respect to the
hearer?s beliefs. Knowing the information
status of the entities participating in a dia-
logue can therefore facilitate its interpreta-
tion. We address the under-investigated prob-
lem of automatically determining the informa-
tion status of discourse entities. Specifically,
we extend Nissim?s (2006) machine learning
approach to information-status determination
with lexical and structured features, and ex-
ploit learned knowledge of the information
status of each discourse entity for coreference
resolution. Experimental results on a set of
Switchboard dialogues reveal that (1) incor-
porating our proposed features into Nissim?s
feature set enables our system to achieve state-
of-the-art performance on information-status
classification, and (2) the resulting informa-
tion can be used to improve the performance
of learning-based coreference resolvers.
1 Introduction
Information status is not a term unfamiliar to re-
searchers working on discourse processing prob-
lems. It describes the extent to which a discourse en-
tity, which is typically a noun phrase (NP), is avail-
able to the hearer given the speaker?s assumptions
about the hearer?s beliefs. According to Nissim et
al. (2004), a discourse entity can be new, old, or me-
diated. Informally, a discourse entity is (1) old to
the hearer if it is known to the hearer and has pre-
viously been referred to in the dialogue, (2) new if
it is unknown to her and has not been previously re-
ferred to; and (3) mediated if it is newly mentioned
in the dialogue but she can infer its identity from
a previously-mentioned entity. Information status
is a subject that has received a lot of attention in
theoretical linguistics (Halliday, 1976; Prince, 1981;
Hajic?ova?, 1984; Vallduv??, 1992; Steedman, 2000).
Knowing the information status of discourse enti-
ties can potentially benefit many NLP applications.
One such task is anaphora resolution. While there is
general belief that definite descriptions are mostly
anaphoric, Vieira and Poesio (2000) empirically
show that only 30% of these NPs are anaphoric.
Without being able to determine whether an NP is
anaphoric, an anaphora resolver will attempt to re-
solve every NP, potentially damaging its precision.
Since new entities are by definition new to the hearer
and therefore cannot refer to a previously-introduced
NP, knowledge of information status could be used
to improve anaphora resolution.
Despite the potential usefulness of information
status in NLP tasks, there has been little work on
learning the information status of discourse entities.
To investigate the plausibility of learning informa-
tion status, Nissim et al (2004) annotate a set of
Switchboard dialogues with such information1 , and
subsequently present a rule-based approach and a
learning-based approach to acquiring such knowl-
edge from the manual annotations (Nissim, 2006).
Our goals in this paper are two-fold. First, we
describe a learning approach to the under-studied
problem of determining the information status of
discourse entities that extends Nissim?s (2006) fea-
ture set with two novel types of features: lexical
features and structured features based on syntactic
parse trees. Second, we employ the automatically
1These and other linguistic annotations on the Switchboard
dialogues were later released by the LDC as part of the NXT
corpus, which is described in detail in Calhoun et al (2010).
1069
acquired knowledge of information status for coref-
erence resolution. Experimental results on Nissim et
al.?s (2004) corpus of Switchboard dialogues show
that (1) adding our linguistic features to Nissim?s
feature set enables our system to outperform her sys-
tem by 8.1% in F-measure, and (2) learned knowl-
edge of information status can be used to improve
coreference resolvers by 1.1?2.6% in F-measure.
The rest of this paper is organized as follows. We
first illustrate with examples the concepts of new,
old, and mediated entities. Then, we describe the
dataset and the feature set that Nissim (2006) used
in her approach. After that, we introduce our lexi-
cal and structured features. Finally, we evaluate the
determination of information status as a standalone
task and in the context of coreference resolution.
2 Old, New, and Mediated Entities
Since the concepts of old, new, and mediated entities
are not widely known to researchers working outside
the area of discourse processing, in this section we
will explain them in more detail.
The terms old and new information have meant
a variety of things over the years (Allerton, 1978;
Prince, 1981; Horn, 1986). Since we use Nissim
et al?s (2004) corpus for training and evaluation,
the definitions of these concepts we present here are
those that Nissim et al used to annotate their cor-
pus. According to Nissim et al, their definitions are
built upon Prince?s (1981), and the categorization
into old, new, and mediated entities resemble those
of Strube (1998) and Eckert and Strube (2001).
Old. As mentioned before, an entity is old if it is
both known to the hearer and has been mentioned in
the conversation. More precisely, an entity is old if
(1) it is coreferential with an entity introduced ear-
lier, (2) it is a generic pronoun, or (3) it is a personal
pronoun referring to the dialogue participants. To
exemplify, consider the following sentences.
(1) I was angry that he destroyed my tent.
(2) You cannot leave until the test is over.
In Example 1, my is an old entity because it is
coreferent with I. In Example 2, You is an old entity
because it is a generic pronoun.
Mediated. An entity is mediated if it has not been
previously introduced in the conversation, but can be
inferred from already-mentioned entities or is gener-
ally known to the hearer. More specifically, an entity
is mediated if (1) it is a generally known entity (e.g.,
the Earth, China, and most proper names), (2) it is
a bound pronoun, or (3) it is an instance of bridging
(i.e., an entity that is inferrable from a related entity
mentioned earlier in the dialogue). As an example,
consider the following sentences.
(3a) He passed by the door of Mary?s house and
saw that the door was painted purple.
(3b) He passed by Mary?s house and saw that
the door was painted purple.
In Example 3a, by the time the hearer processes
the second occurrence of the door, she has already
had a mental entity corresponding to the door (af-
ter processing the first occurrence). As a result, the
second occurrence of the door is an old entity. In
Example 3b, on the other hand, the hearer is not as-
sumed to have any mental representation of the door
in question, but she can infer that the door she saw
was part of Mary?s house. Hence, this occurrence of
the door is a mediated entity. In general, an entity
that is related to an earlier entity via a part-whole
relation or a set-subset relation is mediated.
New. An entity is new if it has not been introduced
in the dialogue and the hearer cannot infer it from
previously mentioned entities.
In case more than one class is appropriate for
a given entity, Nissim et al employ additional tie-
breaking rules. Suppose, for instance, that we have
two occurrences of China in a dialogue. The second
occurrence can be labeled as old (because it is coref-
erential with an earlier entity) or mediated (because
it is a generally known entity). According to Nissim
et al?s rules, the entity will be labeled as old.
3 Dataset
We employ Nissim et al?s (2004) dataset, which
comprises 147 Switchboard dialogues. A total of
68,992 NPs are annotated with information status:
51.2% of them are labeled as old, 34.5% as mediated
(henceforth med), and 14.3% as new. Nissim (2006)
randomly split the instances created from these NPs
into a training set (for classifier training), a develop-
ment set (for feature development), and an evalua-
tion set (for testing). Hence, the NPs from the same
1070
Training Test
old 31358 (51.7%) 3931 (47.4%)
med 20778 (34.2%) 3036 (36.6%)
new 8567 (14.1%) 1322 (16.0%)
total 60703 (100%) 8289 (100%)
Table 1: Information status distribution of NPs.
document may be split across different sets.
Unlike Nissim (2006), we partition the 147 dia-
logues (rather than the instances) into a training set
(117 dialogues) and a test set (30 dialogues). In
other words, we do not randomize the instances, as
we believe that it represents an unrealistic evalua-
tion setting, for the following reasons. First, in prac-
tice, the test dialogues may not be available until test
time. Second, we may want to examine how a sys-
tem performs on a given dialogue. Finally, random-
izing the instances does not allow us to apply learned
knowledge of information status to coreference res-
olution, which needs to be performed for each dia-
logue. The information status distribution of the NPs
in the training and test sets are shown in Table 1.
4 Baseline System
In this section, we describe our baseline system,
which adopts a machine learning approach to deter-
mining the information status of a discourse entity.
Building SVM classifiers for information-status
determination. We employ the support vector
machine (SVM) learner as implemented in the
SVMlight package (Joachims, 1999) to train three
binary classifiers, one for predicting each of the
three possible classes (i.e., new, old, and med), us-
ing a linear kernel in combination with the one-
versus-all training scheme.2 Each training instance
represents a single NP and consists of the seven
morpho-syntactic features that Nissim (2006) used
in her learning-based approach (see Table 2 for an
overview). Following Nissim, we extract the NPs
directly from the gold-standard annotations, but the
features are computed entirely automatically.
2SVM was chosen because it provides the option to employ
kernels. The reason why we train three binary classifiers rather
than just one multi-class classifier (using SVMmulticlass) is that
SVMmulticlass does not permit the use of a non-linear kernel,
which we will need to incorporate structured features later on.
Feature Values
full prev mention numeric
mention time {first,second,more}
partial prev mention {yes,no,NA}
determiner {bare,def,dem,indef,poss,NA}
NP type {pronoun,common,proper,other}
NP length numeric
grammatical role {subject,subjpass,pp,other}
Table 2: Nissim?s feature set.
The seven features are all intuitively useful for
determining information status. For instance, if an
NP, NPk, and a discourse entity that appears before
it have the same string (full prev mention), then NPk
is likely to be an old entity. Mention time is the cat-
egorical version of full prev mention and therefore
serves to detect old entities. Partial prev mention
is useful for detecting mediated entities, especially
those that have a set-subset relation with a preceding
entity. For instance, your dogs would be considered
a partial previous mention of my dogs or my three
dogs. The value ?NA? stands for ?not applicable?,
and is used for pronouns. Determiners and NP type
are likely to be helpful for all three categories. For
instance, indefinite NPs and pronouns are likely to
be new and old, respectively. The ?NP length? fea-
ture is motivated by the observation that old entities
tend to contain less lexical materials than new enti-
ties. For instance, subsequent references to Barack
Obama may simply be Obama.
Applying the classifiers. To determine the infor-
mation status of an NP in a test dialogue, we create
an instance for it as during training and present it
independently to the three binary SVM classifiers,
each of which returns a real value representing the
signed distance of the instance from the hyperplane.
We assign the instance to the class that is associated
with the most positive classification value.
5 Our Features
We propose to extend Nissim?s (2006) feature set
with two types of features.
5.1 Lexical Features
As discussed, an entity should be labeled as med if it
has not been introduced in the dialogue but is gener-
1071
ally known to a human. Whether an entity is ?gener-
ally known? may be easily determined by a human
but not by a machine, since world knowledge is in-
volved in the decision process. In particular, Nis-
sim?s feature set does not contain any features that
encode the notion of a ?generally known? entity.
Hence, it would be desirable to augment Nissim?s
feature set with features that indicate whether an en-
tity is generally known or not. One way to do this is
to (1) create a list of generally known entities, and
then (2) create a binary feature that has the value
True if and only if the entity under consideration ap-
pears in this list. The question, then, is: how can
we obtain the list of generally known entities? We
may manually assemble this list, but this could be
a labor-intensive task. As a result, we propose to
acquire this kind of world knowledge automatically
from annotated data.
Specifically, we augment Nissim?s feature set
with the set of unigrams that appear in the training
data. Given a training/test instance (i.e., discourse
entity), we compute the values of its unigram fea-
tures (henceforth lexical features) as follows. For
each unigram, we check if it appears in the string
representing the discourse entity. If so, its feature
value is 1; otherwise, its value is 0. For instance, if
the entity is the red hat, then all of its lexical features
except the, red, and hat will have a value of 0.
It should perhaps not be too difficult to see why
these lexical features are useful for the information-
status classifier: these features enable the SVM
learner to determine the extent to which a unigram
correlates with each class. For instance, from the an-
notated data, the learner will learn that any instance
of China cannot be labeled as new, and the deci-
sion of whether it should be an old entity or a med
entity depends on whether it is coreferential with a
previously-mentioned entity. Hence, the use of lex-
ical features allows the learner to implicitly acquire
some world knowledge.
We believe that lexicalization is an important step
towards building high-performance text-processing
systems. In fact, lexicalized models have demon-
strated their effectiveness in other areas of language
processing, such as syntactic and semantic parsing.
While lexicalized models may be less portable to
new genres and domains than their unlexicalized
counterparts, we believe that this issue can be han-
dled via domain adaptation techniques and should
not be a reason against lexicalization.
5.2 Structured Features
In Nissim?s (2006) feature set, there are a couple of
features that capture NP-internal information, such
as determiner, NP length, and NP type. However,
there is only one feature that captures the syntactic
context of an NP, grammatical role, which is com-
puted based on the parse tree in which the NP re-
sides. This is arguably a very shallow representation
of its syntactic context. We hypothesize that we can
train more accurate information-status classifiers if
we have access to a richer representation of syntac-
tic context. This motivates us to employ syntactic
parse trees directly as features.
Before describing how this can be done, recall
that in a traditional learning setting, the feature set
employed by an off-the-shelf learning algorithm typ-
ically consists of flat features (i.e., features whose
values are discrete- or real-valued, as the ones de-
scribed in the previous section). Advanced machine
learning algorithms such as SVMs, on the other
hand, have enabled the use of structured features
(i.e., features whose values are structures such as
parse trees), owing to their ability to employ ker-
nels to efficiently compute the similarity between
two potentially complex structures.
Perhaps the main advantage of employing struc-
tured features is simplicity. To understand this ad-
vantage, consider learning in a setting where we can
only employ flat features. If we want to use informa-
tion from a parse tree as features in this setting, we
will need to design heuristics to extract the desired
parse-based features from parse trees. For certain
tasks, designing a good set of heuristics can be time-
consuming and sometimes difficult. On the other
hand, SVMs enable a parse tree to be employed di-
rectly as a structured feature, obviating the need to
design such heuristics.
Given two parse trees (as features), we com-
pute their similarity using a convolution tree ker-
nel (Collins and Duffy, 2001), which efficiently enu-
merates the number of common substructures in the
two trees via dynamic programming. Note, however,
that while we want to use a parse tree directly as a
feature, we do not want to use the entire parse tree as
a feature. Specifically, while using the entire parse
1072
tree enables a richer representation of the syntactic
context than using a partial parse tree, the increased
complexity of the tree also makes it more difficult
for the SVM learner to make generalizations.
To strike a better balance between having a rich
representation of the context and improving the
learner?s ability to generalize, we extract a substruc-
ture from a parse tree and use it as the value of the
structured feature of an instance. Specifically, given
an instance corresponding to discourse entity e, we
extract the substructure from the parse tree contain-
ing e as follows. Let n(e) be the root of the sub-
tree that spans all and only the words in e, and let
Parent(n(e)) be its immediate parent node. We (1)
take the subtree rooted at Parent(n(e)), (2) replace
each leaf node in this subtree with a node labeled
X, (3) replace the subtree rooted at n(e) with a leaf
node labeled Y, and (4) use the subtree rooted at
Parent(n(e)) as the structured feature for the in-
stance corresponding to e. Intuitively, the first three
steps aim to provide generalizations by simplifying
the tree. For instance, step (1) allows us to focus on
using a small window as the context. Steps (2) and
(3) help generalization by ignoring the words within
e and its context. Note that using two labels, X and
Y, enables the kernel to distinguish the discourse en-
tity under consideration from its context within this
substructure. In addition, we simply use a single
node (Y) to represent the discourse entity, since any
NP-internal information has presumably been cap-
tured by the flat features. We compute these struc-
tured features using hand-annotated parse trees.
While structured features have been employed for
a multitude of tasks in syntax, semantics, and in-
formation extraction such as syntactic parsing (e.g.,
Collins (2002)), semantic parsing (e.g., Moschitti
(2004)), named entity recognition (e.g., Cumby and
Roth (2003), and relation extraction (e.g., Zelenko
et al (2003)), the same is not true for discourse
processing tasks. We hope that our use of struc-
tured features for information-status classification
can promote their use in discourse processing.
5.3 Combining Kernels
Recall that the flat features are computed using a
linear kernel, while the structured features are com-
puted using a tree kernel. If we want our learner to
make use of more than one of these types of features,
we need to employ a composite kernel to combine
them. Specifically, we define and employ the fol-
lowing composite kernel:
Kc(F1, F2) = K1(F1, F2) + K2(F1, F2),
where F1 and F2 are the full set of features that rep-
resent the two entities under consideration, and K1
and K2 are the kernels we are combining. To ensure
that both kernels contribute equally to the compos-
ite kernel, we normalize the values they return to the
range [0,1].
6 Evaluation
Next, we evaluate the effectiveness of our features
in improving information-status classification.
6.1 Results and Discussion
Results of four information-status classification sys-
tems are shown in Table 3. Under Original Nis-
sim, we have the results copied verbatim from Nis-
sim?s (2006) paper. Baseline is the aforementioned
baseline system, which is trained using Nissim?s fea-
ture set. Baseline+Lexical is the system trained us-
ing Nissim?s feature set augmented with lexical fea-
tures. Finally, Baseline+Both is the system trained
using Nissim?s feature set augmented with both lex-
ical and structured features. For each system, we
show the recall (R), precision (P), and F-measure (F)
of each of the three classes: old, new, and med. Be-
fore we describe the results, two points deserve men-
tion. First, as noted earlier, Nissim partitioned the
dialogues into training and test folds in a different
way than us. In particular, Original Nissim and the
remaining three systems were not evaluated on the
same set of test instances. Hence, the Original Nis-
sim results are not directly comparable with those of
the other systems. We show them here just to pro-
vide another point of reference. Second, the results
of the remaining three systems were obtained by ag-
gregating the results of three binary SVM classifiers,
as described earlier.
Comparing Baseline and Baseline+Lexical, we
see that augmenting Nissim?s feature set with lexical
features improves the F-measure scores on all three
classes. In particular, the F-measure and recall for
med rise considerably by 3.0 and 7.8, respectively.
This provides indirect empirical support for our ear-
lier hypothesis that the med class can benefit from
1073
Original Nissim Baseline Baseline+Lexical Baseline+Both
R P F R P F R P F R P F
old 91.5 94.1 92.8 91.2 85.8 88.5 88.7 91.7 90.2 93.0 95.2 94.1
med 87.6 68.1 76.6 84.7 62.7 72.1 92.5 63.2 75.1 89.1 70.9 79.0
new 22.3 56.3 32.0 30.2 66.4 41.5 32.1 68.3 43.7 34.4 71.5 46.5
Accuracy 79.5 74.1 76.3 82.2
Table 3: Per-class performance of four information-status classifiers.
the shallow world knowledge that these lexical fea-
tures help to ?extract? from annotated data.
Comparing Baseline+Lexical and Baseline+Both,
we see that the addition of structured features en-
ables a further boost to performance: F-measure in-
creases by 2.8?3.9 for the three classes. These re-
sults substantiate our hypothesis that employing a
richer representation of syntactic context is benefi-
cial to information-status classification.
Comparing Baseline and Baseline+Both, we see
that F-measure improves considerably by 5?6.9 for
the three classes. Overall, these results provide sug-
gestive evidence that both types of features are ef-
fective at improving an information-status classifier
that employs Nissim?s features.
For further comparison, we show the classifica-
tion accuracies of the four systems in the last row
of Table 3. As we can see, adding lexical features
to the baseline features improves accuracy by 2.2%,
and adding structured features further improves ac-
curacy by 5.9%. Our two types of features, when
used in combination with Nissim?s features, improve
the baseline substantially by an accuracy of 8.1%.
Note that while our results and Original Nissim?s
are not directly comparable, the two systems are
consistent in terms of the relative performance for
the three classes: best for old and worst for new. The
poor performance for new is largely a consequence
of its low recall, which can in turn be attributed to its
lower representation in the dataset. Since many new
instances are misclassified, a natural question is: are
these instances misclassified as old or med? Simi-
lar questions can be raised for old and med, despite
their substantially higher recall values than new.
To answer these questions, we need to better
understand the kind of errors made by our ap-
proach. Consequently, we show in Table 4 the con-
fusion matrix generated from the test set for our
C? old med new
G ?
old 3656 257 18
med 167 2706 163
new 17 850 455
Table 4: Confusion matrix for the Baseline+Both
classifier. C=Classifier tag; G=Gold tag
best-performing information-status classifier, Base-
line+Both. The rows and the columns correspond
to the gold tags and the classifier tags, respectively.
As we can see, these numbers seem to suggest the
?in-between? nature of mediated entities: when an
old or new entity is misclassified, it is typically mis-
classified as med (rows 1 and 3); however, when a
med entity is misclassified, it is equally likely to be
misclassified as old and new (row 2).
These results are perhaps not surprisingly, since
intuitively med entities bear some resemblance to
both old and new entities. For instance, the simi-
larity between med and old stems from the fact that
different instances of the same entity (e.g., China)
can receive one of these two labels, with the deci-
sion dependent on whether the entity was previously
mentioned in the dialogue. On the other hand, med
and new are similar in that it may sometimes be dif-
ficult even for a human to determine whether certain
entities should be labeled as med or new, since the
decision depends on whether she believes these en-
tities are generally known or not.
6.2 Relation to Anaphoricity Determination
Anaphoricity determination refers to the task of de-
termining whether an NP is anaphoric or not, where
an NP is considered anaphoric if it is part of a (non-
singleton) coreference chain but is not the head of
the chain (Ng and Cardie, 2002). In other words, an
1074
Anaphoricity Baseline+Ana Baseline+Lexical+Ana Baseline+Both+Ana
R P F R P F R P F R P F
old 91.4 86.6 88.9 91.3 87.3 89.3 90.8 91.7 91.3 92.8 94.9 93.9
med 84.3 63.1 72.2 84.9 64.1 73.1 92.3 64.7 76.1 88.7 71.1 78.9
new 30.8 66.4 42.1 31.1 66.9 42.5 32.9 68.7 44.5 34.1 71.7 46.2
Accuracy 74.7 75.1 77.6 82.0
Table 5: Impact of knowledge of anaphoricity on the information-status classifiers.
NP is anaphoric if and only if it has an antecedent.
Given this definition, anaphoricity determination
bears resemblance to information-status classifica-
tion. For instance, an old entity is anaphoric, since it
has been introduced earlier in the conversation and
therefore have an antecedent. Similarly, a new or
med entity is non-anaphoric, since the entity has not
been previously introduced in the conversation and
therefore cannot have an antecedent.
There has been a lot of recent work on anaphoric-
ity determination (e.g., Bean and Riloff (1999),
Uryupina (2003), Ng (2004), Denis and Baldridge
(2007), Versley et al (2008), Ng (2009), Zhou and
Kong (2009)). Given the similarity between this task
and information-status classification, a natural ques-
tion is: will the anaphoricity features previously de-
veloped by coreference researchers be helpful for
information-status classification? To answer this
question, we (1) assemble a feature set composed
of the 26 anaphoricity features previously used by
Rahman and Ng (2009),3 and then (2) repeat the ex-
periments in Table 3, except that we augment the
feature set used in each of these experiments with
the anaphoricity features we assembled in step (1).
Results with the anaphoricity features are shown
in Table 5. Under Anaphoricity, we have the results
obtained using only the 29 anaphoricity features. As
we can see, these results are comparable to those
obtained using the Baseline features. Comparing
each of Baseline+Ana and Baseline+Lexical+Ana
with the corresponding experiments in Table 3, we
see that the addition of anaphoricity features yields
a mild performance improvement, which is consis-
tent over all three classes. However, comparing the
last column of the two tables, we can see that in the
3These 26 features are derived from those employed by Ng
and Cardie?s (2002) anaphoricity determination system. See
Footnote 2 of Rahman and Ng (2009) for details.
presence of the structured features, the anaphoricity
features do not contribute positively to overall per-
formance. Hence, in the coreference experiments in
the next section, we will not employ anaphoricity
features for information-status classification.
7 Application to Coreference Resolution
Since the significance of information-status classi-
fication stems in part from the potential benefits it
brings to higher-level NLP applications, we deter-
mine whether our information-status classification
systems can offer benefits to learning-based coref-
erence resolution. Since the 147 information-status
annotated dialogues are also coreference annotated,
we use them in our coreference evaluation. To our
knowledge, our work represents the first attempt to
report coreference results on this dataset.
7.1 Coreference Models
While the so-called mention-pair coreference model
has dominated coreference research for more than
a decade since its appearance in the mid-1990s, a
number of new coreference models have been pro-
posed in recent years. To investigate whether these
newer, presumably more sophisticated, coreference
models can better exploit the automatically acquired
information-status information, we will evaluate the
usefulness of information-status information when
used in combination with two different coreference
models, the aforementioned mention-pair model and
the recently-developed cluster-ranking model.
7.1.1 Mention-Pair Model
The mention-pair (MP) model, proposed by Aone
and Bennett (1995) and McCarthy and Lehnert
(1995), is a classifier that determines whether two
NPs are co-referring or not. Each instance i(NPj ,
NPk) corresponds to two NPs, NPj and NPk , and is
represented by 39 features. Table 1 of Rahman and
1075
Ng (2009) contains a detailed description of these
features. Linguistically, they can be divided into
four categories: string-matching, grammatical, se-
mantic, and positional. They can also be categorized
based on whether they are relational or not. Specifi-
cally, relational features capture the relationship be-
tween NPj and NPk , whereas non-relational features
capture the linguistic property of one of these NPs.
We follow Soon et al?s (2001) method for cre-
ating training instances. Specifically, we create (1)
a positive instance for each anaphoric NP NPk and
its closest antecedent NPj ; and (2) a negative in-
stance for NPk paired with each of the intervening
NPs, NPj+1, NPj+2, . . ., NPk?1. The classification
associated with a training instance is either positive
or negative, depending on whether the two NPs are
coreferent. To train the MP model, we use the SVM
learner from SVMlight (Joachims, 1999).4
After training, the classifier is used to identify an
antecedent for an NP in a test text. Specifically,
each NP, NPk, is compared in turn to each preced-
ing NP, NPj , from right to left, and selects NPj as its
antecedent if the pair is classified as coreferent. The
process terminates as soon as an antecedent is found
for NPk or the beginning of the text is reached.
Despite its popularity, the MP model has two
major weaknesses. First, since each candidate an-
tecedent for an NP to be resolved (henceforth an ac-
tive NP) is considered independently of the others,
this model only determines how good a candidate
antecedent is relative to the active NP, but not how
good a candidate antecedent is relative to other can-
didates. So, it fails to answer the critical question of
which candidate antecedent is most probable. Sec-
ond, it has limitations in its expressiveness: the in-
formation extracted from the two NPs alone may not
be sufficient for making a coreference decision.
7.1.2 Cluster-Ranking Model
The cluster-ranking (CR) model, proposed by
Rahman and Ng (2009), addresses the two weak-
nesses of the MP model by combining the strengths
of the entity-mention model (e.g., Luo et al (2004),
Yang et al (2008)) and the mention-ranking model
(e.g., Denis and Baldridge (2008)). Specifically,
the CR model ranks the preceding clusters for an
4For this and subsequent uses of the SVM learner in our
experiments, we set al parameters to their default values.
active NP so that the highest-ranked cluster is the
one to which the active NP should be linked. Em-
ploying a ranker addresses the first weakness, as
a ranker allows all candidates to be compared si-
multaneously. Considering preceding clusters rather
than antecedents as candidates addresses the second
weakness, as cluster-level features (i.e., features that
are defined over any subset of NPs in a preceding
cluster) can be employed.
Since the CR model ranks preceding clusters, a
training instance i(cj , NPk) represents a preceding
cluster cj and an anaphoric NP NPk. Each instance
consists of features that are computed based solely
on NPk as well as cluster-level features, which de-
scribe the relationship between cj and NPk . Mo-
tivated in part by Culotta et al (2007), we create
cluster-level features from the relational features in
our feature set using four predicates: NONE, MOST-
FALSE, MOST-TRUE, and ALL. Specifically, for each
relational feature X, we first convert X into an equiv-
alent set of binary-valued features if it is multi-
valued. Then, for each resulting binary-valued fea-
ture Xb, we create four binary-valued cluster-level
features: (1) NONE-Xb is true when Xb is false be-
tween NPk and each NP in cj ; (2) MOST-FALSE-Xb
is true when Xb is true between NPk and less than half
(but at least one) of the NPs in cj ; (3) MOST-TRUE-
Xb is true when Xb is true between NPk and at least
half (but not all) of the NPs in cj ; and (4) ALL-Xb is
true when Xb is true between NPk and each NP in cj .
We train a cluster ranker to jointly learn
anaphoricity determination and coreference reso-
lution using SVMlight?s ranker-learning algorithm.
Specifically, for each NP, NPk, we create a train-
ing instance between NPk and each preceding clus-
ter cj using the features described above. Since we
are learning a joint model, we need to provide the
ranker with the option to start a new cluster by creat-
ing an additional training instance that contains fea-
tures that solely describes NPk. The rank value of
a training instance i(cj , NPk) created for NPk is the
rank of cj among the competing clusters. If NPk is
anaphoric, its rank is HIGH if NPk belongs to cj , and
LOW otherwise. If NPk is non-anaphoric, its rank is
LOW unless it is the additional training instance de-
scribed above, which has rank HIGH.
After training, the cluster ranker processes the
NPs in a test text in a left-to-right manner. For each
1076
active NP, NPk , we create test instances for it by pair-
ing it with each of its preceding clusters. To allow
for the possibility that NPk is non-anaphoric, we cre-
ate an additional test instance that contains features
that solely describe the active NP (similar to what
we did in the training step above). All these test in-
stances are then presented to the ranker. If the addi-
tional test instance is assigned the highest rank value
by the ranker, then NPk is classified as non-anaphoric
and will not be resolved. Otherwise, NPk is linked to
the cluster that has the highest rank.
7.2 Coreference Experiments
7.2.1 Experimental Setup
The training/test split we use in the coreference
experiments is the same as that in the information-
status experiments. Specifically, we use the train-
ing set to train both the information-status classifier
and our coreference models, apply the information-
status classifier to each discourse entity in the test
set, and have the coreference models resolve all
and only those NPs that are labeled as old by the
information-status classifier. Our decision to allow
the coreference models to resolve only the old enti-
ties is motivated by the fact that med and new entities
have not been previously introduced in the conversa-
tion and therefore do not have antecedents. The NPs
used by the coreference models are the same as those
accessible to the information-status classifier.
We employ two scoring programs, B3 (Bagga and
Baldwin, 1998) and ?3-CEAF (Luo, 2005), to score
the output of a coreference model. Given a gold-
standard (i.e., key) partition, KP , and a system-
generated (i.e., response) partition, RP , B3 com-
putes the recall and precision of each NP and av-
erages these values at the end. Specifically, for each
NP, NPj , B3 first computes the number of NPs that
appear in both KPj and RPj , the clusters containing
NPj in KP and RP , respectively, and then divides
this number by |KPj| and |RPj| to obtain the re-
call and precision of NPj , respectively. On the other
hand, CEAF finds the best one-to-one alignment
between the key clusters and the response clusters
using the Kuhn-Munkres algorithm (Kuhn, 1955),
where the weight of an edge connecting two clusters
is equal to the number of NPs that appear in both
clusters. Precision and recall are equal to the sum of
the weights of the edges in the alignment divided by
the total number of NPs in the response and the key,
respectively.
7.2.2 Results and Discussion
As our baseline, we employ our coreference mod-
els to generate NP partitions on the test documents
without using any knowledge of information status.
Results, reported in terms of recall (R), precision
(P), and F-measure (F) using B3 and ?3-CEAF, are
shown in row 1 of Table 6.5 As we can see, the
baseline achieves B3 F-measures of 69.2 (MP) and
74.5 (CR) and CEAF F-measures of 61.6 (MP) and
68.5 (CR). These results suggest that the CR model
is stronger than the MP model, corroborating previ-
ous empirical findings (Rahman and Ng, 2009).
Next, we examine the impact of learned knowl-
edge of information status on the performance of a
coreference model. Since knowledge of information
status enables a coreference model to focus on re-
solving only the old entities, we hypothesize that the
resulting model will have a higher precision than one
that does not employ such knowledge. An equally
important question is: will the F-measure of the re-
sulting model improve? Since we are employing
knowledge of information status in a pipeline coref-
erence architecture where information-status classi-
fication is performed prior to coreference resolution,
errors made by the (upstream) information-status
classifier may propagate to the (downstream) coref-
erence system. Given this observation, we hypoth-
esize that the answer to the aforementioned ques-
tion depends in part on the accuracy of information-
status classification. In particular, the higher the
accuracy of information-status classification is, the
more likely the F-measure of the downstream coref-
erence model will improve. To test this hypothe-
sis, we conduct experiments where we employ the
knowledge provided by the three information-status
classifiers which, as discussed earlier, perform at
varying levels of accuracy ? the first one using only
Nissim?s features, the second one using both lexical
and Nissim?s features, and the last one using Nis-
sim?s features in combination with lexical and parse-
based features ? for our coreference models.
5Since gold-standard NPs are used in our experiments,
CEAF precision is always equal to CEAF recall. For brevity,
we only report F-measure scores for CEAF in the table.
1077
Mention-Pair Model Cluster-Ranking Model
B3 CEAF B3 CEAF
System R P F F R P F F
No knowledge of information status 78.6 61.8 69.2 61.6 78.2 71.1 74.5 68.5
Nissim features only 73.4 67.3 70.2 62.1 73.6 77.4 75.4 69.7
Nissim+Lexical features 71.0 69.5 70.2 61.9 73.7 77.3 75.4 69.9
Nissim+Lexical+Parse features 74.1 66.8 70.3 62.3 77.3 74.0 75.6 71.1
Perfect information status 76.7 68.1 72.1 66.4 77.1 79.5 78.3 74.2
Table 6: B3 and CEAF coreference results.
Results of the coreference models employing
knowledge provided by the three information-status
classifiers are shown in rows 2?4 of Table 6. As ex-
pected, B3 precision increases in comparison to the
baseline, regardless of the coreference model and the
scoring program. In addition, employing knowledge
of information status always improves coreference
performance: F-measure scores increase by 1.0?
1.1% (B3) and 0.3?0.7% (CEAF) for the MP model,
and by 0.9?1.1% (B3) and 1.2?2.6% (CEAF) for
the CR model. These results suggest that the three
information-status classifiers have achieved the level
of accuracy needed for the coreference models to
improve. On the other hand, it is somewhat surpris-
ing that the three information-status classifiers have
yielded coreference systems that perform at essen-
tially the same level of performance.
To understand why better information-status clas-
sification results do not necessarily yield better
coreference performance, we take a closer look at
the results of the coreference resolver employing
Nissim?s features (henceforth NISSIM) and the re-
solver employing our Nissim+Lexical+Parse fea-
tures (henceforth FULL-FEATURE). Among the old
entities that were correctly classified using our fea-
tures and incorrectly classified by Nissim?s features,
we found that the precision of the FULL-FEATURE
system suffered (since in many cases the corefer-
ence models identified wrong antecedents for these
old entities) whereas the NISSIM system remained
unaffected (since the entities were misclassified and
would not be resolved by the models). In addition,
although many med and new entities were correctly
classified using our features and incorrectly classi-
fied (as old) using Nissim?s features, we found that
in many cases no antecedents were identified for
these misclassified entities and hence the precision
of the NISSIM system was not adversely affected.
Finally, we investigate whether our coreference
system could be improved if it had access to per-
fect knowledge of information status (taken directly
from the gold-standard annotations). This experi-
ment will allow us to determine whether the useful-
ness of knowledge of information status for coref-
erence resolution is limited by the accuracy in com-
puting such knowledge. Results are shown in the
last row of Table 6. As we can see, using per-
fect information-status knowledge yields a corefer-
ence system that improves those that employs auto-
matically acquired information-status knowledge by
1.8?4.1% (MP) and 2.7?3.1% (CR) in F-measure.
This indicates that the accuracy in computing such
knowledge does play a role in determining its use-
fulness for coreference resolution.
8 Conclusions
We examined the problem of automatically deter-
mining the information status of discourse entities in
spoken dialogues. In particular, we augmented Nis-
sim?s feature set with two types of features: lexical
features, which capture in a shallow manner world
knowledge implicitly encoded in the annotated data;
and syntactic parse trees, which provide a richer rep-
resentation of the syntactic context in which a dis-
course entity appears than grammatical roles. Re-
sults on 147 Switchboard dialogues demonstrated
the effectiveness of these features: we obtained a
significant improvement of 8.1% in accuracy over
a information-status classifier trained on Nissim?s
feature set. In addition, we evaluated information-
status classification in the context of coreference
resolution, and showed that automatically acquired
knowledge of information status can be profitably
used to improve coreference systems.
1078
Acknowledgments
We thank the three reviewers for their invaluable
comments on an earlier draft of the paper. This work
was supported in part by NSF Grant IIS-0812261.
References
David J. Allerton. 1978. The notion of ?givenness? and
its relations to presupposition and to theme. Lingua,
44:133?168.
Chinatsu Aone and Scott William Bennett. 1995. Eval-
uating automated and manual acquisition of anaphora
resolution strategies. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics, pages 122?129.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of the
Linguistic Coreference Workshop at the First Interna-
tional Conference on Language Resources and Evalu-
ation, pages 563?566.
David Bean and Ellen Riloff. 1999. Corpus-based iden-
tification of non-anaphoric noun phrases. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics, pages 373?380.
Sasha Calhoun, Jean Carletta, Jason Brenier, Neil Mayo,
Dan Jurafsky, Mark Steedman, and David Beaver.
2010. The NXT-format Switchboard corpus: A rich
resource for investigating the syntax, semantics, prag-
matics and prosody of dialogue. Language Resources
and Evaluation, 44(4):387?419.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14, pages 625?632.
Michael Collins. 2002. Ranking algorithms for named-
entity extraction: Boosting and the voted perceptron.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 489?
496.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In Human Language Technologies 2007:
The Conference of the North American Chapter of the
Association for Computational Linguistics; Proceed-
ings of the Main Conference, pages 81?88.
Chad Cumby and Dan Roth. 2003. On kernel methods
for relational learning. In Proceedings of the 20th In-
ternational Conference on Machine Learning, pages
107?114.
Pascal Denis and Jason Baldridge. 2007. Global, joint
determination of anaphoricity and coreference reso-
lution using integer programming. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 236?243.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?669.
Miriam Eckert and Michael Strube. 2001. Dialogue acts,
synchronising units and anaphora resolution. Journal
of Semantics, 17(1):51?89.
Eva Hajic?ova?. 1984. Topic and focus. In Contributions
to Functional Syntax, Semantics, and Language Com-
prehension (LLSEE 16), pages 189?202. John Ben-
jamins, Amsterdam.
Michael A. K. Halliday. 1976. Notes on transitivity and
theme in English. Journal of Linguistics, 3(2):199?
244.
Laurence R. Horn. 1986. Presupposition, theme, and
variations. In A. Farley et al, editor, Papers from the
Parasession on Pragmatics and Grammatical Theory
at the 22nd Regional Meeting, pages 168?192. CLS.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and Alexan-
der Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning, pages 44?56. MIT Press.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2(83).
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics, pages 135?142.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 25?
32.
Joseph McCarthy and Wendy Lehnert. 1995. Using de-
cision trees for coreference resolution. In Proceedings
of the Fourteenth International Conference on Artifi-
cial Intelligence, pages 1050?1055.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. In Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics, pages 335?342.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In Proceedings of the 19th In-
ternational Conference on Computational Linguistics,
pages 730?736.
1079
Vincent Ng. 2004. Learning noun phrase anaphoricity
to improve conference resolution: Issues in represen-
tation and optimization. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, pages 151?158.
Vincent Ng. 2009. Graph-cut-based anaphoricity deter-
mination for coreference resolution. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 575?
583.
Malvina Nissim, Shipra Dingare, Jean Carletta, and Mark
Steedman. 2004. An annotation scheme for infor-
mation status in dialogue. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation.
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 94?102.
Ellen Prince. 1981. Toward a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?255. New York, N.Y.: Academic Press.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Michael Strube. 1998. Never look back: An alternative
to centering. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and the 17th International Conference on Computa-
tional Linguistics, pages 1251?1257.
Olga Uryupina. 2003. High-precision identification of
discourse new and unique noun phrases. In Proceed-
ings of the ACL Student Research Workshop, pages 80?
86.
Enric Vallduv??. 1992. The Informational Component.
Garland, New York.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008a. Coreference systems
based on kernels methods. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 961?968.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, and
Sheng Li. 2008. An entity-mention model for coref-
erence resolution with inductive logic programming.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 843?851.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
3:1083?1106.
GuoDong Zhou and Fang Kong. 2009. Global learn-
ing of noun phrase anaphoricity in coreference reso-
lution via label propagation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 978?986.
1080
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 777?789, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Resolving Complex Cases of Definite Pronouns:
The Winograd Schema Challenge
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{altaf,vince}@hlt.utdallas.edu
Abstract
We examine the task of resolving complex
cases of definite pronouns, specifically those
for which traditional linguistic constraints
on coreference (e.g., Binding Constraints,
gender and number agreement) as well as
commonly-used resolution heuristics (e.g.,
string-matching facilities, syntactic salience)
are not useful. Being able to solve this task has
broader implications in artificial intelligence:
a restricted version of it, sometimes referred
to as the Winograd Schema Challenge, has
been suggested as a conceptually and practi-
cally appealing alternative to the Turing Test.
We employ a knowledge-rich approach to this
task, which yields a pronoun resolver that out-
performs state-of-the-art resolvers by nearly
18 points in accuracy on our dataset.
1 Introduction
Despite the significant amount of work on pronoun
resolution in the natural language processing com-
munity in the past forty years, the problem is still
far from being solved. Its difficulty stems in part
from its reliance on sophisticated knowledge sources
and inference mechanisms. The sentence pair below,
which we will subsequently refer to as the shout ex-
ample, illustrates how difficult the problem can be:
(1a) Ed shouted at Tim because he crashed the car.
(1b) Ed shouted at Tim because he was angry.
The pronoun he refers to Tim in 1a and Ed in 1b.
Humans can resolve the pronoun easily, but state-
of-the-art coreference resolvers cannot. The reason
is that humans have the kind of world knowledge
needed to resolve the pronouns that machines do not.
Our world knowledge tells us that if someone is an-
gry, he may shout at other people. Since Ed shouted,
he should be the one who was angry. Our world
knowledge also tells us that we may shout at some-
one who made a mistake and that crashing a car is
a mistake. Combining these two pieces of evidence,
we can easily infer that Tim crashed the car.
Our goal in this paper is to examine the resolu-
tion of complex cases of definite pronouns that ap-
pear in sentences exemplified by the shout example.
Specifically, each sentence (1) has two clauses sepa-
rated by a discourse connective (i.e., the connective
appears between the two clauses, just like because
in the shout example), where the first clause con-
tains two or more candidate antecedents (e.g., Ed
and Tim), and the second clause contains the tar-
get pronoun (e.g., he); and (2) the target pronoun
agrees in gender, number, and semantic class with
each candidate antecedent, but does not have any
overlap in content words with any of them. For con-
venience, we will refer to the target pronoun that ap-
pears in this kind of sentences as a difficult pronoun.
Note that many traditional linguistic constraints
on coreference are no longer useful for resolving dif-
ficult pronouns. For instance, syntactic constraints
such as the Binding Constraints will not be useful,
since the pronoun and the candidate antecedents ap-
pear in different clauses separated by a discourse
connective; and constraints concerning agreement in
gender, number, and semantic class will not be use-
ful, since the pronoun and the candidate antecedents
are compatible with respect to all these attributes.
Traditionally important clues provided by various
777
I(a) The city councilmen refused the demonstrators a permit because they feared violence.
I(b) The city councilmen refused the demonstrators a permit because they advocated violence.
II(a) James asked Robert for a favor, but he refused.
II(b) James asked Robert for a favor, but he was refused.
III(a) Keith fired Blaine but he did not regret.
III(b) Keith fired Blaine although he is diligent.
IV(a) Emma did not pass the ball to Janie, although she was open.
IV(b) Emma did not pass the ball to Janie, although she should have.
V(a) Medvedev will cede the presidency to Putin because he is more popular.
V(b) Medvedev will cede the presidency to Putin because he is less popular.
Table 1: Sample twin sentences. The target pronoun in each sentence is italicized, and its antecedent is boldfaced.
string-matching facilities will not be useful either,
since the pronoun and its candidate antecedents do
not have any words in common.
As in the shout example, we ensure that each sen-
tence has a twin. Twin sentences were used ex-
tensively by researchers in the 1970s to illustrate
the difficulty of pronoun resolution (Hirst, 1981).
We consider two sentences as twins if (1) they
are identical up to and possibly including the dis-
course connective; and (2) the difficult pronouns in
them are lexically identical but have different an-
tecedents. The presence of twins implies that syn-
tactic salience, a commonly-used heuristic in pro-
noun resolution that prefers the selection of syntac-
tically salient candidate antecedents, may no longer
be useful, since the candidate in the subject position
is not more likely to be the correct antecedent than
the other candidates.
To enable the reader to get a sense of how hard it is
to resolve difficult pronouns, Table 1 shows sample
twin sentences from our dataset. Note that state-of-
the-art pronoun resolvers (e.g., JavaRAP (Qiu et al
2004), GuiTaR (Poesio and Kabadjov, 2004), as well
as those designed by Mitkov (2002) and Charniak
and Elsner (2009)) and coreference resolvers (e.g.,
BART (Versley et al2008), CherryPicker (Rahman
and Ng, 2009), Reconcile (Stoyanov et al2010),
the Stanford resolver (Raghunathan et al2010; Lee
et al2011)) cannot accurately resolve the difficult
pronouns in these structurally simple sentences, as
they do not have the mechanism to capture the fine
distinctions between twin sentences. In other words,
when given these sentences, the best that the existing
resolvers can do to resolve the pronouns is guess-
ing. This could be surprising to a non-coreference
researcher, but it is indeed the state of the art.
A natural question is: why do existing resolvers
not attempt to handle difficult pronouns? One rea-
son could be that these difficult pronouns do not
appear frequently in standard evaluation corpora
such as MUC, ACE, and OntoNotes (Bagga, 1998;
Haghighi and Klein, 2009). In fact, the Stanford
coreference resolver (Lee et al2011), which won
the CoNLL-2011 shared task on coreference resolu-
tion, adopts the once-popular rule-based approach,
resolving pronouns simply with rules that encode
the aforementioned traditional linguistic constraints
on coreference, such as the Binding constraints and
gender and number agreement.
The infrequency of occurrences of difficult pro-
nouns in these standard evaluation corpora by no
means undermines their significance, however. In
fact, being able to automatically resolve difficult
pronouns has broader implications in artificial intel-
ligence. Recently, Levesque (2011) has argued that
the problem of resolving the difficult pronouns in
a carefully chosen set of twin sentences, which he
refers to as the Winograd Schema Challenge1, could
serve as a conceptually and practically appealing
alternative to the well-known Turing Test (Turing,
1Levesque (2011) defines a Winograd Schema as a small
reading comprehension test involving the question of which of
the two candidate antecedents for the definite pronoun in a given
sentence is its correct antecedent. Levesque names this chal-
lenge after Winograd because of his pioneering attempt to use a
well-known pair of twin sentences ? specifically the first pair
in Table 1 ? to illustrate the difficulty of natural language un-
derstanding (Winograd, 1972). Strictly speaking, we are ad-
dressing a relaxed version of the Challenge: while Levesque
focuses solely on definite pronouns whose resolution requires
background knowledge not expressed in the words of a sen-
tence, we do not impose such a condition on a sentence.
778
1950). The reason should perhaps be clear given the
above discussion: this is an easy task for a subject
who can ?understand? natural language but a chal-
lenging task for one who can only make intelligent
guesses. Levesque believes that ?with a very high
probability?, anything that can resolve correctly a
series of difficult pronouns ?is thinking in the full-
bodied sense we usually reserve for people?. Hence,
being able to make progress on this task enables us
to move one step closer to building an intelligent ma-
chine that can truly understand natural language.
To sum up, an important contribution of our work
is that it opens up a new line of research involving
a problem whose solution requires a deeper under-
standing of a text. With recent advances in knowl-
edge extraction from text, we believe that time is ripe
to tackle this problem. It is worth noting that some
researchers have focused on other kinds of anaphors
that are hard to resolve, including bridging anaphors
(e.g., Poesio et al2004)) and anaphors referring
to abstract entities, such as those realized by verb
phrases in dialogs (e.g., Byron (2002), Strube and
Mu?ller (2003), Mu?ller (2007)). Nevertheless, to our
knowledge, there has been little work that specifi-
cally targets difficult pronouns.
Given the complexity of our task, we investigate
a variety of sophisticated knowledge sources for re-
solving difficult pronouns, and combine them via a
machine learning approach. Note that there has been
a recent surge of interest in extracting world knowl-
edge from online encyclopedias such as Wikipedia
(e.g., Ponzetto and Strube (2006, 2007), Poesio et
al. (2007)), YAGO (e.g., Bryl et al2010), Rahman
and Ng (2011), Uryupina et al2011)), and Free-
base (e.g., Lee et al2011)). However, the resulting
extractions are primarily IS-A relations (e.g., Barack
Obama IS-A U. S. president), which would not be
useful for resolving definite pronouns.
2 Dataset Creation
We asked 30 undergraduate students who are not af-
filiated with this research to compose sentence pairs
(i.e., twin sentences) that conform to the constraints
specified in the introduction. Each student was also
asked to annotate the candidate antecedents, the tar-
get pronoun, and the correct antecedent for each
sentence she composed. Note that a sentence may
contain multiple pronouns, but exactly one of them
? the one explicitly annotated by its author ? is
the target pronoun. Each sentence pair was cross-
checked by one other student to ensure that it (1)
conforms to the desired constraints and (2) does not
contain pronouns with ambiguous antecedents (in
other words, a human should not be confused as
to which candidate antecedent is the correct one).
At the end of the process, 941 sentence pairs were
considered acceptable, and they formed our dataset.
These sentences cover a variety of topics, ranging
from real events (e.g., Iran?s plan to attack the Saudi
ambassador to the U.S.), to events and characters in
movies (e.g., Batman and Robin), to purely imagi-
nary situations (e.g., the shout example). We parti-
tion these sentence pairs into a training set and a test
set following a 70/30 ratio.
While not requested by us, the students annotated
exactly two candidate antecedents for each sentence.
For ease of exposition, we will assume below that
there are two candidate antecedents per sentence.
3 Machine Learning Framework
Since our goal is to determine which of the two can-
didate antecedents is the correct antecedent for the
target pronoun in each sentence, our system assumes
as input the sentence, the target pronoun, and the two
candidate antecedents.
We employ machine learning to combine the
features derived from different knowledge sources.
Specifically, we employ a ranking-based approach.
Ranking-based approaches have been shown to out-
perform their classification-based counterparts (De-
nis and Baldridge, 2007, 2008; Iida et al2003;
Yang et al2003). Given a pronoun and two can-
didate antecedents, we aim to train a ranking model
that ranks the two candidates such that the correct
antecedent is assigned a higher rank.
More formally, given training sentence Sk con-
taining target pronoun Ak, correct antecedent Ck
and incorrect antecedent Ik, we create two feature
vectors, xCAk and xIAk , where xCAk is generated
from Ak and Ck, and xIAk is generated from Ak
and Ik. The training set consists of ordered pairs
of feature vectors (xCAk , xIAk ), and the goal of the
training procedure is to acquire a ranker that mini-
mizes the number of violations of pairwise rankings
779
provided in the training set. We train this ranker us-
ing Joachims? (2002) SVMlight package. It is worth
noting that we do not exploit the fact that each sen-
tence has a twin in training or testing.
After training, the ranker can be applied to the test
instances, which are created in the same way as the
training instances. For each test instance, the target
pronoun is resolved to the higher-ranked candidate
antecedent.
4 Linguistic Features
We derive linguistic features for resolving difficult
pronouns from eight components, as described be-
low. To enable the reader to keep track of these fea-
tures more easily, we summarize them in Table 2.
4.1 Narrative Chains
Consider the following sentence:
(2) Ed punished Tim because he tried to escape.
Humans resolve he to Tim by exploiting the world
knowledge that someone who tried to escape is bad
and therefore should be punished. Such kind of
knowledge can be extracted from narrative chains.
Narrative chains are partially ordered sets of
events centered around a common protagonist, aim-
ing to encode the kind of knowledge provided by
scripts (Schank and Abelson, 1977). While scripts
are hand-written, narrative chains can be learned
from unannotated text. Below is a chain learned by
Chambers and Jurafsky (2008):
borrow-s invest-s spend-s pay-s raise-s lend-s
As we can see, a narrative chain is composed of a
sequence of events (verbs) together with the roles of
the protagonist. Here, ?s? denotes the subject role,
even though a chain can contain a mix of ?s? and ?o?
(the object role). From this chain, we know that the
person who borrows something (probably money)
may invest, spend, pay, or lend it.
We employ narrative chains to heuristically pre-
dict the antecedent for the target pronoun, and en-
code the prediction as a feature. The heuristic de-
cision procedure operates as follows. Given a sen-
tence, we first determine the event the target pro-
noun participates in and its role in the event. As
an example, we determine that in sentence (2) he
participates in the try event and the escape event
Component # Features Features
Narrative Chains 1 NC
Google 4 G1, G2, G3, G4
FrameNet 4 FN1, FN2, FN3, FN4
Heuristic Polarity 3 HPOL1, HPOL2, HPOL3
Learned Polarity 3 LPOL1, LPOL2, LPOL3
Connective-Based 1 CBR
Relation
Semantic Compat. 3 SC1, SC2, SC3
Lexical Features 68,331 antecedent- independent
and dependent features
Table 2: Summary of the features described in Section 4.
as a subject.2 Second, we determine the event(s)
that the candidate antecedents participate in. In (2),
both candidate antecedents participate in the pun-
ish event. Third, we pair each event participated
by each candidate antecedent with each event par-
ticipated by the pronoun. In our example, we would
create two pairs, (punish, try-s) and (punish, escape-
s). Note that try and escape are associated with the
role of the pronoun that we extracted in the first step.
Fourth, for each such pair, we extract all the narra-
tive chains containing both elements in the pair from
Chambers and Jurafsky?s output.3 This step results
in one chain being extracted, which contains punish-
o and escape-s. In other words, the protagonist in
this chain is the subject of an escape event and the
object of a punish event. Fifth, from the extracted
chain, we obtain the role played by the pronoun (i.e.,
the protagonist) in the event in which the candidate
antecedents participate. In our example, the pronoun
plays an object role in the punish event. Finally, we
extract the candidate antecedent that plays the ex-
tracted role, which in our example is the second an-
tecedent, Tim.4
We create a binary feature, NC, which encodes
this heuristic decision, and compute its value as fol-
lows. Assume in the rest of the paper that i1 and
i2 are the feature vectors corresponding to the first
candidate antecedent and the second candidate an-
2Throughout the paper, the subject/object of an event refers
to its deep rather than surface subject/object. We determine
the grammatical role of an NP using the Stanford dependency
parser (de Marneffe et al2006) and a set of simple heuristics.
3We employ narrative chains of length 12, which are
available from http://cs.stanford.edu/people/
nc/schemas/schemas-size12.
4For an alternative way of using narrative chains for coref-
erence resolution, see Irwin et al2011).
780
tecedent, respectively.5 For our running example,
since Tim is predicted to be the antecedent of he,
the value of NC in i2 is 1, and its value in i1 is 0.
For notational convenience, we write NC(i1)=0 and
NC(i2)=1, and will follow this convention when de-
scribing the features in the rest of the paper.
Finally, we note that NC(i1) and NC(i2) will
both be set to zero if (1) the pronoun and the an-
tecedents do not participate in events, or (2) no nar-
rative chains can be extracted in step 4 above, or (3)
step 4 enables us to extract more than one chain and
these chains indicate that the candidate antecedent
can have both a subject role and an object role.
4.2 Google
Consider the following sentences:
(3a) Lions eat zebras because they are predators.
(3b) The knife sliced through the flesh because
it was sharp.
Humans resolve they to Lions in (3a) by exploiting
the world knowledge that predators attack and eat
other animals. Similarly, humans resolve it to the
knife in (3b) by exploiting the world knowledge that
the word sharp can be used to describe a knife but
not flesh. To acquire this kind of world knowledge,
we learn patterns of word usage from the Web by
issuing search queries. To facilitate our discussion,
let us first introduce some notation. Let a sentence
S be denoted by a triple (Z1, Conn, Z2), where Z1
and Z2 are the clauses preceding and following the
discourse connective Conn, respectively; A ? Z2
be the pronoun governed by the verb V ; W be the
sequence of words following V in S; and C1, C2 ?
Z1 be the candidate antecedents.
Given a sentence, we generate four queries: (Q1)
C1V ; (Q2) C2V ; (Q3) C1V W ; and (Q4) C2V W . If
v is a verb-to-be followed by an adjective J , we gen-
erate two more queries: (Q5) JC1 and (Q6) JC2.
To exemplify, six queries are generated for (3b):
(Q1) ?knife was?; (Q2) ?flesh was?; (Q3) ?knife was
sharp?; (Q4) ?flesh was sharp?; (Q5) ?sharp knife?;
and (Q6) ?sharp flesh?. On the other hand, only four
queries are generated for (3a): (Q1) ?lions are?; (Q2)
5The nth candidate antecedent in a sentence is the nth an-
notated NP encountered when processing the sentence in a left-
to-right manner. In sentence (2), Ed is the first candidate an-
tecedent and Tim is the second.
?zebras are?; (Q3) ?lions are predators?; and (Q4)
?zebras are predators?.
Using the counts returned by Google for these
queries, we create four features, G1, G2, G3, and
G4, whose values are determined by Rules 1, 2, 3,
and 4, respectively, as described below.
Rule 1: if count(Q1) > count(Q2) by at
least x% then G1(i1)=1 and G1(i2)=0;
else if count(Q2) > count(Q1) by at least
x% then G1(i2)=1 and G1(i1)=0; else
G1(i1)=G1(i2)=0.
Rule 2: if count(Q3) > count(Q4) by at
least x% then G2(i1)=1 and G2(i2)=0;
else if count(Q4) > count(Q3) by at least
x% then G2(i2)=1 and G2(i1)=0; else
G2(i1)=G2(i2)=0.
Rule 3: if count(Q5) > count(Q6) by at
least x% then G3(i1)=1 and G3(i2)=0;
else if count(Q6) > count(Q5) by at least
x% then G3(i2)=1 and G3(i1)=0; else
G3(i1)=G3(i2)=0.
Rule 4: if one of G1(i1) and G1(i2) is 1,
then G4(i1)=G1(i1) and G4(i2)=G1(i2);
else if one of G2(i1) and G2(i2) is 1,
then G4(i1)=G2(i1) and G4(i2)=G2(i2);
else if one of G3(i1) and G3(i2) is 1,
then G4(i1)=G3(i1) and G4(i2)=G3(i2);
else G4(i1)=G4(i2)=0.
The role of the threshold x should be obvious: it
ensures that a heuristic decision is made only if the
difference between the counts for the two queries are
sufficiently large, because otherwise there is no rea-
son for us to prefer one candidate antecedent to the
other. In all of our experiments, we set x to 20.
Note that other researchers have also used lexico-
syntactic patterns to generate search queries for
bridging anaphora resolution (e.g., Poesio et al
(2004)), other-anaphora resolution (e.g., Modjeska
et al2003)), and learning selectional preferences
for pronoun resolution (e.g., Yang et al2005)).
However, in each of these three cases, the target re-
lations (e.g., the part-whole relation in the case of
bridging anaphora resolution, and the subject-verb
and verb-object relations in the case of selectional
preferences) are specific enough that they can be ef-
fectively captured by specific patterns. For example,
781
to determine whether the wheel is part of the car in
bridging anaphora resolution, Poesio et almploy
queries of the form ?X of Y?, where X and Y would
be replaced with the wheel and the car, respectively.
On the other hand, we are not targeting a particular
type of relation. Rather, we intend to capture world
knowledge like lions rather than zebras are preda-
tors. Such knowledge may not be expressed as a
relation and hence may not be easily captured using
specific patterns. For this reason, we need to employ
patterns as general as those such as Q3 and Q4.
4.3 FrameNet
If we generate search queries as described in the pre-
vious subsection for the shout example, it is unlikely
that Google will return meaningful counts to us. The
reason is that both candidate antecedents in the sen-
tence are proper names belonging to the same type
(which in this case is PERSON).
However, in some cases, we may be able to gener-
ate more meaningful queries from such kind of sen-
tences. Consider the following sentence:
(4) John killed Jim, so he was arrested.
To generate meaningful queries, we make one ob-
servation: John and Jim played different roles in a
kill event. Hence, we can replace these proper names
with their roles. We propose to obtain these roles
from FrameNet (Baker et al1998). More gener-
ally, for each proper name e in a given sentence, we
(1) determine the event in which e is involved (using
the Stanford dependency parser); (2) search for the
FrameNet frame corresponding to the event as well
as e?s role in the event; and (3) replace the name
with its FrameNet role. In our example, since both
names are involved in the kill event, we retrieve the
FrameNet frame for kill. Given that John and Jim are
the subject and object of kill, we can extract their se-
mantic roles directly from the frame, which are killer
and victim, respectively.6 Consequently, we replace
the two names with their extracted semantic roles,
and generate the search queries from the resulting
sentence in the same way as before.
Note that if no frames can be found for the verb in
the first clause, no search queries will be generated.
After obtaining the query counts, we generate four
binary features, FN1, FN2, FN3, FN4, whose values
6We heuristically map grammatical roles to semantic roles.
are computed based on the same four heuristic rules
that were discussed in the previous subsection.
4.4 Heuristic Polarity
Some sentences involve comparing the two candi-
date antecedents. Consider the following sentences:
(5a) John was defeated by Jim in the election
even though he is more popular.
(5b) John was defeated by Jim in the election
because he is more popular.
The pronoun he refers to John in (5a) and Jim in
(5b). To see how we can design an algorithm for re-
solving these pronouns, it would be useful to under-
stand how humans resolve them. The phrase more
popular has a positive sentiment. In (5a), the use
of even though yields a clause of concession, which
flips the polarity of more popular (from positive to
negative), whereas in (5b), the use of because yields
a clause of cause, which does not change the po-
larity of more popular (i.e., more popular remains
positive). Since more popular is used to describe he,
he is ?better? in (5b) but ?worse? in (5a). Now, the
word defeat has a positive sentiment, and since Jim
is the deep subject of defeat, Jim is ?better? and John
is ?worse?. Finally, in (5b), he and Jim are ?better?,
so he is resolved to Jim; on the other hand, in (5a),
he and John are ?worse?, so he is resolved to John.
We automate this (human) method for resolv-
ing pronouns as follows. We begin by determin-
ing whether we can assign a rank value (i.e., ?bet-
ter? or ?worse?) to the pronoun and the two can-
didate antecedents. For instance, to determine the
rank value of the pronoun A, we first determine the
polarity value pA of its anchor word wA, which is
either the verb v for which A serves as the deep sub-
ject, or the adjective modifying A if v does not ex-
ist,7 using Wilson et al (2005b) subjectivity lex-
icon.8 If pA is not NEUTRAL, we check whether
it can be flipped by the context of wA. We con-
sider three kinds of polarity-reversing context: nega-
tion, comparative adverb, and discourse connective.
Specifically, we determine whether wA is negated
using the Stanford dependency parser, which explic-
7In the sentiment analysis and opinion mining literature,
(wA, pA) is known as an opinion-target pair.
8The lexicon contains 8221 words, each of which is hand
labeled with a polarity of POSITIVE, NEGATIVE, or NEUTRAL.
782
itly annotates instances of negation; we determine
the existence of a comparative adverb (e.g., ?more?,
?less?) using the POS tag ?RBR?; and we determine
whether A exists in a clause headed by a polarity-
reversing connective, such as although. After flip-
ping pA by context, we can infer A?s rank value from
it. Specifically, A?s rank value is ?better? if pA is
positive; ?worse? if pA is negative; and ?cannot be
determined? if pA is neutral. The polarity values of
the two candidate antecedents can be determined in
a similar fashion. Note that sometimes we may need
to infer rank values. For example, given the sentence
?Jane is prettier than Jill?, prettier has a positive po-
larity, so its modifying NP, Jane, has a ?better? rank,
and we can infer that Jill?s rank is ?worse?.
We create three features, HPOL1, HPOL2, and
HPOL3, based on our heuristic polarity determina-
tion component. Specifically, if the rank value of
the pronoun or the rank value of one or both of the
candidate antecedents cannot be determined, the val-
ues of all three binary features will be set to zero
for both i1 and i2. Otherwise, we compute the val-
ues of the three features as follows. To compute
HPOL1, which is a binary feature, we (1) employ
a heuristic resolution procedure, which resolves the
pronoun to the candidate antecedent with the same
rank value, and then (2) encode the outcome of this
heuristic procedure as the value of HPOL1. For ex-
ample, since the first candidate antecedent, John, is
predicted to be the antecedent in (5a), HPOL1(i1)=1
and HPOL1(i2)=0. The value of HPOL2 is the
concatenation of the polarity values determined
for the pronoun and the candidate antecedent.
Referring again to (5a), HPOL2(i1)=positive-
positive and HPOL2(i2)=positive-negative. To
compute HPOL3 for a given instance, we sim-
ply take its HPOL2 value and append the
connective to it. Using (5a) as an exam-
ple, HPOL3(i1)=positive-positive-even-though and
HPOL3(i1)=positive-negative-even-though.
4.5 Machine-Learned Polarity
In the previous subsection, we compute the polarity
of a word by updating its prior polarity heuristically
with contextual information. We hypothesized that
polarity could be computed more accurately by em-
ploying a sentiment analyzer that can capture richer
contextual information. For this reason, we employ
OpinionFinder (Wilson et al2005a), which has a
pre-trained classifier for annotating the phrases in a
sentence with their contextual polarity values.
Given a sentence and the polarity values of the
phrases annotated by OpinionFinder, we determine
the rank values of the pronoun and the two candi-
date antecedents by mapping them to the polarized
phrases using the dependency relations provided by
the Stanford dependency parser. We create three bi-
nary features, LPOL1, LPOL2, and LPOL3, whose
values are computed in the same way as HPOL1,
HPOL2, and HPOL3, respectively, except that the
computation here is based on the machine-learned
polarity values rather than the heuristically deter-
mined polarity values.
4.6 Connective-Based Relations
Consider the following sentences:
(6a) Google bought Motorola because they
want its customer base.
(6b) Google bought Motorola because they
are rich.
Humans resolve they to Google in (6a) by exploit-
ing the world knowledge that there is a causal rela-
tion (signaled by the discourse connective because)
between the want event and the buy event. A simi-
lar mechanism is used to resolve they to Google in
(6b): from world knowledge we know that there is a
causal relation between rich and buy.
We automate this (human) method for resolving
pronouns as follows. First, we gather connective-
based relations of this kind from a large, unanno-
tated corpus. In our experiments, we use as our
unannotated corpus the documents in three text cor-
pora (namely, BLLIP, Reuters, and English Giga-
word), but retain only those sentences that con-
tain a single discourse connective and do not be-
gin with the connective. From these sentences,
we collect triples and their frequencies of occur-
rences in the corpus. Each triple is of the form
(V ,Conn,X), where Conn is a discourse connec-
tive, V is a stemmed verb in the clause preceding
Conn, and X is a stemmed verb or an adjective in
the clause following Conn. Each triple essentially
denotes a relation between V and X expressed by
Conn. Conceivably, the strength of the relation in a
triple increases with its frequency count.
783
We use the frequency counts of these triples to
heuristically predict the correct antecedent for a tar-
get pronoun. Given a sentence where Conn is the
discourse connective, X is the stemmed verb gov-
erning the target pronoun A or the adjective modify-
ing A (if X is a to be verb), and V is the stemmed
verb governing the candidate antecedents, we re-
trieve the frequency count of the triple (V ,Conn,X).
If the count is at least 100, we employ a procedure
for heuristically selecting the antecedent for the tar-
get anaphor. Specifically, if X is a verb, then it re-
solves the target pronoun to the candidate antecedent
that has the same grammatical role as the pronoun.
However, if X is an adjective and the sentence does
not involve comparison, then it resolves the target
pronoun to the candidate antecedent serving as the
subject of V .
We create a binary feature, CBR, that encodes
this heuristic decision. In our running example, the
triple (buy, because, want) occurs 860 times in our
corpus, so the pronoun they is resolved to the can-
didate antecedent that occurs as the subject of buy.
Hence, CBR(i1)=1 and CBR(i2)=0. However, had
the triple occurred less than 100 times, both of these
features would have been set to zero.
4.7 Semantic Compatibility
Some of the queries generated by the Google com-
ponent, such as Q1 and Q2, aim to capture the
semantic compatibility between a candidate an-
tecedent, C , and the verb governing the target pro-
noun, V . However, using web search queries to esti-
mate semantic compatibility has potential problems,
including (1) a precision problem: the fact that C
and V appear next to each other in a query does
not necessarily imply that a subject-verb relation ex-
ists between them; and (2) a recall problem: these
queries fail to capture subject-verb relations where
C and V are not immediately adjacent to each other.
To address these potential problems, we com-
pute knowledge of selectional preferences from a
large, unannotated corpus. As before, we cre-
ate our unannotated corpus using the documents in
BLLIP, Reuters, and English Gigaword. Specifi-
cally, we first parse each sentence in the corpus us-
ing the Stanford dependency parser. Then, for each
stemmed verb v and each stemmed noun n in the
corpus, we collect the following statistics: (1) the
number of times n is the subject of v; (2) the num-
ber of times n is the direct object of v; (3) the mutual
information (MI) of v and n (with n as the subject
of v); and (4) the MI of v and n (with n as the direct
object of v).9
To understand how we use these statistics to gen-
erate features for resolving pronouns, consider the
following sentence:
(7) The man stole the neighbor?s bike because
he needed one.
Assuming that the target pronoun and its govern-
ing verb V has grammatical relation GR, we create
three features, SC1, SC2, and SC3, based on our se-
mantic compatibility component. SC1 encodes the
MI value of the head noun of a candidate antecedent
and V (and GR). SC2 is a binary feature whose
value indicates which of the candidate antecedents
has a larger MI value with V (and GR). SC3 is the
same as SC2, except that MI is replaced with corpus
frequency. In other words, SC2 and SC3 employ
different measures to heuristically predict the cor-
rect antecedent for the target pronoun. If the target
pronoun is governed by a to be verb, the values of
these three features will all be set to zero.
Given our running example, we first retrieve
the following corpus-based statistics: MI(need:subj,
man)=0.6322; MI(need:subj, neighbor)=0.3975;
count(need:subj, man)=474; and count(need:subj,
neighbor)=68. Using these statistics, we can then
compute the aforementioned features for our exam-
ple. Specifically, SC1(i1)=0.6322, SC1(i2)=0.3975,
SC2(i1)=1, SC2(i2)=0, SC3(i1)=1, and SC3(i2)=0.
4.8 Lexical Features
We exploit the coreference-annotated training docu-
ments by creating lexical features from them. These
lexical features can be divided into two categories,
depending on whether they are computed based on
the candidate antecedents.
Let us begin with the antecedent-independent fea-
tures. Assuming that W is an arbitrary word in a
sentence S that is not part of a candidate antecedent
and Conn is the connective in S, we create three
types of binary-valued antecedent-independent fea-
tures, namely (1) unigrams, where we create one
9We use the same formula as described in Section 4.2 of
Bergsma and Lin (2006) to compute MI values.
784
feature for each W ; (2) word pairs, where we cre-
ate features by pairing each W appearing before
Conn with each W appearing after Conn, exclud-
ing adjective-noun and noun-adjective pairs10; and
(3) word triples, where we augment each word pair
in (2) with Conn. The value of each feature f indi-
cates the presence or absence of f in S.
Next, we compute the antecedent-dependent fea-
tures. Let (1) HC1 and HC2 be the head words of
candidate antecedents C1 and C2, respectively; (2)
VC1 , VC2 , and VA be the verbs governing C1, C2,
and the target pronoun A, respectively; and (3) JC1 ,
JC2 , and JA be the adjectives modifying C1, C2, and
A, respectively.11 We create from each candidate an-
tecedent four features, each of which is a word pair.
From C1, we create (HC1 , VC1), (HC1 , JC1 ), (HC1 ,
VA), and (HC1 , JA), all of which will appear in the
feature vector corresponding to C1. A similar set of
four features are created from C2. These antecedent-
dependent features are all binary-valued.
It is worth mentioning that while we also consid-
ered word triples in the connective-based relations
component and word pairs in the semantic compat-
ibility component, in those components we deter-
mine their usefulness in an unsupervised manner,
whereas by employing them as lexical features we
determine their usefulness in a supervised manner.
5 Evaluation
5.1 Experimental Setup
Dataset. We report results on the test set, which
comprises 30% of our hand-annotated sentence pairs
(see Section 2 for details).
Evaluation metrics. Results are expressed in
terms of accuracy, which is the percentage of cor-
rectly resolved target pronouns. We also report the
percentages of these pronouns that are (1) not re-
solved and (2) incorrectly resolved.
5.2 Results and Discussion
The Random baseline. Our first baseline is a re-
solver that randomly guesses the antecedent for the
10Pairing an adjective A in one clause with a noun N in an-
other clause may mislead the learner into thinking that N is
modified by A, and hence we do not create such pairs.
11If C1, C2, and A are not modified by adjectives, no
adjective-based features will be created.
target pronoun in each sentence. Since there are
two candidate antecedents per sentence, the Random
baseline should achieve an accuracy of 50%.
The Stanford resolver. Our second baseline is the
Stanford resolver (Lee et al2011), which achieves
the best performance in the CoNLL 2011 shared task
(Pradhan et al2011). As a rule-based resolver, it
does not exploit any coreference-annotated data.
Recall from Section 3 that our system assumes as
input not only a sentence containing a target pronoun
but also the two candidate antecedents. To ensure a
fair comparison, the same input is provided to this
and other baselines. Hence, if the Stanford resolver
decides to resolve the target pronoun, it will resolve
it to one of the two candidate antecedents. However,
if it does not have enough confidence about resolv-
ing it, it will leave it unresolved. Its performance on
the test set is shown in the ?Unadjusted Scores? col-
umn in row 1 of Table 3. As we can see, it correctly
resolves 40.1% of the pronouns, incorrectly resolves
29.8% of them, and does not make any decision on
the remaining 30.1%.
Given that the Random baseline correctly resolves
50% of pronouns and the Stanford resolver correctly
resolves only 40.1% of the pronouns, it is tempting
to conclude that Stanford does not perform as well
as Random. However, recall that Stanford leaves
30.1% of the pronouns unresolved. Hence, to ensure
a fairer comparison, we produce ?adjusted? scores
for the Stanford resolver, where we ?force? it to re-
solve all of the unresolved target pronouns by as-
suming that probabilistically half of them will be re-
solved correctly. This adjusted score is shown in the
?Adjusted Scores? column in row 1 of Table 3. As
we can see, Stanford achieves an accuracy of 55.1%,
which is 5.1 points higher than that of Random.
The Baseline Ranker. To understand whether the
somewhat unsatisfactory Stanford results can be at-
tributed to its inability to exploit the training data,
we employ as our third baseline a mention ranker
that is trained in the same way as our system (see
Section 3), except that it employs 39 commonly-
used linguistic features for learning-based corefer-
ence resolution (see Table 1 of Rahman and Ng
(2009) for a description of these features). Hence,
the performance difference between this Baseline
Ranker and our system can be attributed entirely
785
Unadjusted Scores Adjusted Scores
Coreference System Correct Wrong No Decision Correct Wrong No Decision
1 Stanford 40.07% 29.79% 30.14% 55.14% 44.86% 0.00%
2 Baseline Ranker 47.70% 47.16% 5.14% 50.27% 49.73% 0.00%
3 Stanford+Baseline Ranker 53.49% 43.12% 3.39% 55.19% 44.77% 0.00%
4 Our system 73.05% 26.95% 0.00% 73.05% 26.95% 0.00%
Table 3: Results of the Stanford resolver, the Baseline Ranker, the Combined resolver, and our system.
to the difference between the two linguistic feature
sets. Results of the Baseline Ranker are shown in
row 2 of Table 3. Before score adjustment, it cor-
rectly resolves 47.7% of the target pronouns, incor-
rectly resolves 47.2% of them, and leaves the re-
maining 5.1% unresolved. (Note that we output ?no
decision? if the ranker assigns the same rank value
to both candidate antecedents.) After score adjust-
ment, its accuracy is 50.3%, which is 0.3 points
higher than that of Random but statistically indis-
tinguishable from it.12 On the other hand, its accu-
racy is 4.9 points lower than that of Stanford, and
the difference between their performance is signifi-
cant. While it seems somewhat surprising that a su-
pervised resolver does not perform as well as a rule-
based resolver, neither of them employs knowledge
sources that are particularly useful for our dataset. In
other words, despite given access to annotated data,
the Baseline Ranker may not be able to make effec-
tive use of it due to the lack of useful features.
The Combined resolver. We create a fourth base-
line by combining the Stanford resolver and the
Baseline Ranker. The motivation is that the former
can provide better precision and the latter can pro-
vide better recall by handling ?no decision? cases
not covered by the former. Note that the Baseline
Ranker will be applied to resolve only those pro-
nouns that are left unresolved by Stanford. Results
in row 3 of Table 3 show that the adjusted accuracy
of this Combined resolver is 55.2%, which is sta-
tistically indistinguishable from Stanford?s adjusted
accuracy. Hence, these results show that the addi-
tion of the Baseline Ranker does not help improve
Stanford?s resolution accuracy.
Our system. Results of our system, which is
trained using the features described in Section 4 in
combination with a ranking model, are shown in
row 4 of Table 3. As we can see, our system achieves
12All statistical significance test results in this paper are ob-
tained using the paired t-test, with p < 0.05.
Feature Type Correct Wrong No Decision
All features 73.05% 26.95% 0.00%
?Narrative Chains 68.97% 31.03% 0.00%
?Google 65.96% 34.04% 0.00%
?FrameNet 72.16% 27.84% 0.00%
?Heuristic Polarity 71.45% 28.55% 0.00%
?Learned Polarity 72.70% 27.30% 0.00%
?Connective-Based Rel. 71.28% 28.72% 0.00%
?Semantic Compat. 71.81% 28.19% 0.00%
?Lexical Features 60.11% 25.35% 14.54%
Table 4: Results of feature ablation experiments.
an accuracy of 73.1%, significantly outperforming
the Combined resolver by 17.9 points in accuracy.
These results suggest that our features are more use-
ful for resolving difficult pronouns than those com-
monly used for coreference resolution.
5.3 Feature Analysis
In an attempt to gain additional insight into the per-
formance contribution of each of the eight types of
features used in our system, we conduct feature ab-
lation experiments. The unadjusted scores of these
experiments are shown in Table 4, where each row
shows the performance of the model trained on all
types of features except for the one shown in that
row. For easy reference, the performance of the
model trained on all types of features is shown in
row 1 of the table.
A few points deserve mention. First, perfor-
mance drops significantly whichever feature type is
removed. This suggests that all eight feature types
are contributing positively to overall accuracy. Sec-
ond, the Google-based features and the Lexical Fea-
tures are the most useful, and those generated via
FrameNet and Learned Polarity are the least use-
ful in the presence of other feature types. While it
is somewhat surprising that Learned Polarity is not
more useful than Heuristic Polarity, we speculate
the reason can be attributed to the fact that the cor-
pus on which OpinionFinder was trained was quite
different from ours. Finally, even without using the
786
Feature Type Correct Wrong No Decision
Narrative Chains 30.67% 24.47% 44.86%
Google 33.16% 7.09% 59.75%
FrameNet 7.27% 4.08% 88.65%
Learned Polarity 4.79% 2.66% 92.55%
Heuristic Polarity 7.27% 1.77% 90.96%
Connective-Based Rel. 14.01% 8.69% 77.30%
Semantic Compat. 23.58% 13.12% 63.30%
Lexical Features 56.91% 43.09% 0.00%
Table 5: Results of single-feature coreference models.
Lexical Features, our system still outperforms all the
baseline resolvers: as can been implied from the last
row of Table 4, in the absence of the Lexical Fea-
tures, our resolver achieves an adjusted accuracy of
67.4%, which is only 5.7 points less than that ob-
tained when the full feature set is employed. Hence,
while the Lexical Features are useful, their impor-
tance should not be over-emphasized.
To get a better idea of the utility of each feature
type, we conduct another experiment in which we
train eight models, each of which employs exactly
one type of features. Their unadjusted scores are
shown in Table 5. As we can see, Learned Polarity
has the smallest contribution, whereas the Lexical
Features have the largest contribution.
5.4 Error Analysis
While our resolver significantly outperforms state-
of-the-art resolvers, there is a lot of room for im-
provement. To help direct future research on the res-
olution of difficult pronouns, we analyze the major
sources of errors made by our resolver.
Our analysis reveals that many of the errors cor-
respond to cases that cannot be handled by any of
the eight components of our resolver. To understand
these cases, consider first the strengths and weak-
nesses of Narrative Chains and Google, the two
components that contribute the most to overall per-
formance after Lexical Features.
Google is especially good at capturing facts, such
as lions are predators and zebras are not predators,
helping us correctly resolve sentences such as (5a)
and (5b), as well as those in sentence pair (I) in Ta-
ble 1. However, it may not be good at handling pro-
nouns whose resolution requires an understanding of
the connection between the facts or events described
in the two clauses of a sentence. The reason is that
establishing such a connection requires that we con-
struct a search query composed of information ex-
tracted from both clauses, and the resulting, possi-
bly long, query is likely to receive no hit count due
to data sparseness. Investigating how to construct
such queries while avoiding data sparseness would
be an interesting line of future work.
Narrative chains, on the other hand, are useful
for capturing the relationship between the events de-
scribed in the two clauses. However, they are com-
puted over verbs, and therefore cannot capture such
a relationship when one or both of the events in-
volved are not described by verbs. For example,
narrative chains fail to capture the causal relation
between the event expressed by angry and shout in
sentence (1b). It is also worth mentioning that some
pronouns that could have been resolved using nar-
rative chains are not owing to the coverage and ac-
curacy of Chambers and Jurafsky?s (2008) chains,
but we believe that these recall and precision prob-
lems could be addressed by (1) inducing chains from
a larger corpus and (2) using semantic roles rather
than grammatical roles in the induction process.
Some resolution errors arise from errors in polar-
ity analysis. This can be attributed to the simplicity
of our Heuristic Polarity component: determining
the polarity of a word based on its prior polarity is
too na??ve. Fine-grained polarity analysis would be
a promising solution to this problem (see Pang and
Lee (2008) and Liu (2012) for related work).
6 Conclusions
We investigated the resolution of complex cases of
definite pronouns, a problem that was under exten-
sive discussion by coreference researchers in the
1970s but has received revived interest owing in part
to its relevance to the Turing Test. Our experimental
results indicate that it is a challenge for state-of-the-
art resolvers, and while we proposed new knowledge
sources for addressing this challenge, our resolver
still has a lot of room for improvement. In partic-
ular, our error analysis indicates that further gains
could be achieved via more accurate sentiment anal-
ysis and induction of world knowledge from corpora
or the Web. In addition, we plan to integrate our
resolver into a general-purpose coreference system
and evaluate the resulting resolver on standard eval-
uation corpora such as MUC, ACE, and OntoNotes.
787
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier draft
of the paper. This work was supported in part by
NSF Grants IIS-0812261 and IIS-1147644.
References
Amit Bagga. 1998. Coreference, Cross-Document
Coreference, and Information Extraction Methodolo-
gies. Ph.D. thesis, Duke University.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics and the 17th International
Conference on Computational Linguistics, pages 86?
90.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 33?40.
Volha Bryl, Claudio Guiliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Using background
knowledge to support coreference resolution. In Pro-
ceedings of the 19th European Conference on Artificial
Intelligence, pages 759?764.
Donna K. Byron. 2002. Resolving pronominal reference
to abstract entities. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 80?87.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 787?797.
Eugene Charniak and Micha Elsner. 2009. EM works for
pronoun anaphora resolution. In Proceedings of the
12th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 148?156.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation, pages 449?454.
Pascal Denis and Jason Baldridge. 2007. A ranking ap-
proach to pronoun resolution. In Proceedings of the
Twentieth International Conference on Artificial Intel-
ligence, pages 1588?1593.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?669.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152?1161.
Graeme Hirst. 1981. Anaphora in Natural Language
Understanding. Springer Verlag.
Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji Mat-
sumoto. 2003. Incorporating contextual cues in train-
able models for coreference resolution. In Proceed-
ings of the EACL Workshop on The Computational
Treatment of Anaphora.
Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.
2011. Narrative schema as world knowledge for
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 86?92.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 133?142.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34.
Hector J. Levesque. 2011. The Winograd Schema Chal-
lenge. In AAAI Spring Symposium: Logical Formal-
izations of Commonsense Reasoning.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publishers.
Ruslan Mitkov, Richard Evans, and Constantin Orasan.
2002. A new, fully automatic version of Mitkov?s
knowledge-poor pronoun resolution method. In Al.
Gelbukh, editor, Computational Linguistics and Intel-
ligent Text Processing, pages 169?187. Springer.
Natalia N. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 176?183.
Christoph Mu?ller. 2007. Resolving it, this, and that in
unrestricted multi-party dialog. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 816?823.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval 2(1?2):1?135.
Massimo Poesio and Mijail A. Kabadjov. 2004.
A general-purpose, off-the-shelf anaphora resolution
module: Implementation and preliminary evaluation.
In Proceedings of the 4th International Conference on
Language Resources and Evaluation, pages 663?666.
788
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
pages 143?150.
Massimo Poesio, David Day, Ron Artstein, Jason Dun-
can, Vladimir Eidelman, Claudio Giuliano, Rob Hall,
Janet Hitzeman, Alan Jern, Mijail Kabadjov, Stanley
Yong Wai Keong, Gideon Mann, Alessandro Mos-
chitti, Simone Ponzetto, Jason Smith, Josef Stein-
berger, Michael Strube, Jian Su, Yannick Versley,
Xiaofeng Yang, and Michael Wick. 2007. EL-
ERFED: Final report of the research group on Exploit-
ing Lexical and Encyclopedic Resources For Entity
Disambiguation. Technical report, Summer Workshop
on Language Engineering, Center for Language and
Speech Processing, Johns Hopkins University, Balti-
more, MD.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the Human Language Technology Conference and
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 192?
199.
Simone Paolo Ponzetto and Michael Strube. 2007.
Knowledge derived from Wikipedia for computing se-
mantic relatedness. Journal of Artificial Intelligence
Research, 30:181?212.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 1?27.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A
public reference implementation of the RAP anaphora
resolution algorithm. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation, pages 291?294.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-
sky, and Christopher Manning. 2010. A multi-pass
sieve for coreference resolution. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 492?501.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 814?824.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals, and Understanding. Lawrence Erl-
baum.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. REC-
ONCILE: A coreference resolution research platform.
In Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 156?161.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pages
168?175.
Alan M. Turing. 1950. Computing machinery and intel-
ligence. Mind, 59:433?460.
Olga Uryupina, Massimo Poesio, Claudio Giuliano, and
Kateryna Tymoshenko. 2011. Disambiguation and
filtering methods in using Web knowledge for coref-
erence resolution. In Proceedings of the 24th Interna-
tional Florida Artificial Intelligence Research Society
Conference, pages 317?322.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Proceedings of the ACL-08: HLT Demo Session,
pages 9?12.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.
2005a. Opinionfinder: A system for subjectivity anal-
ysis. In Proceedings of HLT/EMNLP 2005 Interactive
Demonstrations, pages 34?35.
Theresa Wilson, Janyce M. Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Joint
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 347?354.
Terry Winograd. 1972. Understanding Natural Lan-
guage. Academic Press, Inc., New York.
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim
Tan. 2003. Coreference resolution using competitive
learning approach. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 176?183.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 165?172.
789
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1360?1365,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Chinese Zero Pronoun Resolution: Some Recent Advances
Chen Chen and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{yzcchen,vince}@hlt.utdallas.edu
Abstract
We extend Zhao and Ng's (2007) Chinese
anaphoric zero pronoun resolver by (1) using
a richer set of features and (2) exploiting the
coreference links between zero pronouns dur-
ing resolution. Results on OntoNotes show
that our approach significantly outperforms
two state-of-the-art anaphoric zero pronoun re-
solvers. To our knowledge, this is the first
work to report results obtained by an end-to-
end Chinese zero pronoun resolver.
1 Introduction
A zero pronoun (ZP) is a gap in a sentence that is
found when a phonetically null form is used to refer
to a real-world entity. An anaphoric zero pronoun
(AZP) is a ZP that corefers with one or more preced-
ing noun phrases (NPs) in the associated text. Un-
like overt pronouns, ZPs lack grammatical attributes
that are useful for overt pronoun resolution such as
number and gender. This makes ZP resolution more
challenging than overt pronoun resolution.
We aim to improve the state of the art in Chinese
AZP resolution by proposing two extensions. First,
while previous approaches to this task have primarily
focused on employing positional and syntactic fea-
tures (e.g., Zhao and Ng (2007) [Z&N], Kong and
Zhou (2010) [K&Z]), we exploit a richer set of fea-
tures for capturing the context of an AZP and its
candidate antecedents. Second, to alleviate the diffi-
culty of resolving an AZP to an antecedent far away
from it, we break down the process into smaller, in-
termediate steps, where we allow coreference links
between AZPs to be established.
We apply our two extensions to a state-of-the-art
Chinese AZP resolver proposed by Z&N and eval-
uate the resulting resolver on the OntoNotes cor-
pus. Experimental results show that this resolver sig-
nificantly outperforms both Z&N's resolver and an-
other state-of-the-art resolver proposed by K&Z. It
is worth noting that while previous work on Chinese
ZP resolution has reported results obtained via gold
information (e.g., using gold AZPs and extracting
candidate antecedents and other features from gold
syntactic parse trees), this is the first work to report
the results of an end-to-end Chinese ZP resolver.
The rest of this paper is organized as follows. Sec-
tion 2 describes the two baselineAZP resolvers. Sec-
tions 3 and 4 discuss our two extensions. We present
our evaluation results in Section 5 and our conclu-
sions in Section 6.
2 Baseline AZP Resolution Systems
An AZP resolution algorithm takes as input a set
of AZPs produced by an AZP identification system.
Below we first describe the AZP identifier we em-
ploy, followed by our two baseline AZP resolvers.
2.1 Anaphoric Zero Pronoun Identification
We employ two steps to identifyAZPs. In the extrac-
tion step, we heuristically extract candidate ZPs. In
the classification step, we train a classifier to distin-
guish AZPs from non-AZPs.
To implement the extraction step, we use Z&N's
and K&Z's observation: ZPs can only occur before a
VP node in a syntactic parse tree. However, accord-
ing to K&Z, ZPs do not need to be extracted from
every VP: if a VP node occurs in a coordinate struc-
ture or is modified by an adverbial node, then only its
parent VP node needs to be considered. We extract
ZPs from all VPs that satisfy the above constraints.
1360
Syntactic
features
(13)
whether z is the first gap in an IP clause; whether z is the first gap in a subject-less IP clause, and if
so, POS(w1); whether POS(w1) is NT; whether t1 is a verb that appears in a NP or VP; whether Pl is
a NP node; whether Pr is a VP node; the phrasal label of the parent of the node containing POS(t1);
whether V has a NP, VP or CP ancestor; whether C is a VP node; whether there is a VP node whose
parent is an IP node in the path from t1 to C.
Lexical
features
(13)
the words surrounding z and/or their POS tags, including w1, w?1, POS(w1), POS(w?1)+POS(w1),
POS(w1)+POS(w2), POS(w?2)+POS(w?1), POS(w1)+POS(w2)+POS(w3), POS(w?1)+w1, and
w?1+POS(w1); whether w1 is a transitive verb, an intransitive verb or a preposition; whether w?1 is
a transitive verb without an object.
Other fea-
tures (6)
whether z is the first gap in a sentence; whether z is in the headline of the text; the type of the clause in
which z appears; the grammatical role of z; whether w?1 is a punctuation; whether w?1 is a comma.
Table 1: Features for AZP identification. z is a zero pronoun. V is the VP node following z. wi is the ith word to the
right of z (if i is positive) or the ith word to the left of z (if i is negative). C is lowest common ancestor of w?1 and
w1. Pl and Pr are the child nodes of C that are the ancestors of w?1 and w1 respectively.
Features
between a
and z (4)
the sentence distance between a and z; the segment distance between a and z, where segments are
separated by punctuations; whether a is the closest NP to z; whether a and z are siblings in the
associated parse tree.
Features
on a (12)
whether a has an ancestor NP, and if so, whether this NP is a descendent of a's lowest ancestor IP;
whether a has an ancestor VP, and if so, whether this VP is a descendent of a's lowest ancestor IP;
whether a has an ancestor CP; the grammatical role of a; the clause type in which a appears; whether
a is an adverbial NP, a temporal NP, a pronoun or a named entity; whether a is in the headline of the
text.
Features
on z (10)
whether V has an ancestor NP, and if so, whether this NP node is a descendent of V's lowest ancestor
IP; whether V has an ancestor VP, and if so, whether this VP is a descendent of V's lowest ancestor IP;
whether V has an ancestor CP; the grammatical role of z; the type of the clause in which V appears;
whether z is the first or last ZP of the sentence; whether z is in the headline of the text.
Table 2: Features for AZP resolution in the Zhao and Ng (2007) baseline system. z is a zero pronoun. a is a candidate
antecedent of z. V is the VP node following z in the parse tree.
To implement the classification step, we train a
classifier using SVMlight (Joachims, 1999) to distin-
guishAZPs from non-AZPs. We employ 32 features,
13 of which were proposed by Z&N and 19 of which
were proposed by Yang and Xue (2010). A brief de-
scription of these features can be found in Table 1.
2.2 Two Baseline AZP Resolvers
The Zhao and Ng (2007) [Z&N] baseline. In
our implementation of the Z&N baseline, we use
SVMlight to train amention-pairmodel for determin-
ing whether an AZP z and a candidate antecedent
of z are coreferent. We consider all NPs preced-
ing z that do not have the same head as its parent
NP in the parse tree to be z's candidate antecedents.
We use Soon et al's (2001) method to create train-
ing instances: we create a positive instance between
an AZP, z, and its closest overt antecedent, and we
create a negative instance between z and each of the
intervening candidates. Each instance is represented
by the 26 features employed by Z&N. A brief de-
scription of these features can be found in Table 2.
During testing, we adopt the closest-first resolution
strategy, resolving an AZP to the closest candidate
antecedent that is classified as coreferent with it.1
The Kong and Zhou (2010) [K&Z] baseline.
K&Z employ a tree kernel-based approach to AZP
resolution. Like Z&N, K&Z (1) train a mention-
pair model for determining whether an AZP z and
a candidate antecedent of z are coreferent, (2) use
Soon et al's method to create training instances, and
(3) resolve an AZP to its closest coreferent can-
didate antecedent. Unlike Z&N, however, K&Z
use the SVMlight?TK learning algorithm (Moschitti,
1When resolving a goldAZP z, if none of the preceding can-
didate antecedents is classified as coreferent with it, we resolve
it to the candidate that has the highest coreference likelihood
with it. Here, we employ the signed distance from the SVM
hyperplane to measure the coreference likelihood.
1361
2006) to train their model, employing a parse sub-
tree known as a dynamic expansion tree (Zhou et al,
2008) as a structured feature to represent an instance.
3 Extension 1: Novel Features
We propose three kinds of features to better capture
the context of an AZP, as described below.
Antecedent compatibility. AZPs are omitted sub-
jects that precede VP nodes in a sentence's parse
tree. From the VP node, we can extract its head verb
(Predz) and the head of its object NP (Obj), if any.
Note that Predz and Obj contain important contex-
tual information for an AZP.
Next, observe that if a NP is coreferent with an
AZP, it should be able to fill the AZP's gap and be
compatible with the gap's context. Consider the fol-
lowing example:
E1: ?????????????? ?pro???
?????????????
(They are trying that service. That means ?pro?
hope that our visitors can try it when they come in
September.)
The head of the VP following ?pro? is ??
(hope). There are two candidate antecedents, ??
(They) and ???? (that service). If we try us-
ing them to fill this AZP's gap, we know based on
selectional preferences that ???? (They hope)
makes more sense than?????? (that service
hope). We supply the AZP resolver with the fol-
lowing information to help it make these decisions.
First, we find the head word of each candidate an-
tecedent, Headc. Then we form two strings, Headc
+ Predz and Headc + Predz + Obj (if the object
of the VP is present). Finally, we employ them as bi-
nary lexical features, setting their feature values to 1
if and only if they can be extracted from the instance
under consideration. The training data can be used
to determine which of these features are useful.2
Narrative event chains. A narrative event chain is
a partially ordered set of events related by a common
protagonist (Chambers and Jurafsky, 2008). For ex-
ample, we can infer from the chain "borrow-s invest-
s spend-s lend-s" that a person who borrows (pre-
2We tried to apply Kehler et al's (2004) and Yang et
al.'s (2005) methods to learn Chinese selectional preferences
from unlabeled data, but without success.
sumably money) can invest it, spend it, or lend it to
other people.3 Consider the following example:
E2: ???????pro???????????
??????
(The country gives our department money, but all
?pro? provides is exactly what we worked for.)
In E2, ?pro? is coreferent with ?? (The coun-
try), and the presence of the narrative event chain?
??? (gives?provides) suggests that the subjects
of the two events are likely to be coreferent.
However, given the unavailability of induced or
hand-crafted narrative chains in Chinese4, we make
the simplifying assumption that two verbs form a
lexical chain if they are lexically identical.5 We
create two features to exploit narrative event chains
for a candidate NP, c, if it serves as a subject or
object. Specifically, let the verb governing c be
Predc. The first feature, which encodes whether
narrative chains are present, has three possible val-
ues: 0 if Predc and Predz are not the same; 1 if
Predc and Predz are the same and c is a subject;
and 2 if Predc and Predz are the same and c is an
object. The second feature is a binary lexical fea-
ture, Predc+Predz+Subject/Object; its value is
1 if and only if Predc, Predz , and Subject/Object
can be found in the associated instance, where
Subject/Object denotes the grammatical role of c.
Final punctuation hint. We observe that the punc-
tuation (Punc) at the end of a sentence where an
AZP occurs also provides contextual information,
especially in conversation documents. In conversa-
tions, if a sentence containing an AZP ends with a
3"-s" denotes the fact that the protagonist serves as the gram-
matical subject in these events.
4We tried to construct narrative chains for Chinese using
both learning-based and dictionary-based methods. Specifi-
cally, we induced narrative chains using Chambers and Juraf-
sky's (2008) method, but were not successful owing to the lack
of an accurate Chinese coreference resolver. In addition, we
constructed narrative chains using both lexically identical verbs
and the synonyms obtained from a WordNet-like Chinese re-
source called Tongyicicilin, but they did not help improve reso-
lution performance.
5Experiments on the training data show that if an AZP and
a candidate antecedent are subjects of (different occurrences of)
the same verb, then the probability that the candidate antecedent
is coreferent with the AZP is 0.703. This result suggests that our
assumption, though somewhat simplistic, is useful as far as AZP
resolution is concerned.
1362
A:?????????
(A: How is her life now? )
B: ?pro1???????????????
(B: ?pro1? attitude toward life is plain and simple.)
A:??
(A: Yes.)
A: ?pro2???????????
(A: ?pro2? is living in Beijing or the USA?)
B: ?pro3?????
(B: ?pro3? is living in the USA.)
Figure 1: An illustrative example.
question mark, the mention this AZP refers to is less
likely to be the speaker himself6, as illustrated in the
following example:
E3: ?? ?pro????
(Are ?pro? cold in the winter?)
Here, ?pro? refers to the person the speaker talks
with. To capture this information, we create a binary
lexical feature, Headc+Punc, whose value is 1 if
and only if Headc and Punc appear in the instance
under consideration.
4 Extension 2: Zero Pronoun Links
4.1 Motivation
Like an overt pronoun, a ZP whose closest overt
antecedent is far away from it is harder to resolve
than one that has a nearby overt antecedent. How-
ever, a corpus study of our training data reveals that
only 55.2% of the AZPs appear in the same sentence
as their closest overt antecedent, and 22.7% of the
AZPs appear two or more sentences away from their
closest overt antecedent.
Fortunately, we found that some of the difficult-
to-resolve AZPs (i.e., AZPs whose closest overt an-
tecedents are far away from them) are coreferential
with nearby ZPs. Figure 1, which consists of a set of
sentences from a conversation, illustrates this phe-
nomenon. There are three AZPs (denoted by ?proi?,
where 1 ? i ? 3), all of which refer to the overt
pronoun ? (She) in the first sentence. In this ex-
ample, it is fairly easy to resolve ?pro1? correctly,
6One may wonder whether we can similarly identify con-
straints on the antecedents of a ZP from clause conjunctions.
Our preliminary analysis suggests that the answer is no.
Training Test
Documents 1,391 172
Sentences 36,487 6,083
Words 756,063 110,034
ZPs 23,065 3,658
AZPs 12,111 1,713
Table 3: Statistics on the training and test sets.
since its antecedent is the subject of previous sen-
tence. However, ?pro3? and its closest overt an-
tecedent? (She) are four sentences apart. Together
with the fact that there are many intervening candi-
date antecedents, it is not easy for a resolver to cor-
rectly resolve ?pro3?.
To facilitate the resolution of ?pro3? and difficult-
to-resolve AZPs in general, we propose the follow-
ing idea. We allow an AZP resolver to (1) establish
coreferent links between two consecutive ZPs (i.e.,
?pro1???pro2? and ?pro2???pro3? in our exam-
ple), which are presumably easy to establish because
the two AZPs involved are close to each other; and
then (2) treat them as bridges and infer that ?pro3?'s
overt antecedent is? (She).
4.2 Modified Resolution Algorithm
We implement the aforementioned idea by modify-
ing the AZP resolver as follows. Whenwe resolve an
AZP z during testing, we augment the set of candi-
date antecedents for z with the set of AZPs preceding
z. Since we have only specified how to compute fea-
tures for instances composed of an AZP and an overt
candidate antecedent thus far (see Section 2.2), the
question, then, is: how can we compute features for
instances composed of two AZPs?
To answer this question, we first note that the
AZPs in a test text are resolved in a left-to-right man-
ner. Hence, by the time we resolve an AZP z, all the
AZPs preceding z have been resolved. Hence, when
we create a test instance i between z and one of the
preceding AZPs (say y), we create i as if the gap y
was filled with the smallest tree embedding the NP
to which y was resolved.
By allowing coreference links between (presum-
ably nearby) ZPs to be established, we can reason
over the resulting coreference links, treating them as
bridges that can help us find an overt antecedent that
is far away from an AZP.
1363
Gold AZP System AZP System AZP
Gold Parse Tree Gold Parse Tree System Parse Tree
System Variation R P F R P F R P F
K&Z Baseline System 38.0 38.0 38.0 17.7 22.4 19.8 10.6 13.6 11.9
Z&N Baseline System 41.5 41.5 41.5 22.4 24.4 23.3 12.7 14.2 13.4
Z&N Baseline + Contextual Features 46.2 46.2 46.2 25.2 27.5 26.3 14.4 16.1 15.2
Z&N Baseline + Zero Pronoun Links 42.7 42.7 42.7 22.5 24.6 23.5 13.2 14.8 13.9
Full System 47.7 47.7 47.7 25.3 27.6 26.4 14.9 16.7 15.7
Table 4: Resolution results on the test set.
5 Evaluation
5.1 Experimental Setup
Dataset. For evaluation, we employ the portion of
theOntoNotes 4.0 corpus that was used in the official
CoNLL-2012 shared task. The shared task dataset is
composed of a training set, a development set, and
a test set. Since only the training set and the de-
velopment set are annotated with ZPs, we use the
training set for classifier training and reserve the de-
velopment set for testing purposes. Statistics on the
datasets are shown in Table 3. In these datasets, a ZP
is marked as ?pro?. We consider a ZP anaphoric if
it is coreferential with a preceding ZP or overt NP.
Evaluation measures. We express the results of
both AZP identification and AZP resolution in terms
of recall (R), precision (P) and F-score (F).
5.2 Results and Discussion
The three major columns of Table 4 show the re-
sults obtained in three settings, which differ in
terms of whether gold/system AZPs and manu-
ally/automatically constructed parse trees are used to
extract candidate antecedents and features.
In the first setting, the resolvers are provided with
gold AZPs and gold parse trees. Results are shown in
column 1. As we can see, the Z&N baseline signifi-
cantly outperforms the K&Z baseline by 3.5% in F-
score.7 Adding the contextual features, the ZP links,
and both extensions to Z&N increase its F-score sig-
nificantly by 4.7%, 1.2% and 6.2%, respectively.
In the next two settings, the resolvers operate on
the system AZPs provided by the AZP identification
component. When gold parse trees are employed,
the recall, precision and F-score of AZP identifica-
tion are 50.6%, 55.1% and 52.8% respectively. Col-
umn 2 shows the results of the resolvers obtained
7All significance tests are paired t-tests, with p < 0.05.
when these automatically identified AZPs are used.
As we can see, Z&N again significantly outperforms
K&Z by 3.5% in F-score. Adding the contextual fea-
tures, the ZP links, and both extensions to Z&N in-
crease its F-score by 3.0%, 0.2% and 3.1%, respec-
tively. The system with contextual features and the
full system both yield results that are significantly
better than those of the Z&N baseline. A closer ex-
amination of the results reveals why the ZP links are
not effective in improving performance: when em-
ploying systemAZPs, many erroneous ZP linkswere
introduced to the system.
Column 3 shows the results of the resolvers when
we employ system AZPs and the automatically gen-
erated parse trees provided by the CoNLL-2012
shared task organizers to compute candidate an-
tecedents and features. Hence, these are end-to-end
ZP resolution results. To our knowledge, these are
the first reported results on end-to-end Chinese ZP
resolution. Using automatic parse trees, the perfor-
mance on AZP identification drops to 30.8% (R),
34.4% (P) and 32.5% (F). In this setting, Z&N still
outperforms K&Z significantly, though by a smaller
margin when compared to the previous settings. In-
corporating the contextual features, the ZP links, and
both extensions increase the F-score by 1.8%, 0.5%
and 2.3%, respectively. The system with contextual
features and the full system both yield results that are
significantly better than those of the Z&N baseline.
6 Conclusions
We proposed two extensions to a state-of-the-
art Chinese AZP resolver proposed by Zhao and
Ng (2007). Experimental results on the OntoNotes
dataset showed that the resulting resolver signifi-
cantly improved both Zhao and Ng's and Kong and
Zhou's (2010) resolvers, regardless of whether gold
or system AZPs and syntactic parse trees are used.
1364
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier draft
of the paper. This work was supported in part by
NSF Grants IIS-1147644 and IIS-1219142. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the views or official poli-
cies, either expressed or implied, of NSF.
References
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 787--797.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and Alexan-
der Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning, pages 44--56. MIT Press.
Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-
sandr Simma. 2004. Competitive self-trained pronoun
interpretation. In Proceedings of HLT-NAACL 2004:
Short Papers, pages 33--36.
Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for Chinese zero anaphora
resolution. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 882--891.
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language processing. In Proceedings
of the 11th Conference of the European Chapter of the
Association for Computational Linguistics, pages 113-
-120.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521--544.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the Chinese Treebank.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 1382--
1390.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 165--172.
Shanheng Zhao and Hwee Tou Ng. 2007. Identification
and resolution of Chinese zero pronouns: A machine
learning approach. In Proceedings of the 2007 Joint
Conference on Empirical Methods on Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 541--550.
GuoDong Zhou, Fang Kong, and Qiaoming Zhu. 2008.
Context-sensitive convolution tree kernel for pronoun
resolution. In Proceedings of the 3rd International
Joint Conference on Natural Language Processing,
pages 25--31.
1365
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 751?762,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Why are You Taking this Stance?
Identifying and Classifying Reasons in Ideological Debates
Kazi Saidul Hasan and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{saidul,vince}@hlt.utdallas.edu
Abstract
Recent years have seen a surge of interest
in stance classification in online debates.
Oftentimes, however, it is important to de-
termine not only the stance expressed by
an author in her debate posts, but also the
reasons behind her supporting or oppos-
ing the issue under debate. We therefore
examine the new task of reason classifi-
cation in this paper. Given the close in-
terplay between stance classification and
reason classification, we design computa-
tional models for examining how automat-
ically computed stance information can be
profitably exploited for reason classifica-
tion. Experiments on our reason-annotated
corpus of ideological debate posts from
four domains demonstrate that sophisti-
cated models of stances and reasons can
indeed yield more accurate reason and
stance classification results than their sim-
pler counterparts.
1 Introduction
In recent years, researchers have begun exploring
new opinion mining tasks. One such task is debate
stance classification (SC): given a post written for
a two-sided topic discussed in an online debate fo-
rum, determine which of the two sides (i.e., for or
against) its author is taking (Agrawal et al., 2003;
Thomas et al., 2006; Bansal et al., 2008; Soma-
sundaran and Wiebe, 2009; Burfoot et al., 2011;
Hasan and Ng, 2013b). For example, the author of
the post shown in Figure 1 is pro-abortion.
Oftentimes, however, it is important to deter-
mine not only the author?s stance expressed in her
debate posts, but also the reasons why she supports
or opposes the issue under debate. Intuitively,
given a debate topic such as ?Should abortion be
banned?? or ?Do you support Obamacare??, it
[I feel that abortion should remain legal, or rather, parents
should have the power to make the decision themselves and
not face any legal hindrance of any form.]
1
Let us take a
look from the social perspective. [If parents cannot afford
to provide for the child, or if the family is facing financial
constraints, it is understandable that abortion can remain as
one of the options.]
2
Reason 1: Woman?s right to abort
Reason 2: Unwanted babies are threat to their parents? fu-
ture
Figure 1: A sample post on abortion annotated
with reasons.
should not be difficult for us to come up with a set
of reasons people typically use to back up their
stances. Given a set of reasons associated with
each stance in an online debate, the goal of post-
level reason classification is to identify those rea-
son(s) an author uses to back up her stance in her
debate post. A more challenging version of this
task is sentence-level reason classification, where
the goal is to identify not only the reason(s) an au-
thor uses in her post, but also the sentence(s) in
the post that the author uses to describe each of
her reasons. For example, the author of the post
shown in Figure 1 mentions two reasons why she
supports abortion, namely it?s a woman?s right to
abort and unwanted babies are threat to their par-
ents? future, which are mentioned in the first and
third sentences in the post respectively.
Our goal in this paper is to examine post- and
sentence-level reason classification (RC) in ideo-
logical debates. Many online debaters use emo-
tional languages, which may involve sarcasm and
insults, to express their points, thereby making
RC and SC in ideological debates potentially more
challenging than that in other debate settings such
as congressional debates and company-internal
discussions (Walker et al., 2012).
Besides examining the new task of RC in ide-
ological debates, we believe that our work makes
three contributions. First, we propose to address
post-level RC by means of sentence-level RC by
751
(1) determining the reason(s) associated with each
of its sentences (if any), and then (2) taking the
union of the set of reasons associated with all of its
sentences to be the set of reasons associated with
the post. We hypothesize that this sentence-based
approach, which exploits a training set in which
each sentence in a post is labeled with its reason,
would achieve better performance than a multi-
label text classification approach to post-level RC,
which learns to determine the subset of reasons a
post contains directly from a training set in which
each post is labeled with the corresponding set of
reasons. In other words, we hypothesize that we
could achieve better results for post-level RC by
learning from sentence-level than from post-level
reason annotations, as sentence-level reason anno-
tations can enable a learning algorithm to accu-
rately attribute an annotated reason to a particular
portion of a post.
Second, we propose stance-supported RC sys-
tems, hypothesizing that automatically computed
stance information can be profitably exploited for
RC. Since we are exploiting automatically com-
puted (and thus potentially noisy) stance informa-
tion, we hypothesize that the effectiveness of such
information would depend in part on the way it is
exploited in RC systems. As a result, we introduce
a set of stance-supported models for RC, start-
ing with simple pipeline models and then mov-
ing on to joint models with increasing sophisti-
cation. Note that exploiting stance information
by no means guarantees that RC performance will
improve, as an incorrect determination of stance
could lead to an incorrect identification of rea-
sons. Hence, one of our goals is to examine how to
model stances and reasons so that RC can benefit
from stance information.
Finally, since progress on RC is hindered in part
by the lack of an annotated corpus, we make our
reason-annotated dataset publicly available.
1
To
our knowledge, this will be the first publicly avail-
able corpus for sentence- and post-level RC.
2 Corpus and Annotation
We collected debate posts from four popular
domains, Abortion (ABO), Gay Rights (GAY),
Obama (OBA), and Marijuana (MAR), from an
online debate forum
2
. All debates are two-sided,
1
http://www.hlt.utdallas.edu/
?
saidul/
stance/
2
http://www.createdebate.com/
so each post receives one of two stance labels, for
or against, depending on whether the author of
the post supports or opposes abortion, gay rights,
Obama, or the legalization of marijuana respec-
tively. A post?s stance label is given by its author.
Note that each post belongs to a thread, which
is a tree with one or more nodes such that (1) each
node corresponds to a debate post, and (2) a post
y
i
is the parent of another post y
j
if y
j
is a reply
to y
i
. Given a thread, we generate post sequences,
each of which is a path from the root of the thread
to one of its leaves. Hence, a post sequence is an
ordered set of posts such that each post is a reply
to its immediately preceding post in the sequence.
Table 2a shows the statistics of the four stance-
labeled datasets.
While the debate posts contain the stance labels
given by their authors, they are not annotated with
reasons. As part of our study of RC, we annotate
each post with the reasons it gives for its stance.
Our annotation procedure is composed of three
steps. First, two human annotators independently
examined each post and identified the reasons au-
thors present to support their stances (i.e., for and
against) in each domain. Second, they discussed
and agreed on the reasons identified for each do-
main. Third, they independently annotated the text
of each post with reason labels from the post?s do-
main. To do this, they labeled each sentence of a
post with the set of reasons the author expressed in
that sentence. Any sentence that does not belong
to any reason class was assigned the NONE class.
After the annotators completed the aforemen-
tioned steps, they were asked to collapse all the
reason classes that occur in less than 2% of the
sentences annotated with non-NONE classes into
the OTHER class. In other words, all the sentences
that were originally annotated with one of these
infrequent reason classes will now be labeled as
OTHER. Our decision to merge infrequent classes
is motivated by two observations. First, from a
practical point of view, infrequent reasons do not
carry much weight. Second, from a modeling per-
spective, it is often not worth increasing model
complexity by handling infrequent classes. The
resulting set of reason classes for each domain is
shown in Table 1.
A closer examination of the resulting annota-
tions reveals that approximately 3% of the sen-
tences received multiple reason labels. Again, to
avoid the complexity of modeling multi-labeled
752
Domain Stance Reason classes
ABO
for [F1] Abortion is a woman?s right (26%); [F2] Rape victims need it to be legal (7%); [F3] A fetus
is not human (38%); [F4] Mother?s life in danger (5%); [F5] Unwanted babies are ill-treated by
parents (8%); [F6] Birth control fails at times (3%); [F7] Abortion is not murder (3%); [F8] Mother
is not healthy/financially solvent (4%); [F9] Others (6%)
against [A1] Put baby up for adoption (9%); [A2] Abortion kills a life (29%); [A3] An unborn baby is a
human and has the right to live (40%); [A4] Be willing to have the baby if you have sex (14%);
[A5] Abortion is harmful for women (5%); [A6] Others (3%)
GAY
for [F1] Gay marriage is like any other marriage (14%); [F2] Gay people should have the same rights
as straight people (36%); [F3] Gay parents can adopt and ensure a happy life for a baby (10%); [F4]
People are born gay (18%); [F5] Religion should not be used against gay rights (11%); [F6] Others
(11%)
against [A1] Religion does not permit gay marriages (18%); [A2] Gay marriages are not normal/against
nature (39%); [A3] Gay parents can not raise kids properly (11%); [A4] Gay people have problems
and create social issues (16%); [A5] Others (16%)
OBA
for [F1] Fixed the economy (21%); [F2] Ending the wars (7%); [F3] Better than the republican candi-
dates (25%); [F4] Makes good decisions/policies (8%); [F5] Has qualities of a good leader (14%);
[F6] Ensured better healthcare (8%); [F7] Executed effective foreign policies (6%); [F8] Created
more jobs (4%); [F9] Others (7%)
against [A1] Destroyed our economy (26%); [A2] Wars are still on (11%); [A3] Unemployment rate is high
(5%); [A4] Healthcare bill is a failure(9%); [A5] Poor decision-maker (7%); [A6] We have better
republicans than Obama (5%); [A7] Not eligible as a leader (20%); [A8] Ineffective foreign policies
(4%); [A9] Others (13%)
MAR
for [F1] Not addictive (23%); [F2] Used as a medicine (11%); [F3] Legalized marijuana can be con-
trolled and regulated by the government (33%); [F4] Prohibition violates human rights (15%); [F5]
Does not cause any damage to our bodies (6%); [F6] Others (12%)
against [A1] Damages our bodies (23%); [A2] Responsible for brain damage (22%); [A3] If legalized,
people will use marijuana and other drugs more (12%); [A4] Causes crime (9%); [A5] Highly
addictive (17%); [A6] Others (17%)
Table 1: Reason classes and their percentages in the corresponding stance for each domain.
sentences given their rarity, we asked each annota-
tor to pick the reason that was highlighted the most
in each multi-labeled sentence.
Inter-annotator agreement scores at the sentence
level and the post level, expressed in terms of Co-
hen?s Kappa (Carletta, 1996), are shown in Ta-
ble 2b. Given that the majority of sentences were
labeled as NONE, we avoid inflating agreement by
not considering the sentences labeled with NONE
by both annotators when computing Kappa. As we
can see, we achieved substantial post-level agree-
ment and high sentence-level agreement.
The major source of inter-annotator disagree-
ment for all four datasets stems from the fact that
in many cases, the annotators, while agreeing on
the reason class, differ on how long the text span
for a reason should be. This hurts sentence-level
agreement but not post-level agreement, since the
latter only concerns whether a reason was men-
tioned in a post, and explains why the sentence-
level agreement scores are lower than the corre-
sponding post-level scores. Minor sources of dis-
agreement arise from the facts that (1) the anno-
tators selected different reason labels for some of
the multi-labeled sentences, and (2) they tend to
disagree in some cases where authors use sarcasm
ABO GAY OBA MAR
Stance-labeled posts
1741 1376 985 626
for posts (%)
54.9 63.4 53.9 69.5
Average post sequence length
4.1 4.0 2.6 2.5
(a) Statistics of stance-labeled posts
ABO GAY OBA MAR
Reason-labeled posts
463 561 447 432
% of sentences w/ reason tags
20.4 29.8 34.4 43.7
Kappa (sentence)
0.66 0.63 0.61 0.67
Kappa (post)
0.82 0.80 0.78 0.83
(b) Statistics of reason-labeled posts
Table 2: Stance and reason annotation statistics.
to present a reason. Each case of disagreement is
resolved through discussion among the annotators.
3 Baseline RC System
Our baseline system uses a maximum entropy
(MaxEnt) classifier to determine whether a reason
is expressed in a post and/or its sentence(s). We
create one training instance for each sentence in
each post in the training set, using the reason label
as its class label. We represent each instance using
five types of features, as described below.
N-gram features. We encode each unigram and
bigram collected from the training sentences as a
753
binary feature indicating the n-gram?s presence or
absence in a given sentence.
Dependency-based features. To capture the
inter-word relationships that n-grams may not,
we employ the dependency-based features previ-
ously used for stance classification in Anand et
al. (2011). These features have three variants. In
the first variant, the pair of arguments involved in
each dependency relation extracted by a depen-
dency parser is used as a feature. The second vari-
ant is the same as the first except that the head (i.e.,
the first argument in a relation) is replaced by its
part-of-speech tag. The features in the third vari-
ant, the topic-opinion features, are created by re-
placing each sentiment-bearing word in features of
the first two types with its corresponding polarity
label (i.e., + or ?).
Frame-semantic features. While dependency-
based features capture the syntactic dependencies,
frame-semantic features encode the semantic rep-
resentation of the concepts in a sentence. Fol-
lowing our previous work on stance classification
(Hasan and Ng, 2013c), we employ three types
of features computed based on the frame-semantic
parse of each sentence in a post obtained from SE-
MAFOR (Das et al., 2010). Frame-word interac-
tion features encode whether two words appear in
different elements of the same frame. Hence, each
frame-word interaction feature consists of (1) the
name of the frame f from which it is created, and
(2) an unordered word pair in which the words are
taken from two frame elements of f . A frame-pair
feature is represented as a word pair corresponding
to the names of two frames and encodes whether
the target word of the first frame appears within
an element of the second frame. Finally, frame n-
gram features are a variant of word n-grams. For
each word n-gram in the sentence, a frame n-gram
feature is created by replacing one or more words
in the word n-gram with the name of the frame or
the frame element in which the word appears. A
detailed description of these features can be found
in Hasan and Ng (2013c).
Quotation features. We employ two quotation
features. IsQuote is a binary feature that indicates
whether a sentence is a quote or not (i.e., whether
it appeared in its parent post in the post sequence).
Note that if an instance is a quote from a previ-
ous post, it is unlikely that it represents a reason
the author is presenting to support her argument.
Instead, the author may have quoted this before
stating her counter-argument. FollowsQuote is a
binary feature that indicates whether a sentence
follows a sentence for which the IsQuote feature
value is true. Intuitively, a sentence following a
quote is likely to present a counter-argument.
Positional feature. We split each post into four
parts (such that each part contains roughly the
same number of sentences) and create one posi-
tional feature that encodes which part of the post
contains a given sentence. This feature is moti-
vated by our observations on the training data that
(1) reasons are more likely to appear in the second
half of a post and (2) on average more than one-
third of the reasons appear in the last quarter of a
post.
After training, we can apply the resulting RC
system to classify the test instances, which are
generated in the same way as the training in-
stances. Once the sentences of a test post are clas-
sified, we simply assume its post-level reason la-
bels to be the set of reason labels assigned by the
classifier to its sentences.
4 Stance-Supported RC Systems
In this section, we propose a set of systems for
RC. Unlike the baseline RC system, these RC
systems are stance-supported, enabling us to ex-
plore how different ways of modeling automati-
cally computed stances and reasons can improve
RC classification. Below we present our systems
in increasing order of modeling sophistication.
4.1 Pipeline Systems
We examine two pipeline systems, P1 and P2.
Given a set of test posts, both systems first de-
termine the stance of each post and then apply a
stance-specific reason classifier to each of them.
More specifically, both P1 and P2 employ two
stance-specific reason classifiers: one is trained on
all the posts labeled as for and the other is trained
on all the posts labeled as against. Each stance-
specific reason classifier is trained using MaxEnt
on the same feature set as that of the Baseline RC
system. It computes for a particular stance s the
probability P (r|s, t), where r is a reason label and
t is a sentence in a test post p.
P1 and P2 differ only with respect to the SC
model used to stance-label each post. In P1, the
stance s of a post p is determined by applying to p
a stance classifier that computes P (s|p). To train
the classifier, we employ MaxEnt. Each train-
754
ing instance corresponds to a training post and
is represented by all but the quotation and posi-
tional features used to train the Baseline RC sys-
tem, since these two feature types are sentence-
based rather than post-based. After training, the
resulting classifier can be used to stance-label a
post independently of the other posts.
In P2, on the other hand, we recast SC as a se-
quence labeling task. In other words, we train a
SC model that assumes as input a post sequence
and outputs a stance sequence, with one stance la-
bel for each post in the input post sequence. This
choice is motivated by an observation we made
previously (Hasan and Ng, 2013a): since each post
in a sequence is a reply to the preceding post, we
could exploit their dependencies by determining
their stance labels together.
3
As our sequence learner, we employ a maxi-
mum entropy Markov model (MEMM) (McCal-
lum et al., 2000). Given an input post sequence
P
S
= (p
1
, p
2
, . . . , p
n
), the MEMM finds the most
probable stance sequence S = (s
1
, s
2
, . . . , s
n
) by
computing P (S|P
S
), where:
P (S|P
S
) =
n
?
k=1
P (s
k
|s
k?1
, p
k
) (1)
This probability can be computed efficiently via
dynamic programming (DP), using a modified ver-
sion of the Viterbi algorithm (Viterbi, 1967).
There is a caveat, however. Recall that the post
sequences are generated from a thread. Since a
test post may appear in more than one sequence,
different occurrences of it may be assigned differ-
ent stance labels by the MEMM. To determine the
final stance label for the post, we average the prob-
abilities assigned to the for stance over all its oc-
currences; if the average is ? 0.5, then its final
label is for; otherwise, its label is against.
4.2 System based on Joint Inference
One weakness of the pipeline systems is that errors
may propagate from the SC system to the RC sys-
tem. If the stance of a post is incorrectly labeled,
its reasons will also be incorrectly labeled.
To avoid this problem, we employ joint infer-
ence. Specifically, we first train a SC system and
3
While we could similarly recast the problem of assigning
reasons to the sentences in a post as a sequence learning task,
we did not pursue this idea further because preliminary ex-
periments indicated that sequence learning for RC was inef-
fective: there is little, if any, dependency between the reason
labels in consecutive sentences.
a RC system independently of each other. We em-
ploy the Baseline as our RC system, since this is
the only RC system that is not stance-specific. For
the SC system, we employ P2.
Since the SC system and the RC system are
trained independently of each other, their outputs
may not be consistent. For instance, an inconsis-
tency arises if a post is labeled as for but one or
more of its reasons are associated with the oppos-
ing stance. In fact, an inconsistency can arise in
the output of the RC system alone: reasons associ-
ated with both stances may be assigned by the RC
systems to different sentences of a given post.
To enforce consistency, we apply integer lin-
ear programming (ILP) (Roth and Yih, 2004). We
formulate one ILP program for each debate post.
Each ILP program contains two post-stance vari-
ables (x
for
and x
against
) and |T | ? |L
R
| reason
variables (i.e., one indicator variable z
t,r
for each
reason class r and each sentence t), where |T | is
the number of sentences in the post and |L
R
| is the
number of reason labels. Our objective is to maxi-
mize the linear combination of these variables and
their corresponding probabilities assigned by their
respective classifiers (see (2) below) subject to two
types of constraints, the integrity constraints and
the post-reason constraints. The integrity con-
straints ensure that each post is assigned exactly
one stance and each sentence in a post is assigned
exactly one reason class (see the two equality con-
straints in (3)). The post-reason constraints ensure
consistency between the predictions made by the
SC and the RC systems. Specifically, (1) if there
is at least one reason supporting the for stance, the
post must be assigned a for label; and (2) a for
post must have at least one for reason. These con-
straints are defined for the against label as well
(see the constraints in (4)).
Maximize:
?
s?L
S
a
s
x
s
+
1
|T |
|T |
?
t=1
?
r?L
R
b
t,r
z
t,r
(2)
subject to:
?
s?L
S
x
s
= 1, ? t
?
r?L
R
z
t,r
= 1,
x
s
? {0, 1}, z
t,r
? {0, 1}
(3)
? t x
s
? z
t,r
,
|T |
?
t=1
z
t,r
? x
s
(4)
755
Note that (1) a
s
and b
t,r
are two sets of probabil-
ities assigned by the SC and RC systems respec-
tively; (2) L
S
and L
R
denote the set of stance
labels and reason labels respectively; and (3) the
fraction
1
|T |
ensures that both classifiers are con-
tributing equally to the objective function.
4.3 Systems based on Joint Learning
Another way to avoid the error propagation prob-
lem in pipeline systems is to perform joint learn-
ing. In joint learning, the two tasks, SC and RC,
are learned jointly. Below we propose three joint
models in increasing level of sophistication.
J1 is a joint model that, given a test post p, finds
the stance label s and the reason label for each of
the sentences that together maximize the probabil-
ity P (R
p
, s|p), where R
p
= (r
1
, r
2
, . . . , r
n
) is the
sequence of reason labels with r
i
(1 ? i ? n)
being the reason label assigned to t
i
, the i-th sen-
tence in p. Using Chain Rule,
P (R
p
, s|p) = P (s|p)P (R
p
|s, p)
= P (s|p)
n
?
i=1
P (r
i
|s, t
i
)
(5)
Hence, P (R
p
, s|p) can be computed by using
the stance-specific RC classifier and the SC classi-
fier employed in P1.
The second joint model, J2, is the same as
J1, except that we recast SC as a sequence la-
beling task. As before, we employ MEMM to
learn how to predict stance sequences. Given a
post sequence P
S
= (p
1
, p
2
, . . . , p
n
), J2 finds the
stance sequence S = (s
1
, s
2
, . . . , s
n
) and rea-
sons R = (R
1
, R
2
, . . . , R
n
) that jointly maximize
P (R,S|P
S
). Note that R
i
is the sequence of rea-
son labels assigned to the sentences in post i.
The R and S that jointly maximize P (R,S|P
S
)
can be found efficiently via DP, using a modified
version of the Viterbi algorithm. Unlike in P2, in
J2 the decoding process is slightly more compli-
cated because we have to take into account R
i
. Be-
low we show the recursive definitions used to com-
pute the entries in the DP table, where v
k
(h) is the
(k,h)-th entry of the table; P (h|p) is provided by
the MaxEnt stance classifier used in P1; P (h|j, p)
is provided by the MEMM stance classifier used
in P2; P (r
max
i
|h, t
i
) is provided by the stance-
specific reason classifier used in the pipeline sys-
tems; and r
max
i
is the reason label for sentence t
i
that has the highest probability according to the
reason classifier.
Base case:
v
1
(h) = P (h|p)
n
?
i=1
P (r
max
i
|h, t
i
) (6)
Recursive definition:
v
k
(h) = max
j
v
k?1
(j)P (h|j, p)
n
?
i=1
P (r
max
i
|h, t
i
)
(7)
To motivate our third joint model, J3, we make
the following observation. Recall that a post in
a post sequence is a reply to its preceding post.
An inspection of the training data reveals that in
many cases, a reply is a rebuttal to the preced-
ing post, where an author attempts to argue why
the points or reasons raised in the preceding post
are wrong and then provides her reasons for the
opposing stance. Motivated by this observation,
we hypothesize that the reasons mentioned in the
preceding post could be useful for predicting the
reasons in the current post. However, none of the
models we have presented so far makes use of the
reasons predicted for the preceding post.
This motivates the design of J3, which we build
on top of J2. Specifically, to incorporate the reason
labels predicted for the preceding post in a post se-
quence, we augment the feature set of the stance-
specific reason classifiers with a set of reason fea-
tures, with one binary feature for each reason. The
value of a reason feature is 1 if and only if the cor-
responding reason is predicted to be present in the
preceding post. Hence, in J3, we can apply the
same DP equations we used in J2 except that the
set of features used by the reason classifier is aug-
mented with the reason features.
5 Evaluation
While our primary goal is to evaluate the RC sys-
tems introduced in the previous section, we are
also interested in whether SC performance can im-
prove when SC is jointly modeled with RC. More
specifically, our evaluation is driven by the follow-
ing question: will RC performance and SC perfor-
mance improve as we employ more sophisticated
methods for modeling reasons and stances? Be-
fore showing the results, we describe the metrics
for evaluating RC and SC systems.
756
System
ABO GAY OBA MAR
Stance
Reason
Stance
Reason
Stance
Reason
Stance
Reason
Sentence Post Sentence Post Sentence Post Sentence Post
Baseline ? 32.7 45.0 ? 23.3 40.5 ? 19.5 31.5 ? 28.7 44.2
P1 62.8 34.5 46.3 63.4 24.5 43.2 61.0 20.3 33.5 67.2 30.5 47.3
P2 65.1 36.1 47.7 64.2 26.6 45.5 63.8 21.1 34.4 68.5 32.9 48.8
ILP 65.2 36.5 48.4 64.6 28.0 46.7 63.6 22.8 35.0 68.8 33.1 48.9
J1 62.5 36.0 47.6 64.0 26.7 45.6 61.2 23.1 35.7 67.8 33.3 49.2
J2 65.9 37.9 50.6 65.3 29.6 48.5 63.5 24.5 37.1 68.7 34.5 50.5
J3 66.3 39.5 52.3 65.7 31.4 49.8 64.0 25.1 38.0 69.0 35.1 51.1
Table 3: SC accuracies and RC F-scores for our five-fold cross-validation experiments.
5.1 Experimental Setup
We express SC results in terms of accuracy (i.e.,
the percentage of test posts labeled with the cor-
rect stance) and RC results in terms of F-score
micro-averaged over all reason classes except the
NONE class. For each RC system, we report its
sentence-level RC score and post-level RC score,
which are computed over sentences and posts re-
spectively. As mentioned at the end of Section 3,
the set of post-level reason labels of a given post
is automatically obtained by taking the union of
the set of reason labels assigned to each of its sen-
tences. Hence, a reason classifier will be rewarded
as long as it can predict, for any sentence in a test
post, a reason label that the annotators assigned to
some sentence in the same post.
We obtain these scores via five-fold cross-
validation experiments. During fold partition, all
posts that are in the same post sequence are as-
signed to the same fold. All reason and stance
classifiers are domain-specific, meaning that each
of them is trained on sentences/posts from ex-
actly one domain and is applied to classify sen-
tences/posts from the same domain. We use the
Stanford maximum entropy classifier
4
for classifi-
cation and solve ILP programs using lpsolve
5
.
5.2 Results and Discussion
Results are shown in Table 3. Each row corre-
sponds to one of our seven RC systems, showing
its SC accuracy as well as its sentence- and post-
level RC F-scores for each domain.
Let us begin by discussing the RC results. First,
P1 and P2 significantly beat the Baseline on all
4
http://nlp.stanford.edu/software/
classifier.shtml
5
http://sourceforge.net/projects/
lpsolve/
four domains by an average of 1.4 and 3.1 points
at the sentence level and by an average of 2.3 and
3.8 points at the post level respectively.
6
These re-
sults show that stance information can indeed be
profitably used for RC even if it is incorporated
into RC systems in a simple manner. Second,
improving SC through sequence learning can im-
prove RC: the systems in which SC is recast as se-
quence labeling (P2 and J2) perform significantly
better than the corresponding systems that do not
(P1 and J1). Third, ILP significantly beats P2 on
two domains (ABO and GAY) and achieves the
same level of performance as P2 on the remain-
ing domains. These results suggest that joint infer-
ence is no worse (and sometimes even better) than
pipeline learning as far as exploiting stance infor-
mation for RC is concerned. Fourth, the systems
trained via joint learning (J1 and J2) beat their cor-
responding pipeline counterparts (P1 and P2) on
all four domains, significantly so by an average of
2.3 and 2.5 points at the sentence level and by an
average of 2.0 and 2.6 points at the post level re-
spectively, suggesting that joint learning is indeed
a better way to incorporate stance information than
pipeline learning. Finally, J3, the joint system that
exploits reasons predicted for the previous post,
significantly beats J2, the system on which it is
built, by 1.6 and 1.8 points at the sentence level
and by 1.7 and 1.3 points at the post level for ABO
and GAY respectively. It also yields small, statis-
tically insignificant, improvements (0.6 points at
the sentence level and 0.6?0.9 points at the post
level) for the remaining two domains. These re-
sults suggest that the reasons predicted for the pre-
vious post indeed provide useful information for
predicting the current post?s reasons.
Overall, these results are consistent with our hy-
6
All significance tests are paired t-tests (p < 0.05).
757
pothesis that the usefulness of stance information
depends in part on the way it is exploited, and
that RC performance increases as we employ more
sophisticated methods for modeling reasons and
stances. Our best system, J3, significantly beats
the Baseline by an average of 6.7 and 7.5 points at
the sentence and post levels respectively.
As mentioned earlier, a secondary goal of this
work is to determine whether joint modeling can
improve SC as well. For that reason, we com-
pare the performances of the best pipeline model
(P2) and the best joint model (J3) on each domain.
We find that in terms of SC accuracy, J3 is signifi-
cantly better than P2 on ABO and GAY, and yields
slightly, though insignificantly, better performance
on the remaining two domains. In other words, our
results suggest that joint modeling of SC and RC
has a positive impact on SC performance on all
domains, and the impact can sometimes be large
enough to yield significantly better results.
5.3 Further Comparison
We hypothesized in the introduction that the
sentence-based approach to post-level RC would
yield better performance than the multi-label text
classification approach. In Section 5.2, we pre-
sented results of the sentence-based approach to
RC. So, to test this hypothesis, we next evaluate
the multi-label text classification approach to RC.
Recall that the multi-label text classification ap-
proach assumes the following setup. Given a set
of training posts where each post is multi-labeled
with the set of reasons it contains, the goal is to
train a system to determine the set of reasons a
test post contains. Hence, unlike in the sentence-
based approach, in this approach no sentence-level
reason annotations are exploited during training.
We implement this approach by recasting multi-
label text classification as n binary text classifica-
tion tasks, where n is the number of reason classes
for a domain. In the binary classification task for
predicting reason i, we train a binary classifier c
i
using the one-versus-all training scheme. Specif-
ically, to train c
i
, we create one training instance
for each post p in the training set, labeling it as
positive if and only if p contains reason i. Note
that if i is a minority reason, the class distribution
of the resulting training set will be highly skewed
towards the negatives, which will in turn cause the
resulting MaxEnt classifier to be biased towards
predicting a test instance as negative.
To address this problem, we adjust the classifi-
cation thresholds associated with the binary classi-
fiers. Recall that a test instance is classified as pos-
itive by a binary classifier if and only if its prob-
ability of belonging to the positive class is above
the classification threshold used. Hence, adjusting
the threshold amounts to adjusting the number of
test instances classified as positive, thus address-
ing the bias problem mentioned above. Specifi-
cally, we adjust the thresholds of the classifiers as
follows. We train the binary classifiers to optimize
the overall F-score by jointly tuning their classifi-
cation thresholds on 25% of the training data re-
served for development purposes. Since comput-
ing the exact solution to this optimization prob-
lem is computationally expensive, we employ a lo-
cal search algorithm that changes the value of one
threshold at a time to optimize F-score while keep-
ing the remaining thresholds fixed. During testing,
classifier c
i
will classify a test instance as positive
if its probability of belonging to the positive class
is above the corresponding threshold.
We apply this multi-label text classification ap-
proach to obtain post-level RC scores for the Base-
line, P1 and P2. Note that since P1 and P2 are
pipeline systems, the binary classifiers they use to
predict a test post?s reasons depend on the post?s
predicted stance. Specifically, if a test post is pre-
dicted to have a positive (negative) stance, then
only the reason classifiers associated with the pos-
itive (negative) stance will be used to predict the
reasons it contains. On the other hand, this ap-
proach cannot be used in combination with ILP or
the joint models to produce post-level RC scores:
they all require a reason classifier trained on
reason-annotated sentences, which are not avail-
able in the multi-label text classification approach.
Post-level RC results of the Baseline and the
two pipeline systems, P1 and P2, obtained via this
multi-label text classification approach are shown
in Table 4. These scores are significantly lower
than the corresponding scores in Table 3 by 3.2,
2.9, and 3.1 points for the Baseline, P1, and P2 re-
spectively, when averaged over the four domains.
They confirm our hypothesis that the sentence-
based approach to post-level RC is indeed better
than its multi-class text classification counterpart.
5.4 Error Analysis
To get a better understanding of our best-
performing RC system (J3), we examine its major
758
System ABO GAY OBA MAR
Baseline 39.8 37.9 30.1 40.8
P1 41.5 41.0 31.7 44.7
P2 43.3 42.6 32.0 46.3
Table 4: Post-level RC F-scores obtained via the
multi-class text classification approach.
sources of error in this subsection.
For the four domains, 75?83% of the errors
can be attributed to the system?s inability to de-
cide whether a sentence describes a reason or not.
Specifically, in 51?54% of the erroneous cases, a
reason sentence is misclassified as NONE. On the
other hand, 23?30% of the cases are concerned
with assigning a reason label to a NONE sentence.
The remaining 17?25% of the errors concern mis-
labeling a reason sentence with the wrong reason.
A closer examination of the errors reveals that
they resulted primarily from (1) the lack of access
to background knowledge, (2) the failure to pro-
cess complex discourse structures, and (3) the fail-
ure to process sarcastic statements and rhetorical
questions. We present two examples for each of
these three major sources of error from the ABO
and OBA domains in Table 5. In each example,
we show their predicted (P) and gold (G) labels.
Lack of access to background knowledge.
Consider the first example for ABO in Table 5.
Our system misclassifies this sentence in part be-
cause it lacks the background knowledge that ?ge-
netic code? is one of the characteristics of life and
a fetus having it means a fetus has life (A3). Sim-
ilarly, the system cannot determine the reason for
the first OBA example without the knowledge that
?deficit spending? is a term related to the econ-
omy and that increasing it is bad (A1). We believe
some of these relations can be extracted from lex-
ical knowledge bases such as YAGO2 (Suchanek
et al., 2007), Freebase (Bollacker et al., 2008), and
BabelNet (Navigli and Ponzetto, 2012).
Failure to process complex discourse struc-
tures. Our system misclassifies the second ex-
ample for ABO in part because the first part of
the sentence (i.e., Sure, the fetus has the potential
to one day be a person) expresses a meaning that
is completely inverted by the second part. Such
complex discourse structures often lead to classi-
fication errors even for sentences whose interpre-
tation requires no background knowledge. We be-
lieve that this problem can be addressed in part
by a better understanding of the structure of a dis-
course, particularly the relation between two dis-
course segments, using a discourse parser.
Failure to process sarcastic statements and
rhetorical questions. Owing to the nature of
our dataset (i.e., debate posts), many errors arise
from sentences containing sarcasm and/or rhetori-
cal questions. This is especially a problem in long
post sequences, where authors frequently restate
their opponents? positions, sometimes ironically.
A first step towards handling these errors would
therefore be to identify sentences containing sar-
casm and/or rhetorical questions.
6 Related Work
In this section, we discuss related work in the areas
of document-level RC, argument recognition, tex-
tual entailment in online debates, argumentation
mining, and sentiment analysis.
Document-level reason classification. Persing
and Ng (2009) apply a multi-label text classifi-
cation approach to document-level RC of aviation
safety incident reports. Given a set of pre-defined
reasons, their RC system seeks to identify the rea-
sons that can explain why the incident described
in a given report occurred. Their work is dif-
ferent from ours in at least two respects. First,
while our posts occur in post sequences (which
can be profitably exploited in RC, for example, as
in J3), their incident reports were written indepen-
dently of each other. Second, they do not perform
sentence-level RC, as the lack of sentence-level
reason annotations in their dataset prevented them
from training a sentence-level reason classifier.
Argument recognition. Boltuz?ic? and
?
Snajder
(2014) propose a multi-class classification task
called argument recognition in online discussions.
Given a post and a reason for a particular domain,
the task is to predict the extent to which the au-
thor of the post supports or opposes the reason
as measured on a five-point ordinal scale rang-
ing from ?explicitly supports? to ?explicitly op-
poses?. Hence, unlike RC, argument recognition
focuses on the magnitude rather than the exis-
tence of a post-reason relationship. In addition,
Boltuz?ic? and
?
Snajder focus on post-level (rather
than sentence-level) classification and employ per-
fect (rather than predicted) stance information.
Textual entailment in online debates. Given
the title of a debate and a post written in re-
sponse to it, this task seeks to detect arguments in
759
Domain
Background knowledge Complex discourse structure Sarcasm/rhetorical questions
Example P G Example P G Example P G
ABO Science does agree that
the fetus has an individ-
ual genetic code and fits
into the biological defi-
nition of life.
NONE A3 Sure, the fetus has the
potential to one day be
a person, but right now
it is not.
NONE F3 So are there enough
homes for 50,000,000
babies?
F9 F5
OBA Democrats have
increased deficit spend-
ing by 2 trillion dollars
over 2 years.
NONE A1 Bush raised the debt
by two billion for the
wars, Obama has out-
spent that in a week.
A2 A1 I agree, Bush put us in
debt for the next 100
years, so we can blame
Obama forever.
NONE F3
Table 5: Examples of the major sources of error. P and G stand for predicted tag and gold tag respectively.
the post that entail or contradict the title (Cabrio
and Villata, 2012). Hence, this task is concerned
with identifying text segments that correspond to
rationales without a predefined set of rationales,
whereas RC is concerned with both identifying
text segments and classifying them based on a
given set of reasons.
Argumentation mining. The goal of this task is
to extract the argumentative structure of a docu-
ment. Researchers have proposed approaches to
mine the structure of scientific papers (Teufel and
Moens, 2000; Teufel, 2001), product reviews (Vil-
lalba and Saint-Dizier, 2012; Wyner et al., 2012),
newspaper articles (Feng and Hirst, 2011), and le-
gal documents (Bru?ninghaus and Ashley, 2005;
Wyner et al., 2010; Palau and Moens, 2011; Ash-
ley and Walker, 2013). A major difference be-
tween this task and RC is that the argument types
refer to generic structural cues, textual patterns
etc., whereas our reason classes refer to the spe-
cific reasons an author may mention to support her
stance in a domain. For instance, in the case of
a scientific article, the argument types correspond
to general background, description of the paper?s
or some other papers? approach, objective, con-
trastive and/or comparative comments, etc. (Teufel
and Moens, 2000). The argument types for legal
documents refer to legal factors which are either
pro-plaintiff or pro-defendant (Bru?ninghaus and
Ashley, 2005). For instance, for trade secret law
cases, factors such as Waiver-of-Confidentiality
and Disclosure-in-Public-Forum refer to certain
facts strengthening the claim of one of the sides
participating in a case.
Sentiment analysis. RC resembles certain tasks
in sentiment analysis. One such task is pro and con
reason classification in reviews (Kim and Hovy,
2006), where sentences containing opinions as
well as reasons justifying the opinions are to be
extracted and classified as PRO, CON, or NONE.
Hence, this task focuses on categorizing sentences
into coarse-grained, high-level groups (e.g., PRO
vs. CON, POSITIVE vs. NEGATIVE), but does not
attempt to subcategorize the PRO and CON classes
into fine-grained reason classes, unlike RC. Some-
what similar to the PRO and CON sentence classifi-
cation task is the task of determining the relevance
of a sentence in a review for polarity classifica-
tion. Zaidan et al. (2007) coined the term ratio-
nale to refer to any subjective textual content that
contains evidence supporting the author?s opinion
or stance. These rationales, however, may not al-
ways contain reasons. For instance, a sentence that
mentions that the author likes a product is a ra-
tionale, but it does not contain any reason for her
liking it. Methods have been proposed for auto-
matically identifying rationales (e.g., Yessenalina
et al. (2010), Trivedi and Eisenstein (2013)) and
distinguishing subjective from objective materials
in a review (e.g., Pang and Lee (2004), Wiebe and
Riloff (2005), McDonald et al. (2007), Zhao et al.
(2008)). Note that in all these attempts, the end
goal is not to classify sentences, but to employ
the results of sentence classification to improve a
higher-level task, such as sentiment classification.
7 Conclusion
We examined the new task of reason classification.
We exploited stance information for reason classi-
fication, proposing systems of varying complexity
for modeling stances and reasons. Experiments on
our reason-annotated corpus of ideological debate
posts from four domains demonstrate that sophis-
ticated models of stances and reasons can indeed
yield more accurate reason and stance classifica-
tion results than their simpler counterparts. Nev-
ertheless, reason classification remains a challeng-
ing task: the best post-level F-scores are in the low
50s. By making our corpus publicly available, we
hope to stimulate further research on this task.
760
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier
draft of this paper. This work was supported in
part by NSFGrants IIS-1147644 and IIS-1219142.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views or of-
ficial policies, either expressed or implied, of NSF.
References
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In Pro-
ceedings of the 12th International Conference on
World Wide Web, pages 529?535.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis, pages 1?9.
Kevin D. Ashley and Vern R. Walker. 2013. From in-
formation retrieval (IR) to argument retrieval (AR)
for legal cases: Report on a baseline study. In Pro-
ceedings of the 26th InternationalConference on Le-
gal Knowledge and Information System, pages 29?
38.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label
disagreement in the min-cut classification frame-
work. In COLING 2008: Companion volume:
Posters, pages 15?18.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data, pages 1247?1250.
Filip Boltuz?ic? and Jan
?
Snajder. 2014. Back up your
stance: Recognizing arguments in online discus-
sions. In Proceedings of the First Workshop on Ar-
gumentation Mining, pages 49?58.
Stefanie Bru?ninghaus and Kevin D. Ashley. 2005.
Reasoning with textual cases. In Proceedings of the
6th International Conference on Case-Based Rea-
soning, pages 137?151.
Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional
floor-debate transcripts. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1506?1515.
Elena Cabrio and Serena Villata. 2012. Natural lan-
guage arguments: A combined approach. In Pro-
ceedings of the 20th European Conference on Artifi-
cial Intelligence, pages 205?210.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The Kappa statistic. Computational
Linguistics, 22(2):249?254.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. SEMAFOR 1.0: A
probabilistic frame-semantic parser. Technical re-
port, Carnegie Mellon University Technical Report
CMU-LTI-10-001.
Vanessa Wei Feng and Graeme Hirst. 2011. Classi-
fying arguments by scheme. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 987?996.
Kazi Saidul Hasan and Vincent Ng. 2013a. Extra-
linguistic constraints on stance recognition in ide-
ological debates. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 816?821.
Kazi Saidul Hasan and Vincent Ng. 2013b. Frame se-
mantics for stance classification. In Proceedings of
the Seventeenth Conference on Computational Nat-
ural Language Learning, pages 124?132.
Kazi Saidul Hasan and Vincent Ng. 2013c. Stance
classification of ideological debates: Data, mod-
els, features, and constraints. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 1348?1356.
Soo-Min Kim and Eduard Hovy. 2006. Automatic
identification of pro and con reasons in online re-
views. In Proceedings of the COLING/ACL Main
Conference Poster Sessions, pages 483?490.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov mod-
els for information extraction and segmentation. In
Proceedings of the 17th International Conference on
Machine Learning, pages 591?598.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 432?439.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Raquel Mochales Palau and Marie-Francine Moens.
2011. Argumentationmining. Artificial Intelligence
and Law, 19(1):1?22.
761
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics, pages 271?278.
Isaac Persing and Vincent Ng. 2009. Semi-supervised
cause identification from aviation safety reports. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 843?851.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of the Eighth Confer-
ence on ComputationalNatural Language Learning,
pages 1?8.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 226?234.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge. In Proceedings of the 16th International
World Wide Web Conference, pages 697?706.
Simone Teufel and Marc Moens. 2000. What?s yours
and what?s mine: Determining intellectual attribu-
tion in scientific text. In Proceedings of the 2000
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, pages 9?17.
Simone Teufel. 2001. Task-based evaluation of sum-
mary quality: Describing relationships between sci-
entific papers. In Proceedings of the NAACL Work-
shop on Automatic Summarization, pages 12?21.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327?335.
Rakshit Trivedi and Jacob Eisenstein. 2013. Dis-
course connectors for latent subjectivity in sentiment
analysis. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 808?813.
Maria Paz Garcia Villalba and Patrick Saint-Dizier.
2012. Some facets of argument mining for opinion
analysis. In Proceedings of the Fourth International
Conference on Computational Models of Argument,
pages 23?34.
Andrew J. Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decod-
ing algorithm. IEEE Transactions on Information
Theory, 13(2):260?269.
MarilynWalker, Pranav Anand, Rob Abbott, and Ricky
Grant. 2012. Stance classification using dialogic
properties of persuasion. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 592?596.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of the 6th International
Conference on Computational Linguistics and Intel-
ligent Text Processing, pages 486?497.
AdamWyner, Raquel Mochales-Palau, Marie-Francine
Moens, and David Milward. 2010. Approaches to
text mining arguments from legal cases. In Enrico
Francesconi, Simonetta Montemagni, Wim Peters,
and Daniela Tiscornia, editors, Semantic Processing
of Legal Texts: Where the Language of Law Meets
the Law of Language, pages 60?79. Springer-Verlag.
Adam Wyner, Jodi Schneider, Katie Atkinson, and
Trevor J. M. Bench-Capon. 2012. Semi-automated
argumentative analysis of online product reviews. In
Proceedings of the Fourth International Conference
on Computational Models of Argument, pages 43?
50.
Ainur Yessenalina, Yejin Choi, and Claire Cardie.
2010. Automatically generating annotator rationales
to improve sentiment classification. In Proceedings
of the ACL 2010 Conference Short Papers, pages
336?341.
Omar Zaidan, Jason Eisner, and Christine Piatko.
2007. Using ?annotator rationales? to improve ma-
chine learning for text categorization. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 260?267.
Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding re-
dundant features for crfs-based sentence sentiment
classification. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 117?126.
762
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 763?774,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Chinese Zero Pronoun Resolution: An Unsupervised Probabilistic
Model Rivaling Supervised Resolvers
Chen Chen and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{yzcchen,vince}@hlt.utdallas.edu
Abstract
State-of-the-art Chinese zero pronoun res-
olution systems are supervised, thus re-
lying on training data containing manu-
ally resolved zero pronouns. To elimi-
nate the reliance on annotated data, we
present a generative model for unsuper-
vised Chinese zero pronoun resolution.
At the core of our model is a novel hy-
pothesis: a probabilistic pronoun resolver
trained on overt pronouns in an unsuper-
vised manner can be used to resolve zero
pronouns. Experiments demonstrate that
our unsupervised model rivals its state-of-
the-art supervised counterparts in perfor-
mance when resolving the Chinese zero
pronouns in the OntoNotes corpus.
1 Introduction
A zero pronoun (ZP) is a gap in a sentence that
is found when a phonetically null form is used to
refer to a real-world entity. An anaphoric zero
pronoun (AZP) is a ZP that corefers with one or
more preceding noun phrases (NPs) in the asso-
ciated text. Below is an example taken from the
Chinese TreeBank (CTB), where the ZP (denoted
as *pro*) refers to??? (Russia).
[???] ???????????????
*pro*?????????????
([Russia] is a consistent supporter of Milo?evi?,
*pro* has proposed to mediate the political crisis.)
As we can see, ZPs lack grammatical attributes
that are useful for overt pronoun resolution such as
number and gender. This makes ZP resolution
more challenging than overt pronoun resolution.
Automatic ZP resolution is typically composed
of two steps. The first step, AZP identification, in-
volves extracting ZPs that are anaphoric. The sec-
ond step, AZP resolution, aims to identify an an-
tecedent of an AZP. State-of-the-art ZP resolvers
have tackled both of these steps in a supervised
manner, training a classifier for AZP identification
and another one for AZP resolution (e.g., Zhao and
Ng (2007), Chen and Ng (2013)).
In this paper, we focus on the second task, AZP
resolution, designing a model that assumes as in-
put the AZPs in a document and resolves each of
them. Note that the task of AZP resolution alone is
by no means easy: even when gold-standard AZPs
are given, state-of-the-art supervised resolvers can
only achieve an F-score of 47.7% for resolving
Chinese AZPs (Chen and Ng, 2013). For the sake
of completeness, we will evaluate our AZP resolu-
tion model using both gold-standard AZPs as well
as AZPs automatically identified by a rule-based
approach that we propose in this paper.
Our contribution lies in the proposal of the first
unsupervised probabilistic model for AZP resolu-
tion that rivals its supervised counterparts in per-
formance when evaluated on the Chinese portion
of the OntoNotes 5.0 corpus. Its main advan-
tage is that it does not require training data with
manually resolved AZPs. This, together with the
fact that its underlying generative process is not
language-dependent, enables it to be applied to
languages where such annotated data is not read-
ily available. At its core is a novel hypothesis:
we can apply a probabilistic pronoun resolution
model trained on overt pronouns in an unsuper-
vised manner to resolve zero pronouns. Moti-
vated by Cherry and Bergsma's (2005) and Char-
niak and Elsner's (2009) work on unsupervised
English pronoun resolution, we train our unsu-
pervised resolver on Chinese overt pronouns us-
ing the Expectation-Maximization (EM) algorithm
(Dempster et al., 1977).
2 Related Work
Chinese ZP resolution. Early approaches to
Chinese ZP resolution are rule-based. Con-
verse (2006) applied Hobbs' algorithm (Hobbs,
763
1978) to resolve the ZPs in the CTB documents.
Yeh and Chen (2007) hand-engineered a set of
rules for ZP resolution based on Centering The-
ory (Grosz et al., 1995).
In contrast, virtually all recent approaches to
this task are based on supervised learning. Zhao
and Ng (2007) are the first to employ a supervised
learning approach to Chinese ZP resolution. They
trained an AZP resolver by employing syntactic
and positional features in combination with a de-
cision tree learner. Unlike Zhao and Ng, Kong
and Zhou (2010) employed context-sensitive con-
volution tree kernels (Zhou et al., 2008) in their
resolver to model syntactic information. More re-
cently, we extended Zhao and Ng's feature set with
novel features that encode the context surrounding
a ZP and its candidate antecedents, and exploited
the coreference links between ZPs as bridges to
find textually distant antecedents for ZPs (Chen
and Ng, 2013).
ZP resolution for other languages. There have
been rule-based and supervised machine learning
approaches for resolving ZPs in other languages.
For example, to resolve ZPs in Spanish texts,
Ferr?ndez and Peral (2000) proposed a set of hand-
crafted rules that encode preferences for candidate
antecedents. In addition, supervised approaches
have been extensively employed to resolve ZPs
in Korean (e.g., Han (2006)), Japanese (e.g., Seki
et al. (2002), Isozaki and Hirao (2003), Iida et
al. (2006; 2007), Imamura et al. (2009), Iida and
Poesio (2011), Sasano and Kurohashi (2011)), and
Italian (e.g., Iida and Poesio (2011)).
3 Chinese Overt Pronouns
Since our approach relies heavily on Chinese
overt pronouns, in this section we introduce them
by describing their four grammatical attributes,
namely Number, Gender, Person and Ani-
macy. Number has two values, singular and
plural. Gender has three values, neuter, mascu-
line and feminine. Person has three values, first,
second and third. Finally, Animacy has two val-
ues, animate and inanimate.
We exploit ten personal pronouns that have
well-defined grammatical attribute values, namely
? (singular you),? (I),? (he),? (she),? (it),
?? (plural you), ?? (we), ?? (masculine
they),?? (feminine they), and?? (impersonal
they). As can be seen in Table 1, each of them can
be uniquely identified using these four attributes.
Pronouns Number Gender Person Animacy
? (I) singular neuter first animate
? (you) singular neuter second animate
? (he) singular masculine third animate
? (she) singular feminine third animate
? (it) singular neuter third inanimate
?? (you) plural neuter second animate
?? (we) plural neuter first animate
?? (they) plural masculine third animate
?? (they) plural feminine third animate
?? (they) plural neuter third inanimate
Table 1: Attribute values of Chinese overt pronouns.
4 The Generative Model
4.1 Notation
Let p be an overt pronoun in PR, the set of the
10 overt pronouns described in Section 3. C, the
set of candidate antecedents of p, contains all and
only those maximal or modifier NPs that precede
p in the associated text and are at most two sen-
tences away from it.1 k is the context surround-
ing p as well as every candidate antecedent c in
C; k
c
is the context surrounding p and candidate
antecedent c; and l is a binary variable indicat-
ing whether c is the correct antecedent of p. The
set A = {Num,Gen, Per,Ani} has four ele-
ments, which correspond to Number, Gender,
Person and Animacy respectively. a is an at-
tribute in A. Finally, p
a
and c
a
are the attribute
values of p and c with respect to a respectively.
4.2 Training
Our model estimates P (p, k, c, l), the probability
of seeing (1) the overt pronoun p; (2) the context
k surrounding p and its candidate antecedents; (3)
a candidate antecedent c of p; and (4) whether c is
the correct antecedent of p. Since we estimate this
probability from a raw, unannotated corpus, we are
effectively treating p, k, and c as observed data and
l as hidden data.
Owing to the presence of hidden data, we es-
timate the model parameters using the EM algo-
rithm. Specifically, we use EM to iteratively es-
timate the model parameters from data in which
each overt pronoun is labeled with the probability
it corefers with each of its candidate antecedents
and apply the resultingmodel to re-label each overt
pronoun with the probability it corefers with each
of its candidate antecedents. Below we describe
1Only 8% of the overt pronouns in our corpus, the Chi-
nese portion of the OntoNotes 5.0 corpus, do not have any
antecedent in the preceding two sentences.
764
the details of the E-step and the M-step.
4.2.1 E-Step
The goal of the E-step is to compute
P (l=1|p, k, c), the probability that a candi-
date antecedent c is the correct antecedent of p
given context k. Assuming that exactly one of the
p's candidate antecedents is its correct antecedent,
we can rewrite P (l=1|p, k, c) as follows:
P (l=1|p, k, c) =
P (p, k, c, l=1)
?
c
?
?C
P (p, k, c
?
, l=1)
(1)
Applying Chain Rule, we can rewrite
P (p, k, c, l=1) as follows:
P (p, k, c, l=1) = P (p|k, c, l=1) ? P (l=1|k, c)
? P (c|k) ? P (k)
(2)
Next, given l = 1 (i.e., c is the antecedent of p),
we assume that we can generate p from c without
looking at the context.2 Then we represent p using
its grammatical attributes A. We further assume
that p's value with respect to attribute a ? A is
independent of the value of each of its remaining
attributes given the antecedent's value with respect
to a. So we can rewrite P (p|k, c, l=1) as follows:
P (p|k, c, l=1) ? P (p|c, l=1)
? P (p
Num
, p
Gen
, p
Per
, p
Ani
|c, l=1)
?
?
a?A
P (p
a
|c
a
, l=1)
(3)
Moreover, we assume that (1) given p and c's
context, the probability of c being the antecedent
of p is not affected by the context of the other can-
didate antecedents; and (2) k
c
is sufficient for de-
termining whether c is the antecedent of p. So,
P (l=1|k, c) ? P (l=1|k
c
, c) ? P (l=1|k
c
) (4)
Furthermore, we assume that given context k,
each candidate antecedent of p is generated with
equal probability. In other words,
P (c|k) ? P (c
?
|k) ? c, c
?
? C (5)
Given Equations (2), (3), (4) and (5), we can
rewrite P (l=1|p, k, c) as:
P (l=1|p, k, c) =
P (p, k, c, l=1)
?
c
?
?C
P (p, k, c
?
, l=1)
?
?
a?A
P (p
a
|c
a
, l=1) ? P (l=1|k
c
)
?
c
?
?C
?
a?A
P (p
a
|c
?
a
, l=1) ? P (l=1|k
c
?
)
(6)
2This assumption is reasonable because it is fairly easy to
determine which pronoun can be used to refer to a given NP.
As we can see from Equation (6), our model has
two groups of parameters, namely P (p
a
|c
a
, l=1)
and P (l=1|k
c
). Since we have four grammatical
attributes, P (p
a
|c
a
, l=1) contains four sets of pa-
rameters, with one set per attribute. Using Equa-
tion (6) and the current parameter estimates, we
can compute P (l=1|p, k, c).
Two points deserve mention before we describe
the M-step. First, we estimate P (l=1|p, k, c) from
all and only those overt pronouns p ? PR that
are surface or deep subjects in their correspond-
ing sentences. This condition is motivated by our
observation that 99.56% of the ZPs in our evalu-
ation corpus (i.e., OntoNotes 5.0) are surface or
deep subjects. In other words, we impose this con-
dition so that we can focus our efforts on learn-
ing a model for resolving overt pronouns that are
subjects. This is by no means a limitation of our
model: if we were given a corpus in which many
ZPs occur as grammatical objects, we could sim-
ilarly train another model on overt objects. Sec-
ond, since in the E-step we attempt to probabilisti-
cally label every overt pronoun p that satisfies the
condition above, our model is effectively making
the simplifying assumption that every overt pro-
noun is anaphoric. This is clearly an overly sim-
plistic assumption. One way to relax this assump-
tion, which we leave as future work, is to first iden-
tify those pronouns that are anaphoric and then use
EM to estimate the joint probability only from the
anaphoric pronouns.
4.2.2 M-Step
Given P (l=1|p, k, c), the goal of the M-step is to
(re)estimate the model parameters, P (p
a
|c
a
, l=1)
and P (l=1|k
c
), using maximum likelihood esti-
mation. Specifically, P (p
a
|c
a
, l=1) is estimated
as follows:
P (p
a
|c
a
, l=1) =
Count(p
a
, c
a
, l=1) + ?
Count(c
a
, l=1) + ? ? |a|
(7)
where Count(c
a
, l=1) is the expected number of
times c has attribute value c
a
when it is the an-
tecedent of p; |a| is the number of possible values
of attribute a; ? is the Laplace smoothing param-
eter, which we set to 1; and Count(p
a
, c
a
, l=1)
is the expected number of times p has attribute
value p
a
when its antecedent c has attribute value
c
a
. Given attribute values p?
a
and c?
a
, we compute
765
Count(p
?
a
, c
?
a
, l=1) as follows:
Count(p
?
a
, c
?
a
, l=1) =
?
p,c:p
a
=p
?
a
,c
a
=c
?
a
P (l=1|p, k, c)
(8)
Similarly, P (l=1|k
c
) is estimated as follows:
P (l=1|k
c
) =
Count(k
c
, l=1) + ?
Count(k
c
) + ? ? 2
(9)
where Count(k
c
) is the number of times k
c
ap-
pears in the training data, and Count(k
c
, l=1) is
the expected number of times k
c
is the context sur-
rounding a pronoun and its antecedent c. Given
context k?
c
, we compute Count(k?
c
, l=1) as fol-
lows:
Count(k
?
c
, l=1) =
?
k:k
c
=k
?
c
P (l=1|p, k, c) (10)
To start the induction process, we initialize all
parameters with uniform values. Specifically,
P (p
a
|c
a
, l=1) is set to 1
|a|
, and P (l=1|k
c
) is set
to 0.5. Then we iteratively run the E-step and the
M-step until convergence.
There are two important questions we have not
addressed. First, how can we compute the four at-
tribute values of a candidate antecedent (i.e., c
a
for each attribute a), which we need to estimate
P (p
a
|c
a
, l=1)? Second, what features should we
use to represent context k
c
, which we need to esti-
mate P (l=1|k
c
)? We defer the discussion of these
questions to Sections 5 and 6.
4.3 Inference
After training, we can apply the resulting model to
resolve AZPs. Given an AZP z, we determine its
antecedent as follows:
(c?, p?) = argmax
c?C, p?PR
P (l=1|p, k, c) (11)
where PR is our set of 10 Chinese overt pronouns
and C is the set of candidate antecedents of z. In
other words, we apply Formula (11) to eachAZP z,
searching for the candidate antecedent c and overt
pronoun p that maximize P (l=1|p, k, c) when p is
used to fill the ZP gap left behind by z. The c that
results in the maximum probability value over all
overt pronouns in PR is chosen as the antecedent
of z. In essence, since the model is trained on
overt pronouns but is applied to ZPs, we have to
exhaustively fill the ZP's gap under consideration
with each of the 10 overt pronouns in PR during
inference.
Although we can now apply our generative
model to resolve AZPs, the resolution procedure
can be improved further. The improvement is
motivated by a problem we observed previously
(Chen and Ng, 2013): an AZP and its closest an-
tecedent can sometimes be far away from each
other, thus making it difficult to correctly resolve
the AZP. To address this problem, we employ the
following resolution procedure in our experiments.
Given a test document, we process its AZPs in a
left-to-right manner. As soon as we resolve an
AZP to a preceding NP c, we fill the correspond-
ing AZP's gap with c. Hence, when we process
an AZP z, all of its preceding AZPs in the associ-
ated text have been resolved, with their gaps filled
by the NPs they are resolved to. To resolve z, we
create test instances between z and its candidate
antecedents in the same way as described before.
The only difference is that the set of candidate an-
tecedents of z may now include those NPs that are
used to fill the gaps of the AZPs resolved so far. In
other words, this incremental resolution procedure
may increase the number of candidate antecedents
of each AZP z. Some of these additional candidate
antecedents are closer to z than the original candi-
date antecedents, thereby facilitating the resolution
of z. If the model resolves z to the additional can-
didate antecedent that fills the gap left behind by,
say, AZP z?, we postprocess the output by resolv-
ing z to the NP that z? is resolved to.3
5 Attributes of Candidate Antecedents
In this section, we describe how we determine
the four grammatical attribute values (Number,
Gender, Person and Animacy) of a candidate
antecedent c, as they are used to represent c when
estimating P (p
a
|c
a
, l=1) for each attribute a.
5.1 ANIMACY
We determine the Animacy of a candidate an-
tecedent c heuristically. Specifically, we first
check the NP type of c. If c is a pronoun, we look
up its Animacy in Table 1. If c is a named en-
tity, there are two cases to consider: if c is a per-
son4, we label it as animate; otherwise, we label it
as inanimate. If c is a common noun, we look up
the Animacy of its head noun in an automatically
3This postprocessing step is needed because the additional
candidate antecedents are only gap fillers.
4A detailed description of our named entity recognizer can
be found in Chen and Ng (2014).
766
constructed word list WL. If the head noun is not
in WL, we set its Animacy to unknown.
Our method for constructing WL is motivated
by an observation of measure words in Chinese:
some of them only modify inanimate nouns while
others only modify animate nouns. For example,
the nouns modified by the measure word? are al-
ways inanimate, as in??? (one piece of paper).
On the other hand, the nouns modified by the mea-
sure word? are always animate, as in????
(one worker).
Given this observation, we first define two lists,
M
ani
andM
inani
. M
ani
is a list of measure words
that can only modify animate nouns. M
inani
is a
list of measure words that can only modify inan-
imate nouns.5 There exists a special measure
word in Chinese, ?, which can be used to mod-
ify most of the common nouns regardless of their
Animacy. As a result, we remove ? from both
lists. After constructing M
ani
and M
inani
, we (1)
parse the Chinese Gigaword corpus (Parker et al.,
2009), which contains 4,370,600 documents, using
an efficient dependency parser, ctbparser6 (Qian et
al., 2010), and then (2) collect all pairs of words
(m,n), where m is a measure word, n is a com-
mon noun, and there is a NMOD dependency re-
lation between m and n. Finally, we determine
the Animacy of a given common noun n as fol-
lows. First, we retrieve all of the pairs contain-
ing n. Then, we sum over all occurrences of m
in M
ani
(call the sum C
ani
), as well as all occur-
rences of m in M
inani
(call the sum C
inani
). If
C
ani
> C
inani
, we label this common noun as an-
imate; otherwise, we label it as inanimate.
Table 2 shows the learned values of
P (p
Ani
|c
Ani
, l=1). These results are consis-
tent with our intuition: an animate (inanimate)
pronoun is more likely to be generated from
an animate (inanimate) antecedent than from an
inanimate (animate) antecedent. Note that animate
pronouns are more likely to be generated than
inanimate pronouns regardless of the antecedent's
Animacy. This can be attributed to the fact that
94.6% of the pronouns in our corpus are animate.
5.2 GENDER
We determine the Gender of a candidate an-
tecedent c as follows. If c is a pronoun, we look up
its Gender in Table 1. Otherwise, we determine
5We create these two lists with the help of this page:
http://chinesenotes.com/ref_measure_words.htm
6http://code.google.com/p/ctbparser/
`
`
`
`
`
`
`
`
`
`
Antecedent
Pronoun animate inanimate
animate 0.999 0.001
inanimate 0.858 0.142
unknown 0.945 0.055
Table 2: Learned values of P (p
Ani
|c
Ani
, l=1).
its Gender based on its Animacy. Specifically,
if c is inanimate, we set its Gender to neuter.
Otherwise, we determine its gender by looking up
a gender word list constructed by Bergsma and
Lin's (2006) approach. If the word is not in the
list, we set its Gender to masculine by default.
Next, we describe how the aforementioned gen-
der word list is constructed. Following Bergsma
and Lin (2006), we define a dependency path as the
sequence of non-terminal nodes and dependency
labels between two potentially coreferent entities
in a dependency parse tree. From the parsed Chi-
nese Gigaword corpus, we first collect every de-
pendency path that connects two pronouns. For
each path P collected, we compute CL(P ), the
coreference likelihood of P , as follows:
CL(P ) =
N
I
(P )
N
I
(P ) + N
D
(P )
(12)
where N
I
(P ) is the number of times P connects
two identical pronouns, andN
D
(P ) is the number
of times it connects two different pronouns. As-
suming that two identical pronouns in a sentence
are coreferent (Bergsma and Lin, 2006), we can
see that the larger a path's CL value is, the more
likely it is that the two NPs it connects are corefer-
ent. To ensure that we have dependency paths that
are strongly indicative of coreference relations, we
consider a dependency path P a coreferent path if
and only if CL(P ) > 0.8.
Given these coreferent paths, we can compute
theGender of a nounn as follows. First, we com-
pute (1) N
M
(n), the number of coreferent paths
connecting n with a masculine pronoun; and (2)
N
F
(n), the number of coreferent paths connect-
ing n with a feminine pronoun. Then, if N
F
(n) >
N
M
(n), we set n's gender to feminine; otherwise,
we set it to masculine.
Table 3 shows the learned values of
P (p
Gen
|c
Gen
, l=1). These results are con-
sistent with our intuition: a pronoun is a lot more
likely to be generated from an antecedent with the
same Gender than one with a different Gender.
767
``
`
`
`
`
`
`
`
`
Antecedent
Pronoun neuter feminine masculine
neuter 0.864 0.018 0.117
feminine 0.065 0.930 0.005
masculine 0.130 0.041 0.828
Table 3: Learned values of P (p
Gen
|c
Gen
, l=1).
`
`
`
`
`
`
`
`
`
`
Antecedent
Pronoun singular plural
singular 0.861 0.139
plural 0.26 0.74
Table 4: Learned values of P (p
Num
|c
Num
, l=1).
5.3 NUMBER
When computing the Number of a candidate an-
tecedent in English, Charniak and Elsner (2009)
rely on part-of-speech information. For example,
NN and NNP denote singular nouns, whereas NNS
and NNPS denote plural nouns. However, Chi-
nese part-of-speech tags do not provide such in-
formation. Hence, we need a different method for
finding theNumber of a candidate antecedent c in
Chinese. If c is a pronoun, we look up itsNumber
in Table 1. If c is a named entity, its Number is
singular. If c is a common noun, we infer itsNum-
ber from its string: if the string ends with? or is
modified by a quantity word (e.g.,??,??), c
is plural; otherwise, c is singular.
Table 4 shows the learned values of
P (p
Num
|c
Num
, l=1). These results are con-
sistent with our intuition: a pronoun is more likely
to be generated from an antecedent with the same
Number than one with a different Number.
5.4 PERSON
Finally, we compute the Person of a candi-
date antecedent c. Similar to Charniak and El-
sner (2009), we set ? (I) and ?? (we) to first
person, ? (singular you) and ?? (plural you)
to second person, and everything else to third
person. We estimate two sets of probabilities
P (p
Per
|c
Per
, l=1), one where p and c are from the
same speaker, and the other where they are from
different speakers.7 This is based on our observa-
tion thatP (p
Per
|c
Per
, l=1) could be very different
in these two cases.
7We employ a simple heuristic to identify the speaker of
NPs occurring in direct speech: we assume that the speaker
is the subject of the speech's reporting verb. So for example,
we identify Jack as the speaker of This book in the sentence
"This book is good," Jack said.
`
`
`
`
`
`
`
`
`
`
Antecedent
Pronoun first second third
first 0.856 0.119 0.025
second 0.219 0.766 0.016
third 0.289 0.077 0.634
Table 5: Learned values of P (p
Per
|c
Per
, l=1)
(same speaker).
`
`
`
`
`
`
`
`
`
`
Antecedent
Pronoun first second third
first 0.417 0.525 0.057
second 0.75 0.23 0.02
third 0.437 0.229 0.334
Table 6: Learned values of P (p
Per
|c
Per
, l=1)
(different speakers).
Tables 5 and 6 show the learned values of these
two sets of probabilities. These results are consis-
tent with our intuition. In the same-speaker case, a
pronoun is a lot more likely to be generated from
an antecedent with the same speaker than one with
a different speaker. In the different-speaker case,
a first (second) person pronoun is most likely to be
generated from a second (first) person pronoun.
6 Context Features
To fully specify our model, we need to describe
how to represent k
c
, which is needed to compute
P (l=1|k
c
). Recall that k
c
encodes the context sur-
rounding candidate antecedent c and the associated
pronoun p. As described below, we represent k
c
using eight features, some of which are motivated
by previous work on supervised AZP resolution
(e.g., Zhao and Ng (2007), Chen and Ng (2013)).
Note that (1) all but feature 1 are computed based
on syntactic parse trees, and (2) features 2, 3, 6,
and 8 are ternary-valued features.
1. the sentence distance between c and p;
2. whether the node spanning c has an ancestor
NP node; if so, whether this NP node is a de-
scendant of c's lowest ancestor IP node;
3. whether the node spanning c has an ancestor
VP node; if so, whether this VP node is a de-
scendant of c's lowest ancestor IP node;
4. whether vp has an ancestor NP node, where
vp is the VP node spanning the VP that fol-
lows p;
5. whether vp has an ancestor VP node;
768
Training Test
Documents 1,391 172
Sentences 36,487 6,083
Words 756,063 110,034
Overt Subject Pronouns 13,418 ?
AZPs ? 1,713
Table 7: Statistics on the training and test sets.
6. whether p is the first word of a sentence; if
not, whether p is the first word of an IP clause;
7. whether c is a subject whose governing verb
is lexically identical to the verb governing p;
8. whether c is the closest candidate antecedent
with subject grammatical role and is seman-
tically compatible with p's governing verb; if
not, whether c is the first semantically com-
patible candidate antecedent8.
Our approach to determine semantic compatibil-
ity (in feature 8) resembles Kehler et al.'s (2004)
and Yang et al.'s (2005) methods for computing se-
lectional preferences. Specifically, for each verb
and each noun that serves as a subject in Chinese
Gigaword, we compute their mutual information
(MI). Now, given a pronoun p and a candidate an-
tecedent c in the training/test corpus, we retrieve
the MI value of c and p's governing verb. We then
consider them semantically compatible if and only
if their MI value is greater than zero.
7 Evaluation
7.1 Experimental Setup
Datasets. We employ the Chinese portion of the
OntoNotes 5.0 corpus that was used in the official
CoNLL-2012 shared task (Pradhan et al., 2012).
In the CoNLL-2012 data, the training set and de-
velopment set contain ZP coreference annotations,
but the test set does not. Therefore, we train our
models on the training set and perform evaluation
on the development set. Statistics on the datasets
are shown in Table 7. The documents in these
datasets come from six sources, namely Broadcast
News (BN), Newswire (NW), Broadcast Conver-
sation (BC), Telephone Conversation (TC), Web
Blog (WB) and Magazine (MZ).
8 We sort the candidate antecedents of p as follows. We
first consider the subject candidate antecedents in the same
sentence as p from right to left, then the other candidate an-
tecedents in the same sentence from right to left. Next, we
consider the candidate antecedents in the previous sentence,
also preferring candidates that are subjects, but in left-to-right
order. Finally, we consider the candidate antecedents two
sentences back, following the subject-first, left-to-right order.
Evaluation measures. We express the results of
ZP resolution in terms of recall (R), precision (P)
and F-score (F).
Evaluation settings. Following Chen and Ng
(2013), we evaluate our model in three settings.
In Setting 1, we assume the availability of gold
syntactic parse trees and gold AZPs. In Setting 2,
we employ gold syntactic parse trees and system
(i.e., automatically identified) AZPs. Finally, in
Setting 3, we employ system syntactic parse trees
and system AZPs. The gold and system syntactic
parse trees, as well as the gold AZPs, are obtained
from the CoNLL-2012 shared task dataset, while
the system AZPs are identified by the rule-based
approach described in the Appendix.9 Since our
AZP identification approach does not rely on any
labeled data, we are effectively evaluating an end-
to-end unsupervised AZP resolver in Setting 3.
7.2 Results
Baseline systems. We employ seven resolvers
as baseline systems. To gauge the difficulty of
the task, we employ four simple rule-based re-
solvers, which resolve an AZP z to (1) the can-
didate antecedent closest to z (Baseline 1); (2) the
subject NP closest to z (Baseline 2); (3) the clos-
est candidate antecedent that is semantically com-
patible with z (Baseline 3); and (4) the first can-
didate antecedent that is semantically compatible
with z, where the candidate antecedents are vis-
ited according to the order described in Footnote 8
(Baseline 4). These four baselines allow us to
study the role of (1) recency, (2) salience, (3) re-
cency combined with semantic compatibility, and
(4) salience combined with semantic compatibil-
ity in AZP resolution respectively. The remaining
three baselines are state-of-the-art supervised AZP
resolvers, which include our own resolver (Chen
and Ng, 2013) as well as our re-implementations
of Zhao and Ng's (2007) resolver and Kong and
Zhou's (2010) resolver.
The test set results of these seven baseline re-
solvers when evaluated under the three afore-
mentioned evaluation settings are shown in Ta-
ble 8. The system AZPs employed by the rule-
based resolvers are obtained using our rule-based
9One may wonder why we do not train a supervised sys-
tem for identifying AZPs and instead experiment with a rule-
based AZP identification system. The reason is that employ-
ing labeled data defeats the whole purpose of having an unsu-
pervised AZP resolution model: if annotated data is available
for training an AZP identification system, the same data can
be used to train an AZP resolution system.
769
Setting 1: Setting 2: Setting 3:
Gold Parses, Gold Parses, System Parses,
Gold AZPs System AZPs System AZPs
Baseline R P F R P F R P F
Selecting closest candidate antecedent 25.0 25.2 25.1 18.3 10.8 13.6 10.3 6.7 8.1
Selecting closest subject 42.0 43.6 42.8 31.8 19.2 23.9 18.0 11.9 14.4
Selecting closest semantically compatible candidate antecedent 28.5 28.8 28.7 20.5 12.2 15.3 11.7 7.6 9.2
Selecting first semantically compatible candidate antecedent 45.2 45.7 45.5 33.6 20.0 25.1 18.9 12.3 14.9
Zhao and Ng (2007) 41.5 41.5 41.5 22.4 24.4 23.3 12.7 14.2 13.4
Kong and Zhou (2010) 44.9 44.9 44.9 33.0 19.3 24.4 18.7 11.9 14.5
Chen and Ng (2013) 47.7 47.7 47.7 25.3 27.6 26.4 14.9 16.7 15.7
Table 8: AZP resolution results of the baseline systems on the test set.
Setting 1: Gold Parses, Gold AZPs Setting 2: Gold Parses, System AZPs Setting 3: System Parses, System AZPs
Best Baseline Our Model Best Baseline Our Model Best Baseline Our Model
Source R P F R P F R P F R P F R P F R P F
Overall 47.7 47.7 47.7 47.5 47.9 47.7 25.3 27.6 26.4 35.4 21.0 26.4 14.9 16.7 15.7 19.9 12.9 15.7
NW 38.1 38.1 38.1 41.7 41.7 41.7 15.5 21.7 18.1 29.8 24.8 27.0 6.0 12.2 8.0 11.9 13.0 12.4
MZ 34.6 34.6 34.6 34.0 34.2 34.1 18.5 19.6 19.0 24.1 14.5 18.1 6.2 9.4 7.5 6.2 5.2 5.7
WB 46.1 46.1 46.1 47.9 47.9 47.9 21.8 22.0 21.8 37.3 18.7 24.9 8.5 11.4 9.7 19.0 11.3 14.2
BN 47.2 47.2 47.2 52.8 52.8 52.8 21.8 33.2 26.3 31.5 28.1 29.7 14.6 26.3 18.8 18.2 19.5 18.8
BC 52.7 52.7 52.7 49.8 50.3 50.0 23.3 30.7 26.5 38.0 21.0 27.0 12.7 16.2 14.3 20.6 12.4 15.5
TC 51.2 51.2 51.2 45.2 46.7 46.0 43.1 28.2 34.1 42.4 20.3 27.4 33.2 17.1 22.5 32.2 13.3 18.8
Table 9: AZP resolution results of the best baseline and our unsupervised model on the test set.
AZP identification system. On the other hand,
since our supervised resolvers are meant to be re-
implementations of existing resolvers, we follow
previous work and let them employ a supervised
AZP identification system. In particular, we em-
ploy the one described in Chen and Ng (2013).
Several observations can be made about these
results. First, among the rule-based resolvers,
Baseline 4 achieves the best performance, outper-
forming Baselines 1, 2, and 3 by 12.9%, 1.5%,
and 10.8% in F-score respectively when averaged
over the three evaluation settings. From their
relative performance, which remains the same in
the three settings, we can conclude that as far as
AZP resolution is concerned, (1) salience plays a
greater role than recency; and (2) semantic com-
patibility is useful. Second, among the super-
vised baselines, our supervised resolver (Chen and
Ng, 2013) achieves the best performance, outper-
forming Zhao and Ng's resolver and Kong and
Zhou's resolver by 3.9% and 2.0% in F-score re-
spectively when averaged over the three evalua-
tion settings. Finally, comparing the rule-based
resolvers and the learning-based resolvers, we can
see that the best rule-based baseline (Baseline 4)
performs even better than Zhao and Ng's resolver
and Kong and Zhou's resolver.
In the rest of this subsection, we will compare
our unsupervised model against the best baseline,
Chen and Ng's (2013) supervised resolver.
Our model. Results of the best baseline and our
model on the entire test set and each of the six
sources are shown in Table 9. As we can see, our
model achieves the same overall F-score as the best
baseline under all three settings, despite the fact
that it is unsupervised. In fact, our model even out-
performs the best baseline on NW, WB and BN in
Setting 1, NW, WB, BN and BC in Setting 2, and
NW, WB and BC in Setting 3.
It is worth mentioning that while the two re-
solvers achieved the same overall performance,
their outputs differ a lot from each other. Specifi-
cally, the twomodels only agree on the antecedents
of 55% of the AZPs in Setting 1.10
7.3 Ablation Experiments
Impact of P (p
a
|c
a
, l=1) and P (l=1|k
c
). Re-
call that our model is composed of five probability
terms,P (p
a
|c
a
, l=1) for each of the four grammat-
ical attributes and P (l=1|k
c
), the context proba-
bility. To investigate the contribution of context
and each attribute to overall performance, we con-
duct ablation experiments. Specifically, in each
ablation experiment, we remove exactly one prob-
ability term from the model and retrain it.
10Note that it is difficult to directly compare the outputs
produced under Settings 2 and 3: the AZPs identified by the
best baseline are quite different from those identified by our
rule-based system, as can be inferred from the AZP identifi-
cation results in Table 12.
770
Setting 1 Setting 3
System R P F R P F
Full model 47.5 47.9 47.7 19.9 12.9 15.7
? Number 47.5 47.9 47.7 19.7 12.8 15.5
? Gender 44.5 45.0 44.7 19.2 12.5 15.1
? Person 45.2 45.6 45.4 19.1 12.4 15.1
? Animacy 45.1 45.5 45.3 19.1 12.4 15.1
? Context Features 32.9 33.1 33.0 15.2 9.8 11.9
Table 10: Probability term ablation results.
Ablation results under Settings 1 and 3 are
shown in Table 10. As we can see, under Set-
ting 1, afterNumber is ablated, performance does
not drop. We attribute this to the fact that al-
most all candidate antecedents are singular. On the
other hand, when we ablate any of the remaining
three attributes, performance drops significantly
by 2.3?3.0% in overall F-score.11 Similar trends
can be observed with respect to Setting 3: after
Number is ablated, performance only decreases
by 0.2%, while ablating any of the other three at-
tributes results in a drop of 0.6%.
Results after ablating context are shown in the
last row of Table 10. As we can see, the F-score
drops significantly by 14.7% and 3.8% under Set-
tings 1 and 3 respectively. These results illustrate
the importance of context features in our model.
Context feature ablation. Recall that we em-
ployed eight context features to encode the rela-
tionship between a pronoun and a candidate an-
tecedent. To determine the relative contribution
of these eight features to overall performance,
we conduct ablation experiments under Settings 1
and 3. In these ablation experiments, all four gram-
matical attributes are retained in the model.
Ablation results are shown in rows 2?9 of Ta-
ble 11. To facilitate comparison, the F-score of the
model in which all eight context features are used
is shown in row 1. As we can see, feature 8 (the
rule-based feature) is the most useful feature: its
removal causes the F-scores of our resolver to drop
significantly by 6.4% under Setting 1 and 1.5% un-
der Setting 3.
7.4 Error Analysis
To gain additional insights into our full model, we
examine its major sources of error below. To focus
on errors attributable to AZP resolution, we ana-
lyze our full model under Setting 1.
Specifically, we randomly select 100 AZPs that
our model incorrectly resolves under Setting 1.
11All significance tests are paired t-tests, with p < 0.05.
Setting 1 Setting 3
System R P F R P F
Full model 47.5 47.9 47.7 19.9 12.9 15.7
? Feature 1 46.1 46.5 46.3 19.4 12.6 15.3
? Feature 2 46.5 46.9 46.7 19.4 12.6 15.3
? Feature 3 45.3 45.7 45.5 19.1 12.4 15.1
? Feature 4 47.4 47.8 47.6 20.1 13.0 15.8
? Feature 5 47.4 47.8 47.6 19.7 12.8 15.5
? Feature 6 47.1 47.5 47.3 19.6 12.7 15.4
? Feature 7 47.1 47.5 47.3 20.1 13.0 15.8
? Feature 8 41.2 41.6 41.4 18.0 11.8 14.2
Table 11: Context feature ablation results.
We found that 17 errors are attributable to dis-
course disfluency, lack of background knowledge
and subject detection, while the remaining 83 er-
rors can be divided into three types:
Failure to recognize the topics of a document.
Our model incorrectly resolves 32 AZPs that are
coreferent with NPs corresponding to the topics of
the associated documents. Consider the following
example:
[???]????????????????
????*pro*?????????????
([Bali Town] is located in the Northwest of Taipei
Basin. Its administrative area is affiliated with
Taipei County, *pro* is one of Taipei County's 29
towns and cities.)12
The model incorrectly resolves the AZP *pro*
to??? (Its administrative area). The reason is
that the correct antecedent, ??? (Bali Town),
is far from *pro*: there are five candidate an-
tecedents between *pro* and??? (Bali Town).
Note, however, that it is easy for a human to re-
solve *pro* to ??? (Bali Town) because the
whole passage is discussing??? (Bali Town).
Hence, to correctly handle such cases, one may
construct a topic model over the passage and as-
sign each candidate antecedent a prior probability
so that the resulting system favors the selection of
candidates representing the topics as antecedents.
Errors in computing semantic compatibility.
This type of error contributes to 28 of the incor-
rectly resolved AZPs. When computing seman-
tic compatibility in our model, we only consider
the mutual information between a candidate an-
tecedent and the pronoun's governing verb, but in
some cases, additional context needs to be taken
into account. Consider the following example:
12The pronoun Its in the phrase Its administrative area is
inserted into the English translation for the sake of grammat-
icality and correct understanding of the sentence. The corre-
sponding Chinese phrase does not contain any pronoun.
771
[???????]???? [24??????
????]?*pro*??????????
([Marines] killed about [24 unarmed Iraqis], *pro*
include women and six children.)
There are two candidate antecedents in this ex-
ample,??????? (Marines) and 24???
??????? (24 unarmed Iraqis), which we
denote as c
1
and c
2
respectively. The correct an-
tecedent of *pro* is c
2
, while our model wrongly
resolves *pro* to c
1
. Note that both c
1
and c
2
are
compatible with the AZP's governing verb ??
(include). However, if the object of the govern-
ing verb, i.e., ??????? (women and six
children), were also considered, the model could
determine that c
1
is not compatible with the object
while c
2
is, and then correctly resolve *pro* to c
2
.
Failure to recognize and exploit semantically
similar sentences. This type of error contributes
to 23 wrongly resolved AZPs. Recall that an AZP
is omitted for brevity, so the sentence it appears in
often expresses similar meaning to an earlier sen-
tence. However, our model fails to handle such
cases. Consider the following example:
[?????????]?????????.....
*pro*???????
([The command and the onrush of troops] lost con-
nection with each other. ... *pro* cannot connect
with each other.)
The above example shows two sentences that
are separated by some other sentences. The AZP
under consideration is in the last sentence, while
the first sentence contains the correct antecedent
????????? (the command and the on-
rush troops), denoted as c
1
. Our model fails to re-
solve *pro* to c
1
, because there are many com-
peting candidate antecedents between c
1
and AZP.
However, if our model were aware of the similarity
between the constructions appearing after c
1
and
*pro*, i.e., ???????? (lost connection
with each other) and?????? (cannot con-
nect with each other), then it might be able to cor-
rectly resolve the AZP.
8 Conclusion
We proposed an unsupervised model for Chinese
zero pronoun resolution, investigating the novel
hypothesis that an unsupervised probabilistic re-
solver trained on overt pronouns can be applied to
resolve ZPs. To our knowledge, this is the first un-
supervised probabilistic model for this task. Ex-
periments on the OntoNotes 5.0 corpus showed
that our unsupervised model rivaled its state-of-
the-art supervised counterparts in performance.
Acknowledgments
We thank the three anonymous reviewers for their
detailed comments. This work was supported in
part by NSF Grants IIS-1147644 and IIS-1219142.
References
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 33--40.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 148--156.
Chen Chen and Vincent Ng. 2013. Chinese zero pro-
noun resolution: Some recent advances. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1360--1365.
Chen Chen and Vincent Ng. 2014. SinoCorefer-
encer: An end-to-end Chinese event coreference
resolver. In Proceedings of the 9th International
Conference on Language Resources and Evaluation,
pages 4532--4538.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution.
In Proceedings of the Ninth Conference on Natural
Language Learning, pages 88--95.
Susan Converse. 2006. Pronominal Anaphora Resolu-
tion in Chinese. Ph.D. thesis, University of Pennsyl-
vania.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39:1--38.
Antonio Ferr?ndez and Jes?s Peral. 2000. A compu-
tational approach to zero-pronouns in Spanish. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, pages 166--172.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203--226.
Na-Rae Han. 2006. Korean Zero Pronouns: Analysis
and Resolution. Ph.D. thesis, University of Pennsyl-
vania.
Jerry Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311--338.
772
Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ILP solution to zero anaphora resolution. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 804--813.
Ryu Iida, Kentaro Inui, andYujiMatsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
resolution. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 625--632.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007.
Zero-anaphora resolution by learning rich syntactic
pattern features. ACM Transactions on Asian Lan-
guage Information Processing, 6(4).
Kenji Imamura, Kuniko Saito, and Tomoko Izumi.
2009. Discriminative approach to predicate-
argument structure analysis with zero-anaphora res-
olution. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 85--88.
Hideki Isozaki and Tsutomu Hirao. 2003. Japanese
zero pronoun resolution based on ranking rules and
machine learning. In Proceedings of the 2003 Con-
ference on Empirical Methods in Natural Language
Processing, pages 184--191.
Andrew Kehler, Douglas Appelt, Lara Taylor, and
Aleksandr Simma. 2004. The (non)utility of
predicate-argument frequencies for pronoun inter-
pretation. In Proceedings of 2004 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 289--296.
Fang Kong and GuoDong Zhou. 2010. A tree kernel-
based unified framework for Chinese zero anaphora
resolution. In Proceedings of the 2010 Conference
on EmpiricalMethods in Natural Language Process-
ing, pages 882--891.
Robert Parker, David Graff, Ke Chen, Junbo Kong, and
Kazuaki Maeda. 2009. Chinese Gigaword fourth
edition. Linguistic Data Consortium. Philadelphia,
PA.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings of
2012 Joint Conference on EmpiricalMethods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning: Shared Task, pages 1--40.
Xian Qian, Qi Zhang, Xuangjing Huang, and Lide Wu.
2010. 2d trie for fast parsing. In Proceedings of
the 23rd International Conference on Computational
Linguistics, pages 904--912.
Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to Japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of the 5th International Joint Confer-
ence on Natural Language Processing, pages 758--
766.
Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa.
2002. A probabilistic method for analyzing Japanese
anaphora integrating zero pronoun detection and res-
olution. In Proceedings of the 19th International
Conference on Computational Linguistics.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 165--172.
Ching-Long Yeh and Yi-Chun Chen. 2007. Zero
anaphora resolution in Chinese with shallow pars-
ing. Journal of Chinese Language and Computing,
17(1):41--56.
Shanheng Zhao and Hwee Tou Ng. 2007. Identifica-
tion and resolution of Chinese zero pronouns: A ma-
chine learning approach. In Proceedings of the 2007
Joint Conference on Empirical Methods on Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 541--550.
GuoDong Zhou, Fang Kong, and Qiaoming Zhu. 2008.
Context-sensitive convolution tree kernel for pro-
noun resolution. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 25--31.
Appendix: Automatic AZP Identification
Our automatic AZP identification system employs
an ordered set of rules. The first rule is a positive
rule that aims to extract as many candidate AZPs
as possible. It is followed by seven negative rules
that aim to improve precision by filtering out er-
roneous candidate AZPs. Below we first describe
the rules and then evaluate this rule-based system.
Rule 1. Add candidate AZP z if it occurs before
the leftmost word spanned by a VP node vp.
Rule 2. Remove z if its associated vp is in a coor-
dinate structure or modified by an adverbial node.
Rule 3. Remove z if the parent of its associated
vp node is not an IP node.
Rule 4. Remove z if its associated vp has a NP
or QP node as an ancestor.
Rule 5. Remove z if one of the left sibling nodes
of vp is NP, QP, IP or ICP.
Rule 6. Remove z if (1) z does not begin a sen-
tence, (2) the highest node whose spanning word
sequence ends with the left non-comma neighbor
word of z is either NP, QP or IP, and (3) the parent
of this node is VP.
773
Gold Parses System Parses
Systems R P F R P F
Rule-based 72.4 42.3 53.4 42.3 26.8 32.8
Supervised 50.6 55.1 52.8 30.8 34.4 32.5
Table 12: AZP identification results on the test set.
Rule 7. Remove z if vp's lowest IP ancestor has
(1) a VP node as its parent and (2) a VV node as
its left sibling.
Rule 8. Remove z if it begins a document.
To gauge the performance of our rule-based
AZP identification system, we compare it with our
supervised AZP identification system (Chen and
Ng, 2013). Results of the two systems on our test
set are shown in Table 12. As we can see, the F-
scores achieved by the rule-based system is com-
parable to those of the supervised system.
774
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 831?843,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Relieving the Computational Bottleneck: Joint Inference for
Event Extraction with High-Dimensional Features
Deepak Venugopal and Chen Chen and Vibhav Gogate and Vincent Ng
Department of Computer Science and Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
dxv021000@utdallas.edu, {yzcchen,vgogate,vince}@hlt.utdallas.edu
Abstract
Several state-of-the-art event extraction sys-
tems employ models based on Support Vec-
tor Machines (SVMs) in a pipeline architec-
ture, which fails to exploit the joint depen-
dencies that typically exist among events
and arguments. While there have been at-
tempts to overcome this limitation using
Markov Logic Networks (MLNs), it re-
mains challenging to perform joint infer-
ence in MLNs when the model encodes
many high-dimensional sophisticated fea-
tures such as those essential for event ex-
traction. In this paper, we propose a new
model for event extraction that combines
the power of MLNs and SVMs, dwarfing
their limitations. The key idea is to reli-
ably learn and process high-dimensional
features using SVMs; encode the output
of SVMs as low-dimensional, soft formu-
las in MLNs; and use the superior joint in-
ferencing power of MLNs to enforce joint
consistency constraints over the soft for-
mulas. We evaluate our approach for the
task of extracting biomedical events on
the BioNLP 2013, 2011 and 2009 Genia
shared task datasets. Our approach yields
the best F1 score to date on the BioNLP?13
(53.61) and BioNLP?11 (58.07) datasets
and the second-best F1 score to date on the
BioNLP?09 dataset (58.16).
1 Introduction
Event extraction is the task of extracting and la-
beling all instances in a text document that corre-
spond to a pre-defined event type. This task is quite
challenging for a multitude of reasons: events are
often nested, recursive and have several arguments;
there is no clear distinction between arguments and
events; etc. For instance, consider the BioNLP Ge-
nia event extraction shared task (N?edellec et al.,
2013). In this task, participants are asked to extract
instances of a pre-defined set of biomedical events
from text. An event is identified by a keyword
called the trigger and can have an arbitrary number
of arguments that correspond to pre-defined argu-
ment types. The task is complicated by the fact
that an event may serve as an argument of another
event (nested events). An example of the task is
shown in Figure 1. As we can see, event E13 takes
as arguments two events, E14 and E12, which in
turn has E11 as one of its arguments.
A standard method that has been frequently em-
ployed to perform this shared task uses a pipeline
architecture with three steps: (1) detect if a token
is a trigger and assign a trigger type label to it; (2)
for every detected trigger, determine all its argu-
ments and assign types to each detected argument;
and (3) combine the extracted triggers and argu-
ments to obtain events. Though adopted by the
top-performing systems such as the highest scoring
system on the BioNLP?13 Genia shared task (Kim
et al., 2013), this approach is problematic for at
least two reasons. First, as is typical in pipeline
architectures, errors may propagate from one stage
to the next. Second, since each event/argument is
identified and assigned a type independently of the
others, it fails to capture the relationship between
a trigger and its neighboring triggers, an argument
and its neighboring arguments, etc.
More recently, researchers have investigated
joint inference techniques for event extraction us-
ing Markov Logic Networks (MLNs) (e.g., Poon
and Domingos (2007), Poon and Vanderwende
(2010), Riedel and McCallum (2011a)), a statis-
tical relational model that enables us to model the
dependencies between different instances of a data
sample. However, it is extremely challenging to
make joint inference using MLNs work well in
practice (Poon and Domingos, 2007). One reason
is that it is generally difficult to model sophisti-
cated linguistic features using MLNs. The diffi-
831
. . . demonstrated that HOIL-1L interacting protein (HOIP), a ubiquitin ligase that can catalyze the assembly of linear
polyubiquitin chains, is recruited to DC40 in a TRAF2-dependent manner following engagement of CD40 . . .
(a) Sentence fragment
ID Event Type Trigger Arguments
E11 Binding recruited Theme={HOIL-1L interacting protein,CD40}
E12 Regulation dependent Theme=E11, Cause=TRAF2
E13 +ve Regulation following Theme=E12, Cause=E14
E14 Binding engagement Theme=CD40
(b) Events
Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events
extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14.
culty stems from the fact that some of these fea-
tures are extremely high dimensional (e.g., Chen
and Ng (2012), Huang and Riloff (2012b), Li et al.
(2012), Li et al. (2013b), Li et al. (2013c)), and to
reliably learn weights of formulas that encode such
features, one would require an enormous number
of data samples. Moreover, even the complexity of
approximate inference on such models is quite high,
often prohibitively so. For example, a trigram can
be encoded as an MLN formula, Word(w
1
, p?1)?
Word(w
2
, p) ? Word(w
3
, p + 1)? Type(p, T ).
For any given position (p), this formula has W
3
groundings, where W is the number of possible
words, making it too large for learning/inference.
Therefore, current MLN-based systems tend to in-
clude a highly simplified model ignoring powerful
linguistic features. This is problematic because
such features are essential for event extraction.
Our contributions in this paper are two-fold.
First, we propose a novel model for biomedical
event extraction based on MLNs that addresses the
aforementioned limitations by leveraging the power
of Support Vector Machines (SVMs) (Vapnik,
1995; Joachims, 1999) to handle high-dimensional
features. Specifically, we (1) learn SVM models us-
ing rich linguistic features for trigger and argument
detection and type labeling; (2) design an MLN
composed of soft formulas (each of which encodes
a soft constraint whose associated weight indicates
how important it is to satisfy the constraint) and
hard formulas (constraints that always need to be
satisfied, thus having a weight of ?) to capture
the relational dependencies between triggers and
arguments; and (3) encode the SVM output as prior
knowledge in the MLN in the form of soft formulas,
whose weights are computed using the confidence
values generated by the SVMs. This formulation
naturally allows SVMs and MLNs to complement
each other?s strengths and weaknesses: learning
in a large and sparse feature space is much easier
with SVMs than with MLNs, whereas modeling
relational dependencies is much easier with MLNs
than with SVMs.
Our second contribution concerns making infer-
ence with this MLN feasible. Recall that inference
involves detecting and assigning the type label to
all the triggers and arguments. We show that exist-
ing Maximum-a-posteriori (MAP) inference meth-
ods, even the most advanced approximate ones
(e.g., Selman et al. (1996), Marinescu and Dechter
(2009), Sontag and Globerson (2011) ), are infea-
sible on our proposed MLN because of their high
memory cost. Consequently, we identify decompo-
sitions of the MLN into disconnected components
and solve each independently, thereby drastically
reducing the memory requirements.
We evaluate our approach on the BioNLP 2009,
2011 and 2013 Genia shared task datasets. On
the BioNLP?13 dataset, our model significantly
outperforms state-of-the-art pipeline approaches
and achieves the best F1 score to date. On the
BioNLP?11 and BioNLP?09 datasets, our scores
are slightly better and slightly worse respectively
than the best reported results. However, they
are significantly better than state-of-the-art MLN-
based systems.
2 Background
2.1 Related Work
As a core task in information extraction, event ex-
traction has received significant attention in the nat-
ural language processing (NLP) community. The
development and evaluation of large-scale learning-
based event extraction systems was propelled in
part by the availability of annotated corpora pro-
duced as part of the Message Understanding Con-
ferences (MUCs), the Automatic Content Extrac-
tion (ACE) evaluations, and the BioNLP shared
832
tasks on event extraction. Previous work on event
extraction can be broadly divided into two cate-
gories, one focusing on the development of fea-
tures (henceforth feature-based approaches) and
the other focusing on the development of models
(henceforth model-based approaches).
Feature-based approaches. Early work on
feature-based approaches has primarily focused
on designing local sentence-level features such as
token and syntactic features (Grishman et al., 2005;
Ahn, 2006). Later, it was realized that local features
were insufficient to reliably and accurately perform
event extraction in complex domains and therefore
several researchers proposed using high-level fea-
tures. For instance, Ji and Grishman (2008) used
global information from related documents; Gupta
and Ji (2009) extracted implicit time information;
Patwardhan and Riloff (2009) used broader sen-
tential context; Liao and Grishman (2010; 2011)
leveraged document-level cross-event information
and topic-based features; and Huang and Riloff
(2012b) explored discourse properties.
Model-based approaches. The model-based ap-
proaches developed to date have focused on mod-
eling global properties and seldom use rich, high-
dimensional features. To capture global event struc-
ture properties, McClosky et al. (2011a) proposed
a dependency parsing model. To extract event ar-
guments, Li et al. (2013b) proposed an Integer
Linear Programming (ILP) model to encode the
relationship between event mentions. To overcome
the error propagation problem associated with the
pipeline architecture, several joint models have
been proposed, including those that are based on
MLNs (e.g., Poon and Domingos (2007), Riedel et
al. (2009), Poon and Vanderwende (2010)), struc-
tured perceptrons (e.g., Li et al. (2013c)), and dual
decomposition with minimal domain adaptation
(e.g., Riedel and McCallum (2011a; 2011b)).
In light of the high annotation cost required by
supervised learning-based event extraction systems,
several semi-supervised, unsupervised, and rule-
based systems have been proposed. For instance,
Huang and Riloff (2012a) proposed a bootstrap-
ping method to extract event arguments using only
a small amount of annotated data; Lu and Roth
(2012) developed a novel unsupervised sequence
labeling model; Bui et al. (2013) implemented a
rule-based approach to extract biomedical events;
and Ritter et al. (2012) used unsupervised learning
to extract events from Twitter data.
Our work extends prior work by developing a
rich framework that leverages sophisticated feature-
based approaches as well as joint inference using
MLNs. This combination gives us the best of both
worlds because on one hand, it is challenging to
model sophisticated linguistic features using MLNs
while on the other hand, feature-based approaches
employing sophisticated high-dimensional features
suffer from error propagation as the model is gen-
erally not rich enough for joint inference.
2.2 The Genia Event Extraction Task
The BioNLP Shared Task (BioNLP-ST) series
(Kim et al. (2009), Kim et al. (2011a) and N?edellec
et al. (2013)) is designed to tackle the problem of
extracting structured information from the biomedi-
cal literature. The Genia Event Extraction task is ar-
guably the most important of all the tasks proposed
in BioNLP-ST and is also the only task organized
in all three events in the series.
The 2009 edition of the Genia task (Kim et
al., 2009) was conducted on the Genia event
corpus (Kim et al., 2008), which only contains
abstracts of the articles that represent domain
knowledge around NF?B proteins. The 2011 edi-
tion (Kim et al., 2011b) augmented the dataset to
include full text articles, resulting in two collec-
tions, the abstract collection and the full text col-
lection. The 2013 edition (Kim et al., 2013) further
augmented the dataset with recent full text articles
but removed the abstract collection entirely.
The targeted event types have also changed
slightly over the years. Both the 2009 and 2011
editions are concerned with nine fine-grained event
sub-types that can be categorized into three main
types, namely simple, binding and regulation
events. These three main event types can be dis-
tinguished by the kinds of arguments they take. A
simple event can take exactly one protein as its
Theme argument. A binding event can take one
or more proteins as its Theme arguments, and is
therefore slightly more difficult to extract than a
simple event. A regulation event takes exactly one
protein or event as its Theme argument and option-
ally one protein or event as its Cause argument. If
a regulation event takes another event as its Theme
or Cause argument, it will lead to a nested event.
Regulation events are considered the most difficult-
to-extract among the three event types owing in part
to the presence of an optional Cause argument and
their recursive structure. The 2013 edition intro-
833
duced a new event type, protein-mod, and its three
sub-types. Theoretically, a protein-mod event takes
exactly one protein as its Theme argument and
optionally one protein or event as its Cause argu-
ment. In practice, however, it rarely occurs: there
are only six protein-mod events having Cause ar-
guments in the training data for the 2013 edition.
Consequently, our model makes the simplifying
assumption that a protein-mod event can only take
one Theme argument, meaning that we are effec-
tively processing protein-mod events in the same
way as simple events.
2.3 Markov Logic Networks
Statistical relational learning (SRL) (Getoor and
Taskar, 2007) is an emerging field that seeks to
unify logic and probability, and since most NLP
techniques are grounded either in logic or proba-
bility or both, NLP serves as an ideal application
domain for SRL. In this paper, we will employ a
popular SRL approach called Markov logic net-
works (MLNs) (Domingos and Lowd, 2009). At a
high level, an MLN is a set of weighted first-order
logic formulas (f
i
, w
i
), where w
i
is the weight
associated with formula f
i
. Given a set of con-
stants that model objects in the domain, it defines a
Markov network or a log-linear model (Koller and
Friedman, 2009) in which we have one node per
ground first-order atom and a propositional feature
corresponding to each grounding of each first-order
formula. The weight of the feature is the weight of
the corresponding first-order formula.
Formally, the probability of a world ?, which
represents an assignment of values to all ground
atoms in the Markov network, is given by:
Pr(?) =
1
Z
exp
(
?
i
w
i
N(f
i
, ?)
)
where N(f
i
, ?) is the number of groundings of f
i
that evaluate to True in ? and Z is a normalization
constant called the partition function.
The key inference tasks over MLNs are com-
puting the partition function (Z) and the most-
probable explanation given evidence (the MAP
task). Most queries, including those required by
event extraction, can be reduced to these inference
tasks. Formally, the partition function and the MAP
tasks are given by:
Z =
?
?
exp
(
?
i
w
i
N(f
i
, ?)
)
(1)
arg max
?
P (?) = arg max
?
?
i
w
i
N(f
i
, ?) (2)
3 Pipeline Model
We implement a pipeline event extraction system
using SVMs. This pipeline model serves two im-
portant functions: (1) providing a baseline for eval-
uation and (2) producing prior knowledge for the
joint model.
Our pipeline model consists of two steps: trig-
ger labeling and argument labeling. In the trigger
labeling step, we determine whether a candidate
trigger is a true trigger and label each true trigger
with its trigger type. Then, in the argument label-
ing step, we identify the arguments for each true
trigger discovered in the trigger labeling step and
assign a role to each argument.
We recast each of the two steps as a classification
task and employ SVM
multiclass
(Tsochantaridis
et al., 2004) to train the two classifiers. We describe
each step in detail below.
3.1 Trigger Labeling
A preliminary study of the BioNLP?13 training
data suggests that 98.7% of the true triggers? head
words
1
are either verbs, nouns or adjectives. There-
fore, we consider only those words whose part-of-
speech tags belong to the above three categories
as candidate triggers. To train the trigger classifier,
we create one training instance for each candidate
trigger in the training data. If the candidate trigger
is not a trigger, the class label of the corresponding
instance is None; otherwise, the label is the type
of the trigger. Thus, the number of class labels
equals the number of trigger types plus one. Each
training instance is represented by the features de-
scribed in Table 1(a). These features closely mirror
those used in state-of-the-art trigger labeling sys-
tems such as Miwa et al. (2010b) and Bj?orne and
Salakoski (2013).
After training, we apply the resulting trigger clas-
sifier to classify the test instances, which are cre-
ated in the same way as the training instances. If a
test instance is predicted as None by the classifier,
the corresponding candidate trigger is labeled as
a non-trigger; otherwise, the corresponding candi-
date trigger is posited as a true trigger whose type
is the class value assigned by the classifier.
1
Head words are found using Collins? (1999) rules.
834
(a) Features for trigger labeling
Token features The basic token features (see Table 1(c)) computed from (1) the candidate trigger word and (2) the
surrounding tokens in a window of two; character bigrams and trigrams of the candidate trigger word;
word n-grams (n=1,2,3) of the candidate trigger word and its context words in a window of three; whether
the candidate trigger word contains a digit; whether the candidate trigger word contains an upper case
letter; whether the candidate trigger word contains a symbol.
Dependency
features
The basic dependency path features (see Table 1(c)) computed using the shortest paths from the candidate
trigger to (1) the nearest protein word, (2) the nearest protein word to its left, and (3) the nearest protein
word to its right.
Other
features
The distances from the candidate trigger word to (1) the nearest protein word, (2) the nearest protein
word to its left, and (3) the nearest protein word to its right; the number of protein words in the sentence.
(b) Features for argument labeling
Token features Word n-grams (n=1,2,3) of (1) the candidate trigger word and its context in a window of three and (2) the
candidate argument word and its context in a window of three; the basic token features (see Table 1(c))
computed from (1) the candidate trigger word and (2) the candidate argument word; the trigger type of
the candidate trigger word.
Dependency
features
The basic dependency features (see Table 1(c)) computed using the shortest path from the candidate
trigger word to the candidate argument word.
Other
features
The distance between the candidate trigger word and the candidate argument word; the number of
proteins between the candidate trigger word and the candidate argument word; the concatenation of the
candidate trigger word and the candidate argument word; the concatenation of the candidate trigger type
and the candidate argument word.
(c) Basic token and dependency features
Basic token fea-
tures
Six features are computed given a token t, including: (a) the lexical string of t, (b) the lemma of t, (c) the
stem of t obtained using the Porter stemmer (Porter, 1980), (d) the part-of-speech tag of t, (e) whether t
appears as a true trigger in the training data, and (f) whether t is a protein name.
Basic
dependency
features
Six features are computed given a dependency path p, including: (a) the vertex walk in p, (b) the edge
walk in p, (c) the n-grams (n=2,3,4) of the (stemmed) words associated with the vertices in p, (d) the
n-grams (n=2,3,4) of the part-of-speech tags of the words associated with the vertices in p, (e) the
n-grams (n=2,3,4) of the dependency types associated with the edges in p, and (f) the length of p.
Table 1: Features for trigger labeling and argument labeling.
3.2 Argument Labeling
The argument classifier is trained as follows. Each
training instance corresponds to a candidate trigger
and one of its candidate arguments.
2
A candidate
argument for a candidate trigger ct is either a pro-
tein or a candidate trigger that appears in the same
sentence as ct. If ct is not a true trigger, the label of
the associated instance is set toNone. On the other
hand, if ct is a true trigger, we check whether the
candidate argument in the associated instance is in-
deed one of ct?s arguments. If so, the class label of
the instance is the argument?s role; otherwise, the
class label is None. The features used for repre-
senting each training instance, which are modeled
after those used in Miwa et al. (2010b) and Bj?orne
and Salakoski (2013), are shown in Table 1(b).
After training, we can apply the resulting clas-
sifier to classify the test instances, which are cre-
ated in the same way as the training instances. If
a test instance is assigned the class None by the
classifier, the corresponding candidate argument is
classified as not an argument of the trigger. Other-
2
Following the definition of the GENIA event extraction
task, the protein names are provided as part of the input.
wise, the candidate argument is a true argument of
the trigger whose role is the class value assigned
by the classifier.
4 Joint Model
In this section, we describe our Markov logic model
that encodes the relational dependencies in the
shared task and uses the output of the pipeline
model as prior knowledge (soft evidence). We be-
gin by describing the structure of our Markov logic
model, and then describe the parameter learning
and inference algorithms for it.
4.1 MLN Structure
Figure 2 shows our proposed MLN for BioNLP
event extraction, which we refer to as BioMLN.
The MLN contains six predicates.
The query predicates in Figure 2(a) are those
whose assignments are not given during infer-
ence and thus need to be predicted. Predicate
TriggerType(sid,tid,ttype!) is true when the
token located in sentence sid at position tid has
type ttype. ?
ttype
, which denotes the set of con-
stants (or objects) that the logical variable ttype
835
TriggerType(sid,tid,ttype!)
ArgumentRole(sid,aid,tid,arole!)
(a) Query
Simple(sid,tid)
Regulation(sid,tid)
(b) Hidden
Word(sid,tid,word)
DepType(sid,aid,tid,dtype)
(c) Evidence
1. ?t TriggerType(i,j,t).
2. ?a ArgumentRole(i,k,j,a).
3. ?TriggerType(i,j,None) ? ?k ArgumentRole(i,k,j,Theme).
4. Simple(i,j) ?? ?k ArgumentRole(i,k,j,Cause).
5. TriggerType(i,j,None) ? ArgumentRole(i,k,j,None).
6. ?ArgumentRole(i,k,j,None) ??TriggerType(i,k,None) ? Regulation(i,j).
7. Simple(i,j)? TriggerType(i,j,Simple1) ? . . .? TriggerType(i,j,Binding).
8. Regulation(i,j) ? TriggerType(i,j,Reg) ? TriggerType(i,j,PosReg)
? TriggerType(i,j,NegReg).
9. Word(i,j,+w) ? TriggerType(i,j,+t) ? DepType(i,k,j,+d) ? ArgumentRole(i,k,j,+a)
(d) Joint Formulas
Figure 2: The BioMLN structure.
can be instantiated to, includes all possible trigger
types in the dataset plus None (which indicates
that the token is not a trigger). The ?!? symbol mod-
els commonsense knowledge that only one of the
types in the domain ?
ttype
of ttype is true for every
unique combination of sid and tid. Similarly, pred-
icate ArgumentRole(sid,aid,tid,arole!) as-
serts that a token in sentence sid at position aid
plays exactly one argument role, denoted by arole,
with respect to the token at position tid. ?
arole
includes the two argument types, namely, Theme
and Cause plus the additional None that indicates
that the token is not an argument.
The hidden predicates in Figure 2(b) are ?clus-
ters? of trigger types. Predicate Simple(sid,tid)
is true when the token in sentence sid at posi-
tion tid corresponds to one of the Simple event
trigger types (BioNLP?13 has 9 simple events,
BioNLP?09/?11 have 5) or a binding event trig-
ger type. Similarly, Regulation(sid,tid) asserts
that the token in sentence sid at position tid corre-
sponds to any of the three regulation event trigger
types.
The evidence predicates in Figure 2(c) are those
that are always assumed to be known during in-
ference. We define two evidence predicates based
on dependency structures. Word(sid,tid,word) is
true when the word in sentence sid at position tid
is equal to word. DepType(sid,aid,tid,dtype)
asserts that dtype is the dependency type in the de-
pendency parse tree that connects the token at posi-
tion tid to the token at position aid in sentence sid.
If the word at tid and the word at aid are directly
connected in the dependency tree, then dtype is the
label of dependency edge with direction; otherwise
dtype is None.
The MLN formulas, expressing commonsense,
prior knowledge in the domain (Poon and Van-
derwende, 2010; Riedel and McCallum, 2011a),
are shown in Fig. 2(d). All formulas, except For-
mula (9), are hard formulas, meaning that they have
infinite weights. Note that during weight learning,
we only learn the weights of soft formulas.
Formulas (1) and (2) along with the ?!? con-
straint in the predicate definition ensure that the
token types are mutually exclusive and exhaustive.
Formula (3) asserts that every trigger should have
an argument of type Theme, since a Theme argu-
ment is mandatory for any event. Formula (4) mod-
els the constraint that a Simple orBinding trigger
has no arguments of type Cause since only regu-
lation events have a Cause. Formula (5) asserts
that non-triggers have no arguments and vice-versa.
Formula (6) models the constraint that if a token
is both an argument of t and a trigger by itself,
then t must belong to one of the three regulation
trigger types. This formula captures the recursive
relationship between triggers. Formulas (7) and
(8) connect the hidden predicates with the query
predicates. Formula (9) is a soft formula encoding
836
the relationship between triggers and arguments in
a dependency parse tree. It joins a word and the
dependency type label that connects the word token
to the argument token in the dependency parse tree
with the trigger types and argument types of the
two tokens. The ?+? symbol indicates that each
grounding of Formula (9) may have a different
weight.
4.2 Weight Learning
We can learn BioMLN from data either discrimina-
tively or generatively. Since discriminative learning
is much faster than generative learning, we use the
former. In discriminative training, we maximize
the conditional log-likelihood (CLL) of the query
and the hidden variables given an assignment to
the evidence variables. In principle, we can use the
standard gradient descent algorithm for maximiz-
ing the CLL. In each iteration of gradient descent,
we update the weights using the following equation
(cf. Singla and Domingos (2005) and Domingos
and Lowd (2009)):
w
t+1
j
= w
t
j
? ?(E
w
(n
j
)? n
j
) (3)
where w
t
j
represents the weight of the j
th
formula
in the t
th
iteration, n
j
is the number of groundings
in which the j
th
formula is satisfied in the training
data, E
w
(n
j
) is the expected number of ground-
ings in which the j
th
formula is satisfied given the
current weight vector w, and ? is the learning rate.
As such, the update rule given in Equation (3)
is likely to yield poor accuracy because the num-
ber of training examples of some types (e.g.,
None) far outnumber other types. To rectify this
ill-conditioning problem (Singla and Domingos,
2005; Lowd and Domingos, 2007), we divide the
gradient with the number of true groundings in
the data, namely, we compute the gradient using
(E
w
(n
j
)?n
j
)
n
j
.
Another key issue with using Equation (3) is that
computing E
w
(n
j
) requires performing inference
over the MLN. This step is intractable, #P-complete
in the worst case. To circumvent this problem and
for fast, scalable training, we instead propose to
use the voted perceptron algorithm (Collins, 2002;
Singla and Domingos, 2005). This algorithm ap-
proximates E
w
(n
j
) by counting the number of
satisfied groundings of each formula in the MAP
assignment. Computing the MAP assignment is
much easier (although still NP-hard in the worst
case) than computing E
w
(n
j
), and as a result the
voted perceptron algorithm is more scalable than
the standard gradient descent algorithm. In addi-
tion, it converges much faster.
4.3 Testing
In the testing phase, we combine BioMLN with the
output of the pipeline model (see Section 3) to ob-
tain a new MLN, which we refer to as BioMLN
+
.
For every candidate trigger, the SVM trigger clas-
sifier outputs a vector of signed confidence val-
ues (which is proportional to the distance from
the separating hyperplane) of dimension ?
ttype
with one entry for each trigger type. Similarly,
for every candidate argument, the SVM argu-
ment classifier outputs a vector of signed confi-
dence values of dimension ?
arole
with one en-
try for each argument role. In BioMLN
+
, we
model the SVM output as soft evidence, using
two soft unit clauses, TriggerType(i,+j,+t) and
ArgumentRole(i,+k,+j,+a). We use the con-
fidence values to determine the weights of these
clauses. Intuitively, higher (smaller) the confidence,
higher (smaller) the weight.
Specifically, the weights of the soft unit clauses
are set as follows. If the SVM trigger classifier
determines that the trigger in sentence i at po-
sition j belongs to type t with confidence C
i,j
,
then we attach a weight of
C
i,j
?n
i
to the clause
TriggerType(i,j,t). Here, n
i
denotes the num-
ber of trigger candidates in sentence i. Similarly,
if the SVM argument classifier determines that the
token at position k in sentence i belongs to the ar-
gument role a with respect to the token at position
j, with confidence C
?
i,k,j
, then we attach a weight
of
C
?
i,k,j
?
?
n
i
j=1
m
ij
to the clause ArgumentRole(i, k,
j,a). Here, m
ij
denotes the number of argument
candidates for the j
th
trigger candidate in sentence
i. ? and ? act as scale parameters for the confi-
dence values ensuring that the weights don?t get
too large (or too small).
4.4 Inference
As we need to perform MAP inference, both at
training time and at test time, in this subsection we
will describe how to do it efficiently by exploiting
unique properties of our proposed BioMLN.
Naively, we can perform MAP inference by
grounding BioMLN to a Markov network and
then reducing the Markov network by removing
from it all (grounded propositional) formulas that
are inconsistent with the evidence. On the re-
837
duced Markov network, we can then compute the
MAP solution using standard MAP solvers such as
MaxWalkSAT (a state-of-the-art local search based
MAP solver) (Selman et al., 1996) and Gurobi
3
(a
state-of-the-art, parallelized ILP solver).
The problem with the above approach is that
grounding the MLN is infeasible in practice; even
the reduced Markov network is just too large. For
example, assuming a total of |?
sid
| sentences and
a maximum of N tokens in a sentence, Formula (3)
alone has O(|?
sid
|N
3
) groundings. Concretely, at
training time, assuming 1000 sentences with 10
tokens per sentence, Formula (3) itself yields one
million groundings. Clearly, this approach is not
scalable. It turns out, however, that the (ground)
Markov network can be decomposed into several
disconnected components, each of which can be
solved independently. This greatly reduces the
memory requirement of the inference step. Specif-
ically, for every grounding of sid, we get a set of
nodes in the Markov network that are disconnected
from the rest of the Markov network and therefore
independent of the rest of the network. Formally,
Proposition 1. For any world ? of the BioMLN,
P
M
(?) = P
M
i
(?
i
)P
M\M
i
(? \ ?
i
) (4)
where ?
i
is the world ? projected on the ground-
ings of sentence i andM
i
is BioMLN grounded
only using sentence i.
Using Equation (4), it is easy to see that the MLN
M can be decomposed into |?
sid
| disjoint MLNs,
{M
k
}
|?
sid
|
k=1
. The MAP assignment toM can be
computed using,
|?
sid
|
?
i=1
(
arg max
?
i
P
M
i
(?
i
)
)
. This
result ensures that to approximate the expected
counts E
w
(n
j
), it is sufficient to keep exactly one
sentence?s groundings in memory. Specifically,
E
w
(n
j
) can be written as
?
|?
sid
|
k=1
E
w
(n
k
j
), where
E
w
(n
k
j
) indicates the expected number of satisfied
groundings of the j
th
formula in the k
th
sentence.
Since the MAP computation is decomposable, we
can estimate E
w
(n
k
j
) using MAP inference on just
the k
th
sentence.
5 Evaluation
5.1 Experimental Setup
We evaluate our system on the BioNLP?13 (Kim
et al., 2013), ?11 (Kim et al., 2011a) and ?09 (Kim
3
http://www.gurobi.com/
Dataset #Papers #Abstracts #TT #Events
BioNLP?13 (10,10,14) (0,0,0) 13 (2817,3199,3348)
BioNLP?11 (5,5,4) (800,150,260) 9 (10310,4690,5301)
BioNLP?09 (0,0,0) (800,150,260) 9 (8597,1809,3182)
Table 2: Statistics on the BioNLP datasets, which
consist of annotated papers/abstracts from PubMed.
(x, y, z): x in training, y in development and z
in test. #TT indicates the total number of trigger
types. The total number of argument types is 2.
et al., 2009) Genia datasets for the main event ex-
traction shared task. Note that this task is the most
important one for Genia and therefore has the most
active participation. Statistics on the datasets are
shown in Table 2. All our evaluations use the on-
line tool provided by the shared task organizers.
We report scores obtained using the approximate
span, recursive evaluation.
To generate features, we employ the support-
ing resources provided by the organizers. Specif-
ically, sentence split and tokenization are done
using the GENIA tools, while part-of-speech in-
formation is provided by the BLLIP parser that
uses the self-trained biomedical model (McClosky,
2010). Also, we create dependency features from
the parse trees provided by two dependency parsers,
the Enju parser (Miyao and Tsujii, 2008) and the
aforementioned BLLIP parser that uses the self-
trained biomedical model, which results in two sets
of dependency features.
For MAP inference, we use Gurobi, a par-
allelized ILP solver. After inference, a post-
processing step is required to generate biomedi-
cal events from the extracted triggers and argu-
ments. Specifically, for binding events, we em-
ploy a learning-based method similar to Bj?orne and
Salakoski (2011), while for the other events, we
employ a rule-based approach similar to Bj?orne et
al. (2009). Both the SVM baseline system and the
combined MLN+SVM system employ the same
post-processing strategy.
During weight learning, in order to combat the
problem of different initializations yielding radi-
cally different parameter estimates, we start at sev-
eral different initialization points and average the
weights obtained after 100 iterations of gradient
descent. However, we noticed that if we simply
choose random initialization points, the variance of
the weights was quite high and some initialization
points were much worse than others. To counter
this, we use the following method to systematically
838
System Rec. Prec. F1
Our System 48.95 59.24 53.61
EVEX (Hakala et al., 2013) 45.44 58.03 50.97
TEES-2.1 (Bj?orne and Salakoski, 2013) 46.17 56.32 50.74
BIOSEM (Bui et al., 2013) 42.47 62.83 50.68
NCBI (Liu et al., 2013) 40.53 61.72 48.93
DLUTNLP (Li et al., 2013a) 40.81 57.00 47.56
Table 3: Recall (Rec.), Precision (Prec.) and F1
score on the BioNLP?13 test data.
initialize the weights. Let n
i
be the number of sat-
isfied groundings of formula f
i
in the training data
and m
i
be the total number of possible groundings
of f
i
. We use a threshold ? to determine whether
we wish to make the initial weight positive or neg-
ative. If
n
i
m
i
? ?, then we choose the initial weight
uniformly at random from the range [?0.1, 0]. Oth-
erwise, we chose it from the range [0, 0.1]. These
steps ensure that the weights generated from dif-
ferent initialization points have smaller variance.
Also, in the testing phase, we set the scale parame-
ters for the soft evidence as ? = ? = max
c?C
|c|, where
C is the set of SVM confidence values.
5.2 Results on the BioNLP?13 Dataset
Among the three datasets, the BioNLP?13 dataset
is most ?realistic? one because it is the only one
that contains full papers and no abstracts. As a re-
sult, it is also the most challenging dataset among
the three. Table 3 shows the results of our system
along with the results of other top systems pub-
lished in the official evaluation of BioNLP?13. Our
system achieves the best F1-score (an improvement
of 2.64 points over the top-performing system) and
has a much higher recall (mainly because our sys-
tem detects more regulation events which outnum-
ber other event types in the dataset) and a slightly
higher precision than the winning system. Of the
top five teams, NCBI is the only other joint infer-
ence system, which adopts joint pattern matching
to predict triggers and arguments at the same time.
These results illustrate the challenge in using joint
inference effectively. NCBI performed much worse
than the SVM-based pipeline systems, EVEX and
TEES2.1. It was also worse than BIOSEM, a rule-
based system that uses considerable domain exper-
tise. Nevertheless, it was better than DLUTNLP,
another SVM-based system.
Figure 3 compares our baseline pipeline model
with our combined model. We can clearly see that
the combined model has a significantly better F1
score than the pipeline model on most event types.
System Rec. Prec. F1
Our System 53.42 63.61 58.07
Miwa12 (Miwa et al., 2012) 53.35 63.48 57.98
Riedel11 (Riedel et al., 2011) ? ? 56
UTurku (Bj?orne and Salakoski, 2011) 49.56 57.65 53.30
MSR-NLP (Quirk et al., 2011) 48.64 54.71 51.50
Table 4: Results on the BioNLP?11 test data.
The regulation events are considered the most com-
plex events to detect because they have a recursive
structure. At the same time, this structure yields a
large number of joint dependencies. The advantage
of using a rich model such as MLNs can be clearly
seen in this case; the combined model yields a 10
point and 6 point increase in F1-score on the test
data and development data respectively compared
to the pipeline model.
5.3 Results on the BioNLP?11 Dataset
Table 4 shows the results on the BioNLP?11 dataset.
We can see that our system is marginally better than
Miwa12, which is a pipeline-based system. It is
also more than two points better than Riedel11,
a state-of-the-art structured prediction-based joint
inference system. Reidel11 incorporates the Stan-
ford predictions (McClosky et al., 2011b) as fea-
tures in the model. On the two hardest, most
complex tasks, detecting regulation events (which
have recursive structures and more joint dependen-
cies than other event types) and detecting bind-
ing events (which may have multiple arguments),
our system performs better than both Miwa12 and
Riedel11.
4
Specifically, our system?s F1 score for
regulation events is 46.84, while those of Miwa12
and Riedel11 are 45.46 and 44.94 respectively. Our
system?s F1 score for the binding event is 58.79,
while those of Miwa12 and Riedel11 are 56.64 and
48.49 respectively. These results clearly demon-
strate the effectiveness of enforcing joint dependen-
cies along with high-dimensional features.
5.4 Results on the BioNLP?09 Dataset
Table 5 shows the results on the BioNLP?09 dataset.
Our system has a marginally lower score (by 0.11
points) than Miwa12, which is the best performing
system on this dataset. Specifically, our system
achieves a higher recall but a lower precision than
Miwa12. However, note that Miwa12 used co-
reference features while we are able to achieve
4
Detailed results are not shown for any of these three
datasets due to space limitations.
839
SVM MLN+SVM
Type Rec. Prec. F1 Rec. Prec. F1
Simple 64.47 87.89 74.38 73.11 78.99 75.94
Protein-Mod 66.49 79.87 72.57 72.25 69.70 70.95
Binding 39.04 50.00 43.84 48.05 43.84 45.85
Regulation 23.51 56.21 33.15 36.47 50.86 42.48
Overall 37.90 67.88 48.64 48.95 59.24 53.61
(a) Test
SVM MLN+SVM
Type Rec. Prec. F1 Rec. Prec. F1
Simple 55.79 81.63 66.28 63.21 75.10 68.64
Protein-Mod 64.47 87.89 74.38 71.14 85.63 77.72
Binding 31.90 48.77 38.57 47.99 50.00 48.97
Regulation 20.13 52.46 29.10 28.57 43.41 34.46
Overall 34.42 66.14 45.28 43.50 57.45 49.51
(b) Development
Figure 3: Comparison of the combined model (MLN+SVM) with the pipeline model on the BioNLP?13
test and development data.
System Rec. Prec. F1
Miwa12 (Miwa et al., 2012) 52.67 65.19 58.27
Our System 53.96 63.08 58.16
Riedel11 (Riedel et al., 2011) ? ? 57.4
Miwa10 (Miwa et al., 2010a) 50.13 64.16 56.28
Bjorne (Bj?orne et al., 2009) 46.73 58.48 51.95
PoonMLN (Poon&Vanderwende,2010) 43.7 58.6 50.0
RiedelMLN (Riedel et al., 2009) 36.9 55.6 44.4
Table 5: Results on the BioNLP?09 test data. ???
indicates that the corresponding values are not
known.
similar accuracy without the use of co-reference
data. The F1 score of Miwa10, which does not
use co-reference features, is nearly 2 points lower
than that of our system. Our system also has a
higher F1 score than Reidel11, which is the best
joint inference-based system for this task.
On the regulation events, our system (47.55) out-
performs both Miwa12 (45.99) and Riedel11 (46.9),
while on the binding event, our system (59.88) is
marginally worse than Miwa12 (59.91) and signifi-
cantly better than Riedel11 (52.6). As mentioned
earlier, these are the hardest events to extract. Also,
existing MLN-based joint inference systems such
as RiedelMLN and PoonMLN do not achieve state-
of-the-art results because they do not leverage com-
plex, high-dimensional features.
6 Summary and Future Work
Markov logic networks (MLNs) are a powerful
representation that can compactly encode rich rela-
tional structures and ambiguities (uncertainty). As
a result, they are an ideal representation for com-
plex NLP tasks that require joint inference, such
as event extraction. Unfortunately, the superior
representational power greatly complicates infer-
ence and learning over MLN models. Even the
most advanced methods for inference and learning
in MLNs (Gogate and Domingos, 2011) are un-
able to handle complex, high-dimensional features,
and therefore existing MLN systems primarily use
low-dimensional features. This limitation severely
affects the accuracy of MLN-based NLP systems,
and as a result, in some cases their performance
is inferior to pipeline methods that do not employ
joint inference.
In this paper, we presented a general approach
for exploiting the power of high-dimensional lin-
guistic features in MLNs. Our approach involves
reliably processing and learning high-dimensional
features using SVMs and encoding their output as
low-dimensional features in MLNs. We showed
that we could achieve scalable learning and in-
ference in our proposed MLN model by exploit-
ing decomposition. Our results on the BioNLP
shared tasks from ?13, ?11, and ?09 clearly show
that our proposed combination is extremely effec-
tive, achieving the best or second best score on all
three datasets.
In future work, we plan to (1) improve our joint
model by incorporating co-reference information
and developing model ensembles; (2) transfer the
results of this investigation to other complex NLP
tasks that can potentially benefit from joint infer-
ence; and (3) develop scalable inference and learn-
ing algorithms (Ahmadi et al., 2013).
Acknowledgments
This work was supported in part by the AFRL un-
der contract number FA8750-14-C-0021, by the
ARO MURI grant W911NF-08-1-0242, and by the
DARPA Probabilistic Programming for Advanced-
Machine Learning Program under AFRL prime
contract number FA8750-14-C-0005. Any opin-
ions, findings, conclusions, or recommendations
expressed in this paper are those of the authors
and do not necessarily reflect the views or official
policies, either expressed or implied, of DARPA,
AFRL, ARO or the US government.
840
References
Babak Ahmadi, Kristian Kersting, Martin Mladenov,
and Sriraam Natarajan. 2013. Exploiting symme-
tries for scaling loopy belief propagation and rela-
tional training. Machine Learning, 92(1):91?132.
David Ahn. 2006. The stages of event extraction.
In Proceedings of the Workshop on Annotating and
Reasoning About Time and Events, pages 1?8.
Jari Bj?orne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
the BioNLP Shared Task 2011 Workshop, pages 183?
191.
Jari Bj?orne and Tapio Salakoski. 2013. TEES 2.1: Au-
tomated annotation scheme learning in the bionlp
2013 shared task. In Proceedings of the BioNLP
Shared Task 2013 Workshop, pages 16?25.
Jari Bj?orne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18.
Quoc-Chinh Bui, David Campos, Erik van Mulligen,
and Jan Kors. 2013. A fast rule-based approach
for biomedical event extraction. In Proceedings of
the BioNLP Shared Task 2013 Workshop, pages 104?
108.
Chen Chen and Vincent Ng. 2012. Joint modeling for
Chinese event extraction with rich linguistic features.
In Proceedings of the 24th International Conference
on Computational Linguistics, pages 529?544.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia, PA.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Morgan & Claypool, San Rafael, CA.
Lise Getoor and Ben Taskar, editors. 2007. Introduc-
tion to Statistical Relational Learning. MIT Press.
Vibhav Gogate and Pedro Domingos. 2011. Proba-
bilistic theorem proving. In Proceedings of the 27th
Conference on Uncertainty in Artificial Intelligence,
pages 256?265.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. NYU?s English ACE 2005 system description.
In Proceedings of the ACE 2005 Evaluation Work-
shop. Washington.
Prashant Gupta and Heng Ji. 2009. Predicting un-
known time arguments based on cross-event prop-
agation. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 369?372.
Kai Hakala, Sofie Van Landeghem, Tapio Salakoski,
Yves Van de Peer, and Filip Ginter. 2013. EVEX
in ST?13: Application of a large-scale text mining
resource to event extraction and network construc-
tion. In Proceedings of the BioNLP Shared Task
2013 Workshop, pages 26?34.
Ruihong Huang and Ellen Riloff. 2012a. Bootstrapped
training of event extraction classifiers. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 286?295.
Ruihong Huang and Ellen Riloff. 2012b. Modeling
textual cohesion for event extraction. In Proceed-
ings of the 26th AAAI Conference on Artificial Intel-
ligence.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 254?262.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press, Cambridge,
MA, USA.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedi-
cal events from literature. BMC bioinformatics,
9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011a.
Overview of BioNLP shared task 2011. In Pro-
ceedings of the BioNLP Shared Task 2011 Workshop,
pages 1?6.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of Genia event
task in BioNLP shared task 2011. In Proceedings
of the BioNLP Shared Task 2011 Workshop, pages
7?15.
Jin-Dong Kim, Yue Wang, and Yamamoto Yasunori.
2013. The Genia event extraction shared task, 2013
edition - overview. In Proceedings of the BioNLP
Shared Task 2013 Workshop, pages 8?15.
Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.
841
Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Libin
Hou. 2012. Employing compositional semantics
and discourse consistency in Chinese event extrac-
tion. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1006?1016.
Lishuang Li, Yiwen Wang, and Degen Huang. 2013a.
Improving feature-based biomedical event extrac-
tion system by integrating argument information. In
Proceedings of the BioNLP Shared Task 2013 Work-
shop, pages 109?115.
Peifeng Li, Qiaoming Zhu, and Guodong Zhou. 2013b.
Argument inference from relevant event mentions in
Chinese argument extraction. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 1477?1487.
Qi Li, Heng Ji, and Liang Huang. 2013c. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 73?82.
Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 789?797.
Shasha Liao and Ralph Grishman. 2011. Acquiring
topic features to improve event extraction: in pre-
selected and balanced collections. In Proceedings
of the International Conference Recent Advances in
Natural Language Processing 2011, pages 9?16.
Haibin Liu, Karin Verspoor, Donald C. Comeau, An-
drew MacKinlay, and W John Wilbur. 2013. Gen-
eralizing an approximate subgraph matching-based
system to extract events in molecular biology and
cancer genetics. In Proceedings of the BioNLP
Shared Task 2013 Workshop, pages 76?85.
Daniel Lowd and Pedro Domingos. 2007. Efficient
weight learning for markov logic networks. In
Proceedings of the 11th European Conference on
Principles and Practice of Knowledge Discovery in
Databases, pages 200?211.
Wei Lu and Dan Roth. 2012. Automatic event extrac-
tion with structured preference modeling. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 835?844.
Radu Marinescu and Rina Dechter. 2009. AND/OR
branch-and-bound search for combinatorial opti-
mization in graphical models. Artificial Intelligence,
173(16-17):1457?1491.
David McClosky, Mihai Surdeanu, and Chris Manning.
2011a. Event extraction as dependency parsing. In
Proceedings of the Association for Computational
Linguistics: Human Language Technologies, pages
1626?1635.
David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011b. Event extraction as dependency
parsing for BioNLP 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 41?45.
David McClosky. 2010. Any domain parsing: Auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Ph.D. thesis, Brown University,
Providence, RI.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. Evaluating dependency rep-
resentation for event extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 779?787.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and
Jun?ichi Tsujii. 2010b. Event extraction with com-
plex event classification using rich features. Jour-
nal of Bioinformatics and Computational Biology,
8(01):131?146.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou.
2012. Boosting automatic event extraction from the
literature using domain adaptation and coreference
resolution. Bioinformatics, 28(13):1759?1765.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Claire N?edellec, Robert Bossy, Jin-Dong Kim, Jung-
Jae Kim, Tomoko Ohta, Sampo Pyysalo, and Pierre
Zweigenbaum. 2013. Overview of BioNLP shared
task 2013. In Proceedings of the BioNLP Shared
Task 2013 Workshop, pages 1?7.
Siddharth Patwardhan and Ellen Riloff. 2009. A uni-
fied model of phrasal and sentential evidence for in-
formation extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?160.
Hoifung Poon and Pedro Domingos. 2007. Joint in-
ference in information extraction. In Proceedings
of the 22nd National Conference on Artificial Intelli-
gence, pages 913?918.
Hoifung Poon and Lucy Vanderwende. 2010. Joint
inference for knowledge extraction from biomedi-
cal literature. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 813?821.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130?137.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. MSR-NLP entry in
BioNLP shared task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 155?
163.
842
Sebastian Riedel and Andrew McCallum. 2011a. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1?12.
Sebastian Riedel and Andrew McCallum. 2011b. Ro-
bust biomedical event extraction with dual decom-
position and minimal domain adaptation. In Pro-
ceedings of the BioNLP Shared Task 2011 Workshop,
pages 46?50.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A Markov logic approach
to bio-molecular event extraction. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
Shared Task, pages 41?49.
Sebastian Riedel, David McClosky, Mihai Surdeanu,
Andrew McCallum, and Christopher D. Manning.
2011. Model combination for event extraction in
bionlp 2011. In Proceedings of the BioNLP Shared
Task 2011 Workshop, pages 51?55.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from Twit-
ter. In Proceedings of the 18th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 1104?1112.
Bart Selman, Henry Kautz, and Bram Cohen. 1996.
Local Search Strategies for Satisfiability Testing. In
D. S. Johnson and M. A. Trick, editors, Cliques,
Coloring, and Satisfiability: Second DIMACS Im-
plementation Challenge, pages 521?532. American
Mathematical Society, Washington, DC.
Parag Singla and Pedro Domingos. 2005. Discrimina-
tive training of Markov logic networks. In Proceed-
ings of the 20th National Conference on Artificial
Intelligence, pages 868?873.
David Sontag and Amir Globerson. 2011. Introduction
to Dual Decomposition for Inference. Optimization
for Machine Learning.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proceedings of the 21st Interna-
tional Conference on Machine Learning, pages 104?
112.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York, NY.
843
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1127?1138,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Vote Prediction on Comments in Social Polls
Isaac Persing and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{persingq,vince}@hlt.utdallas.edu
Abstract
A poll consists of a question and a set of
predefined answers from which voters can
select. We present the new problem of vote
prediction on comments, which involves
determining which of these answers a
voter selected given a comment she wrote
after voting. To address this task, we ex-
ploit not only the information extracted
from the comments but also extra-textual
information such as user demographic in-
formation and inter-comment constraints.
In an evaluation involving nearly one mil-
lion comments collected from the popu-
lar SodaHead social polling website, we
show that a vote prediction system that ex-
ploits only textual information can be im-
proved significantly when extended with
extra-textual information.
1 Introduction
We introduce in this paper a new opinion mining
task, vote prediction on comments in social polls.
Recall that a poll consists of a question accompa-
nied by a set of predefined answers. A user who
votes on the question will choose one of these an-
swers and will be prompted to enter a comment
giving an explanation of why she chose the an-
swer. Given a poll and a user comment written
in response to it, the task of vote prediction seeks
to determine which predefined answer was chosen
by the author of the comment.
A solution to the vote prediction problem would
contribute significantly to our understanding of the
underlying attitudes of individual social polling
website users. This understanding could be ex-
ploited for tasks such as improving user experi-
ence or directed advertising; if we can predict how
a user will vote on a question, we can make more
accurate guesses about what kind of content/ads
related to the question the user would like to see.
Unfortunately, a major difficulty of vote predic-
tion arises from the casual nature of discussion in
social media. A comment often contains insuffi-
cient information for inferring the user?s vote, or
in some cases may even be entirely absent.
In light of this difficulty, we exploit two addi-
tional types of information in the prediction pro-
cess. First, we employ demographic features de-
rived from user profiles. Demographic features
may be broadly useful for other opinion mining
tasks such as stance classification (Somasundaran
and Wiebe, 2010), as many social media web-
sites like CreateDebate1 allow users to create pro-
files with similar demographic information. Previ-
ous work has attempted to predict such latent fea-
tures (e.g., Rao and Yarowsky (2010), Burger et
al. (2011)) rather than employing them for opin-
ion mining tasks.
Second, we exploit inter-comment constraints
to help us perform joint inference over votes on
different questions. Note that previous work on
debate stance recognition has also employed con-
straints to improve the inference process. Specif-
ically, in stance prediction, it is typical to em-
ploy so-called author constraints (e.g., Thomas
et al. (2006), Bansal et al. (2008), Walker et al.
(2012a), Hasan and Ng (2013)), which specify that
two documents written by the same author for the
same topic should have the same stance. However,
in vote prediction, author constraints are not use-
ful because a user is not permitted to cast more
than one vote per question, unlike in stance pre-
diction, where users may engage in a debate and
therefore post more than once per debate topic.
Consequently, we propose two new types of con-
straints for exploiting inter topic user voting pat-
terns. One constraint involves pairs of authors and
the other involves pairs of questions. These con-
straints are also potentially useful for other opin-
1http://www.createdebate.com/
1127
ion mining tasks involving social media, as social
media sites typically allow users to comment on
multiple topics. Note that enforcing constraints in-
volving two questions is by no means trivial, as the
possible class values associated with the two com-
ments may not necessarily be the same.
Another contribution of our work lies in our
adaptation of the label propagation algorithm (Zhu
and Ghahramani, 2002) to enforce constraints for
vote prediction. Recall that existing stance classi-
fication approaches enforce constraints using min-
imum cut (Thomas et al., 2006), integer linear pro-
gramming (Lu et al., 2012), and loopy belief prop-
agation (Burfoot et al., 2011). Our decision to em-
ploy label propagation stems in part from the in-
ability of loopy belief propagation and integer lin-
ear programming to efficiently process the nearly
one million comments we have, and in part from
the inability of the traditional two-way minimum
cut algorithm to handle multiclass classification.
It is worth noting, however, that other variations
of the label propagation algorithm have been pro-
posed for unrelated NLP tasks such as automati-
cally harvesting temporal facts from the web (e.g.,
Wang et al. (2011) and Wang et al. (2012)).
While we are the first to address the vote predic-
tion task, other researchers have previously used
social media to predict the outcomes of various
events, primarily by analyzing Twitter data. For
example, Tumasjan et al. (2010) and Gayo-Avello
et al. (2011) performed the related task of predict-
ing the outcomes of elections. Rather than pre-
dicting election outcomes, O?Connor et al. (2010)
focused on finding correlations between measures
derived from tweets and the outcomes of politi-
cal events like elections and polls. Finally, Asur
and Huberman (2010) predicted movies? box of-
fice success. These tasks contrast with our task of
vote prediction in that they are concerned with ag-
gregate measures such as the fraction of the vote
each candidate or party will win in an election or
how much money a movie will make at the box
office, whereas vote prediction is concerned with
predicting how individual people will vote on a
much wider variety of news/political topics.
2 Corpus
SodaHead2 is a social polling website where users
vote on and ask questions about a wide variety of
topics ranging from the serious (e.g., ?Should the
2http://www.sodahead.com
U.S. raise the minimum wage??) to the silly (e.g
?What is your favorite kind of pie??). Whenever a
user votes on one of these questions, choosing one
of a set of predefined answers, she is prompted to
enter a comment giving an explanation of why she
chose the answer she did. Our corpus3 consists of
all the comments4 users posted under all featured
questions in the News & Politics category of the
SodaHead website between March 12, 2008 and
August 21, 2013.
This dataset consists of a total of 997,379 com-
ments over 4,803 different questions, so an aver-
age of 208 comments are written in response to
each question. The length of an average comment
is 49 words. As Table 1 illustrates, these questions
may have more than two possible answers, with an
average question having 2.4 possible answers.
Each SodaHead user has her own profile that
contains demographic information about her. As
we can see from Table 2, many users choose to
provide only some information about themselves,
leaving many of the demographic fields blank.
108,462 users posted at least one comment in our
corpus, with an average user commenting on 9.2
of our questions.
3 Baseline Systems
To perform our experiments, we first split our
comments into three sets, a test set for evaluating
performance, a training set for training classifiers,
and a development set for tuning parameters. In
order to ensure that the comparisons of our experi-
ments are valid, we construct our test set using the
same 20% of comments in the dataset regardless
of experiment. Since our goal is to plot a learning
curve illustrating how our various vote prediction
systems perform given different amounts of train-
ing and development data, we vary the size of our
training and development sets across experiments
so that in the smallest experiment, together they
comprise 25% of the remaining (non-test) com-
ments, and in the largest experiment, they com-
3http://www.hlt.utdallas.edu/%7epersingq/SocialPolls/ is
the distribution site for our corpus. We preserve user
anonymity by replacing the original id of each user with a
random number in our corpus.
4A ?comment? is the text a user posted when submitting
her vote on a question. It does not include posts not associ-
ated with a vote (such as responses to other posts) or votes
where the user chose not to enter a comment. Thus, there
is a one-to-one relationship between comments in votes in
our dataset. The vote associated with a comment is always
known.
1128
Question Vote Comment
Who Won Round Two of
the Presidential Debate?
Barack Obama Binders full of women. That is all.
Mitt Romney Obama is inept and a liar. We can?t survive 4 more years of his crazy crap.
What?s the Best Way to
Read a Magazine?
in print Upside down like Luna Lovegood.
online Print costs money. It also doesn?t have a Search function.
on a tablet device since sooooo many people have tablet devices why read it as print or online?
on a smartphone Clicked in print!!! Aargh
Table 1: Sample questions and comments. All of the pre-defined answers for these questions are repre-
sented by one comment.
User ID 3479864 3189372
Age 25-34
Smoker No
Drinker No
Income
Sexual Orientation Straight
Relationship Status Single
Political Views Conservative Moderate
Ethnicity
Looking For
Career Industry
Children Undecided
Education High School
Gender Female Male
Religious Views Other Christian
Employment Status
Weight Type
Table 2: Sample user profiles.
prise 100% of the remaining comments. For each
experiment, we maintain a ratio of three training
comments to one development comment.
Recall that each comment in our dataset is writ-
ten in response to a particular question. For each
test comment, our goal is to predict the user?s an-
swer to the question given the text of her comment.
One of the major inherent difficulties of our task
is that it consists not of one, but of 4,803 sep-
arate multiclass classification problems (one for
each question). As a result, our approach to the
problem necessarily has to be somewhat generic,
as it would be too time-consuming to develop an
appropriate feature set for each question.
3.1 Baseline 1
Our first baseline?s (B
1
) approach employs 4,803
multiclass classifiers (one for each question). Each
classifier is trained on one question?s training set,
representing each comment using only a bias fea-
ture. Each of our classifiers is trained using MAL-
LET?s (McCallum, 2002) implementation of max-
imum entropy (ME) classification. This is equiv-
alent to merely counting the number of training
set comments that voted for each possible answer,
selecting the most frequent answer, then applying
this label to all the comments in the test set. This
majority baseline serves primarily to tell us how
well our more sophisticated baseline performs.
3.2 Baseline 2
Our second baseline (B
2
) is constructed in exactly
the same way as B
1
except that each classifier is
trained using both a bias feature and a standard set
of feature types described below.
3.2.1 Features
Since the questions in our dataset come from the
News & Politics category of the SodaHead web-
site, many of the questions? topics are political.
For that reason, it makes sense to use features
which have been shown to work well on other
political classification problems. We therefore
base our feature set on that used by Walker et
al. (2012b) for political debate classification. Our
features are described below.
N-grams. Unigrams have been shown to per-
form well in ideological debates (Somasundaran
and Wiebe, 2010), so we therefore present our
classifiers with lemmatized unigram, bigram, and
trigram features. We normalize the n-gram feature
vector to unit length to avoid giving undue influ-
ence to longer comments.
Cue Words. Based on other work (Fox Tree
and Schrock, 1999; Fox Tree and Schrock, 2002;
Groen et al., 2010; Walker et al., 2012b), we also
present our classifiers with features representing
the first lemmatized unigram, bigram, and trigram
appearing in each comment. These may be useful
in our task when, for example, a user?s comment
begins with or entirely consists of a restatement of
the answer she chose. So if the possible answers
for a given question are ?Yes? and ?No?, a user
might write in her comment ?Yes. Because ...?,
and this would make the ?CueWord:Yes? feature
useful for classifying this comment.
Emotion Frequency. For each word in a com-
ment, we used the NRC Emotion Word Lexicon
1129
(Mohammad and Yang, 2011) to discover if the
word conveys any emotion. Then, for each emo-
tion or sentiment covered by the lexicon (anger,
anticipation, disgust, fear, joy, sadness, surprise,
trust, positive, or negative) e
i
, we construct a fea-
ture e
i
:
C(e
i
)
total
describing how much of the comment
consists of words conveying emotion e
i
, where
C(e
i
) is the count of words in the comment bear-
ing emotion e
i
and total is the number of words
in the comment. To understand why this fea-
ture may be useful, consider the question ?Does
Sarah Palin deserve VP?? We suspect that users
who post comments laden with words associated
with positive emotions like joy are more likely
to vote ?Yes? because the positive emotions im-
ply they are happy about a Sarah Palin vice presi-
dency. Similarly, users who post comments laden
with negative emotions like anger might be more
likely to vote ?No?.
Dependencies. We use the Stanford Parser (de
Marneffe et al., 2006) to extract a set of depen-
dencies from each comment. For an example of
how dependencies might help in our task, con-
sider the second comment in Table 1. From this
comment, we can extract the dependency triple
dependency:(nsubj,inept,obama), which indicates
that the user who wrote it does not like Obama and
is therefore more likely to have voted for Romney
in the question. Dependency feature vectors are
normalized to unit length.
Emotion Dependencies. To form an emo-
tion dependency feature, we take a regular de-
pendency feature and replace each of its words
where possible with the emotion it evokes as deter-
mined by the NRC Emotion Word Lexicon. Thus
from the dependency:(nsubj,inept,obama) exam-
ple above, we would generate three features: emo-
tiondependency:(nsubj,anger,obama), emotionde-
pendency:(nsubj,disgust,obama), and emotionde-
pendency:(nsubj,negative,obama). These features
help generalize dependencies, and this is use-
ful because predictive features like emotiondepen-
dency:(nsubj,negative,obama) appear frequently
in the comments for this question, but depen-
dency:(nsubj,inept,obama) does not. Emotion De-
pendency feature vectors are normalized to unit
length.
Post Information. Features under this category
just calculate some basic statistics about a com-
ment. These features may be useful because, for
example, the question ?Most Scandalous Politi-
cians of 2008? Who deserves the title?? has six
possible answers, each except the last naming a
particular well-known politician. The last choice
is ?The most scandalous politician of 2008 is ...?
and the user is expected to name a politician in her
comment. It would make sense for users choos-
ing this option to have written longer responses
since they have to name and possibly explain their
choice to users who might not necessarily know
who their chosen politician is.
3.2.2 Feature Selection
Because some of the feature types (n-grams, cue
words, dependencies, and emotion dependencies)
described in the previous subsection are expected
to generate a large number of non-predictive fea-
tures, we trim some of the most irrelevant fea-
tures out of the feature set to avoid memory prob-
lems. Therefore, following Yang and Pedersen
(1997), for each question we calculate the infor-
mation gain of each feature of these types on the
training set. We then remove those features having
the lowest information gain as well as those fea-
tures occurring less than ten times in the dataset.
Early experiments showed that 1,000 was a rea-
sonable number of features to keep, so for all ex-
periments we keep only the top 1,000 features of
these types. Note that we do not apply feature se-
lection to emotion frequency or post information
features, as each of these sets consists of a small
number of real-valued features.
4 Demographic Features
As mentioned in the introduction, a major diffi-
culty inherent to our problem is that in many cases
a comment contains insufficient information for
inferring the underlying vote. Aside from being
short, the comments shown in Table 1 are typ-
ical of comments found in the dataset. Some
comments are like the first and third in the table,
requiring some obscure bit of world knowledge
to understand what the writer is saying. Others
like the fourth only explain why the user did not
choose a particular answer, which is always po-
tentially useful, but sufficient only if the comment
excludes every other possible choice.
Because it is difficult to tell how a user voted
given her comment, we exploit the demographic
information users provide in their profiles as an
additional source of information. Since many
of the questions in our dataset deal with poli-
tics, we anticipate that information about things
1130
such as whether a comment was written by a
conservative or progressive user would be use-
ful for predicting the answers of many comments.
For each comment, we encode demographic in-
formation as features in the following way. For
each field in the user?s profile shown in Table 2
(aside from user ID), we construct a feature of the
form F
i
:V
i
if the user filled in field F
i
with value
V
i
. Thus, any comment made by user 3479864
would include the features Age:25?34, Politi-
calViews:Conservative, Gender:Female, and Reli-
gion:Other.
Here is an example of a comment whose
predicted vote gets corrected by adding demo-
graphic features to our system. For the ques-
tion, ?LPGA Decides to Allow Transgender Com-
petitors: Good or Bad Move for Golf??, user
2252750 writes, ?LPGA ...can let monkeys play if
they wish....nobody gives a rip... bark?. Of the
three possible answers for this question, ?Good
move?, ?Bad move?, and ?Undecided?, our base-
line system without demographics believes that
user 2252750 probably voted for the third, as
?nobody gives a rip? makes him sound apathetic
toward the issue. However, our demographic
system notices that his profile contains ?Reli-
gion:Christian?, and users with this demographic
attribute choose ?Bad Move? 64% of the time.
Thus, demographic features allowed our system to
correctly predict his vote for ?Bad Move?.
Since demographics are also expected to gen-
erate a large number of non-predictive features,
we apply feature selection to them as described in
Section 3.2.2.
5 Enforcing Constraints
We mentioned earlier that an average SodaHead
question contains 208 comments. This implies
that there are only about 31?125 comments5 in
the average training set for one of our ME classi-
fiers. It would be difficult to train a good classi-
fier from a training set this small even if we had
feature sets tailored to work well on each of the
4,803 questions. While we have already attempted
to exploit user information (in the form of de-
mographic features) to help improve our system?s
performance, this approach still treats the task as
4,803 separate classification problems. It does not
allow for the possibility that classification on one
5At the low and high end of the learning curve respec-
tively.
question may be improved by exploiting informa-
tion gleaned from votes on other questions.
One way we might exploit such information is
by first noticing that, for any pair of questions,
there may be multiple users who commented on
both. This overlap between questions allows us to
calculate how predictive a user?s vote on one ques-
tion is of how she will vote on the other. For ex-
ample, on the question ?Who Would You Rather
Have Dinner With??, we found that users who
voted for ?Mitt Romney? were much more likely
to choose ?No, I?m still voting for him? on the
question ?Does Mitt Romney?s ?Entitled? Remarks
Change Your Opinion of Him??. Similarly, users
who voted to have dinner with ?Barack Obama?
were much more likely to vote ?Yes, I?m not vot-
ing for him anymore? on the ?entitled? question.
A system that somehow takes into account this in-
formation might correctly classify a difficult com-
ment on the ?entitled? question if it notices that the
comment was written by a user who commented
on both questions and it knows how the user voted
on the ?dinner? question. We call the kind of con-
straint described here a QuestionPair constraint.
We might also exploit information from other
questions by noticing that there are users who
share similar attitudes on a wide variety of top-
ics in our dataset. We can gauge how often a
pair of users agree with each other by compar-
ing their votes on every question on which they
have both voted where their comments appear in
the training set. So for example, if we see that
two users have agreed on questions about George
H.W. Bush, Bill Clinton, and George W. Bush,
we can guess that they will also agree on a ques-
tion about Barack Obama. Similarly, if they dis-
agreed on all those questions, they are likely to
disagree on the last question. A system that takes
into account this kind of information could cor-
rectly classify an otherwise difficult comment if it
knows how another user voted on this question and
also knows how often the two users agree on other
questions. We call the kind of constraint described
here a VoterPair constraint.
In order to enforce both kinds of constraints,
we introduce a variation of the label propagation
algorithm (Zhu and Ghahramani, 2002). In our
version of the label propagation algorithm, each
comment in our dataset is represented by a node
in a graph. Each node is associated with a proba-
bility distribution indicating the likelihood that the
1131
comment belongs to each of its question?s possible
answers. Thus, when we initialize the graph, each
training set node?s probability distribution is set to
reflect its comment?s actual label (with a proba-
bility of 1 for the comment?s actual label and 0
for each other answer), and each development or
test set node?s probability distribution is set to the
value predicted by another classifier such as B
2
or
B
2
+ Dem since the algorithm is not permitted
to see the comment?s actual label. Lines 7?12 in
Figure 1 describe the graph?s initialization.
Now that we have set up the graph?s nodes, we
need to explain how our graph?s edges work. As
we discussed earlier in this section, the edges in
our graph will represent two kinds of soft con-
straints. Each edge allows one of a node?s neigh-
bors to cast a vote (in the form of a probability dis-
tribution over possible answers) for what it thinks
the node?s answer should be. Let us call the com-
ment node whose label we are trying to predict the
target node and the comment node which casts the
vote the source node.
Our graph contains a QuestionPair edge be-
tween any source and target comments written by
the same user. Since a user cannot comment more
than once on any question, the source and target
comments will occur in two different questions. In
order to determine how the source node votes over
a QuestionPair edge, we need to calculate some
probabilities. In particular, we need to determine
the probability that a user will vote for possible
answer k in the target question Q
I
given that she
voted for answer l in the source question Q
J
:
P (Q
I
k
|Q
J
l
) =
C(Q
I
k
,Q
J
l
)+?
?
m?A(Q
I
)
(C(Q
I
m
,Q
J
l
)+?)
where C(Q
I
n
, Q
J
l
) is the number of users who
voted for answer n in Q
I
and answer l in Q
J
, and
A(Q
I
) is the set of possible answers on Q
I
. We
set ?, the smoothing factor, to 10 since this value
worked well in earlier experiments. The source
node S casts its vote on target node T for the prob-
ability distribution given by:
QP
S,T
(Q
I
k
) =
?
m?A(Q
J
)
P
S
(Q
J
m
)P (Q
I
k
|Q
J
m
)
where P
S
(Q
J
m
) is the probability currently asso-
ciated with answer m in S?s question (Q
J
).
The graph contains a VoterPair edge between
any source and target nodes on the same question
if the users who posted these comments have both
voted on at least one other question together and
their comments on the other question(s) occurred
in the training set. To determine how the source
node votes over a VoterPair edge, we need to cal-
culate the probability that the source and target
users will agree on a generic issue:
P
agr
(U
S
, U
T
) =
C
agr
(U
S
,U
T
)+1
C
agr
(U
S
,U
T
)+C
dis
(U
S
,U
T
)+2
where C
agr
(U
S
, U
T
) is the number of questions
on which users U
S
and U
T
voted for the same
answer and both their comments occurred in the
training set, C
dis
(U
S
, U
T
) is the number of ques-
tions on which U
S
and U
T
voted for different
answers where both their comments occurred in
the training set, and the +1 and +2 are used for
smoothing. The probability distribution that the
source node S votes for on target node T is then
given by:
V P
S,T
(Q
I
k
) = P
S
(Q
I
k
)P
agr
(U
S
, U
T
)
+
?
m?A(Q
I
),
m 6=k
(P
S
(Q
I
m
))
1? P
agr
(U
S
, U
T
)
|A(Q
I
)| ? 1
where P
S
(Q
I
n
) is the probability currently asso-
ciated with answer n in the source node?s question
(Q
I
), and |A(Q
I
)| is the number of possible an-
swers on Q
I
. We divide the second term, which
deals with disagreement, by |A(Q
I
)| ? 1 because,
even if we know that the target and source users
disagreed on the answer to a particular question
and that the source user did not vote for answer
k, there is only a 1
|A(Q
I
)|?1
chance that the target
user voted for answer k since there are |A(Q
I
)|?1
non-k answers to choose from.
Now that we have described how edges are
added to the graph and how source comment nodes
vote over the edges, we are ready to begin iterat-
ing over the label propagation algorithm (line 13 in
Figure 1). For each iteration of the algorithm, we
update each development or test set node?s answer
probability distribution by assigning it a weighted
sum of (1) the initial probability distribution as-
signed to the node, (2) the sum of the Question-
Pair edges? votes, and (3) the sum of the VoterPair
edges? votes (line 16 in Figure 1). Upon comple-
tion of the algorithm, if our soft constraints work
as expected, the new labeling of comment nodes
should be more accurate than their initial labeling.
We tune the parameters W
I
, W
V
, W
Q
, and
iterations jointly by an exhaustive search of the
parameter space to maximize classification accu-
racy on the development set. Each of the weight
parameters is allowed to take one of the values 0,
1, or 2, and the iteration parameter is allowed take
one of the values 0, 1, 2, 3, 4, 5.
1132
1: LabelPropagation(Tr,D, Te, iterations,W
i
,W
V
,W
Q
, I)
2: Inputs:
3: Tr,D, Te: Comments in Training, Development, and Test set
4: iterations: The number of iterations to perform
5: W
i
,W
V
,W
Q
: Weights assigned to initial, VoterPair, and QuestionPair constraints
6: I: Initial answer probability distribution for all comments. Should reflect actual labels for training set comments and
classifier predictions for development and test set comments
7: for all C ? Tr ?D ? Te do
8: Create node representing C
9: C
p
? I
C
10: // C
p
: node C?s current probability distribution over possible answers
11: // I
C
: initial answer probability distribution for comment C
12: end for
13: for j = 1 to iterations do
14: for all node C ? D ? Te do
15: Add all edges targeting node C
16: C
p
? Norm(W
I
I
C
+ W
V
?
k
V P
k,C
+ W
Q
?
k
QP
k,C
)
17: // V P
k,C
, QP
k,C
: kth VoterPair, and kth QuestionPair votes for node C
18: Remove all edges targeting node C
19: end for
20: end for
Figure 1: Our label propagation algorithm.
One may be surprised to notice how we add
edges to the graph in the algorithm only to delete
them three lines later (lines 15 and 18 in Figure 1).
Though edges can be added at any point in the al-
gorithm, one benefit of using the label propagation
algorithm is that it is simple enough that it is not
necessary store all the edges in memory at once.
The only time we need to store an edge is when its
target is being voted on. This means that the label
propagation algorithm can handle large datasets
like ours with huge numbers of nodes and edges
without being prohibitively space-expensive.
6 Evaluation
6.1 Experimental Setup
We mentioned in Section 3 that we split our
dataset of 997,379 comments into a test set com-
prising about 20% of the dataset?s comments and
a training and development set comprising some
fraction of the remaining 80% of the comments.
We actually split the data up like this five different
times so that each comment appears in an experi-
ment?s test set exactly once. In this way, through
the use of five fold cross-validation, we can report
our results on the entire dataset.
6.2 Results and Discussion
Figure 2a shows the accuracy of the predictions
made by various systems. First, let us compare
our first and second baselines. Recall that the first
baseline (B
1
) predicts that all test comments will
have the same label as the majority of training
comments, and the second baseline?s (B
2
) predic-
tions are the output of ME classifiers trained with a
generic feature set. As we can see from the graph,
at very small training set sizes, the standard set of
features supplied to B
2
does little more than con-
fuse the ME learner, as it performs slightly but not
significantly worse6 than the first baseline when
the training/development set comprises only 25%
of the available data. This is understandable, as
25% of an average question?s available data is only
42 comments, an extremely small number of ex-
amples to learn from for most NLP tasks. Clearly
a better approach than the one provided by the sec-
ond baseline is needed. Though the average train-
ing set sizes at the 50%, 75%, and 100% levels are
still relatively small, B
2
significantly outperforms
B
1
at all these levels.
The small improvement sizes yielded by B
2
may be attributable to some of the inherent dif-
ficulties of the problem, particularly that (1) it is
composed of so many (4,803) separate subprob-
lems that it is impractical for us to tailor a unique
feature set for each one, (2) the average question is
associated with a very small number of comments
(about 208), making it difficult to train a reason-
ably good classifier for any question, and (3) many
of the comments contain insufficient information
for inferring the underlying votes. Perhaps some
of our proposed extensions to B
2
can help address
6All significance tests are paired t-tests, with p < 0.05.
Because we calculate a large number of significance results,
the p values we report are obtained using Holm-Bonferroni
multiple testing correction (Holm, 1979).
1133
 62
 63
 64
 65
 66
 67
 68
 69
 70
 25  50  75  100
A
cc
ur
ac
y 
(%
)
Training/Development Set Size (%)
B2+Dem+QPair+VPair
B2+Dem+VPair
B2+Dem+QPair
B2+QPair+VPair
B2+VPair
B2+QPair
B2+Dem
B2
B1
(a) Vote prediction.
 62
 63
 64
 65
 66
 67
 68
 69
 70
 25  50  75  100
A
cc
ur
ac
y 
(%
)
Training/Development Set Size (%)
B1+Dem+QPair+VPair
B1+Dem+VPair
B1+Dem+QPair
B1+QPair+VPair
B1+VPair
B1+QPair
B1+Dem
B1
(b) Arbitrary User Vote Prediction.
Figure 2: Five-fold cross-validation vote prediction learning curves.
some of these problems.
The first improvement we proposed involved
exploiting demographic features provided by users
to help with our prediction tasks. When we com-
bine Dem and B
2
?s feature sets, the resulting
system (B
2
+ Dem) performs better than any of
the systems discussed thus far at all four train-
ing/development set size levels, yielding signifi-
cant improvements over B
2
at all four levels. This
demonstrates that our demographic features are a
useful complement to a standard approach like the
one used by B
2
.
The second improvement we proposed involved
using a variation of the label propagation algo-
rithm to enforce QuestionPair constraints. Ques-
tionPair constraints, recall, allowed us to exploit
the observed voting patterns of users who voted
in the training set on any particular pair of ques-
tions. These constraints were expected to improve
our predictions for any user who voted on both
questions when at least one of their votes appeared
in the test set. System B
2
+ QPair corresponds
to following the algorithm in Figure 1, using sys-
tem B
2
?s ME classifiers to initialize a label prop-
agation graph, and then setting the VoterPair edge
weight (W
V
) to 0, thus allowing only Question-
Pair constraints. When we compare this system to
B
2
, we see that the performance boost Question-
Pair constraints give us over the baseline is consis-
tently greater than the boost given by adding de-
mographic features to it (B
2
+ Dem) across all
training/development set sizes. The improvement
over B
2
is even significant at the 75% and 100%
training/development set sizes.
The last improvement we proposed involved
adding VoterPair constraints to the label propaga-
tion graph. Recall that VoterPair constraints al-
lowed us to exploit how frequently we observed
two users agreeing with each other to predict
whether they will agree on any question they both
voted on. System B
2
+V Pair corresponds to fol-
lowing the label propagation algorithm using B
2
?s
ME classifiers to initialize the graph, then setting
the QuestionPair edge weight (W
Q
) to 0, thus al-
lowing only VoterPair constraints. The addition of
VoterPair constraints yields the largest significant
improvements over B
2
at all four levels, indicating
that, in the absence of our other proposed improve-
ments, VoterPair edge constraints are the most im-
portant addition we can make to our baseline.
While we have now shown that each of our pro-
posed extensions yields significant improvements
over B
2
, this does not necessarily mean that each
one is useful in the presence of the others. For
example, it might be the case that QuestionPair
constraints and Demographic features correct the
same kinds of classification errors, and therefore it
may be sufficient to use either one or the other to
obtain good results, but using both is unnecessary.
To test how useful they are in each other?s pres-
1134
ence, we perform the following experiment. First,
we run the algorithm using all three improvements
(B
2
+Dem+QPair+V Pair in Figure 2a). We
then run the same experiment three more times,
each time removing one of the three extensions.
By measuring how much performance decreases
when we remove each of the three improvements,
we can determine whether each improvement pro-
vides unique useful information, or whether the in-
formation it provides is already being provided by
one of the other improvements.
To see what happens when we remove demo-
graphic features from the full system, we need to
compare B
2
+Dem+QPair+V Pair and B
2
+
QPair+V Pair in Figure 2a. While the decrease
in performance after removing demographic fea-
tures was modest, the difference is nevertheless
significant at all four training/development set
sizes, suggesting that demographic features do
provide unique information to the system.
By comparing line B
2
+ Dem + QPair +
V Pair to line B
2
+Dem+V Pair, we can deter-
mine the impact of QuestionPair constraints. Re-
moving QuestionPair constraints also had a mod-
est impact on the full system?s performance, de-
creasing accuracy at all four training/development
set sizes, significantly so at the 50%, 75%, and
100% levels. Interestingly, the impact of Ques-
tionPair constraints appears to grow with the train-
ing set, while the demographic features appear
to have a greater impact when the training set is
small. We can see this by noting that the two lines
cross at around 55%. This suggests that Question-
Pair constraints are especially useful in problems
where it is cheap to obtain a lot of training data,
but in problems where the data has to be manually
annotated, demographic features are more useful.
Finally, we can compare line B
2
+ Dem +
QPair + V Pair to line B
2
+ Dem + QPair to
see what happens when we remove VoterPair con-
straints from our system. This comparison illus-
trates that VoterPair constraints are by far the most
important improvement we removed from the full
system, as removing them yielded large significant
decreases at all four levels.
Though thus far we have only used it to analyze
the the contributions of different individual im-
provements, the full system B
2
+Dem+QPair+
V Pair is interesting in itself. Of all the systems
we have constructed, it performs the best, yield-
ing improvements of up to 5.18% and 3.88% when
compared to B
1
and B
2
respectively. Its improve-
ments over both baselines are statistically signifi-
cant at all four training/development set sizes.
6.3 Arbitrary User Vote Prediction
One interesting question that we have not yet ad-
dressed is, is it possible to predict how a user
would vote on a question she has not yet seen?
This problem is interesting because an average
question receives votes from only 0.2% of the
users in our dataset, and thus a system for predict-
ing an arbitrary user?s vote would be able to pre-
dict the votes of the other 99.8% of users. A solu-
tion to this prediction problem would have practi-
cal applications in areas such as directed advertis-
ing (e.g., if we could predict how a user would vote
on the magazine question in Table 1, we would
have a better idea of what kinds of reading de-
vices/services would interest her).
We can mimic this problem with our dataset
by treating the comment text associated with test
votes as unseen since we cannot expect an arbi-
trary user to have commented on any particular
question we are interested in7. It does, however,
make sense for us to expect our arbitrary user to
have provided some personal demographic infor-
mation, and thus a system for making these types
of predictions could reasonably make use of de-
mographic features. Similarly, in this situation we
would expect to have knowledge of all users? train-
ing set voting histories. Thus, it would also be rea-
sonable for our system to exploit the QuestionPair
and VoterPair constraints described in Section 5.
Thus, to test how well our system performs on this
task, we repeat all experiments from the previous
section while replacing B
2
(which uses a ME clas-
sifier trained on comment-based features) with B
1
(the most frequent baseline, which uses a ME clas-
sifier trained using only a bias feature). The results
of these experiments are shown in Figure 2b.
If we compare the results from B
1
to B
1
+Dem
(which compliments B
1
?s bias feature with the de-
mographic feature set), we notice that B
1
+Dem
is significantly worse than B
1
at all training set
sizes. This confirms our suspicion from the pre-
7Although we are trying to mimic the situation in which
we predict how an arbitrary user would vote on an arbitrary
question, we caution that the vote data we train and evaluate
on was not obtained from a set of arbitrary SodaHead users. It
consists only of votes from users who chose which questions
they wanted to answer. For this reason, the data we train and
evaluate on for any question might not be a representative
sample of SodaHead users as a whole.
1135
vious section that demographic features by them-
selves serve only to confuse the learner, though we
will see in a moment that they are a helpful sup-
plement to more sophisticated systems.
We can evaluate QuestionPair constraints in this
setting by comparing the results from B
1
to B
1
+
QPair. B
1
+QPair consistently outperforms B
1
at all four training set sizes, significantly so at the
75% and 100% levels, and thus QuestionPair con-
straints are also a useful addition to our system.
VoterPair constraints can be evaluated in this
setting by comparing B
1
to B
1
+ V Pair. B
1
+
V Pair significantly outperforms B
1
at all four
training set sizes, and from the graph it appears
to be our most beneficial improvement.
To evaluate whether demographic features are
useful in the presence of the other improve-
ments, we compare the full system, B
1
+Dem+
QPair + V Pair, to its corresponding version
without demographic features, B
1
+ QPair +
V Pair. Though B
1
+ QPair + V Pair signif-
icantly outperforms the full system at the 25%
training set size, the full system significantly out-
performs B
1
+ QPair + V Pair at the 75% and
100% levels, indicating that in this setting, demo-
graphic features are useful in the presence of a
large training set.
We can evaluate the utility of QuestionPair con-
straints in this setting by comparing the full system
to B
1
+Dem+ V Pair. When we remove Ques-
tionPair constraints, accuracy is consistently low-
ered at all four training set sizes, significantly so
at 50%, 75%, and 100%. This tells us that Ques-
tionPair constraints are useful in this setting.
We can evaluate how useful VoterPair con-
straints are by checking how much B
1
+ Dem +
V Pair+QPair?s performance drops when we re-
move VoterPair constraints from it, yielding B
1
+
Dem + QPair. Performance drops considerably
and significantly at all four training set sizes after
removing VoterPair constraints, suggesting that in
this setting, VoterPair constraints are still the most
important of our proposed improvements.
Finally, while we have already established that
all our proposed improvements can improve per-
formance under both settings (comments visible
and comments invisible), it may be worthwhile
to compare the two sets of experiments to deter-
mine whether the comment features used in sys-
tems with B
2
are useful.
A casual inspection of the two figures shows
that, broadly, each system that uses comment-
based features in Figure 2a tends to slightly out-
perform the most comparable system in Figure 2b.
At the low end of the curves, the two systems often
differ by about 1.0% in absolute accuracy, though
at the high end, the difference tends to be much
smaller, with the full system with comment fea-
tures outperforming the full system without com-
ment features by only 0.3%. Since in this setting
it is reasonable to assume a large training set, this
last result is the one we are most interested in, and
it suggests that our full system?s performance does
not suffer much due to the absence of comment
features.
One final observation we can make is that, when
comments are not visible, demographic features
appear to actively harm the performance of sys-
tems trained on a small amount of data, though
at larger training set sizes they are mostly help-
ful. We can tell this by comparing systems with
demographic features to systems without them in
Figure 2b (e.g., by comparing B
1
+Dem+QPair
to B
1
+QPair or B
1
+Dem+ V Pair to B
1
+
V Pair) at the 25% training set size. This is not
the case in the setting where comments are visi-
ble, as we see that demographic features always
appear helpful in Figure 2a. This reinforces the
notion that demographic features provide useful
information in general, but that they are by them-
selves too sparsely available to do more than con-
fuse the learner. They need to be supplemented by
other information sources in order for the learner
to draw correct conclusions.
7 Conclusion
We examined the task of vote prediction on com-
ments from the SodaHead website. To address this
task, we exploited not only information extracted
from the comments but also extra-textual informa-
tion, including demographic information and two
types of inter-comment constraints, QuestionPair
constraints and VoterPair constraints. Our exper-
iments involving 997,379 comments showed that
each of these extensions significantly improved a
baseline that exploited only textual information,
with VoterPair constraints being the most effective
and demographic information being the least ef-
fective. When used in combination, they obtained
up to a 3.88% improvement in absolute accuracy
over the baseline. To stimulate research on this
task, we make our dataset publicly available.
1136
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier
draft of this paper. This work was supported in
part by NSF Grants IIS-1147644 and IIS-1219142.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views or of-
ficial policies, either expressed or implied, of NSF.
References
Sitaram Asur and Bernardo A. Huberman. 2010. Pre-
dicting the future with social media. In Proceedings
of the 2010 IEEE/WIC/ACM International Confer-
ence on Web Intelligence and Intelligent Agent Tech-
nology, pages 492?499.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label
disagreement in the min-cut classification frame-
work. In COLING 2008: Companion Volume:
Posters, pages 15?18.
Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional
floor-debate transcripts. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1506?1515.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Gender discrimination on
twitter. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1301?1309.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation, pages 449?
454.
Jean E. Fox Tree and Josef C. Schrock. 1999. Dis-
course markers in spontaneous speech: Oh what a
difference an oh makes. Journal of Memory and
Language, 40:280?295.
Jean E. Fox Tree and Josef C. Schrock. 2002. Basic
meanings of you know and i mean. Journal of Prag-
matics, 34:427?447.
Daniel Gayo-Avello, Panagiotis Takis Metaxas, and
Eni Mustafaraj. 2011. Limits of electoral predic-
tions using twitter. In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media, pages 490?493.
Martin Groen, Jan Noyes, and Frans Verstraten. 2010.
The effect of substituting discourse markers on their
role in dialogue. Discourse Processes: A Multidis-
ciplinary Journal, 47:388?420.
Kazi Saidul Hasan and Vincent Ng. 2013. Stance
classification of ideological debates: Data, mod-
els, features, and constraints. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 1348?1356.
Sture Holm. 1979. A simple sequentially rejective
multiple test procedure. Scandinavian Journal of
Statistics, 6:65?70.
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan
Roth. 2012. Unsupervised discovery of opposing
opinion networks from forum discussions. In Pro-
ceedings of the 21st ACM International Conference
on Information and Knowledge Management, pages
1642?1646.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit. http://mallet.cs.
umass.edu.
Saif Mohammad and Tony Yang. 2011. Tracking sen-
timent in mail: How genders differ on emotional
axes. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 70?79.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
Fourth International AAAI Conference on Weblogs
and Social Media, pages 122?129.
Delip Rao and David Yarowsky. 2010. Detecting latent
user properties in social media. In Proceedings of
the NIPS workshop on Machine Learning for Social
Networks.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 116?124.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327?335.
Andranik Tumasjan, Timm Sprenger, Philipp Sandner,
and Isabell Welpe. 2010. Predicting elections with
twitter: What 140 characters reveal about political
sentiment. In Proceedings of the Fourth Interna-
tional AAAI Conference on Weblogs and Social Me-
dia, pages 178?185.
Marilyn Walker, Pranav Anand, Rob Abbott, and Ricky
Grant. 2012a. Stance classification using dialogic
properties of persuasion. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 592?596.
1137
Marilyn A. Walker, Pranav Anand, Rob Abbott, Jean
E. Fox Tree, Craig Martell, and Joseph King. 2012b.
That is your evidence?: Classifying stance in on-
line political debate. Decision Support Systems,
53(4):719?729.
Yafang Wang, Bin Yang, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2011. Harvesting facts from
textual web sources by constrained label propaga-
tion. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Man-
agement, pages 837?846.
Yafang Wang, Maximilian Dylla, Marc Spaniol, and
Gerhard Weikum. 2012. Coupling label propaga-
tion and constraints for temporal fact extraction. In
Proceedings of the ACL 2012 Conference Short Pa-
pers, pages 233?237.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the 14th International Conference
on Machine Learning, pages 412?420.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propaga-
tion. Technical Report CMU-CALD-02-107, CMU
CALD.
1138
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 798?807,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Learning the Fine-Grained Information Status of Discourse Entities
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{altaf,vince}@hlt.utdallas.edu
Abstract
While information status (IS) plays a cru-
cial role in discourse processing, there have
only been a handful of attempts to automat-
ically determine the IS of discourse entities.
We examine a related but more challenging
task, fine-grained IS determination, which
involves classifying a discourse entity as
one of 16 IS subtypes. We investigate the
use of rich knowledge sources for this task
in combination with a rule-based approach
and a learning-based approach. In experi-
ments with a set of Switchboard dialogues,
the learning-based approach achieves an ac-
curacy of 78.7%, outperforming the rule-
based approach by 21.3%.
1 Introduction
A linguistic notion central to discourse processing
is information status (IS). It describes the extent
to which a discourse entity, which is typically re-
ferred to by noun phrases (NPs) in a dialogue, is
available to the hearer. Different definitions of IS
have been proposed over the years. In this paper,
we adopt Nissim et als (2004) proposal, since it
is primarily built upon Prince?s (1992) and Eck-
ert and Strube?s (2001) well-known definitions,
and is empirically shown by Nissim et alto yield
an annotation scheme for IS in dialogue that has
good reproducibility.1
Specifically, Nissim et al(2004) adopt a three-
way classification scheme for IS, defining a dis-
course entity as (1) old to the hearer if it is known
to the hearer and has previously been referred to in
the dialogue; (2) new if it is unknown to her and
1It is worth noting that several IS annotation schemes
have been proposed more recently. See Go?tze et al(2007)
and Riester et al(2010) for details.
has not been previously referred to; and (3) me-
diated (henceforth med) if it is newly mentioned
in the dialogue but she can infer its identity from
a previously-mentioned entity. To capture finer-
grained distinctions for IS, Nissim et alallow an
old or med entity to have a subtype, which subcat-
egorizes an old or med entity. For instance, a med
entity has the subtype set if the NP that refers to
it is in a set-subset relation with its antecedent.
IS plays a crucial role in discourse processing:
it provides an indication of how a discourse model
should be updated as a dialogue is processed in-
crementally. Its importance can be reflected in
part in the amount of attention it has received in
theoretical linguistics over the years (e.g., Halli-
day (1976), Prince (1981), Hajic?ova? (1984), Vall-
duv?? (1992), Steedman (2000)), and in part in the
benefits it can potentially bring to NLP applica-
tions. One task that could benefit from knowledge
of IS is identity coreference: since new entities by
definition have not been previously referred to, an
NP marked as new does not need to be resolved,
thereby improving the precision of a coreference
resolver. Knowledge of fine-grained or subcat-
egorized IS is valuable for other NLP tasks. For
instance, an NP marked as set signifies that it is in
a set-subset relation with its antecedent, thereby
providing important clues for bridging anaphora
resolution (e.g., Gasperin and Briscoe (2008)).
Despite the potential usefulness of IS in NLP
tasks, there has been little work on learning
the IS of discourse entities. To investigate the
plausibility of learning IS, Nissim et al(2004)
annotate a set of Switchboard dialogues with
such information2 , and subsequently present a
2These and other linguistic annotations on the Switch-
board dialogues were later released by the LDC as part of the
NXT corpus, which is described in Calhoun et al(2010).
798
rule-based approach and a learning-based ap-
proach to acquiring such knowledge (Nissim,
2006). More recently, we have improved Nissim?s
learning-based approach by augmenting her fea-
ture set, which comprises seven string-matching
and grammatical features, with lexical and syn-
tactic features (Rahman and Ng, 2011; hence-
forth R&N). Despite the improvements, the per-
formance on new entities remains poor: an F-
score of 46.5% was achieved.
Our goal in this paper is to investigate fine-
grained IS determination, the task of classifying
a discourse entity as one of the 16 IS subtypes
defined by Nissim et al(2004).3 Owing in part
to the increase in the number of categories, fine-
grained IS determination is arguably a more chal-
lenging task than the 3-class IS determination task
that Nissim and R&N investigated. To our knowl-
edge, this is the first empirical investigation of au-
tomated fine-grained IS determination.
We propose a knowledge-rich approach to fine-
grained IS determination. Our proposal is moti-
vated in part by Nissim?s and R&N?s poor per-
formance on new entities, which we hypothesize
can be attributed to their sole reliance on shallow
knowledge sources. In light of this hypothesis,
our approach employs semantic and world knowl-
edge extracted from manually and automatically
constructed knowledge bases, as well as corefer-
ence information. The relevance of coreference to
IS determination can be seen from the definition
of IS: a new entity is not coreferential with any
previously-mentioned entity, whereas an old en-
tity may. While our use of coreference informa-
tion for IS determination and our earlier claim that
IS annotation would be useful for coreference res-
olution may seem to have created a chicken-and-
egg problem, they do not: since coreference reso-
lution and IS determination can benefit from each
other, it may be possible to formulate an approach
where the two tasks can mutually bootstrap.
We investigate rule-based and learning-based
approaches to fine-grained IS determination. In
the rule-based approach, we manually compose
rules to combine the aforementioned knowledge
sources. While we could employ the same knowl-
edge sources in the learning-based approach, we
chose to encode, among other knowledge sources,
3One of these 16 classes is the new type, for which no
subtype is defined. For ease of exposition, we will refer to
the new type as one of the 16 subtypes to be predicted.
the hand-written rules and their predictions di-
rectly as features for the learner. In an evalua-
tion on 147 Switchboard dialogues, our learning-
based approach to fine-grained IS determina-
tion achieves an accuracy of 78.7%, substan-
tially outperforming the rule-based approach by
21.3%. Equally importantly, when employing
these linguistically rich features to learn Nissim?s
3-class IS determination task, the resulting classi-
fier achieves an accuracy of 91.7%, surpassing the
classifier trained on R&N?s state-of-the-art fea-
ture set by 8.8% in absolute accuracy. Improve-
ments on the new class are particularly substan-
tial: its F-score rises from 46.7% to 87.2%.
2 IS Types and Subtypes: An Overview
In Nissim et als (2004) IS classification scheme,
an NP can be assigned one of three main types
(old, med, new) and one of 16 subtypes. Below
we will illustrate their definitions with examples,
most of which are taken from Nissim (2003) or
Nissim et als (2004) dataset (see Section 3).
Old. An NP is marked is old if (i) it is corefer-
ential with an entity introduced earlier, (ii) it is a
generic pronoun, or (iii) it is a personal pronoun
referring to the dialogue participants. Six sub-
types are defined for old entities: identity, event,
general, generic, ident generic, and relative. In
Example 1, my is marked as old with subtype
identity, since it is coreferent with I.
(1) I was angry that he destroyed my tent.
However, if the markable has a verb phrase (VP)
rather than an NP as its antecedent, it will be
marked as old/event, as can be seen in Example
2, where the antecedent of That is the VP put my
phone number on the form.
(2) They ask me to put my phone number
on the form. That I think is not needed.
Other NPs marked as old include (i) relative
pronouns, which have the subtype relative; (ii)
personal pronouns referring to the dialogue par-
ticipants, which have the subtype general, and
(iii) generic pronouns, which have the subtype
generic. The pronoun you in Example 3 is an in-
stance of a generic pronoun.
(3) I think to correct the judicial system,
you have to get the lawyer out of it.
Note, however, that in a coreference chain of
generic pronouns, every element of the chain is
799
assigned the subtype ident generic instead.
Mediated. An NP is marked as med if the en-
tity it refers to has not been previously introduced
in the dialogue, but can be inferred from already-
mentioned entities or is generally known to the
hearer. Nine subtypes are available for med en-
tities: general, bound, part, situation, event, set,
poss, func value, and aggregation.
General is assigned to med entities that are
generally known, such as the Earth, China, and
most proper names. Bound is reserved for bound
pronouns, an instance of which is shown in Ex-
ample 4, where its is bound to the variable of the
universally quantified NP, Every cat.
(4) Every cat ate its dinner.
Poss is assigned to NPs involved in intra-phrasal
possessive relations, including prenominal geni-
tives (i.e., X?s Y) and postnominal genitives (i.e.,
Y of X). Specifically, Y will be marked as poss if
X is old or med; otherwise, Y will be new. For ex-
ample, in cases like a friend?s boat where a friend
is new, boat is marked as new.
Four subtypes, namely part, situation, event,
and set, are used to identify instances of bridg-
ing (i.e., entities that are inferrable from a related
entity mentioned earlier in the dialogue). As an
example, consider the following sentences:
(5a) He passed by the door of Jan?s house
and saw that the door was painted red.
(5b) He passed by Jan?s house and saw that
the door was painted red.
In Example 5a, by the time the hearer processes
the second occurrence of the door, she has already
had a mental entity corresponding to the door (af-
ter processing the first occurrence). As a result,
the second occurrence of the door refers to an
old entity. In Example 5b, on the other hand, the
hearer is not assumed to have any mental repre-
sentation of the door in question, but she can in-
fer that the door she saw was part of Jan?s house.
Hence, this occurrence of the door should be
marked as med with subtype part, as it is involved
in a part-whole relation with its antecedent.
If an NP is involved in a set-subset relation with
its antecedent, it inherits the med subtype set.
This applies to the NP the house payment in Ex-
ample 6, whose antecedent is our monthly budget.
(6) What we try to do to stick to our
monthly budget is we pretty much have
the house payment.
If an NP is part of a situation set up by a
previously-mentioned entity, it is assigned the
subtype situation, as exemplified by the NP a few
horses in the sentence below, which is involved in
the situation set up by John?s ranch.
(7) Mary went to John?s ranch and saw that
there were only a few horses.
Similar to old entities, an NP marked as med may
be related to a previously mentioned VP. In this
case, the NP will receive the subtype event, as ex-
emplified by the NP the bus in the sentence below,
which is triggered by the VP traveling in Miami.
(8) We were traveling in Miami, and the
bus was very full.
If an NP refers to a value of a previously men-
tioned function, such as the NP 30 degrees in Ex-
ample 9, which is related to the temperature, then
it is assigned the subtype func value.
(9) The temperature rose to 30 degrees.
Finally, the subtype aggregation is assigned to co-
ordinated NPs if at least one of the NPs involved
is not new. However, if all NPs in the coordinated
phrase are new, the phrase should be marked as
new. For instance, the NP My son and I in Exam-
ple 10 should be marked as med/aggregation.
(10) I have a son ... My son and I like to
play chess after dinner.
New. An entity is new if it has not been intro-
duced in the dialogue and the hearer cannot infer
it from previously mentioned entities. No subtype
is defined for new entities.
There are cases where more than one IS value
is appropriate for a given NP. For instance, given
two occurrences of China in a dialogue, the sec-
ond occurrence can be labeled as old/identity (be-
cause it is coreferential with an earlier NP) or
med/general (because it is a generally known
entity). To break ties, Nissim (2003) define a
precedence relation on the IS subtypes, which
yields a total ordering on the subtypes. Since
all the old subtypes are ordered before their med
counterparts in this relation, the second occur-
rence of China in our example will be labeled as
old/identity. Owing to space limitations, we refer
the reader to Nissim (2003) for details.
3 Dataset
We employ Nissim et als (2004) dataset, which
comprises 147 Switchboard dialogues. We parti-
800
tion them into a training set (117 dialogues) and a
test set (30 dialogues). A total of 58,835 NPs are
annotated with IS types and subtypes.4 The distri-
butions of NPs over the IS subtypes in the training
set and the test set are shown in Table 1.
Train (%) Test (%)
old/identity 10236 (20.1) 1258 (15.8)
old/event 1943 (3.8) 290 (3.6)
old/general 8216 (16.2) 1129 (14.2)
old/generic 2432 (4.8) 427 (5.4)
old/ident generic 1730 (3.4) 404 (5.1)
old/relative 1241 (2.4) 193 (2.4)
med/general 2640 (5.2) 325 (4.1)
med/bound 529 (1.0) 74 (0.9)
med/part 885 (1.7) 120 (1.5)
med/situation 1109 (2.2) 244 (3.1)
med/event 351 (0.7) 67 (0.8)
med/set 10282 (20.2) 1771 (22.3)
med/poss 1318 (2.6) 220 (2.8)
med/func value 224 (0.4) 31 (0.4)
med/aggregation 580 (1.1) 117 (1.5)
new 7158 (14.1) 1293 (16.2)
total 50874 (100) 7961 (100)
Table 1: Distributions of NPs over IS subtypes. The
corresponding percentages are parenthesized.
4 Rule-Based Approach
In this section, we describe our rule-based ap-
proach to fine-grained IS determination, where we
manually design rules for assigning IS subtypes to
NPs based on the subtype definitions in Section 2,
Nissim?s (2003) IS annotation guidelines, and our
inspection of the IS annotations in the training
set. The motivations behind having a rule-based
approach are two-fold. First, it can serve as a
baseline for fine-grained IS determination. Sec-
ond, it can provide insight into how the available
knowledge sources can be combined into predic-
tion rules, which can potentially serve as ?sophis-
ticated? features for a learning-based approach.
As shown in Table 2, our ruleset is composed of
18 rules, which should be applied to an NP in the
order in which they are listed. Rules 1?7 handle
the assignment of old subtypes to NPs. For in-
stance, Rule 1 identifies instances of old/general,
which comprises the personal pronouns referring
4Not all NPs have an IS type/subtype. For instance, a
pleonastic ?it? does not refer to any real-world entity and
therefore does not have any IS, and so are nouns such as
?course? in ?of course?, ?accident? in ?by accident?, etc.
to the dialogue participants. Note that this and
several other rules rely on coreference informa-
tion, which we obtain from two sources: (1)
chains generated automatically using the Stan-
ford Deterministic Coreference Resolution Sys-
tem (Lee et al 2011)5, and (2) manually iden-
tified coreference chains taken directly from the
annotated Switchboard dialogues. Reporting re-
sults using these two ways of obtaining chains fa-
cilitates the comparison of the IS determination
results that we can realistically obtain using ex-
isting coreference technologies against those that
we could obtain if we further improved exist-
ing coreference resolvers. Note that both sources
provide identity coreference chains. Specifically,
the gold chains were annotated for NPs belong-
ing to old/identity and old/ident generic. Hence,
these chains can be used to distinguish between
old/general NPs and old/ident generic NPs, be-
cause the former are not part of a chain whereas
the latter are. However, they cannot be used
to distinguish between old/general entities and
old/generic entities, since neither of them belongs
to any chains. As a result, when gold chains are
used, Rule 1 will classify all occurrences of ?you?
that are not part of a chain as old/general, regard-
less of whether the pronoun is generic. While the
gold chains alone can distinguish old/general and
old/ident generic NPs, the Stanford chains can-
not distinguish any of the old subtypes in the ab-
sence of other knowledge sources, since it gener-
ates chains for all old NPs regardless of their sub-
types. This implies that Rule 1 and several other
rules are only a very crude approximation of the
definition of the corresponding IS subtypes.
The rules for the remaining old subtypes can be
interpreted similarly. A few points deserve men-
tion. First, many rules depend on the string of
the NP under consideration (e.g., ?they? in Rule 2
and ?whatever? in Rule 4). The decision of which
strings are chosen is based primarily on our in-
spection of the training data. Hence, these rules
are partly data-driven. Second, these rules should
be applied in the order in which they are shown.
For instance, though not explicitly stated, Rule 3
is only applicable to the non-anaphoric ?you? and
?they? pronouns, since Rule 2 has already covered
their anaphoric counterparts. Finally, Rule 7 uses
non-anaphoricity as a test of old/event NPs. The
5The Stanford resolver is available from http://nlp.
stanford.edu/software/corenlp.shtml.
801
1. if the NP is ?I? or ?you? and it is not part of a coreference chain, then
subtype := old/general
2. if the NP is ?you? or ?they? and it is anaphoric, then
subtype := old/ident generic
3. if the NP is ?you? or ?they?, then
subtype := old/generic
4. if the NP is ?whatever? or an indefinite pronoun prefixed by ?some? or ?any? (e.g., ?somebody?), then
subtype := old/generic
5. if the NP is an anaphoric pronoun other than ?that?, or its string is identical to that of a preceding NP, then
subtype := old/ident
6. if the NP is ?that? and it is coreferential with the immediately preceding word, then
subtype := old/relative
7. if the NP is ?it?, ?this? or ?that?, and it is not anaphoric, then
subtype := old/event
8. if the NP is pronominal and is not anaphoric, then
subtype := med/bound
9. if the NP contains ?and? or ?or?, then
subtype := med/aggregation
10. if the NP is a multi-word phrase that (1) begins with ?so much?, ?something?, ?somebody?, ?someone?,
?anything?, ?one?, or ?different?, or (2) has ?another?, ?anyone?, ?other?, ?such?, ?that?, ?of? or ?type?
as neither its first nor last word, or (3) its head noun is also the head noun of a preceding NP, then
subtype := med/set
11. if the NP contains a word that is a hyponym of the word ?value? in WordNet, then
subtype := med/func value
12. if the NP is involved in a part-whole relation with a preceding NP based on information extracted from
ReVerb?s output, then
subtype := med/part
13. if the NP is of the form ?X?s Y? or ?poss-pro Y?, where X and Y are NPs and poss-pro is a possessive
pronoun, then
subtype := med/poss
14. if the NP fills an argument of a FrameNet frame set up by a preceding NP or verb, then
subtype := med/situation
15. if the head of the NP and one of the preceding verbs in the same sentence share the same WordNet
hypernym which is not in synsets that appear one of the top five levels of the noun/verb hierarchy, then
subtype := med/event
16. if the NP is a named entity (NE) or starts with ?the?, then
subtype := med/general
17. if the NP appears in the training set, then
subtype := its most frequent IS subtype in the training set
18. subtype := new
Table 2: Hand-crafted rules for assigning IS subtypes to NPs.
reason is that these NPs have VP antecedents, but
both the gold chains and the Stanford chains are
computed over NPs only.
Rules 8?16 concern med subtypes. Apart from
Rule 8 (med/bound), Rule 9 (med/aggregation),
and Rule 11 (med/func value), which are arguably
crude approximations of the definitions of the
corresponding subtypes, the med rules are more
complicated than their old counterparts, in part
because of their reliance on the extraction of so-
phisticated knowledge. Below we describe the ex-
traction process and the motivation behind them.
Rule 10 concerns med/set. The words and
phrases listed in the rule, which are derived manu-
ally from the training data, provide suggestive ev-
idence that the NP under consideration is a subset
or a specific portion of an entity or concept men-
tioned earlier in the dialogue. Examples include
?another bedroom?, ?different color?, ?somebody
else?, ?any place?, ?one of them?, and ?most other
cities?. Condition 3 of the rule, which checks
whether the head noun of the NP has been men-
tioned previously, is a good test for identity coref-
erence, but since all the old entities have suppos-
802
edly been identified by the preceding rules, it be-
comes a reasonable test for set-subset relations.
For convenience, we identify part-whole rela-
tions in Rule 12 based on the output produced by
ReVerb (Fader et al 2011), an open information
extraction system.6 The output contains, among
other things, relation instances, each of which is
represented as a triple, <A,rel,B>, where rel is
a relation, and A and B are its arguments. To pre-
process the output, we first identify all the triples
that are instances of the part-whole relation us-
ing regular expressions. Next, we create clusters
of relation arguments, such that each pair of ar-
guments in a cluster has a part-whole relation.
This is easy: since part-whole is a transitive rela-
tion (i.e., <A,part,B> and <B,part,C> implies
<A,part,C>), we cluster the arguments by taking
the transitive closure of these relation instances.
Then, given an NP NPi in the test set, we assign
med/part to it if there is a preceding NP NPj such
that the two NPs are in the same argument cluster.
In Rule 14, we use FrameNet (Baker et al
1998) to determine whether med/situation should
be assigned to an NP, NPi. Specifically, we check
whether it fills an argument of a frame set up by
a preceding NP, NPj , or verb. To exemplify, let
us assume that NPj is ?capital punishment?. We
search for ?punishment? in FrameNet to access
the appropriate frame, which in this case is ?re-
wards and punishments?. This frame contains a
list of arguments together with examples. If NPi is
one of these arguments, we assign med/situation
to NPi, since it is involved in a situation (described
by a frame) that is set up by a preceding NP/verb.
In Rule 15, we use WordNet (Fellbaum, 1998)
to determine whether med/event should be as-
signed to an NP, NPi, by checking whether NPi is
related to an event, which is typically described
by a verb. Specifically, we use WordNet to check
whether there exists a verb, v, preceding NPi such
that v and NPi have the same hypernym. If so, we
assign NPi the subtype med/event. Note that we
ensure that the hypernym they share does not ap-
pear in the top five levels of the WordNet noun
and verb hierarchies, since we want them to be
related via a concept that is not overly general.
Rule 16 identifies instances of med/general.
The majority of its members are generally-known
6We use ReVerb ClueWeb09 Extractions 1.1, which
is available from http://reverb.cs.washington.
edu/reverb_clueweb_tuples-1.1.txt.gz.
entities, whose identification is difficult as it re-
quires world knowledge. Consequently, we apply
this rule only after all other med rules are applied.
As we can see, the rule assigns med/general to
NPs that are named entities (NEs) and definite de-
scriptions (specifically those NPs that start with
?the?). The reason is simple. Most NEs are gener-
ally known. Definite descriptions are typically not
new, so it seems reasonable to assign med/general
to them given that the remaining (i.e., unlabeled)
NPs are presumably either new and med/general.
Before Rule 18, which assigns an NP to the new
class by default, we have a ?memorization? rule
that checks whether the NP under consideration
appears in the training set (Rule 17). If so, we
assign to it its most frequent subtype based on its
occurrences in the training set. In essence, this
heuristic rule can help classify some of the NPs
that are somehow ?missed? by the first 16 rules.
The ordering of these rules has a direct impact
on performance of the ruleset, so a natural ques-
tion is: what criteria did we use to order the rules?
We order them in such a way that they respect the
total ordering on the subtypes imposed by Nis-
sim?s (2003) preference relation (see Section 3),
except that we give med/general a lower priority
than Nissim due to the difficulty involved in iden-
tifying generally known entities, as noted above.
5 Learning-Based Approach
In this section, we describe our learning-based ap-
proach to fine-grained IS determination. Since
we aim to automatically label an NP with its IS
subtype, we create one training/test instance from
each hand-annotated NP in the training/test set.
Each instance is represented using five types of
features, as described below.
Unigrams (119704). We create one binary fea-
ture for each unigram appearing in the training
set. Its value indicates the presence or absence
of the unigram in the NP under consideration.
Markables (209751). We create one binary fea-
ture for each markable (i.e., an NP having an IS
subtype) appearing in the training set. Its value is
1 if and only if the markable has the same string
as the NP under consideration.
Markable predictions (17). We create 17 bi-
nary features, 16 of which correspond to the 16
IS subtypes and the remaining one corresponds to
a ?dummy subtype?. Specifically, if the NP un-
803
der consideration appears in the training set, we
use Rule 17 in our hand-crafted ruleset to deter-
mine the IS subtype it is most frequently associ-
ated with in the training set, and then set the value
of the feature corresponding to this IS subtype to
1. If the NP does not appear in the training set, we
set the value of the dummy subtype feature to 1.
Rule conditions (17). As mentioned before, we
can create features based on the hand-crafted rules
in Section 4. To describe these features, let us in-
troduce some notation. Let Rule i be denoted by
Ai ?? Bi, where Ai is the condition that must
be satisfied before the rule can be applied and Bi
is the IS subtype predicted by the rule. We could
create one binary feature from each Ai, and set its
value to 1 if Ai is satisfied by the NP under con-
sideration. These features, however, fail to cap-
ture a crucial aspect of the ruleset: the ordering of
the rules. For instance, Rule i should be applied
only if the conditions of the first i?1 rules are not
satisfied by the NP, but such ordering is not en-
coded in these features. To address this problem,
we capture rule ordering information by defining
binary feature fi as ?A1??A2? . . .?Ai?1?Ai,
where 1 ? i ? 16. In addition, we define a fea-
ture, f18, for the default rule (Rule 18) in a simi-
lar fashion, but since it does not have any condi-
tion, we simply define f18 as ?A1 ? . . . ? ?A16.
The value of a feature in this feature group is 1
if and only if the NP under consideration satis-
fies the condition defined by the feature. Note that
we did not create any features from Rule 17 here,
since we have already generated ?markables? and
?markable prediction? features for it.
Rule predictions (17). None of the features fi?s
defined above makes use of the predictions of our
hand-crafted rules (i.e., the Bi?s). To make use
of these predictions, we define 17 binary features,
one for each Bi, where i = 1, . . . , 16, 18. Specif-
ically, the value of the feature corresponding to
Bi is 1 if and only if fi is 1, where fi is a ?rule
condition? feature as defined above.
Since IS subtype determination is a 16-class
classification problem, we train a multi-class
SVM classifier on the training instances using
SVMmulticlass (Tsochantaridis et al 2004), and
use it to make predictions on the test instances.7
7For all the experiments involving SVMmulticlass, we
set C, the regularization parameter, to 500,000, since pre-
liminary experiments indicate that preferring generalization
6 Evaluation
Next, we evaluate the rule-based approach and
the learning-based approach to determining the IS
subtype of each hand-annotated NP in the test set.
Classification results. Table 3 shows the results
of the two approaches. Specifically, row 1 shows
their accuracy, which is defined as the percent-
age of correctly classified instances. For each
approach, we present results that are generated
based on gold coreference chains as well as auto-
matic chains computed by the Stanford resolver.
As we can see, the rule-based approach
achieves accuracies of 66.0% (gold coreference)
and 57.4% (Stanford coreference), whereas the
learning-based approach achieves accuracies of
86.4% (gold) and 78.7% (Stanford). In other
words, the gold coreference results are better than
the Stanford coreference results, and the learning-
based results are better than the rule-based results.
While perhaps neither of these results are surpris-
ing, we are pleasantly surprised by the extent to
which the learned classifier outperforms the hand-
crafted rules: accuracies increase by 20.4% and
21.3% when gold coreference and Stanford coref-
erence are used, respectively. In other words, ma-
chine learning has ?transformed? a ruleset that
achieves mediocre performance into a system that
achieves relatively high performance.
These results also suggest that coreference
plays a crucial role in IS subtype determination:
accuracies could increase by up to 7.7?8.6% if
we solely improved coreference resolution perfor-
mance. This is perhaps not surprising: IS and
coreference can mutually benefit from each other.
To gain additional insight into the task, we also
show in rows 2?17 of Table 3 the performance
on each of the 16 subtypes, expressed in terms of
recall (R), precision (P), and F-score (F). A few
points deserve mention. First, in comparison to
the rule-based approach, the learning-based ap-
proach achieves considerably better performance
on almost all classes. One that is of particular in-
terest is the new class. As we can see in row 17,
its F-score rises by about 30 points. These gains
are accompanied by a simultaneous rise in recall
and precision. In particular, recall increases by
about 40 points. Now, recall from the introduc-
to overfitting (by setting C to a small value) tends to yield
poorer classification performance. The remaining learning
parameters are set to their default values.
804
Rule-Based Approach Learning-Based Approach
Gold Coreference Stanford Coreference Gold Coreference Stanford Coreference
1 Accuracy 66.0 57.4 86.4 78.7
IS Subtype R P F R P F R P F R P F
2 old/ident 77.5 78.2 77.8 66.1 52.7 58.7 82.8 85.2 84.0 75.8 64.2 69.5
3 old/event 98.6 50.4 66.7 71.3 43.2 53.8 98.3 87.9 92.8 2.4 31.8 4.5
4 old/general 81.9 82.7 82.3 72.3 83.6 77.6 97.7 93.7 95.6 87.8 92.7 90.2
5 old/generic 55.9 55.2 55.5 39.2 39.8 39.5 76.1 87.3 81.3 39.9 85.9 54.5
6 old/ident generic 48.7 77.7 59.9 27.2 51.8 35.7 57.1 87.5 69.1 47.2 44.8 46.0
7 old/relative 55.0 69.2 61.3 55.1 63.4 59.0 98.0 63.0 76.7 99.0 37.5 54.4
8 med/general 29.9 19.8 23.8 29.5 19.6 23.6 91.2 87.7 89.4 84.0 72.2 77.7
9 med/bound 56.4 20.5 30.1 56.4 20.5 30.1 25.7 65.5 36.9 2.7 40.0 5.1
10 med/part 19.5 100.0 32.7 19.5 100.0 32.7 73.2 96.8 83.3 73.2 96.8 83.3
11 med/situation 28.7 100.0 44.6 28.7 100.0 44.6 68.4 95.4 79.7 68.0 97.7 80.2
12 med/event 10.5 100.0 18.9 10.5 100.0 18.9 46.3 100.0 63.3 46.3 100.0 63.3
13 med/set 82.9 61.8 70.8 78.0 59.4 67.4 90.4 87.8 89.1 88.4 86.0 87.2
14 med/poss 52.9 86.0 65.6 52.9 86.0 65.6 93.2 92.4 92.8 90.5 97.6 93.9
15 med/func value 81.3 74.3 77.6 81.3 74.3 77.6 88.1 85.9 87.0 88.1 85.9 87.0
16 med/aggregation 57.4 44.0 49.9 57.4 43.6 49.6 85.2 72.9 78.6 83.8 93.9 88.6
17 new 50.4 65.7 57.0 50.3 65.1 56.7 90.3 84.6 87.4 90.4 83.6 86.9
Table 3: IS subtype accuracies and F-scores. In each row, the strongest result, as well as those that are statistically
indistinguishable from it according to the paired t-test (p < 0.05), are boldfaced.
tion that previous attempts on 3-class IS determi-
nation by Nissim and R&N have achieved poor
performance on the new class. We hypothesize
that the use of shallow features in their approaches
were responsible for the poor performance they
observed, and that using our knowledge-rich fea-
ture set could improve its performance. We will
test this hypothesis at the end of this section.
Other subtypes that are worth discussing
are med/aggregation, med/func value, and
med/poss. Recall that the rules we designed for
these classes were only crude approximations, or,
perhaps more precisely, simplified versions of the
definitions of the corresponding subtypes. For
instance, to determine whether an NP belongs to
med/aggregation, we simply look for occurrences
of ?and? and ?or? (Rule 9), whereas its definition
requires that not all of the NPs in the coordinated
phrase are new. Despite the over-simplicity
of these rules, machine learning has enabled
the available features to be combined in such a
way that high performance is achieved for these
classes (see rows 14?16).
Also worth examining are those classes for
which the hand-crafted rules rely on sophisti-
cated knowledge sources. They include med/part,
which relies on ReVerb; med/situation, which re-
lies on FrameNet; and med/event, which relies on
WordNet. As we can see from the rule-based re-
sults (rows 10?12), these knowledge sources have
yielded rules that achieved perfect precision but
low recall: 19.5% for part, 28.7% for situation,
and 10.5 for event. Nevertheless, the learning
algorithm has again discovered a profitable way
to combine the available features, enabling the F-
scores of these classes to increase by 35.1?50.6%.
While most classes are improved by machine
learning, the same is not true for old/event and
med/bound, whose F-scores are 4.5% (row 3) and
5.1% (row 9), respectively, when Stanford coref-
erence is employed. This is perhaps not surpris-
ing. Recall that the multi-class SVM classifier
was trained to maximize classification accuracy.
Hence, if it encounters a class that is both difficult
to learn and is under-represented, it may as well
aim to achieve good performance on the easier-
to-learn, well-represented classes at the expense
of these hard-to-learn, under-represented classes.
Feature analysis. In an attempt to gain addi-
tional insight into the performance contribution
of each of the five types of features used in the
learning-based approach, we conduct feature ab-
lation experiments. Results are shown in Table 4,
where each row shows the accuracy of the classi-
fier trained on all types of features except for the
one shown in that row. For easy reference, the
accuracy of the classifier trained on all types of
features is shown in row 1 of the table. According
to the paired t-test (p < 0.05), performance drops
significantly whichever feature type is removed.
This suggests that all five feature types are con-
tributing positively to overall accuracy. Also, the
markables features are the least important in the
presence of other feature groups, whereas mark-
805
Feature Type Gold Coref Stanford Coref
All features 86.4 78.7
?rule predictions 77.5 70.0
?markable predictions 72.4 64.7
?rule conditions 81.1 71.0
?unigrams 74.4 58.6
?markables 83.2 75.5
Table 4: Accuracies of feature ablation experiments.
Feature Type Gold Coref Stanford Coref
rule predictions 49.1 45.2
markable predictions 39.7 39.7
rule conditions 58.1 28.9
unigrams 56.8 56.8
markables 10.4 10.4
Table 5: Accuracies of classifiers for each feature type.
able predictions and unigrams are the two most
important feature groups.
To get a better idea of the utility of each feature
type, we conduct another experiment in which we
train five classifiers, each of which employs ex-
actly one type of features. The accuracies of these
classifiers are shown in Table 5. As we can see,
the markables features have the smallest contribu-
tion, whereas unigrams have the largest contribu-
tion. Somewhat interesting are the results of the
classifiers trained on the rule conditions: the rules
are far more effective when gold coreference is
used. This can be attributed to the fact that the
design of the rules was based in part on the defini-
tions of the subtypes, which assume the availabil-
ity of perfect coreference information.
Knowledge source analysis. To gain some in-
sight into the extent to which a knowledge source
or a rule contributes to the overall performance of
the rule-based approach, we conduct ablation ex-
periments: in each experiment, we measure the
performance of the ruleset after removing a par-
ticular rule or knowledge source from it. Specifi-
cally, rows 2?4 of Table 6 show the accuracies of
the ruleset after removing the memorization rule
(Rule 17), the rule that uses ReVerb?s output (Rule
12), and the cue words used in Rules 4 and 10,
respectively. For easy reference, the accuracy of
the original ruleset is shown in row 1 of the ta-
ble. According to the paired t-test (p < 0.05),
performance drops significantly in all three abla-
tion experiments. This suggests that the memo-
rization rule, ReVerb, and the cue words all con-
tribute positively to the accuracy of the ruleset.
Feature Type Gold Coref Stanford Coref
All rules 66.0 57.4
?memorization 62.6 52.0
?ReVerb 64.2 56.6
?cue words 63.8 54.0
Table 6: Accuracies of the simplified ruleset.
R&N?s Features Our Features
IS Type R P F R P F
old 93.5 95.8 94.6 93.8 96.4 95.1
med 89.3 71.2 79.2 93.3 86.0 89.5
new 34.6 71.7 46.7 82.4 72.7 87.2
Accuracy 82.9 91.7
Table 7: Accuracies on IS types.
IS type results. We hypothesized earlier that
the poor performance reported by Nissim and
R&N on identifying new entities in their 3-class
IS classification experiments (i.e., classifying an
NP as old, med, or new) could be attributed to
their sole reliance on lexico-syntactic features. To
test this hypothesis, we (1) train a 3-class classi-
fier using the five types of features we employed
in our learning-based approach, computing the
features based on the Stanford coreference chains;
and (2) compare its results against those obtained
via the lexico-syntactic approach in R&N on our
test set. Results of these experiments, which are
shown in Table 7, substantiate our hypothesis:
when we replace R&N?s features with ours, accu-
racy rises from 82.9% to 91.7%. These gains can
be attributed to large improvements in identifying
new and med entities, for which F-scores increase
by about 40 points and 10 points, respectively.
7 Conclusions
We have examined the fine-grained IS determi-
nation task. Experiments on a set of Switch-
board dialogues show that our learning-based ap-
proach, which uses features that include hand-
crafted rules and their predictions, outperforms its
rule-based counterpart by more than 20%, achiev-
ing an overall accuracy of 78.7% when relying on
automatically computed coreference information.
In addition, we have achieved state-of-the-art re-
sults on the 3-class IS determination task, in part
due to our reliance on richer knowledge sources
in comparison to prior work. To our knowledge,
there has been little work on automatic IS subtype
determination. We hope that our work can stimu-
late further research on this task.
806
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier
draft of the paper. This work was supported
in part by NSF Grants IIS-0812261 and IIS-
1147644.
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project.
In Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics and the
17th International Conference on Computational
Linguistics, Volume 1, pages 86?90.
Sasha Calhoun, Jean Carletta, Jason Brenier, Neil
Mayo, Dan Jurafsky, Mark Steedman, and David
Beaver. 2010. The NXT-format Switchboard cor-
pus: A rich resource for investigating the syntax, se-
mantics, pragmatics and prosody of dialogue. Lan-
guage Resources and Evaluation, 44(4):387?419.
Miriam Eckert and Michael Strube. 2001. Dialogue
acts, synchronising units and anaphora resolution.
Journal of Semantics, 17(1):51?89.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1535?1545.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Caroline Gasperin and Ted Briscoe. 2008. Statisti-
cal anaphora resolution in biomedical texts. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics, pages 257?264.
Michael Go?tze, Thomas Weskott, Cornelia En-
driss, Ines Fiedler, Stefan Hinterwimmer, Svetlana
Petrova, Anne Schwarz, Stavros Skopeteas, and
Ruben Stoel. 2007. Information structure. In
Working Papers of the SFB632, Interdisciplinary
Studies on Information Structure (ISIS). Potsdam:
Universita?tsverlag Potsdam.
Eva Hajic?ova?. 1984. Topic and focus. In Contri-
butions to Functional Syntax, Semantics, and Lan-
guage Comprehension (LLSEE 16), pages 189?202.
John Benjamins, Amsterdam.
Michael A. K. Halliday. 1976. Notes on transitiv-
ity and theme in English. Journal of Linguistics,
3(2):199?244.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford?s multi-pass sieve corefer-
ence resolution system at the CoNLL-2011 shared
task. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 28?34.
Malvina Nissim, Shipra Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for
information status in dialogue. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation, pages 1023?1026.
Malvina Nissim. 2003. Annotation scheme
for information status in dialogue. Available
from http://www.stanford.edu/class/
cs224u/guidelines-infostatus.pdf.
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 94?102.
Ellen F. Prince. 1981. Toward a taxonomy of given-
new information. In P. Cole, editor, Radical Prag-
matics, pages 223?255. New York, N.Y.: Academic
Press.
Ellen F. Prince. 1992. The ZPG letter: Subjects,
definiteness, and information-status. In Discourse
Description: Diverse Analysis of a Fund Raising
Text, pages 295?325. John Benjamins, Philadel-
phia/Amsterdam.
Altaf Rahman and Vincent Ng. 2011. Learning the
information status of noun phrases in spoken dia-
logues. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1069?1080.
Arndt Riester, David Lorenz, and Nina Seemann.
2010. A recursive annotation scheme for referential
information status. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation, pages 717?722.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and struc-
tured output spaces. In Proceedings of the 21st
International Conference on Machine Learning,
pages 104?112.
Enric Vallduv??. 1992. The Informational Component.
Garland, New York.
807
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 720?730,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Translation-Based Projection for Multilingual Coreference Resolution
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{altaf,vince}@hlt.utdallas.edu
Abstract
To build a coreference resolver for a new
language, the typical approach is to first
coreference-annotate documents from this tar-
get language and then train a resolver on these
annotated documents using supervised learn-
ing techniques. However, the high cost asso-
ciated with manually coreference-annotating
documents needed by a supervised approach
makes it difficult to deploy coreference tech-
nologies across a large number of natural lan-
guages. To alleviate this corpus annotation
bottleneck, we examine a translation-based
projection approach to multilingual corefer-
ence resolution. Experimental results on two
target languages demonstrate the promise of
our approach.
1 Introduction
Noun phrase (NP) coreference resolution is the
task of determining which NPs (or mentions) refer
to each real-world entity in a document. Recent
years have witnessed a surge of interest in multilin-
gual coreference resolution. For instance, the ACE
2004/2005 evaluations and SemEval-2010 Shared
Task 1 have both involved coreference resolution in
multiple languages. As evidenced by the partici-
pants in these evaluations, the most common ap-
proach to building a resolver for a new language
is supervised, which involves training a resolver
on coreference-annotated documents from the tar-
get language. Although supervised approaches work
reasonably well, they present a challenge to deploy-
ing coreference technologies across a large number
of natural languages. Specifically, for each new lan-
guage of interest, one has to hire native speakers of
the language to go through the labor-intensive, time-
consuming process of hand-annotating a potentially
large number of documents with coreference anno-
tation before a supervised resolver can be trained.
One may argue that a potential solution to this
corpus annotation bottleneck is to employ an unsu-
pervised or heuristic approach to coreference resolu-
tion, especially in light of the fact that they have re-
cently started to rival their supervised counterparts.
However, by adopting these approaches, we are sim-
ply replacing the corpus annotation bottleneck by
another, possibly equally serious, bottleneck, the
knowledge acquisition bottleneck. Specifically, in
these approaches, one has to employ knowledge of
the target language to design coreference rules (e.g.,
Mitkov (1999), Poon and Domingos (2008), Raghu-
nathan et al (2010)) or sophisticated generative
models (e.g., Haghighi and Klein (2007,2010), Ng
(2008)) to combine the available knowledge sources.
One could argue that designing coreference
rules and generative models may not be as time-
consuming as annotating a large coreference corpus.
This may be true for a well-studied language like
English, where we can easily compose a rule that
disallows coreference between two mentions if they
disagree in number and gender, for instance. How-
ever, computing these features may not be as simple
as we hope for a language like Chinese: the lack of
morphology complicates the determination of num-
ber information, and the fact that most Chinese first
names are used by both genders makes gender deter-
mination difficult. The difficulty in accurately com-
puting features translates to difficulties in compos-
ing coreference rules: for example, the aforemen-
tioned rule involving gender and number agreement,
as well as rules that implement traditional linguistic
720
constraints on coreference, may no longer be accu-
rate and desirable to have if the features involved
cannot be accurately computed. Consequently, we
believe that research in multilingual coreference res-
olution will continue to be dominated by supervised
approaches.
Given the high cost of annotating data with coref-
erence chains, it is crucial to explore methods for
obtaining annotated data in a cost-effective manner.
Motivated in part by this observation, we examine
one such method that has recently shown promise
for a variety of NLP tasks, translation-based projec-
tion, which is composed of three steps. To coref-
erence annotate a text in the target language, we
(1) machine-translate it to a resource-rich language
(henceforth the source language); (2) automatically
produce the desired linguistic annotations (which in
our case are coreference annotations) on the trans-
lated text using the linguistic tool developed for the
source language (which in our case is a coreference
resolver) ; and (3) project the annotations from the
source language to the target language.
Unlike supervised approaches, this projection ap-
proach does not require any coreference-annotated
data from the target language. Equally importantly,
unlike its unsupervised counterparts, this approach
does not require that we have any linguistic knowl-
edge of the target language. In fact, we have no
knowledge of the target languages we employ in our
evaluation. One of our goals is to examine the fea-
sibility of building a coreference resolver for a lan-
guage for which we have no coreference-annotated
data and no linguistic knowledge of the language.
Recall that we view projection as an approach
for alleviating the corpus annotation bottleneck, not
as a solution to the multilingual coreference resolu-
tion problem. In fact, though rarely emphasized in
previous work on applying projection, we note that
projection alone cannot be used to solve multilin-
gual NLP problems, including coreference resolu-
tion. The reason is that every language has its own
idiosyncrasies with respect to linguistic properties,
and projection simply cannot produce annotations
capturing those properties that are specific to the tar-
get language. Our goal in this paper is to explore the
extent to which projection, which does not require
that we have any knowledge of the target language,
can push the limits of multilingual coreference res-
olution. If our results indicate that projection is a
promising approach, then the automatic coreference
annotations it produces can be used to augment the
manual annotations that capture the properties spe-
cific to the target language, thus alleviating the cor-
pus annotation bottleneck.
2 Related Work on Projection
The idea of projecting annotations from a resource-
rich language to a resource-scarce language was
originally proposed by Yarowsky and Ngai (2001)
and subsequently developed by others (e.g., Resnik
(2004), Hwa et al (2005)). These projection al-
gorithms assume as input a parallel corpus for the
source language and the target language. Given the
recent availability of machine translation (MT) ser-
vices on the Web, researchers have focused more
on translated-based projection rather than acquiring
a parallel corpus themselves. MT-based projection
has been applied to various NLP tasks, such as part-
of-speech tagging (e.g., Das and Petrov (2011)),
mention detection (e.g., Zitouni and Florian (2008)),
and sentiment analysis (e.g., Mihalcea et al (2007)).
There have been two initial attempts to apply pro-
jection to create coreference-annotated data for a
resource-poor language, both of which involve pro-
jecting hand-annotated coreference data from En-
glish to Romanian via a parallel corpus. Specifically,
Harabagiu and Maiorano (2000) create an English-
Romanian corpus by manually translating the MUC-
6 corpus into Romanian and manually project the
English annotations to Romanian. On the other
hand, Postolache et al (2006) apply a word align-
ment algorithm to project the hand-annotated En-
glish coreference chains and then manually fix the
projection errors on the Romanian side. Hence,
their goal is different from ours in at least two re-
spects. First, while they employ significant knowl-
edge of the target language to create a clean corefer-
ence corpus, we examine the quality of coreference-
annotated data created via an entirely automatic pro-
cess, determining quality by the performance of the
resolver trained on the data. Second, unlike ours,
neither of these attempts is at the level of defining
a technology for projection annotations that can po-
tentially be deployed across a large number of lan-
guages without coreference-annotated data.
721
3 Translation-Based Projection
Recall that our MT-based projection approach to
coreference resolution is composed of three steps.
Given a text in the target language, we (1) machine-
translate the text to the source language; (2) au-
tomatically produce coreference annotations on the
translated text using a coreference resolver devel-
oped for the source language; and (3) project the
annotations from the source language to the target
language. In this section, we employ our approach
in three settings, which differ in terms of the ex-
tent to which linguistic taggers (e.g., chunkers and
named entity (NE) recognizers) for the target lan-
guage are available. The goal is to examine whether
these linguistic taggers can be profitably exploited to
improve the performance of the projection approach.
Below we assume that English and French are our
source and target languages, respectively.
3.1 Setting 1: No French Taggers Available
In this setting, we assume that we do not have access
to any French tagger that we can exploit to improve
projection. Hence, all we can do is to employ the
three steps involved in the projection approach as
described at the beginning of this section to create
coreference-annotated data for French. Specifically,
we translate a French text to an English text using
GoogleTranslate1 , and create coreference chains for
the translated English text using Reconcile2 (Stoy-
anov et al, 2010). To project mentions from En-
glish to French, we first align the English and French
words in each pair of parallel sentences, and then
project the English mentions onto the French text us-
ing the alignment. However, since the alignment is
noisy, the French words to which the words in the
English mention are aligned may not form a con-
tiguous text span. To fix this problem, we follow
Yarowsky and Ngai (2001) and use the smallest text
span that covers all the aligned French words to cre-
ate the French mention.3 We process the English
mentions in the text in a left-to-right manner, as
processing the mentions sequentially enables us to
ensure that an English mention is not mapped to a
1See http://translate.google.com.
2See http://www.cs.utah.edu/nlp/reconcile.
We use the resolver pre-trained on the Wolverhampton corpus.
3Other methods for projecting mentions can be found in Pos-
tolache et al (2006), for example.
French text span that has already been mapped to by
a previously-processed English mention.4
To align English and French words, we trained
a word alignment model using GIZA++5 (Och and
Ney, 2000) on a parallel corpus comprising the
English-French section of Europarl6 (Koehn, 2005)
as well as all the French texts (and their translated
English counterparts) for which we want to auto-
matically create coreference chains. Following com-
mon practice, we stemmed the parallel corpus us-
ing the Porter stemmer (Porter, 1980) in order to
reduce data sparseness. However, even with stem-
ming, we found that many English words were not
aligned to any French words by the resulting align-
ment model. This would prevent many English men-
tions from being projected to the French side, poten-
tially harming the recall of the French coreference
annotations. To improve alignment coverage, we re-
trained the alignment model by supplying GIZA++
with an English-French bilingual dictionary that we
assembled using three online dictionary databases:
OmegaWiki, Wiktionary, and Universal Dictionary.
Furthermore, if a word w appears in both the English
side and the French side in a pair of parallel sen-
tences, we assume that it has the same orthographic
form in both languages and hence we augment the
bilingual dictionary with the entry (w, w).
Note that the use of a supervised resolver like
Reconcile does not render our approach supervised,
since we can replace it with any resolver, be it super-
vised, heuristic, or unsupervised. In other words, we
treat the resolver built for the source language as a
black box that can produce coreference annotations.
3.2 Setting 2: Mention Extractor Available
Next, we consider a comparatively less resource-
scarce setting where a French mention extractor is
available for identifying mentions in a French text7,
and describe how we can modify the projection ap-
proach to exploit this French mention extractor.
Given a French text we want to coreference-
4While we chose to process the mentions in a left-to-right
manner, any order of processing the mentions would work.
5See http://code.google.com/p/giza-pp/.
6See http://www.statmt.org/europarl/.
7Mention extraction is a term used in Automatic Content
Evaluation to refer to the task of determining the NPs that a
coreference system should consider in the resolution process.
722
annotate, we first translate it to English using
GoogleTranslate and align the French and English
words using a French-to-English word alignment
algorithm. Next, we identify the mentions in the
French text using the given mention extractor, and
project them onto the English text using the NP pro-
jection algorithm described in Setting 1. Finally, we
run Reconcile on the resulting English mentions to
generate coreference chains for the translated text,
and project these chains back to the French text.
As explained before, the performance of this
method is sensitive to the accuracy of the NP projec-
tion algorithm in recovering the English mentions,
which in turn depends on the accuracy of the word
alignment algorithm. To make this method more ro-
bust to noisy word alignment, we make a modifica-
tion to it. Rather than running Reconcile on the men-
tions produced by the NP projection algorithm, we
use Reconcile to identify the mentions directly from
the translated text. After that, we create a mapping
between the English mentions produced by the NP
projection algorithm and those produced by Recon-
cile using a small set of heuristics.
Specifically, let MP be the set of mentions identi-
fied by the NP projection algorithm and MR be the
set of mentions identified by Reconcile. For each
mention mP in MP , we map it to a mention in MR
that shares the same right boundary. If this fails, we
map it to a mention that covers its entire text span. If
this fails again, we map it to a mention that has a par-
tial overlap with it. If this still fails, we assume that
mP is not found by Reconcile and simply add mP to
MR. As before, we process the mentions in MP in
a left-to-right manner in order to ensure that no two
mentions in MP are mapped to the same Reconcile
mention. Finally, we discard all mentions in MR that
are not mapped by any mention in MP , and present
MR to Reconcile for coreference resolution. Since
we now have a 1-to-1 mapping between the Recon-
cile mentions and the French mentions, projecting
the coreference results back to French is trivial.
It may not be immediately clear why the exploita-
tion of the mention extractor in this setting may yield
better coreference annotations than those produced
in Setting 1. To see the reason, recall that one source
of errors inherent in a projection approach is word
alignment errors. In Setting 1, when we tried to
project English mentions to the French text, word
alignment errors would adversely affect the ability
of the NP projection algorithm to correctly define
the boundaries of the French mentions. Since coref-
erence performance depends crucially on the abil-
ity to correctly identify mentions (Stoyanov et al,
2009), the presence of word alignment errors im-
plies that the resulting French coreference annota-
tions could score poorly even if the English coref-
erence annotations produced by Reconcile were of
high quality. In the current setting, on the other
hand, we reduce the sensitivity of coreference per-
formance to word alignment errors via the use of the
French mention extractor to produce more accurate
French mention boundaries.
3.3 Setting 3: Additional Taggers Available
Finally, we consider a setting that is the least
resource-scarce of the three. We assume that in ad-
dition to a French mention extractor, we have access
to other French linguistic taggers (e.g., syntactic and
semantic parsers) that will allow us to generate the
linguistic features needed to train a French resolver
on the projected coreference annotations.
Specifically, assume that Test is a set of French
texts we want to coreference-annotate, and Training
is a set of French texts that is disjoint from Test but is
drawn from the same domain as Test.8 To annotate
the Test texts, we perform the following steps. First,
we employ the French mention extractor in combi-
nation with the method described in Setting 2 to au-
tomatically coreference-annotate the Training texts.
Next, motivated by Kobdani et al (2011), we train
a French coreference resolver on the automatically
coreference-annotated training texts, using the fea-
tures provided by the available linguistic taggers. Fi-
nally, we apply the resolver to generate coreference
chains for each Test text.
Two questions arise. First, is this method neces-
sarily better than the one described in Setting 2? We
hypothesize that the answer is affirmative: not only
can this method exploit the knowledge about the tar-
get language provided by the additional linguistic
taggers, but the resulting coreference resolver may
allow us to generalize from the (noisily labeled) data
and make this method more robust to the noise in-
8We assume that it is easy to assemble the Training set, since
unlabeled texts are typically easy to collect in practice.
723
herent in the projected coreference annotations than
the previously-described methods. Second, is this
method necessarily better than projection via a par-
allel corpus? Like the first question, this is also an
empirical question. Nevertheless, one reason why
this method is intuitively better is that it ensures that
the training and test documents are drawn from the
same domain. On the other hand, when project-
ing annotations via a parallel corpus, we may en-
counter a domain mismatch problem if the parallel
corpus and the test documents come from different
domains, and the coreference resolver may not work
well if it is trained and tested on different domains.
4 Coreference Resolution System
To train the coreference resolver employed in Set-
ting 3 in the previous section, we need to derive
linguistic features from the documents in the target
language. In our experiments, we employ the coref-
erence data sets produced as part of the SemEval-
2010 shared task on Coreference Resolution in Mul-
tiple Languages. The shared task organizers have
made publicly available six data sets that corre-
spond to six European languages. Each data set
comprises not only training and test documents that
are coreference-annotated, but also a number of
word-based linguistic features from which we derive
mention-based linguistic features for training a re-
solver. In this section, we will describe how this re-
solver is trained and then applied to generate coref-
erence chains for unseen documents.
Training the coreference classifier. As our coref-
erence model, we train a mention-pair model, which
is a classifier that determines whether two mentions
are co-referring or not (e.g., Soon et al (2001), Ng
and Cardie (2002)).9 Each instance i(mj ,mk) cor-
responds to mj (a candidate antecedent) and mk (the
mention to be resolved), and is represented by a set
of 23 features shown in Table 1. As we can see, each
feature is either relational, capturing the relation be-
tween mj and mk, or non-relational, capturing the
linguistic property of mk. The possible values of
a relational feature (except LEXICAL) are C (com-
patible), I (incompatible), and NA (the comparison
9Note that any supervised coreference model can be used,
such as an entity-mention model (e.g., Luo et al (2004), Yang
et al (2008)) or a ranking model (e.g., Denis and Baldridge
(2008), Rahman and Ng (2009)).
cannot be made due to missing data). For a non-
relational feature, we refer the reader to the data sets
for the list of possible values.10
We follow Soon et al?s (2001) method for creat-
ing training instances. Specifically, we create (1) a
positive instance for each anaphoric mention mk and
its closest antecedent mj; and (2) a negative instance
for mk paired with each of the intervening mentions,
mj+1,mj+2, . . . ,mk?1. The classification associ-
ated with a training instance is either positive or neg-
ative, depending on whether the two mentions are
coreferent in the associated text. To train the classi-
fier, we use SVMlight (Joachims, 1999).
Applying the classifier to a test text. After train-
ing, the classifier is used to identify an antecedent
for a mention in a test text. Specifically, each men-
tion, mk, is compared to each preceding mention,
mj , from right to left, and mj is selected as the an-
tecedent of mk if the pair is classified as coreferent.
The process terminates as soon as an antecedent is
found for mk or the beginning of the text is reached.
5 Evaluation
We evaluate our MT-based projection approach for
each of the three settings described in Section 3.
5.1 Experimental Setup
Data sets. We use the Spanish and Italian data sets
from the SemEval-2010 shared task on Coreference
Resolution in Multiple Languages.11 Each data set
is composed of a training set and a test set. Statistics
of these data sets are shown in Table 2.
Spanish Italian
Training Set Statistics
number of mentions 78779 24853
number of non-singleton clusters 48681 18376
number of singleton clusters 37336 15984
Test Set Statistics
number of mentions 14133 13394
number of non-singleton clusters 8789 9520
number singleton clusters 6737 8288
Table 2: Statistics of the data sets.
10The data sets can be downloaded from http://stel.
ub.edu/semeval2010-coref/datasets.
11Note, however, that our approach is equally applicable to
other languages evaluated in the shared task.
724
Features describing mk, the mention to be resolved
1 NUM WORDS the number of words in mk
2 COARSE POS the coarse POS of mk (see ?PoS? in Recasens et al (2010))
3 FINE POS the fine-grained POS of mk (see ?PoS type? in Recasens et al (2010))
4 NE the named entity tag of mk if mk is a named entity; else NA
5 SR the semantic role of mk
6 GRAMROLE the grammatical role of mk
7 NUMBER the number of mk
8 GENDER the gender of mk
9 PERSON the person of mk (e.g., first, second, third) if it is pronominal; else NA
Features describing the relationship between mj , a candidate antecedent and mk, the mention to be resolved
10 CS STR MATCH determines whether the mentions are the same string
11 CI STR MATCH same as feature 10, except that case differences are ignored
12 CS SUBSTR MATCH determines whether one mention is a substring of the other
13 CI SUBSTR MATCH same as feature 12, except that case differences are ignored
14 NUMBER MATCH determines whether the mentions agree in number
15 GENDER MATCH determines whether the mentions agree in gender
16 COARSE POS MATCH determines whether the mentions have the same coarse POS tag
17 FINE POS MATCH determines whether the mentions have the same fine-grained POS tag
18 ROLE MATCH determines whether the mentions have the same grammatical role
19 NE MATCH determines whether both are NEs and have the same NE type
20 SR MATCH determines whether the mentions have the same semantic role
21 ALIAS determines whether one mention is an abbreviation or an acronym of the other
22 PERSON MATCH determines whether both mentions are pronominal and have the same person
23 LEXICAL the concatenation of the heads of the two mentions
Table 1: Feature set for coreference resolution.
Scoring programs. To score the output of a coref-
erence resolver, we employ four scoring programs,
MUC (Vilain et al, 1995), B3 (Bagga and Baldwin,
1998), ?3-CEAF (Luo, 2005), and BLANC (Re-
casens and Hovy, 2011), which were downloaded
from the shared task website (see Footnote 10).
Gold-standard versus regular settings. The for-
mat of each data set follows that of a typical CoNLL
shared task data set. In other words, each row cor-
responds to a word in a document; moreover, all but
the last column contain the linguistic features com-
puted for the words, and the last column stores the
coreference information. Some of the features were
computed via automatic means, but some were ex-
tracted from human annotations. Given this distinc-
tion, the shared task organizers defined two evalua-
tion settings: in the regular setting, only the columns
that were computed automatically can be used to de-
rive coreference features for classifier training, and
results should be reported on system mentions; on
the other hand, in the gold-standard setting, only
the columns that were extracted from human annota-
tions can be used to derive coreference features, and
results should be reported on true mentions. We will
present results corresponding to both settings. Note
that these two settings should not be confused with
the three settings described in Section 3.
Mention extraction. Recall that Settings 2 and 3
both assume the availability of a mention extrac-
tor for extracting mentions in the target language.
In our experiments, we extract mentions using two
methods. First, we assume the availability of an
oracle mention extractor that will enable us to ex-
tract true mentions (i.e., gold-standard mentions) di-
rectly from the test texts. Second, we employ simple
heuristics to automatically extract system mentions.
Since coreference performance is sensitive to the
accuracy of mention extraction (Stoyanov et al,
2009), we experiment with several heuristic meth-
ods for extracting system mentions for both Span-
ish and Italian. According to our cross-validation
experiments on the training data, the best heuris-
tic for extracting Spanish mentions is different from
that for extracting Italian mentions. Specifically, for
725
Spanish, the best heuristic method operates as fol-
lows. First, it extracts all the syntactic heads (i.e.,
the word tokens whose gold dependency labels are
SUBJ, PRED, or GMOD). Second, for each syntac-
tic head, it identifies the smallest text span contain-
ing the head and all of its dependents, and creates a
mention from this text span. For Italian, on the other
hand, the best heuristic simply involves creating one
mention for each gold NE. The reason why this sim-
ple heuristic works well is that most of the Italian
mentions are NEs, owing to the fact that abstract
NPs and pronouns are also annotated as NEs in the
Italian data set. When evaluated on the test set, the
heuristic-based mention extractor achieves F-scores
of 80.2 (78.4 recall, 82.1 precision) for Spanish and
92.3 (85.9 recall, 99.6 precision) for Italian.
5.2 Results and Discussion
5.2.1 Supervised Results
Our supervised systems. While our MT-based
projection approach is unsupervised (i.e., it does not
rely on any coreference annotations from the target
language), it would be informative to see the perfor-
mance of the supervised resolvers, since their perfor-
mance can be viewed as a crude upper bound on the
performance of our unsupervised systems. Specif-
ically, we train a mention-pair model on the train-
ing set using the 23 features shown in Table 1 and
SVMlight as the underlying learning algorithm12,
and apply the resulting model in combination with
Soon et al?s clustering algorithm (see Section 4) to
generate coreference chains for the test texts.
Results on the test sets, reported in terms of re-
call (R), precision (P), and F-score (F) computed by
the four coreference scorers, are shown in the first
two rows of Table 3 (Spanish) and Table 4 (Italian).
For convenience, we summarize a system?s perfor-
mance using a single number, which is shown in the
last column (Average) and is obtained by taking a
simple average of the F-scores of the four scorers.
More specifically, row 1, which is marked with a
?G?, and row 2, which is marked with a ?R?, show
the results obtained under the gold-standard setting
and the regular setting, respectively.
As we can see, under the gold-standard setting,
12All SVM learning parameters in this and other experiments
in this paper are set to their default values.
the supervised resolver achieves an average F-score
of 66.1 (Spanish) and 65.9 (Italian). Not surpris-
ingly, under the regular setting, its average F-score
drops statistically significantly13 to 54.6 (Spanish)
and 63.4 (Italian).14
Best systems in the shared task. To determine
whether the upper bounds established by our su-
pervised systems are reasonable, we show the re-
sults of the best-performing resolvers participating
in the shared task for both languages under the gold-
standard and regular settings in rows 3 and 4 of Ta-
bles 3 and 4. Since none of the participating systems
achieved the best score over all four scorers, we re-
port the performance of the system that has the high-
est average F-score. According to the shared task
website, TANL-1 (Attardi et al, 2010) achieved the
best average F-score in the regular setting for Span-
ish, whereas SUCRE (Kobdani and Schu?tze, 2010)
outperformed others in the remaining settings.
Comparing these best shared task results with our
supervised results in rows 1 and 2, we see that our
average F-score for Spanish/Gold is worse than its
shared task counterpart by 0.7 points, but otherwise
our system outperforms in other settings w.r.t. av-
erage F-score, specifically by 5.0 points for Span-
ish/Regular (due to a better MUC F-score), by 3.4?
4.7 points for Italian (due to better CEAF, B3, and
BLANC scores). Overall, these results suggest that
the scores achieved by our systems are at least as
competitive as the best shared task scores.
5.2.2 Unsupervised Results
Next, we evaluate our projection algorithm.
Setting 1. Results of our approach, when applied
in Setting 1, are shown in row 5 of Tables 3 and 4.
Given that it has to operate under the severe condi-
tion where no linguistic taggers are available for the
target language, it is perhaps not surprising to see
that its performance is significantly worse than that
of its supervised counterparts.
Setting 2. Recall that this setting is less resource-
scarce than Setting 1 in that a mention extractor for
13All significance test results in this paper are obtained using
one-way ANOVA, with p set to 0.05.
14Separately, we determined whether the performance drop
in the regular setting is due to the use of automatically computed
features or the use of system mentions, and found that the latter
was almost entirely responsible for the drop.
726
CEAF MUC B3 BLANC Average
Approach R P F R P F R P F R P F F
1 Supervised (G) 68.8 68.8 68.8 58.2 52.6 55.3 76.5 75.1 75.8 62.9 66.1 64.3 66.1
2 Supervised (R) 57.4 60.1 58.8 41.0 46.3 43.5 57.6 64.8 61.0 53.9 65.0 55.2 54.6
3 Shared task best (G) 69.8 69.8 69.8 52.7 58.3 55.3 75.8 79.0 77.4 67.3 62.5 64.5 66.8
4 Shared task best (R) 58.6 60.0 59.3 14.0 48.4 21.7 56.6 79.0 66.0 51.4 74.7 51.4 49.6
5 Setting 1 35.9 52.9 42.8 10.8 48.7 17.7 30.5 63.9 41.3 51.2 72.6 48.7 37.6
6 Setting 2 (True) 65.6 65.6 65.6 16.8 64.7 26.7 64.3 96.9 77.3 52.8 78.8 54.6 56.1
7 Setting 2 (System) 53.2 55.7 54.4 13.4 58.5 21.8 49.8 79.7 61.3 50.7 75.5 49.5 46.8
8 Setting 3 (G) 65.9 65.9 65.9 48.1 45.2 46.6 72.3 72.6 72.5 60.1 61.4 60.7 61.4
9 Setting 3 (R) 55.3 55.3 55.3 34.1 41.6 37.5 55.1 63.6 59.0 53.8 62.1 54.9 51.7
Table 3: Results for Spanish
CEAF MUC B3 BLANC Average
Approach R P F R P F R P F R P F F
1 Supervised (G) 74.5 74.5 74.5 31.8 67.4 43.2 74.4 93.6 82.9 58.4 79.6 62.9 65.9
2 Supervised (R) 73.7 74.3 74.0 31.9 68.0 43.4 60.8 92.5 73.3 58.4 79.6 62.9 63.4
3 Shared task best (G) 66.0 66.0 66.0 48.1 42.3 45.0 76.7 76.9 76.8 54.8 63.5 56.9 61.2
4 Shared task best (R) 57.1 66.2 61.3 50.1 50.7 50.4 63.6 79.2 70.6 55.2 68.3 57.7 60.0
5 Setting 1 17.0 26.0 20.6 8.1 28.5 12.6 14.1 30.5 19.3 50.1 62.9 32.9 21.4
6 Setting 2 (True) 73.3 73.3 73.3 14.2 60.6 23.0 72.9 96.8 83.2 51.9 77.9 53.2 58.2
7 Setting 2 (System) 60.4 70.1 64.9 17.2 68.2 27.5 59.3 97.1 73.6 52.0 82.9 53.4 54.9
8 Setting 3 (G) 64.3 64.3 64.3 28.3 63.3 39.1 65.3 87.4 74.8 55.1 74.7 57.5 58.9
9 Setting 3 (R) 61.1 62.9 61.9 29.5 63.2 40.2 60.3 84.1 70.2 55.3 72.9 58.3 57.7
Table 4: Results for Italian
the target language is available. Results of our al-
gorithm, when operating under Setting 2 using true
mentions and system mentions, are shown in rows
6 and 7 of Tables 3 and 4, respectively. In com-
parison to the results for Setting 1, we see that the
F-scores obtained under Setting 2 increase signifi-
cantly, regardless of (1) the scoring programs and
(2) whether true mentions or system mentions are
used. These results provide evidence for our earlier
hypothesis that our projection algorithm can prof-
itably exploit the linguistic knowledge about the tar-
get language that is available to it. In particular, the
mention extractor helps make our approach less sen-
sitive to word alignment and NP projection errors.
In comparison to our supervised results in rows 1
and 2, our algorithm still lags behind by about 8?10
points in average F-score. However, this should not
be surprising, since our algorithm is unsupervised.
Looking closer at the results, we can see that the
performance lag by our approach can be attributed
to its lower recall: in general, the lag in MUC recall
appears to be more acute than that in B3 and CEAF
recall. Since MUC only scores non-singleton clus-
ters wheres B3 and CEAF score both singleton and
non-singleton clusters, these results suggest that our
approach is better at identifying singleton clusters
than recovering coreference links.
Setting 3. Finally, we evaluate our approach in a
setting where it has access to all the information
available to our supervised resolvers, except for the
gold-standard coreference annotations on the train-
ing sets. Specifically, our approach uses projected
coreference annotations to train a resolver on the
training texts, whereas the supervised resolvers do
so using gold-standard annotations.
Comparing Settings 2 and 3 with respect to true
mentions (rows 6 and 8 of Tables 3 and 4), we see
mixed results. According to MUC and BLANC, the
resolvers in Setting 3 are significantly better than
those in Setting 2 for both languages. According to
B3, the resolvers in Setting 2 are significantly better
than those in Setting 3 for both languages. Accord-
ing to CEAF, the Spanish resolvers in Setting 3 are
significantly better than their counterparts in Setting
2, but the opposite is true for the Italian resolvers.
To understand these somewhat contradictory per-
formance trends, let us first note that the dramatic in-
crease in the MUC F-score can be attributed to large
727
gains in MUC recall. This suggests that the clas-
sifiers being trained in Setting 3 have enabled the
discovery of additional coreference links. In other
words, there are benefits to be obtained just by learn-
ing over noisy coreference annotations, a result that
we believe is quite interesting. However, not all of
these newly discovered coreference links are correct.
The fact that some scoring programs (e.g., B3) are
more sensitive to spurious coreference links than the
others (e.g., MUC) explains these mixed results.
Nevertheless, according to average F-score, the
resolvers in Setting 3 perform significantly better
than those in Setting 2 for both languages: F-score
increases by 5.3 points for Spanish and 0.7 points for
Italian. Similar trends can be observed when com-
paring the two settings w.r.t. system mentions (rows
7 and 9 of Tables 3 and 4): F-score increases by 4.9
points for Spanish and 2.8 points for Italian.
While our Setting 3 results still underperform the
supervised results in rows 1 and 2, we can see that
they achieve 93?94% of the average F-scores of the
supervised Spanish resolvers and 89?91% of the av-
erage F-scores of the supervised Italian resolvers.
Importantly, recall that our approach achieves this
level of performance without relying on any gold-
standard coreference annotations in Spanish and
Italian, and we believe that these results demonstrate
the promise of our MT-based projection approach.
Since these results suggest that our approach can-
not be successfully applied without MT services, a
parallel corpus for learning a word alignment model,
and a mention extractor for the target language, a
natural question is: to what extent do these require-
ments limit the applicability of our approach? While
it is the case that our approach cannot be applied to
a truly resource-scarce language, it can be applied to
the numerous Indian and East European languages
for which the aforementioned requirements are sat-
isfied but coreference-annotated data is not readily
available.
6 Conclusions and Future Work
We explored the under-investigated yet challenging
task of performing coreference resolution for a lan-
guage for which we have no coreference-annotated
data and no linguistic knowledge of the language.
Our translation-based projection approach has the
flexibility to exploit any available knowledge about
the target language. In experiments with Spanish
and Italian, we obtained promising results: our ap-
proach achieved around 90% of the performance of
a supervised resolver when only a mention extrac-
tor for the target language was available. We believe
that this approach has the potential to allow coref-
erence technologies to be deployed across a larger
number of languages than is currently possible, and
that this is just the beginning of a new line of work.
To gain additional insights into our approach,
we plan to pursue several directions. First, we
will isolate the impact of each factor that ad-
versely affects its performance, including errors
in projection, translation, and coreference resolu-
tion in the resource-rich language. Second, we
will perform an empirical comparison of two ap-
proaches to projecting coreference annotations, our
translation-based approach and Camargo de Souza
and Orasan?s (2011) approach, where annotations
are projected via a parallel corpus. Third, rather than
translate from the target to the source language, we
will examine whether it is better to translate all the
coreference-annotated data available in the source
language to the target language, and train a coref-
erence model for the target language on the trans-
lated data. Fourth, since the success of our pro-
jection approach depends heavily on the accuracies
of machine translation as well as coreference res-
olution in the source language, we will determine
whether their accuracies can be improved via an en-
semble approach, where we employ multiple MT
engines and multiple coreference resolvers. Finally,
we plan to employ our approach to alleviate the
corpus-annotation bottleneck, specifically by using
the annotated data it produces to augment the man-
ual coreference annotations that capture the specific
properties of the target language.
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier draft
of the paper. This work was supported in part by
NSF Grants IIS-0812261 and IIS-1147644. Any
opinions, findings, or conclusions expressed in this
paper are those of the authors and do not necessarily
reflect the views or official policies of NSF.
728
References
Giuseppe Attardi, Maria Simi, and Stefano Dei Rossi.
2010. TANL-1: Coreference resolution by parse anal-
ysis and similarity clustering. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 108?111.
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics and the
17th International Conference on Computational Lin-
guistics, pages 79?85.
Jennifer Camargo de Souza and Constantine Orasan.
2011. Can projected chains in parallel corpora help
coreference resolution? In Anaphora Processing and
Applications, pages 59?69. Springer.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 600?609.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?669.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
848?855.
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Pro-
ceedings of Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
385?393.
Sanda Harabagiu and Steven Maiorano. 2000. Multi-
lingual coreference resolution. In Proceedings of the
Sixth Applied Natural Language Processing Confer-
ence, pages 142?149.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311?325.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?lkopf, Christo-
pher Burges, and Alexander Smola, editors, Advances
in Kernel Methods ? Support Vector Learning, pages
44?56. MIT Press, Cambridge, MA.
Hamidreza Kobdani and Hinrich Schu?tze. 2010. SU-
CRE: A modular system for coreference resolution. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 92?95.
Hamidreza Kobdani, Hinrich Schu?tze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrap-
ping coreference resolution using word associations.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 783?792.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit X.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics, pages 135?142.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 25?
32.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007.
Learning multilingual subjective language via cross-
lingual projections. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, pages 976?983..
Ruslan Mitkov. 1999. Multilingual anaphora resolution.
Machine Translation, 14(3?4):281?299.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 104?111.
Vincent Ng. 2008. Unsupervised models for coreference
resolution. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 640?649.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 650?
659.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Oana Postolache, Dan Cristea, and Constantin Orasan.
2006. Transferring coreference chains through word
alignment. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation,
pages 889?892.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-
sky, and Christopher Manning. 2010. A multi-pass
729
sieve for coreference resolution. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 492?501.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand Index for coreference evaluation.
Natural Language Engineering, 17(4):485?510.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 1?8.
Philip Resnik. 2004. Exploiting hidden meanings: Us-
ing bilingual text for monolingual annotation. In Pro-
ceedings of the 5th International Conference on Com-
putational Linguistics and Intelligent Text Processing,
pages 283?299.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 656?664.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence resolution with reconcile. In Proceedings of the
ACL 2010 Conference Short Papers, pages 156?161.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message Understanding Conference,
pages 45?52.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, and
Sheng Li. 2008. An entity-mention model for coref-
erence resolution with inductive logic programming.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 843?851.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the Second Meeting of the North American Chapter of
the Association for Computational Linguistics, pages
200?207.
Imed Zitouni and Radu Florian. 2008. Mention detec-
tion crossing the language barrier. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, pages 600?609.
730
Proceedings of NAACL-HLT 2013, pages 918?927,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Classifying Temporal Relations with Rich Linguistic Knowledge
Jennifer D?Souza and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{jld082000,vince}@hlt.utdallas.edu
Abstract
We examine the task of temporal relation clas-
sification. Unlike existing approaches to this
task, we (1) classify an event-event or event-
time pair as one of the 14 temporal relations
defined in the TimeBank corpus, rather than
as one of the six relations collapsed from the
original 14; (2) employ sophisticated linguis-
tic knowledge derived from a variety of se-
mantic and discourse relations, rather than fo-
cusing on morpho-syntactic knowledge; and
(3) leverage a novel combination of rule-based
and learning-based approaches, rather than re-
lying solely on one or the other. Experiments
with the TimeBank corpus demonstrate that
our knowledge-rich, hybrid approach yields
a 15?16% relative reduction in error over a
state-of-the-art learning-based baseline sys-
tem.
1 Introduction
Recent years have seen a surge of interest in tem-
poral information extraction (IE). Temporal relation
classification, one of the most important temporal
IE tasks, involves classifying a given event-event
pair or event-time pair as one of a set of predefined
temporal relations. The creation of the TimeBank
corpus (Pustejovsky et al, 2003) and the organiza-
tion of the TempEval-1 (Verhagen et al, 2007) and
TempEval-2 (Verhagen et al, 2010) evaluation ex-
ercises have facilitated the development and evalua-
tion of temporal relation classification systems.
Our goal in this paper is to advance the state of
the art in temporal relation classification. Our work
differs from existing work with respect to both the
complexity of the task we are addressing and the ap-
proach we adopt. Regarding task complexity, rather
than focus on six temporal relations as is typically
done in previous work (see Section 2 for more infor-
mation), we address an arguably more challenging
version of the task where we consider all the 14 re-
lations originally defined in the TimeBank corpus.
Our approach to temporal relation classification
can be distinguished from existing approaches in
two respects. The first involves a large-scale ex-
pansion of the linguistic features made available
to the classification system. Recall that exist-
ing approaches have relied primarily on morpho-
syntactic features as well as a few semantic fea-
tures extracted from WordNet synsets and VerbO-
cean?s (Chklovski and Pantel, 2004) semantic rela-
tions. On the other hand, we propose not only novel
lexical and grammatical features, but also sophis-
ticated features involving semantics and discourse.
Most notably, we propose (1) semantic features en-
coding a variety of semantic relations, including
PropBank-style predicate-argument relations as well
as those extracted from the Merriam-Webster dictio-
nary, and (2) discourse features encoding automat-
ically computed Penn Discourse TreeBank (PDTB)
style (Prasad et al, 2008) discourse relations.
Second, while the vast majority of existing ap-
proaches to temporal relation classification are
learning-based, we propose a system architecture in
which we combine a learning-based approach and a
rule-based approach. Our motivation behind adopt-
ing a hybrid approach stems from two hypotheses.
First, a rule-based method could better handle the
skewed class distribution underlying the dataset for
918
our 14-class classification problem. Second, better
decision rules could be formed by leveraging hu-
man insights to combine the available linguistic fea-
tures than by using fully automatic machine learn-
ing methods. Note that while rule-based approaches
have been shown to underperform learning-based
approaches on this task (Mani et al, 2006), to our
knowledge they have not been used in combination
with learning-based approaches. Moreover, while
the rules employed in previous work are created
based on intuition (e.g., Mani et al (2006), Pus?cas?u
(2007)), our rules are created in a data-driven man-
ner via a manual inspection of the annotated tempo-
ral relations in the TimeBank corpus.
Experiments on the TimeBank corpus demon-
strate the effectiveness of our knowledge-rich, hy-
brid approach to temporal relation classification: it
yields a 15?16% relative reduction in error over a
state-of-the-art learning-based baseline system.
To our knowledge, we are the first to (1) report re-
sults for the 14-class temporal relation classification
task on the TimeBank (v1.2) corpus; (2) success-
fully employ automatically computed PDTB-style
discourse relations to improve performance on this
task; and (3) show that a hybrid approach to this
task can yield better results than either a rule-based
or learning-based approach. Note that hybrid ap-
proaches in this spirit were popular in the natural
language processing community back in the mid-90s
(Klavans and Resnik, 1994). We believe that they
are among the most competitive approaches to lan-
guage processing tasks that require complex reason-
ing and should be given more attention in the com-
munity. We release the complete set of rules that we
mined from the TimeBank corpus and used in our
rule-based approach in hopes that our insights into
how features can be combined as decision rules can
benefit researchers interested in this task.
The rest of the paper is organized as follows. Sec-
tion 2 provides an overview of the TimeBank cor-
pus. Sections 3 and 4 describe the baseline system
and our approach, respectively. We present evalua-
tion results in Section 5 and conclude in Section 6.
2 Corpus
For evaluation, we use the TimeBank (v1.2) cor-
pus, which consists of 183 newswire articles. In
each article, the events, times, and their temporal re-
lations are marked up. An event, which can be a
tensed verb, adjective, or nominal, contains various
attributes, including the class of event, tense, aspect,
polarity, and modality. A time expression has a class
attribute, which specifies whether it is a date, time,
duration, or set, and its value is normalized based on
TIMEX3. A temporal relation can be an order rela-
tion, which orders two events (as in sentence (1)), or
an anchor relation, which anchors an event to a time
expression (as in sentence (2)).
(1) A steep rise in world oil prices fol-
lowed the Kuwait invasion.
(2) We are there to stay for a long period.
Each temporal relation has a type. For example,
the relation defined on rise and invasion in (1) has
type After, whereas the relation defined on stay and
period in (2) has type During. Note that a temporal
relation is defined on an ordered pair. For exam-
ple, in (1), the pair (rise, invasion) has type After,
whereas the pair (invasion, rise) has type Before).
14 relation types are defined and used to annotate
the temporal relations in the TimeBank corpus. Ta-
ble 1 provides a brief description of these relation
types and the relevant statistics.
In our experiments, we assume that our tempo-
ral relation classification system is given an event-
event or event-time pair that is known to belong to
one of the 14 relation types defined in TimeBank and
aims to determine its relation type. Following pre-
vious evaluations of the temporal relation classifica-
tion task on the TimeBank corpus (e.g., Mani et al
(2006), Chambers et al (2007)) and in TempEval-
1/2, we assume as input gold events and time ex-
pressions.
Unlike Mani et al (2006) and Chambers et al
(2007), who focus on six relation types (Simul-
taneous, Before, IBefore, Begins, Ends, and In-
cludes), we report results on 14 relation types. Note
that the aforementioned six relation types are cho-
sen by (1) discarding During, During Inv, and
Identity, and (2) combining the two relation types
in each of the five pairs, namely (Before, After),
(IBefore, IAfter), (Includes, Is Included), (Be-
gins, Begun By), and (Ends, Ended By), into a sin-
gle type because they are inverses of each other. In
other words, if a relation instance (e1, e2) is anno-
919
Id Relation Description Total % E-E E-T
1 Simultaneous e1 and e2 happen at the same time or are temporally distinguishable 660 (13.3) 599 61
2 Identity e1 and e2 are coreferent 702 (14.1) 696 6
3 Before e1 happens before e2 in time 689 (13.9) 639 50
4 After e1 happens after e2 in time 744 (15) 681 63
5 IBefore e1 happens immediately before e2 in time 39 (0.8) 38 1
6 IAfter e1 happens immediately after e2 in time 28 (0.6) 25 3
7 Includes As in Ed arrived in Seoul last Sunday (e1=last Sunday; e2=arrived) 758 (15.3) 318 440
8 Is Included As in Ed arrived in Seoul last Sunday (e1=arrived; e2=last Sunday) 762 (15.3) 201 561
9 During e1 persists throughout duration e2 102 (2.1) 19 83
10 During Inv e2 persists throughout duration e1 124 (2.5) 44 80
11 Begins e1 marks the beginning of e2 66 (1.3) 44 22
12 Begun By e2 marks the beginning of e1 61 (1.2) 32 29
13 Ends e1 marks the end of e2 66 (1.3) 21 45
14 Ended By e2 marks the end of e1 170 (3.42) 93 77
Table 1: The 14 temporal relations and their frequency of occurrences in TimeBank (v1.2). Each relation is defined
on an ordered event-event or event-time pair (e1,e2). The ?Total? and ?%? columns show the number and percentage
of instances annotated with the corresponding relation in the corpus, respectively, and the ?E-E? and ?E-T? columns
show the breakdown by the number of event-event pairs and event-time pairs.
tated as After, it is replaced with the instance (e2,
e1) with class Before, and subsequently a relation
classifier is presented with (e2, e1) but not (e1, e2).
On the other hand, our 14-class task is arguably
more challenging since our system has to further dis-
tinguish a relation type from its inverse given an in-
stance in which the two elements are in arbitrary or-
der.
3 Baseline Temporal Relation Classifier
Since the currently best-performing systems for
temporal relation classification are learning-based,
we will employ a learning-based system as our base-
line. Below we describe how we train this baseline.
Without loss of generality, assume that (e1,e2) is
an event-event/event-time pair such that (1) e1 pre-
cedes e2 in the associated text and (2) (e1,e2) be-
longs to one of the 14 TimeBank temporal rela-
tion types. We create one training instance for each
event-event/event-time pair in a training document
that satisfies the two conditions above, labeling it
with the relation type that exists between e1 and e2.
To build a strong baseline, we represent each
instance using 68 linguistic features modeled af-
ter the top-performing temporal relation classifica-
tion systems on TimeBank (e.g., Mani et al (2006),
Chambers et al (2007)) and in the TempEval shared
tasks (e.g., Min et al (2007), Pus?cas?u (2007), Ha et
al. (2010), Llorens et al (2010), Mirroshandel and
Ghassem-Sani (2011)).1 These features can be di-
vided into six categories, as described below.
Lexical (5). The strings of e1 and e2, the head
words of e1 and e2, and a binary feature indicating
whether e1 and e2 have the same string.
Grammatical (33). The POS tags of the head
words of e1 and e2, the POS tags of the five to-
kens preceding and following e1 and e2, the POS
bigram formed from the head word of e1 and its pre-
ceding token, the POS bigram formed from the head
word of e2 and its preceding token, the POS tag pair
formed from the head words of e1 and e2, the prepo-
sitional lexeme of the prepositional phrase (PP) if e1
is headed by a PP (Chambers et al, 2007), the prepo-
sitional lexeme of the PP if e2 is headed by a PP, the
prepositional lexeme of the PP if e1 is governed by
a PP (Mirroshandel and Ghassem-Sani, 2011), the
prepositional lexeme of the PP if e2 is governed by
a PP, the POS of the head of the verb phrase (VP) if
e1 is governed by a VP, the POS of the head of the
VP if e2 is governed by a VP, whether e1 syntacti-
cally dominates e2 (Chambers et al, 2007), and the
shortest path from e1 to e2 in the associated syntac-
tic parse tree. We obtain parse trees and POS tags
using the Stanford CoreNLP tool.2
1Note, however, that these features were designed for the
arguably simpler 6-class temporal relation classification tasks.
2http://nlp.stanford.edu/software/
corenlp.shtml
920
Entity attributes (13). The tense, aspect, modal-
ity, polarity, and event type of e1 and e2 if they are
events (if one of them is a time expression, then the
class attribute will be set to its class and the rest of
them will have the value NULL), pairwise features
formed by pairing up the tense values, the aspect
values, and the class values of e1 and e2.
Semantic (7). The subordinating temporal role to-
ken of e1 if it appears within a temporal semantic
role argument (Llorens et al, 2010), the subordinat-
ing temporal role token of e2 if it appears within a
temporal semantic role argument, the first WordNet
synset to which e1 belongs, the first WordNet synset
to which e2 belongs, and whether e1 and e2 are in the
happens-before, happens-after, and similar relation
according to VerbOcean.3
Distance (1). Are e1 and e2 in the same sentence?
DCT related (3). The temporal relation type be-
tween e1 and the document creation time (DCT) [its
value can be one of the 14 relation types, or NULL
if no relation exists], the temporal relation type be-
tween e2 and the DCT, and whether e1 and e2 have
different relation types with the DCT.
After creating the training instances, we train
a 14-class classifier on them using SVMmulticlass
(Tsochantaridis et al, 2004).4 We then use it to
make predictions on the test instances, which are
generated in the same way as the training instances.
4 Our Hybrid Approach
In this section, we describe our hybrid learning-
based and rule-based approach to temporal relation
classification. Section 4.1 describes our novel fea-
tures, which will be used to augment the baseline
feature set (see Section 3) to train a temporal rela-
tion classifier. Section 4.2 outlines our manual rule
creation process. Section 4.3 discusses how we com-
bine our hand-crafted rules and the learned classifier
to make predictions in our hybrid approach.
3happens-after is not a relation in VerbOcean: we create this
relation simply by inverting the happens-before relation.
4For all the experiments involving SVMmulticlass, we set C,
the regularization parameter, to 10,000, since preliminary ex-
periments indicate that preferring generalization to overfitting
(by setting C to a small value) tends to yield poorer classifica-
tion performance. The remaining learning parameters are set to
their default values.
4.1 Six Types of New Features
4.1.1 Pairwise Features
Recall that some of the features in the baseline fea-
ture set are computed based on either e1 or e2 but
not both. Since our task is to predict the relation be-
tween them, we hypothesize that pairwise features,
which are computed based on both elements, could
better capture the relationship between them.
Specifically, we introduce pairwise versions of the
head word feature and the two prepositional lexeme-
based features in the baseline. In addition, we create
two quadruple-wise features, one by pairing up the
tense and class attribute values of e1 with those of
e2, and the other by pairing up their tense and as-
pect values. Next, we create two trace features, one
based on prepositions and the other on verbs, since
prepositions and verb tenses have been shown to
play an important role in temporal relation classifi-
cation The preposition trace feature is computed by
(1) collecting the list of prepositions along the path
from e1/e2 to the root of its syntactic parse trees, and
(2) concatenating the resulting lists computed from
e1 and e2. The verb trace feature is computed in a
similar manner, except that we collect the POS tags
of the verbs appearing in the corresponding paths.
4.1.2 Dependency Relations
We introduce features computed based on de-
pendency parse trees obtained via the Stanford
CoreNLP tool, motivated by our observation that
some dependency relation types are more closely
associated with certain temporal relation types than
with others. Let us illustrate with an example:
(3) Ed changed his plans as the mood took
him.
In (3), there is a adverbial clause modifier depen-
dency between changed and took, because took ap-
pears in an adverbial clause (headed by as) modify-
ing changed. Intuitively, if the two events partici-
pate in this type of dependency relation and the ad-
verbial clause is headed by as and there is a tempo-
ral relation between them, then it is likely that this
temporal relation is Simultaneous. While the tem-
poral relation type is dependent on the connective
heading the adverbial clause, in general an adverbial
clause modifier dependency between two events im-
plies that their temporal relation is likely to be Si-
921
multaneous, Before, or After.
Given the potential usefulness of dependency re-
lations for temporal relation classification, we cre-
ate dependency-based features as follows. For each
of the 25 dependency relation types produced by
the Stanford parser, we create four binary features:
whether e1/e2 is the governing entity in the relation,
and whether e1/e2 is the dependent in the relation.
4.1.3 Webster Relations
Some events are not connected by a dependency re-
lation but by a lexical relation. We hypothesize that
some of these lexical relations could be useful for
temporal relation classification. Consider the fol-
lowing example.
(4) The phony war has finished and the real
referendum campaign has begun.
In this sentence, the two events, finished and be-
gun, are connected by an antonym relation. Statisti-
cally speaking, if (1) two events are in two clauses
connected by a coordinating conjunction (e.g., and),
(2) one is an antonym of the other, and (3) there is
a temporal relation between them, then the temporal
relation is likely to be Simultaneous.
Given the potential usefulness of lexical rela-
tions for temporal relation classification, we cre-
ate features based on four types of lexical re-
lations present in Webster?s online thesaurus5,
namely synonyms, related-words, near-antonyms,
and antonyms. Specifically, for each event e appear-
ing in TimeBank, we first use the head word of e to
retrieve four lists, which are the lists corresponding
to the synonyms, related words, near-antonyms, and
antonyms of e. Then, given a training/test instance
involving e1 and e2, we create eight binary features:
whether e1 appears in e2?s list of synonyms/related
words/near-antonyms/antonyms, and whether e2 ap-
pears in e1?s list of synonyms/related words/near-
antonyms/antonyms.
4.1.4 WordNet Relations
Previous uses of WordNet for temporal relation clas-
sification are limited to synsets (e.g., Llorens et al
(2010)). We hypothesize that other WordNet lexical
relations could also be useful for the task. Specif-
ically, we employ four types of WordNet relations,
5http://www.merriam-webster.com/
namely hypernyms, hyponyms, troponyms, and sim-
ilar, to create eight binary features for temporal rela-
tion classification. These eight features are created
from the four WordNet relations in the same way as
the eight features were created from the four Web-
ster relations in the previous subsection.
4.1.5 Predicate-Argument Relations
So far we have exploited lexical and dependency
relations for temporal relation classification. We
hypothesize that semantic relations, in particular
predicate-argument relations, could be useful for the
task. Consider the following example.
(5) ?What sector is stepping forward to
pick up the slack?? he asked.
Using SENNA (Collobert et al, 2011), a PropBank-
style semantic role labeler, we know that forward is
in the directional argument of the predicate stepping.
This enables us to infer that an Includes relation ex-
ists between stepping and forward since intuitively
an action includes a direction.
As another example, consider another PropBank-
style predicate-argument relation, cause. Assuming
that e2 is in e1?s cause argument, we can infer that
e2 occurs Before e1 since intuitively the cause of an
action precedes the action.
Consequently, we create features for tempo-
ral relation classification based on four types
of PropBank-style predicate-argument relations,
namely directional, manner, temporal, and cause.
Specifically, using SENNA?s output, we create four
binary features that encode whether argument e2 is
related to predicate e1 through the four types of rela-
tions, and we create another four binary features that
encode whether argument e1 is related to predicate
e2 through the four types of relations.
4.1.6 Discourse Relations
Rhetorical relations such as causation, elaboration
and enablement could aid in tracking the temporal
progression of the discourse (Hitzeman et al, 1995).
Hence, unlike syntactic dependencies and predicate-
argument relations through which we can identify
intra-sentential temporal relations, discourse rela-
tions can potentially be exploited to discover both
inter-sentential and intra-sentential temporal rela-
tions. However, no recent work has attempted to
use discourse relations for temporal relation clas-
922
(6) { Arg1 Hewlett-Packard Co. said it raised its stake in Octel Communications Corp. to 8.5% of the
common shares outstanding. Arg1} { Arg2 RESTATEMENT In a Securities and Exchange Commis-
sion filing, Hewlett-Packard said it now holds 1,384,119 Octel common shares Arg2}.
(7) { Arg1 Reports said that Saudi Arabia told U.S. oil companies of a 15?20 percent cutback in its oil
supply in September. Arg1} { Conn SYNCHRONY Meanwhile Conn} { Arg2 Egypt?s Middle East
Agency said Thursday that Saddam was the target of an assassination attempt. Arg2}
Table 2: Examples illustrating the usefulness of discourse relations for temporal relation classification.
sification. In this subsection, we examine whether
we can improve a temporal relation identifier via
explicit and implicit PDTB-style discourse relations
automatically extracted by Lin et al?s (2013) end-to-
end discourse parser.
Let us first review PDTB-style discourse rela-
tions. Each relation is represented by a triple (Arg1,
sense, Arg2), where Arg1 and Arg2 are the two ar-
guments of the relation and sense is the sense/type
of the relation. A discourse relation can be explicit
or implicit. An explicit relation is triggered by a dis-
course connective. On the other hand, an implicit
relation is not triggered by a discourse connective,
and may exist only between two consecutive sen-
tences. Generally, implicit relations are much harder
to identify than their explicit counterparts.
Next, to motivate why discourse relations can be
useful for temporal relation classification, we use
two examples (see Table 2), one involving an im-
plicit relation (Example (6)) and the other an explicit
relation (Example (7)). For convenience, both sen-
tences are also annotated using Lin et al?s (2013)
discourse parser, which marks up the two arguments
with the Arg1 and Arg2 tags and outputs the rela-
tion sense next to the beginning of Arg2.
In (6), we aim to determine the order relation be-
tween the reporting event said and the occurrence
event filing. The parser determines that a RESTATE-
MENT implicit relation exists between the two sen-
tences. Intuitively, if no asynchronous relations can
be found among the events in two discourse units
connected by the RESTATEMENT relation, then the
temporal relation between two temporally linked
events within these units is likely to be either Iden-
tity or Simultaneous. In this case, we can rule out
Identity: since said and filing belong to different
event classes, they are not coreferent.
In (7), we aim to determine the anchor relation
between the reporting event said and the date Thurs-
day. The parser determines that a SYNCHRONY
explicit relation triggered by Meanwhile exists be-
tween the two sentences. Intuitively, if a temporally
related reporting event and date occur within differ-
ent discourse units connected by the SYNCHRONY
relation, then it is likely that the event Is Included
in the date. Note that without this discourse relation,
it could be difficult for a machine to confidently as-
sociate a reporting event with a date occurring in a
different discourse segment.
Given the potential usefulness of discourse rela-
tions for temporal relation classification, we create
four features based on discourse relations. In the
first feature, if e1 is in Arg1, e2 is in Arg2, and Arg1
and Arg2 possess an explicit relation with sense s,
then its feature value is s; otherwise its value is
NULL. In the second feature, if e2 is in Arg1, e1 is in
Arg2, and Arg1 and Arg2 possess a explicit relation
with sense s, then its feature value is s; otherwise
its value is NULL. The third and fourth features are
computed in the same way as the first two features,
except that they are computed over implicit rather
than explicit relations.
4.2 Manual Rule Creation
As noted before, we adopt a hybrid learning-based
and rule-based approach to temporal relation clas-
sification. Hence, in addition to training a tempo-
ral relation classifier, we also manually design a set
of rules in which each rule returns a temporal rela-
tion type for a given test instance. We hypothesize
that a rule-based approach can complement a purely
learning-based approach, since a human could com-
bine the available linguistic features into rules using
commonsense knowledge that may not be accessible
to a learning algorithm.
The design of the rules is partly based on intu-
923
ition and partly data-driven: we first use our intu-
ition to come up with a rule and then manually re-
fine it based on the observations we made on the
TimeBank data. For this purpose, we partition the
TimeBank documents into five folds of roughly the
same size, reserving three folds for developing our
rules and using the remaining two folds for evaluat-
ing final system performance. We order these rules
in decreasing order of accuracy, where the accuracy
of a rule is defined as the number of times the rule
yields the correct temporal relation type divided by
the number of times it is applied, as measured on the
three development folds. A new instance is classi-
fied using the first applicable rule in the ruleset.
Some of these rules were shown in the previ-
ous subsection when we motivated each feature type
with examples. The complete set of rules can be ac-
cessed via our website.6
4.3 Combining Rules and Machine Learning
We investigate three ways to combine the hand-
crafted rules and the machine-learned classifier.
In the first method, we employ all of the rules as
additional features for training the classifier. The
value of each such feature is the temporal relation
type predicted by the corresponding rule.
The second method can be viewed as an extension
of the first one. Given a test instance, we first apply
to it the ruleset composed only of rules that are at
least 80% accurate. If none of the rules is applicable,
we classify it using the classifier employed in the
first method.7
The third method is essentially the same as the
second, except we do not employ the rules as fea-
tures when training the classifier.
5 Evaluation
5.1 Experimental Setup
Dataset. As mentioned before, we partition the
183 documents in the TimeBank (v1.2) corpus into
five folds of roughly the same size, reserving three
folds (say Folds 1?3) for manual rule development
6http://www.hlt.utdallas.edu/
?
jld082000/
temporal-relations/
7Although this classifier is applied to only those test in-
stances that the rules cannot handle, we did not retrain it on
only those training instances that the rules cannot handle.
and using the remaining two folds (say Folds 4?5)
for testing. We perform two-fold cross-validation
experiments using the two test folds. In the first fold
experiment, we train a temporal relation classifier on
Folds 1?4 and test on Fold 5; and in the second fold
experiment, we train the classifier on all but Fold 4
and test on Fold 4. The results reported in the rest of
the paper are averaged over the two test folds.
Evaluation metrics. We employ accuracy (Acc)
and macro F-score (Fma). Accuracy is the per-
centage of correctly classified test instances, and is
the standard evaluation metric for temporal relation
classification. Since each test instance belongs to
one of the 14 temporal relation types, accuracy is the
same as micro F-score. On the other hand, macro F-
score is rarely used to evaluate this task. We chose it
because it could provide insights into how well our
approach performs on the minority classes.
5.2 Results and Discussion
Table 3 shows the two-fold cross-validation results
for our 14-class temporal relation classification task.
The six columns of the table correspond to six dif-
ferent system architectures. The ?Feature? column
corresponds to a purely learning-based architecture
where the results are obtained simply by training a
temporal relation classifier using the available fea-
tures. The next two columns correspond to two
purely rule-based architectures, differing by whether
all rules are used regardless of their accuracy or
whether only high-accuracy rules (i.e., those that are
at least 80% accurate) are used. The rightmost three
columns correspond to the three ways of combining
rules and machine learning described in Section 4.3.
On the other hand, the rows of the table differ in
terms of what features are available to a system. In
row 1, only the baseline features are available. In the
subsequent rows, the six types of features discussed
in Section 4 are added incrementally to the baseline
feature set. This means that the last row corresponds
to the case where all feature types are used.
A point merits clarification. It may not be imme-
diately clear how to interpret the results under, for
instance, the ?All Rules? column. In other words,
it may not be clear what it means to add the six
types of features incrementally to a rule-based sys-
tem. Recall that one of our goals is to compare
a purely learning-based system with a purely rule-
924
Features All Rules All Rules with Features + Rules + Rules + Features +
accuracy ? 0.8 Rules as Features Features Rules as Features
Feature Type Acc Fma Acc Fma Acc Fma Acc Fma Acc Fma Acc Fma
1 Baseline 45.3 24.9 ? ? ? ? ? ? ? ? ? ?
2 + Pairwise 46.5 25.8 37.6 26.5 5.1 13.9 46.7 26.5 48.0 31.9 48.2 32.1
3 + Dependencies 47.0 25.9 39.0 27.8 6.9 15.7 47.2 26.7 49.2 32.3 49.2 32.6
4 + WordNet 46.9 26.0 43.5 30.4 6.9 15.7 47.5 26.8 49.2 32.3 49.5 32.8
5 + Webster 46.9 25.8 43.3 29.9 6.9 15.7 48.1 26.8 49.2 32.0 50.1 33.1
6 + PropBank 47.2 26.0 44.3 30.5 8.1 16.6 48.0 26.8 49.5 32.2 50.0 33.0
7 + Discourse 48.1 26.6 47.5 35.1 12.8 23.3 48.9 27.5 53.0 36.0 53.4 36.6
Table 3: Two-fold cross-validation accuracies and macro F-scores as features are added incrementally to the baseline.
based system, since we hypothesized that humans
may be better at combining the available features
to form rules than a learning algorithm would be.
To facilitate this comparison, all and only those fea-
tures that are available to a learning-based system in
a given row can be used in hand-crafting the rules
of the rule-based system in the same row. The other
columns involving the use of rules can be interpreted
in a similar manner.
The highest accuracy and macro F-score are
achieved when all types of features are used in
combination with the ?Rules + Features + Rules
as Features? architecture. Specifically, this system
achieves an accuracy of 53.4% and a macro F-score
of 36.6% on the 2000-instance test set. This trans-
lates to a relative error reduction of 15?16% in com-
parison to the baseline result shown in row 1. A
closer examination of these results reveals that the
hand-crafted rules used by the system correctly clas-
sify 239 of the 305 test instances to which they are
applicable. In other words, the rules achieve a preci-
sion of 78.3% and a recall of 15.3% on the test data.
Our results suggest that the rules are effective at
improving performance when they are used to make
classification decisions prior to the application of
the classifier, as the performance of the ?Rules +
Features + Rules as Features? architecture is sig-
nificantly better than that of the ?Features + Rules
as Features? architecture.8 On the other hand, the
?Rules + Features + Rules as Features? architecture
does not benefit from the use of rules as features,
as its performance is statistically indistinguishable
from that of the ?Rules + Features? architecture.
Nevertheless, both ?Rules + Features + Rules as
Features? and ?Rules + Features? are significantly
8Unless otherwise stated, all statistical significance tests are
paired t-tests, with p < 0.05.
better than the remaining four architectures. This
suggests that the best-performing approach for our
14-class temporal relation classification task is the
hybrid approach where high-accuracy rules are first
applied and then the learned classifier is used to clas-
sify those cases that cannot be handled by the rules.
Among the remaining four architectures, ?All
Rules with accuracy ? 0.8?, the version of the rule-
based architecture where only the high-accuracy
rules are used, performs significantly worse than the
others, presumably because the coverage of the rule-
set is low. The results of the two feature-based archi-
tectures, ?Features? and ?Features + Rules as Fea-
tures?, are statistically indistinguishable from each
other at the p < 0.01 level. At the p < 0.05
level, however, their results are mixed: ?Features +
Rules as Features? is better than ?Features? accord-
ing to accuracy, whereas the reverse is true accord-
ing to macro F-score. Combining these results with
those we discussed above concerning the ?Rules +
Features? and ?Rules + Features + Rules as Fea-
tures? architectures, we can conclude that the fea-
tures encoding the hand-crafted rules are (mildly)
useful only when used in combination with a weak-
performing system. Finally, comparing the ?Fea-
tures? architecture and the ?All Rules? architecture,
we also see mixed results: ?Features? is better than
?All Rules? according to accuracy, whereas the re-
verse is true according to macro F-score. These
results confirm our earlier hypothesis that the rule-
based system is indeed better at identifying instances
of minority relation types.
Next, to determine whether the addition of a par-
ticular type of features to the feature set is use-
ful, we apply the paired t-test to each pair of ad-
jacent rows in Table 3. We found that adding
pairwise features, dependency relations, and most
925
Event-Event Event-Time
Feature Type Acc Fma Acc Fma
1 Baseline 36.7 15.6 63.3 19.2
2 + Pairwise 40.4 25.4 64.7 24.2
3 + Dependencies 42.4 28.4 64.9 25.4
4 + WordNet 42.6 28.1 64.7 25.3
5 + Webster 43.0 29.7 64.6 25.3
6 + PropBank 43.2 28.6 64.3 25.1
7 + Discourse 46.8 36.3 65.4 26.4
Table 4: Event-event and event-time classification results
of our best system (Rules + Features+ Rules as features).
importantly, discourse relations significantly im-
proves both accuracy and macro F-score (p < 0.05).
Adding the Webster relations improves accuracy at a
slightly lower significance level (p < 0.07) but does
not significantly improve macro F-score. Some-
what counter-intuitively, the WordNet and predicate-
argument relations are not useful. We speculate that
their failure to improve performance could be at-
tributed to the fact that these relations are extracted
by imperfect analyzers. Additional experiments in-
volving the use of gold-standard quality features are
needed to precisely determine the reason.
Recall that the results shown in Table 3 were com-
puted over both the order (i.e., event-event) and an-
chor (i.e., event-time) temporal relations. To gain
additional insights into our best-performing system,
we show in Table 4 its performance on classify-
ing event-event and event-time relations separately.
In comparison to the baseline, both accuracy and
macro F-score increase significantly when our sys-
tem is used in combination with all feature types.
In particular, our system yields a relative error re-
duction of 16?25% for event-event classification and
6?9% for event-time classification over the base-
line. The pairwise features, as well as dependency
relations and discourse relations, contribute signif-
icantly to the classification of both event-event and
event-time relations.
Finally, we show in Table 5 the per-class results
of the baseline system and our best-performing sys-
tem. As we can see, our system performs signifi-
cantly better than the baseline on all relation types,
owing to a simultaneous rise in recall and precision.
6 Conclusions
We have investigated a knowledge-rich, hybrid ap-
proach to the 14-class temporal relation classifica-
Baseline Our System
Relation R P F R P F
Simultaneous 22.5 30.5 25.9 29.5 39.5 33.8
Identity 56.5 51.5 53.9 59.0 57.5 58.2
Before 39.5 38.5 39.0 50.5 50.5 50.5
After 50.5 35.0 41.4 59.5 44.5 50.9
IBefore 0.0 0.0 0.0 32.5 85.5 47.1
IAfter 0.0 0.0 0.0 5.5 50.0 9.9
Includes 54.5 50.5 52.4 61.0 55.5 58.1
Is Included 71.5 64.5 67.8 74.5 65.0 69.4
During 11.0 31.0 16.2 19.0 34.5 24.5
During Inv 14.0 20.0 16.5 19.5 40.5 26.3
Begins 4.5 10.0 6.2 37.0 43.5 40.0
Begun By 6.5 14.5 9.0 35.0 44.0 39.0
Ends 6.5 10.0 7.9 23.5 70.0 35.2
Ended By 9.0 10.0 9.5 29.0 26.5 27.7
Table 5: Per-class results of the baseline system and our
best system (Rules + Features+ Rules as features).
tion task. Results on the TimeBank corpus show
that our approach achieves a relative error reduction
of 15?16% over a learning-based baseline that em-
ploys a state-of-the-art feature set. Our results sug-
gest that (1) the pairwise features, dependency rela-
tions, and discourse relations are useful for temporal
relation classification; and (2) hand-crafted rules can
better handle the skewed class distribution underly-
ing our dataset via improving minority class predic-
tion. To our knowledge, we are the first to (1) re-
port results for the 14-class temporal relation clas-
sification task on TimeBank; (2) successfully em-
ploy PDTB-style discourse relations to improve this
task; and (3) show that a hybrid approach to this task
can yield better results than either a rule-based or
learning-based approach. To stimulate research on
this task, we make our complete set of hand-crafted
rules available to other researchers. We believe that
hybrid rule-based and learning-based approaches are
promising approaches to language processing tasks
that require complex reasoning and hope that they
will be given more attention in the community.
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier draft
of the paper. This work was supported in part by
NSF Grants IIS-1147644 and IIS-1219142. Any
opinions, findings, or conclusions expressed in this
paper are those of the authors and do not necessarily
reflect the views or official policies of NSF.
926
References
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying temporal relations between events.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics Companion Vol-
ume: Proceedings of the Demo and Poster Sessions,
pages 173?176.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
pages 33?40.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Eun Young Ha, Alok Baikadi, Carlyle Licata, and James
Lester. 2010. NCSU: Modeling temporal relations
with markov logic and lexical ontology. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 341?344.
Janet Hitzeman, Marc Moens, and Claire Grover. 1995.
Algorithms for analysing the temporal structure of dis-
course. In Proceedings of the 7th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 253?260.
Judith Klavans and Philip Resnik, editors. 1994. The
Balancing Act: Combining Symbolic and Statistical
Approaches to Language. Association for Computa-
tional Linguistics.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2013.
A PDTB-styled end-to-end discourse parser. Natural
Language Engineering (to appear).
Hector Llorens, Estela Saquete, and Borja Navarro.
2010. TIPSem (English and Spanish): Evaluating
CRFs and semantic roles in TempEval-2. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 284?291.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 753?760.
Congmin Min, Munirathnam Srikanth, and Abraham
Fowler. 2007. LCC-TE: A hybrid approach to tem-
poral relation identification in news text. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 219?222.
Seyed Abolghasem Mirroshandel and Gholamreza
Ghassem-Sani. 2011. Temporal relation extraction
using expectation maximization. In Proceedings of the
International Conference Recent Advances in Natural
Language Processing 2011, pages 218?225.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation.
Georgiana Pus?cas?u. 2007. WVALI: Temporal relation
identification by syntactico-semantic analysis. In Pro-
ceedings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 484?487.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew
See, David Day, Lisa Ferro, Robert Gaizauskas, Mar-
cia Lazo, Andrea Setzer, and Beth Sundheim. 2003.
The TimeBank corpus. In Corpus Linguistics, pages
647?656.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the 21st International
Conference on Machine Learning, pages 104?112.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 Task 15: TempEval tempo-
ral relation identification. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 75?80.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 Task 13:
TempEval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62.
927
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1396?1411,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Supervised Noun Phrase Coreference Research: The First Fifteen Years
Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
vince@hlt.utdallas.edu
Abstract
The research focus of computational
coreference resolution has exhibited a
shift from heuristic approaches to machine
learning approaches in the past decade.
This paper surveys the major milestones in
supervised coreference research since its
inception fifteen years ago.
1 Introduction
Noun phrase (NP) coreference resolution, the task
of determining which NPs in a text or dialogue re-
fer to the same real-world entity, has been at the
core of natural language processing (NLP) since
the 1960s. NP coreference is related to the task
of anaphora resolution, whose goal is to identify
an antecedent for an anaphoric NP (i.e., an NP
that depends on another NP, specifically its an-
tecedent, for its interpretation) [see van Deemter
and Kibble (2000) for a detailed discussion of the
difference between the two tasks]. Despite its sim-
ple task definition, coreference is generally con-
sidered a difficult NLP task, typically involving
the use of sophisticated knowledge sources and
inference procedures (Charniak, 1972). Compu-
tational theories of discourse, in particular focus-
ing (see Grosz (1977) and Sidner (1979)) and cen-
tering (Grosz et al (1983; 1995)), have heavily
influenced coreference research in the 1970s and
1980s, leading to the development of numerous
centering algorithms (see Walker et al (1998)).
The focus of coreference research underwent a
gradual shift from heuristic approaches to machine
learning approaches in the 1990s. This shift can
be attributed in part to the advent of the statisti-
cal NLP era, and in part to the public availability
of annotated coreference corpora produced as part
of the MUC-6 (1995) and MUC-7 (1998) confer-
ences. Learning-based coreference research has
remained vibrant since then, with results regularly
published not only in general NLP conferences,
but also in specialized conferences (e.g., the bien-
nial Discourse Anaphora and Anaphor Resolution
Colloquium (DAARC)) and workshops (e.g., the
series of Bergen Workshop on Anaphora Resolu-
tion (WAR)). Being inherently a clustering task,
coreference has also received a lot of attention in
the machine learning community.
Fifteen years have passed since the first paper
on learning-based coreference resolution was pub-
lished (Connolly et al, 1994). Our goal in this
paper is to provide NLP researchers with a sur-
vey of the major milestones in supervised coref-
erence research, focusing on the computational
models, the linguistic features, the annotated cor-
pora, and the evaluation metrics that were devel-
oped in the past fifteen years. Note that several
leading coreference researchers have published
books (e.g., Mitkov (2002)), written survey arti-
cles (e.g., Mitkov (1999), Strube (2009)), and de-
livered tutorials (e.g., Strube (2002), Ponzetto and
Poesio (2009)) that provide a broad overview of
coreference research. This survey paper aims to
complement, rather than supersede, these previ-
ously published materials. In particular, while ex-
isting survey papers discuss learning-based coref-
erence research primarily in the context of the in-
fluential mention-pair model, we additionally sur-
vey recently proposed learning-based coreference
models, which attempt to address the weaknesses
of the mention-pair model. Due to space limita-
tions, however, we will restrict our discussion to
the most commonly investigated kind of corefer-
ence relation: the identity relation for NPs, exclud-
ing coreference among clauses and bridging refer-
ences (e.g., part/whole and set/subset relations).
2 Annotated Corpora
The widespread popularity of machine learning
approaches to coreference resolution can be at-
tributed in part to the public availability of an-
1396
notated coreference corpora. The MUC-6 and
MUC-7 corpora, though relatively small (60 doc-
uments each) and homogeneous w.r.t. document
type (newswire articles only), have been exten-
sively used for training and evaluating coreference
models. Equally popular are the corpora produced
by the Automatic Content Extraction (ACE1) eval-
uations in the past decade: while the earlier ACE
corpora (e.g., ACE-2) consist of solely English
newswire and broadcast news articles, the later
ones (e.g., ACE 2005) have also included Chi-
nese and Arabic documents taken from additional
sources such as broadcast conversations, webblog,
usenet, and conversational telephone speech.
Coreference annotations are also publicly avail-
able in treebanks. These include (1) the English
Penn Treebank (Marcus et al, 1993), which is la-
beled with coreference links as part of the Onto-
Notes project (Hovy et al, 2006); (2) the Tu?bingen
Treebank (Telljohann et al, 2004), which is a
collection of German news articles consisting of
27,125 sentences; (3) the Prague Dependency
Treebank (Hajic? et al, 2006), which consists of
3168 news articles taken from the Czech National
Corpus; (4) the NAIST Text Corpus (Iida et al,
2007b), which consists of 287 Japanese news arti-
cles; (5) the AnCora Corpus (Recasens and Mart??,
2009), which consists of Spanish and Catalan jour-
nalist texts; and (6) the GENIA corpus (Ohta et al,
2002), which contains 2000 MEDLINE abstracts.
Other publicly available coreference corpora of
interest include two annotated by Ruslan Mitkov?s
research group: (1) a 55,000-word corpus in
the domain of security/terrorism (Hasler et al,
2006); and (2) training data released as part of the
2007 Anaphora Resolution Exercise (Ora?san et al,
2008), a coreference resolution shared task. There
are also two that consist of spoken dialogues: the
TRAINS93 corpus (Heeman and Allen, 1995) and
the Switchboard data set (Calhoun et al, in press).
Additional coreference data will be available in
the near future. For instance, the SemEval-2010
shared task on Coreference Resolution in Multiple
Languages (Recasens et al, 2009) has promised to
release coreference data in six languages. In addi-
tion, Massimo Poesio and his colleagues are lead-
ing an annotation project that aims to collect large
amounts of coreference data for English via a Web
Collaboration game called Phrase Detectives2.
1http://www.itl.nist.gov/iad/mig/tests/ace/
2http://www.phrasedetectives.org
3 Learning-Based Coreference Models
In this section, we examine three important classes
of coreference models that were developed in the
past fifteen years, namely, the mention-pair model,
the entity-mention model, and ranking models.
3.1 Mention-Pair Model
The mention-pair model is a classifier that deter-
mines whether two NPs are coreferent. It was
first proposed by Aone and Bennett (1995) and
McCarthy and Lehnert (1995), and is one of the
most influential learning-based coreference mod-
els. Despite its popularity, this binary classifica-
tion approach to coreference is somewhat undesir-
able: the transitivity property inherent in the coref-
erence relation cannot be enforced, as it is possible
for the model to determine that A and B are coref-
erent, B and C are coreferent, but A and C are not
coreferent. Hence, a separate clustering mecha-
nism is needed to coordinate the pairwise classifi-
cation decisions made by the model and construct
a coreference partition.
Another issue that surrounds the acquisition of
the mention-pair model concerns the way train-
ing instances are created. Specifically, to deter-
mine whether a pair of NPs is coreferent or not,
the mention-pair model needs to be trained on a
data set where each instance represents two NPs
and possesses a class value that indicates whether
the two NPs are coreferent. Hence, a natural way
to assemble a training set is to create one instance
from each pair of NPs appearing in a training doc-
ument. However, this instance creation method is
rarely employed: as most NP pairs in a text are not
coreferent, this method yields a training set with a
skewed class distribution, where the negative in-
stances significantly outnumber the positives.
As a result, in practical implementations of the
mention-pair model, one needs to specify not only
the learning algorithm for training the model and
the linguistic features for representing an instance,
but also the training instance creation method for
reducing class skewness and the clustering algo-
rithm for constructing a coreference partition.
3.1.1 Creating Training Instances
As noted above, the primary purpose of train-
ing instance creation is to reduce class skewness.
Many heuristic instance creation methods have
been proposed, among which Soon et al?s (1999;
2001) is arguably the most popular choice. Given
1397
an anaphoric noun phrase3, NPk , Soon et al?s
method creates a positive instance between NPk
and its closest preceding antecedent, NPj , and a
negative instance by pairing NPk with each of the
intervening NPs, NPj+1, . . ., NPk?1.
With an eye towards improving the precision of
a coreference resolver, Ng and Cardie (2002c) pro-
pose an instance creation method that involves a
single modification to Soon et al?s method: if NPk
is non-pronominal, a positive instance should be
formed between NPk and its closest preceding non-
pronominal antecedent instead. This modification
is motivated by the observation that it is not easy
for a human, let alne a machine learner, to learn
from a positive instance where the antecedent of a
non-pronominal NP is a pronoun.
To further reduce class skewness, some re-
searchers employ a filtering mechanism on top of
an instance creation method, thereby disallowing
the creation of training instances from NP pairs
that are unlikely to be coreferent, such as NP pairs
that violate gender and number agreement (e.g.,
Strube et al (2002), Yang et al (2003)).
While many instance creation methods are
heuristic in nature (see Uryupina (2004) and Hoste
and Daelemans (2005)), some are learning-based.
For example, motivated by the fact that some
coreference relations are harder to identify than
the others (see Harabagiu et al (2001)), Ng and
Cardie (2002a) present a method for mining easy
positive instances, in an attempt to avoid the inclu-
sion of hard training instances that may complicate
the acquisition of an accurate coreference model.
3.1.2 Training a Coreference Classifier
Once a training set is created, we can train a coref-
erence model using an off-the-shelf learning algo-
rithm. Decision tree induction systems (e.g., C5
(Quinlan, 1993)) are the first and one of the most
widely used learning algorithms by coreference
researchers, although rule learners (e.g., RIPPER
(Cohen, 1995)) and memory-based learners (e.g.,
TiMBL (Daelemans and Van den Bosch, 2005))
are also popular choices, especially in early appli-
cations of machine learning to coreference resolu-
tion. In recent years, statistical learners such as
maximum entropy models (Berger et al, 1996),
voted perceptrons (Freund and Schapire, 1999),
3In this paper, we use the term anaphoric to describe any
NP that is part of a coreference chain but is not the head of
the chain. Hence, proper names can be anaphoric under this
overloaded definition, but linguistically, they are not.
and support vector machines (Joachims, 1999)
have been increasingly used, in part due to their
ability to provide a confidence value (e.g., in the
form of a probability) associated with a classifica-
tion, and in part due to the fact that they can be
easily adapted to train recently proposed ranking-
based coreference models (see Section 3.3).
3.1.3 Generating an NP Partition
After training, we can apply the resulting model
to a test text, using a clustering algorithm to co-
ordinate the pairwise classification decisions and
impose an NP partition. Below we describe some
commonly used coreference clustering algorithms.
Despite their simplicity, closest-first cluster-
ing (Soon et al, 2001) and best-first clustering
(Ng and Cardie, 2002c) are arguably the most
widely used coreference clustering algorithms.
The closest-first clustering algorithm selects as the
antecedent for an NP, NPk, the closest preceding
noun phrase that is classified as coreferent with it.4
However, if no such preceding noun phrase exists,
no antecedent is selected for NPk . The best-first
clustering algorithm aims to improve the precision
of closest-first clustering, specifically by selecting
as the antecedent of NPk the most probable preced-
ing NP that is classified as coreferent with it.
One criticism of the closest-first and best-first
clustering algorithms is that they are too greedy.
In particular, clusters are formed based on a small
subset of the pairwise decisions made by the
model. Moreover, positive pairwise decisions are
unjustifiably favored over their negative counter-
parts. For example, three NPs are likely to end up
in the same cluster in the resulting partition even if
there is strong evidence that A and C are not coref-
erent, as long as the other two pairs (i.e., (A,B) and
(B,C)) are classified as positive.
Several algorithms that address one or both of
these problems have been used for coreference
clustering. Correlation clustering (Bansal et al,
2002), which produces a partition that respects
as many pairwise decisions as possible, is used
by McCallum and Wellner (2004), Zelenko et al
(2004), and Finley and Joachims (2005). Graph
partitioning algorithms are applied on a weighted,
undirected graph where a vertex corresponds to
an NP and an edge is weighted by the pairwise
coreference scores between two NPs (e.g., Mc-
Callum and Wellner (2004), Nicolae and Nico-
4If a probabilistic model is used, we can define a threshold
above which a pair of NPs is considered coreferent.
1398
lae (2006)). The Dempster-Shafer rule (Dempster,
1968), which combines the positive and negative
pairwise decisions to score a partition, is used by
Kehler (1997) and Bean and Riloff (2004) to iden-
tify the most probable NP partition.
Some clustering algorithms bear a closer resem-
blance to the way a human creates coreference
clusters. In these algorithms, not only are the NPs
in a text processed in a left-to-right manner, the
later coreference decisions are dependent on the
earlier ones (Cardie and Wagstaff, 1999; Klenner
and Ailloud, 2008).5 For example, to resolve an
NP, NPk , Cardie and Wagstaff?s algorithm consid-
ers each preceding NP, NPj , as a candidate an-
tecedent in a right-to-left order. If NPk and NPj
are likely to be coreferent, the algorithm imposes
an additional check that NPk does not violate any
constraint on coreference (e.g., gender agreement)
with any NP in the cluster containing NPj before
positing that the two NPs are coreferent.
Luo et al?s (2004) Bell-tree-based algorithm is
another clustering algorithm where the later coref-
erence decisions are dependent on the earlier ones.
A Bell tree provides an elegant way of organizing
the space of NP partitions. Informally, a node in
the ith level of a Bell tree corresponds to an ith-
order partial partition (i.e., a partition of the first
i NPs of the given document), and the ith level of
the tree contains all possible ith-order partial parti-
tions. Hence, a leaf node contains a complete par-
tition of the NPs, and the goal is to search for the
leaf node that contains the most probable partition.
The search starts at the root, and a partitioning of
the NPs is incrementally constructed as we move
down the tree. Specifically, based on the corefer-
ence decisions it has made in the first i?1 levels of
the tree, the algorithm determines at the ith level
whether the ith NP should start a new cluster, or to
which preceding cluster it should be assigned.
While many coreference clustering algorithms
have been developed, there have only been a few
attempts to compare their effectiveness. For ex-
ample, Ng and Cardie (2002c) report that best-
first clustering is better than closest-first cluster-
ing. Nicolae and Nicolae (2006) show that best-
first clustering performs similarly to Bell-tree-
based clustering, but neither of these algorithms
5When applying closest-first and best-first clustering,
Soon et al (2001) and Ng and Cardie (2002c) also process
the NPs in a sequential manner, but since the later decisions
are not dependent on the earlier ones, the order in which the
NPs are processed does not affect their clustering results.
performs as well as their proposed minimum-cut-
based graph partitioning algorithm.
3.1.4 Determining NP Anaphoricity
While coreference clustering algorithms attempt
to resolve each NP encountered in a document,
only a subset of the NPs are anaphoric and there-
fore need to be resolved. Hence, knowledge of the
anaphoricity of an NP can potentially improve the
precision of a coreference resolver.
Traditionally, the task of anaphoricity determi-
nation has been tackled independently of corefer-
ence resolution using a variety of techniques. For
example, pleonastic it has been identified using
heuristic approaches (e.g., Paice and Husk (1987),
Lappin and Leass (1994), Kennedy and Bogu-
raev (1996)), supervised approaches (e.g., Evans
(2001), Mu?ller (2006), Versley et al (2008a)),
and distributional methods (e.g., Bergsma et al
(2008)); and non-anaphoric definite descriptions
have been identified using rule-based techniques
(e.g., Vieira and Poesio (2000)) and unsupervised
techniques (e.g., Bean and Riloff (1999)).
Recently, anaphoricity determination has been
evaluated in the context of coreference resolution,
with results showing that training an anaphoric-
ity classifier to identify and filter non-anaphoric
NPs prior to coreference resolution can improve
a learning-based resolver (e.g., Ng and Cardie
(2002b), Uryupina (2003), Poesio et al (2004b)).
Compared to earlier work on anaphoricity deter-
mination, recently proposed approaches are more
?global? in nature, taking into account the pair-
wise decisions made by the mention-pair model
when making anaphoricity decisions. Examples
of such approaches have exploited techniques in-
cluding integer linear programming (ILP) (Denis
and Baldridge, 2007a), label propagation (Zhou
and Kong, 2009), and minimum cuts (Ng, 2009).
3.1.5 Combining Classification & Clustering
From a learning perspective, a two-step approach
to coreference ? classification and clustering ?
is undesirable. Since the classification model
is trained independently of the clustering algo-
rithm, improvements in classification accuracy
do not guarantee corresponding improvements in
clustering-level accuracy. That is, overall perfor-
mance on the coreference task might not improve.
To address this problem, McCallum and Well-
ner (2004) and Finley and Joachims (2005) elimi-
nate the classification step entirely, treating coref-
1399
erence as a supervised clustering task where a sim-
ilarity metric is learned to directly maximize clus-
tering accuracy. Klenner (2007) and Finkel and
Manning (2008) use ILP to ensure that the pair-
wise classification decisions satisfy transitivity.6
3.1.6 Weaknesses of the Mention-Pair Model
While many of the aforementioned algorithms
for clustering and anaphoricity determination have
been shown to improve coreference performance,
the underlying model with which they are used
in combination ? the mention-pair model ? re-
mains fundamentally weak. The model has two
commonly-cited weaknesses. First, since each
candidate antecedent for an anaphoric NP to be
resolved is considered independently of the oth-
ers, the model only determines how good a candi-
date antecedent is relative to the anaphoric NP, but
not how good a candidate antecedent is relative to
other candidates. In other words, it fails to answer
the question of which candidate antecedent is most
probable. Second, it has limitations in its expres-
siveness: the information extracted from the two
NPs alone may not be sufficient for making an in-
formed coreference decision, especially if the can-
didate antecedent is a pronoun (which is semanti-
cally empty) or a mention that lacks descriptive in-
formation such as gender (e.g., ?Clinton?). Below
we discuss how these weaknesses are addressed by
the entity-mention model and ranking models.
3.2 Entity-Mention Model
The entity-mention model addresses the expres-
siveness problem with the mention-pair model.
To motivate the entity-mention model, consider
an example taken from McCallum and Wellner
(2003), where a document consists of three NPs:
?Mr. Clinton,? ?Clinton,? and ?she.? The mention-
pair model may determine that ?Mr. Clinton? and
?Clinton? are coreferent using string-matching
features, and that ?Clinton? and ?she? are coref-
erent based on proximity and lack of evidence for
gender and number disagreement. However, these
two pairwise decisions together with transitivity
imply that ?Mr. Clinton? and ?she? will end up in
the same cluster, which is incorrect due to gen-
der mismatch. This kind of error arises in part
because the later coreference decisions are not de-
pendent on the earlier ones. In particular, had the
model taken into consideration that ?Mr. Clinton?
6Recently, however, Klenner and Ailloud (2009) have be-
come less optimistic about ILP approaches to coreference.
and ?Clinton? were in the same cluster, it proba-
bly would not have posited that ?she? and ?Clin-
ton? are coreferent. The aforementioned Cardie
and Wagstaff algorithm attempts to address this
problem in a heuristic manner. It would be de-
sirable to learn a model that can classify whether
an NP to be resolved is coreferent with a preced-
ing, possibly partially-formed, cluster. This model
is commonly known as the entity-mention model.
Since the entity-mention model aims to classify
whether an NP is coreferent with a preceding clus-
ter, each of its training instances (1) corresponds
to an NP, NPk , and a preceding cluster, Cj , and
(2) is labeled with either POSITIVE or NEGATIVE,
depending on whether NPk should be assigned to
Cj . Consequently, we can represent each instance
by a set of cluster-level features (i.e., features that
are defined over an arbitrary subset of the NPs in
Cj). A cluster-level feature can be computed from
a feature employed by the mention-pair model by
applying a logical predicate. For example, given
the NUMBER AGREEMENT feature, which deter-
mines whether two NPs agree in number, we can
apply the ALL predicate to create a cluster-level
feature, which has the value YES if NPk agrees in
number with all of the NPs in Cj and NO other-
wise. Other commonly-used logical predicates for
creating cluster-level features include relaxed ver-
sions of the ALL predicate, such as MOST, which
is true if NPk agrees in number with more than half
of the NPs in Cj , and ANY, which is true as long as
NPk agrees in number with just one of the NPs in
Cj . The ability of the entity-mention model to em-
ploy cluster-level features makes it more expres-
sive than its mention-pair counterpart.
Despite its improved expressiveness, the entity-
mention model has not yielded particularly en-
couraging results. For example, Luo et al (2004)
apply the ANY predicate to generate cluster-level
features for their entity-mention model, which
does not perform as well as the mention-pair
model. Yang et al (2004b; 2008a) also investi-
gate the entity-mention model, which produces re-
sults that are only marginally better than those of
the mention-pair model. However, it appears that
they are not fully exploiting the expressiveness of
the entity-mention model, as cluster-level features
only comprise a small fraction of their features.
Variants of the entity-mention model have been
investigated. For example, Culotta et al (2007)
present a first-order logic model that determines
1400
the probability that an arbitrary set of NPs are all
co-referring. Their model resembles the entity-
mention model in that it enables the use of cluster-
level features. Daume? III and Marcu (2005) pro-
pose an online learning model for constructing
coreference chains in an incremental fashion, al-
lowing later coreference decisions to be made by
exploiting cluster-level features that are computed
over the coreference chains created thus far.
3.3 Ranking Models
While the entity-mention model addresses the
expressiveness problem with the mention-pair
model, it does not address the other problem: fail-
ure to identify the most probable candidate an-
tecedent. Ranking models, on the other hand, al-
low us to determine which candidate antecedent
is most probable given an NP to be resolved.
Ranking is arguably a more natural reformula-
tion of coreference resolution than classification,
as a ranker allows all candidate antecedents to be
considered simultaneously and therefore directly
captures the competition among them. Another
desirable consequence is that there exists a nat-
ural resolution strategy for a ranking approach:
an anaphoric NP is resolved to the candidate an-
tecedent that has the highest rank. This contrasts
with classification-based approaches, where many
clustering algorithms have been employed to co-
ordinate the pairwise classification decisions, and
it is still not clear which of them is the best.
The notion of ranking candidate antecedents
can be traced back to centering algorithms, many
of which use grammatical roles to rank forward-
looking centers (see Walker et al (1998)). Rank-
ing is first applied to learning-based coreference
resolution by Connolly et al (1994; 1997), where
a model is trained to rank two candidate an-
tecedents. Each training instance corresponds to
the NP to be resolved, NPk, as well as two candi-
date antecedents, NPi and NPj , one of which is an
antecedent of NPk and the other is not. Its class
value indicates which of the two candidates is bet-
ter. This model is referred to as the tournament
model by Iida et al (2003) and the twin-candidate
model by Yang et al (2003; 2008b). To resolve an
NP during testing, one way is to apply the model to
each pair of its candidate antecedents, and the can-
didate that is classified as better the largest number
of times is selected as its antecedent.
Advances in machine learning have made it pos-
sible to train a mention ranker that ranks all of
the candidate antecedents simultaneously. While
mention rankers have consistently outperformed
the mention-pair model (Versley, 2006; Denis and
Baldridge, 2007b), they are not more expressive
than the mention-pair model, as they are unable
to exploit cluster-level features, unlike the entity-
mention model. To enable rankers to employ
cluster-level features, Rahman and Ng (2009) pro-
pose the cluster-ranking model, which ranks pre-
ceding clusters, rather than candidate antecedents,
for an NP to be resolved. Cluster rankers there-
fore address both weaknesses of the mention-pair
model, and have been shown to improve mention
rankers. Cluster rankers are conceptually similar
to Lappin and Leass?s (1994) heuristic pronoun re-
solver, which resolves an anaphoric pronoun to the
most salient preceding cluster.
An important issue with ranking models that
we have eluded so far concerns the identification
of non-anaphoric NPs. As a ranker simply im-
poses a ranking on candidate antecedents or pre-
ceding clusters, it cannot determine whether an NP
is anaphoric (and hence should be resolved). To
address this problem, Denis and Baldridge (2008)
apply an independently trained anaphoricity clas-
sifier to identify non-anaphoric NPs prior to rank-
ing, and Rahman and Ng (2009) propose a model
that jointly learns coreference and anaphoricity.
4 Knowledge Sources
Another thread of supervised coreference research
concerns the development of linguistic features.
Below we give an overview of these features.
String-matching features can be computed ro-
bustly and typically contribute a lot to the per-
formance of a coreference system. Besides sim-
ple string-matching operations such as exact string
match, substring match, and head noun match
for different kinds of NPs (see Daume? III and
Marcu (2005)), slightly more sophisticated string-
matching facilities have been attempted, includ-
ing minimum edit distance (Strube et al, 2002)
and longest common subsequence (Castan?o et al,
2002). Yang et al (2004a) treat the two NPs in-
volved as two bags of words, and compute their
similarity using metrics commonly-used in infor-
mation retrieval, such as the dot product, with each
word weighted by their TF-IDF value.
Syntactic features are computed based on a
syntactic parse tree. Ge et al (1998) implement
1401
a Hobbs distance feature, which encodes the rank
assigned to a candidate antecedent for a pronoun
by Hobbs?s (1978) seminal syntax-based pronoun
resolution algorithm. Luo and Zitouni (2005) ex-
tract features from a parse tree for implement-
ing Binding Constraints (Chomsky, 1988). Given
an automatically parsed corpus, Bergsma and Lin
(2006) extract from each parse tree a dependency
path, which is represented as a sequence of nodes
and dependency labels connecting a pronoun and
a candidate antecedent, and collect statistical in-
formation from these paths to determine the like-
lihood that a pronoun and a candidate antecedent
connected by a given path are coreferent. Rather
than deriving features from parse trees, Iida et al
(2006) and Yang et al (2006) employ these trees
directly as structured features for pronoun resolu-
tion. Specifically, Yang et al define tree kernels
for efficiently computing the similarity between
two parse trees, and Iida et al use a boosting-based
algorithm to compute the usefulness of a subtree.
Grammatical features encode the grammati-
cal properties of one or both NPs involved in an
instance. For example, Ng and Cardie?s (2002c)
resolver employs 34 grammatical features. Some
features determine NP type (e.g., are both NPs def-
inite or pronouns?). Some determine the grammat-
ical role of one or both of the NPs. Some encode
traditional linguistic (hard) constraints on corefer-
ence. For example, coreferent NPs have to agree
in number and gender and cannot span one an-
other (e.g., ?Google? and ?Google employees?).
There are also features that encode general linguis-
tic preferences either for or against coreference.
For example, an indefinite NP (that is not in ap-
position to an anaphoric NP) is not likely to be
coreferent with any NP that precedes it.
There has been an increasing amount of work on
investigating semantic features for coreference
resolution. One of the earliest kinds of seman-
tic knowledge employed for coreference resolu-
tion is perhaps selectional preference (Dagan and
Itai, 1990; Kehler et al, 2004b; Yang et al, 2005;
Haghighi and Klein, 2009): given a pronoun to be
resolved, its governing verb, and its grammatical
role, we prefer a candidate antecedent that can be
governed by the same verb and be in the same role.
Semantic knowledge has also been extracted from
WordNet and unannotated corpora for computing
the semantic compatibility/similarity between two
common nouns (Harabagiu et al, 2001; Versley,
2007) as well as the semantic class of a noun (Ng,
2007a; Huang et al, 2009). One difficulty with
deriving knowledge from WordNet is that one has
to determine which sense of a given word to use.
Some researchers simply use the first sense (Soon
et al, 2001) or all possible senses (Ponzetto and
Strube, 2006a), while others overcome this prob-
lem with word sense disambiguation (Nicolae and
Nicolae, 2006). Knowledge has also been mined
from Wikipedia for measuring the semantic relat-
edness of two NPs, NPj and NPk (Ponzetto and
Strube (2006a; 2007)), such as: whether NPj/k ap-
pears in the first paragraph of the Wiki page that
has NPk/j as the title or in the list of categories to
which this page belongs, and the degree of overlap
between the two pages that have the two NPs as
their titles (see Poesio et al (2007) for other uses
of encyclopedic knowledge for coreference reso-
lution). Contextual roles (Bean and Riloff, 2004),
semantic relations (Ji et al, 2005), semantic roles
(Ponzetto and Strube, 2006b; Kong et al, 2009),
and animacy (Ora?san and Evans, 2007) have also
been exploited to improve coreference resolution.
Lexico-syntactic patterns have been used to
capture the semantic relatedness between two NPs
and hence the likelihood that they are coreferent.
For instance, given the pattern X is a Y (which is
highly indicative that X and Y are coreferent), we
can instantiate it with a pair of NPs and search
for the instantiated pattern in a large corpus or
the Web (Daume? III and Marcu, 2005; Haghighi
and Klein, 2009). The more frequently the pat-
tern occurs, the more likely they are coreferent.
This technique has been applied to resolve dif-
ferent kinds of anaphoric references, including
other-anaphora (Modjeska et al, 2003; Markert
and Nissim, 2005) and bridging references (Poesio
et al, 2004a). While these patterns are typically
hand-crafted (e.g., Garera and Yarowsky (2006)),
they can also be learned from an annotated cor-
pus (Yang and Su, 2007) or bootstrapped from an
unannotated corpus (Bean and Riloff, 2004).
Despite the large amount of work on discourse-
based anaphora resolution in the 1970s and
1980s (see Hirst (1981)), learning-based resolvers
have only exploited shallow discourse-based fea-
tures, which primarily involve characterizing the
salience of a candidate antecedent by measuring
its distance from the anaphoric NP to be resolved
or determining whether it is in a prominent gram-
matical role (e.g., subject). A notable exception
1402
is Iida et al (2009), who train a ranker to rank
the candidate antecedents for an anaphoric pro-
noun by their salience. It is worth noting that
Tetreault (2005) has employed Grosz and Sid-
ner?s (1986) discourse theory and Veins Theory
(Ide and Cristea, 2000) to identify and remove
candidate antecedents that are not referentially ac-
cessible to an anaphoric pronoun in his heuristic
pronoun resolvers. It would be interesting to in-
corporate this idea into a learning-based resolver.
There are also features that do not fall into any
of the preceding categories. For example, a mem-
orization feature is a word pair composed of the
head nouns of the two NPs involved in an in-
stance (Bengtson and Roth, 2008). Memoriza-
tion features have been used as binary-valued fea-
tures indicating the presence or absence of their
words (Luo et al, 2004) or as probabilistic fea-
tures indicating the probability that the two heads
are coreferent according to the training data (Ng,
2007b). An anaphoricity feature indicates whether
an NP to be resolved is anaphoric, and is typ-
ically computed using an anaphoricity classifier
(Ng, 2004), hand-crafted patterns (Daume? III and
Marcu, 2005), and automatically acquired pat-
terns (Bean and Riloff, 1999). Finally, the outputs
of rule-based pronoun and coreference resolvers
have also been used as features for learning-based
coreference resolution (Ng and Cardie, 2002c).
For an empirical evaluation of the contribution
of a subset of these features to the mention-pair
model, see Bengtson and Roth (2008).
5 Evaluation Issues
Two important issues surround the evaluation of a
coreference resolver. First, how do we obtain the
set of NPs that a resolver will partition? Second,
how do we score the partition it produces?
5.1 Extracting Candidate Noun Phrases
To obtain the set of NPs to be partitioned by a re-
solver, three methods are typically used. In the
first method, the NPs are extracted automatically
from a syntactic parser. The second method in-
volves extracting the NPs directly from the gold
standard. In the third method, a mention detec-
tor is first trained on the gold-standard NPs in the
training texts, and is then applied to automatically
extract system mentions in a test text.7 Note that
7An exception is Daume? III and Marcu (2005), whose
model jointly learns to extract NPs and perform coreference.
these three extraction methods typically produce
different numbers of NPs: the NPs extracted from
a parser tend to significantly outnumber the system
mentions, which in turn outnumber the gold NPs.
The reasons are two-fold. First, in some corefer-
ence corpora (e.g., MUC-6 and MUC-7), the NPs
that are not part of any coreference chain are not
annotated. Second, in corpora such as those pro-
duced by the ACE evaluations, only the NPs that
belong to one of the ACE entity types (e.g., PER-
SON, ORGANIZATION, LOCATION) are annotated.
Owing in large part to the difference in the num-
ber of NPs extracted by these three methods, a
coreference resolver can produce substantially dif-
ferent results when applied to the resulting three
sets of NPs, with gold NPs yielding the best results
and NPs extracted from a parser yielding the worst
(Nicolae and Nicolae, 2006). While researchers
who evaluate their resolvers on gold NPs point out
that the results can more accurately reflect the per-
formance of their coreference algorithm, Stoyanov
et al (2009) argue that such evaluations are unre-
alistic, as NP extraction is an integral part of an
end-to-end fully-automatic resolver.
Whichever NP extraction method is employed,
it is clear that the use of gold NPs can considerably
simplify the coreference task, and hence resolvers
employing different extraction methods should not
be compared against each other.
5.2 Scoring a Coreference Partition
The MUC scorer (Vilain et al, 1995) is the first
program developed for scoring coreference parti-
tions. It has two often-cited weaknesses. As a link-
based measure, it does not reward correctly iden-
tified singleton clusters since there is no corefer-
ence link in these clusters. Also, it tends to under-
penalize partitions with overly large clusters.
To address these problems, two coreference
scoring programs have been developed: B3
(Bagga and Baldwin, 1998) and CEAF (Luo,
2005). Note that both scorers have only been de-
fined for the case where the key partition has the
same set of NPs as the response partition. To apply
these scorers to automatically extracted NPs, dif-
ferent methods have been proposed (see Rahman
and Ng (2009) and Stoyanov et al (2009)).
Since coreference is a clustering task, any
general-purpose method for evaluating a response
partition against a key partition (e.g., Kappa (Car-
letta, 1996)) can be used for coreference scor-
1403
ing (see Popescu-Belis et al (2004)). In practice,
these general-purpose methods are typically used
to provide scores that complement those obtained
via the three coreference scorers discussed above.
It is worth mentioning that there is a trend to-
wards evaluating a resolver against multiple scor-
ers, which can indirectly help to counteract the
bias inherent in a particular scorer. For further dis-
cussion on evaluation issues, see Byron (2001).
6 Concluding Remarks
While we have focused our discussion on super-
vised approaches, coreference researchers have
also attempted to reduce a resolver?s reliance on
annotated data by combining a small amount of
labeled data and a large amount of unlabeled
data using general-purpose semi-supervised learn-
ing algorithms such as co-training (Mu?ller et al,
2002), self-training (Kehler et al, 2004a), and EM
(Cherry and Bergsma, 2005; Ng, 2008). Interest-
ingly, recent results indicate that unsupervised ap-
proaches to coreference resolution (e.g., Haghighi
and Klein (2007; 2010), Poon and Domingos
(2008)) rival their supervised counterparts, casting
doubts on whether supervised resolvers are mak-
ing effective use of the available labeled data.
Another issue that we have not focused on but
which is becoming increasingly important is mul-
tilinguality. While many of the techniques dis-
cussed in this paper were originally developed for
English, they have been applied to learn coref-
erence models for other languages, such as Chi-
nese (e.g., Converse (2006)), Japanese (e.g., Iida
(2007)), Arabic (e.g., Luo and Zitouni (2005)),
Dutch (e.g., Hoste (2005)), German (e.g., Wun-
sch (2010)), Swedish (e.g., Nilsson (2010)), and
Czech (e.g., Ngu
.
y et al (2009)). In addition, re-
searchers have developed approaches that are tar-
geted at handling certain kinds of anaphora present
in non-English languages, such as zero anaphora
(e.g., Iida et al (2007a), Zhao and Ng (2007)).
As Mitkov (2001) puts it, coreference resolution
is a ?difficult, but not intractable problem,? and
we have been making ?slow, but steady progress?
on improving machine learning approaches to the
problem in the past fifteen years. To ensure fur-
ther progress, researchers should compare their re-
sults against a baseline that is stronger than the
commonly-used Soon et al (2001) system, which
relies on a weak model (i.e., the mention-pair
model) and a small set of linguistic features. As re-
cent systems are becoming more sophisticated, we
suggest that researchers make their systems pub-
licly available in order to facilitate performance
comparisons. Publicly available coreference sys-
tems currently include JavaRAP (Qiu et al, 2004),
GuiTaR (Poesio and Kabadjov, 2004), BART (Ver-
sley et al, 2008b), CoRTex (Denis and Baldridge,
2008), the Illinois Coreference Package (Bengt-
son and Roth, 2008), CherryPicker (Rahman and
Ng, 2009), Reconcile (Stoyanov et al, 2010), and
Charniak and Elsner?s (2009) pronoun resolver.
We conclude with a discussion of two ques-
tions regarding supervised coreference research.
First, what is the state of the art? This is not an
easy question, as researchers have been evaluat-
ing their resolvers on different corpora using dif-
ferent evaluation metrics and preprocessing tools.
In particular, preprocessing tools can have a large
impact on the performance of a resolver (Barbu
and Mitkov, 2001). Worse still, assumptions about
whether gold or automatically extracted NPs are
used are sometimes not explicitly stated, poten-
tially causing results to be interpreted incorrectly.
To our knowledge, however, the best results on the
MUC-6 and MUC-7 data sets using automatically
extracted NPs are reported by Yang et al (2003)
(71.3 MUC F-score) and Ng and Cardie (2002c)
(63.4 MUC F-score), respectively;8 and the best
results on the ACE data sets using gold NPs can
be found in Luo (2007) (88.4 ACE-value).
Second, what lessons can we learn from fifteen
years of learning-based coreference research?
The mention-pair model is weak because it makes
coreference decisions based on local informa-
tion (i.e., information extracted from two NPs).
Expressive models (e.g., those that can exploit
cluster-level features) generally offer better perfor-
mance, and so are models that are ?global? in na-
ture. Global coreference models may refer to any
kind of models that can exploit non-local infor-
mation, including models that can consider mul-
tiple candidate antecedents simultaneously (e.g.,
ranking models), models that allow joint learning
for coreference resolution and related tasks (e.g.,
anaphoricity determination), models that can di-
rectly optimize clustering-level (rather than classi-
fication) accuracy, and models that can coordinate
with other components of a resolver, such as train-
ing instance creation and clustering.
8These results by no means suggest that no progress has
been made since 2003: most of the recently proposed coref-
erence models were evaluated on the ACE data sets.
1404
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on an earlier draft of the pa-
per. This work was supported in part by NSF
Grant IIS-0812261. Any opinions, findings, and
conclusions or recommendations expressed are
those of the author and do not necessarily reflect
the views or official policies, either expressed or
implied, of the NSF.
References
Chinatsu Aone and Scott William Bennett. 1995.
Evaluating automated and manual acquisition of
anaphora resolution strategies. In Proceedings of the
33rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 122?129.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the
LREC Workshop on Linguistic Coreference, pages
563?566.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2002.
Correlation clustering. In Proceedings of the 43rd
Annual IEEE Symposium on Foundations of Com-
puter Science, pages 238?247.
Catalina Barbu and Ruslan Mitkov. 2001. Evaluation
tool for rule-based anaphora resolution methods. In
Proceedings of the 39th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 34?41.
David Bean and Ellen Riloff. 1999. Corpus-based
identification of non-anaphoric noun phrases. In
Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 373?
380.
David Bean and Ellen Riloff. 2004. Unsupervised
learning of contextual role knowledge for corefer-
ence resolution. In Human Language Technologies
2004: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 297?
304.
Eric Bengtson and Dan Roth. 2008. Understanding the
values of features for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 294?
303.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39?71.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 33?40.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Distributional identification of non-referential
pronouns. In Proceedings of ACL-08: HLT, pages
10?18.
Donna Byron. 2001. The uncommon denominator: A
proposal for consistent reporting of pronoun resolu-
tion results. Computational Linguistics, 27(4):569?
578.
Sasha Calhoun, Jean Carletta, Jason Brenier, Neil
Mayo, Dan Jurafsky, Mark Steedman, and David
Beaver. (in press). The NXT-format Switchboard
corpus: A rich resource for investigating the syn-
tax, semantics, pragmatics and prosody of dialogue.
Language Resources and Evaluation.
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase
coreference as clustering. In Proceedings of the
1999 Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 82?89.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistic. Computational
Linguistics, 22(2):249?254.
Jose? Castan?o, Jason Zhang, and James Pustejovsky.
2002. Anaphora resolution in biomedical literature.
In Proceedings of the 2002 International Symposium
on Reference Resolution.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics, pages
148?156.
Eugene Charniak. 1972. Towards a Model of Chil-
dren?s Story Comphrension. AI-TR 266, Artificial
Intelligence Laboratory, Massachusetts Institute of
Technology, USA.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning, pages 88?95.
Noam Chomsky. 1988. Language and Problems of
Knowledge. The Managua Lectures. MIT Press,
Cambridge, Massachusetts.
William Cohen. 1995. Fast effective rule induction. In
Proceedings of the 12th International Conference on
Machine Learning, pages 115?123.
Dennis Connolly, John D. Burger, and David S. Day.
1994. A machine learning approach to anaphoric
reference. In Proceedings of International Con-
ference on New Methods in Language Processing,
pages 255?261.
Dennis Connolly, John D. Burger, and David S. Day.
1997. A machine learning approach to anaphoric
reference. In D. Jones and H. Somers, editors, New
Methods in Language Processing, pages 133?144.
UCL Press.
1405
Susan Converse. 2006. Pronominal Anaphora Resolu-
tion in Chinese. Ph.D. thesis, University of Pennsyl-
vania, USA.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for corefer-
ence resolution. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 81?88.
Walter Daelemans and Antal Van den Bosch. 2005.
Memory-Based Language Processing. Cambridge
University Press, Cambridge, UK.
Ido Dagan and Alon Itai. 1990. Automatic processing
of large corpora for the resolution of anaphora ref-
erences. In Proceedings of the 13th International
Conference on Computational Linguistics, pages
330?332.
Hal Daume? III and Daniel Marcu. 2005. A large-
scale exploration of effective global features for a
joint entity detection and tracking model. In Pro-
ceedings of the Human Language Technology Con-
ference and the Conference on Empirical Methods
in Natural Language Processing, pages 97?104.
Arthur Dempster. 1968. A generalization of Bayesian
inference. Journal of the Royal Statistical Society,
30:205?247.
Pascal Denis and Jason Baldridge. 2007a. Global,
joint determination of anaphoricity and coreference
resolution using integer programming. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 236?243.
Pascal Denis and Jason Baldridge. 2007b. A ranking
approach to pronoun resolution. In Proceedings of
the Twentieth International Conference on Artificial
Intelligence, pages 1588?1593.
Pascal Denis and Jason Baldridge. 2008. Special-
ized models and ranking for coreference resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
660?669.
Richard Evans. 2001. Applying machine learning to-
ward an automatic classification of it. Literary and
Linguistic Computing, 16(1):45?57.
Jenny Rose Finkel and Christopher Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL-08: HLT, Short Papers, pages
45?48.
Thomas Finley and Thorsten Joachims. 2005. Super-
vised clustering with support vector machines. In
Proceedings of the 22nd International Conference
on Machine Learning, pages 217?224.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277?296.
Nikesh Garera and David Yarowsky. 2006. Resolving
and generating definite anaphora by modeling hy-
pernymy using unlabeled corpora. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning, pages 37?44.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A
statistical approach to anaphora resolution. In Pro-
ceedings of the Sixth Workshop on Very Large Cor-
pora, pages 161?170.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1983. Providing a unified account of definite
noun phrases in discourse. In Proceedings of the
21st Annual Meeting of the Association for Compu-
tational Linguistics, pages 44?50.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203?226.
Barbara J. Grosz. 1977. The representation and use of
focus in a system for understanding dialogs. In Pro-
ceedings of the Fifth International Joint Conference
on Artificial Intelligence, pages 67?76.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 848?855.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference
resolution in a modular, entity-centered model. In
Proceedings of Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Jarmila
Panevova?, Petr Sgall, Petr Pajas, Jan Ste?pa?nek, Jir???
Havelka, and Marie Mikulova?. 2006. The Prague
Dependency Treebank 2.0. In Linguistic Data Con-
sortium.
Sanda Harabagiu, Ra?zvan Bunescu, and Steven Maio-
rano. 2001. Text and knowledge mining for corefer-
ence resolution. In Proceedings of the 2nd Meeting
of the North American Chapter of the Association
for Computational Linguistics, pages 55?62.
1406
Laura Hasler, Constantin Orasan, and Karin Naumann.
2006. NPs for events: Experiments in coreference
annotation. In Proceedings of the 5th International
Conference on Language Resources and Evaluation,
pages 1167?1172.
Peter Heeman and James Allen. 1995. The TRAINS
spoken dialog corpus. CD-ROM, Linguistic Data
Consortium.
Graeme Hirst. 1981. Discourse-oriented anaphora
resolution in natural language understanding: A re-
view. American Journal of Computational Linguis-
tics, 7(2):85?98.
Jerry Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311?338.
Ve?ronique Hoste and Walter Daelemans. 2005. Com-
paring learning approaches to coreference resolu-
tion. There is more to it than bias. In Proceedings
of the ICML Workshop on Meta-Learning.
Ve?ronique Hoste. 2005. Optimization Issues in Ma-
chine Learning of Coreference Resolution. Ph.D.
thesis, University of Antewerp, Belgium.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human
Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 57?60.
Zhiheng Huang, Guangping Zeng, Weiqun Xu, and
Asli Celikyilmaz. 2009. Accurate semantic class
classifier for coreference resolution. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 1232?1240.
Nancy Ide and Dan Cristea. 2000. A hierarchical ac-
count of referential accessibility. In Proceedings of
the 38th Annual Meeting of the Association for Com-
putational Linguistics, pages 416?424.
Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji
Matsumoto. 2003. Incorporating contextual cues
in trainable models for coreference resolution. In
Proceedings of the EACL Workshop on The Compu-
tational Treatment of Anaphora.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006.
Exploting syntactic patterns as clues in zero-
anaphora resolution. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association
for Computational Linguistics, pages 625?632.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007a.
Zero-anaphora resolution by learning rich syntactic
pattern features. ACM Transactions on Asian Lan-
guage Information Processing, 6(4).
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007b. Annotating a Japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proceedings of the ACL Workshop ?Lin-
guistic Annotation Workshop?, pages 132?139.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2009.
Capturing salience with a trainable cache model for
zero-anaphora resolution. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
647?655.
Ryu Iida. 2007. Combining Linguistic Knowledge and
Machine Learning for Anaphora Resolution. Ph.D.
thesis, Nara Institute of Science and Technology,
Japan.
Heng Ji, David Westbrook, and Ralph Grishman. 2005.
Using semantic relations to refine coreference deci-
sions. In Proceedings of the Human Language Tech-
nology Conference and the Conference on Empiri-
cal Methods in Natural Language Processing, pages
17?24.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and
Alexander Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 44?56. MIT
Press.
Andrew Kehler, Douglas Appelt, Lara Taylor, and
Aleksandr Simma. 2004a. Competitive self-trained
pronoun interpretation. In Proceedings of HLT-
NAACL 2004: Short Papers, pages 33?36.
Andrew Kehler, Douglas Appelt, Lara Taylor, and
Aleksandr Simma. 2004b. The (non)utility of
predicate-argument frequencies for pronoun inter-
pretation. In Human Language Technologies 2004:
The Conference of the North American Chapter of
the Association for Computational Linguistics; Pro-
ceedings of the Main Conference, pages 289?296.
Andrew Kehler. 1997. Probabilistic coreference in in-
formation extraction. In Proceedings of the Second
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 163?173.
Christopher Kennedy and Branimir Boguraev. 1996.
Anaphor for everyone: Pronominal anaphora resolu-
tion without a parser. In Proceedings of the 16th In-
ternational Conference on Computational Linguis-
tics, pages 113?118.
Manfred Klenner and ?Etienne Ailloud. 2008. Enhanc-
ing coreference clustering. In Proceedings of the
Second Workshop on Anaphora Resolution, pages
31?40.
Manfred Klenner and ?Etienne Ailloud. 2009. Op-
timization in coreference resolution is not needed:
A nearly-optimal algorithm with intensional con-
straints. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 442?450.
Manfred Klenner. 2007. Enforcing consistency on
coreference sets. In Proceedings of Recent Ad-
vances in Natural Language Processing.
1407
Fang Kong, GuoDong Zhou, and Qiaoming Zhu. 2009.
Employing the centering theory in pronoun resolu-
tion from the semantic perspective. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 987?996.
Shalom Lappin and Herbert Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535?562.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual
coreference resolution with syntactic features. In
Proceedings of the Human Language Technology
Conference and the Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?
667.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 135?142.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the Human
Language Technology Conference and the Confer-
ence on Empirical Methods in Natural Language
Processing, pages 25?32.
Xiaoqiang Luo. 2007. Coreference or not: A twin
model for coreference resolution. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 73?80.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Katja Markert and Malvina Nissim. 2005. Comparing
knowledge sources for nominal anaphora resolution.
Computational Linguistics, 31(3):367?402.
Andrew McCallum and Ben Wellner. 2003. Toward
conditional models of identity uncertainty with ap-
plication to proper noun coreference. In Proceed-
ings of the IJCAI Workshop on Information Integra-
tion on the Web.
Andrew McCallum and Ben Wellner. 2004. Condi-
tional models of identity uncertainty with applica-
tion to noun coreference. In Advances in Neural In-
formation Proceesing Systems.
Joseph McCarthy and Wendy Lehnert. 1995. Using
decision trees for coreference resolution. In Pro-
ceedings of the Fourteenth International Conference
on Artificial Intelligence, pages 1050?1055.
Ruslan Mitkov. 1999. Anaphora resolution: The
state of the art. Technical Report (Based on the
COLING/ACL-98 tutorial on anaphora resolution),
University of Wolverhampton, Wolverhampton.
Ruslan Mitkov. 2001. Outstanding issues in anaphora
resolution. In Al. Gelbukh, editor, Computational
Linguistics and Intelligent Text Processing, pages
110?125. Springer.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man.
Natalia N. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning
for other-anaphora resolution. In Proceedings of the
2003 Conference on Empirical Methods in Natural
Language Processing, pages 176?183.
MUC-6. 1995. Proceedings of the Sixth Message Un-
derstanding Conference.
MUC-7. 1998. Proceedings of the Seventh Message
Understanding Conference.
Christoph Mu?ller, Stefan Rapp, and Michael Strube.
2002. Applying co-training to reference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 352?
359.
Christoph Mu?ller. 2006. Automatic detection of non-
referential it in spoken multi-party dialog. In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 49?56.
Vincent Ng and Claire Cardie. 2002a. Combining
sample selection and error-driven pruning for ma-
chine learning of coreference rules. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 55?62.
Vincent Ng and Claire Cardie. 2002b. Identifying
anaphoric and non-anaphoric noun phrases to im-
prove coreference resolution. In Proceedings of
the 19th International Conference on Computational
Linguistics, pages 730?736.
Vincent Ng and Claire Cardie. 2002c. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 104?
111.
Vincent Ng. 2004. Learning noun phrase anaphoricity
to improve conference resolution: Issues in repre-
sentation and optimization. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics, pages 151?158.
Vincent Ng. 2007a. Semantic class induction and
coreference resolution. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 536?543.
Vincent Ng. 2007b. Shallow semantics for coreference
resolution. In Proceedings of the Twentieth Inter-
national Joint Conference on Artificial Intelligence,
pages 1689?1694.
1408
Vincent Ng. 2008. Unsupervised models for corefer-
ence resolution. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 640?649.
Vincent Ng. 2009. Graph-cut-based anaphoricity de-
termination for coreference resolution. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 575?583.
Giang Linh Ngu
.
y, Va?clav Nova?k, and Zdene?k
?Zabokrtsky?. 2009. Comparison of classification and
ranking approaches to pronominal anaphora resolu-
tion in Czech. In Proceedings of the SIGDIAL 2009
Conference, pages 276?285.
Cristina Nicolae and Gabriel Nicolae. 2006. Best-
Cut: A graph algorithm for coreference resolution.
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
275?283.
Kristina Nilsson. 2010. Hybrid Methods for Coref-
erence Resolution in Swedish. Ph.D. thesis, Stock-
holm University, Sweden.
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002.
The GENIA corpus: An annotated research abstract
corpus in molecular biology domain. In Proceed-
ings of the Second International Conference on Hu-
man Language Technology Research, pages 82?86.
Constantin Ora?san and Richard Evans. 2007. NP ani-
macy identification for anaphora resolution. Journal
of Artificial Intelligence Research, 29:79 ? 103.
Constantin Ora?san, Dan Cristea, Ruslan Mitkov, and
Anto?nio H. Branco. 2008. Anaphora Resolution
Exercise: An overview. In Proceedings of the 6th
Language Resources and Evaluation Conference,
pages 2801?2805.
Chris Paice and Gareth Husk. 1987. Towards the au-
tomatic recognition of anaphoric features in English
text: the impersonal pronoun ?it?. Computer Speech
and Language, 2:109?132.
Massimo Poesio and Mijail A. Kabadjov. 2004. A
general-purpose, off-the-shelf anaphora resolution
module: Implementation and preliminary evalua-
tion. In Proceedings of the 4th International Confer-
ence on Language Resources and Evaluation, pages
663?668.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004a. Learning to resolve bridg-
ing references. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 143?150.
Massimo Poesio, Olga Uryupina, Renata Vieira, Mijail
Alexandrov-Kabadjov, and Rodrigo Goulart. 2004b.
Discourse-new detectors for definite description res-
olution: A survey and a preliminary proposal. In
Proeedings of the ACL Workshop on Reference Res-
olution.
Massimo Poesio, David Day, Ron Artstein, Jason Dun-
can, Vladimir Eidelman, Claudio Giuliano, Rob
Hall, Janet Hitzeman, Alan Jern, Mijail Kabadjov,
Stanley Yong Wai Keong, Gideon Mann, Alessan-
dro Moschitti, Simone Ponzetto, Jason Smith, Josef
Steinberger, Michael Strube, Jian Su, Yannick Vers-
ley, Xiaofeng Yang, and Michael Wick. 2007. EL-
ERFED: Final report of the research group on Ex-
ploiting Lexical and Encyclopedic Resources For
Entity Disambiguation. Technical report, Summer
Workshop on Language Engineering, Center for
Language and Speech Processing, Johns Hopkins
University, Baltimore, MD.
Simone Paolo Ponzetto and Massimo Poesio. 2009.
State-of-the-art NLP approaches to coreference res-
olution: Theory and practical recipes. In Tutorial
Abstracts of ACL-IJCNLP 2009, page 6.
Simone Paolo Ponzetto and Michael Strube. 2006a.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Human
Language Technologies 2006: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 192?199.
Simone Paolo Ponzetto and Michael Strube. 2006b.
Semantic role labeling for coreference resolution. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 143?146.
Simone Paolo Ponzetto and Michael Strube. 2007.
Knowledge derived from Wikipedia for computing
semantic relatedness. Journal of Artificial Intelli-
gence Research, 30:181?212.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
650?659.
Andrei Popescu-Belis, Lo??s Rigouste, Susanne
Salmon-Alt, and Laurent Romary. 2004. Online
evaluation of coreference resolution. In Proceedings
of the 4th International Conference on Language
Resources and Evaluation, pages 1507?1510.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004.
A public reference implementation of the RAP
anaphora resolution algorithm. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation, pages 291?294.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo,
CA.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977.
1409
Marta Recasens and M. Anto?nia Mart??. 2009. AnCora-
CO: Coreferentially annotated corpora for Spanish
and Catalan. Language Resources and Evaluation,
43(4).
Marta Recasens, Toni Mart??, Mariona Taule?, Llu??s
Ma`rquez, and Emili Sapena. 2009. SemEval-
2010 Task 1: Coreference resolution in multiple lan-
guages. In Proceedings of the Workshop on Seman-
tic Evaluations: Recent Achievements and Future
Directions (SEW-2009), pages 70?75.
Candace Sidner. 1979. Towards a Computational The-
ory of Definite Anaphora Comprehension in English
Discourse. Ph.D. thesis, Massachusetts Institute of
Technology, USA.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
1999. Corpus-based learning for noun phrase coref-
erence resolution. In Proceedings of the 1999 Joint
SIGDAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora,
pages 285?291.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 656?664.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010.
Coreference resolution with Reconcile. In Proceed-
ings of the ACL 2010 Conference Short Papers.
Michael Strube, Stefan Rapp, and Christoph Mu?ller.
2002. The influence of minimum edit distance on
reference resolution. In Proceedings of the 2002
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 312?319.
Michael Strube. 2002. NLP approaches to reference
resolution. In Tutorial Abstracts of ACL 2002, page
124.
Michael Strube. 2009. Anaphernresolution. In Com-
puterlinguistik und Sprachtechnologie. Eine Ein-
fuhrung. Springer, Heidelberg, Germany, 3rd edi-
tion.
Heike Telljohann, Erhard Hinrichs, and Sandra Ku?bler.
2004. The tu?ba-d/z treebank: Annotating German
with a context-free backbone. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation, pages 2229?2235.
Joel Tetreault. 2005. Empirical Evaluations of
Pronoun Resolution. Ph.D. thesis, University of
Rochester, USA.
Olga Uryupina. 2003. High-precision identification of
discourse new and unique noun phrases. In Proceed-
ings of the ACL Student Research Workshop, pages
80?86.
Olga Uryupina. 2004. Linguistically motivated sample
selection for coreference resolution. In Proceedings
of the 5th Discourse Anaphora and Anaphor Reso-
lution Colloquium.
Kees van Deemter and Rodger Kibble. 2000. On core-
ferring: Coreference in MUC and related annotation
schemes. Computational Linguistics, 26(4):629?
637.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008a. Coreference sys-
tems based on kernels methods. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 961?968.
Yannick Versley, Simone Paolo Ponzetto, Massimo
Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008b.
BART: A modular toolkit for coreference resolution.
In Proceedings of the ACL-08: HLT Demo Session,
pages 9?12.
Yannick Versley. 2006. A constraint-based approach
to noun phrase coreference resolution in German
newspaper text. In Konferenz zur Verarbeitung
Natu?rlicher Sprache.
Yannick Versley. 2007. Antecedent selection tech-
niques for high-recall coreference resolution. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
496?505.
Renata Vieira and Massimo Poesio. 2000. Process-
ing definite descriptions in corpora. In S. Botley
and A. McEnery, editors, Corpus-based and Compu-
tational Approaches to Discourse Anaphora, pages
189?212. UCL Press.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message Understanding Confer-
ence, pages 45?52.
Marilyn Walker, Aravind Joshi, and Ellen Prince, edi-
tors. 1998. Centering Theory in Discourse. Oxford
University Press.
Holger Wunsch. 2010. Rule-based and Memory-based
Pronoun Resolution for German: A Comparison and
Assessment of Data Sources. Ph.D. thesis, Univer-
sity of Tu?bingen, Germany.
Xiaofeng Yang and Jian Su. 2007. Coreference reso-
lution using semantic relatedness information from
automatically discovered patterns. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics, pages 528?535.
1410
Xiaofeng Yang, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2003. Coreference resolution us-
ing competitive learning approach. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 176?183.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2004a.
Improving noun phrase coreference resolution by
matching strings. In Proceedings of the First In-
ternational Joint Conference on Natural Language
Processing, pages 22?31.
Xiaofeng Yang, Jian Su, GuoDong Zhou, and
Chew Lim Tan. 2004b. An NP-cluster based ap-
proach to coreference resolution. In Proceedings of
the 20th International Conference on Computational
Linguistics, pages 226?232.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005.
Improving pronoun resolution using statistics-based
semantic compatibility information. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 165?172.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel based pronoun resolution with structured
syntactic knowledge. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association
for Computational Linguistics, pages 41?48.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan,
and Sheng Li. 2008a. An entity-mention model
for coreference resolution with inductive logic pro-
gramming. In Proceedings of ACL-08: HLT, pages
843?851.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2008b. A
twin-candidate model for learning-based anaphora
resolution. Computational Linguistics, 34(3):327?
356.
Dmitry Zelenko, Chinatsu Aone, and Jason Tibbetts.
2004. Coreference resolution for information ex-
traction. In Proceedings of the ACL Workshop on
Reference Resolution and its Applications, pages 9?
16.
Shanheng Zhao and Hwee Tou Ng. 2007. Identifica-
tion and resolution of Chinese zero pronouns: A ma-
chine learning approach. In Proceedings of the 2007
Joint Conference on Empirical Methods on Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 541?550.
GuoDong Zhou and Fang Kong. 2009. Global learn-
ing of noun phrase anaphoricity in coreference res-
olution via label propagation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 978?986.
1411
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 814?824,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Coreference Resolution with World Knowledge
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{altaf,vince}@hlt.utdallas.edu
Abstract
While world knowledge has been shown to
improve learning-based coreference resolvers,
the improvements were typically obtained by
incorporating world knowledge into a fairly
weak baseline resolver. Hence, it is not clear
whether these benefits can carry over to a
stronger baseline. Moreover, since there has
been no attempt to apply different sources of
world knowledge in combination to corefer-
ence resolution, it is not clear whether they of-
fer complementary benefits to a resolver. We
systematically compare commonly-used and
under-investigated sources of world knowl-
edge for coreference resolution by applying
them to two learning-based coreference mod-
els and evaluating them on documents anno-
tated with two different annotation schemes.
1 Introduction
Noun phrase (NP) coreference resolution is the task
of determining which NPs in a text or dialogue refer
to the same real-world entity. The difficulty of the
task stems in part from its reliance on world knowl-
edge (Charniak, 1972). To exemplify, consider the
following text fragment.
Martha Stewart is hoping people don?t run out on her.
The celebrity indicted on charges stemming from . . .
Having the (world) knowledge that Martha Stewart
is a celebrity would be helpful for establishing the
coreference relation between the two NPs. One may
argue that employing heuristics such as subject pref-
erence or syntactic parallelism (which prefers re-
solving an NP to a candidate antecedent that has the
same grammatical role) in this example would also
allow us to correctly resolve the celebrity (Mitkov,
2002), thereby obviating the need for world knowl-
edge. However, since these heuristics are not per-
fect, complementing them with world knowledge
would be an important step towards bringing coref-
erence systems to the next level of performance.
Despite the usefulness of world knowledge for
coreference resolution, early learning-based coref-
erence resolvers have relied mostly on morpho-
syntactic features (e.g., Soon et al (2001), Ng and
Cardie (2002), Yang et al (2003)). With recent ad-
vances in lexical semantics research and the devel-
opment of large-scale knowledge bases, researchers
have begun to employ world knowledge for corefer-
ence resolution. World knowledge is extracted pri-
marily from three data sources, web-based encyclo-
pedia (e.g., Ponzetto and Strube (2006), Uryupina
et al (2011)), unannotated data (e.g., Daume? III
and Marcu (2005), Ng (2007)), and coreference-
annotated data (e.g., Bengtson and Roth (2008)).
While each of these three sources of world knowl-
edge has been shown to improve coreference resolu-
tion, the improvements were typically obtained by
incorporating world knowledge (as features) into a
baseline resolver composed of a rather weak coref-
erence model (i.e., the mention-pair model) and a
small set of features (i.e., the 12 features adopted
by Soon et al?s (2001) knowledge-lean approach).
As a result, some questions naturally arise. First,
can world knowledge still offer benefits when used
in combination with a richer set of features? Sec-
ond, since automatically extracted world knowledge
is typically noisy (Ponzetto and Poesio, 2009), are
recently-developed coreference models more noise-
tolerant than the mention-pair model, and if so, can
they profit more from the noisily extracted world
knowledge? Finally, while different world knowl-
814
edge sources have been shown to be useful when ap-
plied in isolation to a coreference system, do they of-
fer complementary benefits and therefore can further
improve a resolver when applied in combination?
We seek answers to these questions by conduct-
ing a systematic evaluation of different world knowl-
edge sources for learning-based coreference reso-
lution. Specifically, we (1) derive world knowl-
edge from encyclopedic sources that are under-
investigated for coreference resolution, including
FrameNet (Baker et al, 1998) and YAGO (Suchanek
et al, 2007), in addition to coreference-annotated
data and unannotated data; (2) incorporate such
knowledge as features into a richer baseline feature
set that we previously employed (Rahman and Ng,
2009); and (3) evaluate their utility using two coref-
erence models, the traditional mention-pair model
(Soon et al, 2001) and the recently developed
cluster-ranking model (Rahman and Ng, 2009).
Our evaluation corpus contains 410 documents,
which are coreference-annotated using the ACE an-
notation scheme as well as the OntoNotes annota-
tion scheme (Hovy et al, 2006). By evaluating on
two sets of coreference annotations for the same set
of documents, we can determine whether the use-
fulness of world knowledge sources for coreference
resolution is dependent on the underlying annotation
scheme used to annotate the documents.
2 Preliminaries
In this section, we describe the corpus, the NP ex-
traction methods, the coreference models, and the
evaluation measures we will use in our evaluation.
2.1 Data Set
We evaluate on documents that are coreference-
annotated using both the ACE annotation scheme
and the OntoNotes annotation scheme, so that we
can examine whether the usefulness of our world
knowledge sources is dependent on the underlying
coreference annotation scheme. Specifically, our
data set is composed of the 410 English newswire
articles that appear in both OntoNotes-2 and ACE
2004/2005. We partition the documents into a train-
ing set and a test set following a 80/20 ratio.
ACE and OntoNotes employ different guide-
lines to annotate coreference chains. A major
difference between the two annotation schemes is
that ACE only concerns establishing coreference
chains among NPs that belong to the ACE entity
types, whereas OntoNotes does not have this re-
striction. Hence, the OntoNotes annotation scheme
should produce more coreference chains (i.e., non-
singleton coreference clusters) than the ACE anno-
tation scheme for a given set of documents. For our
data set, the OntoNotes scheme yielded 4500 chains,
whereas the ACE scheme yielded only 3637 chains.
Another difference between the two annotation
schemes is that singleton clusters are annotated in
ACE but not OntoNotes. As discussed below, the
presence of singleton clusters may have an impact
on NP extraction and coreference evaluation.
2.2 NP Extraction
Following common practice, we employ different
methods to extract NPs from the documents anno-
tated with the two annotation schemes.
To extract NPs from the ACE-annotated docu-
ments, we train a mention extractor on the train-
ing texts (see Section 5.1 of Rahman and Ng (2009)
for details), which recalls 83.6% of the NPs in the
test set. On the other hand, to extract NPs from the
OntoNotes-annotated documents, the same method
should not be applied. To see the reason, recall that
only the NPs in non-singleton clusters are annotated
in these documents. Training a mention extractor
on these NPs implies that we are learning to ex-
tract non-singleton NPs, which are typically much
smaller in number than the entire set of NPs. In
other words, doing so could substantially simplify
the coreference task. Consequently, we follow the
approach adopted by traditional learning-based re-
solvers and employ an NP chunker to extract NPs.
Specifically, we use the markable identification sys-
tem in the Reconcile resolver (Stoyanov et al, 2010)
to extract NPs from the training and test texts. This
identifier recalls 77.4% of the NPs in the test set.
2.3 Coreference Models
We evaluate the utility of world knowledge using the
mention-pair model and the cluster-ranking model.
2.3.1 Mention-Pair Model
The mention-pair (MP) model is a classifier that
determines whether two NPs are coreferent or not.
815
Each instance i(NPj , NPk) corresponds to NPj and
NPk, and is represented by a Baseline feature set con-
sisting of 39 features. Linguistically, these features
can be divided into four categories: string-matching,
grammatical, semantic, and positional. These fea-
tures can also be categorized based on whether they
are relational or not. Relational features capture
the relationship between NPj and NPk, whereas non-
relational features capture the linguistic property of
one of these two NPs. Since space limitations pre-
clude a description of these features, we refer the
reader to Rahman and Ng (2009) for details.
We follow Soon et al?s (2001) method for cre-
ating training instances: we create (1) a positive
instance for each anaphoric NP, NPk, and its clos-
est antecedent, NPj ; and (2) a negative instance for
NPk paired with each of the intervening NPs, NPj+1,
NPj+2, . . ., NPk?1. The classification of a training
instance is either positive or negative, depending on
whether the two NPs are coreferent in the associated
text. To train the MP model, we use the SVM learn-
ing algorithm from SVMlight (Joachims, 2002).1
After training, the classifier is used to identify an
antecedent for an NP in a test text. Specifically, each
NP, NPk , is compared in turn to each preceding NP,
NPj , from right to left, and NPj is selected as its an-
tecedent if the pair is classified as coreferent. The
process terminates as soon as an antecedent is found
for NPk or the beginning of the text is reached.
Despite its popularity, the MP model has two
major weaknesses. First, since each candidate an-
tecedent for an NP to be resolved (henceforth an ac-
tive NP) is considered independently of the others,
this model only determines how good a candidate
antecedent is relative to the active NP, but not how
good a candidate antecedent is relative to other can-
didates. So, it fails to answer the critical question of
which candidate antecedent is most probable. Sec-
ond, it has limitations in its expressiveness: the in-
formation extracted from the two NPs alone may not
be sufficient for making a coreference decision.
2.3.2 Cluster-Ranking Model
The cluster-ranking (CR) model addresses the two
weaknesses of the MP model by combining the
strengths of the entity-mention model (e.g., Luo et
1For this and subsequent uses of the SVM learner in our
experiments, we set al parameters to their default values.
al. (2004), Yang et al (2008)) and the mention-
ranking model (e.g., Denis and Baldridge (2008)).
Specifically, the CR model ranks the preceding clus-
ters for an active NP so that the highest-ranked clus-
ter is the one to which the active NP should be
linked. Employing a ranker addresses the first weak-
ness, as a ranker allows all candidates to be com-
pared simultaneously. Considering preceding clus-
ters rather than antecedents as candidates addresses
the second weakness, as cluster-level features (i.e.,
features that are defined over any subset of NPs in a
preceding cluster) can be employed. Details of the
CR model can be found in Rahman and Ng (2009).
Since the CR model ranks preceding clusters, a
training instance i(cj , NPk) represents a preceding
cluster, cj , and an anaphoric NP, NPk. Each instance
consists of features that are computed based solely
on NPk as well as cluster-level features, which de-
scribe the relationship between cj and NPk . Mo-
tivated in part by Culotta et al (2007), we create
cluster-level features from the relational features in
our feature set using four predicates: NONE, MOST-
FALSE, MOST-TRUE, and ALL. Specifically, for each
relational feature X, we first convert X into an equiv-
alent set of binary-valued features if it is multi-
valued. Then, for each resulting binary-valued fea-
ture Xb, we create four binary-valued cluster-level
features: (1) NONE-Xb is true when Xb is false be-
tween NPk and each NP in cj ; (2) MOST-FALSE-Xb
is true when Xb is true between NPk and less than half
(but at least one) of the NPs in cj ; (3) MOST-TRUE-
Xb is true when Xb is true between NPk and at least
half (but not all) of the NPs in cj ; and (4) ALL-Xb is
true when Xb is true between NPk and each NP in cj .
We train a cluster ranker to jointly learn
anaphoricity determination and coreference reso-
lution using SVMlight?s ranker-learning algorithm.
Specifically, for each NP, NPk, we create a training
instance between NPk and each preceding cluster cj
using the features described above. Since we are
learning a joint model, we need to provide the ranker
with the option to start a new cluster by creating an
additional training instance that contains the non-
relational features describing NPk . The rank value
of a training instance i(cj , NPk) created for NPk is the
rank of cj among the competing clusters. If NPk is
anaphoric, its rank is HIGH if NPk belongs to cj , and
LOW otherwise. If NPk is non-anaphoric, its rank is
816
LOW unless it is the additional training instance de-
scribed above, which has rank HIGH.
After training, the cluster ranker processes the
NPs in a test text in a left-to-right manner. For each
active NP, NPk , we create test instances for it by pair-
ing it with each of its preceding clusters. To allow
for the possibility that NPk is non-anaphoric, we cre-
ate an additional test instance as during training. All
these test instances are then presented to the ranker.
If the additional test instance is assigned the highest
rank value, then we create a new cluster containing
NPk. Otherwise, NPk is linked to the cluster that has
the highest rank. Note that the partial clusters pre-
ceding NPk are formed incrementally based on the
predictions of the ranker for the first k ? 1 NPs.
2.4 Evaluation Measures
We employ two commonly-used scoring programs,
B3 (Bagga and Baldwin, 1998) and CEAF (Luo,
2005), both of which report results in terms of recall
(R), precision (P), and F-measure (F) by comparing
the gold-standard (i.e., key) partition, KP , against
the system-generated (i.e., response) partition, RP .
Briefly, B3 computes the R and P values of each
NP and averages these values at the end. Specifi-
cally, for each NP, NPj , B3 first computes the number
of common NPs in KPj and RPj , the clusters con-
taining NPj in KP and RP , respectively, and then
divides this number by |KPj | and |RPj | to obtain
the R and P values of NPj , respectively. On the other
hand, CEAF finds the best one-to-one alignment be-
tween the key clusters and the response clusters.
A complication arises when B3 is used to score
a response partition containing automatically ex-
tracted NPs. Recall that B3 constructs a mapping
between the NPs in the response and those in the
key. Hence, if the response is generated using gold-
standard NPs, then every NP in the response is
mapped to some NP in the key and vice versa. In
other words, there are no twinless (i.e., unmapped)
NPs (Stoyanov et al, 2009). This is not the case
when automatically extracted NPs are used, but the
original description of B3 does not specify how
twinless NPs should be scored (Bagga and Baldwin,
1998). To address this problem, we set the recall
and precision of a twinless NP to zero, regardless of
whether the NP appears in the key or the response.
Note that CEAF can compare partitions with twin-
less NPs without any modification, since it operates
by finding the best alignment between the clusters in
the two partitions.
Additionally, in order not to over-penalize a re-
sponse partition, we remove all the twinless NPs in
the response that are singletons. The rationale is
simple: since the resolver has successfully identified
these NPs as singletons, it should not be penalized,
and removing them avoids such penalty.
Since B3 and CEAF align NPs/clusters, the lack
of singleton clusters in the OntoNotes annotations
implies that the resulting scores reflect solely how
well a resolver identifies coreference links and do
not take into account how well it identifies singleton
clusters.
3 Extracting World Knowledge
In this section, we describe how we extract world
knowledge for coreference resolution from three
different sources: large-scale knowledge bases,
coreference-annotated data and unannotated data.
3.1 World Knowledge from Knowledge Bases
We extract world knowledge from two large-scale
knowledge bases, YAGO and FrameNet.
3.1.1 Extracting Knowledge from YAGO
We choose to employ YAGO rather than the more
popularly-used Wikipedia due to its potentially
richer knowledge, which comprises 5 million facts
extracted from Wikipedia and WordNet. Each fact
is represented as a triple (NPj , rel, NPk), where rel
is one of the 90 YAGO relation types defined on
two NPs, NPj and NPk . Motivated in part by previ-
ous work (Bryl et al, 2010; Uryupina et al, 2011),
we employ the two relation types that we believe
are most useful for coreference resolution, TYPE
and MEANS. TYPE is essentially an IS-A relation.
For instance, the triple (AlbertEinstein, TYPE,
physicist) denotes the fact that Albert Einstein
is a physicist. MEANS provides different ways of
expressing an entity, and therefore allows us to deal
with synonymy and ambiguity. For instance, the two
triples (Einstein, MEANS, AlbertEinstein)
and (Einstein, MEANS, AlfredEinstein)
denote the facts that Einstein may refer to the physi-
cist Albert Einstein and the musicologist Alfred Ein-
stein, respectively. Hence, the presence of one or
817
both of these relations between two NPs provides
strong evidence that the two NPs are coreferent.
YAGO?s unification of the information in
Wikipedia and WordNet enables it to extract
facts that cannot be extracted with Wikipedia
or WordNet alne, such as (MarthaStewart,
TYPE, celebrity). To better appreciate YAGO?s
strengths, let us see how this fact was extracted.
YAGO first heuristically maps each of the Wiki
categories in the Wiki page for Martha Stewart
to its semantically closest WordNet synset. For
instance, the Wiki category AMERICAN TELE-
VISION PERSONALITIES is mapped to the synset
corresponding to sense #2 of the word personality.
Then, given that personality is a direct hyponym of
celebrity in WordNet, YAGO extracts the desired
fact. This enables YAGO to extract facts that cannot
be extracted with Wikipedia or WordNet alne.
We incorporate the world knowledge from YAGO
into our coreference models as a binary-valued fea-
ture. If the MP model is used, the YAGO feature
for an instance will have the value 1 if and only if
the two NPs involved are in a TYPE or MEANS re-
lation. On the other hand, if the CR model is used,
the YAGO feature for an instance involving NPk and
preceding cluster c will have the value 1 if and only
if NPk has a TYPE or MEANS relation with any of
the NPs in c. Since knowledge extraction from web-
based encyclopedia is typically noisy (Ponzetto and
Poesio, 2009), we use YAGO to determine whether
two NPs have a relation only if one NP is a named
entity (NE) of type person, organization, or location
according to the Stanford NE recognizer (Finkel et
al., 2005) and the other NP is a common noun.
3.1.2 Extracting Knowledge from FrameNet
FrameNet is a lexico-semantic resource focused on
semantic frames (Baker et al, 1998). As a schematic
representation of a situation, a frame contains the
lexical predicates that can invoke it as well as the
frame elements (i.e., semantic roles). For example,
the JUDGMENT COMMUNICATION frame describes
situations in which a COMMUNICATOR communi-
cates a judgment of an EVALUEE to an ADDRESSEE.
This frame has COMMUNICATOR and EVALUEE as
its core frame elements and ADDRESSEE as its non-
core frame elements, and can be invoked by more
than 40 predicates, such as acclaim, accuse, com-
mend, decry, denounce, praise, and slam.
To better understand why FrameNet contains po-
tentially useful knowledge for coreference resolu-
tion, consider the following text segment:
Peter Anthony decries program trading as ?limiting the
game to a few,? but he is not sure whether he wants to
denounce it because ...
To establish the coreference relation between it and
program trading, it may be helpful to know that de-
cry and denounce appear in the same frame and the
two NPs have the same semantic role.
This example suggests that features encoding both
the semantic roles of the two NPs under considera-
tion and whether the associated predicates are ?re-
lated? to each other in FrameNet (i.e., whether they
appear in the same frame) could be useful for iden-
tifying coreference relations. Two points regarding
our implementation of these features deserve men-
tion. First, since we do not employ verb sense dis-
ambiguation, we consider two predicates related as
long as there is at least one semantic frame in which
they both appear. Second, since FrameNet-style se-
mantic role labelers are not publicly available, we
use ASSERT (Pradhan et al, 2004), a semantic role
labeler that provides PropBank-style semantic roles
such as ARG0 (the PROTOAGENT, which is typi-
cally the subject of a transitive verb) and ARG1 (the
PROTOPATIENT, which is typically its direct object).
Now, assuming that NPj and NPk are the argu-
ments of two stemmed predicates, predj and predk ,
we create 15 features using the knowledge extracted
from FrameNet and ASSERT as follows. First, we
encode the knowledge extracted from FrameNet as
one of three possible values: (1) predj and predk
are in the same frame; (2) they are both predicates
in FrameNet but never appear in the same frame;
and (3) one or both predicates do not appear in
FrameNet. Second, we encode the semantic roles of
NPj and NPk as one of five possible values: ARG0-
ARG0, ARG1-ARG1, ARG0-ARG1, ARG1-ARG0,
and OTHERS (the default case).2 Finally, we create
15 binary-valued features by pairing the 3 possible
values extracted from FrameNet and the 5 possible
values provided by ASSERT. Since these features
2We focus primarily on ARG0 and ARG1 because they are
the most important core arguments of a predicate and may pro-
vide more useful information than other semantic roles.
818
are computed over two NPs, we can employ them di-
rectly for the MP model. Note that by construction,
exactly one of these features will have a non-zero
value. For the CR model, we extend their definitions
so that they can be computed between an NP, NPk,
and a preceding cluster, c. Specifically, the value of
a feature is 1 if and only if its value between NPk and
one of the NPs in c is 1 under its original definition.
The above discussion assumes that the two NPs
under consideration serve as predicate arguments. If
this assumption fails, we will not create any features
based on FrameNet for these two NPs.
To our knowledge, FrameNet has not been ex-
ploited for coreference resolution. However, the
use of related verbs is similar in spirit to Bean and
Riloff?s (2004) use of patterns for inducing contex-
tual role knowledge, and the use of semantic roles is
also discussed in Ponzetto and Strube (2006).
3.2 World Knowledge from Annotated Data
Since world knowledge is needed for coreference
resolution, a human annotator must have employed
world knowledge when coreference-annotating a
document. We aim to design features that can ?re-
cover? such world knowledge from annotated data.
3.2.1 Features Based on Noun Pairs
A natural question is: what kind of world knowl-
edge can we extract from annotated data? We may
gather the knowledge that Barack Obama is a U.S.
president if we see these two NPs appearing in the
same coreference chain. Equally importantly, we
may gather the commonsense knowledge needed for
determining non-coreference. For instance, we may
discover that a lion and a tiger are unlikely to refer
to the same real-world entity after realizing that they
never appear in the same chain in a large number of
annotated documents. Note that any features com-
puted based on WordNet distance or distributional
similarity are likely to incorrectly suggest that lion
and tiger are coreferent, since the two nouns are sim-
ilar distributionally and according to WordNet.
Given these observations, one may collect the
noun pairs from the (coreference-annotated) train-
ing data and use them as features to train a resolver.
However, for these features to be effective, we need
to address data sparseness, as many noun pairs in
the training data may not appear in the test data.
To improve generalization, we instead create dif-
ferent kinds of noun-pair-based features given an
annotated text. To begin with, we preprocess each
document. A training text is preprocessed by ran-
domly replacing 10% of its common nouns with the
label UNSEEN. If an NP, NPk , is replaced with UN-
SEEN, all NPs that have the same string as NPk will
also be replaced with UNSEEN. A test text is prepro-
cessed differently: we simply replace all NPs whose
strings are not seen in the training data with UN-
SEEN. Hence, artificially creating UNSEEN labels
from a training text will allow a learner to learn how
to handle unseen words in a test text.
Next, we create noun-pair-based features for the
MP model, which will be used to augment the Base-
line feature set. Here, each instance corresponds to
two NPs, NPj and NPk , and is represented by three
groups of binary-valued features.
Unseen features are applicable when both NPj
and NPk are UNSEEN. Either an UNSEEN-SAME fea-
ture or an UNSEEN-DIFF feature is created, depend-
ing on whether the two NPs are the same string be-
fore being replaced with the UNSEEN token.
Lexical features are applicable when neither NPj
nor NPk is UNSEEN. A lexical feature is an ordered
pair consisting of the heads of the NPs. For a pro-
noun or a common noun, the head is the last word of
the NP; for a proper name, the head is the entire NP.
Semi-lexical features aim to improve generaliza-
tion, and are applicable when neither NPj nor NPk is
UNSEEN. If exactly one of NPj and NPk is tagged
as a NE by the Stanford NE recognizer, we create
a semi-lexical feature that is identical to the lexical
feature described above, except that the NE is re-
placed with its NE label. On the other hand, if both
NPs are NEs, we check whether they are the same
string. If so, we create a *NE*-SAME feature, where
*NE* is replaced with the corresponding NE label.
Otherwise, we check whether they have the same NE
tag and a word-subset match (i.e., whether the word
tokens in one NP appears in the other?s list of word
tokens). If so, we create a *NE*-SUBSAME feature,
where *NE* is replaced with their NE label. Other-
wise, we create a feature that is the concatenation of
the NE labels of the two NPs.
The noun-pair-based features for the CR model
can be generated using essentially the same method.
Specifically, since each instance now corresponds to
819
an NP, NPk, and a preceding cluster, c, we can gener-
ate a noun-pair-based feature by applying the above
method to NPk and each of the NPs in c, and its value
is the number of times it is applicable to NPk and c.
3.2.2 Features Based on Verb Pairs
As discussed above, features encoding the seman-
tic roles of two NPs and the relatedness of the asso-
ciated verbs could be useful for coreference resolu-
tion. Rather than encoding verb relatedness, we may
replace verb relatedness with the verbs themselves
in these features, and have the learner learn directly
from coreference-annotated data whether two NPs
serving as the objects of decry and denounce are
likely to be coreferent or not, for instance.
Specifically, assuming that NPj and NPk are the
arguments of two stemmed predicates, predj and
predk , in the training data, we create five features
as follows. First, we encode the semantic roles of
NPj and NPk as one of five possible values: ARG0-
ARG0, ARG1-ARG1, ARG0-ARG1, ARG1-ARG0,
and OTHERS (the default case). Second, we create
five binary-valued features by pairing each of these
five values with the two stemmed predicates. Since
these features are computed over two NPs, we can
employ them directly for the MP model. Note that
by construction, exactly one of these features will
have a non-zero value. For the CR model, we extend
their definitions so that they can be computed be-
tween an NP, NPk , and a preceding cluster, c. Specif-
ically, the value of a feature is 1 if and only if its
value between NPk and one of the NPs in c is 1 un-
der its original definition.
The above discussion assumes that the two NPs
under consideration serve as predicate arguments. If
this assumption fails, we will not create any features
based on verb pairs for these two NPs.
3.3 World Knowledge from Unannotated Data
Previous work has shown that syntactic apposi-
tions, which can be extracted using heuristics from
unannotated documents or parse trees, are a useful
source of world knowledge for coreference resolu-
tion (e.g., Daume? III and Marcu (2005), Ng (2007),
Haghighi and Klein (2009)). Each extraction is an
NP pair such as <Barack Obama, the president>
and <Eastern Airlines, the carrier>, where the first
NP in the pair is a proper name and the second NP is
a common NP. Low-frequency extractions are typi-
cally assumed to be noisy and discarded.
We combine the extractions produced by Fleis-
chman et al (2003) and Ng (2007) to form a
database consisting of 1.057 million NP pairs, and
create a binary-valued feature for our coreference
models using this database. If the MP model is used,
this feature will have the value 1 if and only if the
two NPs appear as a pair in the database. On the
other hand, if the CR model is used, the feature for
an instance involving NPk and preceding cluster c
will have the value 1 if and only if NPk and at least
one of the NPs in c appears as a pair in the database.
4 Evaluation
4.1 Experimental Setup
As described in Section 2, we use as our evalua-
tion corpus the 411 documents that are coreference-
annotated using the ACE and OntoNotes annota-
tion schemes. Specifically, we divide these docu-
ments into five (disjoint) folds of roughly the same
size, training the MP model and the CR model us-
ing SVMlight on four folds and evaluate their per-
formance on the remaining fold. The linguistic fea-
tures, as well as the NPs used to create the training
and test instances, are computed automatically. We
employ B3 and CEAF as described in Section 2.3 to
score the output of a coreference system.
4.2 Results and Discussion
4.2.1 Baseline Models
Since our goal is to evaluate the effectiveness of
the features encoding world knowledge for learning-
based coreference resolution, we employ as our
baselines the MR model and the CR model trained
on the Baseline feature set, which does not con-
tain any features encoding world knowledge. For
the MP model, the Baseline feature set consists of
the 39 features described in Section 2.3.1; for the
CR model, the Baseline feature set consists of the
cluster-level features derived from the 39 features
used in the Baseline MP model (see Section 2.3.2).
Results of the MP model and the CR model em-
ploying the Baseline feature set are shown in rows 1
and 8 of Table 1, respectively. Each row contains the
B3 and CEAF results of the corresponding corefer-
ence model when it is evaluated using the ACE and
820
ACE OntoNotes
B3 CEAF B3 CEAF
Feature Set R P F R P F R P F R P F
Results for the Mention-Pair Model
1 Base 56.5 69.7 62.4 54.9 66.3 60.0 50.4 56.7 53.3 48.9 54.5 51.5
2 Base+YAGO Types (YT) 57.3 70.3 63.1 58.7 67.5 62.8 51.7 57.9 54.6 50.3 55.6 52.8
3 Base+YAGO Means (YM) 56.7 70.0 62.7 55.3 66.5 60.4 50.6 57.0 53.6 49.3 54.9 51.9
4 Base+Noun Pairs (WP) 57.5 70.6 63.4 55.8 67.4 61.1 51.6 57.6 54.4 49.7 55.4 52.4
5 Base+FrameNet (FN) 56.4 70.9 62.8 54.9 67.5 60.5 50.5 57.5 53.8 48.8 55.1 51.8
6 Base+Verb Pairs (VP) 56.9 71.3 63.3 55.2 67.6 60.8 50.7 57.9 54.0 49.0 55.4 52.0
7 Base+Appositives (AP) 56.9 70.0 62.7 55.6 66.9 60.7 50.3 57.1 53.5 49.1 55.1 51.9
Results for the Cluster-Ranking Model
8 Base 61.7 71.2 66.1 59.6 68.8 63.8 53.4 59.2 56.2 51.1 57.3 54.0
9 Base+YAGO Types (YT) 63.5 72.4 67.6 61.7 70.0 65.5 54.8 60.6 57.6 52.4 58.9 55.4
10 Base+YAGO Means (YM) 62.0 71.4 66.4 59.9 69.1 64.1 53.9 59.5 56.6 51.4 57.5 54.3
11 Base+Noun Pairs (WP) 64.1 73.4 68.4 61.3 70.1 65.4 55.9 62.1 58.8 53.5 59.1 56.2
12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3 64.2 53.5 60.0 56.6 51.1 57.9 54.3
13 Base+Verb Pairs (VP) 62.1 72.2 66.8 60.1 69.3 64.4 54.4 60.1 57.1 51.9 58.2 54.9
14 Base+Appositives (AP) 63.1 71.7 67.1 60.5 69.4 64.6 54.1 60.1 56.9 51.9 57.8 54.7
Table 1: Results obtained by applying different types of features in isolation to the Baseline system.
ACE OntoNotes
B3 CEAF B3 CEAF
Feature Set R P F R P F R P F R P F
Results for the Mention-Pair Model
1 Base 56.5 69.7 62.4 54.9 66.3 60.0 50.4 56.7 53.3 48.9 54.5 51.5
2 Base+YT 57.3 70.3 63.1 58.7 67.5 62.8 51.7 57.9 54.6 50.3 55.6 52.8
3 Base+YT+YM 57.8 70.9 63.6 59.1 67.9 63.2 52.1 58.3 55.0 50.8 56.0 53.3
4 Base+YT+YM+WP 59.5 71.9 65.1 57.5 69.4 62.9 53.1 59.2 56.0 51.5 57.1 54.1
5 Base+YT+YM+WP+FN 59.6 72.1 65.3 57.2 69.7 62.8 53.1 59.5 56.2 51.3 57.4 54.2
6 Base+YT+YM+WP+FN+VP 59.9 72.5 65.6 57.8 70.0 63.3 53.4 59.8 56.4 51.8 57.7 54.6
7 Base+YT+YM+WP+FN+VP+AP 59.7 72.4 65.4 57.6 69.8 63.1 53.2 59.8 56.3 51.5 57.6 54.4
Results for the Cluster-Ranking Model
8 Base 61.7 71.2 66.1 59.6 68.8 63.8 53.4 59.2 56.2 51.1 57.3 54.0
9 Base+YT 63.5 72.4 67.6 61.7 70.0 65.5 54.8 60.6 57.6 52.4 58.9 55.4
10 Base+YT+YM 63.9 72.6 68.0 62.1 70.4 66.0 55.2 61.0 57.9 52.8 59.1 55.8
11 Base+YT+YM+WP 66.1 75.4 70.4 62.9 72.4 67.3 57.7 64.4 60.8 55.1 61.6 58.2
12 Base+YT+YM+WP+FN 66.3 75.1 70.4 63.1 72.3 67.4 57.3 64.1 60.5 54.7 61.2 57.8
13 Base+YT+YM+WP+FN+VP 66.6 75.9 70.9 63.5 72.9 67.9 57.7 64.4 60.8 55.1 61.6 58.2
14 Base+YT+YM+WP+FN+VP+AP 66.4 75.7 70.7 63.3 72.9 67.8 57.6 64.3 60.8 55.0 61.5 58.1
Table 2: Results obtained by adding different types of features incrementally to the Baseline system.
OntoNotes annotations as the gold standard. As we
can see, the MP model achieves F-measure scores of
62.4 (B3) and 60.0 (CEAF) on ACE and 53.3 (B3)
and 51.5 (CEAF) on OntoNotes, and the CR model
achieves F-measure scores of 66.1 (B3) and 63.8
(CEAF) on ACE and 56.2 (B3) and 54.0 (CEAF)
on OntoNotes. Also, the results show that the CR
model is stronger than the MP model, corroborating
previous empirical findings (Rahman and Ng, 2009).
4.2.2 Incorporating World Knowledge
Next, we examine the usefulness of world knowl-
edge for coreference resolution. The remaining rows
in Table 1 show the results obtained when different
types of features encoding world knowledge are ap-
plied to the Baseline system in isolation. The best
result for each combination of data set, evaluation
measure, and coreference model is boldfaced.
Two points deserve mention. First, each type
of features improves the Baseline, regardless of the
coreference model, the evaluation measure, and the
annotation scheme used. This suggests that all these
feature types are indeed useful for coreference reso-
lution. It is worth noting that in all but a few cases
involving the FrameNet-based and appositive-based
features, the rise in F-measure is accompanied by a
821
1. The Bush White House is breeding non-duck ducks the same way the Nixon White House did: It hops on an
issue that is unopposable ? cleaner air, better treatment of the disabled, better child care. The President came
up with a good bill, but now may end up signing the awful bureaucratic creature hatched on Capitol Hill.
2. The tumor, he suggested, developed when the second, normal copy also was damaged. He believed colon
cancer might also arise from multiple ?hits? on cancer suppressor genes, as it often seems to develop in stages.
Table 3: Examples errors introduced by YAGO and FrameNet.
simultaneous rise in recall and precision. This is per-
haps not surprising: as the use of world knowledge
helps discover coreference links, recall increases;
and as more (relevant) knowledge is available to
make coreference decisions, precision increases.
Second, the feature types that yield the best im-
provement over the Baseline are YAGO TYPE and
Noun Pairs. When the MP model is used, the best
coreference system improves the Baseline by 1?
1.3% (B3) and 1.3?2.8% (CEAF) in F-measure. On
the other hand, when the CR model is used, the best
system improves the Baseline by 2.3?2.6% (B3) and
1.7?2.2% (CEAF) in F-measure.
Table 2 shows the results obtained when the dif-
ferent types of features are added to the Baseline one
after the other. Specifically, we add the feature types
in this order: YAGO TYPE, YAGO MEANS, Noun
Pairs, FrameNet, Verb Pairs, and Appositives. In
comparison to the results in Table 1, we can see that
better results are obtained when the different types
of features are applied to the Baseline in combina-
tion than in isolation, regardless of the coreference
model, the evaluation measure, and the annotation
scheme used. The best-performing system, which
employs all but the Appositive features, outperforms
the Baseline by 3.1?3.3% in F-measure when the
MR model is used and by 4.1?4.8% in F-measure
when the CR model is used. In both cases, the
gains in F-measure are accompanied by a simulta-
neous rise in recall and precision. Overall, these
results seem to suggest that the CR model is mak-
ing more effective use of the available knowledge
than the MR model, and that the different feature
types are providing complementary information for
the two coreference models.
4.3 Example Errors
While the different types of features we considered
improve the performance of the Baseline primarily
via the establishment of coreference links, some of
these links are spurious. Sentences 1 and 2 of Table
3 show the spurious coreference links introduced by
the CR model when YAGO and FrameNet are used,
respectively. In sentence 1, while The President and
Bush are coreferent, YAGO caused the CR model
to establish the spurious link between The President
and Nixon owing to the proximity of the two NPs
and the presence of this NP pair in the YAGO TYPE
relation. In sentence 2, FrameNet caused the CR
model to establish the spurious link between The tu-
mor and colon cancer because these two NPs are the
ARG0 arguments of develop and arise, which appear
in the same semantic frame in FrameNet.
5 Conclusions
We have examined the utility of three major
sources of world knowledge for coreference resolu-
tion, namely, large-scale knowledge bases (YAGO,
FrameNet), coreference-annotated data (Noun Pairs,
Verb Pairs), and unannotated data (Appositives), by
applying them to two learning-based coreference
models, the mention-pair model and the cluster-
ranking model, and evaluating them on documents
annotated with the ACE and OntoNotes annotation
schemes. When applying the different types of fea-
tures in isolation to a Baseline system that does not
employ world knowledge, we found that all of them
improved the Baseline regardless of the underlying
coreference model, the evaluation measure, and the
annotation scheme, with YAGO TYPE and Noun
Pairs yielding the largest performance gains. Nev-
ertheless, the best results were obtained when they
were applied in combination to the Baseline system.
We conclude from these results that the different fea-
ture types we considered are providing complemen-
tary world knowledge to the coreference resolvers,
and while each of them provides fairly small gains,
their cumulative benefits can be substantial.
822
Acknowledgments
We thank the three reviewers for their invaluable
comments on an earlier draft of the paper. This work
was supported in part by NSF Grant IIS-0812261.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at The First Interna-
tional Conference on Language Resources and Eval-
uation, pages 563?566.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics and the 17th International
Conference on Computational Linguistics, Volume 1,
pages 86?90.
David Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference reso-
lution. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, pages
297?304.
Eric Bengtson and Dan Roth. 2008. Understanding the
values of features for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 294?303.
Volha Bryl, Claudio Giuliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Using background
knowledge to support coreference resolution. In Pro-
ceedings of the 19th European Conference on Artificial
Intelligence, pages 759?764.
Eugene Charniak. 1972. Towards a Model of Children?s
Story Comphrension. AI-TR 266, Artificial Intelli-
gence Laboratory, Massachusetts Institute of Technol-
ogy.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In Human Language Technologies 2007:
The Conference of the North American Chapter of the
Association for Computational Linguistics; Proceed-
ings of the Main Conference, pages 81?88.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 97?104.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?669.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
363?370.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online ques-
tion answering: Answering questions before they are
asked. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pages
1?7.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152?1161.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, pages 57?60.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 133?142.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics, pages 135?142.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 25?
32.
Ruslan Mitkov. 2002. Anaphora Resolution. Longman.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 104?111.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of the Twentieth Inter-
national Joint Conference on Artificial Intelligence,
pages 1689?1694.
Simone Paolo Ponzetto and Massimo Poesio. 2009.
State-of-the-art NLP approaches to coreference reso-
lution: Theory and practical recipes. In Tutorial Ab-
stracts of ACL-IJCNLP 2009, page 6.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
823
of the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 192?199.
Sameer S. Pradhan, Wayne H. Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. 2004. Shallow
semantic parsing using support vector machines. In
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 233?240.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 656?664.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence resolution with Reconcile. In Proceedings of
the ACL 2010 Conference Short Papers, pages 156?
161.
Fabian Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2007. YAGO: A core of semantic knowledge unifying
wordnet and wikipedia. In Proceedings of the World
Wide Web Conference, pages 697?706.
Olga Uryupina, Massimo Poesio, Claudio Giuliano, and
Kateryna Tymoshenko. 2011. Disambiguation and
filtering methods in using web knowledge for coref-
erence resolution. In Proceedings of the 24th Interna-
tional Florida Artificial Intelligence Research Society
Conference.
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim
Tan. 2003. Coreference resolution using competitive
learning approach. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 176?183.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, and
Sheng Li. 2008. An entity-mention model for coref-
erence resolution with inductive logic programming.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 843?851.
824
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 260?269,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modeling Thesis Clarity in Student Essays
Isaac Persing and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{persingq,vince}@hlt.utdallas.edu
Abstract
Recently, researchers have begun explor-
ing methods of scoring student essays with
respect to particular dimensions of qual-
ity such as coherence, technical errors,
and relevance to prompt, but there is rel-
atively little work on modeling thesis clar-
ity. We present a new annotated corpus
and propose a learning-based approach to
scoring essays along the thesis clarity di-
mension. Additionally, in order to pro-
vide more valuable feedback on why an
essay is scored as it is, we propose a sec-
ond learning-based approach to identify-
ing what kinds of errors an essay has that
may lower its thesis clarity score.
1 Introduction
Automated essay scoring, the task of employing
computer technology to evaluate and score writ-
ten text, is one of the most important educational
applications of natural language processing (NLP)
(see Shermis and Burstein (2003) and Shermis et
al. (2010) for an overview of the state of the art
in this task). A major weakness of many ex-
isting scoring engines such as the Intelligent Es-
say AssessorTM(Landauer et al, 2003) is that they
adopt a holistic scoring scheme, which summa-
rizes the quality of an essay with a single score and
thus provides very limited feedback to the writer.
In particular, it is not clear which dimension of
an essay (e.g., style, coherence, relevance) a score
should be attributed to. Recent work addresses this
problem by scoring a particular dimension of es-
say quality such as coherence (Miltsakaki and Ku-
kich, 2004), technical errors, Relevance to Prompt
(Higgins et al, 2004), and organization (Persing
et al, 2010). Essay grading software that provides
feedback along multiple dimensions of essay qual-
ity such as E-rater/Criterion (Attali and Burstein,
2006) has also begun to emerge.
Nevertheless, there is an essay scoring dimen-
sion for which few computational models have
been developed ? thesis clarity. Thesis clarity
refers to how clearly an author explains the thesis
of her essay, i.e., the position she argues for with
respect to the topic on which the essay is written.1
An essay with a high thesis clarity score presents
its thesis in a way that is easy for the reader to
understand, preferably but not necessarily directly,
as in essays with explicit thesis sentences. It addi-
tionally contains no errors such as excessive mis-
spellings that make it more difficult for the reader
to understand the writer?s purpose.
Our goals in this paper are two-fold. First, we
aim to develop a computational model for scoring
the thesis clarity of student essays. Because there
are many reasons why an essay may receive a low
thesis clarity score, our second goal is to build a
system for determining why an essay receives its
score. We believe the feedback provided by this
system will be more informative to a student than
would a thesis clarity score alone, as it will help
her understand which aspects of her writing need
to be improved in order to better convey her the-
sis. To this end, we identify five common errors
that impact thesis clarity, and our system?s pur-
pose is to determine which of these errors occur
in a given essay. We evaluate our thesis clarity
scoring model and error identification system on a
data set of 830 essays annotated with both thesis
clarity scores and errors.
In sum, our contributions in this paper are three-
fold. First, we develop a scoring model and error
identification system for the thesis clarity dimen-
sion on student essays. Second, we use features
explicitly designed for each of the identified error
1An essay?s thesis is the overall message of the entire es-
say. This concept is unbound from the the concept of thesis
sentences, as even an essay that never explicitly states its the-
sis in any of its sentences may still have an overall message
that can be inferred from the arguments it makes.
260
Topic Languages Essays
Most university degrees are the-
oretical and do not prepare stu-
dents for the real world. They are
therefore of very little value.
13 131
The prison system is outdated.
No civilized society should pun-
ish its criminals: it should reha-
bilitate them.
11 80
In his novel Animal Farm,
George Orwell wrote ?All men
are equal but some are more
equal than others.? How true is
this today?
10 64
Table 1: Some examples of writing topics.
types in order to train our scoring model, in con-
trast to many existing systems for other scoring di-
mensions, which use more general features devel-
oped without the concept of error classes. Third,
we make our data set consisting of thesis clarity
annotations of 830 essays publicly available in or-
der to stimulate further research on this task. Since
progress in thesis clarity modeling is hindered in
part by the lack of a publicly annotated corpus, we
believe that our data set will be a valuable resource
to the NLP community.
2 Corpus Information
We use as our corpus the 4.5 million word Interna-
tional Corpus of Learner English (ICLE) (Granger
et al, 2009), which consists of more than 6000 es-
says written by university undergraduates from 16
countries and 16 native languages who are learn-
ers of English as a Foreign Language. 91% of the
ICLE texts are argumentative. We select a sub-
set consisting of 830 argumentative essays from
the ICLE to annotate and use for training and test-
ing of our models of essay thesis clarity. Table 1
shows three of the thirteen topics selected for an-
notation. Fifteen native languages are represented
in the set of essays selected for annotation.
3 Corpus Annotation
For each of the 830 argumentative essays, we ask
two native English speakers to (1) score it along
the thesis clarity dimension and (2) determine the
subset of the five pre-defined errors that detracts
from the clarity of its thesis.
Scoring. Annotators evaluate the clarity of each
essay?s thesis using a numerical score from 1 to
4 at half-point increments (see Table 2 for a de-
scription of each score). This contrasts with pre-
vious work on essay scoring, where the corpus is
Score Description of Thesis Clarity
4 essay presents a very clear thesis and requires
little or no clarification
3 essay presents a moderately clear thesis but
could benefit from some clarification
2 essay presents an unclear thesis and would
greatly benefit from further clarification
1 essay presents no thesis of any kind and it is
difficult to see what the thesis could be
Table 2: Descriptions of the meaning of scores.
annotated with a binary decision (i.e., good or bad)
for a given scoring dimension (e.g., Higgins et al
(2004)). Hence, our annotation scheme not only
provides a finer-grained distinction of thesis clar-
ity (which can be important in practice), but also
makes the prediction task more challenging.
To ensure consistency in annotation, we ran-
domly select 100 essays to have graded by both
annotators. Analysis of these essays reveals that,
though annotators only exactly agree on the the-
sis clarity score of an essay 36% of the time, the
scores they apply are within 0.5 points in 62% of
essays and within 1.0 point in 85% of essays. Ta-
ble 3 shows the number of essays that receive each
of the seven scores for thesis clarity.
score 1.0 1.5 2.0 2.5 3.0 3.5 4.0
essays 4 9 52 78 168 202 317
Table 3: Distribution of thesis clarity scores.
Error identification. To identify what kinds of
errors make an essay?s thesis unclear, we ask one
of our annotators to write 1?4 sentence critiques
of thesis clarity on 527 essays, and obtain our list
of five common error classes by categorizing the
things he found to criticize. We present our anno-
tators with descriptions of these five error classes
(see Table 4), and ask them to assign zero or more
of the error types to each essay.
It is important to note that we ask our anno-
tators to mark an essay with one of these errors
only when the error makes the thesis less clear. So
for example, an essay whose thesis is irrelevant to
the prompt but is explicitly and otherwise clearly
stated would not be marked as having a Relevance
to Prompt error. If the irrelevant thesis is stated
in such a way that its inapplicability to the prompt
causes the reader to be confused about what the
essay?s purpose is, however, then the essay would
be assigned a Relevance to Prompt error.
To measure inter-annotator agreement on error
identification, we ask both annotators to identify
261
Id Error Description
CP Confusing Phrasing The thesis is phrased oddly, making it hard to understand the writer?s point.
IPR Incomplete Prompt Response The thesis seems to leave some part of a multi-part prompt unaddressed.
R Relevance to Prompt The apparent thesis?s weak relation to the prompt causes confusion.
MD Missing Details The thesis leaves out important detail needed to understand the writer?s point.
WP Writer Position The thesis describes a position on the topic without making it clear that this is
the position the writer supports.
Table 4: Descriptions of thesis clarity errors.
the errors in the same 100 essays that were doubly-
annotated with thesis clarity scores. We then com-
pute Cohen?s Kappa (Carletta, 1996) on each er-
ror from the two sets of annotations, obtaining an
average Kappa value of 0.75, which indicates fair
agreement. Table 5 shows the number of essays
assigned to each of the five thesis clarity errors.
As we can see, Confusing Phrasing, Incomplete
Prompt Response, and Relevance to Prompt are
the major error types.
error CP IPR R MD WP
essays 152 123 142 47 39
Table 5: Distribution of thesis clarity errors.
Relationship between clarity scores and error
classes. To determine the relationship between
thesis clarity scores and the five error classes, we
train a linear SVM regressor using the SVMlight
software package (Joachims, 1999) with the five
error types as independent variables and the re-
duction in thesis clarity score due to errors as the
dependent variable. More specifically, each train-
ing example consists of a target, which we set to
the essay?s thesis clarity score minus 4.0, and six
binary features, each of the first five representing
the presence or absence of one of the five errors in
the essay, and the sixth being a bias feature which
we always set to 1. Representing the reduction in
an essay?s thesis clarity score with its thesis clarity
score minus 4.0 allows us to more easily interpret
the error and bias weights of the trained system,
as under this setup, each error?s weight should be a
negative number reflecting how many points an es-
say loses due to the presence of that error. The bias
feature allows for the possibility that an essay may
lose points from its thesis clarity score for prob-
lems not accounted for in our five error classes.
By setting this bias feature to 1, we tell our learner
that an essay?s default score may be less than 4.0
because these other problems may lower the aver-
age score of otherwise perfect essays.
After training, we examined the weight param-
eters of the learned regressor and found that they
were all negative: ?0.6 for CP, ?0.5998 for IPR,
?0.8992 for R, ?0.6 for MD, ?0.8 for WP, and
?0.1 for the bias. These results are consistent
with our intuition that each of the enumerated er-
ror classes has a negative impact on thesis clarity
score. In particular, each has a demonstrable neg-
ative impact, costing essays an average of more
than 0.59 points when it occurs. Moreover, this set
of errors accounts for a large majority of all errors
impacting thesis clarity because unenumerated er-
rors cost essays an average of only one-tenth of
one point on the four-point thesis clarity scale.
4 Error Classification
In this section, we describe in detail our system for
identifying thesis clarity errors.
4.1 Model Training and Application
We recast the problem of identifying which the-
sis clarity errors apply to an essay as a multi-label
classification problem, wherein each essay may be
assigned zero or more of the five pre-defined er-
ror types. To solve this problem, we train five bi-
nary classifiers, one for each error type, using a
one-versus-all scheme. So in the binary classifi-
cation problem for identifying error ei, we create
one training instance from each essay in the train-
ing set, labeling the instance as positive if the es-
say has ei as one of its labels, and negative other-
wise. Each instance is represented by seven types
of features, including two types of baseline fea-
tures (Section 4.2) and five types of features we
introduce for error identification (Section 4.3).
After creating training instances for error ei, we
train a binary classifier, bi, for identifying which
test essays contain error ei. We use SVMlight for
classifier training with the regularization param-
eter, C , set to ci. To improve classifier perfor-
mance, we perform feature selection. While we
employ seven types of features (see Sections 4.2
and 4.3), only the word n-gram features are sub-
ject to feature selection.2 Specifically, we employ
2We do not apply feature selection to the remaining fea-
262
the top ni n-gram features as selected according to
information gain computed over the training data
(see Yang and Pedersen (1997) for details). Fi-
nally, since each classifier assigns a real value to
each test essay presented to it indicating its con-
fidence that the essay should be assigned error ei,
we employ a classification threshold ti to decide
how high this real value must be in order for our
system to conclude that an essay contains error ei.
Using held-out validation data, we jointly tune
the three parameters in the previous paragraph, ci,
ni, and ti, to optimize the F-score achieved by bi
for error ei.3 However, an exact solution to this op-
timization problem is computationally expensive.
Consequently, we find a local maximum by em-
ploying the simulated annealing algorithm (Kirk-
patrick et al, 1983), altering one parameter at a
time to optimize F-score by holding the remaining
parameters fixed.
After training the classifiers, we use them to
classify the test set essays. The test instances are
created in the same way as the training instances.
4.2 Baseline Features
Our Baseline system for error classification em-
ploys two types of features. First, since labeling
essays with thesis clarity errors can be viewed as
a text categorization task, we employ lemmatized
word unigram, bigram, and trigram features that
occur in the essay that have not been removed by
the feature selection parameter ni. Because the
essays vary greatly in length, we normalize each
essay?s set of word features to unit length.
The second type of baseline features is based on
random indexing (Kanerva et al, 2000). Random
indexing is ?an efficient, scalable and incremen-
tal alternative? (Sahlgren, 2005) to Latent Seman-
tic Indexing (Deerwester et al, 1990; Landauer
ture types since each of them includes only a small number
of overall features that are expected to be useful.
3For parameter tuning, we employ the following values.
ci may be assigned any of the values 102, 103, 104, 105, or
106. ni may be assigned any of the values 3000, 4000, 5000,
or ALL, where ALL means all features are used. For ti, we
split the range of classification values bi returns for the test set
into tenths. ti may take the values 0.0, 0.1, 0.2, . . ., 1.0, and
X, where 0.0 classifies all instances as negative, 0.1 classifies
only instances bi assigned values in the top tenth of the range
as positive, and so on, and X is the default threshold, labeling
essays as positive instances of ei only if bi returns for them a
value greater than 0. It was necessary to assign ti in this way
because the range of values classifiers return varies greatly
depending on which error type we are classifying and which
other parameters we use. This method gives us reasonably
fine-grained thresholds without having to try an unreasonably
large number of values for ti.
and Dutnais, 1997) which allows us to automat-
ically generate a semantic similarity measure be-
tween any two words. We train our random in-
dexing model on over 30 million words of the En-
glish Gigaword corpus (Parker et al, 2009) using
the S-Space package (Jurgens and Stevens, 2010).
We expect that features based on random index-
ing may be particularly useful for the Incomplete
Prompt Response and Relevance to Prompt errors
because they may help us find text related to the
prompt even if some of its components have been
rephrased (e.g., an essay may talk about ?jail?
rather than ?prison?, which is mentioned in one
of the prompts). For each essay, we therefore gen-
erate four random indexing features, one encoding
the entire essay?s similarity to the prompt, another
encoding the essay?s highest individual sentence?s
similarity to the prompt, a third encoding the high-
est entire essay similarity to one of the prompt sen-
tences, and finally one encoding the highest indi-
vidual sentence similarity to an individual prompt
sentence. Since random indexing does not pro-
vide a straightforward way to measure similar-
ity between groups of words such as sentences
or essays, we use Higgins and Burstein?s (2007)
method to generate these features.
4.3 Novel Features
Next, we introduce five types of novel features.
Spelling. One problem we note when examining
the information gain top-ranked features for the
Confusing Phrasing error is that there are very few
common confusing phrases that can contribute to
this error. Errors of this type tend to be unique, and
hence are not very useful for error classification
(because we are not likely to see the same error
in the training and test sets). We notice, however,
that there are a few misspelled words at the top of
the list. This makes sense because a thesis sen-
tence containing excessive misspellings may be
less clear to the reader. Even the most common
spelling errors, however, tend to be rare. Further-
more, we ask our annotators to only annotate an
error if it makes the thesis less clear. The mere
presence of an awkward phrase or misspelling is
not enough to justify the Confusing Phrasing label.
Hence, we introduce a misspelling feature whose
value is the number of spelling errors in an essay?s
most-misspelled sentence.4
4We employ SCOWL (http://wordlist.
sourceforge.net/) as our dictionary, assuming that a
263
Keywords. Improving the prediction of major-
ity classes can greatly enhance our system?s over-
all performance. Hence, since we have introduced
the misspelling feature to enhance our system?s
performance on one of the more frequently occur-
ring errors (Confusing Phrasing), it makes sense
to introduce another type of feature to improve
performance on the other two most frequent er-
rors, Incomplete Prompt Response and Relevance
to Prompt. For this reason, we introduce keyword
features. To use this feature, we first examine each
of the 13 essay prompts, splitting it into its com-
ponent pieces. For our purposes, a component of
a prompt is a prompt substring such that, if an es-
say does not address it, it may be assigned the In-
complete Prompt Response label. Then, for each
component, we manually select the most impor-
tant (primary) and second most important (sec-
ondary) words that it would be good for a writer
to use to address the component. To give an ex-
ample, the lemmatized version of the third com-
ponent of the second essay in Table 1 is ?it should
rehabilitate they?. For this component we selected
?rehabilitate? as a primary keyword and ?society?
as a secondary keyword. To compute one of our
keyword features, we compute the random index-
ing similarity between the essay and each group of
primary keywords taken from components of the
essay?s prompt and assign the feature the lowest
of these values. If this feature has a low value, that
suggests that the essay may have an Incomplete
Prompt Response error because the essay proba-
bly did not respond to the part of the prompt from
which this value came. To compute another of the
keyword features, we count the numbers of com-
bined primary and secondary keywords the essay
contains from each component of its prompt, and
divide each number by the total number of primary
and secondary features for that component. If the
greatest of these fractions has a low value, that in-
dicates the essay?s thesis might not be very Rele-
vant to the Prompt.5
Aggregated word n-gram features. Other
ways we could measure our system?s performance
(such as macro F-score) would consider our
system?s performance on the less frequent errors
no less important than its performance on the
word that does not appear in the dictionary is misspelled.
5Space limitations preclude a complete listing of the key-
word features. See our website at http://www.hlt.
utdallas.edu/
?
persingq/ICLE/ for the complete
list.
most frequent errors. For this reason, it now
makes sense for us to introduce a feature tailored
to help our system do better at identifying the
least-frequent error types, Missing Details and
Writer Position, each of which occurs in fewer
than 50 essays. To help with identification of
these error classes, we introduce aggregated
word n-gram features. While we mention in the
previous section one of the reasons regular word
n-gram features can be expected to help with
these error classes, one of the problems with
regular word n-gram features is that it is fairly
infrequent for the exact same useful phrase to
occur too frequently. Additionally, since there are
numerous word n-grams, some infrequent ones
may just by chance only occur in positive training
set instances, causing the learner to think they
indicate the positive class when they do not. To
address these problems, for each of the five error
classes ei, we construct two Aggregated word
features Aw+i and Aw?i. For each essay, Aw+i
counts the number of word n-grams we believe
indicate that an essay is a positive example of ei,
and Aw?i counts the number of word n-grams
we believe indicate an essay is not an example of
ei. Aw+ n-grams for the Missing Details error
tend to include phrases like ?there is something?
or ?this statement?, while Aw? ngrams are often
words taken directly from an essay?s prompt.
N-grams used for Writer Position?s Aw+ tend
to suggest the writer is distancing herself from
whatever statement is being made such as ?every
person?, but n-grams for this error?s Aw? feature
are difficult to find. Since Aw+i and Aw?i are
so error specific, they are only included in an
essay?s feature representation when it is presented
to learner bi. So while aggregated word n-grams
introduce ten new features, each learner bi only
sees two of these (Aw+i and Aw?i).
We construct the lists of word n-grams that are
aggregated for use as the Aw+ and Aw? fea-
ture values in the following way. For each error
class ei, we sort the list of all features occurring
at least ten times in the training set by information
gain. A human annotator then manually inspects
the top thousand features in each of the five lists
and sorts each list?s features into three categories.
The first category for ei?s list consists of features
that indicate an essay may be a positive instance.
Each word n-gram from this list that occurs in an
essay increases the essay?s Aw+i value by one.
264
Similarly, any word n-gram sorted into the second
category, which consists of features the annotator
thinks indicate a negative instance of ei, increases
the essay?s Aw? value by one. The third category
just contains all the features the annotator did not
believe were useful enough to either class, and we
make no further use of those features. For most er-
ror types, only about 12% of the top 1000 features
get sorted into one of the first two categories.
POS n-grams. We might further improve our
system?s performance on the Missing Details er-
ror type by introducing a feature that aggregates
part-of-speech (POS) tag n-grams in the same way
that the Aw features aggregate word n-gram fea-
tures. For this reason, we include POS tag 1, 2,
3, and 4-grams in the set of features we sort in
the previous paragraph. For each error ei, we se-
lect POS tag n-grams from the top thousand fea-
tures of the information gain sorted list to count
toward the Ap+i and Ap?i aggregation features.
We believe this kind of feature may help improve
performance on Missing Details because the list
of features aggregated to generate the Ap+i fea-
ture?s value includes POS n-gram features like CC
? NN ? (scare quotes). This feature type may also
help with Confusing Phrasing because the list of
POS tag n-grams our annotator generated for its
Ap+i contains useful features like DT NNS VBZ
VBN (e.g., ?these signals has been?), which cap-
tures noun-verb disagreement.
Semantic roles. Our last aggregated feature is
generated using FrameNet-style semantic role la-
bels obtained using SEMAFOR (Das et al, 2010).
For each sentence in our data set, SEMAFOR
identifies each semantic frame occurring in the
sentence as well as each frame element that par-
ticipates in it. For example, a semantic frame
may describe an event that occurs in a sentence,
and the event?s frame elements may be the peo-
ple or objects that participate in the event. For
a more concrete example, consider the sentence
?They said they do not believe that the prison sys-
tem is outdated?. This sentence contains a State-
ment frame because a statement is made in it. One
of the frame elements participating in the frame is
the Speaker ?they?. From this frame, we would
extract a feature pairing the frame together with
its frame element to get the feature ?Statement-
Speaker-they?. This feature indicates that the es-
say it occurs in might be a positive instance of the
Writer Position error since it tells us the writer is
attributing some statement being made to someone
else. Hence, this feature along with several oth-
ers like ?Awareness-Cognizer-we all? are useful
when constructing the lists of frame features for
Writer Position?s aggregated frame features Af+i
and Af?i. Like every other aggregated feature,
Af+i and Af?i are generated for every error ei.
5 Score Prediction
Because essays containing thesis clarity errors
tend to have lower thesis clarity scores than essays
with fewer errors, we believe that thesis clarity
scores can be predicted for essays by utilizing the
same features we use for identifying thesis clarity
errors. Because our score prediction system uses
the same feature types we use for thesis error iden-
tification, each essay?s vector space representation
remains unchanged. Only its label changes to one
of the values in Table 2 in order to reflect its thesis
clarity score. To make use of the fact that some
pairs of scores are more similar than others (e.g.,
an essay with a score of 3.5 is more similar to an
essay with a score of 4.0 than it is to one with a
score of 1.0), we cast thesis clarity score predic-
tion as a regression rather than classification task.
Treating thesis clarity score prediction as a re-
gression problem removes our need for a classi-
fication threshold parameter like the one we use
in the error identification problem, but if we use
SVMlight?s regression option, it does not remove
the need for tuning a regularization parameter, C ,
or a feature selection parameter, n.6 We jointly
tune these two parameters to optimize perfor-
mance on held-out validation data by performing
an exhaustive search in the parameter space.7
After we select the features, construct the essay
instances, train a regressor on training set essays,
and tune parameters on validation set essays, we
can use the regressor to obtain thesis clarity scores
on test set essays.
6Before tuning the feature selection parameter, we have to
sort the list of n-gram features occurring the training set. To
enable the use of information gain as the sorting criterion, we
treat each distinct score as its own class.
7The absence of the classification threshold parameter and
the fact that we do not need to train multiple learners, one for
each score, make it feasible for us to do two things. First, we
explore a wider range of values for the two parameters: we
allow C to take any value from 100, 101, 102, 103, 104, 105,
106, or 107, and we allow n to take any value from 1000,
2000, 3000, 4000, 5000, or ALL. Second, we exhaustively
explore the space defined by these parameters in order to ob-
tain an exact solution to the parameter optimization problem.
265
6 Evaluation
In this section, we evaluate our systems for error
identification and scoring. All the results we re-
port are obtained via five-fold cross-validation ex-
periments. In each experiment, we use 3/5 of our
labeled essays for model training, another 1/5 for
parameter tuning, and the final 1/5 for testing.
6.1 Error Identification
Evaluation metrics. To evaluate our thesis clar-
ity error type identification system, we compute
precision, recall, micro F-score, and macro F-
score, which are calculated as follows. Let tpi be
the number of test essays correctly labeled as posi-
tive by error ei?s binary classifier bi; pi be the total
number of test essays labeled as positive by bi; and
gi be the total number of test essays that belong to
ei according to the gold standard. Then, the preci-
sion (Pi), recall (Ri), and F-score (Fi) for bi and
the macro F-score (F?) of the combined system for
one test fold are calculated by
Pi =
tpi
pi
,Ri =
tpi
gi
, Fi =
2PiRi
Pi +Ri
, F? =
?
i Fi
5 .
However, the macro F-score calculation can be
seen as giving too much weight to the less frequent
errors. To avoid this problem, we also calculate
for each system the micro precision, recall, and F-
score (P, R, and F), where
P =
?
i tpi?
i pi
,R =
?
i tpi?
i gi
,F = 2PRP + R .
Since we perform five-fold cross-validation,
each value we report for each of these measures
is an average over its values for the five folds.8
Results and discussion. Results on error iden-
tification, expressed in terms of precision, recall,
micro F-score, and macro F-score are shown in
the first four columns of Table 6. Our Baseline
system, which only uses word n-gram and random
indexing features, seems to perform uniformly
poorly across both micro and macro F-scores (F
and F?; see row 1). The per-class results9 show
that, since micro F-score places more weight on
the correct identification of the most frequent er-
rors, the system?s micro F-score (31.1%) is fairly
close to the average of the scores obtained on the
three most frequent error classes, CP, IPR, and R,
8This averaging explains why the formula for F does not
exactly hold in the Table 6 results.
9Per-class results are not shown due to space limitations.
Error Identification Scoring
System P R F F? S1 S2 S3
1 B 24.8 44.7 31.1 24.0 .658 .517 .403
2 Bm 24.2 44.2 31.2 25.3 .654 .515 .402
3 Bmk 29.2 44.2 34.9 26.7 .663 .490 .369
4 Bmkw 28.5 49.6 35.5 31.4 .651 .484 .374
5 Bmkwp 34.2 49.6 40.4 34.6 .671 .483 .377
6 Bmkwpf 33.6 54.4 41.4 37.3 .672 .486 .382
Table 6: Five-fold cross-validation results for the-
sis clarity error identification and scoring.
and remains unaffected by very low F-scores on
the two remaining infrequent classes.10
When we add the misspelling feature to the
baseline, resulting in the system called Bm
(row 2), the micro F-score sees a very small, in-
significant improvement.11 What is pleasantly sur-
prising, however, is that, even though the mis-
spelling features were developed for the Confus-
ing Phrasing error type, they actually have more
of a positive impact on Missing Details and Writer
Position, bumping their individual error F-scores
up by about 5 and 3 percent respectively. This sug-
gests that spelling difficulties may be correlated
with these other essay-writing difficulties, despite
their apparent unrelatedness. This effect is strong
enough to generate the small, though insignificant,
gain in macro F-score shown in the table.
When we add keyword features to the system,
micro F-score increases significantly by 3.7 points
(row 3). The micro per-class results reveal that,
as intended, keyword features improve Incomplete
Prompt Response and Relevance to Prompt?s F-
scores reveals that they do by 6.4 and 9.2 percent-
age points respectively. The macro F-scores reveal
this too, though the macro F-score gains are 3.2
points and 11.5 points respectively. The macro F-
score of the overall system would likely have im-
proved more than shown in the table if the addition
of keyword features did not simultaneously reduce
Missing Details?s score by several points.
While we hoped that adding aggregated word
n-gram features to the system (row 4) would be
able to improve performance on Confusing Phras-
ing due to the presence of phrases such as ?in uni-
versity be? in the error?s Aw+i list, there turned
out to be few such common phrases in the data set,
10Since parameters for optimizing micro F-score and
macro F-score are selected independently, the per-class F-
scores associated with micro F-score are different than those
used for calculating macro F-score. Hence, when we discuss
per-class changes influencing micro F-score, we refer to the
former set, and otherwise we refer to the latter set.
11All significance tests are paired t-tests, with p < 0.05.
266
so performance on this class remains mostly un-
changed. This feature type does, however, result
in major improvements to micro and macro perfor-
mance on Missing Details and Writer Position, the
other two classes this feature was designed to help.
Indeed, the micro F-score versions of Missing De-
tails and Writer Position improve by 15.3 and 10.8
percentage points respectively. Since these are mi-
nority classes, however, the large improvements
result in only a small, insignificant improvement
in the overall system?s micro F-score. The macro
F-score results for these classes, however, improve
by 6.5% and 17.6% respectively, giving us a nearly
5-point, statistically significant bump in macro F-
score after we add this feature.
Confusing Phrasing has up to now stubbornly
resisted any improvement, even when we added
features explicitly designed to help our system do
better on this error type. When we add aggregated
part of speech n-gram features on top of the pre-
vious system, that changes dramatically. Adding
these features makes both our system?s F-scores
on Confusing Phrasing shoot up almost 8%, re-
sulting in a significant, nearly 4.9% improvement
in overall micro F-score and a more modest but
insignificant 3.2% improvement in macro F-score
(row 5). The micro F-score improvement can
also be partly attributed to a four point improve-
ment in Incomplete Prompt Response?s micro F-
score. The 13.7% macro F-score improvement of
the Missing Details error plays a larger role in the
overall system?s macro F-score improvement than
Confusing Phrasing?s improvement, however.
The improvement we see in micro F-score when
we add aggregated frame features (row 6) can be
attributed almost solely to improvements in classi-
fication of the minority classes. This is surprising
because, as we mentioned before, minority classes
tend to have a much smaller impact on overall
micro F-score. Furthermore, the overall micro
F-score improvement occurrs despite declines in
the performances on two of the majority class er-
rors. Missing Details and Writer Position?s mi-
cro F-score performances increase by 19.1% and
13.4%. The latter is surprising only because of
the magnitude of its improvement, as this feature
type was explicitly intended to improve its perfor-
mance. We did not expect this aggregated feature
type to be especially useful for Missing Details er-
ror identification because very few of these types
of features occur in its Af+i list, and there are
none in its Af?i list. The few that are in the for-
mer list, however, occur fairly often and look like
fairly good indicators of this error (both the exam-
ples ?Event-Event-it? and ?Categorization-Item-
that? occur in the positive list, and both do seem
vague, indicating more details are to be desired).
Overall, this system improves our base-
line?s macro F-score performance significantly by
13.3% and its micro F-score performance signifi-
cantly by 10.3%. As we progressed, adding each
new feature type to the baseline system, there was
no definite and consistent pattern to how the pre-
cisions and recalls changed in order to produce
the universal increases in the F-scores that we ob-
served for each new system. Both just tended to
jerkily progress upward as new feature types were
added. This confirms our intuition about these fea-
tures ? namely that they do not all uniformly im-
prove our performance in the same way. Some aim
to improve precision by telling us when essays are
less likely to be positive instances of an error class,
such as any of the Aw?i, Ap?i, or Af?i features,
and others aim to tell us when an essay is more
likely to be a positive instance of an error.
6.2 Scoring
Scoring metrics. We design three evaluation
metrics to measure the error of our thesis clarity
scoring system. The S1 metric measures the fre-
quency at which a system predicts the wrong score
out of the seven possible scores. Hence, a system
that predicts the right score only 25% of the time
would receive an S1 score of 0.75.
The S2 metric measures the average distance
between the system?s score and the actual score.
This metric reflects the idea that a system that
estimates scores close to the annotator-assigned
scores should be preferred over a system whose
estimations are further off, even if both systems
estimate the correct score at the same frequency.
Finally, the S3 metric measures the average
square of the distance between a system?s the-
sis clarity score estimations and the annotator-
assigned scores. The intuition behind this metric
is that not only should we prefer a system whose
estimations are close to the annotator scores, but
we should also prefer one whose estimations are
not too frequently very far away from the annota-
tor scores. These three scores are given by:
1
N
?
Aj 6=E?j
1, 1N
N?
i=1
|Aj ? Ej|,
1
N
N?
i=1
(Aj ? Ej)2
267
where Aj , Ej , and E?j are the annotator assigned,
system estimated, and rounded system estimated
scores12 respectively for essay j, and N is the
number of essays.
Results and discussion. Results on scoring are
shown in the last three columns of Table 6. We
see that the thesis clarity score predicting variation
of the Baseline system, which employs as features
only word n-grams and random indexing features,
predicts the wrong score 65.8% of the time. Its
predicted score is on average 0.517 points off of
the actual score, and the average squared distance
between the predicted and actual scores is 0.403.
We observed earlier that a high number of mis-
spellings may be positively correlated with one
or more unrelated errors. Adding the misspelling
feature to the scoring systems, however, only
yields minor, insignificant improvements to their
performances under the three scoring metrics.
While adding keyword features on top of this
system does not improve the frequency with which
the right score is predicted, it both tends to move
the predictions closer to the actual thesis clar-
ity score value (as evidenced by the significant
improvement in S2) and ensures that predicted
scores will not too often stray too far from the
actual value (as evidenced by the significant im-
provement in S3). Overall, the scoring model em-
ploying the Bmk feature set performs significantly
better than the Baseline scoring model with re-
spect to two out of three scoring metrics.
The only remaining feature type whose addition
yields a significant performance improvement is
the aggregated word feature type, which improves
system Bmk?s S2 score significantly while having
an insignificant impact on the other S metrics.
Neither of the remaining aggregative features
yields any significant improvements in perfor-
mance. This is a surprising finding since, up un-
til we introduced aggregated part-of-speech tag n-
gram features into our regressor, each additional
feature that helped with error classification made
at least a small but positive contribution to at least
two out of the three S scores. These aggregative
features, which proved to be very powerful when
assigning error labels, are not as useful for thesis
12Since our regressor assigns each essay a real value rather
than an actual valid thesis clarity score, it would be difficult
to obtain a reasonable S1 score without rounding the system
estimated score to one of the possible values. For that rea-
son, we round the estimated score to the nearest of the seven
scores the human annotators were permitted to assign (1.0,
1.5, 2.0, 2.5, 3.0, 3.5, 4.0) only when calculating S1.
S1 (Bmkw) S2 (Bmkwp) S3 (Bmk)
Gold .25 .50 .75 .25 .50 .75 .25 .50 .75
1.0 3.5 3.5 3.5 3.0 3.2 3.5 3.1 3.2 3.3
1.5 2.5 3.0 3.0 2.8 3.1 3.2 2.6 3.0 3.2
2.0 3.0 3.0 3.5 3.0 3.2 3.5 3.0 3.1 3.4
2.5 3.0 3.5 3.5 3.0 3.3 3.6 3.0 3.3 3.5
3.0 3.0 3.5 3.5 3.1 3.4 3.5 3.1 3.3 3.5
3.5 3.5 3.5 4.0 3.2 3.4 3.6 3.2 3.4 3.5
4.0 3.5 3.5 4.0 3.4 3.6 3.8 3.4 3.5 3.7
Table 7: Regressor scores for top three systems.
clarity scoring.
To more closely examine the behavior of the
best scoring systems, in Table 7 we chart the dis-
tributions of scores they predict for each gold stan-
dard score. As an example of how to read this ta-
ble, consider the number 2.8 appearing in row 1.5
in the .25 column of the S2 (Bmkwp) region. This
means that 25% of the time, when system Bmkwp
(which obtains the best S2 score) is presented with
a test essay having a gold standard score of 1.5,
it predicts that the essay has a score less than or
equal to 2.8 for the S2 metric.
From this table, we see that each of the best sys-
tems has a strong bias toward predicting more fre-
quent scores as there are no numbers less than 3.0
in the 50% columns, and about 82.8% of all essays
have gold standard scores of 3.0 or above. Never-
theless, no system relies entirely on bias, as evi-
denced by the fact that each column in the table
has a tendency for its scores to ascend as the gold
standard score increases, implying that the sys-
tems have some success at predicting lower scores
for essays with lower gold standard scores.
Finally, we note that the difference in error
weighting between the S2 and S3 scoring metrics
appears to be having its desired effect, as there is a
strong tendency for each entry in the S3 subtable
to be less than or equal to its corresponding entry
in the S2 subtable due to the greater penalty the
S3 metric imposes for predictions that are very far
away from the gold standard scores.
7 Conclusion
We examined the problem of modeling thesis clar-
ity errors and scoring in student essays. In addition
to developing these models, we proposed novel
features for use in our thesis clarity error model
and employed these features, each of which was
explicitly designed for one or more of the error
types, to train our scoring model. We make our
thesis clarity annotations publicly available in or-
der to stimulate further research on this task.
268
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier
draft of the paper. This work was supported in
part by NSF Grants IIS-1147644 and IIS-1219142.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views or of-
ficial policies, either expressed or implied, of NSF.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with E-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3).
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The Kappa statistic. Computational
Linguistics, 22(2):249?254.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic frame-semantic
parsing. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 948?956.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses universitaires
de Louvain.
Derrick Higgins and Jill Burstein. 2007. Sentence sim-
ilarity measures for essay coherence. In Proceed-
ings of the 7th International Workshop on Computa-
tional Semantics.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects
of coherence in student essays. In Human Lan-
guage Technologies: The 2004 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 185?192.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning, Chapter 11, pages 169?
184. MIT Press, Cambridge, MA.
David Jurgens and Keith Stevens. 2010. The S-Space
package: An open source package for word space
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30?35.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for Latent
Semantic Analysis. In Proceedings the 22nd Annual
Conference of the Cognitive Science Society, pages
103?106.
Scott Kirkpatrick, C. D. Gelatt, and Mario P. Vecchi.
1983. Optimization by simulated annealing. Sci-
ence, 220(4598):671?680.
Thomas K Landauer and Susan T. Dumais. 1997. A
solution to Plato?s problem: The Latent Semantic
Analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
pages 211?240.
Thomas K. Landauer, Darrell Laham, and Peter W.
Foltz. 2003. Automated scoring and annotation of
essays with the Intelligent Essay AssessorTM. In Au-
tomated Essay Scoring: A Cross-Disciplinary Per-
spective, pages 87?112. Lawrence Erlbaum Asso-
ciates, Inc., Mahwah, NJ.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring sys-
tems. Natural Language Engineering, 10(1):25?55.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth
Edition. Linguistic Data Consortium, Philadelphia.
Isaac Persing, Alan Davis, and Vincent Ng. 2010.
Modeling organization in student essays. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 229?
239.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Proceedings of the Methods and Appli-
cations of Semantic Indexing Workshop at the 7th In-
ternational Conference on Terminology and Knowl-
edge Engineering.
Mark D. Shermis and Jill C. Burstein. 2003. Au-
tomated Essay Scoring: A Cross-Disciplinary Per-
spective. Lawrence Erlbaum Associates, Inc., Mah-
wah, NJ.
Mark D. Shermis, Jill Burstein, Derrick Higgins, and
Klaus Zechner. 2010. Automated essay scoring:
Writing assessment and instruction. In International
Encyclopedia of Education (3rd edition). Elsevier,
Oxford, UK.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the 14th International Conference
on Machine Learning, pages 412?420.
269
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 816?821,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extra-Linguistic Constraints on Stance Recognition in Ideological Debates
Kazi Saidul Hasan and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{saidul,vince}@hlt.utdallas.edu
Abstract
Determining the stance expressed by an
author from a post written for a two-
sided debate in an online debate forum
is a relatively new problem. We seek to
improve Anand et al?s (2011) approach
to debate stance classification by model-
ing two types of soft extra-linguistic con-
straints on the stance labels of debate
posts, user-interaction constraints and ide-
ology constraints. Experimental results on
four datasets demonstrate the effectiveness
of these inter-post constraints in improv-
ing debate stance classification.
1 Introduction
While a lot of work on document-level opinion
mining has involved determining the polarity ex-
pressed in a customer review (e.g., whether a re-
view is ?thumbs up? or ?thumbs down?) (see Pang
and Lee (2008) and Liu (2012) for an overview
of the field), researchers have begun exploring
new opinion mining tasks in recent years. One
such task is debate stance classification: given
a post written for a two-sided topic discussed in
an online debate forum (e.g., ?Should abortion be
banned??), determine which of the two sides (i.e.,
for and against) its author is taking.
Debate stance classification is potentially more
interesting and challenging than polarity classifi-
cation for at least two reasons. First, while in po-
larity classification sentiment-bearing words and
phrases have proven to be useful (e.g., ?excellent?
correlates strongly with the positive polarity), in
debate stance classification it is not uncommon to
find debate posts where stances are not expressed
in terms of sentiment words, as exemplified in Fig-
ure 1, where the author is for abortion.
Second, while customer reviews are typically
written independently of other reviews in an on-
line forum, the same is not true for debate posts. In
The fetus is simply a part of the mother?s body and she
can have an abortion because it is her human rights. Also
I take this view because every woman can face with sit-
uation when two lives are at stake and the moral obli-
gation is to save the one closest at hand ? namely, that
of the mother, whose life is always more immediate than
that of the unborn child within her body. Permission for
an abortion could then be based on psychiatric consider-
ations such as prepartum depression, especially if there
is responsible psychiatric opinion that a continued preg-
nancy raises the strong probability of suicide in a clini-
cally depressed patient.
Figure 1: A sample post on abortion.
a debate forum, debate posts form threads, where
later posts often support or oppose the viewpoints
raised in earlier posts in the same thread.
Previous approaches to debate stance classifica-
tion have focused on three debate settings, namely
congressional floor debates (Thomas et al, 2006;
Bansal et al, 2008; Balahur et al, 2009; Yesse-
nalina et al, 2010; Burfoot et al, 2011), company-
internal discussions (Murakami and Raymond,
2010), and online social, political, and ideologi-
cal debates in public forums (Agrawal et al, 2003;
Somasundaran and Wiebe, 2010; Wang and Rose?,
2010; Biran and Rambow, 2011; Hasan and Ng,
2012). As Walker et al (2012) point out, debates
in public forums differ from congressional debates
and company-internal discussions in terms of lan-
guage use. Specifically, online debaters use color-
ful and emotional language to express their points,
which may involve sarcasm, insults, and question-
ing another debater?s assumptions and evidence.
These properties can potentially make stance clas-
sification of online debates more challenging than
that of the other two types of debates.
Our goal in this paper is to improve the state-
of-the-art supervised learning approach to debate
stance classification of online debates proposed by
Anand et al (2011), focusing in particular on ideo-
logical debates. Specifically, we hypothesize that
there are two types of soft extra-linguistic con-
straints on the stance labels of debate posts that,
816
Number ?for? % of posts Average thread
Domain of posts posts (%) in a thread length
ABO 1741 54.9 75.1 4.1
GAY 1376 63.4 74.5 4.0
OBA 985 53.9 57.1 2.6
MAR 626 69.5 58.0 2.5
Table 1: Statistics of the four datasets.
if explicitly modeled, could improve a learning-
based stance classification system. We refer to
these two types of inter-post constraints as user-
interaction constraints and ideology constraints.
We show how they can be learned from stance-
annotated debate posts in Sections 4.1 and 4.2, re-
spectively.
2 Datasets
For our experiments, we collect debate posts
from four popular domains, Abortion (ABO),
Gay Rights (GAY), Obama (OBA), and Marijuana
(MAR), from an online debate forum1. All de-
bates are two-sided, so each post receives one of
two domain labels, for or against, depending on
whether the author of the post supports or opposes
abortion, gay rights, Obama, or the legalization of
marijuana.
We construct one dataset for each domain (see
Table 1 for statistics). The fourth column of the
table shows the percentage of posts in each domain
that appear in a thread. More precisely, a thread
is a tree with one or more nodes such that (1) each
node corresponds to a debate post, and (2) a post yi
is the parent of another post yj if yj is a reply to yi.
Given a thread, we can generate post sequences,
each of which is a path from the root of the thread
to one of its leaves.
3 Baseline Systems
We employ as baselines two stance classification
systems, Anand et al?s (2011) approach and an en-
hanced version of it, as described below.
Our first baseline, Anand et al?s approach is a
supervised method that trains a stance classifier
for determining whether the stance expressed in
a debate post is for or against the topic. Hence,
we create one training instance from each post in
the training set, using the stance it expresses as
its class label. Following Anand et al, we repre-
sent a training instance using three types of lexico-
syntactic features, which are briefly summarized
in Table 2. In our implementation, we train the
1http://www.createdebate.com/
Feature type Features
Basic Unigrams, bigrams, syntactic and POS-
generalized dependencies
Sentiment LIWC counts, opinion dependencies
Argument Cue words, repeated punctuation, context
Table 2: Anand et al?s features.
stance classifier using SVMlight (Joachims, 1999).
After training, we can apply the classifier to clas-
sify the test instances, which are generated in the
same way as the training instances.
Related work on stance classification of con-
gressional debates has found that enforcing au-
thor constraints (ACs) can improve classification
performance (e.g., Thomas et al (2006), Bansal et
al. (2008), Burfoot et al (2011), Lu et al (2012),
Walker et al (2012)). ACs are a type of inter-
post constraints that specify that two posts written
by the same author for the same debate domain
should have the same stance. We hypothesize that
ACs could similarly be used to improve stance
classification of ideological debates, and therefore
propose a second baseline where we enhance the
first baseline with ACs. Enforcing ACs is simple.
We first use the learned stance classifier to classify
the test posts as in the first baseline, and then post-
process the labels of the test posts. Specifically,
we sum up the confidence values2 assigned to the
set of test posts written by the same author for the
same debate domain. If the sum is positive, then
we label all the posts in this set as for; otherwise
we label them as against.
4 Extra-Linguistic Constraints
In this section, we introduce two types of inter-
post constraints on debate stance classification.
4.1 User-Interaction Constraints
We call the first type of constraints user-
interaction constraints (UCs). UCs are motivated
by the observation that the stance labels of the
posts in a post sequence are not independent of
each other. Consider the post sequence in Fig-
ure 2, where each post is a response to the preced-
ing post. It shows an opening anti-abortion post
(P1), followed by a pro-abortion comment (P2),
which is in turn followed by another anti-abortion
view (P3). While this sequence contains alternat-
ing posts from opposing stances, in general there
is no hard constraint on the stance of a post given
2We use as the confidence value the signed distance of the
associated test point from the SVM hyperplane.
817
[P1: Anti-abortion] There are thousands of people who
want to take these children because they cannot have their
own. If you do not want a child, have it and put it up for
adoption. At least you will be preserving a human life rather
than killing one.
[P2: Pro-abortion] I agree that if people don?t want
their babies, they should have the choice of putting it
up for adoption. But it should not be made compulsory,
which is essentially what happens if you ban abortion.
[P3: Anti-abortion] Why should it not be made
compulsory? Those children have as much right to
live as you and I. Besides, no one loses with adop-
tion, so why wouldn?t you utilize it?
Figure 2: A sample post sequence. P2 and P3 are
replies to P1 and P2, respectively.
the preceding sequence of posts. Nevertheless, we
found that in our training data, a for (against) post
is followed by a against (for) post 80% of the time.
UCs aim to model the regularities in how users
interact with each other in a post sequence as soft
constraints. These kinds of soft constraints can be
naturally encoded as factors over adjacent posts in
a post sequence (see Kschischang et al (2001)),
which can in turn be learned by recasting stance
classification as a sequence labeling task. In our
experiments, we seek to derive the best sequence
of stance labels for each post sequence of length ?
1 using a Conditional Random Field (CRF) (Laf-
ferty et al, 2001).
We train the CRF model using the CRF im-
plementation in Mallet (McCallum, 2002). Each
training sequence corresponds to a post sequence.
Each post in a sequence is represented using the
same set of features as in the baselines.
After training, the resulting CRF model can be
used to assign a stance sequence to each test post
sequence. There is a caveat, however. Since a
given test post may appear in more than one se-
quence, different occurrences of it may be as-
signed different stance labels by the CRF. To deter-
mine the final stance label for the post, we average
the probabilities assigned to the for stance over all
its occurrences; if the average is ? 0.5, then its
final label is for; otherwise, its label is against.
4.2 Ideology Constraints
Next, we introduce our second type of inter-post
constraints, ideology constraints (ICs). ICs are
cross-domain, author-based constraints: they are
only applicable to debate posts written by the same
author in different domains. ICs model the fact
that for some authors, their stances on various is-
sues are determined in part by their ideological
values, and in particular, their stances on different
issues may be correlated. For example, someone
who opposes abortion is likely to be a conserva-
tive and has a good chance of opposing gay rights.
ICs aim to capture this kind of inter-domain corre-
lation of stances. Below we describe how we im-
plement ICs and show how they can be integrated
with ACs.
4.2.1 Implementing Ideology Constraints
We first compute a set of conditional probabil-
ities, P (stance(dq )=sd|stance(dp)=sc), where (1)
dp, dq ? Domains (i.e., the set of four domains),
(2) sc, sd ? {for, against}, and (3) dp 6= dq .
To compute P (stance(dq )=sd|stance(dp)=sc), we
(1) determine for each author a in the train-
ing set and each domain dp the stance of a
in dp (denoted by author-stance(dp ,a)), where
author-stance(dp ,a) is computed as the majority
stance labels associated with the debate posts
in the training set that a wrote for dp; and
(2) compute P (stance(dq )=sd|stance(dp)=sc) as
the ratio of
?
a?A Count(author-stance(dp ,a)=sc,
author-stance(dq ,a)=sd) to
?
a?A Count(author-
stance(dp,a)=sc), where A is the set of authors in
the training set who posted in both dp and dq. It
should be fairly easy to see that these conditional
probabilities measure the degree of correlation be-
tween the stances in different domains.
4.2.2 Inference Using ILP
Recall that in our second baseline, we employ
ACs to postprocess the output of the stance clas-
sifier simply by summing up the confidence val-
ues assigned to the posts written by the same au-
thor for the same debate domain. However, since
we now want to enforce two types of inter-post
constraints (namely, ACs and ICs), we will have
to employ a more sophisticated inference mecha-
nism. Previous work has focused on employing
graph minimum cut (MinCut) as the inference al-
gorithm. However, since MinCut suffers from the
weakness of not being able to enforce negative
constraints (i.e., two posts cannot receive the same
label) (Bansal et al, 2008), we propose to use in-
teger linear programming (ILP) as the underlying
inference mechanism. Below we show how to im-
plement ACs and ICs within the ILP framework.
Owing to space limitations, we refer the reader
to Roth and Yih (2004) for details of the ILP
framework. Briefly, ILP seeks to optimize an
objective function subject to a set of linear con-
818
straints. Below we focus on describing the ILP
program and how the ACs and ICs can be encoded.
Let Y = y1, . . . , yn be the set of debate posts.
For each yi, we create one (binary-valued) indi-
cator variable xi, which will be used in the ILP
program. Let pi = P (for|yi) be the ?benefit? of
setting xi to 1, where P (for|yi) is provided by the
CRF. Consequently, after optimization, yi?s stance
is for if its xi is set to 1. We optimize the following
objective function:
max
?
i
pixi + (1? pi)(1? xi)
subject to a set of linear constraints, which encode
the ACs and the ICs, as described below.
Implementing author constraints. If yi and yj
are composed by the same author, we ensure that
xi and xj will be assigned the same value by em-
ploying the linear constraint |xi ? xj| = 0.
Implementing ideology constraints. For con-
venience, below we use the notation introduced in
Section 4.2.1, and assume that yi and yj are two
arbitrary posts written by the same author in do-
mains dp and dq, respectively.
Case 1: If P (stance(dq )=for|stance(dp)=for) ? t,
we want to ensure that xi=1 =? xj=1.3 This can
be achieved using the constraint (1?xj) ? (1?xi).
Case 2: If P (stance(dq )=against|stance(dp )=against)
? t, we want to ensure that xi=0 =? xj=0. This
can be achieved using the constraint xj ? xi.
Case 3: If P (stance(dq )=against|stance(dp )=for)
? t, we want to ensure that xi=1 =? xj=0. This
can be achieved using the constraint xj ? (1?xi).
Case 4: If P (stance(dq )=for|stance(dp)=against)
? t, we want to ensure that xi=0 =? xj=1. This
can be achieved using the constraint (1?xj) ? xi.
Two points deserve mention. First, cases 3 and
4 correspond to negative constraints, and unlike in
MinCut, they can be implemented easily in ILP.
Second, if ICs are used, one ILP program will be
created to perform inference over the debate posts
in all four domains.
5 Evaluation
5.1 Experimental Setup
Results are expressed in terms of accuracy ob-
tained via 5-fold cross validation, where accuracy
3Intuitively, if this condition is satisfied, it means that
there is sufficient evidence that the two nodes from differ-
ent domains should have the same stance, and so we convert
the soft ICs into (hard) linear constraints in ILP. Note that t is
a threshold to be tuned using development data.
System ABO GAY OBA MAR
Anand 61.4 62.6 58.1 66.9
Anand+AC 72.0 64.9 62.7 67.8
Anand+AC+UC 73.7 69.9 64.1 75.4
Anand+AC+UC+IC 74.9 70.9 72.7 75.4
Table 3: 5-fold cross-validation accuracies.
is the percentage of test instances correctly classi-
fied. Since all experiments require the use of de-
velopment data for parameter tuning, we use three
folds for model training, one fold for development,
and one fold for testing in each fold experiment.
5.2 Results
Results are shown in Table 3. Row 1 shows the
results of the Anand et al (2011) baseline (see
Section 3) on the four datasets, obtained by train-
ing a SVM stance classifier using the SVMlight
software.4 Row 2 shows the results of the sec-
ond baseline, Anand et al?s system enhanced with
ACs. As we can see, incorporating ACs into
Anand et al?s system improves its performance
significantly on all datasets and yields a system
that achieves an average improvement of 4.6 ac-
curacy points.5
Next, we incorporate our first type of con-
straints, UCs, into the better of the two baselines
(i.e., the second baseline). Results of applying the
CRF for modeling UCs to the test posts and post-
processing them using the ACs are shown in row 3
of Table 3. As we can see, incorporating UCs into
the second baseline significantly improves its per-
formance and yields a system that achieves an av-
erage improvement of 3.93 accuracy points.
Finally, we incorporate our second type of con-
straints, ICs, effectively performing inference over
the CRF output using ILP with ACs and ICs as the
inter-post constraints. Results of this experiment
are shown in row 4 of Table 3. As we can see, in-
corporating the ICs significantly improves the per-
formance of the system on all but MAR and yields
a system that achieves an average improvement of
2.7 accuracy points.
Overall, our inter-post constraints yield a stance
classification system that significantly outper-
forms the better baseline on all four datasets, with
an average improvement of 6.63 accuracy points.
4For all SVM experiments, the regularization parameter C
is tuned using development data, but the remaining learning
parameters are set to their default values.
5All significance tests are paired t-tests, with p < 0.05.
819
5.3 Discussion
Next, we make some observations on the results of
applying ICs to our datasets.
First, ICs do not improve the MAR dataset. An
examination of the domains reveals the reason. We
find three pairs of ICs involving the other three do-
mains ? ABO, GAY, and OBA ? in our training
data. More specifically, the stances of the posts
written by an author for these three domains are
all positively co-related. In other words, if an au-
thor supports abortion, it is likely that she supports
both gay rights and Obama as well. On the other
hand, we find no co-relation between MAR and
the remaining domains. This means that no ICs
can be established between the posts in MAR and
those in the remaining domains.
Second, the improvement resulting from the ap-
plication of ICs is much larger on the OBA dataset
than on ABO and GAY. The reason can be at-
tributed to the fact that ICs exist more frequently
between OBA and ABO and between OBA and
GAY than between ABO and GAY. Specifically,
ICs are seen in all five folds of the data in the
first two pairs of domains, whereas they are seen
in only two folds in the last pair of domains.
6 Related Work
Previous work has investigated the use of extra-
linguistic constraints to improve stance classifica-
tion. Introduced by Thomas et al (2006), ACs are
arguably the most commonly used extra-linguistic
constraints. Since then, they have been employed
and extended in different ways (see, for example,
Bansal et al (2008), Burfoot et al (2011), Lu et al
(2012), and Walker et al (2012)).
ICs are different from ACs in at least two re-
spects. First, ICs are softer than ACs, so accu-
rate modeling of ICs has to be based on stance-
annotated data. Although we employ ICs as hard
constraints (owing in part to our use of the ILP
framework), they can be used directly as soft con-
straints in other frameworks, such as MinCut. Sec-
ond, ICs are inter-domain constraints, whereas
ACs are intra-domain constraints. To our knowl-
edge, this is the first time inter-domain constraints
are employed for stance classification.
There has been work related to the modeling of
user interaction in a post sequence. Recall that be-
tween two adjacent posts in a post sequence that
have opposing stances, there exists a rebuttal link.
Walker et al (2012) employ manually identified
rebuttal links as hard inter-post constraints dur-
ing inference. However, since automatic discov-
ery of rebuttal links is a non-trivial problem, em-
ploying gold rebuttal links substantially simplifies
the stance classification task. Lu et al (2012), on
the other hand, predict whether a link is of type
agreement or disagreement using a bootstrapped
classifier. Anand et al (2011) do not predict links.
Instead, hypothesizing that the content of the pre-
ceding post in a post sequence would be useful
for predicting the stance of the current post, they
employ features computed based on the preceding
post when training a stance classifier. Hence, un-
like us, they classify each post independently of
the others, whereas we classify the posts in a se-
quence in dependent relation to each other.
The ILP framework has been applied to perform
joint inference for a variety of stance prediction
tasks. Lu et al (2012) address the task of discov-
ering opposing opinion networks, where the goal
is to partition the authors in a debate (e.g., gay
rights) based on whether they support or oppose
the given issue. To this end, they employ ILP
to coordinate different sources of information. In
our previous work on debate stance classification
(Hasan and Ng, 2012), we employ ILP to coor-
dinate the output of two classifiers: a post-stance
classifier, which determines the stance of a debate
post written for a domain (e.g., gay rights); and
a topic-stance classifier, which determines the au-
thor?s stance on each topic mentioned in her post
(e.g., gay marriage, gay adoption). In this work,
on the other hand, we train only one classifier,
but use ILP to coordinate two types of constraints,
ACs and ICs.
7 Conclusions
We examined the under-studied task of stance
classification of ideological debates. Employing
our two types of extra-linguistic constraints yields
a system that outperforms an improved version of
Anand et al?s approach by 2.9?10 accuracy points.
While the effectiveness of ideology constraints de-
pends to some extent on the ?relatedness? of the
underlying ideological domains, we believe that
the gains they offer will increase with the num-
ber of authors posting in different domains and the
number of related domains.6
6Only a small fraction of the authors posted in multiple
domains in our datasets: 12% and 5% of them posted in two
and three domains, respectively.
820
References
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In Pro-
ceedings of the 12th International Conference on
World Wide Web, WWW ?03, pages 529?535.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis (WASSA 2011), pages 1?9.
Alexandra Balahur, Zornitsa Kozareva, and Andre?s
Montoyo. 2009. Determining the polarity and
source of opinions expressed in political debates. In
Proceedings of the 10th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, CICLing ?09, pages 468?480.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label
disagreement in the min-cut classification frame-
work. In Proceedings of the 22nd International
Conference on Computational Linguistics: Com-
panion volume: Posters, pages 15?18.
Or Biran and Owen Rambow. 2011. Identifying justi-
fications in written dialogs. In Proceedings of the
2011 IEEE Fifth International Conference on Se-
mantic Computing, ICSC ?11, pages 162?168.
Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional
floor-debate transcripts. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1506?1515.
Kazi Saidul Hasan and Vincent Ng. 2012. Predict-
ing stance in ideological debate with rich linguistic
knowledge. In Proceedings of the 24th International
Conference on Computational Linguistics: Posters,
pages 451?460.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods -
Support Vector Learning, pages 44?56. MIT Press.
Frank Kschischang, Brendan J. Frey, and Hans-Andrea
Loeliger. 2001. Factor graphs and the sum-product
algorithm. IEEE Transactions on Information The-
ory, 47:498?519.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, pages 282?
289.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publishers.
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan
Roth. 2012. Unsupervised discovery of opposing
opinion networks from forum discussions. In Pro-
ceedings of the 21st ACM International Conference
on Information and Knowledge Management, CIKM
?12, pages 1642?1646.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit. http://
mallet.cs.umass.edu.
Akiko Murakami and Rudy Raymond. 2010. Support
or oppose? Classifying positions in online debates
from reply activities and opinion expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of the Eighth Confer-
ence on Computational Natural Language Learning,
pages 1?8.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 116?124.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327?335.
Marilyn Walker, Pranav Anand, Rob Abbott, and Ricky
Grant. 2012. Stance classification using dialogic
properties of persuasion. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 592?596.
Yi-Chia Wang and Carolyn P. Rose?. 2010. Making
conversational structure explicit: Identification of
initiation-response pairs within online discussions.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 673?676.
Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010. Multi-level structured models for document-
level sentiment classification. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 1046?1056.
821
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262?1273,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Automatic Keyphrase Extraction: A Survey of the State of the Art
Kazi Saidul Hasan and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{saidul,vince}@hlt.utdallas.edu
Abstract
While automatic keyphrase extraction has
been examined extensively, state-of-the-
art performance on this task is still much
lower than that on many core natural lan-
guage processing tasks. We present a sur-
vey of the state of the art in automatic
keyphrase extraction, examining the major
sources of errors made by existing systems
and discussing the challenges ahead.
1 Introduction
Automatic keyphrase extraction concerns ?the au-
tomatic selection of important and topical phrases
from the body of a document? (Turney, 2000). In
other words, its goal is to extract a set of phrases
that are related to the main topics discussed in a
given document (Tomokiyo and Hurst, 2003; Liu
et al, 2009b; Ding et al, 2011; Zhao et al, 2011).
Document keyphrases have enabled fast and ac-
curate searching for a given document from a large
text collection, and have exhibited their potential
in improving many natural language processing
(NLP) and information retrieval (IR) tasks, such
as text summarization (Zhang et al, 2004), text
categorization (Hulth and Megyesi, 2006), opin-
ion mining (Berend, 2011), and document index-
ing (Gutwin et al, 1999).
Owing to its importance, automatic keyphrase
extraction has received a lot of attention. However,
the task is far from being solved: state-of-the-art
performance on keyphrase extraction is still much
lower than that on many core NLP tasks (Liu et al,
2010). Our goal in this paper is to survey the state
of the art in keyphrase extraction, examining the
major sources of errors made by existing systems
and discussing the challenges ahead.
2 Corpora
Automatic keyphrase extraction systems have
been evaluated on corpora from a variety of
sources ranging from long scientific publications
to short paper abstracts and email messages. Ta-
ble 1 presents a listing of the corpora grouped by
their sources as well as their statistics.
1
There are
at least four corpus-related factors that affect the
difficulty of keyphrase extraction.
Length The difficulty of the task increases with
the length of the input document as longer doc-
uments yield more candidate keyphrases (i.e.,
phrases that are eligible to be keyphrases (see Sec-
tion 3.1)). For instance, each Inspec abstract has
on average 10 annotator-assigned keyphrases and
34 candidate keyphrases. In contrast, a scientific
paper typically has at least 10 keyphrases and hun-
dreds of candidate keyphrases, yielding a much
bigger search space (Hasan and Ng, 2010). Conse-
quently, it is harder to extract keyphrases from sci-
entific papers, technical reports, and meeting tran-
scripts than abstracts, emails, and news articles.
Structural consistency In a structured doc-
ument, there are certain locations where a
keyphrase is most likely to appear. For instance,
most of a scientific paper?s keyphrases should ap-
pear in the abstract and the introduction. While
structural information has been exploited to ex-
tract keyphrases from scientific papers (e.g., title,
section information) (Kim et al, 2013), web pages
(e.g., metadata) (Yih et al, 2006), and chats (e.g.,
dialogue acts) (Kim and Baldwin, 2012), it is most
useful when the documents from a source exhibit
structural similarity. For this reason, structural in-
formation is likely to facilitate keyphrase extrac-
tion from scientific papers and technical reports
because of their standard format (i.e., standard
sections such as abstract, introduction, conclusion,
etc.). In contrast, the lack of structural consistency
in other types of structured documents (e.g., web
pages, which can be blogs, forums, or reviews)
1
Many of the publicly available corpora can be found
in http://github.com/snkim/AutomaticKeyphraseExtraction/
and http://code.google.com/p/maui-indexer/downloads/list.
1262
Source Dataset/Contributor
Statistics
Documents Tokens/doc Keys/doc
Paper abstracts Inspec (Hulth, 2003)? 2,000 <200 10
Scientific papers
NUS corpus (Nguyen and Kan, 2007)? 211 ?8K 11
citeulike.org (Medelyan et al, 2009)? 180 - 5
SemEval-2010 (Kim et al, 2010b)? 284 >5K 15
Technical reports NZDL (Witten et al, 1999)? 1,800 - -
News articles
DUC-2001 (Wan and Xiao, 2008b)? 308 ?900 8
Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6
Web pages Yih et al (2006) 828 - -
Hammouda et al (2005)? 312 ?500 -
Blogs (Grineva et al, 2009) 252 ?1K 8
Meeting transcripts ICSI (Liu et al, 2009a) 161 ?1.6K 4
Emails Enron corpus (Dredze et al, 2008)? 14,659 - -
Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10
Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (?).
may render structural information less useful.
Topic change An observation commonly ex-
ploited in keyphrase extraction from scientific ar-
ticles and news articles is that keyphrases typically
appear not only at the beginning (Witten et al,
1999) but also at the end (Medelyan et al, 2009)
of a document. This observation does not neces-
sarily hold for conversational text (e.g., meetings,
chats), however. The reason is simple: in a conver-
sation, the topics (i.e., its talking points) change as
the interaction moves forward in time, and so do
the keyphrases associated with a topic. One way
to address this complication is to detect a topic
change in conversational text (Kim and Baldwin,
2012). However, topic change detection is not al-
ways easy: while the topics listed in the form of an
agenda at the beginning of formal meeting tran-
scripts can be exploited, such clues are absent in
casual conversations (e.g., chats).
Topic correlation Another observation com-
monly exploited in keyphrase extraction from
scientific articles and news articles is that the
keyphrases in a document are typically related to
each other (Turney, 2003; Mihalcea and Tarau,
2004). However, this observation does not nec-
essarily hold for informal text (e.g., emails, chats,
informal meetings, personal blogs), where people
can talk about any number of potentially uncorre-
lated topics. The presence of uncorrelated topics
implies that it may no longer be possible to exploit
relatedness and therefore increases the difficulty of
keyphrase extraction.
3 Keyphrase Extraction Approaches
A keyphrase extraction system typically operates
in two steps: (1) extracting a list of words/phrases
that serve as candidate keyphrases using some
heuristics (Section 3.1); and (2) determining
which of these candidate keyphrases are correct
keyphrases using supervised (Section 3.2) or un-
supervised (Section 3.3) approaches.
3.1 Selecting Candidate Words and Phrases
As noted before, a set of phrases and words is
typically extracted as candidate keyphrases using
heuristic rules. These rules are designed to avoid
spurious instances and keep the number of candi-
dates to a minimum. Typical heuristics include (1)
using a stop word list to remove stop words (Liu et
al., 2009b), (2) allowing words with certain part-
of-speech tags (e.g., nouns, adjectives, verbs) to be
candidate keywords (Mihalcea and Tarau, 2004;
Wan and Xiao, 2008b; Liu et al, 2009a), (3) al-
lowing n-grams that appear in Wikipedia article
titles to be candidates (Grineva et al, 2009), and
(4) extracting n-grams (Witten et al, 1999; Hulth,
2003; Medelyan et al, 2009) or noun phrases
(Barker and Cornacchia, 2000; Wu et al, 2005)
that satisfy pre-defined lexico-syntactic pattern(s)
(Nguyen and Phan, 2009).
Many of these heuristics have proven effective
with their high recall in extracting gold keyphrases
from various sources. However, for a long docu-
ment, the resulting list of candidates can be long.
Consequently, different pruning heuristics have
been designed to prune candidates that are un-
likely to be keyphrases (Huang et al, 2006; Kumar
and Srinathan, 2008; El-Beltagy and Rafea, 2009;
You et al, 2009; Newman et al, 2012).
3.2 Supervised Approaches
Research on supervised approaches to keyphrase
extraction has focused on two issues: task refor-
mulation and feature design.
1263
3.2.1 Task Reformulation
Early supervised approaches to keyphrase extrac-
tion recast this task as a binary classification prob-
lem (Frank et al, 1999; Turney, 1999; Witten et
al., 1999; Turney, 2000). The goal is to train a
classifier on documents annotated with keyphrases
to determine whether a candidate phrase is a
keyphrase. Keyphrases and non-keyphrases are
used to generate positive and negative examples,
respectively. Different learning algorithms have
been used to train this classifier, including na??ve
Bayes (Frank et al, 1999; Witten et al, 1999),
decision trees (Turney, 1999; Turney, 2000), bag-
ging (Hulth, 2003), boosting (Hulth et al, 2001),
maximum entropy (Yih et al, 2006; Kim and Kan,
2009), multi-layer perceptron (Lopez and Romary,
2010), and support vector machines (Jiang et al,
2009; Lopez and Romary, 2010).
Recasting keyphrase extraction as a classifica-
tion problem has its weaknesses, however. Recall
that the goal of keyphrase extraction is to identify
the most representative phrases for a document.
In other words, if a candidate phrase c
1
is more
representative than another candidate phrase c
2
, c
1
should be preferred to c
2
. Note that a binary clas-
sifier classifies each candidate keyphrase indepen-
dently of the others, and consequently it does not
allow us to determine which candidates are better
than the others (Hulth, 2004; Wang and Li, 2011).
Motivated by this observation, Jiang et al
(2009) propose a ranking approach to keyphrase
extraction, where the goal is to learn a ranker
to rank two candidate keyphrases. This pairwise
ranking approach therefore introduces competi-
tion between candidate keyphrases, and has been
shown to significantly outperform KEA (Witten
et al, 1999; Frank et al, 1999), a popular su-
pervised baseline that adopts the traditional super-
vised classification approach (Song et al, 2003;
Kelleher and Luz, 2005).
3.2.2 Features
The features commonly used to represent an in-
stance for supervised keyphrase extraction can be
broadly divided into two categories.
3.2.2.1 Within-Collection Features
Within-collection features are computed based
solely on the training documents. These features
can be further divided into three types.
Statistical features are computed based on sta-
tistical information gathered from the training
documents. Three such features have been exten-
sively used in supervised approaches. The first
one, tf*idf (Salton and Buckley, 1988), is com-
puted based on candidate frequency in the given
text and inverse document frequency (i.e., number
of other documents where the candidate appears).
2
The second one, the distance of a phrase, is de-
fined as the number of words preceding its first
occurrence normalized by the number of words in
the document. Its usefulness stems from the fact
that keyphrases tend to appear early in a docu-
ment. The third one, supervised keyphraseness,
encodes the number of times a phrase appears as
a keyphrase in the training set. This feature is de-
signed based on the assumption that a phrase fre-
quently tagged as a keyphrase is more likely to be
a keyphrase in an unseen document. These three
features form the feature set of KEA (Witten et al,
1999; Frank et al, 1999), and have been shown to
perform consistently well on documents from var-
ious sources (Yih et al, 2006; Kim et al, 2013).
Other statistical features include phrase length and
spread (i.e., the number of words between the first
and last occurrences of a phrase in the document).
Structural features encode how different in-
stances of a candidate keyphrase are located in
different parts of a document. A phrase is more
likely to be a keyphrase if it appears in the ab-
stract or introduction of a paper or in the metadata
section of a web page. In fact, features that en-
code how frequently a candidate keyphrase occurs
in various sections of a scientific paper (e.g., in-
troduction, conclusion) (Nguyen and Kan, 2007)
and those that encode the location of a candidate
keyphrase in a web page (e.g., whether it appears
in the title) (Chen et al, 2005; Yih et al, 2006)
have been shown to be useful for the task.
Syntactic features encode the syntactic pat-
terns of a candidate keyphrase. For example, a
candidate keyphrase has been encoded as (1) a
PoS tag sequence, which denotes the sequence of
part-of-speech tag(s) assigned to its word(s); and
(2) a suffix sequence, which is the sequence of
morphological suffixes of its words (Yih et al,
2006; Nguyen and Kan, 2007; Kim and Kan,
2009). However, ablation studies conducted on
web pages (Yih et al, 2006) and scientific articles
2
A tf*idf-based baseline, where candidate keyphrases are
ranked and selected according to tf*idf, has been widely used
by both supervised and unsupervised approaches (Zhang et
al., 2005; Dredze et al, 2008; Paukkeri et al, 2008; Grineva
et al, 2009).
1264
(Kim and Kan, 2009) reveal that syntactic features
are not useful for keyphrase extraction in the pres-
ence of other feature types.
3.2.2.2 External Resource-Based Features
External resource-based features are computed
based on information gathered from resources
other than the training documents, such as lex-
ical knowledge bases (e.g., Wikipedia) or the
Web, with the goal of improving keyphrase extrac-
tion performance by exploiting external knowl-
edge. Below we give an overview of the exter-
nal resource-based features that have proven use-
ful for keyphrase extraction.
Wikipedia-based keyphraseness is computed as
a candidate?s document frequency multiplied by
the ratio of the number of Wikipedia articles where
the candidate appears as a link to the number of
articles where it appears (Medelyan et al, 2009).
This feature is motivated by the observation that
a candidate is likely to be a keyphrase if it occurs
frequently as a link in Wikipedia. Unlike super-
vised keyphraseness, Wikipedia-based keyphrase-
ness can be computed without using documents
annotated with keyphrases and can work even if
there is a mismatch between the training domain
and the test domain.
Yih et al (2006) employ a feature that en-
codes whether a candidate keyphrase appears in
the query log of a search engine, exploiting the ob-
servation that a candidate is potentially important
if it was used as a search query. Terminological
databases have been similarly exploited to encode
the salience of candidate keyphrases in scientific
papers (Lopez and Romary, 2010).
While the aforementioned external resource-
based features attempt to encode how salient a
candidate keyphrase is, Turney (2003) proposes
features that encode the semantic relatedness be-
tween two candidate keyphrases. Noting that can-
didate keyphrases that are not semantically re-
lated to the predicted keyphrases are unlikely to
be keyphrases in technical reports, Turney em-
ploys coherence features to identify such can-
didate keyphrases. Semantic relatedness is en-
coded in the coherence features as two candidate
keyphrases? pointwise mutual information, which
Turney computes by using the Web as a corpus.
3.3 Unsupervised Approaches
Existing unsupervised approaches to keyphrase
extraction can be categorized into four groups.
3.3.1 Graph-Based Ranking
Intuitively, keyphrase extraction is about finding
the important words and phrases from a docu-
ment. Traditionally, the importance of a candi-
date has often been defined in terms of how related
it is to other candidates in the document. Infor-
mally, a candidate is important if it is related to (1)
a large number of candidates and (2) candidates
that are important. Researchers have computed re-
latedness between candidates using co-occurrence
counts (Mihalcea and Tarau, 2004; Matsuo and
Ishizuka, 2004) and semantic relatedness (Grineva
et al, 2009), and represented the relatedness in-
formation collected from a document as a graph
(Mihalcea and Tarau, 2004; Wan and Xiao, 2008a;
Wan and Xiao, 2008b; Bougouin et al, 2013).
The basic idea behind a graph-based approach
is to build a graph from the input document and
rank its nodes according to their importance us-
ing a graph-based ranking method (e.g., Brin and
Page (1998)). Each node of the graph corresponds
to a candidate keyphrase from the document and
an edge connects two related candidates. The
edge weight is proportional to the syntactic and/or
semantic relevance between the connected candi-
dates. For each node, each of its edges is treated
as a ?vote? from the other node connected by the
edge. A node?s score in the graph is defined recur-
sively in terms of the edges it has and the scores of
the neighboring nodes. The top-ranked candidates
from the graph are then selected as keyphrases for
the input document. TextRank (Mihalcea and Ta-
rau, 2004) is one of the most well-known graph-
based approaches to keyphrase extraction.
This instantiation of a graph-based approach
overlooks an important aspect of keyphrase ex-
traction, however. A set of keyphrases for a doc-
ument should ideally cover the main topics dis-
cussed in it, but this instantiation does not guaran-
tee that all the main topics will be represented by
the extracted keyphrases. Despite this weakness, a
graph-based representation of text was adopted by
many approaches that propose different ways of
computing the similarity between two candidates.
3.3.2 Topic-Based Clustering
Another unsupervised approach to keyphrase
extraction involves grouping the candidate
keyphrases in a document into topics, such that
each topic is composed of all and only those
candidate keyphrases that are related to that topic
(Grineva et al, 2009; Liu et al, 2009b; Liu et
1265
al., 2010). There are several motivations behind
this topic-based clustering approach. First, a
keyphrase should ideally be relevant to one or
more main topic(s) discussed in a document
(Liu et al, 2010; Liu et al, 2012). Second, the
extracted keyphrases should be comprehensive
in the sense that they should cover all the main
topics in a document (Liu et al, 2009b; Liu et al,
2010; Liu et al, 2012). Below we examine three
representative systems that adopt this approach.
KeyCluster Liu et al (2009b) adopt a
clustering-based approach (henceforth KeyClus-
ter) that clusters semantically similar candidates
using Wikipedia and co-occurrence-based statis-
tics. The underlying hypothesis is that each of
these clusters corresponds to a topic covered in
the document, and selecting the candidates close
to the centroid of each cluster as keyphrases
ensures that the resulting set of keyphrases covers
all the topics of the document.
While empirical results show that KeyCluster
performs better than both TextRank and Hulth?s
(2003) supervised system, KeyCluster has a poten-
tial drawback: by extracting keyphrases from each
topic cluster, it essentially gives each topic equal
importance. In practice, however, there could
be topics that are not important and these topics
should not have keyphrase(s) representing them.
Topical PageRank (TPR) Liu et al (2010) pro-
pose TPR, an approach that overcomes the afore-
mentioned weakness of KeyCluster. It runs Tex-
tRank multiple times for a document, once for
each of its topics induced by a Latent Dirichlet Al-
location (Blei et al, 2003). By running TextRank
once for each topic, TPR ensures that the extracted
keyphrases cover the main topics of the document.
The final score of a candidate is computed as the
sum of its scores for each of the topics, weighted
by the probability of that topic in that document.
Hence, unlike KeyCluster, candidates belonging to
a less probable topic are given less importance.
TPR performs significantly better than both
tf*idf and TextRank on the DUC-2001 and Inspec
datasets. TPR?s superior performance strength-
ens the hypothesis of using topic clustering for
keyphrase extraction. However, though TPR is
conceptually better than KeyCluster, Liu et al did
not compare TPR against KeyCluster.
CommunityCluster Grineva et al (2009) pro-
pose CommunityCluster, a variant of the topic
clustering approach to keyphrase extraction. Like
TPR, CommunityCluster gives more weight to
more important topics, but unlike TPR, it extracts
all candidate keyphrases from an important topic,
assuming that a candidate that receives little focus
in the text should still be extracted as a keyphrase
as long as it is related to an important topic. Com-
munityCluster yields much better recall (without
losing precision) than extractors such as tf*idf,
TextRank, and the Yahoo! term extractor.
3.3.3 Simultaneous Learning
Since keyphrases represent a dense summary of a
document, researchers hypothesized that text sum-
marization and keyphrase extraction can poten-
tially benefit from each other if these tasks are per-
formed simultaneously. Zha (2002) proposes the
first graph-based approach for simultaneous sum-
marization and keyphrase extraction, motivated by
a key observation: a sentence is important if it con-
tains important words, and important words ap-
pear in important sentences. Wan et al (2007) ex-
tend Zha?s work by adding two assumptions: (1)
an important sentence is connected to other im-
portant sentences, and (2) an important word is
linked to other important words, a TextRank-like
assumption. Based on these assumptions, Wan et
al. (2007) build three graphs to capture the asso-
ciation between the sentences (S) and the words
(W) in an input document, namely, a S?S graph,
a bipartite S?W graph, and a W?W graph. The
weight of an edge connecting two sentence nodes
in a S?S graph corresponds to their content simi-
larity. An edge weight in a S?W graph denotes the
word?s importance in the sentence it appears. Fi-
nally, an edge weight in a W?W graph denotes the
co-occurrence or knowledge-based similarity be-
tween the two connected words. Once the graphs
are constructed for an input document, an itera-
tive reinforcement algorithm is applied to assign
scores to each sentence and word. The top-scored
words are used to form keyphrases.
The main advantage of this approach is that it
combines the strengths of both Zha?s approach
(i.e., bipartite S?W graphs) and TextRank (i.e., W?
W graphs) and performs better than both of them.
However, it has a weakness: like TextRank, it does
not ensure that the extracted keyphrases will cover
all the main topics. To address this problem, one
can employ a topic clustering algorithm on the W?
W graph to produce the topic clusters, and then en-
sure that keyphrases are chosen from every main
topic cluster.
1266
3.3.4 Language Modeling
Many existing approaches have a separate, heuris-
tic module for extracting candidate keyphrases
prior to keyphrase ranking/extraction. In contrast,
Tomokiyo and Hurst (2003) propose an approach
(henceforth LMA) that combines these two steps.
LMA scores a candidate keyphrase based on
two features, namely, phraseness (i.e., the ex-
tent to which a word sequence can be treated as
a phrase) and informativeness (i.e., the extent to
which a word sequence captures the central idea of
the document it appears in). Intuitively, a phrase
that has high scores for phraseness and informa-
tiveness is likely to be a keyphrase. These feature
values are estimated using language models (LMs)
trained on a foreground corpus and a background
corpus. The foreground corpus is composed of
the set of documents from which keyphrases are
to be extracted. The background corpus is a large
corpus that encodes general knowledge about the
world (e.g., the Web). A unigram LM and an n-
gram LM are constructed for each of these two
corpora. Phraseness, defined using the foreground
LM, is calculated as the loss of information in-
curred as a result of assuming a unigram LM (i.e.,
conditional independence among the words of the
phrase) instead of an n-gram LM (i.e., the phrase
is drawn from an n-gram LM). Informativeness is
computed as the loss that results because of the
assumption that the candidate is sampled from the
background LM rather than the foreground LM.
The loss values are computed using Kullback-
Leibler divergence. Candidates are ranked accord-
ing to the sum of these two feature values.
In sum, LMA uses a language model rather than
heuristics to identify phrases, and relies on the lan-
guage model trained on the background corpus to
determine how ?unique? a candidate keyphrase is
to the domain represented by the foreground cor-
pus. The more unique it is to the foreground?s do-
main, the more likely it is a keyphrase for that do-
main. While the use of language models to iden-
tify phrases cannot be considered a major strength
of this approach (because heuristics can identify
phrases fairly reliably), the use of a background
corpus to identify candidates that are unique to the
foreground?s domain is a unique aspect of this ap-
proach. We believe that this idea deserves further
investigation, as it would allow us to discover a
keyphrase that is unique to the foreground?s do-
main but may have a low tf*idf value.
4 Evaluation
In this section, we describe metrics for evaluating
keyphrase extraction systems as well as state-of-
the-art results on commonly-used datasets.
4.1 Evaluation Metrics
Designing evaluation metrics for keyphrase ex-
traction is by no means an easy task. To score
the output of a keyphrase extraction system, the
typical approach, which is also adopted by the
SemEval-2010 shared task on keyphrase extrac-
tion, is (1) to create a mapping between the
keyphrases in the gold standard and those in the
system output using exact match, and then (2)
score the output using evaluation metrics such as
precision (P), recall (R), and F-score (F).
Conceivably, exact match is an overly strict con-
dition, considering a predicted keyphrase incor-
rect even if it is a variant of a gold keyphrase.
For instance, given the gold keyphrase ?neural
network?, exact match will consider a predicted
phrase incorrect even if it is an expanded version
of the gold keyphrase (?artificial neural network?)
or one of its morphological (?neural networks?) or
lexical (?neural net?) variants. While morphologi-
cal variations can be handled using a stemmer (El-
Beltagy and Rafea, 2009), other variations may
not be handled easily and reliably.
Human evaluation has been suggested as a pos-
sibility (Matsuo and Ishizuka, 2004), but it is time-
consuming and expensive. For this reason, re-
searchers have experimented with two types of
automatic evaluation metrics. The first type of
metrics addresses the problem with exact match.
These metrics reward a partial match between a
predicted keyphrase and a gold keyphrase (i.e.,
overlapping n-grams) and are commonly used
in machine translation (MT) and summarization
evaluations. They include BLEU, METEOR, NIST,
and ROUGE. Nevertheless, experiments show that
these MT metrics only offer a partial solution to
problem with exact match: they can only detect a
subset of the near-misses (Kim et al, 2010a).
The second type of metrics focuses on how a
system ranks its predictions. Given that two sys-
tems A and B have the same number of correct
predictions, binary preference measure (Bpref)
and mean reciprocal rank (MRR) (Liu et al, 2010)
will award more credit to A than to B if the ranks
of the correct predictions in A?s output are higher
than those in B?s output. R-precision (R
p
) is an
1267
IR metric that focuses on ranking: given a docu-
ment with n gold keyphrases, it computes the pre-
cision of a system over its n highest-ranked can-
didates (Zesch and Gurevych, 2009). The motiva-
tion behind the design of R
p
is simple: a system
will achieve a perfect R
p
value if it ranks all the
keyphrases above the non-keyphrases.
4.2 The State of the Art
Table 2 lists the best scores on some popular evalu-
ation datasets and the corresponding systems. For
example, the best F-scores on the Inspec test set,
the DUC-2001 dataset, and the SemEval-2010 test
set are 45.7, 31.7, and 27.5, respectively.
3
Two points deserve mention. First, F-scores de-
crease as document length increases. These re-
sults are consistent with the observation we made
in Section 2 that it is more difficult to extract
keyphrases correctly from longer documents. Sec-
ond, recent unsupervised approaches have rivaled
their supervised counterparts in performance (Mi-
halcea and Tarau, 2004; El-Beltagy and Rafea,
2009; Liu et al, 2009b). For example, KP-Miner
(El-Beltagy and Rafea, 2010), an unsupervised
system, ranked third in the SemEval-2010 shared
task with an F-score of 25.2, which is comparable
to the best supervised system scoring 27.5.
5 Analysis
With the goal of providing directions for future
work, we identify the errors commonly made by
state-of-the-art keyphrase extractors below.
5.1 Error Analysis
Although a few researchers have presented a sam-
ple of their systems? output and the corresponding
gold keyphrases to show the differences between
them (Witten et al, 1999; Nguyen and Kan, 2007;
Medelyan et al, 2009), a systematic analysis of
the major types of errors made by state-of-the-art
keyphrase extraction systems is missing.
To fill this gap, we ran four keyphrase extrac-
tion systems on four commonly-used datasets of
varying sources, including Inspec abstracts (Hulth,
2003), DUC-2001 news articles (Over, 2001), sci-
entific papers (Kim et al, 2010b), and meeting
transcripts (Liu et al, 2009a). Specifically, we ran-
domly selected 25 documents from each of these
3
A more detailed analysis of the results of the SemEval-
2010 shared task and the approaches adopted by the partici-
pating systems can be found in Kim et al (2013).
Dataset
Approach and System
[Supervised?]
Score
P R F
Abstracts
(Inspec)
Topic clustering
(Liu et al, 2009b) [?]
35.0 66.0 45.7
Blogs
Topic community detection
(Grineva et al, 2009) [?]
35.1 61.5 44.7
News
(DUC
-2001)
Graph-based ranking
for extended neighborhood
(Wan and Xiao, 2008b) [?]
28.8 35.4 31.7
Papers
(SemEval
-2010)
Statistical, semantic, and
distributional features
(Lopez and Romary, 2010) [X]
27.2 27.8 27.5
Table 2: Best scores achieved on various datasets.
four datasets and manually analyzed the output of
the four systems, including tf*idf, the most fre-
quently used baseline, as well as three state-of-the-
art keyphrase extractors, of which two are unsu-
pervised (Wan and Xiao, 2008b; Liu et al, 2009b)
and one is supervised (Medelyan et al, 2009).
Our analysis reveals that the errors fall into four
major types, each of which contributes signifi-
cantly to the overall errors made by the four sys-
tems, despite the fact that the contribution of each
of these error types varies from system to system.
Moreover, we do not observe any significant dif-
ference between the types of errors made by the
four systems other than the fact that the super-
vised system has the expected tendency to predict
keyphrases seen in the training data. Below we
describe these four major types of errors.
Overgeneration errors are a major type of pre-
cision error, contributing to 28?37% of the overall
error. Overgeneration errors occur when a system
correctly predicts a candidate as a keyphrase be-
cause it contains a word that appears frequently in
the associated document, but at the same time er-
roneously outputs other candidates as keyphrases
because they contain the same word. Recall that
for many systems, it is not easy to reject a non-
keyphrase containing a word with a high term fre-
quency: many unsupervised systems score a can-
didate by summing the score of each of its compo-
nent words, and many supervised systems use un-
igrams as features to represent a candidate. To be
more concrete, consider the news article on athlete
Ben Johnson in Figure 1, where the keyphrases are
boldfaced. As we can see, the word Olympic(s)
has a significant presence in the document. Con-
sequently, many systems not only correctly predict
Olympics as a keyphrase, but also erroneously pre-
dict Olympic movement as a keyphrase, yielding
overgeneration errors.
Infrequency errors are a major type of re-
1268
Canadian Ben Johnson left the Olympics today ?in a
complete state of shock,? accused of cheating with drugs
in the world?s fastest 100-meter dash and stripped of
his gold medal. The prize went to American Carl
Lewis. Many athletes accepted the accusation that John-
son used a muscle-building but dangerous and illegal an-
abolic steroid called stanozolol as confirmation of what
they said they know has been going on in track and field.
Two tests of Johnson?s urine sample proved positive and
his denials of drug use were rejected today. ?This is
a blow for the Olympic Games and the Olympic move-
ment,? said International Olympic Committee President
Juan Antonio Samaranch.
Figure 1: A news article on Ben Johnson from the
DUC-2001 dataset. The keyphrases are boldfaced.
call error contributing to 24?27% of the overall
error. Infrequency errors occur when a system
fails to identify a keyphrase owing to its infre-
quent presence in the associated document (Liu
et al, 2011). Handling infrequency errors is a
challenge because state-of-the-art keyphrase ex-
tractors rarely predict candidates that appear only
once or twice in a document. In the Ben Johnson
example, many keyphrase extractors fail to iden-
tify 100-meter dash and gold medal as keyphrases,
resulting in infrequency errors.
Redundancy errors are a type of precision er-
ror contributing to 8?12% of the overall error. Re-
dundancy errors occur when a system correctly
identifies a candidate as a keyphrase, but at the
same time outputs a semantically equivalent can-
didate (e.g., its alias) as a keyphrase. This type
of error can be attributed to a system?s failure
to determine that two candidates are semantically
equivalent. Nevertheless, some researchers may
argue that a system should not be penalized for re-
dundancy errors because the extracted candidates
are in fact keyphrases. In our example, Olympics
and Olympic games refer to the same concept, so
a system that predicts both of them as keyphrases
commits a redundancy error.
Evaluation errors are a type of recall error con-
tributing to 7?10% of the overall error. An evalu-
ation error occurs when a system outputs a can-
didate that is semantically equivalent to a gold
keyphrase, but is considered erroneous by a scor-
ing program because of its failure to recognize
that the predicted phrase and the corresponding
gold keyphrase are semantically equivalent. In
other words, an evaluation error is not an error
made by a keyphrase extractor, but an error due
to the naivety of a scoring program. In our exam-
ple, while Olympics and Olympic games refer to
the same concept, only the former is annotated as
keyphrase. Hence, an evaluation error occurs if a
system predicts Olympic games but not Olympics
as a keyphrase and the scoring program fails to
identify them as semantically equivalent.
5.2 Recommendations
We recommend that background knowledge be
extracted from external lexical databases (e.g.,
YAGO2 (Suchanek et al, 2007), Freebase (Bol-
lacker et al, 2008), BabelNet (Navigli and
Ponzetto, 2012)) to address the four types of er-
rors discussed above.
First, we discuss how redundancy errors could
be addressed by using the background knowledge
extracted from external databases. Note that if we
can identify semantically equivalent candidates,
then we can reduce redundancy errors. The ques-
tion, then, is: can background knowledge be used
to help us identify semantically equivalent candi-
dates? To answer this question, note that Freebase,
for instance, has over 40 million topics (i.e., real-
world entities such as people, places, and things)
from over 70 domains (e.g., music, business, ed-
ucation). Hence, before a system outputs a set of
candidates as keyphrases, it can use Freebase to
determine whether any of them is mapped to the
same Freebase topic. Referring back to our run-
ning example, both Olympics and Olympic games
are mapped to a Freebase topic called Olympic
games. Based on this information, a keyphrase ex-
tractor can determine that the two candidates are
aliases and should output only one of them, thus
preventing a redundancy error.
Next, we discuss how infrequency errors
could be addressed using background knowledge.
A natural way to handle this problem would be
to make an infrequent keyphrase frequent. To ac-
complish this, we suggest exploiting an influen-
tial idea in the keyphrase extraction literature: the
importance of a candidate is defined in terms of
how related it is to other candidates in the text (see
Section 3.3.1). In other words, if we could relate
an infrequent keyphrase to other candidates in the
text, we could boost its importance.
We believe that this could be accomplished us-
ing background knowledge. The idea is to boost
the importance of infrequent keyphrases using
their frequent counterparts. Consider again our
running example. All four systems have managed
to identify Ben Johnson as a keyphrase due to its
1269
significant presence. Hence, we can boost the im-
portance of 100-meter dash and gold medal if we
can relate them to Ben Johnson.
To do so, note that Freebase maps a candi-
date to one or more pre-defined topics, each of
which is associated with one or more types. Types
are similar to entity classes. For instance, the
candidate Ben Johnson is mapped to a Freebase
topic with the same name, which is associated
with Freebase types such as Person, Athlete, and
Olympic athlete. Types are defined for a specific
domain in Freebase. For instance, Person, Ath-
lete, and Olympic athlete are defined in the People,
Sports, and Olympics domains, respectively. Next,
consider the two infrequent candidates, 100-meter
dash and gold medal. 100-meter dash is mapped
to the topic Sprint of type Sports in the Sports do-
main, whereas gold medal is mapped to a topic
with the same name of type Olympic medal in the
Olympics domain. Consequently, we can relate
100-meter dash to Ben Johnson via the Sports do-
main (i.e., they belong to different types under the
same domain). Additionally, gold medal can be
related to Ben Johnson via the Olympics domain.
As discussed before, the relationship between
two candidates is traditionally established using
co-occurrence information. However, using co-
occurrence windows has its shortcomings. First,
an ad-hoc window size cannot capture related can-
didates that are not inside the window. So it
is difficult to predict 100-meter dash and gold
medal as keyphrases: they are more than 10 tokens
away from frequent words such as Johnson and
Olympics. Second, the candidates inside a window
are all assumed to be related to each other, but it is
apparently an overly simplistic assumption. There
have been a few attempts to design Wikipedia-
based relatedness measures, with promising ini-
tial results (Grineva et al, 2009; Liu et al, 2009b;
Medelyan et al, 2009).
4
Overgeneration errors could similarly be ad-
dressed using background knowledge. Recall that
Olympic movement is not a keyphrase in our ex-
ample although it includes an important word (i.e.,
Olympic). Freebase maps Olympic movement to
a topic with the same name, which is associated
with a type called Musical Recording in the Mu-
sic domain. However, it does not map Olympic
4
Note that it may be difficult to employ our recommen-
dations to address infrequency errors in informal text with
uncorrelated topics because the keyphrases it contains may
not be related to each other (see Section 2).
movement to any topic in the Olympics domain.
The absence of such a mapping in the Olympics
domain could be used by a keyphrase extractor as
a supporting evidence against predicting Olympic
movement as a keyphrase.
Finally, as mentioned before, evaluation errors
should not be considered errors made by a sys-
tem. Nevertheless, they reveal a problem with the
way keyphrase extractors are currently evaluated.
To address this problem, one possibility is to con-
duct human evaluations. Cheaper alternatives in-
clude having human annotators identify semanti-
cally equivalent keyphrases during manual label-
ing, and designing scoring programs that can au-
tomatically identify such semantic equivalences.
6 Conclusion and Future Directions
We have presented a survey of the state of the art
in automatic keyphrase extraction. While unsu-
pervised approaches have started to rival their su-
pervised counterparts in performance, the task is
far from being solved, as reflected by the fairly
poor state-of-the-art results on various commonly-
used evaluation datasets. Our analysis revealed
that there are at least three major challenges ahead.
1. Incorporating background knowledge.
While much recent work has focused on algo-
rithmic development, keyphrase extractors need
to have a deeper ?understanding? of a document
in order to reach the next level of performance.
Such an understanding can be facilitated by the
incorporation of background knowledge.
2. Handling long documents. While it may be
possible to design better algorithms to handle the
large number of candidates in long documents, we
believe that employing sophisticated features, es-
pecially those that encode background knowledge,
will enable keyphrases and non-keyphrases to be
distinguished more easily even in the presence of
a large number of candidates.
3. Improving evaluation schemes. To more ac-
curately measure the performance of keyphrase
extractors, they should not be penalized for evalu-
ation errors. We have suggested several possibili-
ties as to how this problem can be addressed.
Acknowledgments
We thank the anonymous reviewers for their de-
tailed and insightful comments on earlier drafts of
this paper. This work was supported in part by
NSF Grants IIS-1147644 and IIS-1219142.
1270
References
Ken Barker and Nadia Cornacchia. 2000. Using noun
phrase heads to extract document keyphrases. In
Proceedings of the 13th Biennial Conference of the
Canadian Society on Computational Studies of In-
telligence, pages 40?52.
G?abor Berend. 2011. Opinion expression mining by
exploiting keyphrase extraction. In Proceedings of
the 5th International Joint Conference on Natural
Language Processing, pages 1162?1170.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data, pages 1247?1250.
Adrien Bougouin, Florian Boudin, and B?eatrice Daille.
2013. Topicrank: Graph-based topic ranking for
keyphrase extraction. In Proceedings of the 6th In-
ternational Joint Conference on Natural Language
Processing, pages 543?551.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks, 30(1?7):107?117.
Mo Chen, Jian-Tao Sun, Hua-Jun Zeng, and Kwok-Yan
Lam. 2005. A practical system of keyphrase ex-
traction for web pages. In Proceedings of the 14th
ACM International Conference on Information and
Knowledge Management, pages 277?278.
Zhuoye Ding, Qi Zhang, and Xuanjing Huang. 2011.
Keyphrase extraction from online news using binary
integer programming. In Proceedings of the 5th In-
ternational Joint Conference on Natural Language
Processing, pages 165?173.
Mark Dredze, Hanna M. Wallach, Danny Puller, and
Fernando Pereira. 2008. Generating summary key-
words for emails using topics. In Proceedings of the
13th International Conference on Intelligent User
Interfaces, pages 199?206.
Samhaa R. El-Beltagy and Ahmed A. Rafea. 2009.
KP-Miner: A keyphrase extraction system for En-
glish and Arabic documents. Information Systems,
34(1):132?144.
Samhaa R. El-Beltagy and Ahmed Rafea. 2010. KP-
Miner: Participation in SemEval-2. In Proceedings
of the 5th International Workshop on Semantic Eval-
uation, pages 190?193.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of 16th International Joint Conference on Arti-
ficial Intelligence, pages 668?673.
Maria Grineva, Maxim Grinev, and Dmitry Lizorkin.
2009. Extracting key terms from noisy and multi-
theme documents. In Proceedings of the 18th In-
ternational Conference on World Wide Web, pages
661?670.
Carl Gutwin, Gordon Paynter, Ian Witten, Craig Nevill-
Manning, and Eibe Frank. 1999. Improving brows-
ing in digital libraries with keyphrase indexes. De-
cision Support Systems, 27:81?104.
Khaled M. Hammouda, Diego N. Matute, and Mo-
hamed S. Kamel. 2005. CorePhrase: Keyphrase ex-
traction for document clustering. In Proceedings of
the 4th International Conference on Machine Learn-
ing and Data Mining in Pattern Recognition, pages
265?274.
Kazi Saidul Hasan and Vincent Ng. 2010. Conun-
drums in unsupervised keyphrase extraction: Mak-
ing sense of the state-of-the-art. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 365?373.
Chong Huang, Yonghong Tian, Zhi Zhou, Charles X.
Ling, and Tiejun Huang. 2006. Keyphrase extrac-
tion using semantic networks structure analysis. In
Proceedings of the 6th International Conference on
Data Mining, pages 275?284.
Anette Hulth and Be?ata B. Megyesi. 2006. A study
on automatically extracted keywords in text catego-
rization. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 537?544.
Anette Hulth, Jussi Karlgren, Anna Jonsson, Henrik
Bostr?om, and Lars Asker. 2001. Automatic key-
word extraction using domain knowledge. In Pro-
ceedings of the 2nd International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, pages 472?482.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, pages 216?
223.
Anette Hulth. 2004. Enhancing linguistically ori-
ented automatic keyword extraction. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics: Short Papers, pages 17?
20.
Xin Jiang, Yunhua Hu, and Hang Li. 2009. A rank-
ing approach to keyphrase extraction. In Proceed-
ings of the 32nd International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 756?757.
Daniel Kelleher and Saturnino Luz. 2005. Automatic
hypertext keyphrase detection. In Proceedings of the
19th International Joint Conference on Artificial In-
telligence, pages 1608?1609.
1271
Su Nam Kim and Timothy Baldwin. 2012. Extracting
keywords from multi-party live chats. In Proceed-
ings of the 26th Pacific Asia Conference on Lan-
guage, Information, and Computation, pages 199?
208.
Su Nam Kim and Min-Yen Kan. 2009. Re-examining
automatic keyphrase extraction approaches in scien-
tific articles. In Proceedings of the ACL-IJCNLP
Workshop on Multiword Expressions, pages 9?16.
Su Nam Kim, Timothy Baldwin, and Min-Yen Kan.
2010a. Evaluating n-gram based evaluation metrics
for automatic keyphrase extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 572?580.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010b. SemEval-2010 Task 5:
Automatic keyphrase extraction from scientific arti-
cles. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, pages 21?26.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2013. Automatic keyphrase
extraction from scientific articles. Language Re-
sources and Evaluation, 47(3):723?742.
Niraj Kumar and Kannan Srinathan. 2008. Automatic
keyphrase extraction from scientific documents us-
ing n-gram filtration technique. In Proceedings of
the 8th ACM Symposium on Document Engineering,
pages 199?208.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009a. Unsupervised approaches for automatic key-
word extraction using meeting transcripts. In Pro-
ceedings of Human Language Technologies: The
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 620?628.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong
Sun. 2009b. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 257?266.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase extrac-
tion via topic decomposition. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 366?376.
Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, and
Maosong Sun. 2011. Automatic keyphrase extrac-
tion by bridging vocabulary gap. In Proceedings of
the 15th Conference on Computational Natural Lan-
guage Learning, pages 135?144.
Zhiyuan Liu, Chen Liang, and Maosong Sun. 2012.
Topical word trigger model for keyphrase extraction.
In Proceedings of the 24th International Conference
on Computational Linguistics, pages 1715?1730.
Patrice Lopez and Laurent Romary. 2010. HUMB:
Automatic key term extraction from scientific arti-
cles in GROBID. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
248?251.
Yutaka Matsuo and Mitsuru Ishizuka. 2004. Key-
word extraction from a single document using word
co-occurrence statistical information. International
Journal on Artificial Intelligence Tools, 13.
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1318?1327.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 404?411.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
David Newman, Nagendra Koilada, Jey Han Lau, and
Timothy Baldwin. 2012. Bayesian text segmenta-
tion for index term identification and keyphrase ex-
traction. In Proceedings of the 24th International
Conference on Computational Linguistics, pages
2077?2092.
Thuy Dung Nguyen and Min-Yen Kan. 2007.
Keyphrase extraction in scientific publications. In
Proceedings of the International Conference on
Asian Digital Libraries, pages 317?326.
Chau Q. Nguyen and Tuoi T. Phan. 2009. An
ontology-based approach for key phrase extraction.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the Association for Computa-
tional Linguistics and the 4th International Joint
Conference on Natural Language Processing: Short
Papers, pages 181?184.
Paul Over. 2001. Introduction to DUC-2001: An in-
trinsic evaluation of generic news text summariza-
tion systems. In Proceedings of the 2001 Document
Understanding Conference.
Mari-Sanna Paukkeri, Ilari T. Nieminen, Matti P?oll?a,
and Timo Honkela. 2008. A language-independent
approach to keyphrase extraction and evaluation. In
Proceedings of the 22nd International Conference
on Computational Linguistics: Companion Volume:
Posters, pages 83?86.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513?
523.
1272
Min Song, Il-Yeol Song, and Xiaohua Hu. 2003.
KPSpotter: A flexible information gain-based
keyphrase extraction system. In Proceedings of the
5th ACM International Workshop on Web Informa-
tion and Data Management, pages 50?53.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge. In Proceedings of the 16th International
World Wide Web Conference, pages 697?706.
Takashi Tomokiyo and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL Workshop on Multiword Ex-
pressions, pages 33?40.
Peter Turney. 1999. Learning to extract keyphrases
from text. National Research Council Canada, In-
stitute for Information Technology, Technical Report
ERB-1057.
Peter Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2:303?336.
Peter Turney. 2003. Coherent keyphrase extraction
via web mining. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence,
pages 434?439.
Xiaojun Wan and Jianguo Xiao. 2008a. Col-
labRank: Towards a collaborative approach to
single-document keyphrase extraction. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics, pages 969?976.
Xiaojun Wan and Jianguo Xiao. 2008b. Single
document keyphrase extraction using neighborhood
knowledge. In Proceedings of the 23rd AAAI Con-
ference on Artificial Intelligence, pages 855?860.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Towards an iterative reinforcement approach for si-
multaneous document summarization and keyword
extraction. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 552?559.
Chen Wang and Sujian Li. 2011. CoRankBayes:
Bayesian learning to rank under the co-training
framework and its application in keyphrase extrac-
tion. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Man-
agement, pages 2241?2244.
Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. KEA:
Practical automatic keyphrase extraction. In Pro-
ceedings of the 4th ACM Conference on Digital Li-
braries, pages 254?255.
Yi-Fang Brook Wu, Quanzhi Li, Razvan Stefan Bot,
and Xin Chen. 2005. Domain-specific keyphrase
extraction. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management, pages 283?284.
Wen-Tau Yih, Joshua Goodman, and Vitor R. Carvalho.
2006. Finding advertising keywords on web pages.
In Proceedings of the 15th International Conference
on World Wide Web, pages 213?222.
Wei You, Dominique Fontaine, and Jean-Paul Barth`es.
2009. Automatic keyphrase extraction with a
refined candidate set. In Proceedings of the
IEEE/WIC/ACM International Joint Conference on
Web Intelligence and Intelligent Agent Technology,
pages 576?579.
Torsten Zesch and Iryna Gurevych. 2009. Approxi-
mate matching for evaluating keyphrase extraction.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
2009, pages 484?489.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement
principle and sentence clustering. In Proceedings
of 25th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 113?120.
Yongzheng Zhang, Nur Zincir-Heywood, and Evange-
los Milios. 2004. World Wide Web site summariza-
tion. Web Intelligence and Agent Systems, 2:39?53.
Yongzheng Zhang, Nur Zincir-Heywood, and Evange-
los Milios. 2005. Narrative text classification for
automatic key phrase extraction in web document
corpora. In Proceedings of the 7th ACM Interna-
tional Workshop on Web Information and Data Man-
agement, pages 51?58.
Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn
Achanauparp, Ee-Peng Lim, and Xiaoming Li.
2011. Topical keyphrase extraction from Twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 379?388.
1273
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1534?1543,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Modeling Prompt Adherence in Student Essays
Isaac Persing and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{persingq,vince}@hlt.utdallas.edu
Abstract
Recently, researchers have begun explor-
ing methods of scoring student essays with
respect to particular dimensions of qual-
ity such as coherence, technical errors,
and prompt adherence. The work on
modeling prompt adherence, however, has
been focused mainly on whether individ-
ual sentences adhere to the prompt. We
present a new annotated corpus of essay-
level prompt adherence scores and pro-
pose a feature-rich approach to scoring es-
says along the prompt adherence dimen-
sion. Our approach significantly outper-
forms a knowledge-lean baseline prompt
adherence scoring system yielding im-
provements of up to 16.6%.
1 Introduction
Automated essay scoring, the task of employing
computer technology to evaluate and score writ-
ten text, is one of the most important educational
applications of natural language processing (NLP)
(see Shermis and Burstein (2003) and Shermis et
al. (2010) for an overview of the state of the art
in this task). A major weakness of many ex-
isting scoring engines such as the Intelligent Es-
say AssessorTM(Landauer et al, 2003) is that they
adopt a holistic scoring scheme, which summa-
rizes the quality of an essay with a single score and
thus provides very limited feedback to the writer.
In particular, it is not clear which dimension of
an essay (e.g., style, coherence, relevance) a score
should be attributed to. Recent work addresses this
problem by scoring a particular dimension of es-
say quality such as coherence (Miltsakaki and Ku-
kich, 2004), technical errors, organization (Pers-
ing et al, 2010), and thesis clarity (Persing and
Ng, 2013). Essay grading software that provides
feedback along multiple dimensions of essay qual-
ity such as E-rater/Criterion (Attali and Burstein,
2006) has also begun to emerge.
Our goal in this paper is to develop a com-
putational model for scoring an essay along an
under-investigated dimension ? prompt adher-
ence. Prompt adherence refers to how related an
essay?s content is to the prompt for which it was
written. An essay with a high prompt adherence
score consistently remains on the topic introduced
by the prompt and is free of irrelevant digressions.
To our knowledge, little work has been done
on scoring the prompt adherence of student essays
since Higgins et al (2004). Nevertheless, there are
major differences between Higgins et al?s work
and our work with respect to both the way the task
is formulated and the approach. Regarding task
formulation, while Higgins et al focus on classi-
fying each sentence as having either good or bad
adherence to the prompt, we focus on assigning
a prompt adherence score to the entire essay, al-
lowing the score to range from one to four points
at half-point increments. As far as the approach
is concerned, Higgins et al adopt a knowledge-
lean approach to the task, where almost all of
the features they employ are computed based on
a word-based semantic similarity measure known
as Random Indexing (Kanerva et al, 2000). On
the other hand, we employ a large variety of fea-
tures, including lexical and knowledge-based fea-
tures that encode how well the concepts in an es-
say match those in the prompt, LDA-based fea-
tures that provide semantic generalizations of lex-
ical features, and ?error type? features that encode
different types of errors the writer made that are
related to prompt adherence.
In sum, our contributions in this paper are two-
fold. First, we develop a scoring model for the
prompt adherence dimension on student essays us-
ing a feature-rich approach. Second, in order to
stimulate further research on this task, we make
our data set consisting of prompt adherence an-
1534
Topic Languages Essays
Most university degrees are the-
oretical and do not prepare stu-
dents for the real world. They are
therefore of very little value.
13 131
The prison system is outdated.
No civilized society should pun-
ish its criminals: it should reha-
bilitate them.
11 80
In his novel Animal Farm,
George Orwell wrote ?All men
are equal but some are more
equal than others.? How true is
this today?
10 64
Table 1: Some examples of writing topics.
notations of 830 essays publicly available. Since
progress in prompt adherence modeling is hin-
dered in part by the lack of a publicly annotated
corpus, we believe that our data set will be a valu-
able resource to the NLP community.
2 Corpus Information
We use as our corpus the 4.5 million word Interna-
tional Corpus of Learner English (ICLE) (Granger
et al, 2009), which consists of more than 6000 es-
says written by university undergraduates from 16
countries and 16 native languages who are learn-
ers of English as a Foreign Language. 91% of the
ICLE texts are argumentative. We select a subset
consisting of 830 argumentative essays from the
ICLE to annotate for training and testing of our
essay prompt adherence scoring system. Table 1
shows three of the 13 topics selected for annota-
tion. Fifteen native languages are represented in
the set of annotated essays.
3 Corpus Annotation
We ask human annotators to score each of the 830
argumentative essays along the prompt adherence
dimension. Our annotators were selected from
over 30 applicants who were familiarized with the
scoring rubric and given sample essays to score.
The six who were most consistent with the ex-
pected scores were given additional essays to an-
notate. Annotators evaluated how well each es-
say adheres to its prompt using a numerical score
from one to four at half-point increments (see Ta-
ble 2 for a description of each score). This con-
trasts with previous work on prompt adherence es-
say scoring, where the corpus is annotated with a
binary decision (i.e., good or bad) (e.g., Higgins
et al (2004; 2006), Louis and Higgins (2010)).
Hence, our annotation scheme not only provides
Score Description of Prompt Adherence
4 essay fully addresses the prompt and consis-
tently stays on topic
3 essay mostly addresses the prompt or occasion-
ally wanders off topic
2 essay does not fully address the prompt or con-
sistently wanders off topic
1 essay does not address the prompt at all or is
completely off topic
Table 2: Descriptions of the meaning of scores.
a finer-grained distinction of prompt adherence
(which can be important in practice), but also
makes the prediction task more challenging.
To ensure consistency in annotation, we ran-
domly select 707 essays to have graded by mul-
tiple annotators. Analysis reveals that the Pear-
son?s correlation coefficient computed over these
doubly annotated essays is 0.243. Though annota-
tors exactly agree on the prompt adherence score
of an essay only 38% of the time, the scores they
apply fall within 0.5 points in 66% of essays and
within 1.0 point in 89% of essays. For the sake
of our experiments, whenever annotators disagree
on an essay?s prompt adherence score, we assign
the essay the average of all annotations rounded to
the nearest half point. Table 3 shows the number
of essays that receive each of the seven scores for
prompt adherence.
score 1.0 1.5 2.0 2.5 3.0 3.5 4.0
essays 0 0 8 44 105 230 443
Table 3: Distribution of prompt adherence scores.
4 Score Prediction
In this section, we describe in detail our system for
predicting essays? prompt adherence scores.
4.1 Model Training and Application
We cast the problem of predicting an essay?s
prompt adherence score as 13 regression prob-
lems, one for each prompt. Each essay is repre-
sented as an instance whose label is the essay?s
true score (one of the values shown in Table 3)
with up to seven types of features including base-
line (Section 4.2) and six other feature types pro-
posed by us (Section 4.3). Our regressors may as-
sign an essay any score in the range of 1.0?4.0.
Using regression captures the fact that some
pairs of scores are more similar than others (e.g.,
an essay with a prompt adherence score of 3.5 is
more similar to an essay with a score of 4.0 than it
is to one with a score of 1.0). A classification sys-
1535
tem, by contrast, may sometimes believe that the
scores 1.0 and 4.0 are most likely for a particu-
lar essay, even though these scores are at opposite
ends of the score range.
Using a different regressor for each prompt cap-
tures the fact that it may be easier for an essay to
adhere to some prompts than to others, and com-
mon problems students have writing essays for
one prompt may not apply to essays written in re-
sponse to another prompt. For example, in essays
written in response to the prompt ?Marx once said
that religion was the opium of the masses. If he
was alive at the end of the 20th century, he would
replace religion with television,? students some-
times write essays about all the evils of television,
forgetting that their essay is only supposed to be
about whether it is ?the opium of the masses?. Stu-
dents are less likely to make an analogous mistake
when writing for the prompt ?Crime does not pay.?
After creating training instances for prompt p
i
,
we train a linear regressor, r
i
, with regularization
parameter c
i
for scoring test essays written in re-
sponse to p
i
using the linear SVM regressor imple-
mented in the LIBSVM software package (Chang
and Lin, 2001). All SVM-specific learning param-
eters are set to their default values except c
i
, which
we tune to maximize performance on held-out val-
idation data.
After training the classifiers, we use them to
classify the test set essays. The test instances are
created in the same way as the training instances.
4.2 Baseline Features
Our baseline system for score prediction employs
various features based on Random Indexing.
1. Random Indexing Random Indexing (RI) is
?an efficient, scalable and incremental alterna-
tive? (Sahlgren, 2005) to Latent Semantic Index-
ing (Deerwester et al, 1990; Landauer and Dut-
nais, 1997) which allows us to automatically gen-
erate a semantic similarity measure between any
two words. We train our RI model on over 30 mil-
lion words of the English Gigaword corpus (Parker
et al, 2009) using the S-Space package (Jurgens
and Stevens, 2010). We expect that features based
on RI will be useful for prompt adherence scor-
ing because they may help us find text related
to the prompt even if some of its concepts have
have been rephrased (e.g., an essay may talk about
?jail? rather than ?prison?, which is mentioned in
one of the prompts), and because they have al-
ready proven useful for the related task of deter-
mining which sentences in an essay are related to
the prompt (Higgins et al, 2004).
For each essay, we therefore attempt to adapt
the RI features used by Higgins et al (2004) to
our problem of prompt adherence scoring. We do
this by generating one feature encoding the entire
essay?s similarity to the prompt, another encoding
the essay?s highest individual sentence?s similarity
to the prompt, a third encoding the highest entire
essay similarity to one of the prompt sentences,
another encoding the highest individual sentence
similarity to an individual prompt sentence, and fi-
nally one encoding the entire essay?s similarity to
a manually rewritten version of the prompt that ex-
cludes extraneous material (such as ?In his novel
Animal Farm, George Orwell wrote,? which is in-
troductory material from the third prompt in Ta-
ble 1). Our RI feature set necessarily excludes
those features from Higgins et al that are not
easily translatable to our problem since we are
concerned with an entire essay?s adherence to its
prompt rather than with each of its sentences? re-
latedness to the prompt. Since RI does not pro-
vide a straightforward way to measure similar-
ity between groups of words such as sentences
or essays, we use Higgins and Burstein?s (2007)
method to generate these features.
4.3 Novel Features
Next, we introduce six types of novel features.
2. N-grams As our first novel feature, we use
the 10,000 most important lemmatized unigram,
bigram, and trigram features that occur in the es-
say. N-grams can be useful for prompt adherence
scoring because they can capture useful words and
phrases related to a prompt. For example, words
and phrases like ?university degree?, ?student?,
and ?real world? are relevant to the first prompt in
Table 1, so it is more likely that an essay adheres
to the prompt if they appear in the essay.
We determine the ?most important? n-gram fea-
tures using information gain computed over the
training data (Yang and Pedersen, 1997). Since the
essays vary greatly in length, we normalize each
essay?s set of n-gram features to unit length.
3. Thesis Clarity Keywords Our next set of fea-
tures consists of the keyword features we intro-
duced in our previous work on essay thesis clarity
scoring (Persing and Ng, 2013). Below we give an
overview of these keyword features and motivate
1536
why they are potentially useful for prompt adher-
ence scoring.
The keyword features were formed by first ex-
amining the 13 essay prompts, splitting each into
its component pieces. As an example of what is
meant by a ?component piece?, consider the first
prompt in Table 1. The components of this prompt
would be ?Most university degrees are theoreti-
cal?, ?Most university degrees do not prepare stu-
dents for the real world?, and ?Most university de-
grees are of very little value.?
Then the most important (primary) and second
most important (secondary) words were selected
from each prompt component, where a word was
considered ?important? if it would be a good word
for a student to use when stating her thesis about
the prompt. So since the lemmatized version of the
third component of the second prompt in Table 1
is ?it should rehabilitate they?, ?rehabilitate? was
selected as a primary keyword and ?society? as a
secondary keyword.
Features are then computed based on these key-
words. For instance, one thesis clarity keyword
feature is computed as follows. The RI similarity
measure is first taken between the essay and each
group of the prompt?s primary keywords. The fea-
ture then gets assigned the lowest of these values.
If this feature has a low value, that suggests that
the student ignored the prompt component from
which the value came when writing the essay.
To compute another of the thesis clarity key-
word features, the numbers of combined primary
and secondary keywords the essay contains from
each component of its prompt are counted. These
numbers are then divided by the total count of pri-
mary and secondary features in their respective
components. The greatest of the fractions gener-
ated in this way is encoded as a feature because if
it has a low value, that indicates the essay?s thesis
may not be very relevant to the prompt.1
4. Prompt Adherence Keywords The thesis
clarity keyword features described above were in-
tended for the task of determining how clear an
essay?s thesis is, but since our goal is instead to de-
termine how well an essay adheres to its prompt,
it makes sense to adapt keyword features to our
task rather than to adopt keyword features ex-
1Space limitations preclude a complete listing of the the-
sis clarity keyword features. See our website at http:
//www.hlt.utdallas.edu/
?
persingq/ICLE/ for
the complete list.
actly as they have been used before. For this
reason, we construct a new list of keywords for
each prompt component, though since prompt ad-
herence is more concerned with what the student
says about the topics than it is with whether or
not what she says about them is stated clearly,
our keyword lists look a little different than the
ones discussed above. For an example, we ear-
lier alluded to the problem of students merely dis-
cussing all the evils of television for the prompt
?Marx once said that religion was the opium of the
masses. If he was alive at the end of the 20th cen-
tury, he would replace religion with television.?
Since the question suggests that students discuss
whether television is analogous to religion in this
way, our set of prompt adherence keywords for
this prompt contains the word ?religion? while the
previously discussed keyword sets do not. This
is because a thesis like ?Television is bad? can be
stated very clearly without making any reference
to religion at all, and so an essay with a thesis like
this can potentially have a very high thesis clarity
score. It should not, however, have a very high
prompt adherence score, as the prompt asked the
student to discuss whether television is like reli-
gion in a particular way, so religion should be at
least briefly addressed for an essay to be awarded
a high prompt adherence score.
Additionally, our prompt adherence keyword
sets do not adopt the notions of primary and sec-
ondary groups of keywords for each prompt com-
ponent, instead collecting all the keywords for a
component into one set because ?secondary? key-
words tend to be things that are important when we
are concerned with what a student is saying about
the topic rather than just how clearly she said it.
We form two types of features from prompt ad-
herence keywords. While both types of features
measure how much each prompt component was
discussed in an essay, they differ in how they en-
code the information. To obtain feature values of
the first type, we take the RI similarities between
the whole essay and each set of prompt adherence
keywords from the prompt?s components. This
results in one to three features, as some prompts
have one component while others have up to three.
We obtain feature values of the second type as
follows. For each component, we count the num-
ber of prompt adherence keywords the essay con-
tains. We divide this number by the number of
prompt adherence keywords we identified from
1537
the component. This results in one to three fea-
tures since a prompt has one to three components.
5. LDA Topics A problem with the features we
have introduced up to this point is that they have
trouble identifying topics that are not mentioned
in the prompt, but are nevertheless related to the
prompt. These topics should not diminish the es-
say?s prompt adherence score because they are at
least related to prompt concepts. For example,
consider the prompt ?All armies should consist en-
tirely of professional soldiers: there is no value in
a system of military service.? An essay contain-
ing words like ?peace?, ?patriotism?, or ?training?
are probably not digressions from the prompt, and
therefore should not be penalized for discussing
these topics. But the various measures of keyword
similarities described above will at best not notice
that anything related to the prompt is being dis-
cussed, and at worst, this might have effects like
lowering some of the RI similarity scores, thereby
probably lowering the prompt adherence score the
regressor assigns to the essay. While n-gram fea-
tures do not have exactly the same problem, they
would still only notice that these example words
are related to the prompt if multiple essays use the
same words to discuss these concepts. For this
reason, we introduce Latent Dirichlet Allocation
(LDA) (Blei et al, 2003) features.
In order to construct our LDA features, we
first collect all essays written in response to each
prompt into its own set. Note that this feature type
exploits unlabeled data: it includes all essays in
the ICLE responding to our prompts, not just those
in our smaller annotated 830 essay dataset. We
then use the MALLET (McCallum, 2002) imple-
mentation of LDA to build a topic model of 1,000
topics around each of these sets of essays. This
results in what we can think of as a soft clustering
of words into 1,000 sets for each prompt, where
each set of words represents one of the topics LDA
identified being discussed in the essays for that
prompt. So for example, the five most impor-
tant words in the most frequently discussed topic
for the military prompt we mentioned above are
?man?, ?military?, ?service?, ?pay?, and ?war?.
We also use the MALLET-generated topic
model to tell us how much of each essay is spent
discussing each of the 1,000 topics. The model
might tell us, for example, that a particular essay
written on the military prompt spends 35% of the
time discussing the ?man?, ?military?, ?service?,
?pay?, and ?war? topic and 65% of the time dis-
cussing a topic whose most important words are
?fully?, ?count?, ?ordinary?, ?czech?, and ?day?.
Since the latter topic is discussed so much in the
essay and does not appear to have much to do with
the military prompt, this essay should probably
get a bad prompt adherence score. We construct
1,000 features from this topic model, one for each
topic. Each feature?s value is obtained by using
the topic model to tell us how much of the essay
was spent discussing the feature?s corresponding
topic. From these features, our regressor should
be able to learn which topics are important to a
good prompt adherent essay.
6. Manually Annotated LDA Topics A weak-
ness of the LDA topics feature type is that it may
result in a regressor that has trouble distinguishing
between an infrequent topic that is adherent to the
prompt and one that just represents an irrelevant
digression. This is because an infrequent topic
may not appear in the training set often enough for
the regressor to make this judgment. We introduce
the manually annotated LDA topics feature type to
address this problem.
In order to construct manually annotated LDA
topic features, we first build 13 topic models, one
for each prompt, just as described in the section
on LDA topic features. Rather than requesting
models of 1,000 topics, however, we request mod-
els of only 100 topics2. We then go through all
13 lists of 100 topics as represented by their top
ten words, manually annotating each topic with a
number from 0 to 5 representing how likely it is
that the topic is adherent to the prompt. A topic
labeled 5 is very likely to be related to the prompt,
where a topic labeled 0 appears totally unrelated.
Using these annotations alongside the topic dis-
tribution for each essay that the topic models pro-
vide us, we construct ten features. The first five
features encode the sum of the contributions to an
essay of topics annotated with a number ? 1, the
sum of the contributions to an essay of topics an-
notated with a number ? 2, and so on up to 5.
The next five features are similar to the last,
with one feature taking on the sum of the contri-
butions to an essay of topics annotated with the
number 0, another feature taking on the sum of the
2We use 100 topics for each prompt in the manually an-
notated version of LDA features rather than the 1,000 topics
we use in the regular version of LDA features because 1,300
topics are not too costly to annotate, but manually annotating
13,000 topics would take too much time.
1538
contributions to an essay of topics annotated with
the number 1, and so on up to 4. We do not include
a feature for topics annotated with the number 5
because it would always have the same value as
the feature for topics ? 5.
Features like these should give the regressor a
better idea how much of an essay is composed of
prompt-related arguments and discussion and how
much of it is irrelevant to the prompt, even if some
of the topics occurring in it are too infrequent to
judge just from training data.
7. Predicted Thesis Clarity Errors In our pre-
vious work on essay thesis clarity scoring (Persing
and Ng, 2013), we identified five classes of errors
that detract from the clarity of an essay?s thesis:
Confusing Phrasing. The thesis is phrased oddly,
making it hard to understand the writer?s point.
Incomplete Prompt Response. The thesis leaves
some part of a multi-part prompt unaddressed.
Relevance to Prompt. The apparent thesis?s weak
relation to the prompt causes confusion.
Missing Details. The thesis leaves out an impor-
tant detail needed to understand the writer?s point.
Writer Position. The thesis describes a position
on the topic without making it clear that this is the
position the writer supports.
We hypothesize that these errors, though orig-
inally intended for thesis clarity scoring, could
be useful for prompt adherence scoring as well.
For instance, an essay that has a Relevance to
Prompt error or an Incomplete Prompt Response
error should intuitively receive a low prompt ad-
herence score. For this reason, we introduce fea-
tures based on these errors to our feature set for
prompt adherence scoring3.
While each of the essays in our data set was pre-
viously annotated with these thesis clarity errors,
in a realistic setting a prompt adherence scoring
system will not have access to these manual error
labels. As a result, we first need to predict which
of these errors is present in each essay. To do this,
we train five maximum entropy classifiers for each
prompt, one for each of the five thesis clarity er-
rors, using MALLET?s (McCallum, 2002) imple-
mentation of maximum entropy classification. In-
stances are presented to classifier for prompt p for
error e in the following way. If a training essay
is written in response to p, it will be used to gen-
3See our website at http://www.hlt.utdallas.
edu/
?
persingq/ICLE/ for the complete list of error an-
notations.
erate a training instance whose label is 1 if e was
annotated for it or 0 otherwise. Since error pre-
diction and prompt adherence scoring are related
problems, the features we associate with this in-
stance are features 1?6 which we have described
earlier in this section. The classifier is then used
to generate probabilities telling us how likely it is
that each test essay has error e.
Then, when training our regressor for prompt
adherence scoring, we add the following features
to our instances. We add a binary feature indicat-
ing the presence or absence of each error. Or in
the case of test essays, the feature takes on a real
value from 0 to 1 indicating how likely the classi-
fier thought it was that the essay had each of the
errors. This results in five additional features, one
for each error.
5 Evaluation
In this section, we evaluate our system for prompt
adherence scoring. All the results we report
are obtained via five-fold cross-validation exper-
iments. In each experiment, we use 3
5
of our la-
beled essays for model training, another 1
5
for pa-
rameter tuning, and the final 1
5
for testing.
5.1 Experimental Setup
5.1.1 Scoring Metrics
We employ four evaluation metrics. As we will see
below, S1, S2, and S3 are error metrics, so lower
scores imply better performance. In contrast, PC
is a correlation metric, so higher correlation im-
plies better performance.
The simplest metric, S1, measures the fre-
quency at which a system predicts the wrong score
out of the seven possible scores. Hence, a system
that predicts the right score only 25% of the time
would receive an S1 score of 0.75.
The S2 metric measures the average distance
between a system?s score and the actual score.
This metric reflects the idea that a system that pre-
dicts scores close to the annotator-assigned scores
should be preferred over a system whose predic-
tions are further off, even if both systems estimate
the correct score at the same frequency.
The S3 metric measures the average square
of the distance between a system?s score predic-
tions and the annotator-assigned scores. The in-
tuition behind this system is that not only should
we prefer a system whose predictions are close
to the annotator scores, but we should also prefer
1539
one whose predictions are not too frequently very
far away from the annotator scores. These three
scores are given by:
1
N
?
A
j
6=E
?
j
1,
1
N
N
?
i=1
|A
j
? E
j
|,
1
N
N
?
i=1
(A
j
? E
j
)
2
where A
j
, E
j
, and E?
j
are the annotator assigned,
system predicted, and rounded system predicted
scores4 respectively for essay j, and N is the num-
ber of essays.
The last metric, PC , computes Pearson?s cor-
relation coefficient between a system?s predicted
scores and the annotator-assigned scores. PC
ranges from ?1 to 1. A positive (negative) PC
implies that the two sets of predictions are posi-
tively (negatively) correlated.
5.1.2 Parameter Tuning
As mentioned earlier, for each prompt p
i
, we train
a linear regressor r
i
using LIBSVM with regular-
ization parameter c
i
. To optimize our system?s
performance on the three error measures described
previously, we use held-out validation data to in-
dependently tune each of the c
i
values5. Note that
each of the c
i
values can be tuned independently
because a c
i
value that is optimal for predicting
scores for p
i
essays with respect to any of the error
performance measures is necessarily also the opti-
mal c
i
when measuring that error on essays from
all prompts. However, this is not case with Pear-
son?s correlation coefficient, as the PC value for
essays from all 13 prompts cannot be simplified as
a weighted sum of the PC values obtained on each
individual prompt. In order to obtain an optimal
result as measured by PC , we jointly tune the c
i
parameters to optimize the PC value achieved by
our system on the same held-out validation data.
However, an exact solution to this optimization
problem is computationally expensive, as there are
too many (713) possible combinations of c values
to exhaustively search. Consequently, we find a
local maximum by employing the simulated an-
4Since our regressor assigns each essay a real value rather
than an actual valid score, it would be difficult to obtain a
reasonable S1 score without rounding the system estimated
score to one of the possible values. For that reason, we round
the estimated score to the nearest of the seven scores the hu-
man annotators were permitted to assign (1.0, 1.5, 2.0, 2.5,
3.0, 3.5, 4.0) only when calculating S1. For other scoring
metrics, we only round the predictions to 1.0 or 4.0 if they
fall outside the 1.0?4.0 range.
5For parameter tuning, we employ the following values.
c
i
may be assigned any of the values 100 101, 102, 103, 104,
10
5
, or 106.
System S1 S2 S3 PC
Baseline .517 .368 .234 .233
Our System .488 .348 .197 .360
Table 4: Five-fold cross-validation results for
prompt adherence scoring.
nealing algorithm (Kirkpatrick et al, 1983), alter-
ing one c
i
value at a time to optimize PC while
holding the remaining parameters fixed.
5.2 Results and Discussion
Five-fold cross-validation results on prompt ad-
herence score prediction are shown in Table 4. On
the first line, this table shows that our baseline sys-
tem, which recall uses only various RI features,
predicts the wrong score 51.7% of the time. Its
predictions are off by an average of .368 points,
and the average squared distance between its pre-
dicted score and the actual score is .234. In addi-
tion, its predicted scores and the actual scores have
a Pearson correlation coefficient of 0.233.
The results from our system, which uses all
seven feature types described in Section 4, are
shown in row 2 of the table. Our system obtains
S1, S2, S3, and PC scores of .488, .348, .197,
and .360 respectively, yielding a significant im-
provement over the baseline with respect to S2,
S3, and PC with p < 0.05, p < 0.01, and p < 0.06
respectively6 . While our system yields improve-
ments by all four measures, its improvement over
the baseline S1 score is not significant. These re-
sults mean that the greatest improvements our sys-
tem makes are that it ensures that our score pre-
dictions are not too often very far away from an
essay?s actual score, as making such predictions
would tend to drive up S3, yielding a relative er-
ror reduction in S3 of 15.8%, and it also ensures
a better correlation between predicted and actual
scores, thus yielding the 16.6% improvement in
PC .
7 It also gives more modest improvements in
how frequently exactly the right score is predicted
(S1) and is better at predicting scores closer to the
actual scores (S2).
5.3 Feature Ablation
To gain insight into how much impact each of the
feature types has on our system, we perform fea-
6All significance tests are paired t-tests.
7These numbers are calculated B?O
B?P
where B is the base-
line system?s score, O is our system?s score, and P is a per-
fect score. Perfect scores for error measures and PC are 0
and 1 respectively.
1540
ture ablation experiments in which we remove the
feature types from our system one-by-one.
Results of the ablation experiments when per-
formed using the four scoring metrics are shown in
Table 5. The top line of each subtable shows what
our system?s score would be if we removed just
one of the feature types from our system. So to see
how our system performs by the S1 metric if we
remove only predicted thesis clarity error features,
we would look at the first row of results of Ta-
ble 5(a) under the column headed by the number 7
since predicted thesis clarity errors are the seventh
feature type introduced in Section 4. The number
here tells us that our system?s S1 score without
this feature type is .502. Since Table 4 shows that
when our system includes this feature type (along
with all the other feature types), it obtains an S1
score of .488, this feature type?s removal costs our
system .014 S1 points, and thus its inclusion has a
beneficial effect on the S1 score.
From row 1 of Table 5(a), we can see that re-
moving feature 4 yields a system with the best S1
score in the presence of the other feature types in
this row. For this reason, we permanently remove
feature 4 from the system before we generate the
results on line 2. Thus, we can see what happens
when we remove both feature 4 and feature 5 by
looking at the second entry in row 2. And since
removing feature 6 harms performance least in the
presence of row 2?s other feature types, we perma-
nently remove both 4 and 6 from our feature set
when we generate the third row of results. We it-
eratively remove the feature type that yields a sys-
tem with the best performance in this way until we
get to the last line, where only one feature type is
used to generate each result.
Since the feature type whose removal yields the
best system is always the rightmost entry in a line,
the order of column headings indicates the rela-
tive importance of the feature types, with the left-
most feature types being most important to per-
formance and the rightmost feature types being
least important in the presence of the other fea-
ture types. This being the case, it is interesting to
note that while the relative importance of differ-
ent feature types does not remain exactly the same
if we measure performance in different ways, we
can see that some feature types tend to be more im-
portant than others in a majority of the four scor-
ing metrics. Features 2 (n-grams), 3 (thesis clarity
keywords), and 6 (manually annotated LDA top-
(a) Results using the S1 metric
3 5 1 7 2 6 4
.527 .502 .512 .502 .511 .500 .488
.527 .502 .512 .501 .513 .500
.525 .508 .505 .505 .504
.513 .527 .520 .513
.523 .520 .506
.541 .527
(b) Results using the S2 metric
2 6 3 1 4 5 7
.356 .350 .348 .350 .349 .348 .348
.351 .349 .348 .348 .348 .347
.351 .349 .348 .348 .347
.350 .349 .348 .348
.358 .351 .349
.362 .352
(c) Results using the S3 metric
2 6 1 5 4 7 3
.221 .201 .197 .197 .197 .197 .196
.215 .201 .197 .196 .196 .196
.212 .203 .199 .197 .196
.212 .203 .199 .197
.212 .203 .199
.223 .204
(d) Results using the PC metric
6 3 2 1 7 5 4
.326 .332 .303 .344 .348 .348 .361
.326 .332 .304 .343 .348 .348
.324 .337 .292 .345 .352
.322 .337 .297 .346
.316 .321 .323
.218 .325
Table 5: Feature ablation results. In each subtable,
the first row shows how our system would perform if each
feature type was removed. We remove the least important
feature type, and show in the next row how the adjusted sys-
tem would perform without each remaining type. For brevity,
a feature type is referred to by its feature number: (1) RI; (2)
n-grams; (3) thesis clarity keywords; (4) prompt adherence
keywords; (5) LDA topics; (6) manually annotated LDA top-
ics; and (7) predicted thesis clarity errors.
ics) tend to be the most important feature types,
as they tend to be the last feature types removed
in the ablation subtables. Features 1 (RI) and 5
(LDA topics) are of middling importance, with
neither ever being removed first or last, and each
tending to have a moderate effect on performance.
Finally, while features 4 (prompt adherence key-
words) and 7 (predicted thesis clarity errors) may
by themselves provide useful information to our
system, in the presence of the other feature types
they tend to be the least important to performance
as they are often the first feature types removed.
While there is a tendency for some feature types
to always be important (or unimportant) regardless
of which scoring metric is used to measure per-
1541
S1 S2 S3 PC
Gold .25 .50 .75 .25 .50 .75 .25 .50 .75 .25 .50 .75
2.0 3.35 3.56 3.79 3.40 3.52 3.73 3.06 3.37 3.64 3.06 3.37 3.64
2.5 3.43 3.63 3.80 3.25 3.52 3.79 3.24 3.45 3.67 3.24 3.46 3.73
3.0 3.64 3.78 3.85 3.56 3.70 3.90 3.52 3.65 3.74 3.52 3.66 3.79
3.5 3.73 3.81 3.88 3.63 3.78 3.90 3.59 3.70 3.81 3.60 3.74 3.85
4.0 3.76 3.84 3.88 3.70 3.83 3.90 3.63 3.75 3.84 3.66 3.78 3.88
Table 6: Regressor scores for our system.
formance, the relative importance of different fea-
ture types does not always remain consistent if we
measure performance in different ways. For ex-
ample, while we identified feature 3 (thesis clar-
ity keywords) as one of the most important fea-
ture types generally due to its tendency to have a
large beneficial impact on performance, when we
are measuring performance using S3, it is the least
useful feature type. Furthermore, its removal in-
creases the S3 score by a small amount, meaning
that its inclusion actually makes our system per-
form worse with respect to S3. Though feature 3 is
an extreme example, all feature types fluctuate in
importance, as we see when we compare their or-
ders of removal among the four ablation subtables.
Hence, it is important to know how performance
is measured when building a system for scoring
prompt adherence.
Feature 3 is not the only feature type whose re-
moval sometimes has a beneficial impact on per-
formance. As we can see in Table 5(b), the re-
moval of features 4, 5, and 7 improves our sys-
tem?s S2 score by .001 points. The same effect
occurs in Table 5(c) when we remove features 4,
7, and 3. These examples illustrate that under
some scoring metrics, the inclusion of some fea-
ture types is actively harmful to performance. For-
tunately, this effect does not occur in any other
cases than the two listed above, as most feature
types usually have a beneficial or at least neutral
impact on our system?s performance.
For those feature types whose effect on perfor-
mance is neutral in the first lines of ablation results
(feature 4 in S1, features 3, 5, and 7 in S2, and fea-
tures 1, 4, 5, and 7 in S3), it is important to note
that their neutrality does not mean that they are
unimportant. It merely means that they do not im-
prove performance in the presence of other feature
types. We can see this is the case by noting that
they are not all the least important feature types in
their respective subtables as indicated by column
order. For example, by the time feature 1 gets per-
manently removed in Table 5(c), its removal harms
performance by .002 S3 points.
5.4 Analysis of Predicted Scores
To more closely examine the behavior of our sys-
tem, in Table 6 we chart the distributions of scores
it predicts for essays having each gold standard
score. As an example of how to read this table,
consider the number 3.06 appearing in row 2.0 in
the .25 column of the S3 region. This means that
25% of the time, when our system with parameters
tuned for optimizing S3 is presented with a test es-
say having a gold standard score of 2.0, it predicts
that the essay has a score less than or equal to 3.06.
From this table, we see that our system has a
strong bias toward predicting more frequent scores
as there are no numbers less than 3.0 in the table,
and about 93.7% of all essays have gold standard
scores of 3.0 or above. Nevertheless, our system
does not rely entirely on bias, as evidenced by the
fact that each column in the table has a tendency
for its scores to ascend as the gold standard score
increases, implying that our system has some suc-
cess at predicting lower scores for essays with
lower gold standard prompt adherence scores.
Another interesting point to note about this ta-
ble is that the difference in error weighting be-
tween the S2 and S3 scoring metrics appears to be
having its desired effect, as every entry in the S3
subtable is less than its corresponding entry in the
S2 subtable due to the greater penalty the S3 met-
ric imposes for predictions that are very far away
from the gold standard scores.
6 Conclusion
We proposed a feature-rich approach to the under-
investigated problem of predicting essay-level
prompt adherence scores on student essays. In an
evaluation on 830 argumentative essays selected
from the ICLE corpus, our system significantly
outperformed a Random Indexing based baseline
by several evaluation metrics. To stimulate further
research on this task, we make all our annotations,
including our prompt adherence scores, the LDA
topic annotations, and the error annotations pub-
licly available.
1542
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with E-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: A library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/
?
cjlin/libsvm.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent smeantic analysis. Jour-
nal of American Society of Information Science,
41(6):391?407.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses universitaires
de Louvain.
Derrick Higgins and Jill Burstein. 2007. Sentence sim-
ilarity measures for essay coherence. In Proceed-
ings of the 7th International Workshop on Computa-
tional Semantics.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects
of coherence in student essays. In Human Lan-
guage Technologies: The 2004 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 185?192.
Derrick Higgins, Jill Burstein, and Yigal Attali. 2006.
Identifying off-topic student essays without topic-
specific training data. Natural Language Engineer-
ing, 12(2):145?159.
David Jurgens and Keith Stevens. 2010. The S-Space
package: An open source package for word space
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30?35.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In Proceedings of the 22nd An-
nual Conference of the Cognitive Science Society,
pages 103?106.
Scott Kirkpatrick, C. D. Gelatt, and Mario P. Vecchi.
1983. Optimization by simulated annealing. Sci-
ence, 220(4598):671?680.
Thomas K. Landauer and Susan T. Dutnais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
pages 211?240.
Thomas K. Landauer, Darrell Laham, and Peter W.
Foltz. 2003. Automated scoring and annotation of
essays with the Intelligent Essay AssessorTM? In Au-
tomated Essay Scoring: A Cross-Disciplinary Per-
spective, pages 87?112. Lawrence Erlbaum Asso-
ciates, Inc., Mahwah, NJ.
Annie Louis and Derrick Higgins. 2010. Off-topic
essay detection using short prompt texts. In Pro-
ceedings of the NAACL HLT 2010 Fifth Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 92?95.
Andrew Kachites McCallum. 2002. MALLET: A
Machine Learning for Language Toolkit. http:
//mallet.cs.umass.edu.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring sys-
tems. Natural Language Engineering, 10(1):25?55.
Robert Parker, David Graf, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth
Edition. Linguistic Data Consortium, Philadelphia.
Isaac Persing and Vincent Ng. 2013. Modeling the-
sis clarity in student essays. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
260?269.
Isaac Persing, Alan Davis, and Vincent Ng. 2010.
Modeling organization in student essays. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 229?
239.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Semantic
Indexing Workshop at the 7th International Confer-
ence on Terminology and Knowledge Engineering.
Mark D. Shermis and Jill C. Burstein. 2003. Au-
tomated Essay Scoring: A Cross-Disciplinary Per-
spective. Lawrence Erlbaum Associates, Inc., Mah-
wah, NJ.
Mark D. Shermis, Jill Burstein, Derrick Higgins, and
Klaus Zechner. 2010. Automated essay scoring:
Writing assessment and instruction. In International
Encyclopedia of Education (3rd edition). Elsevier,
Oxford, UK.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the 14th International Conference
on Machine Learning, pages 412?420.
1543
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30?35,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Scoring Coreference Partitions of Predicted Mentions:
A Reference Implementation
Sameer Pradhan
1
, Xiaoqiang Luo
2
, Marta Recasens
3
,
Eduard Hovy
4
, Vincent Ng
5
and Michael Strube
6
1
Harvard Medical School, Boston, MA,
2
Google Inc., New York, NY
3
Google Inc., Mountain View, CA,
4
Carnegie Mellon University, Pittsburgh, PA
5
HLTRI, University of Texas at Dallas, Richardson, TX,
6
HITS, Heidelberg, Germany
sameer.pradhan@childrens.harvard.edu, {xql,recasens}@google.com,
hovy@cmu.edu, vince@hlt.utdallas.edu, michael.strube@h-its.org
Abstract
The definitions of two coreference scoring
metrics?B
3
and CEAF?are underspeci-
fied with respect to predicted, as opposed
to key (or gold) mentions. Several varia-
tions have been proposed that manipulate
either, or both, the key and predicted men-
tions in order to get a one-to-one mapping.
On the other hand, the metric BLANC was,
until recently, limited to scoring partitions
of key mentions. In this paper, we (i) ar-
gue that mention manipulation for scoring
predicted mentions is unnecessary, and po-
tentially harmful as it could produce unin-
tuitive results; (ii) illustrate the application
of all these measures to scoring predicted
mentions; (iii) make available an open-
source, thoroughly-tested reference imple-
mentation of the main coreference eval-
uation measures; and (iv) rescore the re-
sults of the CoNLL-2011/2012 shared task
systems with this implementation. This
will help the community accurately mea-
sure and compare new end-to-end corefer-
ence resolution algorithms.
1 Introduction
Coreference resolution is a key task in natural
language processing (Jurafsky and Martin, 2008)
aiming to detect the referential expressions (men-
tions) in a text that point to the same entity.
Roughly over the past two decades, research in
coreference (for the English language) had been
plagued by individually crafted evaluations based
on two central corpora?MUC (Hirschman and
Chinchor, 1997; Chinchor and Sundheim, 2003;
Chinchor, 2001) and ACE (Doddington et al,
2004). Experimental parameters ranged from us-
ing perfect (gold, or key) mentions as input for
purely testing the quality of the entity linking al-
gorithm, to an end-to-end evaluation where pre-
dicted mentions are used. Given the range of
evaluation parameters and disparity between the
annotation standards for the two corpora, it was
very hard to grasp the state of the art for the
task of coreference. This has been expounded in
Stoyanov et al (2009). The activity in this sub-
field of NLP can be gauged by: (i) the contin-
ual addition of corpora manually annotated for
coreference?The OntoNotes corpus (Pradhan et
al., 2007; Weischedel et al, 2011) in the general
domain, as well as the i2b2 (Uzuner et al, 2012)
and THYME (Styler et al, 2014) corpora in the
clinical domain would be a few examples of such
emerging corpora; and (ii) ongoing proposals for
refining the existing metrics to make them more
informative (Holen, 2013; Chen and Ng, 2013).
The CoNLL-2011/2012 shared tasks on corefer-
ence resolution using the OntoNotes corpus (Prad-
han et al, 2011; Pradhan et al, 2012) were an
attempt to standardize the evaluation settings by
providing a benchmark annotated corpus, scorer,
and state-of-the-art system results that would al-
low future systems to compare against them. Fol-
lowing the timely emphasis on end-to-end evalu-
ation, the official track used predicted mentions
and measured performance using five coreference
measures: MUC (Vilain et al, 1995), B
3
(Bagga
and Baldwin, 1998), CEAF
e
(Luo, 2005), CEAF
m
(Luo, 2005), and BLANC (Recasens and Hovy,
2011). The arithmetic mean of the first three was
the task?s final score.
An unfortunate setback to these evaluations had
its root in three issues: (i) the multiple variations
of two of the scoring metrics?B
3
and CEAF?
used by the community to handle predicted men-
tions; (ii) a buggy implementation of the Cai and
Strube (2010) proposal that tried to reconcile these
variations; and (iii) the erroneous computation of
30
the BLANC metric for partitions of predicted men-
tions. Different interpretations as to how to com-
pute B
3
and CEAF scores for coreference systems
when predicted mentions do not perfectly align
with key mentions?which is usually the case?
led to variations of these metrics that manipulate
the gold standard and system output in order to
get a one-to-one mention mapping (Stoyanov et
al., 2009; Cai and Strube, 2010). Some of these
variations arguably produce rather unintuitive re-
sults, while others are not faithful to the original
measures.
In this paper, we address the issues in scor-
ing coreference partitions of predicted mentions.
Specifically, we justify our decision to go back
to the original scoring algorithms by arguing that
manipulation of key or predicted mentions is un-
necessary and could in fact produce unintuitive re-
sults. We demonstrate the use of our recent ex-
tension of BLANC that can seamlessly handle pre-
dicted mentions (Luo et al, 2014). We make avail-
able an open-source, thoroughly-tested reference
implementation of the main coreference evalua-
tion measures that do not involve mention manip-
ulation and is faithful to the original intentions of
the proposers of these metrics. We republish the
CoNLL-2011/2012 results based on this scorer, so
that future systems can use it for evaluation and
have the CoNLL results available for comparison.
The rest of the paper is organized as follows.
Section 2 provides an overview of the variations
of the existing measures. We present our newly
updated coreference scoring package in Section 3
together with the rescored CoNLL-2011/2012 out-
puts. Section 4 walks through a scoring example
for all the measures, and we conclude in Section 5.
2 Variations of Scoring Measures
Two commonly used coreference scoring metrics
?B
3
and CEAF?are underspecified in their ap-
plication for scoring predicted, as opposed to key
mentions. The examples in the papers describing
these metrics assume perfect mentions where pre-
dicted mentions are the same set of mentions as
key mentions. The lack of accompanying refer-
ence implementation for these metrics by its pro-
posers made it harder to fill the gaps in the speci-
fication. Subsequently, different interpretations of
how one can evaluate coreference systems when
predicted mentions do not perfectly align with key
mentions led to variations of these metrics that ma-
nipulate the gold and/or predicted mentions (Stoy-
anov et al, 2009; Cai and Strube, 2010). All these
variations attempted to generate a one-to-one map-
ping between the key and predicted mentions, as-
suming that the original measures cannot be ap-
plied to predicted mentions. Below we first pro-
vide an overview of these variations and then dis-
cuss the unnecessity of this assumption.
Coining the term twinless mentions for those
mentions that are either spurious or missing from
the predicted mention set, Stoyanov et al (2009)
proposed two variations to B
3
? B
3
all
and B
3
0
?to
handle them. In the first variation, all predicted
twinless mentions are retained, whereas the lat-
ter discards them and penalizes recall for twin-
less predicted mentions. Rahman and Ng (2009)
proposed another variation by removing ?all and
only those twinless system mentions that are sin-
gletons before applying B
3
and CEAF.? Follow-
ing upon this line of research, Cai and Strube
(2010) proposed a unified solution for both B
3
and
CEAF
m
, leaving the question of handling CEAF
e
as future work because ?it produces unintuitive
results.? The essence of their solution involves
manipulating twinless key and predicted mentions
by adding them either from the predicted parti-
tion to the key partition or vice versa, depend-
ing on whether one is computing precision or re-
call. The Cai and Strube (2010) variation was used
by the CoNLL-2011/2012 shared tasks on corefer-
ence resolution using the OntoNotes corpus, and
by the i2b2 2011 shared task on coreference res-
olution using an assortment of clinical notes cor-
pora (Uzuner et al, 2012).
1
It was later identified
by Recasens et al (2013) that there was a bug in
the implementation of this variation in the scorer
used for the CoNLL-2011/2012 tasks. We have
not tested the correctness of this variation in the
scoring package used for the i2b2 shared task.
However, it turns out that the CEAF metric (Luo,
2005) was always intended to work seamlessly on
predicted mentions, and so has been the case with
the B
3
metric.
2
In a latter paper, Rahman and Ng
(2011) correctly state that ?CEAF can compare par-
titions with twinless mentions without any modifi-
cation.? We will look at this further in Section 4.3.
We argue that manipulations of key and re-
sponse mentions/entities, as is done in the exist-
ing B
3
variations, not only confound the evalu-
ation process, but are also subject to abuse and
can seriously jeopardize the fidelity of the evalu-
1
Personal communication with Andreea Bodnari, and
contents of the i2b2 scorer code.
2
Personal communication with Breck Baldwin.
31
ation. Given space constraints we use an exam-
ple worked out in Cai and Strube (2010). Let
the key contain an entity with mentions {a, b, c}
and the prediction contain an entity with mentions
{a, b, d}. As detailed in Cai and Strube (2010,
p. 29-30, Tables 1?3), B
3
0
assigns a perfect pre-
cision of 1.00 which is unintuitive as the system
has wrongly predicted a mention d as belonging to
the entity. For the same prediction, B
3
all
assigns a
precision of 0.556. But, if the prediction contains
two entities {a, b, d} and {c} (i.e., the mention c
is added as a spurious singleton), then B
3
all
preci-
sion increases to 0.667 which is counter-intuitive
as it does not penalize the fact that c is erroneously
placed in its own entity. The version illustrated in
Section 4.2, which is devoid of any mention ma-
nipulations, gives a precision of 0.444 in the first
scenario and the precision drops to 0.333 in the
second scenario with the addition of a spurious
singleton entity {c}. This is a more intuitive be-
havior.
Contrary to both B
3
and CEAF, the BLANC mea-
sure (Recasens and Hovy, 2011) was never de-
signed to handle predicted mentions. However, the
implementation used for the SemEval-2010 shared
task as well as the one for the CoNLL-2011/2012
shared tasks accepted predicted mentions as input,
producing undefined results. In Luo et al (2014)
we have extended the BLANC metric to deal with
predicted mentions
3 Reference Implementation
Given the potential unintuitive outcomes of men-
tion manipulation and the misunderstanding that
the original measures could not handle twinless
predicted mentions (Section 2), we redesigned the
CoNLL scorer. The new implementation:
? is faithful to the original measures;
? removes any prior mention manipulation,
which might depend on specific annotation
guidelines among other problems;
? has been thoroughly tested to ensure that it
gives the expected results according to the
original papers, and all test cases are included
as part of the release;
? is free of the reported bugs that the CoNLL
scorer (v4) suffered (Recasens et al, 2013);
? includes the extension of BLANC to handle
predicted mentions (Luo et al, 2014).
This is the open source scoring package
3
that
we present as a reference implementation for the
3
http://code.google.com/p/reference-coreference-scorers/
SYSTEM MD MUC B
3
CEAF BLANC CONLL
m e AVERAGE
F
1
F
1
1
F
2
1
F
1
F
3
1
CoNLL-2011; English
lee 70.7 59.6 48.9 53.0 46.1 48.8 51.5
sapena 68.4 59.5 46.5 51.3 44.0 44.5 50.0
nugues 69.0 58.6 45.0 48.4 40.0 46.0 47.9
chang 64.9 57.2 46.0 50.7 40.0 45.5 47.7
stoyanov 67.8 58.4 40.1 43.3 36.9 34.6 45.1
santos 65.5 56.7 42.9 45.1 35.6 41.3 45.0
song 67.3 60.0 41.4 41.0 33.1 30.9 44.8
sobha 64.8 50.5 39.5 44.2 39.4 36.3 43.1
yang 63.9 52.3 39.4 43.2 35.5 36.1 42.4
charton 64.3 52.5 38.0 42.6 34.5 35.7 41.6
hao 64.3 54.5 37.7 41.9 31.6 37.0 41.3
zhou 62.3 49.0 37.0 40.6 35.0 35.0 40.3
kobdani 61.0 53.5 34.8 38.1 34.1 32.6 38.7
xinxin 61.9 46.6 34.9 37.7 31.7 35.0 37.7
kummerfeld 62.7 42.7 34.2 38.8 35.5 31.0 37.5
zhang 61.1 47.9 34.4 37.8 29.2 35.7 37.2
zhekova 48.3 24.1 23.7 23.4 20.5 15.4 22.8
irwin 26.7 20.0 11.7 18.5 14.7 6.3 15.5
CoNLL-2012; English
fernandes 77.7 70.5 57.6 61.4 53.9 58.8 60.7
martschat 75.2 67.0 54.6 58.8 51.5 55.0 57.7
bjorkelund 75.4 67.6 54.5 58.2 50.2 55.4 57.4
chang 74.3 66.4 53.0 57.1 48.9 53.9 56.1
chen 73.8 63.7 51.8 55.8 48.1 52.9 54.5
chunyang 73.7 63.8 51.2 55.1 47.6 52.7 54.2
stamborg 73.9 65.1 51.7 55.1 46.6 54.4 54.2
yuan 72.5 62.6 50.1 54.5 46.0 52.1 52.9
xu 72.0 66.2 50.3 51.3 41.3 46.5 52.6
shou 73.7 62.9 49.4 53.2 46.7 50.4 53.0
uryupina 70.9 60.9 46.2 49.3 42.9 46.0 50.0
songyang 68.8 59.8 45.9 49.6 42.4 45.1 49.4
zhekova 67.1 53.5 35.7 39.7 32.2 34.8 40.5
xinxin 62.8 48.3 35.7 38.0 31.9 36.5 38.6
li 59.9 50.8 32.3 36.3 25.2 31.9 36.1
CoNLL-2012; Chinese
chen 71.6 62.2 55.7 60.0 55.0 54.1 57.6
yuan 68.2 60.3 52.4 55.8 50.2 43.2 54.3
bjorkelund 66.4 58.6 51.1 54.2 47.6 44.2 52.5
xu 65.2 58.1 49.5 51.9 46.6 38.5 51.4
fernandes 66.1 60.3 49.6 54.4 44.5 49.6 51.5
stamborg 64.0 57.8 47.4 51.6 41.9 45.9 49.0
uryupina 59.0 53.0 41.7 46.9 37.6 41.9 44.1
martschat 58.6 52.4 40.8 46.0 38.2 37.9 43.8
chunyang 61.6 49.8 39.6 44.2 37.3 36.8 42.2
xinxin 55.9 48.1 38.8 42.9 34.5 37.9 40.5
li 51.5 44.7 31.5 36.7 25.3 30.4 33.8
chang 47.6 37.9 28.8 36.1 29.6 25.7 32.1
zhekova 47.3 40.6 28.1 31.4 21.2 22.9 30.0
CoNLL-2012; Arabic
fernandes 64.8 46.5 42.5 49.2 46.5 38.0 45.2
bjorkelund 60.6 47.8 41.6 46.7 41.2 37.9 43.5
uryupina 55.4 41.5 36.1 41.4 35.0 33.0 37.5
stamborg 59.5 41.2 35.9 40.0 32.9 34.5 36.7
chen 59.8 39.0 32.1 34.7 26.0 30.8 32.4
zhekova 41.0 29.9 22.7 31.1 25.9 18.5 26.2
li 29.7 18.1 13.1 21.0 17.3 8.4 16.2
Table 1: Performance on the official, closed track
in percentages using all predicted information for
the CoNLL-2011 and 2012 shared tasks.
community to use. It is written in perl and stems
from the scorer that was initially used for the
SemEval-2010 shared task (Recasens et al, 2010)
and later modified for the CoNLL-2011/2012
shared tasks.
4
Partitioning detected mentions into entities (or
equivalence classes) typically comprises two dis-
tinct tasks: (i) mention detection; and (ii) coref-
erence resolution. A typical two-step coreference
algorithm uses mentions generated by the best
4
We would like to thank Emili Sapena for writing the first
version of the scoring package.
32
a     b
c
de fg
h
a     bc
de
hi i
f    g f    g
h i
cd
a     b
Solid: KeyDashed: Response Solid: KeyDashed: partition wrt Response Solid: Partition wrt KeyDashed: Response
Figure 1: Example key and response entities along
with the partitions for computing the MUC score.
possible mention detection algorithm as input to
the coreference algorithm. Therefore, ideally one
would want to score the two steps independently
of each other. A peculiarity of the OntoNotes
corpus is that singleton referential mentions are
not annotated, thereby preventing the computation
of a mention detection score independently of the
coreference resolution score. In corpora where all
referential mentions (including singletons) are an-
notated, the mention detection score generated by
this implementation is independent of the corefer-
ence resolution score.
We used this reference implementation to
rescore the CoNLL-2011/2012 system outputs for
the official task to enable future comparisons with
these benchmarks. The new CoNLL-2011/2012
results are in Table 1. We found that the over-
all system ranking remained largely unchanged for
both shared tasks, except for some of the lower
ranking systems that changed one or two places.
However, there was a considerable drop in the
magnitude of all B
3
scores owing to the combi-
nation of two things: (i) mention manipulation, as
proposed by Cai and Strube (2010), adds single-
tons to account for twinless mentions; and (ii) the
B
3
metric allows an entity to be used more than
once as pointed out by Luo (2005). This resulted
in a drop in the CoNLL averages (B
3
is one of the
three measures that make the average).
4 An Illustrative Example
This section walks through the process of com-
puting each of the commonly used metrics for
an example where the set of predicted mentions
has some missing key mentions and some spu-
rious mentions. While the mathematical formu-
lae for these metrics can be found in the original
papers (Vilain et al, 1995; Bagga and Baldwin,
1998; Luo, 2005), many misunderstandings dis-
cussed in Section 2 are due to the fact that these
papers lack an example showing how a metric is
computed on predicted mentions. A concrete ex-
ample goes a long way to prevent similar misun-
derstandings in the future. The example is adapted
from Vilain et al (1995) with some slight modifi-
cations so that the total number of mentions in the
key is different from the number of mentions in
the prediction. The key (K) contains two entities
with mentions {a, b, c} and {d, e, f, g} and the re-
sponse (R) contains three entities with mentions
{a, b}; {c, d} and {f, g, h, i}:
K =
K
1
? ?? ?
{a, b, c}
K
2
? ?? ?
{d, e, f, g} (1)
R =
R
1
? ?? ?
{a, b}
R
2
? ?? ?
{c, d}
R
3
? ?? ?
{f, g, h, i}. (2)
Mention e is missing from the response, and men-
tions h and i are spurious in the response. The fol-
lowing sections use R to denote recall and P for
precision.
4.1 MUC
The main step in the MUC scoring is creating the
partitions with respect to the key and response re-
spectively, as shown in Figure 1. Once we have
the partitions, then we compute the MUC score by:
R =
?
N
k
i=1
(|K
i
| ? |p(K
i
)|)
?
N
k
i=1
(|K
i
| ? 1)
=
(3? 2) + (4? 3)
(3? 1) + (4? 1)
= 0.40
P =
?
N
r
i=1
(|R
i
| ? |p
?
(R
i
)|)
?
N
r
i=1
(|R
i
| ? 1)
=
(2? 1) + (2? 2) + (4? 3)
(2? 1) + (2? 1) + (4? 1)
= 0.40,
where K
i
is the i
th
key entity and p(K
i
) is the
set of partitions created by intersecting K
i
with
response entities (cf. the middle sub-figure in Fig-
ure 1); R
i
is the i
th
response entity and p
?
(R
i
) is
the set of partitions created by intersectingR
i
with
key entities (cf. the right-most sub-figure in Fig-
ure 1); and N
k
and N
r
are the number of key and
response entities, respectively.
The MUC F
1
score in this case is 0.40.
4.2 B
3
For computing B
3
recall, each key mention is as-
signed a credit equal to the ratio of the number of
correct mentions in the predicted entity contain-
ing the key mention to the size of the key entity to
which the mention belongs, and the recall is just
33
the sum of credits over all key mentions normal-
ized over the number of key mentions. B
3
preci-
sion is computed similarly, except switching the
role of key and response. Applied to the example:
R =
?
N
k
i=1
?
N
r
j=1
|K
i
?R
j
|
2
|K
i
|
?
N
k
i=1
|K
i
|
=
1
7
? (
2
2
3
+
1
2
3
+
1
2
4
+
2
2
4
) =
1
7
?
35
12
? 0.42
P =
?
N
k
i=1
?
N
r
j=1
|K
i
?R
j
|
2
|R
j
|
?
N
r
i=1
|R
j
|
=
1
8
? (
2
2
2
+
1
2
2
+
1
2
2
+
2
2
4
) =
1
8
?
4
1
= 0.50
Note that terms with 0 value are omitted. The B
3
F
1
score is 0.46.
4.3 CEAF
The first step in the CEAF computation is getting
the best scoring alignment between the key and
response entities. In this case the alignment is
straightforward. Entity R
1
aligns with K
1
and R
3
aligns with K
2
. R
2
remains unaligned.
CEAF
m
CEAF
m
recall is the number of aligned mentions
divided by the number of key mentions, and preci-
sion is the number of aligned mentions divided by
the number of response mentions:
R =
|K
1
? R
1
|+ |K
2
? R
3
|
|K
1
|+ |K
2
|
=
(2 + 2)
(3 + 4)
? 0.57
P =
|K
1
? R
1
|+ |K
2
? R
3
|
|R
1
|+ |R
2
|+ |R
3
|
=
(2 + 2)
(2 + 2 + 4)
= 0.50
The CEAF
m
F
1
score is 0.53.
CEAF
e
We use the same notation as in Luo (2005):
?
4
(K
i
, R
j
) to denote the similarity between a key
entity K
i
and a response entity R
j
. ?
4
(K
i
, R
j
) is
defined as:
?
4
(K
i
, R
j
) =
2? |K
i
? R
j
|
|K
i
|+ |R
j
|
.
CEAF
e
recall and precision, when applied to this
example, are:
R =
?
4
(K
1
, R
1
) + ?
4
(K
2
, R
3
)
N
k
=
(2?2)
(3+2)
+
(2?2)
(4+4)
2
= 0.65
P =
?
4
(K
1
, R
1
) + ?
4
(K
2
, R
3
)
N
r
=
(2?2)
(3+2)
+
(2?2)
(4+4)
3
? 0.43
The CEAF
e
F
1
score is 0.52.
4.4 BLANC
The BLANC metric illustrated here is the one in
our implementation which extends the original
BLANC (Recasens and Hovy, 2011) to predicted
mentions (Luo et al, 2014).
Let C
k
and C
r
be the set of coreference links
in the key and response respectively, and N
k
and
N
r
be the set of non-coreference links in the key
and response respectively. A link between a men-
tion pair m and n is denoted by mn; then for the
example in Figure 1, we have
C
k
= {ab, ac, bc, de, df, dg, ef, eg, fg}
N
k
= {ad, ae, af, ag, bd, be, bf, bg, cd, ce, cf, cg}
C
r
= {ab, cd, fg, fh, fi, gh, gi, hi}
N
r
= {ac, ad, af, ag, ah, ai, bc, bd, bf, bg, bh, bi,
cf, cg, ch, ci, df, dg, dh, di}.
Recall and precision for coreference links are:
R
c
=
|C
k
? C
r
|
|C
k
|
=
2
9
? 0.22
P
c
=
|C
k
? C
r
|
|C
r
|
=
2
8
= 0.25
and the coreference F-measure, F
c
? 0.23. Sim-
ilarly, recall and precision for non-coreference
links are:
R
n
=
|N
k
?N
r
|
|N
k
|
=
8
12
? 0.67
P
n
=
|N
k
?N
r
|
|N
r
|
=
8
20
= 0.40,
and the non-coreference F-measure, F
n
= 0.50.
So the BLANC score is
F
c
+F
n
2
? 0.36.
5 Conclusion
We have cleared several misunderstandings about
coreference evaluation metrics, especially when a
response contains imperfect predicted mentions,
and have argued against mention manipulations
during coreference evaluation. These misunder-
standings are caused partially by the lack of il-
lustrative examples to show how a metric is com-
puted on predicted mentions not aligned perfectly
with key mentions. Therefore, we provide detailed
steps for computing all four metrics on a represen-
tative example. Furthermore, we have a reference
implementation of these metrics that has been rig-
orously tested and has been made available to the
public as open source software. We reported new
scores on the CoNLL 2011 and 2012 data sets,
which can serve as the benchmarks for future re-
search work.
Acknowledgments
This work was partially supported by grants
R01LM10090 from the National Library of
Medicine and IIS-1219142 from the National Sci-
ence Foundation.
34
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
LREC, pages 563?566.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In
Proceedings of SIGDIAL, pages 28?36.
Chen Chen and Vincent Ng. 2013. Linguistically
aware coreference evaluation metrics. In Pro-
ceedings of the Sixth IJCNLP, pages 1366?1374,
Nagoya, Japan, October.
Nancy Chinchor and Beth Sundheim. 2003. Mes-
sage understanding conference (MUC) 6. In
LDC2003T13.
Nancy Chinchor. 2001. Message understanding con-
ference (MUC) 7. In LDC2001T02.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ACE) program-tasks, data, and evaluation. In
Proceedings of LREC.
Lynette Hirschman and Nancy Chinchor. 1997. Coref-
erence task definition (v3.0, 13 jul 97). In Proceed-
ings of the 7th Message Understanding Conference.
Gordana Ilic Holen. 2013. Critical reflections on
evaluation practices in coreference resolution. In
Proceedings of the NAACL-HLT Student Research
Workshop, pages 1?7, Atlanta, Georgia, June.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics, and Speech Recognition. Prentice Hall. Second
Edition.
Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and
Eduard Hovy. 2014. An extension of BLANC to
system mentions. In Proceedings of ACL, Balti-
more, Maryland, June.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of HLT-EMNLP,
pages 25?32.
Sameer Pradhan, Eduard Hovy, Mitchell Marcus,
Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A Unified Rela-
tional Semantic Representation. International Jour-
nal of Semantic Computing, 1(4):405?419.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of CoNLL: Shared Task, pages 1?27.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of CoNLL: Shared Task, pages 1?40.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of
EMNLP, pages 968?977.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of
ACL, pages 814?824.
Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand Index for coreference eval-
uation. Natural Language Engineering, 17(4):485?
510.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M. Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of SemEval,
pages 1?8.
Marta Recasens, Marie-Catherine de Marneffe, and
Chris Potts. 2013. The life and death of discourse
entities: Identifying singleton mentions. In Pro-
ceedings of NAACL-HLT, pages 627?633.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL-IJCNLP, pages
656?664.
William F. Styler, Steven Bethard an Sean Finan,
Martha Palmer, Sameer Pradhan, Piet C de Groen,
Brad Erickson, Timothy Miller, Chen Lin, Guergana
Savova, and James Pustejovsky. 2014. Temporal
annotation in the clinical domain. Transactions of
Computational Linguistics, 2(April):143?154.
Ozlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler
Forbush, John Pestian, and Brett R South. 2012.
Evaluating the state of the art in coreference res-
olution for electronic medical records. Journal of
American Medical Informatics Association, 19(5),
September.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model theo-
retic coreference scoring scheme. In Proceedings of
the 6th Message Understanding Conference, pages
45?52.
Ralph Weischedel, Eduard Hovy, Mitchell Marcus,
Martha Palmer, Robert Belvin, Sameer Pradhan,
Lance Ramshaw, and Nianwen Xue. 2011.
OntoNotes: A large training corpus for enhanced
processing. In Joseph Olive, Caitlin Christian-
son, and John McCary, editors, Handbook of Natu-
ral Language Processing and Machine Translation:
DARPA Global Autonomous Language Exploitation.
Springer.
35
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 56?63,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Combining the Best of Two Worlds:
A Hybrid Approach to Multilingual Coreference Resolution
Chen Chen and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{yzcchen,vince}@hlt.utdallas.edu
Abstract
We describe our system for the CoNLL-2012
shared task, which seeks to model corefer-
ence in OntoNotes for English, Chinese, and
Arabic. We adopt a hybrid approach to
coreference resolution, which combines the
strengths of rule-based methods and learning-
based methods. Our official combined score
over all three languages is 56.35. In particu-
lar, our score on the Chinese test set is the best
among the participating teams.
1 Introduction
TheCoNLL-2012 shared task extends last year's task
on coreference resolution from a monolingual to a
multilingual setting (Pradhan et al, 2012). Unlike
the SemEval-2010 shared task on Coreference Reso-
lution inMultiple Languages (Recasens et al, 2010),
which focuses on coreference resolution in European
languages, the CoNLL shared task is arguably more
challenging: it focuses on three languages that come
from very different language families, namely En-
glish, Chinese, and Arabic.
We designed a system for resolving references in
all three languages. Specifically, we participated
in four tracks: the closed track for all three lan-
guages, and the open track for Chinese. In compari-
son to last year's participating systems, our resolver
has two distinguishing characteristics. First, unlike
last year's resolvers, which adopted either a rule-
based method or a learning-based method, we adopt
a hybrid approach to coreference resolution, attempt-
ing to combine the strengths of both methods. Sec-
ond, while last year's resolvers did not exploit genre-
specific information, we optimize our system's pa-
rameters with respect to each genre.
Our decision to adopt a hybrid approach is mo-
tivated by the observation that rule-based meth-
ods and learning-based methods each have their
unique strengths. As shown by the Stanford coref-
erence resolver (Lee et al, 2011), the winner of
last year's shared task, many coreference relations in
OntoNotes can be identified using a fairly small set
of simple hand-crafted rules. On the other hand, our
prior work on machine learning for coreference res-
olution suggests that coreference-annotated data can
be profitably exploited to (1) induce lexical features
(Rahman and Ng, 2011a, 2011b) and (2) optimize
system parameters with respect to the desired coref-
erence evaluation measure (Ng, 2004, 2009).
Our system employs a fairly standard architecture,
performing mention detection prior to coreference
resolution. As we will see, however, the parameters
of these two components are optimized jointly with
respect to the desired evaluation measure.
In the rest of this paper, we describe the men-
tion detection component (Section 2) and the coref-
erence resolution component (Section 3), show how
their parameters are jointly optimized (Section 4),
and present evaluation results on the development set
and the official test set (Section 5).
2 Mention Detection
To build a mention detector that strikes a relatively
good balance between precision and recall, we em-
ploy a two-step approach. First, in the extrac-
tion step, we identify named entities (NEs) and em-
ploy language-specific heuristics to extract mentions
56
from syntactic parse trees, aiming to increase our up-
per bound on recall as much as possible. Then, in
the pruning step, we aim to improve precision by
employing both language-specific heuristic pruning
and language-independent learning-based pruning.
Section 2.1 describes the language-specific heuris-
tics for extraction and pruning, and Section 2.2 de-
scribes our learning-based pruning method.
2.1 Heuristic Extraction and Pruning
English. During extraction, we create a candidate
mention from a contiguous text span s if (1) s is a
PRP or an NP in a syntactic parse tree; or (2) s cor-
responds to a NE that is not a PERCENT, MONEY,
QUANTITY or CARDINAL. During pruning, we
remove a candidate mentionmk if (1)mk is embed-
ded within a larger mentionmj such thatmj andmk
have the same head, where the head of a mention is
detected using Collins's (1999) rules; (2) mk has a
quantifier or a partitive modifier; or (3) mk is a sin-
gular common NP, with the exception that we retain
mentions related to time (e.g., "today").
Chinese. Similar to English mention extraction,
we create Chinese mentions from all NP and QP
nodes in syntactic parse trees. During pruning, we
remove a candidate mentionmk if (1)mk is embed-
ded within a larger mentionmj such thatmj andmk
have the same head, except if mj and mk appear
in a newswire document since, unlike other docu-
ment annotations, Chinese newswire document an-
notations do consider such pairs coreferent; (2) mk
is a NE that is a PERCENT, MONEY, QUANTITY
and CARDINAL; or (3) mk is an interrogative pro-
noun such as "?? [what]", "?? [where]".
Arabic. We employ as candidate mentions all the
NPs extracted from syntactic parse trees, removing
those that are PERCENT, MONEY, QUANTITY or
CARDINAL.
2.2 Learning-Based Pruning
While the heuristic pruning method identifies can-
didate mentions, it cannot determine which candi-
date mentions are likely to be coreferent. To improve
pruning (and hence the precision of mention detec-
tion), we employ learning-based pruning, where we
employ the training data to identify and subsequently
discard those candidate mentions that are not likely
to be coreferent with other mentions.
Language Recall Precision F-Score
English 88.59 40.56 55.64
Chinese 85.74 42.52 56.85
Arabic 81.49 21.29 33.76
Table 1: Mention detection results on the development set
obtained prior to coreference resolution.
Specifically, for each mention mk in the test set
that survives heuristic pruning, we compute its men-
tion coreference probability, which indicates the
likelihood that the head noun of mk is coreferent
with another mention. If this probability does not
exceed a certain threshold tC , we will remove mk
from the list of candidate mentions. Section 4 dis-
cusses how tC is jointly learned with the parameters
of the coreference resolution component to optimize
the coreference evaluation measure.
We estimate the mention coreference probability
ofmk from the training data. Specifically, since only
non-singleton mentions are annotated in OntoNotes,
we can compute this probability as the number of
times mk 's head noun is annotated (as a gold men-
tion) divided by the total number of timesmk 's head
noun appears. If mk 's head noun does not appear in
the training set, we set its coreference probability to
1, meaning that we let it pass through the filter. In
other words, we try to be conservative and do not
filter any mention for which we cannot compute the
coreference probability.
Table 1 shows the mention detection results of the
three languages on the development set after heuris-
tic extraction and pruning but prior to learning-based
pruning and coreference resolution.
3 Coreference Resolution
Like the mention detection component, our corefer-
ence resolution component employs heuristics and
machine learning. More specifically, we employ
Stanford's multi-pass sieve approach (Lee et al,
2011) for heuristic coreference resolution, but since
most of these sieves are unlexicalized, we seek to im-
prove the multi-pass sieve approach by incorporat-
ing lexical information using machine learning tech-
niques. As we will see below, while different sieves
are employed for different languages, the way we in-
corporate lexical information into the sieve approach
is the same for all languages.
57
3.1 The Multi-Pass Sieve Approach
A sieve is composed of one or more heuristic rules.
Each rule extracts a coreference relation between
two mentions based on one or more conditions. For
example, one rule in Stanford's discourse processing
sieve posits two mentions as coreferent if two con-
ditions are satisfied: (1) they are both pronouns; and
(2) they are produced by the same speaker.
Sieves are ordered by their precision, with the
most precise sieve appearing first. To resolve a set
of mentions in a document, the resolver makes mul-
tiple passes over them: in the i-th pass, it attempts
to use only the rules in the i-th sieve to find an an-
tecedent for each mention mk. Specifically, when
searching for an antecedent formk, its candidate an-
tecedents are visited in an order determined by their
positions in the associated parse tree (Haghighi and
Klein, 2009). The partial clustering of the mentions
created in the i-th pass is then passed to the i+1-th
pass. Hence, later passes can exploit the informa-
tion computed by previous passes, but a coreference
link established earlier cannot be overridden later.
3.2 The Sieves
3.2.1 Sieves for English
Our sieves for English are modeled after those em-
ployed by the Stanford resolver (Lee et al, 2011),
which is composed of 12 sieves.1 Since we partic-
ipated in the closed track, we re-implemented the
10 sieves that do not exploit external knowledge
sources. These 10 sieves are listed under the "En-
glish" column in Table 2. Specifically, we leave out
the Alias sieve and the Lexical Chain sieve, which
compute semantic similarity using information ex-
tracted from WordNet, Wikipedia, and Freebase.
3.2.2 Sieves for Chinese
Recall that for Chinese we participated in both the
closed track and the open track. The sieves we em-
ploy for both tracks are the same, except that we use
NE information to improve some of the sieves in the
system for the open track.2 To obtain automatic NE
annotations, we employ a NE model that we trained
on the gold NE annotations in the training data.
1Table 1 of Lee et al's (2011) paper listed 13 sieves, but one
of them was used for mention detection.
2Note that the use of NEs puts a Chinese resolver in the open
track.
English Chinese
Discourse Processing Chinese Head Match
Exact String Match Discourse Processing
Relaxed String Match Exacth String Match
Precise Constructs Precise Constructs
Strict Head Match A?C Strict Head Match A?C
Proper Head Match Proper Head Match
Relaxed Head Match Pronouns
Pronouns --
Table 2: Sieves for English and Chinese (listed in the or-
der in which they are applied).
The Chinese resolver is composed of 9 sieves,
as shown under the "Chinese" column of Table 2.
These sieves are implemented in essentially the same
way as their English counterparts except for a few
of them, which are modified in order to account for
some characteristics specific to Chinese or the Chi-
nese coreference annotations. As described in de-
tail below, we introduce a new sieve, the Chinese
Head Match sieve, and modify two existing sieves,
the Precise Constructs sieve, and the Pronoun sieve.
1. Chinese Head Match sieve: Recall from Sec-
tion 2 that the Chinese newswire articles were
coreference-annotated in such away that amen-
tion and its embedding mention can be coref-
erent if they have the same head. To iden-
tify these coreference relations, we employ the
Same Head sieve, which posits two mentions
mj and mk as coreferent if they have the same
head and mk is embedded within mj . There is
an exception to this rule, however: if mj is a
coordinated NP composed of two or more base
NPs, and mk is just one of these base NPs, the
two mentions will not be considered coreferent
(e.g., ??????? [Charles and Diana]
and??? [Diana]).
2. Precise Constructs sieve: Recall from Lee
et al (2011) that the Precise Constructs sieve
posits two mentions as coreferent based on in-
formation such as whether one is an acronym of
the other and whether they form an appositive
or copular construction. We incorporate addi-
tional rules to this sieve to handle specific cases
of abbreviations in Chinese: (a) Abbreviation
of foreign person names, e.g., ??????
? [Saddam Hussein] and ??? [Saddam].
(b) Abbreviation of Chinese person names, e.g.,
58
??? [Chen President] and ?????
[Chen Shui-bian President]. (c) Abbreviation
of country names, e.g, ?? [Do country] and
???? [Dominica].
3. Pronouns sieve: The Pronouns sieve resolves
pronouns by exploiting grammatical informa-
tion such as the gender and number of a men-
tion. While such grammatical information is
provided to the participants for English, the
same is not true for Chinese.
To obtain such grammatical information for
Chinese, we employ a simple method, which
consists of three steps.
First, we employ simple heuristics to extract
grammatical information from those Chinese
NPs for which such information can be easily
inferred. For example, we can heuristically de-
termine that the gender, number and animacy
for ? [she] is {Female, Single and Animate};
and for?? [they] is {Unknown, Plural, Inani-
mate}. In addition, we can determine the gram-
matical attributes of a mention by its named
entity information. For example, a PERSON
can be assigned the grammatical attributes {Un-
known, Single, Animate}.
Next, we bootstrap from these mentions with
heuristically determined grammatical attribute
values. This is done based on the observation
that all mentions in the same coreference chain
should agree in gender, number, and animacy.
Specifically, given a training text, if one of the
mentions in a coreference chain is heuristically
labeled with grammatical information, we au-
tomatically annotate all the remaining mentions
with the same grammatical attribute values.
Finally, we automatically create six word lists,
containing (1) animate words, (2) inanimate
words, (3) male words, (4) female words, (5)
singular words, and (6) plural words. Specif-
ically, we populate these word lists with the
grammatically annotated mentions from the
previous step, where each element of a word
list is composed of the head of a mention and a
count indicating the number of times the men-
tion is annotated with the corresponding gram-
matical attribute value.
We can then apply these word lists to determine
the grammatical attribute values of mentions in
a test text. Due to the small size of these word
lists, and with the goal of improving precision,
we consider two mentions to be grammatically
incompatible if for one of these three attributes,
onemention has anUnknown value whereas the
other has a known value.
As seen in Table 2, our Chinese resolver does
not have the Relaxed String Match sieve, unlike its
English counterpart. Recall that this sieve marks
two mentions as coreferent if the strings after drop-
ping the text following their head words are identical
(e.g.,MichaelWolf, andMichaelWolf, a contributing
editor for "New York"). Since person names in Chi-
nese are almost always composed of a single word
and that heads are seldom followed by other words
in Chinese, we believe that Relaxed HeadMatch will
not help identify Chinese coreference relations. As
noted before, cases of Chinese person name abbrevi-
ation will be handled by the Precise Constructs sieve.
3.2.3 Sieves for Arabic
We only employ one sieve for Arabic, the exact
match sieve. While we experimented with additional
sieves such as the Head Match sieve and the Pro-
nouns sieve, we ended up not employing them be-
cause they do not yield better results.
3.3 Incorporating Lexical Information
Asmentioned before, we improve the sieve approach
by incorporating lexical information.
To exploit lexical information, we first compute
lexical probabilities. Specifically, for each pair of
mentions mj and mk in a test text, we first com-
pute two probabilities: (1) the string-pair probability
(SP-Prob), which is the probability that the strings
of the two mentions, sj and sk, are coreferent; and
(2) the head-pair probability (HP-Prob), which is the
probability that the head nouns of the two mentions,
hj and hk, are coreferent. For better probability esti-
mation, we preprocess the training data and the two
mentions by (1) downcasing (but not stemming) each
English word, and (2) replacing each Arabic word w
by a string formed by concatenating w with its lem-
matized form, its Buckwalter form, and its vocalized
Buckwalter form. Note that SP-Prob(mj ,mk) (HP-
59
Prob(mj ,mk)) is undefined if one or both of sj (hj)
and sk (hk) do not appear in the training set.
Next, we exploit these lexical probabilities to im-
prove the resolution of mj and mk by presenting
two extensions to the sieve approach. The first ex-
tension aims to improve the precision of the sieve
approach. Specifically, before applying any sieve,
we check whether SP-Prob(mj ,mk) ? tSPL or HP-
Prob(mj ,mk)? tHPL for some thresholds tSPL and
tHPL. If so, our resolver will bypass all of the
sieves and simply posit mj and mk as not corefer-
ent. In essence, we use the lexical probabilities to
improve precision, specifically by positing twomen-
tions as not coreferent if there is "sufficient" infor-
mation in the training data for us to make this de-
cision. Note that if one of the lexical probabilities
(say SP-Prob(mj ,mk)) is undefined, we only check
whether the condition on the other probability (in this
case HP(mj ,mk) ? tHPL) is satisfied. If both of
them are undefined, this pair of mentions will sur-
vive this filter and be processed by the sieve pipeline.
The second extension, on the other hand, aims to
improve recall. Specifically, we create a new sieve,
the Lexical Pair sieve, which we add to the end of
the sieve pipeline and which posits two mentionsmj
and mk as coreferent if SP-Prob(mj ,mk) ? tSPU
or HP-Prob(mj ,mk) ? tHPU . In essence, we use
the lexical probabilities to improve recall, specifi-
cally by positing two mentions as coreferent if there
is "sufficient" information in the training data for
us to make this decision. Similar to the first ex-
tension, if one of the lexical probabilities (say SP-
Prob(mj ,mk)) is undefined, we only check whether
the condition on the other probability (in this case
HP(mj ,mk) ? tHPU ) is satisfied. If both of them
are undefined, the Lexical Pair sieve will not process
this pair of mentions.
The four thresholds, tSPL, tHPL, tSPU , and
tHPU , will be tuned to optimize coreference perfor-
mance on the development set.
4 Parameter Estimation
As discussed before, we learn the system parameters
to optimize coreference performance (which, for the
shared task, is Uavg, the unweighted average of the
three commonly-used evaluation measures, MUC,
B3, and CEAFe) on the development set. Our sys-
tem has two sets of tunable parameters. So far, we
have seen one set of parameters, namely the five lex-
ical probability thresholds, tC , tSPL, tHPL, tSPU ,
and tHPU . The second set of parameters contains the
rule relaxation parameters. Recall that each rule in
a sieve may be composed of one or more conditions.
We associate with condition i a parameter ?i, which
is a binary value that controls whether condition i
should be removed or not. In particular, if ?i=0, con-
dition iwill be dropped from the corresponding rule.
The motivation behind having the rule relaxation pa-
rameters should be clear: they allow us to optimize
the hand-crafted rules using machine learning. This
section presents two algorithms for tuning these two
sets of parameters on the development set.
Before discussing the parameter estimation algo-
rithms, recall from the introduction that one of the
distinguishing features of our approach is that we
build genre-specific resolvers. In other words, for
each genre of each language, we (1) learn the lexi-
cal probabilities from the corresponding training set;
(2) obtain optimal parameter values ?1 and ?2 for
the development set using parameter estimation al-
gorithms 1 and 2 respectively; and (3) among?1 and
?2, take the one that yields better performance on
the development set to be the final set of parameter
estimates for the resolver.
Parameter estimation algorithm 1. This algo-
rithm learns the two sets of parameters in a sequential
fashion. Specifically, it first tunes the lexical proba-
bility thresholds, assuming that all the rule relaxation
parameters are set to one. To tune the five probabil-
ity thresholds, we try all possible combinations of
the five probability thresholds and select the combi-
nation that yields the best performance on the devel-
opment set. To ensure computational tractability, we
allow each threshold to have the following possible
values. For tC , the possible values are?0.1, 0, 0.05,
0.1, . . ., 0.3; for tSPL and tHPL, the possible values
are ?0.1, 0, 0.05, 0.15, . . ., 0.45; and for tSPU and
tHPU , the possible values are 0.55, 0.65, . . ., 0.95,
1.0 and 1.1. Note that the two threshold values?0.1
and 1.1 render a probability threshold useless. For
example, if tC = ?0.1, that means all mentions will
survive learning-based pruning in the mention detec-
tion component. As another example, if tSPU and
tHPU are both 1.1, it means that the String Pair sieve
60
will be useless because it will not posit any pair of
mentions as coreferent.
Given the optimal set of probability thresholds, we
tune the rule relaxation parameters. To do so, we ap-
ply the backward elimination feature selection algo-
rithm, viewing each condition as a feature that can be
removed from the "feature set". Specifically, all the
parameters are initially set to one, meaning that all
the conditions are initially present. In each iteration
of backward elimination, we identify the condition
whose removal yields the highest score on the de-
velopment set and remove it from the feature set. We
repeat this process until all conditions are removed,
and identify the subset of the conditions that yields
the best score on the development set.
Parameter estimation algorithm 2. In this algo-
rithm, we estimate the two sets of parameters in an
interleaved, iterative fashion, where in each itera-
tion, we optimize exactly one parameter from one
of the two sets. More specifically, (1) in iteration
2n, we optimize the (n mod 5)-th lexical probabil-
ity threshold while keeping the remaining parame-
ters constant; and (2) in iteration 2n+1, we optimize
the (n mod m)-th rule relaxation parameter while
keeping the remaining parameters constant, where
n = 1, 2, . . ., and m is the number of rule relax-
ation parameters. When optimizing a parameter in a
given iteration, the algorithm selects the value that,
when used in combination with the current values of
the remaining parameters, optimizes theUavg value
on the development set. We begin the algorithm by
initializing all the rule relaxation parameters to one;
tC , tSPL and tHPL to ?0.1; and tSPU and tHPU
to 1.1. This parameter initialization is equivalent to
the configuration where we employ all and only the
hand-crafted rules as sieves and do not apply learn-
ing to perform any sort of optimization at all.
5 Results and Discussion
The results of our Full coreference resolver on the
development set with optimal parameter values are
shown in Table 3. As we can see, both the men-
tion detection results and the coreference results (ob-
tained via MUC, B3, and CEAFe) are expressed in
terms of recall (R), precision (P), and F-measure (F).
In addition, to better understand the role played by
the two sets of system parameters, we performed ab-
lation experiments, showing for each language-track
combination the results obtained without tuning (1)
the rule relaxation parameters (? ?i's); (2) the proba-
bility thresholds (? tj 's); and (3) any of these param-
eters (? ?i's & tj). Note that (1) we do not have any
rule relaxation parameters for the Arabic resolver
owing to its simplicity; and (2) for comparison pur-
poses, we show the results of the Stanford resolver
for English in the row labeled "Lee et al (2011)".
A few points regarding the results in Table 3 de-
serve mention. First, these mention detection re-
sults are different from those shown in Table 1: here,
the scores are computed over the mentions that ap-
pear in the non-singleton clusters in the coreference
partitions produced by a resolver. Second, our re-
implementation of the Stanford resolver is as good
as the original one. Third, parameter tuning is com-
paratively less effective for Chinese, presumably be-
cause we spent more time on engineering the sieves
for Chinese than for the other languages. Fourth,
our score on Arabic is the lowest among the three
languages, primarily because Arabic is highly inflec-
tional and we have little linguistic knowledge of the
language to design effective sieves. Finally, these
results and our official test set results (Table 4), as
well as our supplementary evaluation results on the
test set obtained using gold mention boundaries (Ta-
ble 5) and gold mentions (Table 6), exhibit similar
performance trends.
Table 7 shows the optimal parameter values ob-
tained for the Full resolver on the development set.
Since there are multiple genres for English and Chi-
nese, we show in the table the probability thresholds
averaged over all the genres and the corresponding
standard deviation values. For the rule relaxation
parameters, among the 36 conditions in the English
sieves and the 61 conditions in the Chinese sieves,
we show the number of conditions being removed
(when averaged over all the genres) and the corre-
sponding standard deviation values. Overall, differ-
ent conditions were removed for different genres.
To get a better sense of the usefulness of
the probability thresholds, we show in Tables 8
and 9 some development set examples of cor-
rectly and incorrectly identified/pruned mentions
and coreferent/non-coreferent pairs for English and
Chinese, respectively. Note that no Chinese exam-
ples for tC are shown, since its tuned value cor-
61
Mention Detect. MUC B-CUBED CEAFe Avg
Language Track System R P F R P F R P F R P F F
English Closed Full 74.8 75.6 75.2 65.6 67.3 66.4 69.1 74.7 71.8 49.8 47.9 48.8 62.3
? ?i 's 75.2 73.4 74.3 64.6 65.8 65.2 68.5 74.1 71.2 48.8 47.6 48.2 61.5
? tj 's 76.4 73.0 74.7 65.1 65.3 65.2 68.6 73.8 71.1 48.6 48.3 48.4 61.6
? ?i 's & tj 's 75.2 72.8 74.0 64.2 64.8 64.5 68.0 73.4 70.6 47.8 47.1 47.5 60.8
Lee et al (2011) 74.1 72.5 73.3 64.3 64.9 64.6 68.2 73.1 70.6 47.0 46.3 46.7 60.6
Chinese Closed Full 72.2 72.7 72.4 62.4 65.8 64.1 70.8 77.7 74.1 52.3 48.9 50.5 62.9
? ?i 's 71.3 72.8 71.9 61.8 66.7 64.2 70.2 78.2 74.0 52.2 47.6 49.9 62.6
? tj 's 72.7 71.1 71.9 62.3 64.8 63.5 70.7 77.1 73.8 51.2 48.8 50.0 62.4
? ?i 's & tj 's 71.7 71.4 71.5 61.5 65.1 63.3 70.0 77.6 73.6 51.3 47.9 49.5 62.1
Chinese Open Full 73.1 72.6 72.9 63.5 67.2 65.3 71.6 78.2 74.8 52.5 48.9 50.7 63.6
? ?i 's 72.5 73.1 72.8 63.2 67.0 65.1 71.3 78.1 74.5 52.4 48.7 50.4 63.3
? tj 's 72.8 72.5 72.7 63.5 66.5 65.0 71.4 77.8 74.5 51.9 48.9 50.4 63.3
? ?i 's & tj 's 72.4 72.5 72.4 63.0 66.3 64.6 71.0 77.8 74.3 51.7 48.5 50.1 63.0
Arabic Closed Full 56.6 64.5 60.3 40.4 42.8 41.6 58.9 62.7 60.7 40.4 37.8 39.1 47.1
? tj 's 52.0 64.3 57.5 33.1 40.2 36.3 53.4 67.9 59.8 41.9 34.2 37.6 44.6
Table 3: Results on the development set with optimal parameter values.
Mention Detect. MUC B-CUBED CEAFe Avg
Language Track System R P F R P F R P F R P F F
English Closed Full 75.1 72.6 73.8 63.5 64.0 63.7 66.6 71.5 69.0 46.7 46.2 46.4 59.7
Chinese Closed Full 71.1 72.1 71.6 59.9 64.7 62.2 69.7 77.8 73.6 53.4 48.7 51.0 62.2
Chinese Closed Full 71.5 73.5 72.4 62.5 67.1 64.7 71.2 78.4 74.6 53.6 49.1 51.3 63.5
Arabic Closed Full 56.2 64.0 59.8 38.1 40.0 39.0 60.6 62.5 61.5 41.9 39.8 40.8 47.1
Table 4: Official results on the test set.
Mention Detect. MUC B-CUBED CEAFe Avg
Language Track System R P F R P F R P F R P F F
English Closed Full 74.8 75.7 75.2 63.3 66.8 65.0 65.4 73.6 69.2 48.8 44.9 46.8 60.3
Chinese Closed Full 82.0 79.0 80.5 70.8 72.1 71.4 74.4 79.9 77.0 58.0 56.4 57.2 68.6
Chinese Open Full 82.4 80.1 81.2 73.5 74.3 73.9 76.3 80.5 78.3 58.2 57.3 57.8 70.0
Arabic Closed Full 57.2 62.6 59.8 38.7 39.2 39.0 61.5 61.8 61.7 41.6 40.9 41.2 47.3
Table 5: Supplementary results on the test set obtained using gold mention boundaries and predicted parse trees.
Mention Detect. MUC B-CUBED CEAFe Avg
Language Track System R P F R P F R P F R P F F
English Closed Full 80.8 100 89.4 72.3 89.4 79.9 64.6 85.9 73.8 76.3 46.4 57.7 70.5
Chinese Closed Full 84.7 100 91.7 76.6 92.4 83.8 73.0 91.4 81.2 83.6 57.9 68.4 77.8
Chinese Open Full 84.8 100 91.8 78.1 93.2 85.0 75.0 91.6 82.5 84.0 59.2 69.4 79.0
Arabic Closed Full 58.3 100 73.7 41.7 63.2 50.3 50.0 75.3 60.1 64.6 36.2 46.4 52.3
Table 6: Supplementary results on the test set obtained using gold mentions and predicted parse trees.
tC tHPL tSPL tHPU tSPU Rule Relaxation
Language Track Avg. St.Dev. Avg. St.Dev. Avg. St.Dev. Avg. St.Dev. Avg. St.Dev. Avg. St.Dev.
English Closed ?0.06 0.11 ?0.04 0.08 ?0.06 0.12 0.90 0.23 0.60 0.05 6.13 1.55
Chinese Closed ?0.10 0.00 ?0.08 0.06 0.00 0.95 1.01 0.22 0.88 0.27 4.67 1.63
Chinese Open ?0.10 0.00 ?0.08 0.06 ?0.05 0.05 1.01 0.22 0.88 0.27 5.83 1.94
Arabic Closed 0.05 0.00 0.00 0.00 ?0.10 0.00 1.10 0.00 0.15 0.00 0.00 0.00
Table 7: Optimal parameter values.
responds to the case where no mentions should be
pruned.
6 Conclusion
We presented a multilingual coreference resolver de-
signed for the CoNLL-2012 shared task. We adopted
62
Parameter Correct Incorrect
tC no problem; the same that; that idea
tHPL (people,that); (both of you,that) (ours,they); (both of you,us)
tSPL (first,first); (the previous year,its) (China,its); (Taiwan,its)
tHPU (The movie's,the film); (Firestone,the company's) (himself,he); (My,I)
tSPU (Barak,the Israeli Prime Minister); (she,the woman); (Taiwan,the island)
(Kostunica,the new Yugoslav President)
Table 8: Examples of correctly & incorrectly identified/pruned English mentions and coreferent/non-coreferent pairs.
Parameter Correct Incorrect
tC --- ---
tHPL (????,??); (????,?) (?????,??); (??,?)
tSPL (??,??); (???,???) (??,??); (??,?)
tHPU (??,????); (??,???) (???,??); (??,?)
tSPU (??,????); (??,??); (??,?) ; (????,??)
Table 9: Examples of correctly & incorrectly identified/pruned Chinese mentions and coreferent/non-coreferent pairs.
a hybrid approach to coreference resolution, which
combined the advantages of rule-based methods and
learning-based methods. Specifically, we proposed
two extensions to Stanford's multi-pass sieve ap-
proach, which involved the incorporation of lexical
information using machine learning and the acqui-
sition of genre-specific resolvers. Experimental re-
sults demonstrated the effectiveness of these exten-
sions, whether or not they were applied in isolation
or in combination.
In future work, we plan to explore other ways
to combine rule-based methods and learning-based
methods for coreference resolution, as well as im-
prove the performance of our resolver on Arabic.
Acknowledgments
We thank the two anonymous reviewers for their
comments on the paper. This work was supported in
part by NSF Grants IIS-0812261 and IIS-1147644.
References
Michael John Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 1152-
-1161.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford's multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28--34.
Vincent Ng. 2004. Learning noun phrase anaphoricity
to improve conference resolution: Issues in represen-
tation and optimization. In Proceedings of the 42nd
Meeting of the Association for Computational Linguis-
tics, pages 151--158.
Vincent Ng. 2009. Graph-cut-based anaphoricity deter-
mination for coreference resolution. In Proceedings of
the 2009 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 575--583.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes, In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning.
Altaf Rahman and Vincent Ng. 2011a. Coreference reso-
lution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 814--824.
Altaf Rahman and Vincent Ng. 2011b. Narrowing the
modeling gap: A cluster-ranking approach to corefer-
ence resolution. Journal of Artificial Intelligence Re-
search, 40:469--521.
Marta Recasens, Llu?s M?rquez, Emili Sapena,
M. Ant?nia Mart?, Mariona Taul?, V?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 1--8.
63
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 152?156,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Simple Yet Powerful Native Language Identification on TOEFL11 
 
 
Ching-Yi Wu Po-Hsiang Lai Yang Liu     Vincent Ng 
University of Texas at Dallas Emerging Technology Lab Samsung R&D - Dallas University of Texas at Dallas 
800 W Campbell Rd 1301 Lookout Drive 800 W Campbell Rd 
Richardson, TX, USA Plano, TX, USA Richardson, TX, USA 
cxw120631@utdallas.edu s.lai@samsung.com yangl@hlt.utdallas.edu 
vince@hlt.utdallas.edu 
 
 
 
 
 
 
Abstract 
Native language identification (NLI) is the 
task to determine the native language of the 
author based on an essay written in a second 
language.  NLI is often treated as a classifica-
tion problem.  In this paper, we use the 
TOEFL11 data set which consists of more 
data, in terms of the amount of essays and 
languages, and less biased across prompts, i.e., 
topics, of essays.  We demonstrate that even 
using word level n-grams as features, and sup-
port vector machine (SVM) as a classifier can 
yield nearly 80% accuracy. We observe that 
the accuracy of a binary-based word level n-
gram representation (~80%) is much better 
than the performance of a frequency-based 
word level n-gram representation (~20%).  
Notably, comparable results can be achieved 
without removing punctuation marks, suggest-
ing a very simple baseline system for NLI. 
1 Introduction 
Native language identification (NLI) is an emerg-
ing field in the natural language processing com-
munity and machine learning community (Koppel 
et al, 2005; Blanchard et al, 2013). It is a task to 
identify the native language (L1) of an author 
based on his/her texts written in a second language.  
The application of NLI can bring many benefits, 
such as providing a learner adaptive feedback of 
their writing errors based on the native language 
for educational purposes (Koppel et al, 2005; 
Blanchard et al, 2013).  
NLI can be viewed as a classification problem.  
In a classification problem, a classifier is first 
trained using a set of training examples.  Each 
training example is represented as a set of features, 
along with a class label.  After a classifier is 
trained, the classifier is evaluated using a testing 
set (Murphy, 2012). Good data representation often 
yields a better classification performance (Murphy, 
2012).  Often time, the simpler representations 
might produce better performance.  In this work, 
we demonstrate that a binary-based word level n-
gram representation yields much better perform-
ance than a frequency-based word level n-gram 
representation.  In addition, we observed that re-
moving punctuation marks in an essay does not 
make too much difference in a classification per-
formance. 
The contributions of this paper are to demon-
strate the usefulness of a binary-based word level 
n-gram representation, and a very simple baseline 
system without the need of removing punctuation 
marks and stop words. 
This paper is organized as the following.  In 
Section 2, we present related literatures.  
TOEFL11 data set is introduced in Section 3.  In 
Section 4, our features and system design are de-
scribed.  The results are presented in Section 5, 
followed by conclusion in Section 6. 
 
 
152
2 Related Work 
The work by Koppel et al (2005) is the first study 
to investigate native language identification.  They 
use the International Corpus of Learner English 
(ICLE).  They set up this task as a classification 
problem studied in machine learning community.  
They use three types of features: function words, 
character n-gram, errors and idiosyncrasies, e.g. 
spelling and grammatical errors.   For errors and 
idiosyncrasies, they used Microsoft Office Word to 
detect those errors.  Their features were evaluated 
on a subset of the ICLE corpus, including essays 
sampled from five native languages (Russian, 
Czech, Bulgarian, French and Spanish) with 10-
fold cross validation.  They achieve an accuracy of 
80.2% by combining all of the features and using a 
support vector machine as the classification algo-
rithm. In addition, Tsur and Rappoport (2007) 
show that using character n-gram only on the ICLE 
can yield an accuracy of 66%.   
The work from Kochmar (2011) identifies an 
author?s native language using error analysis.  She 
suggests that writers with different native lan-
guages generate different grammatical error pat-
terns. Instead of using ICLE, this work uses a 
different corpus, English learner essays from the 
Cambridge Learner Corpus. She uses SVM on 
manually annotated spelling and grammatical er-
rors along with lexical features. 
Most of the systems described in NLI literature 
reach good performance in predicting an author?s 
native language, using character n-gram and part of 
speech n-gram as features (Blanchard et al, 2013).  
In recent years, various studies have started to look 
into complex features in order to improve the per-
formance.  Wong and Dras (2009) use contrastive 
analysis, a systematic analysis of structural simi-
larities and differences in a pair of languages.  A 
writer?s native language influences the target lan-
guage they aim to learn. They explore the impact 
of three English as Second Language (ESL) error 
types, subject-verb disagreement, noun-number 
disagreement and determiner errors, and use a sub-
set of ICLE with 7 languages.   However, although 
the determiner error feature seems useful, when it 
is combined with a baseline model of lexical fea-
tures, the classification performance is not signifi-
cantly improved (Wong and Dras, 2009). 
Wong and Dras (2011) use complex features 
such as production rules from two parsers and 
reranking features into the classification frame-
work, incorporating lexical features of Koppel et al 
(2005).  They achieve a classification performance 
of 81.71% on the 7-native-languages NLI, slightly 
better than 80.2% accuracy of the original Koppel 
et al (2005). 
Note that although the International Corpus of 
Learner English (ICLE) is used in most of the NLI 
studies, ICLE has been known to have fewer es-
says, and a skewed distribution toward topics of 
essays (Blanchard et al, 2013).  In addition, even 
though there are 16 native languages in ICLE, as 
each language has different numbers of essays, 
most work often uses different subsets of 7 native 
languages, which makes comparison harder across 
different studies (Blanchard et al, 2013). The NLI 
shared task 2013 provides a new data set, namely 
the TOEFL11 (Blanchard et al, 2013), which ad-
dresses these issues.  As previously discussed, 
complex features do not necessarily improve clas-
sification accuracy.  In this work, we use 
TOEFL11 to investigate the classification per-
formance using simple word n-gram based features.  
3 Data  
In this work, we use TOEFL11 as our corpus.  
TOEFL11 is a new data set for NLI (Blanchard et 
al., 2013). There are 11 native languages, including 
Arabic (ARA), Chinese (CHI), French (French), 
German (GER), Hindi (HIN), Italian (ITA), Japa-
nese (JPN), Korean (KOR), Spanish (SPA), Telugu 
(TEL), and Turkish (TUR).  Authors write essays 
based on 8 different topics in English.  There are 
1,100 essays for each language, and sampled from 
8 different topics, i.e., prompts.    Each essay is 
also annotated with an English proficiency level 
(low/medium/high) determined by assessment spe-
cialists.  Among 12,100 essays, there are 9,900 
essays in the training set, 1,100 essays in the de-
velopment set, i.e., validation set in machine learn-
ing, and 1,100 essays in the testing set.  In the 
training set and the development set, there are 
equal numbers of essays from each of the 11 native 
languages. By using TOEFL11, it makes our 
analysis less biased toward a specific topic of es-
says (Blanchard et al, 2013).  
 
 
 
153
4 NIL System Design 
In this section, we describe our NLI system, the 
features, and the classifier we use. 
4.1 Data Preprocessing 
Each essay is tokenized, and then capitalizations 
are removed.  Note that we did not remove English 
stop words, which might be useful to discriminate 
the native language for a writer.  For example, 
function words, which belong to stop words, such 
as ?the?, ?at?, ?which?, have been proven to be ef-
fective to distinguish native language for writers 
(Koppel et al, 2005).  There are two settings: ei-
ther punctuation marks are removed or kept.   
When punctuation marks are kept, they are viewed 
the same as word in constructing n-grams.  For 
example, in the sentence ?NLI is fun.?, ?fun .? is 
viewed as a bigram. 
4.2 Features 
In our system, word level n-grams are used to rep-
resent an essay.  Previous studies have shown that 
word level n-grams are useful in determining the 
native language of a writer (Bykh and Meurers, 
2012).  One reasonable hypothesis is that non-
native English writers with the same native lan-
guages tend to choose more similar words to ex-
press the same or similar concepts.  In addition, the 
combination of a sequence of words might also be 
affected by the different native language of writers.  
Therefore, word n-gram is useful to distinguish the 
native language of a writer.  Even though some 
previous studies have looked into using word level 
n-grams as features, how to use word level n-
grams has not been explored too much yet on 
TOEFL11 corpus.  To our knowledge, the most 
recent study by Blanchard et al (2013) started to 
research the effect of different forms of word level 
n-gram representations. 
There could be many ways to represent an essay 
by word level n-grams.  One possible representa-
tion of an essay is to use the frequency of a spe-
cific word n-gram, i.e., the number of times a 
specific word n-gram appears in an essay divided 
by the number of times all word n-grams appear in 
an essay.  In this representation, an essay is a vec-
tor whose elements are the frequency of different 
word n-grams in the essay.  Another possible rep-
resentation is to use binary representation, i.e., 1 
indicates this word n-gram is in this essay, 0 indi-
cates this word n-gram is not in this essay.  One 
interesting question to ask is:  
Which representation can be more informative 
to distinguish the native language of writers of es-
says? 
 Here we compare the performance of a fre-
quency-based word level n-gram representation 
and a binary-based word level n-gram representa-
tion. We included all word level n-grams in the 
training set, without any frequency cutoff.  For 
both binary-based and frequency-based representa-
tions, we run the experiments on the two settings:  
punctuation marks are either removed or kept. 
In addition to word level n-grams, since 
TOEFL11 also consists of English proficiency lev-
els evaluated by assessment experts, we also in-
cluded it to test whether this feature might improve 
the classification performance.  All of the features 
used in our system are summarized in Table 1.  
Besides each feature described above, we have also 
combined different features to test whether various 
combinations of features might improve the accu-
racy performance.  Here, we simply aggregated 
different features, for example, all word level uni-
grams, combined with all word level bigrams. 
4.3 Classifier 
Previous literatures have used various methods 
such as Na?ve Bayse, logistic regression and sup-
port vector machine on NLI problem.  As it has 
been shown that when representing an essay in 
order to perform a classification task, it often re-
sults in an essay being represented in a very high 
dimensional space.  Since support vector machine 
(SVM) is known to be adaptive when the feature 
dimension is high, we chose SVM as our classifi-
cation algorithm.   We also compared the results 
from Na?ve Bayse for an experimental purpose and 
found that SVM is better. We use SVM-Light for 
our system (Joachims, 1999).  We then train our 
SVM classifier on the training set (n=9900), and 
test the trained classifier on the testing set 
(n=1100). 
 
 
 
 
 
 
154
5 Results and Discussions 
5.1 Results 
Table 1 and Table 2 show the accuracies on the 
testing set for the different feature sets, when punc-
tuation marks are removed or kept respectively.  
As the results demonstrated, the accuracies of word 
level bigram are better than unigram using a bi-
nary-based representation.  When combining word 
level unigram and bigram, the accuracy is im-
proved in a binary-based representation.  This is 
consistent when punctuations are either removed or 
kept.  This observation is consistent with the exist-
ing NLI literatures: when combining word n-grams, 
it seems to improve the accuracy of the classifier, 
compared with a word n-gram alone. But we do 
not observe too much difference when punctuation 
marks are removed or kept, using both unigram 
and bigram. In fact, including punctuation marks 
lead to high accuracies in many scenarios, espe-
cially in unigram in a frequency-based representa-
tion, suggesting the usage of punctuation marks 
varies across native languages.   
 
Features 
Performance of  
Binary Word n-
gram Representa-
tion 
Performance of 
Freq. Word n-
gram Representa-
tion 
word unigram 70.91% 25.36% 
word bigram 76.00% 17.64% 
word unigram 
and  
word bigram 
79.73% 23.36% 
Table 1 Accuracy of Different Feature Sets, without 
Punctuation Marks 
 
Features 
Performance of 
Binary Word n-
gram Representa-
tion 
Performance of 
Freq. Word n-
gram Representa-
tion 
word unigram 70.18% 30.00% 
word bigram 77.09% 18.73% 
word unigram 
and  
word bigram 
79.45% 28.73% 
Table 2 Accuracy of Different Feature Sets, with 
Punctuation Marks 
 
Table 3 shows the confusion matrix of classifi-
cation performance, using unigram and bigram, in 
a binary-based representation when punctuation 
marks are removed. We observe that some of na-
tive languages, such as German, Italian, and Chi-
nese, lead to better classification accuracy than for 
Korean, Spanish, and Arabic. 
 
 ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Preci-sion 
Re-
call 
F-
measure 
ARA 75 1 5 3 1 3 1 1 3 4 3 78.9 75.0 76.9
CHI 3 86 0 0 1 0 5 4 0 0 1 81.9 86.0 83.9
FRE 1 1 79 7 3 4 2 0 1 0 2 77.5 79.0 78.2
GER 3 1 2 87 1 1 1 0 2 0 2 79.8 87.0 83.3
HIN 1 2 1 2 77 0 0 0 5 10 2 74.0 77.0 75.5
ITA 0 0 6 4 0 85 0 0 3 0 2 83.3 85.0 84.2
JPN 2 2 1 0 0 1 86 3 2 0 3 77.5 86.0 81.5
KOR 0 8 2 1 1 0 14 72 1 1 0 82.8 72.0 77.0
SPA 4 0 6 3 4 6 1 1 70 1 4 78.7 70.0 74.1
TEL 1 0 0 1 15 0 0 0 0 82 1 83.7 82.0 82.8
TUR 5 4 0 1 1 2 1 6 2 0 78 79.6 78.0 78.8
Average Performance: 79.7%.   Precision, Recall, F-measures are in %. 
Table 3 Confusion Matrix on Testing Set 
5.2 Binary Based of Word N-Gram Repre-
sentation 
We observe that the accuracy of a binary-based 
word level n-gram representation in our system is 
significantly better than a frequency-based repre-
sentation.  This is similar to the result reported by 
Blanchard et al, (2013) in TOEFL11 corpus.  The 
differences between their system and ours are that 
the system developed by Blanchard et al, (2013) 
used logistic regression with L1-regularzation, in-
stead of SVM and they did not remove all punctua-
tion marks and special characters.   
This might imply that a frequency-based word 
n-gram representation do not capture the character-
istics of the data. This might be because the data 
resides in a high dimension space, and the frequen-
cies of word level n-grams would be skewed.  In a 
future study, one might investigate a better repre-
sentation form and other complex features that 
have a stronger interpretative power of the data.  
5.3 Effects of Proficiency Level 
In our results, we have included English profi-
ciency level (low/medium/high) as a feature pro-
vided by assessment experts.  However, we did not 
find a strong improvement in accuracies, for ex-
ample, 79.13% using a binary-based word level n-
grams when punctuation marks removed.  We 
think this might be because only one feature will 
155
not dramatically change the accuracies.  This may 
be due to the fact word n-grams have already con-
tributed a large amount of features.  
6 Conclusion 
In this paper, we used a new data set, TOEFL11 to 
investigate NLI. In the most existing literatures, 
ICLE corpus was used. However, ICLE has fewer 
data and is known to be biased to topics of essays.  
The newly released corpus, TOEFL11 addresses 
these two drawbacks, which is useful for NLI 
community.  Support vector machine (SVM) was 
used as a classifier in our system.  We have dem-
onstrated that a binary-based word level n-gram 
representation has resulted in a significantly better 
performance compared to a frequency-based n-
gram representation.  We observed that there is not 
much difference in classification accuracies when 
punctuation removed or kept, when combining 
both unigram and bigram.  Interestingly, a fre-
quency-based word unigram with punctuation 
marks outperforms than the case without punctua-
tion marks, suggesting the potential of utilizing 
punctuation marks in NLI.  In addition, English 
proficiency level has also been included in our fea-
ture set, but did not yield a significant improve-
ment in accuracy.  As most of the essays are 
represented in a high dimension space using word 
level n-grams, we are looking into feature selection 
to reduce dimensionality and how to represent 
those features in order to improve accuracy, as 
well as other features.  
References  
Blanchard, D., Tetreault, J., Higgins, D., Cahill, A., and 
Chodorow, M. 2013. TOEFL11: A Corpus of Non-
Native English.  Educational Testing Service.  
Bykh, S. and Meurers, D. 2012. Native Language Iden-
tification using Recurring n-grams - Investigating 
Abstraction and Domain Dependence. In Proceed-
ings of COLING 2012, 425-440, Mumbai, India. The 
COLING 2012 Organizing Committee. 
Joachims, T. 1999. Making large-Scale SVM Learning 
Practical. Advances in Kernel Methods - Support 
Vector Learning, B. Sch?lkopf and C. Burges and A. 
Smola (ed.), MIT-Press.  
Kochmar, E. 2011. Identification of a writer?s native 
language by error analysis. Master?s thesis, Univer-
sity of Cambridge. 
Koppel, M., Schler, J., and Zigdon, K. 2005. Automati-
cally determining an anonymous author?s native lan-
guage. In ISI, 209?217. 
Murphy, K. P. 2012. Machine learning: a probabilistic 
perspective. MIT Press.  
Tsur, O. and Rappoport, A. 2007. Using classifier fea-
tures for studying the effect of native language on the 
choice of written second language words. In Pro-
ceedings of the Workshop on Cognitive Aspects of 
Computational Language Acquisition, 9?16, Prague, 
Czech Republic. Association for Computational Lin-
guistics. 
Wong, S.-M. J. and Dras, M. 2009. Contrastive analysis 
and native language identification. In Proceedings of 
the Australasian Language Technology Association 
Workshop 2009, 53?61, Sydney, Australia. 
Wong, S.-M. J. and Dras, M. 2011. Exploiting parse 
structures for native language identification. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, 1600?1610, 
Edinburgh, Scotland, UK. Association for Computa-
tional Linguistics. 
156
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 124?132,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Frame Semantics for Stance Classification
Kazi Saidul Hasan and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{saidul,vince}@hlt.utdallas.edu
Abstract
Determining the stance expressed by an
author from a post written for a two-sided
debate in an online debate forum is a
relatively new problem in opinion min-
ing. We extend a state-of-the-art learning-
based approach to debate stance classifica-
tion by (1) inducing lexico-syntactic pat-
terns based on syntactic dependencies and
semantic frames that aim to capture the
meaning of a sentence and provide a gen-
eralized representation of it; and (2) im-
proving the classification of a test post via
a novel way of exploiting the information
in other test posts with the same stance.
Empirical results on four datasets demon-
strate the effectiveness of our extensions.
1 Introduction
Given a post written for a two-sided topic in an
online debate forum (e.g., ?Should abortion be al-
lowed??), the task of debate stance classification
involves determining which of the two sides (i.e.,
for or against) its author is taking. For example, a
stance classification system should determine that
the author of the following post is anti-abortion.
Post 1: Abortion has been legal for decades and no
one seems to have a problem with it. That?s ridicu-
lous! There are millions of people in the world
who would love to have children but can?t.
Previous approaches to debate stance classifica-
tion have focused on three debate settings, namely
congressional floor debates (Thomas et al, 2006;
Bansal et al, 2008; Balahur et al, 2009; Yesse-
nalina et al, 2010; Burfoot et al, 2011), company-
internal discussions (Murakami and Raymond,
2010), and online social, political, and ideologi-
cal debates in public forums (Agrawal et al, 2003;
Somasundaran and Wiebe, 2010; Wang and Rose?,
2010; Biran and Rambow, 2011; Hasan and Ng,
2012). As Walker et al (2012) point out, debates
in public forums differ from congressional debates
and company-internal discussions in terms of lan-
guage use. Specifically, online debaters use color-
ful and emotional language to express their points,
which may involve sarcasm, insults, and question-
ing another debater?s assumptions and evidence.
These properties can potentially make stance clas-
sification of online debates more challenging than
that of the other two types of debates.
Our goal in this paper is to improve the state
of the art in stance classification of online de-
bates, focusing in particular on ideological de-
bates. Specifically, we present two extensions,
one linguistic and the other extra-linguistic, to
the state-of-the-art supervised learning approach
to this task proposed by Anand et al (2011). In our
linguistic extension, we induce patterns from each
sentence in the training set using syntactic depen-
dencies and semantic frames that aim to capture
the meaning of a sentence and provide a general-
ized representation of it. Note that while Anand et
al.?s lexico-syntactic approach aims to generalize
from a sentence using syntactic dependencies, we
aim to generalize using semantic frames. As we
will see in Section 4, not only is there no guaran-
tee that syntactic dependencies can retain or suf-
ficiently capture the meaning of a sentence during
the generalization process, it is in fact harder to
generalize from syntactic dependencies than from
semantic frames. In our extra-linguistic extension,
we improve the classification of a test post via a
novel way of exploiting the information in other
test posts with the same stance.
We evaluate our approach to stance classifica-
tion of ideological debates on datasets collected
for four domains from online debate forums. Ex-
perimental results demonstrate the effectiveness of
our approach: it outperforms an improved version
of Anand et al?s approach by 2.6?7.0 accuracy
points on the four domains.
124
Number of % of ?for?
Domain posts posts
ABO 1741 54.9
GAY 1376 63.4
OBA 985 53.9
MAR 626 69.5
Table 1: Statistics of the four datasets.
The rest of the paper is organized as follows.
We first present our datasets in Section 2. Sec-
tion 3 describes our two learning-based baseline
systems for stance classification. Sections 4 and 5
discuss our two extensions. Finally, we show eval-
uation results in Section 6 and present conclusions
in Section 7.
2 Datasets
For our experiments, we collect debate posts
from four popular domains, Abortion (ABO),
Gay Rights (GAY), Obama (OBA), and Marijuana
(MAR). Each post should receive one of two do-
main labels, for or against, depending on whether
the author of the post supports or opposes abor-
tion, gay rights, Obama, or the legalization of mar-
ijuana. To see how we obtain these domain labels,
let us first describe the data collection process in
more detail.
We collect our debate posts for the four domains
from an online debate forum1. In each domain,
there are several two-sided debates. Each debate
has a subject (e.g., ?Abortion should be banned?)
for which a number of posts were written by dif-
ferent authors. Each post is manually tagged with
its author?s stance (i.e., yes or no) on the debate
subject. Since the label of each post represents the
subject stance but not the domain stance, we need
to automatically convert the former to the latter.
For example, for the subject ?Abortion should be
banned?, the subject stance yes implies that the au-
thor opposes abortion, and hence the domain label
for the corresponding label should be against.
We construct one dataset for each domain.
Statistics of these datasets are shown in Table 1.
3 Baseline Systems
We employ as baselines two stance classification
systems, Anand et al?s (2011) approach and an en-
hanced version of it, as described below.
Our first baseline, Anand et al?s approach, is
a supervised method that trains a stance classifier
1http://www.createdebate.com/
for determining whether the stance expressed in
a debate post is for or against. Hence, we cre-
ate one training instance from each post in the
training set, using the stance it expresses as its
class label. Following Anand et al, we repre-
sent a training instance using five types of fea-
tures: n-grams, document statistics, punctuations,
syntactic dependencies, and, if applicable, the set
of features computed for the immediately preced-
ing post in its thread. Their n-gram features in-
clude both the unigrams and bigrams in a post,
as well as its first unigram, first bigram, and first
trigram. The features based on document statis-
tics include the post length, the number of words
per sentence, the percentage of words with more
than six letters, and the percentage of words as
pronouns and sentiment words. The punctuation
features are composed of the repeated punctuation
symbols in a post. The dependency-based features
have three variants. In the first variant, the pair
of arguments involved in each dependency rela-
tion extracted by a dependency parser is used as a
feature. The second variant is the same as the first
except that the head (i.e., the first argument in a re-
lation) is replaced by its part-of-speech (POS) tag.
The features in the third variant, the topic-opinion
features, are created by replacing each feature
from the first two types that contains a sentiment
word with the corresponding polarity label (i.e.,
+ or ?). For instance, given the sentence ?John
hates guns?, the topic-opinion features John? and
guns? are generated, since ?hate? has a negative
polarity and it is connected to ?John? and ?guns?
via the nsubj and dobj relations, respectively. In
our implementation, we train the stance classifier
using SVMlight (Joachims, 1999). After training,
we can apply the stance classifier to classify the
test instances, which are generated in the same
way as the training instances.
Related work on stance classification of con-
gressional debates has found that enforcing author
constraints (ACs) can improve classification per-
formance (e.g., Thomas et al (2006), Burfoot et al
(2011), Lu et al (2012)). ACs are a type of inter-
post constraints that specify that two posts written
by the same author for the same debate domain
should have the same stance. We hypothesize that
ACs could similarly be used to improve stance
classification of ideological debates, and therefore
propose a second baseline where we enhance the
first baseline with ACs. Enforcing ACs is simple.
125
We first use the learned stance classifier to classify
the test posts as in the first baseline, and then post-
process the labels of the test posts. Specifically,
we sum up the confidence values2 assigned to the
set of test posts written by the same author for the
same debate domain. If the sum is positive, then
we label all the posts in this set as for; otherwise
we label them as against.
4 Semantic Generalization
Our first extension to Anand et al?s (2011) ap-
proach involves semantic generalization.
To motivate this extension, let us take a closer
look at Anand et al?s attempt to generalize using
syntactic dependencies. Note that any approach
that aims to generalize using syntactic dependen-
cies suffers from several weaknesses. First, the
semantic relationship between the pair of lexical
items involved in each of these features is not en-
coded. This means that the resulting features do
not adequately capture the meaning of the under-
lying sentence. Second, replacing a word with
its POS tag is a syntactic, not semantic, gener-
alization, and doing so further abstracts the re-
sulting feature from the meaning of the under-
lying sentence. Above all, while the resulting
features are intended to improve generalizations,
they can provide very limited generalizations. To
see why, consider two semantically similar sen-
tences ?I hate arrogant people? and ?I dislike ar-
rogant people?. Ideally, any features that intend to
provide a generalized representation of these sen-
tences should be able to encode the fact that they
are semantically similar. However, Anand et al?s
features would fail to do so because they cannot
capture the fact that ?hate? and ?dislike? are se-
mantically similar.
In the rest of this section we describe how we
generate a semantic generalization of a sentence
to capture its meaning. Our approach to seman-
tic generalization involves (1) inducing from the
training data a set of patterns that aim to provide
a semantic generalization of the sentences in the
training posts and (2) using them in combination
with the baseline systems to classify a test post.
Below we describe these two steps in detail.
4.1 Step 1: Pattern Induction
This step is composed of two sub-steps.
2We use as the confidence value the signed distance of the
associated test point from the SVM hyperplane.
4.1.1 Sub-step 1: Topic Extraction
For each domain, we extract a list of topics. We
define a topic as a word sequence that (1) starts
with zero or more adjectives and ends with one or
more nouns and (2) appears in at least five posts
from the domain. Using this method, for example,
we can extract ?abortion?, ?partial-birth abortion?,
?birth control?, etc., as the topics for Abortion.
4.1.2 Sub-step 2: Pattern Creation
Given a sentence, we create patterns to capture its
information using syntactic dependencies and se-
mantic frames.3 These patterns can be divided into
three types, as described below. For ease of expo-
sition, we will use the two (semantically equiva-
lent) sentences below as our running examples and
see what patterns are created from them.
(1) Some people hate guns.
(2) Some people do not like guns.
Subject-Frame-Object (SFO) patterns. We
create a set of SFO patterns for a transitive verb
if (1) it is a frame target4; (2) its subject (respec-
tively object) is a topic; and (3) its object (respec-
tively subject) is a frame target. In sentence (1),
hate is the target of the frame Experiencer focus
(henceforth EF), its subject, people, is a topic, and
its object, guns is the target of the frame Weapon.
As a result, we create a set of SFO patterns, each
of which is represented as a 6-tuple. More specifi-
cally, we create the 8 SFO patterns shown in the
first column of Table 2. Pattern 1 says that (1)
this is an SFO pattern; (2) the subject is the word
people; (3) the frame name of the verb is EF; (4)
the frame name of the object is Weapon; (5) the
verb is not negated (POS); and (6) we don?t care
(DC) whether the verb is sentiment-bearing. If the
verb is sentiment-bearing (in this case, hate has a
negative sentiment), we create another pattern that
is the same as the first one, except that DC is re-
placed with its sentiment value (see Pattern 2).
Next, note that since the subject of hate is the
target of the frame People and its object is a topic,
we need to create patterns in a similar manner,
resulting in Patterns 3 and 4. Note that People
in these two patterns (with ?P? capitalized) is the
3We use the Stanford parser (de Marneffe and Manning,
2008) and SEMAFOR (Das et al, 2010) to obtain depen-
dency relations and semantic frames, respectively.
4A word w is the target of a frame f if f is assigned to
w to generalize its meaning. For example, assassination, kill,
and terminate are the targets of the frame Killing.
126
1 <SFO:people:EF:Weapon:POS:DC> 9 <SFO:people:EF:Weapon:NEG:DC> 17 <DF:dobj:EF:Weapon:POS:DC>
2 <SFO:people:EF:Weapon:POS:?> 10 <SFO:people:EF:Weapon:POS:?> 18 <DF:dobj:EF:Weapon:POS:?>
3 <SFO:People:EF:guns:POS:DC> 11 <SFO:People:EF:guns:NEG:DC> 19 <DF:dobj:EF:guns:POS:DC>
4 <SFO:People:EF:guns:POS:?> 12 <SFO:People:EF:guns:POS:?> 20 <DF:dobj:EF:guns:POS:?>
5 <SFO:people:EF:DC:POS:DC> 13 <SFO:people:EF:DC:NEG:DC> 21 <FET:people:Experiencer:EF:POS:DC>
6 <SFO:people:EF:DC:POS:?> 14 <SFO:people:EF:DC:POS:?> 22 <FET:people:Experiencer:EF:POS:?>
7 <SFO:DC:EF:guns:POS:DC> 15 <SFO:DC:EF:guns:NEG:DC> 23 <FET:guns:Content:EF:POS:DC>
8 <SFO:DC:EF:guns:POS:?> 16 <SFO:DC:EF:guns:POS:?> 24 <FET:guns:Content:EF:POS:?>
Table 2: Sample patterns created for sentences (1) and (2).
name of the frame People, not the word people ap-
pearing in the sentence.
To provide better generalization, we create a
simplified version of each SFO pattern by replac-
ing the frame name representing subject/object
with the value DC. This results in Patterns 5?8.
For sentence (2), we can generate patterns in a
similar manner, resulting in Patterns 9?16. For ex-
ample, Pattern 9 contains the element NEG, which
encodes the fact that the verb like is negated. Pat-
tern 10 deserves discussion. Since the positive
sentiment-bearing verb like is negated, the senti-
ment value of Pattern 10 is ?, which encodes the
fact that not like has a negative sentiment. The
negation value of Pattern 10 is POS rather than
NEG, reflecting the fact that not like does not ap-
pear in a negative context. In other words, the
sentiment value needs to be flipped if the verb
is negated, and so may the negation value. It is
worth noting that Patterns 2 and 10 are identical,
which provides suggestive evidence that sentences
(1) and (2) are semantically equivalent.
Dependency-Frame (DF) patterns. We create
a set of DF patterns for a dependency relation d
if (1) both arguments of d are frame targets or (2)
the head is a frame target and the dependent is a
topic. For example, in the dependency relation
dobj(hate,guns), both hate and guns are frame tar-
gets, as discussed above, and guns is a topic, so a
set of DF patterns (Patterns 17?20 in Table 2) will
be created from it. A DF pattern is represented as
a 6-tuple. For example, Pattern 17 says that (1)
this is a DF pattern; (2) the relation type is dobj;
(3) the frame name of the head is EF; (4) the frame
name of the dependent is Weapon; (5) the head is
not negated; and (6) we don?t care about the sen-
timent of the head. Pattern 18 is the same as Pat-
tern 17, except that it takes into account the senti-
ment value of the verb. Patterns 19 and 20 replaces
the frame name of the dependent with the topic
name, which is guns. The negation and sentiment
values are computed in the same way as those in
the SFO patterns.
Frame-Element-Topic (FET) patterns. We
create one FET pattern for every (v,fe) pair in
a sentence where v is a verb and a frame target,
and fe is a topic and a frame element of v?s
frame.5 In sentence (1), people is a topic and
it is assigned the role Experiencer, so two FET
patterns (Patterns 21 and 22) are created. Also,
since guns is a topic and it is assigned the role
Content, two additional FET patterns (Patterns 23
and 24) are created. The negation and sentiment
values are computed in the same way as those in
the SFO patterns.
4.2 Step 2: Classification
In this step, we will use the patterns learned in
Step 1 in combination with the baseline systems to
classify a test post. A simple way to combine the
learned patterns with the baseline systems would
be to augment the feature set they employ with the
learned patterns. One potential weakness of this
method is that the impact of these patterns could
be undermined by the fact that they are signifi-
cantly outnumbered by the baseline features, par-
ticularly the n-gram features.
For this reason, we decided to train another
stance classifier, which we will refer to as the
semantics-based classifier, cs. Like the base-
line stance classifier cb, (1) cs is trained using
SVMlight, (2) each training instance for cs corre-
sponds to a training post, and (3) its class label is
the stance the post expresses. Unlike cb, however,
the features employed by cs are created from the
learned patterns. Specifically, from each pattern
we create one binary feature whose value is 1 if
and only if the corresponding pattern is applicable
to the training post under consideration.
A natural question, then, is: how can we com-
bine the decisions made by cb and cs? To answer
this question, we applied both classifiers to the de-
5Note that since fe is a frame element of v?s frame, it is
assigned a semantic role.
127
System ABO GAY OBA MAR
cb 60.3 63.2 59.5 67.1
cs 56.1 58.7 56.0 65.2
Table 3: Development set accuracies.
System ABO GAY OBA MAR
cb 22.9 18.5 24.1 9.6
cs 17.6 14.3 19.4 7.2
Table 4: Percentage of posts predicted correctly
by one but not both classifiers on the development
set.
velopment set for each domain and obtained the
results in Table 3. As we can see, cs performs sig-
nificantly worse than cb for all domains.6
At first glance, we should just abandon cs
because of its consistently poorer performance.
However, since the two classifiers are trained on
disjoint feature sets (one is lexico-syntactic and
the other semantic), we hypothesize that the mis-
takes they made on the development set could be
complementary. To confirm this hypothesis, we
compute the percentage of posts in the develop-
ment set that are correctly classified by one but not
the other. Results of this experiment are shown in
Table 4. As we can see, these results are largely
consistent with our hypothesis. For instance, for
ABO, 22.9% of the posts are classified correctly
only by cb but not cs, whereas 17.6% of them are
classified correctly only by cs but not cb.
Given these results, we hypothesize that perfor-
mance could be improved by combining the pre-
dictions made by cb and cs. Since cb consistently
outperforms cs on all datasets, we use cs to make a
prediction if and only if (1) cb cannot predict con-
fidently and (2) cs can predict confidently. This
preference for cb is encoded in the following rule-
based strategy for classifying a test post p, where
the rules are applied in the order in which they are
listed.
Rule 1: if cb can classify p confidently, then use
cb?s prediction.
Rule 2: if cs can classify p confidently, use cs?s
prediction.
Rule 3: use cb?s prediction.
The next question is: how do we define con-
fidence? Since cb and cs are SVM-based clas-
sifiers, the data points that are closer to the hy-
perplane are those whose labels the SVM is less
6All significance tests are paired t-tests, with p < 0.05.
confident about. Hence, we define confidence for
classifier ci by the interval [conf il , conf iu], where
conf il < 0 and conf iu > 0 are signed distances
from the hyperplane defining ci. Specifically, we
say that a point p is confidently classified by ci if
and only if p lies outside the interval defined by
conf il and conf iu. Since we have two classifiers,
cb and cs, we need to define two intervals (i.e., four
numbers). Rather than defining these four num-
bers by hand, we tune them jointly so that the ac-
curacy of our combination strategy on the devel-
opment set is maximized.7
There is a caveat, however. Recall that when
applying this extension, we need to compute the
signed distances of every post p from cb and cs
to determine which classifier will be used to clas-
sify p. The question, then, is: when applying this
extension to the second baseline (the Anand et al
baseline extended with ACs) where all the posts
written by the same author for the same domain
should have the same stance, how should their
signed distances be computed? We adopt a sim-
ple solution: we take the average of the signed
distances of all such posts from the correspond-
ing hyperplane and set the signed distance of each
such post to the average value.
5 Exploiting Same-Stance Posts
To classify a debate post p in the test set, we have
so far exploited only the information extracted
from p itself. However, it is conceivable that we
can improve the classification of p by exploiting
the information extracted from other test posts that
have the same stance as p. This is the goal of our
second extension.
To see why doing so can improve the classifi-
cation of p, we make a simple observation: some
posts are easier to classify than the others. Typi-
cally, posts containing expressions that are strong
indicators of the stance label are easier to classify
than those that do not. As an example, consider
the following posts:
Post 2: I don?t think abortion should be illegal.
Post 3: What will you do if a woman?s life is in
danger while she?s pregnant? Do you still want to
sacrifice her life simply because the fetus is alive?
It should be fairly easy for a human to see that
the authors of both posts support abortion. How-
ever, Post 2 is arguably easier to classify than
7For parameter tuning, for each of the four numbers we
tried the values from ?0.5 to +0.5 with a step value of 0.001.
128
Post 3: Post 2 has an easy-to-determine stance,
whereas Post 3 has a couple of rhetorical questions
that may be difficult for a machine to understand.
Hence, we might be able to improve the classifica-
tion of Post 3 by exploiting information from other
posts that have the same stance as itself (which in
this case would be Post 2).
In practice, however, we are not given the infor-
mation of which posts have the same stance. In
the two subsections below, we discuss two sim-
ple methods of determining whether two posts are
likely to have the same stance.
5.1 Using Same-Author Information
The first method, which we will refer to as M1, is
fairly straightforward: we posit that two posts are
likely to have the same stance if they are written
by the same author. Given a test post p to be clas-
sified, we can use this method to identify a sub-
set of p?s same-stance posts. For convenience, we
denote this set as SameStancePosts(p). The ques-
tion, then, is: how can we exploit information in
SameStancePosts(p) to improve the classification
of p? One way would be to combine the con-
tent of the posts in SameStancePosts(p) with that
of p (i.e., by taking the union of all the binary-
valued feature vectors), and use the class value of
the combined post as the class value of p. How-
ever, rather than simply combining all the posts
to form one big post, we generalize this idea by
(1) generating all possible combinations of posts
in SameStancePosts(p); (2) for each such combi-
nation, combine it with p; (3) classify each combi-
nation obtained in (2) using the SVM classifier; (4)
sum the confidence values of all the combinations;
and (5) use the signed value as the class value of p.
Note that if SameStancePosts(p) contains n posts,
the number of possible combinations is
?n
i=0
(n
i
)
.
For efficiency reasons, we allow each combination
to contain at most 10 posts.
At first glance, it seems that the combination
method described in the previous paragraph is an
alternative implementation of ACs. (Recall that
ACs are inter-post constraints that ensure that two
posts written by the same author for the same do-
main should receive the same label.) Neverthe-
less, there are two major differences between our
combination method and ACs. First, in ACs, the
same-author posts can only interact via the confi-
dence values assigned to them. On the other hand,
in our proposal, the same-author posts interact via
Feature Definition
SameDebate whether authors posted in same debate
SameThread whether authors posted in same thread
Replied whether one author replied to the other
Table 5: Interaction features for the author-
agreement classifier.
feature sharing. In other words, in ACs, the same-
author posts interact after they are classified by
the stance classifier, whereas in our proposal, the
interaction occurs before the posts are classified.
Second, in ACs, all the same-author posts receive
the same stance label. On the other hand, this is
not necessarily the case in our proposal, because
two same-author posts can be classified using dif-
ferent combinations. In other words, ACs and our
combination method are not the same. In fact, they
can be used in conjunction with each other.
5.2 Finding Similar-Minded Authors
Using M1 to identify same-stance posts has a po-
tential weakness. If an author has composed a
small number of posts, then the number of com-
binations that can be generated will be small. In
the extreme case, if an author has composed just
one post p, then no combinations will be gener-
ated using M1.
To enable p to benefit from our idea of ex-
ploiting same-stance posts, we propose another
method to identify same-stance posts, M2, which
is a generalization of M1. In M2, we posit
that two posts are likely to have the same stance
if they are written by the same author or by
similar-minded authors. Given test post p, we
can compute SameStancePosts(p) using the defi-
nition of M2, and apply the same 5-step combina-
tion method described in the previous subsection
to SameStancePosts(p) to classify p.
The remaining question is: given an author,
a, in the test set, how do we compute his set of
similar-minded authors, Asimilar? To do this, we
train a binary author-agreement classifier on the
training set to generate Asimilar for a. Specifi-
cally, each training instance corresponds to a pair
of authors in the training set having one of two
class labels, agree (i.e., authors have the same
stance) and disagree (i.e., authors have opposing
stances). We represent each instance with two
types of features. Features of the first type are ob-
tained by taking the difference of the feature vec-
tors corresponding to the two authors under con-
sideration, where the feature vector of an author is
129
obtained by taking the union of the feature vectors
corresponding to all of the posts written by her.
Taking the difference would allow the learner to
focus on those features whose values differ in the
feature vectors. For the second type of features,
we use author interaction information encoded as
three binary features (see Table 5 for their defi-
nitions), which capture how authors interact with
each other in a debate thread. After training the
classifier, we apply it to classify the author-pairs
in the test set. Then, for each author a, we com-
pute her k-nearest authors based on the magnitude
of their agreement, where k is tuned to maximize
accuracy on the development data.8 Finally, we
take Asimilar to be the set of k-nearest authors.
6 Evaluation
6.1 Experimental Setup
Results are expressed in terms of accuracy ob-
tained via 5-fold cross validation, where accuracy
is the percentage of test instances correctly classi-
fied. Since all experiments require the use of de-
velopment data for parameter tuning, we use three
folds for model training, one fold for development,
and one fold for testing in each fold experiment.
6.2 Results
Results are shown in Table 6. Row 1 shows the
results of the Anand et al (2011) baseline on the
four datasets, obtained by training a stance classi-
fier using the SVMlight package.9 Row 2 shows
the results of the second baseline, Anand et al?s
system enhanced with ACs. As we can see, incor-
porating ACs into Anand et al?s system improves
its performance significantly on all datasets and
yields a system that achieves an average improve-
ment of 4.6 accuracy points.
Next, we incorporate our first extension, pattern
induction, into the better of the two baselines (i.e.,
the second baseline). Results of combining cb and
cs to classify the test posts (together with the ACs)
are shown in row 3 of Table 6. As we can see, in-
corporating pattern induction into the second base-
line significantly improves its performance on all
four datasets and yields a system that achieves an
average improvement of 2.48 accuracy points.
Before incorporating our second extension, let
8We tested values of k from 1 to 7.
9For all SVM experiments, the regularization parameter C
is tuned using development data, but the remaining learning
parameters are set to their default values.
System ABO GAY OBA MAR
cb 61.4 62.6 58.1 66.9
cb+AC 72.0 64.9 62.7 67.8
cb+cs+AC 73.2 68.0 64.2 71.9
cbs+AC 71.8 65.0 60.2 67.9
cb+cs+M1+AC 74.8 69.1 69.7 73.2
cb+cs+M2+AC 75.9 70.6 71.2 75.3
Table 6: 5-fold cross-validation accuracies.
us recall our earlier hypothesis that combining cb
and cs using our method would be better than
training just one classifier that combines the fea-
tures used by cb and cs. The reason behind our
hypothesis was that simply combining the feature
sets would undermine the impact of pattern-based
features because they would be significantly out-
numbered by the features in cb. To confirm this
hypothesis, we showed in row 4 of Table 6 the
results of this experiment, where we trained one
classifier on all the features used by cb and cs.
As we can see, this classifier (referred to as cbs in
the table) together with the ACs performs signif-
icantly worse than the cb+cs+AC system (row 3)
on all datasets. In fact, the cb+AC system (row 2)
outperforms the cbs+AC system on OBA, but they
are statistically indistinguishable on the remaining
datasets. These results suggest that combining the
pattern-based features with the baseline features
into one feature set renders the former ineffective.
Finally, we incorporate our second extension,
the one that involves generating combinations of
test posts written by the same author (M1) and by
both the same author and similar-minded authors
(M2). Results of these experiments are shown in
rows 5?6 of Table 6. The M1-based system sig-
nificantly outperforms cb+cs+AC on all four do-
mains, yielding an average improvement of 2.4 ac-
curacy points. The M2-based system further beats
the M1-based system by 1.5 accuracy points on
average, and their performance difference is sig-
nificant on all but the ABO domain.
Overall, our two extensions yield a stance clas-
sification system that significantly outperforms the
better baseline on all four datasets, with an average
improvement of 6.4 accuracy points.
Given the better performance of the
combination-based systems, a natural ques-
tion is: can we further improve performance
by applying our combination methods to gen-
erate artificial posts and use them as additional
training instances? To answer this question, we
apply both M1 and M2 to generate additional
130
training instances, using a random selection of
same-stance authors in place of M2?s k-nearest
neighbor method. However, neither method yields
an improvement in performance over the method
on which it is based. We speculate that since all
the posts in the training combinations are already
present in the training set as individual posts,
they are more likely to be farther away from the
hyperplane than the individual posts, meaning
that they are less likely to be support vectors. This
in turn implies that they are less likely to affect
classification performance.
6.3 Error Analysis
To gain additional insights into our approach, we
performed a qualitative analysis of the errors pro-
duced by our best-performing system below.
Failure to accumulate decisions from several
clues. Authors often express their stance using a
group of sentences where the latter sentence(s) in-
dicate the actual stance and the initial sentence(s)
may give a false impression about the author?s
stance. Consider Post 1 (see Section 1) and Post 4.
Post 4: I agree abortion creates stress and pain. I
agree it kills a potential life. That does not mean
it is right to ban abortion.
In Post 1, the author is anti-abortion, whereas
in Post 4, the author is pro-abortion. However,
the first sentence in Post 1 gives a misleading clue
about the author?s stance, and so do the first two
sentences in Post 4. Since all the systems dis-
cussed in the paper operate on one sentence at a
time, they are all prone to such errors. One way
to address this problem could be to determine how
adjacent sentences are related to each other via the
use of discourse relations.
Presence of materials irrelevant to stance. Be-
cause of the informal style of writing, we often
find long posts with one or two sentences indicat-
ing the actual stance of the author. The rest of such
posts often include descriptions of an author?s per-
sonal experience, comments or questions directed
to other authors etc. Such long posts are frequently
misclassified for all four domains. Consider the
following example.
Post 5: Marijuana should at least be decriminal-
ized. Driving stoned, however, is something totally
different and should definitely be a crime. Also,
weed can?t kill you, unlike cigarettes and alcohol.
In my opinion cigarettes should definitely be ille-
gal, but they?re so ingrained into our culture that I
doubt that is going to happen any time soon.
In this post, the author supports the legalization
of marijuana. However, the only useful hints about
her stance are ?marijuana should at least be de-
criminalized? and ?weed can?t kill you?. The rest
of the post is not helpful for stance classification.
Convoluted posts appearing later in long post
sequences. As a post sequence gets longer, au-
thors tend to focus on specific aspects of a de-
bate and consequently, it becomes more difficult to
classify their stances, even with the context-based
features (features taken from the immediately pre-
ceding post) proposed by Anand et al Consider
the following post sequence, where only the first
post (P1) and the nth post (Pn) are shown due to
space limitations.
[P1: Anti-Obama] Obama is a pro-abortionist. Killing ba-
bies is wrong so stop doing it. The new health reform bill
is not good. There are some good things but more worse
than good. You could have just passed some laws instead of
making a whole bill.
? ? ?
[Pn: Pro-Obama] Killing fetuses isn?t wrong. Be-
sides, we could use those fetuses for stem cell re-
search.
As we can see, the author of P1 does not sup-
port Obama because of his pro-abortion views. In
Pn, a pro-Obama author explains why she thinks
abortion is not wrong. However, without the con-
text from P1 that Obama is pro-abortion, it is not
easy for a machine to classify Pn correctly. This
problem is more serious in ABO and GAY than in
the other domains as the average length of a post
sequence in these two domains is larger.
7 Conclusions
We examined the under-studied task of stance
classification of ideological debates. Employing
our two extensions yields a system that outper-
forms an improved version of Anand et al?s ap-
proach by 2.6?7.0 accuracy points. In particular,
while existing approaches to debate stance classi-
fication have primarily employed lexico-syntactic
features, to our knowledge this is the first attempt
to employ FrameNet for this task to induce fea-
tures that aim to capture the meaning and pro-
vide semantic generalizations of a sentence. In
addition, our method for identifying and exploit-
ing same-stance posts during the inference proce-
dure provides further gains when used on top of
our FrameNet extension.
131
References
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In
Proceedings of the 12th international conference on
World Wide Web, WWW ?03, pages 529?535.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis (WASSA 2011), pages 1?9.
Alexandra Balahur, Zornitsa Kozareva, and Andre?s
Montoyo. 2009. Determining the polarity and
source of opinions expressed in political debates. In
Proceedings of the 10th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, CICLing ?09, pages 468?480.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label
disagreement in the min-cut classification frame-
work. In Proceedings of the 22nd International
Conference on Computational Linguistics: Com-
panion volume: Posters, pages 15?18.
Or Biran and Owen Rambow. 2011. Identifying justi-
fications in written dialogs. In Proceedings of the
2011 IEEE Fifth International Conference on Se-
mantic Computing, ICSC ?11, pages 162?168.
Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional
floor-debate transcripts. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1506?1515.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic frame-semantic
parsing. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 948?956.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies
representation. In Proceedings of the COLING
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation, CrossParser ?08, pages 1?8.
Kazi Saidul Hasan and Vincent Ng. 2012. Predict-
ing stance in ideological debate with rich linguistic
knowledge. In Proceedings of the 24th International
Conference on Computational Linguistics: Posters,
pages 451?460.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and
Alexander Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 44?56. MIT
Press.
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan
Roth. 2012. Unsupervised discovery of opposing
opinion networks from forum discussions. In Pro-
ceedings of the 21st ACM International Conference
on Information and Knowledge Management, CIKM
?12, pages 1642?1646.
Akiko Murakami and Rudy Raymond. 2010. Support
or oppose? Classifying positions in online debates
from reply activities and opinion expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, CAAGET ?10, pages 116?
124.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327?335.
Marilyn Walker, Pranav Anand, Rob Abbott, and Ricky
Grant. 2012. Stance classification using dialogic
properties of persuasion. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 592?596.
Yi-Chia Wang and Carolyn P. Rose?. 2010. Making
conversational structure explicit: Identification of
initiation-response pairs within online discussions.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 673?676.
Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010. Multi-level structured models for document-
level sentiment classification. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 1046?1056.
132
