Proceedings of NAACL HLT 2009: Short Papers, pages 245?248,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Quadratic Features and Deep Architectures for Chunking
Joseph Turian and James Bergstra and Yoshua Bengio
Dept. IRO, Universite? de Montre?al
Abstract
We experiment with several chunking models.
Deeper architectures achieve better gener-
alization. Quadratic filters, a simplification
of a theoretical model of V1 complex cells,
reliably increase accuracy. In fact, logistic
regression with quadratic filters outperforms
a standard single hidden layer neural network.
Adding quadratic filters to logistic regression
is almost as effective as feature engineering.
Despite predicting each output label indepen-
dently, our model is competitive with ones
that use previous decisions.
1 Introduction
There are three general approaches to improving
chunking performance: engineer better features,
improve inference, and improve the model.
Manual feature engineering is a common direc-
tion. One technique is to take primitive features
and manually compound them. This technique is
common, and most NLP systems use n-gram based
features (Carreras and Ma`rquez, 2003; Ando and
Zhang, 2005, for example). Another approach is
linguistically motivated feature engineering, e.g.
Charniak and Johnson (2005).
Other works have looked in the direction of
improving inference. Rather than predicting each
decision independently, previous decisions can be
included in the inference process. In this work,
we use the simplest approach of modeling each
decision independently.
The third direction is by using a better model. If
modeling capacity can be added without introducing
too many extra degrees of freedom, generalization
could be improved. One approach for compactly
increasing capacity is to automatically induce
intermediate features through the composition of
non-linearities, for example SVMs with a non-linear
kernel (Kudo and Matsumoto, 2001), inducing
compound features in a CRF (McCallum, 2003),
neural networks (Henderson, 2004; Bengio and Le-
Cun, 2007), and boosting decision trees (Turian and
Melamed, 2006). Recently, Bergstra et al (2009)
showed that capacity can be increased by adding
quadratic filters, leading to improved generalization
on vision tasks. This work examines how well
quadratic filters work for an NLP task. Compared to
manual feature engineering, improved models are
appealing because they are less task-specific.
We experiment on the task of chunking (Sang and
Buchholz, 2000), a syntactic sequence labeling task.
2 Sequence labeling
Besides Collobert and Weston (2008), previous
work on sequence labeling usually use previous
decisions in predicting output labels. Here we do
not take advantage of the dependency between suc-
cessive output labels. Our approach predicts each
output label independently of the others. This allows
us to ignore inference during training: The model
maximizes the conditional likelihood of each output
label independent of the output label of other tokens.
We use a sliding window approach. The output
label for a particular focus token xi is predicted
based upon k? tokens before and after xi. The entire
window is of size k = 2 ? k? + 1. Nearly all work on
sequence labeling uses a sliding window approach
(Kudo and Matsumoto, 2001; Zhang et al, 2002;
245
204 204 204 204 204 204 204
150 150 150 150 150 150 150tok
win
out
in
400
23
?h
?q
?q
Figure 1: Illustration of our baseline I-T-W-O model (see
Secs. 4 and 5.1). The input layer comprises seven tokens
with 204 dimensions each. Each token is passed through
a shared 150-dimensional token feature extractor. These
7 ? 150 features are concatenated and 400 features are
extracted from them in the window layer. These 400 fea-
tures are the input to the final 23-class output prediction.
Feature extractors ?q and ?h are described in Section 3.
Carreras and Ma`rquez, 2003; Ando and Zhang,
2005, for example). We assume that each token x
can be transformed into a real-valued feature vector
?(x) with l entries. The feature function will be
described in Section 4.
A standard approach is as follows: We first
concatenate the features of k tokens into one vector
[?(xi?k?), . . . , ?(xi+k?)] of length k ? l entries. We can
then pass [?(xi?k?), . . . , ?(xi+k?)] to a feature extractor
over the entire window followed by an output
log-linear layer.
Convolutional architectures can help when there
is a position-invariant aspect to the input. In machine
vision, parameters related to one part of the image
are sometimes restricted to be equal to parameters
related to another part (LeCun et al, 1998). A
convolutional approach to sequence labeling is as
follows: At the lowest layer we extract features from
individual tokens using a shared feature extractor.
These higher-level individual token features are then
concatenated, and are passed to a feature extractor
over the entire window.
In our baseline approach, we apply one convolu-
tional layer of feature extraction to each token (one
token layer), followed by a concatenation, followed
by one layer of feature extraction over the entire
window (one window layer), followed by a 23-D
output prediction using multiclass logistic regres-
sion. We abbreviate this architecture as I-T-W-O
(inputtokenwindowoutput). See Figure 1 for
an illustration of this architecture.
3 Quadratic feature extractors
The most common feature extractor in the literature
is a linear filter h followed by a non-linear squashing
(activation) function ?:
f (x) = ?(h(x)), h(x) = b +Wx. (1)
In our experiments, we use the softsign squash-
ing function ?(z) = z/(1 + |z|). n-class lo-
gistic regression predicts ?(h(x)), where softmax
?i(z) = exp(zi)/?k exp(zk). Rust et al (2005) argues
that complex cells in the V1 area of visual cortex
are not well explained by Eq. 1, but are instead
better explained by a model that includes quadratic
interactions between regions of the receptive field.
Bergstra et al (2009) approximate the model of
Rust et al (2005) with a simpler model of the
form given in Eq. 2.? In this model, the pre-squash
transformation q includes J quadratic filters:
f (x) = ?(q(x)), q(x) =
?
?????????
b +Wx +
?
? J?
j=1
(V jx)2
?
?????????
(2)
where b,W, and V1 . . .VJ are tunable parameters.
In the vision experiments of Bergstra et al
(2009), using quadratic filters improved the gen-
eralization of the trained architecture. We were
interested to see if the increased capacity would
also be beneficial in language tasks. For our logistic
regression (I-O) experiments, the architecture is
specifically I??qO, i.e. output O is the softmax
? applied to the quadratic transform q of the input
I. Like Bergstra et al (2009), in architectures with
hidden layers, we apply the quadratic transform q
in all layers except the final layer, which uses linear
transform h. For example, I-T-W-O is specifically
I??qT??qW??hO, as shown in Figure 1. Future
work will explore if generalization is improved by
using q in the final layer.
4 Features
Here is a detailed description of the types of features
we use, with number of dimensions:
? embeddings. We map each word to a real-valued
50-dimensional embedding. These embeddings
were obtained by Collobert and Weston (2008), and
? Bergstra et al (2009) do not use a sqrt in Eq. 2. We found that
sqrt improves optimization and gives better generalization.
246
were induced based upon a purely unsupervised
training strategy over the 631 million words in the
English Wikipedia.
? POS-tag. Part-of-speech tags were assigned auto-
matically, and are part of the CoNLL data. 45 dim.
? label frequencies. Frequency of each label
assigned to this word in the training and validation
data. From Ando and Zhang (2005). 23 dim.
? type(first character). The type of the first charac-
ter of the word. type(x) = ?A? if x is a capital letter,
?a? if x is a lowercase letter, ?n? if x is a digit, and x
otherwise. From Collins (2002). 20 dim.
? word length. The length of the word. 20 dim.
? compressed word type. We convert each char-
acter of the word into its type. We then remove any
repeated consecutive type. For example, ?Label-
making? ? ?Aa-a?. From Collins (2002). 46 dim.
The last three feature types are based upon ortho-
graphic information. There is a combined total of
204 features per token.
5 Experiments
We follow the conditions in the CoNLL-2000
shared task (Sang and Buchholz, 2000). Of the 8936
training sentences, we used 1000 randomly sampled
sentences (23615 words) for validation.
5.1 Training details
The optimization criterion used during training is
the maximization of the sum (over word positions)
of the per-token log-likelihood of the correct deci-
sion. Stochastic gradient descent is performed using
a fixed learning rate ? and early stopping. Gradients
are estimated using a minibatch of 8 examples. We
found that a learning rate of 0.01, 0.0032, or 0.001
was most effective.
In all our experiments we use a window size
of 7 tokens. In preliminary experiments, smaller
windows yielded poorer results, and larger ones
were no better. Layer sizes of extracted features
were chosen to optimize validation F1.
5.2 Results
We report chunk F-measure (F1). In some tables
we also report Acc, the per-token label accuracy,
post-Viterbi decoding.
Figure 2 shows that using quadratic filters reliably
improves generalization on all architectures. For
the I-T-W-O architecture, quadratic filters increase
91.5%
92%
92.5%
93%
93.5%
94%
94.5%
95%
0  1  2  4  8  1691.5%
92%
92.5%
93%
93.5%
94%
94.5%
95%
# of quadratic filters
I-T-W-O (baseline)I-W-O (1 hidden layer NN)I-O (LogReg)
Figure 2: Validation F1 (y-axis) as we vary the number
of quadratic filters (x-axis), over different model archi-
tectures. Both architecture depth and quadratic filters
improve validation F1.
Architecture #qf Acc F1
I-O 16 96.45 93.94
I-W(400)-O 4 96.66 94.39
I-T(150)-W(566)-O 2 96.85 94.77
I-T(150)-W(310)-W(310)-O 4 96.87 94.82
Table 1: Architecture experiments on validation data.
The first column describes the layers in the architecture.
(The architecture in Figure 1 is I-T(150)-W(400)-O.)
The second column gives the number of quadratic filters.
For each architecture, the layer sizes and number of
quadratic filters are chosen to maximize validation F1.
Deeper architectures achieve higher F1 scores.
validation F1 by an absolute 0.31. Most surpris-
ingly, logistic regression with 16 filters achieves
F1=93.94, which outperforms the 93.83 of a stan-
dard (0 filter) single hidden layer neural network.
With embeddings as the only features, logreg
with 0 filters achieves F1=85.36. By adding all
features, we can raise the F1 to 91.96. Alternately,
by adding 16 filters, we can raise the F1 to 91.60. In
other words, adding filters is nearly as effective as
our manual feature engineering.
Table 1 shows the result of varying the overall
architecture. Deeper architectures achieve higher
F1 scores. Table 2 compares the model as we lesion
off different features. POS tags and the embeddings
were the most important features.
We applied our best model overall (I-T-W-W-O
in Table 1) to the test data. Results are shown in
247
Feature set Acc F1
default 96.81 94.69
no orthographic features 96.84 94.62
no label frequencies 96.77 94.58
no POS tags 96.60 94.22
no embeddings 96.40 93.97
only embeddings 96.18 93.53
Table 2: Results on validation of varying the feature set,
for the architecture in Figure 1 with 4 quadratic filters.
NP F1 Prc Rcl F1
AZ05 94.70 94.57 94.20 94.39
KM01 94.39 93.89 93.92 93.91
I-T-W-W-O 94.44 93.72 93.91 93.81
CM03 94.41 94.19 93.29 93.74
SP03 94.38 - - -
Mc03 93.96 - - -
AZ05- - 93.83 93.37 93.60
ZDJ02 93.89 93.54 93.60 93.57
Table 3: Test set results for Ando and Zhang (2005), Kudo
and Matsumoto (2001), our I-T-W-W-O model, Carreras
and Ma`rquez (2003), Sha and Pereira (2003), McCallum
(2003), Zhang et al (2002), and our best I-O model.
AZ05- is Ando and Zhang (2005) using purely supervised
training, not semi-supervised training. Scores are noun
phrase F1, and overall chunk precision, recall, and F1.
Table 3. We are unable to compare to Collobert and
Weston (2008) because they use a different training
and test set. Our model predicts all labels in the
sequence independently. All other works in Table 3
use previous decisions when making the current
label decision. Our approach is nonetheless compet-
itive with approaches that use this extra information.
6 Conclusions
Many NLP approaches underfit important linguistic
phenomena. We experimented with new techniques
for increasing chunker model capacity: adding
depth (automatically inducing intermediate features
through the composition of non-linearities), and
including quadratic filters. Higher accuracy was
achieved by deeper architectures, i.e. ones with
more intermediate layers of automatically tuned fea-
ture extractors. Although they are a simplification of
a theoretical model of V1 complex cells, quadratic
filters reliably improved generalization in all archi-
tectures. Most surprisingly, logistic regression with
quadratic filters outperformed a single hidden layer
neural network without. Also, with logistic regres-
sion, adding quadratic filters was almost as effective
as manual feature engineering. Despite predicting
each output label independently, our model is
competitive with ones that use previous decisions.
Acknowledgments
Thank you to Ronan Collobert, Le?on Bottou, and
NEC Labs for access to their word embeddings, and
to NSERC and MITACS for financial support.
References
R. Ando and T. Zhang. A high-performance semi-
supervised learning method for text chunking. In ACL,
2005.
Y. Bengio and Y. LeCun. Scaling learning algorithms
towards AI. In Large Scale Kernel Machines. 2007.
J. Bergstra, G. Desjardins, P. Lamblin, and Y. Bengio.
Quadratic polynomials learn better image features. TR
1337, DIRO, Universite? de Montre?al, 2009.
X. Carreras and L. Ma`rquez. Phrase recognition by
filtering and ranking with perceptrons. In RANLP, 2003.
E. Charniak and M. Johnson. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In ACL, 2005.
M. Collins. Ranking algorithms for named entity extrac-
tion: Boosting and the voted perceptron. In ACL, 2002.
R. Collobert and J. Weston. A unified architecture for
natural language processing: Deep neural networks with
multitask learning. In ICML, 2008.
J. Henderson. Discriminative training of a neural
network statistical parser. In ACL, 2004.
T. Kudo and Y. Matsumoto. Chunking with support
vector machines. In NAACL, 2001.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient
based learning applied to document recognition. IEEE,
86(11):2278?2324, November 1998.
A. McCallum. Efficiently inducing features of condi-
tional random fields. In UAI, 2003.
N. Rust, O. Schwartz, J. A. Movshon, and E. Simoncelli.
Spatiotemporal elements of macaque V1 receptive fields.
Neuron, 46(6):945?956, 2005.
E. T. Sang and S. Buchholz. Introduction to the
CoNLL-2000 shared task: Chunking. In CoNLL, 2000.
F. Sha and F. C. N. Pereira. Shallow parsing with
conditional random fields. In HLT-NAACL, 2003.
J. Turian and I. D. Melamed. Advances in discriminative
parsing. In ACL, 2006.
T. Zhang, F. Damerau, and D. Johnson. Text chunking
based on a generalization of Winnow. JMLR, 2, 2002.
248
 	
	ffProceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 873?880,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Advances in Discriminative Parsing
Joseph Turian and I. Dan Melamed
{lastname}@cs.nyu.edu
Computer Science Department
New York University
New York, New York 10003
Abstract
The present work advances the accu-
racy and training speed of discrimina-
tive parsing. Our discriminative parsing
method has no generative component, yet
surpasses a generative baseline on con-
stituent parsing, and does so with mini-
mal linguistic cleverness. Our model can
incorporate arbitrary features of the in-
put and parse state, and performs fea-
ture selection incrementally over an ex-
ponential feature space during training.
We demonstrate the flexibility of our ap-
proach by testing it with several pars-
ing strategies and various feature sets.
Our implementation is freely available at:
http://nlp.cs.nyu.edu/parser/.
1 Introduction
Discriminative machine learning methods have
improved accuracy on many NLP tasks, including
POS-tagging, shallow parsing, relation extraction,
and machine translation. Some advances have also
been made on full syntactic constituent parsing.
Successful discriminative parsers have relied on
generative models to reduce training time and
raise accuracy above generative baselines (Collins
& Roark, 2004; Henderson, 2004; Taskar et al,
2004). However, relying on information from a
generative model might prevent these approaches
from realizing the accuracy gains achieved by dis-
criminative methods on other NLP tasks. Another
problem is training speed: Discriminative parsers
are notoriously slow to train.
In the present work, we make progress towards
overcoming these obstacles. We propose a flexi-
ble, end-to-end discriminative method for training
parsers, demonstrating techniques that might also
be useful for other structured prediction problems.
The proposed method does model selection with-
out ad-hoc smoothing or frequency-based feature
cutoffs. It requires no heuristics or human effort
to optimize the single important hyper-parameter.
The training regime can use all available informa-
tion from the entire parse history. The learning al-
gorithm projects the hand-provided features into a
compound feature space and performs incremen-
tal feature selection over this large feature space.
The resulting parser achieves higher accuracy than
a generative baseline, despite not using a genera-
tive model as a feature.
Section 2 describes the parsing algorithm. Sec-
tion 3 presents the learning method. Section 4
presents experiments with discriminative parsers
built using these methods. Section 5 compares our
approach to related work.
2 Parsing Algorithm
The following terms will help to explain our work.
A span is a range over contiguous words in the in-
put. Spans cross if they overlap but neither con-
tains the other. An item is a (span, label) pair. A
state is a partial parse, i.e. a set of items, none
of whose spans may cross. A parse inference is
a (state, item) pair, i.e. a state and an item to be
added to it. The frontier of a state consists of the
items with no parents yet. The children of a candi-
date inference are the frontier items below the item
to be inferred, and the head of a candidate infer-
ence is the child item chosen by English head rules
(Collins, 1999, pp. 238?240). A parse path is a
sequence of parse inferences. For some input sen-
tence and training parse tree, a state is correct if
the parser can infer zero or more additional items
to obtain the training parse tree, and an inference
873
is correct if it leads to a correct state.
Given input sentence s, the parser searches for
parse p? out of the possible parses P(s):
p? = arg min
p?P(s)
C?(p) (1)
where C?(p) is the cost of parse p under model ?:
C?(p) =
?
i?p
c?(i) (2)
Section 3.1 describes how to compute c?(i). Be-
cause c?(i) ? R+, the cost of a partial parse mono-
tonically increases as we add items to it.
The parsing algorithm considers a succession
of states. The initial state contains terminal items,
whose labels are the POS tags given by the tagger
of Ratnaparkhi (1996). Each time we pop a state
from the agenda, c? computes the costs for the
candidate bottom-up inferences generated from
that state. Each candidate inference results in a
successor state to be placed on the agenda.
The cost function c? can consider arbitrary
properties of the input and parse state. We are not
aware of any tractable solution to Equation 1, such
as dynamic programming. Therefore, the parser
finds p? using a variant of uniform-cost search.
The parser implements the search using an agenda
that stores entire states instead of single items.
Each time a state is popped from the agenda, the
parser uses depth-first search starting from the
state that was popped until it (greedily) finds a
complete parse. In preliminary experiments, this
search strategy was faster than standard uniform-
cost search (Russell & Norvig, 1995).
3 Training Method
3.1 General Setting
Our training set I consists of candidate inferences
from the parse trees in the training data. From
each training inference i ? I we generate the tuple
?X(i), y(i), b(i)?. X(i) is a feature vector describing
i, with each element in {0, 1}. We will use X f (i) to
refer to the element of X(i) that pertains to feature
f . y(i) = +1 if i is correct, and y(i) = ?1 if not.
Some training examples might be more important
than others, so each is given a bias b(i) ? R+, as
detailed in Section 3.3.
The goal during training is to induce a hypothe-
sis h?(i), which is a real-valued inference scoring
function. In the present work, h? is a linear model
parameterized by a real vector ?, which has one
entry for each feature f :
h?(i) = ? ? X(i) =
?
f
? f ? X f (i) (3)
The sign of h?(i) predicts the y-value of i and the
magnitude gives the confidence in this prediction.
The training procedure optimizes ? to minimize
the expected risk R? over training set I. R? is the
objective function, a combination of loss function
L? and regularization term ??:
R?(I) = L?(I) + ?? (4)
The loss of the inference set decomposes into the
loss of individual inferences:
L?(I) =
?
i?I
l?(i) (5)
In principle, l? can be any loss function, but in the
present work we use the log-loss (Collins et al,
2002):
l?(i) = b(i) ? ln(1 + exp(???(i))) (6)
and ??(i) is the margin of inference i:
??(i) = y(i) ? h?(i) (7)
Inference cost c?(i) in Equation 2 is l?(i) com-
puted using y(i) = +1 and b(i) = 1, i.e.:
c?(i) = ln(1 + exp(?h?(i))) (8)
?? in Equation 4 is a regularizer, which penal-
izes complex models to reduce overfitting and gen-
eralization error. We use the `1 penalty:
?? =
?
f
? ? |? f | (9)
where ? is a parameter that controls the strength
of the regularizer. This choice of objective R? is
motivated by Ng (2004), who suggests that, given
a learning setting where the number of irrelevant
features is exponential in the number of train-
ing examples, we can nonetheless learn effectively
by building decision trees to minimize the `1-
regularized log-loss. On the other hand, Ng (2004)
suggests that most of the learning algorithms com-
monly used by discriminative parsers will overfit
when exponentially many irrelevant features are
present.1
Learning over an exponential feature space is
the very setting we have in mind. A priori, we de-
fine only a set A of simple atomic features (given
1including the following learning algorithms:
? unregularized logistic regression
? logistic regression with an `2 penalty (i.e. a Gaussian prior)
? SVMs using most kernels
? multilayer neural nets trained by backpropagation
? the perceptron algorithm
874
in Section 4). The learner then induces compound
features, each of which is a conjunction of possi-
bly negated atomic features. Each atomic feature
can have one of three values (yes/no/don?t care),
so the size of the compound feature space is 3|A|,
exponential in the number of atomic features. It
was also exponential in the number of training ex-
amples in our experiments (|A| ? |I|).
3.2 Boosting `1-Regularized Decision Trees
We use an ensemble of confidence-rated decision
trees (Schapire & Singer, 1999) to represent h?.2
The path from the root to each node n in a decision
tree corresponds to some compound feature f , and
we write ?(n) = f . To score an inference i using
a decision tree, we percolate the inference?s fea-
tures X(i) down to a leaf n and return confidence
??(n). An inference i percolates down to node n iff
X?(n) = 1. Each leaf node n keeps track of the pa-
rameter value ??(n).3 The score h?(i) given to an
inference i by the whole ensemble is the sum of the
confidences returned by the trees in the ensemble.
Listing 1 Outline of training algorithm.
1: procedure T????(I)
2: ensemble? ?
3: ?? ?
4: while dev set accuracy is increasing do
5: t ? tree with one (root) node
6: while the root node cannot be split do
7: decay `1 parameter ?
8: while some leaf in t can be split do
9: split the leaf to maximize gain
10: percolate every i ? I to a leaf node
11: for each leaf n in t do
12: update ??(n) to minimize R?
13: append t to ensemble
Listing 1 presents our training algorithm. At
the beginning of training, the ensemble is empty,
? = 0, and the `1 parameter ? is set to? (Steps 1.2
and 1.3). We train until the objective cannot be fur-
ther reduced for the current choice of ?. We then
determine the accuracy of the parser on a held-out
development set using the previous ? value (be-
fore it was decreased), and stop training when this
2Turian and Melamed (2005) reported that decision trees
applied to parsing have higher accuracy and training speed
than decision stumps, so we build full decision trees rather
than stumps.
3Any given compound feature can appear in more than one
tree, but each leaf node has a distinct confidence value. For
simplicity, we ignore this possibility in our discussion.
accuracy reaches a plateau (Step 1.4). Otherwise,
we relax the regularization penalty by decreasing
? (Steps 1.6 and 1.7) and continue training. In this
way, instead of choosing the best ? heuristically,
we can optimize it during a single training run
(Turian & Melamed, 2005).
Each training iteration (Steps 1.5?1.13) has sev-
eral steps. First, we choose some compound fea-
tures that have high magnitude gradient with re-
spect to the objective function. We do this by
building a new decision tree, whose leaves rep-
resent the chosen compound features (Steps 1.5?
1.9). Second, we confidence-rate each leaf to min-
imize the objective over the examples that per-
colate down to that leaf (Steps 1.10?1.12). Fi-
nally, we append the decision tree to the ensem-
ble and update parameter vector ? accordingly
(Step 1.13). In this manner, compound feature se-
lection is performed incrementally during train-
ing, as opposed to a priori.
Our strategy minimizing the objective R?(I)
(Equation 4) is a variant of steepest descent
(Perkins et al, 2003). To compute the gradient of
the unpenalized loss L? with respect to the param-
eter ? f of feature f , we have:
?L?(I)
?? f
=
?
i?I
?l?(i)
???(i) ?
???(i)
?? f
(10)
where:
???(i)
?? f
= y(i) ? X f (i) (11)
Using Equation 6, we define the weight of an ex-
ample i under the current model as the rate at
which loss decreases as the margin of i increases:
w?(i) = ? ?l?(i)
???(i) = b(i) ?
1
1 + exp(??(i)) (12)
Recall that X f (i) is either 0 or 1. Combining Equa-
tions 10?12 gives:
?L?(I)
?? f
= ?
?
i?I
X f (i)=1
y(i) ? w?(i) (13)
We define the gain of feature f as:
G?(I; f ) = max
(
0,
??????
?L?(I)
?? f
??????
? ?
)
(14)
Equation 14 has this form because the gradient of
the penalty term is undefined at ? f = 0. This dis-
continuity is why `1 regularization tends to pro-
duce sparse models. If G?(I; f ) = 0, then the ob-
jective R?(I) is at its minimum with respect to pa-
rameter ? f . Otherwise, G?(I; f ) is the magnitude
875
of the gradient of the objective as we adjust ? f in
the appropriate direction.
To build each decision tree, we begin with a root
node. The root node corresponds to a dummy ?al-
ways true? feature. We recursively split nodes by
choosing a splitting feature that will allow us to in-
crease the gain. Node n with corresponding com-
pound feature ?(n) = f can be split by atomic fea-
ture a if:
G?(I; f ? a) + G?(I; f ? ?a) > G?(I; f ) (15)
If no atomic feature satisfies the splitting crite-
rion in Equation 15, then n becomes a leaf node
of the decision tree and ??(n) becomes one of the
values to be optimized during the parameter up-
date step. Otherwise, we choose atomic feature a?
to split node n:
a? = arg max
a?A
(G?(I; f ? a) + G?(I; f ? ?a))
(16)
This split creates child nodes n1 and n2, with
?(n1) = f ? a? and ?(n2) = f ? ?a?.
Parameter update is done sequentially on only
the most recently added compound features, which
correspond to the leaves of the new decision tree.
After the entire tree is built, we percolate exam-
ples down to their appropriate leaf nodes. We then
choose for each leaf node n the parameter ??(n)
that minimizes the objective over the examples in
that leaf. A convenient property of decision trees
is that the leaves? compound features are mutually
exclusive. Their parameters can be directly opti-
mized independently of each other using a line
search over the objective.
3.3 The Training Set
We choose a single correct path from each training
parse tree, and the training examples correspond to
all candidate inferences considered in every state
along this path.4 In the deterministic setting there
is only one correct path, so example generation
is identical to that of Sagae and Lavie (2005). If
parsing proceeds non-deterministically then there
might be multiple paths that lead to the same final
parse, so we choose one randomly. This method
of generating training examples does not require a
working parser and can be run prior to any train-
ing. The disadvantage of this approach is that it
minimizes the error of the parser at correct states
only. It does not account for compounded error or
4Nearly all of the examples generated are negative (y = ?1).
teach the parser to recover from mistakes grace-
fully.
Turian and Melamed (2005) observed that uni-
form example biases b(i) produced lower accuracy
as training progressed, because the induced clas-
sifiers minimized the error per example. To min-
imize the error per state, we assign every train-
ing state equal value and share half the value uni-
formly among the negative examples for the ex-
amples generated from that state and the other half
uniformly among the positive examples.
We parallelize training by inducing 26 label
classifiers (one for each non-terminal label in the
Penn Treebank). Parallelization might not uni-
formly reduce training time because different la-
bel classifiers train at different rates. However, par-
allelization uniformly reduces memory usage be-
cause each label classifier trains only on inferences
whose consequent item has that label.
4 Experiments
Discriminative parsers are notoriously slow to
train. For example, Taskar et al (2004) took sev-
eral months to train on the ? 15 word sentences
in the English Penn Treebank (Dan Klein, p.c.).
The present work makes progress towards faster
discriminative parser training: our slowest classi-
fier took fewer than 5 days to train. Even so, it
would have taken much longer to train on the en-
tire treebank. We follow Taskar et al (2004) in
training and testing on ? 15 word sentences in
the English Penn Treebank (Taylor et al, 2003).
We used sections 02?21 for training, section 22
for development, and section 23 for testing, pre-
processed as per Table 1. We evaluated our parser
using the standard PARSEVAL measures (Black et
al., 1991): labelled precision, labelled recall, and
labelled F-measure (Prec., Rec., and F1, respec-
tively), which are based on the number of non-
terminal items in the parser?s output that match
those in the gold-standard parse.5
As mentioned in Section 2, items are inferred
bottom-up and the parser cannot infer any item
that crosses an item already in the state. Although
there are O(n2) possible (span, label) pairs over a
frontier containing n items, we reduce this to the
? 5 ? n inferences that have at most five children.6
5The correctness of a stratified shuffling test has been called
into question (Michael Collins, p.c.), so we are not aware of
any valid significance tests for observed differences in PAR-
SEVAL scores.
6Only 0.57% of non-terminals in the preprocessed develop-
876
Table 1 Steps for preprocessing the data. Starred steps are performed only when parse trees are available
in the data (e.g. not on test data).
1. * Strip functional tags and trace indices, and remove traces.
2. * Convert PRT to ADVP. (This convention was established by Magerman (1995).)
3. Remove quotation marks (i.e. terminal items tagged ?? or ??). (Bikel, 2004)
4. * Raise punctuation. (Bikel, 2004)
5. Remove outermost punctuation.a
6. * Remove unary projections to self (i.e. duplicate items with the same span and label).
7. POS tag the text using the tagger of Ratnaparkhi (1996).
8. Lowercase headwords.
aAs pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation might discard useful infor-
mation. Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser after adding
punctuation features, one of which encoded the sentence-final punctuation.
To ensure the parser does not enter an infinite loop,
no two items in a state can have both the same
span and the same label. Given these restrictions
on candidate inferences, there were roughly 40
million training examples generated in the train-
ing set. These were partitioned among the 26 con-
stituent label classifiers. Building a decision tree
(Steps 1.5?1.9 in Listing 1) using the entire ex-
ample set I can be very expensive. We estimate
loss gradients (Equation 13) using a sample of the
inference set, which gives a 100-fold increase in
training speed (Turian & Melamed, 2006).
Our atomic feature set A contains 300K fea-
tures, each of the form ?is there an item in group
J whose label/headword/headtag/headtagclass is
?X???.7 Possible values of ?X? for each predicate
are collected from the training data. For 1 ? n ? 3,
possible values for J are:
? the first/last n child items
? the first n left/right context items
? the n children items left/right of the head
? the head item.
The left and right context items are the frontier
items to the left and right of the children of the
candidate inference, respectively.
4.1 Different Parsing Strategies
To demonstrate the flexibility of our learn-
ing procedure, we trained three different
parsers: left-to-right (l2r), right-to-left (r2l),
ment set have more than five children.
7The predicate headtagclass is a supertype of the headtag.
Given our compound features, these are not strictly necessary,
but they accelerate training. An example is ?proper noun,?
which contains the POS tags given to singular and plural
proper nouns. Space constraints prevent enumeration of the
headtagclasses, which are instead provided at the URL given
in the abstract.
Table 2 Results on the development set, training
and testing using only ? 15 word sentences.
active
? features % Rec. % Prec. F1
l2r 0.040 11.9K 89.86 89.63 89.74
b.u. 0.020 13.7K 89.92 89.84 89.88
r2l 0.014 14.0K 90.66 89.81 90.23
and non-deterministic bottom-up (b.u.). The
non-deterministic parser was allowed to choose
any bottom-up inference. The other two parsers
were deterministic: bottom-up inferences had
to be performed strictly left-to-right or right-
to-left, respectively. We stopped training when
each parser had 15K active features. Figure 1
shows the accuracy of the different runs over the
development set as training progressed. Table 2
gives the PARSEVAL scores of these parsers at
their optimal `1 penalty setting. We found that
the perplexity of the r2l model was low so that,
in 85% of the sentences, its greedy parse was the
optimal one. The l2r parser does poorly because
its decisions were more difficult than those of the
other parsers. If it inferred far-right items, it was
more likely to prevent correct subsequent infer-
ences that were to the left. But if it inferred far-left
items, then it went against the right-branching
tendency of English sentences. The left-to-right
parser would likely improve if we were to use a
left-corner transform (Collins & Roark, 2004).
Parsers in the literature typically choose some
local threshold on the amount of search, such as
a maximum beam width. With an accurate scor-
ing function, restricting the search space using
a fixed beam width might be unnecessary. In-
stead, we imposed a global threshold on explo-
ration of the search space. Specifically, if the
877
Figure 1 F1 scores on the development set of the
Penn Treebank, using only ? 15 word sentences.
The x-axis shows the number of non-zero param-
eters in each parser, summed over all classifiers.
85%
86%
87%
88%
89%
90%
15K10K5K2.5K1.5K
De
vel
. F-
me
asu
re
total number of non-zero parameters
right-to-leftleft-to-rightbottom up
parser has found some complete parse and has
explored at least 100K states (i.e. scored at least
100K inferences), search stopped prematurely and
the parser would return the (possibly sub-optimal)
current best complete parse. The l2r and r2l
parsers never exceeded this threshold, and al-
ways found the optimal complete parse. However,
the non-deterministic bottom-up parser?s search
was cut-short in 28% of the sentences. The non-
deterministic parser can reach each parse state
through many different paths, so it searches a
larger space than a deterministic parser, with more
redundancy.
To gain a better understanding of the weak-
nesses of our parser, we examined a sample of
50 development sentences that the r2l parser did
not get entirely correct. Roughly half the errors
were due to noise and genuine ambiguity. The re-
maining errors fell into three types, occurring with
roughly the same frequency:
? ADVPs and ADJPs The r2l parser had F1 =
81.1% on ADVPs, and F1 = 71.3% on ADJPs. An-
notation of ADJP and ADVP in the PTB is inconsis-
tent, particularly for unary projections.
? POS Tagging Errors Many of the parser?s er-
rors were due to incorrect POS tags. In future work
we will integrate POS-tagging as inferences of the
parser, allowing it to entertain competing hypothe-
ses about the correct tagging.
? Bilexical dependencies Although compound
features exist to detect affinities between words,
the parser had difficulties with bilexical depen-
dency decisions that were unobserved in the train-
ing data. The classifier would need more training
data to learn these affinities.
Figure 2 F1 scores of right-to-left parsers with dif-
ferent atomic feature sets on the development set
of the Penn Treebank, using only ? 15 word sen-
tences.
85%
86%
87%
88%
89%
90%
91%
30K20K10K5K2.5K1.5K
De
vel
. F-
me
asu
re
total number of non-zero parameters
kitchen sinkbaseline
4.2 More Atomic Features
We compared our right-to-left parser with the
baseline set of atomic features to one with a far
richer atomic feature set, including unbounded
context features, length features, and features of
the terminal items. This ?kitchen sink? parser
merely has access to many more item groups J, de-
scribed in Table 3. All features are all of the form
given earlier, except for length features (Eisner &
Smith, 2005). Length features compute the size of
one of the groups of items in the indented list in
Table 3. The feature determines if this length is
equal to/greater than to n, 0 ? n ? 15. The kitchen
sink parser had 1.1 million atomic features, 3.7
times the number available in the baseline. In fu-
ture work, we plan to try linguistically more so-
phisticated features (Charniak & Johnson, 2005)
as well as sub-tree features (Bod, 2003; Kudo et
al., 2005).
Figure 2 shows the accuracy of the right-to-
left parsers with different atomic feature sets over
the development set as training progressed. Even
though the baseline training made progress more
quickly than the kitchen sink, the kitchen sink?s F1
surpassed the baseline?s F1 early in training, and at
6.3K active parameters it achieved a development
set F1 of 90.55%.
4.3 Test Set Results
To situate our results in the literature, we compare
our results to those reported by Taskar et al (2004)
and Turian and Melamed (2005) for their dis-
criminative parsers, which were also trained and
tested on ? 15 word sentences. We also compare
our parser to a representative non-discriminative
878
Table 3 Item groups available in the kitchen sink run.
? the first/last n child items, 1 ? n ? 4
? the first n left/right context items, 1 ? n ? 4
? the n children items left/right of the head, 1 ? n ? 4
? the nth frontier item left/right of the leftmost/head/rightmost child item, 1 ? n ? 3
? the nth terminal item left/right of the leftmost/head/rightmost terminal item dominated by the item
being inferred, 1 ? n ? 3
? the leftmost/head/rightmost child item of the leftmost/head/rightmost child item
? the following groups of frontier items:
? all items
? left/right context items
? non-leftmost/non-head/non-rightmost child items
? child items left/right of the head item, inclusive/exclusive
? the terminal items dominated by one of the item groups in the indented list above
Table 4 Results of parsers on the test set, training
and testing using only ? 15 word sentences.
% Rec. % Prec. F1
Turian and Melamed (2005) 86.47 87.80 87.13
Bikel (2004) 87.85 88.75 88.30
Taskar et al (2004) 89.10 89.14 89.12
kitchen sink 89.26 89.55 89.40
parser (Bikel, 2004)8, the only one that we were
able to train and test under exactly the same ex-
perimental conditions (including the use of POS
tags from the tagger of Ratnaparkhi (1996)). Ta-
ble 4 shows the PARSEVAL results of these four
parsers on the test set.
5 Comparison with Related Work
Our parsing approach is based upon a single end-
to-end discriminative learning machine. Collins
and Roark (2004) and Taskar et al (2004) beat
the generative baseline only after using the stan-
dard trick of using the output from a generative
model as a feature. Henderson (2004) finds that
discriminative training was too slow, and reports
accuracy higher than generative models by dis-
criminatively reranking the output of his genera-
tive model. Unlike these state-of-the-art discrimi-
native parsers, our method does not (yet) use any
information from a generative model to improve
training speed or accuracy. As far as we know, we
present the first discriminative parser that does not
use information from a generative model to beat a
8Bikel (2004) is a ?clean room? reimplementation of the
Collins (1999) model with comparable accuracy.
generative baseline (the Collins model).
The main limitation of our work is that we can
do training reasonably quickly only on short sen-
tences because a sentence with n words gener-
ates O(n2) training inferences in total. Although
generating training examples in advance with-
out a working parser (Turian & Melamed, 2005)
is much faster than using inference (Collins &
Roark, 2004; Henderson, 2004; Taskar et al,
2004), our training time can probably be de-
creased further by choosing a parsing strategy with
a lower branching factor. Like our work, Ratna-
parkhi (1999) and Sagae and Lavie (2005) gener-
ate examples off-line, but their parsing strategies
are essentially shift-reduce so each sentence gen-
erates only O(n) training examples.
An advantage of our approach is its flexibility.
As our experiments showed, it is quite simple to
substitute in different parsing strategies. Although
we used very little linguistic information (the head
rules and the POS tag classes), our model could
also start with more sophisticated task-specific
features in its atomic feature set. Atomic features
that access arbitrary information are represented
directly without the need for an induced interme-
diate representation (cf. Henderson, 2004).
Other papers (Clark & Curran, 2004; Kaplan
et al, 2004, e.g.) have applied log-linear mod-
els to parsing. These works are based upon con-
ditional models, which include a normalization
term. However, our loss function forgoes normal-
ization, which means that it is easily decomposed
into the loss of individual inferences (Equation 5).
879
Decomposition of the loss allows the objective to
be optimized in parallel. This might be an ad-
vantage for larger structured prediction problems
where there are more opportunities for paralleliza-
tion, for example machine translation.
The only important hyper-parameter in our
method is the `1 penalty factor. We optimize it
as part of the training process, choosing the value
that maximizes accuracy on a held-out develop-
ment set. This technique stands in contrast to more
ad-hoc methods for choosing hyper-parameters,
which may require prior knowledge or additional
experimentation.
6 Conclusion
Our work has made advances in both accuracy
and training speed of discriminative parsing. As
far as we know, we present the first discriminative
parser that surpasses a generative baseline on con-
stituent parsing without using a generative compo-
nent, and it does so with minimal linguistic clev-
erness. Our approach performs feature selection
incrementally over an exponential feature space
during training. Our experiments suggest that the
learning algorithm is overfitting-resistant, as hy-
pothesized by Ng (2004). If this is the case, it
would reduce the effort required for feature engi-
neering. An engineer can merely design a set of
atomic features whose powerset contains the req-
uisite information. Then, the learning algorithm
can perform feature selection over the compound
feature space, avoiding irrelevant compound fea-
tures.
In future work, we shall make some standard
improvements. Our parser should infer its own
POS tags to improve accuracy. A shift-reduce
parsing strategy will generate fewer training in-
ferences, and might lead to shorter training times.
Lastly, we plan to give the model linguistically
more sophisticated features. We also hope to ap-
ply the model to other structured prediction tasks,
such as syntax-driven machine translation.
Acknowledgments
The authors would like to thank Chris Pike,
Cynthia Rudin, and Ben Wellington, as well as
the anonymous reviewers, for their helpful com-
ments and constructive criticism. This research
was sponsored by NSF grants #0238406 and
#0415933.
References
Bikel, D. M. (2004). Intricacies of Collins? parsing model.
Computational Linguistics, 30(4).
Black, E., Abney, S., Flickenger, D., Gdaniec, C., Grishman,
R., Harrison, P., et al (1991). A procedure for quantitatively
comparing the syntactic coverage of English grammars. In
Speech and Natural Language.
Bod, R. (2003). An efficient implementation of a new DOP
model. In EACL.
Charniak, E., & Johnson, M. (2005). Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In ACL.
Clark, S., & Curran, J. R. (2004). Parsing the WSJ using
CCG and log-linear models. In ACL.
Collins, M. (1999). Head-driven statistical models for natu-
ral language parsing. Doctoral dissertation.
Collins, M. (2003). Head-driven statistical models for natural
language parsing. Computational Linguistics, 29(4).
Collins, M., & Roark, B. (2004). Incremental parsing with
the perceptron algorithm. In ACL.
Collins, M., Schapire, R. E., & Singer, Y. (2002). Logis-
tic regression, AdaBoost and Bregman distances. Machine
Learning, 48(1-3).
Eisner, J., & Smith, N. A. (2005). Parsing with soft and hard
constraints on dependency length. In IWPT.
Henderson, J. (2004). Discriminative training of a neural
network statistical parser. In ACL.
Kaplan, R. M., Riezler, S., King, T. H., Maxwell, III, J. T.,
Vasserman, A., & Crouch, R. (2004). Speed and accuracy
in shallow and deep stochastic parsing. In HLT/NAACL.
Kudo, T., Suzuki, J., & Isozaki, H. (2005). Boosting-based
parse reranking with subtree features. In ACL.
Magerman, D. M. (1995). Statistical decision-tree models
for parsing. In ACL.
Ng, A. Y. (2004). Feature selection, `1 vs. `2 regularization,
and rotational invariance. In ICML.
Perkins, S., Lacker, K., & Theiler, J. (2003). Grafting: Fast,
incremental feature selection by gradient descent in func-
tion space. Journal of Machine Learning Research, 3.
Ratnaparkhi, A. (1996). A maximum entropy part-of-speech
tagger. In EMNLP.
Ratnaparkhi, A. (1999). Learning to parse natural language
with maximum entropy models. Machine Learning, 34(1-
3).
Russell, S., & Norvig, P. (1995). Artificial intelligence: A
modern approach.
Sagae, K., & Lavie, A. (2005). A classifier-based parser with
linear run-time complexity. In IWPT.
Schapire, R. E., & Singer, Y. (1999). Improved boosting us-
ing confidence-rated predictions. Machine Learning, 37(3).
Taskar, B., Klein, D., Collins, M., Koller, D., & Manning, C.
(2004). Max-margin parsing. In EMNLP.
Taylor, A., Marcus, M., & Santorini, B. (2003). The Penn
Treebank: an overview. In A. Abeille? (Ed.), Treebanks:
Building and using parsed corpora (chap. 1).
Turian, J., & Melamed, I. D. (2005). Constituent parsing by
classification. In IWPT.
Turian, J., & Melamed, I. D. (2006). Computational chal-
lenges in parsing by classification. In HLT-NAACL work-
shop on computationally hard problems and joint inference
in speech and language processing.
880
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 141?151,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Constituent Parsing by Classification
Joseph Turian and I. Dan Melamed
{lastname}@cs.nyu.edu
Computer Science Department
New York University
New York, New York 10003
Abstract
Ordinary classification techniques can
drive a conceptually simple constituent
parser that achieves near state-of-the-art
accuracy on standard test sets. Here we
present such a parser, which avoids some
of the limitations of other discriminative
parsers. In particular, it does not place
any restrictions upon which types of fea-
tures are allowed. We also present sev-
eral innovations for faster training of dis-
criminative parsers: we show how train-
ing can be parallelized, how examples
can be generated prior to training with-
out a working parser, and how indepen-
dently trained sub-classifiers that have
never done any parsing can be effectively
combined into a working parser. Finally,
we propose a new figure-of-merit for best-
first parsing with confidence-rated infer-
ences. Our implementation is freely avail-
able at: http://cs.nyu.edu/?turian/
software/parser/
1 Introduction
Discriminative machine learning methods have im-
proved accuracy on many NLP tasks, such as POS-
tagging (Toutanova et al, 2003), machine translation
(Och & Ney, 2002), and relation extraction (Zhao &
Grishman, 2005). There are strong reasons to believe
the same would be true of parsing. However, only
limited advances have been made thus far, perhaps
due to various limitations of extant discriminative
parsers. In this paper, we present some innovations
aimed at reducing or eliminating some of these lim-
itations, specifically for the task of constituent pars-
ing:
? We show how constituent parsing can be per-
formed using standard classification techniques.
? Classifiers for different non-terminal labels can be
induced independently and hence training can be
parallelized.
? The parser can use arbitrary information to evalu-
ate candidate constituency inferences.
? Arbitrary confidence scores can be aggregated in
a principled manner, which allows beam search.
In Section 2 we describe our approach to parsing. In
Section 3 we present experimental results.
The following terms will help to explain our work.
A span is a range over contiguous words in the in-
put sentence. Spans cross if they overlap but nei-
ther contains the other. An item (or constituent) is
a (span, label) pair. A state is a set of parse items,
none of which may cross. A parse inference is a pair
(S , i), given by the current state S and an item i to be
added to it. A parse path (or history) is a sequence
of parse inferences over some input sentence (Klein
& Manning, 2001). An item ordering (ordering, for
short) constrains the order in which items may be in-
ferred. In particular, if we prescribe a complete item
ordering, the parser is deterministic (Marcus, 1980)
and each state corresponds to a unique parse path.
For some input sentence and gold-standard parse, a
state is correct if the parser can infer zero or more
additional items to obtain the gold-standard parse. A
parse path is correct if it leads to a correct state. An
141
inference is correct if adding its item to its state is
correct.
2 Parsing by Classification
Recall that with typical probabilistic parsers, our
goal is to output the parse ?P with the highest like-
lihood for the given input sentence x:
?P = arg max
P?P(x)
Pr(P) (1)
= arg max
P?P(x)
?
I?P
Pr(I) (2)
or, equivalently,
= arg max
P?P(x)
?
I?P
log(Pr(I)) (3)
where each I is a constituency inference in the parse
path P.
In this work, we explore a generalization in which
each inference I is assigned a real-valued confidence
score Q(I) and individual confidences are aggre-
gated using some function A, which need not be a
sum or product:
?P = arg max
P?P(x)
A
I?P
Q(I) (4)
In Section 2.1 we describe how we induce scoring
function Q(I). In Section 2.2 we discuss the aggre-
gation function A. In Section 2.3 we describe the
method used to restrict the size of the search space
over P(x).
2.1 Learning the Scoring Function Q(I)
During training, our goal is to induce the scoring
function Q, which assigns a real-valued confidence
score Q(I) to each candidate inference I (Equa-
tion 4). We treat this as a classification task: If infer-
ence I is correct, we would like Q(I) to be a positive
value, and if inference I is incorrect, we would like
Q(I) to be a negative value.
Training discriminative parsers can be computa-
tionally very expensive. Instead of having a single
classifier score every inference, we parallelize train-
ing by inducing 26 sub-classifiers, one for each con-
stituent label ? in the Penn Treebank (Taylor, Mar-
cus, & Santorini, 2003): Q(I?) = Q?(I?), where
Q? is the ?-classifier and I? is an inference that in-
fers a constituent with label ?. For example, the VP-
classifier QVP would score the VP-inference in Fig-
ure 1, preferably assigning it a positive confidence.
Figure 1 A candidate VP-inference, with head-
children annotated using the rules given in (Collins,
1999).
VP (was)
NP (timing) VBD / was ADJP (perfect)
DT / The NN / timing JJ / perfect
Each ?-classifier is independently trained on training
set E?, where each example e? ? E? is a tuple (I?, y),
I? is a candidate ?-inference, and y ? {?1}. y = +1 if
I? is a correct inference and ?1 otherwise. This ap-
proach differs from that of Yamada and Matsumoto
(2003) and Sagae and Lavie (2005), who parallelize
according to the POS tag of one of the child items.
2.1.1 Generating Training Examples
Our method of generating training examples does
not require a working parser, and can be run prior to
any training. It is similar to the method used in the
literature by deterministic parsers (Yamada & Mat-
sumoto, 2003; Sagae & Lavie, 2005) with one ex-
ception: Depending upon the order constituents are
inferred, there may be multiple bottom-up paths that
lead to the same final parse, so to generate training
examples we choose a single random path that leads
to the gold-standard parse tree.1 The training ex-
amples correspond to all candidate inferences con-
sidered in every state along this path, nearly all of
which are incorrect inferences (with y = ?1). For
instance, only 4.4% of candidate NP-inferences are
correct.
2.1.2 Training Algorithm
During training, for each label ? we induce scor-
ing function Q? to minimize the loss over training
examples E?:
Q? = arg min
Q??
?
(I?,y)?E?
L(y ? Q??(I?)) (5)
1 The particular training tree paths used in our experiments are
included in the aforementioned implementation so that our
results can be replicated under the same experimental condi-
tions.
142
where y ? Q?(I?) is the margin of example (I?, y).
Hence, the learning task is to maximize the margins
of the training examples, i.e. induce scoring function
Q? such that it classifies correct inferences with pos-
itive confidence and incorrect inferences with nega-
tive confidence. In our work, we minimized the lo-
gistic loss:
L(z) = log(1 + exp(?z)) (6)
i.e. the negative log-likelihood of the training sam-
ple.
Our classifiers are ensembles of decisions trees,
which we boost (Schapire & Singer, 1999) to min-
imize the above loss using the update equations
given in Collins, Schapire, and Singer (2002). More
specifically, classifier QT? is an ensemble comprising
decision trees q1?, . . . , q
T
? , where:
QT? (I?) =
T
?
t=1
qt?(I?) (7)
At iteration t, decision tree qt? is grown, its leaves
are confidence-rated, and it is added to the ensemble.
The classifier for each constituent label is trained in-
dependently, so we henceforth omit ? subscripts.
An example (I, y) is assigned weight wt(I, y):2
wt(I, y) = 1
1 + exp(y ? Qt?1(I)) (8)
The total weight of y-value examples that fall in leaf
f is W tf ,y:
W tf ,y =
?
(I,y?)?E
y?=y, I? f
wt(I, y) (9)
and this leaf has loss Ztf :
Ztf = 2 ?
?
W tf ,+ ?W
t
f ,? (10)
Growing the decision tree: The loss of the entire
decision tree qt is
Z(qt) =
?
leaf f?qt
Ztf (11)
2 If we were to replace this equation with wt(I, y) =
exp(y?Qt?1(I))?1, but leave the remainder of the algorithm un-
changed, this algorithm would be confidence-rated AdaBoost
(Schapire & Singer, 1999), minimizing the exponential loss
L(z) = exp(?z). In preliminary experiments, however, we
found that the logistic loss provided superior generalization
accuracy.
We will use Zt as a shorthand for Z(qt). When grow-
ing the decision tree, we greedily choose node splits
to minimize this Z (Kearns & Mansour, 1999). In
particular, the loss reduction of splitting leaf f us-
ing feature ? into two children, f ? ? and f ? ??, is
?Ztf (?):
?Ztf (?) = Ztf ? (Ztf?? + Ztf???) (12)
To split node f , we choose the ?? that reduces loss
the most:
?? = arg max
???
?Ztf (?) (13)
Confidence-rating the leaves: Each leaf f is
confidence-rated as ?tf :
?tf =
1
2
? log
W tf ,+ + 
W tf ,? + 
(14)
Equation 14 is smoothed by the  term (Schapire
& Singer, 1999) to prevent numerical instability in
the case that either W tf ,+ or W
t
f ,? is 0. In our ex-
periments, we used  = 10?8. Although our exam-
ple weights are unnormalized, so far we?ve found
no benefit from scaling  as Collins and Koo (2005)
suggest. All inferences that fall in a particular leaf
node are assigned the same confidence: if inference
I falls in leaf node f in the tth decision tree, then
qt(I) = ?tf .
2.1.3 Calibrating the Sub-Classifiers
An important concern is when to stop growing the
decision tree. We propose the minimum reduction
in loss (MRL) stopping criterion: During training,
there is a value ?t at iteration t which serves as a
threshold on the minimum reduction in loss for leaf
splits. If there is no splitting feature for leaf f that
reduces loss by at least ?t then f is not split. For-
mally, leaf f will not be bisected during iteration t if
max??? ?Ztf (?) < ?t. The MRL stopping criterion
is essentially `0 regularization:?t corresponds to the
`0 penalty parameter and each feature with non-zero
confidence incurs a penalty of ?t, so to outweigh the
penalty each split must reduce loss by at least ?t.
?t decreases monotonically during training at
the slowest rate possible that still allows train-
ing to proceed. We start by initializing ?1 to ?,
and at the beginning of iteration t we decrease ?t
only if the root node ? of the decision tree can-
not be split. Otherwise, ?t is set to ?t?1. Formally,
143
?t = min(?t?1,max??? ?Zt?(?)). In this manner, the
decision trees are induced in order of decreasing ?t.
During training, the constituent classifiers Q?
never do any parsing per se, and they train at dif-
ferent rates: If ? , ??, then ?t? isn?t necessarily
equal to ?t?? . We calibrate the different classifiers by
picking some meta-parameter ?? and insisting that
the sub-classifiers comprised by a particular parser
have all reached some fixed ? in training. Given ??,
the constituent classifier for label ? is Qt?, where
?t? ? ?? > ?
t+1
? . To obtain the final parser, we
cross-validate ??, picking the value whose set of con-
stituent classifiers maximizes accuracy on a devel-
opment set.
2.1.4 Types of Features used by the Scoring
Function
Our parser operates bottom-up. Let the frontier of
a state be the top-most items (i.e. the items with no
parents). The children of a candidate inference are
those frontier items below the item to be inferred, the
left context items are those frontier items to the left
of the children, and the right context items are those
frontier items to the right of the children. For exam-
ple, in the candidate VP-inference shown in Figure 1,
the frontier comprises the NP, VBD, and ADJP items,
the VBD and ADJP items are the children of the VP-
inference (the VBD is its head child), the NP is the left
context item, and there are no right context items.
The design of some parsers in the literature re-
stricts the kinds of features that can be usefully and
efficiently evaluated. Our scoring function and pars-
ing algorithm have no such limitations. Q can, in
principle, use arbitrary information from the history
to evaluate constituent inferences. Although some of
our feature types are based on prior work (Collins,
1999; Klein & Manning, 2003; Bikel, 2004), we
note that our scoring function uses more history in-
formation than typical parsers.
All features check whether an item has some
property; specifically, whether the item?s la-
bel/headtag/headword is a certain value. These fea-
tures perform binary tests on the state directly, un-
like Henderson (2003) which works with an inter-
mediate representation of the history. In our baseline
setup, feature set ? contained five different feature
types, described in Table 1.
Table 2 Feature item groups.
? all children
? all non-head children
? all non-leftmost children
? all non-rightmost children
? all children left of the head
? all children right of the head
? head-child and all children left of the head
? head-child and all children right of the head
2.2 Aggregating Confidences
To get the cumulative score of a parse path P, we ap-
ply aggregatorA over the confidences Q(I) in Equa-
tion 4. Initially, we definedA in the customary fash-
ion as summing the loss of each inference?s confi-
dence:
?P = arg max
P?P(x)
?
?
?
?
?
?
?
?
?
I?P
L (Q(I))
?
?
?
?
?
?
?
(15)
with the logistic loss L as defined in Equation 6. (We
negate the final sum because we want to minimize
the loss.) This definition of A is motivated by view-
ing L as a negative log-likelihood given by a logistic
function (Collins et al, 2002), and then using Equa-
tion 3. It is also inspired by the multiclass loss-based
decoding method of Schapire and Singer (1999).
With this additive aggregator, loss monotonically in-
creases as inferences are added, as in a PCFG-based
parser in which all productions decrease the cumu-
lative probability of the parse tree.
In preliminary experiments, this aggregator gave
disappointing results: precision increased slightly,
but recall dropped sharply. Exploratory data analy-
sis revealed that, because each inference incurs some
positive loss, the aggregator very cautiously builds
the smallest trees possible, thus harming recall. We
had more success by defining A to maximize the
minimum confidence. Essentially,
?P = arg max
P?P(x)
min
I?P
Q(I) (16)
Ties are broken according to the second lowest con-
fidence, then the third lowest, and so on.
2.3 Search
Given input sentence x, we choose the parse path P
in P(x) with the maximum aggregated score (Equa-
tion 4). Since it is computationally intractable to
144
Table 1 Types of features.
? Child item features test if a particular child item has some property. E.g. does the item one right of the
head have headword ?perfect?? (True in Figure 1)
? Context item features test if a particular context item has some property. E.g. does the first item of left
context have headtag NN? (True)
? Grandchild item features test if a particular grandchild item has some property. E.g. does the leftmost
child of the rightmost child item have label JJ? (True)
? Exists features test if a particular group of items contains an item with some property. E.g. does some
non-head child item have label ADJP? (True) Exists features select one of the groups of items specified in
Table 2. Alternately, they can select the terminals dominated by that group. E.g. is there some terminal
item dominated by non-rightmost children items that has headword ?quux?? (False)
consider every possible sequence of inferences, we
use beam search to restrict the size of P(x). As
an additional guard against excessive computation,
search stopped if more than a fixed maximum num-
ber of states were popped from the agenda. As usual,
search also ended if the highest-priority state in the
agenda could not have a better aggregated score than
the best final parse found thus far.
3 Experiments
Following Taskar, Klein, Collins, Koller, and Man-
ning (2004), we trained and tested on ? 15 word sen-
tences in the English Penn Treebank (Taylor et al,
2003), 10% of the entire treebank by word count.3
We used sections 02?21 (9753 sentences) for train-
ing, section 24 (321 sentences) for development,
and section 23 (603 sentences) for testing, prepro-
cessed as per Table 3. We evaluated our parser us-
ing the standard PARSEVAL measures (Black et
al., 1991): labelled precision, recall, and F-measure
(LPRC, LRCL, and LFMS, respectively), which are
computed based on the number of constituents in the
parser?s output that match those in the gold-standard
parse. We tested whether the observed differences in
PARSEVAL measures are significant at p = 0.05 us-
ing a stratified shuffling test (Cohen, 1995, Section
5.3.2) with one million trials.4
As mentioned in Section 1, the parser cannot in-
fer any item that crosses an item already in the state.
3 There was insufficient time before deadline to train on all
sentences.
4 The shuffling test we used was originally implemented
by Dan Bikel (http://www.cis.upenn.edu/?dbikel/
software.html) and subsequently modified to compute p-
values for LFMS differences.
We placed three additional candidacy restrictions
on inferences: (a) Items must be inferred under the
bottom-up item ordering; (b) To ensure the parser
does not enter an infinite loop, no two items in a state
can have both the same span and the same label;
(c) An item can have no more than K = 5 children.
(Only 0.24% of non-terminals in the preprocessed
development set have more than five children.) The
number of candidate inferences at each state, as well
as the number of training examples generated by the
algorithm in Section 2.1.1, is proportional to K. In
our experiment, there were roughly |E?| ? 1.7 mil-
lion training examples for each classifier.
3.1 Baseline
In the baseline setting, context item features (Sec-
tion 2.1.4) could refer to the two nearest items of
context in each direction. The parser used a beam
width of 1000, and was terminated in the rare event
that more than 10,000 states were popped from the
agenda. Figure 2 shows the accuracy of the base-
line on the development set as training progresses.
Cross-validating the choice of ?? against the LFMS
(Section 2.1.3) suggested an optimum of ?? = 1.42.
At this ??, there were a total of 9297 decision tree
splits in the parser (summed over all constituent
classifiers), LFMS = 87.16, LRCL = 86.32, and
LPRC = 88.02.
3.2 Beam Width
To determine the effect of the beam width on the
accuracy, we evaluated the baseline on the devel-
opment set using a beam width of 1, i.e. parsing
entirely greedily (Wong & Wu, 1999; Kalt, 2004;
Sagae & Lavie, 2005). Table 4 compares the base-
145
Table 3 Steps for preprocessing the data. Starred steps are performed only on input with tree structure.
1. * Strip functional tags and trace indices, and remove traces.
2. * Convert PRT to ADVP. (This convention was established by Magerman (1995).)
3. Remove quotation marks (i.e. terminal items tagged ?? or ??). (Bikel, 2004)
4. * Raise punctuation. (Bikel, 2004)
5. Remove outermost punctuation.a
6. * Remove unary projections to self (i.e. duplicate items with the same span and label).
7. POS tag the text using Ratnaparkhi (1996).
8. Lowercase headwords.
9. Replace any word observed fewer than 5 times in the (lower-cased) training sentences with UNK.
a As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation may discard useful information.
It?s also worth noting that Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser
after adding punctuation features, one of which encoded the sentence-final punctuation.
Figure 2 PARSEVAL scores of the baseline on the ? 15 words development set of the Penn Treebank. The
top x-axis shows accuracy as the minimum reduction in loss ?? decreases. The bottom shows the correspond-
ing number of decision tree splits in the parser, summed over all classifiers.
74%
76%
78%
80%
82%
84%
86%
88%
90%
 20000 10000 5000 2500 1000 250
74%
76%
78%
80%
82%
84%
86%
88%
90%
0.341.02.75.0102540120
PA
R
SE
VA
L 
sc
or
e
Total # of splits
Minimum reduction in loss
Labelled precision
Labelled F-measure
Labelled recall
line results on the development set with a beam
width of 1 and a beam width of 1000.5 The wider
beam seems to improve the PARSEVAL scores of
the parser, although we were unable to detect a sta-
tistically significant improvement in LFMS on our
relatively small development set.
5 Using a beam width of 100,000 yielded output identical to
using a beam width of 1000.
3.3 Context Size
Table 5 compares the baseline to parsers that could
not examine as many context items. A significant
portion of the baseline?s accuracy is due to contex-
tual clues, as evidenced by the poor accuracy of the
no context run. However, we did not detect a signif-
icant difference between using one context item or
two.
146
Table 4 PARSEVAL results on the ? 15 words
development set of the baseline, varying the beam
width. Also, the MRL that achieved this LFMS and
the total number of decision tree splits at this MRL.
Dev Dev Dev MRL #splits
LFMS LRCL LPRC ?? total
Beam=1 86.36 86.20 86.53 2.03 7068
Baseline 87.16 86.32 88.02 1.42 9297
Table 5 PARSEVAL results on the ? 15 words de-
velopment set, given the amount of context avail-
able. is statistically significant. The score differences
between ?context 0? and ?context 1? are significant,
whereas the differences between ?context 1? and the
baseline are not.
Dev Dev Dev MRL #splits
LFMS LRCL LPRC ?? total
Context 0 75.15 75.28 75.03 3.38 3815
Context 1 86.93 85.78 88.12 2.45 5588
Baseline 87.16 86.32 88.02 1.42 9297
Table 6 PARSEVAL results of decision stumps on
the ? 15 words development set, through 8200
splits. The differences between the stumps run and
the baseline are statistically significant.
Dev Dev Dev MRL #splits
LFMS LRCL LPRC ?? total
Stumps 85.72 84.65 86.82 2.39 5217
Baseline 87.07 86.05 88.12 1.92 7283
3.4 Decision Stumps
Our features are of relatively fine granularity. To test
if a less powerful machine could provide accuracy
comparable to the baseline, we trained a parser in
which we boosted decisions stumps, i.e. decision
trees of depth 1. Stumps are equivalent to learning
a linear discriminant over the atomic features. Since
the stumps run trained quite slowly, it only reached
8200 splits total. To ensure a fair comparison, in Ta-
ble 6 we chose the best baseline parser with at most
8200 splits. The LFMS of the stumps run on the de-
velopment set was 85.72%, significantly less accu-
rate than the baseline.
For example, Figure 3 shows a case where NP
classification better served by the informative con-
junction ?1 ? ?2 found by the decision trees. Given
Figure 3 An example of a decision (a) stump and
(b) tree for scoring NP-inferences. Each leaf?s value
is the confidence assigned to all inferences that fall
in this leaf. ?1 asks ?does the first child have a de-
terminer headtag??. ?2 asks ?does the last child have
a noun label??. NP classification is better served by
the informative conjunction ?1??2 found by the de-
cision trees.
(a)
?1
true f alse
+0.5 0
(b)
?1
true f alse
?2
true f alse
0
+1.0 -0.2
Table 7 PARSEVAL results of deterministic parsers
on the ? 15 words development set through 8700
splits. A shaded cell means that the difference be-
tween this value and that of the baseline is statisti-
cally significant. All differences between l2r and r2l
are significant.
Dev Dev Dev MRL #splits
LFMS LRCL LPRC ?? total
l2r 83.61 82.71 84.54 3.37 2157
r2l 85.76 85.37 86.15 3.39 1881
Baseline 87.07 86.05 88.12 1.92 7283
the sentence ?The man left?, at the initial state there
are six candidate NP-inferences, one for each span,
and ?(NP The man)? is the only candidate inference
that is correct. ?1 is true for the correct inference and
two of the incorrect inferences (?(NP The)? and ?(NP
The man left)?). ?1 ? ?2, on the other hand, is true
only for the correct inference, and so it is better at
discriminating NPs over this sample.
3.5 Deterministic Parsing
Our baseline parser simulates a non-deterministic
machine, as at any state there may be several correct
decisions. We trained deterministic variations of the
parser, for which we imposed strict left-to-right (l2r)
and right-to-left (r2l) item orderings. For these vari-
ations we generated training examples using the cor-
responding unique path to each gold-standard train-
ing tree. The r2l run reached only 8700 splits to-
tal, so in Table 7 we chose the best baseline and l2r
147
Table 8 PARSEVAL results of the full vocabulary
parser on the ? 15 words development set. The dif-
ferences between the full vocabulary run and the
baseline are not statistically significant.
Dev Dev Dev MRL #splits
LFMS LRCL LPRC ? total
Baseline 87.16 86.32 88.02 1.42 9297
Full vocab 87.50 86.85 88.15 1.27 10711
parser with at most 8700 splits.
r2l parsing is significantly more accurate than l2r.
The reason is that the deterministic runs (l2r and r2l)
must avoid prematurely inferring items that come
later in the item ordering. This puts the l2r parser
in a tough spot. If it makes far-right decisions, it?s
more likely to prevent correct subsequent decisions
that are earlier in the l2r ordering, i.e. to the left.
But if it makes far-left decisions, then it goes against
the right-branching tendency of English sentences.
In contrast, the r2l parser is more likely to be correct
when it infers far-right constituents.
We also observed that the accuracy of the de-
terministic parsers dropped sharply as training pro-
gressed (See Figure 4). This behavior was unex-
pected, as the accuracy curve levelled off in every
other experiment. In fact, the accuracy of the deter-
ministic parsers fell even when parsing the training
data. To explain this behavior, we examined the mar-
gin distributions of the r2l NP-classifier (Figure 5).
As training progressed, the NP-classifier was able to
reduce loss by driving up the margins of the incor-
rect training examples, at the expense of incorrectly
classifying a slightly increased number of correct
training examples. However, this is detrimental to
parsing accuracy. The more correct inferences with
negative confidence, the less likely it is at some state
that the highest confidence inference is correct. This
effect is particularly pronounced in the deterministic
setting, where there is only one correct inference per
state.
3.6 Full Vocabulary
As in traditional parsers, the baseline was smoothed
by replacing any word that occurs fewer than five
times in the training data with the special token UNK
(Table 3.9). Table 8 compares the baseline to a full
vocabulary run, in which the vocabulary contained
all words observed in the training data. As evidenced
by the results therein, controlling for lexical sparsity
did not significantly improve accuracy in our setting.
In fact, the full vocabulary run is slightly more ac-
curate than the baseline on the development set, al-
though this difference was not statistically signifi-
cant. This was a late-breaking result, and we used
the full vocabulary condition as our final parser for
parsing the test set.
3.7 Test Set Results
Table 9 shows the results of our best parser on the
? 15 words test set, as well as the accuracy reported
for a recent discriminative parser (Taskar et al,
2004) and scores we obtained by training and test-
ing the parsers of Charniak (2000) and Bikel (2004)
on the same data. Bikel (2004) is a ?clean room?
reimplementation of the Collins parser (Collins,
1999) with comparable accuracy. Both Charniak
(2000) and Bikel (2004) were trained using the gold-
standard tags, as this produced higher accuracy on
the development set than using Ratnaparkhi (1996)?s
tags.
3.8 Exploratory Data Analysis
To gain a better understanding of the weaknesses of
our parser, we examined a sample of 50 develop-
ment sentences that the full vocabulary parser did
not get entirely correct. Besides noise and cases of
genuine ambiguity, the following list outlines all er-
ror types that occurred in more than five sentences,
in roughly decreasing order of frequency. (Note that
there is some overlap between these groups.)
? ADVPs and ADJPs A disproportionate amount of
the parser?s error was due to ADJPs and ADVPs.
Out of the 12.5% total error of the parser on the
development set, an absolute 1.0% was due to
ADVPs, and 0.9% due to ADJPs. The parser had
LFMS = 78.9%,LPRC = 82.5%,LRCL = 75.6%
on ADVPs, and LFMS = 68.0%,LPRC =
71.2%,LRCL = 65.0% on ADJPs.
These constructions can sometimes involve tricky
attachment decisions. For example, in the frag-
ment ?to get fat in times of crisis?, the parser?s
output was ?(VP to (VP get (ADJP fat (PP in (NP
(NP times) (PP of (NP crisis)))))))? instead of the
correct construction ?(VP to (VP get (ADJP fat) (PP
in (NP (NP times) (PP of (NP crisis))))))?.
148
Figure 4 LFMS of the baseline and the deterministic runs on the ? 15 words development set of the Penn
Treebank. The x-axis shows the LFMS as training progresses and the number of decision tree splits in-
creases.
 74
 76
 78
 80
 82
 84
 86
 88
 8700 5000 2500 1000 250
 74
 76
 78
 80
 82
 84
 86
 88
Pa
rs
ev
al
 F
M
S
Total # of splits
Baseline
Right-to-left
Left-to-right
Figure 5 The margin distributions of the r2l NP-classifier, early in training and late in training, (a) over the
incorrect training examples and (b) over the correct training examples.
(a)
-20
 0
 20
 40
 60
 80
 100
 120
 140
 160
 0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  1
M
ar
gi
n
Percentile
Late in training
Early in training
(b)
-40
-30
-20
-10
 0
 10
 20
 0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  1
M
ar
gi
n
Percentile
Late in training
Early in training
The amount of noise present in ADJP and ADVP
annotations in the PTB is unusually high. Annota-
tion of ADJP and ADVP unary projections is partic-
ularly inconsistent. For example, the development
set contains the sentence ?The dollar was trading
sharply lower in Tokyo .?, with ?sharply lower?
bracketed as ?(ADVP (ADVP sharply) lower)?.
?sharply lower? appears 16 times in the complete
training section, every time bracketed as ?(ADVP
sharply lower)?, and ?sharply higher? 10 times,
always as ?(ADVP sharply higher)?. Because of the
high number of negative examples, the classifiers?
149
Table 9 PARSEVAL results of on the ? 15 words test set of various parsers in the literature. The differ-
ences between the full vocabulary run and Bikel or Charniak are significant. Taskar et al (2004)?s output
was unavailable for significance testing, but presumably its differences from the full vocab parser are also
significant.
Test Test Test Dev Dev Dev
LFMS LRCL LPRC LFMS LRCL LPRC
Full vocab 87.13 86.47 87.80 87.50 86.85 88.15
Bikel (2004) 88.85 88.31 89.39 86.82 86.43 87.22
Taskar et al (2004) 89.12 89.10 89.14 89.98 90.22 89.74
Charniak (2000) 90.09 90.01 90.17 89.50 89.69 89.32
bias is to cope with the noise by favoring negative
confidences predictions for ambiguous ADJP and
ADVP decisions, hence their abysmal labelled re-
call. One potential solution is the weight-sharing
strategy described in Section 3.5.
? Tagging Errors Many of the parser?s errors
were due to poor tagging. Preprocessing sentence
?Would service be voluntary or compulsory ??
gives ?would/MD service/VB be/VB voluntary/JJ
or/CC UNK/JJ? and, as a result, the parser brack-
ets ?service . . . compulsory? as a VP instead of
correctly bracketing ?service? as an NP. We also
found that the tagger we used has difficulties with
completely capitalized words, and tends to tag
them NNP. By giving the parser access to the same
features used by taggers, especially rich lexical
features (Toutanova et al, 2003), the parser might
learn to compensate for tagging errors.
? Attachment decisions The parser does not de-
tect affinities between certain word pairs, so it has
difficulties with bilexical dependency decisions.
In principle, bilexical dependencies can be rep-
resented as conjunctions of feature given in Sec-
tion 2.1.4. Given more training data, the parser
might learn these affinities.
4 Conclusions
In this work, we presented a near state-of-the-
art approach to constituency parsing which over-
comes some of the limitations of other discrimina-
tive parsers. Like Yamada and Matsumoto (2003)
and Sagae and Lavie (2005), our parser is driven by
classifiers. Even though these classifiers themselves
never do any parsing during training, they can be
combined into an effective parser. We also presented
a beam search method under the objective function
of maximizing the minimum confidence.
To ensure efficiency, some discriminative parsers
place stringent requirements on which types of fea-
tures are permitted. Our approach requires no such
restrictions and our scoring function can, in prin-
ciple, use arbitrary information from the history to
evaluate constituent inferences. Even though our
features may be of too fine granularity to dis-
criminate through linear combination, discrimina-
tively trained decisions trees determine useful fea-
ture combinations automatically, so adding new fea-
tures requires minimal human effort.
Training discriminative parsers is notoriously
slow, especially if it requires generating examples by
repeatedly parsing the treebank (Collins & Roark,
2004; Taskar et al, 2004). Although training time
is still a concern in our setup, the situation is ame-
liorated by generating training examples in advance
and inducing one-vs-all classifiers in parallel, a tech-
nique similar in spirit to the POS-tag parallelization
in Yamada and Matsumoto (2003) and Sagae and
Lavie (2005).
This parser serves as a proof-of-concept, in that
we have not fully exploited the possibilities of en-
gineering intricate features or trying more complex
search methods. Its flexibility offers many oppor-
tunities for improvement, which we leave to future
work.
Acknowledgments
The authors would like to thank Dan Bikel, Mike
Collins, Ralph Grishman, Adam Meyers, Mehryar
Mohri, Satoshi Sekine, and Wei Wang, as well as the
anonymous reviewers, for their helpful comments
150
and constructive criticism. This research was spon-
sored by an NSF CAREER award, and by an equip-
ment gift from Sun Microsystems.
References
Bikel, D. M. (2004). Intricacies of Collins? pars-
ing model. Computational Linguistics, 30(4),
479?511.
Black, E., Abney, S., Flickenger, D., Gdaniec, C.,
Grishman, R., Harrison, P., et al (1991).
A procedure for quantitatively comparing the
syntactic coverage of English grammars. In
Speech and Natural Language (pp. 306?311).
Charniak, E. (2000). A maximum-entropy-inspired
parser. In NAACL (pp. 132?139).
Cohen, P. R. (1995). Empirical methods for artificial
intelligence. MIT Press.
Collins, M. (1999). Head-driven statistical models
for natural language parsing. Unpublished
doctoral dissertation, UPenn.
Collins, M. (2003). Head-driven statistical models
for natural language parsing. Computational
Linguistics, 29(4), 589?637.
Collins, M., & Koo, T. (2005). Discriminative
reranking for natural language parsing. Com-
putational Linguistics, 31(1), 25?69.
Collins, M., & Roark, B. (2004). Incremental pars-
ing with the perceptron algorithm. In ACL.
Collins, M., Schapire, R. E., & Singer, Y. (2002).
Logistic regression, AdaBoost and Bregman
distances. Machine Learning, 48(1-3), 253?
285.
Henderson, J. (2003). Inducing history representa-
tions for broad coverage statistical parsing. In
HLT/NAACL.
Kalt, T. (2004). Induction of greedy controllers
for deterministic treebank parsers. In EMNLP
(pp. 17?24).
Kearns, M. J., & Mansour, Y. (1999). On the boost-
ing ability of top-down decision tree learning
algorithms. Journal of Computer and Systems
Sciences, 58(1), 109?128.
Klein, D., & Manning, C. D. (2001). Parsing and
hypergraphs. In IWPT (pp. 123?134).
Klein, D., & Manning, C. D. (2003). Accurate un-
lexicalized parsing. In ACL (pp. 423?430).
Magerman, D. M. (1995). Statistical decision-tree
models for parsing. In ACL (pp. 276?283).
Marcus, M. P. (1980). Theory of syntactic recogni-
tion for natural languages. MIT Press.
Och, F. J., & Ney, H. (2002). Discriminative training
and maximum entropy models for statistical
machine translation. In ACL.
Ratnaparkhi, A. (1996). A maximum entropy part-
of-speech tagger. In EMNLP (pp. 133?142).
Sagae, K., & Lavie, A. (2005). A classifier-based
parser with linear run-time complexity. In
IWPT.
Schapire, R. E., & Singer, Y. (1999). Improved
boosting using confidence-rated predictions.
Machine Learning, 37(3), 297?336.
Taskar, B., Klein, D., Collins, M., Koller, D., &
Manning, C. (2004). Max-margin parsing.
In EMNLP (pp. 1?8).
Taylor, A., Marcus, M., & Santorini, B. (2003). The
Penn Treebank: an overview. In A. Abeille?
(Ed.), Treebanks: Building and using parsed
corpora (pp. 5?22).
Toutanova, K., Klein, D., Manning, C. D., & Singer,
Y. (2003). Feature-rich part-of-speech tag-
ging with a cyclic dependency network. In
HLT/NAACL (pp. 252?259).
Wong, A., & Wu, D. (1999). Learning a
lightweight robust deterministic parser. In
EUROSPEECH.
Yamada, H., & Matsumoto, Y. (2003). Statistical
dependency analysis with support vector ma-
chines. In IWPT.
Zhao, S., & Grishman, R. (2005). Extracting rela-
tions with integrated information using kernel
methods. In ACL.
151
Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 17?24,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Computational Challenges in Parsing by Classification
Joseph Turian and I. Dan Melamed
{lastname}@cs.nyu.edu
Computer Science Department
New York University
New York, New York 10003
Abstract
This paper presents a discriminative
parser that does not use a generative
model in any way, yet whose accu-
racy still surpasses a generative base-
line. The parser performs feature selec-
tion incrementally during training, as op-
posed to a priori, which enables it to
work well with minimal linguistic clever-
ness. The main challenge in building this
parser was fitting the training data into
memory. We introduce gradient sampling,
which increased training speed 100-fold.
Our implementation is freely available at
http://nlp.cs.nyu.edu/parser/.
1 Introduction
Discriminative machine learning methods have im-
proved accuracy on many NLP tasks, including
POS-tagging, shallow parsing, relation extraction,
and machine translation. However, only limited ad-
vances have been made on full syntactic constituent
parsing. Successful discriminative parsers have used
generative models to reduce training time and raise
accuracy above generative baselines (Collins &
Roark, 2004; Henderson, 2004; Taskar et al, 2004).
However, relying upon information from a gener-
ative model might limit the potential of these ap-
proaches to realize the accuracy gains achieved by
discriminative methods on other NLP tasks. Another
difficulty is that discriminative parsing approaches
can be very task-specific and require quite a bit of
trial and error with different hyper-parameter values
and types of features.
In the present work, we make progress towards
overcoming these obstacles. We propose a flexible,
well-integrated method for training discriminative
parsers, demonstrating techniques that might also
be useful for other structured learning problems.
The learning algorithm projects the hand-provided
atomic features into a compound feature space and
performs incremental feature selection from this
large feature space. We achieve higher accuracy than
a generative baseline, despite not using the standard
trick of including an underlying generative model.
Our training regime does model selection without
ad-hoc smoothing or frequency-based feature cut-
offs, and requires no heuristics to optimize the single
hyper-parameter.
We discuss the computational challenges we over-
came to build this parser. The main difficulty is that
the training data fit in memory only using an indirect
representation,1 so the most costly operation during
training is accessing the features of a particular ex-
ample. We show how to train a parser effectively un-
der these conditions. We also show how to speed up
training by using a principled sampling method to
estimate the loss gradients used in feature selection.
?2 describes the parsing algorithm. ?3 presents
the learning method and techniques used to reduce
training time. ?4 presents experiments with discrim-
inative parsers built using these methods. ?5 dis-
1Similar memory limitations exist in other large-scale NLP
tasks. Syntax-driven SMT systems are typically trained on
an order of magnitude more sentences than English parsers,
and unsupervised estimation methods can generate an arbitrary
number of negative examples (Smith & Eisner, 2005).
17
cusses possible issues in scaling to larger example
sets.
2 Parsing Algorithm
The following terms will help to explain our work.
A span is a range over contiguous words in the in-
put. Spans cross if they overlap but neither contains
the other. An item is a (span, label) pair. A state is a
partial parse, i.e. a set of items, none of whose spans
cross. A parse inference is a (state, item) pair, i.e. a
state and a (consequent) item to be added to it. The
frontier of a state consists of the items with no par-
ents yet. The children of an inference are the frontier
items below the item to be inferred, and the head of
an inference is the child item chosen by head rules
(Collins, 1999, pp. 238?240). A parse path is a se-
quence of parse inferences. For some input sentence
and training parse tree, a state is correct if the parser
can infer zero or more additional items to obtain the
training parse tree and an inference is correct if it
leads to a correct state.
Now, given input sentence s we compute:
p? = arg min
p?P(s)
?
????????
?
i?p
l(i)
?
????????
(1)
where P(s) are possible parses of the sentence, and
the loss (or cost) l of parse p is summed over the
inferences i that lead to the parse. To find p?, the
parsing algorithm considers a sequence of states.
The initial state contains terminal items, whose la-
bels are the POS tags given by Ratnaparkhi (1996).
The parser considers a set of (bottom-up) inferences
at each state. Each inference results in a successor
state to be placed on the agenda. The loss function
l can consider arbitrary properties of the input and
parse state,2 which precludes a tractable dynamic
programming solution to Equation 1. Therefore, we
do standard agenda-based parsing, but instead of
items our agenda stores entire states, as per more
general best-first search over parsing hypergraphs
(Klein & Manning, 2001). Each time we pop a state
from the agenda, l computes a loss for the bottom-
up inferences generated from that state. If the loss
of the popped state exceeds that of the current best
complete parse, search is done and we have found
the optimal parse.
2I.e. we make no context-free assumptions.
3 Training Method
3.1 General Setting
From each training inference i ? I we generate the
tuple ?X(i), y(i), b(i)?. X(i) is a feature vector de-
scribing i, with each element in {0, 1}. The observed
y-value y(i) ? {?1,+1} is determined by whether i
is a correct inference or not. Some training exam-
ples might be more important than others, so each is
given an initial bias b(i) ? R+.
Our goal during training is to induce a real-valued
inference scoring function (hypothesis) h(i;?),
which is a linear model parameterized by a vector
? of reals:
h(i;?) = ? ? X(i) =
?
f
? f ? X f (i) (2)
Each f is a feature. The sign of h(i;?) predicts the
y-value of i and the magnitude gives the confidence
in this prediction.
The training procedure optimizes ? to minimize
the expected risk R:
R(I;?) = L(I;?) + ?(?) (3)
In principle, L can be any loss function, but in the
present work we use the log-loss (Collins et al,
2002):
L(I;?) =
?
i?I
l(i;?) =
?
i?I
b(i) ? ?(?(i;?)) (4)
where:
?(?) = ln(1 + exp(??)) (5)
and the margin of inference i under the current
model ? is:
?(i;?) = y(i) ? h(i;?) (6)
For a particular choice of ?, l(i) in Equation 1 is
computed according to Equation 4 using y(i) = +1
and b(i) = 1.
?(?) in Equation 3 is a regularizer, which penal-
izes overly complex models to reduce overfitting and
generalization error. We use the `1 penalty:
?(?) =
?
f
? ? |? f | (7)
where ? is the `1 parameter that controls the strength
of the regularizer. This choice of objective R is mo-
tivated by Ng (2004), who suggests that, given a
18
learning setting where the number of irrelevant fea-
tures is exponential in the number of training exam-
ples, we can nonetheless learn effectively by build-
ing decision trees to minimize the `1-regularized
log-loss. Conversely, Ng (2004) suggests that most
of the learning algorithms commonly used by dis-
criminative parsers will overfit when exponentially
many irrelevant features are present.3
Learning over an exponential feature space is the
very setting we have in mind. A priori, we define
only a set A of simple atomic features (see ?4).
However, the learner induces compound features,
each of which is a conjunction of possibly negated
atomic features. Each atomic feature can have three
values (yes/no/don?t care), so the size of the com-
pound feature space is 3|A|, exponential in the num-
ber of atomic features. It was also exponential in
the number of training examples in our experiments
(|A| ? |I|).
We use an ensemble of confidence-rated deci-
sion trees (Schapire & Singer, 1999) to represent h.4
Each node in a decision tree corresponds to a com-
pound feature, and the leaves of the decision trees
keep track of the parameter values of the compound
features they represent. To score an inference using
a decision tree, we percolate the inference down to
a leaf and return that leaf?s confidence. The overall
score given to an inference by the whole ensemble
is the sum of the confidences returned by the trees in
the ensemble.
3.2 Boosting `1-Regularized Decision Trees
Listing 1 presents our training algorithm. (Sampling
will be explained in ?3.3. Until then, assume that
the sample S is the entire training set I.) At the be-
ginning of training, the ensemble is empty, ? = 0,
and the `1 parameter ? is set to?. We train until the
objective cannot be further reduced for the current
choice of ?. We then relax the regularization penalty
by decreasing ? and continuing training. We also de-
3including the following learning algorithms:
? unregularized logistic regression
? logistic regression with an `2 penalty (i.e. a Gaussian prior)
? SVMs using most kernels
? multilayer neural nets trained by backpropagation
? the perceptron algorithm
4Turian and Melamed (2005) show that that decision trees ap-
plied to parsing have higher accuracy and training speed than
decision stumps.
Listing 1 Training algorithm.
1: procedure T????(I)
2: ensemble? ?
3: h(i)? 0 for all i ? I
4: for T = 1 . . .? do
5: S ? priority sample I
6: extract X(i) for all i ? S
7: build decision tree t using S
8: percolate every i ? I to a leaf node in t
9: for each leaf f in t do
10: choose ? f to minimize R
11: add ? f to h(i) for all i in this leaf
termine the accuracy of the parser on a held-out de-
velopment set using the previous ? value (before it
was decreased), and can stop training when this ac-
curacy plateaus. In this way, instead of choosing the
best ? heuristically, we can optimize it during a sin-
gle training run (Turian & Melamed, 2005).
Our strategy for optimizing ? to minimize the ob-
jective R (Equation 3) is a variant of steepest descent
(Perkins et al, 2003). Each training iteration has
several steps. First, we choose some new compound
features that have high magnitude gradient with re-
spect to the objective function. We do this by build-
ing a new decision tree, whose leaves represent the
new compound features.5 Second, we confidence-
rate each leaf to minimize the objective over the ex-
amples that percolate down to that leaf. Finally, we
append the decision tree to the ensemble and up-
date parameter vector ? accordingly. In this manner,
compound feature selection is performed incremen-
tally during training, as opposed to a priori.
To build each decision tree, we begin with a root
node, and we recursively split nodes by choosing a
splitting feature that will allow us to decrease the
objective. We have:
?L(I;?)
?? f
=
?
i?I
?l(i;?)
??(i;?) ?
??(i;?)
?? f
(8)
where:
??(i;?)
?? f
= y(i) ? X f (i) (9)
We define the weight of an example under the cur-
rent model as:
w(i;?) = ? ?l(i;?)
??(i;?) = b(i) ?
1
1 + exp(?(i;?)) . (10)
5Any given compound feature can appear in more than one
tree.
19
and:
W y?f (I;?) =
?
i?I
X f (i)=1,y(i)=y?
w(i;?) (11)
Combining Equations 8?11 gives:6
?L
?? f
= W?1f ?W
+1
f (12)
We define the gain G f of feature f as:
G f = max
(
0,
??????
?L
?? f
??????
? ?
)
(13)
Equation 13 has this form because the gradient of the
penalty term is undefined at ? f = 0. This discontinu-
ity is why `1 regularization tends to produce sparse
models. If G f = 0, then the objective R is at its min-
imum with respect to parameter ? f . Otherwise, G f
is the magnitude of the gradient of the objective as
we adjust ? f in the appropriate direction.
The gain of splitting node f using some atomic
feature a is defined as
?G f (a) = G f?a + G f??a (14)
We allow node f to be split only by atomic features
a that increase the gain, i.e. ?G f (a) > G f . If no such
feature exists, then f becomes a leaf node of the de-
cision tree and ? f becomes one of the values to be
optimized during the parameter update step. Other-
wise, we choose atomic feature a? to split node f :
a? = arg max
a?A
?G f (a) (15)
This split creates child nodes f ? a? and f ??a?. If no
root node split has positive gain, then training has
converged for the current choice of `1 parameter ?.
Parameter update is done sequentially on only the
most recently added compound features, which cor-
respond to the leaves of the new decision tree. After
the entire tree is built, we percolate examples down
to their appropriate leaf nodes. We then choose for
each leaf node f the parameter ? f that minimizes the
objective R over the examples in that leaf. Decision
trees ensure that these compound features are mu-
tually exclusive, so they can be directly optimized
independently of each other using a line search over
the objective R.
6Since ? is fixed during a particular training iteration and I is
fixed throughout training, we omit parameters (I;?) henceforth.
3.3 Sampling for Faster Feature Selection
Building a decision tree using the entire example set
I can be very expensive, which we will demonstrate
in ?4.2. However, feature selection can be effective
even if we don?t examine every example. Since the
weight of high-margin examples can be several or-
ders of magnitude lower than that of low-margin ex-
amples (Equation 10), the contribution of the high-
margin examples to feature weights (Equation 11)
will be insignificant. Therefore, we can ignore most
examples during feature selection as long as we have
good estimates of feature weights, which in turn give
good estimates of the loss gradients (Equation 12).
As shown in Step 1.5 of Listing 1, before building
each decision tree we use priority sampling (Duffield
et al, 2005) to choose a small subset of the ex-
amples according to the example weights given by
the current classifier, and the tree is built using only
this subset. We make the sample small enough that
its entire atomic feature matrix will fit in memory.
To optimize decision tree building, we compute and
cache the sample?s atomic feature matrix in advance
(Step 1.6).
Even if the sample is missing important informa-
tion in one iteration, the training procedure is capa-
ble of recovering it from samples used in subsequent
iterations. Moreover, even if a sample?s gain esti-
mates are inaccurate and the feature selection step
chooses irrelevant compound features, confidence
updates are based upon the entire training set and
the regularization penalty will prevent irrelevant fea-
tures from having their parameters move away from
zero.
3.4 The Training Set
Our training set I contains all inferences considered
in every state along the correct path for each gold-
standard parse tree (Sagae & Lavie, 2005).7 This
method of generating training examples does not re-
quire a working parser and can be run prior to any
training. The downside of this approach is that it
minimizes the error of the parser at correct states
only. It does not account for compounded error or
teach the parser to recover from mistakes gracefully.
7Since parsing is done deterministically right-to-left, there can
be no more than one correct inference at each state.
20
Turian and Melamed (2005) observed that uni-
form example biases b(i) produced lower accuracy
as training progressed, because the induced classi-
fiers minimized the example-wise error. Since we
aim to minimize the state-wise error, we express this
bias by assigning every training state equal value,
and?for the examples generated from that state?
sharing half the value uniformly among the nega-
tive examples and the other half uniformly among
the positive examples.
Although there are O(n2) possible spans over a
frontier containing n items, we reduce this to the
O(n) inferences that cannot have more than 5 chil-
dren. With no restriction on the number of children,
there would be O(n2) bottom-up inferences at each
state. However, only 0.57% of non-terminals in the
preprocessed development set have more than five
children.
Like Turian and Melamed (2005), we parallelize
training by inducing 26 label classifiers (one for
each non-terminal label in the Penn Treebank). Par-
allelization might not uniformly reduce training time
because different label classifiers train at different
rates. However, parallelization uniformly reduces
memory usage because each label classifier trains
only on inferences whose consequent item has that
label. Even after parallelization, the atomic feature
matrix cannot be cached in memory. We can store
the training inferences in memory using only an in-
direct representation. More specifically, for each in-
ference i in the training set, we cache in memory
several values: a pointer i to a tree cut, its y-value
y(i), its bias b(i), and its confidence h(i) under the
current model. We cache h(i) throughout training be-
cause it is needed both in computing the gradient of
the objective during decision tree building (Step 1.7)
as well as subsequent minimization of the objective
over the decision tree leaves (Step 1.10). We update
the confidences at the end of each training iteration
using the newly added tree (Step 1.11).
The most costly operation during training is to ac-
cess the feature values in X(i). An atomic feature
test determines the value Xa(i) for a single atomic
feature a by examining the tree cut pointed to by in-
ference i. Alternately, we can perform atomic fea-
ture extraction, i.e. determine all non-zero atomic
features over i.8 Extraction is 100?1000 times more
expensive than a single test, but is necessary during
decision tree building (Step 1.7) because we need
the entire vector X(i) to accumulate inferences in
children nodes. Essentially, for each inference i that
falls in some node f , we accumulate w(i) in Wy(i)f?a
for all a with Xa(i) = 1. After all the inferences in a
node have been accumulated, we try to split the node
(Equation 15). The negative child weights are each
determined as Wyf??a = W
y
f ?W
y
f?a.
4 Experiments
We follow Taskar et al (2004) and Turian and
Melamed (2005) in training and testing on ? 15
word sentences in the English Penn Treebank (Tay-
lor et al, 2003). We used sections 02?21 for train-
ing, section 22 for development, and section 23,
for testing. We use the same preprocessing steps as
Turian and Melamed (2005): during both training
and testing, the parser is given text POS-tagged by
the tagger of Ratnaparkhi (1996), with capitalization
stripped and outermost punctuation removed.
For reasons given in Turian and Melamed (2006),
items are inferred bottom-up right-to-left. As men-
tioned in ?2, the parser cannot infer any item that
crosses an item already in the state. To ensure the
parser does not enter an infinite loop, no two items
in a state can have both the same span and the same
label. Given these restrictions, there were roughly 40
million training examples. These were partitioned
among the constituent label classifiers.
Our atomic feature set A contains features of
the form ?is there an item in group J whose la-
bel/headword/headtag/headtagclass9 is ?X???. Pos-
sible values of ?X? for each predicate are collected
from the training data. Some examples of possible
values for J include the last n child items, the first n
left context items, all right context items, and the ter-
minal items dominated by the non-head child items.
Space constraints prevent enumeration of the head-
tagclasses and atomic feature templates, which are
8Extraction need not take the na??ve approach of performing |A|
different tests, and can be optimized by using knowledge about
the nature of the atomic feature templates.
9The predicate headtagclass is a supertype of the headtag.
Given our compound features, these are not strictly neces-
sary, but they accelerate training. An example is ?proper noun,?
which contains the POS tags given to singular and plural proper
nouns.
21
Figure 1 F1 score of our parser on the development
set of the Penn Treebank, using only ? 15 word sen-
tences. The dashed line indicates the percent of NP
example weight lost due to sampling. The bottom
x-axis shows the number of non-zero parameters in
each parser, summed over all label classifiers.
7.5K5K2.5K1.5K1K 84%
85%
86%
87%
88%
89%
90%
91%5.42.51.00.5
De
vel
. F-
me
asu
re
total number of non-zero parameters
training time (days)
0%
5%
10%
15%
20%
25%
30%
35%
we
igh
t lo
st d
ue 
to s
am
plin
g
instead provided at the URL given in the abstract.
These templates gave 1.1 million different atomic
features. We experimented with smaller feature sets,
but found that accuracy was lower. Charniak and
Johnson (2005) use linguistically more sophisticated
features, and Bod (2003) and Kudo et al (2005) use
sub-tree features, all of which we plan to try in fu-
ture work.
We evaluated our parser using the standard PAR-
SEVAL measures (Black et al, 1991): labelled
precision, labelled recall, and labelled F-measure
(Prec., Rec., and F1, respectively), which are based
on the number of non-terminal items in the parser?s
output that match those in the gold-standard parse.
The solid curve Figure 1 shows the accuracy of
the parser over the development set as training pro-
gressed. The parser exceeded 89% F-measure af-
ter 2.5 days of training. The peak F-measure was
90.55%, achieved at 5.4 days using 6.3K active
parameters. We omit details given by Turian and
Melamed (2006) in favor of a longer discussion in
?4.2.
4.1 Test Set Results
To situate our results in the literature, we compare
our results to those reported by Taskar et al (2004)
and Turian and Melamed (2005) for their discrimi-
native parsers, which were also trained and tested on
? 15 word sentences. We also compare our parser
to a representative non-discriminative parser (Bikel,
Table 1 PARSEVAL results of parsers on the test
set, using only ? 15 word sentences.
F1 % Rec. % Prec. %
Turian and Melamed (2005) 87.13 86.47 87.80
Bikel (2004) 88.30 87.85 88.75
Taskar et al (2004) 89.12 89.10 89.14
our parser 89.40 89.26 89.55
Table 2 Profile of an NP training iteration, given
in seconds, using an AMD Opteron 242 (64-bit,
1.6Ghz). Steps refer to Listing 1.
Step Description mean stddev %
1.5 Sample 1.5s 0.07s 0.7%
1.6 Extraction 38.2s 0.13s 18.6%
1.7 Build tree 127.6s 27.60s 62.3%
1.8 Percolation 31.4s 4.91s 15.3%
1.9?11 Leaf updates 6.2s 1.75s 3.0%
1.5?11 Total 204.9s 32.6s 100.0%
2004),10 the only one that we were able to train and
test under exactly the same experimental conditions
(including the use of POS tags from Ratnaparkhi
(1996)). Table 1 shows the PARSEVAL results of
these four parsers on the test set.
4.2 Efficiency
40% of non-terminals in the Penn Treebank are
NPs. Consequently, the bottleneck in training is
induction of the NP classifier. It was trained on
1.65 million examples. Each example had an aver-
age of 440 non-zero atomic features (stddev 123),
so the direct representation of each example re-
quires a minimum 440 ? sizeof(int) = 1760 bytes,
and the entire atomic feature matrix would re-
quire 1760 bytes ? 1.65 million = 2.8 GB. Con-
versely, an indirectly represent inference requires
no more 32 bytes: two floats (the cached confi-
dence h(i) and the bias term b(i)), a pointer to a
tree cut (i), and a bool (the y-value y(i)). Indi-
rectly storing the entire example set requires only
32 bytes ? 1.65 million = 53 MB plus the treebank
and tree cuts, a total of 400 MB in our implementa-
tion.
We used a sample size of |S | = 100, 000 examples
to build each decision tree, 16.5 times fewer than
the entire example set. The dashed curve in Figure 1
10Bikel (2004) is a ?clean room? reimplementation of the
Collins (1999) model with comparable accuracy.
22
shows the percent of NP example weight lost due
to sampling. As training progresses, fewer examples
are informative to the model. Even though we ignore
94% of examples during feature selection, sampling
loses less than 1% of the example weight after a day
of training.
The NP classifier used in our final parser was
an ensemble containing 2316 trees, which took
five days to build. Overall, there were 96871 de-
cision tree leaves, only 2339 of which were non-
zero. There were an average of 40.4 (7.4 std-
dev) decision tree splits between the root of a
tree and a non-zero leaf, and nearly all non-
zero leaves were conjunctions of atomic fea-
ture negations (e.g. ?(some child item is a verb) ?
?(some child item is a preposition)). The non-zero
leaf confidences were quite small in magnitude
(0.107 mean, 0.069 stddev) but the training exam-
ple margins over the entire ensemble were nonethe-
less quite high: 11.7 mean (2.92 stddev) for correct
inferences, 30.6 mean (11.2 stddev) for incorrect in-
ferences.
Table 2 profiles an NP training iteration, in which
one decision tree is created and added to the
NP ensemble. Feature selection in our algorithm
(Steps 1.5?1.7) takes 1.5+38.2+127.6 = 167.3s, far
faster than in na??ve approaches. If we didn?t do sam-
pling but had 2.8GB to spare, we could eliminate the
extraction step (Step 1.6) and instead cache the en-
tire atomic feature matrix before the loop. However,
tree building (Step 1.7) scales linearly in the number
of examples, and would take 16.5 ?127.6s = 2105.4s
using the entire example set. If we didn?t do sam-
pling and couldn?t cache the atomic feature matrix,
tree building would also require repeatedly perform-
ing extraction. The number of individual feature ex-
tractions needed to build a single decision tree is the
sum over the internal nodes of the number of exam-
ples that percolate down to that node. There are an
average of 40.8 (7.8 stddev) internal nodes in each
tree and most of the examples fall in nearly all of
them. This property is caused by the lopsided trees
induced under `1 regularization. A conservative es-
timate is that each decision tree requires 25 extrac-
tions times the number of examples. So extraction
would add at least 25 ? 16.5 ? 38.2s = 15757.5s on
top of 2105.40s, and hence building each decision
tree would take at least (15757.5+2105.40)/167.3 ?
100 times as long as it does currently.
Our decision tree ensembles contain over two or-
ders of magnitude more compound features than
those in Turian and Melamed (2005). Our overall
training time was roughly equivalent to theirs. This
ratio corroborates the above estimate.
5 Discussion
The NP classifier was trained only on the 1.65 mil-
lion NP examples in the 9753 training sentences with
? 15 words (168.8 examples/sentence). The number
of examples generated is quadratic in the sentence
length, so there are 41.7 million NP examples in all
39832 training sentences of the whole Penn Tree-
bank (1050 examples/sentence), 25 times as many
as we are currently using.
The time complexity of each step in the train-
ing loop (Steps 1.5?11) is linear over the number
of examples used by that step. When we scale up
to the full treebank, feature selection will not re-
quire a sample 25 times larger, so it will no longer
be the bottleneck in training. Instead, each itera-
tion will be dominated by choosing leaf confidences
and then updating the cached example confidences,
which would require 25 ? (31.4s + 6.2s) = 940s per
iteration. These steps are crucial to the current train-
ing algorithm, because it is important to have exam-
ple confidences that are current with respect to the
model. Otherwise, we cannot determine the exam-
ples most poorly classified by the current model, and
will have no basis for choosing an informative sam-
ple.
We might try to save training time by building
many decision trees over a single sample and then
updating the confidences of the entire example set
using all the new trees. But, if this confidence up-
date is done using feature tests, then we have merely
deferred the cost of the confidence update over the
entire example set. The amount of training done on
a particular sample is proportional to the time sub-
sequently spent updating confidences over the entire
example set. To spend less time doing confidence
updates, we must use a training regime that is sub-
linear with respect to the training time. For exam-
ple, Riezler (2004) reports that the `1 regularization
term drives many of the model?s parameters to zero
during conjugate gradient optimization, which are
23
then pruned before subsequent optimization steps to
avoid numerical instability. Instead of building de-
cision tree(s) at each iteration, we could perform n-
best feature selection followed by parallel optimiza-
tion of the objective over the sample.
The main limitation of our work so far is that
we can do training reasonably quickly only on short
sentences, because a sentence with n words gen-
erates O(n2) training inferences in total. Although
generating training examples in advance without a
working parser (Sagae & Lavie, 2005) is much faster
than using inference (Collins & Roark, 2004; Hen-
derson, 2004; Taskar et al, 2004), our training time
can probably be decreased further by choosing a
parsing strategy with a lower branching factor. Like
our work, Ratnaparkhi (1999) and Sagae and Lavie
(2005) generate examples off-line, but their parsing
strategies are essentially shift-reduce so each sen-
tence generates only O(n) training examples.
6 Conclusion
Our work has made advances in both accuracy and
training speed of discriminative parsing. As far as
we know, we present the first discriminative parser
that surpasses a generative baseline on constituent
parsing without using a generative component, and
it does so with minimal linguistic cleverness.
The main bottleneck in our setting was memory.
We could store the examples in memory only using
an indirect representation. The most costly opera-
tion during training was accessing the features of a
particular example from this indirect representation.
We showed how to train a parser effectively under
these conditions. In particular, we used principled
sampling to estimate loss gradients and reduce the
number of feature extractions. This approximation
increased the speed of feature selection 100-fold.
We are exploring methods for scaling training
up to larger example sets. We are also investigat-
ing the relationship between sample size, training
time, classifier complexity, and accuracy. In addi-
tion, we shall make some standard improvements
to our parser. Our parser should infer its own POS
tags. A shift-reduce parsing strategy will generate
fewer examples, and might lead to shorter training
time. Lastly, we plan to give the model linguistically
more sophisticated features. We also hope to apply
the model to other structured learning tasks, such as
syntax-driven SMT.
References
Bikel, D. M. (2004). Intricacies of Collins? parsing model.
Computational Linguistics.
Black, E., Abney, S., Flickenger, D., Gdaniec, C., Grishman,
R., Harrison, P., et al (1991). A procedure for quantitatively
comparing the syntactic coverage of English grammars. In
Speech and Natural Language.
Bod, R. (2003). An efficient implementation of a new DOP
model. In EACL.
Charniak, E., & Johnson, M. (2005). Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In ACL.
Collins, M. (1999). Head-driven statistical models for natural
language parsing. Doctoral dissertation.
Collins, M., & Roark, B. (2004). Incremental parsing with the
perceptron algorithm. In ACL.
Collins, M., Schapire, R. E., & Singer, Y. (2002). Logistic re-
gression, AdaBoost and Bregman distances. Machine Learn-
ing, 48(1-3).
Duffield, N., Lund, C., & Thorup, M. (2005). Prior-
ity sampling estimating arbitrary subset sums. (http:
//arxiv.org/abs/cs.DS/0509026)
Henderson, J. (2004). Discriminative training of a neural net-
work statistical parser. In ACL.
Klein, D., & Manning, C. D. (2001). Parsing and hypergraphs.
In IWPT.
Kudo, T., Suzuki, J., & Isozaki, H. (2005). Boosting-based
parse reranking with subtree features. In ACL.
Ng, A. Y. (2004). Feature selection, `1 vs. `2 regularization, and
rotational invariance. In ICML.
Perkins, S., Lacker, K., & Theiler, J. (2003). Grafting: Fast,
incremental feature selection by gradient descent in function
space. Journal of Machine Learning Research, 3.
Ratnaparkhi, A. (1996). A maximum entropy part-of-speech
tagger. In EMNLP.
Ratnaparkhi, A. (1999). Learning to parse natural language
with maximum entropy models. Machine Learning, 34(1-3).
Riezler, S. (2004). Incremental feature selection of `1 regular-
ization for relaxed maximum-entropy modeling. In EMNLP.
Sagae, K., & Lavie, A. (2005). A classifier-based parser with
linear run-time complexity. In IWPT.
Schapire, R. E., & Singer, Y. (1999). Improved boosting using
confidence-rated predictions. Machine Learning, 37(3).
Smith, N. A., & Eisner, J. (2005). Contrastive estimation: Train-
ing log-linear models on unlabeled data. In ACL.
Taskar, B., Klein, D., Collins, M., Koller, D., & Manning, C.
(2004). Max-margin parsing. In EMNLP.
Taylor, A., Marcus, M., & Santorini, B. (2003). The Penn Tree-
bank: an overview. In A. Abeille? (Ed.), Treebanks: Building
and using parsed corpora (chap. 1).
Turian, J., & Melamed, I. D. (2005). Constituent parsing by
classification. In IWPT.
Turian, J., & Melamed, I. D. (2006). Advances in discriminative
parsing. In ACL.
24
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384?394,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Word representations:
A simple and general method for semi-supervised learning
Joseph Turian
De?partement d?Informatique et
Recherche Ope?rationnelle (DIRO)
Universite? de Montre?al
Montre?al, Que?bec, Canada, H3T 1J4
lastname@iro.umontreal.ca
Lev Ratinov
Department of
Computer Science
University of Illinois at
Urbana-Champaign
Urbana, IL 61801
ratinov2@uiuc.edu
Yoshua Bengio
De?partement d?Informatique et
Recherche Ope?rationnelle (DIRO)
Universite? de Montre?al
Montre?al, Que?bec, Canada, H3T 1J4
bengioy@iro.umontreal.ca
Abstract
If we take an existing supervised NLP sys-
tem, a simple and general way to improve
accuracy is to use unsupervised word
representations as extra word features. We
evaluate Brown clusters, Collobert and
Weston (2008) embeddings, and HLBL
(Mnih & Hinton, 2009) embeddings
of words on both NER and chunking.
We use near state-of-the-art supervised
baselines, and find that each of the three
word representations improves the accu-
racy of these baselines. We find further
improvements by combining different
word representations. You can download
our word features, for off-the-shelf use
in existing NLP systems, as well as our
code, here: http://metaoptimize.
com/projects/wordreprs/
1 Introduction
By using unlabelled data to reduce data sparsity
in the labeled training data, semi-supervised
approaches improve generalization accuracy.
Semi-supervised models such as Ando and Zhang
(2005), Suzuki and Isozaki (2008), and Suzuki
et al (2009) achieve state-of-the-art accuracy.
However, these approaches dictate a particular
choice of model and training regime. It can be
tricky and time-consuming to adapt an existing su-
pervised NLP system to use these semi-supervised
techniques. It is preferable to use a simple and
general method to adapt existing supervised NLP
systems to be semi-supervised.
One approach that is becoming popular is
to use unsupervised methods to induce word
features?or to download word features that have
already been induced?plug these word features
into an existing system, and observe a significant
increase in accuracy. But which word features are
good for what tasks? Should we prefer certain
word features? Can we combine them?
A word representation is a mathematical object
associated with each word, often a vector. Each
dimension?s value corresponds to a feature and
might even have a semantic or grammatical
interpretation, so we call it a word feature.
Conventionally, supervised lexicalized NLP ap-
proaches take a word and convert it to a symbolic
ID, which is then transformed into a feature vector
using a one-hot representation: The feature vector
has the same length as the size of the vocabulary,
and only one dimension is on. However, the
one-hot representation of a word suffers from data
sparsity: Namely, for words that are rare in the
labeled training data, their corresponding model
parameters will be poorly estimated. Moreover,
at test time, the model cannot handle words that
do not appear in the labeled training data. These
limitations of one-hot word representations have
prompted researchers to investigate unsupervised
methods for inducing word representations over
large unlabeled corpora. Word features can be
hand-designed, but our goal is to learn them.
One common approach to inducing unsuper-
vised word representation is to use clustering,
perhaps hierarchical. This technique was used by
a variety of researchers (Miller et al, 2004; Liang,
2005; Koo et al, 2008; Ratinov & Roth, 2009;
Huang & Yates, 2009). This leads to a one-hot
representation over a smaller vocabulary size.
Neural language models (Bengio et al, 2001;
Schwenk & Gauvain, 2002; Mnih & Hinton,
2007; Collobert & Weston, 2008), on the other
hand, induce dense real-valued low-dimensional
384
word embeddings using unsupervised approaches.
(See Bengio (2008) for a more complete list of
references on neural language models.)
Unsupervised word representations have
been used in previous NLP work, and have
demonstrated improvements in generalization
accuracy on a variety of tasks. But different word
representations have never been systematically
compared in a controlled way. In this work, we
compare different techniques for inducing word
representations, evaluating them on the tasks of
named entity recognition (NER) and chunking.
We retract former negative results published in
Turian et al (2009) about Collobert and Weston
(2008) embeddings, given training improvements
that we describe in Section 7.1.
2 Distributional representations
Distributional word representations are based
upon a cooccurrence matrix F of size W?C, where
W is the vocabulary size, each row Fw is the ini-
tial representation of word w, and each column Fc
is some context. Sahlgren (2006) and Turney and
Pantel (2010) describe a handful of possible de-
sign decisions in contructing F, including choice
of context types (left window? right window? size
of window?) and type of frequency count (raw?
binary? tf-idf?). Fw has dimensionality W, which
can be too large to use Fw as features for word w in
a supervised model. One can map F to matrix f of
size W ? d, where d  C, using some function g,
where f = g(F). fw represents word w as a vector
with d dimensions. The choice of g is another de-
sign decision, although perhaps not as important
as the statistics used to initially construct F.
The self-organizing semantic map (Ritter &
Kohonen, 1989) is a distributional technique
that maps words to two dimensions, such that
syntactically and semantically related words are
nearby (Honkela et al, 1995; Honkela, 1997).
LSA (Dumais et al, 1988; Landauer et al,
1998), LSI, and LDA (Blei et al, 2003) induce
distributional representations over F in which
each column is a document context. In most of the
other approaches discussed, the columns represent
word contexts. In LSA, g computes the SVD of F.
Hyperspace Analogue to Language (HAL) is
another early distributional approach (Lund et al,
1995; Lund & Burgess, 1996) to inducing word
representations. They compute F over a corpus of
160 million word tokens with a vocabulary size W
of 70K word types. There are 2?W types of context
(columns): The first or second W are counted if the
word c occurs within a window of 10 to the left or
right of the word w, respectively. f is chosen by
taking the 200 columns (out of 140K in F) with
the highest variances. ICA is another technique to
transform F into f . (Va?yrynen & Honkela, 2004;
Va?yrynen & Honkela, 2005; Va?yrynen et al,
2007). ICA is expensive, and the largest vocab-
ulary size used in these works was only 10K. As
far as we know, ICA methods have not been used
when the size of the vocab W is 100K or more.
Explicitly storing cooccurrence matrix F can be
memory-intensive, and transforming F to f can
be time-consuming. It is preferable that F never
be computed explicitly, and that f be constructed
incrementally. R?ehu?r?ek and Sojka (2010) describe
an incremental approach to inducing LSA and
LDA topic models over 270 millions word tokens
with a vocabulary of 315K word types. This is
similar in magnitude to our experiments.
Another incremental approach to constructing f
is using a random projection: Linear mapping g is
multiplying F by a random matrix chosen a pri-
ori. This random indexing method is motivated
by the Johnson-Lindenstrauss lemma, which states
that for certain choices of random matrix, if d is
sufficiently large, then the original distances be-
tween words in F will be preserved in f (Sahlgren,
2005). Kaski (1998) uses this technique to pro-
duce 100-dimensional representations of docu-
ments. Sahlgren (2001) was the first author to use
random indexing using narrow context. Sahlgren
(2006) does a battery of experiments exploring
different design decisions involved in construct-
ing F, prior to using random indexing. However,
like all the works cited above, Sahlgren (2006)
only uses distributional representation to improve
existing systems for one-shot classification tasks,
such as IR, WSD, semantic knowledge tests, and
text categorization. It is not well-understood
what settings are appropriate to induce distribu-
tional word representations for structured predic-
tion tasks (like parsing and MT) and sequence la-
beling tasks (like chunking and NER). Previous
research has achieved repeated successes on these
tasks using clustering representations (Section 3)
and distributed representations (Section 4), so we
focus on these representations in our work.
3 Clustering-based word representations
Another type of word representation is to induce
a clustering over words. Clustering methods and
385
distributional methods can overlap. For example,
Pereira et al (1993) begin with a cooccurrence
matrix and transform this matrix into a clustering.
3.1 Brown clustering
The Brown algorithm is a hierarchical clustering
algorithm which clusters words to maximize the
mutual information of bigrams (Brown et al,
1992). So it is a class-based bigram language
model. It runs in time O(V ?K2), where V is the size
of the vocabulary and K is the number of clusters.
The hierarchical nature of the clustering means
that we can choose the word class at several
levels in the hierarchy, which can compensate for
poor clusters of a small number of words. One
downside of Brown clustering is that it is based
solely on bigram statistics, and does not consider
word usage in a wider context.
Brown clusters have been used successfully in
a variety of NLP applications: NER (Miller et al,
2004; Liang, 2005; Ratinov & Roth, 2009), PCFG
parsing (Candito & Crabbe?, 2009), dependency
parsing (Koo et al, 2008; Suzuki et al, 2009), and
semantic dependency parsing (Zhao et al, 2009).
Martin et al (1998) presents algorithms for
inducing hierarchical clusterings based upon word
bigram and trigram statistics. Ushioda (1996)
presents an extension to the Brown clustering
algorithm, and learn hierarchical clusterings of
words as well as phrases, which they apply to
POS tagging.
3.2 Other work on cluster-based word
representations
Lin and Wu (2009) present a K-means-like
non-hierarchical clustering algorithm for phrases,
which uses MapReduce.
HMMs can be used to induce a soft clustering,
specifically a multinomial distribution over pos-
sible clusters (hidden states). Li and McCallum
(2005) use an HMM-LDA model to improve
POS tagging and Chinese Word Segmentation.
Huang and Yates (2009) induce a fully-connected
HMM, which emits a multinomial distribution
over possible vocabulary words. They perform
hard clustering using the Viterbi algorithm.
(Alternately, they could keep the soft clustering,
with the representation for a particular word token
being the posterior probability distribution over
the states.) However, the CRF chunker in Huang
and Yates (2009), which uses their HMM word
clusters as extra features, achieves F1 lower than
a baseline CRF chunker (Sha & Pereira, 2003).
Goldberg et al (2009) use an HMM to assign
POS tags to words, which in turns improves
the accuracy of the PCFG-based Hebrew parser.
Deschacht and Moens (2009) use a latent-variable
language model to improve semantic role labeling.
4 Distributed representations
Another approach to word representation is to
learn a distributed representation. (Not to be
confused with distributional representations.)
A distributed representation is dense, low-
dimensional, and real-valued. Distributed word
representations are called word embeddings. Each
dimension of the embedding represents a latent
feature of the word, hopefully capturing useful
syntactic and semantic properties. A distributed
representation is compact, in the sense that it can
represent an exponential number of clusters in the
number of dimensions.
Word embeddings are typically induced us-
ing neural language models, which use neural
networks as the underlying predictive model
(Bengio, 2008). Historically, training and testing
of neural language models has been slow, scaling
as the size of the vocabulary for each model com-
putation (Bengio et al, 2001; Bengio et al, 2003).
However, many approaches have been proposed
in recent years to eliminate that linear dependency
on vocabulary size (Morin & Bengio, 2005;
Collobert & Weston, 2008; Mnih & Hinton, 2009)
and allow scaling to very large training corpora.
4.1 Collobert and Weston (2008) embeddings
Collobert and Weston (2008) presented a neural
language model that could be trained over billions
of words, because the gradient of the loss was
computed stochastically over a small sample of
possible outputs, in a spirit similar to Bengio and
Se?ne?cal (2003). This neural model of Collobert
and Weston (2008) was refined and presented in
greater depth in Bengio et al (2009).
The model is discriminative and non-
probabilistic. For each training update, we
read an n-gram x = (w1, . . . ,wn) from the corpus.
The model concatenates the learned embeddings
of the n words, giving e(w1) ? . . . ? e(wn), where
e is the lookup table and ? is concatenation.
We also create a corrupted or noise n-gram
x? = (w1, . . . ,wn?q, w?n), where w?n , wn is chosen
uniformly from the vocabulary.1 For convenience,
1In Collobert and Weston (2008), the middle word in the
386
we write e(x) to mean e(w1) ? . . . ? e(wn). We
predict a score s(x) for x by passing e(x) through
a single hidden layer neural network. The training
criterion is that n-grams that are present in the
training corpus like x must have a score at least
some margin higher than corrupted n-grams like
x?. Specifically: L(x) = max(0, 1? s(x) + s(x?)). We
minimize this loss stochastically over the n-grams
in the corpus, doing gradient descent simultane-
ously over the neural network parameters and the
embedding lookup table.
We implemented the approach of Collobert and
Weston (2008), with the following differences:
? We did not achieve as low log-ranks on the
English Wikipedia as the authors reported in
Bengio et al (2009), despite initially attempting
to have identical experimental conditions.
?We corrupt the last word of each n-gram.
? We had a separate learning rate for the em-
beddings and for the neural network weights.
We found that the embeddings should have a
learning rate generally 1000?32000 times higher
than the neural network weights. Otherwise, the
unsupervised training criterion drops slowly.
? Although their sampling technique makes train-
ing fast, testing is still expensive when the size of
the vocabulary is large. Instead of cross-validating
using the log-rank over the validation data as
they do, we instead used the moving average of
the training loss on training examples before the
weight update.
4.2 HLBL embeddings
The log-bilinear model (Mnih & Hinton, 2007) is
a probabilistic and linear neural model. Given an
n-gram, the model concatenates the embeddings
of the n ? 1 first words, and learns a linear model
to predict the embedding of the last word. The
similarity between the predicted embedding and
the current actual embedding is transformed
into a probability by exponentiating and then
normalizing. Mnih and Hinton (2009) speed up
model evaluation during training and testing by
using a hierarchy to exponentially filter down
the number of computations that are performed.
This hierarchical evaluation technique was first
proposed by Morin and Bengio (2005). The
model, combined with this optimization, is called
the hierarchical log-bilinear (HLBL) model.
n-gram is corrupted. In Bengio et al (2009), the last word in
the n-gram is corrupted.
5 Supervised evaluation tasks
We evaluate the hypothesis that one can take an
existing, near state-of-the-art, supervised NLP
system, and improve its accuracy by including
word representations as word features. This
technique for turning a supervised approach into a
semi-supervised one is general and task-agnostic.
However, we wish to find out if certain word
representations are preferable for certain tasks.
Lin and Wu (2009) finds that the representations
that are good for NER are poor for search query
classification, and vice-versa. We apply clus-
tering and distributed representations to NER
and chunking, which allows us to compare our
semi-supervised models to those of Ando and
Zhang (2005) and Suzuki and Isozaki (2008).
5.1 Chunking
Chunking is a syntactic sequence labeling task.
We follow the conditions in the CoNLL-2000
shared task (Sang & Buchholz, 2000).
The linear CRF chunker of Sha and Pereira
(2003) is a standard near-state-of-the-art baseline
chunker. In fact, many off-the-shelf CRF imple-
mentations now replicate Sha and Pereira (2003),
including their choice of feature set:
? CRF++ by Taku Kudo (http://crfpp.
sourceforge.net/)
? crfsgd by Le?on Bottou (http://leon.
bottou.org/projects/sgd)
? CRFsuite by by Naoaki Okazaki (http://
www.chokkan.org/software/crfsuite/)
We use CRFsuite because it makes it sim-
ple to modify the feature generation code,
so one can easily add new features. We
use SGD optimization, and enable negative
state features and negative transition fea-
tures. (?feature.possible transitions=1,
feature.possible states=1?)
Table 1 shows the features in the baseline chun-
ker. As you can see, the Brown and embedding
features are unigram features, and do not partici-
pate in conjunctions like the word features and tag
features do. Koo et al (2008) sees further accu-
racy improvements on dependency parsing when
using word representations in compound features.
The data comes from the Penn Treebank, and
is newswire from the Wall Street Journal in 1989.
Of the 8936 training sentences, we used 1000
randomly sampled sentences (23615 words) for
development. We trained models on the 7936
387
? Word features: wi for i in {?2,?1, 0,+1,+2},
wi ? wi+1 for i in {?1, 0}.
? Tag features: wi for i in {?2,?1, 0,+1,+2},
ti ? ti+1 for i in {?2,?1, 0,+1}. ti ? ti+1 ? ti+2
for i in {?2,?1, 0}.
? Embedding features [if applicable]: ei[d] for i
in {?2,?1, 0,+1,+2}, where d ranges over the
dimensions of the embedding ei.
? Brown features [if applicable]: substr(bi, 0, p)
for i in {?2,?1, 0,+1,+2}, where substr takes
the p-length prefix of the Brown cluster bi.
Table 1: Features templates used in the CRF chunker.
training partition sentences, and evaluated their
F1 on the development set. After choosing hy-
perparameters to maximize the dev F1, we would
retrain the model using these hyperparameters on
the full 8936 sentence training set, and evaluate
on test. One hyperparameter was l2-regularization
sigma, which for most models was optimal at 2 or
3.2. The word embeddings also required a scaling
hyperparameter, as described in Section 7.2.
5.2 Named entity recognition
NER is typically treated as a sequence prediction
problem. Following Ratinov and Roth (2009), we
use the regularized averaged perceptron model.
Ratinov and Roth (2009) describe different
sequence encoding like BILOU and BIO, and
show that the BILOU encoding outperforms BIO,
and the greedy inference performs competitively
to Viterbi while being significantly faster. Ac-
cordingly, we use greedy inference and BILOU
text chunk representation. We use the publicly
available implementation from Ratinov and Roth
(2009) (see the end of this paper for the URL). In
our baseline experiments, we remove gazetteers
and non-local features (Krishnan & Manning,
2006). However, we also run experiments that
include these features, to understand if the infor-
mation they provide mostly overlaps with that of
the word representations.
After each epoch over the training set, we
measured the accuracy of the model on the
development set. Training was stopped after the
accuracy on the development set did not improve
for 10 epochs, generally about 50?80 epochs
total. The epoch that performed best on the
development set was chosen as the final model.
We use the following baseline set of features
from Zhang and Johnson (2003):
? Previous two predictions yi?1 and yi?2
? Current word xi
? xi word type information: all-capitalized,
is-capitalized, all-digits, alphanumeric, etc.
? Prefixes and suffixes of xi, if the word contains
hyphens, then the tokens between the hyphens
? Tokens in the window c =
(xi?2, xi?1, xi, xi+1, xi+2)
? Capitalization pattern in the window c
? Conjunction of c and yi?1.
Word representation features, if present, are used
the same way as in Table 1.
When using the lexical features, we normalize
dates and numbers. For example, 1980 becomes
*DDDD* and 212-325-4751 becomes *DDD*-
*DDD*-*DDDD*. This allows a degree of abstrac-
tion to years, phone numbers, etc. This delexi-
calization is performed separately from using the
word representation. That is, if we have induced
an embedding for 12/3/2008 , we will use the em-
bedding of 12/3/2008 , and *DD*/*D*/*DDDD*
in the baseline features listed above.
Unlike in our chunking experiments, after we
chose the best model on the development set, we
used that model on the test set too. (In chunking,
after finding the best hyperparameters on the
development set, we would combine the dev
and training set and training a model over this
combined set, and then evaluate on test.)
The standard evaluation benchmark for NER
is the CoNLL03 shared task dataset drawn from
the Reuters newswire. The training set contains
204K words (14K sentences, 946 documents), the
test set contains 46K words (3.5K sentences, 231
documents), and the development set contains
51K words (3.3K sentences, 216 documents).
We also evaluated on an out-of-domain (OOD)
dataset, the MUC7 formal run (59K words).
MUC7 has a different annotation standard than
the CoNLL03 data. It has several NE types that
don?t appear in CoNLL03: money, dates, and
numeric quantities. CoNLL03 has MISC, which
is not present in MUC7. To evaluate on MUC7,
we perform the following postprocessing steps
prior to evaluation:
1. In the gold-standard MUC7 data, discard
(label as ?O?) all NEs with type NUM-
BER/MONEY/DATE.
2. In the predicted model output on MUC7 data,
discard (label as ?O?) all NEs with type MISC.
388
These postprocessing steps will adversely affect
all NER models across-the-board, nonetheless
allowing us to compare different models in a
controlled manner.
6 Unlabled Data
Unlabeled data is used for inducing the word
representations. We used the RCV1 corpus, which
contains one year of Reuters English newswire,
from August 1996 to August 1997, about 63
millions words in 3.3 million sentences. We
left case intact in the corpus. By comparison,
Collobert and Weston (2008) downcases words
and delexicalizes numbers.
We use a preprocessing technique proposed
by Liang, (2005, p. 51), which was later used
by Koo et al (2008): Remove all sentences that
are less than 90% lowercase a?z. We assume
that whitespace is not counted, although this
is not specified in Liang?s thesis. We call this
preprocessing step cleaning.
In Turian et al (2009), we found that all
word representations performed better on the
supervised task when they were induced on the
clean unlabeled data, both embeddings and Brown
clusters. This is the case even though the cleaning
process was very aggressive, and discarded more
than half of the sentences. According to the
evidence and arguments presented in Bengio et al
(2009), the non-convex optimization process for
Collobert and Weston (2008) embeddings might
be adversely affected by noise and the statistical
sparsity issues regarding rare words, especially
at the beginning of training. For this reason, we
hypothesize that learning representations over the
most frequent words first and gradually increasing
the vocabulary?a curriculum training strategy
(Elman, 1993; Bengio et al, 2009; Spitkovsky
et al, 2010)?would provide better results than
cleaning.
After cleaning, there are 37 million words (58%
of the original) in 1.3 million sentences (41% of
the original). The cleaned RCV1 corpus has 269K
word types. This is the vocabulary size, i.e. how
many word representations were induced. Note
that cleaning is applied only to the unlabeled data,
not to the labeled data used in the supervised tasks.
RCV1 is a superset of the CoNLL03 corpus.
For this reason, NER results that use RCV1
word representations are a form of transductive
learning.
7 Experiments and Results
7.1 Details of inducing word representations
The Brown clusters took roughly 3 days to induce,
when we induced 1000 clusters, the baseline in
prior work (Koo et al, 2008; Ratinov & Roth,
2009). We also induced 100, 320, and 3200
Brown clusters, for comparison. (Because Brown
clustering scales quadratically in the number of
clusters, inducing 10000 clusters would have
been prohibitive.) Because Brown clusters are
hierarchical, we can use cluster supersets as
features. We used clusters at path depth 4, 6, 10,
and 20 (Ratinov & Roth, 2009). These are the
prefixes used in Table 1.
The Collobert and Weston (2008) (C&W)
embeddings were induced over the course of a
few weeks, and trained for about 50 epochs. One
of the difficulties in inducing these embeddings is
that there is no stopping criterion defined, and that
the quality of the embeddings can keep improving
as training continues. Collobert (p.c.) simply
leaves one computer training his embeddings
indefinitely. We induced embeddings with 25, 50,
100, or 200 dimensions over 5-gram windows.
In comparison to Turian et al (2009), we use
improved C&W embeddings in this work:
? They were trained for 50 epochs, not just 20
epochs.
? We initialized all embedding dimensions uni-
formly in the range [-0.01, +0.01], not [-1,+1].
For rare words, which are typically updated only
143 times per epoch2, and given that our embed-
ding learning rate was typically 1e-6 or 1e-7, this
means that rare word embeddings will be concen-
trated around zero, instead of spread out randomly.
The HLBL embeddings were trained for 100
epochs (7 days).3 Unlike our Collobert and We-
ston (2008) embeddings, we did not extensively
tune the learning rates for HLBL. We used a learn-
ing rate of 1e-3 for both model parameters and
embedding parameters. We induced embeddings
with 100 dimensions over 5-gram windows, and
embeddings with 50 dimensions over 5-gram win-
dows. Embeddings were induced over one pass
2A rare word will appear 5 (window size) times per
epoch as a positive example, and 37M (training examples per
epoch) / 269K (vocabulary size) = 138 times per epoch as a
corruption example.
3The HLBL model updates require fewer matrix mul-
tiplies than Collobert and Weston (2008) model updates.
Additionally, HLBL models were trained on a GPGPU,
which is faster than conventional CPU arithmetic.
389
approach using a random tree, not two passes with
an updated tree and embeddings re-estimation.
7.2 Scaling of Word Embeddings
Like many NLP systems, the baseline system con-
tains only binary features. The word embeddings,
however, are real numbers that are not necessarily
in a bounded range. If the range of the word
embeddings is too large, they will exert more
influence than the binary features.
We generally found that embeddings had zero
mean. We can scale the embeddings by a hy-
perparameter, to control their standard deviation.
Assume that the embeddings are represented by a
matrix E:
E ? ? ? E/stddev(E) (1)
? is a scaling constant that sets the new standard
deviation after scaling the embeddings.
(a)
 93.6
 93.8
 94
 94.2
 94.4
 94.6
 94.8
 0.001  0.01  0.1  1
Va
lid
ati
on
 F1
Scaling factor ?
C&W, 50-dim
HLBL, 50-dimC&W, 200-dimC&W, 100-dim
HLBL, 100-dimC&W, 25-dim
baseline
(b)
 89
 89.5
 90
 90.5
 91
 91.5
 92
 92.5
 0.001  0.01  0.1  1
Va
lid
ati
on
 F1
Scaling factor ?
C&W, 200-dimC&W, 100-dimC&W, 25-dimC&W, 50-dim
HLBL, 100-dim
HLBL, 50-dim
baseline
Figure 1: Effect as we vary the scaling factor ? (Equa-
tion 1) on the validation set F1. We experiment with
Collobert and Weston (2008) and HLBL embeddings of var-
ious dimensionality. (a) Chunking results. (b) NER results.
Figure 1 shows the effect of scaling factor ?
on both supervised tasks. We were surprised
to find that on both tasks, across Collobert and
Weston (2008) and HLBL embeddings of various
dimensionality, that all curves had similar shapes
and optima. This is one contributions of our
work. In Turian et al (2009), we were not
able to prescribe a default value for scaling the
embeddings. However, these curves demonstrate
that a reasonable choice of scale factor is such that
the embeddings have a standard deviation of 0.1.
7.3 Capacity of Word Representations
(a)
 94.1
 94.2
 94.3
 94.4
 94.5
 94.6
 94.7
 100  320  1000  3200
 25  50  100  200
Va
lid
ati
on
 F1
# of Brown clusters
# of embedding dimensions
C&W
HLBL
Brown
baseline
(b)
 90
 90.5
 91
 91.5
 92
 92.5
 100  320  1000  3200
 25  50  100  200
Va
lid
ati
on
 F1
# of Brown clusters
# of embedding dimensions
C&W
Brown
HLBL
baseline
Figure 2: Effect as we vary the capacity of the word
representations on the validation set F1. (a) Chunking
results. (b) NER results.
There are capacity controls for the word
representations: number of Brown clusters, and
number of dimensions of the word embeddings.
Figure 2 shows the effect on the validation F1 as
we vary the capacity of the word representations.
In general, it appears that more Brown clusters
are better. We would like to induce 10000 Brown
clusters, however this would take several months.
In Turian et al (2009), we hypothesized on
the basis of solely the HLBL NER curve that
higher-dimensional word embeddings would give
higher accuracy. Figure 2 shows that this hy-
pothesis is not true. For NER, the C&W curve is
almost flat, and we were suprised to find the even
25-dimensional C&W word embeddings work so
well. For chunking, 50-dimensional embeddings
had the highest validation F1 for both C&W and
HLBL. These curves indicates that the optimal
capacity of the word embeddings is task-specific.
390
System Dev Test
Baseline 94.16 93.79
HLBL, 50-dim 94.63 94.00
C&W, 50-dim 94.66 94.10
Brown, 3200 clusters 94.67 94.11
Brown+HLBL, 37M 94.62 94.13
C&W+HLBL, 37M 94.68 94.25
Brown+C&W+HLBL, 37M 94.72 94.15
Brown+C&W, 37M 94.76 94.35
Ando and Zhang (2005), 15M - 94.39
Suzuki and Isozaki (2008), 15M - 94.67
Suzuki and Isozaki (2008), 1B - 95.15
Table 2: Final chunking F1 results. In the last section, we
show how many unlabeled words were used.
System Dev Test MUC7
Baseline 90.03 84.39 67.48
Baseline+Nonlocal 91.91 86.52 71.80
HLBL 100-dim 92.00 88.13 75.25
Gazetteers 92.09 87.36 77.76
C&W 50-dim 92.27 87.93 75.74
Brown, 1000 clusters 92.32 88.52 78.84
C&W 200-dim 92.46 87.96 75.51
C&W+HLBL 92.52 88.56 78.64
Brown+HLBL 92.56 88.93 77.85
Brown+C&W 92.79 89.31 80.13
HLBL+Gaz 92.91 89.35 79.29
C&W+Gaz 92.98 88.88 81.44
Brown+Gaz 93.25 89.41 82.71
Lin and Wu (2009), 3.4B - 88.44 -
Ando and Zhang (2005), 27M 93.15 89.31 -
Suzuki and Isozaki (2008), 37M 93.66 89.36 -
Suzuki and Isozaki (2008), 1B 94.48 89.92 -
All (Brown+C&W+HLBL+Gaz), 37M 93.17 90.04 82.50
All+Nonlocal, 37M 93.95 90.36 84.15
Lin and Wu (2009), 700B - 90.90 -
Table 3: Final NER F1 results, showing the cumulative
effect of adding word representations, non-local features, and
gazetteers to the baseline. To speed up training, in combined
experiments (C&W plus another word representation),
we used the 50-dimensional C&W embeddings, not the
200-dimensional ones. In the last section, we show how
many unlabeled words were used.
7.4 Final results
Table 2 shows the final chunking results and Ta-
ble 3 shows the final NER F1 results. We compare
to the state-of-the-art methods of Ando and Zhang
(2005), Suzuki and Isozaki (2008), and?for
NER?Lin and Wu (2009). Tables 2 and 3 show
that accuracy can be increased further by combin-
ing the features from different types of word rep-
resentations. But, if only one word representation
is to be used, Brown clusters have the highest ac-
curacy. Given the improvements to the C&W em-
beddings since Turian et al (2009), C&W em-
beddings outperform the HLBL embeddings. On
chunking, there is only a minute difference be-
tween Brown clusters and the embeddings. Com-
(a)
 0
 50
 100
 150
 200
 250
0 1 10 100 1K 10K 100K 1M
# o
f p
er-
tok
en
 er
ror
s (t
est
 se
t)
Frequency of word in unlabeled data
C&W, 50-dim
Brown, 3200 clusters
(b)
 0
 50
 100
 150
 200
 250
0 1 10 100 1K 10K 100K 1M
# o
f p
er-
tok
en
 er
ror
s (t
est
 se
t)
Frequency of word in unlabeled data
C&W, 50-dim
Brown, 1000 clusters
Figure 3: For word tokens that have different frequency
in the unlabeled data, what is the total number of per-token
errors incurred on the test set? (a) Chunking results. (b) NER
results.
bining representations leads to small increases in
the test F1. In comparison to chunking, combin-
ing different word representations on NER seems
gives larger improvements on the test F1.
On NER, Brown clusters are superior to the
word embeddings. Since much of the NER F1
is derived from decisions made over rare words,
we suspected that Brown clustering has a superior
representation for rare words. Brown makes
a single hard clustering decision, whereas the
embedding for a rare word is close to its initial
value since it hasn?t received many training
updates (see Footnote 2). Figure 3 shows the total
number of per-token errors incurred on the test
set, depending upon the frequency of the word
token in the unlabeled data. For NER, Figure 3 (b)
shows that most errors occur on rare words, and
that Brown clusters do indeed incur fewer errors
for rare words. This supports our hypothesis
that, for rare words, Brown clustering produces
better representations than word embeddings that
haven?t received sufficient training updates. For
chunking, Brown clusters and C&W embeddings
incur almost identical numbers of errors, and
errors are concentrated around the more common
391
words. We hypothesize that non-rare words have
good representations, regardless of the choice
of word representation technique. For tasks like
chunking in which a syntactic decision relies upon
looking at several token simultaneously, com-
pound features that use the word representations
might increase accuracy more (Koo et al, 2008).
Using word representations in NER brought
larger gains on the out-of-domain data than on the
in-domain data. We were surprised by this result,
because the OOD data was not even used during
the unsupervised word representation induction,
as was the in-domain data. We are curious to
investigate this phenomenon further.
Ando and Zhang (2005) present a semi-
supervised learning algorithm called alternating
structure optimization (ASO). They find a low-
dimensional projection of the input features that
gives good linear classifiers over auxiliary tasks.
These auxiliary tasks are sometimes specific
to the supervised task, and sometimes general
language modeling tasks like ?predict the missing
word?. Suzuki and Isozaki (2008) present a semi-
supervised extension of CRFs. (In Suzuki et al
(2009), they extend their semi-supervised ap-
proach to more general conditional models.) One
of the advantages of the semi-supervised learning
approach that we use is that it is simpler and more
general than that of Ando and Zhang (2005) and
Suzuki and Isozaki (2008). Their methods dictate
a particular choice of model and training regime
and could not, for instance, be used with an NLP
system based upon an SVM classifier.
Lin and Wu (2009) present a K-means-like
non-hierarchical clustering algorithm for phrases,
which uses MapReduce. Since they can scale
to millions of phrases, and they train over 800B
unlabeled words, they achieve state-of-the-art
accuracy on NER using their phrase clusters.
This suggests that extending word representa-
tions to phrase representations is worth further
investigation.
8 Conclusions
Word features can be learned in advance in an
unsupervised, task-inspecific, and model-agnostic
manner. These word features, once learned, are
easily disseminated with other researchers, and
easily integrated into existing supervised NLP
systems. The disadvantage, however, is that ac-
curacy might not be as high as a semi-supervised
method that includes task-specific information
and that jointly learns the supervised and unsu-
pervised tasks (Ando & Zhang, 2005; Suzuki &
Isozaki, 2008; Suzuki et al, 2009).
Unsupervised word representations have been
used in previous NLP work, and have demon-
strated improvements in generalization accuracy
on a variety of tasks. Ours is the first work to
systematically compare different word repre-
sentations in a controlled way. We found that
Brown clusters and word embeddings both can
improve the accuracy of a near-state-of-the-art
supervised NLP system. We also found that com-
bining different word representations can improve
accuracy further. Error analysis indicates that
Brown clustering induces better representations
for rare words than C&W embeddings that have
not received many training updates.
Another contribution of our work is a default
method for setting the scaling parameter for
word embeddings. With this contribution, word
embeddings can now be used off-the-shelf as
word features, with no tuning.
Future work should explore methods for
inducing phrase representations, as well as tech-
niques for increasing in accuracy by using word
representations in compound features.
Replicating our experiments
You can visit http://metaoptimize.com/
projects/wordreprs/ to find: The word
representations we induced, which you can
download and use in your experiments; The code
for inducing the word representations, which you
can use to induce word representations on your
own data; The NER and chunking system, with
code for replicating our experiments.
Acknowledgments
Thank you to Magnus Sahlgren, Bob Carpenter,
Percy Liang, Alexander Yates, and the anonymous
reviewers for useful discussion. Thank you to
Andriy Mnih for inducing his embeddings on
RCV1 for us. Joseph Turian and Yoshua Bengio
acknowledge the following agencies for re-
search funding and computing support: NSERC,
RQCHP, CIFAR. Lev Ratinov was supported by
the Air Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the
author and do not necessarily reflect the view of
the Air Force Research Laboratory (AFRL).
392
References
Ando, R., & Zhang, T. (2005). A high-
performance semi-supervised learning method
for text chunking. ACL.
Bengio, Y. (2008). Neural net language models.
Scholarpedia, 3, 3881.
Bengio, Y., Ducharme, R., & Vincent, P. (2001).
A neural probabilistic language model. NIPS.
Bengio, Y., Ducharme, R., Vincent, P., & Jauvin,
C. (2003). A neural probabilistic language
model. Journal of Machine Learning Research,
3, 1137?1155.
Bengio, Y., Louradour, J., Collobert, R., &
Weston, J. (2009). Curriculum learning. ICML.
Bengio, Y., & Se?ne?cal, J.-S. (2003). Quick train-
ing of probabilistic neural nets by importance
sampling. AISTATS.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003).
Latent dirichlet alocation. Journal of Machine
Learning Research, 3, 993?1022.
Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra,
V. J. D., & Lai, J. C. (1992). Class-based n-gram
models of natural language. Computational
Linguistics, 18, 467?479.
Candito, M., & Crabbe?, B. (2009). Improving gen-
erative statistical parsing with semi-supervised
word clustering. IWPT (pp. 138?141).
Collobert, R., & Weston, J. (2008). A unified
architecture for natural language processing:
Deep neural networks with multitask learning.
ICML.
Deschacht, K., & Moens, M.-F. (2009). Semi-
supervised semantic role labeling using the
Latent Words Language Model. EMNLP (pp.
21?29).
Dumais, S. T., Furnas, G. W., Landauer, T. K.,
Deerwester, S., & Harshman, R. (1988). Using
latent semantic analysis to improve access to
textual information. SIGCHI Conference on
Human Factors in Computing Systems (pp.
281?285). ACM.
Elman, J. L. (1993). Learning and development
in neural networks: The importance of starting
small. Cognition, 48, 781?799.
Goldberg, Y., Tsarfaty, R., Adler, M., & Elhadad,
M. (2009). Enhancing unlexicalized parsing
performance using a wide coverage lexicon,
fuzzy tag-set mapping, and EM-HMM-based
lexical probabilities. EACL.
Honkela, T. (1997). Self-organizing maps of
words for natural language processing applica-
tions. Proceedings of the International ICSC
Symposium on Soft Computing.
Honkela, T., Pulkki, V., & Kohonen, T. (1995).
Contextual relations of words in grimm tales,
analyzed by self-organizing map. ICANN.
Huang, F., & Yates, A. (2009). Distributional rep-
resentations for handling sparsity in supervised
sequence labeling. ACL.
Kaski, S. (1998). Dimensionality reduction by
random mapping: Fast similarity computation
for clustering. IJCNN (pp. 413?418).
Koo, T., Carreras, X., & Collins, M. (2008).
Simple semi-supervised dependency parsing.
ACL (pp. 595?603).
Krishnan, V., & Manning, C. D. (2006). An
effective two-stage model for exploiting non-
local dependencies in named entity recognition.
COLING-ACL.
Landauer, T. K., Foltz, P. W., & Laham, D. (1998).
An introduction to latent semantic analysis.
Discourse Processes, 259?284.
Li, W., & McCallum, A. (2005). Semi-supervised
sequence modeling with syntactic topic models.
AAAI.
Liang, P. (2005). Semi-supervised learning
for natural language. Master?s thesis, Mas-
sachusetts Institute of Technology.
Lin, D., & Wu, X. (2009). Phrase clustering
for discriminative learning. ACL-IJCNLP (pp.
1030?1038).
Lund, K., & Burgess, C. (1996). Producing
highdimensional semantic spaces from lexical
co-occurrence. Behavior Research Methods,
Instrumentation, and Computers, 28, 203?208.
Lund, K., Burgess, C., & Atchley, R. A. (1995).
Semantic and associative priming in high-
dimensional semantic space. Cognitive Science
Proceedings, LEA (pp. 660?665).
Martin, S., Liermann, J., & Ney, H. (1998). Algo-
rithms for bigram and trigram word clustering.
Speech Communication, 24, 19?37.
Miller, S., Guinness, J., & Zamanian, A. (2004).
Name tagging with word clusters and discrim-
inative training. HLT-NAACL (pp. 337?342).
393
Mnih, A., & Hinton, G. E. (2007). Three
new graphical models for statistical language
modelling. ICML.
Mnih, A., & Hinton, G. E. (2009). A scalable
hierarchical distributed language model. NIPS
(pp. 1081?1088).
Morin, F., & Bengio, Y. (2005). Hierarchical
probabilistic neural network language model.
AISTATS.
Pereira, F., Tishby, N., & Lee, L. (1993). Distri-
butional clustering of english words. ACL (pp.
183?190).
Ratinov, L., & Roth, D. (2009). Design chal-
lenges and misconceptions in named entity
recognition. CoNLL.
Ritter, H., & Kohonen, T. (1989). Self-organizing
semantic maps. Biological Cybernetics,
241?254.
Sahlgren, M. (2001). Vector-based semantic
analysis: Representing word meanings based
on random labels. Proceedings of the Semantic
Knowledge Acquisition and Categorisation
Workshop, ESSLLI.
Sahlgren, M. (2005). An introduction to random
indexing. Methods and Applications of Seman-
tic Indexing Workshop at the 7th International
Conference on Terminology and Knowledge
Engineering (TKE).
Sahlgren, M. (2006). The word-space model:
Using distributional analysis to represent syn-
tagmatic and paradigmatic relations between
words in high-dimensional vector spaces.
Doctoral dissertation, Stockholm University.
Sang, E. T., & Buchholz, S. (2000). Introduction
to the CoNLL-2000 shared task: Chunking.
CoNLL.
Schwenk, H., & Gauvain, J.-L. (2002). Connec-
tionist language modeling for large vocabulary
continuous speech recognition. International
Conference on Acoustics, Speech and Signal
Processing (ICASSP) (pp. 765?768). Orlando,
Florida.
Sha, F., & Pereira, F. C. N. (2003). Shal-
low parsing with conditional random fields.
HLT-NAACL.
Spitkovsky, V., Alshawi, H., & Jurafsky, D.
(2010). From baby steps to leapfrog: How ?less
is more? in unsupervised dependency parsing.
NAACL-HLT.
Suzuki, J., & Isozaki, H. (2008). Semi-supervised
sequential labeling and segmentation using
giga-word scale unlabeled data. ACL-08: HLT
(pp. 665?673).
Suzuki, J., Isozaki, H., Carreras, X., & Collins, M.
(2009). An empirical study of semi-supervised
structured conditional models for dependency
parsing. EMNLP.
Turian, J., Ratinov, L., Bengio, Y., & Roth, D.
(2009). A preliminary evaluation of word
representations for named-entity recognition.
NIPS Workshop on Grammar Induction, Repre-
sentation of Language and Language Learning.
Turney, P. D., & Pantel, P. (2010). From frequency
to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research.
Ushioda, A. (1996). Hierarchical clustering of
words. COLING (pp. 1159?1162).
Va?yrynen, J., & Honkela, T. (2005). Compar-
ison of independent component analysis and
singular value decomposition in word context
analysis. AKRR?05, International and Interdis-
ciplinary Conference on Adaptive Knowledge
Representation and Reasoning.
Va?yrynen, J. J., & Honkela, T. (2004). Word cat-
egory maps based on emergent features created
by ICA. Proceedings of the STeP?2004 Cogni-
tion + Cybernetics Symposium (pp. 173?185).
Finnish Artificial Intelligence Society.
Va?yrynen, J. J., Honkela, T., & Lindqvist, L.
(2007). Towards explicit semantic features
using independent component analysis. Pro-
ceedings of the Workshop Semantic Content
Acquisition and Representation (SCAR). Stock-
holm, Sweden: Swedish Institute of Computer
Science.
R?ehu?r?ek, R., & Sojka, P. (2010). Software frame-
work for topic modelling with large corpora.
LREC.
Zhang, T., & Johnson, D. (2003). A robust risk
minimization based named entity recognition
system. CoNLL.
Zhao, H., Chen, W., Kit, C., & Zhou, G.
(2009). Multilingual dependency learning: a
huge feature engineering method to semantic
dependency parsing. CoNLL (pp. 55?60).
394
