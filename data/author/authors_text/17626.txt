Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 193?198,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Two Knives Cut Better Than One:
Chinese Word Segmentation with Dual Decomposition
Mengqiu Wang
Computer Science Department
Stanford University
Stanford, CA 94305
Rob Voigt
Linguistics Department
Stanford University
Stanford, CA 94305
{mengqiu,manning}@cs.stanford.edu robvoigt@stanford.edu
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
Abstract
There are two dominant approaches to
Chinese word segmentation: word-based
and character-based models, each with re-
spective strengths. Prior work has shown
that gains in segmentation performance
can be achieved from combining these
two types of models; however, past efforts
have not provided a practical technique
to allow mainstream adoption. We pro-
pose a method that effectively combines
the strength of both segmentation schemes
using an efficient dual-decomposition al-
gorithm for joint inference. Our method
is simple and easy to implement. Ex-
periments on SIGHAN 2003 and 2005
evaluation datasets show that our method
achieves the best reported results to date
on 6 out of 7 datasets.
1 Introduction
Chinese text is written without delimiters between
words; as a result, Chinese word segmentation
(CWS) is an essential foundational step for many
tasks in Chinese natural language processing. As
demonstrated by (Shi and Wang, 2007; Bai et
al., 2008; Chang et al, 2008; Kummerfeld et al,
2013), the quality and consistency of segmentation
has important downstream impacts on system per-
formance in machine translation, POS tagging and
parsing.
State-of-the-art performance in CWS is high,
with F-scores in the upper 90s. Still, challenges
remain. Unknown words, also known as out-of-
vocabulary (OOV) words, lead to difficulties for
word- or dictionary-based approaches. Ambiguity
can cause errors when the appropriate segmenta-
tion is determined contextually, such as?? (?tal-
ent?) and? /? (?just able?) (Gao et al, 2003).
There are two primary classes of models:
character-based, where the foundational units for
processing are individual Chinese characters (Xue,
2003; Tseng et al, 2005; Zhang et al, 2006;
Wang et al, 2010), and word-based, where the
units are full words based on some dictionary or
training lexicon (Andrew, 2006; Zhang and Clark,
2007). Sun (2010) details their respective theo-
retical strengths: character-based approaches bet-
ter model the internal compositional structure of
words and are therefore more effective at inducing
new OOV words; word-based approaches are bet-
ter at reproducing the words of the training lexi-
con and can capture information from significantly
larger contextual spans. Prior work has shown per-
formance gains from combining these two types
of models to exploit their respective strengths, but
such approaches are often complex to implement
and computationally expensive.
In this work, we propose a simple and prin-
cipled joint decoding method for combining
character-based and word-based segmenters based
on dual decomposition. This method has strong
optimality guarantees and works very well empir-
ically. It is easy to implement and does not re-
quire retraining of existing character- and word-
based segmenters. Perhaps most importantly, this
work presents a much more practical and usable
form of classifier combination in the CWS context
than existing methods offer.
Experimental results on standard SIGHAN
2003 and 2005 bake-off evaluations show that our
model outperforms the character and word base-
lines by a significant margin. In particular, out
approach improves OOV recall rates and segmen-
tation consistency, and gives the best reported re-
sults to date on 6 out of 7 datasets.
2 Models for CWS
Here we describe the character-based and word-
based models we use as baselines, review existing
approaches to combination, and describe our algo-
rithm for joint decoding with dual decomposition.
193
2.1 Character-based Models
In the most commonly used contemporary ap-
proach to character-based segmentation, first pro-
posed by (Xue, 2003), CWS is seen as a charac-
ter sequence tagging task, where each character
is tagged on whether it is at the beginning, mid-
dle, or end of a word. Conditional random fields
(CRF) (Lafferty et al, 2001) have been widely
adopted for this task, and give state-of-the-art re-
sults (Tseng et al, 2005). In a first-order linear-
chain CRF model, the conditional probability of a
label sequence y given a word sequence x is de-
fined as:
P (y|x) =
1
Z
|y|
?
t=1
exp (? ? f(x, y
t
, y
t+1
))
f(x, y
t
, y
t?1
) are feature functions that typically
include surrounding character n-gram and mor-
phological suffix/prefix features. These types of
features capture the compositional properties of
characters and are likely to generalize well to un-
known words. However, the Markov assumption
in CRF limits the context of such features; it is
difficult to capture long-range word features in this
model.
2.2 Word-based Models
Word-based models search through lists of word
candidates using scoring functions that directly
assign scores to each. Early word-based seg-
mentation work employed simple heuristics like
dictionary-lookup maximum matching (Chen and
Liu, 1992). More recently, Zhang and Clark
(2007) reported success using a linear model
trained with the average perceptron algorithm
(Collins, 2002). Formally, given input x, their
model seeks a segmentation y such that:
F (y|x) = max
y?GEN(x)
(? ? ?(y))
F (y|x) is the score of segmentation result y.
Searching through the entire GEN(x) space is
intractable even with a local model, so a beam-
search algorithm is used. The search algorithm
consumes one character input token at a time, and
iterates through the existing beams to score two
new alternative hypotheses by either appending
the new character to the last word in the beam, or
starting a new word at the current position.
Algorithm 1 Dual decomposition inference algo-
rithm, and modified Viterbi and beam-search algo-
rithms.
?i ? {1 to |x|} : ?k ? {0, 1} : u
i
(k) = 0
for t? 1 to T do
y
c?
= argmax
y
P (y
c
|x) +
?
i?|x|
u
i
(y
c
i
)
y
w?
= argmax
y?GEN(x)
F (y
w
|x)?
?
j?|x|
u
j
(y
w
j
)
if y
c?
= y
w?
then
return (y
c?
,y
w?
)
end if
for all i ? {1 to |x|} do
?k ? {0, 1} : u
i
(k) = u
i
(k) + ?
t
(2k ? 1)(y
w?
i
?
y
c?
i
)
end for
end for
return (y
c?
,y
w?
)
Viterbi:
V
1
(1) = 1, V
1
(0) = 0
for i = 2 to |x| do
?k ? {0, 1} : V
i
(k) = argmax
k
?
P
i
(k|k
?
)V
i?1
k
?
+
u
i
(k)
end for
Beam-Search:
for i = 1 to |x| do
for item v = {w
0
, ? ? ? , w
j
} in beam(i) do
append x
i
to w
j
, score(v)
+
= u
i
(0)
v = {w
0
, ? ? ? , w
j
, x
i
}, score(v)
+
= u
i
(1)
end for
end for
2.3 Combining Models with Dual
Decomposition
Various mixing approaches have been proposed to
combine the above two approaches (Wang et al,
2006; Lin, 2009; Sun et al, 2009; Sun, 2010;
Wang et al, 2010). These mixing models perform
well on standard datasets, but are not in wide use
because of their high computational costs and dif-
ficulty of implementation.
Dual decomposition (DD) (Rush et al, 2010)
offers an attractive framework for combining these
two types of models without incurring high costs
in model complexity (in contrast to (Sun et al,
2009)) or decoding efficiency (in contrast to bag-
ging in (Wang et al, 2006; Sun, 2010)). DD has
been successfully applied to similar situations for
combining local with global models; for example,
in dependency parsing (Koo et al, 2010), bilingual
sequence tagging (Wang et al, 2013) and word
alignment (DeNero and Macherey, 2011).
The idea is that jointly modelling both
character-sequence and word information can be
computationally challenging, so instead we can try
to find outputs that the two models are most likely
194
Academia Sinica Peking Univ.
R P F
1
R
oov
C R P F
1
R
oov
C
Char-based CRF 95.2 93.6 94.4 58.9 0.064 94.6 95.3 94.9 77.8 0.089
Word-based Perceptron 95.8 95.0 95.4 69.5 0.060 94.1 95.5 94.8 76.7 0.099
Dual-decomp 95.9 94.9 95.4 67.7 0.055 94.8 95.7 95.3 78.7 0.086
City Univ. of Hong Kong Microsoft Research
R P F
1
R
oov
C R P F
1
R
oov
C
Char-based CRF 94.7 94.0 94.3 76.1 0.065 96.4 96.6 96.5 71.3 0.074
Word-based Perceptron 94.3 94.0 94.2 71.7 0.073 97.0 97.2 97.1 74.6 0.063
Dual-decomp 95.0 94.4 94.7 75.3 0.062 97.3 97.4 97.4 76.0 0.055
Table 1: Results on SIGHAN 2005 datasets. R
oov
denotes OOV recall, and C denotes segmentation
consistency. Best number in each column is highlighted in bold.
to agree on. Formally, the objective of DD is:
max
y
c
,y
w
P (y
c
|x) + F (y
w
|x) s.t. y
c
= y
w
(1)
where y
c
is the output of character-based CRF, y
w
is the output of word-based perceptron, and the
agreements are expressed as constraints. s.t. is
a shorthand for ?such that?.
Solving this constrained optimization problem
directly is difficult. Instead, we take the La-
grangian relaxation of this term as:
L (y
c
,y
w
,U) = (2)
P (y
c
|x) + F (y
w
|x) +
?
i?|x|
u
i
(y
c
i
? y
w
i
)
where U is the set of Lagrangian multipliers that
consists of a multiplier u
i
at each word position i.
We can rewrite the original objective with the
Lagrangian relaxation as:
max
y
c
,y
w
min
U
L (y
c
,y
w
,U) (3)
We can then form the dual of this problem by
taking the min outside of the max, which is an up-
per bound on the original problem. The dual form
can then be decomposed into two sub-components
(the two max problems in Eq. 4), each of which is
local with respect to the set of Lagrangian multi-
pliers:
min
U
(
max
y
c
?
?
P (y
c
|x) +
?
i?|x|
u
i
(y
c
i
)
?
?
(4)
+max
y
w
?
?
F (y
w
|x)?
?
j?|x|
u
j
(y
w
j
)
?
?
)
This method is called dual decomposition (DD)
(Rush et al, 2010). Similar to previous work
(Rush and Collins, 2012), we solve this DD prob-
lem by iteratively updating the sub-gradient as de-
picted in Algorithm 1.
1
In each iteration, if the
best segmentations provided by the two models do
not agree, then the two models will receive penal-
ties for the decisions they made that differ from the
other. This penalty exchange is similar to message
passing, and as the penalty accumulates over itera-
tions, the two models are pushed towards agreeing
with each other. We also give an updated Viterbi
decoding algorithm for CRF and a modified beam-
search algorithm for perceptron in Algorithm 1. T
is the maximum number of iterations before early
stopping, and ?
t
is the learning rate at time t. We
adopt a learning rate update rule from Koo et al
(2010) where ?
t
is defined as
1
N
, where N is the
number of times we observed a consecutive dual
value increase from iteration 1 to t.
3 Experiments
We conduct experiments on the SIGHAN 2003
(Sproat and Emerson, 2003) and 2005 (Emer-
son, 2005) bake-off datasets to evaluate the ef-
fectiveness of the proposed dual decomposition
algorithm. We use the publicly available Stan-
ford CRF segmenter (Tseng et al, 2005)
2
as our
character-based baseline model, and reproduce
the perceptron-based segmenter from Zhang and
Clark (2007) as our word-based baseline model.
We adopted the development setting from
(Zhang and Clark, 2007), and used CTB sections
1-270 for training and sections 400-931 for devel-
opment in hyper-parameter setting; for all results
given in tables, the models are trained and eval-
uated on the standard train/test split for the given
dataset. The optimized hyper-parameters used are:
1
See Rush and Collins (2012) for a full introduction to
DD.
2
http://nlp.stanford.edu/software/segmenter.shtml
195
`2
regularization parameter ? in CRF is set to
3; the perceptron is trained for 10 iterations with
beam size 200; dual decomposition is run to max
iteration of 100 (T in Algo. 1) with step size 0.1
(?
t
in Algo. 1).
Beyond standard precision (P), recall (R) and
F
1
scores, we also evaluate segmentation consis-
tency as proposed by (Chang et al, 2008), who
have shown that increased segmentation consis-
tency is correlated with better machine transla-
tion performance. The consistency measure cal-
culates the entropy of segmentation variations ?
the lower the score the better. We also report
out-of-vocabulary recall (R
oov
) as an estimation of
the model?s generalizability to previously unseen
words.
4 Results
Table 1 shows our empirical results on SIGHAN
2005 dataset. Our dual decomposition method
outperforms both the word-based and character-
based baselines consistently across all four sub-
sets in both F
1
and OOV recall (R
oov
). Our
method demonstrates a robustness across domains
and segmentation standards regardless of which
baseline model was stronger. Of particular note
is DD?s is much more robust in R
oov
, where the
two baselines swing a lot. This is an important
property for downstream applications such as en-
tity recognition. The DD algorithm is also more
consistent, which would likely lead to improve-
ments in applications such as machine translation
(Chang et al, 2008).
The improvement over our word- and character-
based baselines is also seen in our results on the
earlier SIGHAN 2003 dataset. Table 2 puts our
method in the context of earlier systems for CWS.
Our method achieves the best reported score on 6
out of 7 datasets.
5 Discussion and Error Analysis
On the whole, dual decomposition produces state-
of-the-art segmentations that are more accurate,
more consistent, and more successful at induc-
ing OOV words than the baseline systems that it
combines. On the SIGHAN 2005 test set, in
over 99.1% of cases the DD algorithm converged
within 100 iterations, which gives an optimality
guarantee. In 77.4% of the cases, DD converged
in the first iteration. The number of iterations to
convergence histogram is plotted in Figure 1.
SIGHAN 2005
AS PU CU MSR
Best 05 95.2 95.0 94.3 96.4
Zhang et al 06 94.7 94.5 94.6 96.4
Z&C 07 94.6 94.5 95.1 97.2
Sun et al 09 - 95.2 94.6 97.3
Sun 10 95.2 95.2 95.6 96.9
Dual-decomp 95.4 95.3 94.7 97.4
SIGHAN 2003
Best 03 96.1 95.1 94.0
Peng et al 04 95.6 94.1 92.8
Z&C 07 96.5 94.0 94.6
Dual-decomp 97.1 95.4 94.9
Table 2: Performance of dual decomposition in
comparison to past published results on SIGHAN
2003 and 2005 datasets. Best reported F
1
score
for each dataset is highlighted in bold. Z&C 07
refers to Zhang and Clark (2007). Best 03, 05 are
results of the winning systems for each dataset in
the respective shared tasks.
Error analysis In many cases the relative con-
fidence of each model means that dual decom-
position is capable of using information from
both sources to generate a series of correct
segmentations better than either baseline model
alone. The example below shows a difficult-to-
segment proper name comprised of common char-
acters, which results in undersegmentation by the
character-based CRF and oversegmentation by the
word-based perceptron, but our method achieves
the correct middle ground.
Gloss Tian Yage / ?s / creations
Gold ??? /? /??
CRF ???? /??
PCPT ?? /? /? /??
DD ??? /? /??
A powerful feature of the dual decomposition
approach is that it can generate correct segmenta-
tion decisions in cases where a voting or product-
of-experts model could not, since joint decod-
ing allows the sharing of information at decod-
ing time. In the following example, both baseline
models miss the contextually clear use of the word
?? (?sweets / snack food?) and instead attach?
to the prior word to produce the otherwise com-
mon compound ??? (?a little bit?); dual de-
composition allows the model to generate the cor-
rect segmentation.
Gloss Enjoy / a bit of / snack food / , ...
Gold ?? /?? /?? /?
CRF ?? /??? /? /?
PCPT ?? /??? /? /?
DD ?? /?? /?? /?
196
Figure 1: No. of iterations till DD convergence.
We found more than 400 such surprisingly ac-
curate instances in our dual decomposition output.
Finally, since dual decomposition is a method of
joint decoding, it is still liable to reproduce errors
made by the constituent systems.
6 Conclusion
In this paper we presented an approach to Chinese
word segmentation using dual decomposition for
system combination. We demonstrated that this
method allows for joint decoding of existing CWS
systems that is more accurate and consistent than
either system alone, and further achieves the best
performance reported to date on standard datasets
for the task. Perhaps most importantly, our ap-
proach is straightforward to implement and does
not require retraining of the underlying segmenta-
tion models used. This suggests its potential for
broader applicability in real-world settings than
existing approaches to combining character-based
and word-based models for Chinese word segmen-
tation.
Acknowledgements
We gratefully acknowledge the support of the U.S.
Defense Advanced Research Projects Agency
(DARPA) Broad Operational Language Transla-
tion (BOLT) program through IBM. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of DARPA, or
the US government.
References
Galen Andrew. 2006. A hybrid Markov/semi-Markov
conditional random field for sequence segmentation.
In Proceedings of EMNLP.
Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.
2008. Improving word alignment by adjusting chi-
nese word segmentation. In Proceedings of the third
International Joint Conference on Natural Lan-
guage Processing (IJCNLP).
Pichuan Chang, Michel Galley, and Chris Manning.
2008. Optimizing chinese word segmentation for
machine translation performance. In Proceedings of
the ACL Workshop on Statistical Machine Transla-
tion.
Keh-Jiann Chen and Shing-Huan Liu. 1992. Word
identification for mandarin chinese sentences. In
Proceedings of COLING.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of ACL.
Thomas Emerson. 2005. The second international
Chinese word segmentation bakeoff. In Proceed-
ings of the fourth SIGHAN workshop on Chinese
language Processing.
Jianfeng Gao, Mu Li, and Chang-Ning Huang. 2003.
Improved source-channel models for Chinese word
segmentation. In Proceedings of ACL.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of EMNLP.
Jonathan K. Kummerfeld, Daniel Tse, James R. Cur-
ran, and Dan Klein. 2013. An empirical examina-
tion of challenges in chinese parsing. In Proceed-
ings of ACL-Short.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of 18th International
Conference on Machine Learning (ICML).
Dekang Lin. 2009. Combining language modeling and
discriminative classification for word segmentation.
In Proceedings of the 10th International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing).
Alexander M. Rush and Michael Collins. 2012. A tu-
torial on dual decomposition and Lagrangian relax-
ation for inference in natural language processing.
JAIR, 45:305?362.
197
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural
language processing. In Proceedings of EMNLP.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of
Joint Conferences on Artificial Intelligence (IJCAI).
Richard Sproat and Thomas Emerson. 2003. The
first international Chinese word segmentation bake-
off. In Proceedings of the second SIGHAN work-
shop on Chinese language Processing.
Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun?ichi Tsujii. 2009. A dis-
criminative latent variable chinese segmenter with
hybrid word/character information. In Proceedings
of HLT-NAACL.
Weiwei Sun. 2010. Word-based and character-
basedword segmentation models: Comparison and
combination. In Proceedings of COLING.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurasfky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the fourth SIGHAN
workshop on Chinese language Processing.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and
Xihong Wu. 2006. Chinese word segmentation with
maximum entropy and n-gram language model. In
Proceedings of the fifth SIGHAN workshop on Chi-
nese language Processing.
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A character-based joint model for chinese word seg-
mentation. In Proceedings of COLING.
Mengqiu Wang, Wanxiang Che, and Christopher D.
Manning. 2013. Joint word alignment and bilingual
named entity recognition using dual decomposition.
In Proceedings of ACL.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Compu-
tational Linguistics and Chinese Language Process-
ing, pages 29?48.
Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In Proceedings of ACL.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro
Sumita. 2006. Subword-based tagging by condi-
tional random fields for Chinese word segmentation.
In Proceedings of HLT-NAACL.
198
Workshop on Computational Linguistics for Literature, pages 18?25,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Towards a Literary Machine Translation:
The Role of Referential Cohesion
Rob Voigt Dan Jurafsky
Center for East Asian Studies Department of Linguistics
Stanford University Stanford University
robvoigt@stanford.edu jurafsky@stanford.edu
Abstract
What is the role of textual features above the 
sentence  level  in  advancing  the  machine 
translation of literature? This paper examines 
how  referential  cohesion  is  expressed  in 
literary  and  non-literary  texts  and  how  this 
cohesion affects translation. We first show in a 
corpus study on English that literary texts use 
more dense reference chains to express greater 
referential  cohesion  than  news.  We  then 
compare the referential  cohesion of machine 
versus  human  translations  of  Chinese 
literature and news. While human translators 
capture  the  greater  referential  cohesion  of 
literature,  Google  translations  perform  less 
well at capturing literary cohesion. Our results 
suggest  that  incorporating discourse  features 
above  the  sentence  level  is  an  important 
direction for MT research if it is to be applied 
to literature.
Introduction
The concept of literary machine translation 
might  seem at  first  to  be a  near-contradiction in 
terms.  The  field  of  machine  translation  has 
traditionally aimed its sights at  the translation of 
technical or otherwise informative texts,  with the 
strongest focus on newswire and other informative 
texts relevant to the goals of government funders.
Nevertheless, the prospect of literary MT is 
appealing. Human translation of literary texts is an 
extremely time- and money-intensive task, but one 
that  is  a crucial  element of the global  system of 
transcultural  literary exchange.  From a  technical 
standpoint, since ?by definition, literature is the art 
that  uses  language?  (Chapman  1973),  literary 
translation  represents  perhaps  the  strongest 
formulation  of  the  machine  translation  problem. 
Jonathan  Slocum,  writing  in  1985,  essentially 
rejects  the  idea of  literary MT altogether,  noting 
that  it  is  serendipitous  for  technical  MT  that 
emphasis  is  placed  on  semantic  fidelity  to  the 
source text, whereas literary translation must take 
into  account  larger  considerations  such  as  style 
with which ?computers do not  fare well.?  Given 
the explosion of statistical  methodologies in MT, 
are we now at a point where we can hope to begin 
tackling some of the  questions associated with a 
potential literary machine translation?
This  problem  is  severely  understudied. 
Regardless of the plausibility (or even desirability) 
of  eventually  using  MT to  produce  full-fledged 
translations  of  literary  texts,  a  serious 
consideration  of  the  unique  difficulties  posed  by 
literary translation may well serve to push forward 
our computational understanding of literature and 
the language of translation.
In particular,  literary translation seems to 
demand  that  we  address  larger-scale  textual 
features  beyond  the  sentence-level  approach 
commonly  employed  by  contemporary  MT 
systems.  There  is  a  substantial  body of  work by 
scholars  in  the  field  of  translation  studies 
addressing  greater-than-sentence-level  textual 
features  from a  linguistic  and  literary-theoretical 
perspective,  and  this  existing  work  can  offer 
conceptual understanding and a parallel vocabulary 
with  which  to  discuss  progress  in  this  regard  in 
machine translation. 
Eugene Nida (1964), for example, used the 
terms  ?formal  equivalence?  and  ?dynamic 
equivalence? to  differentiate  between translations 
aiming to  replicate  the  form of  their  source  and 
those aiming to replicate the source text's effects 
on its  readers.  Hatim and Mason (1995)  brought 
the  ?seven  standards  of  textuality?  set  forth  by 
Beaugrande  and  Dressler  (1981)  into  the 
translation studies context as metrics for evaluating 
the  ?expectation-fulfilling?  or  ?expectation-
defying? outcome of a translated text. 
18
Cohesion  is  defined  by  Beaugrande  and 
Dressler  as  ?concern[ing]  the  ways  in  which the 
components  of  the  textual  world,  i.e.,  the 
configuration  of  concepts  and  relations  which 
underlie the surface text,  are mutually accessible 
and  relevant."  Cohesion  considers  the  limited 
human capacity for storing the ?surface materials? 
of a text long enough to relate them semantically 
during the act of reading.
We therefore  propose to  study referential 
cohesion (Halliday and Hasan 1976), the relation 
between co-referring entities in a narrative, as an 
important  component  of  cohesion.   Referential 
cohesion  has  a  significant  literature  in  natural 
language processing (Grosz et al 1995, Mani et al 
1998, Marcu 2000, Karamanis et al 2004, Kibble 
and  Power  2004,  Elsner  and  Charniak  2008, 
Barzilay  and  Lapata  2008,  inter  alia)  as  does 
automatic  coreference  resolution,  which  has 
significantly increased in accuracy in recent years 
(Bengston  and  Roth  2008,  Haghighi  and  Klein 
2009, Haghighi and Klein 2010, Rahman and Ng 
2011, Pradhan et al 2011, Lee et al 2011).
We formulate  and test  two hypotheses in 
this position paper: First, we anticipate that given 
stylistic  considerations  and  their  fundamental 
narrative  function,  prose  literary  texts  are 
inherently ?more cohesive? than news. Second, in 
light of the aforementioned necessity for ?dynamic 
equivalence?  in  the  literary  translation,  we 
anticipate that current machine translation systems, 
built  with  newswire  texts  in  mind,  will  be  less 
successful at conveying cohesion for literary texts 
than for news.
2. Investigating Literary Cohesion
Our first preliminary experiment examines 
how  referential  cohesion  in  literary  texts  differs 
from  news  text  by  examining  coreference  in  a 
monolingual  English-language  corpus,  without 
considering machine-translated texts.
We created a small corpus of twelve short 
stories  for  comparison  with  twelve  recent  long-
form news stories from the New York Times, Wall 
Street Journal, The Atlantic, and the news blog The 
Daily Beast. The stories chosen were written by a 
variety  of  authors:  Isaac  Asimov,  J.D.  Salinger, 
Edgar Allen Poe, Tobias Wolff, Vladimir Nabokov, 
Sir  Arthur  Conan  Doyle,  Shirley  Jackson,  Jack 
London,  Mark  Twain,  Willa  Cather,  Ambrose 
Bierce,  and  Stephen  Crane  ?  in  the  interest  of 
avoiding over-specificity to any particular genre or 
style.  The  corpus  thus  included  12  short  stories 
with  76,260  words  and  12  news  articles  with 
23,490  words,  for  a  total  corpus  size  of  24 
documents and 99,750 words.
We used standard publicly-available  NLP 
tools to process the corpus. We used the Stanford 
CoreNLP suite1 to tokenize and sentence-split both 
the human and MT versions of each text and then 
to run the multi-pass sieve coreference resolution 
system described in Lee et al (2011). 
This  system  works  by  making  multiple 
passes  over  the  text,  first  doing  recall-oriented 
mention  extraction,  then  resolving  coreference 
through a series of sieves moving from highest to 
lowest  precision.  This  system  is  state-of-the-art, 
with a B3 F1 score of 68.9 with no gold mention 
boundaries  on  the  CoNLL 2011  shared  task  test 
set.  Nevertheless,  it  is  likely  to  introduce  some 
measure of noise into our results.
For the rest of the paper we use the term 
?cluster? to refer to clusters agglomerated by the 
system  that  co-refer  to  the  same  entity,  and 
?mention? to refer to individual instances of each 
entity in the text.
Clusters per 
100 Tokens
Mentions per 
100 Tokens
Density:
Mentions 
per Cluster
Short 
Stories
3.6 19.3 5.4
News 
Text
3.9 15.0 3.9
Table  1.  Cohesion  as  measured  by  coreference  in 
literary  vs.  non-literary  texts.  Figures  given  are  the 
overall average across all documents.
Table 1 reports the numbers of clusters and 
mentions (normalized per 100 tokens). The literary 
texts had the same number of clusters (entities) as 
the news texts (one-tailed t-test,  p = 0.080), albeit 
with a  trend towards fewer  clusters  in  literature. 
But  literary text had more mentions (p < 0.001), 
and a higher number of mentions per cluster (p < 
0.001) than the news texts. 
The  results  of  this  preliminary  study 
suggest that the literary text tended to discuss the 
same number of entities as the non-fiction, but to 
1 Available online at 
nlp.stanford.edu/software/corenlp.shtml
19
Suddenly,  the nurse resorted to direct measures.  She 
seized  the boy?s upper arm in one  hand and dipped 
the other in the milk. She dashed the milk across his 
lips,  so  that  it dripped  down  cheeks  and  receding 
chin.
...
Always,  his frightened eyes were on  her, watching, 
watching for the one false move.  She found herself 
soothing  him, trying to move  her hand very slowly 
toward  his hair, letting  him see  it every inch of the 
way, see there was no harm in it. And she succeeded 
in stroking his hair for an instant.
?
Instead, she turned on the night light and moved the 
bed. The poor thing was huddled in the corner, knees 
up against  his chin,  looking up at  her with  blurred 
and apprehensive eyes.
?
She looked down at those eager brown eyes turned up 
to hers and passed her hands softly through his thick, 
curly hair. 
Figure  1.  Human  markup  of  cohesion  throughout 
Asimov's ?The Ugly Little Boy.? Recurring entities are 
color-coded: red is the character Edith Fellowes, grey is 
her hands, blue is the character Timmie, light green is 
his eyes, dark green is his chin, yellow is his hair, and 
magenta is  the milk.  This sample contains  149 words 
and 7 recurring entities with a total of 29 mentions.
mention each entity more often.  In other words, 
literary text uses more dense reference chains as a 
way of creating a higher level of cohesion. 
Figures  1  and  2  provide  representative 
examples, hand-labeled for coreference, to offer a 
qualitative intuition for this difference in cohesion. 
In the literary example in Figure 1 we find seven 
recurring entities with an average of 4.1 mentions 
each.  In  the  news  example  in  Figure  2  we  find 
seven  recurring  entities  but  only  3.0  average 
mentions,  resulting  in  qualitatively  less  dense 
reference chains in the news sample.
Our  results  are  consistent  with  Biber 
(1988),  whose  factor  analysis  study  found  that 
fiction tended to have a high frequency of third-
person  personal  pronouns.  This  is  true  in  our 
corpus;  third-person pronouns occur  57.7% more 
in the fiction as opposed to the non-fiction texts 
(16.9  vs  10.7  occurrences  per  100  words).  But 
even  when  we  count  ignoring  third-person 
pronouns, we found a greater density of mentions 
per cluster for literature than for news (4.0 vs 3.3, 
p = 0.015). The result that literature seems to have 
more to say about each entity thus extends and
Two studies have found that  weight-loss  operations 
worked much better than  the standard therapies for 
Type  2  diabetes in  obese  and  overweight  people 
whose blood sugar was out of control. Those who had 
surgery, which stapled the  stomach and rerouted the 
small  intestine,  were  much  more  likely  to  have  a 
complete  remission  of  diabetes,  or  to  need  less 
medicine,  than  people who  were  given  the  typical 
regimen of drugs, diet and exercise.
...
The new studies, published on Monday by The New 
England  Journal  of  Medicine,  are  the  first  to 
rigorously  compare  medical  treatment with  these 
particular  stomach and intestinal  operations as ways 
to  control  diabetes.  Doctors had  been  noticing  for 
years that weight-loss operations, also called bariatric 
surgery, could sometimes get rid of Type 2  diabetes. 
But they had no hard data.
...
One  of  the  studies,  conducted  at  the  Catholic 
University in Rome, compared two types of  surgery 
with usual medical treatment.
Figure 2. Human markup of cohesion throughout a NYT 
news article. Recurring entities are color-coded, similar 
to  the  above.  This  sample  contains  152 words  and  7 
recurring entities with a total of 21 mentions.
explains  Biber's  finding  that  literature  has  more 
third-person pronouns.
While  our  results  are  suggestive,  they 
remain  preliminary.   A more  detailed  follow-up 
will need to look at the specific realization of the 
mentions and the kind of local coherence relations 
that  link them (Althaus et al  2004,  Poesio et  al. 
2004,  Barzilay  and  Lapata  2008,  Elsner  and 
Charniak  2008),  and  to  investigate  the  different 
aspects  of  referential  chains  with  larger  corpora 
and more varying genres.
3. MT Success at Conveying Cohesion
To evaluate the impact of this difference in 
expressed  cohesion  on  machine  translation 
systems, we compared coreference output between 
human  and  machine  translations  of  literary  and 
informative texts from Chinese.  For this task we 
chose  a  small  dataset  of  sixteen  short  stories  in 
Chinese by the early 20th-century author Lu Xun 
(??) and their corresponding English translations 
by  Gladys  Yang.  We  chose  Lu  Xun  for  his 
prominence  as  the  ?father  of  modern  Chinese 
literature? and vernacular style, and because Yang's 
English translations are widely accepted as being 
20
of  high  quality  by  the  literary  community.  For 
comparison to news text, we chose a series of six 
long-form  articles  from  the  magazine  Sinorama 
and  their  corresponding  English  reference 
translations in the  LDC's ?Chinese English News 
Magazine  Parallel  Text?  corpus  (LDC2005T10). 
These  magazine  texts  were  chosen  because  the 
brief newswire texts often used in MT evaluation 
are too short to allow for meaningful textual-level 
comparisons  of  this  sort.  Thus  our  corpus 
contained  16  human-translated  short  stories  with 
90,712 words, 16 machine-translated short stories 
with 82,475 words, 6 human-translated magazine 
articles  with  45,310  words,  and  6  machine-
translated magazine articles with 39,743 words, for 
a total size of 44 documents and 258,240 words.
We  used  Google  Translate  as  our  MT 
translation  engine,  first  because  the  large  web-
based resources behind that system might help to 
mitigate  the  inevitable  complication  of  domain 
specificity in the training data, and second because 
of  its  social  position  internationally  as  the  most 
likely  way  average  readers  might  encounter 
machine translation. 
We first used Google Translate to produce 
machine  translations  of  both  the  literary  and 
magazine texts, and then used the Lee et al (2011) 
coreference  system  in  Stanford  CoreNLP  as 
described above to evaluate cohesion on both the 
human  and  machine  English  translations.  As 
acknowledged  in  the  prior  section,  automatic 
coreference is likely to introduce some amount of 
noise, but there is no reason to think that this noise 
would be biased in any particular direction for MT.
Results  from the  coreference  analysis  of 
the literary and magazine texts are shown in Table 
2.  The results  in  the  two rows labeled ?Human? 
substantiate our findings from the previous section. 
The human translations of the short stories have a 
significantly (p  =  0.003)  higher  referential  chain 
density  (5.2)  than  the  human  translations  of  the 
magazine  pieces  (4.2).  Translators,  or  at  least 
Gladys  Yang  in  these  translations,  seem  to  act 
similarly to  source-text  writers  in  creating  more 
dense referential  chains in literature than in non-
fiction genres.
In order to study the success of machine 
translation in dealing with cohesion, we took the 
human translations as a gold standard in each case, 
using this translation to normalize the number of 
clusters and mentions to the length of the reference
Clusters per
100 Tokens
Mentions per 
100 Tokens
Density:
Mentions 
per Cluster
Short Story
   Human 3.7 19.0 5.2
   Machine 4.1 16.4 3.8
Magazine
   Human 3.9 16.0 4.2
   Machine 3.9 14.0 3.7
Table 2. Cohesion as measured by coreference in human 
and machine translations of  Lu Xun short  stories  and 
Sinorama magazine articles. The first two columns are 
normalized  to  the  length  of  the  human  ?gold? 
translations,  and figures given are the overall  average 
across all documents.
documents to address the length variance caused 
by the MT system.
The  results  in  Table  2  show  little 
underclustering for the MT output.  The number of 
clusters (entities) in the machine translations (4.1 
and 3.9) do not differ from the human translations 
(3.7 and 3.9), (p = 0.074), although there is a trend 
toward underclustering for literature.
The main difference we see is in referential 
chain  density  (mentions  per  cluster).  Whereas 
these  experiments  reconfirm  the  trend  towards 
more  mentions  per  cluster  in  literature  than 
informative  text,  referential  chains  in  the  MT 
output do not differ between the two genres. The 
machine translation only captures 79.4% (13,846 
vs.  17,438)  of  the  human-translated  mentions  in 
the literary texts.
In  the  literary  genre  the  automatic 
coreference system finds more than one additional 
mention per  cluster  in  the  human translations  as 
compared  to  MT  (p  <  0.001),  while  in  the 
magazine case the human and MT translations are 
the same, though there is a similar trend towards 
less  dense  referential  chains  in  MT output  (p  = 
0.055).
4. Examples and Discussion
It  is  worth  first  acknowledging  the 
somewhat  surprising  ability  of  MT  to  maintain 
cohesion in both domains. The fact that a system 
operating  almost  exclusively  on  a  sentence-by-
sentence basis is able to maintain upwards of three-
quarters  of  the  mentions  in  the  difficult  and 
linguistically distant context of Chinese-to-English 
21
MT is remarkable in and of itself, and speaks to the 
relative success of modern MT. There is, of course, 
no  guarantee  that  these  mentions  found  by  the 
coreference system are in fact all the correct ones, 
so the true figure is likely somewhat lower, but a 
qualitative  examination  of  the  system's  output 
shows that they are largely accurate.
What is actually causing the discrepancies 
in  cohesion  noted  above  as  regards  our  two 
domains? Below we look at some specific cases of 
reduced cohesion in our results from the Lu Xun 
story ?Flight to the Moon.? In these examples the 
human  translator  was  forced  to  rely  on  greater-
than-sentence-level features of the text to effect an 
appropriately  cohesive  translation  that  the  MT 
system was unable to convey.
Zero Anaphora
Zero  anaphora  is  a  well-documented  and 
common linguistic phenomena in Chinese (Li and 
Thompson  1979,  Huang  1989).  Kim  (2000) 
investigated subject drop in Chinese and English, 
finding  that  English  overtly specifies  subjects  in 
96% of cases, while the figure for Chinese is only 
64%, and a significant amount of prior work has 
focused  on  the  computational  identification  and 
resolution  of  zero  anaphora in  Chinese (see  Yeh 
and  Chen  2001,  Converse  2006,  Zhao  and  Ng 
2007,  Kong  and  Zhou  2010).  The  following 
example sentences demonstrate this difficulty. 
Human Translation
    When the big game was finished  they ate 
wild boars, rabbits and pheasants. He was such a fine 
archer, he could shoot as much as he pleased.
Machine Translation
        Later large animal shot down, ate wild boar, 
rabbit pheasant; shooting method and high strength, 
many as you want.
Original Chinese
        ?????????????????
????????????
Figure 3. Reduced cohesion via zero anaphora in MT 
output. Relevant mentions are hand-annotated in bold.
In  a  qualitative  analysis  of  our  results, 
problems  such  as  these  were  by  far  the  most 
common  cause  of  cohesion  errors,  and  as  the 
reader will notice, they often lead to an output that 
loses crucial elements for maintaining the cohesion 
of the narrative, such as in this case the distinction 
between the husband/wife couple, ?they,? and the 
husband individually, ?he.? 
Inconsistent Reference
Having  no  process  for  maintaining 
consistency of reference to entities in the narrative, 
the  following  non-consecutive  coreferencing 
sentences illustrate how in the MT version of the 
text  the  cohesiveness  of  the  ?hen? cluster  in  the 
original is lost.
Human Translation
-"Who are you? Why have you shot my best black 
laying hen?"
-"What! A hen?" he echoed nervously. "I thought  it 
was a wood pigeon."
-"Imagine mistaking a hen for a wood pigeon!"
-"I am Yi." While saying this he saw that his arrow 
had pierced the hen's heart, killing it outright.
-"What about this hen?"
-"She was my best: she laid me an egg every day."
-"I'll give you these for your hen"
Machine Translation
-"Who are you what? How good black  hen shot to 
the top of my house?"
-"Ah!  Chicken?  I  only  said  a  wood  pigeon 
partridge," he said in dismay.
-"hens do  not  know,  will  be  treated  as  the  wood 
pigeon partridge"
-"I Yi Yi." He said, to see his shot arrows, is being 
consistently the heart of the hen, of course, died
-"Chicken how to do it?"
-"Lost my best hen every day to lay eggs."
-"they brought lost your chicken."
Original Chinese
-????????????????????
??"
-??????????????? ?? ??????
 
-"??????????????"
-? ??????? ??????????????
????????????
-??? ? ?????
-????????????????"
-"????????"
Figure 4. Reduced cohesion via inconsistent reference in 
MT output.  Relevant  mentions  are  hand-annotated  in 
bold.
The reader will notice that in the original 
Chinese,  ji (? ,  lit.  ?chicken?) is used here as a 
22
shortened  version  of  muji (?? ,  lit.  ?hen?)  in 
colloquial  speech,  which  the  human  translator 
clearly  notes  and  translates  each  mention 
consistently to maintain cohesion. Similarly, being 
that  number is  not  explicitly marked in  Chinese, 
the MT system translates  lian muji (??? ,  lit. 
?even hen?) as ?hens? instead of catching that here 
 ?? refers back to the entity being discussed.
De (?) Drops
It is common in Chinese for the noun head 
of a nominalization formed by the particle de (?) 
to  be  implicit,  yet  in  many  cases  the  human 
translator will add it for clarity and, presumably, to 
maintain cohesion.
Human Translation
"There are those who know my name."
      
Machine Translation
?Some people is one to know."
Original Chinese
?                                       ? ? ? ? ? ? ? ?? ??" 
Exist  some  people be  one  hear  then    know  NOM
Figure  5.  Reduced  cohesion  via  de dropping  in  MT 
output. Relevant mentions are hand-annotated in bold.
This  phenomenon  reminds  of  translation 
theorist  Mona  Baker's  (1996)  concept  of 
?explicitation?: ?an overall tendency to spell things 
out rather than leave them implicit in translation.? 
Indeed, Olohan and Baker (2000) demonstrate this 
empirically using the Translational English Corpus, 
finding  a  strong  tendency  in  translated  texts  to 
explicitly  mark  the  ?that?-connective  following 
words such as ?say,? ?tell,? ?promise,? and so on 
where it could have been omitted. 
5. Implications and Future Research
We  found  in  two  separate  analyses that 
literary texts had more dense reference chains than 
informative  texts.  This  result  supports  our 
hypothesis  that  literary  texts  are  indeed  more 
cohesive in general than informative texts; that is 
to  say,  the  stylistic  and  narrative  demands  of 
literature  lead  to  prose  being  more  cohesively 
?about?  its  subjects  than  news.  It  remains  to 
replicate  this  experiment  on  a  large,  carefully 
sampled  cross-genre  corpus  to  confirm  these 
preliminary findings,  perhaps  integrating  a  more 
complex measure of cohesion as in Barzilay and 
Lapata (2008).
We  also  found  that  MT  systems  had 
difficulty  in  conveying  the  cohesion  in  literary 
texts. Of course these results are preliminary and 
may be confounded by the nature of the training 
data  used  by  modern  MT systems.  The  uses  of 
Google  Translate  as  an  MT system and  longer-
form  magazine  articles  as  our  informative  texts 
were aimed at mitigating these concerns to some 
extent, but for now these results primarily serve as 
indicative of the need for further research in this 
area.
Cohesion, as well, is only one of the seven 
?standards of textuality? put forth by Beaugrande 
and Dressler  (1981)  and taken up by Hatim and 
Mason (1997) in the translation context. Some of 
these  have  an  existing  literature  addressing  their 
computational  identification  and  analysis  (eg. 
Morris and Hirst 1991), in which cases we might 
apply existing methods to identify genre effects in 
literary text.  For  others,  such  as  situationality,  it 
remains  to  investigate  appropriate  computational 
analogues  for  large-scale  automatic  analysis  and 
application  to  literary  text.  Studies  addressing 
relevant  textual-level  concerns  in  literature  show 
increasing promise,  such as  Elson et  al.  (2010)'s 
work  in  automatically extracting  social  networks 
from fiction.
Once  these  sorts  of  genre  effects  in 
literature are more clearly understood, they can be 
addressed  on  a  large  scale  for  comparisons 
between  machine-  and  human-translated  literary 
texts  in  the  manner  carried  out  in  this  paper,  in 
order to identify further potential stumbling blocks 
for  machine  translation  on  the  textual  level  as 
regards  literary  texts.  Our  preliminary  work  as 
presented  here  suggests,  at  the  very  least,  the 
potential value and necessity of such analyses if we 
are  to  make  progress  towards  a  true  literary 
machine translation.
    Acknowledgements
Thanks  to  Heeyoung  Lee  for  help  with  the 
coreference  system,  three  anonymous  reviewers  for  their 
careful  reading  and  helpful  comments,  and  the  U.S. 
Department of Education for the Foreign Language and Area 
Studies grant that helped fund this research.
23
    References
Althaus,  Ernst,  Nikiforos  Karamanis,  and 
Alexander  Koller.  2004.  Computing 
locallycoherent discourses. In ACL.
Baker,  Mona.  1996.  Corpus-based  translation 
studies:  The  challenges  that  lie  ahead.  In 
Terminology, LSP and Translation: Studies in 
language  engineering. John  Benjamins, 
Amsterdam.
Barzilay,  Regina  and  Mirella  Lapata.  2008.
Modeling  Local  Coherence:  An  Entity-based 
Approach. Computational Linguistics, 34(1).
Beaugrande, Robert and Wolfgang Dressler. 1981. 
Introduction  to  Text  Linguistics.  Longman, 
London.
Bengston, E. and Dan Roth. 2008. Understanding 
the value of features for coreference resolution. 
In EMNLP.
Biber, Douglas. 1988. Variation across speech and 
writing. Cambridge  University  Press, 
Cambridge.
Chapman,  Raymond.  1973.  Linguistics  and 
Literature. Edward Arnold, London.
Converse,  Susan.  2006.  Pronominal  anaphora 
resolution for Chinese. Ph.D. thesis.
Elsner,  Micha  and  Eugene  Charniak.  2008. 
Coreference-inspired  Coherence  Modeling.  In 
Proceedings of ACL 2008. 
Elson,  David,  Nicholas  Dames,  and  Kathleen 
McKeown.  2010.  Extracting  social  networks 
from literary fiction. In ACL.
Grosz,  Barbara,  Aravind  K.  Joshi,  and  Scott 
Weinstein.  1995.  Centering:  A framework  for 
modeling  the  local  coherence  of  discourse. 
Computational Linguistics, 21(2).
Haghighi,  Aria  and  Dan  Klein.  2009.  Simple 
coreference  resolution  with  rich  syntactic  and 
semantic features. In EMNLP.
Haghighi, Aria and Dan Klein. 2010. Coreference 
resolution in a modular, entity-centered model. 
In HLT-NAACL.
Halliday,  M.  A.  K.  and  Ruqaiya  Hasan.  1976. 
Cohesion in English. Longman, London.
Hatim, Basil and Ian Mason. 1997. The Translator 
as Communicator. Routledge, London.
Huang, James C.-T. 1989. Pro drop in Chinese, a 
generalized control approach. In O, Jaeggli and 
K. Safir,  editors,  The Null  Subject  Parameter. 
D. Reidel Dordrecht.
Karamanis,  Nikiforos,  Massimo  Poesio,  Chris 
Mellish, and Jon Oberlander. 2004. Evaluating 
centering-based  metrics  of  coherence  for  text 
structuring using a reliably annotated corpus. In 
ACL.
Kibble,  Rodger  and  Richard  Power.  2004. 
Optimizing  Referential  Coherence  in  Text 
Generation. Computational Linguistics 30(4).
Kim, Young-Joo. 2000. Subject/object drop in the 
acquisition  of  Korean:  A  cross-linguistic 
comparison.  Journal of East Asian Linguistics, 
9(4).
Kong,  Fang  and  Guodong  Zhou,  2010.  A Tree 
Kernel-based  Unified  Framework  for  Chinese 
Zero Anaphora Resolution. In EMNLP.
Lee,  Heeyoung,  Yves  Peirsman,  Angel  Chang, 
Nathanael  Chambers,  Mihai  Surdeanu,  Dan 
Jurafsky.  Stanford's  Multi-Pass  Sieve 
Coreference Resolution System at the CoNLL-
2011 Shared Task. 2011.  In Proceedings of the  
CoNLL-2011 Shared Task.
Li,  Charles  and Sandra Thompson.  1979.  Third-
person pronouns and zero-anaphora in Chinese 
discourse. Syntax and Semantics, 12:311-335.
Ma,  Xiaoyi.  2005.  Chinese  English  News 
Magazine Parallel Text. LDC2005T10.
Mani, Inderjeet, Barbara Gates, and Eric Bloedorn. 
1998.  Using Cohesion and Coherence Models 
for Text Summarization. In AAAI.
Marcu, Daniel. 2000. The Theory and Practice of  
Discourse  Parsing  and  Summarization.  MIT 
Press, Cambridge, MA.
Morris,  Jane  and  Graeme  Hirst.  1991.  Lexical 
Cohesion Computed by Thesaural Relations as 
an  Indicator  of  the  Structure  of  Text. 
Computational Linguistics, 17(1).
Nida,  Eugene.  1964.  Towards  a  Science  of  
Translating. Brill, Leiden.
Olohan, Maeve and Mona Baker. 2000. Reporting 
that in  translated  English:  Evidence  for 
subconscious processes of explicitation? Across  
Languages and Cultures 1.
Poesio, Massimo, Rosemary Stevenson, Barbara di 
Eugenio, and Janet Hitzeman, 2004. Centering: 
A  Parametric  theory  and  its  instantiations. 
Computational Linguistics, 30(3).
Pradhan,  Sameer,  Lance  Ramshaw,  Mitchell 
Marcus, Martha Palmer, Ralph Weischedel, and 
Nianwen Xue. 2011. CoNLL-2011 Shared Task: 
Modeling  Unrestricted  Coreference  in 
OntoNotes. In CoNLL.
24
Rahman, Altaf and Vincent Ng. 2011. Coreference 
resolution with world knowledge. In ACL.
Slocum,  Jonathan.  1985.  A Survey  of  Machine 
Translation:  its  History,  Current  Status,  and 
Future  Prospects.  Computational  Linguistics, 
11(1).
Zhao,  Shanheng  and  Hwee  Tou  Ng.  2007. 
Identification and Resolution of Chinese Zero 
Pronouns:  A Machine  Learning  Approach.  In 
Proceedings  of  EMNLP  CoNLL  Joint  
Conference.
25
Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 17?22,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
Tradition and Modernity in 20th Century Chinese Poetry
Rob Voigt
Center for East Asian Studies
Stanford University
robvoigt@stanford.edu
Dan Jurafsky
Linguistics Department
Stanford University
jurafsky@stanford.edu
Abstract
Scholars of Chinese literature note that
China?s tumultuous literary history in the
20th century centered around the uncomfort-
able tensions between tradition and modernity.
In this corpus study, we develop and auto-
matically extract three features to show that
the classical character of Chinese poetry de-
creased across the century. We also find that
Taiwan poets constitute a surprising excep-
tion to the trend, demonstrating an unusually
strong connection to classical diction in their
work as late as the ?50s and ?60s.
1 Introduction
For virtually all of Chinese history through the fall
of the Qing Dynasty, poetry was largely written in
Classical Chinese and accessible to a small, edu-
cated fraction of the population. With the rise of the
May Fourth Movement in 1919, prominent intellec-
tuals such as Hu Shi and Lu Xun began to advocate
for and produce a fresh vernacular literature.
This upheaval of tradition has been much dis-
cussed in literary studies; Michelle Yeh calls ver-
nacular poetry ?a self-proclaimed iconoclast strug-
gling against a formidable predecessor: the heritage
of three millennia of classical poetry? (Yeh, 1991).
While some propose that the May Fourth intel-
lectuals ?abolished the classical language and all of
its literary genres? (Hockx and Smits, 2003), others
make more measured claims: Mao Chen, for exam-
ple, maintains that ?a special relationship to tradi-
tion informs all phases of cultural activity during the
May Fourth period? (Chen, 1997).
Julia Lin notes that the period following the May
Fourth Movement through 1937 saw ?the most ex-
citing and diverse experimentation in the history of
modern Chinese poetry? (Lin, 1973). Much of this
experimentation was concerned with the question
of modernity versus tradition, wherein some poets
?adapt[ed] the reality of the modern spoken lan-
guage to what they felt was the essence of the old
classical Chinese forms? (Haft, 1989).
The founding of the People?s Republic of China
in 1949 was a second major turning point in the
century, when ?the Communists in one cataclysmic
sweep [...] ruthlessly altered the course of the arts?
and poetry ?became totally subservient to the dic-
tates of the party? (Lin, 1973). With the ?physi-
cal removal of the old cultural leadership,? many of
whom fled to Taiwan, this period saw a substantial
?vacuum in literature and the arts? (McDougall and
Louie, 1997).
Post-Mao, publication restrictions gradually loos-
ened and earlier cultural journals re-entered circu-
lation. Poetry began to reclaim its audience, and a
Chinese avant-garde associated with the ?Misty Po-
ets? developed (McDougall and Louie, 1997).
However, we lack broad-scale empirical evidence
of the linguistic features that constituted the shift
from tradition to modernity. Therefore, we propose
a study that asks: To what extent were classical po-
etic forms and classical language immediately dis-
carded with the advent of vernacular poetry? What
is the status of classical language after 1949 and
amidst the Maoist era, when we might expect its to-
tal absence? Does more contemporary poetry still
draw connections to classical language?
17
2 Prior Work on Chinese Poetry in NLP
The majority of existing studies in NLP on Chinese
poetry deal exclusively with the classical language.
Jiang and Zhou (2008) explore the problem of
classical Chinese poetic couplets, and to develop a
system to generate them automatically using tech-
niques from machine translation.
Fang et al (2009) use an ontology of imagery de-
veloped by Lo (2008) to identify imagery in classical
Chinese poems, and develop a parser that is able to
extract tree structures that identify complex imagis-
tic language in the same.
More recent work develops useful resources for
understanding classical poetry. Lee (2012) develops
a corpus of classical Chinese poems that are word-
segmented and annotated with nested part-of-speech
tags that allow for different interpretations of ?word-
hood? - a non-trivial concept in considering Chinese
texts classical and modern. Lee and Kong (2012)
introduce a large-scale dependency treebank anno-
tated on a corpus of 8th-century poems.
To our knowledge, there is no existing computa-
tional work that attempts to understand the develop-
ment of modern Chinese poetry over time.
3 Data Collection
For this project, we use a corpus of modern po-
ems collected on the site ?Chinese Poetry Treasury?
(?????, www.shigeku.com) entitled the ?Se-
lected Database of Chinese Modern Poetry? (??
?????????). It is important to note that
the poems in this collection were hand-selected by
the group running the site for their canonicity, so our
data are biased towards those poems that have, in a
sense, ?stood the test of time? in the eyes of a main-
land Chinese readership.
This corpus is distributed through their site as a
collection of html documents, one page per poet,
which include brief biographical information for the
poet and a collection of their works. We use unix
command-line tools (sed, tr, iconv, grep) and basic
python scripting to process these documents into a
usable corpus with each poem as a separate, clean
file, segmented character-by-character. 1
1Scripts and further information are available here:
http://nlp.stanford.edu/robvoigt/chpoetry/
The site categorizes poets by their ?most active?
decade, from the 1920s through the 1990s, and we
extract this metadata to allow for comparisons over
time. In our analysis, however, a methodological im-
pediment arose: namely, the Cultural Revolution.
As discussed in the introduction, this tumultuous
period severely disrupted the developmental path of
modern Chinese literature. Indeed, we find in our
corpus that almost none of the poets tagged as active
in the ?50s and ?60s were mainland Chinese, but in-
stead Taiwanese poets who fled to the island at the
climax of the Chinese Civil War.
For this reason, combined with the potential nois-
iness induced by the fact that decade tags are per-
poet instead of per-poem, we manually identify Tai-
wan poets and divide our corpus into three subsets
for analysis: ?early modern? poetry in the 1920s and
?30s; ?late modern? poetry in the ?40s interrupted by
the Maoist era but resuming in the late ?70s, ?80s,
and ?90s; and ?Taiwan? poetry by Taiwan natives
and transplanted mainlanders in Taiwan post-1949.
After pre-processing, our full corpus for analysis
(denoted Eval in Table 1) contains 3,611 poems by
305 poets, with a total of 1,128,428 Chinese charac-
ters. This size is large enough for meaningful com-
putational results, but small enough to allow for sig-
nificant qualitative analysis.
We will later define metrics for evaluating the
?classicality? of individual characters and radicals,
so we process auxiliary corpora (denoted Aux in Ta-
ble 1) of classical poetry and contemporary prose.
For classical Chinese, we use a large corpus, from
the same source (www.shigeku.com), of poems from
the Tang Dynasty (618-907 AD), often considered
the greatest classical era for Chinese poetry. For
modern Chinese, we use a subset of a machine trans-
lation bi-text, comprised primarily of contemporary
newswire, legal, and other prose texts.2
Since we aim to discover the overall ?classicality?
of association for individual characters, our auxil-
iary corpora are cross-genre to exaggerate the ef-
fects ? a high ?classicality? score will indicate both
a period-specific classicality and a classical poetic
genre association.
2From the BOLT Phase 1 Evaluation training data; see
http://www.nist.gov/itl/iad/mig/bolt p1.cfm
18
Table 1: Corpus inventory.
Poems Chars Vocab
Eval Early 351 89,226 3,299
Taiwan 513 126,369 3,878
Late 2,747 912,833 4,852
Aux Classical 2,712,685 6,263
Modern 9,405,549 5,517
4 Methodology
Speak in the language of the time in which you live.
? Hu Shi, 1917
As suggested in the introduction, modern poetry
is distinguished linguistically from classical poetry
in its explicit shift to the use of vernacular language.
Classical poetry is formalized, concise, and imagis-
tic. We propose three features to operationalize this
classicality and computationally observe the shift to
a poetic vernacular across the 20th century.
Final Rhyme Classical Chinese poetry in general
has a highly regular structure, following strict metri-
cal and rhyming conventions, and most prominently
employs a highly consistent end-rhyme. We use the
CJKLIB python library3 to obtain the pronunciation
for the last character in each line of each poem. The
pronunciation of a given Chinese character may be
divided into precisely one consonant (known as an
?initial?) and one vowel (known as a ?final?).
We therefore qualify a given line as ?rhyming? if
the last character of any line within a 3-line window
shares its vowel final pronunciation, and for each
poem calculate the proportion of rhyming lines.
Character-based Probability Ratio Inspired by
the work of Underwood and Sellers (2012) in track-
ing shifts in literary diction in English poetry, we use
our auxiliary corpora of Tang Dynasty poems and
modern Chinese language text to create two simple
metrics for understanding the ?classicality? of poetic
diction.
The extreme concision of classical poetry ?fo-
cuses attention on the characters themselves? (Hin-
ton, 2010), with common classical forms containing
as few as ten or twenty characters. To analyze clas-
sical diction, for each character we aim to get a ratio
describing how classical it sounds.
3http://code.google.com/p/cjklib/
For this metric, we calculate the probability of
each character occurring in its respective corpus us-
ing add-one smoothing. We then define the score
for a given character as the difference of the char-
acter?s log likelihood of occurring in the classical
auxiliary corpus with its log likelihood of occur-
ring in the modern auxiliary corpus. Scores range
from -8 to +8, where a higher score indicates a more
?classically?-tinged character.
We find these scores match up well with intu-
ition. In the highly negative range, we find recently-
invented, conversational, and grammatical charac-
ters unique to the modern vernacular. In the highly
positive range, we find rareified literary, poetic
characters. In the range surrounding 0.0, we find
many common, persistent characters whose mean-
ings have changed little over time. Selected exam-
ples of these scores can be seen in Table 2.
Table 2: Example classicality scores for selected charac-
ters on the Character-based Probability Ratio metric.
Character Meaning Score
HIGHLY CLASSICAL
? yu To meet; to encounter 7.94
? qin A thin quilt used to cover 6.42
a corpse in a coffin
? xiao A type of bamboo flute 5.99
? liu Willow 4.68
SIMILAR ACROSS PERIODS
? ting Listen; hear 0.64
? qu? To go; towards 0.61
? zhi Directly -0.11
? shou To receive; to harvest -0.53
HIGHLY MODERN
? ni Second-person pronoun -4.49
? gou Sufficient; enough -6.02
? ne Sentence-final particle -6.67
? ta Third-person female pronoun -7.82
We calculate a score for a given poem on this met-
ric by simply taking the average of the character-
based probability ratio for each character in the
poem. These results are denoted Char in Table 4.
Radical-based Probability Ratio This metric is
fundamentally similar to the above character-based
method, but offers the potential to provide a different
kind of insight. The majority of Chinese characters
are compositional, with a semantic component and a
phonetic component.
19
We start from the intuition that contemporary
texts will be more likely to use characters that con-
tain the ? (kou, ?mouth?) radical as their seman-
tic component, because this radical is commonly
found in modern conversational particles that were
not used in ancient texts. We generalize this hypoth-
esis and consider that the use of characters with cer-
tain semantic radicals is correlated with the classi-
cality of a text.
We again use the CJKLIB python library to pro-
cess our auxiliary corpora, extracting the seman-
tic component radical from each character and cal-
culating the ratio of its probability of occurrence,
with add-one smoothing, in the auxiliary classical
and modern corpora. As above, we obtain the ratio
scores for each radical, and score each poem in our
corpus by averaging these scores for each character
in the poem.
While these scores are less immediately acces-
sible to intuition than those of the character-based
metric, the radical-based scores, with examples seen
in Table 3, demonstrate a consistency that parallels
the character-based scores.
The semantic radicals most prevalent in classical
poetry include those signifying bird, horse, valley,
mountain, ghost, dragon, and so on; classical po-
etry has a pastoral and mythological aesthetic that
is directly reflected in the distribution of its radi-
cals. Conversely, modern prose is more likely to use
semantic radicals related to work, family, money,
speech, and movement; they convey the practical re-
alism of contemporary conversational speech.
Table 3: Example classicality scores for selected seman-
tic radicals on the Radical-based Probability Ratio metric.
Radical Meaning Score
HIGHLY CLASSICAL
? gui Ghost 2.18
? shan Mountain 2.09
? chong Insect 1.43
SIMILAR ACROSS PERIODS
? nu? Female 0.01
? wen Culture; language -0.02
? sheng Life; birth -0.01
HIGHLY MODERN
? shou Hand -0.48
? yan Words; speech -0.61
? li Force; work -0.94
4.1 Diachronic Statistical Analysis
We began from the hypothesis that each of the met-
rics described above will demonstrate, broadly, that
the classical nature of Chinese poetry decreased over
the course of the 20th century. The raw statistical
counts for our features can been seen in Table 4.
Table 4: Raw feature statistics across sub-corpora.
Higher values in the AVG rows indicate a greater ?classi-
cality.? For all three features, classicality decreased over
the century, with the exception of Taiwan.
Early Taiwan Late
Rhyme AVG 0.281 0.244 0.226
STDDEV 0.193 0.169 0.152
Char AVG -0.695 -0.620 -0.882
STDDEV 0.494 0.446 0.404
Radical AVG -0.072 -0.081 -0.116
STDDEV 0.121 0.105 0.097
We calculate the presence of the ?classical? fea-
tures defined above for each subset, and compute
a binary logistic regression with the scikit-learn
python library (Pedregosa et al, 2011)4 to find cor-
relation coefficients for those features between the
?early modern? and ?late modern? subsets.
5 Results and Discussion
Several claims from the literary community are well-
supported by our results.
Logistic regression reveals a significant down-
ward trend for our features as we shift from ?early
modern? to ?late modern? poetry (R2 = 0.89), in-
dicating decreased use of end-rhyme, increased use
of modern characters, and increased prevalence of
modern semantic radicals over the course of the cen-
tury.
Though the early works use more classical char-
acters on the whole, we also observe a higher statisti-
cal variance for all metrics in the ?20s and ?30s, sup-
porting the literary hypothesis that the May Fourth
period was one of increased experimentation that
later settled into a somewhat more consistent moder-
nity.
We find, however, less support for the idea that
Chinese modern poets ?abolished the classical lan-
guage? in their work (Hockx and Smits, 2003).
4http://scikit-learn.org
20
Throughout the century we find repeated instances
of highly classical language, with individual poems
reaching a maximum character-based probability ra-
tio of 0.70 in the ?early? works, 0.76 in the ?late?
works, and 0.87 in the ?Taiwan? works; compare
these with an average score of 1.20 for the auxiliary
classical dataset overall. Considering that a score of
0.0 would indicate an equal distribution of weight
between ?classical? and ?modern? characters, it?s
clear that these 20th-century poems still contain a
substantial proportion of characters drawn from the
classical language.
Poems from Taiwan in the ?50s and ?60s offer
perhaps the most interesting results in this study.
It?s notable in the first place that poets in our cor-
pus selected as worth remembering by contempo-
rary mainland Chinese from the most authoritarian
period of Communist control are almost exclusively
from Taiwanese authors. Furthermore, the dip to-
wards modernity we see in ?40s mainland poetry was
rejected in the next decade by those mainland poets
who found themselves in Taiwan after 1949; the Tai-
wan poems bear far greater resemblance to the early
subset of our data than to the late.
This finding parallels work on this period from lit-
erary scholars. Yvonne Chang writes that in ?50s
and ?60s Taiwan, valorization of traditional Chi-
nese culture and romanticization of the early 20th-
century Nationalist period in mainland China was
heavily encouraged. In particular, the concept of ??
??? (chun wenxue, ?pure literature?) gained pop-
ularity in Taiwan?s literary circles, and with it came
a resurgence of more traditional diction and forms
(Chang, 1993).
Fangming Chen further describes poetry in post-
war Taiwan as a political outlet for the Kuomintang,
the sole ruling party of Taiwan at the time, as they
?forcefully brought Chinese nationalism? to the is-
land. Poets who demonstrated a deep ?nostalgia? for
the ?motherland? of mainland China were far more
likely to be rewarded with cultural resources such as
grants and publication money, being that the govern-
ment had a vested interest in keeping the public on
board with plans to ?reclaim the homeland? (Chen,
2007). It is fascinating, then, that we observe this
tendency computationally with a return to the levels
of classicality seen in ?20s and ?30s mainland China.
In spite of these encouraging results, this work has
several limitations. Our reliance on decade-based la-
bels applied to poets, rather than poems, introduces
significant noise. The outlier behavior observed in
Taiwan poets is indicative of the need for a better
understanding of regional differences, and a com-
parison with a similarly isolated Sinophone region
such as Hong Kong would be productive in this re-
gard. In both cases, information extraction tech-
niques might allow us to tag poems with their date of
publication and poets with their hometown, facilitat-
ing fine-grained analysis, as would a broader dataset
that goes beyond the modern canon.
6 Conclusion
In this paper, we computationally operationalized
three features that successfully track the declining
influence of classical poetic style and language in
20th-century Chinese poetry. We identified Taiwan
poets as an outlier in the dataset, and found empiri-
cal evidence for the political externalities of the ?50s
and ?60s that called for a return to a nostalgic clas-
sicism. In this way, this work presents a promising
first step to a thorough empirical understanding of
the development of modern Chinese poetry.
Acknowledgments
Thanks to three anonymous reviewers for detailed
and insightful comments. This research was sup-
ported in part by the Foreign Language and Area
Studies Fellowships, United States Department of
Education.
References
Sung-sheng Yvonne Chang. 1993. Modernism and the
Nativist Resistance. Duke University Press: Durham
and London.
Fangming Chen. 2007. Postmodern or Postcolonial? An
Inquiry into Postwar Taiwanese Literary History. In
Writing Taiwan, David Der-wei Wang and Carlos Ro-
jas, eds. Duke University Press, Durham and London.
Mao Chen. 1997. Between Tradition and Change. Uni-
versity Press of America, Lanham, MA.
Alex Chengyu Fang, Fengju Lo, and Cheuk Kit Chinn.
2009. Adapting NLP and Corpus Analysis Techniques
to Structured Imagery Analysis in Classical Chinese
Poetry. In Workshop Adaptation of Language Re-
sources and Technology to New Domains, Borovets,
Bulgaria.
21
Lloyd Haft. 1989. A Selective Guide to Chinese Litera-
ture: 1900-1949. E.J. Brill, New York.
David Hinton, ed. 2010. Classical Chinese Poetry: An
Anthology. Farrar, Straus, and Giroux.
Michel Hockx and Ivo Smits, eds. 2003. Reading East
Asian Writing: The Limits of Literary Theory. Rout-
ledgeCurzon, London and New York.
Long Jiang and Ming Zhou. 2008. Generating Chinese
Couplets using a Statistical MT Approach. In COL-
ING.
John Lee. 2012. A Classical Chinese Corpus with
Nested Part-of-Speech Tags. In Proceedings of the 6th
EACLWorkshop on Language Technology for Cultural
Heritage, Social Sciences, and Humanities, Avignon,
France.
John Lee and Yin Hei Kong. 2012. A Dependency Tree-
bank of Classical Chinese Poems. In NAACL-HLT,
Montreal, Canada.
Julia Lin. 1973. Modern Chinese Poetry: An Introduc-
tion. University of Washington Press, Seattle, WA.
Fengju Lo. 2008. The Research of Building a Semantic
Cetegory System Based on the Language Characteris-
tic of Chinese Poetry. In Proceedings of the 9th Cross-
Strait Symposium on Library Information Science.
Lu Zhiwei. 1984. Five Lectures on Chinese Poetry. Joint
Publishing Co., Hong Kong.
Bonnie McDougall and Kam Louie, eds. 1997. The Lit-
erature of China in the Twentieth Century. Hurst and
Company, London.
Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, Jake Vanderplas, Alexandre Passos,
David Cournapeau, Matthieu Brucher, Matthieu Per-
rot, and E?douard Duchesnay. 2011. Scikit-learn: Ma-
chine Learning in Python. Journal of Machine Learn-
ing Research. 12:2825-2830
Ted Underwood and Jordan Sellers. 2012.
The Emergence of Literary Diction. The
Journal of Digital Humanities, 1(2).
http://journalofdigitalhumanities.org/1-2/the-
emergence-of-literary-diction-by-ted-underwood-
and-jordan-sellers/
Michelle Yeh. 1991. Modern Chinese Poetry: Theory
and Practice since 1917. Yale University Press, New
Haven, CT.
22
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 148?153,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Feature-Rich Phrase-based Translation: Stanford University?s Submissionto the WMT 2013 Translation Task
Spence Green, Daniel Cer, Kevin Reschke, Rob Voigt*, John Bauer
Sida Wang, Natalia Silveira?, Julia Neidert and Christopher D. Manning
Computer Science Department, Stanford University
*Center for East Asian Studies, Stanford University
?Department of Linguistics, Stanford University
{spenceg,cerd,kreschke,robvoigt,horatio,sidaw,natalias,jneid,manning}@stanford.edu
Abstract
We describe the Stanford University NLP
Group submission to the 2013 Workshop
on Statistical Machine Translation Shared
Task. We demonstrate the effectiveness of a
new adaptive, online tuning algorithm that
scales to large feature and tuning sets. For
both English-French and English-German,
the algorithm produces feature-rich mod-
els that improve over a dense baseline and
compare favorably to models tuned with
established methods.
1 Introduction
Green et al (2013b) describe an online, adaptive
tuning algorithm for feature-rich translation mod-
els. They showed considerable translation quality
improvements over MERT (Och, 2003) and PRO
(Hopkins and May, 2011) for two languages in a
research setting. The purpose of our submission to
the 2013 Workshop on Statistical Machine Trans-
lation (WMT) Shared Task is to compare the algo-
rithm to more established methods in an evaluation.
We submitted English-French (En-Fr) and English-
German (En-De) systems, each with over 100k fea-
tures tuned on 10k sentences. This paper describes
the systems and also includes new feature sets and
practical extensions to the original algorithm.
2 Translation Model
Our machine translation (MT) system is Phrasal
(Cer et al, 2010), a phrase-based system based on
alignment templates (Och and Ney, 2004). Like
many MT systems, Phrasal models the predictive
translation distribution p(e|f ;w) directly as
p(e|f ;w) = 1Z(f) exp
[
w>?(e, f)
]
(1)
where e is the target sequence, f is the source se-
quence, w is the vector of model parameters, ?(?)
is a feature map, and Z(f) is an appropriate nor-
malizing constant. For many years the dimension
of the feature map ?(?) has been limited by MERT,
which does not scale past tens of features.
Our submission explores real-world translation
quality for high-dimensional feature maps and as-
sociated weight vectors. That case requires a more
scalable tuning algorithm.
2.1 Online, Adaptive Tuning Algorithm
FollowingHopkins andMay (2011) we castMT tun-
ing as pairwise ranking. Consider a single source
sentence f with associated references e1:k. Let d
be a derivation in an n-best list of f that has the
target e = e(d) and the feature map ?(d). Define
the linear model scoreM(d) = w ? ?(d). For any
derivation d+ that is better than d? under a gold
metric G, we desire pairwise agreement such that
G
(
e(d+), e1:k
)
> G
(
e(d?), e1:k
)
?? M(d+) > M(d?)
Ensuring pairwise agreement is the same as ensur-
ing w ? [?(d+)? ?(d?)] > 0.
For learning, we need to select derivation pairs
(d+, d?) to compute difference vectors x+ =
?(d+) ? ?(d?). Then we have a 1-class separa-
tion problem trying to ensure w ? x+ > 0. The
derivation pairs are sampled with the algorithm of
Hopkins and May (2011). Suppose that we sample
s pairs for source sentence ft to compute a set of
difference vectors Dt = {x1:s+ }. Then we optimize
`t(w) = `(Dt, w) = ?
?
x+?Dt
log 11 + e?w?x+
(2)
which is the familiar logistic loss. Hopkins and
May (2011) optimize (2) in a batch algorithm
that alternates between candidate generation (i.e.,
n-best list or lattice decoding) and optimization
(e.g., L-BFGS). We instead use AdaGrad (Duchi
148
et al, 2011), a variant of stochastic gradient de-
scent (SGD) in which the learning rate is adapted
to the data. Informally, AdaGrad scales the weight
updates according to the geometry of the data ob-
served in earlier iterations. Consider a particu-
lar dimension j of w, and let scalars vt = wt,j ,
gt = ?j`t(wt?1), and Gt = ?ti=1 g2i . The Ada-Grad update rule is
vt = vt?1 ? ? G?1/2t gt (3)
Gt = Gt?1 + g2t (4)
In practice,Gt is a diagonal approximation. IfGt =
I , observe that (3) is vanilla SGD.
In MT systems, the feature map may generate
exponentially many irrelevant features, so we need
to regularize (3). The L1 norm of the weight vec-
tor is known to be an effective regularizer in such
a setting (Ng, 2004). An efficient way to apply
L1 regularization is the Forward-Backward split-
ting (FOBOS) framework (Duchi and Singer, 2009),
which has the following two-step update:
wt? 12 = wt?1 ? ?t?1?`t?1(wt?1) (5)
wt = argmin
w
1
2?w ? wt? 12 ?
2
2 + ?t?1r(w)
(6)
where (5) is just an unregularized gradient descent
step and (6) balances the regularization term r(w)
with staying close to the gradient step.
For L1 regularization we have r(w) = ?||w||1
and the closed-form solution to (6) is
wt = sign(wt? 12 )
[
|wt? 12 | ? ?t?1?
]
+
(7)
where [x]+ = max(x, 0) is the clipping function
that in this case sets a weight to 0 when it falls below
the threshold ?t?1?.
Online algorithms are inherently sequential; this
algorithm is no exception. If we want to scale the
algorithm to large tuning sets, then we need to par-
allelize the weight updates. Green et al (2013b)
describe the parallelization technique that is imple-
mented in Phrasal.
2.2 Extensions to (Green et al, 2013b)
Sentence-Level Metric We previously used the
gold metric BLEU+1 (Lin and Och, 2004), which
smoothes bigram precisions and above. This metric
worked well with multiple references, but we found
that it is less effective in a single-reference setting
like WMT. To make the metric more robust, Nakov
et al (2012) extended BLEU+1 by smoothing both
the unigram precision and the reference length. We
found that this extension yielded a consistent +0.2
BLEU improvement at test time for both languages.
Subsequent experiments on the data sets of Green
et al (2013b) showed that standard BLEU+1 works
best for multiple references.
Custom regularization parameters Green et al
(2013b) showed that large feature-rich models over-
fit the tuning sets. We discovered that certain fea-
tures caused greater overfitting than others. Custom
regularization strengths for each feature set are one
solution to this problem. We found that technique
largely fixed the overfitting problem as shown by
the learning curves presented in section 5.1.
Convergence criteria Standard MERT imple-
mentations approximate tuning BLEU by re-
ranking the previous n-best lists with the updated
weight vector. This approximation becomes infeasi-
ble for large tuning sets, and is less accurate for algo-
rithms like ours that do not accumulate n-best lists.
We approximate tuning BLEU by maintaining the
1-best hypothesis for each tuning segment. At the
end of each epoch, we compute corpus-level BLEU
from this hypothesis set. We flush the set of stored
hypotheses before the next epoch begins. Although
memory-efficient, we find that this approximation
is less dependable as a convergence criterion than
the conventional method. Whereas we previously
stopped the algorithm after four iterations, we now
select the model according to held-out accuracy.
3 Feature Sets
3.1 Dense Features
The baseline ?dense? model has 19 features: the
nine Moses (Koehn et al, 2007) baseline features, a
hierarchical lexicalized re-ordering model (Galley
and Manning, 2008), the (log) bitext count of each
translation rule, and an indicator for unique rules.
The final dense feature sets for each language
differ slightly. The En-Fr system incorporates a
second language model. The En-De system adds a
future cost component to the linear distortion model
(Green et al, 2010).The future cost estimate allows
the distortion limit to be raised without a decrease
in translation quality.
149
3.2 Sparse Features
Sparse features do not necessarily fire on each hy-
pothesis extension. Unlike prior work on sparseMT
features, our feature extractors do not filter features
based on tuning set counts. We instead rely on the
regularizer to select informative features.
Several of the feature extractors depend on
source-side part of speech (POS) sequences and
dependency parses. We created those annotations
with the Stanford CoreNLP pipeline.
Discriminative Phrase Table A lexicalized in-
dicator feature for each rule in a derivation. The
feature weights can be interpreted as adjustments
to the associated dense phrase table features.
Discriminative Alignments A lexicalized indi-
cator feature for the phrase-internal alignments in
each rule in a derivation. For one-to-many, many-to-
one, and many-to-many alignments we extract the
clique of aligned tokens, perform a lexical sort, and
concatenate the tokens to form the feature string.
Discriminative Re-ordering A lexicalized indi-
cator feature for each rule in a derivation that ap-
pears in the following orientations: monotone-with-
next, monotone-with-previous, non-monotone-
with-next, non-monotone-with-previous. Green
et al (2013b) included the richer non-monotone
classes swap and discontinuous. However, we found
that these classes yielded no significant improve-
ment over the simpler non-monotone classes. The
feature weights can be interpreted as adjustments
to the generative lexicalized re-ordering model.
Source Content-Word Deletion Count-based
features for source content words that are ?deleted?
in the target. Content words are nouns, adjectives,
verbs, and adverbs. A deleted source word is ei-
ther unaligned or aligned to one of the 100 most
frequent target words in the target bitext. For each
deleted word we increment both the feature for the
particular source POS and an aggregate feature for
all parts of speech. We add similar but separate
features for head content words that are either un-
aligned or aligned to frequent target words.
Inverse Document Frequency Numeric fea-
tures that compare source and target word frequen-
cies. Let idf(?) return the inverse document fre-
quency of a token in the training bitext. Suppose
a derivation d = {r1, r2, . . . , rn} is composed of
n translation rules, where e(r) is the target side of
the rule and f(r) is the source side. For each rule
Bilingual Monolingual
Sentences Tokens Tokens
En-Fr 5.0M 289M 1.51B
En-De 4.4M 223M 1.03B
Table 1: Gross corpus statistics after data selection
and pre-processing. The En-Fr monolingual counts
include French Gigaword 3 (LDC2011T10).
r that translates j source tokens to i target tokens
we compute
q =
?
i
idf(e(r)i)?
?
j
idf(f(r)j) (8)
We add two numeric features, one for the source and
another for the target. When q > 0 we increment
the target feature by q; when q < 0 we increment
the target feature by |q|. Together these features
penalize asymmetric rules that map rare words to
frequent words and vice versa.
POS-based Re-ordering The lexicalized dis-
criminative re-ordering model is very sparse, so we
added re-ordering features based on source parts of
speech. When a rule is applied in a derivation, we
extract the associated source POS sequence along
with the POS sequences from the previous and next
rules. We add a ?with-previous? indicator feature
that is the conjunction of the current and previous
POS sequences; the ?with-next? indicator feature is
created analogously. This feature worked well for
En-Fr, but not for En-De.
4 Data Preparation
Table 1 describes the pre-processed corpora from
which our systems are built.
4.1 Data Selection
We used all of the monolingual and parallel En-
De data allowed in the constrained condition. We
incorporated all of the French monolingual data,
but sampled a 5M-sentence bitext from the approx-
imately 40M available En-Fr parallel sentences.
To select the sentences we first created a ?target?
corpus by concatenating the tuning and test sets
(newstest2008?2013). Then we ran the feature
decay algorithm (FDA) (Bi?ici and Yuret, 2011),
which samples sentences that most closely resem-
ble the target corpus. FDA is a principled method
for reducing the phrase table size by excluding less
relevant training examples.
150
4.2 Tokenization
We tokenized the English (source) data according
to the Penn Treebank standard (Marcus et al, 1993)
with Stanford CoreNLP. The French data was to-
kenized with packages from the Stanford French
Parser (Green et al, 2013a), which implements a
scheme similar to that used in the French Treebank
(Abeill? et al, 2003).
German is more complicated due to pervasive
compounding. We first tokenized the data with the
same English tokenizer. Then we split compounds
with the lattice-based model (Dyer, 2009) in cdec
(Dyer et al, 2010). To simplify post-processing we
added segmentation markers to split tokens, e.g.,
?berschritt? ?ber #schritt.
4.3 Alignment
We aligned both bitexts with the Berkeley Aligner
(Liang et al, 2006) configured with standard set-
tings. We symmetrized the alignments according
to the grow-diag heuristic.
4.4 Language Modeling
We estimated unfiltered 5-gram language models
using lmplz (Heafield et al, 2013) and loaded them
with KenLM (Heafield, 2011). For memory effi-
ciency and faster loading we also used KenLM to
convert the LMs to a trie-based, binary format. The
German LM included all of the monolingual data
plus the target side of the En-De bitext. We built
an analogous model for French. In addition, we
estimated a separate French LM from the Gigaword
data.1
4.5 French Agreement Correction
In French verbs must agree in number and person
with their subjects, and adjectives (and some past
participles) must agree in number and gender with
the nouns they modify. On their own, phrasal align-
ment and target side language modeling yield cor-
rect agreement inflection most of the time. For
verbs, we find that the inflections are often accurate:
number is encoded in the English verb and subject,
and 3rd person is generally correct in the absence
of a 1st or 2nd person pronoun. However, since En-
glish does not generally encode gender, adjective
inflection must rely on language modeling, which
is often insufficient.
1The MT system learns significantly different weights for
the two LMs: 0.086 for the primary LM and 0.044 for the
Gigaword LM.
To address this problem we apply an automatic
inflection correction post-processing step. First, we
generate dependency parses of our system?s out-
put using BONSAI (Candito and Crabb?, 2009),
a French-specific extension to the Berkeley Parser
(Petrov et al, 2006). Based on these dependencies,
we match adjectives with the nouns they modify
and past participles with their subjects. Then we
use Lefff (Sagot, 2010), a machine-readable French
lexicon, to determine the gender and number of the
noun and to choose the correct inflection for the
adjective or participle.
Applied to our 3,000 sentence development set,
this correction scheme produced 200 corrections
with perfect accuracy. It produces a slight (?0.014)
drop in BLEU score. This arises from cases where
the reference translation uses a synonymous but
differently gendered noun, and consequently has
different adjective inflection.
4.6 German De-compounding
Split German compounds must be merged after
translation. This process often requires inserting
affixes (e.g., s, en) between adjacent tokens in the
compound. Since the German compounding rules
are complex and exception-laden, we rely on a dic-
tionary lookup procedure with backoffs. The dic-
tionary was constructed during pre-processing. To
compound the final translations, we first lookup
the compound sequence?which is indicated by
segmentation markers?in the dictionary. If it is
present, then we use the dictionary entry. If the com-
pound is novel, then for each pair of words to be
compounded, we insert the suffix most commonly
appended in compounds to the first word of the pair.
If the first word itself is unknown in our dictionary,
we insert the suffix most commonly appended after
the last three characters. For example, words end-
ing with ung most commonly have an s appended
when they are used in compounds.
4.7 Recasing
Phrasal includes an LM-based recaser (Lita et al,
2003), which we trained on the target side of the
bitext for each language. On the newstest2012 de-
velopment data, the German recaser was 96.8% ac-
curate and the French recaser was 97.9% accurate.
5 Translation Quality Experiments
During system development we tuned on
newstest2008?2011 (10,570 sentences) and tested
151
#iterations #features tune newstest2012 newstest2013?
Dense 10 20 30.26 31.12 ?
Feature-rich 11 207k 32.29 31.51 29.00
Table 2: En-Fr BLEU-4 [% uncased] results. The tuning set is newstest2008?2011. (?) newstest2013 is
the cased score computed by the WMT organizers.
#iterations #features tune newstest2012 newstest2013?
Dense 10 19 16.83 18.45 ?
Feature-rich 13 167k 17.66 18.70 18.50
Table 3: En-De BLEU-4 [% uncased] results.
on newstest2012 (3,003 sentences). We compare
the feature-rich model to the ?dense? baseline.
The En-De system parameters were: 200-best
lists, a maximum phrase length of 8, and a distortion
limit of 6 with future cost estimation. The En-Fr
system parameters were: 200-best lists, a maximum
phrase length of 8, and a distortion limit of 5.
The online tuning algorithm used a default learn-
ing rate ? = 0.03 and a mini-batch size of 20. We
set the regularization strength ? to 10.0 for the dis-
criminative re-ordering model, 0.0 for the dense
features, and 0.1 otherwise.
5.1 Results
Tables 2 and 3 show En-Fr and En-De results, re-
spectively. The ?Feature-rich? model, which con-
tains the full complement of dense and sparse fea-
tures, offers ameager improvement over the ?Dense?
baseline. This result contrasts with the results
of Green et al (2013b), who showed significant
translation quality improvements over the same
dense baseline for Arabic-English and Chinese-
English. However, they had multiple target refer-
ences, whereas the WMT data sets have just one.
We speculate that this difference is significant. For
example, consider a translation rule that rewrites
to a 4-gram in the reference. This event can in-
crease the sentence-level score, thus encouraging
the model to upweight the rule indicator feature.
More evidence of overfitting can be seen in Fig-
ure 1, which shows learning curves on the devel-
opment set for both language pairs. Whereas the
dense model converges after just a few iterations,
the feature-rich model continues to creep higher.
Separate experiments on a held-out set showed that
generalization did not improve after about eight
iterations.
6 Conclusion
We submitted a feature-rich MT system to WMT
2013. While sparse features did offer a measur-
able improvement over a baseline dense feature set,
the gains were not as significant as those shown
by Green et al (2013b). One important difference
between the two sets of results is the number of ref-
erences. Their NIST tuning and test sets had four
references; the WMT data sets have just one. We
speculate that sparse features tend to overfit more
in this setting. Individual features can greatly in-
fluence the sentence-level metric and thus become
large components of the gradient. To combat this
phenomenon we experimented with custom reg-
ularization strengths and a more robust sentence-
level metric. While these two improvements greatly
reduced the model size relative to (Green et al,
2013b), a generalization problem remained. Nev-
ertheless, we showed that feature-rich models are
now competitive with the state-of-the-art.
Acknowledgments This work was supported by the Defense
Advanced Research Projects Agency (DARPA) Broad Opera-
tional Language Translation (BOLT) program through IBM.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not
necessarily reflect the view of DARPA or the US government.
References
A. Abeill?, L. Cl?ment, and A. Kinyon, 2003. Building
a treebank for French, chapter 10. Kluwer.
E. Bi?ici and D. Yuret. 2011. Instance selection for
machine translation using feature decay algorithms.
In WMT.
M. Candito and B. Crabb?. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In IWPT.
152
ll l l l l
l l l l
l
l
l l
l l
l l
l l
29
30
31
32
1 2 3 4 5 6 7 8 9 10Epoch
BLEU
 new
test2
008?
2011
Model
l
l
densefeature?rich
(a) En-Fr tuning
l
l
l l l l l l
l l
l
l
l l l
l l
l l l
7.5
10.0
12.5
15.0
17.5
1 2 3 4 5 6 7 8 9 10Epoch
BLEU
 new
test2
008?
2011
Model
l
l
densefeature?rich
(b) En-De tuning
Figure 1: BLEU-4 [% uncased] Learning curves on newstest2008?2011 with loess trend lines.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In HLT-
NAACL, Demonstration Session.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899?2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
et al 2010. cdec: A decoder, alignment, and learn-
ing framework for finite-state and context-free trans-
lation models. In ACL System Demonstrations.
C. Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In NAACL.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
S. Green, M. Galley, and C. D. Manning. 2010. Im-
proved models of distortion cost for statistical ma-
chine translation. In HLT-NAACL.
S. Green, M-C. de Marneffe, and C. D. Manning.
2013a. Parsing models for identifying multiword
expressions. Computational Linguistics, 39(1):195?
227.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013b.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
K. Heafield. 2011. KenLM: Faster and smaller lan-
guage model queries. In WMT.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.
2003. tRuEcasIng. In ACL.
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19:313?330.
P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing
for sentence-level BLEU+1 yields short translations.
In COLING.
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regular-ization, and rotational invariance. In ICML.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In ACL.
B. Sagot. 2010. The Lefff, a freely available and
large-coverage morphological and syntactic lexicon
for French. In LREC.
153
