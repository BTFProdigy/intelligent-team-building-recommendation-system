Statistical Sentence Condensation using Ambiguity Packing and Stochastic
Disambiguation Methods for Lexical-Functional Grammar
Stefan Riezler and Tracy H. King and Richard Crouch and Annie Zaenen
Palo Alto Research Center, 3333 Coyote Hill Rd., Palo Alto, CA 94304
{riezler|thking|crouch|zaenen}@parc.com
Abstract
We present an application of ambiguity pack-
ing and stochastic disambiguation techniques
for Lexical-Functional Grammars (LFG) to the
domain of sentence condensation. Our system
incorporates a linguistic parser/generator for
LFG, a transfer component for parse reduc-
tion operating on packed parse forests, and a
maximum-entropy model for stochastic output
selection. Furthermore, we propose the use of
standard parser evaluation methods for auto-
matically evaluating the summarization qual-
ity of sentence condensation systems. An ex-
perimental evaluation of summarization qual-
ity shows a close correlation between the au-
tomatic parse-based evaluation and a manual
evaluation of generated strings. Overall sum-
marization quality of the proposed system is
state-of-the-art, with guaranteed grammatical-
ity of the system output due to the use of a
constraint-based parser/generator.
1 Introduction
Recent work in statistical text summarization has put for-
ward systems that do not merely extract and concate-
nate sentences, but learn how to generate new sentences
from ?Summary, Text? tuples. Depending on the cho-
sen task, such systems either generate single-sentence
?headlines? for multi-sentence text (Witbrock and Mittal,
1999), or they provide a sentence condensation module
designed for combination with sentence extraction sys-
tems (Knight and Marcu, 2000; Jing, 2000). The chal-
lenge for such systems is to guarantee the grammatical-
ity and summarization quality of the system output, i.e.
the generated sentences need to be syntactically well-
formed and need to retain the most salient information of
the original document. For example a sentence extraction
system might choose a sentence like:
The UNIX operating system, with implementations
from Apples to Crays, appears to have the advan-
tage.
from a document, which could be condensed as:
UNIX appears to have the advantage.
In the approach of Witbrock and Mittal (1999), selec-
tion and ordering of summary terms is based on bag-
of-words models and n-grams. Such models may well
produce summaries that are indicative of the original?s
content; however, n-gram models seem to be insufficient
to guarantee grammatical well-formedness of the system
output. To overcome this problem, linguistic parsing and
generation systems are used in the sentence condensation
approaches of Knight and Marcu (2000) and Jing (2000).
In these approaches, decisions about which material to in-
clude/delete in the sentence summaries do not rely on rel-
ative frequency information on words, but rather on prob-
ability models of subtree deletions that are learned from
a corpus of parses for sentences and their summaries.
A related area where linguistic parsing systems
have been applied successfully is sentence simplifica-
tion. Grefenstette (1998) presented a sentence reduction
method that is based on finite-state technology for lin-
guistic markup and selection, and Carroll et al (1998)
present a sentence simplification system based on linguis-
tic parsing. However, these approaches do not employ
statistical learning techniques to disambiguate simplifi-
cation decisions, but iteratively apply symbolic reduction
rules, producing a single output for each sentence.
The goal of our approach is to apply the fine-grained
tools for stochastic Lexical-Functional Grammar (LFG)
parsing to the task of sentence condensation. The system
presented in this paper is conceptualized as a tool that can
be used as a standalone system for sentence condensation
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 118-125
                                                         Proceedings of HLT-NAACL 2003
or simplification, or in combination with sentence extrac-
tion for text-summarization beyond the sentence-level. In
our system, to produce a condensed version of a sen-
tence, the sentence is first parsed using a broad-coverage
LFG grammar for English. The parser produces a set of
functional (f )-structures for an ambiguous sentence in a
packed format. It presents these to the transfer compo-
nent in a single packed data structure that represents in
one place the substructures shared by several different in-
terpretations. The transfer component operates on these
packed representations and modifies the parser output to
produce reduced f -structures. The reduced f -structures
are then filtered by the generator to determine syntac-
tic well-formedness. A stochastic disambiguator using a
maximum entropy model is trained on parsed and manu-
ally disambiguated f -structures for pairs of sentences and
their condensations. Using the disambiguator, the string
generated from the most probable reduced f -structure
produced by the transfer system is chosen. In contrast
to the approaches mentioned above, our system guaran-
tees the grammaticality of generated strings through the
use of a constraint-based generator for LFG which uses
a slightly tighter version of the grammar than is used by
the parser. As shown in an experimental evaluation, sum-
marization quality of our system is high, due to the com-
bination of linguistically fine-grained analysis tools and
expressive stochastic disambiguation models.
A second goal of our approach is to apply the standard
evaluation methods for parsing to an automatic evaluation
of summarization quality for sentence condensation sys-
tems. Instead of deploying costly and non-reusable hu-
man evaluation, or using automatic evaluation methods
based on word error rate or n-gram match, summariza-
tion quality can be evaluated directly and automatically
by matching the reduced f -structures that were produced
by the system against manually selected f -structures that
were produced by parsing a set of manually created con-
densations. Such an evaluation only requires human labor
for the construction and manual structural disambigua-
tion of a reusable gold standard test set. Matching against
the test set can be done automatically and rapidly, and
is repeatable for development purposes and system com-
parison. As shown in an experimental evaluation, a close
correspondence can be established for rankings produced
by the f -structure based automatic evaluation and a man-
ual evaluation of generated strings.
2 Statistical Sentence Condensation in the
LFG Framework
In this section, each of the system components will be
described in more detail.
2.1 Parsing and Transfer
In this project, a broad-coverage LFG gram-
mar and parser for English was employed (see
Riezler et al (2002)). The parser produces a set of
context-free constituent (c-)structures and associated
functional (f -)structures for each input sentence, repre-
sented in packed form (see Maxwell and Kaplan (1989)).
For sentence condensation we are only interested in the
predicate-argument structures encoded in f -structures.
For example, Fig. 1 shows an f -structure manually
selected out of the 40 f -structures for the sentence:
A prototype is ready for testing, and Leary hopes to
set requirements for a full system by the end of the
year.
The transfer component for the sentence condensation
system is based on a component previously used in a ma-
chine translation system (see Frank (1999)). It consists
of an ordered set of rules that rewrite one f -structure
into another. Structures are broken down into flat lists
of facts, and rules may add, delete, or change individ-
ual facts. Rules may be optional or obligatory. In the case
of optional rules, transfer of a single input structure may
lead to multiple alternate output structures. The transfer
component is designed to operate on packed input from
the parser and can also produce packed representations
of the condensation alternatives, using methods adapted
from parse packing.1
An example rule that (optionally) removes an adjunct
is shown below:
+adjunct(X,Y), in-set(Z,Y) ?=>
delete-node(Z,r1), rule-trace(r1,del(Z,X)).
This rule eliminates an adjunct, Z, by deleting the fact that
Z is contained within the set of adjuncts, Y, associated
with the expression X. The + before the adjunct(X,Y)
fact marks this fact as one that needs to be present for the
rule to be applied, but which is left unaltered by the rule
application. The in-set(Z,Y) fact is deleted. Two
new facts are added. delete-node(Z,r1) indicates
that the structure rooted at node Z is to be deleted, and
rule-trace(r1,del(Z,X)) adds a trace of this
rule to an accumulating history of rule applications. This
history records the relation of transferred f -structures to
the original f -structure and is available for stochastic dis-
ambiguation.
Rules used in the sentence condensation transfer sys-
tem include the optional deletion of all intersective ad-
juncts (e.g., He slept in the bed. can become He slept.,
but He did not sleep. cannot become He did sleep. or He
1The packing feature of the transfer component could not
be employed in these experiments since the current interface
to the generator and stochastic disambiguation component still
requires unpacked representations.
"A prototype is ready for testing , and Leary hopes to set requirements for a full system by the end of the year."
?be<[93:ready]>[30:prototype]?PRED
?prototype?PRED
countGRAINNTYPE
?a?PREDDET?FORM a, DET?TYPE indefDETSPEC
CASE nom, NUM sg, PERS 330
SUBJ
?ready<[30:prototype]>?PRED [30:prototype]SUBJADEGREE positive, ATYPE predicative93XCOMP
?for<[141:test]>?PRED
?test?PRED
gerundGRAINNTYPE
CASE acc, NUM sg, PERS 3, PFORM for, VTYPE main141
OBJ
ADV?TYPE vpadv, PSEM unspecified, PTYPE sem125
ADJUNCT
MOOD indicative, PERF ?_, PROG ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular[252:hope]>s73
?hope<[235:Leary], [280:set]>?PRED
?Leary?PRED
properGRAIN
namePROPERNSEMNTYPE
ANIM +, CASE nom, NUM sg, PERS 3235
SUBJ
?set<[235:Leary], [336:requirement], [355:for]>?PRED [235:Leary]SUBJ
?requirement?PRED
unspecifiedGRAINNTYPE
CASE acc, NUM pl, PERS 3336
OBJ
?for<[391:system]>?PRED
?system?PRED
?full?PREDADEGREE positive, ADJUNCT?TYPE nominal, ATYPE attributive398ADJUNCT
unspecifiedGRAINNTYPE
?a?PREDDET?FORM a, DET?TYPE indefDETSPEC
CASE acc, NUM sg, PERS 3, PFORM for391
OBJ
PSEM unspecified, PTYPE sem355
OBL
?by<[469:end]>?PRED
?end?PRED
?of<[519:year]>?PRED
?year?PRED
countGRAINNTYPE
?the?PREDDET?FORM the, DET?TYPE defDETSPEC
CASE acc, NUM sg, PERS 3, PFORM of519
OBJ
ADJUNCT?TYPE nominal, PSEM unspecified, PTYPE sem512
ADJUNCT
countGRAINNTYPE
?the?PREDDET?FORM the, DET?TYPE defDETSPEC
CASE acc, NUM sg, PERS 3, PFORM by469
OBJ
ADV?TYPE vpadv, PSEM unspecified, PTYPE sem451
ADJUNCT
PERF ?_, PROG ?_TNS?ASP
INF?FORM to, PASSIVE ?, VTYPE main280
XCOMP
MOOD indicative, PERF ?_, PROG ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE main252
COORD +_, COORD?FORM and, COORD?LEVEL ROOT197
Figure 1: F -structure for non-condensed sentence.
slept.), the optional deletion of parts of coordinate struc-
tures (e.g., They laughed and giggled. can become They
giggled.), and certain simplifications (e.g. It is clear that
the earth is round. can become The earth is round. but
It seems that he is asleep. cannot become He is asleep.).
For example, one possible post-transfer output of the sen-
tence in Fig. 1 is shown in Fig. 2.
2.2 Stochastic Selection and Generation
The transfer rules are independent of the grammar and are
not constrained to preserve the grammaticality or well-
formedness of the reduced f-structures. Some of the re-
duced structures therefore may not correspond to any En-
glish sentence, and these are eliminated from future con-
sideration by using the generator as a filter. The filter-
ing is done by running each transferred structure through
the generator to see whether it produces an output string.
If it does not, the structure is rejected. For example, for
the f -structure in Fig. 1, the transfer system proposed
32 possible reductions. After filtering these structures by
generation, 16 reduced f -structures comprising possible
"A prototype is ready for testing."
?be  <[93:ready]>[30:prototype]?PRED
?prototype ?PRED
countGRAINNTYPE
?a?PREDDET?FORM  a, DET?TYPE  indefDETSPEC
CASE nom, NUM sg, PERS 330
SUBJ
?ready<[30:prototype]>?PRED [30:prototype]SUBJADEGREE  positive , ATYPE  predicative93XCOMP
?for<[141:test]>?PRED
?test ?PRED
gerundGRAINNTYPE
CASE acc, NUM sg, PERS 3, PFORM for, VTYPE main141
OBJ
ADV?TYPE  vpadv , PSEM  unspecified , PTYPE  sem125
ADJUNCT
MOOD	  indicative, PERF  ?_, PROG  ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular73
Figure 2: Gold standard f -structure reduction.
condensations of the input sentence survive. The 16 well-
formed structures correspond to the following strings that
were outputted by the generator (note that a single struc-
ture may correspond to more than one string and a given
string may correspond to more than one structure):
A prototype is ready.
A prototype is ready for testing.
Leary hopes to set requirements for a full system.
A prototype is ready and Leary hopes to set require-
ments for a full system.
A prototype is ready for testing and Leary hopes to
set requirements for a full system.
Leary hopes to set requirements for a full system by
the end of the year.
A prototype is ready and Leary hopes to set require-
ments for a full system by the end of the year.
A prototype is ready for testing and Leary hopes to
set requirements for a full system by the end of the
year.
In order to guarantee non-empty output for the over-
all condensation system, the generation component has
to be fault-tolerant in cases where the transfer system op-
erates on a fragmentary parse, or produces non-valid f -
structures from valid input f -structures. Robustness tech-
niques currently applied to the generator include insertion
and deletion of features in order to match invalid transfer-
output to the grammar rules and lexicon. Furthermore,
repair mechanisms such as repairing subject-verb agree-
ment from the subject?s number value are employed. As
a last resort, a fall-back mechanism to the original un-
condensed f -structure is used. These techniques guaran-
tee that a non-empty set of reduced f -structures yielding
grammatical strings in generation is passed on to the next
system component. In case of fragmentary input to the
transfer component, grammaticaliy of the output is guar-
anteed for the separate fragments. In other words, strings
generated from a reduced fragmentary f -structure will be
as grammatical as the string that was fed into the parsing
component.
After filtering by the generator, the remaining f -
structures were weighted by the stochastic disambigua-
tion component. Similar to stochastic disambiguation for
constraint-based parsing (Johnson et al, 1999; Riezler et
al., 2002), an exponential (a.k.a. log-linear or maximum-
entropy) probability model on transferred structures is es-
timated from a set of training data. The data for estima-
tion consists of pairs of original sentences y and gold-
standard summarized f -structures s which were manu-
ally selected from the transfer output for each sentence.
For training data {(sj , yj)}mj=1 and a set of possible sum-
marized structures S(y) for each sentence y, the objective
was to maximize a discriminative criterion, namely the
conditional likelihood L(?) of a summarized f -structure
given the sentence. Optimization of the function shown
below was performed using a conjugate gradient opti-
mization routine:
L(?) = log
m?
j=1
e??f(sj)
?
s?S(yj)
e??f(s)
.
At the core of the exponential probability model is a vec-
tor of property-functions f to be weighted by parameters
?. For the application of sentence condensation, 13,000
property-functions of roughly three categories were used:
? Property-functions indicating attributes, attribute-
combinations, or attribute-value pairs for f -structure
attributes (? 1,000 properties)
? Property-functions indicating co-occurences of verb
stems and subcategorization frames (? 12,000 prop-
erties)
? Property-functions indicating transfer rules used to
arrive at the reduced f - structures (? 60 properties).
A trained probability model is applied to unseen data
by selecting the most probable transferred f -structure,
yielding the string generated from the selected struc-
ture as the target condensation. The transfered f -structure
chosen for our current example is shown in Fig. 3.
"A prototype is ready."
?be  <[93:ready]>[30:prototype]?PRED
?prototype ?PRED
countGRAINNTYPE
?a?PRED
DET?FORM a, DET?TYPE indefDETSPEC
CASE nom, NUM  sg, PERS  330
SUBJ
?ready<[30:prototype]>?PRED [30:prototype]SUBJ
ADEGREE positive , ATYPE predicative93XCOMP

MOOD indicative, PERF ?_, PROG ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular73
Figure 3: Transferred f -structure chosen by system.
This structure was produced by the following set of
transfer rules, where var refers to the indices in the rep-
resentation of the f -structure:
rtrace(r13,keep(var(98),of)),
rtrace(r161,keep(system,var(85))),
rtrace(r1,del(var(91),set,by)),
rtrace(r1,del(var(53),be,for)),
rtrace(r20,equal(var(1),and)),
rtrace(r20,equal(var(2),and)),
rtrace(r2,del(var(1),hope,and)),
rtrace(r22,delb(var(0),and)).
These rules delete the adjunct of the first conjunct (for
testing), the adjunct of the second conjunct (by the end
of the year), the rest of the second conjunct (Leary hopes
to set requirements for a full system), and the conjunction
itself (and).
3 A Method for Automatic Evaluation of
Sentence Summarization
Evaluation of quality of sentence condensation systems,
and of text summarization and simplification systems in
general, has mostly been conducted as intrinsic evalua-
tion by human experts. Recently, Papineni et al?s (2001)
proposal for an automatic evaluation of translation sys-
tems by measuring n-gram matches of the system out-
put against reference examples has become popular for
evaluation of summarization systems. In addition, an au-
tomatic evaluation method based on context-free deletion
decisions has been proposed by Jing (2000). However, for
summarization systems that employ a linguistic parser as
an integral system component, it is possible to employ
the standard evaluation techniques for parsing directly
to an evaluation of summarization quality. A parsing-
based evaluation allows us to measure the semantic as-
pects of summarization quality in terms of grammatical-
functional information provided by deep parsers. Further-
more, human expertise was necessary only for the cre-
ation of condensed versions of sentences, and for the
manual disambiguation of parses assigned to those sen-
tences. Given such a gold standard, summarization qual-
ity of a system can be evaluated automatically and re-
peatedly by matching the structures of the system out-
put against the gold standard structures. The standard
metrics of precision, recall, and F-score from statisti-
cal parsing can be used as evaluation metrics for mea-
suring matching quality: Precision measures the number
of matching structural items in the parses of the sys-
tem output and the gold standard, out of all structural
items in the system output?s parse; recall measures the
number of matches, out of all items in the gold stan-
dard?s parse. F-score balances precision and recall as
(2 ? precision ? recall)/(precision + recall).
For the sentence condensation system presented above,
the structural items to be matched consist of rela-
tion(predicate, argument) triples. For example, the gold-
standard f -structure of Fig. 2 corresponds to 23 depen-
dency relations, the first 14 of which are shared with the
reduced f -structure chosen by the stochastic disambigua-
tion system:
tense(be:0, pres),
mood(be:0, indicative),
subj(be:0, prototype:2),
xcomp(be:0, ready:1),
stmt_type(be:0, declarative),
vtype(be:0, copular),
subj(ready:1, prototype:2),
adegree(ready:1, positive),
atype(ready:1, predicative),
det(prototype:2, a:7),
num(prototype:2, sg),
pers(prototype:2, 3),
det_form(a:7, a),
det_type(a:7, indef),
adjunct(be:0, for:12),
obj(for:12, test:14),
adv_type(for:12, vpadv),
psem(for:12, unspecified),
ptype(for:12, semantic),
num(test:14, sg),
pers(test:14, 3),
pform(test:14, for),
vtype(test:14, main).
Matching these f -structures against each other corre-
sponds to a precision of 1, recall of .61, and F-score of
.76.
The fact that our method does not rely on a compar-
ison of the characteristics of surface strings is a clear
advantage. Such comparisons are bad at handling exam-
ples which are similar in meaning but differ in word or-
der or vary structurally, such as in passivization or nom-
inalization. Our method handles such examples straight-
forwardly. Fig. 4 shows two serialization variants of the
condensed sentence of Fig. 2. The f -structures for these
examples are similar to the f -structure assigned to the
gold standard condensation shown in Fig. 2 (except for
the relations ADJUNT-TYPE:parenthetical ver-
sus ADV-TYPE:vpadv versus ADV-TYPE:sadv). An
evaluation of summarization quality that is based on
matching f -structures will treat these examples equally,
whereas an evaluation based on string matching will yield
different quality scores for different serializations.
"A prototype, for testing, is ready."
?be  <[221:ready]>[30:prototype]?PRED
?prototype ?PRED
countGRAINNTYPE
?a?PREDDET?FORM  a, DET?TYPE  indefDETSPEC
CASE nom, NUM sg, PERS 330
SUBJ
?ready<[30:prototype]>?PRED [30:prototype]SUBJADEGREE  positive , ATYPE  predicative221XCOMP
?for<[117:test]>?PRED
?test ?PRED
gerundGRAINNTYPECASE acc, NUM sg, PERS 3, PFORM for, VTYPE main117OBJADJUNCT?TYPE  parenthetical , PSEM  unspecified , PTYPE  sem73
ADJUNCT
MOOD  indicative, PERF  ?_, PROG  ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular201
"For testing, a prototype is ready."
?be  <[177:ready]>[131:prototype]?PRED
?prototype ?PRED
countGRAINNTYPE
?a?PREDDET?FORM  a, DET?TYPE  indefDETSPEC
CASE nom, NUM sg, PERS 3131
SUBJ
?ready<[131:prototype]>?PRED [131:prototype]SUBJADEGREE  positive , ATYPE  predicative177XCOMP
?for<[27:test]>?PRED
?test ?PRED
gerundGRAINNTYPECASE acc, NUM sg, PERS 3, PFORM for, VTYPE main27OBJADV?TYPE  sadv, PSEM  unspecified , PTYPE  sem11
ADJUNCT
MOOD  indicative, PERF  ?_, PROG  ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular83
Figure 4: F -structure for word-order variants of gold
standard condensation.
In the next section, we present experimental results
of an automatic evaluation of the sentence condensation
system described above. These results show a close cor-
respondence between automatically produced evaluation
results and human judgments on the quality of generated
condensed strings.
4 Experimental Evaluation
The sentences and condensations we used are taken from
data for the experiments of Knight and Marcu (2000),
which were provided to us by Daniel Marcu. These data
consist of pairs of sentences and their condensed versions
that have been extracted from computer-news articles and
abstracts of the Ziff-Davis corpus. Out of these data, we
parsed and manually disambiguated 500 sentence pairs.
These included a set of 32 sentence pairs that were used
for testing purposes in Knight and Marcu (2000). In or-
der to control for the small corpus size of this test set, we
randomly extracted an additional 32 sentence pairs from
the 500 parsed and disambiguated examples as a second
test set. The rest of the 436 randomly selected sentence
pairs were used to create training data. For the purpose
of discriminative training, a gold-standard of transferred
f -structures was created from the transfer output and the
manually selected f -structures for the condensed strings.
This was done automatically by selecting for each exam-
ple the transferred f -structure that best matched the f -
structure annotated for the condensed string.
In the automatic evaluation of f -structure match, three
different system variants were compared. Firstly, ran-
domly chosen transferred f -structures were matched
against the manually selected f -structures for the man-
ually created condensations. This evaluation constitutes
a lower bound on the F-score against the given gold
standard. Secondly, matching results for transferred f -
structures yielding the maximal F-score against the gold
standard were recorded, giving an upper bound for the
system. Thirdly, the performance of the stochastic model
within the range of the lower bound and upper bound was
measured by recording the F-score for the f -structure that
received highest probability according to the learned dis-
tribution on transferred structures.
In order to make our results comparable to the re-
sults of Knight and Marcu (2000) and also to investigate
the correspondence between the automatic evaluation and
human judgments, a manual evaluation of the strings gen-
erated by these system variants was conducted. Two hu-
man judges were presented with the uncondensed sur-
face string and five condensed strings that were displayed
in random order for each test example. The five con-
densed strings presented to the human judges contained
(1) strings generated from three randomly selected f -
structures, (2) the strings generated from the f -structures
which were selected by the stochastic model, and (3) the
manually created gold-standard condensations extracted
from the Ziff-Davis abstracts. The judges were asked
to judge summarization quality on a scale of increasing
quality from 1 to 5 by assessing how well the generated
strings retained the most salient information of the orig-
inal uncondensed sentences. Grammaticality of the sys-
tem output is optimal and not reported separately. Results
for both evaluations are reported for two test corpora of
32 examples each. Testset I contains the sentences and
condensations used to evaluate the system described in
Knight and Marcu (2000). Testset II consists of another
randomly extracted 32 sentence pairs from the same do-
main, prepared in the same way.
Fig. 5 shows evaluation results for a sentence conden-
sation run that uses manually selected f -structures for
the original sentences as input to the transfer component.
These results demonstrate how the condenstation system
performs under the optimal circumstances when the parse
chosen as input is the best available. Fig. 6 applies the
same evaluation data and metrics to a sentence conden-
sation experiment that performs transfer from packed f -
structures, i.e. transfer is performed on all parses for an
ambiguous sentence instead of on a single manually se-
lected parse. Alternatively, a single input parse could be
selected by stochastic models such as the one described
in Riezler et al (2002). A separate phase of parse disam-
biguation, and perhaps the effects of any errors that this
might introduce, can be avoided by transferring from all
parses for an ambiguous sentence. This approach is com-
putationally feasible, however, only if condensation can
be carried all the way through without unpacking. Our
technology is not yet able to do this (in particular, as men-
tioned earlier, we have not yet implemented a method for
stochastic disambiguation on packed f -structures). How-
ever, we conducted a preliminary assessment of this pos-
sibility by unpacking and enumerating the transferred f -
structures. For many sentences this resulted in more can-
didates than we could operate on in the available time
and space, and in those cases we arbitrarily set a cut-off
on the number of transferred f -structures we considered.
Since transferred f -structures are produced according to
the number of rules applied to transfer them, in this setup
the transfer system produces smaller f -structures first,
and cuts off less condensed output. The result of this ex-
periment, shown in Fig. 6, thus provides a conservative
estimate on the quality of the condensations we might
achieve with a full-packing implementation.
In Figs. 5 and 6, the first row shows F-scores for a
random selection, the system selection, and the best pos-
sible selection from the transfer output against the gold
standard. The second rows show summarization quality
scores for generations from a random selection and the
system selection, and for the human-written condensa-
tion. The third rows report compression ratios. As can
testset I lowerbound
system
selection
upper
bound
F-score 58% 67.3% 77.2 %
sum-quality 2.0 3.5 4.4
compr. 50.2% 60.4% 54.9%
testset II lowerbound
system
selection
upper
bound
F-score 59% 65.4% 83.3%
sum-quality 2.1 3.4 4.6
compr. 52.7% 65.9% 56.8%
Figure 5: Sentence condensation from manually selected
f -structure for original uncondensed sentences.
be seen from these tables, the ranking of system variants
produced by the automatic and manual evaluation con-
firm a close correlation between the automatic evaluation
and human judgments. A comparison of evaluation re-
sults across colums, i.e. across selection variants, shows
that a stochastic selection of transferred f -structures is
indeed important. Even if all f -structures are transferred
from the same linguistically rich source, and all gener-
ated strings are grammatical, a reduction in error rate of
around 50% relative to the upper bound can be achieved
by stochastic selection. In contrast, a comparison be-
tween transfer runs with and without perfect disambigua-
tion of the original string shows a decrease of about 5% in
F-score, and of only .1 points for summarization quality
when transferring from packed parses instead of from the
manually selected parse. This shows that it is more im-
portant to learn what a good transferred f -structure looks
like than to have a perfect f -structure to transfer from.
The compression rates associated with the systems that
used stochastic selection is around 60%, which is accept-
able, but not as aggressive as human-written condensa-
tions. Note that in our current implementation, in some
cases the transfer component was unable to operate on
the packed representation. In those cases a parse was cho-
sen at random as a conservative estimate of transfer from
all parses. This fall-back mechanism explains the drop in
F-score for the upper bound in comparing Figs. 5 and 6.
5 Conclusion
We presented an approach to sentence condensation
that employs linguistically rich LFG grammars in a
parsing/generation-based stochastic sentence condensa-
tion system. Fine-grained dependency structures are out-
put by the parser, then modified by a highly expressive
transfer system, and filtered by a constraint-based gener-
ator. Stochastic selection of generation-filtered reduced
structures uses a powerful Maximum-Entropy model.
As shown in an experimental evaluation, summarization
testset I lowerbound
system
selection
upper
bound
F-score 55.2% 63.0% 72.0%
sum-quality 2.1 3.4 4.4
compres. 46.5% 61.6% 54.9%
testset II lowerbound
system
selection
upper
bound
F-score 54% 59.7% 76.0 %
sum-quality 1.9 3.3 4.6
compres. 50.9% 60.0% 56.8%
Figure 6: Sentence condensation from packed f -
structures for original uncondensed sentences.
quality of the system output is state-of-the-art, and gram-
maticality of condensed strings is guaranteed. Robustness
techniques for parsing and generation guarantee that the
system produces non-empty output for unseen input.
Overall, the summarization quality achieved by
our system is similar to the results reported in
Knight and Marcu (2000). This might seem disappoint-
ing considering the more complex machinery employed
in our approach. It has to be noted that these re-
sults are partially due to the somewhat artificial na-
ture of the data that were used in the experiments of
Knight and Marcu (2000) and therefore in our experi-
ments: The human-written condensations in the data set
extracted from the Ziff-Davis corpus show the same
word order as the original sentences and do not exhibit
any structural modification that are common in human-
written summaries. For example, humans tend to make
use of structural modifications such as nominalization
and verb alternations such as active/passive or transi-
tive/intransitive alternations in condensation. Such alter-
nations can easily be expressed in our transfer-based
approach, whereas they impose severe problems to ap-
proaches that operate only on phrase structure trees. In
the given test set, however, the condensation task re-
stricted to the operation of deletion. A creation of addi-
tional condensations for the original sentences other than
the condensed versions extracted from the human-written
abstracts would provide a more diverse test set, and fur-
thermore make it possible to match each system output
against any number of independent human-written con-
densations of the same original sentence. This idea of
computing matching scores to multiple reference exam-
ples was proposed by Alshawi et al (1998), and later by
Papineni et al (2001) for evaluation of machine transla-
tion systems. Similar to these proposals, an evaluation
of condensation quality could consider multiple reference
condensations and record the matching score against the
most similar example.
Another desideratum for future work is to carry
condensation all the way through without unpacking
at any stage. Work on employing packing techniques
not only for parsing and transfer, but also for genera-
tion and stochastic selection is currently underway (see
Geman and Johnson (2002)). This will eventually lead to
a system whose components work on packed represen-
tations of all or n-best solutions, but completely avoid
costly unpacking of representations.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
1998. Automatic acquisition of hierarchical trans-
duction models for machine translation. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics (ACL?98), Montreal, Que-
bec, Canada.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of english newspaper text to assist aphasic readers. In
Proceedings of the AAAI Workshop on Integrating Arti-
ficial Intelligence and Assistive Technology, Madison,
WI.
Anette Frank. 1999. From parallel grammar develop-
ment towards machine translation. In Proceedings of
the MT Summit VII. MT in the Great Translation Era,
pages 134?142. Kent Ridge Digital Labs, Singapore.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochas-
tic unification-based grammars. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?02), Philadelphia, PA.
Gregory Grefenstette. 1998. Producing intelligent tele-
graphic text reduction to provide an audio scanning
service for the blind. In Proceedings of the AAAI
Spring Workshop on Intelligent Text Summarization,
Stanford, CA.
Hongyan Jing. 2000. Sentence reduction for automatic
text summarization. In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP?00),
Seattle, WA.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?99), College Park, MD.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization?step one: Sentence compression. In
Proceedings of the 17th National Conference on Arti-
ficial Intelligence (AAAI-2000), Austin, TX.
John Maxwell and Ronald M. Kaplan. 1989. An
overview of disjunctive constraint satisfaction. In Pro-
ceedings of the International Workshop on Parsing
Technologies, Pittsburgh, PA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. Technical Report IBM Re-
search Division Technical Report, RC22176 (W0190-
022), Yorktown Heights, N.Y.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
summarization: A statistical approach to generating
highly condensed non-extractive summaries. In Pro-
ceedings of the 22nd ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
Berkeley, CA.
Speed and Accuracy in Shallow and Deep Stochastic Parsing
Ronald M. Kaplan , Stefan Riezler , Tracy Holloway King
John T. Maxwell III, Alexander Vasserman and Richard Crouch
Palo Alto Research Center, 3333 Coyote Hill Rd., Palo Alto, CA 94304
{kaplan|riezler|king|maxwell|avasserm|crouch}@parc.com
Abstract
This paper reports some experiments that com-
pare the accuracy and performance of two
stochastic parsing systems. The currently pop-
ular Collins parser is a shallow parser whose
output contains more detailed semantically-
relevant information than other such parsers.
The XLE parser is a deep-parsing system that
couples a Lexical Functional Grammar to a log-
linear disambiguation component and provides
much richer representations theory. We mea-
sured the accuracy of both systems against a
gold standard of the PARC 700 dependency
bank, and also measured their processing times.
We found the deep-parsing system to be more
accurate than the Collins parser with only a
slight reduction in parsing speed.1
1 Introduction
In applications that are sensitive to the meanings ex-
pressed by natural language sentences, it has become
common in recent years simply to incorporate publicly
available statistical parsers. A state-of-the-art statistical
parsing system that enjoys great popularity in research
systems is the parser described in Collins (1999) (hence-
forth ?the Collins parser?). This system not only is fre-
quently used for off-line data preprocessing, but also
is included as a black-box component for applications
such as document summarization (Daume and Marcu,
2002), information extraction (Miller et al, 2000), ma-
chine translation (Yamada and Knight, 2001), and ques-
tion answering (Harabagiu et al, 2001). This is be-
1This research has been funded in part by contract #
MDA904-03-C-0404 awarded from the Advanced Research and
Development Activity, Novel Intelligence from Massive Data
program. We would like to thank Chris Culy whose original ex-
periments inspired this research.
cause the Collins parser shares the property of robustness
with other statistical parsers, but more than other such
parsers, the categories of its parse-trees make grammati-
cal distinctions that presumably are useful for meaning-
sensitive applications. For example, the categories of
the Model 3 Collins parser distinguish between heads,
arguments, and adjuncts and they mark some long-
distance dependency paths; these distinctions can guide
application-specific postprocessors in extracting impor-
tant semantic relations.
In contrast, state-of-the-art parsing systems based on
deep grammars mark explicitly and in much more de-
tail a wider variety of syntactic and semantic dependen-
cies and should therefore provide even better support for
meaning-sensitive applications. But common wisdom has
it that parsing systems based on deep linguistic grammars
are too difficult to produce, lack coverage and robustness,
and also have poor run-time performance. The Collins
parser is thought to be accurate and fast and thus to repre-
sent a reasonable trade-off between ?good-enough? out-
put, speed, and robustness.
This paper reports on some experiments that put this
conventional wisdom to an empirical test. We investi-
gated the accuracy of recovering semantically-relevant
grammatical dependencies from the tree-structures pro-
duced by the Collins parser, comparing these dependen-
cies to gold-standard dependencies which are available
for a subset of 700 sentences randomly drawn from sec-
tion 23 of the Wall Street Journal (see King et al (2003)).
We compared the output of the XLE system, a
deep-grammar-based parsing system using the English
Lexical-Functional Grammar previously constructed as
part of the Pargram project (Butt et al, 2002), to the
same gold standard. This system incorporates sophisti-
cated ambiguity-management technology so that all pos-
sible syntactic analyses of a sentence are computed in
an efficient, packed representation (Maxwell and Ka-
plan, 1993). In accordance with LFG theory, the output
includes not only standard context-free phrase-structure
trees but also attribute-value matrices (LFG?s f(unctional)
structures) that explicitly encode predicate-argument re-
lations and other meaningful properties. XLE selects the
most probable analysis from the potentially large candi-
date set by means of a stochastic disambiguation com-
ponent based on a log-linear (a.k.a. maximum-entropy)
probability model (Riezler et al, 2002). The stochas-
tic component is also ?ambiguity-enabled? in the sense
that the computations for statistical estimation and selec-
tion of the most probable analyses are done efficiently
by dynamic programming, avoiding the need to unpack
the parse forests and enumerate individual analyses. The
underlying parsing system also has built-in robustness
mechanisms that allow it to parse strings that are outside
the scope of the grammar as a shortest sequence of well-
formed ?fragments?. Furthermore, performance parame-
ters that bound parsing and disambiguation work can be
tuned for efficient but accurate operation.
As part of our assessment, we also measured the pars-
ing speed of the two systems, taking into account all
stages of processing that each system requires to produce
its output. For example, since the Collins parser depends
on a prior part-of-speech tagger (Ratnaparkhi, 1996), we
included the time for POS tagging in our Collins mea-
surements. XLE incorporates a sophisticated finite-state
morphology and dictionary lookup component, and its
time is part of the measure of XLE performance.
Performance parameters of both the Collins parser and
the XLE system were adjusted on a heldout set consist-
ing of a random selection of 1/5 of the PARC 700 depen-
dency bank; experimental results were then based on the
other 560 sentences. For Model 3 of the Collins parser, a
beam size of 1000, and not the recommended beam size
of 10000, was found to optimize parsing speed at little
loss in accuracy. On the same heldout set, parameters of
the stochastic disambiguation system and parameters for
parsing performance were adjusted for a Core and a Com-
plete version of the XLE system, differing in the size of
the constraint-set of the underlying grammar.
For both XLE and the Collins parser we wrote con-
version programs to transform the normal (tree or f-
structure) output into the corresponding relations of
the dependency bank. This conversion was relatively
straightforward for LFG structures (King et al, 2003).
However, a certain amount of skill and intuition was
required to provide a fair conversion of the Collins
trees: we did not want to penalize configurations in the
Collins trees that encoded alternative but equally legit-
imate representations of the same linguistic properties
(e.g. whether auxiliaries are encoded as main verbs or
aspect features), but we also did not want to build into
the conversion program transformations that compensate
for information that Collins cannot provide without ap-
pealing to additional linguistic resources (such as identi-
fying the subjects of infinitival complements). We did not
include the time for dependency conversion in our mea-
sures of performance.
The experimental results show that stochastic parsing
with the Core LFG grammar achieves a better F-score
than the Collins parser at a roughly comparable parsing
speed. The XLE system achieves 12% reduction in error
rate over the Collins parser, that is 77.6% F-score for the
XLE system versus 74.6% for the Collins parser, at a cost
in parsing time of a factor of 1.49.
2 Stochastic Parsing with LFG
2.1 Parsing with Lexical-Functional Grammar
The grammar used for this experiment was developed in
the ParGram project (Butt et al, 2002). It uses LFG as a
formalism, producing c(onstituent)-structures (trees) and
f(unctional)-structures (attribute value matrices) as out-
put. The c-structures encode constituency and linear or-
der. F-structures encode predicate-argument relations and
other grammatical information, e.g., number, tense, state-
ment type. The XLE parser was used to produce packed
representations, specifying all possible grammar analyses
of the input.
In our system, tokenization and morphological analy-
sis are performed by finite-state transductions arranged in
a compositional cascade. Both the tokenizer and the mor-
phological analyzer can produce multiple outputs. For ex-
ample, the tokenizer will optionaly lowercase sentence
initial words, and the morphological analyzer will pro-
duce walk +Verb +Pres +3sg and walk +Noun +Pl for
the input form walks. The resulting tokenized and mor-
phologically analyzed strings are presented to the sym-
bolic LFG grammar.
The grammar can parse input that has XML de-
limited named entity markup: <company>Columbia
Savings</company> is a major holder of so-called junk
bonds. To allow the grammar to parse this markup,
the tokenizer includes an additional tokenization of the
strings whereby the material between the XML markup
is treated as a single token with a special morphologi-
cal tag (+NamedEntity). As a fall back, the tokenization
that the string would have received without that markup
is also produced. The named entities have a single mul-
tiword predicate. This helps in parsing both because it
means that no internal structure has to be built for the
predicate and because predicates that would otherwise be
unrecognized by the grammar can be parsed (e.g., Cie.
Financiere de Paribas). As described in section 5, it was
also important to use named entity markup in these ex-
periments to more fairly match the analyses in the PARC
700 dependency bank.
To increase robustness, the standard grammar is aug-
mented with a FRAGMENT grammar. This allows sen-
tences to be parsed as well-formed chunks specified by
the grammar, in particular as Ss, NPs, PPs, and VPs, with
unparsable tokens possibly interspersed. These chunks
have both c-structures and f-structures corresponding to
them. The grammar has a fewest-chunk method for de-
termining the correct parse.
The grammar incorporates a version of Optimality
Theory that allows certain (sub)rules in the grammar to be
prefered or disprefered based on OT marks triggered by
the (sub)rule (Frank et al, 1998). The Complete version
of the grammar uses all of the (sub)rules in a multi-pass
system that depends on the ranking of the OT marks in
the rules. For example, topicalization is disprefered, but
the topicalization rule will be triggered if no other parse
can be built. A one-line rewrite of the Complete grammar
creates a Core version of the grammar that moves the ma-
jority of the OT marks into the NOGOOD space. This ef-
fectively removes the (sub)rules that they mark from the
grammar. So, for example, in the Core grammar there is
no topicalization rule, and sentences with topics will re-
ceive a FRAGMENT parse. This single-pass Core grammar
is smaller than the Complete grammar and hence is faster.
The XLE parser also allows the user to adjust per-
formance parameters bounding the amount of work that
is done in parsing for efficient but accurate operation.
XLE?s ambiguity management technology takes advan-
tage of the fact that relatively few f-structure constraints
apply to constituents that are far apart in the c-structure,
so that sentences are typically parsed in polynomial time
even though LFG parsing is known to be an NP-complete
problem. But the worst-case exponential behavior does
begin to appear for some constructions in some sentences,
and the computational effort is limited by a SKIMMING
mode whose onset is controlled by a user-specified pa-
rameter. When skimming, XLE will stop processing the
subtree of a constituent whenever the amount of work ex-
ceeds that user-specified limit. The subtree is discarded,
and the parser will move on to another subtree. This guar-
antees that parsing will be finished within reasonable lim-
its of time and memory but at a cost of possibly lower
accuracy if it causes the best analysis of a constituent
to be discarded. As a separate parameter, XLE also lets
the user limit the length of medial constituents, i.e., con-
stituents that do not appear at the beginning or the end
of a sentence (ignoring punctuation). The rationale be-
hind this heuristic is to limit the weight of constituents in
the middle of the sentence but still to allow sentence-final
heavy constituents. This discards constituents in a some-
what more principled way as it tries to capture the psy-
cholinguistic tendency to avoid deep center-embedding.
When limiting the length of medial constituents, cubic-
time parsing is possible for sentences up to that length,
even with a deep, non-context-free grammar, and linear
parsing time is possible for sentences beyond that length.
The Complete grammar achieved 100% coverage of
section 23 as unseen unlabeled data: 79% as full parses,
21% FRAGMENT and/or SKIMMED parses.
2.2 Dynamic Programming for Estimation and
Stochastic Disambiguation
The stochastic disambiguation model we employ defines
an exponential (a.k.a. log-linear or maximum-entropy)
probability model over the parses of the LFG grammar.
The advantage of this family of probability distributions
is that it allows the user to encode arbitrary properties
of the parse trees as feature-functions of the probability
model, without the feature-functions needing to be inde-
pendent and non-overlapping. The general form of con-
ditional exponential models is as follows:
p?(x|y) = Z?(y)
?1e??f(x)
where Z?(y) =
?
x?X(y) e
??f(x) is a normalizing con-
stant over the set X(y) of parses for sentence y, ? is
a vector of log-parameters, f is a vector of feature-
values, and ? ? f(x) is a vector dot product denoting the
(log-)weight of parse x.
Dynamic-programming algorithms that allow the ef-
ficient estimation and searching of log-linear mod-
els from a packed parse representation without enu-
merating an exponential number of parses have
been recently presented by Miyao and Tsujii (2002)
and Geman and Johnson (2002). These algorithms can
be readily applied to the packed and/or-forests of
Maxwell and Kaplan (1993), provided that each conjunc-
tive node is annotated with feature-values of the log-
linear model. In the notation of Miyao and Tsujii (2002),
such a feature forest ? is defined as a tuple ?C,D, r, ?, ??
where C is a set of conjunctive nodes, D is a set of dis-
junctive nodes, r ? C is the root node, ? : D ? 2C is
a conjunctive daughter function, and ? : C ? 2D is a
disjunctive daughter function.
A dynamic-programming solution to the problem of
finding most probable parses is to compute the weight
?d of each disjunctive node as the maximum weight of
its conjunctive daugher nodes, i.e.,
?d = max
c??(d)
?c (1)
and to recursively define the weight ?c of a conjunctive
node as the product of the weights of all its descendant
disjunctive nodes and of its own weight:
?c =
?
d??(c)
?d e
??f(c) (2)
Keeping a trace of the maximally weighted choices in a
computaton of the weight ?r of the root conjunctive node
r allows us to efficiently recover the most probable parse
of a sentence from the packed representation of its parses.
The same formulae can be employed for an effi-
cient calculation of probabilistic expectations of feature-
functions for the statistical estimation of the parameters
?. Replacing the maximization in equation 1 by a sum-
mation defines the inside weight of disjunctive node. Cor-
respondingly, equation 2 denotes the inside weight of a
conjunctive node. The outside weight ?c of a conjunctive
node is defined as the outside weight of its disjunctive
mother node(s):
?c =
?
{d|c??(d)}
?d (3)
The outside weight of a disjunctive node is the sum of
the product of the outside weight(s) of its conjunctive
mother(s), the weight(s) of its mother(s), and the inside
weight(s) of its disjunctive sister(s):
?d =
?
{c|d??(c)}
{?c e
??f(c)
?
{d?|d???(c),d? 6=d}
?d?} (4)
From these formulae, the conditional expectation of a
feature-function fi can be computed from a chart with
root node r for a sentence y in the following way:
?
x?X(y)
e??f(x)fi(x)
Z?(y)
=
?
c?C
?c?cfi(c)
?r
(5)
Formula 5 is used in our system to compute expectations
for discriminative Bayesian estimation from partially la-
beled data using a first-order conjugate-gradient routine.
For a more detailed description of the optimization prob-
lem and the feature-functions we use for stochastic LFG
parsing see Riezler et al (2002). We also employed a
combined `1 regularization and feature selection tech-
nique described in Riezler and Vasserman (2004) that
considerably speeds up estimation and guarantees small
feature sets for stochastic disambiguation. In the experi-
ments reported in this paper, however, dynamic program-
ming is crucial for efficient stochastic disambiguation,
i.e. to efficiently find the most probable parse from a
packed parse forest that is annotated with feature-values.
There are two operations involved in stochastic disam-
biguation, namely calculating feature-values from a parse
forest and calculating node weights from a feature forest.
Clearly, the first one is more expensive, especially for
the extraction of values for non-local feature-functions
over large charts. To control the cost of this compu-
tation, our stochastic disambiguation system includes
a user-specified parameter for bounding the amount of
work that is done in calculating feature-values. When the
user-specified threshold for feature-value calculation is
reached, this computation is discontinued, and the dy-
namic programming calculation for most-probable-parse
search is computed from the current feature-value anno-
tation of the parse forest. Since feature-value computa-
tion proceeds incrementally over the feature forest, i.e.
for each node that is visited all feature-functions that ap-
ply to it are evaluated, a complete feature annotation can
be guaranteed for the part of the and/or-forest that is vis-
ited until discontinuation. As discussed below, these pa-
rameters were set on a held-out portion of the PARC700
which was also used to set the Collins parameters.
In the experiments reported in this paper, we used a
threshold on feature-extraction that allowed us to cut off
feature-extraction in 3% of the cases at no loss in accu-
racy. Overall, feature extraction and weight calculation
accounted for 5% of the computation time in combined
parsing and stochastic selection.
3 The Gold-Standard Dependency Bank
We used the PARC 700 Dependency Bank (DEPBANK)
as the gold standard in our experiments. The DEPBANK
consists of dependency annotations for 700 sentences that
were randomly extracted from section 23 of the UPenn
Wall Street Journal (WSJ) treebank. As described by
(King et al, 2003), the annotations were boot-strapped
by parsing the sentences with a LFG grammar and trans-
forming the resulting f-structures to a collection of depen-
dency triples in the DEPBANK format. To prepare a true
gold standard of dependencies, the tentative set of depen-
dencies produced by the robust parser was then corrected
and extended by human validators2. In this format each
triple specifies that a particular relation holds between a
head and either another head or a feature value, for ex-
ample, that the SUBJ relation holds between the heads
run and dog in the sentence The dog ran. Average sen-
tence length of sentences in DEPBANK is 19.8 words, and
the average number of dependencies per sentence is 65.4.
The corpus is freely available for research and evaluation,
as are documentation and tools for displaying and prun-
ing structures.3
In our experiments we used a Reduced version of the
DEPBANK, including just the minimum set of dependen-
cies necessary for reading out the central semantic rela-
tions and properties of a sentence. We tested against this
Reduced gold standard to establish accuracy on a lower
bound of the information that a meaning-sensitive appli-
cation would require. The Reduced version contained all
the argument and adjunct dependencies shown in Fig.
1, and a few selected semantically-relevant features, as
shown in Fig. 2. The features in Fig. 2 were chosen be-
2The resulting test set is thus unseen to the grammar and
stochastic disambiguation system used in our experiments. This
is indicated by the fact that the upperbound of F-score for the
best matching parses for the experiment grammar is in the range
of 85%, not 100%.
3http://www2.parc.com/istl/groups/nltt/fsbank/
Function Meaning
adjunct adjuncts
aquant adjectival quantifiers (many, etc.)
comp complement clauses (that, whether)
conj conjuncts in coordinate structures
focus int fronted element in interrogatives
mod noun-noun modifiers
number numbers modifying nouns
obj objects
obj theta secondary objects
obl oblique
obl ag demoted subject of a passive
obl compar comparative than/as clauses
poss possessives (John?s book)
pron int interrogative pronouns
pron rel relative pronouns
quant quantifiers (all, etc.)
subj subjects
topic rel fronted element in relative clauses
xcomp non-finite complements
verbal and small clauses
Figure 1: Grammatical functions in DEPBANK.
cause it was felt that they were fundamental to the mean-
ing of the sentences, and in fact they are required by the
semantic interpreter we have used in a knowledge-based
application (Crouch et al, 2002).
Feature Meaning
adegree degree of adjectives and adverbs
(positive, comparative, superlative)
coord form form of a coordinating
conjunction (e.g., and, or)
det form form of a determiner (e.g., the, a)
num number of nouns (sg, pl)
number type cardinals vs. ordinals
passive passive verb (e.g., It was eaten.)
perf perfective verb (e.g., have eaten)
precoord form either, neither
prog progressive verb (e.g., were eating)
pron form form of a pronoun (he, she, etc.)
prt form particle in a particle verb
(e.g., They threw it out.)
stmt type statement type (declarative,
interrogative, etc.)
subord form subordinating conjunction (e.g. that)
tense tense of the verb (past, present, etc.)
Figure 2: Selected features for Reduced DEPBANK
.
As a concrete example, the dependency list in Fig. 3 is
the Reduced set corresponding to the following sentence:
He reiterated his opposition to such funding,
but expressed hope of a compromise.
An additional feature of the DEPBANK that is relevant
to our comparisons is that dependency heads are rep-
resented by their standard citation forms (e.g. the verb
swam in a sentence appears as swim in its dependencies).
We believe that most applications will require a conver-
sion to canonical citation forms so that semantic relations
can be mapped into application-specific databases or on-
tologies. The predicates of LFG f-structures are already
represented as citation forms; for a fair comparison we
ran the leaves of the Collins tree through the same stem-
mer modules as part of the tree-to-dependency transla-
tion. We also note that proper names appear in the DEP-
BANK as single multi-word expressions without any in-
ternal structure. That is, there are no dependencies hold-
ing among the parts of people names (A. Boyd Simpson),
company names (Goldman, Sachs & Co), and organiza-
tion names (Federal Reserve). This multiword analysis
was chosen because many applications do not require
the internal structure of names, and the identification of
named entities is now typically carried out by a separate
non-syntactic pre-processing module. This was captured
for the LFG parser by using named entity markup and for
the Collins parser by creating complex word forms with
a single POS tag (section 5).
conj(coord?0, express?3)
conj(coord?0, reiterate?1)
coord form(coord?0, but)
stmt type(coord?0, declarative)
obj(reiterate?1, opposition?6)
subj(reiterate?1, pro?7)
tense(reiterate?1, past)
obj(express?3, hope?15)
subj(express?3, pro?7)
tense(express?3, past)
adjunct(opposition?6, to?11)
num(opposition?6, sg)
poss(opposition?6, pro?19)
num(pro?7, sg)
pron form(pro?7, he)
obj(to?11, funding?13)
adjunct(funding?13, such?45)
num(funding?13, sg)
adjunct(hope?15, of?46)
num(hope?15, sg)
num(pro?19, sg)
pron form(pro?19, he)
adegree(such?45, positive)
obj(of?46, compromise?54)
det form(compromise?54, a)
num(compromise?54, sg)
Figure 3: Reduced dependency relations for He reiterated
his opposition to such funding, but expressed hope of a
compromise.
4 Conversion to Dependency Bank Format
A conversion routine was required for each system to
transform its output so that it could be compared to the
DEPBANK dependencies. While it is relatively straightfor-
ward to convert LFG f-structures to the dependency bank
format because the f-structure is effectively a dependency
format, it is more difficult to transform the output trees of
the Model 3 Collins parser in a way that fairly allocates
both credits and penalties.
LFG Conversion We discarded the LFG tree structures
and used a general rewriting system previously developed
for machine translation to rewrite the relevant f-structure
attributes as dependencies (see King et al (2003)). The
rewritings involved some deletions of irrelevant features,
some systematic manipulations of the analyses, and some
trivial respellings. The deletions involved features pro-
duced by the grammar but not included in the PARC 700
such as negative values of PASS, PERF, and PROG and
the feature MEASURE used to mark measure phrases. The
manipulations are more interesting and are necessary to
map systematic differences between the analyses in the
grammar and those in the dependency bank. For example,
coordination is treated as a set by the LFG grammar but as
a single COORD dependency with several CONJ relations
in the dependency bank. Finally, the trivial rewritings
were used to, for example, change STMT-TYPE decl in
the grammar to STMT-TYPE declarative in the de-
pendency bank. For the Reduced version of the PARC
700 substantially more features were deleted.
Collins Model 3 Conversion An abbreviated represen-
tation of the Collins tree for the example above is shown
in Fig. 4. In this display we have eliminated the head lex-
ical items that appear redundantly at all the nonterminals
in a head chain, instead indicating by a single number
which daughter is the head. Thus, S?2 indicates that the
head of the main clause is its second daughter, the VP,
and its head is its first VP daughter. Indirectly, then, the
lexical head of the S is the first verb reiterated.
(TOP?1
(S?2 (NP-A?1 (NPB?1 He/PRP))
(VP?1 (VP?1 reiterated/VBD
(NP-A?1 (NPB?2 his/PRP$
opposition/NN)
(PP?1 to/TO
(NPB?2 such/JJ
funding/NN))))
but/CC
(VP?1 expressed/VBD
(NP-A?1 (NPB?1 hope/NN)
(PP?1 of/IN
(NP-A?1 (NPB?2 a/DT
compromise/NN))))))))
Figure 4: Collins Model 3 tree for He reiterated his op-
position to such funding, but expressed hope of a compro-
mise.
The Model 3 output in this example includes standard
phrase structure categories, indications of the heads, and
the additional -A marker to distinguish arguments from
adjuncts. The terminal nodes of this tree are inflected
forms, and the first phase of our conversion replaces them
with their citation forms (the verbs reiterate and express,
and the decapitalized and standardized he for He and his).
We also adjust for systematic differences in the choice of
heads. The first conjunct tends to be marked as the head
of a coordination in Model 3 output, whereas the depen-
dency bank has a more symmetric representation: it in-
troduces a new COORD head and connects that up to the
conjunction, and it uses a separate CONJ relation for each
of the coordinated items. Similarly, Model 3 identifies
the syntactic markers to and that as the heads of com-
plements, whereas the dependency bank treats these as
selectional features and marks the main predicate of the
complements as the head. These adjustments are carried
out without penalty. We also compensate for the differ-
ences in the representation of auxiliaries: Model 3 treats
these as main verbs with embedded complements instead
of the PERF, PROG, and PASSIVE features of the DEP-
BANK, and our conversion flattens the trees so that the
features can be read off.
The dependencies are read off after these and a few
other adjustments are made. NPs under VPs are read off
either as objects or adjuncts, depending on whether or
not the NP is annotated with the argument indicator (-A)
as in this example; the -A presumably would be miss-
ing in a sentence like John arrived Friday, and Friday
would be treated as an ADJUNCT. Similarly, NP-As un-
der S are read off as subject. In this example, however,
this principle of conversion does not lead to a match with
the dependency bank: in the DEPBANK grammatical rela-
tions that are factored out of conjoined structures are dis-
tributed back into those structures, to establish the correct
semantic dependencies (in this case, that he is the subject
of both reiterate and express and not of the introduced
coord). We avoided the temptation of building coordinate
distribution into the conversion routine because, first, it is
not always obvious from the Model 3 output when dis-
tribution should take place, and second, that would be
a first step towards building into the conversion routine
the deep lexical and syntactic knowledge (essentially the
functional component of our LFG grammar) that the shal-
low approach explicitly discounts4.
For the same reasons our conversion routine does not
identify the subjects of infinitival complements with par-
ticular arguments of matrix verbs. The Model 3 trees pro-
vide no indication of how this is to be done, and in many
cases the proper assignment depends on lexical informa-
tion about specific predicates (to capture, for example, the
well-known contrast between promise and persuade).
Model 3 trees also provide information about certain
4However, we did explore a few of these additional transfor-
mations and found only marginal F-score increases.
long-distance dependencies, by marking with -g annota-
tions the path between a filler and a gap and marking the
gap by an explicit TRACE in the terminal string. The filler
itself is not clearly identified, but our conversion treats
all WH categories under SBAR as potential fillers and
attempts to propagate them down the gap-chain to link
them up to appropriate traces.
In sum, it is not a trivial matter to convert a Model 3
tree to an appropriate set of dependency relations, and the
process requires a certain amount of intuition and skill.
For our experiments we tried to define a conversion that
gives appropriate credit to the dependencies that can be
read from the trees without relying on an undue amount
of sophisticated linguistic knowledge5.
5 Experiments
We conducted our experiments by preparing versions of
the test sentences in the form appropriate to each sys-
tem. We used a configuration of the XLE parser that ex-
pects sentences conforming to ordinary text conventions
to appear in a file separated by double line-feeds. A cer-
tain amount of effort was required to remove the part-of-
speech tags and labeled brackets of the WSJ corpus in a
way that restored the sentences to a standard English for-
mat (for example, to remove the space between wo and n?t
that remains when the POS tags are removed). Since the
PARC 700 treats proper names as multiword expressions,
we then augmented the input strings with XML markup
of the named entities. These are parsed by the grammar
as described in section 2. We used manual named entity
markup for this experiment because our intent is to mea-
sure parsing technology independent of either the time
or errors of an automatic named-entity extractor. How-
ever, in other experiments with an automatic finite-state
extractor, we have found that the time for named-entity
recognition is negligible (on the order of seconds across
the entire corpus) and makes relatively few errors, so that
the results reported here are good approximations of what
might be expected in more realistic situations.
As input to the Collins parser, we used the part-of-
speech tagged version of section 23 that was provided
with the parser. From this we extracted the 700 sentences
in the PARC 700. We then modified them to produce
named entity input so that the parses would match the
PARC 700. This was done by putting underscores be-
tween the parts of the named entity and changing the final
part of speech tag to the appropriate one (usually NNP)
if necessary. (The number of words indicated at the be-
ginning of the input string was also reduced accordingly.)
An example is shown in (1).
5The results of this conversion are available at
http://www2.parc.com/istl/groups/nltt/fsbank/
(1) Sen. NNP Christopher NNP Dodd NNP ??
Sen. Christopher Dodd NNP
After parsing, the underscores were converted to spaces
to match the PARC 700 predicates.
Before the final evaluation, 1/5 of the PARC 700 de-
pendency bank was randomly extracted as a heldout set.
This set was used to adjust the performance parameters of
the XLE system and the Collins parser so as to optimize
parsing speed without losing accuracy. For example, the
limit on the length of medial phrases was set to 20 words
for the XLE system (see Sec. 2), and a regularizer penalty
of 10 was found optimal for the `1 prior used in stochas-
tic disambiguation. For the Collins parser, a beam size
of 1000 was found to improve speed considerably at lit-
tle cost in accuracy. Furthermore, the np-bracketing flag
(npbflag) was set to 0 to produce an extended set of NP
levels for improved argument/adjunct distinction6. The fi-
nal evaluation was done on the remaining 560 examples.
Timing results are reported in seconds of CPU time7. POS
tagging of the input to the Collins parser took 6 seconds
and this was added to the timing result of the Collins
parser. Time spent for finite-state morphology and dictio-
nary lookup for XLE is part of the measure of its timing
performance. We did not include the time for dependency
extraction or stemming the Collins output.
Table 1 shows timing and accuracy results for the Re-
duced dependency set. The parser settings compared are
Model 3 of the Collins parser adjusted to beam size 1000,
and the Core and Complete versions of the XLE sys-
tem, differing in the size of the grammar?s constraint-
set. Clearly, both versions of the XLE system achieve a
significant reduction in error rate over the Collins parser
(12% for the core XLE system and 20% for the complete
system) at an increase in parsing time of a factor of only
1.49 for the core XLE system. The complete version gives
an overall improvement in F-score of 5% over the Collins
parser at a cost of a factor of 5 in parsing time.
Table 1: Timing and accuracy results for Collins parser
and Complete and Core versions of XLE system on Re-
duced version of PARC 700 dependency bank.
time prec. rec. F-score
LFG core 298.88 79.1 76.2 77.6
LFG complete 985.3 79.4 79.8 79.6
Collins 1000 199.6 78.3 71.2 74.6
6A beam size of 10000 as used in Collins (1999) improved
the F-score on the heldout set only by .1% at an increase of pars-
ing time by a factor of 3. Beam sizes lower than 1000 decreased
the heldout F-score significantly.
7All experiments were run on one CPU of a dual proces-
sor AMD Opteron 244 with 1.8 GHz and 4GB main memory.
Loading times are included in CPU times.
6 Conclusion
We presented some experiments that compare the accu-
racy and performance of two stochastic parsing systems,
the shallow Collins parser and the deep-grammar-based
XLE system. We measured the accuracy of both systems
against a gold standard derived from the PARC 700 de-
pendency bank, and also measured their processing times.
Contrary to conventional wisdom, we found that the shal-
low system was not substantially faster than the deep
parser operating on a core grammar, while the deep sys-
tem was significantly more accurate. Furthermore, ex-
tending the grammar base of the deep system results in
much better accuracy at a cost of a factor of 5 in speed.
Our experiment is comparable to recent work on read-
ing off Propbank-style (Kingsbury and Palmer, 2002)
predicate-argument relations from gold-standard tree-
bank trees and automatic parses of the Collins parser.
Gildea and Palmer (2002) report F-score results in the
55% range for argument and boundary recognition based
on automatic parses. From this perspective, the nearly
75% F-score that is achieved for our deterministic rewrit-
ing of Collins? trees into dependencies is remarkable,
even if the results are not directly comparable. Our scores
and Gildea and Palmer?s are both substantially lower than
the 90% typically cited for evaluations based on labeled
or unlabeled bracketing, suggesting that extracting se-
mantically relevant dependencies is a more difficult, but
we think more valuable, task.
References
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In Proceedings of COL-
ING2002, Workshop on Grammar Engineering and
Evaluation, pages 1?7.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
D. Crouch, C. Condoravdi, R. Stolle, T.H. King,
V. de Paiva, J. Everett, and D. Bobrow. 2002. Scal-
ability of redundancy detection in focused document
collections. In Proceedings of Scalable Natural Lan-
guage Understanding, Heidelberg.
Hal Daume and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL?02), Philadelphia, PA.
Anette Frank, Tracy H. King, Jonas Kuhn, and John
Maxwell. 1998. Optimality theory style constraint
ranking in large-scale LFG grammars. In Proceedings
of the Third LFG Conference.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochatic
unification-based grammars. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?02), Philadelphia, PA.
Dan Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL?02), Philadelphia.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, Rada
Mihalcea, Mihai Surdeanu, Ra?zvan Bunescu, Roxana
G??rju, Vasile Rus, and Paul Mora?rescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of the 39th
Annual Meeting and 10th Conference of the European
Chapter of the Asssociation for Computational Lin-
guistics (ACL?01), Toulouse, France.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the Work-
shop on ?Linguistically Interpreted Corpora? at the
10th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (LINC?03), Bu-
dapest, Hungary.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?02), Las Palmas, Spain.
John Maxwell and Ron Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
the 1st Conference of the North American Chapter of
the Association for Computational Linguistics (ANLP-
NAACL 2000), Seattle, WA.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the Human Language Technology Conference
(HLT?02), San Diego, CA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP-
1.
Stefan Riezler and Alexander Vasserman. 2004. Gradi-
ent feature testing and `1 regularization for maximum
entropy parsing. Submitted for publication.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting and 10th Conference of the Eu-
ropean Chapter of the Asssociation for Computational
Linguistics (ACL?01), Toulouse, France.
Parsing the Wall Street Journal using a Lexical-Functional Grammar and
Discriminative Estimation Techniques
Stefan Riezler Tracy H. King Ronald M. Kaplan
Palo Alto Research Center Palo Alto Research Center Palo Alto Research Center
Palo Alto, CA 94304 Palo Alto, CA 94304 Palo Alto, CA 94304
riezler@parc.com thking@parc.com kaplan@parc.com
Richard Crouch John T. Maxwell III Mark Johnson
Palo Alto Research Center Palo Alto Research Center Brown University
Palo Alto, CA 94304 Palo Alto, CA 94304 Providence, RI 02912
crouch@parc.com maxwell@parc.com mj@cs.brown.edu
Abstract
We present a stochastic parsing system
consisting of a Lexical-Functional Gram-
mar (LFG), a constraint-based parser and
a stochastic disambiguation model. We re-
port on the results of applying this sys-
tem to parsing the UPenn Wall Street
Journal (WSJ) treebank. The model com-
bines full and partial parsing techniques
to reach full grammar coverage on unseen
data. The treebank annotations are used
to provide partially labeled data for dis-
criminative statistical estimation using ex-
ponential models. Disambiguation perfor-
mance is evaluated by measuring matches
of predicate-argument relations on two
distinct test sets. On a gold standard of
manually annotated f-structures for a sub-
set of the WSJ treebank, this evaluation
reaches 79% F-score. An evaluation on a
gold standard of dependency relations for
Brown corpus data achieves 76% F-score.
1 Introduction
Statistical parsing using combined systems of hand-
coded linguistically fine-grained grammars and
stochastic disambiguation components has seen con-
siderable progress in recent years. However, such at-
tempts have so far been confined to a relatively small
scale for various reasons. Firstly, the rudimentary
character of functional annotations in standard tree-
banks has hindered the direct use of such data for
statistical estimation of linguistically fine-grained
statistical parsing systems. Rather, parameter esti-
mation for such models had to resort to unsupervised
techniques (Bouma et al, 2000; Riezler et al, 2000),
or training corpora tailored to the specific grammars
had to be created by parsing and manual disam-
biguation, resulting in relatively small training sets
of around 1,000 sentences (Johnson et al, 1999).
Furthermore, the effort involved in coding broad-
coverage grammars by hand has often led to the spe-
cialization of grammars to relatively small domains,
thus sacrificing grammar coverage (i.e. the percent-
age of sentences for which at least one analysis is
found) on free text. The approach presented in this
paper is a first attempt to scale up stochastic parsing
systems based on linguistically fine-grained hand-
coded grammars to the UPenn Wall Street Journal
(henceforth WSJ) treebank (Marcus et al, 1994).
The problem of grammar coverage, i.e. the fact
that not all sentences receive an analysis, is tack-
led in our approach by an extension of a full-
fledged Lexical-Functional Grammar (LFG) and a
constraint-based parser with partial parsing tech-
niques. In the absence of a complete parse, a so-
called ?FRAGMENT grammar? allows the input to be
analyzed as a sequence of well-formed chunks. The
set of fragment parses is then chosen on the basis
of a fewest-chunk method. With this combination of
full and partial parsing techniques we achieve 100%
grammar coverage on unseen data.
Another goal of this work is the best possible ex-
ploitation of the WSJ treebank for discriminative es-
timation of an exponential model on LFG parses. We
define discriminative or conditional criteria with re-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 271-278.
                         Proceedings of the 40th Annual Meeting of the Association for
CS 1: FRAGMENTS
Sadj[fin]
S[fin]
NP
D 
the
NPadj
AP[attr]
A
golden
NPzero
N
share
VPall[fin]
VP[pass,fin]
AUX[pass,fin]
was
VPv[pass]
V[pass]
scheduled
VPinf
VPinf?pos
PARTinf
to
VPall[base]
VPv[base]
V[base]
expire
PPcl
PP
P
at
NP
D
the
NPadj
NPzero
N
beginning
FRAGMENTS
TOKEN
of
"The golden share was scheduled to expire at the beginning of"
?schedule<NULL, [132:expire]>[11:share]?PRED
?share?PRED 
?golden<[11:share]>?PRED  [11:share]SUBJADEGREE positive , ADJUNCT?TYPE nominal, ATYPE attributive23ADJUNCT
unspecifiedGRAINNTYPE
DET?FORM  the _, DET?TYPE  defDETSPEC
CASE nom , NUM  sg, PERS   311
SUBJ
?expire<[11:share]>?PRED  [11:share]SUBJ
?at<[170:beginning]>?PRED
?beginning ?PRED 
GERUND +, GRAIN unspecifiedNTYPE
DET?FORM  the _, DET?TYPE  defDETSPEC
CASE acc, NUM  sg, PCASE   at, PERS   3170
OBJ
ADV?TYPE	  vpadv
 , PSEM   locative, PTYPE   sem164
ADJUNCT	
INF?FORM to , PASSIVE   ?, VTYPE  main132
XCOMP
MOOD indicative, TENSE pastTNS?ASP
PASSIVE +, STMT?TYPE decl, VTYPE main67
FIRST
ofTOKEN229FIRST3218REST3188
Figure 1: FRAGMENT c-/f-structure for The golden share was scheduled to expire at the beginning of
spect to the set of grammar parses consistent with
the treebank annotations. Such data can be gathered
by applying labels and brackets taken from the tree-
bank annotation to the parser input. The rudimen-
tary treebank annotations are thus used to provide
partially labeled data for discriminative estimation
of a probability model on linguistically fine-grained
parses.
Concerning empirical evaluation of disambigua-
tion performance, we feel that an evaluation measur-
ing matches of predicate-argument relations is more
appropriate for assessing the quality of our LFG-
based system than the standard measure of match-
ing labeled bracketing on section 23 of the WSJ
treebank. The first evaluation we present measures
matches of predicate-argument relations in LFG f-
structures (henceforth the LFG annotation scheme)
to a gold standard of manually annotated f-structures
for a representative subset of the WSJ treebank. The
evaluation measure counts the number of predicate-
argument relations in the f-structure of the parse
selected by the stochastic model that match those
in the gold standard annotation. Our parser plus
stochastic disambiguator achieves 79% F-score un-
der this evaluation regime.
Furthermore, we employ another metric which
maps predicate-argument relations in LFG f-
structures to the dependency relations (henceforth
the DR annotation scheme) proposed by Carroll et
al. (1999). Evaluation with this metric measures the
matches of dependency relations to Carroll et al?s
gold standard corpus. For a direct comparison of our
results with Carroll et al?s system, we computed an
F-score that does not distinguish different types of
dependency relations. Under this measure we obtain
76% F-score.
This paper is organized as follows. Section 2
describes the Lexical-Functional Grammar, the
constraint-based parser, and the robustness tech-
niques employed in this work. In section 3 we
present the details of the exponential model on LFG
parses and the discriminative statistical estimation
technique. Experimental results are reported in sec-
tion 4. A discussion of results is in section 5.
2 Robust Parsing using LFG
2.1 A Broad-Coverage LFG
The grammar used for this project was developed in
the ParGram project (Butt et al, 1999). It uses LFG
as a formalism, producing c(onstituent)-structures
(trees) and f(unctional)-structures (attribute value
matrices) as output. The c-structures encode con-
stituency. F-structures encode predicate-argument
relations and other grammatical information, e.g.,
number, tense. The XLE parser (Maxwell and Ka-
plan, 1993) was used to produce packed represen-
tations, specifying all possible grammar analyses of
the input.
The grammar has 314 rules with regular expres-
sion right-hand sides which compile into a collec-
tion of finite-state machines with a total of 8,759
states and 19,695 arcs. The grammar uses several
lexicons and two guessers: one guesser for words
recognized by the morphological analyzer but not
in the lexicons and one for those not recognized.
As such, most nouns, adjectives, and adverbs have
no explicit lexical entry. The main verb lexicon con-
tains 9,652 verb stems and 23,525 subcategorization
frame-verb stem entries; there are also lexicons for
adjectives and nouns with subcategorization frames
and for closed class items.
For estimation purposes using the WSJ treebank,
the grammar was modified to parse part of speech
tags and labeled bracketing. A stripped down ver-
sion of the WSJ treebank was created that used
only those POS tags and labeled brackets relevant
for determining grammatical relations. The WSJ la-
beled brackets are given LFG lexical entries which
constrain both the c-structure and the f-structure of
the parse. For example, the WSJ?s ADJP-PRD la-
bel must correspond to an AP in the c-structure and
an XCOMP in the f-structure. In this version of the
corpus, all WSJ labels with -SBJ are retained and
are restricted to phrases corresponding to SUBJ in
the LFG grammar; in addition, it contains NP under
VP (OBJ and OBJth in the LFG grammar), all -LGS
tags (OBL-AG), all -PRD tags (XCOMP), VP under
VP (XCOMP), SBAR- (COMP), and verb POS tags
under VP (V in the c-structure). For example, our
labeled bracketing of wsj 1305.mrg is [NP-SBJ His
credibility] is/VBZ also [PP-PRD on the line] in the
investment community.
Some mismatches between the WSJ labeled
bracketing and the LFG grammar remain. These
often arise when a given constituent fills a gram-
matical role in more than one clause. For exam-
ple, in wsj 1303.mrg Japan?s Daiwa Securities Co.
named Masahiro Dozen president., the noun phrase
Masahiro Dozen is labeled as an NP-SBJ. However,
the LFG grammar treats it as the OBJ of the ma-
trix clause. As a result, the labeled bracketed version
of this sentence does not receive a full parse, even
though its unlabeled, string-only counterpart is well-
formed. Some other bracketing mismatches remain,
usually the result of adjunct attachment. Such mis-
matches occur in part because, besides minor mod-
ifications to match the bracketing for special con-
structions, e.g., negated infinitives, the grammar was
not altered to mirror the idiosyncrasies of the WSJ
bracketing.
2.2 Robustness Techniques
To increase robustness, the standard grammar has
been augmented with a FRAGMENT grammar. This
grammar parses the sentence as well-formed chunks
specified by the grammar, in particular as Ss, NPs,
PPs, and VPs. These chunks have both c-structures
and f-structures corresponding to them. Any token
that cannot be parsed as one of these chunks is
parsed as a TOKEN chunk. The TOKENs are also
recorded in the c- and f-structures. The grammar has
a fewest-chunk method for determining the correct
parse. For example, if a string can be parsed as two
NPs and a VP or as one NP and an S, the NP-S
option is chosen. A sample FRAGMENT c-structure
and f-structure are shown in Fig. 1 for wsj 0231.mrg
(The golden share was scheduled to expire at the
beginning of), an incomplete sentence; the parser
builds one S chunk and then one TOKEN for the
stranded preposition.
A final capability of XLE that increases cov-
erage of the standard-plus-fragment grammar is a
SKIMMING technique. Skimming is used to avoid
timeouts and memory problems. When the amount
of time or memory spent on a sentence exceeds
a threshhold, XLE goes into skimming mode for
the constituents whose processing has not been
completed. When XLE skims these remaining con-
stituents, it does a bounded amount of work per sub-
tree. This guarantees that XLE finishes processing
a sentence in a polynomial amount of time. In pars-
ing section 23, 7.2% of the sentences were skimmed;
26.1% of these resulted in full parses, while 73.9%
were FRAGMENT parses.
The grammar coverage achieved 100% of section
23 as unseen unlabeled data: 74.7% as full parses,
25.3% FRAGMENT and/or SKIMMED parses.
3 Discriminative Statistical Estimation
from Partially Labeled Data
3.1 Exponential Models on LFG Parses
We employed the well-known family of exponential
models for stochastic disambiguation. In this paper
we are concerned with conditional exponential mod-
els of the form:
p?(x|y) = Z?(y)
?1e??f(x)
where X(y) is the set of parses for sentence y,
Z?(y) =
?
x?X(y) e
??f(x) is a normalizing con-
stant, ? = (?1, . . . , ?n) ? IRn is a vector of
log-parameters, f = (f1, . . . , fn) is a vector of
property-functions fi : X ? IR for i = 1, . . . , n
on the set of parses X , and ? ? f(x) is the vector dot
product
?n
i=1 ?ifi(x).
In our experiments, we used around 1000
complex property-functions comprising information
about c-structure, f-structure, and lexical elements
in parses, similar to the properties used in Johnson
et al (1999). For example, there are property func-
tions for c-structure nodes and c-structure subtrees,
indicating attachment preferences. High versus low
attachment is indicated by property functions count-
ing the number of recursively embedded phrases.
Other property functions are designed to refer to
f-structure attributes, which correspond to gram-
matical functions in LFG, or to atomic attribute-
value pairs in f-structures. More complex property
functions are designed to indicate, for example, the
branching behaviour of c-structures and the (non)-
parallelism of coordinations on both c-structure and
f-structure levels. Furthermore, properties refering
to lexical elements based on an auxiliary distribution
approach as presented in Riezler et al (2000) are
included in the model. Here tuples of head words,
argument words, and grammatical relations are ex-
tracted from the training sections of the WSJ, and
fed into a finite mixture model for clustering gram-
matical relations. The clustering model itself is then
used to yield smoothed probabilities as values for
property functions on head-argument-relation tuples
of LFG parses.
3.2 Discriminative Estimation
Discriminative estimation techniques have recently
received great attention in the statistical machine
learning community and have already been applied
to statistical parsing (Johnson et al, 1999; Collins,
2000; Collins and Duffy, 2001). In discriminative es-
timation, only the conditional relation of an analysis
given an example is considered relevant, whereas in
maximum likelihood estimation the joint probability
of the training data to best describe observations is
maximized. Since the discriminative task is kept in
mind during estimation, discriminative methods can
yield improved performance. In our case, discrimi-
native criteria cannot be defined directly with respect
to ?correct labels? or ?gold standard? parses since
the WSJ annotations are not sufficient to disam-
biguate the more complex LFG parses. However, in-
stead of retreating to unsupervised estimation tech-
niques or creating small LFG treebanks by hand, we
use the labeled bracketing of the WSJ training sec-
tions to guide discriminative estimation. That is, dis-
criminative criteria are defined with respect to the set
of parses consistent with the WSJ annotations.1
The objective function in our approach, denoted
by P (?), is the joint of the negative log-likelihood
?L(?) and a Gaussian regularization term ?G(?)
on the parameters ?. Let {(yj , zj)}mj=1 be a set of
training data, consisting of pairs of sentences y and
partial annotations z, let X(y, z) be the set of parses
for sentence y consistent with annotation z, and let
X(y) be the set of all parses produced by the gram-
mar for sentence y. Furthermore, let p[f ] denote the
expectation of function f under distribution p. Then
P (?) can be defined for a conditional exponential
model p?(z|y) as:
P (?) = ?L(?)?G(?)
= ? log
m?
j=1
p?(zj |yj) +
n?
i=1
?2i
2?2i
= ?
m?
j=1
log
?
X(yj ,zj)
e??f(x)
?
X(yj)
e??f(x)
+
n?
i=1
?2i
2?2i
= ?
m?
j=1
log
?
X(yj ,zj)
e??f(x)
+
m?
j=1
log
?
X(yj)
e??f(x) +
n?
i=1
?2i
2?2i
.
Intuitively, the goal of estimation is to find model pa-
1An earlier approach using partially labeled data for estimat-
ing stochastics parsers is Pereira and Schabes?s (1992) work on
training PCFG from partially bracketed data. Their approach
differs from the one we use here in that Pereira and Schabes
take an EM-based approach maximizing the joint likelihood of
the parses and strings of their training data, while we maximize
the conditional likelihood of the sets of parses given the corre-
sponding strings in a discriminative estimation setting.
rameters which make the two expectations in the last
equation equal, i.e. which adjust the model param-
eters to put all the weight on the parses consistent
with the annotations, modulo a penalty term from
the Gaussian prior for too large or too small weights.
Since a closed form solution for such parame-
ters is not available, numerical optimization meth-
ods have to be used. In our experiments, we applied
a conjugate gradient routine, yielding a fast converg-
ing optimization algorithm where at each iteration
the negative log-likelihood P (?) and the gradient
vector have to be evaluated.2 For our task the gra-
dient takes the form:
?P (?) =
?
?P (?)
??1
,
?P (?)
??2
, . . . ,
?P (?)
??n
?
, and
?P (?)
??i
= ?
m?
j=1
(
?
x?X(yj ,zj)
e??f(x)fi(x)
?
x?X(yj ,zj)
e??f(x)
?
?
x?X(yj)
e??f(x)fi(x)
?
x?X(yj)
e??f(x)
) +
?i
?2i
.
The derivatives in the gradient vector intuitively are
again just a difference of two expectations
?
m?
j=1
p?[fi|yj , zj ] +
m?
j=1
p?[fi|yj ] +
?i
?2i
.
Note also that this expression shares many common
terms with the likelihood function, suggesting an ef-
ficient implementation of the optimization routine.
4 Experimental Evaluation
4.1 Training
The basic training data for our experiments are sec-
tions 02-21 of the WSJ treebank. As a first step, all
sections were parsed, and the packed parse forests
unpacked and stored. For discriminative estimation,
this data set was restricted to sentences which re-
ceive a full parse (in contrast to a FRAGMENT or
SKIMMED parse) for both its partially labeled and
its unlabeled variant. Furthermore, only sentences
2An alternative numerical method would be a combination
of iterative scaling techniques with a conditional EM algorithm
(Jebara and Pentland, 1998). However, it has been shown exper-
imentally that conjugate gradient techniques can outperform it-
erative scaling techniques by far in running time (Minka, 2001).
which received at most 1,000 parses were used.
From this set, sentences of which a discriminative
learner cannot possibly take advantage, i.e. sen-
tences where the set of parses assigned to the par-
tially labeled string was not a proper subset of the
parses assigned the unlabeled string, were removed.
These successive selection steps resulted in a fi-
nal training set consisting of 10,000 sentences, each
with parses for partially labeled and unlabeled ver-
sions. Altogether there were 150,000 parses for par-
tially labeled input and 500,000 for unlabeled input.
For estimation, a simple property selection pro-
cedure was applied to the full set of around 1000
properties. This procedure is based on a frequency
cutoff on instantiations of properties for the parses
in the labeled training set. The result of this proce-
dure is a reduction of the property vector to about
half its size. Furthermore, a held-out data set was
created from section 24 of the WSJ treebank for ex-
perimental selection of the variance parameter of the
prior distribution. This set consists of 120 sentences
which received only full parses, out of which the
most plausible one was selected manually.
4.2 Testing
Two different sets of test data were used: (i) 700 sen-
tences randomly extracted from section 23 of the
WSJ treebank and given gold-standard f-structure
annotations according to our LFG scheme, and (ii)
500 sentences from the Brown corpus given gold
standard annotations by Carroll et al (1999) accord-
ing to their dependency relations (DR) scheme.3
Annotating the WSJ test set was bootstrapped
by parsing the test sentences using the LFG gram-
mar and also checking for consistency with the
Penn Treebank annotation. Starting from the (some-
times fragmentary) parser analyses and the Tree-
bank annotations, gold standard parses were created
by manual corrections and extensions of the LFG
parses. Manual corrections were necessary in about
half of the cases. The average sentence length of
the WSJ f-structure bank is 19.8 words; the average
number of predicate-argument relations in the gold-
standard f-structures is 31.2.
Performance on the LFG-annotated WSJ test set
3Both corpora are available online. The WSJ f-structure
bank at www.parc.com/istl/groups/nltt/fsbank/, and Carroll et
al.?s corpus at www.cogs.susx.ac.uk/lab/nlp/carroll/greval.html.
was measured using both the LFG and DR metrics,
thanks to an f-structure-to-DR annotation mapping.
Performance on the DR-annotated Brown test set
was only measured using the DR metric.
The LFG evaluation metric is based on the com-
parison of full f-structures, represented as triples
relation(predicate, argument). The predicate-
argument relations of the f-structure for one parse of
the sentence Meridian will pay a premium of $30.5
million to assume $2 billion in deposits. are shown
in Fig. 2.
number($:9, billion:17) number($:24, million:4)
detform(premium:3, a) mood(pay:0, indicative)
tense(pay:0, fut) adjunct(million:4, ?30.5?:28)
adjunct(premium:3, of:23) adjunct(billion:17, ?2?:19)
adjunct($:9, in:11) adjunct(pay:0, assume:7)
obj(pay:0, premium:3) stmttype(pay:0, decl)
subj(pay:0, ?Meridian?:5) obj(assume:7, $:9)
obj(of:23, $:24) subj(assume:7, pro:8)
obj(in:11, deposit:12) prontype(pro:8, null)
stmttype(assume:7, purpose)
Figure 2: LFG predicate-argument relation represen-
tation
The DR annotation for our example sentence, ob-
tained via a mapping from f-structures to Carroll et
al?s annotation scheme, is shown in Fig. 3.
(aux pay will) (subj pay Meridian )
(detmod premium a) (mod million 30.5)
(mod $ million) (mod of premium $)
(dobj pay premium ) (mod billion 2)
(mod $ billion) (mod in $ deposit)
(dobj assume $ ) (mod to pay assume)
Figure 3: Mapping to Carroll et al?s dependency-
relation representation
Superficially, the LFG and DR representations are
very similar. One difference between the annotation
schemes is that the LFG representation in general
specifies more relation tuples than the DR represen-
tation. Also, multiple occurences of the same lex-
ical item are indicated explicitly in the LFG rep-
resentation but not in the DR representation. The
main conceptual difference between the two an-
notation schemes is the fact that the DR scheme
crucially refers to phrase-structure properties and
word order as well as to grammatical relations in
the definition of dependency relations, whereas the
LFG scheme abstracts away from serialization and
phrase-structure. Facts like this can make a correct
mapping of LFG f-structures to DR relations prob-
lematic. Indeed, we believe that we still underesti-
mate by a few points because of DR mapping diffi-
culties. 4
4.3 Results
In our evaluation, we report F-scores for both types
of annotation, LFG and DR, and for three types
of parse selection, (i) lower bound: random choice
of a parse from the set of analyses (averaged over
10 runs), (ii) upper bound: selection of the parse
with the best F-score according to the annotation
scheme used, and (iii) stochastic: the parse selected
by the stochastic disambiguator. The error reduc-
tion row lists the reduction in error rate relative to
the upper and lower bounds obtained by the stochas-
tic disambiguation model. F-score is defined as 2 ?
precision? recall/(precision+ recall).
Table 1 gives results for 700 examples randomly
selected from section 23 of the WSJ treebank, using
both LFG and DR measures.
Table 1: Disambiguation results for 700 randomly
selected examples from section 23 of the WSJ tree-
bank using LFG and DR measures.
LFG DR
upper bound 84.1 80.7
stochastic 78.6 73.0
lower bound 75.5 68.8
error reduction 36 35
The effect of the quality of the parses on disam-
biguation performance can be illustrated by break-
ing down the F-scores according to whether the
parser yields full parses, FRAGMENT, SKIMMED, or
SKIMMED+FRAGMENT parses for the test sentences.
The percentages of test examples which belong to
the respective classes of quality are listed in the first
row of Table 2. F-scores broken down according to
classes of parse quality are recorded in the follow-
4See Carroll et al (1999) for more detail on the DR an-
notation scheme, and see Crouch et al (2002) for more de-
tail on the differences between the DR and the LFG annotation
schemes, as well as on the difficulties of the mapping from LFG
f-structures to DR annotations.
ing rows. The first column shows F-scores for all
parses in the test set, as in Table 1. The second col-
umn shows the best F-scores when restricting atten-
tion to examples which receive only full parses. The
third column reports F-scores for examples which
receive only non-full parses, i.e. FRAGMENT or
SKIMMED parses or SKIMMED+FRAGMENT parses.
Columns 4-6 break down non-full parses according
to examples which receive only FRAGMENT, only
SKIMMED, or only SKIMMED+FRAGMENT parses.
Results of the evaluation on Carroll et al?s Brown
test set are given in Table 3. Evaluation results for
the DR measure applied to the Brown corpus test set
broken down according to parse-quality are shown
in Table 2.
In Table 3 we show the DR measure along with an
evaluation measure which facilitates a direct com-
parison of our results to those of Carroll et al
(1999). Following Carroll et al (1999), we count
a dependency relation as correct if the gold stan-
dard has a relation with the same governor and de-
pendent but perhaps with a different relation-type.
This dependency-only (DO) measure thus does not
reflect mismatches between arguments and modi-
fiers in a small number of cases. Note that since
for the evaluation on the Brown corpus, no heldout
data were available to adjust the variance parame-
ter of a Bayesian model, we used a plain maximum-
likelihood model for disambiguation on this test set.
Table 3: Disambiguation results on 500 Brown cor-
pus examples using DO measure and DR measures.
DO DR
Carroll et al (1999) 75.1 -
upper bound 82.0 80.0
stochastic 76.1 74.0
lower bound 73.3 71.7
error reduction 32 33
5 Discussion
We have presented a first attempt at scaling up a
stochastic parsing system combining a hand-coded
linguistically fine-grained grammar and a stochas-
tic disambiguation model to the WSJ treebank.
Full grammar coverage is achieved by combining
specialized constraint-based parsing techniques for
LFG grammars with partial parsing techniques. Fur-
thermore, a maximal exploitation of treebank anno-
tations for estimating a distribution on fine-grained
LFG parses is achieved by letting grammar analyses
which are consistent with the WSJ labeled bracket-
ing define a gold standard set for discriminative es-
timation. The combined system trained on WSJ data
achieves full grammar coverage and disambiguation
performance of 79% F-score on WSJ data, and 76%
F-score on the Brown corpus test set.
While disambiguation performance of around
79% F-score on WSJ data seems promising, from
one perspective it only offers a 3% absolute im-
provement over a lower bound random baseline.
We think that the high lower bound measure high-
lights an important aspect of symbolic constraint-
based grammars (in contrast to treebank gram-
mars): the symbolic grammar already significantly
restricts/disambiguates the range of possible analy-
ses, giving the disambiguator a much narrower win-
dow in which to operate. As such, it is more appro-
priate to assess the disambiguator in terms of reduc-
tion in error rate (36% relative to the upper bound)
than in terms of absolute F-score. Both the DR and
LFG annotations broadly agree in their measure of
error reduction.
The lower reduction in error rate relative to the
upper bound for DR evaluation on the Brown corpus
can be attributed to a corpus effect that has also been
observed by Gildea (2001) for training and testing
PCFGs on the WSJ and Brown corpora.5
Breaking down results according to parse quality
shows that irrespective of evaluation measure and
corpus, around 4% overall performance is lost due
to non-full parses, i.e. FRAGMENT, or SKIMMED, or
SKIMMED+FRAGMENT parses.
Due to the lack of standard evaluation measures
and gold standards for predicate-argument match-
ing, a comparison of our results to other stochastic
parsing systems is difficult. To our knowledge, so
far the only direct point of comparison is the parser
of Carroll et al (1999) which is also evaluated on
Carroll et al?s test corpus. They report an F-score
5Gildea reports a decrease from 86.1%/86.6% re-
call/precision on labeled bracketing to 80.3%/81% when
going from training and testing on the WSJ to training on the
WSJ and testing on the Brown corpus.
Table 2: LFG F-scores for the 700 WSJ test examples and DR F-scores for the 500 Brown test examples
broken down according to parse quality.
WSJ-LFG all full non-full fragments skimmed skimmed+fragments
% of test set 100 74.7 25.3 20.4 1.4 3.4
upper bound 84.1 88.5 73.4 76.7 70.3 61.3
stochastic 78.6 82.5 69.0 72.4 66.6 56.2
lower bound 75.5 78.4 67.7 71.0 63.0 55.9
Brown-DR all full non-full fragments skimmed skimmed+fragments
% of test set 100 79.6 20.4 20.0 2.0 1.6
upper bound 80.0 84.5 65.4 65.4 56.0 53.5
stochastic 74.0 77.9 61.5 61.5 52.8 50.0
lower bound 71.1 74.8 59.2 59.1 51.2 48.9
of 75.1% for a DO evaluation that ignores predicate
labels, counting only dependencies. Under this mea-
sure, our system achieves 76.1% F-score.
References
Gosse Bouma, Gertjan von Noord, and Robert Malouf.
2000. Alpino: Wide-coverage computational analysis
of Dutch. In Proceedings of Computational Linguis-
tics in the Netherlands, Amsterdam, Netherlands.
Miriam Butt, Tracy King, Maria-Eugenia Nin?o, and
Fre?de?rique Segond. 1999. A Grammar Writer?s Cook-
book. Number 95 in CSLI Lecture Notes. CSLI Publi-
cations, Stanford, CA.
John Carroll, Guido Minnen, and Ted Briscoe. 1999.
Corpus annotation for parser evaluation. In Proceed-
ings of the EACL workshop on Linguistically Inter-
preted Corpora (LINC), Bergen, Norway.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14(NIPS?01), Van-
couver.
Michael Collins. 2000. Discriminative reranking for nat-
ural language processing. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML?00), Stanford, CA.
Richard Crouch, Ronald M. Kaplan, Tracy H. King, and
Stefan Riezler. 2002. A comparison of evaluation
metrics for a broad-coverage stochastic parser. In Pro-
ceedings of the ?Beyond PARSEVAL? Workshop at the
3rd International Conference on Language Resources
and Evaluation (LREC?02), Las Palmas, Spain.
Dan Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of 2001 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Pittsburgh, PA.
Tony Jebara and Alex Pentland. 1998. Maximum con-
ditional likelihood via bound maximization and the
CEM algorithm. In Advances in Neural Information
Processing Systems 11 (NIPS?98).
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?99), College Park, MD.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop.
John Maxwell and Ron Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Thomas Minka. 2001. Algorithms for maximum-
likelihood logistic regression. Department of Statis-
tics, Carnegie Mellon University.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics (ACL?92),
Newark, Delaware.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized Stochastic Modeling of
Constraint-Based Grammars using Log-Linear Mea-
sures and EM Training. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL?00), Hong Kong.
Entailment, Intensionality and Text Understanding
Cleo Condoravdi, Dick Crouch, Valeria de Paiva, Reinhard Stolle, Daniel G. Bobrow
PARC
3333 Coyote Hill Road
Palo Alto, CA, USA, 94304
rdc+@parc.com
Abstract
We argue that the detection of entailment and
contradiction relations between texts is a min-
imal metric for the evaluation of text under-
standing systems. Intensionality, which is
widespread in natural language, raises a number
of detection issues that cannot be brushed aside.
We describe a contexted clausal representation,
derived from approaches in formal semantics,
that permits an extended range of intensional
entailments and contradictions to be tractably
detected.
1 Introduction
What are the appropriate metrics for evaluating perfor-
mance in text understanding? There is probably no one
universal measure that suffices, leading to a collection
of metrics for evaluating different facets of text under-
standing. This paper makes the case for the inclusion of
one particular evaluation metric in this collection: namely
the detection of entailment and contradiction relations be-
tween texts / portions of texts.
Relations of entailment and contradiction are the key
data of semantics, as traditionally viewed as a branch of
linguistics. The ability to recognize such semantic rela-
tions is clearly not a sufficient criterion for language un-
derstanding: there is more to language understanding than
just being able to tell that one sentence follows from an-
other. But we would argue that it is a minimal, necessary
criterion. If you understand sentences (1) and (2), then
you can recognize that they are contradictory.
(1) No civilians were killed in the Najaf suicide bomb-
ing.
(2) Two civilians died in the Najaf suicide bombing.
Conversely, if you fail to recognize the contradiction, then
you cannot have understood (1) and (2).
In proposing an evaluation metric, the onus is on the
proposer to do a number of things. First, to show that the
metric measures something real and useful: in this case,
that entailment and contradiction detection (ECD) mea-
sures an important facet of language understanding, and
that it correlates with the ability to develop useful appli-
cations (section 2). Second, to indicate the range of tech-
nical challenges that the metric raises: section 3 empha-
sizes one of these ? the need to deal with intensional en-
tailments, and the wisdom of drawing on the large body
of relevant work in formal semantics in attempting to do
so. Third, to show that the metric is not impossibly diffi-
cult for current technologies to satisfy, so that it encour-
ages technological progress rather than stunting it: sec-
tion 4 discusses a prototype system (described more fully
in (Crouch et al, 2002)) to argue that, with current tech-
nology, ECD is a realistic though challenging metric.
2 Entailment and Contradiction Metrics
2.1 Theoretical Justification
The ability to recognize entailment and contradiction re-
lations is a consequence of language understanding, as
examples (1)?(2) show. But before concluding that en-
tailment and contradiction detection is a suitable evalua-
tion metric for text understanding, two cautionary points
should be addressed. First, it cannot be a sufficient met-
ric, since there is more to understanding than entailment
and contradiction, and we should ask what aspects of un-
derstanding it does not evaluate. Second, we need to be
reasonably sure that it is a necessary metric, and does
not measure some merely accidental manifestation of un-
derstanding. To give an analogy, clearing up spots is a
consequence of curing infections like measles; but clear-
ing spots is a poor metric, especially if success can be
achieved by bleaching spots off the skin or covering them
with make-up. A measles-cure metric should address the
presence of the infection, and not just its symptoms.
In terms of (in)sufficicency, we should note that under-
standing a text implies two abilities. (i) You can relate the
text to the world, and know what the world would have
to be like if the text were true or if you followed instruc-
tions contained in it.1 (ii) You can relate the text to other
texts, and can tell where texts agree or disagree in what
they say. Clearly, entailment and contradiction detection
directly measures only the second ability.
In terms of necessity, there are two points to be made.
The first is simply an appeal to intuition. Given a pre-
theoretical grasp of what language understanding is, the
ability to draw inferences and detect entailments and con-
tradictions just does seem to be part of understanding, and
not merely an accidental symptom of it. The second point
is more technical. Suppose we assume the standard ma-
chinery of modern logic, linking proof theory and model
theory. Then a proof-theoretic ability to detect entail-
ments and contradictions between expressions is intrin-
sically linked to a model-theoretic ability to relate those
expressions to (abstract) models of the world. In other
words, the abilities to relate texts to texts and texts to
the world are connected, and there are at least some ap-
proaches that show how success in the former feeds into
success in the latter.
The reference to logic and in particular to model the-
ory is deliberate. It provides an arsenal of tools for deal-
ing with entailment and contradiction, and there is also
a large body of work in formal semantics linking natural
language to these tools. One should at least consider mak-
ing use of these resources. However, it is important not
to characterize entailment and contradiction so narrowly
as to preclude other methods. There needs to be room for
probabilistic / Bayesian notions of inference, e.g. (Pearl,
1991), as well as attempting to use corpus based methods
to detect entailment / subsumption, e.g. the use of TF-
IDF by (Monz and de Rijke, 2001). That is, one can agree
on the importance of entailment and contradiction detec-
tion as an evaluation mertic, while disagreeing on the best
methods for achieving success.
2.2 Practical Justification
Even if we grant that entailment and contradiction detec-
tion (ECD) measures a core aspect of language under-
standing, it does not follow that it measures a useful as-
pect of understanding. However, we can point to at least
two application areas that directly demonstrate the utility
of the metric.
The first is an application that we are actually work-
1Knowing what the world would be like if the text were true
is not the same as being able to tell if the text is true. I know how
things would have to be for it to be true that ?There is no greatest
pair of prime numbers,   and   , such that   	
 .? But
I have no idea how to tell whether this is true or not.
ing on, concerning quality maintenance for document col-
lections. The Eureka system includes a large textual
database containing engineer-authored documents (tips)
about the repair and maintenance of printers and photo-
copiers. Over time, duplicate and inconsistent material
builds up, undermining the utility of the database to field
engineers. Human validators who maintain the quality
of the document collection would benefit from ECD text
analysis tools that locate points of contradiction and en-
tailment between different but related tips in the database.
A second application building fairly directly on ECD
would be yes-no question answering. Positive or negative
answers to yes-no questions can be characterized as those
that (respectively) entail or contradict a declarative form
of the query. Yes-no question answering would be useful
for autonomous systems that attempt to interpret and act
on information acquired from textual sources, rather than
merely pre-filtering it for human interpretation and action.
Despite its relevance to applications like the above, one
of the advantages of ECD is a degree of task neutrality.
Entailment and contradiction relations can be character-
ized independently of the use, if any, to which they are
put. Many other reasonable metrics for language under-
standing are not so task neutral. For example, in a dia-
logue system one measure of understanding would be suc-
cess in taking a (task) appropriate action or making an ap-
propriate response. However, it can be non-trivial to de-
termine how much of this success is due to language un-
derstanding and how much due to prior understanding of
the task: a good, highly constraining task model can over-
come many deficiencies in language processing.
Task neutrality is not the same as domain or genre neu-
trality. ECD can depend on domain knowledge. For ex-
ample, if I do not know that belladonna and deadly night-
shade name the same plant, I will not recognize that an
instruction to uproot belladonna entails an instruction to
uproot deadly nightshade. But this is arguably a failure of
botanical knowledge, not a lapse in language understand-
ing. We will return to the issue of domain dependence
later. However, there are many instances where ECD does
not depend on domain knowledge, e.g. (1)?(2) or (3)?(4)
(taken, with simplifications, from the Eureka corpus).
(3) Corrosion caused intermittent electrical contact.
(4) Corrosion prevented continuous electrical contact.
One does not need to be an electrician to recognize the po-
tential equivalence of (3) and (4); merely that intermittent
means non-continuous, so that causing something to be
intermittent can be the same as preventing it from being
continuous. And even in cases where domain knowledge
is required, ECD is still also reliant on linguistic knowl-
edge of this kind.
The success of methods for ECD may also depend on
genre. For newswire stories (Monz and de Rijke, 2001)
reports that TF-IDF performs well in detecting subsump-
tion (i.e. entailment) between texts. This may be a con-
sequence of the way that newswires convey generally
consistent information about particular individuals and
events: reference to the same entities is highly correlated
with subsumption in such a genre. The use of PLSA on
the Eureka corpus (Brants and Stolle, 2002) was less suc-
cessful: the corpus has less reference to concrete events
and individuals, and contains conflicting diagnoses and
recommendations for repair actions.
3 Intensionality
The detection of entailments and contradictions between
pieces of text raises a number of technical challenges, in-
cluding but not limited to the following. (a) Ambigu-
ity is ubiquitous in natural language, and poses an espe-
cial problem for text processing, where longer sentences
tend to increase grammatical ambiguity, and where it is
not generally possible to enter into clarificatory dialogues
with the text author. Ambiguity impacts ECD because se-
mantic relations may hold under some interpretations but
not under others. (b) Reference resolution in the broad
sense of determining that two texts talk about the same
things, rather than the narrower sense of intra-text pro-
noun resolution, is also crucial to ECD. Entailment and
contradiction relations presuppose shared subject matter,
and reference resolution plays a role in establishing this.
(c) World/domain knowledge, as we noted before, can be
involved in establishing entailment and contradiction re-
lations. (d) Representations that enable ECD must be de-
rived from texts. What should these representations be
like, and how should they be derived? At a bare minimum
some level of parsing to obtain predicate-argument struc-
tures seems necessary, but how much more than this is re-
quired?
We cannot address all of these issues in this paper, and
so will focus on the last one. In particular, we want to
point out that intensional constructions are commonplace
in text, and that simple first-order predicate-argument
structures are inadequate for detecting intensional entail-
ments and contradictions. Within the formal semantics
literature since at least Montague, the phenomena raised
by intensionality are well known and extensively studied,
though not always satisfactorily dealt with. Yet this has
been poorly reflected in computational work relating lan-
guage understanding and knowledge representation. For-
mal semanticists have the luxury of not having to per-
form automated inference on their semantic representa-
tions, and can trade tractability for expressiveness. Com-
putational applications on the other hand have traded ex-
pressiveness for tractability, either by trying to shoe-horn
everything into an ill-fitting first-order representation, or
by coding up special purpose and not easily generaliz-
able methods for dealing with particular intensional phe-
nomena in special tasks and domains. None of these ap-
proaches are particularly satisfactory for the task of de-
tecting substantial numbers of entailment and contradic-
tion relations between texts. A more balanced trade-off is
required, and we suggest at least one way in which ma-
chinery from formal semantics can be adapted to support
this.
3.1 Intensionality is pervasive
Intensionality extends beyond the conventional examples
of propositional attitudes (beliefs, desires etc) and formal
semanticists seeking unicorns. Any predication that has
a a proposition, fact or property denoting argument intro-
duces intensionality. Almost every lexical item that takes
a clausal or predicative argument should be seen as inten-
sional. As an anecdotal test of how common this is, in-
spection of 100 Eureka tips about the workaday world of
printer and copier repair showed that 453 out of 1586 sen-
tences contained at least one verb sub-categorizing for a
clausal argument. Some randomly selected examples of
intensional constructions are given in (5).
(5) a. When the rods are removed and replaced it is
very easy to hit the glass tab and break it off.
b. The weight of the ejected sets is not sufficient to
keep the exit switch depressed.
c. This is a workaround but also disables the ability
to use the duplex tray after pressing the ?Inter-
rupt? button, which should be explained to the
customer.
d. Machines using the defective toner may require
repair or replacement of the Cleaner Assembly.
Nor is intensionality confined to lexical items taking
clausal or predicative arguments, as sentences (3) and (4)
demonstrate. Prevention and causation (of central im-
portance within the Eureka domain) are inherently inten-
sional notions To say that ?A prevented B? is to say that
there was an occurrence of A and no occurrence of B, but
that had A not occurred B would have occurred. Simi-
larly, to say that ?A caused B? is to say that there was an
occurrence of both A and B, but that had there been no oc-
currence of A there would have been no occurence of B.
Both refer to things or events materialized in one context
but not in another. It is plain that we cannot give a seman-
tic analysis for (6a) along the lines of (6b)
(6) a. Corrosion prevented continuous contact.
b.  
	Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 31?36,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Local Textual Inference: can it be defined or circumscribed?
Annie Zaenen
Palo Alto Research Center
3333, Coyote Hill Road
Palo Alto, CA 94304
zaenen@parc.com
Lauri Karttunen
Palo Alto Research Center
3333, Coyote Hill Road
Palo Alto, CA 94304
karttunen@parc.com
Richard Crouch
Palo Alto Research Center
3333, Coyote Hill Road
Palo Alto, CA 94304
crouch@parc.com
Abstract
This paper argues that local textual in-
ferences come in three well-defined vari-
eties (entailments, conventional implica-
tures/presuppositions, and conversational
implicatures) and one less clearly defined
one, generally available world knowledge.
Based on this taxonomy, it discusses some
of the examples in the PASCAL text suite
and shows that these examples do not fall
into any of them. It proposes to enlarge the
test suite with examples that are more di-
rectly related to the inference patterns dis-
cussed.
1 Introduction
The PASCAL initiative on ?textual entailment? had
the excellent idea of proposing a competition testing
NLP systems on their ability to understand language
separate from the ability to cope with world knowl-
edge. This is obviously a welcome endeavor: NLP
systems cannot be held responsible for knowledge
of what goes on in the world but no NLP system can
claim to ?understand? language if it can?t cope with
textual inferences. The task also shies away from
creative metaphorical or metonymic use of language
and makes the assumption that referential assign-
ments remain constant for entities that are described
in the same way. These all seem good features of the
proposal as it stands.
Looking at the challenge as it was put before the
community, however, we feel that it might be useful
to try to circumscribe more precisely what exactly
should count as linguistic knowledge. In this paper
we make a stab at this in the hope of getting a discus-
sion going. For reasons that will become clear, we
prefer to talk about TEXTUAL INFERENCES rather
than about textual entailments when referring to the
general enterprise. We first explicitate what we think
should be covered by the term textual inferences, we
then look at the PASCAL development suite in the
light of our discussion and we conclude with a short
proposal for extensions to the test suite.
Before even starting at this, a point of clarification
needs to be made: the correspondence of a linguis-
tic object to an object in the real world goes beyond
what can be learned from the text itself. When some-
body says or writes The earth is flat or The king of
France is bald because (s)he is a liar or ill-informed,
nothing in these linguistic expressions in themselves
alerts us to the fact that they do not correspond to sit-
uations in the real world (we leave texts in which the
author signals consciously or unconsiously that he is
lying or fibbing out of consideration here.) What the
text does is give us information about the stance its
author takes vis-a`-vis the events or states described.
It is thus useful to distinguish between two ingre-
dients that go into determining the truth value of an
utterance, one is the trustworthiness of the utterer
and the other is the stance of the utterer vis-a`-vis
the truth of the content. The latter we will call the
veridicity of the content. When we talk about tex-
tual inferences we are only interested in veridicity
not in the truth which lies beyond what can be in-
ferred from texts. Or, maybe more realistically, we
assume a trustworthy author so that veridical state-
ments are also true.
31
2 Varieties of local textual inferences
Under this assumption of trustworthiness, semantics
and pragmatics as practiced by philosophers and lin-
guists can give us some insights that are of practical
relevance. Work done in the last century has led re-
searchers to distinguish between entailments, con-
ventional implicatures and conversational implica-
tures. We describe these three classes of inferences
and illustrate why the distinctions are important for
NLP.
2.1 Entailments
The most uncontroversional textual inferences are
those that can be made on the basis of what is as-
serted in a text. If the author makes the statement
that Tony Hall arrived in Baghdad on Sunday night,
then we can conclude that Tony Hall was in Bagh-
dad on Sunday night (keeping referring expressions
constant, as proposed in the PASCAL task). The sec-
ond sentence is true when the first is true (assum-
ing we are talking about the same Tony Hall, the
same Baghdad and the same Sunday) just by virtue
of what the words mean.
In simple examples such as that in (1)
(1) Bill murdered John.
Bill killed John.
one can go to a resource such as WordNet, look up
murder, discover that it means kill with some fur-
ther conditions. ?Ontologies? or thesauruses typi-
cally order terms in a hierarchy that encodes a re-
lation from less specific at the top of the hierarchy
to more specific at the bottom. In simple clauses
the replacement of a more specific term with a less
specific one, ensures an upward monotonic relation
between these sentences. As is well known this re-
lation is inversed when the sentences are negated.1
(2) Bill didn?t murder John.
does not entail Bill didn?t kill John.
but
(3) Bill didn?t kill John.
does entail Bill didn?t murder John.
Monotonicity relations also hold when adjectival
modification is introduced as in (4)
1A sentence is downward monotonic iff it remains true when
it is narrowed. A sentence is upward monotonic when it remains
true when it is broadened.
(4) Ames was a clever spy.
entails Ames was a spy.
Again negation reverses the entailment:
(5) Ames wasn?t a spy.
entails Ames wasn?t a clever spy.
Quantifiers, easily among the most intensively
studied lexical items, also exhibit upward or down-
ward monotonicity.2 To give just one example:
(6) All companies have to file annual reports.
entails All Fortune 500 companies have to file
annual reports.
but
(7) All companies have to file annual reports.
does not entail All companies have to file an-
nual reports to the SEC.
The fact that there are both upwards monotonic
and downwards monotonic expressions means that
simple matching on an inclusion of relevant mate-
rial cannot work as a technique to detect entailments.
Upward monotone expressions preserve truth by
leaving out material whereas downward monotone
expressions don?t: adding material to them can be
truth preserving.3
Apart from a more specific/less specific relation,
lexical items can establish a part-subpart relation be-
tween the events they describe. If we followed the
first sentence in (1) by
(8) John died.
we would still have a lexical inference. In this case
one in which the event described in the second sen-
tence is a subpart of the event described in the first.
The investigation of entailments leads one to dis-
tinguish several types of lexical items that have pre-
dictable effects on meaning that can be exploited to
discover sentences that are inferentially related (by
real entailments in this case). Other examples are
scope bearing elements (an aspect of meaning that
often leads to ambiguities which are not always eas-
ily perceived) and perception reports.
2A quantifier Q is downward monotonic with respect to its
restrictor ? iff ((Q ?) ?) remains true when the ? is narrowed,
e.g. from companies to Fortune 500 companies. A quantifier Q
is upward monotonic with respect to its scope ? iff ((Q ?) ?)
remains true when ? is broadened, e.g. from have to file reports
to the SCE to just have to file reports.
3Dagan and Glickman (2004) explore inferencing by syn-
tactic pattern matching techniques but consider only upward
monotonic expressions. Their proposal ensures loss of recall
on downward monotonic expressions.
32
Two types of relations deserve special mention
here because they are pervasive and they are at the
borderline between linguistic and world knowledge:
temporal relations and spatial relations. Whether
knowing that Tuesday follows Monday or that there
are leap years and non-leap years is linguistic knowl-
edge or world knowledge might not be totally clear
but it is clear that one wants this information to be
part of what textual entailment can draw upon. The
consequences in a Eucledian space of the place and
movement of objects are similar. There is a rich set
of entailment relations that builds on these temporal
and spatial notions.
2.2 Conventional Implicatures4
Apart from making assertions, however, an author
will often ?conventionally implicate? certain things.
We use here the term conventional implicature for
what has been called by that name or labeled as (se-
mantic) presupposition. Some of us have argued
elsewhere there is no need for a distinction between
these two notions (Karttunen and Peters, 1979) and
that presupposition is a less felicitous term because
it tends to be confused with ?old information?.
Traditionally these implications are not consid-
ered to be part of what makes the sentence true, but
the author is COMMITTED to them and we consider
them part of what textual inferences should be based
on. We take this position because we think it is rea-
sonable, for IE tasks, to assume that material that is
conventionally implicated can be used in the same
way as assertions, for instance, to provide answers
to questions. When somebody says Bill acknowl-
edges that the earth is round, we know something
about the author?s as well as Bill?s beliefs in the mat-
ter, namely that the author is committed to the belief
that the earth is round.
If all conventionally implied material were also
discourse old information, this might not matter very
much as the same information would be available
elsewhere in the text, but often conventionally im-
plied material is new information that is presented
as not being under discussion. Conventional impli-
catures are a rich source of information for IE tasks
because the material presented in them is supposed
4For more on conventional implicatures, see e.g. Karttunen
and Peters (1979) and Potts (2005)
to be non-controversial. In newspapers and other in-
formation sources they are a favorite way to distin-
guish background knowledge, that the reader might
have or not, without confusing it with what is news-
worthy in the report at hand. A very common ex-
ample of this, exploited in the PASCAL test suite, is
the use of appositives. illustrated in the following
example:
(9) The New York Times reported that Hanssen,
who sold FBI secrets to the Russians, could face
the death penalty.
Did Hanssen sell FBI reports to the Russians?
YES
From the perspective of IE tasks, the way conven-
tional implicatures behave under negation is one rea-
son to pay close attention to them. The following
examples illustrate this:
(10) Kerry realized that Bush was right.
Bush was right.
(11) Kerry didn?t realize that Bush was right.
Bush was right.
Other types of embedded clauses that are conven-
tionally implicated are temporal adverbials (except
those introduced by before or until. Other types of
material that can introduce a conventional implica-
ture are adverbial expressions such as evidently and
simple adverbs such as again or still.
It is important to point out that the syntactic struc-
ture doesn?t guide the interpretation here. Consider
the following contrast:
(12) As the press reported, Ames was a successful
spy.
conventionally implicates that Ames was a success-
ful spy, but
(13) According to the press, Ames was a successful
spy.
does not.
2.3 Conversational Implicatures5
Authors can be held responsible for more than just
assertions and conventional implicatures. Conversa-
tional implicatures are another type of author com-
mitment. A conversational implicature rests on the
assumption that, in absence of evidence to the con-
trary, a collaborative author will say as much as she
5For more on conversational implicatures, see e.g. Grice
(1989) and Horn (2003)
33
knows. So if Sue says that she has four children,
we tend to conclude that she has no more than four.
This type of implicature can be destroyed without
any contradiction arising: He not only ate some of
the cake, he ate all of it. Within the context of a tex-
tual inference task such as that defined in the PAS-
CAL initiative, it is clear that inferences based on
conversational implicatures might be wrong: PAS-
CAL doesn?t give the context. In a more developed
type of inference task, a distinction should be made
between this type of inference and the ones we dis-
cussed earlier, but when inferencing is reduced to
one sentence it seems more reasonable to take gen-
eralized conversational implicatures into account as
bona fide cases of inferences (except of course if
they are cancelled in the sentence itself, as in the
example above).
(14) I had the time to read your paper.
conversationally implies that I read your paper. But
it could be followed by but I decided to go play ten-
nis instead.
(15) Some soldiers were killed.
conversationally implies Not all soldiers were killed.
But it could be cancelled by In fact we fear that all
of them are dead.
(16) He certainly has three children.
conversationally implies He doesn?t have more than
three children but it could be followed by In fact he
has five, three daughters and two sons.
Apart from the general conversational implica-
tures, implicatures can also arise by virtue of some-
thing being said or not said in a particular context. If
in a letter of recommendation, one praises the can-
didate?s handwriting without saying anything about
his intellectual abilities, this allows the reader to
draw some conclusions. We assume here that this
type of inference is not part of the PASCAL task, as
too little context is given for it to be reliably calcu-
lated.
One might agree with the analysis of various
sources of author commitment given above but be
of the opinion that it doesn?t matter because, given
enough data, it will come out in the statistical wash.
We doubt, however, that this will happen any time
soon without some help: the semantic distinctions
are rather subtle and knowing about them will help
develop adequate features for statistical training.
It might also be thought that the generalizations
that we need here can be reduced to syntactic dis-
tinctions. We don?t have the space to show in great
detail that this is not the case but some reflection
on and experimentation with the examples given
throughout this paper will convince the reader that
this is not the cases. For instance, if one replaces the
adjective clever with the equally good adjective al-
leged in (4) above, the entailment relation between
the sentences doesn?t hold anymore. Substituting
show for realize in (11) has the same effect.
2.4 Some world knowledge?
In our mind this exhausts the ways in which an au-
thor can be held responsible for her writings on the
basis of text internal elements. Textual inferences
are based on textual material that is either an en-
tailment of what is explicitly asserted, or material
that conventionally or conversationally implied by
the author. These inferences can be made solely on
the basis of the way the meaning of the words and
construction she uses are related to other words and
constructions in the language. But even in a task that
tries to separate out linguistic knowledge from world
knowledge, it is not possible to avoid the latter com-
pletely. There is world knowledge that underlies just
about everything we say or write: the societies we
live in use a common view of time to describe events
and rely on the assumptions of Euclidean geometry,
leading to shared calendars and measurement sys-
tems. It would be impossible to separate these from
linguistic knowledge. Then there is knowledge that
is commonly available and static, e.g. that Baghdad
is in Iraq. It seems pointless to us to exclude the
appeal to such knowledge from the test suite but it
would be good to define it more explicitly.
3 The PASCAL development suite.
We now discuss some of the PASCAL development
set examples in the light of the discussion above and
explain why we think some of them do not belong
in a textual inference task. First a number of PAS-
CAL examples are based on spelling variants or even
spelling mistakes. While it is clear that coping with
this type of situation is important for NLP applica-
tions we think they do not belong in a textual infer-
ence test bed. We first discuss a couple of examples
34
that we think should not have been in the test suite
and then some that do not confirm to our view on
inferencing but which might belong in a textual in-
ference test suite.
3.1 Errors?
A problem arises with an example like the follow-
ing:
(17) A farmer who was in contact with cows suffer-
ing from BSE ? the so-called mad cow disease
? has died from what is regarded as the human
form of the disease.
Bovine spongiform encephalopathy is another
name for the ?mad cow disease?.
TRUE
If one googles BSE, one finds that it is an abbre-
viation that can stand for many things, including
the Bombay, Bulgarian, Baku or Bahrain Stock Ex-
change, Breast Self-Examination, and Brain Sur-
face Extractor. To select the right alternative, one
needs the knowledge that ?bovine spongiform en-
cephalopathy? is a name of a disease and the other
competing BSE expansions are not.
The authors of the PASCAL test suite don?t seem
to allow for as much world knowledge when they
mark the following relation as FALSE.
(18) ?I just hope I don?t become so blissful I be-
come boring? ? Nirvana leader Kurt Cobain
said, giving meaning to his ?Teen Spirit? coda,
a denial.
?Smells Like Teen Spirit? is a song by Nirvana.
FALSE
Apparently, it is NOT OK to know that the Nirvana
song ?Smells like Teen Spirit? is often referred to as
?Teen Spirit?. But why should we then know that
bovine spongiform encephalopathy is a disease?
The test suite also contains examples that can only
be classified as plain errors. A couple of examples
are the following:
(19) Green cards are becoming more difficult to ob-
tain.
Green card is now difficult to receive.
TRUE
Something that is becoming more difficult can still
be easy, if it starts out that way.
(20) Hippos do come into conflict with people quite
often.
Hippopotamus attacks human.
TRUE
For somebody who knows a lot about hippos it might
be reasonable to assume that a conflict is necessarily
an attack but in general there is no inference: conflict
is the less general term and attack the more specific
one.
(21) A statement said to be from al Qaida claimed
the terror group had killed one American and
kidnapped another in Riyadh.
A U.S. citizen working in Riyadh has been kid-
napped.
TRUE
This seems betray a rather implausible belief in the
claims of al Qaida and while we are assuming that
the author of the text is trustworthy, this assumption
does not extend to the sources he invokes. In this
case especially, the use of claim can be construed as
indication the doubt of the author about the veracity
of what the source says.
(22) Wal-Mart is being sued by a number of its
female employees who claim they were kept
out of jobs in management because they were
women.
Wal-Mart is sued for sexual discrimination.
TRUE
A minute of reflection will make clear that here the
relation between the two sentences involves quite a
bit of specialized legal knowledge and goes beyond
textual inferencing. How is sexual discrimination
different from sexual harassment?
(23) South Korean?s deputy foreign minister says
his country won?t change its plan to send 3000
soldiers to Iraq.
South Korea continues to send troops.
TRUE
We assume that in context the second sentence
means that South Korea continues to plan to send
troops but normally continue does not mean con-
tinue to plan and the first sentence certainly doesn?t
imply that South Korea has already sent troops. Here
the way the test suite has been put together leads
to odd results. A headline is paired up with a full
sentence. Headlines are not meant to be understood
completely out of context and it would be prudent to
use them sparingly in inference tasks of the sort pro-
posed here. We discuss other consequences of the
way the test suite was constructed in the next sub-
section with examples that to our mind need some
kind of accommodation.
35
3.2 Not a textual inference as such but . . .
There are a couple of examples such as the following
in the test suite:
(24) The White House failed to act on the domes-
tic threat from al Qaida prior to September 11,
2001.
White House ignored the threat of attack.
TRUE
Here there is no entailment either way and surely
fail to act is not a synonym of ignore. The examples
are due to the way the PASCAL test suite was put to-
gether. It was evidently at least in part developed by
finding snippets of text that refer to the same event
in different news sources; this is a fertile method for
finding inferences but it will lead to the inclusion of
some material that mixes factual description and var-
ious APPRECIATIONS of the described facts. For in-
stance in (24) above, two different authors described
what the White house did, putting a different spin
on it. While the fact described in both cases was
the same, the appreciations that the two renderings
give, while both negative, are not equivalent. But
although there is no legitimate inference for the sen-
tences as a whole, they both entail that the White
House did not act. Here the test suite is the victim of
its self imposed constraints, namely that the relation
has to be established between two sentences found
in ?real? text. We propose to give up this constraint.
Another maybe simpler illustration of the same
problem is (25):
(25) The report catalogues 10 missed opportunities.
The report lists 10 missed opportunities.
Although catalogue and list do not have the same
meaning, they may in some cases be used inter-
changeably because, again, there is a common en-
tailment:
(26) According to the report, there were 10 missed
opportunities.
One can conceive of a thesaurus where catalogue
and list would have a low level common hypernym
(in WordNet they don?t) or a statistically inferred
word class that would make the common entailment
explicit, but that relation should not be confused
with an inference between the two sentences in (25).
4 A proposal for some refinements
As the discussion above has shown, the way the test
suite was put together leads sometimes to the in-
clusion of material that should not be there given
the definition of the task. Most of the data that
form the basis of PASCAL are extracted from differ-
ent newspaper articles about the same event, often
from the same newswire. This means that the infor-
mation packaging is very similar, reducing the con-
structional and lexical range that can be used to ex-
press a same idea. This situation will not pertain in
the more general setting of question answering and
many types of paraphrases or inferences that would
be useful for question answering in general will not
be found or will be very rare in PASCAL-like suites.
We would propose to augment the types of pairs
that one can get through the PASCAL extraction tech-
niques with some that take the type of relations that
we have discussed explicitly into account. It can be
objected that this introduces a new level of artificial-
ity by allowing made-up sentences but the separa-
tion of world knowledge from linguistic knowledge
is in any case artificial. But it is necessary because
we will not be able to solve the inferencing problem
without slicing the task into manageable pieces.
Acknowledgments
This article was supported in part by the Advanced
Research and Development Agency (ARDA) within
the program for Advanced Question Answering for
Intelligence (AQUAINT). Thanks to all the members
of PARC?s AQUAINT team.
References
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variablity. In Learning Methods for Text Understand-
ing and Mining, Grenoble, January.
Paul H. Grice. 1989. Studies in the Way of Words. Har-
vard University, Cambridge, MA.
Larry Horn. 2003. Implicature. In Horn and Ward, edi-
tors, Handbook of Pragmatics. Blackwell, Oxford.
Lauri Karttunen and Stanley Peters. 1979. Conventional
implicature. In Choon-Kyu Oh and David A. Dinneen,
editors, Syntax and Semantics, Volume 11: Presuppo-
sition, pages 1?56. Academic Press, New York.
Christopher Potts. 2005. The Logic of Conventional Im-
plicatures. Oxford Studies in Theoretical Linguistics.
Oxford University Press, Oxford.
36
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 16?21,
Prague, June 2007. c?2007 Association for Computational Linguistics
Precision-focused Textual Inference
D. G. Bobrow, C. Condoravdi, R. Crouch, V. de Paiva, L. Karttunen, T. H. King, R. Nairn, L. Price, A. Zaenen
Palo Alto Research Center
Abstract
This paper describes our system as used in
the RTE3 task. The system maps premise and
hypothesis pairs into an abstract knowledge
representation (AKR) and then performs en-
tailment and contradiction detection (ECD)
on the resulting AKRs. Two versions of ECD
were used in RTE3, one with strict ECD and
one with looser ECD.
1 Introduction
In the RTE textual entailment challenge, one is given
a source text T and a hypothesis H, and the task is to
decide whether H can be inferred from T. Our sys-
tem interprets inference in a strict way. Given the
knowledge of the language embedded in the system,
does the hypothesis logically follow from the infor-
mation embedded in the text? Thus we are empha-
sizing precision, particularly in question-answering.
This was reflected in our results in the RTE3 chal-
lenge. We responded correctly with YES to relatively
few of the examples, but on the QA-type examples,
we achieved 90-95% average precision.
The methodology employed is to use the linguis-
tic information to map T and H onto a logical form in
AKR, our Abstract Knowledge Representation. The
AKR is designed to capture the propositions the au-
thor of a statement is committed to. For the sake of
ECD, the representation of T may include elements
that are not directly expressed in the text. For ex-
ample, in the AKR of John bought a car includes the
fact that the car was sold. The AKR of John forgot to
buy milk includes the fact that John did not buy milk.
Our reasoning algorithm tries to determine whether
the AKR of H is subsumed by the AKR of T and detect
cases when they are in conflict.
The Entailment and Contradiction Detection
(ECD) algorithm makes a distinction that is not part
of the basic RTE challenge. If T entails the negation
of H, we answer NO (Contradiction). On the other
Process Output
Text-Breaking Delimited sentences
Named-entity recognition Type-marked Entities
Morphological Analysis Word stems plus features
LFG Parsing Functional Structure
Semantic processing Scope, Predicate-
argument structure
AKR rules Conceptual, Contextual,
Temporal Structure
Figure 1: The processing pipeline: processes with
their ambiguity-enabled packed outputs
hand, if there is no direct entailment we answer UN-
KNOWN. We do not try to construct a likely scenario
that would link T and H. Nor have we tried to col-
lect data on phrases that would tend to indicate such
likely associations between T and H. That approach
is clearly very useful (e.g. (Hickl et al, 2006)), and
could be used as a backup strategy with our more
formal entailment approach. We have chosen to fo-
cus on strict structural and lexical entailments.
This paper describes the processing pipeline for
mapping to AKR, the ECD algorithm, the challenges
we faced in processing the RTE data and a summary
of our results on RTE3.
2 Process Pipeline
Figure 1 shows the processing pipeline for mapping
texts to AKR. The input is a text of one or more
sentences.
All components of the system are ?ambiguity en-
abled? (Maxwell and Kaplan, 1991). This allows
each component to accept ambiguous input in a
?packed? format, process it without unpacking the
ambiguities, and then pass packed input to the next
stage. The syntactic component, LFG Parsing, also
has a stochastic disambiguation system which al-
lows us to pass the n-best on to the semantics (Rie-
zler et al, 2002); for the RTE3 challenge, we used
16
n=50.
The parser takes the output of the morphology
(i.e. a series of lemmata with their tags) and pro-
duces a tree (constituent-structure) and a depen-
dency structure (functional-structure) represented as
an attribute-value matrix. The functional-structure
is of primary importance for the semantics and
AKR. In particular, it encodes predicate-argument
relations, including long-distance dependencies, and
provides other syntactic features (e.g. number, tense,
noun type).
The output of the syntax is input for the seman-
tics that is produced by an ambiguity enabled packed
rewriting system. The semantics is described in de-
tail in (Crouch and King, 2006). Semantic process-
ing assigns scope to scope-bearing elements such as
negation and normalizes the output of the syntax.
This normalization includes reformulating syntactic
passives as actives (e.g. The cake was eaten by Mary.
/ Mary ate the cake.), resolving many null pronouns
(e.g. Laughing, John entered the room / Johni laugh-
ing, Johni entered the room.), and canonicalizing
measure phrases, comparatives, and dates. More
complex normalizations involve converting nominal
deverbals into the equivalent verbal form, identify-
ing arguments of the verb from the arguments of
the nominal (Gurevich et al, 2006). For example,
the semantic representation of Iraq?s destruction of
its WMD is similar to the representation of Iraq de-
stroyed its WMD.
The final main task of the semantics rules is to
convert words into concepts and syntactic grammat-
ical functions into roles. The mapping onto concepts
uses WordNet (Fellbaum, 1998) to map words into
lists of synsets. The named entity types provided by
the morphology and syntax are used to create more
accurate mapping of proper nouns since these are
not systematically represented in WordNet. The se-
mantic rules use the grammatical function subcat-
egorization information from the verb and the role
information found in extended VerbNet (Kipper et
al., 2000) to map syntactic subjects, objects, and
obliques into more abstract thematic roles such as
Agent, Theme, and Goal (Crouch and King, 2005).
This mapping into thematic-style roles allows the
system to correctly align the arguments in pairs like
(1) and (2), something which is impossible using just
syntactic functions. In the first, the object and sub-
ject have a common thematic role in the alternation
between transitive and intransitive; while in the sec-
ond, the common role is shared by the subjects.
(1) John broke the vasesyn:object,sem:patient.
The vasesyn:subject,sem:patient broke.
(2) Johnsyn:subject,sem:agent ate the cake.
Johnsyn:subject,sem:agent ate.
The goal of these semantic normalizations is to
abstract away from the syntactic representation so
that sentences with similar meaning have similar se-
mantic representations. However, the semantics is
still fundamentally a linguistic level of representa-
tion; further abstraction towards the meaning is done
in the mapping from semantics to AKR. The AKR
is the level of representation that is used to deter-
mine entailment and contradiction in our RTE3 sys-
tem. A preliminary description of its logic was pro-
vided in (Bobrow et al, 2005). The AKR mapping
converts grammatical tense and temporal modifiers
into temporal relations, identifies anaphoric refer-
ents and makes explicit the implied relation between
complement clauses and the main verb (e.g. for
manage, fail) (Nairn et al, 2006). AKR also deals
with standard phrases that are equivalent to simple
vocabulary terms. For example, take a flight to New
York is equivalent to fly to New York. These uses
of ?light? verbs (e.g. take, give) are not included
in synonyms found in WordNet. Another class of
phrasal synonyms involve inchoatives (e.g. take a
turn for the worse/worsen). We included a special
set of transformation rules for phrasal synonyms:
some of the rules are part of the mapping from se-
mantics to AKR while others are part of the ECD
module. The mapping to AKR is done using the same
ambiguity-enabled ordered rewriting system that the
semantics uses, allowing the AKR mapping system
to efficiently process the packed output of the se-
mantics.
The AKR for a sentence like Bush claimed that
Iraq possessed WMDs in Figure 2 introduces two
contexts: a top level context t, representing the com-
mitments of the speaker of sentence, and an embed-
ded context claim cx:37 representing the state of af-
fairs according to Bush?s claim. The two contexts
are related via the Topic role of the claim event.
The representation contains terms like claim:37 or
17
Conceptual Structure
subconcept(claim:37,[claim-1,. . .,claim-5])
role(Topic,claim:37,claim cx:37)
role(Agent,claim:37,Bush:1)
subconcept(Bush:1,[person-1])
alias(Bush:1,[Bush])
role(cardinality restriction,Bush:1,sg)
subconcept(possess:24,[possess-1,own-1,possess-3])
role(Destination,possess:24,wmd:34)
role(Agent,possess:24,Iraq:19)
subconcept(Iraq:19,[location-1,location-4])
alias(Iraq:19,[Iraq])
role(cardinality restriction,Iraq:19,sg)
subconcept(wmd:34,
[weapon of mass destruction-1])
role(cardinality restriction,wmd:34,pl)
Contextual Structure
context(t)
context(claim cx:37)
context relation(t,claim cx:37,crel(Topic,claim:37))
instantiable(Bush:1,t)
instantiable(Iraq:19,t)
instantiable(claim:37,t)
instantiable(Iraq:19,claim cx:37)
instantiable(possess:24,claim cx:37)
instantiable(wmd:34,claim cx:37)
Temporal Structure
temporalRel(After,Now,claim:37)
temporalRel(After,claim:37,possess:24)
Figure 2: AKR for Bush claimed that Iraq possessed
WMDs.
Bush:1 which refer to the kinds of object that the
sentence is talking about. The subconcept facts ex-
plicitly link these terms to their concepts in Word-
Net. Thus claim:37 is stated to be some subkind
of the type claim-1, etc., and wmd:34 to be some
subkind of the type weapon of mass destruction-
1. Terms like claim:37 and wmd:34 do not refer
to individuals, but to concepts (or types or kinds).
Saying that there is some subconcept of the kind
weapon of mass destruction-1, where this subcon-
cept is further restricted to be a kind of WMD pos-
sessed by Iraq, does not commit you to saying that
there are any instances of this subconcept.
The instantiable assertions capture the commit-
ments about the existence of the kinds of object de-
scribed. In the top-level context t, there is a com-
mitment to an instance of Bush and of a claim:37
event made by him. However, there is no top-level
commitment to any instances of wmd:34 possessed
by Iraq:19. These commitments are only made in
the embedded claim cx:37 context. It is left open
whether these embedded commitments correspond,
or not, to the beliefs of the speaker. Two distinct
levels of structure can thus be discerned in AKR: a
conceptual structure and a contextual structure. The
conceptual structure, through use of subconcept and
role assertions, indicates the subject matter. The
contextual structure indicates commitments as to the
existence of the subject matter via instantiability as-
sertions linking concepts to contexts, and via context
relations linking contexts to contexts. In addition,
there is a temporal structure that situates the events
described with respect to the time of utterance and
temporally relates them to one another.
3 Entailment and Contradiction Detection
ECD is implemented as another set of rewrite rules,
running on the same packed rewrite system used to
generate the AKR representations. The rules (i) align
concept and context terms in text (T) and hypoth-
esis (H) AKRs, (ii) calculate concept subsumption
orderings between aligned T and H terms, and (iii)
check instantiability and uninstantiability claims in
the light of subsumption orderings to determine
whether T entails H, T contradicts H, or T neither
entails not contradicts H. For the purposes of RTE3,
both contradiction and neither contradiction nor en-
tailment are collapsed into a NO (does not follow)
judgment.
One of the novel features of this approach is that
T and H representations do not need to be disam-
biguated before checking for entailment or contra-
diction. The approach is able to detect if there is one
reading of T that entails (or contradicts) one reading
of H. The T and H passages can in effect mutually
disambiguate one another through the ECD. For ex-
ample, although plane and level both have multiple
readings, they can both refer to a horizontal surface,
and in that sense The plane is dry entails The level is
dry, and vice versa.
The first phase of ECD aligns concepts and con-
text terms in the T and H AKRs. Concepts are repre-
18
sented as lists of WordNet hypernym lists, in Word-
Net sense order. Two concept terms can be aligned
if a sense synset of one term (i.e. the first element
of one of the term?s hypernym lists) is contained in
a hypernym list of the other term. The alignment
can be weighted according to word sense; so a con-
cept overlap on the first senses of a T and H term
counts for more than a concept overlap on the n and
mth senses. However, no weightings were used in
RTE3. For named entities, alignment demands not
only a concept overlap, but also an intersection in
the ?alias? forms of the proper nouns. For exam-
ple,?George Bush? may be aligned with ?George?
or with ?Bush?. Context alignment relies on associ-
ating each context with an indexing concept, usually
the concept for the main verb in the clause heading
the context. Contexts are then aligned on the basis
of these concept indices.
Typically, an H term can align with more than one
T term. In such cases all possible alignments are
proposed, but the alignment rules put the alternative
alignments in different parts of the choice space.
Having aligned T and H terms, rules are applied to
determine concept specificity and subsumption rela-
tions between aligned terms. Preliminary judgments
of specificity are made by looking for hypernym in-
clusion. For example, an H term denoting the con-
cept ?person? is less specific than a T term denot-
ing ?woman?. These preliminary judgments need to
be revised in the light of role restrictions modifying
the terms: a ?tall person? is neither more nor less
specific than a ?woman?. Revisions to specificity
judgments also take into account cardinality modi-
fiers: while ?person? is less specific than ?woman?,
?all persons? is judged to be more specific than ?all
women?.
With judgments of concept specificity in place,
it is possible to determine entailment relations on
the basis of (un)instantiability claims in the T and
H AKRs. For example, suppose the T and H AKRs
contain the facts in (3).
(3) T: instantiable(C T, Ctx T)
H: instantiable(C H, Ctx H)
where concept C T is aligned with C H, C T is
judged to be more specific than C H, and context
Ctx T is aligned with context Ctx H. In this case,
the hypothesis instantiability claim is entailed by
the text instantiability claim (existence of something
more specific entails existence of something more
general). This being so, the H instantiability claim
can be deleted without loss of information.
If instead we had the (un)instantiability claims in
(4) for the same alignments and specificity relations,
(4) T: instantiable(C T, Ctx T)
H: uninstantiable(C H, Ctx H)
we would have a contradiction: the text says that
there is something of the more specific type C T,
whereas the hypothesis says there are no things of
the more general type C H. In this case, the rules
explicitly flag a contradiction.
Once all (un)instantiability claims have been
compared, it is possible to judge whether the text en-
tails or contradicts the hypothesis. Entailed hypothe-
sis (un)instantiability assertions are deleted from the
representation. Consequently, if there is one T and H
AKR readings and one set of alignments under which
all the H (un)instantiability assertions have been re-
moved, then there is an entailment of H by T. If
there is a pair of readings and a set of alignments
under which a contradiction is flagged, then there
is a contradiction. If there is no pair of readings or
set of alignments under which there is either an en-
tailment or a contradiction, then T and H are merely
consistent with one another. There are exceptional
cases such as (5) where one reading of T entails H
and another reading contradicts it.
(5) T: John did not wait to call for help.
H: John called for help.
Our ECD rules detect such cases.
WordNet often misses synonyms needed for the
alignment in the ECD. In particular, the hierarchy
and synsets for verbs are one of WordNet?s least de-
veloped parts. To test the impact of the missing syn-
onyms, we developed a variation on the ECD algo-
rithm that allows loose matching.
First, in concept alignment, if a verb concept in H
does not align with any verb concept in T, then we
permit it to (separately) align with all the text verb
concepts. We do not permit the same loose align-
ment for noun concepts, since we judge WordNet
information to be more reliable for nouns. This free
alignment of verbs might sound risky, but in gen-
eral these alignments will not lead to useful concept
19
specificity judgments unless the T and H verbs have
very similar arguments / role restrictions.
When such a loose verb alignment is made, we
explicitly record this fact in a justification term in-
cluded in the alignment fact. Similarly, when judg-
ing concept specificity, each rule that applies adds a
term to a list of justifications recorded as part of the
fact indicating the specificity relation. This means
that when the final specificity judgments are deter-
mined, each judgment has a record of the sequence
of decisions made to reach it.
(Un)instantiability comparisons are made as in
strict matching. However, the criteria for detect-
ing an entailment are selectively loosened. If no
contradiction is flagged, and there is a pairing of
readings and alignments under which just a single
H instantiability assertion is left standing, then this
is allowed through as a loose entailment. However,
further rules are applied to block those loose entail-
ments that are deemed inappropriate. These block-
ing rules look at the form of the justification terms
gathered based on specificity judgments.
These blocking rules are manually selected. First,
a loose matching run is made without any block-
ing rules. Results are dumped for each T-H pair,
recording the expected logical relation and the jus-
tifications collected. Blocking rules are created by
detecting patterns of justification that are associated
with labeled non-entailments. One such blocking
rule says that if you have just a single H instantia-
bility left, but the specificity justifications leading to
this have been shown to be reliable on training data,
then the instantiability should not be eliminated as a
loose entailment.
4 Challenges in Processing the RTE Data
The RTE3 data set contains inconsistencies in
spelling and punctuation between the text and the
hypothesis. To handle these, we did an automatic
prepass where we compared the strings in the pas-
sage text to those in the hypothesis. Some of the
special cases that we handled include:
? Normalize capitalization and spacing
? Identify acronyms and shorten names
? Title identification
? Spelling correction
Role names in VerbNet are in part intended to cap-
ture the relation of the argument to the event be-
ing described by the verb. For example, an object
playing an Agent role is causally involved in the
event, while an object playing a Theme or Patient
role is only supposed to be affected. This allows
participants in an action to be identified regardless
of the syntactic frame chosen to represent the verb;
this was seen in (1) and (2). Sometimes the roles
from VerbNet are not assigned in such a way as to
allow such transparent identification across frames
or related verbs. Consider an example. In Ed trav-
els/goes to Boston VerbNet identifies Ed as playing a
Theme role. However, in Ed flies to Boston VerbNet
assigns Ed an Agent role; this difference can make
determining contradiction and entailment between T
and H difficult. We have tried to compensate in our
ECD, by using a backoff strategy where fewer role
names are used (by projecting down role names to
the smaller set). As we develop the system further,
we continue to experiment with which set of roles
works best for which tasks.
Another open issue involves identifying alterna-
tive ways vague relations among objects appear in
text. We do not match the expression the Boston
team with the team from Boston. To improve our re-
call, we are considering loose matching techniques.
5 Summary of our results on RTE3
We participated in the RTE challenge as a way to
understand what our particular techniques could do
with respect to a more general version of textual en-
tailment. The overall experiment was quite enlight-
ening. Tables 1 and 2 summarize how we did on the
RTE3 challenge. System 1 is our standard system
with strict ECD. System 2 used the looser set of ECD
rules.
Gold Sys Cor- R P F
YES YES rect
IE 105 6 5 0.048 0.83 0.20
IR 87 4 4 0.046 1.00 0.21
QA 106 10 9 0.085 0.90 0.28
SUM 112 11 7 0.063 0.64 0.20
Total 410 31 25 0.060 0.84 0.22
Table 1: System 1 with Strict ECD
20
Gold Sys Cor- R P F
YES YES rect
IE 105 15 10 0.095 0.67 0.25
IR 87 6 4 0.046 0.67 0.18
QA 106 14 13 0.12 0.93 0.34
SUM 112 17 10 0.089 0.59 0.23
Total 410 52 37 0.088 0.71 0.25
Table 2: System 2 with Loose ECD
As can be seen, we answered very few of the ques-
tions; only 31 of the possible 410 with a YES answer.
However, for those we did answer (requiring only
linguistic, and not world knowledge), we achieved
high precision: up to 90% on QA. However, we were
not perfect even from this perspective. Here are sim-
plified versions of the errors where our system an-
swered YES, and the answer should be NO with an
analysis of what is needed in the system to correct
the error.
The wrong result in (6) is due to our incomplete
coverage of intensional verbs (seek, want, look for,
need, etc.).
(6) T: The US sought the release of hostages.
H: Hostages were released.
The object of an intensional verb cannot be assumed
to exist or to occur. Intensional verbs need to be
marked systematically in our lexicon.
The problem with (7) lies in the lack of treatment
for generic sentences.
(7) T: Girls and boys are segregated in high school
during sex education class.
H: Girls and boys are segregated in high school.
The natural interpretation of H is that girls and boys
are segregated in high school ALL THE TIME. Be-
cause we do not yet handle generic sentences prop-
erly, our algorithm for calculating specificity pro-
duces the wrong result here. It judges segregation in
H to be less specific than in T whereas the opposite
is in fact the case. Adding the word ?sometimes? to
H would make our YES the correct answer.
The distinction between generic and episodic
readings is difficult to make but crucial for the in-
terpretation of bare plural noun phrases such as girls
and boys. For example, the most likely interpreta-
tion of Counselors are available is episodic: SOME
counselors are available. But Experts are highly
paid is weighted towards a generic reading: MOST
IF NOT ALL experts get a good salary.
These examples are indicative of the subtlety of
analysis necessary for high precision textual infer-
ence.
References
Danny Bobrow, Cleo Condoravdi, Richard Crouch,
Ronald Kaplan, Lauri Karttunen, Tracy Holloway
King, Valeria de Paiva, and Annie Zaenen. 2005. A
basic logic for textual inference. In Proceedings of the
AAAI Workshop on Inference for Textual Question An-
swering.
Dick Crouch and Tracy Holloway King. 2005. Unify-
ing lexical resources. In Proceedings of the Interdisci-
plinary Workshop on the Identification and Represen-
tation of Verb Features and Verb Classes.
Dick Crouch and Tracy Holloway King. 2006. Se-
mantics via F-structure rewriting. In Proceedings of
LFG06. CSLI On-line Publications.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,
John Maxwell, and Paula Newman. 2007. XLE docu-
mentation. Available on-line.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Olga Gurevich, Richard Crouch, Tracy Holloway King,
and Valeria de Paiva. 2006. Deverbal nouns in knowl-
edge representation. In Proceedings of FLAIRS 2006.
Andres Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing textual entailment with LCC?s GROUNDHOG
system. In The Second PASCAL Recognising Textual
Entailment Challenge.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon. In
AAAI-2000 17th National Conference on Artificial In-
telligence.
John Maxwell and Ron Kaplan. 1991. A method for
disjunctive constraint satisfaction. Current Issues in
Parsing Technologies.
Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of ICoS-5.
Stefan Riezler, Tracy Holloway King, Ron Kaplan, Dick
Crouch, John Maxwell, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-
Functional Grammar and discriminative estimation
techniques. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
21
