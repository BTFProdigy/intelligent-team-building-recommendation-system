Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 146?154,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The Role of Implicit Argumentation in Nominal SRL
Matt Gerber
Dept. of Computer Science
Michigan State University
gerberm2@msu.edu
Joyce Y. Chai
Dept. of Computer Science
Michigan State University
jchai@cse.msu.edu
Adam Meyers
Dept. of Computer Science
New York University
meyers@cs.nyu.edu
Abstract
Nominals frequently surface without overtly
expressed arguments. In order to measure the
potential benefit of nominal SRL for down-
stream processes, such nominals must be ac-
counted for. In this paper, we show that a
state-of-the-art nominal SRL system with an
overall argument F1 of 0.76 suffers a perfor-
mance loss of more than 9% when nominals
with implicit arguments are included in the
evaluation. We then develop a system that
takes implicit argumentation into account, im-
proving overall performance by nearly 5%.
Our results indicate that the degree of implicit
argumentation varies widely across nominals,
making automated detection of implicit argu-
mentation an important step for nominal SRL.
1 Introduction
In the past few years, a number of studies have
focused on verbal semantic role labeling (SRL).
Driven by annotation resources such as FrameNet
(Baker et al, 1998) and PropBank (Palmer et al,
2005), many systems developed in these studies
have achieved argument F1 scores near 80% in
large-scale evaluations such as the one reported by
Carreras and Ma`rquez (2005).
More recently, the automatic identification of
nominal argument structure has received increased
attention due to the release of the NomBank cor-
pus (Meyers, 2007a). NomBank annotates predicat-
ing nouns in the same way that PropBank annotates
predicating verbs. Consider the following example
of the verbal predicate distribute from the PropBank
corpus:
(1) Freeport-McMoRan Energy Partners will be
liquidated and [Arg1 shares of the new
company] [Predicate distributed] [Arg2 to the
partnership?s unitholders].
The NomBank corpus contains a similar instance of
the deverbal nominalization distribution:
(2) Searle will give [Arg0 pharmacists] [Arg1
brochures] [Arg1 on the use of prescription
drugs] for [Predicate distribution] [Location in
their stores].
This instance demonstrates the annotation of split ar-
guments (Arg1) and modifying adjuncts (Location),
which are also annotated in PropBank. In cases
where a nominal has a verbal counterpart, the inter-
pretation of argument positions Arg0-Arg5 is con-
sistent between the two corpora.
In addition to deverbal (i.e., event-based) nomi-
nalizations, NomBank annotates a wide variety of
nouns that are not derived from verbs and do not de-
note events. An example is given below of the parti-
tive noun percent:
(3) Hallwood owns about 11 [Predicate %] [Arg1 of
Integra].
In this case, the noun phrase headed by the predicate
% (i.e., ?about 11% of Integra?) denotes a fractional
part of the argument in position Arg1.
Since NomBank?s release, a number of studies
have applied verbal SRL techniques to the task of
nominal SRL. For example, Liu and Ng (2007) re-
ported an argument F1 of 0.7283. Although this
result is encouraging, it does not take into account
nominals that surface without overt arguments. Con-
sider the following example:
(4) The [Predicate distribution] represents [NP
available cash flow] [PP from the partnership]
[PP between Aug. 1 and Oct. 31].
146
As in (2), distribution in (4) has a noun phrase and
multiple prepositional phrases in its environment,
but not one of these constituents is an argument to
distribution in (4); rather, any arguments are implic-
itly supplied by the surrounding discourse. As de-
scribed by Meyers (2007a), instances such as (2) are
called ?markable? because they contain overt argu-
ments, and instances such as (4) are called ?unmark-
able? because they do not. In the NomBank corpus,
only markable instances have been annotated.
Previous evaluations (e.g., those by Jiang and
Ng (2006) and Liu and Ng (2007)) have been based
on markable instances, which constitute 57% of all
instances of nominals from the NomBank lexicon.
In order to use nominal SRL systems for down-
stream processing, it is important to develop and
evaluate techniques that can handle markable as well
as unmarkable nominal instances. To address this
issue, we investigate the role of implicit argumenta-
tion for nominal SRL. This is, in part, inspired by the
recent CoNLL Shared Task (Surdeanu et al, 2008),
which was the first evaluation of syntactic and se-
mantic dependency parsing to include unmarkable
nominals. In this paper, we extend this task to con-
stituent parsing with techniques and evaluations that
focus specifically on implicit argumentation in nom-
inals.
We first present our NomBank SRL system,
which improves the best reported argument F1 score
in the markable-only evaluation from 0.7283 to
0.7630 using a single-stage classification approach.
We show that this system, when applied to all nomi-
nal instances, achieves an argument F1 score of only
0.6895, a loss of more than 9%. We then present
a model of implicit argumentation that reduces this
loss by 46%, resulting in an F1 score of 0.7235 on
the more complete evaluation task. In our analyses,
we find that SRL performance varies widely among
specific classes of nominals, suggesting interesting
directions for future work.
2 Related work
Nominal SRL is related to nominal relation interpre-
tation as evaluated in SemEval (Girju et al, 2007).
Both tasks identify semantic relations between a
head noun and other constituents; however, the tasks
focus on different relations. Nominal SRL focuses
primarily on relations that hold between nominaliza-
tions and their arguments, whereas the SemEval task
focuses on a range of semantic relations, many of
which are not applicable to nominal argument struc-
ture.
Early work in identifying the argument struc-
ture of deverbal nominalizations was primarily rule-
based, using rule sets to associate syntactic con-
stituents with semantic roles (Dahl et al, 1987;
Hull and Gomez, 1996; Meyers et al, 1998). La-
pata (2000) developed a statistical model to classify
modifiers of deverbal nouns as underlying subjects
or underlying objects, where subject and object de-
note the grammatical position of the modifier when
linked to a verb.
FrameNet and NomBank have facilitated machine
learning approaches to nominal argument struc-
ture. Gildea and Jurafsky (2002) presented an early
FrameNet-based SRL system that targeted both ver-
bal and nominal predicates. Jiang and Ng (2006)
and Liu and Ng (2007) have tested the hypothe-
sis that methodologies and representations used in
PropBank SRL (Pradhan et al, 2005) can be ported
to the task of NomBank SRL. These studies report
argument F1 scores of 0.6914 and 0.7283, respec-
tively. Both studies also investigated the use of fea-
tures specific to the task of NomBank SRL, but ob-
served only marginal performance gains.
NomBank argument structure has also been used
in the recent CoNLL Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies (Surdeanu
et al, 2008). In this task, systems were required to
identify syntactic dependencies, verbal and nominal
predicates, and semantic dependencies (i.e., argu-
ments) for the predicates. For nominals, the best se-
mantic F1 score was 0.7664 (Surdeanu et al, 2008);
however this score is not directly comparable to the
NomBank SRL results of Liu and Ng (2007) or the
results in this paper due to a focus on different as-
pects of the problem (see the end of section 5.2 for
details).
3 NomBank SRL
Given a nominal predicate, an SRL system attempts
to assign surrounding spans of text to one of 23
classes representing core arguments, adjunct argu-
ments, and the null or non-argument. Similarly to
147
verbal SRL, this task is traditionally formulated as
a two-stage classification problem over nodes in the
syntactic parse tree of the sentence containing the
predicate.1 In the first stage, each parse tree node is
assigned a binary label indicating whether or not it
is an argument. In the second stage, argument nodes
are assigned one of the 22 non-null argument types.
Spans of text subsumed by labeled parse tree nodes
constitute arguments of the predication.
3.1 An improved NomBank SRL baseline
To investigate the effects of implicit argumenta-
tion, we first developed a system based on previ-
ous markable-only approaches. Our system follows
many of the traditions above, but differs in the fol-
lowing ways. First, we replace the standard two-
stage pipeline with a single-stage logistic regression
model2 that predicts arguments directly. Second,
we model incorporated arguments (i.e., predicates
that are also arguments) with a simple maximum
likelihood model that predicts the most likely argu-
ment label for a predicate based on counts from the
training data. Third, we use the following heuris-
tics to resolve argument conflicts: (1) If two argu-
ments overlap, the one with the higher probability is
kept. (2) If two non-overlapping arguments are of
the same type, the one with the higher probability
is kept unless the two nodes are siblings, in which
case both are kept. Heuristic (2) accounts for split
argument constructions.
Our NomBank SRL system uses features that are
selected with a greedy forward search strategy sim-
ilar to the one used by Jiang and Ng (2006). The
top half of Table 2 (next page) lists the selected ar-
gument features.3 We extracted training nodes from
sections 2-21 of NomBank, used section 24 for de-
velopment and section 23 for testing. All parse
trees were generated by Charniak?s re-ranking syn-
tactic parser (Charniak and Johnson, 2005). Follow-
ing the evaluation methodology used by Jiang and
Ng (2006) and Liu and Ng (2007), we obtained sig-
1The syntactic parse can be based on ground-truth annota-
tion or derived automatically, depending on the evaluation.
2We use LibLinear (Fan et al, 2008).
3For features requiring the identification of support verbs,
we use the annotations provided in NomBank. Preliminary ex-
periments show a small loss when using automatic support verb
identification.
Dev. F1 Testing F1
Jiang and Ng (2006) 0.6677 0.6914
Liu and Ng (2007) - 0.7283
This paper 0.7454 0.7630
Table 1: Markable-only NomBank SRL results for ar-
gument prediction using automatically generated parse
trees. The f-measure statistics were calculated by ag-
gregating predictions across all classes. ?-? indicates
that the result was not reported.
Markable-only All-token % loss
P 0.7955 0.6577 -17.32
R 0.7330 0.7247 -1.13
F1 0.7630 0.6895 -9.63
Table 3: Comparison of the markable-only and all-
token evaluations of the baseline argument model.
nificantly better results, as shown in Table 1 above.4
3.2 The effect of implicit nominal arguments
The presence of implicit nominal arguments
presents challenges that are not taken into account
by the evaluation described above. To assess the im-
pact of implicit arguments, we evaluated our Nom-
Bank SRL system over each token in the testing
section. The system attempts argument identifica-
tion for all singular and plural nouns that have at
least one annotated instance in the training portion
of the NomBank corpus (morphological variations
included).
Table 3 gives a comparison of the results from the
markable-only and all-token evaluations. As can be
seen, assuming that all known nouns take overt argu-
ments results in a significant performance loss. This
loss is due primarily to a drop in precision caused by
false positive argument predictions made for nomi-
nals with implicit arguments.
4 Accounting for implicit arguments in
nominal SRL
A natural solution to the problem described above
is to first distinguish nominals that bear overt
arguments from those that do not. We treat this
4As noted by Carreras and Ma`rquez (2005), the discrepancy
between the development and testing results is likely due to
poorer syntactic parsing performance on the development sec-
tion.
148
A
rg
u
m
en
tf
ea
tu
re
s
# Description N S
1 12 & parse tree path from n to pred
2 Position of n relative to pred & parse tree path from n to pred *
3 First word subsumed by n
4 12 & position of n relative to pred
5 12 & 14
6 Head word of n?s parent *
7 Last word subsumed n
8 n?s syntactic category & length of parse tree path from n to pred
9 First word of n?s right sibling * *
10 Production rule that expands the parent of pred
11 Head word of the right-most NP in n if n is a PP *
12 Stem of pred
13 Parse tree path from n to the lowest common ancestor of n and pred
14 Head word of n
15 12 & n?s syntactic category
16 Production rule that expands n?s parent * *
17 Parse tree path from n to the nearest support verb *
18 Last part of speech (POS) subsumed by n *
19 Production rule that expands n?s left sibling *
20 Head word of n, if the parent of n is a PP
21 The POS of the head word of the right-most NP under n if n is a PP
... Features 22-31 are available upon request 0 3
N
o
m
in
al
fe
at
u
re
s
1 n?s ancestor subcategorization frames (ASF) (see section 4) *
2 n?s word
3 Syntactic category of n?s right sibling
4 Parse tree paths from n to each support verb *
5 Last word of n?s left sibling * *
6 Parse tree path from n to previous nominal, with lexicalized source (see section 4) *
7 Last word of n?s right sibling *
8 Production rule that expands n?s left sibling * *
9 Syntactic category of n *
10 PropBank markability score (see section 4) *
11 Parse tree path from n to previous nominal, with lexicalized source and destination *
12 Whether or not n is followed by PP *
13 Parse tree path from n to previous nominal, with lexicalized destination *
14 Head word of n?s parent *
15 Whether or not n surfaces before a passive verb * *
16 First word of n?s left sibling *
17 Parse tree path from n to closest support verb, with lexicalized destination *
18 Whether or not n is a head *
19 Head word of n?s right sibling
20 Production rule that expands n?s parent * *
21 Parse tree paths from n to all support verbs, with lexicalized destinations *
22 First word of n?s right sibling * *
23 Head word of n?s left sibling *
24 If n is followed by a PP, the head of that PP?s object *
25 Parse tree path from n to previous nominal *
26 Token distance from n to previous nominal *
27 Production rule that expands n?s grandparent *
Table 2: Features, sorted by gain in selection algorithm. & denotes concatenation. The last two columns indicate
(N)ew features (not used in Liu and Ng (2007)) and features (S)hared by the argument and nominal models.
149
as a binary classification task over token nodes.
Once a nominal has been identified as bearing
overt arguments, it is processed with the argument
identification model developed in the previous
section. To classify nominals, we use the features
shown in the bottom half of Table 2, which were
selected with the same algorithm used for the
argument classification model. As shown by Table
2, the sets of features selected for argument and
nominal classification are quite different, and many
of the features used for nominal classification have
not been previously used. Below, we briefly explain
a few of these features.
Ancestor subcategorization frames (ASF)
As shown in Table 2, the most informative feature
is ASF. For a given token t, ASF is actually a set
of sub-features, one for each parse tree node above
t. Each sub-feature is indexed (i.e., named) by its
distance from t. The value of an ASF sub-feature
is the production rule that expands the correspond-
ing node in the tree. An ASF feature with two
sub-features is depicted below for the token ?sale?:
VP: ASF2 = V P ? V,NP
V (made) NP: ASF1 = NP ? Det,N
Det (a) N (sale)
Parse tree path lexicalization A lexicalized parse
tree path is one in which surface tokens from the
beginning or end of the path are included in the path.
This is a finer-grained version of the traditional
parse tree path that captures the joint behavior of
the path and the tokens it connects. For example,
in the tree above, the path from ?sale? to ?made?
with a lexicalized source and destination would be
sale : N ? NP ? V P ? V : made. Lexicalization
increases sparsity; however, it is often preferred
by the feature selection algorithm, as shown in the
bottom half of Table 2.
PropBank markability score This feature is
the probability that the context (? 5 words) of a de-
verbal nominal is generated by a unigram language
model trained over the PropBank argument words
for the corresponding verb. Entities are normalized
Precision Recall F1
Baseline 0.5555 0.9784 0.7086
MLE 0.6902 0.8903 0.7776
LibLinear 0.8989 0.8927 0.8958
Table 4: Evaluation results for identifying nominals
with explicit arguments.
to their entity type using BBN?s IdentiFinder, and
adverbs are normalized to their related adjective us-
ing the ADJADV dictionary provided by NomBank.
The normalization of adverbs is motivated by the
fact that adverbial modifiers of verbs typically have
a corresponding adjectival modifier for deverbal
nominals.
5 Evaluation results
Our evaluation methodology reflects a practical sce-
nario in which the nominal SRL system must pro-
cess each token in a sentence. The system can-
not safely assume that each token bears overt argu-
ments; rather, this decision must be made automat-
ically. In section 5.1, we present results for the au-
tomatic identification of nominals with overt argu-
ments. Then, in section 5.2, we present results for
the combined task in which nominal classification is
followed by argument identification.
5.1 Nominal classification
Following standard practice, we train the nomi-
nal classifier over NomBank sections 2-21 using
LibLinear and automatically generated syntactic
parse trees. The prediction threshold is set to the
value that maximizes the nominal F1 score on
development section (24), and the resulting model
is tested over section 23. For comparison, we
implemented the following simple classifiers.
Baseline nominal classifier Classifies a token
as overtly bearing arguments if it is a singular or
plural noun that is markable in the training data.
As shown in Table 4, this classifier achieves nearly
perfect recall.5
MLE nominal classifier Operates similarly to
5Recall is less than 100% due to (1) part-of-speech errors
from the syntactic parser and (2) nominals that were not anno-
tated in the training data but exist in the testing data.
150
00.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0.0
5 0.1 0.1
5 0.2 0.2
5 0.3
(0.2
5) 0
.
35 0.4 0.4
5 0.5 0.5
5 0.6
(0.5
) 0.
65 0.7 0.7
5
(0.7
5) 0
.
8
0.8
5 0.9 0.9
5 1
Observed markable probability
%
 
o
f n
o
m
in
al
 
in
st
an
ce
s
(a) Distribution of nominals. Each interval on the x-axis denotes a set of nominals that are markable between (x?5)%
and x% of the time in the training data. The y-axis denotes the percentage of all nominal instances in TreeBank that
is occupied by nominals in the interval. Quartiles are marked below the intervals. For example, quartile 0.25 indicates
that one quarter of all nominal instances are markable 35% of the time or less.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.0
5 0.1 0.1
5 0.2 0.2
5 0.3 0.3
5 0.4 0.4
5 0.5 0.5
5 0.6 0.6
5 0.7 0.7
5 0.8 0.8
5 0.9 0.9
5 1
Observed markable probability
Pr
ed
ic
at
e 
n
o
m
in
al
 
F1
Baseline
LibLinear
(b) Nominal classification performance with respect to the
distribution in Figure 1a. The y-axis denotes the combined
F1 for nominals in the interval.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.0
5 0.1 0.1
5 0.2 0.2
5 0.3 0.3
5 0.4 0.4
5 0.5 0.5
5 0.6 0.6
5 0.7 0.7
5 0.8 0.8
5 0.9 0.9
5 1
Observed markable probability
A
rg
u
m
en
t F
1
Baseline
MLE
LibLinear
(c) All-token argument classification performance with re-
spect to the distribution in Figure 1a. The y-axis denotes the
combined F1 for nominals in the interval.
Figure 1: Evaluation results with respect to the distribution of nominals in TreeBank.
the baseline classifier, but also produces a score
for the classification. The value of the score is
equal to the probability that the nominal bears overt
arguments, as observed in the training data. A
prediction threshold is imposed on this score as
determined by the development data (t = 0.23).
As shown by Table 4, this exchanges recall for
precision and leads to a significant increase in the
overall F1 score.
The last row in Table 4 shows the results for
the LibLinear nominal classifier, which significantly
outperforms the others, achieving balanced preci-
sion and recall scores near 0.9. In addition, it is
able to recover from part-of-speech errors because
it does not filter out non-noun instances; rather, it
combines part-of-speech information with other lex-
ical and syntactic features to classify nominals.
Interesting observations can be made by grouping
nominals according to the probability with which
they are markable in the corpus. Figure 1a gives
the overall distribution of markable nominals in the
training data. As shown, 50% of nominal instances
are markable only 65% of the time or less, making
nominal classification an important first step. Using
this view of the data, Figure 1b presents the over-
all F1 scores for the baseline and LibLinear nominal
151
classifiers.6 As expected, gains in nominal classi-
fication diminish as nominals become more overtly
associated with arguments. Furthermore, nominals
that are rarely markable (i.e., those in interval 0.05)
remain problematic due to a lack of positive training
instances and the unbalanced nature of the classifi-
cation task.
5.2 Combined nominal-argument classification
We now turn to the task of combined nominal-
argument classification. In this task, systems must
first identify nominals that bear overt arguments. We
evaluated three configurations based on the nominal
classifiers from the previous section. Each config-
uration uses the argument classification model from
section 3.
As shown in Table 3, overall argument classifi-
cation F1 suffers a loss of more than 9% under the
assumption that all known nouns bear overt argu-
ments. This corresponds precisely to using the base-
line nominal classifier in the combined nominal-
argument task. The MLE nominal classifier is able
to reduce this loss by 25% to an F1 of 0.7080. The
LibLinear nominal classifier reduces this loss by
46%, resulting in an overall argument classification
F1 of 0.7235. This improvement is the direct result
of filtering out nominal instances that do not bear
overt arguments.
Similarly to the nominal evaluation, we can view
argument classification performance with respect to
the probability that a nominal bears overt arguments.
This is shown in Figure 1c for the three configura-
tions. The configuration using the MLE nominal
classifier obtains an argument F1 of zero for nom-
inals below its prediction threshold. Compared to
the baseline nominal classifier, the LibLinear clas-
sifier achieves argument classification gains as large
as 150.94% (interval 0.05), with an average gain of
52.87% for intervals 0.05 to 0.4. As with nomi-
nal classification, argument classification gains di-
minish for nominals that express arguments more
overtly - we observe an average gain of only 2.15%
for intervals 0.45 to 1.00. One possible explana-
tion for this is that the argument prediction model
has substantially more training data for the nomi-
nals in intervals 0.45 to 1.00. Thus, even if the nom-
6Baseline and MLE are identical above the MLE threshold.
Nominals
Deverbal Deverbal-like Other
Baseline 0.7975 0.6789 0.6757
MLE 0.8298 0.7332 0.7486
LibLinear 0.9261 0.8826 0.8905
Arguments
Baseline 0.7059 0.6738 0.7454
MLE 0.7206 0.6641 0.7675
LibLinear 0.7282 0.7178 0.7847
Table 5: Nominal and argument F1 scores for dever-
bal, deverbal-like, and other nominals in the all-token
evaluation.
inal classifier makes a false positive prediction in the
0.45 to 1.00 interval range, the argument model may
correctly avoid labeling any arguments.
As noted in section 2, these results are not di-
rectly comparable to the results of the recent CoNLL
Shared Task (Surdeanu et al, 2008). This is due to
the fact that the semantic labeled F1 in the Shared
Task combines predicate and argument predictions
into a single score. The same combined F1 score for
our best two-stage nominal SRL system (logistic re-
gression nominal and argument models) is 0.7806;
however, this result is not precisely comparable be-
cause we do not identify the predicate role set as re-
quired by the CoNLL Shared Task.
5.3 NomLex-based analysis of results
As demonstrated in section 1, NomBank annotates
many classes of deverbal and non-deverbal nomi-
nals, which have been categorized on syntactic and
semantic bases in NomLex-PLUS (Meyers, 2007b).
To help understand what types of nominals are par-
ticularly affected by implicit argumentation, we fur-
ther analyzed performance with respect to these
classes.
Figure 2a shows the distribution of nominals
across classes defined by the NomLex resource. As
shown in Figure 2b, many of the most frequent
classes exhibit significant gains. For example, the
classification of partitive nominals (13% of all nom-
inal instances) with the LibLinear classifier results
in gains of 55.45% and 33.72% over the baseline
and MLE classifiers, respectively. For the 5 most
common classes, which constitute 82% of all nomi-
nals instances, we observe average gains of 27.47%
and 19.30% over the baseline and MLE classifiers,
152
00.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
no
m
pa
rtit
ive
no
m
like
re
lat
ion
al
no
m
ing
att
rib
ute
en
vir
on
m
en
t
ab
ility
no
m
ad
j
wo
rk-
of-
ar
t
gro
up
no
m
ad
jlike job
sh
ar
e
ev
en
t
typ
e
ve
rs
ion
ha
llm
ar
k
ab
le-
no
m fie
ld
NomLex class
%
 
o
f n
o
m
in
al
 
in
st
an
ce
s
(a) Distribution of nominals across the NomLex classes. The
y-axis denotes the percentage of all nominal instances that is
occupied by nominals in the class.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
no
m
pa
rtit
ive
no
m
like
re
lat
ion
al
no
m
ing
att
rib
ute
en
vir
on
m
en
t
ab
ility
no
m
ad
j
wo
rk-
of-
ar
t
gro
up
no
m
ad
jlike job
sh
ar
e
ev
en
t
typ
e
ve
rs
ion
ha
llm
ar
k
ab
le-
no
m fie
ld
NomLex class
Pr
ed
ic
at
e 
n
o
m
in
al
 
F1
Baseline
MLE
LibLinear
(b) Nominal classification performance with respect to the
NomLex classes in Figure 2a. The y-axis denotes the com-
bined F1 for nominals in the class.
Figure 2: Evaluation results with respect to NomLex classes.
respectively.
Table 5 separates nominal and argument classifi-
cation results into sets of deverbal (NomLex class
nom), deverbal-like (NomLex class nom-like), and
all other nominalizations. A deverbal-like nominal
is closely related to some verb, although not mor-
phologically. For example, the noun accolade shares
argument interpretation with award, but the two are
not morphologically related. As shown by Table 5,
nominal classification tends to be easier - and ar-
gument classification harder - for deverbals when
compared to other types of nominals. The differ-
ence in argument F1 between deverbal/deverbal-like
nominals and the others is due primarily to relational
nominals, which are relatively easy to classify (Fig-
ure 2b); additionally, relational nominals exhibit a
high rate of argument incorporation, which is eas-
ily handled by the maximum-likelihood model de-
scribed in section 3.1.
6 Conclusions and future work
The application of nominal SRL to practical NLP
problems requires a system that is able to accurately
process each token it encounters. Previously, it was
unclear whether the models proposed by Jiang and
Ng (2006) and Liu and Ng (2007) would operate ef-
fectively in such an environment. The systems de-
scribed by Surdeanu et al (2008) are designed with
this environment in mind, but their evaluation did
not focus on the issue of implicit argumentation.
These two problems motivate the work presented in
this paper.
Our contribution is three-fold. First, we improve
upon previous nominal SRL results using a single-
stage classifier with additional new features. Sec-
ond, we show that this model suffers a substantial
performance degradation when evaluated over nom-
inals with implicit arguments. Finally, we identify a
set of features - many of them new - that can be used
to reliably detect nominals with explicit arguments,
thus significantly increasing the performance of the
nominal SRL system.
Our results also suggest interesting directions for
future work. As described in section 5.2, many nom-
inals do not have enough labeled training data to
produce accurate argument models. The general-
ization procedures developed by Gordon and Swan-
son (2007) for PropBank SRL and Pado? et al (2008)
for NomBank SRL might alleviate this problem.
Additionally, instead of ignoring nominals with im-
plicit arguments, we would prefer to identify the im-
plicit arguments using information contained in the
surrounding discourse. Such inferences would help
connect entities and events across sentences, provid-
ing a fuller interpretation of the text.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their helpful suggestions. The first two
authors were supported by NSF grants IIS-0535112
and IIS-0347548, and the third author was supported
by NSF grant IIS-0534700.
153
References
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In Christian Boitet
and Pete Whitelock, editors, Proceedings of the Thirty-
Sixth Annual Meeting of the Association for Computa-
tional Linguistics and Seventeenth International Con-
ference on Computational Linguistics, pages 86?90,
San Francisco, California. Morgan Kaufmann Publish-
ers.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28:245?288.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic re-
lations between nominals. In Proceedings of the 4th
International Workshop on Semantic Evaluations.
A. Gordon and R. Swanson. 2007. Generalizing seman-
tic role annotations across syntactically similar verbs.
In Proceedings of ACL, pages 192?199.
Z. Jiang and H. Ng. 2006. Semantic role labeling of
nombank: A maximum entropy approach. In Proceed-
ings of the 2006 Conference on Empirical Methods in
Natural Language Processing.
Maria Lapata. 2000. The automatic interpretation
of nominalizations. In Proceedings of the Seven-
teenth National Conference on Artificial Intelligence
and Twelfth Conference on Innovative Applications of
Artificial Intelligence, pages 716?721. AAAI Press /
The MIT Press.
Chang Liu and Hwee Ng. 2007. Learning predictive
structures for semantic role labeling of nombank. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 208?215,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Adam Meyers. 2007a. Annotation guidelines for nom-
bank - noun argument structure for propbank. Techni-
cal report, New York University.
Adam Meyers. 2007b. Those other nombank dictionar-
ies. Technical report, New York University.
Sebastian Pado?, Marco Pennacchiotti, and Caroline
Sporleder. 2008. Semantic role assignment for event
nominalisations by leveraging verbal data. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 665?
672, Manchester, UK, August. Coling 2008 Organiz-
ing Committee.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Sameer Pradhan, Wayne Ward, and James H. Martin.
2005. Towards robust semantic role labeling. In Asso-
ciation for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England, August. Coling 2008 Organizing Committee.
154
Semantic Role Labeling of Implicit
Arguments for Nominal Predicates
Matthew Gerber?
University of Virginia
Joyce Y. Chai??
Michigan State University
Nominal predicates often carry implicit arguments. Recent work on semantic role labeling has
focused on identifying arguments within the local context of a predicate; implicit arguments,
however, have not been systematically examined. To address this limitation, we have manually
annotated a corpus of implicit arguments for ten predicates from NomBank. Through analysis
of this corpus, we find that implicit arguments add 71% to the argument structures that are
present in NomBank. Using the corpus, we train a discriminative model that is able to identify
implicit arguments with an F1 score of 50%, significantly outperforming an informed baseline
model. This article describes our investigation, explores a wide variety of features important for
the task, and discusses future directions for work on implicit argument identification.
1. Introduction
Recent work has shown that semantic role labeling (SRL) can be applied to nominal
predicates in much the same way as verbal predicates (Liu and Ng 2007; Johansson and
Nugues 2008; Gerber, Chai, and Meyers 2009). In general, the nominal SRL problem
is formulated as follows: Given a predicate that is annotated in NomBank as bear-
ing arguments, identify these arguments within the clause or sentence that contains
the predicate. As shown in our previous work (Gerber, Chai, and Meyers 2009), this
problem definition ignores the important fact that many nominal predicates do not
bear arguments in the local context. Such predicates need to be addressed in order
for nominal SRL to be used by downstream applications such as automatic question
answering, information extraction, and statistical machine translation.
Gerber, Chai, and Meyers (2009) showed that it is possible to accurately identify
nominal predicates that bear arguments in the local context. This makes the nominal
SRL system applicable to text that does not contain annotated predicates. The system
does not address a fundamental question regarding arguments of nominal predicates,
however: If an argument is missing from the local context of a predicate, might the
argument be located somewhere in the wider discourse? Most prior work on nominal
? 151 Engineer?s Way, University of Virginia, Charlottesville, VA 22904.
E-mail: matt.gerber@virginia.edu.
?? 3115 Engineering Building, Michigan State University, East Lansing, MI 48824.
E-mail: jchai@cse.msu.edu.
Submission received: 4 August 2011; revised version received: 23 December 2011; accepted for publication:
7 February 2012.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 4
and verbal SRL has stopped short of answering this question, opting instead for an
approach that only labels local arguments and thus ignores predicates whose arguments
are entirely non-local. This article directly addresses the issue of non-local (or implicit)
argument identification for nominal predicates.
As an initial example, consider the following sentence, which is taken from the Penn
TreeBank (Marcus, Santorini, and Marcinkiewicz 1993):
(1) A SEC proposal to ease [arg1 reporting] [predicate requirements] [arg2
for some company executives] would undermine the usefulness of
information on insider trades, professional money managers contend.
The NomBank (Meyers 2007) role set for requirement is shown here:
Frame for requirement, role set 1:
arg0: the entity that is requiring something
arg1: the entity that is required
arg2: the entity of which something is being required
In Example (1), the predicate has been annotated with the local argument labels pro-
vided by NomBank. As shown, NomBank does not annotate an arg0 for this instance of
the requirement predicate; a reasonable interpretation of the sentence, however, is that
SEC is the entity that is requiring something.1 This article refers to arguments such as
SEC in Example (1) as implicit. In this work, the notion of implicit argument covers any
argument that is not annotated by NomBank.2
Building on Example (1), consider the following sentence, which directly follows
Example (1) in the corresponding TreeBank document:
(2) Money managers make the argument in letters to the agency about
[arg1 rule] [predicate changes] proposed this past summer.
The NomBank role set for change is as follows:
Frame for change, role set 1:
arg0: the entity that initiates the change
arg1: the entity that is changed
arg2: the initial state of the changed entity
arg3: the final state of the changed entity
Similarly to the previous example, Example (2) shows the local argument labels pro-
vided by NomBank. These labels only indicate that rules have been changed. For a
full interpretation, Example (2) requires an understanding of Example (1). Without
1 The Securities and Exchange Commission (SEC) is responsible for enforcing investment laws in the
United States.
2 NomBank annotates arguments in the noun phrase headed by the predicate as well as arguments
brought in by so-called support verb structures. See Meyers (2007) for details.
756
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
the sentence from Example 1, the reader has no way of knowing that the agency in
Example (2) actually refers to the same entity as SEC in Example (1). As part of the
reader?s comprehension process, this entity is identified as the filler for the arg0 role in
Example (2). This identification must occur in order for these two sentences to form a
coherent discourse.
From these examples, it is clear that the scope of implicit arguments quite naturally
spans sentence boundaries. Thus, if one wishes to recover implicit arguments as part of
the SRL process, the argument search space must be expanded beyond the traditional,
single-sentence window used in virtually all prior SRL research. What can we hope
to gain from such a fundamental modification of the problem? Consider the following
question, which targets Examples (1) and (2):
(3) Who changed the rules regarding reporting requirements?
Question (3) is a factoid question, meaning it has a short, unambiguous answer in the
targeted text. This type of question has been studied extensively in the Text Retrieval
Conference Question Answering (QA) Track (Dang, Kelly, and Lin 2007). Using the
evaluation data from this track, Pizzato and Molla? (2008) showed that SRL can improve
the accuracy of a QA system; a traditional SRL system alone, however, is not enough
to recover the implied answer to Question (3): SEC or the agency. Successful implicit
argument identification provides the answer in this case.
This article presents an in-depth study of implicit arguments for nominal predi-
cates.3 The following section surveys research related to implicit argument identifica-
tion. Section 3 describes the study?s implicit argument annotation process and the data
it produced. The implicit argument identification model is formulated in Section 4 and
evaluated in Section 5. Discussion of results is provided in Section 6, and the article
concludes in Section 7.
2. Related Work
The research presented in this article is related to a wide range of topics in cognitive
science, linguistics, and natural language processing (NLP). This is partly due to the
discourse-based nature of the problem. In single-sentence SRL, one can ignore the dis-
course aspect of language and still obtain high marks in an evaluation (for examples, see
Carreras and Ma`rquez 2005 and Surdeanu et al 2008); implicit argumentation, however,
forces one to consider the discourse context in which a sentence exists. Much has been
said about the importance of discourse to language understanding, and this section
will identify the points most relevant to implicit argumentation.
2.1 Discourse Comprehension in Cognitive Science
The traditional view of sentence-level semantics has been that meaning is composi-
tional. That is, one can derive the meaning of a sentence by carefully composing the
meanings of its constituent parts (Heim and Kratzer 1998). There are counterexamples
to a compositional theory of semantics (e.g., idioms), but those are more the exception
than the rule. Things change, however, when one starts to group sentences together
3 This article builds on our previous work (Gerber and Chai 2010).
757
Computational Linguistics Volume 38, Number 4
to form coherent textual discourses. Consider the following examples, borrowed from
Sanford (1981, page 5):
(4) Jill came bouncing down the stairs.
(5) Harry rushed off to get the doctor.
Examples (4) and (5) describe three events: bounce, rush, and get. These events are
intricately related. One cannot simply create a conjunction of the propositions bounce,
rush, and get and expect to arrive at the author?s intended meaning, which presumably
involves Jill?s becoming injured by her fall and Harry?s actions to help her. The mutual
dependence of these sentences can be further shown by considering a variant of the
situation described in Examples (4) and (5):
(6) Jill came bouncing down the stairs.
(7) Harry rushed over to kiss her.
The interpretation of Example (6) is vastly different from the interpretation of Exam-
ple (4). In Example (4), Jill becomes injured whereas in Example (6) she is quite happy.
Examples (4?7) demonstrate the fact that sentences do not have a fixed, compo-
sitional interpretation; rather, a sentence?s interpretation depends on the surrounding
context. The standard compositional theory of sentential semantics largely ignores con-
textual information provided by other sentences. The single-sentence approach to SRL
operates similarly. In both of these methods, the current sentence provides all of the
semantic information. In contrast to these methods?and aligned with the preceding
discussion?this article presents methods that rely heavily on surrounding sentences
to provide additional semantic information. This information is used to interpret the
current sentence in a more complete fashion.
Examples (4?7) also show that the reader?s knowledge plays a key role in discourse
comprehension. Researchers in cognitive science have proposed many models of reader
knowledge. Schank and Abelson (1977) proposed stereotypical event sequences called
scripts as a basis for discourse comprehension. In this approach, readers fill in a dis-
course?s semantic gaps with knowledge of how a typical event sequence might unfold.
In Examples (4) and (5), the reader knows that people typically call on a doctor only
if someone is hurt. Thus, the reader automatically fills the semantic gap caused by the
ambiguous predicate bounce with information about doctors and what they do. Similar
observations have been made by van Dijk (1977, page 4), van Dijk and Kintsch (1983,
page 303), Graesser and Clark (1985, page 14), and Carpenter, Miyake, and Just (1995).
Inspired by these ideas, the model developed in this article relies partly on large text
corpora, which are treated as repositories of typical event sequences. The model uses
information extracted from these event sequences to identify implicit arguments.
2.2 Automatic Relation Discovery
Examples (4) and (5) in the previous section show that understanding the relationships
between predicates is a key part of understanding a textual discourse. In this section, we
review work on automatic predicate relationship discovery, which attempts to extract
these relationships automatically.
758
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Lin and Pantel (2001) proposed a system that automatically identifies relationships
similar to the following:
(8) X eats Y ? X likes Y
This relationship creates a mapping between the participants of the two predicates.
One can imagine using such a mapping to fill in the semantic gaps of a discourse that
describes a typical set of events in a restaurant. In such a discourse, the author probably
will not state directly that X likes Y; the reader might need to infer this in order to make
sense of the fact that X left a large tip for the waiter, however.
Lin and Pantel (2001) created mappings such as the one in Example (8) using a
variation of the so-called ?distributional hypothesis? posited by Harris (1985), which
states that words occurring in similar contexts tend to have similar meanings. Lin and
Pantel applied the same notion of similarity to dependency paths. For example, the
inference rule in Example 8 is identified by examining the sets of words in the two X
positions and the sets of words in the two Y positions. When the two pairs of sets are
similar, it is implied that the two dependency paths from X to Y are similar as well. In
Example (8), the two dependency paths are as follows:
X
subject???? eats object???? Y
X
subject???? likes object???? Y
One drawback of this method is that it assumes the implication is symmetric. Although
this assumption is correct in many cases, it often leads to invalid inferences. In Exam-
ple 8, it is not always true that if X likes Y then X will eat Y. The opposite?that X eating
Y implies X likes Y?is more plausible but not certain.
Bhagat, Pantel, and Hovy (2007) extended the work of Lin and Pantel (2001) to
handle cases of asymmetric relationships. The basic idea proposed by Bhagat, Pantel,
and Hovy is that, when considering a relationship of the form ?x, p1, y? ? ?x, p2, y?, if p1
occurs in significantly more contexts (i.e., has more options for x and y) than p2, then p2
is likely to imply p1 but not vice versa. Returning to Example 8, we see that the correct
implication will be derived if likes occurs in significantly more contexts than eats. The
intuition is that the more general concept (i.e., like) will be associated with more contexts
and is more likely to be implied by the specific concept (i.e., eat). As shown by Bhagat,
Pantel, and Hovy, the system built around this intuition is able to effectively identify
the directionality of many inference rules.
Zanzotto, Pennacchiotti, and Pazienza (2006) presented another study aimed at
identifying asymmetric relationships between verbs. For example, the asymmetric en-
tailment relationship X wins ?? X plays holds, but the opposite (X plays ?? X wins) does
not. This is because not all those who play a game actually win. To find evidence for
this automatically, the authors examined constructions such as the following (adapted
from Zanzotto, Pennacchiotti, and Pazienza [2006]):
(9) The more experienced tennis player won the match.
The underlying idea behind the authors? approach is that asymmetric relationships such
as X wins ?? X plays are often entailed by constructions involving agentive, nominalized
verbs as the logical subjects of the main verb. In Example (9), the agentive nominal
759
Computational Linguistics Volume 38, Number 4
?player? is logical subject to ?won?, the combination of which entails the asymmetric
relationship of interest. Thus, to validate such an asymmetric relationship, Zanzotto,
Pennacchiotti, and Pazienza (2006) examined the frequency of the ?player win? colloca-
tion using Google hit counts as a proxy for actual corpus statistics.
A number of other studies (e.g., Szpektor et al 2004, Pantel et al 2007) have been
conducted that are similar to that work. In general, such work focuses on the automatic
acquisition of entailment relationships between verbs. Although this work has often
been motivated by the need for lexical?semantic information in tasks such as automatic
question answering, it is also relevant to the task of implicit argument identification
because the derived relationships implicitly encode a participant role mapping between
two predicates. For example, given a missing arg0 for a like predicate and an explicit
arg0 = John for an eat predicate in the preceding discourse, inference rule (8) would
help identify the implicit arg0 = John for the like predicate.
The missing link between previous work on verb relationship identification and the
task of implicit argument identification is that previous verb relations are not defined
in terms of the argn positions used by NomBank. Rather, positions like subject and object
are used. In order to identify implicit arguments in NomBank, one needs inference rules
between specific argument positions (e.g., eat:arg0 and like:arg0). In the current article,
we propose methods of automatically acquiring these fine-grained relationships for
verbal and nominal predicates using existing corpora. We also propose a method of
using these relationships to recover implicit arguments.
2.3 Coreference Resolution
The referent of a linguistic expression is the real or imagined entity to which the expres-
sion refers. Coreference, therefore, is the condition of two linguistic expressions having
the same referent. In the following examples from the Penn TreeBank, the underlined
spans of text are coreferential:
(10) ?Carpet King sales are up 4% this year,? said owner Richard Rippe.
(11) He added that the company has been manufacturing carpet since 1967.
Non-trivial instances of coreference (e.g., Carpet King and the company) allow the author
to repeatedly mention the same entity without introducing redundancy into the dis-
course. Pronominal anaphora is a subset of coreference in which one of the referring
expressions is a pronoun. For example, he in Example (11) refers to the same entity as
Richard Rippe in Example (10). These examples demonstrate noun phrase coreference.
Events, indicated by either verbal or nominal predicates, can also be coreferential when
mentioned multiple times in a document (Wilson 1974; Chen and Ji 2009).
For many years, the Automatic Content Extraction (ACE) series of large-scale eval-
uations (NIST 2008) has provided a test environment for systems designed to identify
these and other coreference relations. Systems based on the ACE data sets typically take
a supervised learning approach to coreference resolution in general (Versley et al 2008)
and pronominal anaphor in particular (Yang, Su, and Tan 2008).
A phenomenon similar to the implicit argument has been studied in the context
of Japanese anaphora resolution, where a missing case-marked constituent is viewed
as a zero-anaphoric expression whose antecedent is treated as the implicit argument
of the predicate of interest. This behavior has been annotated manually by Iida et al
(2007), and researchers have applied standard SRL techniques to this corpus, resulting
760
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
in systems that are able to identify missing case?marked expressions in the surrounding
discourse (Imamura, Saito, and Izumi 2009). Sasano, Kawahara, and Kurohashi (2004)
conducted similar work with Japanese indirect anaphora. The authors used automati-
cally derived nominal case frames to identify antecedents. However, as noted by Iida
et al, grammatical cases do not stand in a one-to-one relationship with semantic roles
in Japanese (the same is true for English).
Many other discourse-level phenomena interact with coreference. For example,
Centering Theory (Grosz, Joshi, and Weinstein 1995) focuses on the ways in which
referring expressions maintain (or break) coherence in a discourse. These so-called ?cen-
tering shifts? result from a lack of coreference between salient noun phrases in adjacent
sentences. Discourse Representation Theory (DRT) (Kamp and Reyle 1993) is another
prominent treatment of referring expressions. DRT embeds a theory of coreference into
a first-order, compositional semantics of discourse.
2.4 Identifying Implicit Arguments
Past research on the actual task of implicit argument identification tends to be sparse.
Palmer et al (1986) describe what appears to be the first computational treatment of
implicit arguments. In that work, Palmer et al manually created a repository of knowl-
edge concerning entities in the domain of electronic device failures. This knowledge,
along with hand-coded syntactic and semantic processing rules, allowed the system to
identify implicit arguments across sentence boundaries. As a simple example, consider
the following two sentences (borrowed from Palmer et al [1986]):
(12) Disk drive was down at 11/16-2305.
(13) Has select lock.
Example (13) does not specify precisely which entity has select lock. The domain knowl-
edge, however, tells the system that only disk drive entities can have such a property.
Using this knowledge, the system is able to search the local context and make explicit
the implied fact that the disk drive from Example (12) has select lock.
A similar line of work was pursued by Whittemore, Macpherson, and Carlson
(1991), who offer the following example of implicit argumentation (page 21):
(14) Pete bought a car.
(15) The salesman was a real jerk.
In Example (14), the buy event is not associated with an entity representing the seller.
This entity is introduced in Example (15) as the salesman, whose semantic properties
satisfy the requirements of the buy event. Whittemore, Macpherson, and Carlson (1991)
build up the event representation incrementally using a combination of semantic prop-
erty constraints and DRT.
The systems developed by Palmer et al (1986) and Whittemore, Macpherson,
and Carlson (1991) are quite similar. They both make use of semantic constraints on
arguments, otherwise known as selectional preferences. Selectional preferences have
received a significant amount of attention over the years, with the work of Ritter,
Mausam, and Etzioni (2010) being some of the most recent. The model developed in
761
Computational Linguistics Volume 38, Number 4
the current article uses a variety of selectional preference measures to identify implicit
arguments.
The implicit argument identification systems described herein were not widely de-
ployed due to their reliance on hand-coded, domain-specific knowledge that is difficult
to create. Much of this knowledge targeted basic syntactic and semantic constructions
that now have robust statistical models (e.g., those created by Charniak and Johnson
[2005] for syntax and Punyakanok et al [2005] for semantics). With this information
accounted for, it is easier to approach the problem of implicit argumentation. Subse-
quently, we describe a series of recent investigations that have led to a surge of interest
in statistical implicit argument identification.
Fillmore and Baker (2001) provided a detailed case study of FrameNet frames as a
basis for understanding written text. In their case study, Fillmore and Baker manually
build up a semantic discourse structure by hooking together frames from the various
sentences. In doing so, the authors resolve some implicit arguments found in the dis-
course. This process is an interesting step forward; the authors did not provide concrete
methods to perform the analysis automatically, however.
Nielsen (2004) developed a system that is able to detect the occurrence of verb
phrase ellipsis. Consider the following sentences:
(16) John kicked the ball.
(17) Bill [did], too.
The bracketed text in Example (17) is a placeholder for the verb phrase kicked the ball
in Example (16), which has been elided (i.e., left out). Thus, in Example (17), Bill can
be thought of as an implicit argument to some kicking event that is not mentioned. If
one resolved the verb phrase ellipsis, then the implicit agent (Bill) would be recovered.4
Nielsen (2004) created a system able to detect the presence of ellipses, producing the
bracketing in Example (17). Ellipsis resolution (i.e., figuring out precisely which verb
phrase is missing) was described by Nielsen (2005). Implicit argument identification for
nominal predicates is complementary to verb phrase ellipsis resolution: Both work to
make implicit information explicit.
Burchardt, Frank, and Pinkal (2005) suggested that frame elements from various
frames in a text could be linked to form a coherent discourse interpretation (this is
similar to the idea described by Fillmore and Baker [2001]). The linking operation
causes two frame elements to be viewed as coreferent. Burchardt, Frank, and Pinkal
(2005) propose to learn frame element linking patterns from observed data; the authors
did not implement and evaluate such a method, however. Building on the work of
Burchardt, Frank, and Pinkal, this article presents a model of implicit arguments that
uses a quantitative analysis of naturally occurring coreference patterns.
In our previous work, we demonstrated the importance of filtering out nominal
predicates that take no local arguments (Gerber, Chai, and Meyers 2009). This approach
leads to appreciable gains for certain nominals. The approach does not attempt to
actually recover implicit arguments, however.
4 Identification of the implicit patient in Example (17) (the ball) should be sensitive to the phenomenon of
sense anaphora. If Example (16) was changed to ?a ball,? then we would have no implicit patient in
Example (17).
762
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Most recently, Ruppenhofer et al (2009) proposed SemEval Task 10, ?Linking
Events and Their Participants in Discourse,? which evaluated implicit argument iden-
tification systems over a common test set. The task organizers annotated implicit
arguments across entire passages, resulting in data that cover many distinct predi-
cates, each associated with a small number of annotated instances. As described by
Ruppenhofer et al (2010), three submissions were made to the competition, with two of
the submissions attempting the implicit argument identification part of the task. Chen
et al (2010) extended a standard SRL system by widening the candidate window to
include constituents from other sentences. A small number of features based on the
FrameNet frame definitions were extracted for these candidates, and prediction was
performed using a log-linear model. Tonelli and Delmonte (2010) also extended a stan-
dard SRL system. Both of these systems achieved an implicit argument F1 score of less
than 0.02. The organizers and participants appear to agree that training data sparseness
was a significant problem. This is likely the result of the annotation methodology: Entire
documents were annotated, causing each predicate to receive a very small number of
annotated examples.
In contrast to the evaluation described by Ruppenhofer et al (2010), the study
presented in this article focused on a select group of nominal predicates. To help prevent
data sparseness, the size of the group was small, and the predicates were carefully
chosen to maximize the observed frequency of implicit argumentation. We annotated a
large number of implicit arguments for this group of predicates with the goal of training
models that generalize well to the testing data. In the following section, we describe
the implicit argument annotation process and resulting data set.
3. Implicit Argument Annotation and Analysis
As shown in the previous section, the existence of implicit arguments has been rec-
ognized for quite some time. This type of information, however, was not formally
annotated until Ruppenhofer et al (2010) conducted their SemEval task on implicit
argument identification. There are two reasons why we chose to create an independent
data set for implicit arguments. The first reason is the aforementioned sparsity of the
SemEval data set. The second reason is that the SemEval data set is not built on top of
the Penn TreeBank, which is the gold-standard syntactic base for all work in this article.
Working on top of the Penn TreeBank makes the annotations immediately compatible
with PropBank, NomBank, and a host of other resources that also build on the TreeBank.
3.1 Data Annotation
3.1.1 Predicate Selection. Implicit arguments are a relatively new subject of annotation in
the field. To effectively use our limited annotation resources and allow the observation
of interesting behaviors, we decided to focus on a select group of nominal predicates.
Predicates in this group were required to meet the following criteria:
1. A selected predicate must have an unambiguous role set. This criterion
corresponds roughly to an unambiguous semantic sense and is motivated
by the need to separate the implicit argument behavior of a predicate from
its semantic meaning.
2. A selected predicate must be derived from a verb. This article focuses
primarily on the event structure of texts. Nominal predicates derived
from verbs denote events, but there are other, non-eventive predicates in
763
Computational Linguistics Volume 38, Number 4
NomBank (e.g., the partitive predicate indicated by the ?%? symbol).
This criterion also implies that the annotated predicates have correlates
in PropBank with semantically compatible role sets.
3. A selected predicate should have a high frequency in the Penn TreeBank
corpus. This criterion ensures that the evaluation results say as much as
possible about the event structure of the underlying corpus. We calculated
frequency with basic counting over morphologically normalized
predicates (i.e., bids and bid are counted as the same predicate).
4. A selected predicate should express many implicit arguments. Of course,
this can only be estimated ahead of time because no data exist to compute
it. To estimate this value for a predicate p, we first calculated Np, the
average number of roles expressed by p in NomBank. We then calculated
Vp, the average number of roles expressed by the verb form of p in
PropBank. We hypothesized that the difference Vp ? Np gives an
indication of the number of implicit arguments that might be present in
the text for a nominal instance of p. The motivation for this hypothesis is as
follows. Most verbs must be explicitly accompanied by specific arguments
in order for the resulting sentence to be grammatical. The following
sentences are ungrammatical if the parenthesized portion is left out:
(18) *John loaned (the money to Mary).
(19) *John invested (his money).
Examples (18) and (19) indicate that certain arguments must explicitly
accompany loan and invest. In nominal form, these predicates can exist
without such arguments and still be grammatical:
(20) John?s loan was not repaid.
(21) John?s investment was huge.
Note, however, that Examples (20) and (21) are not reasonable things to
write unless the missing arguments were previously mentioned in the text.
This is precisely the type of noun that should be targeted for implicit
argument annotation. The value of Vp ? Np thus quantifies the desired
behavior.
Predicates were filtered according to criteria 1 and 2 and ranked according to the
product of criteria 3 and 4. We then selected the top ten, which are shown in the first
column of Table 1. The role sets (i.e., argument definitions) for these predicates can be
found in the Appendix on page 790.
3.1.2 Annotation Procedure. We annotated implicit arguments for instances of the ten se-
lected nominal predicates. The annotation process proceeded document-by-document.
For a document d, we annotated implicit arguments as follows:
1. Select from d all non-proper singular and non-proper plural nouns that are
morphologically related to the ten predicates in Table 1.
764
G
erber
an
d
C
h
ai
SR
L
of
Im
p
licitA
rgu
m
en
ts
for
N
om
in
alP
red
icates
Table 1
Annotation data analysis. Columns are defined as follows: (1) the annotated predicate, (2) the number of predicate instances that were annotated,
(3) the average number of implicit arguments per predicate instance, (4) of all roles for all predicate instances, the percentage filled by NomBank
arguments, (5) the average number of NomBank arguments per predicate instance, (6) the average number of PropBank arguments per instance of the
verb form of the predicate, (7) of all roles for all predicate instances, the percentage filled by either NomBank or implicit arguments, (8) the average
number of combined NomBank/implicit arguments per predicate instance. SD indicates the standard deviation with respect to an average.
Pre-annotation Post-annotation
Role avg. (SD)
Pred. # Pred. # Imp./pred. Role coverage (%) Noun Verb Role coverage (%) Noun role avg. (SD)
bid 88 1.4 26.9 0.8 (0.6) 2.2 (0.6) 73.9 2.2 (0.9)
sale 184 1.0 24.2 1.2 (0.7) 2.0 (0.7) 44.0 2.2 (0.9)
loan 84 1.0 22.1 1.1 (1.1) 2.5 (0.5) 41.7 2.1 (1.1)
cost 101 0.9 26.2 1.0 (0.7) 2.3 (0.5) 47.5 1.9 (0.6)
plan 100 0.8 30.8 1.2 (0.8) 1.8 (0.4) 50.0 2.0 (0.4)
investor 160 0.7 35.0 1.1 (0.2) 2.0 (0.7) 57.5 1.7 (0.6)
price 216 0.6 42.5 1.7 (0.5) 1.7 (0.5) 58.6 2.3 (0.6)
loss 104 0.6 33.2 1.3 (0.9) 2.0 (0.6) 48.1 1.9 (0.7)
investment 102 0.5 15.7 0.5 (0.7) 2.0 (0.7) 33.3 1.0 (1.0)
fund 108 0.5 8.3 0.3 (0.7) 2.0 (0.3) 21.3 0.9 (1.2)
Overall 1,247 0.8 28.0 1.1 (0.8) 2.0 (0.6) 47.8 1.9 (0.9)
(1) (2) (3) (4) (5) (6) (7) (8)
765
Computational Linguistics Volume 38, Number 4
2. By design, each selected noun has an unambiguous role set. Thus,
given the arguments supplied for a noun by NomBank, one can consult
the noun?s role set to determine which arguments are missing.5
3. For each missing argument position, search the current sentence and
all preceding sentences for a suitable implicit argument. Annotate all
suitable implicit arguments in this window.
4. When possible, match the textual bounds of an implicit argument
to the textual bounds of an argument given by either PropBank or
NomBank. This was done to maintain compatibility with these and
other resources.
In the remainder of this article, we will use iargn to refer to an implicit argument
position n. We will use argn to refer to an argument provided by PropBank or NomBank.
We will use p to mark predicate instances. Example (22) provides a sample annotation
for an instance of the investment predicate:
(22) [iarg0 Participants] will be able to transfer [iarg1 money] to [iarg2 other
investment funds]. The [p investment] choices are limited to [iarg2 a stock
fund and a money-market fund].
NomBank does not associate this instance of investment with any arguments; one can
easily identify the investor (iarg0), the thing invested (iarg1), and two mentions of the
thing invested in (iarg2) within the surrounding discourse, however.
Of course, not all implicit argument decisions are as easy as those in Example (22).
Consider the following contrived example:
(23) People in other countries could potentially consume large amounts of
[iarg0? Coke].
(24) Because of this, there are [p plans] to expand [iarg0 the company?s]
international presence.
Example (24) contains one mention of the iarg0 (the agentive planner). It might be
tempting to also mark Coke in Example (23) as an additional iarg0; the only reasonable
interpretation of Coke in 23 is as a consumable fluid, however. Fluids cannot plan things,
so this annotation should not be performed. This is a case of sense ambiguity between
Coke as a company and Coke as a drink. In all such cases, the annotator was asked to
infer the proper sense before applying an implicit argument label.
Lastly, it should be noted that we placed no restrictions on embedded argu-
ments. PropBank and NomBank do not allow argument extents to overlap. Tra-
ditional SRL systems such as the one created by Punyakanok, Roth, and Yih
(2008) model this constraint explicitly to arrive at the final label assignment; as the
5 See Appendix A for the list of role sets used in this study.
766
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
following example shows, however, this constraint should not be applied to implicit
arguments:
(25) Currently, the rules force [iarg0 executives, directors and other corporate
insiders] to report purchases and [p sales] [arg1 of [iarg0 their] companies?
shares] within about a month after the transaction.
Despite its embedded nature, the pronoun their in Example (25) is a perfectly reasonable
implicit argument (the seller) for the marked predicate. Systems should be required to
identify such arguments; thus, we included them in our annotations.
3.1.3 Inter-annotator Agreement. Implicit argument annotation is a difficult task because
it combines the complexities of traditional SRL annotation with those of coreference
annotation. To assess the reliability of the annotation process described previously, we
compared our annotations to those provided by an undergraduate linguistics student
who, after a brief training period, re-annotated a portion of the data set. For each miss-
ing argument position, the student was asked to identify the textually closest acceptable
implicit argument within the current and preceding sentences. The argument position
was left unfilled if no acceptable constituent could be found. For a missing argument
position iargn, the student?s annotation agreed with our own if both identified the same
implicit argument or both left iargn unfilled. The student annotated 480 of the 1,247
predicate instances shown in Table 1.
We computed Cohen?s chance-corrected kappa statistic for inter-annotator agree-
ment (Cohen 1960), which is based on two quantities:
po = observed probability of agreement
pc = probability of agreement by chance
The quantity 1 ? pc indicates the probability of a chance disagreement. The quantity
po ? pc indicates the probability of agreement that cannot be accounted for by chance
alone. Finally, Cohen defines ? as follows:
? =
po ? pc
1 ? pc
Cohen?s kappa thus gives the probability that a chance-expected disagreement will not
occur. When agreement is perfect, ? = 1. If the observed agreement is less than the
expected chance agreement, then ? will be negative. As noted by Di Eugenio and Glass
(2004), researchers have devised different scales to assess ?. Many NLP researchers use
the scale created by Krippendorff (1980):
? < 0.67 low agreement
0.67 ? ? < 0.8 moderate agreement
? ? 0.8 strong agreement
Di Eugenio and Glass (2004) note, however, that this scale has not been rigorously
defended, even by Krippendorff (1980) himself.
767
Computational Linguistics Volume 38, Number 4
For the implicit argument annotation data, observed and chance agreement are
defined as follows:
po =
?
iargn
agree(iargn)
N
pc =
?
iargn
PA(n) ? PB(n) ? random agree(iargn) + (1 ? PA(n)) ? (1 ? PB(n))
N (1)
where N is the total number of missing argument positions that need to be annotated,
agree is equal to 1 if the two annotators agreed on iargn and 0 otherwise, PA(n) and PB(n)
are the observed prior probabilities that annotators A and B assign the argument label n
to a filler, and random agree is equal to the probability that both annotators would select
the same implicit argument for iargn when choosing randomly from the discourse. In
Equation (1), terms to the right of + denote the probability that the two annotators
agreed on iargn because they did not identify a filler for it.
Using these values for po and pc, Cohen?s kappa indicated an agreement of 64.3%.
According to the scale of Krippendorff (1980), this value is borderline between low and
moderate agreement. Possible causes for this low agreement include the brief training
period for the linguistics student and the sheer complexity of the annotation task. If one
considers only those argument positions for which both annotators actually located an
implicit filler, Cohen?s kappa indicates an agreement of 93.1%. This shows that much
of the disagreement concerned the question of whether a filler was present. Having
agreed that a filler was present, the annotators consistently selected the same filler.
Subsequently, we demonstrate this situation with actual data. First, we present our
annotations for two sentences from the same document:
(26) Shares of UAL, the parent of [iarg1 United Airlines], were extremely active
all day Friday, reacting to news and rumors about the proposed [iarg2 $6.79
billion] buy-out of [iarg1 the airline] by an employee?management group.
(27) And 10 minutes after the UAL trading halt came news that the UAL group
couldn?t get financing for [arg0 its] [p bid].
In Example (27), the predicate is marked along with the explicit arg0 argument. Our
task is to locate the implicit iarg1 (the entity bid for) and the implicit iarg2 (the amount
of the bid). We were able to locate these entities in a previous sentence (Example (26)).
Next, we present the second annotator?s (i.e., the student?s) annotations for the same
two sentences:
(28) Shares of UAL, the parent of [iarg1 United Airlines], were extremely active
all day Friday, reacting to news and rumors about the proposed $6.79
billion buy-out of [iarg1 the airline] by an employee?management group.
(29) And 10 minutes after the UAL trading halt came news that the UAL group
couldn?t get financing for [arg0 its] [p bid].
768
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
As shown in Example (28), the second annotator agreed with our identification of the
iarg1; the second annotator did not mark an implicit iarg2, however, despite the fact that
it can be inferred. We believe this type of error can be addressed with additional train-
ing. The student?s annotations were only used to compute agreement. We performed
all training and evaluation using randomized cross-validation over the annotations we
created.
3.2 Annotation Analysis
We carried out this annotation process on the standard training (2?21), development
(24), and testing (23) sections of the Penn TreeBank. Table 1 summarizes the results. In
this section, we highlight key pieces of information found in this table.
3.2.1 Implicit Arguments are Frequent. Column (3) of Table 1 shows that most predicate
instances are associated with at least one implicit argument. Implicit arguments vary
across predicates, with bid exhibiting (on average) more than one implicit argument
per instance versus the 0.5 implicit arguments per instance of the investment and fund
predicates. It turned out that the latter two predicates have unique senses that preclude
implicit argumentation (more on this in Section 6).
3.2.2 Implicit Arguments Create Fuller Event Descriptions. Role coverage for a predicate
instance is equal to the number of filled roles divided by the number of roles in the
predicate?s role set. Role coverage for the marked predicate in Example (22) is 0/3 for
NomBank-only arguments and 3/3 when the annotated implicit arguments are also
considered. Returning to Table 1, the fourth column gives role coverage percentages
for NomBank-only arguments. The seventh column gives role coverage percentages
when both NomBank arguments and the annotated implicit arguments are considered.
Overall, the addition of implicit arguments created a 71% relative (20-point absolute)
gain in role coverage across the 1,247 predicate instances that we annotated.
3.2.3 The Vp ? Np Predicate Selection Metric Behaves as Desired. The predicate selection
method used the Vp ? Np metric to identify predicates whose instances are likely to take
implicit arguments. Column (5) in Table 1 shows that (on average) nominal predicates
have 1.1 arguments in NomBank; this compared to the 2.0 arguments per verbal form
of the predicates in PropBank (compare columns (5) and (6)). We hypothesized that
this difference might indicate the presence of approximately one implicit argument per
predicate instance. This hypothesis is confirmed by comparing columns (6) and (8):
When considering implicit arguments, many nominal predicates express approximately
the same number of arguments on average as their verbal counterparts.
3.2.4 Most Implicit Arguments Are Nearby. In addition to these analyses, we examined the
location of implicit arguments in the discourse. Figure 1 shows that approximately 56%
of the implicit arguments in our data can be resolved within the sentence containing
the predicate. Approximately 90% are found within the previous three sentences. The
remaining implicit arguments require up to 4?6 sentences for resolution. These obser-
vations are important; they show that searching too far back in the discourse is likely to
769
Computational Linguistics Volume 38, Number 4
Figure 1
Location of implicit arguments. Of all implicitly filled argument positions, the y-axis indicates
the percentage that are filled at least once within the number of sentences indicated by the x-axis
(multiple fillers may exist for the same position).
produce many false positives without a significant increase in recall. Section 6 discusses
additional implications of this skewed distribution.
4. Implicit Argument Model
4.1 Model Formulation
Given a nominal predicate instance p with a missing argument position iargn, the task
is to search the surrounding discourse for a constituent c that fills iargn. The implicit
argument model conducts this search over all constituents that are marked with a core
argument label (arg0, arg1, etc.) associated with a NomBank or PropBank predicate.
Thus, the model assumes a pipeline organization in which a document is initially
analyzed by traditional verbal and nominal SRL systems. The core arguments from
this stage then become candidates for implicit argumentation. Adjunct arguments are
excluded.
A candidate constituent c will often form a coreference chain with other constituents
in the discourse. Consider the following abridged sentences, which are adjacent in their
Penn TreeBank document:
(30) [Mexico] desperately needs investment.
(31) Conservative Japanese investors are put off by [Mexico?s] investment
regulations.
(32) Japan is the fourth largest investor in [c Mexico], with 5% of the total
[p investments].
NomBank does not associate the labeled instance of investment with any arguments, but
it is clear from the surrounding discourse that constituent c (referring to Mexico) is the
thing being invested in (the iarg2). When determining whether c is the iarg2 of investment,
770
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Table 2
Primary feature groups used by the model. The third column gives the number of features in the
group, and the final column gives the number of features from the group that were ranked in the
top 20 among all features.
Feature group Resources used # Top 20
(1) Textual semantics PropBank, NomBank 13 4
(2) Ontologies FrameNet, VerbNet, WordNet 8 4
(3) Filler-independent Penn TreeBank 35 7
(4) Corpus statistics Gigaword, Verbal SRL, Nominal SRL 9 1
(5) Textual discourse Penn Discourse Bank 1 0
(6) Other Penn TreeBank 15 4
one can draw evidence from other mentions in c?s coreference chain. Example (30) states
that Mexico needs investment. Example (31) states that Mexico regulates investment.
These propositions, which can be derived via traditional SRL analyses, should increase
our confidence that c is the iarg2 of investment in Example (32).
Thus, the unit of classification for a candidate constituent c is the three-tuple
?p, iargn, c??, where c? is a coreference chain comprising c and its coreferent constituents.6
We defined a binary classification function Pr(+| ?p, iargn, c??) that predicts the probabil-
ity that the entity referred to by c fills the missing argument position iargn of predicate
instance p. In the remainder of this article, we will refer to c as the primary filler,
differentiating it from other mentions in the coreference chain c?. This distinction is
necessary because our evaluation requires the model to select at most one filler (i.e., c)
for each missing argument position. In the following section, we present the feature
set used to represent each three-tuple within the classification function.
4.2 Model Features
Appendix Table B.1 lists all features used by the model described in the previous
section. For convenience, Table 2 presents a high-level grouping of the features and
the resources used to compute them. The broadest distinction to be made is whether a
feature depends on elements of c?. Features in Group 3 do not, whereas all others do.
The features in Group 3 characterize the predicate?argument position being filled (p and
iargn), independently of the candidate filler. This group accounts for 43% of the features
and 35% of those in the top 20.7 The remaining features depend on elements of c? in some
way. Group 1 features characterize the tuple using the SRL propositions contained in
the text being evaluated. Group 2 features place the ?p, iargn, c?? tuple into a manually
constructed ontology and compute a value based on the structure of that ontology.
Group 4 features compute statistics of the tuple within a large corpus of semantically
analyzed text. Group 5 contains a single feature that captures the discourse structure
properties of the tuple. Group 6 contains all other features, most of which capture
the syntactic relationships between elements of c? and p. In the following sections, we
provide detailed examples of features from each group shown in Table 2.
6 We used OpenNLP for coreference identification: http://opennlp.sourceforge.net.
7 Features were ranked according to the order in which they were selected during feature selection
(Section 5.3 for details).
771
Computational Linguistics Volume 38, Number 4
4.2.1 Group 1: Features Derived from the Semantic Content of the Text. Feature 1 was often
selected first by the feature selection algorithm. This feature captures the semantic
properties of the candidate filler c? and the argument position being filled. Consider
the following Penn TreeBank sentences:
(33) [arg0 The two companies] [p produce] [arg1 market pulp, containerboard
and white paper]. The goods could be manufactured closer to customers,
saving [p shipping] costs.
Here we are trying to fill the iarg0 of shipping. Let c? contain a single mention, The two
companies, which is the arg0 of produce. Feature 1 takes a value of produce ? arg0 ? ship ?
arg0. This value, which is derived from the text itself, asserts that producers are also
shippers. To reduce data sparsity, we generalized the predicates to their WordNet synset
IDs (creating Feature 4). We also generalized the predicates and arguments to their
VerbNet thematic roles using SemLink (creating Feature 23). Although the generalized
features rely on ontologies, they do so in a trivial way that does not take advantage of
the detailed structure of the ontologies. Such structure is used by features in the next
group.
4.2.2 Group 2: Features Derived from Manually Constructed Ontologies. Feature 9 captures
the semantic relationship between predicate?argument positions by examining paths
between frame elements in FrameNet. SemLink8 maps PropBank argument positions to
their FrameNet frame elements. For example, the arg1 position of sell maps to the Goods
frame element of the Sell frame. NomBank argument positions (e.g., arg1 of sale) can
be mapped to FrameNet by first converting the nominal predicate to its verb form. By
mapping predicate?argument structures into FrameNet, one can take advantage of the
rich network of frame?frame relations provided by the resource.
The value of Feature 9 has the following general form:
(34) Frame1.FE1
Rel1???? Frame2.FE2
Rel2???? . . .
Reln?1?????? Framen.FEn
This path describes how the frame elements at either end are related. For example,
consider the frame element path between the arg1 of sell and the arg1 of buy, both of
which denote the goods being transferred:
(35) Sell.Goods Inherits????? Give.Theme Causes???? Get.Theme Inherited by??????? Buy.Goods
This path can be paraphrased as follows: things that are sold (Sell.Goods) are part of
a more general give scenario (Give.Theme) that can also be viewed as a get scenario
(Get.Theme) in which the buyer receives something (Buy.Goods). This complex world
knowledge is represented compactly using the relationships defined in FrameNet. In
our experiments, we searched all possible frame element paths of length five or less
that use the following relationships:
 Causative?of
8 http://verbs.colorado.edu/semlink.
772
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
 Inchoative?of
 Inherits
 Precedes
 Subframe?of
Feature 9 is helpful in situations such as the following (contrived):
(36) Consumers bought many [c cars] this year at reduced prices.
(37) [p Sales] are expected to drop when the discounts are eliminated.
In Example (37) we are looking for the iarg1 (thing sold) of sale. The path shown in
Example (35) indicates quite clearly that the candidate cars from Example (36), being
the entity purchased, is a suitable filler for this position. Lastly, note that the value
for Feature 9 is the actual path instead of a numeric value. When c? contains multiple
coreferential elements, this feature can be instantiated using multiple values (i.e., paths).
Ultimately, these instantiations are binarized into the LibLinear input format, so the
existence of multiple feature values does not pose a problem.
Feature 59 is similar to Feature 9 (the frame element path) except that it cap-
tures the distance between predicate?argument positions within the VerbNet hierarchy.
Consider the following VerbNet classes:
13.2 lose, refer, relinquish, remit, resign, restore, gift, hand out, pass out, shell out
13.5.1.1 earn, fetch, cash, gain, get, save, score, secure, steal
The path from earn to lose in the VerbNet hierarchy is as follows:
(38) 13.5.1.1 ? 13.5.1 ? 13.5 ? 13 ? 13.2
The path in Example (38) is four links long. Intuitively, earn and lose are related to each
other?they describe two possible outcomes of a financial transaction. The VerbNet path
quantifies this intuition, with shorter paths indicating closer relationships. This informa-
tion can be used to identify implicit arguments in situations such as the following from
the Penn TreeBank (abridged):
(39) [c Monsanto Co.] is expected to continue reporting higher [p earnings].
(40) The St. Louis-based company is expected to report that [p losses] are
narrowing.
In Example (40) we are looking for the iarg0 (i.e., entity losing something) for the loss
predicate. According to SemLink, this argument position maps to the 13.2.Agent role in
VerbNet. In Example (39), we find the candidate implicit argument Monsanto Co., which
is the arg0 to the earning predicate in that sentence. This argument position maps to
the 13.5.1.1.Agent role in VerbNet. These two VerbNet roles are related according to the
VerbNet path in Example (38), producing a value for Feature 59 of four. This relatively
small value supports an inference of Monsanto Co. as the iarg0 for loss. It is important to
note that a VerbNet path only exists when the thematic roles are identical. For example, a
VerbNet path would not exist between 13.5.1.1.Theme and 13.2.Agent because the roles
773
Computational Linguistics Volume 38, Number 4
are not compatible. Lastly, recall that c? might contain multiple coreferential elements.
In such a situation, the minimum path length is selected as the value for this feature.
4.2.3 Group 3: Filler-independent Features. Many of the features used by the model do not
depend on elements of c?. These features are usually specific to a particular predicate.
Consider the following example:
(41) Statistics Canada reported that its [arg1 industrial?product] [p price] index
dropped 2% in September.
The ?[p price] index? collocation is rarely associated with an arg0 in NomBank or with
an iarg0 in the annotated data (both argument positions denote the seller). Feature 25
accounts for this type of behavior by encoding the syntactic head of p?s right sibling.
The value of Feature 25 for Example 41 is price:index. Contrast this with the following:
(42) [iarg0 The company] is trying to prevent further [p price] drops.
The value of Feature 25 for Example (42) is price:drop. This feature captures an important
distinction between the two uses of price: the former cannot easily take an iarg0, whereas
the latter can. Many other features in Table B.1 depend only on the predicate and have
values that take the form predicate:feature value.
4.2.4 Group 4: Features Derived from Corpus Statistics. Feature 13 is inspired by the work
of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative
event sequences using pointwise mutual information (PMI) between syntactic positions.
We extended this PMI score to semantic arguments instead of syntactic dependencies.
Thus, the value for this feature is computed as follows:
pmi(?p1, argi? ,
?
p2, argj
?
) = log
Pcoref pmi(?p1, argi? ,
?
p2, argj
?
)
Pcoref pmi(?p1, argi? , ?)Pcoref pmi(
?
p2, argj
?
, ?)
(2)
We computed Equation (2) by applying verbal SRL (Punyakanok, Roth, and Yih
2008), nominal SRL (Gerber, Chai, and Meyers 2009), and coreference identification
(OpenNLP) to the Gigaword corpus (Graff 2003); because these tools are not fast enough
to process all 1,000,000 documents in the corpus, however, we selected subsets of
the corpus for each p1/p2 combination observed in our implicit argument data. We
first indexed the Gigaword corpus using the Lucene search engine.9 We then queried
this index using the simple boolean query ?p1 AND p2,? which retrieved documents
relevant to the predicates considered in Equation (2). Assuming the resulting data have
N coreferential pairs of arguments, the numerator in Equation (2) is defined as follows:
Pcoref pmi(?p1, argi? ,
?
p2, argj
?
) =
#coref (?p1, argi? ,
?
p2, argj
?
)
N (3)
In Equation (3), #coref returns the number of times the given argument positions
are found to be coreferential. In order to penalize low-frequency observations with
9 http://lucene.apache.org.
774
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
artificially high scores, we used the simple discounting method described by Pantel
and Ravichandran (2004) resulting in the following modification of Equation (3):
x = #coref (?p1, argi? ,
?
p2, argj
?
)
Pcoref pmi(?p1, argi? ,
?
p2, argj
?
) = xN ?
x
x + 1
(4)
Thus, if two argument positions are rarely observed as coreferent, the value xx+1 will
be small, reducing the PMI score. The denominator in Equation (2) is computed with a
similar discount factor:
x1 = #coref (?p1, argi? , ?)
x2 = #coref (
?
p2, argj
?
, ?)
Pcoref pmi(?p1, argi? , ?)Pcoref pmi(
?
p2, argj
?
, ?) = x1x2
(N2) min(x1,x2 )min(x1,x2 )+1
(5)
Thus, if either of the argument positions is rarely observed as coreferent with other
argument positions, the value min(x1,x2 )min(x1,x2 )+1 will be small, making the denominator of
Equation (2) large, reducing the PMI score. In general, the discount factors reduce the
PMI score for argument positions that are not frequent in the corpus.
We refer to Equation (2) as a targeted PMI score because it relies on data that have
been chosen specifically for the calculation at hand. Table 3 shows a sample of targeted
PMI scores between the arg1 of loss and other argument positions. There are two things
to note about this data: First, the argument positions listed are all naturally related to
the arg1 of loss. Second, the discount factor changes the final ranking by moving the
less frequent recoup predicate from a raw rank of 1 to a discounted rank of 3, preferring
instead the more common win predicate.
The information in Table 3 is useful in situations such as the following (contrived):
(43) Mary won [c the tennis match].
(44) [arg0 John?s] [p loss] was not surprising.
In Example (44) we are looking for the iarg1 of loss. The information in Table 3 strongly
suggests that the marked candidate c, being the arg1 of win, would be a suitable filler
Table 3
Targeted PMI scores between the arg1 of loss and other argument positions. The second column
gives the number of times that the argument position in the row is found to be coreferent with
the arg1 of the loss predicate. A higher value in this column results in a lower discount factor.
See Equation (4) for the discount factor.
Argument position #coref with loss.arg1 Raw PMI score Discounted PMI score
win.arg1 37 5.68 5.52
gain.arg1 10 5.13 4.64
recoup.arg1 2 6.99 4.27
steal.arg1 4 5.18 4.09
possess.arg1 3 5.10 3.77
775
Computational Linguistics Volume 38, Number 4
for this position. Lastly, note that if c were to form a coreference chain with other
constituents, it would be possible to calculate multiple PMI scores. In such cases, the
targeted PMI feature uses the maximum of all scores.
Feature 27 captures the selectional preference of a predicate p for the elements in c?
with respect to argument position iargn. In general, selectional preference scores denote
the strength of attraction for a predicate?argument position to a particular word or
class of words. To calculate the value for this feature, we used the information?theoretic
model proposed by Resnik (1996), which is defined as follows:
Pref (p, argn, s ? WordNet) =
Pr(s|p, argn)log
Pr(s|p, argn)
Pr(s)
Z (6)
Z =
?
si?WordNet
Pr(si|p, argn)log
Pr(si|p, argn)
Pr(si)
In Equation (6), Pref calculates the preference for a WordNet synset s in the given
predicate?argument position. Prior and posterior probabilities for s were calculated by
examining the arguments present in the Penn TreeBank combined with 20,000 docu-
ments randomly selected from the Gigaword corpus. PropBank and NomBank supplied
arguments for the Penn TreeBank, and we used the aforementioned verbal and nominal
SRL systems to extract arguments from Gigaword. The head word for each argument
was mapped to its WordNet synsets, and counts for these synsets were updated as
suggested by Resnik (1996). Note that a synset s that is not observed in the training
data will receive a score of zero because Pr(s|p, argn) will be zero.
Equation (6) computes the preference of a predicate?argument position for a synset;
a single word can map to multiple synsets if its sense is ambiguous, however. Given a
word w and its synsets s1, s2, . . . , sn, the preference of a predicate?argument position for
w is defined as follows:
Pref (p, argn,w) =
?
si Pref (p, argn, si)
n (7)
That is, the preference for a word is computed as the average preference across all possi-
ble synsets. The final value for Feature 27 is computed using the word-based preference
score defined in Equation (7). Given a candidate implicit argument c? comprising the
primary filler c and its coreferent mentions, the following value is obtained:
Pref (p, iargn, c
?) = min
f?c?
Pref (p, argn, f ) (8)
In Equation (8), each f is the syntactic head of a constituent from c?. The value of
Equation (8) is in (??,+?), with larger values indicating higher preference for c as
the implicit filler of position iargn.
Feature 33 implements the suggestion of Burchardt, Frank, and Pinkal (2005) that
implicit arguments might be identified using observed coreference patterns in a large
corpus of text. Our implementation of this feature uses the same data used for the
previous feature: arguments extracted from the Penn TreeBank and 20,000 documents
randomly selected from Gigaword. Additionally, we identified coreferent arguments
in this corpus using OpenNLP. Using this information, we calculated the probability
of coreference between any two argument positions. As with Feature 13, we used
776
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
discounting to penalize low?frequency observations, producing an estimate of
coreference probability as follows:
Corefjoint = #coref (?p1, argi? ,
?
p2, argj
?
)
Corefmarginal = #coref (?p1, argi? , ?)
Pcoref (?p1, argi? ,
?
p2, argj
?
) =
Corefjoint
Corefmarginal
?
Corefjoint
Corefjoint + 1
?
Corefmarginal
Corefmarginal + 1
(9)
In Equation (9), Pcoref should be read as ?the probability that ?p1, argi? is coref-
erential with
?
p2, argj
?
given ?p1, argi? is coreferential with something.? For example,
we observed that the arg1 for predicate reassess (the entity reassessed) is coreferential
with six other constituents in the corpus. Table 4 lists the argument positions with
which this argument is coreferential along with the raw and discounted probabilities.
The discounted probabilities can help identify the implicit argument in the following
contrived examples:
(45) Senators must rethink [c their strategy for the upcoming election].
(46) The [p reassessment] must begin soon.
In Example (46) we are looking for the iarg1 of reassess. Table 4 tells us that the marked
candidate (an arg1 to rethink) is likely to fill this missing argument position. When
c forms a coreference chain with other constituents, this feature uses the minimum
coreference probability between the implicit argument position and elements in the
chain.
4.2.5 Group 5: Features Derived from the Discourse Structure of the Text. Feature 67 identifies
the discourse relation (if any) that holds between the candidate constituent c and the
filled predicate p. Consider the following example:
(47) [iarg0 SFE Technologies] reported a net loss of $889,000 on sales of $23.4
million.
(48) That compared with an operating [p loss] of [arg1 $1.9 million] on sales of
$27.4 million in the year?earlier period.
In this case, a comparison discourse relation (signaled by the underlined text) holds be-
tween the first and second sentence. The coherence provided by this relation encourages
Table 4
Coreference probabilities between reassess.arg1 and other argument positions. See Equation (9)
for details on the discount factor.
Argument Raw coreference probability Discounted coreference probability
rethink.arg1 3/6 = 0.5 0.32
define.arg1 2/6 = 0.33 0.19
redefine.arg1 1/6 = 0.17 0.07
777
Computational Linguistics Volume 38, Number 4
an inference that identifies the marked iarg0 (the loser). The value for this feature is the
name of the discourse relation (e.g., comparison) whose two discourse units cover the
candidate (iarg0 above) and filled predicate (p above). Throughout our investigation,
we used gold-standard discourse relations provided by the Penn Discourse TreeBank
(Prasad et al 2008).
4.2.6 Group 6: Other Features. A few other features that were prominent according to our
feature selection process are not contained in the groups described thus for. Feature 2
encodes the sentence distance from c (the primary filler) to the predicate for which we
are filling the implicit argument position. The prominent position of this feature agrees
with our previous observation that most implicit arguments can be resolved within a
few sentences of the predicate (see Figure 1 on p. 770). Feature 3 is another simple yet
highly ranked feature. This feature concatenates the head of an element of c? with p
and iargn. For example, in sentences (45) and (46), this feature would have a value of
strategy ? reassess ? arg1, asserting that strategies are reassessed. Feature 5 generalizes
this feature by replacing the head word with its WordNet synset.
4.2.7 Comparison with Features for Traditional SRL. The features described thus far are
quite different from those used in previous work to identify arguments in the traditional
nominal SRL setting (see the work of Gerber, Chai, and Meyers 2009). The most impor-
tant feature used in traditional SRL?the syntactic parse tree path?is notably absent.
This difference is due to the fact that syntactic information, although present, does not
play a central role in the implicit argument model. The most important features are
those that capture semantic properties of the implicit predicate?argument position and
the candidate filler for that position.
4.3 Post-processing for Final Output Selection
Without loss of generality, assume there exists a predicate instance p with two missing
argument positions iarg0 and iarg1. Also assume that there are three candidate fillers
c1, c2, and c3 within the candidate window. The discriminative model will calculate the
probability that each candidate fills each missing argument position. Graphically:
iarg0 iarg1
c1 0.3 0.4
c2 0.1 0.05
c3 0.6 0.3
There exist two constraints on possible assignments of candidates to positions. First, a
candidate may not be assigned to more than one missing argument position. To enforce
this constraint, only the top-scoring cell in each row is retained, leading to the following:
iarg0 iarg1
c1 - 0.4
c2 0.1 -
c3 0.6 -
778
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Second, a missing argument position can only be filled by a single candidate. To enforce
this constraint, only the top-scoring cell in each column is retained, leading to the
following:
iarg0 iarg1
c1 - 0.4
c2 - -
c3 0.6 -
Having satisfied these constraints, a threshold t is imposed on the remaining cell prob-
abilities.10 Cells with probabilities less than t are cleared. Assuming that t = 0.42, the
final assignment would be as follows:
iarg0 iarg1
c1 - -
c2 - -
c3 0.6 -
In this case, c3 fills iarg0 with probability 0.6 and iarg1 remains unfilled. The latter
outcome is desirable because not all argument positions have fillers that are present
in the discourse.
5. Evaluation
5.1 Data
All evaluations in this study were performed using a randomized cross-validation
configuration. The 1,247 predicate instances were annotated document by document.
In order to remove any confounding factors caused by specific documents, we first
randomized the annotated predicate instances. Following this, we split the predicate
instances evenly into ten folds and used each fold as testing data for a model trained on
the instances outside the fold. This evaluation set-up is an improvement versus the one
we previously reported (Gerber and Chai 2010), in which fixed partitions were used for
training, development, and testing.
During training, the system was provided with annotated predicate instances. The
system identified missing argument positions and generated a set of candidates for
each such position. A candidate three-tuple ?p, iargn, c?? was given a positive label if the
candidate implicit argument c (the primary filler) was annotated as filling the missing
argument position; otherwise, the candidate three-tuple was given a negative label.
During testing, the system was presented with each predicate instance and was required
to identify all implicit arguments for the predicate.
10 The threshold t is learned from the training data. The learning mechanism is explained in the following
section.
779
Computational Linguistics Volume 38, Number 4
Throughout the evaluation process we assumed the existence of gold-standard
PropBank and NomBank information in all documents. This factored out errors from
traditional SRL and affected the following stages of system operation:
 Missing argument identification. The system was required to figure out
which argument positions were missing. Each of the ten predicates was
associated with an unambiguous role set, so determining the missing
argument positions amounted to comparing the existing local arguments
with the argument positions listed in the predicate?s role set. Because
gold-standard local NomBank arguments were used, this stage produced
no errors.
 Candidate generation. As mentioned in Section 4.1, the set of candidates
for a missing argument position contains constituents labeled with a core
(e.g., arg0) PropBank or NomBank argument label. We used gold-standard
PropBank and NomBank arguments; it is not the case that all annotated
implicit arguments are given a core argument label by PropBank or
NomBank, however. Thus, despite the gold-standard argument labels,
this stage produced errors in which the system failed to generate a
true-positive candidate for an implicit argument position. Approximately
96% of implicit argument positions are filled by gold-standard PropBank
or NomBank arguments.
 Feature extraction. Many of the features described in Section 4.2 rely on
underlying PropBank and NomBank argument labels. For example, the
top-ranked Feature 1 relates the argument position of the candidate to the
missing argument position. In our experiments, values for this feature
contained no errors because gold-standard PropBank and NomBank labels
were used. Note, however, that many features were derived from the
output of an automatic SRL process that occasionally produced errors
(e.g., Feature 13, which used PMI scores between automatically identified
arguments). These errors were present in both the training and evaluation
stages.
We also assumed the existence of gold-standard syntactic structure when possible.
This was done in order to focus our investigation on the semantic nature of implicit
arguments.
5.2 Scoring Metrics
We evaluated system performance using the methodology proposed by Ruppenhofer
et al (2010). For each missing argument position of a predicate instance, the system
was required to either (1) identify a single constituent that fills the missing argument
position or (2) make no prediction and leave the missing argument position unfilled. To
give partial credit for inexact argument bounds, we scored predictions using the Dice
coefficient, which is defined as follows:
Dice(Predicted,True) =
2 ? |Predicted
?
True|
|Predicted|+ |True| (10)
780
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Predicted contains the tokens that the model believes fill the implicit argument position.
True is the set of tokens from a single annotated constituent that fills the missing
argument position. The model?s prediction receives a score equal to the maximum Dice
overlap across any one of the annotated fillers (AF):
Score(Predicted) = max
True?AF
Dice(Predicted,True) (11)
Precision is equal to the summed prediction scores divided by the number of argument
positions filled by the model. Recall is equal to the summed prediction scores divided
by the number of argument positions filled in the annotated data. Predictions not cover-
ing the head of a true filler were assigned a score of zero.11 For example, consider
the following true and predicted labelings:
(49) True labeling: [iarg0 Participants] will be able to transfer [iarg1 money] to
[iarg2 other investment funds]. The [p investment] choices are limited to
[iarg2 a stock fund and a money-market fund].
(50) Predicted labeling: Participants will be able to transfer [iarg1 money] to
other [iarg2 investment funds]. The [p investment] choices are limited to a
stock fund and a money-market fund.
In the ground-truth (49) there are three implicit argument positions to fill. The hypo-
thetical system has made predictions for two of the positions. The prediction scores are:
Score(iarg1 money) = Dice(money,money) = 1
Score(iarg2 investment funds) = max{Dice(investment funds, other investment funds),
Dice(investment funds, a stock . . . money?market fund)}
= max{0.8, 0} = 0.8
Precision, recall, and F1 for the example predicate are calculated as follows:
Precision = 1.82 = 0.9
Recall = 1.83 = 0.6
F1 =
2 ? Precision ? Recall
Precision + Recall
= 0.72
We calculated the F1 score for the entire testing fold by aggregating the counts used in
the above precision and recall calculations. Similarly, we aggregated the counts across
all folds to arrive at a single F1 score for the evaluated system.
We used a bootstrap resampling technique similar to those developed by Efron and
Tibshirani (1993) to test the significance of the performance difference between various
systems. Given a test pool comprising M missing argument positions iargn along with
11 Our evaluation methodology differs slightly from that of Ruppenhofer et al (2010) in that we use the
Dice metric to compute precision and recall, whereas Ruppenhofer et al reported the Dice metric
separately from exact-match precision and recall.
781
Computational Linguistics Volume 38, Number 4
the predictions by systems A and B for each iargn, we calculated the exact p-value of the
performance difference as follows:
1. Create r random resamples from M with replacement.
2. For each resample Ri, compute the system performance difference
dRi = ARi ? BRi and store dri in D.
3. Find the largest symmetric interval [min,max] around the mean of D that
does not include zero.
4. The exact p-value equals the percentage of elements in D that are not in
[min,max].
Experiments have shown that this simple approach provides accurate estimates of
significance while making minimal assumptions about the underlying data distribution
(Efron and Tibshirani 1993). Similar randomization tests have been used to evaluate
information extraction systems (Chinchor, Lewis, and Hirschmant 1993).
5.3 LibLinear Model Configuration
Given a testing fold Ftest and a training fold Ftrain, we performed floating forward
feature subset selection using only the information contained in Ftrain. We used an
algorithm similar to the one described by Pudil, Novovicova, and Kittler (1994). As
part of the feature selection process, we conducted a grid search for the best c and w
LibLinear parameters, which govern the per-class cost of mislabeling instances from
a particular class (Fan et al 2008). Setting per-class costs helps counter the effects of
class size imbalance, which is severe even when selecting candidates from the current
and previous few sentences (most candidates are negative). We ran the feature selection
and grid search processes independently for each Ftrain. As a result, the feature set and
model parameters are slightly different for each fold.12 For all folds, we used LibLinear?s
logistic regression solver and a candidate selection window of two sentences prior. As
shown in Figure 1, this window imposes a recall upper bound of approximately 85%.
The post-processing prediction threshold t was learned using a brute-force search that
maximized the system?s performance over the data in Ftrain.
5.4 Baseline and Oracle Models
We compared the supervised model with the simple baseline heuristic defined below:
Fill iargn for predicate instance p with the nearest constituent in the two-sentence
candidate window that fills argn for a different instance of p, where all nominal
predicates are normalized to their verbal forms.
The normalization allows, for example, an existing arg0 for the verb invested to fill an
iarg0 for the noun investment. This heuristic outperformed a more complicated heuristic
that relied on the PMI score described in Section 4.2. We also evaluated an oracle model
that made gold-standard predictions for candidates within the two-sentence prediction
window.
12 See Appendix Table C.1 for a per-fold listing of features and model parameters.
782
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
5.5 Results
Table 5 presents the evaluation results for implicit argument identification. Overall,
the discriminative model increased F1 performance by 21.4 percentage points (74.1%)
compared to the baseline (p < 0.0001). Predicates with the highest number of implicit
arguments (sale and price) showed F1 increases of 13.7 and 17.5 percentage points,
respectively (p < 0.001 for both differences). As expected, oracle precision is 100% for
all predictions, and the F1 difference between the discriminative and oracle systems is
significant at p < 0.0001 for all test sets. See the Appendix for a per-fold breakdown of
results and a listing of features and model parameters used for each fold.
We also measured human performance on this task by running the undergraduate
assistant?s annotations against a small portion of the evaluation data comprising 275
filled implicit arguments. The assistant achieved an overall F1 score of 56.0% using the
same two-sentence candidate window used by the baseline, discriminative, and oracle
models. Using an infinite candidate window, the assistant increased F1 performance
to 64.2%. Although these results provide a general idea about the performance upper
bound, they are not directly comparable to the cross-validated results shown in Table 5
because the assistant did not annotate the entire data set.
6. Discussion
6.1 Training Set Size
As described in Section 3.1, implicit argument annotation is an expensive process.
Thus, it is important to understand whether additional annotation would benefit the
ten predicates considered. In order to estimate the potential benefits, we measured the
effect of training set size on system performance. We retrained the discriminative model
for each evaluation fold using incrementally larger subsets of the complete training set
for the fold. Figure 2 shows the results, which indicate minimal gains beyond 80% of
the training set. Based on these results, we feel that future work should emphasize
feature and model development over training data expansion, as gains appear to trail
off significantly.
6.2 Feature Assessment
Previously (Gerber and Chai 2010), we assessed the importance of various implicit
argument feature groups by conducting feature ablation tests. In each test, the discrimi-
native model was retrained and reevaluated without a particular group of features. We
summarize the findings of this study in this section.
6.2.1 Semantic Roles are Essential. We observed statistically significant losses when ex-
cluding features that relate the semantic roles of elements in c? to the semantic role of the
missing argument position. For example, Feature 1 appears as the top-ranked feature
in eight out of ten fold evaluations (see Appendix Table C.1). This feature is formed
by concatenating the filling predicate?argument position with the filled predicate?
argument position, producing values such as invest.arg0-lose.arg0. This value indicates
that the entity performing the investing is also the entity losing something. This type of
commonsense knowledge is essential to the task of implicit argument identification.
783
C
om
p
u
tation
alL
in
gu
istics
V
olu
m
e
38,N
u
m
ber
4
Table 5
Overall evaluation results for implicit argument identification. The second column gives the number of ground?truth implicitly filled argument
positions for the predicate instances. P, R, and F1 indicate precision, recall, and F?measure (? = 1), respectively. pexact is the bootstrapped exact
p-value of the F1 difference between two systems, where the systems are (B)aseline, (D)iscriminative, and (O)racle.
Baseline Discriminative Oracle
# Imp. args. P R F1 P R F1 pexact(B,D) P R F1 pexact(D,O)
sale 181 57.0 27.7 37.3 59.2 44.8 51.0 0.0003 100.0 72.4 84.0 <0.0001
price 138 67.1 23.3 34.6 56.0 48.7 52.1 <0.0001 100.0 78.3 87.8 <0.0001
bid 124 66.7 14.5 23.8 60.0 36.3 45.2 <0.0001 100.0 60.5 75.4 <0.0001
investor 108 30.0 2.8 5.1 46.7 39.8 43.0 <0.0001 100.0 84.3 91.5 <0.0001
cost 86 60.0 10.5 17.8 62.5 50.9 56.1 <0.0001 100.0 86.0 92.5 <0.0001
loan 82 63.0 20.7 31.2 67.2 50.0 57.3 <0.0001 100.0 89.0 94.2 <0.0001
plan 77 72.7 20.8 32.3 59.6 44.1 50.7 0.0032 100.0 87.0 93.1 <0.0001
loss 62 78.8 41.9 54.7 72.5 59.7 65.5 0.0331 100.0 88.7 94.0 <0.0001
fund 56 66.7 10.7 18.5 80.0 35.7 49.4 <0.0001 100.0 66.1 79.6 <0.0001
investment 52 28.9 10.6 15.5 32.9 34.2 33.6 0.0043 100.0 80.8 89.4 <0.0001
Overall 966 61.4 18.9 28.9 57.9 44.5 50.3 <0.0001 100.0 78.0 87.6 <0.0001
784
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Figure 2
Effect of training set size on performance of discriminative model. The x-axis indicates the
percentage of training data used, and the y-axis indicates the overall F1 score that results.
6.2.2 Other Information is Important. Our 2010 study also found that semantic roles are
only one part of the solution. Using semantic roles in isolation also produced statistically
significant losses. This indicates that other features contribute useful information to
the task.
6.2.3 Discourse Structure Is not Essential. We also tested the effect of removing discourse
relations (Feature 67) from the model. Discourse structure has received a significant
amount of attention in NLP; it remains a very challenging problem, however, with
state-of-the-art systems attaining F1 scores in the mid-40% range (Sagae 2009). Our 2010
work as well as the updated work presented in this article used gold-standard discourse
relations from the Penn Discourse TreeBank. As shown by Sagae (2009), these relations
are difficult to extract in a practical setting. In our 2010 work, we showed that removing
discourse relations from the model did not have a statistically significant effect on
performance. Thus, this information should be removed in practical applications of the
model, at least until better uses for it can be identified.
6.2.4 Relative Feature Importance. We extended earlier findings by assessing the relative
importance of the features. We aggregated the feature rank information given in Ap-
pendix Table C.1. For each evaluation fold, each feature received a point value equal
to its reciprocal rank within the feature list. Thus, a feature appearing at rank 5 for a
fold would receive 15 = 0.2 points for that fold. We totaled these points across all folds,
arriving at the values shown in the final column of Appendix Table B.1. The scores
confirm the earlier findings. The highest scoring feature relates the semantic roles of
the candidate argument to the missing argument position. Non-semantic information
such as the sentence distance (Feature 2) also plays a key role. Discourse structure is
consistently ranked near the bottom of the list (Feature 67).
6.3 Error Analysis
Table 6 lists the errors made by the system and their frequencies. As shown, the single
most common error (type 1) occurred when a true filler was classified but an incor-
rect filler had a higher score. This occurred in approximately 31% of the error cases.
785
Computational Linguistics Volume 38, Number 4
Table 6
Implicit argument error analysis. The second column indicates the type of error that was made
and the third column gives the percentage of all errors that fall into each type.
# Description %
1 A true filler was classified but an incorrect filler scored higher 30.6
2 A true filler did not exist but a prediction was made 22.4
3 A true filler existed within the window but was not a candidate 21.1
4 A true filler scored highest but below the threshold 15.9
5 A true filler existed but not within the window 10.0
Often, though, the system did not classify a true implicit argument because such a
candidate was not generated. Without such a candidate, the system stood no chance of
making a correct prediction. Errors 3 and 5 combined (also 31%) describe this behavior.
Type 3 errors resulted when implicit arguments were not core (i.e., argn) arguments
to other predicates. To reduce class imbalance, the system only used core arguments
as candidates; this came at the expense of increased type 3 errors, however. In many
cases, the true implicit argument filled a non-core (i.e., adjunct) role within PropBank
or NomBank.
Type 5 errors resulted when the true implicit arguments for a predicate were outside
the candidate window. Oracle recall (see Table 5) indicates the nominals that suffered
most from windowing errors. For example, the sale predicate was associated with the
highest number of true implicit arguments, but only 72% of those could be resolved
within the two-sentence candidate window. Empirically, we found that extending the
candidate window uniformly for all predicates did not increase F1 performance because
additional false positives were identified. The oracle results suggest that predicate-
specific window settings might offer some advantage for predicates such as fund and
bid, which take arguments at longer ranges.
Error types 2 and 4 are directly related to the prediction confidence threshold t.
The former would be reduced by increasing t and thus filtering out bad predictions.
The latter would be reduced by lowering t and allowing more true fillers into the final
output. It is unclear whether either of these actions would increase overall performance,
however.
6.4 The Investment and Fund Predicates
In Section 4.2, we discussed the price predicate, which frequently occurs in the ?[p price]
index? collocation. We observed that this collocation is rarely associated with either an
overt arg0 or an implicit iarg0. Similar observations can be made for the investment and
fund predicates. Although these two predicates are frequent, they are rarely associated
with implicit arguments: investment takes only 52 implicit arguments and fund takes
only 56 implicit arguments (see Table 5). This behavior is due in large part to collocations
such as ?[p investment] banker,? ?stock [p fund],? and ?mutual [p fund],? which use
predicate senses that are not eventive and take no arguments. Such collocations also
violate the assumption that differences between the PropBank and NomBank argument
structure for a predicate are indicative of implicit arguments (see Section 3.1 for this
assumption).
Despite their lack of implicit arguments, it is important to account for predicates
such as investment and fund because the incorrect prediction of implicit arguments for
786
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
them can lower precision. This is precisely what happened for the investment predicate
(P = 33%). The model incorrectly identified many implicit arguments for instances such
as ?[p investment] banker? and ?[p investment] professional,? which take no arguments.
The right context of investment should help the model avoid this type of error; however
in many cases this was not enough evidence to prevent a false positive prediction.
It might be helpful to distinguish eventive nominals from non-eventive ones, given
the observation that some non-eventive nominals rarely take arguments. Additional
investigation is needed to address this type of error.
6.5 Improvements versus the Baseline
The baseline heuristic covers the simple case where identical predicates share argu-
ments in the same position. Because the discriminative model also uses this information
(see Feature 8), it is interesting to examine cases where the baseline heuristic failed
but the discriminative model succeeded. Such cases represent more difficult inferences.
Consider the following sentence:
(51) Mr. Rogers recommends that [p investors] sell [iarg2 takeover?related
stock].
Neither NomBank nor the baseline heuristic associate the marked predicate in Exam-
ple (51) with any arguments; the feature-based model was able to correctly identify
the marked iarg2 as the entity being invested in, however. This inference relied on a
number of features that connect the invest event to the sell event (e.g., Features 1, 4,
and 76). These features captured a tendency of investors to sell the things they have
invested in.
We conclude our discussion with an example of a complex extra-sentential implicit
argument. Consider the following adjacent sentences:
(52) [arg0 Olivetti] [p exported] $25 million in ?embargoed, state-of-the-art,
flexible manufacturing systems to the Soviet aviation industry.?
(53) [arg0 Olivetti] reportedly began [p shipping] these tools in 1984.
(54) [iarg0 Olivetti] has denied that it violated the rules, asserting that the
shipments were properly licensed.
(55) However, the legality of these [p sales] is still an open question.
In Example (55), we are looking for the iarg0 of sale. As shown, the discriminative model
was able to correctly identify Olivetti from Example (54) as the implied filler of this
argument position. The inference involved two key steps. First, the model identified
coreferent mentions of Olivetti in Examples (52) and (53). In these sentences, Olivetti
participates in the marked exporting and shipping events. Second, the model identified
a tendency for exporters and shippers to also be sellers (e.g., Features 1, 4, and 23 made
large contributions to the prediction). Using this knowledge, the system extracted infor-
mation that could not be extracted by the baseline heuristic or a traditional SRL system.
787
Computational Linguistics Volume 38, Number 4
6.6 Comparison with Previous Results
In a previous study, we reported initial results for the task of implicit argument identi-
fication (Gerber and Chai 2010). This article presents two major advancements versus
our prior work. First, this article presents a more rigorous evaluation set-up, which was
not used in our previous study. Our previous study used fixed partitions of training,
development, and testing data. As a result, feature and model parameter selections
overfit the development data; we observed a 23-point difference in F1 between the
development (65%) and testing (42%) partitions. The small size of the testing set alo
led to small sample sizes and large p-values during significance testing. The cross-
validated approach reported in this article alleviated both problems. The F1 difference
between training and testing was approximately 10 percentage points for all folds, and
all of the data were used for testing, leading to more accurate p-values. It is not possible
to directly compare the evaluation scores in the two studies; the methodology in the
current article is preferable for the reasons mentioned, however.
Second, this article presents a wider range of features compared with the features
described in our previous study. In particular, we experimented with corpus statistics
derived from sub-corpora that were specifically tailored to the predicate instance under
consideration. See, for example, Feature 13 in Appendix B, which computed PMI scores
between arguments found in a custom sub-corpus of text. This feature was ranked
highly by a few of the evaluation folds (see Appendix B for feature rankings).
7. Conclusions
Previous work provided a partial solution to the problem of nominals with implicit
arguments (Gerber, Chai, and Meyers 2009). The model described in that work is able
to accurately identify nominals that take local arguments, thus filtering out predicates
whose arguments are entirely implicit. This increases standard nominal SRL perfor-
mance by reducing the number of false positive argument predictions; all implicit
arguments remain unidentified, however, leaving a large portion of the corresponding
event structures unrecognized.
This article presents our investigation of implicit argument identification for nom-
inal predicates. The study was based on a manually created corpus of implicit argu-
ments, which is freely available for research purposes. Our results show that models
can be trained by incorporating information from a variety of ontological and corpus-
based sources. The study?s primary findings include the following:
1. Implicit arguments are frequent. Given the predicates in a document,
there exist a fixed number of possible arguments that can be filled
according to NomBank?s predicate role sets. Role coverage is defined as
the fraction of these roles that are actually filled by constituents in the text.
Using NomBank as a baseline, the study found that role coverage
increases by 71% when implicit arguments are taken into consideration.
2. Implicit arguments can be automatically identified. Using the annotated
data, we constructed a feature-based supervised model that is able to
automatically identify implicit arguments. This model relies heavily on
the traditional, single-sentence SRL structure of both nominal and verbal
predicates. By unifying these sources of information, the implicit argument
788
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
model provides a more coherent picture of discourse semantics than is
typical in most recent work (e.g., the evaluation conducted by Surdeanu
et al [2008]). The model demonstrates substantial gains over an informed
baseline, reaching an overall F1 score of 50% and per-predicate scores in
the mid-50s and mid-60s. These results are among the first for this task.
3. Much work remains. The study presented in the current article was very
focused: Only ten different predicates were analyzed. The goal was to
carefully examine the underlying linguistic properties of implicit
arguments. This examination produced many features that have not been
used in other SRL studies. The results are encouraging; a direct application
of the model to all NomBank predicates will require a substantial
annotation effort, however. This is because many of the most important
features are lexicalized on the predicate being analyzed and thus cannot be
generalized to novel predicates. Additional information might be
extracted from VerbNet, which groups related verbs together. Features
from this resource might generalize better because they apply to entire sets
of verbs (and verb-based nouns). Additionally, the model would benefit
from a deeper understanding of the relationships that obtain between
predicates in close textual proximity. Often, predicates themselves head
arguments to other predicates, and, as a result, borrow arguments from
those predicates following certain patterns. The work of Blanco and
Moldovan (2011) addresses this issue directly with the use of composition
rules. These rules would be helpful for implicit argument identification.
Lastly, it should be noted that the prediction model described in this article
is quite simple. Each candidate is independently classified as filling
each missing argument position, and a heuristic post-processing step is
performed to arrive at the final labeling. This approach ignores the joint
behavior of semantic arguments. We have performed a preliminary
investigation of joint implicit argument structures (Gerber, Chai, and Bart
2011); as described in that work, however, many issues remain concerning
joint implicit argument identification.
789
Computational Linguistics Volume 38, Number 4
Appendix A: Role Sets for the Annotated Predicates
Listed here are the role sets for the ten predicates used in this article.
Role set for bid:
Arg0: bidder
Arg1: thing being bid for
Arg2: amount of the bid
Role set for sale:
Arg0: seller
Arg1: thing sold
Arg2: buyer
Arg3: price paid
Arg4: beneficiary of sale
Role set for loan:
Arg0: giver
Arg1: thing given
Arg2: entity given to
Arg3: loan against
(collateral)
Arg4: interest rate
Role set for cost:
Arg1: commodity
Arg2: price
Arg3: buyer
Arg4: secondary commodity
Role set for plan:
Arg0: planner
Arg1: thing planned
Arg2: beneficiary of plan
Arg3: secondary plan
Role set for investor:
Arg0: investor
Arg1: thing invested
Arg2: thing invested in
Role set for price:
Arg0: seller
Arg1: commodity
Arg2: price
Arg3: secondary commodity
Role set for loss:
Arg0: entity losing
something
Arg1: thing lost
Arg2: entity gaining thing
lost
Arg3: source of loss
Role set for investment:
Arg0: investor
Arg1: thing invested
Arg2: thing invested in
Role set for fund:
Arg0: funder
Arg1: thing funded
Arg2: amount of funding
Arg3: beneficiary
790
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Appendix B: Implicit Argument Features
Table B.1
Features for determining whether c fills iargn of predicate p. For each mention f (denoting a
filler) in the coreference chain c?, pf , and argf are the predicate and argument position of f .
Unless otherwise noted, all argument positions (e.g., argn and iargn) should be interpreted as the
integer label n instead of the underlying word content of the argument. The & symbol denotes
concatenation; for example, a feature value of ?p & iargn? for the iarg0 position of sale would be
?sale-0.? Features marked with an asterisk (*) are explained in Section 4.2. Features marked with
a dagger (?) require external text corpora that have been automatically processed by existing
NLP components (e.g., SRL systems). The final column gives a heuristic ranking score for the
features across all evaluation folds (see Section 6.2 for discussion).
# Feature value description Importance score
1* For every f , pf & argf & p & iargn. 8.2
2* Sentence distance from c to p. 4.0
3* For every f , the head word of f & the verbal form of p & iargn. 3.6
4* Same as 1 except generalizing pf and p to their WordNet synsets. 3.3
5* Same as 3 except generalizing f to its WordNet synset. 1.0
6 Whether or not c and p are themselves arguments to the same
predicate.
1.0
7 p & the semantic head word of p?s right sibling. 0.7
8 Whether or not any argf and iargn have the same integer argument
position.
0.7
9* Frame element path between argf of pf and iargn of p in FrameNet
(Baker, Fillmore, and Lowe 1998).
0.6
10 Percentage of elements in c? that are subjects of a copular for which p
is the object.
0.6
11 Whether or not the verb forms of pf and p are in the same VerbNet
class and argf and iargn have the same thematic role.
0.6
12 p & the last word of p?s right sibling. 0.6
13*? Maximum targeted PMI between argf of pf and iargn of p. 0.6
14 p & the number of p?s right siblings. 0.5
15 Percentage of elements in c? that are objects of a copular for which p
is the subject.
0.5
16 Frequency of the verbal form of p within the document. 0.5
17 p & the stemmed content words in a one?word window around p. 0.5
18 Whether or not p?s left sibling is a quantifier (many, most, all, etc.).
Quantified predicates tend not to take implicit arguments.
0.4
19 Percentage of elements in c? that are copular objects. 0.4
20 TF cosine similarity between words from arguments of all pf and
words from arguments of p.
0.4
21 Whether the path defined in 9 exists. 0.4
22 Percentage of elements in c? that are copular subjects. 0.4
23* For every f , the VerbNet class/role of pf /argf & the class/role of
p/iargn.
0.4
24 Percentage of elements in c? that are indefinite noun phrases. 0.4
25* p & the syntactic head word of p?s right sibling. 0.3
26 p & the stemmed content words in a two-word window around p. 0.3
27*? Minimum selectional preference between any f and iargn of p.
Uses the method described by Resnik (1996) computed over
an SRL-parsed version of the Penn TreeBank and Gigaword
(Graff 2003) corpora.
0.3
28 p & p?s synset in WordNet. 0.3
29? Same as 27 except using the maximum. 0.3
791
Computational Linguistics Volume 38, Number 4
Table B.1
(continued)
# Feature value description Importance score
30 Average per?sentence frequency of the verbal form of p within the
document.
0.3
31 p itself. 0.3
32 p & whether p is the head of its parent. 0.3
33*? Minimum coreference probability between argf of pf and iargn of p. 0.3
34 p & whether p is before a passive verb. 0.3
35 Percentage of elements in c? that are definite noun phrases. 0.3
36 Percentage of elements in c? that are arguments to other predicates. 0.3
37 Maximum absolute sentence distance from any f to p. 0.3
38 p & p?s syntactic category. 0.2
39 TF cosine similarity between the role description of iargn and the
concatenated role descriptions of all argf .
0.2
40 Average TF cosine similarity between each argn of each pf and the
corresponding argn of p, where ns are equal.
0.2
41 Same as 40 except using the maximum. 0.2
42 Same as 40 except using the minimum. 0.2
43 p & the head of the following prepositional phrase?s object. 0.2
44 Whether any f is located between p and any of the arguments
annotated by NomBank for p. When true, this feature rules out
false positives because it implies that the NomBank annotators
considered and ignored f as a local argument to p.
0.2
45 Number of elements in c?. 0.2
46 p & the first word of p?s right sibling. 0.2
47 p & the grammar rule that expands p?s parent. 0.2
48 Number of elements in c? that are arguments to other predicates. 0.2
49 Nominal form of p & iargn. 0.2
50 p & the syntactic parse tree path from p to the nearest passive verb. 0.2
51 Same as 37 except using the minimum. 0.2
52? Same as 33 except using the average. 0.2
53 Verbal form of p & iargn. 0.2
54 p & the first word of p?s left sibling. 0.2
55 Average per-sentence frequency of the nominal form of p within the
document.
0.2
56 p & the part of speech of p?s parent?s head word. 0.2
57? Same as 33 except using the maximum. 0.2
58 Same as 37 except using the average. 0.1
59* Minimum path length between argf of pf and iargn of p within
VerbNet (Kipper 2005).
0.1
60 Frequency of the nominal form of p within the document. 0.1
61 p & the number of p?s left siblings. 0.1
62 p & p?s parent?s head word. 0.1
63 p & the syntactic category of p?s right sibling. 0.1
64 p & p?s morphological suffix. 0.1
65 TF cosine similarity between words from all f and words from the
role description of iargn.
0.1
66 Percentage of elements in c? that are quantified noun phrases. 0.1
67* Discourse relation whose two discourse units cover c (the primary
filler) and p.
0.1
68 For any f , the minimum semantic similarity between pf and p using
the method described by Wu and Palmer (1994) over WordNet
(Fellbaum 1998).
0.1
792
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Table B.1
(continued)
# Feature value description Importance score
69 p & whether or not p is followed by a prepositional phrase. 0.1
70 p & the syntactic head word of p?s left sibling. 0.1
71 p & the stemmed content words in a three-word window around p. 0.1
72 Syntactic category of c & iargn & the verbal form of p. 0.1
73 Nominal form of p & the sorted integer argument indexes (the ns)
from all argn of p.
0.1
74 Percentage of elements in c? that are sentential subjects. 0.1
75 Whether or not the integer position of any argf equals that of iargn. 0.1
76? Same as 13 except using the average. 0.1
77? Same as 27 except using the average. 0.1
78 p & p?s parent?s syntactic category. 0.1
79 p & the part of speech of the head word of p?s right sibling. 0.1
80 p & the semantic head word of p?s left sibling. 0.1
81? Maximum targeted coreference probability between argf of pf
and iargn of p. This is a hybrid feature that calculates the coreference
probability of Feature 33 using the corpus tuning method of
Feature 13.
0.1
793
C
om
p
u
tation
alL
in
gu
istics
V
olu
m
e
38,N
u
m
ber
4
Appendix C: Per-fold Implicit Argument Identification Results
Table C.1
Per-fold implicit argument identification results. Columns are defined as follows: (1) fold used for testing, (2) selected features in rank order,
(3) baseline F1, (4) LibLinear cost parameter, (5) LibLinear weight for the positive class, (6) implicit argument confidence threshold, (7) discriminative
F1, (8) oracle F1. A bias of 1 was used for all LibLinear models.
Baseline Discriminative (LibLinear) Oracle
Fold Features F1 (%) c w+ t F1 (%) F1 (%)
1 1, 2, 3, 11, 32, 8, 27, 22, 31, 10, 20, 53, 6, 16, 24, 40, 30, 38, 72, 69,
73, 19, 28, 42, 48, 64, 44, 36, 37, 12, 7
31.7 0.25 4 0.39260 47.1 86.7
2 1, 3, 2, 4, 17, 13, 28, 11, 6, 18, 25, 12, 56, 29, 16, 53, 41, 31, 46, 10, 7,
51, 15, 22
32 0.25 256 0.80629 51.5 86.9
3 4, 3, 2, 8, 7, 6, 59, 20, 9, 62, 37, 39, 41, 19, 10, 15, 11, 35, 61, 44, 42,
40, 32, 30, 16, 75, 33, 24
35.3 0.25 256 0.90879 55.8 88.1
4 1, 2, 5, 13, 8, 49, 6, 35, 34, 14, 15, 18, 36, 28, 20, 45, 3, 43, 24, 48, 10,
29, 12, 30, 33, 65, 31, 22, 61, 16, 27, 41, 60, 55, 64
27.8 0.25 4 0.38540 45.8 86.5
5 1, 2, 26, 3, 4, 23, 5, 63, 55, 6, 12, 44, 42, 65, 7, 71, 18, 15, 10, 14, 52,
34, 19, 24, 50, 58
25.8 0.125 1024 0.87629 45.9 88
6 1, 3, 2, 14, 23, 38, 25, 39, 16, 6, 21, 68, 70, 58, 9, 22, 18, 31, 60, 10,
64, 15, 66, 19, 30, 51, 56, 28
34.8 0.25 256 0.87759 55.4 90.8
7 1, 2, 4, 3, 47, 54, 43, 7, 33, 9, 67, 24, 36, 50, 40, 12, 21 22.9 0.25 256 0.81169 46.3 87.4
8 1, 3, 2, 4, 9, 7, 14, 12, 6, 46, 30, 18, 19, 36, 48, 42, 37, 45, 60, 56, 61,
51, 15, 10, 41, 40, 25, 31, 11, 39, 62, 69, 34, 16, 33, 8, 38, 20, 78, 44,
55, 80, 53, 50, 52, 49, 24, 28, 57
27.1 0.0625 512 0.92019 47.4 87.2
9 1, 5, 2, 4, 3, 21, 27, 10, 15, 9, 57, 35, 16, 25, 37, 33, 45, 24, 46, 29, 19,
34, 51, 50, 22, 48, 32, 11, 12, 58, 41, 8, 76, 18, 30, 40, 77, 6, 66, 44,
43, 79, 81, 20
23 0.0625 32 0.67719 54.1 85.5
10 4, 3, 2, 17, 1, 13, 29, 12, 11, 52, 10, 15, 6, 16, 9, 22, 7, 21, 57, 19, 74,
34, 45, 20, 66
28.4 0.0625 512 0.89769 53.2 88.5
(1) (2) (3) (4) (5) (6) (7) (8)
794
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Acknowledgments
We would like to thank the anonymous
reviewers for their many insightful
comments and suggestions. This work
was partially supported by NSF grants
IIS-0347548 and IIS-0840538.
References
Baker, Collin, Charles Fillmore, and
John Lowe. 1998. The Berkeley FrameNet
project. In Proceedings of the Thirty-Sixth
Annual Meeting of the Association for
Computational Linguistics and Seventeenth
International Conference on Computational
Linguistics, pages 86?90, San Francisco, CA.
Bhagat, Rahul, Patrick Pantel, and Eduard
Hovy. 2007. LEDIR: An unsupervised
algorithm for learning directionality
of inference rules. In Proceedings of the
2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP-CoNLL), pages 161?170,
Prague.
Blanco, Eduardo and Dan Moldovan. 2011.
A model for composing semantic relations.
In Proceedings of the 9th International
Conference on Computational Semantics
(IWCS 2011), pages 45?54, Oxford.
Burchardt, Aljoscha, Anette Frank, and
Manfred Pinkal. 2005. Building text
meaning representations from contextually
related frames?a case study. In Proceedings
of the Sixth International Workshop on
Computational Semantics, Tilburg.
Carpenter, Patricia A., Akira Miyake,
and Marcel Adam Just. 1995. Language
comprehension: Sentence and discourse
processing. Annual Review of Psychology,
46:91?120.
Carreras, Xavier and Llu??s Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning, pages 152?164,
Ann Arbor, MI.
Chambers, Nathanael and Dan Jurafsky.
2008. Unsupervised learning of narrative
event chains. In Proceedings of the
Association for Computational Linguistics,
pages 789?797, Columbus, OH.
Charniak, Eugene and Mark Johnson.
2005. Coarse-to-fine n-best parsing
and MaxEnt discriminative reranking.
In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics,
pages 173?180, Ann Arbor, MI.
Chen, Desai, Nathan Schneider, Dipanjan
Das, and Noah A. Smith. 2010. Semafor:
Frame argument resolution with log-linear
models. In Proceedings of the 5th
International Workshop on Semantic
Evaluation, pages 264?267, Uppsala.
Chen, Zheng and Heng Ji. 2009. Graph-based
event coreference resolution. In Proceedings
of the 2009 Workshop on Graph-based
Methods for Natural Language Processing
(TextGraphs-4), pages 54?57, Suntec.
Chinchor, Nancy, David D. Lewis, and
Lynette Hirschmant. 1993. Evaluating
message understanding systems:
An analysis of the third message
understanding conference. Computational
Linguistics, 19(3):409?450.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales.
Educational and Psychological
Measurement, 20(1):37?46.
Dang, Hoa Trang, Diane Kelly, and Jimmy J.
Lin. 2007. Overview of the TREC 2007
question answering track. In Proceedings
of the Fifteenth TREC. Available at
trec.nist.gov/pubs/trec15/t15
proceedings.html.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: a second look.
Computational Linguistics, 30(1):95?101.
Efron, Bradley and Robert J. Tibshirani. 1993.
An Introduction to the Bootstrap. Chapman
& Hall, New York.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui
Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008. LIBLINEAR: A Library for Large
Linear Classification. Journal of Machine
Learning Research, 9:1871?1874.
Fellbaum, Christiane. 1998. WordNet:
An Electronic Lexical Database (Language,
Speech, and Communication). The MIT Press,
Cambridge, MA.
Fillmore, C. J. and C. F. Baker. 2001.
Frame semantics for text understanding.
In Proceedings of WordNet and Other
Lexical Resources Workshop, NAACL,
Pittsburgh, PA.
Gerber, Matthew and Joyce Chai. 2010.
Beyond NomBank: A study of implicit
arguments for nominal predicates. In
Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics, pages 1583?1592, Uppsala.
Gerber, Matthew, Joyce Chai, and Robert
Bart. 2011. A joint model of implicit
arguments for nominal predicates. In
Proceedings of the ACL 2011 Workshop on
Relational Models of Semantics, pages 63?71,
Portland, OR.
795
Computational Linguistics Volume 38, Number 4
Gerber, Matthew, Joyce Chai, and Adam
Meyers. 2009. The role of implicit
argumentation in nominal SRL. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 146?154,
Boulder, CO.
Graesser, Arthur C. and Leslie F. Clark.
1985. Structures and Procedures of Implicit
Knowledge. Ablex Publishing Corporation,
New York.
Graff, David. 2003. English Gigaword.
Linguistic Data Consortium,
Philadelphia, PA.
Grosz, Barbara J., Aravind K. Joshi, and
Scott Weinstein. 1995. Centering:
A framework for modeling the local
coherence of discourse. Computational
Linguistics, 21(2):203?225.
Harris, Zellig. 1985. Distributional structure.
In J. J. Katz, editor, The Philosophy of
Linguistics. Oxford University Press,
New York, pages 26?47.
Heim, Irene and Angelika Kratzer. 1998.
Semantics in Generative Grammar.
Blackwell, Oxford.
Iida, Ryu, Mamoru Komachi, Kentaro Inui,
and Yuji Matsumoto. 2007. Annotating a
Japanese text corpus with predicate-
argument and coreference relations. In
Proceedings of the Linguistic Annotation
Workshop in ACL-2007, pages 132?139,
Prague.
Imamura, Kenji, Kuniko Saito, and
Tomoko Izumi. 2009. Discriminative
approach to predicate?argument structure
analysis with zero-anaphora resolution.
In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 85?88,
Suntec.
Johansson, Richard and Pierre Nugues. 2008.
Dependency-based syntactic?semantic
analysis with PropBank and NomBank.
In CoNLL 2008: Proceedings of the Twelfth
Conference on Computational Natural
Language Learning, pages 183?187,
Manchester.
Kamp, Hans and Uwe Reyle. 1993. From
Discourse to Logic. Kluwer, Dordrecht.
Kipper, Karin. 2005. VerbNet: A
Broad-coverage, Comprehensive Verb Lexicon.
Ph.D. thesis, Department of Computer
and Information Science, University of
Pennsylvania, Philadelphia.
Krippendorff, Klaus. 1980. Content Analysis:
An Introduction to Its Methodology. Sage
Publications, Thousand Oaks, CA.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for
question-answering. Natural Language
Engineering, 7(4):343?360.
Liu, Chang and Hwee Ng. 2007. Learning
predictive structures for semantic role
labeling of nombank. In Proceedings of the
45th Annual Meeting of the Association of
Computational Linguistics, pages 208?215,
Prague.
Marcus, Mitchell, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: the
Penn TreeBank. Computational Linguistics,
19:313?330.
Meyers, Adam. 2007. Annotation guidelines
for NomBank?noun argument structure
for PropBank. Technical report, New York
University.
Nielsen, Leif Arda. 2004. Verb phrase
ellipsis detection using automatically
parsed text. In COLING ?04: Proceedings
of the 20th international conference on
Computational Linguistics, pages 1093?1099,
Geneva.
Nielsen, Leif Arda. 2005. A corpus-based
study of Verb Phrase Ellipsis Identification
and Resolution. Ph.D. thesis, King?s
College, London.
NIST, 2008. The ACE 2008 Evaluation Plan.
National Institute of Standards and
Technology, Gaithersburg, MD.
Palmer, Martha S., Deborah A. Dahl,
Rebecca J. Schiffman, Lynette Hirschman,
Marcia Linebarger, and John Dowding.
1986. Recovering implicit information. In
Proceedings of the 24th Annual Meeting of the
Association for Computational Linguistics,
pages 10?19, Morristown, NJ.
Pantel, Patrick, Rahul Bhagat, Bonaventura
Coppola, Timothy Chklovski, and Eduard
Hovy. 2007. ISP: Learning inferential
selectional preferences. In Human Language
Technologies 2007: The Conference of the
North American Chapter of the Association
for Computational Linguistics; Proceedings
of the Main Conference, pages 564?571,
Rochester, NY.
Pantel, Patrick and Deepak Ravichandran.
2004. Automatically labeling semantic
classes. In HLT-NAACL 2004: Main
Proceedings, pages 321?328, Boston, MA.
Pizzato, Luiz Augusto and Diego Molla?.
2008. Indexing on semantic roles for
question answering. In COLING 2008:
Proceedings of the 2nd Workshop on
Information Retrieval for Question
Answering, pages 74?81, Manchester.
796
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Prasad, Rashmi, Alan Lee, Nikhil Dinesh,
Eleni Miltsakaki, Geraud Campion,
Aravind Joshi, and Bonnie Webber. 2008.
Penn discourse treebank version 2.0.
Linguistic Data Consortium, University
of Pennsylvania, Philadelphia.
Pudil, P., J. Novovicova, and J. Kittler.
1994. Floating search methods in feature
selection. Pattern Recognition Letters,
15:1119?1125.
Punyakanok, Vasin, Peter Koomen,
Dan Roth, and Wen-tau Yih. 2005.
Generalized inference with multiple
semantic role labeling systems. In
Proceedings of CoNLL-2005 Shared Task,
pages 181?184, Ann Arbor, MI.
Punyakanok, Vasin, Dan Roth, and Wen-Tau
Yih. 2008. The importance of syntactic
parsing and inference in semantic role
labeling. Computational Linguistics,
34(2):257?287.
Resnik, Philip. 1996. Selectional constraints:
An information-theoretic model and its
computational realization. Cognition,
61:127?159.
Ritter, Alan, Mausam, and Oren Etzioni.
2010. A latent dirichlet alocation method
for selectional preferences. In Proceedings of
the 48th Annual Meeting of the Association for
Computational Linguistics, pages 424?434,
Uppsala.
Ruppenhofer, Josef, Caroline Sporleder,
Roser Morante, Collin Baker, and
Martha Palmer. 2009. Semeval-2010
task 10: Linking events and their
participants in discourse. In Proceedings
of the Workshop on Semantic Evaluations:
Recent Achievements and Future
Directions (SEW-2009), pages 106?111,
Boulder, CO.
Ruppenhofer, Josef, Caroline Sporleder,
Roser Morante, Collin Baker, and
Martha Palmer. 2010. Semeval-2010
task 10: Linking events and their
participants in discourse. In Proceedings
of the 5th International Workshop on
Semantic Evaluation, pages 45?50,
Uppsala.
Sagae, Kenji. 2009. Analysis of discourse
structure with syntactic dependencies
and data-driven shift-reduce parsing.
In Proceedings of the 11th International
Conference on Parsing Technologies
(IWPT?09), pages 81?84, Paris.
Sanford, A. J. 1981. Understanding Written
Language. John Wiley & Sons Ltd,
Hoboken, NJ.
Sasano, Ryohei, Daisuke Kawahara, and
Sadao Kurohashi. 2004. Automatic
construction of nominal case frames
and its application to indirect anaphora
resolution. In Proceedings of COLING 2004,
pages 1201?1207, Geneva.
Schank, Roger C. and Robert P. Abelson.
1977. Scripts, Plans, Goals and
Understanding: an Inquiry into Human
Knowledge Structures. Lawrence Erlbaum,
Hillsdale, NJ.
Surdeanu, Mihai, Richard Johansson, Adam
Meyers, Llu??s Ma`rquez, and Joakim Nivre.
2008. The CoNLL 2008 shared task on
joint parsing of syntactic and semantic
dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational
Natural Language Learning, pages 159?177,
Manchester.
Szpektor, Idan, Hristo Tanev, Ido Dagan,
and Bonaventura Coppola. 2004. Scaling
Web-based acquisition of entailment
relations. In Proceedings of Empirical
Methods in Natural Language Processing,
pages 41?48, Barcelona.
Tonelli, Sara and Rodolfo Delmonte.
2010. Venses++: Adapting a deep
semantic processing system to the
identification of null instantiations.
In Proceedings of the 5th International
Workshop on Semantic Evaluation,
pages 296?299, Uppsala.
van Dijk, T. A. 1977. Semantic macro
structures and knowledge frames
in discourse comprehension. In
M. A. Just and P. A. Carpenter, editors,
Cognitive Processes in Comprehension.
Lawrence Erlbaum, Hillsdale, NJ,
pages 3?32.
van Dijk, Teun A. and Walter Kintsch. 1983.
Strategies of Discourse Comprehension.
Academic Press, Waltham, MA.
Versley, Yannick, Simone Paolo Ponzetto,
Massimo Poesio, Vladimir Eidelman,
Alan Jern, Jason Smith, Xiaofeng Yang,
and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference
resolution. In Proceedings of the 6th
International Conference on Language
Resources and Evaluation, pages 9?12,
Marrakech.
Whittemore, Greg, Melissa Macpherson,
and Greg Carlson. 1991. Event-building
through role-filling and anaphora
resolution. In Proceedings of the 29th
Annual Meeting on Association for
Computational Linguistics, pages 17?24,
Morristown, NJ.
Wilson, N. L. 1974. Facts, events, and their
identity conditions. Philosophical Studies,
25:303?321.
797
Computational Linguistics Volume 38, Number 4
Wu, Zhibiao and Martha Palmer. 1994.
Verb semantics and lexical selection.
In Proceedings of the 32nd Annual
Meeting of the Association for
Computational Linguistics,
pages 133?138, Las Cruces, NM.
Yang, Xiaofeng, Jian Su, and Chew Lim Tan.
2008. A twin-candidate model for
learning-based anaphora resolution.
Computational Linguistics, 34(3):327?356.
Zanzotto, Fabio Massimo, Marco
Pennacchiotti, and Maria Teresa Pazienza.
2006. Discovering asymmetric entailment
relations between verbs using selectional
preferences. In ACL-44: Proceedings
of the 21st International Conference on
Computational Linguistics and the 44th
Annual Meeting of the Association for
Computational Linguistics, pages 849?856,
Morristown, NJ.
798
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583?1592,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Beyond NomBank:
A Study of Implicit Arguments for Nominal Predicates
Matthew Gerber and Joyce Y. Chai
Department of Computer Science
Michigan State University
East Lansing, Michigan, USA
{gerberm2,jchai}@cse.msu.edu
Abstract
Despite its substantial coverage, Nom-
Bank does not account for all within-
sentence arguments and ignores extra-
sentential arguments altogether. These ar-
guments, which we call implicit, are im-
portant to semantic processing, and their
recovery could potentially benefit many
NLP applications. We present a study of
implicit arguments for a select group of
frequent nominal predicates. We show that
implicit arguments are pervasive for these
predicates, adding 65% to the coverage of
NomBank. We demonstrate the feasibil-
ity of recovering implicit arguments with
a supervised classification model. Our re-
sults and analyses provide a baseline for
future work on this emerging task.
1 Introduction
Verbal and nominal semantic role labeling (SRL)
have been studied independently of each other
(Carreras and Ma`rquez, 2005; Gerber et al, 2009)
as well as jointly (Surdeanu et al, 2008; Hajic? et
al., 2009). These studies have demonstrated the
maturity of SRL within an evaluation setting that
restricts the argument search space to the sentence
containing the predicate of interest. However, as
shown by the following example from the Penn
TreeBank (Marcus et al, 1993), this restriction ex-
cludes extra-sentential arguments:
(1) [arg0 The two companies] [pred produce]
[arg1 market pulp, containerboard and white
paper]. The goods could be manufactured
closer to customers, saving [pred shipping]
costs.
The first sentence in Example 1 includes the Prop-
Bank (Kingsbury et al, 2002) analysis of the ver-
bal predicate produce, where arg0 is the agentive
producer and arg1 is the produced entity. The sec-
ond sentence contains an instance of the nominal
predicate shipping that is not associated with argu-
ments in NomBank (Meyers, 2007).
From the sentences in Example 1, the reader can
infer that The two companies refers to the agents
(arg0) of the shipping predicate. The reader can
also infer that market pulp, containerboard and
white paper refers to the shipped entities (arg1
of shipping).1 These extra-sentential arguments
have not been annotated for the shipping predi-
cate and cannot be identified by a system that re-
stricts the argument search space to the sentence
containing the predicate. NomBank also ignores
many within-sentence arguments. This is shown
in the second sentence of Example 1, where The
goods can be interpreted as the arg1 of shipping.
These examples demonstrate the presence of argu-
ments that are not included in NomBank and can-
not easily be identified by systems trained on the
resource. We refer to these arguments as implicit.
This paper presents our study of implicit ar-
guments for nominal predicates. We began our
study by annotating implicit arguments for a se-
lect group of predicates. For these predicates, we
found that implicit arguments add 65% to the ex-
isting role coverage of NomBank.2 This increase
has implications for tasks (e.g., question answer-
ing, information extraction, and summarization)
that benefit from semantic analysis. Using our an-
notations, we constructed a feature-based model
for automatic implicit argument identification that
unifies standard verbal and nominal SRL. Our re-
sults indicate a 59% relative (15-point absolute)
gain in F1 over an informed baseline. Our analy-
ses highlight strengths and weaknesses of the ap-
proach, providing insights for future work on this
emerging task.
1In PropBank and NomBank, the interpretation of each
role (e.g., arg0) is specific to a predicate sense.
2Role coverage indicates the percentage of roles filled.
1583
In the following section, we review related re-
search, which is historically sparse but recently
gaining traction. We present our annotation effort
in Section 3, and follow with our implicit argu-
ment identification model in Section 4. In Section
5, we describe the evaluation setting and present
our experimental results. We analyze these results
in Section 6 and conclude in Section 7.
2 Related work
Palmer et al (1986) made one of the earliest at-
tempts to automatically recover extra-sentential
arguments. Their approach used a fine-grained do-
main model to assess the compatibility of candi-
date arguments and the slots needing to be filled.
A phenomenon similar to the implicit argu-
ment has been studied in the context of Japanese
anaphora resolution, where a missing case-marked
constituent is viewed as a zero-anaphoric expres-
sion whose antecedent is treated as the implicit ar-
gument of the predicate of interest. This behavior
has been annotated manually by Iida et al (2007),
and researchers have applied standard SRL tech-
niques to this corpus, resulting in systems that
are able to identify missing case-marked expres-
sions in the surrounding discourse (Imamura et
al., 2009). Sasano et al (2004) conducted sim-
ilar work with Japanese indirect anaphora. The
authors used automatically derived nominal case
frames to identify antecedents. However, as noted
by Iida et al, grammatical cases do not stand in
a one-to-one relationship with semantic roles in
Japanese (the same is true for English).
Fillmore and Baker (2001) provided a detailed
case study of implicit arguments (termed null in-
stantiations in that work), but did not provide con-
crete methods to account for them automatically.
Previously, we demonstrated the importance of fil-
tering out nominal predicates that take no local ar-
guments (Gerber et al, 2009); however, this work
did not address the identification of implicit ar-
guments. Burchardt et al (2005) suggested ap-
proaches to implicit argument identification based
on observed coreference patterns; however, the au-
thors did not implement and evaluate such meth-
ods. We draw insights from all three of these
studies. We show that the identification of im-
plicit arguments for nominal predicates leads to
fuller semantic interpretations when compared to
traditional SRL methods. Furthermore, motivated
by Burchardt et al, our model uses a quantitative
analysis of naturally occurring coreference pat-
terns to aid implicit argument identification.
Most recently, Ruppenhofer et al (2009) con-
ducted SemEval Task 10, ?Linking Events and
Their Participants in Discourse?, which evaluated
implicit argument identification systems over a
common test set. The task organizers annotated
implicit arguments across entire passages, result-
ing in data that cover many distinct predicates,
each associated with a small number of annotated
instances. In contrast, our study focused on a se-
lect group of nominal predicates, each associated
with a large number of annotated instances.
3 Data annotation and analysis
3.1 Data annotation
Implicit arguments have not been annotated within
the Penn TreeBank, which is the textual and syn-
tactic basis for NomBank. Thus, to facilitate
our study, we annotated implicit arguments for
instances of nominal predicates within the stan-
dard training, development, and testing sections of
the TreeBank. We limited our attention to nom-
inal predicates with unambiguous role sets (i.e.,
senses) that are derived from verbal role sets. We
then ranked this set of predicates using two pieces
of information: (1) the average difference between
the number of roles expressed in nominal form (in
NomBank) versus verbal form (in PropBank) and
(2) the frequency of the nominal form in the cor-
pus. We assumed that the former gives an indica-
tion as to how many implicit roles an instance of
the nominal predicate might have. The product of
(1) and (2) thus indicates the potential prevalence
of implicit arguments for a predicate. To focus our
study, we ranked the predicates in NomBank ac-
cording to this product and selected the top ten,
shown in Table 1.
We annotated implicit arguments document-by-
document, selecting all singular and plural nouns
derived from the predicates in Table 1. For each
missing argument position of each predicate in-
stance, we inspected the local discourse for a suit-
able implicit argument. We limited our attention to
the current sentence as well as all preceding sen-
tences in the document, annotating all mentions of
an implicit argument within this window.
In the remainder of this paper, we will use iargn
to refer to an implicit argument position n. We
will use argn to refer to an argument provided by
PropBank or NomBank. We will use p to mark
1584
Pre-annotation Post-annotation
Role average
Predicate # Role coverage (%) Noun Verb Role coverage (%) Noun role average
price 217 42.4 1.7 1.7 55.3 2.2
sale 185 24.3 1.2 2.0 42.0 2.1
investor 160 35.0 1.1 2.0 54.6 1.6
fund 109 8.7 0.4 2.0 21.6 0.9
loss 104 33.2 1.3 2.0 46.9 1.9
plan 102 30.9 1.2 1.8 49.3 2.0
investment 102 15.7 0.5 2.0 33.3 1.0
cost 101 26.2 1.1 2.3 47.5 1.9
bid 88 26.9 0.8 2.2 72.0 2.2
loan 85 22.4 1.1 2.5 41.2 2.1
Overall 1,253 28.0 1.1 2.0 46.2 1.8
Table 1: Predicates targeted for annotation. The second column gives the number of predicate instances
annotated. Pre-annotation numbers only include NomBank annotations, whereas Post-annotation num-
bers include NomBank and implicit argument annotations. Role coverage indicates the percentage of
roles filled. Role average indicates how many roles, on average, are filled for an instance of a predicate?s
noun form or verb form within the TreeBank. Verbal role averages were computed using PropBank.
predicate instances. Below, we give an example
annotation for an instance of the investment predi-
cate:
(2) [iarg0 Participants] will be able to transfer
[iarg1 money] to [iarg2 other investment
funds]. The [p investment] choices are
limited to [iarg2 a stock fund and a
money-market fund].
NomBank does not associate this instance of in-
vestment with any arguments; however, we were
able to identify the investor (iarg0), the thing in-
vested (iarg1), and two mentions of the thing in-
vested in (iarg2).
Our data set was also independently annotated
by an undergraduate linguistics student. For each
missing argument position, the student was asked
to identify the closest acceptable implicit argu-
ment within the current and preceding sentences.
The argument position was left unfilled if no ac-
ceptable constituent could be found. For a miss-
ing argument position, the student?s annotation
agreed with our own if both identified the same
constituent or both left the position unfilled. Anal-
ysis indicated an agreement of 67% using Cohen?s
kappa coefficient (Cohen, 1960).
3.2 Annotation analysis
Role coverage for a predicate instance is equal to
the number of filled roles divided by the number
of roles in the predicate?s lexicon entry. Role cov-
erage for the marked predicate in Example 2 is
0/3 for NomBank-only arguments and 3/3 when
the annotated implicit arguments are also consid-
ered. Returning to Table 1, the third column gives
role coverage percentages for NomBank-only ar-
guments. The sixth column gives role coverage
percentages when both NomBank arguments and
the annotated implicit arguments are considered.
Overall, the addition of implicit arguments created
a 65% relative (18-point absolute) gain in role cov-
erage across the 1,253 predicate instances that we
annotated.
The predicates in Table 1 are typically associ-
ated with fewer arguments on average than their
corresponding verbal predicates. When consid-
ering NomBank-only arguments, this difference
(compare columns four and five) varies from zero
(for price) to a factor of five (for fund). When im-
plicit arguments are included in the comparison,
these differences are reduced and many nominal
predicates express approximately the same num-
ber of arguments on average as their verbal coun-
terparts (compare the fifth and seventh columns).
In addition to role coverage and average count,
we examined the location of implicit arguments.
Figure 1 shows that approximately 56% of the im-
plicit arguments in our data can be resolved within
the sentence containing the predicate. The remain-
ing implicit arguments require up to forty-six sen-
1585
0.4
0.5
0.6
0.7
0.8
0.9
1
0 2 4 6 8 10 12 18 28 46Sentences prior
I
m
p
l
i
c
i
t
 
a
r
g
u
m
e
n
t
s
 
r
e
s
o
l
v
e
d
Figure 1: Location of implicit arguments. For
missing argument positions with an implicit filler,
the y-axis indicates the likelihood of the filler be-
ing found at least once in the previous x sentences.
tences for resolution; however, a vast majority of
these can be resolved within the previous few sen-
tences. Section 6 discusses implications of this
skewed distribution.
4 Implicit argument identification
4.1 Model formulation
In our study, we assumed that each sentence in a
document had been analyzed for PropBank and
NomBank predicate-argument structure. Nom-
Bank includes a lexicon listing the possible ar-
gument positions for a predicate, allowing us to
identify missing argument positions with a simple
lookup. Given a nominal predicate instance p with
a missing argument position iargn, the task is to
search the surrounding discourse for a constituent
c that fills iargn. Our model conducts this search
over all constituents annotated by either PropBank
or NomBank with non-adjunct labels.
A candidate constituent c will often form a
coreference chain with other constituents in the
discourse. Consider the following abridged sen-
tences, which are adjacent in their Penn TreeBank
document:
(3) [Mexico] desperately needs investment.
(4) Conservative Japanese investors are put off
by [Mexico?s] investment regulations.
(5) Japan is the fourth largest investor in
[c Mexico], with 5% of the total
[p investments].
NomBank does not associate the labeled instance
of investment with any arguments, but it is clear
from the surrounding discourse that constituent c
(referring to Mexico) is the thing being invested in
(the iarg2). When determining whether c is the
iarg2 of investment, one can draw evidence from
other mentions in c?s coreference chain. Example
3 states that Mexico needs investment. Example
4 states that Mexico regulates investment. These
propositions, which can be derived via traditional
SRL analyses, should increase our confidence that
c is the iarg2 of investment in Example 5.
Thus, the unit of classification for a candi-
date constituent c is the three-tuple ?p, iargn, c??,
where c? is a coreference chain comprising c and
its coreferent constituents.3 We defined a binary
classification function Pr(+| ?p, iargn, c??) that
predicts the probability that the entity referred to
by c fills the missing argument position iargn of
predicate instance p. In the remainder of this pa-
per, we will refer to c as the primary filler, dif-
ferentiating it from other mentions in the corefer-
ence chain c?. In the following section, we present
the feature set used to represent each three-tuple
within the classification function.
4.2 Model features
Starting with a wide range of features, we per-
formed floating forward feature selection (Pudil
et al, 1994) over held-out development data com-
prising implicit argument annotations from section
24 of the Penn TreeBank. As part of the feature
selection process, we conducted a grid search for
the best per-class cost within LibLinear?s logistic
regression solver (Fan et al, 2008). This was done
to reduce the negative effects of data imbalance,
which is severe even when selecting candidates
from the current and previous few sentences. Ta-
ble 2 shows the selected features, which are quite
different from those used in our previous work to
identify traditional semantic arguments (Gerber et
al., 2009).4 Below, we give further explanations
for some of the features.
Feature 1 models the semantic role relationship
between each mention in c? and the missing argu-
ment position iargn. To reduce data sparsity, this
feature generalizes predicates and argument posi-
tions to their VerbNet (Kipper, 2005) classes and
3We used OpenNLP for coreference identification:
http://opennlp.sourceforge.net
4We have omitted many of the lowest-ranked features.
Descriptions of these features can be obtained by contacting
the authors.
1586
# Feature value description
1* For every f , the VerbNet class/role of pf /argf concatenated with the class/role of p/iargn.
2* Average pointwise mutual information between ?p, iargn? and any ?pf , argf ?.
3 Percentage of all f that are definite noun phrases.
4 Minimum absolute sentence distance from any f to p.
5* Minimum pointwise mutual information between ?p, iargn? and any ?pf , argf ?.
6 Frequency of the nominal form of p within the document that contains it.
7 Nominal form of p concatenated with iargn.
8 Nominal form of p concatenated with the sorted integer argument indexes from all argn of p.
9 Number of mentions in c?.
10* Head word of p?s right sibling node.
11 For every f , the synset (Fellbaum, 1998) for the head of f concatenated with p and iargn.
12 Part of speech of the head of p?s parent node.
13 Average absolute sentence distance from any f to p.
14* Discourse relation whose two discourse units cover c (the primary filler) and p.
15 Number of left siblings of p.
16 Whether p is the head of its parent node.
17 Number of right siblings of p.
Table 2: Features for determining whether c fills iargn of predicate p. For each mention f (denoting a
f iller) in the coreference chain c?, we define pf and argf to be the predicate and argument position of f .
Features are sorted in descending order of feature selection gain. Unless otherwise noted, all predicates
were normalized to their verbal form and all argument positions (e.g., argn and iargn) were interpreted
as labels instead of word content. Features marked with an asterisk are explained in Section 4.2.
semantic roles using SemLink.5 For explanation
purposes, consider again Example 1, where we are
trying to fill the iarg0 of shipping. Let c? contain
a single mention, The two companies, which is the
arg0 of produce. As described in Table 2, fea-
ture 1 is instantiated with a value of create.agent-
send.agent, where create and send are the VerbNet
classes that contain produce and ship, respectively.
In the conversion to LibLinear?s instance repre-
sentation, this instantiation is converted into a sin-
gle binary feature create.agent-send.agent whose
value is one. Features 1 and 11 are instantiated
once for each mention in c?, allowing the model
to consider information from multiple mentions of
the same entity.
Features 2 and 5 are inspired by the work
of Chambers and Jurafsky (2008), who inves-
tigated unsupervised learning of narrative event
sequences using pointwise mutual information
(PMI) between syntactic positions. We used a sim-
ilar PMI score, but defined it with respect to se-
mantic arguments instead of syntactic dependen-
cies. Thus, the values for features 2 and 5 are
computed as follows (the notation is explained in
5http://verbs.colorado.edu/semlink
the caption for Table 2):
pmi(?p, iargn? , ?pf , argf ?) =
log
Pcoref (?p, iargn? , ?pf , argf ?)
Pcoref (?p, iargn? , ?)Pcoref (?pf , argf ? , ?)
(6)
To compute Equation 6, we first labeled a subset of
the Gigaword corpus (Graff, 2003) using the ver-
bal SRL system of Punyakanok et al (2008) and
the nominal SRL system of Gerber et al (2009).
We then identified coreferent pairs of arguments
using OpenNLP. Suppose the resulting data has
N coreferential pairs of argument positions. Also
suppose that M of these pairs comprise ?p, argn?
and ?pf , argf ?. The numerator in Equation 6 is
defined as MN . Each term in the denominator is
obtained similarly, except that M is computed as
the total number of coreference pairs compris-
ing an argument position (e.g., ?p, argn?) and any
other argument position. Like Chambers and Ju-
rafsky, we also used the discounting method sug-
gested by Pantel and Ravichandran (2004) for low-
frequency observations. The PMI score is some-
what noisy due to imperfect output, but it provides
information that is useful for classification.
1587
Feature 10 does not depend on c? and is specific
to each predicate. Consider the following exam-
ple:
(7) Statistics Canada reported that its [arg1
industrial-product] [p price] index dropped
2% in September.
The ?[p price] index? collocation is rarely associ-
ated with an arg0 in NomBank or with an iarg0 in
our annotations (both argument positions denote
the seller). Feature 10 accounts for this type of be-
havior by encoding the syntactic head of p?s right
sibling. The value of feature 10 for Example 7 is
price:index. Contrast this with the following:
(8) [iarg0 The company] is trying to prevent
further [p price] drops.
The value of feature 10 for Example 8 is
price:drop. This feature captures an important dis-
tinction between the two uses of price: the for-
mer rarely takes an iarg0, whereas the latter often
does. Features 12 and 15-17 account for predicate-
specific behaviors in a similar manner.
Feature 14 identifies the discourse relation (if
any) that holds between the candidate constituent
c and the filled predicate p. Consider the following
example:
(9) [iarg0 SFE Technologies] reported a net loss
of $889,000 on sales of $23.4 million.
(10) That compared with an operating [p loss] of
[arg1 $1.9 million] on sales of $27.4 million
in the year-earlier period.
In this case, a comparison discourse relation (sig-
naled by the underlined text) holds between the
first and sentence sentence. The coherence pro-
vided by this relation encourages an inference that
identifies the marked iarg0 (the loser). Through-
out our study, we used gold-standard discourse re-
lations provided by the Penn Discourse TreeBank
(Prasad et al, 2008).
5 Evaluation
We trained the feature-based logistic regression
model over 816 annotated predicate instances as-
sociated with 650 implicitly filled argument posi-
tions (not all predicate instances had implicit ar-
guments). During training, a candidate three-tuple
?p, iargn, c?? was given a positive label if the can-
didate implicit argument c (the primary filler) was
annotated as filling the missing argument position.
To factor out errors from standard SRL analyses,
the model used gold-standard argument labels pro-
vided by PropBank and NomBank. As shown in
Figure 1 (Section 3.2), implicit arguments tend to
be located in close proximity to the predicate. We
found that using all candidate constituents cwithin
the current and previous two sentences worked
best on our development data.
We compared our supervised model with the
simple baseline heuristic defined below:6
Fill iargn for predicate instance p
with the nearest constituent in the two-
sentence candidate window that fills
argn for a different instance of p, where
all nominal predicates are normalized to
their verbal forms.
The normalization allows an existing arg0 for the
verb invested to fill an iarg0 for the noun in-
vestment. We also evaluated an oracle model
that made gold-standard predictions for candidates
within the two-sentence prediction window.
We evaluated these models using the methodol-
ogy proposed by Ruppenhofer et al (2009). For
each missing argument position of a predicate in-
stance, the models were required to either (1) iden-
tify a single constituent that fills the missing argu-
ment position or (2) make no prediction and leave
the missing argument position unfilled. We scored
predictions using the Dice coefficient, which is de-
fined as follows:
2 ? |Predicted
?
True|
|Predicted|+ |True|
(11)
Predicted is the set of tokens subsumed by the
constituent predicted by the model as filling a
missing argument position. True is the set of
tokens from a single annotated constituent that
fills the missing argument position. The model?s
prediction receives a score equal to the maxi-
mum Dice overlap across any one of the annotated
fillers. Precision is equal to the summed predic-
tion scores divided by the number of argument po-
sitions filled by the model. Recall is equal to the
summed prediction scores divided by the number
of argument positions filled in our annotated data.
Predictions not covering the head of a true filler
were assigned a score of zero.
6This heuristic outperformed a more complicated heuris-
tic that relied on the PMI score described in section 4.2.
1588
Baseline Discriminative Oracle
# Imp. # P R F1 P R F1 p R F1
sale 64 60 50.0 28.3 36.2 47.2 41.7 44.2 0.118 80.0 88.9
price 121 53 24.0 11.3 15.4 36.0 32.6 34.2 0.008 88.7 94.0
investor 78 35 33.3 5.7 9.8 36.8 40.0 38.4 < 0.001 91.4 95.5
bid 19 26 100.0 19.2 32.3 23.8 19.2 21.3 0.280 57.7 73.2
plan 25 20 83.3 25.0 38.5 78.6 55.0 64.7 0.060 82.7 89.4
cost 25 17 66.7 23.5 34.8 61.1 64.7 62.9 0.024 94.1 97.0
loss 30 12 71.4 41.7 52.6 83.3 83.3 83.3 0.020 100.0 100.0
loan 11 9 50.0 11.1 18.2 42.9 33.3 37.5 0.277 88.9 94.1
investment 21 8 0.0 0.0 0.0 40.0 25.0 30.8 0.182 87.5 93.3
fund 43 6 0.0 0.0 0.0 14.3 16.7 15.4 0.576 50.0 66.7
Overall 437 246 48.4 18.3 26.5 44.5 40.4 42.3 < 0.001 83.1 90.7
Table 3: Evaluation results. The second column gives the number of predicate instances evaluated.
The third column gives the number of ground-truth implicitly filled argument positions for the predicate
instances (not all instances had implicit arguments). P , R, and F1 indicate precision, recall, and F-
measure (? = 1), respectively. p-values denote the bootstrapped significance of the difference in F1
between the baseline and discriminative models. Oracle precision (not shown) is 100% for all predicates.
Our evaluation data comprised 437 predicate in-
stances associated with 246 implicitly filled ar-
gument positions. Table 3 presents the results.
Predicates with the highest number of implicit ar-
guments - sale and price - showed F1 increases
of 8 points and 18.8 points, respectively. Over-
all, the discriminative model increased F1 perfor-
mance 15.8 points (59.6%) over the baseline.
We measured human performance on this task
by running our undergraduate assistant?s annota-
tions against the evaluation data. Our assistant
achieved an overall F1 score of 58.4% using the
same candidate window as the baseline and dis-
criminative models. The difference in F1 between
the discriminative and human results had an ex-
act p-value of less than 0.001. All significance
testing was performed using a two-tailed bootstrap
method similar to the one described by Efron and
Tibshirani (1993).
6 Discussion
6.1 Feature ablation
We conducted an ablation study to measure the
contribution of specific feature sets. Table 4
presents the ablation configurations and results.
For each configuration, we retrained and retested
the discriminative model using the features de-
scribed. As shown, we observed significant losses
when excluding features that relate the seman-
tic roles of mentions in c? to the semantic role
Percent change (p-value)
Configuration P R F1
Remove 1,2,5 -35.3
(< 0.01)
-36.1
(< 0.01)
-35.7
(< 0.01)
Use 1,2,5 only -26.3
(< 0.01)
-11.9
(0.05)
-19.2
(< 0.01)
Remove 14 0.2
(0.95)
1.0
(0.66)
0.7
(0.73)
Table 4: Feature ablation results. The first column
lists the feature configurations. All changes are
percentages relative to the full-featured discrimi-
native model. p-values for the changes are indi-
cated in parentheses.
of the missing argument position (first configura-
tion). The second configuration tested the effect of
using only the SRL-based features. This also re-
sulted in significant performance losses, suggest-
ing that the other features contribute useful infor-
mation. Lastly, we tested the effect of removing
discourse relations (feature 14), which are likely
to be difficult to extract reliably in a practical set-
ting. As shown, this feature did not have a statis-
tically significant effect on performance and could
be excluded in future applications of the model.
6.2 Unclassified true implicit arguments
Of all the errors made by the system, approxi-
mately 19% were caused by the system?s failure to
1589
generate a candidate constituent c that was a cor-
rect implicit argument. Without such a candidate,
the system stood no chance of identifying a cor-
rect implicit argument. Two factors contributed to
this type of error, the first being our assumption
that implicit arguments are also core (i.e., argn)
arguments to traditional SRL structures. Approxi-
mately 8% of the overall error was due to a failure
of this assumption. In many cases, the true im-
plicit argument filled a non-core (i.e., adjunct) role
within PropBank or NomBank.
More frequently, however, true implicit argu-
ments were missed because the candidate window
was too narrow. This accounts for 12% of the
overall error. Oracle recall (second-to-last col-
umn in Table 3) indicates the nominals that suf-
fered most from windowing errors. For exam-
ple, the sale predicate was associated with the
highest number of true implicit arguments, but
only 80% of those could be resolved within the
two-sentence candidate window. Empirically, we
found that extending the candidate window uni-
formly for all predicates did not increase perfor-
mance on the development data. The oracle re-
sults suggest that predicate-specific window set-
tings might offer some advantage.
6.3 The investment and fund predicates
In Section 4.2, we discussed the price predicate,
which frequently occurs in the ?[p price] index?
collocation. We observed that this collocation
is rarely associated with either an overt arg0 or
an implicit iarg0. Similar observations can be
made for the investment and fund predicates. Al-
though these two predicates are frequent, they are
rarely associated with implicit arguments: invest-
ment takes only eight implicit arguments across its
21 instances, and fund takes only six implicit ar-
guments across its 43 instances. This behavior is
due in large part to collocations such as ?[p in-
vestment] banker?, ?stock [p fund]?, and ?mutual
[p fund]?, which use predicate senses that are not
eventive. Such collocations also violate our as-
sumption that differences between the PropBank
and NomBank argument structure for a predicate
are indicative of implicit arguments (see Section
3.1 for this assumption).
Despite their lack of implicit arguments, it is
important to account for predicates such as in-
vestment and fund because incorrect prediction of
implicit arguments for them can lower precision.
This is precisely what happened for the fund pred-
icate, where the model incorrectly identified many
implicit arguments for ?stock [p fund]? and ?mu-
tual [p fund]?. The left context of fund should help
the model avoid this type of error; however, our
feature selection process did not identify any over-
all gains from including this information.
6.4 Improvements versus the baseline
The baseline heuristic covers the simple case
where identical predicates share arguments in the
same position. Thus, it is interesting to examine
cases where the baseline heuristic failed but the
discriminative model succeeded. Consider the fol-
lowing sentence:
(12) Mr. Rogers recommends that [p investors]
sell [iarg2 takeover-related stock].
Neither NomBank nor the baseline heuristic asso-
ciate the marked predicate in Example 12 with any
arguments; however, the feature-based model was
able to correctly identify the marked iarg2 as the
entity being invested in. This inference captured a
tendency of investors to sell the things they have
invested in.
We conclude our discussion with an example of
an extra-sentential implicit argument:
(13) [iarg0 Olivetti] has denied that it violated
the rules, asserting that the shipments were
properly licensed. However, the legality of
these [p sales] is still an open question.
As shown in Example 13, the system was able to
correctly identify Olivetti as the agent in the sell-
ing event of the second sentence. This inference
involved two key steps. First, the system identified
coreferent mentions of Olivetti that participated in
exporting and supplying events (not shown). Sec-
ond, the system identified a tendency for exporters
and suppliers to also be sellers. Using this knowl-
edge, the system extracted information that could
not be extracted by the baseline heuristic or a tra-
ditional SRL system.
7 Conclusions and future work
Current SRL approaches limit the search for ar-
guments to the sentence containing the predicate
of interest. Many systems take this assumption
a step further and restrict the search to the predi-
cate?s local syntactic environment; however, pred-
icates and the sentences that contain them rarely
1590
exist in isolation. As shown throughout this paper,
they are usually embedded in a coherent and se-
mantically rich discourse that must be taken into
account. We have presented a preliminary study
of implicit arguments for nominal predicates that
focused specifically on this problem.
Our contribution is three-fold. First, we have
created gold-standard implicit argument annota-
tions for a small set of pervasive nominal predi-
cates.7 Our analysis shows that these annotations
add 65% to the role coverage of NomBank. Sec-
ond, we have demonstrated the feasibility of re-
covering implicit arguments for many of the pred-
icates, thus establishing a baseline for future work
on this emerging task. Third, our study suggests
a few ways in which this research can be moved
forward. As shown in Section 6, many errors were
caused by the absence of true implicit arguments
within the set of candidate constituents. More in-
telligent windowing strategies in addition to al-
ternate candidate sources might offer some im-
provement. Although we consistently observed
development gains from using automatic coref-
erence resolution, this process creates errors that
need to be studied more closely. It will also be
important to study implicit argument patterns of
non-verbal predicates such as the partitive percent.
These predicates are among the most frequent in
the TreeBank and are likely to require approaches
that differ from the ones we pursued.
Finally, any extension of this work is likely to
encounter a significant knowledge acquisition bot-
tleneck. Implicit argument annotation is difficult
because it requires both argument and coreference
identification (the data produced by Ruppenhofer
et al (2009) is similar). Thus, it might be produc-
tive to focus future work on (1) the extraction of
relevant knowledge from existing resources (e.g.,
our use of coreference patterns from Gigaword) or
(2) semi-supervised learning of implicit argument
models from a combination of labeled and unla-
beled data.
Acknowledgments
We would like to thank the anonymous review-
ers for their helpful questions and comments. We
would also like to thank Malcolm Doering for his
annotation effort. This work was supported in part
by NSF grants IIS-0347548 and IIS-0840538.
7Our annotation data can be freely downloaded at
http://links.cse.msu.edu:8000/lair/projects/semanticrole.html
References
Aljoscha Burchardt, Anette Frank, and Manfred
Pinkal. 2005. Building text meaning representa-
tions from contextually related frames - a case study.
In Proceedings of the Sixth International Workshop
on Computational Semantics.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 789?797, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):3746.
Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman & Hall, New
York.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Commu-
nication). The MIT Press, May.
C.J. Fillmore and C.F. Baker. 2001. Frame semantics
for text understanding. In Proceedings of WordNet
and Other Lexical Resources Workshop, NAACL.
Matthew Gerber, Joyce Y. Chai, and Adam Meyers.
2009. The role of implicit argumentation in nominal
SRL. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics,
pages 146?154, Boulder, Colorado, USA, June.
David Graff. 2003. English Gigaword. Linguistic
Data Consortium, Philadelphia.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proceedings of the Linguistic Annotation
Workshop in ACL-2007, page 132139.
1591
Kenji Imamura, Kuniko Saito, and Tomoko Izumi.
2009. Discriminative approach to predicate-
argument structure analysis with zero-anaphora res-
olution. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 85?88, Suntec, Sin-
gapore, August. Association for Computational Lin-
guistics.
P. Kingsbury, M. Palmer, and M. Marcus. 2002.
Adding semantic annotation to the Penn TreeBank.
In Proceedings of the Human Language Technology
Conference (HLT?02).
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, Department
of Computer and Information Science University of
Pennsylvania.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn TreeBank. Computa-
tional Linguistics, 19:313?330.
Adam Meyers. 2007. Annotation guidelines for
NomBank - noun argument structure for PropBank.
Technical report, New York University.
Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-
man, Lynette Hirschman, Marcia Linebarger, and
John Dowding. 1986. Recovering implicit infor-
mation. In Proceedings of the 24th annual meeting
on Association for Computational Linguistics, pages
10?19, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Patrick Pantel and Deepak Ravichandran. 2004.
Automatically labeling semantic classes. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
321?328, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Rashmi Prasad, Alan Lee, Nikhil Dinesh, Eleni Milt-
sakaki, Geraud Campion, Aravind Joshi, and Bonnie
Webber. 2008. Penn discourse treebank version 2.0.
Linguistic Data Consortium, February.
P. Pudil, J. Novovicova, and J. Kittler. 1994. Floating
search methods in feature selection. Pattern Recog-
nition Letters, 15:1119?1125.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and infer-
ence in semantic role labeling. Comput. Linguist.,
34(2):257?287.
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2009.
Semeval-2010 task 10: Linking events and their
participants in discourse. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-2009),
pages 106?111, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2004. Automatic construction of nominal
case frames and its application to indirect anaphora
resolution. In Proceedings of Coling 2004, pages
1201?1207, Geneva, Switzerland, Aug 23?Aug 27.
COLING.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL 2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL 2008:
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 159?177,
Manchester, England, August. Coling 2008 Orga-
nizing Committee.
1592
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 43?51,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Open-domain Commonsense Reasoning Using Discourse Relations from a
Corpus of Weblog Stories
Matt Gerber
Department of Computer Science
Michigan State University
gerberm2@msu.edu
Andrew S. Gordon and Kenji Sagae
Institute for Creative Technologies
University of Southern California
{gordon,sagae}@ict.usc.edu
Abstract
We present a method of extracting open-
domain commonsense knowledge by apply-
ing discourse parsing to a large corpus of per-
sonal stories written by Internet authors. We
demonstrate the use of a linear-time, joint syn-
tax/discourse dependency parser for this pur-
pose, and we show how the extracted dis-
course relations can be used to generate open-
domain textual inferences. Our evaluations
of the discourse parser and inference models
show some success, but also identify a num-
ber of interesting directions for future work.
1 Introduction
The acquisition of open-domain knowledge in sup-
port of commonsense reasoning has long been a
bottleneck within artificial intelligence. Such rea-
soning supports fundamental tasks such as textual
entailment (Giampiccolo et al, 2008), automated
question answering (Clark et al, 2008), and narra-
tive comprehension (Graesser et al, 1994). These
tasks, when conducted in open domains, require vast
amounts of commonsense knowledge pertaining to
states, events, and their causal and temporal relation-
ships. Manually created resources such as FrameNet
(Baker et al, 1998), WordNet (Fellbaum, 1998), and
Cyc (Lenat, 1995) encode many aspects of com-
monsense knowledge; however, coverage of causal
and temporal relationships remains low for many do-
mains.
Gordon and Swanson (2008) argued that the
commonsense tasks of prediction, explanation, and
imagination (collectively called envisionment) can
be supported by knowledge mined from a large cor-
pus of personal stories written by Internet weblog
authors.1 Gordon and Swanson (2008) identified
three primary obstacles to such an approach. First,
stories must be distinguished from other weblog
content (e.g., lists, recipes, and reviews). Second,
stories must be analyzed in order to extract the im-
plicit commonsense knowledge that they contain.
Third, inference mechanisms must be developed that
use the extracted knowledge to perform the core en-
visionment tasks listed above.
In the current paper, we present an approach to
open-domain commonsense inference that addresses
each of the three obstacles identified by Gordon and
Swanson (2008). We built on the work of Gordon
and Swanson (2009), who describe a classification-
based approach to the task of story identification.
The authors? system produced a corpus of approx-
imately one million personal stories, which we used
as a starting point. We applied efficient discourse
parsing techniques to this corpus as a means of ex-
tracting causal and temporal relationships. Further-
more, we developed methods that use the extracted
knowledge to generate textual inferences for de-
scriptions of states and events. This work resulted
in an end-to-end prototype system capable of gen-
erating open-domain, commonsense inferences us-
ing a repository of knowledge extracted from un-
structured weblog text. We focused on identifying
1We follow Gordon and Swanson (2009) in defining a story
to be a ?textual discourse that describes a specific series of
causally related events in the past, spanning a period of time
of minutes, hours, or days, where the author or a close associate
is among the participants.?
43
strengths and weaknesses of the system in an effort
to guide future work.
We structure our presentation as follows: in Sec-
tion 2, we present previous research that has inves-
tigated the use of large web corpora for natural lan-
guage processing (NLP) tasks. In Section 3, we de-
scribe an efficient method of automatically parsing
weblog stories for discourse structure. In Section 4,
we present a set of inference mechanisms that use
the extracted discourse relations to generate open-
domain textual inferences. We conclude, in Section
5, with insights into story-based envisionment that
we hope will guide future work in this area.
2 Related work
Researchers have made many attempts to use the
massive amount of linguistic content created by
users of the World Wide Web. Progress and chal-
lenges in this area have spawned multiple workshops
(e.g., those described by Gurevych and Zesch (2009)
and Evert et al (2008)) that specifically target the
use of content that is collaboratively created by In-
ternet users. Of particular relevance to the present
work is the weblog corpus developed by Burton et
al. (2009), which was used for the data challenge
portion of the International Conference on Weblogs
and Social Media (ICWSM). The ICWSM weblog
corpus (referred to here as Spinn3r) is freely avail-
able and comprises tens of millions of weblog en-
tries posted between August 1st, 2008 and October
1st, 2008.
Gordon et al (2009) describe an approach to
knowledge extraction over the Spinn3r corpus using
techniques described by Schubert and Tong (2003).
In this approach, logical propositions (known as fac-
toids) are constructed via approximate interpreta-
tion of syntactic analyses. As an example, the sys-
tem identified a factoid glossed as ?doors to a room
may be opened?. Gordon et al (2009) found that
the extracted factoids cover roughly half of the fac-
toids present in the corresponding Wikipedia2 arti-
cles. We used a subset of the Spinn3r corpus in
our work, but focused on discourse analyses of en-
tire texts instead of syntactic analyses of single sen-
tences. Our goal was to extract general causal and
temporal propositions instead of the fine-grained
2http://en.wikipedia.org
properties expressed by many factoids extracted by
Gordon et al (2009).
Clark and Harrison (2009) pursued large-scale
extraction of knowledge from text using a syntax-
based approach that was also inspired by the work
of Schubert and Tong (2003). The authors showed
how the extracted knowledge tuples can be used
to improve syntactic parsing and textual entailment
recognition. Bar-Haim et al (2009) present an ef-
ficient method of performing inference with such
knowledge.
Our work is also related to the work of Persing
and Ng (2009), in which the authors developed a
semi-supervised method of identifying the causes of
events described in aviation safety reports. Simi-
larly, our system extracts causal (as well as tem-
poral) knowledge; however, it does this in an open
domain and does not place limitations on the types
of causes to be identified. This greatly increases
the complexity of the inference task, and our results
exhibit a corresponding degradation; however, our
evaluations provide important insights into the task.
3 Discourse parsing a corpus of stories
Gordon and Swanson (2009) developed a super-
vised classification-based approach for identifying
personal stories within the Spinn3r corpus. Their
method achieved 75% precision on the binary task
of predicting story versus non-story on a held-out
subset of the Spinn3r corpus. The extracted ?story
corpus? comprises 960,098 personal stories written
by weblog users. Due to its large size and broad
domain coverage, the story corpus offers unique op-
portunities to NLP researchers. For example, Swan-
son and Gordon (2008) showed how the corpus can
be used to support open-domain collaborative story
writing.3
As described by Gordon and Swanson (2008),
story identification is just the first step towards com-
monsense reasoning using personal stories. We ad-
dressed the second step - knowledge extraction -
by parsing the corpus using a Rhetorical Structure
Theory (Carlson and Marcu, 2001) parser based on
the one described by Sagae (2009). The parser
performs joint syntactic and discourse dependency
3The system (called SayAnything) is available at
http://sayanything.ict.usc.edu
44
parsing using a stack-based, shift-reduce algorithm
with runtime that is linear in the input length. This
lightweight approach is very efficient; however, it
may not be quite as accurate as more complex, chart-
based approaches (e.g., the approach of Charniak
and Johnson (2005) for syntactic parsing).
We trained the discourse parser over the causal
and temporal relations contained in the RST corpus.
Examples of these relations are shown below:
(1) [cause Packages often get buried in the load]
[result and are delivered late.]
(2) [before Three months after she arrived in L.A.]
[after she spent $120 she didn?t have.]
The RST corpus defines many fine-grained rela-
tions that capture causal and temporal properties.
For example, the corpus differentiates between re-
sult and reason for causation and temporal-after and
temporal-before for temporal order. In order to in-
crease the amount of available training data, we col-
lapsed all causal and temporal relations into two
general relations causes and precedes. This step re-
quired normalization of asymmetric relations such
as temporal-before and temporal-after.
To evaluate the discourse parser described above,
we manually annotated 100 randomly selected we-
blog stories from the story corpus produced by Gor-
don and Swanson (2009). For increased efficiency,
we limited our annotation to the generalized causes
and precedes relations described above. We at-
tempted to keep our definitions of these relations
in line with those used by RST. Following previous
discourse annotation efforts, we annotated relations
over clause-level discourse units, permitting rela-
tions between adjacent sentences. In total, we an-
notated 770 instances of causes and 1,009 instances
of precedes.
We experimented with two versions of the RST
parser, one trained on the fine-grained RST rela-
tions and the other trained on the collapsed relations.
At testing time, we automatically mapped the fine-
grained relations to their corresponding causes or
precedes relation. We computed the following ac-
curacy statistics:
Discourse segmentation accuracy For each pre-
dicted discourse unit, we located the reference
discourse unit with the highest overlap. Accu-
racy for the predicted discourse unit is equal to
the percentage word overlap between the refer-
ence and predicted discourse units.
Argument identification accuracy For each dis-
course unit of a predicted discourse relation,
we located the reference discourse unit with the
highest overlap. Accuracy is equal to the per-
centage of times that a reference discourse rela-
tion (of any type) holds between the reference
discourse units that overlap most with the pre-
dicted discourse units.
Argument classification accuracy For the subset
of instances in which a reference discourse re-
lation holds between the units that overlap most
with the predicted discourse units, accuracy is
equal to the percentage of times that the pre-
dicted discourse relation matches the reference
discourse relation.
Complete accuracy For each predicted discourse
relation, accuracy is equal to the percentage
word overlap with a reference discourse rela-
tion of the same type.
Table 1 shows the accuracy results for the fine-
grained and collapsed versions of the RST discourse
parser. As shown in Table 1, the collapsed version
of the discourse parser exhibits higher overall ac-
curacy. Both parsers predicted the causes relation
much more often than the precedes relation, so the
overall scores are biased toward the scores for the
causes relation. For comparison, Sagae (2009) eval-
uated a similar RST parser over the test section of
the RST corpus, obtaining precision of 42.9% and
recall of 46.2% (F1 = 44.5%).
In addition to the automatic evaluation described
above, we also manually assessed the output of the
discourse parsers. One of the authors judged the
correctness of each extracted discourse relation, and
we found that the fine-grained and collapsed ver-
sions of the parser performed equally well with a
precision near 33%; however, throughout our exper-
iments, we observed more desirable discourse seg-
mentation when working with the collapsed version
of the discourse parser. This fact, combined with the
results of the automatic evaluation presented above,
45
Fine-grained RST parser Collapsed RST parser
Accuracy metric causes precedes overall causes precedes overall
Segmentation 36.08 44.20 36.67 44.36 30.13 43.10
Argument identification 25.00 33.33 25.86 26.15 23.08 25.87
Argument classification 66.15 50.00 64.00 79.41 83.33 79.23
Complete 22.20 28.88 22.68 31.26 21.21 30.37
Table 1: RST parser evaluation. All values are percentages.
led us to use the collapsed version of the parser in
all subsequent experiments.
Having developed and evaluated the discourse
parser, we conducted a full discourse parse of the
story corpus, which comprises more than 25 million
sentences split into nearly 1 million weblog entries.
The discourse parser extracted 2.2 million instances
of the causes relation and 220,000 instances of the
precedes relation. As a final step, we indexed the
extracted discourse relations with the Lucene infor-
mation retrieval engine.4 Each discourse unit (two
per discourse relation) is treated as a single docu-
ment, allowing us to query the extracted relations
using information retrieval techniques implemented
in the Lucene toolkit.
4 Generating textual inferences
As mentioned previously, Gordon and Swan-
son (2008) cite three obstacles to performing com-
monsense reasoning using weblog stories. Gordon
and Swanson (2009) addressed the first (story col-
lection). We addressed the second (story analysis)
by developing a discourse parser capable of extract-
ing causal and temporal relations from weblog text
(Section 3). In this section, we present a prelimi-
nary solution to the third problem - reasoning with
the extracted knowledge.
4.1 Inference method
In general, we require an inference method that takes
as input the following things:
1. A description of the state or event of interest.
This is a free-text description of any length.
2. The type of inference to perform, either causal
or temporal.
4Available at http://lucene.apache.org
3. The inference direction, either forward or back-
ward. Forward causal inference produces the
effects of the given state or event. Backward
causal inference produces causes of the given
state or event. Similarly, forward and back-
ward temporal inferences produce subsequent
and preceding states and events, respectively.
As a simple baseline approach, we implemented the
following procedure. First, given a textual input de-
scription d, we query the extracted discourse units
using Lucene?s modified version of the vector space
model over TF-IDF term weights. This produces a
ranked list Rd of discourse units matching the input
description d. We then filterRd, removing discourse
units that are not linked to other discourse units by
the given relation and in the given direction. Each el-
ement of the filtered Rd is thus linked to a discourse
unit that could potentially satisfy the inference re-
quest.
To demonstrate, we perform forward causal infer-
ence using the following input description d:
(3) John traveled the world.
Below, we list the three top-ranked discourse units
that matched d (left-hand side) and their associated
consequents (right-hand side):
1. traveling the world? to murder
2. traveling from around the world to be there ?
even though this crowd was international
3. traveled across the world? to experience it
In a na??ve way, one might simply choose the top-
ranked clause in Rd and select its associated clause
as the answer to the inference request; however, in
the example above, this would incorrectly generate
?to murder? as the effect of John?s traveling (this is
46
more appropriately viewed as the purpose of trav-
eling). The other effect clauses also appear to be
incorrect. This should not come as much of a sur-
prise because the ranking was generated soley from
the match score between the input description and
the causes in Rd, which are quite relevant.
One potential problem with the na??ve selection
method is that it ignores information contained in
the ranked list R?d of clauses that are associated with
the clauses in Rd. In our experiments, we often
observed redundancies in R?d that captured general
properties of the desired inference. Intuitively, con-
tent that is shared across elements ofR?d could repre-
sent the core meaning of the desired inference result.
In what follows, we describe various re-rankings
of R?d using this shared content. For each model
described, the final inference prediction is the top-
ranked element of R?d.
Centroid similarity To approximate the shared
content of discourse units in R?d, we treat each
discourse unit as a vector of TF scores. We then
compute the average vector and re-rank all dis-
course units in R?d based on their cosine simi-
larity with the average vector. This favors infer-
ence results that ?agree? with many alternative
hypotheses.
Description score scaling In this approach, we in-
corporate the score from Rd into the centroid
similarity score, multiplying the two and giving
equal weight to each. This captures the intu-
ition that the top-ranked element of R?d should
represent the general content of the list but
should also be linked to an element of Rd that
bears high similarity to the given state or event
description d.
Log-length scaling When working with the cen-
troid similarity score, we often observed top-
ranked elements of R?d that were only a few
words in length. This was typically the case
when components from sparse TF vectors in
R?d matched well with components from the
centroid vector. Ideally, we would like more
lengthy (but not too long) descriptions. To
achieve this, we multiplied the centroid simi-
larity score by the logarithm of the word length
of the discourse unit in R?d.
Description score/log-length scaling In this ap-
proach, we combine the description score scal-
ing and log-length scaling, multiplying the cen-
troid similarity by both and giving equal weight
to all three factors.
4.2 Evaluating the generated textual inferences
To evaluate the inference re-ranking models de-
scribed above, we automatically generated for-
ward/backward causal and temporal inferences for
five documents (265 sentences) drawn randomly
from the story corpus. For simplicity, we gener-
ated an inference for each sentence in each docu-
ment. Each inference re-ranking model is able to
generate four textual inferences (forward/backward
causal/temporal) for each sentence. In our experi-
ments, we only kept the highest-scoring of the four
inferences generated by a model. One of the authors
then manually evaluated the final predictions for cor-
rectness. This was a subjective process, but it was
guided by the following requirements:
1. The generated inference must increase the lo-
cal coherence of the document. As described
by Graesser et al (1994), readers are typically
required to make inferences about the text that
lead to a coherent understanding thereof. We
required the generated inferences to aid in this
task.
2. The generated inferences must be globally
valid. To demonstrate global validity, consider
the following actual output:
(4) I didn?t even need a jacket (until I got
there).
In Example 4, the system-generated forward
temporal inference is shown in parentheses.
The inference makes sense given its local con-
text; however, it is clear from the surround-
ing discourse (not shown) that a jacket was not
needed at any point in time (it happened to be
a warm day). As a result, this prediction was
tagged as incorrect.
Table 2 presents the results of the evaluation. As
shown in the table, the top-performing models are
those that combine centroid similarity with one or
both of the other re-ranking heuristics.
47
Re-ranking model Inference accuracy (%)
None 10.19
Centroid similarity 12.83
Description score scaling 17.36
Log-length scaling 12.83
Description score/log-length scaling 16.60
Table 2: Inference generation evaluation results.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.0
5 0.1 0.1
5 0.2 0.2
5 0.3 0.3
5 0.4 0.4
5 0.5 0.5
5 0.6 0.6
5 0.7 0.7
5 0.8 0.8
5 0.9 0.9
5 1
Confidence-ordered percentage of all 
inferences
In
fe
re
nc
e 
ac
cu
ra
cy None
Centroid similarity
Description score scaling
Log-length scaling
Combined scaling
Figure 1: Inference rate versus accuracy. Values along the x-axis indicate that the top-scoring x% of all inferences
were evaluated. Values along the y-axis indicate the prediction accuracy.
The analysis above demonstrates the relative per-
formance of the models when making inferences for
all sentences; however it is probably the case that
many generated inferences should be rejected due to
their low score. Because the output scores of a single
model can be meaningfully compared across predic-
tions, it is possible to impose a threshold on the in-
ference generation process such that any prediction
scoring at or below the threshold is withheld. We
varied the prediction threshold from zero to a value
sufficiently large that it excluded all predictions for
a model. Doing so demonstrates the trade-off be-
tween making a large number of textual inferences
and making accurate textual inferences. Figure 1
shows the effects of this variable on the re-ranking
models. As shown in Figure 1, the highest infer-
ence accuracy is reached by the re-ranker that com-
bines description score and log-length scaling with
the centroid similarity measure. This accuracy is at-
tained by keeping the top 25% most confident infer-
ences.
5 Conclusions
We have presented an approach to commonsense
reasoning that relies on (1) the availability of a large
corpus of personal weblog stories and (2) the abil-
ity to analyze and perform inference with these sto-
ries. Our current results, although preliminary, sug-
gest novel and important areas of future exploration.
We group our observations according to the last two
problems identified by Gordon and Swanson (2008):
story analysis and envisioning with the analysis re-
sults.
5.1 Story analysis
As in other NLP tasks, we observed significant per-
formance degradation when moving from the train-
ing genre (newswire) to the testing genre (Internet
48
weblog stories). Because our discourse parser relies
heavily on lexical and syntactic features for classi-
fication, and because the distribution of the feature
values varies widely between the two genres, the
performance degradation is to be expected. Recent
techniques in parser adaptation for the Brown corpus
(McClosky et al, 2006) might be usefully applied to
the weblog genre as well.
Our supervised classification-based approach to
discourse parsing could also be improved with ad-
ditional training data. Causal and temporal relations
are instantiated a combined 2,840 times in the RST
corpus, with a large majority of these being causal.
In contrast, the Penn Discourse TreeBank (Prasad et
al., 2008) contains 7,448 training instances of causal
relations and 2,763 training instances of temporal
relations. This represents a significant increase in
the amount of training data over the RST corpus. It
would be informative to compare our current results
with those obtained using a discourse parser trained
on the Penn Discourse TreeBank.
One might also extract causal and temporal rela-
tions using traditional semantic role analysis based
on FrameNet (Baker et al, 1998) or PropBank
(Kingsbury and Palmer, 2003). The former defines a
number of frames related to causation and temporal
order, and roles within the latter could be mapped to
standard thematic roles (e.g., cause) via SemLink.5
5.2 Envisioning with the analysis results
We believe commonsense reasoning based on we-
blog stories can also be improved through more so-
phisticated uses of the extracted discourse relations.
As a first step, it would be beneficial to explore alter-
nate input descriptions. As presented in Section 4.2,
we make textual inferences at the sentence level for
simplicity; however, it might be more reasonable to
make inferences at the clause level, since clauses are
the basis for RST and Penn Discourse TreeBank an-
notation. This could result in the generation of sig-
nificantly more inferences due to multi-clause sen-
tences; thus, more intelligent inference filtering will
be required.
Our models use prediction scores for the tasks
of rejecting inferences and selecting between mul-
tiple candidate inferences (i.e., forward/backward
5Available at http://verbs.colorado.edu/semlink
causal/temporal). Instead of relying on prediction
scores for these tasks, it might be advantageous to
first identify whether or not envisionment should be
performed for a clause, and, if it should, what type
and direction of envisionment would be best. For
example, consider the following sentence:
(5) [clause1 John went to the store] [clause2
because he was hungry].
It would be better - from a local coherence perspec-
tive - to infer the cause of the second clause instead
of the cause of the first. This is due to the fact that a
cause for the first clause is explicitly stated, whereas
a cause for the second clause is not. Inferences made
about the first clause (e.g., that John went to the store
because his dog was hungry), are likely to be unin-
formative or in conflict with explicitly stated infor-
mation.
Example 5 raises the important issue of context,
which we believe needs to be investigated further.
Here, context refers to the discourse that surrounds
the clause or sentence for which the system is at-
tempting to generate a textual inference. The con-
text places a number of constraints on allowable in-
ferences. For example, in addition to content-based
constraints demonstrated in Example 5, the context
limits pronoun usage, entity references, and tense.
Violations of these constraints will reduce local co-
herence.
Finally, the story corpus, with its vast size, is
likely to contain a significant amount of redundancy
for common events and states. Our centroid-based
re-ranking heuristics are inspired by this redun-
dancy, and we expect that aggregation techniques
such as clustering might be of some use when ap-
plied to the corpus as a whole. Having identified
coherent clusters of causes, it might be easier to find
a consequence for a previously unseen cause.
In summary, we have presented preliminary re-
search into the task of using a large, collaboratively
constructed corpus as a commonsense knowledge
repository. Rather than relying on hand-coded on-
tologies and event schemas, our approach relies on
the implicit knowledge contained in written natu-
ral language. We have demonstrated the feasibility
of obtaining the discourse structure of such a cor-
pus via linear-time parsing models. Furthermore,
49
we have introduced inference procedures that are ca-
pable of generating open-domain textual inferences
from the extracted knowledge. Our evaluation re-
sults suggest many opportunities for future work in
this area.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their helpful comments and sugges-
tions. The project or effort described here has
been sponsored by the U.S. Army Research, Devel-
opment, and Engineering Command (RDECOM).
Statements and opinions expressed do not necessar-
ily reflect the position or the policy of the United
States Government, and no official endorsement
should be inferred.
References
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In Christian Boitet
and PeteWhitelock, editors, Proceedings of the Thirty-
Sixth Annual Meeting of the Association for Computa-
tional Linguistics and Seventeenth International Con-
ference on Computational Linguistics, pages 86?90,
San Francisco, California. MorganKaufmann Publish-
ers.
Roy Bar-Haim, Jonathan Berant, and Ido Dagan. 2009.
A compact forest for scalable inference over entail-
ment and paraphrase rules. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1056?1065, Singapore, Au-
gust. Association for Computational Linguistics.
K. Burton, A. Java, and I. Soboroff. 2009. The icwsm
2009 spinn3r dataset. In Proceedings of the Third An-
nual Conference on Weblogs and Social Media.
Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging manual. Technical Report ISI-TR-545, ISI, July.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
Peter Clark and Phil Harrison. 2009. Large-scale extrac-
tion and use of knowledge from text. In K-CAP ?09:
Proceedings of the fifth international conference on
Knowledge capture, pages 153?160, New York, NY,
USA. ACM.
Peter Clark, Christiane Fellbaum, Jerry R. Hobbs, Phil
Harrison, William R. Murray, and John Thompson.
2008. Augmenting WordNet for Deep Understanding
of Text. In Johan Bos and Rodolfo Delmonte, editors,
Semantics in Text Processing. STEP 2008 Conference
Proceedings, volume 1 of Research in Computational
Semantics, pages 45?57. College Publications.
Stefan Evert, Adam Kilgarriff, and Serge Sharoff, edi-
tors. 2008. 4th Web as Corpus Workshop Can we beat
Google?
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Communi-
cation). The MIT Press, May.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, and Bill Dolan. 2008. The
fourth fascal recognizing textual entailment challenge.
In Proceedings of the First Text Analysis Conference.
Andrew Gordon and Reid Swanson. 2008. Envision-
ing with weblogs. In International Conference on New
Media Technology.
Andrew Gordon and Reid Swanson. 2009. Identifying
personal stories in millions of weblog entries. In Third
International Conference on Weblogs and Social Me-
dia.
Jonathan Gordon, Benjamin Van Durme, and Lenhart
Schubert. 2009. Weblogs as a source for extracting
general world knowledge. In K-CAP ?09: Proceed-
ings of the fifth international conference on Knowledge
capture, pages 185?186, New York, NY, USA. ACM.
A. C. Graesser, M. Singer, and T. Trabasso. 1994. Con-
structing inferences during narrative text comprehen-
sion. Psychological Review, 101:371?395.
Iryna Gurevych and Torsten Zesch, editors. 2009. The
Peoples Web Meets NLP: Collaboratively Constructed
Semantic Resources.
Paul Kingsbury and Martha Palmer. 2003. Propbank: the
next level of treebank. In Proceedings of Treebanks
and Lexical Theories.
Douglas B. Lenat. 1995. Cyc: a large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33?38.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In ACL-44: Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics, pages 337?344, Morristown, NJ,
USA. Association for Computational Linguistics.
Isaac Persing and Vincent Ng. 2009. Semi-supervised
cause identification from aviation safety reports. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 843?851, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Rashmi Prasad, Alan Lee, Nikhil Dinesh, Eleni Milt-
sakaki, Geraud Campion, Aravind Joshi, and Bonnie
50
Webber. 2008. Penn discourse treebank version 2.0.
Linguistic Data Consortium, February.
Kenji Sagae. 2009. Analysis of discourse structure with
syntactic dependencies and data-driven shift-reduce
parsing. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT?09), pages
81?84, Paris, France, October. Association for Com-
putational Linguistics.
Lenhart Schubert and Matthew Tong. 2003. Extract-
ing and evaluating general world knowledge from the
brown corpus. In Proceedings of the HLT-NAACL
2003 workshop on Text meaning, pages 7?13, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Reid Swanson and AndrewGordon. 2008. Say anything:
A massively collaborative open domain story writing
companion. In First International Conference on In-
teractive Digital Storytelling.
51
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 63?71,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
A Joint Model of Implicit Arguments for Nominal Predicates
Matthew Gerber and Joyce Y. Chai
Department of Computer Science
Michigan State University
East Lansing, Michigan, USA
{gerberm2,jchai}@cse.msu.edu
Robert Bart
Computer Science and Engineering
University of Washington
Seattle, Washington, USA
rbart@cs.washington.edu
Abstract
Many prior studies have investigated the re-
covery of semantic arguments for nominal
predicates. The models in many of these stud-
ies have assumed that arguments are indepen-
dent of each other. This assumption simpli-
fies the computational modeling of semantic
arguments, but it ignores the joint nature of
natural language. This paper presents a pre-
liminary investigation into the joint modeling
of implicit arguments for nominal predicates.
The joint model uses propositional knowledge
extracted from millions of Internet webpages
to help guide prediction.
1 Introduction
Much recent work on semantic role labeling has fo-
cused on joint models of arguments. This work is
motivated by the fact that one argument can either
promote or inhibit the presence of another argument.
Because most of this work has been done for verbal
SRL, nominal SRL has lagged behind somewhat. In
particular, the ?implicit? nominal SRL model cre-
ated by Gerber and Chai (2010) does not address
joint argument structures. Implicit arguments are
similar to standard SRL arguments, a primary differ-
ence being their ability to cross sentence boundaries.
In the model created by Gerber and Chai, implicit ar-
gument candidates are classified independently and
a heuristic post-processing method is applied to de-
rive the final structure. This paper presents a prelim-
inary joint implicit argument model.
Consider the following sentences:1
1We will use the notation of Gerber and Chai (2010), where
(1) [c1 The president] is currently struggling to
manage [c2 the country?s economy].
(2) If he cannot get it under control, [p loss] of
[arg1 the next election] might result.
In Example 2, we are searching for the iarg0 of loss
(the entity that is losing). The sentence in Exam-
ple 1 supplies two candidates c1 and c2. If one only
considers the predicate loss, then c1 and c2 would
both be reasonable fillers for the iarg0: presidents
often lose things (e.g., votes and allegiance) and
economies often lose things (e.g., jobs and value).
However, the sentence in Example 2 supplies addi-
tional information. It tells the reader that the next
election is the entity being lost. Given this infor-
mation, one would likely prefer c1 over c2 because
economies don?t generally lose elections, whereas
presidents often do. This type of inference is com-
mon in textual discourses because authors assume
a shared knowledge base with their readers. This
knowledge base contains information about events
and their typical participants (e.g., the fact that pres-
idents lose elections but economies do not).
The model presented in this paper relies on a
knowledge base constructed by automatically min-
ing semantic propositions from Internet webpages.
These propositions help to identify likely joint im-
plicit argument configurations. In the following sec-
tion, we review work on joint inference within se-
mantic role labeling. In Sections 4 and 5, we present
the joint implicit argument model and its features.
Evaluation results for this model are given in Sec-
standard nominal arguments are indicated with argn and im-
plicit arguments are indicated with iargn.
63
tion 6. The joint model contains many simplifying
assumptions, which we address in Section 7. We
conclude in Section 8.
2 Related work
A number of recent studies have shown that seman-
tic arguments are not independent and that system
performance can be improved by taking argument
dependencies into account. Consider the following
examples, due to Toutanova et al (2008):
(3) [Temporal The day] that [arg0 the ogre]
[Predicate cooked] [arg1 the children] is still
remembered.
(4) [arg1 The meal] that [arg0 the ogre]
[Predicate cooked] [Beneficiary the children]
is still remembered.
These examples demonstrate the importance of
inter-argument dependencies. The change from day
in Example 3 to meal in Example 4 affects more
than just the Temporal label: additionally, the arg1
changes to Beneficiary, even though the underlying
text (the children) does not change. To capture this
dependency, Toutanova el al. first generate an n-
best list of argument labels for a predicate instance.
They then re-rank this list using joint features that
describe multiple arguments simultaneously. The
features help prevent globally invalid argument con-
figurations (e.g., ones with multiple arg0 labels).
Punyakanok et al (2008) formulate a variety of
constraints on argument configurations. For exam-
ple, arguments are not allowed to overlap the predi-
cate, nor are they allowed to overlap each other. The
authors treat these constraints as binary variables
within an integer linear program, which is optimized
to produce the final labeling.
Ritter et al (2010) investigated joint selectional
preferences. Traditionally, a selectional preference
model provides the strength of association between
a predicate-argument position and a specific textual
expression. Returning to Examples 1 and 2, one
sees that the selectional preference for president and
economy in the iarg0 position of loss should be high.
Ritter et al extended this single-argument model
using a joint formulation of Latent Dirichlet Allo-
cation (LDA) (Blei et al, 2003). In the generative
version of joint LDA, text for the argument posi-
tions is generated from a common hidden variable.
This approach reflects the intuition behind Exam-
ples 1 and 2 and would help identify president as the
iarg0. Training data for the model was drawn from
a large corpus of two-argument tuples extracted by
the TextRunner system, which we describe next.
Both Ritter et al?s model and the model described
in this paper rely heavily on information extracted
by the TextRunner system (Banko et al, 2007).
The TextRunner system extracts tuples from Inter-
net webpages in an unsupervised fashion. One key
difference between TextRunner and other informa-
tion extraction systems is that TextRunner does not
use a closed set of relations (compare to the work
described by ACE (2008)). Instead, the relation set
is left open, leading to the notion of Open Informa-
tion Extraction (OIE). Although OIE often has lower
precision than traditional information extraction, it
is able to extract a wider variety of relations at preci-
sion levels that are often useful (Banko and Etzioni,
2008).
3 Using TextRunner to assess joint
argument assignments
Returning again to Examples 1 and 2, one can query
TextRunner in the following way:
arg0 : ?
Predicate : lose2
arg1 : election
In the TextRunner system, arg0 typically indicates
the Agent and arg1 typically indicates the Theme.
TextRunner provides many tuples in response to this
query, two of which are shown below:
(5) Usually, [arg0 the president?s party]
[Predicate loses] [arg1 seats in the mid-term
election].
(6) [arg0 The president] [Predicate lost] [arg1 the
election].
The tuples present in these sentences give strong in-
dicators about the type of entity that loses elections.
2Nominal predicates are mapped to their verbal forms using
information provided by the NomBank lexicon.
64
Given all of the returned tuples, only a single one
involves economy in the arg0 position:
(7) Any president will take credit for [arg0 a good
economy] or [Predicate lose] [arg1 an
election] over a bad one.
In Example 7, TextRunner has not analyzed the ar-
guments correctly (president should be the arg0, not
economy).3 In Section 5, we show how evidence
from the tuple lists can be aggregated such that cor-
rect analyses (5 and 6) are favored over incorrect
analyses (7). The primary contribution of this paper
is an exploration of how the aggregated evidence can
be used to identify implicit arguments (e.g., presi-
dent in Example 1).
4 Joint model formulation
To simplify the experimental setting, the model de-
scribed in this paper targets the specific situation
where a predicate instance p takes an implicit iarg0
and an implicit iarg1.4 Whereas the model proposed
by Gerber and Chai (2010) classifies candidates for
these positions independently, the model in this pa-
per classifies joint structures by evaluating the fol-
lowing binary prediction function:
P (+| ?p, iarg0, ci, iarg1, cj?) (8)
Equation 8 gives the probability of the joint assign-
ment of ci to iarg0 and cj to iarg1. Given a set of n
candidates c1, . . . , cn ? C , the best labeling is found
by considering all possible assignments of ci and cj :
argmax
(ci,cj)?CxC s.t. i 6=j
P (+| ?p, iarg0, ci, iarg1, cj?)
(9)
Consider modified versions of Examples 1 and 2:
(10) [c1 The president] is currently struggling to
manage [c2 the country?s economy].
(11) If he cannot get it under control before [c3 the
next election], a [p loss] might result.
3Banko and Etzioni (2008) cite a precision score of 88% for
their system.
4This simplifying assumption does not hold for real data,
and is addressed further in Section 7.2.
In this case, we are looking for the iarg0 as well as
the iarg1 for the loss predicate. Three candidates c1,
c2, and c3 are marked. The joint model would eval-
uate the following probabilities, taking the highest
scoring to be the final assignment:
P (+| ?loss, iarg0, president, iarg1, economy?)
*P (+| ?loss, iarg0, president, iarg1, election?)
P (+| ?loss, iarg0, economy, iarg1, president?)
P (+| ?loss, iarg0, economy, iarg1, election?)
P (+| ?loss, iarg0, election, iarg1, president?)
P (+| ?loss, iarg0, election, iarg1, economy?)
Intuitively, only the starred item should have a high
probability. In the following section, we describe
how these probabilities can be estimated using in-
formation extracted by TextRunner.
5 Joint model features
As mentioned in Section 2, the TextRunner system
has been extracting massive amounts of knowledge
in the form of tuples such as the following:
?president, lose, election?
The database of tuples can be queried by supplying
one or more of the tuple arguments. For example,
the following is a partial result list for the query
?president, lose, ??:
?Kenyan president, lose, election?
?president?s party, lose seat in, election?
?president, lose, ally?
The final position in each of these tuples (e.g.,
election) provides a single answer to the question
?What might a president lose??. Aggregation begins
by generalizing each answer to its WordNet synset
(glosses are shown after the arrows):
?Kenyan president, lose, election? ? a vote
?president?s party, lose seat in, election? (same)
?president, lose, ally? ? friendly nation
In cases where a tuple argument has multiple
WordNet senses, the tuple is mapped to the most
common sense as listed in the WordNet database.
65
Having mapped each tuple to its synset, each synset
is ranked according to the number of tuples that
it covers. For the query ?president, lose, ??, this
produces the following ranked list of WordNet
synsets (only the top five are shown, with the
number in parentheses indicating how many tuples
are covered):
1. election (77)
2. war (51)
3. vote (39)
4. people (34)
5. support (26)
...
The synsets above indicate likely answers to the pre-
vious question of ?What might a president lose??.
In a similar manner, one can answer a question
such as ?What might lose an election?? using tu-
ples extracted by TextRunner. The procedure de-
scribed above produces the following ranked list of
WordNet synsets to answer this question:
...
9. people (62)
10. Republican (51)
11. Republican party (51)
12. Hillary (50)
13. president (49)
...
In this case, the expected answer (president) ranks
13th in the list of answer synsets. It is important
to note that lower ranked answers are not necessar-
ily incorrect answers. It is a simple fact that a wide
variety of entities can lose an election. Items 9-13
are all reasonable answers to the original question
of what might lose an election.
The two symmetric questions defined and an-
swered above are closely connected to the implicit
argument situation discussed in Examples 10 and
11. In Example 11, one is searching for the implicit
iarg0 and iarg1 to the loss predicate. Candidates ci
and cj that truly fill these positions should be com-
patible with questions in the following forms:
Question: What did ci lose?
Answer: cj
Question: What entity lost cj?
Answer: ci
If either of these question-answer pairs is not satis-
fied, then the joint assignment of ci to iarg0 and cj
to iarg1 should be considered unlikely. Using the
first question-answer pair above as an example, sat-
isfaction is determined in the following way:
1. Query TextRunner for ?ci, lose, ??, retrieving
the top n tuples.
2. Map the final argument of each tuple to its
WordNet synset and rank the synsets by fre-
quency, producing the ranked list A of answer
synsets.
3. Map cj to its most common WordNet synset
synsetcj and determine whether synsetcj ex-
ists in A. If it does, the question-answer pair is
satisfied.
Some additional processing is required to determine
whether synsetcj exists in A. This is due to the hi-
erarchical organization of WordNet. For example,
suppose that synsetcj is the synset containing ?pri-
mary election? and A contains synsets paraphrased
as follows:
1. election
2. war
3. vote
...
synsetcj does not appear directly in this list; how-
ever, its existence in the list is implied by the follow-
ing hypernymy path within WordNet:
primary election is-a??? election
Intuitively, if synsetcj is connected to a highly
ranked synset in A by a short path, then one has ev-
idence that synsetcj answers the original question.
66
The evidence is weaker if the path is long, as in the
following example:
open primary is-a??? direct primary
is-a??? primary election is-a??? election
Additionally, a path between more specific synsets
(i.e., those lower in the hierarchy) indicates a
stronger relationship than a path between more gen-
eral synsets (i.e., those higher in the hierarchy).
These two situations are depicted in Figure 1. The
synset similarity metric defined by Wu and Palmer
(1994) combines the path length and synset depth
intuitions into a single numeric score that is defined
as follows:
2 ? depth(lca(synset1, synset2))
depth(synset1) + depth(synset2)
(12)
In Equation 12, lca returns the lowest common an-
cestor of the two synsets within the WordNet is-a
hierarchy.
To summarize, Equation 12 indicates the strength
of association between synsetcj (e.g., primary elec-
tion) and a ranked synset synseta from A that an-
swers a question such as ?What might a president
lose??. If the association between synsetcj and
synseta is small, then the assignment of cj to iarg1
is unlikely. The process works similarly for assess-
ing ci as the filler of iarg0. In what follows, we
quantify this intuition with features used to repre-
sent the conditioning information in Equation 8.
Feature 1: Maximum association strength. Given
the conditioning variables in Equation 8, there are
two questions that can be asked:
Question: What did ci p?
Answer: cj
Question: What entity p cj?
Answer: ci
Each of these questions produces a ranked list of
answer synsets using the approach described previ-
ously. The synset for each answer string will match
zero or more of the answer synsets, and each of these
matches will be associated with a similarity score as
defined in Equation 12. Feature 1 considers all such
similarity scores and selects the maximum. A high
value for this feature indicates that one (or both) of
the candidates (ci or cj) is likely to fill its associated
implicit argument position.
Feature 2: Maximum reciprocal rank. Of all the
answer matches described for Feature 1, Feature 2
selects the highest ranking and forms the reciprocal
rank. Thus, values for Feature 2 are in [0,1] with
larger values indicating matches with higher ranked
answer synsets.
Feature 3: Number of matches. This feature
records the total number of answer string matches
from either of the questions described for Feature 1.
Feature 4: Sum reciprocal rank. Feature 2 consid-
ers answer synset matches from either of the posed
questions; ideally, each question-answer pair should
have some influence on the probability estimate in
Equation 8. Feature 4 looks at the answer synset
matches from each question individually. The match
with highest rank for each question is selected, and
the reciprocal rank 2r1 + r2 is computed. The value
of this feature is zero if either of the questions fails
to produce a matching answer synset.
Features 5 and 6: Local classification scores. The
joint model described in this paper does not replace
the local prediction model presented by Gerber and
Chai (2010). The latter uses a wide variety of impor-
tant features that cannot be ignored. Like previous
joint models (e.g., the one described by Toutanova et
al. (2008)), the joint model works on top of the lo-
cal prediction model, whose scores are incorporated
into the joint model as feature-value pairs. Given the
local prediction scores for the iarg0 and iarg1 posi-
tions in Equation 8, the joint model forms two fea-
tures: (1) the sum of the scores for ci filling iarg0
and cj filling iarg1, and (2) the product of these two
scores.
6 Evaluation
We evaluated the joint model described in the pre-
vious sections over the manually annotated implicit
67
entity (a)
physical entity (b)
thing
body of water (c)
bay (d)
matter
abstract entity
Figure 1: Effect of depth on WordNet synset similarity. All links indicate is-a relationships. Although the link
distance from (a) to (b) equals the distance from (c) to (d), the latter are more similar due to their lower depth within
the WordNet hierarchy.
argument data created by Gerber and Chai (2010).
This dataset contains full-text implicit argument
annotations for approximately 1,200 predicate in-
stances within the Penn TreeBank. As mentioned
in Section 4, all experiments were conducted us-
ing predicate instances that take an iarg0 and iarg1
in the ground-truth annotations. We used a ten-
fold cross-validation setup and the evaluation met-
rics proposed by Ruppenhofer et al (2009), which
were also used by Gerber and Chai. For each evalu-
ation fold, features were selected using only the cor-
responding training data and the greedy selection al-
gorithm proposed by Pudil et al (1994), which starts
with an empty feature set and incrementally adds
features that provide the highest gains.
For comparison with Gerber and Chai?s model,
we also evaluated the local prediction model on the
evaluation data. Because this model predicted im-
plicit arguments independently, it continued to use
the heuristic post-processing algorithm to arrive at
the final labeling. However, the prediction threshold
t was eliminated because the system could safely as-
sume that a true filler for the iarg0 and iarg1 posi-
tions existed.
Table 1 presents the evaluation results. The first
thing to note is that these results are not comparable
to the results presented by Gerber and Chai (2010).
In general, performance is much higher because
predicate instances reliably take implicit arguments
in the iarg0 and iarg1 positions. The overall perfor-
mance increase versus the local model is relatively
small (approximately 1 percentage point); however,
the bid predicate in particular showed a substantial
increase (greater than 11 percentage points).
7 Discussion
7.1 Example improvement versus local model
The bid and investment predicates showed the
largest increase for the joint model versus the local
model. Below, we give an example of the investment
predicate for which the joint model correctly identi-
fied the iarg0 and the local model did not.
(13) [Big investors] can decide to ride out market
storms without jettisoning stock.
(14) Most often, [c they] do just that, because
stocks have proved to be the best-performing
long-term [Predicate investment], attracting
about $1 trillion from pension funds alone.
Both models identified the iarg1 as money from a
prior sentence (not shown). The local model in-
correctly predicted $1 trillion in Example 14 as the
iarg0 for the investment event. This mistake demon-
strates a fundamental limitation of the local model:
it cannot detect simple incompatibilities in the pre-
dicted argument structure. It does not know that
?money investing money? is a rare or impossible
event in the real world.
For the joint model?s prediction, consider the con-
stituent marked with c in Example 14. This con-
68
Local model Joint model
# Imp. args. P R F1 P R F1
price 40 65.0 65.0 65.0 67.5 67.5 67.5
sale 34 86.5 86.5 86.5 84.3 84.3 84.3
plan 30 60.0 60.0 60.0 56.7 56.7 56.7
bid 26 66.7 66.7 66.7 78.2 78.2 78.2
fund 18 83.3 83.3 83.3 83.3 83.3 83.3
loss 14 100.0 100.0 100.0 100.0 100.0 100.0
loan 12 63.6 58.3 60.9 50.0 50.0 50.0
investment 8 57.1 50.0 53.3 62.5 62.5 62.5
Overall 182 72.6 71.8 72.2 73.1 73.1 73.1
Table 1: Joint implicit argument evaluation results. The second column gives the total number of implicit arguments
in the ground-truth annotations. P , R, and F1 indicate precision, recall, and f-measure (? = 1) as defined by Ruppen-
hofer et al (2009).
stituent is resolved to Big investors in the preceding
sentence. Thus, the two relevant questions are as
follows:
Question: What did big investors invest?
Answer: money
Question: What entity invested money?
Answer: big investors
The first question produces the following ranked list
of answer synsets (the number in parentheses indi-
cates the number of answer tuples mapped to the
synset):
money (71)
amount (38)
million (38)
billion (22)
capital (21)
As shown, the answer string of money matches the
top-ranked answer synset. The second question pro-
duces the following ranked list of answer synsets:
company (642)
people (460)
government (275)
business (75)
investor (70)
In this case, the answer string Big investors matches
the fifth answer synset. The combined evidence
of these two question-answer pairs allows the joint
system to successfully identify Big investors as the
iarg0 of the investment predicate in Example 14.
7.2 Toward a generally applicable joint model
The joint model presented in this paper assumes that
all predicate instances take an iarg0 and iarg1. This
assumption clearly does not hold for real data (these
positions are often not expressed in the text), but re-
laxing it will require investigation of the following
issues:
1. Explicit arguments should also be considered
when determining whether a candidate c fills
an implicit argument position iargn. The mo-
tivation here is similar to that given elsewhere
in this paper: arguments (whether implicit or
explicit) are not independent. This is demon-
strated by Example 2 at the beginning of this
paper, where election is an explicit argument to
the predicate and affects the implicit argument
inference. The model developed in this paper
only considers jointly occurring implicit argu-
ments.
2. Other implicit argument positions (e.g.,
iarg2, iarg3, etc.) need to be accounted
for as well. This will present a challenge
when it comes to extracting the necessary
69
propositions from TextRunner. Currently,
TextRunner only handles tuples of the form
?arg0, p, arg1?. Other argument positions are
not directly analyzed by the system; however,
because TextRunner also returns the sentence
from which a tuple is extracted, these addi-
tional argument positions could be extracted in
the following way:
(a) For an instance of the sale predicate
with an arg0 of company, to find
likely arg2 fillers (the entity purchas-
ing the item), query TextRunner with
?company, sell, ??.
(b) Perform standard verbal SRL on the sen-
tences for the resulting tuples, identifying
any arg2 occurrences.
(c) Cluster and rank the arg2 fillers according
to the method described in this paper.
This approach combines Open Information Ex-
traction with traditional information extraction
(i.e., verbal SRL).
3. Computational complexity and probability
estimation is a problem for many joint mod-
els. The model presented in this paper quickly
becomes computationally intractable when the
number of candidates and implicit argument
positions becomes moderately large. This is
because Equation 9 considers all possible as-
signments of candidates to implicit argument
positions. With as few as thirty candidates and
five argument positions (not uncommon), one
must evaluate 30!/25! = 17, 100, 720 possible
assignments. Although this particular formula-
tion is not tractable, one based on dynamic pro-
gramming or heuristic search might give rea-
sonable results. Efficient estimation of the joint
probability via Gibbs sampling would also be a
possible approach (Resnik and Hardisty, 2010).
8 Conclusions
Many prior studies have investigated the recovery
of semantic arguments for nominal predicates. The
models in many of these studies have assumed that
the arguments are independent of each other. This
assumption simplifies the computational modeling
of semantic arguments, but it ignores the joint na-
ture of natural language. In order to take advantage
of the information provided by jointly occurring ar-
guments, the independent prediction models must be
enhanced.
This paper has presented a preliminary investiga-
tion into the joint modeling of implicit arguments
for nominal predicates. The model relies heavily
on information extracted by the TextRunner extrac-
tion system, which pulls propositional tuples from
millions of Internet webpages. These tuples encode
world knowledge that is necessary for resolving se-
mantic arguments in general and implicit arguments
in particular. This paper has proposed methods of
aggregating tuple knowledge to guide implicit argu-
ment resolution. The aggregated knowledge is ap-
plied via a re-ranking model that operates on top
of the local prediction model described in previous
work.
The performance gain across all predicate in-
stances is relatively small; however, larger gains are
observed for the bid and investment predicates. The
improvement in Example 14 shows that the joint
model is capable of correcting a bad local predic-
tion using information extracted by the TextRunner
system. This type of information is not used by the
local prediction model.
Although the results in this paper show that some
improvement is possible through the use of a joint
model of implicit arguments, a significant amount
of future work will be required to make the model
widely applicable.
References
ACE, 2008. The ACE 2008 Evaluation Plan. NIST, 1.2d
edition, August.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
70
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
Matthew Gerber and Joyce Chai. 2010. Beyond Nom-
Bank: A study of implicit arguments for nominal pred-
icates. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1583?1592, Uppsala, Sweden, July. Association for
Computational Linguistics.
P. Pudil, J. Novovicova, and J. Kittler. 1994. Floating
search methods in feature selection. Pattern Recogni-
tion Letters, 15:1119?1125.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Comput. Linguist., 34(2):257?
287.
Philip Resnik and Eric Hardisty. 2010. Gibbs sampling
for the uninitiated. Technical report, University of
Maryland, June.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2009. Semeval-
2010 task 10: Linking events and their participants in
discourse. In Proceedings of the Workshop on Seman-
tic Evaluations: Recent Achievements and Future Di-
rections (SEW-2009), pages 106?111, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Comput. Linguist., 34(2):161?191.
Zhibiao Wu and Martha Palmer. 1994. Verb seman-
tics and lexical selection. In Proceedings of the 32nd
Annual Meeting of the Association for Computational
Linguistics, pages 133?138, Las Cruces, New Mex-
ico, USA, June. Association for Computational Lin-
guistics.
71
