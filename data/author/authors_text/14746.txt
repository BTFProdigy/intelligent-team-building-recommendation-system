2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592?596,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Stance Classification using Dialogic Properties of Persuasion
Marilyn A. Walker, Pranav Anand, Robert Abbott and Ricky Grant
Baskin School of Engineering & Linguistics Department
University of California Santa Cruz
Santa Cruz, Ca. 95064, USA
maw,panand,abbott,rgrant@soe.ucsc.edu
Abstract
Public debate functions as a forum for both
expressing and forming opinions, an impor-
tant aspect of public life. We present results
for automatically classifying posts in online
debate as to the position, or STANCE that the
speaker takes on an issue, such as Pro or Con.
We show that representing the dialogic struc-
ture of the debates in terms of agreement rela-
tions between speakers, greatly improves per-
formance for stance classification, over mod-
els that operate on post content and parent-
post context alone.
1 Introduction
Public debate functions as a forum for both express-
ing and forming opinions. Three factors affect opin-
ion formation, e.g. the perlocutionary uptake of de-
bate arguments (Cialdini, 2000; Petty and Cacioppo,
1988; Petty et al, 1981). First, there is the ARGU-
MENT itself, i.e. the propositions discussed along
with the logical relations between them. Second is
the SOURCE of the argument (Chaiken, 1980), e.g.
the speaker?s expertise, or agreement relations be-
tween speakers. The third factor consists of proper-
ties of the AUDIENCE such as prior beliefs, social
identity, personality, and cognitive style (Davies,
1998). Perlocutionary uptake in debates primar-
ily occurs in the audience, who may be undecided,
while debaters typically express a particular position
or STANCE on an issue, e.g. Pro or Con, as in the
online debate dialogues in Figs. 1, 2, and 3.
Previous computational work on debate covers
three different debate settings: (1) congressional de-
Post Stance Utterance
P1 PRO I feel badly for your ignorance because although
there maybe a sliver of doubt that mankind may
have evolved from previous animals, there is no
doubt that the Earth and the cosmos have gone
through evolution and are continuing to do so
P2 CON As long as there are people who doubt evolu-
tion, both lay and acedamia, then evolution is in
doubt. And please don?t feel bad for me. I am
perfectly secure in my ?ignorance?.
P3 PRO By that measure, as long as organic chemistry,
physics and gravity are in doubt by both lay and
acedamia, then organic chemistry, physics and
gravity are in doubt. Gravity is a theory. Why
aren?t you giving it the same treatment you do
to evolution? Or is it because you are ignorant?
Angelic Falling anyone?
P4 CON I?m obviously ignorant. Look how many times
i?ve been given the title. ?Gravity is a theory.
Why aren?t you giving it the same treatment you
do to evolution?? Because it doesn?t carry the
same weight. ;P
Figure 1: All posts linked via rebuttal links. The topic
was ?Evolution?, with sides ?Yes, I Believe? vs. ?No, I
Dont Believe?.
bates (Thomas et al, 2006; Bansal et al, 2008;
Yessenalina et al, 2010; Balahur et al, 2009; Bur-
foot et al, 2011); (2) company-internal discussion
sites (Murakami and Raymond, 2010; Agrawal et
al., 2003); and (3) online social and political public
forums (Somasundaran and Wiebe, 2009; Somasun-
daran and Wiebe, 2010; Wang and Rose?, 2010; Bi-
ran and Rambow, 2011). Debates in online public
forums (e.g. Fig. 1) differ from debates in congress
and on company discussion sites in two ways.
First, the language is different. Online debaters
are highly involved, often using emotional and col-
orful language to make their points. These debates
are also personal, giving a strong sense of the indi-
592
vidual making the argument, and whether s/he fa-
vors emotive or factual modes of expression, e.g.
Let me answer.... NO! (P2 in Fig. 3). Other com-
mon features are sarcasm, e.g. I?m obviously igno-
rant. Look how many times i?ve been given the ti-
tle (P4 in Fig. 1), questioning another?s evidence or
assumptions: Yes there is always room for human
error, but is one accident that hasn?t happened yet
enough cause to get rid of a capital punishment? (P2
in Fig. 3), and insults: Or is it because you are ig-
norant? (P3 in Fig. 1). These properties may func-
tion to engage the audience and persuade them to
form a particular opinion, but they make computa-
tional analysis of such debates challenging, with the
best performance to date averaging 64% over several
topics (Somasundaran and Wiebe, 2010).
Post Stance Utterance
P1 Superman Batman is no match for superman. Not
only does he have SUPERnatural powers as
opposed to batman?s wit and gadgetry, but
his powers have increased in number over
the years. For example, when Superman?s
prowess was first documented in the comics
he did not have x-ray vision. It wasn?t until
his story was told on radio that he could see
through stuff. So no matter what new weapon
batman could obtain, Superman would add an-
other SUPERnatural weapon to foil the Caped
crusader.
P2 Batman Superman GAVE Batman a krytonite ring so
that Batman could take him down should he
need to. Superman did this because he knows
Batman is the only guy that could do it.
P3 Superman But, not being privy to private conversations
with S-man, you wouldn?t know that, being the
humble chap that he is, S-man allowed batman
the victory because he likes the bat and wanted
him to mantain some credibility. Honest.
P4 Batman Hmmm, this is confusing. Since we all know
that Supes doesn?t lie and yet at the time of
him being beaten by Batman he was under the
control of Poison Ivy and therefore could NOT
have LET Batman win on purpose. I have to
say that I am beginning to doubt you really are
friends with Supes at all.
Figure 2: All posts linked via rebuttal links. The topic
was ?Superman vs. Batman?
Second, the affordances of different online debate
sites provide differential support for dialogic rela-
tions between forum participants. For example, the
research of Somasundaran and Wiebe (2010), does
not explicitly model dialogue or author relations.
However debates in our corpus vary greatly by topic
on two dialogic factors: (1) the percent of posts that
are rebuttals to prior posts, and (2) the number of
Post Stance Utterance
P1 CON 69 people have been released from death row
since 1973 these people could have been killed
if there cases and evidence did not come up rong
also these people can have lost 20 years or more
to a false coviction. it is only a matter of time till
some one is killed yes u could say there doing
a good job now but it has been shown so many
times with humans that they will make the hu-
man error and cost an innocent person there life.
P2 PRO Yes there is always room for human error, but
is one accident that hasn?t happened yet enough
cause to get rid of a capital punishment? Let me
answer...NO! If you ban the death penalty crime
will skyrocket. It is an effective deterannce for
crime. The states that have strict death penalty
laws have less crime than states that don?t (Texas
vs. Michigan) Texas?s crime rate is lower than
Michigan and Texas has a higher population!!!!
Figure 3: Posts linked via rebuttal links. The topic was
?Capital Punishment?, and the argument was framed as
?Yes we should keep it? vs. ?No we should not?.
posts per author. The first 5 columns of Table 2
shows the variation in these dimensions by topic.
In this paper we show that information about di-
alogic relations between authors (SOURCE factors)
improves performance for STANCE classification,
when compared to models that only have access to
properties of the ARGUMENT. We model SOURCE
relations with a graph, and add this information to
classifiers operating on the text of a post. Sec. 2
describes the corpus and our approach. Our cor-
pus is publicly available, see (Walker et al, 2012).
We show in Sec. 3 that modeling source properties
improves performance when the debates are highly
dialogic. We leave a more detailed comparison to
previous work to Sec. 3 so that we can contrast pre-
vious work with our approach.
2 Experimental Method and Approach
Our corpus consists of two-sided debates from Con-
vinceme.net for 14 topics that range from play-
ful debates such as Superman vs. Batman (Fig. 2
to more heated political topics such as the Death
Penalty (Fig. 3. In total the corpus consists of 2902
two-sided debates (36,307 posts), totaling 3,080,874
words; the topic labelled debates which we use in
our experiments contain 575,818 words. On Con-
vinceme, a person starts a debate by posting a topic
or a question and providing sides such as for vs.
against. Debate participants can then post argu-
ments for one side or the other, essentially self-
593
labelling their post for stance. Convinceme pro-
vides three possible sources of dialogic structure,
SIDE, REBUTTAL LINKS and TEMPORAL CONTEXT.
Timestamps for posts are only available by day and
there are no agreement links. Here, we use the self-
labelled SIDE as the stance to be predicted.
Set/Factor Description
Basic Number of Characters in post, Average Word
Length, Unigrams, Bigrams
Sentiment LIWC counts and frequencies, Opinion De-
pendencies, LIWC Dependencies, negation
Argument Cue Words, Repeated Punctuation, Context,
POS-Generalized Dependencies, Quotes
Table 1: Feature Sets
We construct features from the posts, along with a
representation of the parent post as context, and use
those features in several base classifiers. As shown
in Table 1, we distinguish between basic features,
such as length of the post and the words and bi-
grams in the post, and features capturing sentiment
and subjectivity, including using the LIWC tool for
emotion labelling (Pennebaker et al, 2001) and de-
riving generalized dependency features using LIWC
categories, as well as some limited aspects of the
argument structure, such as cue words signalling
rhetorical relations between posts, POS generalized
dependencies, and a representation of the parent post
(context). Only rebuttal posts have a parent post, and
thus values for the context features.
50-10
Figure 4: Sample maxcut to ConvinceMe siding. Sym-
bols (circle, cross, square, triangles) indicate authors and
fill colors (white,black) indicate true side. Rebuttal links
are marked by black edges, same-author links by red;
weights are 50 and -10, respectively. Edges in the max-
cut are highlighted in yellow, and the nodes in each cut
set are bounded by the green dotted line.
We then construct a graph (V,E) representing the
dialogue structure, using the rebuttal links and au-
thor identifiers from the forums site. Each node V
of the graph is a post, and edges E indicate dialogic
relations of agreement and disagreement between
posts. We assume only that authors always agree
with themselves, and that rebuttal links indicate dis-
agreement. Agreement links based on the inference
that if A, B disagree with C they agree with each
other were not added to the graph.
Maxcut attempts to partition a graph into two
sides. Fig. 4 illustrates a sample result of applying
MaxCut. Edges connecting the partitions are said
to be cut, while those within partitions are not. The
goal is to maximize the sum of cut edge weights. By
making edge weights high we reward the algorithm
for cutting the edge, by making edge weights nega-
tive we penalize the algorithm for cutting the edge.
Rebuttal links were assigned a weight +100/(num-
ber of rebuttals). Same author links were assigned a
weight -60/(number of posts by author). If author A
rebutted author B at some point, then a weight of 50
was assigned to all edges connecting posts by author
A and posts by author B. If author B rebutted author
A as well, that 50 was increased to 100. We applied
the MaxCut partitioning algorithm to this graph, and
then we orient each of the components automati-
cally using a traditional supervised classifier. We
consider each component separately where compo-
nents are defined using the original (pre-MaxCut)
graph. For each pair of partition side p ? {P0, P1}
and classifier label l ? {L0, L1}, we compute a
score Sp,l by summing the margins of all nodes as-
signed to that partition and label. We then compute
and compare the score differences for each partition.
Dp = Sp,L1 ? Sp,L0 If DP0 < DP1 , then nodes in
partition P0 should be assigned label L0 and nodes
in P1 should be assigned label L1. Likewise, if
DP0 > DP1 , then nodes in partition P0 should be as-
signed label L1 and nodes in P1 should be assigned
label L0. If DP0 = DP1 , then we orient the compo-
nent with a coin flip.
3 Results and Discussion
Table 2 summarizes our results for the base classi-
fier (JRIP) compared to using MaxCut over the so-
cial network defined by author and rebuttal links.
We report results for experiments using all the fea-
594
Topic Characteristics MaxCut Algorithm JRIP Algorithm
Topic Posts Rebs P/A A> 1p MLE Acc F1 P R Acc F1 P R
Abortion 607 64% 2.73 42% 53% 82% 0.82 0.78 0.88 55% 0.55 0.52 0.59
Cats v. Dogs 162 40% 1.60 24% 53% 80% 0.78 0.80 0.76 61% 0.55 0.59 0.51
Climate Change 207 65% 2.92 41% 50% 64% 0.66 0.63 0.69 61% 0.62 0.60 0.63
Comm. v. Capitalism 214 62% 2.97 46% 55% 70% 0.67 0.66 0.68 53% 0.49 0.48 0.49
Death Penalty 331 60% 2.40 45% 56% 35% 0.31 0.29 0.34 55% 0.46 0.48 0.44
Evolution 818 66% 3.74 53% 58% 82% 0.78 0.78 0.79 56% 0.49 0.48 0.50
Existence Of God 852 76% 4.16 51% 56% 75% 0.73 0.70 0.76 52% 0.49 0.47 0.51
Firefox v. IE 233 38% 1.27 15% 79% 76% 0.47 0.44 0.49 72% 0.33 0.34 0.33
Gay Marriage 560 56% 2.01 28% 65% 84% 0.77 0.74 0.81 60% 0.43 0.43 0.44
Gun Control 135 59% 2.08 45% 63% 37% 0.24 0.21 0.27 53% 0.24 0.30 0.20
Healthcare 112 79% 3.11 53% 55% 73% 0.71 0.69 0.72 60% 0.49 0.56 0.44
Immigration 78 58% 1.95 33% 54% 33% 0.21 0.23 0.19 53% 0.39 0.48 0.33
Iphone v. Blackberry 25 44% 1.14 14% 67% 88% 0.80 0.86 0.75 71% 0.46 0.60 0.38
Israel v. Palestine 64 33% 3.37 53% 58% 85% 0.82 0.79 0.85 49% 0.48 0.42 0.56
Mac v. PC 126 37% 1.85 24% 52% 19% 0.18 0.17 0.18 46% 0.46 0.45 0.48
Marijuana legalization 229 45% 1.52 25% 71% 73% 0.56 0.52 0.60 63% 0.34 0.35 0.34
Star Wars vs. LOTR 102 44% 1.38 26% 53% 63% 0.62 0.60 0.65 63% 0.62 0.60 0.65
Superman v. Batman 146 30% 1.39 20% 54% 50% 0.40 0.44 0.37 56% 0.47 0.52 0.43
Table 2: Results. KEY: Number of posts on the topic (Posts). Percent of Posts linked by Rebuttal links (Rebs). Posts
per author (P/A). Authors with more than one post (A > 1P). Majority Class Baseline (MLE).
tures with ?2 feature selection; we use JRIP as the
base classifier because margins are used by the auto-
matic MaxCut graph orientation algorithm. Exper-
iments with different learners (NB, SVM) did not
yield significant differences from JRIP. The results
show that, in general, representing dialogic infor-
mation in terms of a network of relations between
posts yields very large improvements. In the few
topics where performance is worse (Death Penalty,
Gun Control, Mac vs. PC, Superman vs. Batman),
the MaxCut graph gets oriented to the stance sides
the wrong way, so that the cut actually groups the
posts correctly into sides, but then assigns them to
the wrong side. For Maxcut, as expected, there are
significant correlations between the % of Rebuttals
in a debate and Precision (R = .16 ) and Recall (R=
.22), as well as between Posts/Author and Precision
(R = .25) and Recall (R = .43). This clearly indi-
cates that the degree of dialogic behavior (the graph
topology) has a strong influence on results per topic.
These results would be even stronger if all MaxCut
graphs were oriented correctly.
(Somasundaran and Wiebe, 2010) present an un-
supervised approach using ICA to stance classifica-
tion, showing that identifying argumentation struc-
ture improves performance, with a best performance
averaging 64% accuracy over all topics, but as high
as 70% for some topics. Other research classifies
the speaker?s side in a corpus of congressional floor
debates (Thomas et al, 2006; Bansal et al, 2008;
Balahur et al, 2009; Burfoot et al, 2011). Thomas
et al(2006) achieved accuracies of 71.3% by using
speaker agreement information in the graph-based
MinCut/Maxflow algorithm, as compared to accura-
cies around 70% via an an SVM classifier operating
on content alone. The best performance to date on
this corpus achieves accuracies around 82% for dif-
ferent graph-based approaches as compared to 76%
accuracy for content only classification (Burfoot et
al., 2011). Other work applies MaxCut to the reply
structure of company discussion forums, showing
that rules for identifying agreement (Murakami and
Raymond, 2010), defined on the textual content of
the post yield performance improvements over using
reply structures alone (Malouf and Mullen, 2008;
Agrawal et al, 2003)
Our results are not strictly comparable since we
use a different corpus with different properties, but
to our knowledge this is the first application of Max-
Cut to stance classification that shows large perfor-
mance improvements from modeling dialogic rela-
tions. In future work, we plan to explore whether
deeper linguistic features can yield large improve-
ments in both the base classifier and in MaxCut re-
sults, and to explore better ways of automatically
orienting the MaxCut graph to stance side. We also
hope to develop much better context features and to
make even more use of dialogue structure.
595
References
R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. 2003.
Mining newsgroups using networks arising from so-
cial behavior. In Proceedings of the 12th international
conference on World Wide Web, pages 529?535. ACM.
A. Balahur, Z. Kozareva, and A. Montoyo. 2009. Deter-
mining the polarity and source of opinions expressed
in political debates. Computational Linguistics and
Intelligent Text Processing, pages 468?480.
M. Bansal, C. Cardie, and L. Lee. 2008. The power
of negative thinking: Exploiting label disagreement in
the min-cut classification framework. Proceedings of
COLING: Companion volume: Posters, pages 13?16.
O. Biran and O. Rambow. 2011. Identifying justifica-
tions in written dialogs. In 2011 Fifth IEEE Inter-
national Conference on Semantic Computing (ICSC),
pages 162?168. IEEE.
C. Burfoot, S. Bird, and T. Baldwin. 2011. Collective
classification of congressional floor-debate transcripts.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 1506?1515. As-
sociation for Computational Linguistics.
S. Chaiken. 1980. Heuristic versus systematic informa-
tion processing and the use of source versus message
cues in persuasion. Journal of personality and social
psychology, 39(5):752.
Robert B. Cialdini. 2000. Influence: Science and Prac-
tice (4th Edition). Allyn & Bacon.
M.F. Davies. 1998. Dogmatism and belief formation:
Output interference in the processing of supporting
and contradictory cognitions. Journal of personality
and social psychology, 75(2):456.
R. Malouf and T. Mullen. 2008. Taking sides: User clas-
sification for informal online political discourse. In-
ternet Research, 18(2):177?190.
A. Murakami and R. Raymond. 2010. Support or
Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Association for Computational Linguistics.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.
LIWC: Linguistic Inquiry and Word Count.
Richard E. Petty and John T. Cacioppo. 1988. The ef-
fects of involvement on responses to argument quan-
tity and quality: Central and peripheral routes to per-
suasion. Journal of Personality and Social Psychol-
ogy, 46(1):69?81.
R.E. Petty, J.T. Cacioppo, and R. Goldman. 1981. Per-
sonal involvement as a determinant of argument-based
persuasion. Journal of Personality and Social Psy-
chology, 41(5):847.
S. Somasundaran and J. Wiebe. 2009. Recognizing
stances in online debates. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1-Volume
1, pages 226?234. Association for Computational Lin-
guistics.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In Proceedings
of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in
Text, pages 116?124. Association for Computational
Linguistics.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from Con-
gressional floor-debate transcripts. In Proceedings of
the 2006 conference on empirical methods in natural
language processing, pages 327?335. Association for
Computational Linguistics.
M. Walker, P. Anand, J. Fox Tree, R. Abbott, and J. King.
2012. A corpus for research on deliberation and
debate. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
LREC 2012, Istanbul, Turkey, May 23-25, 2012.
Y.C. Wang and C.P. Rose?. 2010. Making conversational
structure explicit: identification of initiation-response
pairs within online discussions. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 673?676. Association for
Computational Linguistics.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level sentiment
classification. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1046?1056. Association for Computational
Linguistics.
596
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 2?11,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
How can you say such things?!?:
Recognizing Disagreement in Informal Political Argument
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E. Fox Tree,
Robeson Bowmani, and Joseph King
University of California Santa Cruz
abbott|maw@soe.ucsc.edu,panand|foxtree@ucsc.edu
Abstract
The recent proliferation of political and so-
cial forums has given rise to a wealth of freely
accessible naturalistic arguments. People can
?talk? to anyone they want, at any time, in
any location, about any topic. Here we use
a Mechanical Turk annotated corpus of forum
discussions as a gold standard for the recog-
nition of disagreement in online ideological
forums. We analyze the utility of meta-post
features, contextual features, dependency fea-
tures and word-based features for signaling
the disagreement relation. We show that us-
ing contextual and dialogic features we can
achieve accuracies up to 68% as compared to
a unigram baseline of 63%.
1 Introduction
The recent proliferation of political and social fo-
rums has given rise to a wealth of freely accessible
naturalistic arguments. People can ?talk? to anyone
they want, at any time, in any location, about any
topic. Their conversations range from current polit-
ical topics such as national health care to religious
questions such as the meaning of biblical passages.
See Figure 1. We aim to automatically derive rep-
resentations of the discourse structure of such argu-
ments and to gain a deeper theoretical and empirical
understanding of the linguistic reflexes of perlocu-
tionary acts such as persuasion (Austin, 1965).
The study of the structure of argumentative com-
munication has a long lineage in psychology (Cial-
dini, 2000) and rhetoric (Hunter, 1987), but the his-
torical lack of a large corpus of naturalistic exam-
Topic Q-R: Post
Evolution Q: How can you say such things? The Bible says that
God CREATED over and OVER and OVER again! And
you reject that and say that everything came about by
evolution? If you reject the literal account of the Cre-
ation in Genesis, you are saying that God is a liar! If you
cannot trust God?s Word from the first verse, how can
you know that the rest of it can be trusted?
R: It?s not a literal account unless you interpret it that
way.
Gay
mar-
riage
Q: Gavin Newsom- I expected more from him when I
supported him in the 2003 election. He showed himself
as a family-man/Catholic, but he ended up being the ex-
act oppisate, supporting abortion, and giving homosexu-
als marriage licenses. I love San Francisco, but I hate the
people. Sometimes, the people make me want to move
to Sacramento or DC to fix things up.
R: And what is wrong with giving homosexuals the right
to settle down with the person they love? What is it to
you if a few limp-wrists get married in San Francisco?
Homosexuals are people, too, who take out their garbage,
pay their taxes, go to work, take care of their dogs, and
what they do in their bedroom is none of your business.
Abortion Q: Equality is not defined by you or me. It is defined by
the Creator who created men.
R: Actually I think it is defined by the creator who cre-
ated all women. But in reality your opinion is gibberish.
Equality is, like every other word, defined by the people
who use the language. Currently it means ?the same?.
People aren?t equal because they are not all the same.
Any attempt to argue otherwise is a display of gross stu-
pidity.
Figure 1: Sample Quote/Response Pairs
ples has limited empirical work to a handful of gen-
res (e.g., editorials or simulated negotiations). Ar-
gumentation is above all tactical. Thus being able
to effectively model it would afford us a glimpse
of pragmatics beyond the conversational turn. More
practically, an increasing portion of information and
opinion exchange online occurs in natural dialogue,
in forums, in webpage comments, and in the back
2
and forth of short messages (e.g., Facebook status
updates, tweets, etc.) Effective models of argumen-
tative discourse thus have clear applications in auto-
matic summarization, information retrieval, or pre-
dicting real-world events such as how well a new
product is being received or the outcome of a popu-
lar vote on a topic (Bollen et al, 2011).
In this paper, we focus on an important initial task
for the recognition of argumentative structure: auto-
matic identification of agreement and disagreement.
We introduce the ARGUE corpus, an annotated col-
lection of 109,553 forum posts (11,216 discussion
threads) from the debate website 4forums.com. On
4forums, a person starts a discussion by posting a
topic or a question in a particular category, such as
society, politics, or religion. Some example topics
can be seen in Table 1. Forum participants can then
post their opinions, choosing whether to respond di-
rectly to a previous post or to the top level topic (start
a new thread). These discussions are essentially di-
alogic; however the affordances of the forum such
as asynchrony, and the ability to start a new thread
rather than continue an existing one, leads to dia-
logic structures that are different than other multi-
party informal conversations (Fox Tree, 2010). An
additional source of dialogic structure in these dis-
cussions, above and beyond the thread structure, is
the use of the quote mechanism, in which partici-
pants often break a previous post down into the com-
ponents of its argument and respond to each compo-
nent in turn. Many posts include quotations of previ-
ous posts. Because we hypothesize that these posts
are more targeted at a particular proposition that the
poster wants to comment on, than posts and replies
in general, we focus here on understanding the rela-
tionship between a quoted text and a response, and
the linguistic reflexes of those relationships. Exam-
ples of quote/response pairs for several of our topics
are provided in Figure 1.
The most similar work to our own is that of
Wang & Rose (2010) who analyzed Usenet fo-
rum quote/response structures. This work did not
distinguish agreement vs. disagreement across
quote/response pairs. Rather they show that they can
use a variant of LSA to improve accuracy for identi-
fying a parent post, given a response post, with 70%
accuracy. Other similar work uses Congressional
debate transcripts or blogs or other social media to
develop methods for distinguishing agreement from
disagreement or to distinguish rebuttals from out-of-
context posts (Thomas et al, 2006; Bansal et al,
2008; Awadallah et al, 2010; Walker et al, ; Bur-
foot, 2008; Mishne and Glance, 2006; Popescu and
Pennacchiotti, 2010). These methods are directly
applicable, but the genre of the language is so dif-
ferent from our informal forums that the results are
not directly comparable. Work by Somasundaran &
Wiebe (2009, 2010) has examined debate websites
and focused on automatically determining the stance
of a debate participant with respect to a particular is-
sue. This work has treated each post as a text to be
classified in terms of stance, for a particular topic,
and shown that discourse relations such as conces-
sions and the identification of argumentation triggers
improves performance . Their work, along with oth-
ers, also indicates that for such tasks it is difficult to
beat a unigram baseline (Pang and Lee, 2008). Other
work has focused on the social network structure
of online forums (Murakami and Raymond, 2010;
Agrawal et al, 2003). However, Agarwal?s work as-
sumed that adjacent posts always disagree, and did
not use any of the information in the text. Murakami
& Raymond (2010) show that simple rules defined
on the textual content of the post can improve over
Agarwal?s results.
Section 2 discusses our corpus in more detail, de-
scribes how we collected annotations using Mechan-
ical Turk, and presents results of a corpus analysis
of the use of particular discourse cues. Section 3 de-
scribes how we set up classification experiments for
distinguishing agreement from disagreement, and
Section 4 presents our results for agreement classifi-
cation. We also characterize the linguistic reflexes of
this relation. We analyze the utility of meta-post fea-
tures, contextual features, dependency features and
word-based features for signaling the disagreement
relation. We show that using contextual and dia-
logic features we can achieve accuracies up to 68%
as compared to a unigram baseline of 63%.
2 Data and Corpus Analysis
Table 1 provides an overview of some of the charac-
teristics of our corpus by topic. Figure 2 shows the
wording of the survey questions that we posted for
each quote/response as Mechanical Turk hits.
3
Topic Discs Posts NumA P/A A>1P PL Agree Sarcasm Emote Attack Nasty
evolution 872 10292 580 17.74 76% 576 10% 6% 16% 13% 9%
gun control 825 7968 411 19.39 66% 521 11% 8% 21% 16% 12%
abortion 564 7354 574 12.81 69 % 454 9% 6% 31% 16% 12%
gay marriage 305 3586 342 10.49 69% 522 13% 9% 23% 12% 8%
existence of God 105 1581 258 6.13 66% 569 11 % 7% 26% 14% 10%
healthcare 81 702 112 6.27 64% 522 14% 10% 34% 17% 17%
communism vs. capitalism 38 585 110 5.32 59% 393 23% 8% 15% 8% 0%
death penalty 25 500 138 3.62 62% 466 25% 5% 5% 5% 5%
climate change 40 361 116 3.11 55% 375 20% 9% 17% 26% 17%
marijuana legalization 13 160 72 2.22 38% 473 5% 2% 20% 5% 5%
Table 1: Characteristics of Different Topics. KEY: Number of discussions and posts on the topic (Discs, Posts).
Number of authors (NumA). Posts per author (P/A). Authors with more than one post (A > 1P). Median post Length
in Characters (PL). The remainder of the columns are the annotations shown in Figure 2. Percentage of posts that
agree (Agree%), use sarcasm (Sarcasm%), are emotional (Emote), attack the previous poster (Attack), and are
nasty (Nasty). The scalar values are threshholded at -1,1.
Our corpus is derived from a debate oriented in-
ternet forum called 4forums.com. It is a typical in-
ternet forum built on the vBulletin software. People
initiate discussions (threads) and respond to others?
posts. Each thread has a tree-like dialogue structure.
Each post has author information and a timestamp
with minute resolution. Many posts include quota-
tions of previous posts. For this work we chose to
focus on quotations because they establish a clear re-
lationship between the quoted text and the response.
Our corpus consists of 11,216 discussions and
109,553 posts by 2764 authors. We hand annotated
discussions for topic from a set of previously identi-
fied contentious political and social issues. The web-
site is tailored to a US audience and our topics are
somewhat US centric. Table 1 describes features of
our topics in order of decreasing discussion count.
When restricted to these topics, the corpus consists
of 2868 discussions, 33,089 posts, and 1302 authors.
Many posts include quotations. Overall 60,382
posts contain one or more quotation. Within our
topics of interest, nearly 20,000 posts contain quota-
tions. We defined a quote-response pair (Q-R pair)
where the response was the portion of the respond-
ing post directly following a quotation but preceding
any additional quotations.
We selected 10,003 Q-R pairs from the topics
of interest for a Mechanical Turk annotation task.
These were biased by cue word to ensure adequate
data for discourse marker analysis (See Section 2.1.
For this task we showed annotators seven Q-R pairs
and asked them to judge Agreement/Disagreement
and a set of other measures as shown in Figure 2.
Most of our measures were scalar; we chose to do
this because previous work on estimating the rela-
tionship between MTurk annotations and expert an-
notations suggest that taking the means of scalar
annotations could be a good way to reduce noise
in MTurk annotations (Snow et al, 2008). For all
of the measures annotated, the Turkers were not
given additional definitions of their meaning. For
example, we let Turkers to use their native intu-
itions about what it means for a post to be sarcas-
tic, since previous work suggests that non-specialists
tend to collapse all forms of verbal irony under the
term sarcastic (Bryant and Fox Tree, 2002). We
did not ask Turkers to distinguish between sarcasm
and other forms of verbal irony such as hyperbole,
understatement, rhetorical questions and jocularity
(Gibbs, 2000).
Agreement was a scalar judgment on an 11 point
scale [-5,5] implemented with a slider. The anno-
tators were also able to signal uncertainty with an
CAN?T TELL option. Each of the pairs was anno-
tated by 5-7 annotators. We showed the first 155
characters of each quote and each response. We also
provided a SHOW MORE button which expanded the
post to its full length. After annotation, we removed
a number of Q-R pairs in cases where a clear link
between the quote and a previous post could not be
established, e.g. the source quoted was not another
post, but the NY Times. This left us with 8,242 Q-R
pairs for our final analysis. Resampling to a natural
distribution left us with 2,847 pairs which we used
to build our machine learning test set. We used the
remaining annotated and unannotated pairs for de-
4
velopment.
Type ? Survey Question
S 0.62 Agree/Disagree: Does the respondent agree or dis-
agree with the prior post?
S 0.32 Fact/Emotion: Is the respondent attempting to
make a fact based argument or appealing to feel-
ings and emotions?
S 0.42 Attack/Insult: Is the respondent being support-
ive/respectful or are they attacking/insulting in
their writing?
B 0.22 Sarcasm: Is the respondent using sarcasm?
S 0.46 Nice/Nasty: Is the respondent attempting to be nice
or is their attitude fairly nasty?
Figure 2: Mechanical Turk Annotations (Binary = B and
Scalar = S) and level of agreement as Krippendorff?s ?.
Figure 3 provides examples from the end
points and means of the annotations for three
of the questions, Respect/Insult, Sarcasm, and
Fact/Emotion. Nice/Nasty and Respect/Insult are
strongly correlated by worker annotations r(54003)
= 0.84, p < 2.2e-16 and both weakly corre-
lated with Agree/Disagree ratings (r(54003) = 0.32
and r(54003)=0.36, respectively; p < 2.2e-16)
and Fact/Emotion ratings (r(54003) = 0.32 and
r(54003)=0.31, respectively; p < 2.2e-16), while
Agree/Disagree and Fact/Emotion ratings show the
smallest correlation, r(54003)=0.11, p < 2.2e-16.
For the linguistic marker correlations discussed be-
low we averaged scores across annotators, a process
which sharpened correlations (e.g., Respect/Insult
means correlate with Agree/Disagree means more
strongly (r(5393) = 0.51) as well as Nice/Nasty
means (r(5393) = 0.91; Agree/Disagree is far less
correlated with Fact/Emotion (r(5393) = 0.07). In-
terannotator agreement was computed using Krip-
pendorff?s ? (due to the variability in number of an-
notators that completed each hit), assuming an ordi-
nal scale for all measures except sarcasm; see Fig-
ure 2. The low agreement for Sarcasm accords with
native intuition ? it is the class with the least de-
pendence on lexicalization and the most subject to
inter-speaker stylistic variation. The relatively low
results for Fact/Emotion is perhaps due to the emo-
tional charge many ideological arguments engender;
informal examination of posts that showed the most
disagreement in this category often showed a cut-
ting comment or a snide remark at the end of a post,
which was was ignored by some annotators and ev-
idence for others (one Emotional post in Figure 3 is
clearly an insult, but was uniformly labeled as -5 by
all annotators).
2.1 Discourse Markers
Both psychological research on discourse processes
(Fox Tree and Schrock, 1999; Fox Tree and Schrock,
2002; Groen et al, 2010) and computational work
on agreement (Galley et al, 2004) indicate that dis-
course markers are strongly associated with partic-
ular pragmatic functions. Because of their salient
position, we test the role of turn-initial markers in
predicting upcoming content (Fox Tree and Schrock,
2002; Groen et al, 2010). Based on manual inspec-
tion of a subset of the corpus, we constructed a list of
20 discourse markers; 17 of these occurred at least
50 times in a quote response (upper bound of 700
samples): actually, and, because, but, I believe, I
know, I see, I think, just, no, oh, really, so, well,
yes, you know, you mean. All of their occurrences
became part of the 10,003 Q-R pairs annotated.
The top discourse markers highlighting disagree-
ment were really (67% read a response beginning
with this marker as prefacing a disagreement with
a prior post), no (66%), actually (60%), but (58%),
so (58%), and you mean (57%). At this point, the
next most disagreeable category was the unmarked
category, with about 50% of respondents interpret-
ing an unmarked post as disagreeing. On the other
hand, the most agreeable marker was yes (73% read
a response beginning with this marker as prefacing
an agreement) followed by I know (64%), I believe
(62%), I think (61%), and just (57%). The other
markers were close to the unmarked category: and
(50%), because (51%), oh (51%), I see (52%), you
know (54%), and well (55%).
The overall agreement on sarcasm was low, as in
other computational work on recognizing sarcasm
(Davidov et al, 2010). At most, only 31% of re-
spondents agreed that the material after a discourse
marker was sarcastic, with the most sarcastic mark-
ers being you mean (31%), oh (29%), really (24%),
so (22%), and I see (21%). Only 15% of respon-
dents rated the unmarked category as sarcastic (e.g.,
fewer than 1 out of 6 respondents). The cues I think
(10%), I believe (9%), and actually (10%) were the
least sarcastic markers.
Taken together, these ratings suggest that the cues
really, you mean, and so can be used to indicate both
5
Class Very High Degree Neutral Very Low Degree
Insult
or
Attack
Well, you have proven yoruself to be a
man with no brain, that is for sure. The
definition that was given was the one that
scientists use, not the layperson.
The empire you defend is tyrannical.
They are responsible for the death of mil-
lions.
Very well put.
Is that what you said right be-
fore they started banning assault
weapons?...Obviously, you?re gullible.
Since you?re such a brainiac and all, why
don?t you visit the UN website and see
what your beloved UN is up to?
Bad comparisons. A fair comparison
would be comparing the total number of
defensive gun uses to the total number
of gun crimes (not just limiting it to gun
homicides).
In some cases yes, in others no. If the
mutation gives a huge advantage, then
there will be a decline in the size of the
gene pool for a while (eg when the Aus-
tralian rabbit population...
Sarcasm My pursuit of happiness is denied by
trees existing. Let?s burn them down and
destroy the environment. It?s much bet-
ter than me being unhappy.
An interesting analysis of that article you
keep quoting from the World Net Daily
[url]
I would suggest you look at the faero is-
land mouse then. That is a new species,
and it is not man doing it, but rather na-
ture itself.
Like the crazy idea the Earth goes around
the Sun.
Indeed there is no diffrence it is still a
dead baby but throwing a baby in a trash
can and leaving it for dead is far more
cruel than abortion.
Too late, drug usage has already created
those epidemics. Legalizing drugs may
increase some of them temporarily, but
they already exist.
Emotion-
based
Argu-
ment
Really! You can prove that most pro-
lifers don?t care about women?...it is id-
iotic thinking like this that makes me re-
spect you less and less.
Fine by me. First, I don?t consider hav-
ing a marriage recognized by govern-
ment to be a ?right?. Second, I?ve said
many times I don?t think government
should be in the marriage business at all.
Sure. Here is an explanation. The 14C
Method. That is from the Radiocarbon
WEB info site by the Waikato Radio-
carbon Dating Lab of the University of
Waikato (New Zeland).
I love Jesus John the Beloved is my most
favorite writer throughout time If you
think I have a problem with a follower
of Jesus your wrong. I have a problem
with the Christians
I agree that the will to survive is an amaz-
ing phenomenon when put to the test.
But I do not agree with your statement
of life at *any* cost. There will always
be a time when the humane/loving thing
to do is to let an infant/child/adult go.
Heller is about determining the answer to
a long standing question on the nature of
the Second Amendment, and how much
gun control is legally allowed. Roe v.
Wade is about finding legal precedent for
the murder of unborn children. I see ab-
solutely no comparison between the two.
Figure 3: Sample Responses for the Insult, Sarcasm, and Fact/Feeling spectrums
disagreement and sarcasm. However, but, no, and
actually can be used for disagreement, but not sar-
casm. And I know (14% sarcastic, similar to None),
I believe, and I think can be used for non-sarcastic
agreement.
From informal analyses, we hypothesized that re-
ally and oh might indicate sarcasm. While we found
evidence supporting this for really, it was not the
case for oh. Instead, oh was used to indicate emo-
tion; it was the discourse marker with the highest
ratings of feeling over fact.
Despite the fact that it would seem that disagree-
ment would be positively correlated with sarcasm,
disagreement and sarcasm were not related. There
were two tests possible. One tested the percentage of
people who identified an item as disagreeing against
the percentage of people who identified it as sar-
casm, r(16) = -.27, p = .27 (tested on 17 discourse
markers plus the None category). The other tested
the degree of disagreement (from -5 to +5) against
the percentage of people who identified the post as
sarcastic, r(16) = -.33, p = .18.
However, we did observe relationships between
sarcasm and other variables. Two results support the
argument that sarcasm is emotional and personal.
The more sarcastic, the nastier (rather than nicer),
r(16) = .87, p < .001. In addition, the more sarcas-
tic, the more emotional (over factual) respondents
were judged to be, r(16) = .62, p = .006 Taken to-
gether, these analyses suggest that sarcasm is emo-
tional and personal, but not necessarily a sign of dis-
agreement.
3 Machine Learning Experimental Setup
For our experiments we used the Weka machine
learning toolkit. All results are from 10 fold cross-
validation on a balanced test set. Unless otherwise
mentioned, we used thresholds of 1 and -1 on the
mean agreement judgment to determine agreement
6
and disagreement respectively. We omitted those Q-
R pairs which were judged neutral (mean annotator
judgment in the (-1,1) range).
As described above, from the original 10,003 Q-
R pairs we applied certain constraints (notably re-
quirement that we be able to identify the originating
post) which left us with 8,242. We then resampled
to obtain a natural distribution leaving us with 2,847
pairs. Applying the (-1,1) threshold and balancing
the result yielded a test set of 682 Q-R pairs.
3.1 Classifiers
Our experiments used two simple classifiers: Naive-
Bayes and JRip. NaiveBayes makes a strict indepen-
dence assumption and can be swamped by the sheer
number of features we used, but it is a solid baseline
and does a decent job of suggesting which features
are more powerful. JRip is a rule based classifier
which produces a compact model suitable for hu-
man consumption and quick application. JRip is not
without its own limitations but, for our task, it shows
better results than NaiveBayes. The model it builds
uses only a handful of features.
3.2 Feature Extraction
Our aim was to develop features for the automatic
identification of agreement and disagreement that
would do well on the task and provide useful base-
lines for comparisons with previous and future work.
Features are grouped into sets as shown in Table 2
and discussed in more detail below.
Set Description/Examples
MetaPost Non-lexical features. E.g. posterid, time be-
tween posts, etc.
Unigrams,
Bigrams
Word and Word Pair frequencies
Cue Words Initial unigram, bigram, and trigram
Punctuation Collapsed into one of the following: ??, !!, ?!
LIWC LIWC measures and frequencies
Dependencies Dependencies derived from the Stanford
Parser.
Generalized De-
pendencies
Dependency features generalized with re-
spect to POS of the head word and opinion
polarity of both words.
Table 2: Feature Sets, Descriptions, and Examples
Unigrams, Bigrams, Trigrams. Results of pre-
vious work suggest that a unigram baseline can be
difficult to beat for certain types of debates (Walker
et al, ; Somasundaran and Wiebe, 2010). Thus we
derived both unigrams and bigrams as features. We
captured the final token as a feature by padding with
-nil- tokens when building the bigrams. See below
for comments on initial uni/bi/tri-grams.
MetaPost Info. Previous work suggested that
non-lexical features like poster ids and the time be-
tween posts might contain indicators of disagree-
ment. People on these forums get to know one an-
other and often enjoy repeatedly arguing with the
same person. In addition, we hypothesized that the
?heat? of a particular conversation could be corre-
lated with rapid-fire exchanges, as indicated by short
time periods between posts.
Thus these features involve structure outside of
the quote/response text. This includes author infor-
mation, time between posts, the log10 of the time
between posts, the number of other quotes in the
response, whether the quote responds to a post by
the response?s author, the percent of the quoted post
which is actually quoted, whether the quoted post is
by the same author as the response (there were only
an handful of these), whether the response mentions
the quote author by name, and whether the response
is longer than the quote.
The forum software effectively does this annota-
tion for us so there is no reason not to consider it as
a clue in our quest to understand and interpret online
dialogue.
Discourse Markers. Previous work on dialogue
analysis has repeatedly noted the discourse func-
tions of particular discourse markers, and our corpus
analysis above also suggests their use in this par-
ticular dataset (Hirschberg and Litman, 1993; Fox
Tree, 2010; Schiffrin, 1987; Di Eugenio et al, 1997;
Moser and Moore, 1995). However, because dis-
course markers can be stacked up Oh, so really we
decided to represent this feature as post initial uni-
grams, bigrams and trigrams.
Repeated Punctuation. Informal analyses of our
data suggested that repeated sequential use of partic-
ular types of punctuation such as !! and ?? did not
mean the same thing as simple counts or frequen-
cies of punctuation across a whole post. Thus we
developed distinct features for a subset of these rep-
etitions.
LIWC. We also derived features using the Lin-
guistics Inquiry Word Count tool (LIWC-2001)
(Pennebaker et al, 2001). LIWC classifies words
7
into 69 categories and counts how many words get
classified into each category. Some LIWC features
that we expect to be important are words per sen-
tence (WPS), pronominal forms, and positive and
negative emotion words.
Dependency and Generalized Dependency. We
used the Stanford parser to extract dependency fea-
tures for each quote and response (De Marneffe et
al., 2006; Klein and Manning, 2003). The depen-
dency parse for a given sentence is a set of triples,
composed of a grammatical relation and the pair
of words for which the grammatical relation holds
(reli, wj , wk), where reli is the dependency relation
among words wj and wk. The word wj is the HEAD
of the dependency relation.
Following (Joshi and Penstein-Rose?, 2009) we ex-
tracted generalized dependency features by leaving
one dependency element lexicalized and generaliz-
ing the other to part of speech. Joshi & Rose?s re-
sults suggested that this approach would work better
than either fully lexicalized or fully generalized de-
pendency features.
Opinion Dependencies. Somasundaran & Wiebe
(2009) introduce the concept of features that iden-
tify the TARGET of opinion words. Inspired by this
approach, we used the MPQA dictionary of opinion
words to select the subset of dependency and gen-
eralized dependency features in which those opin-
ion words appear. For these features we replace the
opinion words with their positive or negative polar-
ity equivalents.
Cosine Similarity. This feature is based on previ-
ous work on threading. We derive cosine-similarity
measure using tf-idf vectors where the document
frequency was derived from the entire topic re-
stricted corpus.
Annotations. We also add features represent-
ing information that we do not currently derive au-
tomatically, but which might be automatically de-
rived in future work based on annotations in the cor-
pus. These include the topic and Mechanical Turk
annotations for Fact/Emotion, Respect/Insult, Sar-
casm, and Nasty/Nice, which could reasonably be
expected to be recognized independently of Agree-
ment/Disagreement.
Feature
type
Selected Features
Meta number-of-other-quotes, percent-quoted, author-quote-
USERNAME
Initial
n-gram
yes, so, I agree, well said, really?, I don?t know
Bigram that you, ? -nil-, you have, evolution is
Depend-
ency
dep-nsubj(agree, i), dep-nsubj(think, you), dep-prep-
with(agree, you)
Opinion
Depen-
dency
dep-opinion-nsubj(negative, you), dep-opinion-
dep(proven, negative), dep-opinion-aux(positive,
to)
Anno-
tations
topic-gay marriage, mean-response-nicenasty, mean-
unsure-sarcasm
Table 3: Some of the more useful features for each cate-
gory, using ?2 for feature selection.
Figure 4: Sample model learned using JRip. The num-
bers represent (total instances covered by a rule / number
incorrectly labeled). This particular model was built on
development data.
4 Results
Table 3 shows features which were selected for each
of our feature categories using a ?2 test for fea-
ture selection. These results vindicate our interest
in discourse markers as cues to argument structure,
as well as the importance of the generalized depen-
dency features and opinion target pairs (Wang and
Rose?, 2010; Somasundaran and Wiebe, 2009). Fig-
ure 4 shows a sample model learned using JRip.
We limit our pair-wise comparisons between clas-
sifiers and feature sets to those corresponding to par-
8
Feats NB JRip?2
Uni,UniCue 0.578 0.626
BOW 0.598 0.654
Meta 0.579 0.588
Response Local 0.600 0.666
Quote Local 0.531 0.588
Both Local 0.601 0.682
Meta+Local 0.603 0.654
All 0.603 0.632
Just Annotations 0.765 0.814
All+Annotations 0.603 0.795
Table 4: Accuracies on a balanced test set (random base-
line: 0.5). NB = NaiveBayes. JRip?2 = Jripper with ?2
feature selection on the training set during cross valida-
tion. BOW = Unigrams, CueWords, Bigrams, Trigrams,
LIWC, Repeated Punctuation. Response/Quote/Both
Local uses only those features which exist in the text of
the response or quote respectively. It consists of LIWC,
dependencies, generalized dependencies, the various n-
grams, and length measures.
ticular hypotheses. We conducted five tests with
Bonferroni correction to .01 for a .05 level of sig-
nificance.
While we hypothesized that more sophisticated
linguistic features would improve over unigram fea-
tures alone, a paired t-test using the results in Table 4
indicate that there is no statistical difference be-
tween the performance of JRip using only response
local features (JRip,ResponseLocal), as compared to
the Unigram,UniCue features (t(9) = 2.18, p = .06).
However, a paired t-test using the results in
Table 4 indicate that there is a statistical dif-
ference between the performance of JRip using
local features from both the quote and the re-
sponse, (JRip,BothLocal) as compared to the Uni-
gram,UniCue features (t(9) = 3.94, p =.003). This
shows that the contextual features do matter, even
though (JRip,BothLocal) does not provide signifi-
cant improvements over (JRip,Response Local) (t(9)
= .92, p = .38).
In general, examination of the table suggests
that the JRip classifier performs better than Naive
Bayes. A paired t-test indicates that there is a sta-
tistical difference between the performance of JRip
using local features from both the quote and the
response, (JRip,BothLocal) (JRip,BothLocal) and
Naive Bayes using local features from both the quote
and the response, (NB,BothLocal) (t(9) = 3.43, p =
.007).
In addition, with an eye toward the future, we ex-
amined whether automatic recognition of sarcasm,
attack/insult, fact/feeling nice/nasty could possibly
improve results for recognizing disagreement. Us-
ing the human annotations as a proxy for automatic
results, we get classification accuracies of over 81%
(JRip,JustAnnotations). This suggests it might be
possible to improve results over our best current re-
sults (JRip,BothLocal) (t(9) = 6.09, p < .001).
Another interesting fact, is that despite its use in
previous work for threading, the cosine similarity
between the quote and response did not improve ac-
curacy for the classifiers we tested, over and above
the use of text-based contextual features. Further
investigation is required to draw conclusions about
this or similar metrics (LSA, PMI, etc.).
5 Discussion and Conclusion
In this paper, we have introduced a new collection
of internet forum posts, the ARGUE corpus, col-
lected across a range of ideological topics, and con-
taining scalar Agreement/Disagreement annotations
over quote-response pairs within a post. We have
demonstrated that we can achieve a significant im-
provement over a unigram baseline agreement de-
tection system using features from both a response
and the quote being responded to.
Beyond agreement, the ARGUE corpus contains
finer-grained annotations for degrees of insult, nas-
tiness, and emotional appeal, as well as the pres-
ence of sarcasm. We have demonstrated that these
classes (especially insult and nastiness) correlate
with agreement. While the utility of these classes
as features for agreement detection is dependent on
how easily they are learned, in closing we note that
they also afford us a richer understanding of how ar-
gumentative conversation flows. In section 2.1.2, we
outlined how they can yield understanding of the po-
tential functions of a discourse particle within a par-
ticular post. They may as allow us to understand the
extent to which participants react in kind, rewarding
insult with insult or kindness in turn. In future work,
we hope to turn to these conversational dynamics.
In future work, it would be useful to
build a ternary classifier which labels
9
Agree/Disagree/Neutral, thus reflecting the true
distribution of these dialogue acts in the data.
Additionally, the proportion of agreeing utterances
varies widely across media so it may be desirable to
add an appropriate prior when adapting the model
to a new dataset.
Acknowledgments
This work was funded by Grant NPS-BAA-03 to
UCSC and and through the Intelligence Advanced
Research Projects Activity (IARPA) through the
Army Research Laboratory. We?d like to thank
Craig Martell for helpful discussions over the course
of this project, and the anonymous reviewers for use-
ful feedback. We would also like to thank Michael
Minor and Jason Aumiller for their contributions to
scripting and the database.
References
R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. 2003.
Mining newsgroups using networks arising from so-
cial behavior. In Proceedings of the 12th international
conference on World Wide Web, pages 529?535. ACM.
J.L. Austin. 1965. How to do things with words. Oxford
University Press, New York.
R. Awadallah, M. Ramanath, and G. Weikum. 2010.
Language-model-based pro/con classification of polit-
ical text. In Proceeding of the 33rd international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 747?748. ACM.
M. Bansal, C. Cardie, and L. Lee. 2008. The power
of negative thinking: Exploiting label disagreement in
the min-cut classification framework. Proceedings of
COLING: Companion volume: Posters, pages 13?16.
J. Bollen, H. Mao, and X. Zeng. 2011. Twitter mood
predicts the stock market. Journal of Computational
Science.
G.A. Bryant and J.E. Fox Tree. 2002. Recognizing ver-
bal irony in spontaneous speech. Metaphor and sym-
bol, 17(2):99?119.
C. Burfoot. 2008. Using multiple sources of agree-
ment information for sentiment classification of polit-
ical transcripts. In Australasian Language Technology
Association Workshop 2008, volume 6, pages 11?18.
Robert B. Cialdini. 2000. Influence: Science and Prac-
tice (4th Edition). Allyn & Bacon.
D. Davidov, O. Tsur, and A. Rappoport. 2010. Semi-
supervised recognition of sarcastic sentences in twitter
and amazon. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning,
pages 107?116. Association for Computational Lin-
guistics.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC, vol-
ume 6, pages 449?454. Citeseer.
Barbara Di Eugenio, Johanna D. Moore, and Massimo
Paolucci. 1997. Learning features that predict cue
usage. In Proceedings of the 35th Annual Meet-
ing of the Association for Computational Linguistics,
ACL/EACL 97, pages 80?87.
J.E. Fox Tree and J.C. Schrock. 1999. Discourse Mark-
ers in Spontaneous Speech: Oh What a Difference
an Oh Makes. Journal of Memory and Language,
40(2):280?295.
J.E. Fox Tree and J.C. Schrock. 2002. Basic mean-
ings of you know and I mean. Journal of Pragmatics,
34(6):727?747.
J. E. Fox Tree. 2010. Discourse markers across speak-
ers and settings. Language and Linguistics Compass,
3(1):113.
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in
conversational speech: Use of bayesian networks to
model pragmatic dependencies. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, pages 669?es. Association for Com-
putational Linguistics.
R.W. Gibbs. 2000. Irony in talk among friends.
Metaphor and Symbol, 15(1):5?27.
M. Groen, J. Noyes, and F. Verstraten. 2010. The Effect
of Substituting Discourse Markers on Their Role in
Dialogue. Discourse Processes: A Multidisciplinary
Journal, 47(5):33.
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
putational Linguistics, 19(3):501?530.
John E. Hunter. 1987. A model of compliance-
gaining message selection. Communication Mono-
graphs, 54(1):54?63.
M. Joshi and C. Penstein-Rose?. 2009. Generalizing de-
pendency features for opinion mining. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 313?316. Association for Computational
Linguistics.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
G. Mishne and N. Glance. 2006. Leave a reply: An anal-
ysis of weblog comments. In Third annual workshop
on the Weblogging ecosystem. Citeseer.
10
Margaret G. Moser and Johanna Moore. 1995. Inves-
tigating cue selection and placement in tutorial dis-
course. In ACL 95, pages 130?137.
A. Murakami and R. Raymond. 2010. Support or
Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Association for Computational Linguistics.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.
LIWC: Linguistic Inquiry and Word Count.
A.M. Popescu and M. Pennacchiotti. 2010. Detecting
controversial events from twitter. In Proceedings of
the 19th ACM international conference on Information
and knowledge management, pages 1873?1876. ACM.
Deborah Schiffrin. 1987. Discourse Markers. Cam-
bridge University Press, Cambridge, U.K.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263. Association for
Computational Linguistics.
S. Somasundaran and J. Wiebe. 2009. Recognizing
stances in online debates. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1-Volume
1, pages 226?234. Association for Computational Lin-
guistics.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In Proceedings
of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in
Text, pages 116?124. Association for Computational
Linguistics.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from Con-
gressional floor-debate transcripts. In Proceedings of
the 2006 conference on empirical methods in natural
language processing, pages 327?335. Association for
Computational Linguistics.
Marilyn Walker, Rob Abbott, Pranav Anand, Jean E. Fox
Tree, Robeson Bowmani, and Michael Minor. Cats
Rule and Dogs Drool: Classifying Stance in Online
Debate.
Y.C. Wang and C.P. Rose?. 2010. Making conversational
structure explicit: identification of initiation-response
pairs within online discussions. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 673?676. Association for
Computational Linguistics.
11
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 1?9,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Cats Rule and Dogs Drool!: Classifying Stance in Online Debate
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox Tree,
Robeson Bowmani, and Michael Minor
University of California Santa Cruz
Abstract
A growing body of work has highlighted the
challenges of identifying the stance a speaker
holds towards a particular topic, a task that in-
volves identifying a holistic subjective dispo-
sition. We examine stance classification on
a corpus of 4873 posts across 14 topics on
ConvinceMe.net, ranging from the playful to
the ideological. We show that ideological de-
bates feature a greater share of rebuttal posts,
and that rebuttal posts are significantly harder
to classify for stance, for both humans and
trained classifiers. We also demonstrate that
the number of subjective expressions varies
across debates, a fact correlated with the per-
formance of systems sensitive to sentiment-
bearing terms. We present results for iden-
tifing rebuttals with 63% accuracy, and for
identifying stance on a per topic basis that
range from 54% to 69%, as compared to un-
igram baselines that vary between 49% and
60%. Our results suggest that methods that
take into account the dialogic context of such
posts might be fruitful.
1 Introduction
Recent work has highlighted the challenges of iden-
tifying the STANCE that a speaker holds towards a
particular political, social or technical topic. Clas-
sifying stance involves identifying a holistic subjec-
tive disposition, beyond the word or sentence (Lin
et al, 2006; Malouf and Mullen, 2008; Greene and
Resnik, 2009; Somasundaran and Wiebe, 2009; So-
masundaran and Wiebe, 2010). Our work is inspired
by the large variety of such conversations now freely
available online, and our observation that the contex-
tual affordances of different debate and discussion
websites vary a great deal. One important contex-
tual variable, discussed at length below, is the per-
centage of posts that are rebuttals to previous posts,
which varies in our data from 34% to 80%. The abil-
ity to explicitly rebut a previous post gives these de-
bates both monologic and dialogic properties (Biber,
1991; Crystal, 2001; Fox Tree, 2010); Compare Fig-
ure 1 to Figure 2. We believe that discussions con-
taining many rebuttal links require a different type of
analysis than other types of debates or discussions.
Dialogic Capital Punishment
Studies have shown that using the death penalty saves 4 to 13 lives
per execution. That alone makes killing murderers worthwhile.
What studies? I have never seen ANY evidence that capital pun-
ishment acts as a deterrant to crime. I have not seen any evidence
that it is ?just? either.
When Texas and Florida were executing people one after the other
in the late 90?s, the murder rates in both states plunged, like Rosie
O?donnel off a diet.. .
That?s your evidence? What happened to those studies? In the
late 90s a LOT of things were different than the periods preceding
and following the one you mention. We have no way to determine
what of those contributed to a lower murder rate, if indeed there
was one. You have to prove a cause and effect relationship and
you have failed.
Figure 1: Capital Punishment discussions with posts
linked via rebuttal links.
This paper utilizes 1113 two-sided debates (4873
posts) from Convinceme.net for 14 different debate
topics. See Table 1. On Convinceme, a person starts
a debate by posting a topic or a question and provid-
ing sides such as for vs. against. Debate participants
can then post arguments for one side or the other, es-
sentially self-labelling their post for stance. These
debates may be heated and emotional, discussing
weighty issues such as euthanasia and capital pun-
ishment, such as the example in Figure 1. But they
also appear to be a form of entertainment via playful
1
debate. Popular topics on Convinceme.net over the
past 4 years include discussions of the merits of Cats
vs. Dogs, or Pirates vs. Ninjas (almost 1000 posts).
See Figure 3.
Monologic Capital Punishment
I value human life so much that if someone takes one than his
should be taken. Also if someone is thinking about taking a life
they are less likely to do so knowing that they might lose theirs
Death Penalty is only a costlier version of a lifetime prison sen-
tence, bearing the exception that it offers euthanasia to criminals
longing for an easy escape, as opposed to a real punishment.
There is no proof that the death penalty acts as a deterrent, plus
due to the finalty of the sentence it would be impossible to amend
a mistaken conviction which happens with regualrity especially
now due to DNA and improved forensic science.
Actually most hardened criminals are more afraid to live-then die.
I?d like to see life sentences without parole in lieu of capital pun-
ishment with hard labor and no amenities for hard core repeat
offenders, the hell with PC and prisoner?s rights-they lose priv-
eledges for their behaviour.
Figure 2: Posts on the topic Capital punishment without
explicit link structure. The discussion topic was ?Death
Penalty?, and the argument was framed as yes we should
keep it vs. no we should not.
Our long term goal is to understand the dis-
course and dialogic structure of such conversations.
This could be useful for: (1) creating automatic
summaries of each position on an issue (Sparck-
Jones, 1999); (2) gaining a deeper understanding
of what makes an argument persuasive (Marwell
and Schmitt, 1967); and (3) identifying the lin-
guistic reflexes of perlocutionary acts such as per-
suasion and disagreement (Walker, 1996; Greene
and Resnik, 2009; Somasundaran and Wiebe, 2010;
Marcu, 2000). As a first step, in this paper we aim
to automatically identify rebuttals, and identify the
speaker?s stance towards a particular topic.
Dialogic Cats vs. Dogs
Since we?re talking much of $hit, then Dogs rule! Cat poo is ex-
tremely foul to one?s nostrils you?ll regret ever handling a cat.
Stick with dogs, they?re better for your security, and poo?s not too
bad. Hah!
Dog owners seem infatuated with handling sh*t. Cat owners don?t
seem to share this infatuation.
Not if they?re dog owners who live in the country. If your dog
sh*ts in a field you aren?t going to walk out and pick it up.
Cat owners HAVE to handle sh*t, they MUST clean out a litter
box...so suck on that!
Figure 3: Cats vs. Dogs discussions with posts linked by
rebuttal links.
The most similar work to our own is that of So-
masundaran & Wiebe (2009, 2010) who also focus
on automatically determining the stance of a debate
participant with respect to a particular issue. Their
data does not provide explicit indicators of dialogue
structure such as are provided by the rebuttal links
in Convinceme. Thus, this work treats each post as
a monologic text to be classified in terms of stance,
for a particular topic. They show that discourse re-
lations such as concessions and the identification of
argumentation triggers improves performance over
sentiment features alone (Somasundaran and Wiebe,
2009; Somasundaran and Wiebe, 2010). This work,
along with others, indicates that for such tasks it is
difficult to beat a unigram baseline (Pang and Lee,
2008).
Other similar related work analyzes Usenet forum
quote/response structures (Wang and Rose?, 2010).
We believe quote/response pairs have a similar dis-
course structure to the rebuttal post pairs in Con-
vinceme, but perhaps with the linguistic reflexes
of stance expressed even more locally. However
agreement vs. disagreement is not labelled across
quote/response pairs and Wang & Rose (2010) do
not attempt to distinguish these different discourse
relations. Rather they show that they can use a vari-
ant of LSA to identify a parent post, given a response
post, with approximately 70% accuracy. A recent
paper by (Abbott et al, 2011) examines agreement
and disagreement in quote/response pairs in idealog-
ical and nonidealogical online forum discussions,
and shows that you can distinguish the agreement
relation with 68% accuracy. Their results indicate
that contextual features do improve performance for
identifying the agreement relation between quotes
and responses.
Other work has utilized the social network struc-
ture of online forums, either with or without tex-
tual features of particular posts (Malouf and Mullen,
2008; Mishne and Glance, 2006; Murakami and
Raymond, 2010; Agrawal et al, 2003). However
this work does not examine the way that the dia-
logic structure varies by topic, as we do, and the
threading structure of their debates does not dis-
tinguish between agreement and disagreement re-
sponses. (Mishne and Glance, 2006) show that most
replies to blog posts are disagreements, while Agar-
wal?s work assumed that adjacent posts always dis-
agree, and did not use any of the information in the
text. Murakami & Raymond (2010) show that sim-
ple rules for identifying disagreement, defined on
the textual content of the post, can improve over
Agarwal?s results and (Malouf and Mullen, 2008)
show that a combination of textual and social net-
2
work features provides the best performance. We
leave the incorporation of social network informa-
tion for stance classification to future work.
Section 3 discusses our corpus in more detail, and
presents the results of a human debate-side classi-
fication task conducted on Mechanical Turk. Sec-
tion 3 describes two different machine learning ex-
periments: one for identifying rebuttals and the other
for automatically determining stance. Section 4
presents our results. We show that we can iden-
tify rebuttals with 63% accuracy, and that using sen-
timent, subjectivity and dialogic features, we can
achieve debate-side classification accuracies, on a
per topic basis, that range from 54% to 69%, as com-
pared to unigram baselines that vary between 49%
and 60%.
2 Corpus Description and Analysis
Table 1 provides an overview of our corpus. Our
corpus consists of 1113 two-sided debates (4873
posts) from Convinceme.net for 12 topics ranging
from playful debates such as Cats vs. Dogs to more
heated political topics such as Capital Punishment.
In Table 1, the topics above the line are either tech-
nical or playful, while the topics below the line are
ideological. In total the corpus consists of 2,722,340
words; the topic labeled debates which we use in our
experiments contain 507,827 words.
Convinceme provides three possible sources of
dialogic structure: (1) the SIDE that a post is placed
on indicates the poster?s stance with respect to the
original debate topic, and thus can be considered as a
response to that post; (2) REBUTTAL LINKS between
posts which are explicitly indicated by the poster us-
ing the affordances of the site; and (3) the TEMPO-
RAL CONTEXT of the debate, i.e. the state of the
debate at a particular point in time, which a debate
participant orients to in framing their post.
Topics vary a great deal in terms of their dialogic
structure and linguistic expression. In Table 1, the
columns providing counts for different variables are
selected to illustrate ways in which topics differ in
the form and style of the argument and in its sub-
jective content. One important variable is the per-
centage of the topic posts that are linked into a re-
buttal dialogic structure (Rebuttals). Some of these
differences can be observed by comparing the dia-
logic and monologic posts for the Capital Punish-
ment topic in Figures 1 and 2 to those for the Cats
vs. Dogs topic in Figures 3 and 4. Ideological
Monologic Cats vs. Dogs
First of all, cats are about a thousand times easier to care for.
You don?t have to walk them or bathe them because they?re smart
enough to figure out all that stuff on their own. Plus, they have the
common courtesy to do their business in the litter box, instead of
all over your house and yard. Just one of the many reasons cats
rule and dogs, quite literally drool!
Say, you had a bad day at work, or a bad breakup, you just wanna
go home and cry. A cat would just look at you like ?oh ok, you?re
home? and then walk away. A dog? Let?s see, the dog would most
likely wiggle its tail, with tongue sticking out and head tilted - the
?you?re home! i missed you so much, let?s go snuggle in front of
the TV and eat ice-cream? look. What more do I need to say?
Figure 4: Posts on the topic Cats vs. Dogs without ex-
plicit rebuttal links.
topics display more author investment; people feel
more strongly about these issues. This is shown by
the fact that there are more rebuttals per topic and
more posts per author (P/A) in the topics below the
line in Table 1. It follows that these topics have a
much higher degree of context-dependence in each
post, since posts respond directly to the parent post.
Rebuttals exhibit more markers of dialogic interac-
tion: greater pronominalization (especially you as
well as propositional anaphora such as that and it),
ellipsis, and dialogic cue words; Figure 5 shows the
difference in counts of ?you? between rebuttals and
non-rebuttals (Rebuttals x? = 9.6 and Non-Rebuttals
x? = 8.5, t(27) = 24.94, p < .001). Another indi-
cation of author investment is the percentage of au-
thors with more than one post (A > 1P). Post Length
(PL), on the other hand, is not significantly corre-
lated with degree of investment in the topic.
Figure 5: Kernel density estimates for ?you? counts across
rebuttals (green) and non-rebuttals (red).
Other factors we examined were words per sen-
3
Post and Threading Variables Normalized LIWC Variables
Topic Posts Rebuttals P/A A > 1p PL Pro WPS 6LTR PosE NegE
Cats v. Dogs 148 40% 1.68 26% 242 3.30 -1.95 -2. 43 1.70 .30
Firefox vs. IE 218 40% 1.28 16% 167 -0.11 -0.84 0.53 1.23 -0.81
Mac vs. PC 126 47% 1.85 24% 347 0.52 0.28 -0.85 -0.11 -1.05
Superman/Batman 140 34% 1.41 21% 302 -0.57 -1.78 -0.43 1.21 .99
2nd Amendment 134 59% 2.09 45% 385 -1.38 1.74 0.58 -1.04 0.38
Abortion 594 70% 2.82 43% 339 0.63 -0.27 -0.41 -0.95 0.68
Climate Change 202 69% 2.97 40% 353 -0.74 1.23 0.57 -1.25 -0.63
Communism vs. Capitalism 212 70% 3.03 47% 348 -0.76 -0.15 1.09 0.39 -0.55
Death Penalty 324 62% 2.44 45% 389 -0.15 -0.40 0.49 -1.13 2.90
Evolution 798 76% 3.91 55% 430 -0.80 -1.03 1.34 -0.57 -0.94
Exist God 844 77% 4.24 52% 336 0.43 -0.10 0.34 -0.24 -0.32
Gay Marriage 505 65% 2.12 29% 401 -0.13 .86 .85 -0.42 -0.01
Healthcare 110 80% 3.24 56% 280 0.28 1.54 .99 0.14 -0.42
Marijuana Legalization 214 52% 1.55 26% 423 0.14 0.37 0.53 -0.86 0.50
Table 1: Characteristics of Different Topics. Topics below the line are considered ?ideological?. Normalized LIWC
variable z-scores are significant when more than 1.94 standard deviations away from the mean (two-tailed).
KEY: Number of posts on the topic (Posts). Percent of Posts linked by Rebuttal links (Rebuttals). Posts per author
(P/A). Authors with more than one post (A > 1P). Post Length in Characters (PL). Pro = percent of the words as
pronominals. WPS = Words per sentence. 6LTR = percent of words that are longer than 6 letters. PosE positive
emotion words. NegE negative emotion words.
tence (WPS), the length of words used (6LTR)
which typically indicates scientific or low frequency
words, the use of pronominal forms (Pro), and
the use of positive and negative emotion words
(PosE,NegE) (Pennebaker et al, 2001). For exam-
ple, Table 1 shows that discussions about Cats vs.
Dogs consist of short simple words in short sen-
tences with relatively high usage of positive emo-
tion words and pronouns, whereas 2nd amendment
debates use relatively longer sentences, and death
penalty debates (unsurprisingly) use a lot of nega-
tive emotion words.
Human Topline. The best performance for sid-
ing ideological debates in previous work is approx-
imately 64% accuracy over all topics, for a collec-
tion of 2nd Amendment, Abortion, Evolution, and
Gay Rights debate posts (Somasundaran and Wiebe,
2010). Their best performance is 70% for the 2nd
amendment topic. The website that these posts were
collected from apparently did not support dialogic
threading, and thus there are no explicitly linked re-
buttals in this data set. Given the dialogic nature
of our data, as indicated by the high percentage of
rebuttals in the ideological debates, we first aim to
determine how difficult it is for humans to side an
individual post from a debate without context. To
our knowledge, none of the previous work on de-
bate side classification has attempted to establish a
human topline.
We set up a Mechanical Turk task by randomly se-
lected a subset of our data excluding the first post on
each side of a debate and debates with fewer than 6
posts on either side. Each of our 12 topics consists of
more than one debate: each debate was mapped by
hand to the topic and topic-siding (as in (Somasun-
daran and Wiebe, 2010)). We selected equal num-
bers of posts for each topic for each side, and cre-
ated 132 tasks (Mechanical Turk HITs). Each HIT
consisted of choosing the correct side for 10 posts
divided evenly, and selected randomly without re-
placement, from two debates. For each debate we
presented a title, side labels, and the initial post on
each side. For each post we presented the first 155
characters with a SEE MORE button which expanded
the post to its full length. Each HIT was judged by 9
annotators using Mechanical Turk with each anno-
tator restricted to at most 30 HITS (300 judgments).
Since many topics were US specific and we wanted
annotators with a good grasp of English, we required
Turkers to have a US IP address.
Figure 6 plots the number of annotators over all
topics who selected the ?true siding? as the side that
the post was on. We defined ?true siding? for this
purpose as the side that the original poster placed
their post. Figure 6 illustrates that humans often
placed the post on the wrong side. The majority of
posters agreed with the true siding 78.26% of the
time. The Fleiss? kappa statistic was 0.2656.
Importantly and interestingly, annotator accuracy
varied across topics in line with rebuttal percentage.
Annotators correctly labeled 94 of 100 posts for Cats
vs. Dogs but only managed 66 of 100 for the Cli-
4
Figure 6: Accuracies of Human Mechanical Turk judges
at selecting the True Siding of a post without context.
mate Change topic. This suggests that posts may
be difficult to side without context, which is what
one might expect given their dialogic nature. Rebut-
tals were clearly harder to side: annotators correctly
sided non-rebuttals 87% of the time, but only man-
aged 73% accuracy for rebuttals. Since all of the less
serious topics consisted of ?50% rebuttals while all
of the more serious ideological debates had >50%
rebuttals, 76% of ideological posts were sided cor-
rectly, while 85% of non-ideological posts were cor-
rectly sided. See Table 2.
Class Correct Total Accuracy
Rebuttal 606 827 0.73
Non-Rebuttal 427 493 0.87
Table 2: Human Agreement on Rebuttal Classification
Looking at the data by hand revealed that when
nearly all annotators agreed with each other but dis-
agreed with the self-labeled side, the user posted on
the wrong side (either due to user error, or because
the user was rebutting an argument the parent post
raised, not the actual conclusion).
The difficult-to-classify posts (where only 4-6 an-
notators were correct) were more complex. Our
analysis suggests that in 28% of these cases, the an-
notators were simply wrong, perhaps only skimming
a post when the stance indicator was buried deep in-
side it. Our decision to show only the first 155 char-
acters of each post by default (with a SHOW MORE
button) may have contributed to this error. An ad-
ditional 39% were short comments or ad hominem
responses, that showed disagreement, but no indi-
cation of side and 17% were ambiguous out of con-
text. A remaining 10% were meta-debate comments,
either about whether there were only two sides, or
whether the argument was meaningful. Given the
differences in siding difficulty depending on rebut-
tal status, in Section 4 we present results for both
rebuttal and stance classification.
3 Features and Learning Methods
Our experiments were conducted with the Weka
toolkit. All results are from 10 fold cross-validation
on a balanced test set. In the hand examination of
annotators siding performance, 101 posts were de-
termined to have incorrect self-labeling for side. We
eliminated these posts and their descendants from
the experiments detailed below. This resulted in a
dataset of 4772 posts. We used two classifiers with
different properties: NaiveBayes and JRip. JRip is
a rule based classifier which produces a compact
model suitable for human consumption and quick
application. Table 3 provides a summary of the fea-
tures we extract for each post. We describe and mo-
tivate these feature sets below.
Set Description/Examples
Post Info IsRebuttal, Poster
Unigrams Word frequencies
Bigrams Word pair frequencies
Cue Words Initial unigram, bigram, and trigram
Repeated
Punctuation
Collapsed into one of the following: ??, !!, ?!
LIWC LIWC measures and frequencies
Dependencies Dependencies derived from the Stanford Parser.
Generalized
Dependen-
cies
Dependency features generalized with respect to
POS of the head word and opinion polarity of
both words.
Opinion De-
pendencies
Subset of Generalized Dependencies with opin-
ion words from MPQA.
Context Fea-
tures
Matching Features used for the post from the par-
ent post.
Table 3: Feature Sets, Descriptions, and Examples
Counts, Unigrams, Bigrams. Previous work
suggests that the unigram baseline can be difficult to
beat for certain types of debates (Somasundaran and
Wiebe, 2010). Thus we derived both unigrams and
bigrams as features. We also include basic counts
such as post length.
Cue Words. We represent each posts initial un-
igram, bigram and trigram sequences to capture the
useage of cue words to mark responses of particular
type, such as oh really, so, and well; these features
were based on both previous work and our exami-
nation of the corpus (Fox Tree and Schrock, 1999;
Fox Tree and Schrock, 2002; Groen et al, 2010).
5
Repeated Punctuation. Our informal analyses
suggested that repeated sequential use of particular
types of punctuation such as !! and ?? did not mean
the same thing as simple counts or frequencies of
punctuation across a whole post. Thus we developed
distinct features for a subset of these repetitions.
LIWC. We also derived features using the Lin-
guistics Inquiry Word Count tool (LIWC-2001)
(Pennebaker et al, 2001). LIWC provides meta-
level conceptual categories for words to use in word
counts. Some LIWC features that we expect to be
important are words per sentence (WPS), pronomi-
nal forms (Pro), and positive and negative emotion
words (PosE) and (NegE). See Table 1.
Syntactic Dependency. Previous research in
this area suggests the utility of dependency struc-
ture to determine the TARGET of an opinion word
(Joshi and Penstein-Rose?, 2009; Somasundaran and
Wiebe, 2009; Somasundaran and Wiebe, 2010). The
dependency parse for a given sentence is a set of
triples, composed of a grammatical relation and the
pair of words for which the grammatical relation
holds (reli, wj , wk), where reli is the dependency
relation among words wj and wk. The word wj is
the HEAD of the dependency relation. We use the
Stanford parser to parse the utterances in the posts
and extract dependency features (De Marneffe et al,
2006; Klein and Manning, 2003).
Generalized Dependency. To create generalized
dependencies, we ?back off? the head word in each
of the above features to its part-of-speech tag (Joshi
and Penstein-Rose?, 2009). Joshi & Rose?s results
suggested that this approach would work better than
either fully lexicalized or fully generalized depen-
dency features. We call these POS generalized de-
pendencies in the results below.
Opinion Dependencies. Somasundaran & Wiebe
(2009) introduced features that identify the TAR-
GET of opinion words. Inspired by this approach,
we used the MPQA dictionary of opinion words
to select the subset of dependency and generalized
dependency features in which those opinion words
appear. For these features we replace the opinion
words with their positive or negative polarity equiv-
alents (Lin et al, 2006).
Context Features. Given the difficulty annota-
tors had in reliably siding rebuttals as well as their
prevalence in the corpus, we hypothesize that fea-
tures representing the parent post could be helpful
for classification. Here, we use a naive represen-
tation of context, where for all the feature types in
Table 3, we construct both parent features and post
features. For top-level parentless posts, the parent
features were null.
Figure 7: Model for distinguishing rebuttals vs. nonre-
buttals across all topics.
4 Results
The primary aim of our experiments was to deter-
mine the potential contribution, to debate side clas-
sification performance, of contextual dialogue fea-
tures, such as linguistic reflexes indicating a poster?s
orientation to a previous post or information from a
parent post. Because we believed that identification
of whether a post is a rebuttal or not might be help-
ful in the long term for debate-side classification, we
also establish a baseline for rebuttal classification.
4.1 Rebuttal Classification Results
The differences in human performance for siding de-
pended on rebuttal status. Our experiments on re-
buttal classification using the rule-based JRip clas-
sifer on a 10-fold cross-validation of our dataset pro-
6
duced 63% accuracy. Figure 7 illustrates a sample
model learned for distinguishing rebuttals from non-
rebuttals across all topics. The Figure shows that,
although we used the full complement of lexical and
syntactic features detailed above, the learned rules
were almost entirely based on LIWC and unigram
lexical features, such as 2nd person pronouns (7/8
rules), quotation marks (4/8 rules), question marks
(3/8), and negation (4/8), all of which correlated
with rebuttals. Other features that are used at several
places in the tree are LIWC Social Processes, LIWC
references to people, and LIWC Inclusive and Ex-
clusive. One tree node reflects the particular concern
with bodily functions that characterizes the Cats vs.
Dogs debate as illustrated in Figure 3.
4.2 Automatic Debate-Side Classification
Results
We first compared accuracies using Naive Bayes to
JRip for all topics for all feature sets. A paired t-test
showed that Naive Bayes over all topics and feature
sets was consistently better than JRip (p < .0001).
Thus the rest of our analysis and the results in Ta-
ble 4 focus on the Naive Bayes results.
Table 4 presents results for automatic debate
side classification using different feature sets and the
Naive Bayes learner which performs best over all
topics. In addition to classifying using only post-
internal features, we ran a parallel set of experiments
adding contextual features representing the parent
post, as described in Section 3. The results in Table
4 are divided under the headers Without Context and
With Context depending on whether features from
the parent post were used if it existed (e.g. in the
case of rebuttals).
We conducted paired t-tests over all topics simul-
taneously to examine the utility of different feature
sets. We compared unigrams to LIWC, opinion gen-
eralized dependencies, POS generalized dependen-
cies, and all features. We also compared experi-
ments using context features to experiments using
no contextual features. In general, our results in-
dicate that if the data are aggregated over all top-
ics, that indeed it is very difficult to beat the uni-
gram baseline. Across all topics there are generally
no significant differences between experiments con-
ducted with unigrams and other features. The mean
accuracies across all topics for unigrams vs. LIWC
features was 54.35% for unigrams vs. 52.83% for
LIWC. The mean accuracies for unigram vs POS
generalized dependencies was 54.35% vs. 52.64%,
and for unigrams vs. all features was Unigram
54.35% vs 54.62%. The opinion generalized de-
pendencies features actually performed significantly
worse than unigrams with an accuracy of 49% vs.
54.35% (p < .0001).
It is interesting to note that in general the unigram
accuracies are significantly below what Somasun-
daran and Wiebe achieve (who report overall uni-
gram of 62.5%). This suggests a difference between
the debate posts in their corpus and the Convinceme
data we used which may be related to the proportion
of rebuttals.
The overal lack of impact for either the POS gen-
eralized dependency features (GDepP) or the Opin-
ion generalized dependency features (GDep0) is
surprising given that they improve accuracy for other
similar tasks (Joshi and Penstein-Rose?, 2009; Soma-
sundaran and Wiebe, 2010). While our method of
extracting the GDepP features is identical to (Joshi
and Penstein-Rose?, 2009), our method for extracting
GDepO is an approximation of the method of (So-
masundaran and Wiebe, 2010), that does not rely on
selecting particular patterns indicating the topics of
arguing by using a development set.
The LIWC feature set, which is based on a lexi-
cal hierarchy that includes social features, negative
and positive emotion, and psychological processes,
is the only feature set that appears to have the po-
tential to systematically show improvement over a
good range of topics. We believe that further analy-
sis is needed; we do not want to handpick topics for
which particular feature sets perform well.
Our results also showed that context did not seem
to help uniformly over all topics. The mean per-
formance over all topics for contextual features us-
ing the combination of all features and the Naive
Bayes learner was 53.0% for context and 54.62%
for no context (p = .15%, not significant). Interest-
ing, the use of contextual features provided surpris-
ingly greater performance for particular topics. For
example for 2nd Amendment, unigrams with con-
text yield a performance of 69.23% as opposed to
the best performing without context features using
LIWC of 64.10%. The best performance of (So-
masundaran and Wiebe, 2010) is also 70% for the
2nd amendment topic. For the Healthcare topic,
LIWC with context features corresponds to an accu-
racy of 60.64% as opposed to GDepP without con-
text performance of 54.26%. For Communism vs.
Capitism, LIWC with context features gives an ac-
curacy of 56.55% as opposed to accuracies actually
7
Without Context With Context
Turk Uni LIWC GdepO GdepP All Uni LIWC GdepO GdepP All
Cats v. Dogs 94 59.23 55.38 56.15 61.54 62.31 50.77 56.15 55.38 60.77 50.00
Firefox vs. IE 74 51.25 53.75 43.75 48.75 50.00 51.25 53.75 52.50 52.50 51.25
Mac vs. PC 76 53.33 56.67 55.00 50.83 56.67 53.33 55.83 56.67 49.17 54.17
Superman Batman 89 54.84 45.97 42.74 45.97 54.03 50.00 57.26 43.55 50.81 53.23
2nd Amendment 69 56.41 64.10 51.28 58.97 57.69 69.23 61.54 44.87 52.56 67.95
Abortion 75 50.97 51.56 50.58 52.14 51.17 51.36 53.70 51.75 53.70 50.78
Climate Change 66 53.65 58.33 38.02 46.35 50.52 48.96 56.25 38.02 38.54 48.96
Comm vs. Capitalism 68 48.81 47.02 46.43 47.02 48.81 45.83 56.55 47.02 51.19 48.81
Death Penalty 79 51.80 53.96 46.76 49.28 52.52 51.80 56.12 56.12 57.55 53.24
Evolution 72 57.24 48.36 54.93 56.41 57.24 54.11 46.22 50.82 52.14 52.96
Existence of God 73 52.71 51.14 49.72 52.42 51.99 52.28 52.28 50.14 53.42 51.42
Gay Marriage 88 60.28 56.11 56.11 58.61 59.44 56.94 52.22 54.44 53.61 54.72
Healthcare 86 52.13 51.06 51.06 54.26 52.13 45.74 60.64 59.57 57.45 53.19
MJ Legalization 81 57.55 46.23 43.40 53.77 59.43 52.83 46.23 49.06 49.06 50.94
Table 4: Accuracies achieved using different feature sets and 10-fold cross validation as compared to the human
topline from MTurk. Best accuracies are shown in bold for each topic in each row. KEY: Human topline results
(Turk). Unigram features (Uni). Linguistics Inquiry Word Count features (LIWC). Generalized dependency features
containing MPQA terms (GdepO) & POS tags (GdepP). NaiveBayes was used, no attribute selection was applied.
below the majority class baseline for all of the fea-
tures without context.
Should we conclude anything from the fact that
6 of the topics are idealogical, out of the 7 topics
where contextual features provide the best perfor-
mance? We believe that the significantly greater per-
centage of rebuttals for these topics should give a
greater weight to contextual features, so it would be
useful to examine stance classification performance
on the subset of the posts that are rebuttals. We be-
lieve that context is important; our conclusion is that
our current contextual features are naive ? they are
not capturing the relationship between a post and a
parent post. Sequential models or at least better con-
textual features are needed.
The fact that we should be able to do much better
is indicated clearly by the human topline, shown in
the column labelled Turk in Table 4. Even without
context, and with the difficulties siding rebuttals, the
human annotators achieve accuracies ranging from
66% to 94%.
5 Discussion
This paper examines two problems in online-
debates: rebuttal classification and debate-side or
stance classification. Our results show that we can
identify rebuttals with 63% accuracy, and that using
lexical and contextual features such as those from
LIWC, we can achieve debate-side classification ac-
curacies on a per topic basis that range from 54% to
69%, as compared to a unigram baselines that vary
between 49% and 60%. These are the first results
that we are aware of that establish a human topline
for debate side classification. These are also the first
results that we know of for identifying rebuttals in
such debates.
Our results for stance classification are mixed.
While we show that for many topics we can beat
a unigram baseline given more intelligent features,
we do not beat the unigram baseline when we com-
bine our data across all topics. In addition, we are
not able to show across all topics that our contex-
tual features make a difference, though clearly use of
context should make a difference in understanding
these debates, and for particular topics, classifica-
tion results using context are far better than the best
feature set without any contextual features. In fu-
ture work, we hope to develop more intelligent fea-
tures for representing context and improve on these
results. We also plan to make our corpus available
to other researchers in the hopes that it will stimu-
late further work analyzing the dialogic structure of
such debates.
Acknowledgments
This work was funded by Grant NPS-BAA-03 to
UCSC and Intelligence Advanced Research Projects
Activity (IARPA) through the Army Research Lab-
oratory to UCSC by subcontract from the University
of Maryland. We?d like to thank Craig Martell and
Joseph King for helpful discussions over the course
of this project, and the anonymous reviewers for use-
ful feedback. We would also like to thank Jason Au-
miller for his contributions to the database.
8
References
Rob Abbott, Marilyn Walker, Jean E. Fox Tree, Pranav
Anand, Robeson Bowmani, and Joseph King. 2011.
How can you say such things?!?: Recognizing Dis-
agreement in Informal Political Argument. In Pro-
ceedings of the ACL Workshop on Language and So-
cial Media.
R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. 2003.
Mining newsgroups using networks arising from so-
cial behavior. In Proceedings of the 12th international
conference on World Wide Web, pages 529?535. ACM.
D. Biber. 1991. Variation across speech and writing.
Cambridge Univ Pr.
David Crystal. 2001. Language and the Internet. Cam-
bridge University Press.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC, vol-
ume 6, pages 449?454. Citeseer.
J.E. Fox Tree and J.C. Schrock. 1999. Discourse Mark-
ers in Spontaneous Speech: Oh What a Difference
an Oh Makes. Journal of Memory and Language,
40(2):280?295.
J.E. Fox Tree and J.C. Schrock. 2002. Basic mean-
ings of you know and I mean. Journal of Pragmatics,
34(6):727?747.
J. E. Fox Tree. 2010. Discourse markers across speak-
ers and settings. Language and Linguistics Compass,
3(1):113.
S. Greene and P. Resnik. 2009. More than words: Syn-
tactic packaging and implicit sentiment. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
503?511. Association for Computational Linguistics.
M. Groen, J. Noyes, and F. Verstraten. 2010. The Effect
of Substituting Discourse Markers on Their Role in
Dialogue. Discourse Processes: A Multidisciplinary
Journal, 47(5):33.
M. Joshi and C. Penstein-Rose?. 2009. Generalizing de-
pendency features for opinion mining. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 313?316. Association for Computational
Linguistics.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
W.H. Lin, T. Wilson, J. Wiebe, and A. Hauptmann. 2006.
Which side are you on?: identifying perspectives at
the document and sentence levels. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning, pages 109?116. Association for
Computational Linguistics.
R. Malouf and T. Mullen. 2008. Taking sides: User clas-
sification for informal online political discourse. In-
ternet Research, 18(2):177?190.
Daniel Marcu. 2000. Perlocutions: The Achilles?
heel of Speech Act Theory. Journal of Pragmatics,
32(12):1719?1741.
G. Marwell and D. Schmitt. 1967. Dimensions of
compliance-gaining behavior: An empirical analysis.
sociomety, 30:350?364.
G. Mishne and N. Glance. 2006. Leave a reply: An anal-
ysis of weblog comments. In Third annual workshop
on the Weblogging ecosystem. Citeseer.
A. Murakami and R. Raymond. 2010. Support or
Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Association for Computational Linguistics.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.
LIWC: Linguistic Inquiry and Word Count.
S. Somasundaran and J. Wiebe. 2009. Recognizing
stances in online debates. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1-Volume
1, pages 226?234. Association for Computational Lin-
guistics.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In Proceedings
of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in
Text, pages 116?124. Association for Computational
Linguistics.
Karen Sparck-Jones. 1999. Automatic summarizing;
factors and directions. In Inderjeet Mani and Mark
Maybury, editors, Advances in Automatic Text Summa-
rization. MIT Press.
Marilyn A. Walker. 1996. Inferring acceptance and re-
jection in dialogue by default rules of inference. Lan-
guage and Speech, 39-2:265?304.
Y.C. Wang and C.P. Rose?. 2010. Making conversational
structure explicit: identification of initiation-response
pairs within online discussions. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 673?676. Association for
Computational Linguistics.
9
