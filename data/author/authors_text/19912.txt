Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2042?2051, Dublin, Ireland, August 23-29 2014.
RED: A Reference Dependency Based MT Evaluation Metric
Hui Yu
??
Xiaofeng Wu
?
Jun Xie
?
Wenbin Jiang
?
Qun Liu
??
Shouxun Lin
?
?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?
University of Chinese Academy of Sciences
{yuhui,xiejun,jiangwenbin,sxlin}@ict.ac.cn
?
CNGL, School of Computing, Dublin City University
{xiaofengwu,qliu}@computing.dcu.ie
Abstract
Most of the widely-used automatic evaluation metrics consider only the local fragments of the
references and translations, and they ignore the evaluation on the syntax level. Current syntax-
based evaluation metrics try to introduce syntax information but suffer from the poor pars-
ing results of the noisy machine translations. To alleviate this problem, we propose a novel
dependency-based evaluation metric which only employs the dependency information of the ref-
erences. We use two kinds of reference dependency structures: headword chain to capture the
long distance dependency information, and fixed and floating structures to capture the local con-
tinuous ngram. Experiment results show that our metric achieves higher correlations with human
judgments than BLEU, TER and HWCM on WMT 2012 and WMT 2013. By introducing extra
linguistic resources and tuning parameters, the new metric gets the state-of-the-art performance
which is better than METEOR and SEMPOS on system level, and is comparable with METEOR
on sentence level on WMT 2012 and WMT 2013.
1 Introduction
Automatic machine translation (MT) evaluation plays an important role in the evolution of MT. It not
only evaluates the performance of MT systems, but also makes the development of MT systems rapider
(Och, 2003). According to the type of the employed information, the automatic MT evaluation metrics
can be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-based
metrics.
The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR
(Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturing
the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect
the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric
(HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and
syntactic/semantic-role overlap (Gim?enez and M`arquez, 2007) , suffer from the parsing of the potentially
noisy machine translations, so the improvement of their performance is restricted due to the serious
parsing errors. Semantic-based metrics, such as MEANT (Lo et al., 2012; Lo and Wu, 2013), have the
similar problem that the accuracy of semantic role labeling (SRL) can also drop due to the errors in
translations. To avoid the parsing of potentially noisy translations, the CCG based metric (Mehay and
Brew, 2007) only uses the parsing result of reference and employs 2-gram dependents, but it did not
achieve the state-of-the-art performance.
In this paper, we propose a novel dependency tree based MT evaluation metric. The new metric only
employs the reference dependency tree, leaving the translation unparsed to avoid the error propagation.
We use two kinds of reference dependency structures in our metric. One is the headword chain (Liu and
Gildea, 2005) which can capture long distance dependency information. The other is fixed and floating
structure (Shen et al., 2010) which can capture local continuous ngram. When calculating the matching
score between the headword chain and the translation, we use a distance-based similarity. Experiment
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2042
results show that our metric achieves higher correlations with human judgments than BLEU, TER and
HWCM on WMT 2012 and WMT 2013. After introducing extra resources and tuning parameters on
WMT 2010, the new metric is better than METEOR and SEMPOS on system level and comparable with
METEOR on sentence level on WMT 2012 and WMT2013.
The remainder of this paper is organized as follows. Section 2 describes our new reference dependency
based MT evaluation metric. In Section 3, we introduce some extra resources to this new metric. Section
4 presents the parameter tuning for the new metric. Section 5 gives the experiment results. Conclusions
and future work are discussed in Section 6.
2 RED: A Reference Dependency Based MT Evaluation Metric
The new metric is a REference Dependency based automatic evaluation metric, so we name it RED.
We present the new metric detailedly in this section. The description of dependency ngrams is given in
Section 2.1. The method to score the dependency ngram is presented in Section 2.2. At last, the method
of calculating the final score is introduced in Section 2.3.
2.1 Two Kinds of Dependency Ngrams
To capture both the long distance dependency information and the local continuous ngrams, we use both
the headword chain and the fixed-floating structures in our new metric, which correspond to the two
kinds of dependency ngram (dep-ngram), headword chain ngram and fixed-floating ngram.
Figure 1: An example of dependency tree.
Figure 2: Different kinds of structures extracted
from the dependency tree in Figure 1. (a): Head-
word chain. (b): Fixed structure. (c): Floating struc-
ture.
2.1.1 Headword chain
Headword chain is a sequence of words which corresponds to a path in the dependency tree (Liu and
Gildea, 2005). For example, Figure 2(a) is a 3-word headword chain extracted from the dependency tree
in Figure 1. Headword chain can represent the long distance dependency information, but cannot capture
most of the continuous ngrams. In our metric, headword chain corresponds to the headword chain ngram
in which the positions of the words are considered. So the form of headword chain ngram is expressed
as (w1
pos1
, w2
pos2
, ..., wn
posn
), where n is the length of the headword chain ngram. For example, the
headword chain in Figure 2(a) is expressed as (saw
2
, with
5
,magnifier
7
).
2.1.2 Fixed and floating structures
Fixed and floating structures are defined in Shen et al. (2010). Fixed structures consist of a sub-root with
children, each of which must be a complete constituent. They are called fixed dependency structures
because the head is known or fixed. For example, Figure 2(b) shows a fixed structure. Floating structures
consist of a number of consecutive sibling nodes of a common head, but the head itself is unspecified.
Each of the siblings must be a complete constituent. Figure 2(c) shows a floating structure. Fixed-
floating structures correspond to fixed-floating ngrams in our metric. Fixed-floating ngrams don?t need
the position information, and can be simply expressed as (w1, w2, ..., wn), where n is the length of the
2043
Figure 3: An example of calculating matching score for a headword chain ngram
(saw
2
, with
5
,magnifier
7
). dis r
1
and dis r
2
are the distances between the corresponding two
words in the reference. dis h
1
and dis h
2
are the distances between the corresponding two words in the
hypothesis.
fixed-floating ngram. For example, the fixed structure in Figure 2(b) and the floating structre in Figure
2(c) can be expressed as (I, saw, an, ant) and (an, ant, with, a,magnifier) respectively.
2.2 Scoring Dep-ngrams
Headword chain ngrams may not be continuous, while fixed-floating ngrams must be continuous. So the
scoring methods of the two kinds of dep-ngrams are different, and we introduce the two scoring methods
in Section 2.2.1 and Section 2.2.2 respectively.
2.2.1 Scoring headword chain ngram
For a headword chain ngram (w1
pos1
, w2
pos2
, ..., wn
posn
), if we can find all these n words in the string
of the translation with the same order as they appear in the reference sentence, we consider it a match and
the matching score is a distance-based similarity which is calculated by the relative distance, otherwise it
is not a match and the score is 0. The matching score is a decimal value between 0 and 1, which is more
suitable than just use integer 0 and 1. For example, if the distance between two words in reference is 1,
but the distance in two different hypotheses are 2 and 5 respectively. It?s more reasonable to score them
0.5 and 0.2 rather than 1 and 0.
The relative distance dis r
i
between every two adjacent words in this kind of dep-ngram is calculated
by Formula (1), where pos
wi
is the position of word wi in the sentence. In Formula (1), we have
1 ? i ? n ? 1 and n is the length of the dep-ngram. Then a vector (dis r
1
, dis r
2
, ..., dis r
n?1
) is
obtained. In the same way, we obtain vector (dis h
1
, dis h
2
, ..., dis h
n?1
) for the translation side.
dis r
i
= |pos
w(i+1)
? pos
wi
| (1)
The matching score p
(d,hyp)
for a headword chain ngram (d) and the translation (hyp) is calculated
according to Formula (2), where n > 1. When the length of the dep-ngram equals 1, the matching score
equals 1 if the translation has the same word, otherwise, the matching score equals 0.
p
(d,hyp)
=
?
?
?
exp(?
?
n?1
i=1
|dis r
i
? dis h
i
|
n? 1
) if match
0 if unmatch
(2)
An example illustrating the calculation of the matching score p
(d,hyp)
is shown in Figure 3. There is
a 3-word headword chain ngram (saw
2
, with
5
,magnifier
7
) in the dependency tree of the reference.
2044
For this dep-3gram, the words are represented with underline in the reference dependency tree and the
reference sentence in Figure 3. We can also find all the same three underlined words in the translation
with the same order as they appear in the reference. Therefore, there is a match for this dep-3gram. To
compute the matching score between this dep-3gram and the translation, we have:
? Calculate the distance
dis r
1
= |pos
with
? pos
saw
| = |5? 2| = 3 dis r
2
= |pos
magnifier
? pos
with
| = |7? 5| = 2
dis h
1
= |pos
with
? pos
saw
| = |5? 2| = 3 dis h
2
= |pos
magnifier
? pos
with
| = |6? 5| = 1
? Get the matching score as Formula (3) according to Formula (2). d denotes
(saw
2
, with
5
,magnifier
7
) and hyp denotes the translation in the example.
p
(d,hyp)
= exp(?
|dis r
1
? dis h
1
|+ |dis r
2
? dis h
2
|
3? 1
) = exp(?
|3? 3|+ |2? 1|
3? 1
) = exp(?0.5)
(3)
We also tried other methods to calculate the matching score, such as the cosine distance and the
absolute distance, but the relative distance performed best. For a headword chain ngram with more than
one matches in the translation, we choose the one with the highest matching score.
2.2.2 Scoring fixed-floating ngram
The words in the fixed-floating ngram are continuous, so we restrict the matched string in the translation
also to being continuous. That means, for a fixed-floating ngram (w1, w2, ..., wn), if we can find all these
n words continuous in the translation with the same order as they appear in the reference, we think the
dep-ngram can match with the translation. The matching score can be obtained by Formula (4), where d
stands for a fixed-floating ngram and hyp stands for the translation.
p
(d,hyp)
=
{
1 if match
0 if unmatch
(4)
2.3 Scoring RED
In the new metric, we use Fscore to obtain the final score. Fscore is calculated by Formula (5), where ?
is a value between 0 and 1.
Fscore =
precision ? recall
? ? precision+ (1? ?) ? recall
(5)
The dep-ngrams of the reference and the string of the translation are used to calculate the precision and
recall. In order to calculate precision, the number of the dep-ngrams in the translation should be given,
but there is no dependency tree for the translation in our method. We know that the number of dep-
ngrams has an approximate linear relationship with the length of the sentence, so we use the length of
the translation to replace the number of the dep-ngrams in the translation dependency tree. Recall can
be calculated directly since we know the number of the dep-ngrams in the reference. The precision and
recall are computed as follows.
precision =
?
d?D
n
p
(d,hyp)
len
h
, recall =
?
d?D
n
p
(d,hyp)
count
n(ref)
D
n
is the set of dep-ngrams with the length of n. len
h
is the length of the translation. count
n(ref)
is the
number of the dep-ngrams with the length of n in the reference.
2045
The final score of RED is achieved using Formula (6), in which a weighted sum of the dep-ngrams?
Fscore is calculated. w
ngram
(0 ? w
ngram
? 1) is the weight of dep-ngram with the length of n. Fscore
n
is the Fscore for the dep-ngrams with the length of n.
RED =
N
?
n=1
(w
ngram
? Fscore
n
) (6)
3 Introducing Extra Resources
Many automatic evaluation metrics can only find the exact match between the reference and the transla-
tion, and the information provided by the limited number of references is not sufficient. Some evaluation
metrics, such as TERp (Snover et al., 2009) and METOER, introduce extra resources to expand the
reference information. We also introduce some extra resources to RED, such as stem, synonym and
paraphrase. The words within a sentence can be classified into content words and function words. The
effects of the two kinds of words are different and they shouldn?t have the same matching score, so we
introduce a parameter to distinguish them. The methods of applying these resources are introduced as
follows.
? Stem and Synonym
Stem(Porter, 2001) and synonym (WordNet
1
) are introduced to RED in the following three steps.
First, we obtain the alignment with Meteor Aligner (Denkowski and Lavie, 2011) in which not only
exact match but also stem and synonym are considered. We use stem and synonym together with
exact match as three match modules. Second, the alignment is used to match for a dep-ngram. We
think the dep-ngram can match with the translation if the following conditions are satisfied. 1) Each
of the words in the dep-ngram has a matched word in the translation according to the alignment;
2) The words in dep-ngram and the matched words in translation appear in the same order; 3) The
matched words in translation must be continuous if the dep-ngram is a fixed-floating ngram. At last,
the match module score of a dep-ngram is calculated according to Formula (7). Different match
modules have different effects, so we give them different weights.
s
mod
=
?
n
i=1
w
m
i
n
, 0 ? w
m
i
? 1 (7)
m
i
is the match module (exact, stem or synonym) of the ith word in a dep-ngram. w
m
i
is the match
module weight of the ith word in a dep-ngram. n is the number of words in a dep-ngram.
? Paraphrase
When introducing paraphrase, we don?t consider the dependency tree of the reference, because
paraphrases may not be contained in the headword chain and fixed-floating structures. First, the
alignment is obtained with METEOR Aligner, only considering paraphrase. Second, the matched
paraphrases are extracted from the alignment and defined as paraphrase-ngram. The score of a
paraphrase is 1? w
par
, where w
par
is the weight of paraphrase-ngram.
? Function word
We introduce a parameter w
fun
(0 ? w
fun
? 1) to distinguish function words and content words.
w
fun
is the weight of function words. The function word score of a dep-ngram or paraphrase-ngram
is computed according to Formula (8).
s
fun
=
C
fun
? w
fun
+ C
con
? (1? w
fun
)
C
fun
+ C
con
(8)
C
fun
is the number of function words in the dep-ngram or paraphrase-ngram. C
con
is the number
of content words in the dep-ngram or paraphrase-ngram.
1
http://wordnet.princeton.edu/
2046
We use RED-plus (REDp) to represent RED with extra resources, and the final score are calculated as
Formula (9), in which Fscore
p
is obtained using precison
p
and recall
p
as Formula (10).
REDp =
N
?
n=1
(w
ngram
? Fscore
p
n
) (9)
Fscore
p
=
precision
p
? recall
p
? ? precision
p
+ (1? ?) ? recall
p
(10)
precision
p
and recall
P
in Formula (10) are calculated as follows.
precision
p
=
score
par
n
+ score
dep
n
len
h
, recall
p
=
score
par
n
+ score
dep
n
count
n
(ref) + count
n
(par)
len
h
is the length of the translation. count
n(ref)
is the number of the dep-ngrams with the length of n
in the reference. count
n
(par) is the number of paraphrases with length of n in reference. score
par
n
is
the match score of paraphrase-ngrams with the length of n. score
dep
n
is the match score of dep-ngrams
with the length of n. score
par
n
and score
dep
n
are calculated as follows.
score
par
n
=
?
par?P
n
(1? w
par
? s
fum
) , score
dep
n
=
?
d?D
n
(p
(d,hyp)
? s
mod
? s
fun
)
P
n
is the set of paraphrase-ngrams with the length of n. D
n
is the set of dep-ngrams with the length of n.
4 Parameter Tuning
There are several parameters in REDp, and different parameter values can make the performance of
REDp different. For example,w
ngram
represents the weight of dep-ngram with the length of n. The
effect of ngrams with different lengths are different, and they shouldn?t have the same weight. So we can
tune the parameters to find their best values.
We try a preliminary optimization method to tune parameters in REDp. A heuristic search is employed
and the parameters are classified into two subsets. The parameter optimization is a grid search over the
two subsets of parameters. When searching Subset 1, the parameters in Subset 2 are fixed, and then
Subset 1 and Subset 2 are exchanged to finish this iteration. Several iterations are executed to finish the
parameter tuning process. This heuristic search may not find the global optimum but it can save a lot of
time compared with exhaustive search. The optimization goal is to maximize the sum of Spearman?s ?
rank correlation coefficient on system level and Kendall?s ? correlation coefficient on sentence level. ?
is calculated using the following equation.
? = 1?
6
?
d
2
i
n(n
2
? 1)
where d
i
is the difference between the human rank and metric?s rank for system i. n is the number of
systems. ? is calculated as follows.
? =
number of concordant pairs? number of discordant pairs
number of concordant pairs + number of discordant pairs
The data of into-English tasks in WMT 2010 are used to tune parameters. The tuned parameters are
listed in Table 1.
5 Experiments
5.1 Data
The test sets in experiments are WMT 2012 and WMT 2013. The language pairs are German-to-English
(de-en), Czech-to-English (cz-en), French-to-English (fr-en), Spanish-to-English (es-en) and Russian-to-
English (ru-en). The number of translation systems for each language pair are showed in Table 2. For
each language pair, there are 3003 sentences in WMT 2012 and 3000 sentences in WMT 2013.
2047
Parameter ? w
fun
w
exact
w
stem
w
syn
w
par
w
1gram
w
2gram
w
3gram
tuned values 0.9 0.2 0.9 0.6 0.6 0.6 0.6 0.5 0.1
Table 1: Parameter values after tuning on WMT 2010. ? is from Formula (10). w
fun
is the weight of
function word. w
exact
, w
stem
andw
syn
are the weights of the three match modules ?exact stem synonym?
respectively. w
par
is the weight of paraphrase-ngram. w
1gram
, w
2gram
and w
3gram
are the weights of
dep-ngram with the length of 1, 2 and 3 respectively.
Language pairs cz-en de-en es-en fr-en ru-en
WMT2012 6 16 12 15 -
WMT2013 12 23 17 19 23
Table 2: The number of translation systems for each language pair on WMT 2012 and WMT 2013.
We parsed the reference into constituent tree by Berkeley parser
2
and then converted the constituent
tree into dependency tree by Penn2Malt
3
. Presumably, the performance of the new metric will be better
if the dependency trees are labeled by human. Reference dependency trees are labeled only once and can
be used forever so it will not increase costs.
5.2 Baselines
In the experiments, we compare the performance of our metric with the widely-used lexicon-based met-
rics such as BLEU
4
, TER
5
and METEOR
6
, dependency-based metric HWCM and semantic-based metric
SEMPOS (Mach?a?cek and Bojar, 2011) which has the best performance on system level according to the
published results of WMT 2012.
The results of BLEU are obtained using 4-gram with smoothing option. The version of TER is 0.7.25.
The results of METEOR are obtained by Version 1.4 with task option ?rank?. We re-implement HWCM
which employs an epsilon value of 10
?3
to replace zero for smoothing purpose. The correlations of
SEMPOS are obtained from the published results of WMT 2012 and WMT 2013.
5.3 Experiment Results
The experiments on both system level and sentence level are carried out. On system level, the correlations
are calculated using Spearman?s rank correlation coefficient ? (Pirie, 1988). Kendall?s rank correlation
coefficient ? (Kendall, 1938) is employed to evaluate the sentence level correlation. Our method performs
best when the maximum length of dep-ngram is set to 3, so we only present the results with the maximum
length of 3. RED represents the new metric with exact match and the parameter values are set as follows.
? = 0.5. w
1gram
= w
2gram
= w
3gram
= 1/3. REDp represents the new metric with extra resources
and tuned parameter values which are listed in Table (1).
5.3.1 System level correlations
The system level correlations are shown in Table 3. RED is better than BLEU, TER and HWCM on
average on both WMT 2012 and WMT 2013, which reflects that using syntactic information and only
parsing the reference side are helpful. REDp gets the best result on all of the language pairs except
cz-en on WMT 2012. The significant improvement from RED to REDp illustrates the effect of extra
resources and the parameter tuning. Stem, synonym and paraphrase can enrich the reference and provide
extra knowledge for automatic evaluation metric. There are several parameters in REDp, and different
parameter values can make the performance of REDp different. So the performance can be optimized
through parameter tuning. SEMPOS got the best correlation according to the published results of WMT
2
http://code.google.com/p/berkeleyparser/downloads/list
3
http://stp.lingfil.uu.se/
?
nivre/research/Penn2Malt.html
4
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl
5
http://www.cs.umd.edu/
?
snover/tercom
6
http://www.cs.cmu.edu/
?
alavie/METEOR/download/meteor-1.4.tgz
2048
2012, and METEOR got the best correlation according to the published results of WMT 2013 on into-
English task on system level. REDp gets better result than SEMPOS and METEOR on both WMT 2012
and WMT 2013, so REDp achieves the state-of-the-art performance on system level.
data WMT 2012 WMT 2013
Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en ave
BLEU .886 .671 .874 .811 .811 .936 .895 .888 .989 .670 .876
TER .886 .624 .916 .821 .812 .800 .833 .825 .951 .581 .798
HWCM .943 .762 .937 .818 .865 .902 .904 .886 .951 .756 .880
METEOR .657 .885 .951 .843 .834 .964 .961 .979 .984 .789 .935
SEMPOS .943 .924 .937 .804 .902 .955 .919 .930 .938 .823 .913
RED 1.0 .759 .951 .818 .882 .964 .951 .930 .989 .725 .912
REDp .943 .947 .965 .843 .925 .982 .973 .986 .995 .800 .947
Table 3: System level correlations on WMT 2012 and WMT 2013. The value in bold is the best result in
each column. ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.
5.3.2 Sentence level correlations
The sentence level correlations on WMT 2012 and WMT 2013 are shown in Table 4. RED is better than
BLEU and HWCM on all the language pairs, which reflects the effectiveness of syntactic information
and only parsing the reference. By introducing extra resources and parameter tuning, REDp achieves
significant improvement over RED. Stem, synonym and paraphrase can enrich the reference and provide
extra knowledge for automatic evaluation metric. There are several parameters in REDp, and different
parameter values can make the performance of REDp different. A better performance can be exploited
through parameter tuning. From the results of REDp and METEOR, we can see that REDp gets the
comparable results with METEOR on sentence level on both WMT 2012 and WMT 2013.
data WMT 2012 WMT 2013
Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en ave
BLEU .157 .191 .189 .210 .187 .199 .220 .259 .224 .162 .213
HWCM .158 .207 .203 .204 .193 .187 .208 .247 .227 .175 .209
METEOR .212 .275 .249 .251 .247 .265 .293 .324 .264 .239 .277
RED .165 .218 .203 .221 .202 .210 .239 .292 .246 .196 .237
REDp .212 .271 .234 .250 .242 .259 .290 .323 .260 .223 .271
Table 4: Sentence level correlations on WMT 2012 and WMT 2013. The value in bold is the best result
in each column. ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.
6 Conclusion and Future Work
In this paper, we propose a reference dependency based automatic MT evaluation metric RED. The
new metric only uses the dependency trees of the reference, which avoids the parsing of the potentially
noisy translations. Both long distance dependency information and the local continuous ngrams are
captured by the new metric. The experiment results indicate that RED achieves better correlations than
BLEU, TER and HWCM on both system level and sentence level. REDp, the improved version of RED
through adding extra resources and preliminary parameter tuning, gets state-of-the-art results which are
better than METEOR and SEMPOS on system level. On sentence level, REDp gets the comparable
performance with METEOR.
In the future, we will use the dependency forest instead of the dependency tree to reduce the effect
of parsing errors. We will also apply RED and REDp to the tuning process of SMT to improve the
translation quality.
2049
Acknowledgements
The authors were supported by National Natural Science Foundation of China (Contract 61202216)
and National Natural Science Foundation of China (Contract 61379086). Qun Liu?s work was partially
supported by the Science Foundation Ireland (Grant No. 07/CE/I1142) as part of the CNGL at Dublin
City University. Sincere thanks to the three anonymous reviewers for their thorough reviewing and
valuable suggestions.
References
Boxing Chen and Roland Kuhn. 2011. Amber: A modified bleu, enhanced ranking metric. In Proceedings of
the Sixth Workshop on Statistical Machine Translation, pages 71?77, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Boxing Chen, Roland Kuhn, and George Foster. 2012. Improving amber, an mt evaluation metric. In Proceedings
of the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 59?63, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation
of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages
85?91. Association for Computational Linguistics.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguistic features for automatic evaluation of heterogenous mt systems.
In Proceedings of the Second Workshop on Statistical Machine Translation, pages 256?264. Association for
Computational Linguistics.
Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81?93.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an automatic metric for mt evaluation with high levels of
correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation,
StatMT ?07, pages 228?231, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,
pages 25?32.
Chi-kiu Lo and Dekai Wu. 2013. MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame
based MT evaluation metric. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages
422?428, Sofia, Bulgaria, August. Association for Computational Linguistics.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. 2012. Fully automatic semantic mt evaluation. In Pro-
ceedings of the Seventh Workshop on Statistical Machine Translation, pages 243?252, Montr?eal, Canada, June.
Association for Computational Linguistics.
Matou?s Mach?a?cek and Ond?rej Bojar. 2011. Approximating a deep-syntactic metric for mt evaluation and tun-
ing. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 92?98. Association for
Computational Linguistics.
Dennis Mehay and Chris Brew. 2007. BLEUTRE: Flattening Syntactic Dependencies for MT Evaluation. In
Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).
F.J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics-Volume 1, pages 160?167. Association for Computa-
tional Linguistics.
Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007. Dependency-based automatic evaluation for
machine translation. In Proceedings of the NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Sta-
tistical Translation, SSST ?07, pages 80?87, Stroudsburg, PA, USA. Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages
311?318. Association for Computational Linguistics.
W Pirie. 1988. Spearman rank correlation coefficient. Encyclopedia of statistical sciences.
2050
Martin F Porter. 2001. Snowball: A language for stemming algorithms.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-dependency statistical machine translation. Compu-
tational Linguistics, 36(4):649?671.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the
Americas, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie J Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or hter?:
exploring different human judgments with a tunable mt metric. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 259?268. Association for Computational Linguistics.
2051
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 213?218,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The CNGL-DCU-Prompsit Translation Systems for WMT13
Raphael Rubino?, Antonio Toral?, Santiago Cort?s Va?llo?,
Jun Xie?, Xiaofeng Wu?, Stephen Doherty[, Qun Liu?
?NCLT, Dublin City University, Ireland
?Prompsit Language Engineering, Spain
?ICT, Chinese Academy of Sciences, China
?,[CNGL, Dublin City University, Ireland
?,?{rrubino, atoral, xfwu, qliu}@computing.dcu.ie
?santiago@prompsit.com
?junxie@ict.ac.cn
[stephen.doherty@dcu.ie
Abstract
This paper presents the experiments con-
ducted by the Machine Translation group
at DCU and Prompsit Language Engineer-
ing for the WMT13 translation task. Three
language pairs are considered: Spanish-
English and French-English in both direc-
tions and German-English in that direc-
tion. For the Spanish-English pair, the use
of linguistic information to select paral-
lel data is investigated. For the French-
English pair, the usefulness of the small in-
domain parallel corpus is evaluated, com-
pared to an out-of-domain parallel data
sub-sampling method. Finally, for the
German-English system, we describe our
work in addressing the long distance re-
ordering problem and a system combina-
tion strategy.
1 Introduction
This paper presents the experiments conducted
by the Machine Translation group at DCU1 and
Prompsit Language Engineering2 for the WMT13
translation task on three language pairs: Spanish-
English, French-English and German-English.
For these language pairs, the language and trans-
lation models are built using different approaches
and datasets, thus presented in this paper in sepa-
rate sections.
In Section 2, the systems built for the Spanish-
English pair in both directions are described. We
investigate the use of linguistic information to se-
lect parallel data. In Section 3, we present the sys-
tems built for the French-English pair in both di-
1http://www.nclt.dcu.ie/mt/
2http://www.prompsit.com/
rections. The usefulness of the small in-domain
parallel corpus is evaluated, compared to an out-
of-domain parallel data sub-sampling method. In
Section 4, for the German-English system, aiming
at exploring the long distance reordering problem,
we first describe our efforts in a dependency tree-
to-string approach, before combining different hi-
erarchical systems with a phrase-based system and
show a significant improvement over three base-
line systems.
2 Spanish-English
This section describes the experimental setup for
the Spanish-English language pair.
2.1 Setting
Our setup uses the MOSES toolkit, version
1.0 (Koehn et al, 2007). We use a pipeline
with the phrase-based decoder with standard pa-
rameters, unless noted otherwise. The decoder
uses cube pruning (-cube-pruning-pop-limit 2000
-s 2000), MBR (-mbr-size 800 -mbr-scale 1) and
monotone at punctuation reordering.
Individual language models (LMs), 5-gram and
smoothed using a simplified version of the im-
proved Kneser-Ney method (Chen and Goodman,
1996), are built for each monolingual corpus using
IRSTLM 5.80.01 (Federico et al, 2008). These
LMs are then interpolated with IRSTLM using
the test set of WMT11 as the development set. Fi-
nally, the interpolated LMs are merged into one
LM preserving the weights using SRILM (Stol-
cke, 2002).
We use all the parallel corpora available for
this language pair: Europarl (EU), News Com-
mentary (NC), United Nations (UN) and Common
Crawl (CC). Regarding monolingual corpora, we
use the freely available monolingual corpora (Eu-
213
roparl, News Commentary, News 2007?2012) as
well as the target side of several parallel corpora:
Common Crawl, United Nations and 109 French?
English corpus (only for English as target lan-
guage). Both the parallel and monolingual data
are tokenised and truecased using scripts from the
MOSES toolkit.
2.2 Data selection
The main contribution in our participation regards
the selection of parallel data. We follow the
perplexity-based approach to filter monolingual
data (Moore and Lewis, 2010) extended to filter
parallel data (Axelrod et al, 2011). In our case, we
do not measure perplexity only on word forms but
also using different types of linguistic information
(lemmas and named entities) (Toral, 2013).
We build LMs for the source and target sides
of the domain-specific corpus (in our case NC)
and for a random subset of the non-domain-
specific corpus (EU, UN and CC) of the same size
(number of sentences) of the domain-specific cor-
pus. Each parallel sentence s in the non-domain-
specific corpus is then scored according to equa-
tion 1 where PPIsl(s) is the perplexity of s in
the source side according to the domain-specific
LM and PPOsl(s) is the perplexity of s in the
source side according to the non-domain-specific
LM. PPItl(s) and PPOtl(s) contain the corre-
sponding values for the target side.
score(s) = 12 ? (PPIsl(s)? PPOsl(s))
+(PPItl(s)? PPOtl(s)) (1)
Table 1 shows the results obtained using four
models: word forms (forms), forms and named en-
tities (forms+nes), lemmas (lem) and lemmas and
named entities (lem+nes). Details on these meth-
ods can be found in Toral (2013).
For each corpus we selected two subsets (see in
bold in Table 1), the one for which one method
obtained the best perplexity (top 5% of EU us-
ing forms, 2% of UN using lemmas and 50% of
CC using forms and named entities) and a big-
ger one used to compare the performance in SMT
(top 14% of EU using lemmas and named entities
(lem+nes), top 12% of UN using forms and named
entities and the whole CC). These subsets are used
as training data in our systems.
As we can see in the table, the use of lin-
guistic information allows to obtain subsets with
lower perplexity than using solely word forms, e.g.
1057.7 (lem+nes) versus 1104.8 (forms) for 14%
of EU. The only exception to this is the subset that
comprises the top 5% of EU, where perplexity us-
ing word forms (957.9) is the lowest one.
corpus size forms forms+nes lem lem+nes
EU 5% 957.9 987.2 974.3 1005.514% 1104.8 1058.7 1111.6 1057.7
UN 2% 877.1 969.6 866.6 962.212% 1203.2 1130.9 1183.8 1131.6
CC 50% 573.0 547.2 574.5 546.4100% 560.1 560.1 560.1 560.1
Table 1: Perplexities in data selection
2.3 Results
Table 2 presents the results obtained. Note that
these were obtained during development and thus
the systems are tuned on WMT?s 2011 test set and
tested on WMT?s 2012 test set.
All the systems share the same LM. The first
system (no selection) is trained with the whole NC
and EU. The second (small) and third (big) sys-
tems use as training data the whole NC and sub-
sets of EU (5% and 14%, respectively), UN (2%
and 12%, respectively) and CC (50% and 100%,
respectively), as shown in Table 1.
System #sent. BLEU BLEUcased
no selection 2.1M 31.99 30.96
small 1.4M 33.12 32.05
big 3.8M 33.49 32.43
Table 2: Number of sentences and BLEU scores
obtained on the WMT12 test set for the different
systems on the EN?ES translation task.
The advantage of data selection is clear. The
second system, although smaller in size compared
to the first (1.4M sentence pairs versus 2.1M),
takes its training from a more varied set of data,
and its performance is over one absolute BLEU
point higher.
When comparing the two systems that rely on
data selection, one might expect the one that uses
data with lower perplexity (small) to perform bet-
ter. However, this is not the case, the third system
(big) performing around half an absolute BLEU
point higher than the second (small). This hints
at the fact that perplexity alone is not an optimal
metric for data selection, but size should also be
considered. Note that the size of system 3?s phrase
table is more than double that of system 2.
214
3 French-English
This section describe the particularities of the MT
systems built for the French-English language pair
in both directions. The goal of the experimen-
tal setup presented here is to evaluate the gain of
adding small in-domain parallel data into a trans-
lation system built on a sub-sample of the out-of-
domain parallel data.
3.1 Data Pre-processing
All the available parallel and monolingual data for
the French-English language pair, including the
last versions of LDC Gigaword corpora, are nor-
malised and special characters are escaped using
the scripts provided by the shared task organisers.
Then, the corpora are tokenised and for each lan-
guage a true-case model is built on the concatena-
tion of all the data after removing duplicated sen-
tences, using the scripts included in MOSES dis-
tribution. The corpora are then true-cased before
being used to build the language and the transla-
tion models.
3.2 Language Model
To build our final language models, we first build
LMs on each corpus individually. All the monolin-
gual corpora are considered, as well as the source
or target side of the parallel corpora if the data
are not already in the monolingual data. We build
modified Kneser-Ney discounted 5-gram LMs us-
ing the SRILM toolkit for each corpus and sepa-
rate the LMs in three groups: one in-domain (con-
taining news-commentary and news crawl cor-
pora), another out-of-domain (containing Com-
mon Crawl, Europarl, UN and 109 corpora), and
the last one with LDC Gigaword LMs (the data
are kept separated by news source, as distributed
by LDC). The LMs in each group are linearly in-
terpolated based on their perplexities obtained on
the concatenation of all the development sets from
previous WMT translation tasks. The same devel-
opment corpus is used to linearly interpolate the
in-domain and LDC LMs. We finally obtain two
LMs, one containing out-of-domain data which is
only used to filter parallel data, and another one
containing in-domain data which is used to filter
parallel data, tuning the translation model weights
and at decoding time. Details about the number of
n-grams in each language model are presented in
Table 3.
French English
out in out in
1-gram 4.0 3.3 4.2 10.7
2-gram 43.0 44.0 48.2 161.9
3-gram 54.2 61.8 63.4 256.8
4-gram 99.7 119.2 103.2 502.7
5-gram 136.4 165.0 125.4 680.7
Table 3: Number of n-grams (in millions) for the
in-domain and out-of-domain LMs in French and
English.
3.3 Translation Model
Two phrase-based translation models are built
using MGIZA++ (Gao and Vogel, 2008) and
MOSES3, with the default alignment heuris-
tic (grow-diag-final) and bidirectional reordering
models. The first translation model is in-domain,
built with the news-commentary corpus. The sec-
ond one is built on a sample of all the other paral-
lel corpora available for the French-English lan-
guage pair. Both corpora are cleaned using the
script provided with Moses, keeping the sentences
with a length below 80 words. For the second
translation model, we used the modified Moore-
Lewis method based on the four LMs (two per
language) presented in section 3.2. The sum of
the source and target perplexity difference is com-
puted for each sentence pair of the corpus. We set
an acceptance threshold to keep a limited amount
of sentence pairs. The kept sample finally con-
tains ? 3.7M sentence pairs to train the translation
model. Statistics about this data sample and the
news-commentary corpus are presented in Table 4.
The test set of WMT12 translation task is used to
optimise the weights for the two translation mod-
els with the MERT algorithm. For this tuning step,
the limit of target phrases loaded per source phrase
is set to 50. We also use a reordering constraint
around punctuation marks. The same parameters
are used during the decoding of the test set.
news-commentary sample
tokens FR 4.7M 98.6M
tokens EN 4.0M 88.0M
sentences 156.5k 3.7M
Table 4: Statistics about the two parallel corpora,
after pre-processing, used to train the translation
models.
3Moses version 1.0
215
3.4 Results
The two translation models presented in Sec-
tion 3.3 allow us to design three translation sys-
tems: one using only the in-domain model, one
using only the model built on the sub-sample of
the out-of-domain data, and one using both mod-
els by giving two decoding paths to Moses. For
this latter system, the MERT algorithm is also used
to optimise the translation model weights. Results
obtained on the WMT13 test set, measured with
the official automatic metrics, are presented in Ta-
ble 5. The submitted system is the one built on
the sub-sample of the out-of-domain parallel data.
This system was chosen during the tuning step be-
cause it reached the highest BLEU scores on the
development corpus, slightly above the combina-
tion of the two translation models.
News-Com. Sample Comb.
FR-EN
BLEUdev 26.9 30.0 29.9
BLEU 27.0 30.8 30.4
BLEUcased 26.1 29.8 29.3
TER 62.9 58.9 59.3
EN-FR
BLEUdev 27.1 29.7 29.6
BLEU 26.6 29.6 29.4
BLEUcased 25.8 28.7 28.5
TER 65.1 61.8 62.0
Table 5: BLEU and TER scores obtained by our
systems. BLEUdev is the score obtained on the
development set given by MERT, while BLEU,
BLEUcased and TER are obtained on the test set
given by the submission website.
For both FR-EN and EN-FR tasks, the best re-
sults are reached by the system built on the sub-
sample taken from the out-of-domain parallel data.
Using only News-Commentary to build a trans-
lation model leads to acceptable BLEU scores,
with regards to the size of the training corpus.
When the sub-sample of the out-of-domain par-
allel data is used to build the translation model,
adding a model built on News-Commentary does
not improve the results. The difference between
these two systems in terms of BLEU score (both
cased sensitive and insensitive) indicates that sim-
ilar results can be achieved, however it appears
that the amount of sentence pairs in the sample
is large enough to limit the impact of the small
in-domain corpus parallel. Further experiments
are still required to determine the minimum sam-
ple size needed to outperform both the in-domain
system and the combination of the two translation
models.
4 German-English
In this section we describe our work on German
to English subtask. Firstly we describe the De-
pendency tree to string method which we tried but
unfortunately failed due to short of time. Secondly
we discuss the baseline system and the preprocess-
ing we performed. Thirdly a system combination
method is described.
4.1 Dependency Tree to String Method
Our original plan was to address the long distance
reordering problem in German-English transla-
tion. We use Xie?s Dependency tree to string
method(Xie et al, 2011) which obtains good re-
sults on Chinese to English translation and ex-
hibits good performance at long distance reorder-
ing as our decoder.
We use Stanford dependency parser4 to parse
the English side of the data and Mate-Tool5 for
the German side. The first set of experiments did
not lead to encouraging results and due to insuffi-
cient time, we decide to switch to other decoders,
based on statistical phrase-based and hierarchical
approaches.
4.2 Baseline System
In this section we describe the three baseline sys-
tem we used as well as the preprocessing technolo-
gies and the experiments set up.
4.2.1 Preprocessing and Corpus
We first use the normalisation scripts provided by
WMT2013 to normalise both English and Ger-
man side. Then we escape special characters on
both sides. We use Stanford tokeniser for English
and OpenNLP tokeniser6 for German. Then we
train a true-case model using with Europarl and
News-Commentary corpora, and true-case all the
corpus we used. The parallel corpus is filtered
with the standard cleaning scripts provided with
4http://nlp.stanford.edu/software/
lex-parser.shtml
5http://code.google.com/p/mate-tools/
6http://opennlp.sourceforge.net/
models-1.5/
216
MOSES. We split the German compound words
with jWordSplitter7.
All the corpus provided for the shared task are
used for training our translation models, while
WMT2011 and WMT2012 test sets are used to
tune the models parameters. For the LM, we
use all the monolingual data provided, including
LDC Gigaword. Each LM is trained with the
SRILM toolkit, before interpolating all the LMs
according to their weights obtained by minimiz-
ing the perplexity on the tuning set (WMT2011
and WMT2012 test sets). As SRILM can only
interpolate 10 LMs, we first interpolate a LM with
Europarl, News Commentary, News Crawl (2007-
2012, each year individually, 6 separate parts),
then we interpolate a new LM with this interpo-
lated LM and LDC Gigawords (we kept the Gi-
gaword subsets separated according to the news
sources as distributed by LDC, which leads to 7
corpus).
4.2.2 Three baseline systems
We use the data set up described by the former
subsection and build up three baseline systems,
namely PB MOSES (phrase-based), Hiero MOSES
(hierarchical) and CDEC (Dyer et al, 2010). The
motivation of choosing Hierarchical Models is to
address the German-English?s long reorder prob-
lem. We want to test the performance of CDEC and
Hiero MOSES and choose the best. PB MOSES is
used as our benchmark. The three results obtained
on the development and test sets for the three base-
line system and the system combination are shown
in the Table 6.
Development Test
PB MOSES 22.0 24.0
Hiero MOSES 22.1 24.4
CDEC 22.5 24.4
Combination 23.0 24.8
Table 6: BLEU scores obtained by our systems on
the development and test sets for the German to
English translation task.
From the Table 6 we can see that on develop-
ment set, CDEC performs the best, and its much
better than MOSES?s two decoder, but on test
set, Hiero MOSES and CDEC performs as well as
each other, and they both performs better than PB
Model.
7http://www.danielnaber.de/
jwordsplitter/
4.3 System Combination
We also use a word-level combination strat-
egy (Rosti et al, 2007) to combine the three trans-
lation hypotheses. To combine these systems, we
first use the Minimum Bayes-Risk (MBR) (Kumar
and Byrne, 2004) decoder to obtain the 5 best hy-
pothesis as the alignment reference for the Con-
fusion Network (CN) (Mangu et al, 2000). We
then use IHMM (He et al, 2008) to choose the
backbone build the CN and finally search for and
generate the best translation.
We tune the system parameters on development
set with Simple-Simplex algorithm. The param-
eters for system weights are set equal. Other pa-
rameters like language model, length penalty and
combination coefficient are chosen when we see a
good improvement on development set.
5 Conclusion
This paper presented a set of experiments con-
ducted on Spanish-English, French-English and
German-English language pairs. For the Spanish-
English pair, we have explored the use of linguistic
information to select parallel data and use this as
the training for SMT. However, the comparison of
the performance obtained using this method and
the purely statistical one (i.e. perplexity on word
forms) remains to be carried out. Another open
question regards the optimal size of the selected
data. As we have seen, minimum perplexity alone
cannot be considered an optimal metric since us-
ing a larger set, even if it has higher perplexity,
allowed us to obtain notably higher BLEU scores.
The question is then how to decide the optimal size
of parallel data to select.
For the French-English language pair, we inves-
tigated the usefulness of the small in-domain par-
allel data compared to out-of-domain parallel data
sub-sampling. We show that with a sample con-
taining ? 3.7M sentence pairs extracted from the
out-of-domain parallel data, it is not necessary to
use the small domain-specific parallel data. Fur-
ther experiments are still required to determine the
minimum sample size needed to outperform both
the in-domain system and the combination of the
two translation models.
Finally, for the German-English language pair,
we presents our exploitation of long ordering
problem. We compared two hierarchical models
with one phrase-based model, and we also use a
system combination strategy to further improve
217
the translation systems performance.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme FP7/2007-2013 under
grant agreement PIAP-GA-2012-324414 (Abu-
MaTran) and through Science Foundation Ireland
as part of the CNGL (grant 07/CE/I1142).
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7?12. Association for Computational
Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In INTER-
SPEECH, pages 1618?1621. ISCA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57. Association for
Computational Linguistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 98?107. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: word er-
ror minimization and other applications of confu-
sion networks. Computer Speech & Language,
14(4):373?400.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Antti-Veikko I Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple
machine translation systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 228?235.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In John H. L. Hansen and
Bryan L. Pellom, editors, INTERSPEECH. ISCA.
Antonio Toral. 2013. Hybrid Selection of Language
Model Training Data Using Linguistic Information
and Perplexity. In Proceedings of the Second Work-
shop on Hybrid Approaches to Machine Translation
(HyTra), ACL 2013.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 216?226. Association for Computational
Linguistics.
218
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 435?439,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
DCU Participation in WMT2013 Metrics Task
Xiaofeng Wu?, Hui Yu?,Qun Liu?
?CNGL, Dublin City University, Ireland
?ICT, Chinese Academy of Sciences, China
?{xfwu, qliu}@computing.dcu.ie
?yuhui@ict.ac.cn
Abstract
In this paper, we propose a novel syntac-
tic based MT evaluation metric which only
employs the dependency information in
the source side. Experimental results show
that our method achieves higher correla-
tion with human judgments than BLEU,
TER, HWCM and METEOR at both sen-
tence and system level for all of the four
language pairs in WMT 2010.
1 Introduction
Automatic evaluation plays a more important role
in the evolution of machine translation. At the ear-
liest stage, the automatic evaluation metrics only
use the lexical information, in which, BLEU (Pap-
ineni et al, 2002) is the most popular one. BLEU
is simple and effective. Most of the researchers
regard BLEU as their primary evaluation metric
to develop and compare MT systems. However,
BLEU only employs the lexical information and
cannot adequately reflect the structural level sim-
ilarity. Translation Error Rate (TER) (Snover et
al., 2006) measures the number of edits required to
change the hypothesis into one of the references.
METEOR (Lavie and Agarwal, 2007), which de-
fines loose unigram matching between the hypoth-
esis and the references with the help of stem-
ming and Wordnet-looking-up, is also a lexical
based method and achieves the first-class human-
evaluation-correlation score. AMBER (Chen and
Kuhn, 2011; Chen et al, 2012) incorporates recall,
extra penalties and some text processing variants
on the basis of BLEU. The main weakness of all
the above lexical based methods is that they cannot
adequately reflect the structural level similarity.
To overcome the weakness of the lexical based
methods, many syntactic based metrics were pro-
posed. Liu and Gildea (2005) proposed STM, a
constituent tree based approach, and HWCM, a
dependency tree based approach.
Both of the two methods compute the similar-
ity between the sub-trees of the hypothesis and the
reference. Owczarzak et al(2007a; 2007b; 2007c)
presented a method using the Lexical-Functional
Grammar (LFG) dependency tree. MAXSIM
(Chan and Ng, 2008) and the method proposed
by Zhu et al(2010) also employed the syntac-
tic information in association with lexical infor-
mation.With the syntactic information which can
reflect structural information, the correlation with
the human judgments can be improved to a certain
extent.
As we know that the hypothesis is potentially
noisy, and these errors expand through the parsing
process. Thus the power of syntactic information
could be considerably weakened.
In this paper, we attempt to overcome the short-
coming of the syntactic based methods and pro-
pose a novel dependency based MT evaluation
metric. The proposed metric only employs the ref-
erence dependency tree which contains both the
lexical and syntactic information, leaving the hy-
pothesis side unparsed to avoid the error propaga-
tion. In our metric, F-score is calculated using the
string of hypothesis and the dependency based n-
grams which are extracted from the reference de-
pendency tree.
Experimental results show that our method
achieves higher correlation with human judgments
than BLEU, HWCM, TER and METEOR at both
sentence level and system level for all of the four
language pairs in WMT 2010.
2 Background: HWCM
HWCM is a dependency based metric which ex-
tracts the headword chains, a sequence of words
which corresponds to a path in the dependency
tree, from both the hypothesis and the reference
dependency tree. The score of HWCM is obtained
435
Figure 1: The dependency tree of the reference
Figure 2: The dependency tree of the hypothesis
by formula (1).
HWCM = 1D
D?
n=1
?
g?chainn(hyp) countclip(g)?
g?chainn(hyp) count(g)
(1)
In formula (1), D is the maximum length of the
headword chain. chainn(hyp) denotes the set of
the headword chains with length of n in the tree of
hypothesis. count(g) denotes the number of times
g appears in the headword chain of the hypothe-
sis dependency tree and countclip(g) denotes the
clipped number of times when g appears in the the
headword chain of the reference dependency trees.
Clipped means that the count computed from the
headword chain of the hypothesis tree should not
exceed the maximum number of times when g oc-
curs in headword chain of any single reference
tree. The following are two sentences represent-
ing as reference and hypothesis, and Figure 1 and
Figure 2 are the dependency trees respectively.
reference: It is not for want of trying .
hypothesis: This is not for lack of trying .
In the example above, there are 8 1-word, 7 2-
word and 3 3-word headword chains in the hy-
pothesis dependency tree. The number of 1-word
and 2-word headword chains in the hypothesis tree
which can match their counterparts in the refer-
ence tree is 5 and 4 respectively. The 3-word head-
word chains in the hypothesis dependency tree are
is for lack, for lack of and lack of trying. Due to
the difference in the dependency structures, they
have no matches in the reference side.
3 A Novel Dependency Based MT
Evaluation Method
In this new method, we calculate F-score using the
string of hypothesis and the dep-n-grams which
are extracted from the reference dependency tree.
The new method is named DEPREF since it is
a DEPendency based method only using depen-
dency tree of REference to calculate the F-score.
In DEPREF, after the parsing of the reference sen-
tences, there are three steps below being carried
out. 1) Extracting the dependency based n-gram
(dep-n-gram) in the dependency tree of the refer-
ence. 2) Matching the dep-n-gram with the string
of hypothesis. 3) Obtaining the final score of a hy-
pothesis. The detail description of our method will
be found in paper (Liu et al, 2013) . We only give
the experiment results in this paper.
4 Experiments
Both the sentence level evaluation and the system
level evaluation are conducted to assess the per-
formance of our automatic metric. At the sentence
level evaluation, Kendall?s rank correlation coeffi-
cient ? is used. At the system level evaluation, the
Spearman?s rank correlation coefficient ? is used.
4.1 Data
There are four language pairs in our experiments
including German-to-English, Czech-to-English,
French-to-English and Spanish-to-English, which
are all derived from WMT2010. Each of the
four language pairs consists of 2034 sentences and
the references of the four language pairs are the
same. There are 24 translation systems for French-
to-English, 25 for German-to-English, 12 for
Czech-to-English and 15 for Spanish-to-English.
We parsed the reference into constituent tree by
Berkeley parser and then converted the constituent
tree into dependency tree by Penn2Malt 1. Pre-
sumably, we believe that the performance will be
even better if the dependency trees are manually
revised.
In the experiments, we compare the perfor-
mance of our metric with the widely used lexical
based metrics BLEU, TER, METEOR and a de-
pendency based metric HWCM. In order to make
a fair comparison with METEOR which is known
to perform best when external resources like stem
and synonym are provided, we also provide results
of DEPREF with external resources.
1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
436
Metrics Czech-English German-English Spanish-English French-English
BLEU 0.2554 0.2748 0.2805 0.2197
TER 0.2526 0.2907 0.2638 0.2105
HWCM
N=1 0.2067 0.2227 0.2188 0.2022
N=2 0.2587 0.2601 0.2408 0.2399
N=3 0.2526 0.2638 0.2570 0.2498
N=4 0.2453 0.2672 0.2590 0.2436
DEPREF 0.3337 0.3498 0.3190 0.2656
Table 1.A Sentence level correlations of the metrics without external resources.
Metrics Czech-English German-English Spanish-English French-English
METEOR 0.3186 0.3482 0.3258 0.2745
DEPREF 0.3281 0.3606 0.3326 0.2834
Table 1.B Sentence level correlations of the metrics with stemming and synonym.
Table 1: The sentence level correlations with the human judgments for Czech-to-English, German-to-
English, Spanish-to-English and French-to-English. The number in bold is the maximum value in each
column. N stands for the max length of the headword chains in HWCM in Table 1.A.
4.2 Sentence-level Evaluation
Kendall?s rank correlation coefficient ? is em-
ployed to evaluate the correlation of all the MT
evaluation metrics and human judgements at the
sentence level. A higher value of ? means a bet-
ter ranking similarity with the human judges. The
correlation scores of the four language pairs and
the average scores are shown in Table 1.A (without
external resources) and Table 1.B (with stemming
and synonym), Our method performs best when
maximum length of dep-n-gram is set to 3, so we
present only the results when the maximum length
of dep-n-gram equals 3.
From Table 1.A, we can see that all our methods
are far more better than BLEU, TER and HWCM
when there is no external resources applied on all
of the four language pairs. In Table 1.B, external
resources is considered. DEPREF is also better
than METEOR on the four language pairs. From
the comparison between Table 1.A and Table 1.B,
we can conclude that external resources is help-
ful for DEPREF on most of the language pairs.
When comparing DEPREF without external re-
sources with METEOR, we find that DEPREF ob-
tains better results on Czech-English and German-
English.
4.3 System-level Evaluation
We also evaluated the metrics with the human
rankings at the system level to further investigate
the effectiveness of our metrics. The matching of
the words in DEPREF is correlated with the posi-
tion of the words, so the traditional way of com-
puting system level score, like what BLEU does,
is not feasible for DEPREF. Therefore, we resort
to the way of adding the sentence level scores to-
gether to obtain the system level score. At system
level evaluation, we employ Spearman?s rank cor-
relation coefficient ?. The correlations of the four
language pairs and the average scores are shown
in Table 2.A (without external resources) and Ta-
ble 2.B (with stem and synonym).
From Table 2.A, we can see that the correla-
tion of DEPREF is better than BLEU, TER and
HWCM on German-English, Spanish-English and
French-English. On Czech-English, our metric
DEPREF is better than BLEU and TER. In Table
2.B (with stem and synonym), DEPREF obtains
better results than METEOR on all of the language
pairs except one case that DEPREF gets the same
result as METEOR on Czech-English. When com-
paring DEPREF without external resources with
METEOR, we can find that DEPREF gets bet-
ter result than METEOR on Spanish-English and
French-English.
From Table 1 and Table 2, we can conclude
that, DEPREF without external resources can ob-
tain comparable result with METEOR, and DE-
PREF with external resources can obtain better re-
sults than METEOR. The only exception is that at
the system level evaluation, Czech-English?s best
score is abtained by HWCM. Notice that there are
only 12 systems in Czech-English, which means
there are only 12 numbers to be sorted, we believe
437
Metrics Czech-English German-English Spanish-English French-English
BLEU 0.8400 0.8808 0.8681 0.8391
TER 0.7832 0.8923 0.9033 0.8330
HWCM
N=1 0.8392 0.7715 0.7231 0.6730
N=2 0.8671 0.8600 0.7670 0.8026
N=3 0.8811 0.8831 0.8286 0.8209
N=4 0.8811 0.9046 0.8242 0.8148
DEPREF 0.8392 0.9238 0.9604 0.8687
Table 2.A System level correlations of the metrics without external resources.
Metrics Czech-English German-English Spanish-English French-English
METEOR 0.8392 0.9269 0.9516 0.8652
DEPREF 0.8392 0.9331 0.9692 0.8730
Table 2.B System level correlations of the metrics with stemming and synonym.
Table 2: The system level correlations with the human judgments for Czech-to-English, German-to-
English, Spanish-to-English and French-to-English. The number in bold is the maximum value in each
column. N stands for the max length of the headword chains in HWCM in Table 2.A.
the spareness issure is more serious in this case.
5 Conclusion
In this paper, we propose a new automatic MT
evaluation method DEPREF. The experiments are
carried out at both sentence-level and system-level
using four language pairs from WMT 2010. The
experiment results indicate that DEPREF achieves
better correlation than BLEU, HWCM, TER and
METEOR at both sentence level and system level.
References
Yee Seng Chan and Hwee Tou Ng. 2008. Maxsim: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62.
Boxing Chen and Roland Kuhn. 2011. Amber: A
modified bleu, enhanced ranking metric. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 71?77, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Boxing Chen, Roland Kuhn, and George Foster. 2012.
Improving amber, an mt evaluation metric. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, WMT ?12, pages 59?63, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT ?07, pages 228?231, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization, pages 25?32.
Q. Liu, H. Yu, X. Wu, J. Xie, Y. Lu, and S. Lin.
2013. A Novel Dependency Based MT Evaluation
Method. Under Review.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007a. Dependency-based automatic eval-
uation for machine translation. In Proceedings of
the NAACL-HLT 2007/AMTA Workshop on Syntax
and Structure in Statistical Translation, SSST ?07,
pages 80?87, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007b. Evaluating machine translation with
lfg dependencies. Machine Translation, 21(2):95?
119, June.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007c. Labelled dependencies in machine
translation evaluation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
StatMT ?07, pages 104?111, Stroudsburg, PA, USA.
Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th annual
meeting on association for computational linguis-
tics, pages 311?318. Association for Computational
Linguistics.
438
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Junguo Zhu, Muyun Yang, Bo Wang, Sheng Li, and
Tiejun Zhao. 2010. All in strings: a powerful string-
based automatic mt evaluation metric with multi-
ple granularities. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, COLING ?10, pages 1533?1540, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
439
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 136?141,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The DCU-ICTCAS MT system at WMT 2014 on German-English
Translation Task
Liangyou Li
?
, Xiaofeng Wu
?
, Santiago Cort
?
es Va??llo
?
Jun Xie
?
, Andy Way
?
, Qun Liu
??
?
CNGL Centre for Global Intelligent Content, School of Computing
Dublin City University, Dublin 9, Ireland
?
Key Laboratory of Intelligent Information Processing, Institute of Computing Technology
Chinese Academy of Sciences, Beijing, China
{liangyouli,xiaofengwu,scortes,away,qliu}@computing.dcu.ie
xiejun@ict.ac.cn
Abstract
This paper describes the DCU submis-
sion to WMT 2014 on German-English
translation task. Our system uses phrase-
based translation model with several pop-
ular techniques, including Lexicalized
Reordering Model, Operation Sequence
Model and Language Model interpolation.
Our final submission is the result of sys-
tem combination on several systems which
have different pre-processing and align-
ments.
1 Introduction
On the German-English translation task of WMT
2014, we submitted a system which is built with
Moses phrase-based model (Koehn et al., 2007).
For system training, we use all provided
German-English parallel data, and conducted sev-
eral pre-processing steps to clean the data. In ad-
dition, in order to improve the translation quali-
ty, we adopted some popular techniques, includ-
ing three Lexicalized Reordering Models (Axel-
rod et al., 2005; Galley and Manning, 2008), a 9-
gram Operation Sequence Model (Durrani et al.,
2011) and Language Model interpolation on sev-
eral datasets. And then we use system combina-
tion on several systems with different settings to
produce the final outputs.
Our phrase-based systems are tuned with k-best
MIRA (Cherry and Foster, 2012) on development
set. We set the maximum iteration to be 25.
The Language Models in our systems are
trained with SRILM (Stolcke, 2002). We trained
Corpus Filtered Out (%)
Bilingual 7.17
Monolingual (English) 1.05
Table 1: Results of language detection: percentage
of filtered out sentences
a 5-gram model with Kneser-Ney discounting
(Chen and Goodman, 1996).
In the next sections, we will describe our system
in detail. In section 2, we will explain our pre-
processing steps on corpus. Then in section 3, we
will describe some techniques we have tried for
this task and the experiment results. In section 4,
our final configuration for submitted system will
be presented. And we conclude in the last section.
2 Pre-processing
We use all the training data for German-English
translation, including Europarl, News Commen-
tary and Common Crawl. The first thing we no-
ticed is that some Non-German and Non-English
sentences are included in our training data. So we
apply Language Detection (Shuyo, 2010) for both
monolingual and bilingual corpora. For mono-
lingual data (only including English sentences in
our task), we filter out sentences which are detect-
ed as other language with probability more than
0.999995. And for bilingual data, A sentence
pair is filtered out if the language detector detect-
s a different language with probability more than
0.999995 on either the source or the target. The
filtering results are given in Table 1.
In our experiment, German compound word-
s are splitted based on frequency (Koehn and
136
Knight, 2003). In addition, for both monolingual
and bilingual data, we apply tokenization, nor-
malizing punctuation and truecasing using Moses
scripts. For parallel training data, we also filter out
sentence pairs containing more than 80 tokens on
either side and sentence pairs whose length ratio
between source and target side is larger than 3.
3 Techniques
In our preliminary experiments, we take newstest
2013 as our test data and newstest 2008-2012 as
our development data. In total, we have more
than 10,000 sentences for tuning. The tuning step
would be very time-consuming if we use them al-
l. So in this section, we use Feature Decay Al-
gorithm (FDA) (Bic?ici and Yuret, 2014) to select
2000 sentences as our development set. Table 2
shows that system performance does not increase
with larger tuning set and the system using only
2K sentences selected by FDA is better than the
baseline tuned with all the development data.
In this section, alignment model is trained
by MGIZA++ (Gao and Vogel, 2008) with
grow-diag-final-and heuristic function.
And other settings are mostly default values in
Moses.
3.1 Lexicalized Reordering Model
German and English have different word order
which brings a challenge in German-English ma-
chine translation. In our system, we adopt three
Lexicalized Reordering Models (LRMs) for ad-
dressing this problem. They are word-based LRM
(wLRM), phrase-based LRM (pLRM) and hierar-
chal LRM (hLRM).
These three models have different effect on the
translation. Word-based and phrase-based LRMs
are focus on local reordering phenomenon, while
hierarchical LRM could be applied into longer re-
ordering problem. Figure 1 shows the differences
(Galley and Manning, 2008). And Table 3 shows
effectiveness of different LRMs.
In our system based on Moses, we
use wbe-msd-bidirectional-fe,
phrase-msd-bidirectional-fe and
hier-mslr-bidirectional-fe to specify
these three LRMs. From Table 2, we could see
that LRMs significantly improves the translation.
Figure 1: Occurrence of a swap according to
the three orientation models: word-based, phrase-
based, and hierarchical. Black squares represen-
t word alignments, and gray squares represen-
t blocks identified by phrase-extract. In (a), block
b
i
= (e
i
, f
a
i
) is recognized as a swap according to
all three models. In (b), b
i
is not recognized as a
swap by the word-based model. In (c), b
i
is rec-
ognized as a swap only by the hierarchical model.
(Galley and Manning, 2008)
3.2 Operation Sequence Model
The Operation Sequence Model (OSM) (Durrani
et al., 2011) explains the translation procedure as
a linear sequence of operations which generates
source and target sentences in parallel. Durrani
et al. (2011) defined four translation operations:
Generate(X,Y), Continue Source Concept, Gener-
ate Source Only (X) and Generate Identical, as
well as three reordering operations: Insert Gap,
Jump Back(W) and Jump Forward. These oper-
ations are described as follows.
? Generate(X,Y) make the words in Y and the
first word in X added to target and source
string respectively.
? Continue Source Concept adds the word in
the queue from Generate(X,Y) to the source
string.
? Generate Source Only (X) puts X in the
source string at the current position.
? Generate Identical generates the same word
for both sides.
? Insert Gap inserts a gap in the source side for
future use.
? Jump Back (W) makes the position for trans-
lation be the Wth closest gap to the current
position.
? Jump Forward moves the position to the in-
dex after the right-most source word.
137
Systems Tuning Set newstest 2013
Baseline ? 24.1
+FDA ? 24.2
+LRMs 24.0 25.4
+OSM 24.4 26.2
+LM Interpolation 24.6 26.4
+Factored Model ? 25.9
+Sparse Feature 25.6 25.9
+TM Combination 24.1 25.4
+OSM Interpolation 24.4 26.0
Table 2: Preliminary results on tuning set and test set (newstest 2013). All scores on test set are case-
sensitive BLEU[%] scores. And scores on tuning set are case-insensitive BLEU[%] directly from tuning
result. Baseline uses all the data from newstest 2008-2012 for tuning.
Systems Tuning Set (uncased) newstest 2013
Baseline+FDA ? 24.2
+wLRM 23.8 25.1
+pLRM 23.9 25.2
+hLRM 24.0 25.4
+pLRM 23.8 25.1
+hLRM 23.7 25.2
Table 3: System BLEU[%] scores when different LRMs are adopted.
The probability of an operation sequence O =
(o
1
o
2
? ? ? o
J
) is:
p(O) =
J
?
j=1
p(o
j
|o
j?n+1
? ? ? o
j?1
) (1)
where n indicates the number of previous opera-
tions used.
In this paper we train a 9-gram OSM on train-
ing data and integrate this model directly into log-
linear framework (OSM is now available to use
in Moses). Our experiment shows OSM improves
our system by about 0.8 BLEU (see Table 2).
3.3 Language Model Interpolation
In our baseline, Language Model (LM) is trained
on all the monolingual data provided. In this sec-
tion, we try to build a large language model by in-
cluding data from English Gigaword fifth edition
(only taking partial data with size of 1.6G), En-
glish side of UN corpus and English side of 10
9
French-English corpus. Instead of training a s-
ingle model on all data, we interpolate language
models trained on each subset (monolingual data
provided is splitted into three parts: News 2007-
2013, Europarl and News Commentary) by tuning
weights to minimize perplexity of language model
measured on the target side of development set.
In our experiment, after interpolation, the lan-
guage model doesn?t get a much lower perplexity,
but it slightly improves the system, as shown in
Table 2.
3.4 Other Tries
In addition to the techniques mentioned above, we
also try some other approaches. Unfortunately al-
l of these methods described in this section are
non-effective in our experiments. The results are
shown in Table 2.
? Factored Model (Koehn and Hoang, 2007):
We tried to integrate a target POS factored
model into our system with a 9-gram POS
language model to address the problem of
word selection and word order. But ex-
periment doesn?t show improvement. The
English POS is from Stanford POS Tagger
(Toutanova et al., 2003).
? Translation Model Combination: In this ex-
periment, we try to use the method of (Sen-
nrich, 2012) to combine phrase tables or re-
ordering tables from different subsets of data
138
to minimize perplexity measured on develop-
ment set. We try to split the training data in
two ways. One is according to data source,
resulting in three subsets: Europarl, News
Commentary and Common Crawl. Another
one is to use data selection. We use FDA to
select 200K sentence pairs as in-domain data
and the rest as out-domain data. Unfortunate-
ly both experiments failed. In Table 2, we on-
ly report results of phrase table combination
on FDA-based data sets.
? OSM Interpolation: Since OSM in our sys-
tem could be taken as a special language
model, we try to use the idea of interpolation
similar with language model to make OSM
adapted to some data. Training data are s-
plitted into two subsets with FDA. We train
9-gram OSM on each subsets and interpolate
them according to OSM trained on the devel-
opment set.
? Sparse Features: For each source phrase,
there is usually more than one corresponding
translation option. Each different translation
may be optimal in different contexts. Thus
in our systems, similar to (He et al., 2008)
which proposed a Maximum Entropy-based
rule selection for the hierarchical phrase-
based model, features which describe the
context of phrases, are designed to select the
right translation. But different with (He et
al., 2008), we use sparse features to mod-
el the context. And instead of using syn-
tactic POS, we adopt independent POS-like
features: cluster ID of word. In our experi-
ment mkcls was used to cluster words into 50
groups. And all features are generalized to
cluster ID.
4 Submission
Based on our preliminary experiments in the sec-
tion above, we use LRMs, OSM and LM inter-
polation in our final system for newstest 2014.
But as we find that Language Models trained on
UN corpus and 10
9
French-English corpus have
a very high perplexity and in order to speed up
the translation by reducing the model size, in this
section, we interpolate only three language model-
s from monolingual data provided, English Giga-
word fifth edition and target side of training data.
In addition, we also try some different methods for
final submission. And the results are shown in Ta-
ble 4.
? Development Set Selection: Instead of using
FDA which is dependent on test set, we use
the method of (Nadejde et al., 2013) to se-
lect tuning set from newstest 2008-2013 for
the final system. We only keep 2K sentences
which have more than 30 words and higher
BLEU score. The experiment result is shown
in Table 4 ( The system is indicated as Base-
line).
? Pre-processing: In our preliminary exper-
iments, sentences are tokenized without
changing hyphen. Thus we build another sys-
tem where all the hyphens are tokenized ag-
gressively.
? SyMGIZA++: Better alignment could lead to
better translation. So we carry out some ex-
periments on SyMGIZA++ aligner (Junczys-
Dowmunt and Sza, 2012), which modifies the
original IBM/GIZA++ word alignment mod-
els to allow to update the symmetrized mod-
els between chosen iterations of the original
training algorithms. Experiment shows this
new alignment improves translation quality.
? Multi-alignment Selection: We also try to use
multi-alignment selection (Tu et al., 2012)
to generate a ?better? alignment from three
alignmens: MGIZA++ with function grow-
diag-final-and, SyMGIZA++ with function
grow-diag-final-and and fast alignment (Dy-
er et al., 2013). Although this method show
comparable or better result on development
set, it fails on test set.
Since we build a few systems with different
setting on Moses phrase-based model, a straight-
forward thinking is to obtain the better transla-
tion from several different translation systems. So
we use system combination (Heafield and Lavie,
2010) on the 1-best outputs of three systems (in-
dicated with
?
in table 4). And this results in our
best system so far, as shown in Table 4. In our final
submission, this result is taken as primary.
5 Conclusion
This paper describes our submitted system to
WMT 2014 in detail. This system is based on
139
Systems Tuning Set newstest 2014
Baseline
?
34.2 25.6
+SyMGIZA++
?
34.3 26.0
+Multi-Alignment Selection 34.4 25.6
+Hyphen-Splitted 33.9 25.9
+SyMGIZA++
?
34.0 26.0
+Multi-Alignment Selection 34.0 25.7
System Combination ? 26.5
Table 4: Experiment results on newstest 2014. We report case-sensitive BLEU[%] score on test set and
case-insensitive BLEU[%] on tuning set which is directly from tuning result. Baseline is the phrase-based
system with LRMs, OSM and LM interpolation on smaller datasets, tuned with selected development set.
Systems indicated with
?
are used for system combination.
Moses phrase-based model, and integrates Lexi-
calized Reordering Models, Operation Sequence
Model and Language Model interpolation. Al-
so system combination is used on several system-
s which have different pre-processing and align-
ment.
Acknowledgments
This work is supported by EC Marie-Curie initial
training Network EXPERT (EXPloiting Empiri-
cal appRoaches to Translation) project (http:
//expert-itn.eu). Thanks to Johannes Lev-
eling for his help on German compound splitting.
And thanks to Jia Xu and Jian Zhang for their ad-
vice and help on this paper and experiments.
References
Amittai Axelrod, Ra Birch Mayne, Chris Callison-
burch, Miles Osborne, and David Talbot. 2005. Ed-
inburgh system description for the 2005 iwslt speech
translation evaluation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT).
Ergun Bic?ici and Deniz Yuret. 2014. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP).
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAA-
CL HLT ?12, pages 427?436, Stroudsburg, PA, US-
A. Association for Computational Linguistics.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 1045?1054, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of NAACL.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?
57, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 321?328, Manchester, UK,
August. Coling 2008 Organizing Committee.
Kenneth Heafield and Alon Lavie. 2010. Combining
machine translation output with open source: The
Carnegie Mellon multi-engine machine translation
scheme. The Prague Bulletin of Mathematical Lin-
guistics, 93:27?36, January.
Marcin Junczys-Dowmunt and Arkadiusz Sza. 2012.
Symgiza++: Symmetrized word alignment models
for statistical machine translation. In Pascal Bouvry,
MieczysawA. Kopotek, Franck Leprvost, Magorza-
ta Marciniak, Agnieszka Mykowiecka, and Henryk
140
Rybiski, editors, Security and Intelligent Informa-
tion Systems, volume 7053 of Lecture Notes in Com-
puter Science, pages 379?390. Springer Berlin Hei-
delberg.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the Tenth Conference on European Chapter of the
Association for Computational Linguistics - Volume
1, EACL ?03, pages 187?193, Stroudsburg, PA, US-
A. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Maria Nadejde, Philip Williams, and Philipp Koehn.
2013. Edinburgh?s syntax-based machine transla-
tion systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 170?
176, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 539?549,
Avignon, France, April. Association for Computa-
tional Linguistics.
Nakatani Shuyo. 2010. Language detection library for
java.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proceedings of the Internation-
al Conference Spoken Language Processing, pages
901?904, Denver, CO.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ?03, pages 173?180, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012. Combining mul-
tiple alignments to improve machine translation. In
COLING (Posters), pages 1249?1260.
141
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215?220,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
DCU-Lingo24 Participation in WMT 2014 Hindi-English Translation task
Xiaofeng Wu, Rejwanul Haque*, Tsuyoshi Okita
Piyush Arora, Andy Way, Qun Liu
CNGL, Centre for Global Intelligent Content
School of Computing, Dublin City University
Dublin 9, Ireland
{xf.wu,tokita,parora,away,qliu}@computing.dcu.ie
*Lingo24, Edinburgh, UK
rejwanul.haque@lingo24.com
Abstract
This paper describes the DCU-Lingo24
submission to WMT 2014 for the Hindi-
English translation task. We exploit
miscellaneous methods in our system,
including: Context-Informed PB-SMT,
OOV Word Conversion (OWC), Multi-
Alignment Combination (MAC), Oper-
ation Sequence Model (OSM), Stem-
ming Align and Normal Phrase Extraction
(SANPE), and Language Model Interpola-
tion (LMI). We also describe various pre-
processing steps we tried for Hindi in this
task.
1 Introduction
This paper describes the DCU-Lingo24 submis-
sion to WMT 2014 for the Hindi-English transla-
tion task.
All our experiments on WMT 2014 are built
upon the Moses phrase-based model (PB-SMT)
(Koehn et al., 2007) and tuned with MERT
(Och, 2003). Starting from this baseline system,
we exploit various methods including Context-
Informed PB-SMT (CIPBSMT), zero-shot learn-
ing (Palatucci et al., 2009) using neural network-
based language modelling (Bengio et al., 2000;
Mikolov et al., 2013) for OOV word conversion,
various lexical reordering models (Axelrod et al.,
2005; Galley and Manning, 2008), various Mul-
tiple Alignment Combination (MAC) (Tu et al.,
2012), Operation Sequence Model (OSM) (Dur-
rani et al., 2011) and Language Model Interpola-
tion(LMI).
In the next section, the preprocessing steps are
explained. In Section 3 a detailed explanation of
the technique we exploit is provided. Then in Sec-
tion 4, we provide our experimental results and re-
sultant discussion.
2 Pre-processing Steps
We use all the training data provided for Hindi?
English translation. Following Bojar et al. (2010),
we apply a number of normalisation methods on
the Hindi corpus. The HindEnCorp parallel cor-
pus compiles several sources of parallel data. We
observe that the source-side (Hindi) of the TIDES
data source contains font-related noise, i.e. many
Hindi sentences are a mixture of two different en-
codings: UTF-8
1
and WX
2
notations. We pre-
pared a WX-to-UTF-8 font conversion script for
Hindi which converts all WX encoded characters
into UTF-8, thus removing all WX encoding ap-
pearing in the TIDES data.
We also observe that a portion of the English
training corpus contained the following bracket-
like sequences of characters: -LRB-, -LSB-, -
LCB-, -RRB-, -RSB-, and -RCB-.
3
For consis-
tency, those character sequences in the training
data were replaced by the corresponding brackets.
For English ? both monolingual and the target
side of the bilingual data ? we perform tokeniza-
tion, normalization of punctuation, and truecasing.
For parallel training data, we filter sentences pairs
containing more than 80 tokens on either side and
1
http://en.wikipedia.org/wiki/UTF-8
2
http://en.wikipedia.org/wiki/WX_notation
3
The acronyms stand for (Left|Right)
(Round|Square|Curly) Bracket.
215
sentence pairs with length difference larger than 3
times.
3 Techniques Deployed
3.1 Combination of Various Lexical
Reordering Model (LRM)
Clearly, Hindi and English have quite different
word orders, so we adopt three lexical reordering
models to address this problem. They are word-
based LRM and phrase-based LRM, which mainly
focus on local reordering phenomena, and hierar-
chical phrase-based LRM, which mainly focuses
on longer distance reordering (Galley and Man-
ning, 2008).
3.2 Operation Sequence Model
The Operation Sequence Model (OSM) of Dur-
rani et al. (2011) defines four translation opera-
tions: Generate(X,Y), Continue Source Concept,
Generate Source Only (X) and Generate Identical,
as well as three reordering operations: Insert Gap,
Jump Back(W) and Jump Forward.
The probability of an operation sequence O =
(o
1
o
2
? ? ? o
J
) is calculated as in (1):
p(O) =
J
?
j=1
p(o
j
|o
j?n+1
? ? ? o
j?1
) (1)
where n indicates the number of previous opera-
tions used.
We employ a 9-order OSM in our framework.
3.3 Language Model Interpolation (LMI)
We build a large language model by including data
from the English Gigaword fifth edition, the En-
glish side of the UN corpus, the English side of the
10
9
French?English corpus and the English side of
the Hindi?English parallel data provided by the or-
ganisers. We interpolate language models trained
using each dataset, with the monolingual data pro-
vided split into three parts (news 2007-2013, Eu-
roparl (?) and news commentary) and the weights
tuned to minimize perplexity on the target side of
the devset.
The language models in our systems are trained
with SRILM (Stolcke, 2002). We train a 5-gram
model with Kneser-Ney discounting (Chen and
Goodman, 1996).
3.4 Context-informed PB-SMT
Haque et al. (2011) express a context-dependent
phrase translation as a multi-class classification
problem, where a source phrase with given addi-
tional context information is classified into a dis-
tribution over possible target phrases. The size of
this distribution needs to be limited, and would
ideally omit irrelevant target phrase translations
that the standard PB-SMT (Koehn et al., 2003) ap-
proach would normally include. Following Haque
et al. (2011), we derive a context-informed feature
?
h
mbl
that is expressed as the conditional probabil-
ity of the target phrase e?
k
given the source phrase
?
f
k
and its context information (CI), as in (2):
?
h
mbl
= log P(e?
k
|
?
f
k
,CI(
?
f
k
)) (2)
Here, CI may include any feature that can pro-
vide useful information to disambiguate the given
source phrase. In our experiment, we use CCG su-
pertag (Steedman, 2000) as a contextual features.
CCG supertag expresses the specific syntactic be-
haviour of a word in terms of the arguments it
takes, and more generally the syntactic environ-
ment in which it appears.
We consider the CCG supertags of the context
words, as well as of the focus phrase itself. In our
model, the supertag of a multi-word focus phrase
is the concatenation of the supertags of the words
composing that phrase. We generate a window
of size 2l + 1 features (we set l:=2), including
the concatenated complex supertag of the focus
phrase. Accordingly, the supertag-based contex-
tual information (CI
st
) is described as in (3):
CI
st
(
?
f
k
) = {st(f
i
k
?l
), ..., st(f
i
k
?1
), st(
?
f
k
),
st(f
j
k
+1
), ..., st(f
j
k
+l
)}
(3)
For the Hindi-to-English translation task, we use
part-of-speech (PoS) tags
4
of the source phrase
and the neighbouring words as the contextual fea-
ture, owing to the fact that supertaggers are readily
available only for English.
We use a memory-based machine learning
(MBL) classifier (TRIBL: (Daelemans, 2005))
5
that is able to estimate P(e?
k
|
?
f
k
,CI(
?
f
k
)) by
similarity-based reasoning over memorized
nearest-neighbour examples of source?target
phrase translations. Thus, we derive the feature
?
h
mbl
defined in Equation (2). In addition to
?
h
mbl
,
4
In order to obtain PoS tags of Hindi words,
we used the LTRC shallow parser for Hindi from
http://ltrc.iiit.ac.in/analyzer/hindi/shallow-parser-hin-
4.0.fc8.tar.gz.
5
An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl.
216
we derive a simple two-valued feature
?
h
best
,
defined in Equation (4):
?
h
best
=
{
1 if e?
k
maximizes P(e?
k
|
?
f
k
,CI(
?
f
k
))
u 0 otherwise
(4)
where
?
h
best
is set to 1 when e?
k
is one of the tar-
get phrases with highest probability according to
P(e?
k
|
?
f
k
,CI(
?
f
k
)) for each source phrase
?
f
k
; oth-
erwise
?
h
best
is set to 0.000001. We performed ex-
periments by integrating these two features
?
h
mbl
and
?
h
best
directly into the log-linear model of
Moses. Their weights are optimized using mini-
mum error-rate training (MERT)(Och, 2003) on a
held-out development set for each of the experi-
ments.
3.5 Morphological Segmentation
Haque et al. (2012) applied a morphological suffix
separation process in a Bengali-to-English trans-
lation task and showed that suffix separation sig-
nificantly reduces data sparseness in the Bengali
corpus. They also showed an SMT model trained
on the suffix-stripped training data significantly
outperforms the state-of-the-art PB-SMT baseline.
Like Bengali, Hindi is a morphologically very rich
and highly inflected Indian language. As done
previously for Bengali-to-English (Haque et al.,
2012), we employ a suffix-stripping method for
lemmatizing inflected Hindi words in the WMT
Hindi-to-English translation task. Following Das-
gupta and Ng (2006), we developed an unsu-
pervised morphological segmentation method for
Hindi. We also used a Hindi lightweight stem-
mer (Ramanathan and Rao, 2003) in order to pre-
pare a training corpus with only Hindi stems. We
prepared Hindi-to-English SMT systems on the
both types of training data (i.e. suffix-stripped and
stemmed).
6
3.6 Multi-Alignment Combination (MAC)
Word alignment is a critical component of MT
systems. Various methods for word alignment
have been proposed, and different models can pro-
duce signicantly different outputs. For example,
Tu et al. (2012) demonstrates that the alignment
agreement between the two best-known alignment
tools, namely Giza++(Och and Ney, 2003) and
6
Suffixes were separated and completely removed from
the training data.
the Berkeley aligner
7
(Liang et al., 2006), is be-
low 70%. Taking into consideration the small size
of the the corpus, in order to extract more ef-
fective phrase tables, we concatenate three align-
ments: Giza++ with grow-diag-final-and, Giza++
with intersection, and that derived from the Berke-
ley aligner.
3.7 Stemming Alignment and Normal Phrase
Extraction (SANPE)
The rich morphology of Hindi will cause word
alignment sparsity, which results in poor align-
ment quality. Furthermore, word stemming on
the Hindi side usually results in too many English
words being aligned to one stemmed Hindi word,
i.e. we encounter the problem of phrase over-
extraction. Therefore, we conduct word alignment
with the stemmed version of Hindi, and then at
the phrase extraction step, we replace the stemmed
form with the original Hindi form.
3.8 OOV Word Conversion Method
Our algorithm for OOV word conversion uses the
recently developed zero-shot learning (Palatucci
et al., 2009) using neural network language mod-
elling (Bengio et al., 2000; Mikolov et al., 2013).
The same technique is used in (Okita et al., 2014).
This method requires neither parallel nor compa-
rable corpora, but rather two monolingual corpora.
In our context, we prepare two monolingual cor-
pora on both sides, which are neither parallel nor
comparable, and a small amount of already known
correspondences between words on the source and
target sides (henceforth, we refer to this as the
?dictionary?). Then, we train both sides with the
neural network language model, and use a contin-
uous space representation to project words to each
other on the basis of a small amount of correspon-
dences in the dictionary. The following algorithm
shows the steps involved:
1. Prepare the monolingual source and target
sentences.
2. Prepare the dictionary which consists of U
entries of source and target sentences com-
prising non-stop-words.
3. Train the neural network language model on
the source side and obtain the real vectors of
X dimensions for each word.
7
http://code.google.com/p/berkeleyaligner/
217
4. Train the neural network language model on
the target side and obtain the real vectors of
X dimensions for each word.
5. Using the real vectors obtained in the above
steps, obtain the linear mapping between the
dictionary items in two continuous spaces us-
ing canonical component analysis (CCA).
In our experiments we use U the same as the en-
tries of Wiki corpus, which is provided among
WMT14 corpora, and X as 50. The resulted pro-
jection by this algorithm can be used as the OOV
word conversion which projects from the source
language which among OOV words into the tar-
get language. The overall algorithm which uses
the projection which we build in the above step is
shown in the following.
1. Collect unknown words in the translation out-
puts.
2. Do Hindi named-entity recognition (NER) to
detect noun phrases.
3. If they are noun phrases, do the projection
from each unknown word in the source side
into the target words (We use the projection
prepared in the above steps). If they are not
noun phrases, run the transliteration to con-
vert each of them.
We perform Hindi NER by training CRF++ (Kudo
et al., 2004) using the Hindi named entity corpus,
and use the Hindi shallow parser (Begum et al.,
2008) for preprocessing of the inputs.
4 Results and Discussion
4.1 Data
We conduct our experiments on the standard
datasets released in the WMT14 shared translation
task. We use HindEnCorp
8
(Bojar et al., 2014)
parallel corpus for MT system building. We also
used the CommonCrawl Hindi monolingual cor-
pus (Bojar et al., 2014) in order to build an addi-
tional language model for Hindi.
For the Hindi-to-English direction, we also em-
ployed monolingual English data used in the other
translation tasks for building the English language
model.
8
http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/
4.2 Moses Baseline
We employ a standard Moses PB-SMT model as
our baseline. The Hindi side is preprocessed but
unstemmed. We use Giza++ to perform word
alignment, the phrase table is extracted via the
grow-diag-final-and heuristic and the max-phrase-
length is set to 7.
4.3 Automatic Evaluation
Experiments BLEU
Moses Baseline 8.7
Context-Based 9.4
Context-Based + CommonCrawl LM 11.4
Table 1: BLEU scores of the English-to-Hindi MT
Systems on the WMT test set.
Experiments BLEU
Moses Baseline 10.1
Context-Based 10.1
Suffix-Stripped 10.0
OWC 11.2
OSM 10.3
Three LRMs 10.5
MAC 10.7
SANPE 10.6
LMI 10.9
LMI+SANPE+MAC+ThreeLRMs+OSM 11.7
Table 2: BLEU scores of the Hindi-to-English MT
Systems on the WMT test set.
We prepared a number of MT systems for both
English-to-Hindi and Hindi-to-English, and sub-
mitted their runs in the WMT 2014 Evaluation
Matrix. The BLEU scores of the different English-
to-Hindi MT systems (Moses Baseline, Context-
Based (CCG) MT system, and Context-Based
(CCG) MT system with an additional LM built
on the CommonCrawl Hindi monolingual corpus
(Bojar et al., 2014)) on the WMT 2014 English-
to-Hindi test set are reported in Table 1. As can
be seen from Table 1, Context-Based (CCG) MT
system produces 0.7 BLEU points improvement
(8.04% relative) over the Moses Baseline. When
we add an additional large LM built on the Com-
monCrawl data to the Context-Based (CCG) MT
system, we achieved a 2 BLEU-point improve-
ment (21.3% relative) (cf. last row in Table 1) over
218
the Context-Based (CCG) MT system.
9
The BLEU scores of the different Hindi-to-
English MT systems on the WMT 2014 Hindi-
to-English test set are reported in Table 2. The
first row of Table 2 shows the BLEU score for
the Baseline MT system. We note that the per-
formance of the Context-Based (PoS) MT system
obtains identical performance to the Moses base-
line (10.1 BLEU points) on the WMT 2014 Hindi-
to-English test set.
We employed a source language (Hindi) nor-
malisation technique, namely suffix separation,
but unfortunately this did not bring about any
improvement for the Hindi-to-English translation
task. The improvement gained by individually
employing OSM, three lexical reordering mod-
els, Multi-alignment Combination, Stem-align and
normal Phrase Extraction and Language Model In-
terpolation can be seen in Table 2. Our best sys-
tem is achieved by combining OSM, Three LMR,
MAC, SANPE and LMI, which results in a 1.6
BLEU point improvement over the Baseline.
5 Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the CNGL Centre for Global Intelligent Content
(www.cngl.ie) at Dublin City University.
References
Amittai Axelrod, Ra Birch Mayne, Chris Callison-
burch, Miles Osborne, and David Talbot. 2005. Ed-
inburgh system description for the 2005 iwslt speech
translation evaluation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT).
Rafiya Begum, Samar Husain, Arun Dhwaj,
Dipti Misra Sharma, Lakshmi Bai, and Rajeev
Sangal. 2008. Dependency annotation scheme for
indian languages. In Proceedings of The Third In-
ternational Joint Conference on Natural Language
Processing (IJCNLP).
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Proceedings of Neural Information Systems.
Ond Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in english-to-hindi machine translation.
In LREC.
9
Please note that this is an unconstrained submission.
Ondrej Bojar, V. Diatka, Rychly P., Pavel Stranak,
A. Tamchyna, and Daniel Zeman. 2014. Hindi-
english and hindi-only corpus for machine transla-
tion. In LREC.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Walter Daelemans. 2005. Memory-based language
processing. Cambridge University Press.
Sajib Dasgupta and Vincent Ng. 2006. Unsupervised
morphological parsing of bengali. Language Re-
sources and Evaluation, 40(3-4):311?330.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 1045?1054, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Rejwanul Haque, Sudip Kumar Naskar, Antal van den
Bosch, and Andy Way. 2011. Integrating source-
language context into phrase-based statistical ma-
chine translation. Machine translation, 25(3):239?
285.
Rejwanul Haque, Sergio Penkale, Jie Jiang, and Andy
Way. 2012. Source-side suffix stripping for bengali-
to-english smt. In Asian Language Processing
(IALP), 2012 International Conference on, pages
193?196. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
219
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying conditional random fields to
japanese morphological analysis. In Proceedings of
EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104?
111. Association for Computational Linguistics.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. ArXiv.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Tsuyoshi Okita, Ali Hosseinzadeh Vahid, Andy Way,
and Qun Liu. 2014. Dcu terminology translation
system for medical query subtask at wmt14.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Neural Information Pro-
cessing Systems (NIPS), December.
Ananthakrishnan Ramanathan and Durgesh D Rao.
2003. A lightweight stemmer for hindi. In the Pro-
ceedings of EACL.
Mark Steedman. 2000. The syntactic process, vol-
ume 35. MIT Press.
Andreas Stolcke. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference Spoken Language Processing,
pages 901?904, Denver, CO.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012. Combining mul-
tiple alignments to improve machine translation. In
COLING (Posters), pages 1249?1260.
220
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 420?425,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
RED: DCU-CASICT Participation in WMT2014 Metrics Task
Xiaofeng Wu
?
, Hui Yu
?
, Qun Liu
??
?
CNGL Centre for Global Intelligent Content
School of Computing, Dublin City University
Dublin 9, Ireland
?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
Beijing, China
{xiaofengwu,qliu}@computing.dcu.ie, yuhui@ict.ac.cn
Abstract
Based on the last year?s DCU-CASIST
participation on WMT metrics task, we
further improve our model in the follow-
ing ways: 1) parameter tuning 2) support
languages other than English. We tuned
our system on all the data of WMT 2010,
2012 and 2013. The tuning results as well
as the WMT 2014 test results are reported.
1 Introduction
Automatic evaluation plays a more and more im-
portant role in the evolution of machine transla-
tion. There are roughly two categories can be seen:
namely lexical information based and structural
information based.
1) Lexical information based approaches,
among which, BLEU (?), Translation Error Rate
(TER) (?) and METEOR (?) are the most popular
ones and, with simplicity as their merits, cannot
adequately reflect the structural level similarity.
2) A lot of structural level based methods
have been exploited to overcome the weakness
of the lexical based methods, including Syntactic
Tree Model(STM)(?), a constituent tree based ap-
proach, and Head Word Chain Model(HWCM)(?),
a dependency tree based approach. Both of
the methods compute the similarity between the
sub-trees of the hypothesis and the reference.
Owczarzak et al (?; ?; ?) presented a method
using the Lexical-Functional Grammar (LFG) de-
pendency tree. MAXSIM (?) and the method pro-
posed by Zhu et al (?) also employed the syntac-
tic information in association with lexical infor-
mation. As we know that the hypothesis is poten-
tially noisy, and these errors are enlarged through
the parsing process. Thus the power of syntactic
information could be considerably weakened.
In this paper, based on our attempt on WMT
metrics task 2013 (?), we propose a metrics named
RED ( REference Dependency based automatic
evaluation method). Our metrics employs only the
reference dependency tree which contains both the
lexical and syntactic information, leaving the hy-
pothesis side unparsed to avoid error propagation.
2 Parameter Tuning
In RED, we use F -score as our final score.
F -score is calculated by Formula (1), where ? is
a value between 0 and 1.
F -score =
precision ? recall
? ? precision+ (1? ?) ? recall
(1)
The dependency tree of the reference and the
string of the translation are used to calculate the
precision and recall. In order to calculate preci-
sion, the number of the dep-ngrams (the ngrams
obtained from dependency tree
1
) should be given,
but there is no dependency tree for the transla-
tion in our method. We know that the number
of dep-ngrams has an approximate linear relation-
ship with the length of the sentence, so we use the
length of the translation to replace the number of
the dep-ngrams in the translation dependency tree.
Recall can be calculated directly since we know
the number of the dep-ngrams in the reference.
The precision and recall are computed as follows.
precision
n
=
?
d?D
n
p
(d,hyp)
len
h
recall
n
=
?
d?D
n
p
(d,hyp)
count
n(ref)
D
n
is the set of dep-ngrams with the length of n.
len
h
is the length of the translation. count
n(ref)
is the number of the dep-ngrams with the length
of n in the reference. p
(d,hpy)
is 0 if there is no
match and a positive number between 0 and 1 oth-
erwise(?).
1
We define two types of dep-ngrams: 1) the head word
chain(?); 2) fix-floating(?))
420
The final score of RED is achieved using For-
mula (2), in which a weighted sum of the dep-
ngrams? F -score is calculated. w
ngram
(0 ?
w
ngram
? 1) is the weight of dep-ngram with the
length of n. F -score
n
is the F -score for the dep-
ngrams with the length of n.
RED =
N
?
n=1
(w
ngram
? F -score
n
) (2)
Other parameters to be tuned includes:
? Stem and Synonym
Stem(?) and synonym (WordNet
2
) are intro-
duced with the following three steps. First,
we obtain the alignment with METEOR
Aligner (?) in which not only exact match
but also stem and synonym are considered.
We use stem, synonym and exact match as the
three match modules. Second, the alignment
is used to match for a dep-ngram. We think
the dep-ngram can match with the transla-
tion if the following conditions are satisfied.
1) Each of the words in the dep-ngram has
a matched word in the translation according
to the alignment; 2) The words in dep-ngram
and the matched words in translation appear
in the same order; 3) The matched words
in translation must be continuous if the dep-
ngram is a fixed-floating ngram. At last, the
match module score of a dep-ngram is cal-
culated according to Formula (3). Different
match modules have different effects, so we
give them different weights.
s
mod
=
?
n
i=1
w
m
i
n
, 0 ? w
m
i
? 1 (3)
m
i
is the match module (exact, stem or syn-
onym) of the ith word in a dep-ngram. w
m
i
is the match module weight of the ith word in
a dep-ngram. n is the number of words in a
dep-ngram.
? Paraphrase
When introducing paraphrase, we don?t con-
sider the dependency tree of the reference,
because paraphrases may not be contained in
the head word chain and fixed-floating struc-
tures. Therefore we first obtain the align-
2
http://wordnet.princeton.edu/
ment with METEOR Aligner, only consid-
ering paraphrase; Then, the matched para-
phrases are extracted from the alignment and
defined as paraphrase-ngram. The score of
a paraphrase is 1 ? w
par
, where w
par
is the
weight of paraphrase-ngram.
? Function word
We introduce a parameterw
fun
(0 ? w
fun
?
1) to distinguish function words and content
words. w
fun
is the weight of function words.
The function word score of a dep-ngram or
paraphrase-ngram is computed according to
Formula (4).
s
fun
=
C
fun
? w
fun
+ C
con
? (1? w
fun
)
C
fun
+ C
con
(4)
C
fun
is the number of function words in the
dep-ngram or paraphrase-ngram. C
con
is the
number of content words in the dep-ngram or
paraphrase-ngram.
REDp =
N
?
n=1
(w
ngram
? F -score
pn
) (5)
F -score
p
=
precision
p
? recall
p
? ? precision
p
+ (1? ?) ? recall
p
(6)
precision
p
and recall
P
in Formula (6) are cal-
culated as follows.
precision
p
=
score
par
n
+ score
dep
n
len
h
recall
p
=
score
par
n
+ score
dep
n
count
n
(ref) + count
n
(par)
len
h
is the length of the translation. count
n(ref)
is the number of the dep-ngrams with the length
of n in the reference. count
n
(par) is the num-
ber of paraphrases with length of n in refer-
ence. score
par
n
is the match score of paraphrase-
ngrams with the length of n. score
dep
n
is the
match score of dep-ngrams with the length of n.
score
par
n
and score
dep
n
are calculated as follows.
score
par
n
=
?
par?P
n
(1? w
par
? s
fum
)
score
dep
n
=
?
d?D
n
(p
(d,hyp)
? s
mod
? s
fun
)
421
Metrics BLEU TER HWCM METEOR RED RED-sent RED-syssent
WMT 2010
cs-en 0.255 0.253 0.245 0.319 0.328 0.342 0.342
de-en 0.275 0.291 0.267 0.348 0.361 0.371 0.375
es-en 0.280 0.263 0.259 0.326 0.333 0.344 0.347
fr-en 0.220 0.211 0.244 0.275 0.283 0.329 0.328
ave 0.257 0.254 0.253 0.317 0.326 0.346 0.348
WMT 2012
cs-en 0.157 - 0.158 0.212 0.165 0.218 0.212
de-en 0.191 - 0.207 0.275 0.218 0.283 0.279
es-en 0.189 - 0.203 0.249 0.203 0.255 0.256
fr-en 0.210 - 0.204 0.251 0.221 0.250 0.253
ave 0.186 - 0.193 0.246 0.201 0.251 0.250
WMT 2013
cs-en 0.199 - 0.153 0.265 0.228 0.260 0.256
de-en 0.220 - 0.182 0.293 0.267 0.298 0.297
es-en 0.259 - 0.220 0.324 0.312 0.330 0.326
fr-en 0.224 - 0.194 0.264 0.257 0.267 0.266
ru-en 0.162 - 0.136 0.239 0.200 0.262 0.225
ave 0.212 - 0.177 0.277 0.252 0.283 0.274
WMT 2014
hi-en - - - 0.420 - 0.383 0.386
de-en - - - 0.334 - 0.336 0.338
cs-en - - - 0.282 - 0.283 0.283
fr-en - - - 0.406 - 0.403 0.404
ru-en - - - 0.337 - 0.328 0.329
ave - - - 0.355 - 0.347 0.348
Table 1: Sentence level correlations tuned on WMT 2010, 2012 and 2013; tested on WMT 2014. The
value in bold is the best result in each raw. ave stands for the average result of the language pairs on each
year. RED stands for our untuned system, RED-sent is G.sent.2, RED-syssent is G.sent.1
P
n
is the set of paraphrase-ngrams with the
length of n. D
n
is the set of dep-ngrams with the
length of n.
There are totally nine parameters in RED. We
tried two parameter tuning strategies: Genetic
search algorithm (?) and a Grid search over two
subsets of parameters. The results of Grid search
is more stable, therefore our final submission is
based upon Grid search. We separate the 9 pa-
rameters into two subsets. When searching Sub-
set 1, the parameters in Subset 2 are fixed, and
vice versa. Several iterations are executed to fin-
ish the parameter tuning process. For system
level coefficient score, we set two optimization
goals: G.sys.1) to maximize the sum of Spear-
man?s ? rank correlation coefficient on system
level and Kendall?s ? correlation coefficient on
sentence level or G.sys.2) only the former; For
sentence level coefficient score, we also set two
optimization goals: G.sent.1) the same as G.sys.1,
G.sent.2) only the latter part of G.sys.1.
3 Experiments
In this section we report the experimental results
of RED on the tuning set, which is the combi-
nation of WMT2010, WMT2012 and WMT2013
data set, as well as the test results on the
WMT2014. Both the sentence level evaluation and
the system level evaluation are conducted to assess
the performance of our automatic metrics. At the
sentence level evaluation, Kendall?s rank correla-
tion coefficient ? is used. At the system level eval-
uation, the Spearman?s rank correlation coefficient
? is used.
3.1 Data
There are four language pairs in WMT2010 and
WMT2012 including German-English, Czech-
English, French-English and Spanish-English. For
WMT2013, except these 4 language pairs, there is
also Russian-English. As the test set, WMT 2014
has also five language pairs, but the organizer re-
moved Spanish-English and replace it with Hindi-
English. For into-English tasks, we parsed the En-
422
Metrics BLEU TER HWCM METEOR RED RED-sys RED-syssent
WMT 2010
cs-en 0.840 0.783 0.881 0.839 0.839 0.937 0.881
de-en 0.881 0.892 0.905 0.927 0.933 0.95 0.948
es-en 0.868 0.903 0.824 0.952 0.969 0.965 0.969
fr-en 0.839 0.833 0.815 0.865 0.873 0.875 0.905
ave 0.857 0.852 0.856 0.895 0.903 0.931 0.925
WMT 2012
cs-en 0.886 0.886 0.943 0.657 1 1 1
de-en 0.671 0.624 0.762 0.885 0.759 0.935 0.956
es-en 0.874 0.916 0.937 0.951 0.951 0.965 0.958
fr-en 0.811 0.821 0.818 0.843 0.818 0.871 0.853
ave 0.810 0.811 0.865 0.834 0.882 0.942 0.941
WMT 2013
cs-en 0.936 0.800 0.818 0.964 0.964 0.982 0.972
de-en 0.895 0.833 0.816 0.961 0.951 0.958 0.978
es-en 0.888 0.825 0.755 0.979 0.930 0.979 0.965
fr-en 0.989 0.951 0.940 0.984 0.989 0.995 0.984
ru-en 0.670 0.581 0.360 0.789 0.725 0.847 0.821
ave 0.875 0.798 0.737 0.834 0.935 0.952 0.944
WMT 2014
hi-en 0.956 0.618 - 0.457 - 0.676 0.644
de-en 0.831 0.774 - 0.926 - 0.897 0.909
cs-en 0.908 0.977 - 0.980 - 0.989 0.993
fr-en 0.952 0.952 - 0.975 - 0.981 0.980
ru-en 0.774 0.796 - 0.792 - 0.803 0.797
ave 0.826 0.740 - 0.784 - 0.784 0.770
Table 2: System level correlations tuned on WMT 2010, 2012 and 2013, tested on 2014. The value in
bold is the best result in each raw. ave stands for the average result of the language pairs on each year.
RED stands for our untuned system, RED-sys is G.sys.2, RED-syssent is G.sys.1
Metrics BLEU TER METEOR RED RED-sent RED-syssent
WMT 2010
en-fr 0.33 0.31 0.369 0.338 0.390 0.369
en-de 0.15 0.08 0.166 0.141 0.214 0.185
WMT 2012
en-fr - - 0.26 0.171 0.273 0.266
en-de - - 0.180 0.129 0.200 0.196
WMT 2013
en-fr - - 0.236 0.220 0.237 0.239
en-de - - 0.203 0.185 0.215 0.219
WMT 2014
en-fr - - 0.278 - 0.297 0.293
en-de - - 0.233 - 0.236 0.229
Table 3: Sentence level correlations tuned on WMT 2010, 2012 and 2013, and tested on 2014. The
value in bold is the best result in each raw. RED stands for our untuned system, RED-sent is G.sent.2,
RED-syssent is G.sent.1
glish reference into constituent tree by Berkeley
parser and then converted the constituent tree into
dependency tree by Penn2Malt
3
. We also con-
ducted English-to-French and English-to-German
experiments. The German and French dependency
parser we used is Mate-Tool
4
.
3
http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
4
https://code.google.com/p/mate-tools/
In the experiments, we compare the perfor-
mance of our metric with the widely used lexi-
cal based metrics BLEU, TER, METEOR and a
dependency based metrics HWCM. The results of
RED are provided with exactly the same external
resources like METEOR. The results of BLEU,
TER and METOER are obtained from official re-
port of WMT 2010, 2012, 2013 and 2014 (if they
423
Metrics BLEU TER METEOR RED RED-sys RED-syssent
WMT 2010
en-fr 0.89 0.89 0.912 0.881 0.932 0.928
en-de 0.66 0.65 0.688 0.657 0.734 0.734
WMT 2012
en-fr 0.80 0.69 0.82 0.639 0.914 0.914
en-de 0.22 0.41 0.180 0.143 0.243 0.243
WMT 2013
en-fr 0.897 0.912 0.924 0.914 0.931 0.936
en-de 0.786 0.854 0.879 0.85 0.8 0.8
WMT 2014
en-fr 0.934 0.953 0.940 - 0.942 0.943
en-de 0.065 0.163 0.128 - 0.047 0.047
Table 4: System level correlations for English to Franch and German, tuned on WMT 2010, 2012 and
2013; tested on WMT 2014. The value in bold is the best result in each raw. RED stands for our untuned
system, RED-sys is G.sys.2, RED-syssent is G.sys.1
are available). The experiments of HWCM is per-
formed by us.
3.2 Sentence-level Evaluation
Kendall?s rank correlation coefficient ? is em-
ployed to evaluate the correlation of all the MT
evaluation metrics and human judgements at the
sentence level. A higher value of ? means a bet-
ter ranking similarity with the human judges. The
correlation scores of are shown in Table 1. Our
method performs best when maximum length of
dep-n-gram is set to 3, so we present only the
results when the maximum length of dep-n-gram
equals 3. From Table 1, we can see that: firstly, pa-
rameter tuning improve performance significantly
on all the three tuning sets; secondly, although
the best scores in the column RED-sent are much
more than RED-syssent, but the outperform is
very small, so by setting these two optimization
goals, RED can achieve comparable performance;
thirdly, without parameter tuning, RED does not
perform well on WMT 2012 and 2013, and even
with parameter tuning, RED does not outperform
METEOR as much as WMT 2010; lastly, on the
test set, RED does not outperform METEOR.
3.3 System-level Evaluation
We also evaluated the RED scores with the human
rankings at the system level to further investigate
the effectiveness of our metrics. The matching of
the words in RED is correlated with the position
of the words, so the traditional way of computing
system level score, like what BLEU does, is not
feasible for RED. Therefore, we resort to the way
of adding the sentence level scores together to ob-
tain the system level score. At system level evalu-
ation, we employ Spearman?s rank correlation co-
efficient ?. The correlations and the average scores
are shown in Table 2.
From Table 2, we can see similar trends as in
Table 1 with the following difference: firstly, with-
out parameter tuning, RED perform comparably
with METEOR on all the three tuning sets; sec-
ondly, on test set, RED also perform comparably
with METEOR. thirdly, RED perform very bad on
Hindi-English, which is a newly introduced task
this year.
3.4 Evaluation of English to Other
Languages
We evaluate both sentence level and system level
score of RED on English to French and German.
The reason we only conduct these two languages
are that the dependency parsers are more reliable
in these two languages. The results are shown in
Table 3 and 4.
From Table 3 and 4 we can see that the tuned
version of RED still perform slightly better than
METEOR with the only exception of system level
en-de.
4 Conclusion
In this paper, based on the last year?s DCU-
CASICT submission, we further improved our
method, namely RED. The experiments are car-
ried out at both sentence-level and system-level
using to-English and from-English corpus. The
experiment results indicate that although RED
achieves better correlation than BLEU, HWCM,
TER and comparably performance with METEOR
at both sentence level and system level, the per-
formance is not stable on all language pairs, such
as the sentence level correlation score of Hindi to
424
English and the system level score of English to
German. To further study the sudden diving of the
performance is our future work.
Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the CNGL Centre for Global Intelligent Content
(www.cngl.ie) at Dublin City University and Na-
tional Natural Science Foundation of China (Grant
61379086).
References
Ergun Bic?ici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272?283. As-
sociation for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2008. Maxsim: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 85?91, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT ?07, pages 228?231, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization, pages 25?32.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007a. Dependency-based automatic eval-
uation for machine translation. In Proceedings of
the NAACL-HLT 2007/AMTA Workshop on Syntax
and Structure in Statistical Translation, SSST ?07,
pages 80?87, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007b. Evaluating machine translation with
lfg dependencies. Machine Translation, 21(2):95?
119, June.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007c. Labelled dependencies in machine
translation evaluation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
StatMT ?07, pages 104?111, Stroudsburg, PA, USA.
Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th annual
meeting on association for computational linguis-
tics, pages 311?318. Association for Computational
Linguistics.
Martin F Porter. 2001. Snowball: A language for stem-
ming algorithms.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649?671.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Xiaofeng Wu, Hui Yu, and Qun Liu. 2013. Dcu partic-
ipation in wmt2013 metrics task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation. Association for Computational Linguistics.
H. Yu, X. Wu, Q. Liu, and S. Lin. 2014. RED: A
Reference Dependency Based MT Evaluation Met-
ric. In To be published.
Junguo Zhu, Muyun Yang, Bo Wang, Sheng Li, and
Tiejun Zhao. 2010. All in strings: a powerful string-
based automatic mt evaluation metric with multi-
ple granularities. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, COLING ?10, pages 1533?1540, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
425
