A Resource-light Approach to Russian Morphology: Tagging Russian using
Czech resources
Jiri Hana and Anna Feldman and Chris Brew
Department of Linguistics
Ohio State University
Columbus, OH 43210
Abstract
In this paper, we describe a resource-light system
for the automatic morphological analysis and tag-
ging of Russian. We eschew the use of extensive
resources (particularly, large annotated corpora and
lexicons), exploiting instead (i) pre-existing anno-
tated corpora of Czech; (ii) an unannotated corpus
of Russian. We show that our approach has benefits,
and present what we believe to be one of the first full
evaluations of a Russian tagger in the openly avail-
able literature.
1 Introduction
Morphological processing and part-of-speech tag-
ging are essential for many NLP tasks, including
machine translation, information retrieval and pars-
ing. In this paper, we describe a resource-light ap-
proach to the tagging of Russian. Because Russian
is a highly inflected language with a high degree
of morpheme homonymy (cf. Table 11) the tags in-
volved are more numerous and elaborate than those
typically used for English. This complicates the tag-
ging task, although as has been previously noted
(Elworthy, 1995), the increased complexity of the
tags does not necessarily translate into a more de-
manding tagging task. Because no large annotated
corpora of Russian are available to us, we instead
chose to use an annotated corpus of Czech. Czech
is sufficiently similar to Russian that it is reasonable
to suppose that information about Czech will be rel-
evant in some way to the tagging of Russian.
The languages share many linguistic properties (free
word order and a rich morphology which plays
a considerable role in determining agreement and
argument relationships). We created a morpho-
logical analyzer for Russian, combined the results
with information derived from Czech and used the
TnT (Brants, 2000) tagger in a number of differ-
1All Russian examples in this paper are transcribed in the
Roman alphabet. Our system is able to analyze Russian texts
in both Cyrillic and various transcriptions.
krasiv-a beautiful (short adjective, feminine)
muz?-a husband (noun, masc., sing., genitive)
husband (noun, masc., sing., accusative)
okn-a window (noun, neuter, sing., genitive)
window (noun, neuter, pl., nominative)
window (noun, neuter, pl., accusative)
knig-a book (noun, fem., sing., nominative)
dom-a house (noun, masc., sing., genitive)
house (noun, masc., pl., nominative)
house (noun, masc., pl., accusative)
skazal-a say (verb, fem., sing., past tense)
dv-a two (numeral, masc., nominative)
Table 1: Homonymy of the a ending
ent ways, including a a committee-based approach,
which turned out to give the best results. To eval-
uate the results, we morphologically annotated (by
hand) a small corpus of Russian: part of the transla-
tion of Orwell?s ?1984? from the MULTEXT-EAST
project (Ve?ronis, 1996).
2 Why TnT?
Readers may wonder why we chose to use TnT,
which was not designed for Slavic languages. The
short answer is that it is convenient and successful,
but the following two sections address the issue in
rather more detail.
2.1 The encoding of lexical information in TnT
TnT records some lexical information in the emis-
sion probabilities of its second order Markov
Model. Since Russian and Czech do not use the
same words we cannot use this information (at least
not directly) to tag Russian. Given this, the move
from Czech to Russian involves a loss of detailed
lexical information. Therefore we implemented a
morphological analyzer for Russian, the output of
which we use to provide surrogate emission proba-
bilities for the TnT tagger (Brants, 2000). The de-
tails are described below in section 4.2.
2.2 The modelling of word order in TnT
Both Russian and Czech have relatively free word
order, so it may seem an odd choice to use a Markov
model (MM) tagger. Why should second order
MM be able to capture useful facts about such lan-
guages? Firstly, even if a language has the poten-
tial for free word order, it may still turn out that
there are recurring patterns in the progressions of
parts-of-speech attested in a training corpus. Sec-
ondly, n-gram models including MM have indeed
been shown to be successful for various Slavic lan-
guages, e.g., Czech (Hajic? et al, 2001) or Slovene
(Dz?eroski et al, 2000); although not as much as
for English. This shows that the transitional in-
formation captured by the second-order MM from
a Czech or Slovene corpus is useful for Czech or
Slovene.2 The present paper shows that transitional
information acquired from Czech is also useful for
Russian.
3 Russian versus Czech
A deep comparative analysis of Czech and Russian
is far beyond the scope of this paper. However, we
would like to mention just a number of the most im-
portant facts. Both languages are Slavic (Czech is
West Slavonic, Russian is East Slavonic). Both have
extensive morphology whose role is important in
determining the grammatical functions of phrases.
In both languages, the main verb agrees in person
and number with subject; adjectives agree in gen-
der, number and case with nouns. Both languages
are free constituent order languages. The word or-
der in a sentence is determined mainly by discourse.
It turns out that the word order in Czech and Russian
is very similar. For instance, old information mostly
precedes new information. The ?neutral? order in
the two languages is Subject-Verb-Object. Here is a
parallel Czech-Russian example from our develop-
ment corpus:
(1) a. [Czech]
Byl
wasMasc.Past
jasny?,
brightMasc.Sg.Nom
studeny?
coldMasc.Sg.Nom
dubnovy?
AprilMasc.Sg.Nom
den
dayMasc.Sg.Nom
i
and
hodiny
clocksFem.P l.Nom
odb??jely
strokeFem.P l.Past
tr?ina?ctou.
thirteenthFem.Sg.Acc
b. [Russian]
2Respectively, and if the techniques in the present paper
generalize, probably also irrespectively.
Byl
wasMasc.Past
jasnyj,
brightMasc.Sg.Nom
xolodnyj
coldMasc.Sg.Nom
aprel?skij
AprilMasc.Sg.Nom
den?
dayMasc.Sg.Nom
i
and
c?asy
clocksPl.Nom
probili
strokePl.Past
trinadtsat?.
thirteenAcc
?It was a bright cold day in April, and the
clocks were striking thirteen.? [from Orwell?s
?1984?]
Of course, not all utterances are so similar. Sec-
tion 5.4 briefly mentions how to improve the utility
of the corpus by eradicating some of the systematic
differences.
4 Realization
4.1 The tag system
We adopted the Czech tag system (Hajic?, 2000) for
Russian. Every tag is represented as a string of 15
symbols each corresponding to one morphological
category. For example, the word vidjela is assigned
the tag VpFS- - -XR-AA- - -, because it is a verb (V),
past participle (p), feminine (F), singular (S), does
not distinguish case (-), possessive gender (-), pos-
sessive number (-), can be any person (X), is past
tense (R), is not gradable (-), affirmative (A), active
voice (A), and does not have any stylistic variants
(the final hyphen).
No. Description Abbr. No. of values
Cz Ru
1 POS P 12 12
2 SubPOS ? detailed POS S 75 32
3 Gender g 11 5
4 Number n 6 4
5 Case c 9 8
6 Possessor?s Gender G 5 4
7 Possessor?s Number N 3 3
8 Person p 5 5
9 Tense t 5 5
10 Degree of comparison d 4 4
11 Negation a 3 3
12 Voice v 3 3
13 Unused 1 1
14 Unused 1 1
15 Variant, Style V 10 2
Table 2: Overview and comparison of the tagsets
The tagset used for Czech (4290+ tags) is larger
than the tagset we use for Russian (about 900 tags).
There is a good theoretical reason for this choice
? Russian morphological categories usually have
fewer values (e.g., 6 cases in Russian vs. 7 in Czech;
Czech often has formal and colloquial variants of
the same morpheme); but there is also an immedi-
ate practical reason ? the Czech tag system is very
elaborate and specifically devised to serve multiple
needs, while our tagset is designed solely to capture
the core of Russian morphology, as we need it for
our primary purpose of demonstrating the portabil-
ity and feasibility of our technique. Still, our tagset
is much larger than the Penn Treebank tagset, which
uses only 36 non-punctuation tags (Marcus et al,
1993).
4.2 Morphological analysis
In this section we describe our approach to a
resource-light encoding of salient facts about the
Russian lexicon. Our techniques are not as rad-
ical as previously explored unsupervised methods
(Goldsmith, 2001; Yarowsky and Wicentowski,
2000), but are designed to be feasible for languages
for which serious morphological expertise is un-
available to us. We use a paradigm-based morphol-
ogy that avoids the need to explicitly create a large
lexicon. The price that we pay for this is overgener-
ation. Most of these analyses look very implausible
to a Russian speaker, but significantly increasing the
precision would be at the cost of greater develop-
ment time than our resource-light approach is able
to commit. We wish our work to be portable at least
to other Slavic languages, for which we assume that
elaborate morphological analyzers will not be avail-
able. We do use two simple pre-processing methods
to decrease the ambiguity of the results handed to
the tagger ? longest ending filtering and an automat-
ically acquired lexicon of stems. These were easy to
implement and surprisingly effective.
Our analyzer captures just a few textbook facts
about the Russian morphology (Wade, 1992), ex-
cluding the majority of exceptions and including in-
formation about 4 declension classes of nouns, 3
conjugation classes of verbs. In total our database
contains 80 paradigms. A paradigm is a set of end-
ings and POS tags that can go with a particular set
of stems. Thus, for example, the paradigm in Table
3 is a set of inflections that go with the masculine
stems ending on the ?hard? consonants, e.g., slon
?elephant?, stol ?table?.
Unlike the traditional notions of stem and ending,
for us a stem is the part of the word that does not
change within its paradigm, and the ending is the
part of the word that follows such a stem. For ex-
ample, the forms of the verb moc?? ?can.INF?: mogu
?1sg?, moz?es?? ?2sg?, moz?et ?3sg?, etc. are analyzed as
0 NNMS1 - - - - - - - - - - y NNMP1 - - - - - - - - - -
a NNMS2 - - - - - - - - - - ov NNMP2 - - - - - - - - - -
u NNMS3 - - - - - - - - - - am NNMP3 - - - - - - - - - -
a NNMS4 - - - - - - - - - - ov NNMP4 - - - - - - - - - -
u NNMS4 - - - - - - - - - 1
e NNMS6 - - - - - - - - - - ax NNMP6 - - - - - - - - - -
u NNMS6 - - - - - - - - - 1
om NNMS7 - - - - - - - - - - ami NNMP7 - - - - - - - - - -
Table 3: A paradigm for ?hard? consonant mascu-
line nouns
the stem mo followed by the endings gu, z?es??, z?et. A
more linguistically oriented analysis would involve
the endings u, es??, et and phonological alternations
in the stem. All stem internal variations are treated
as suppletion.3
Unlike the morphological analyzers that exist for
Russian (Segalovich and Titov, 2000; Segalovich,
2003; Segalovich and Maslov, 1989; Kovalev, 2002;
Mikheev and Liubushkina, 1995; Yablonsky, 1999;
Segalovich, 2003; Kovalev, 2002, among others)
(Segalovich, 2003; Kovalev, 2002; Mikheev and Li-
ubushkina, 1995; Yablonsky, 1999, among others),
our analyzer does not rely on a substantial manu-
ally created lexicon. This is in keeping with our aim
of being resource-light. When analyzing a word,
the system first checks a list of monomorphemic
closed-class words and then segments the word into
all possible prefix-stem-ending triples.4 The result
has quite good coverage (95.4%), but the average
ambiguity is very high (10.9 tags/token), and even
higher for open class words. We therefore have two
strategies for reducing ambiguity.
4.2.1 Longest ending filtering (LEF)
The first approach to ambiguity reduction is based
on a simple heuristic ? the correct ending is usually
one of the longest candidate endings. In English, it
would mean that if a word is analyzed either as hav-
ing a zero ending or an -ing ending, we would con-
sider only the latter; obviously, in the vast majority
of cases that would be the correct analysis. In addi-
tion, we specify that a few long but very rare end-
ings should not be included in the maximum length
calculation (e.g., 2nd person pl. imperative).
3We do in fact have a very similar analysis, the analyzer?s
run-time representation of the paradigms is automatically pro-
duced from a more compact and linguistically attractive spec-
ification of the paradigms. It is possible to specify the ba-
sic paradigms and then specify the subparadigms, exceptions
and paradigms involving phonological changes by referring to
them.
4Currently, we consider only two inflectional prefixes ? neg-
ative ne and superlative nai.
4.2.2 Deriving a lexicon
The second approach uses a large raw corpus5 to
generate an open class lexicon of possible stems
with their paradigms. In this paper, we can only
sketch the method, for more details see (Hana and
Feldman, to appear). It is based on the idea that
open-class lemmata are likely to occur in more than
one form. First, we run the morphological analyzer
on the text (without any filtering), then we add to
the lexicon those entries that occurred with at least a
certain number of distinct forms and cover the high-
est number of forms. If we encounter the word talk-
ing, using the information about paradigms, we can
assume that it is either the -ing form of the lemma
talk or that it is a monomorphemic word (such as
sibling). Based on this single form we cannot really
say more. However, if we also encounter the forms
talk, talks and talked, the former analysis seems
more probable; and therefore, it seems reasonable
to include the lemma talk as a verb into the lexi-
con. If we encountered also talkings, talkinged and
talkinging, we would include both lemmata talk and
talking as verbs.
Obviously, morphological analysis based on such
a lexicon overgenerates, but it overgenerates much
less than if based on the endings alone. For ex-
ample, for the word form partii of the lemma par-
tija ?party?, our analysis gives 8 possibilities ? the
5 correct ones (noun fem sg gen/dat/loc sg and pl
nom/acc) and 3 incorrect ones (noun masc sg loc,
pl nom, and noun neut pl acc; note that only gen-
der is incorrect). Analysis based on endings alone
would allow 20 possibilities ? 15 of them incorrect
(including adjectives and an imperative).
4.3 Tagging
We use the TnT tagger (Brants, 2000), an imple-
mentation of the Viterbi algorithm for second order
Markov models. We train the transition probabili-
ties on Czech (1.5M tokens of the Prague Depen-
dency Treebank (Be?mova? et al, 1999)). We ob-
tain surrogate emission probabilities by running our
morphological analyzer, then assuming a uniform
distribution over the resulting emissions.
5 Experiments
5.1 Corpora
For evaluation purposes, we selected and morpho-
logically annotated (by hand) a small portion from
5We used The Uppsala Russian Corpus (1M tokens), which
is freely available from Uppsala University at http://www.
slaviska.uu.se/ryska/corpus.html.
the Russian translation of Orwell?s ?1984?. This cor-
pus contains 4011 tokens and 1858 types. For devel-
opment, we used another part of ?1984?. Since we
want to work with minimal language resources, the
development corpus is intentionally small ? 1788 to-
kens. We used it to test our hypotheses and tune the
parameters of our tools.
In the following sections, we discuss our experi-
ments and report the results. Note that we do not
report the results for tag position 13 and 14, since
these positions are unused; and therefore, always
trivially correct.
5.2 Morphological analysis
As can be seen from Table 4, morphological anal-
ysis without any filters gives good recall (although
on a non-fiction text it would probably be lower),
but also very high average ambiguity. Both fil-
ters (the longest-ending filter and automatically ac-
quired lexicon) reduce the ambiguity significantly;
the former producing a considerable drop of recall,
the latter retaining high recall. However, we do best
if we first attempt lexical lookup, then apply LEF
to the words not found. This keeps recall reason-
ably high at the same time as decreasing ambiguity.
As expected, performance increases with the size of
the unannotated Russian corpus used to generate the
lexicon. All subsequent experimental results were
obtained using this best filter combination, i.e., the
combination of the lexicon based on the 1Mword
corpus and LEF.
LEF no no no yes yes yes
Lexicon based on 0 100K 1M 0 100K 1M
recall 95.4 94 93.1 84.4 88.3 90.4
avg ambig (tag/word) 10.9 7.0 4.7 4.1 3.5 3.1
Tagging ? accuracy 50.7 62.1 67.5 62.1 66.8 69.4
Table 4: Morph. analysis with various parameters
5.3 Tagging
Table 7 summarizes the results of our taggers on test
data. Our baseline is produced by the morphologi-
cal analyzer without any filters followed by a tagger
randomly selecting a tag among the tags offered by
the morphological analyzer. The direct-full tag col-
umn shows the result of the TNT tagger with transi-
tion probabilities obtained directly from the Czech
corpus and the emission symbols based on the mor-
phological analyzer with the best filters.
To further improve the results, we used two tech-
niques: (i) we modified the training corpus to re-
move some systematic differences between Czech
and Russian (5.4); (ii) we trained batteries of tag-
gers on subtags to address the data sparsity problem
(5.5 and 5.6).
5.4 Russification
We experimented with ?russified? models. We
trained the TnT tagger on the Czech corpus with
modifications that made the structure of training
data look more like Russian. For example, plural
adjectives and participles in Russian, unlike Czech,
do not distinguish gender.
(2) a. Nadan??
Giftedmasc.pl
muz?i
men
soutez?ili.
competedmasc.pl
?Gifted sportsmen were competing.? [Cz]
b. Nadane?
Giftedfem.pl
z?eny
women
soutez?ily.
competedfem.pl
?Gifted women were competing.? [Cz]
c. Nadana?
Giftedneut.pl
de?vc?ata
girlsneut
soute?z?ila.
competingneut.pl
?Gifted girls were competing.? [Cz]
d. Talantlivye
Giftedpl
muz?c?iny/z?ens?c?iny
men/women
sorevnovalis?.
competedpl
?Gifted men/women were competing.?[Ru]
Negation in Czech is in the majority of cases is ex-
pressed by the prefix ne-, whereas in Russian it is
very common to see a separate particle (ne) instead:
(3) a. Nic
nothing
ner?ekl.
not-said
?He didn?t say anything.? [Cz]
b. On
he
nic?ego
nothing
ne
not
skazal.
said
?He didn?t say anything.? [Ru]
In addition, reflexive verbs in Czech are formed by a
verb followed by a reflexive clitic, whereas in Rus-
sian, the reflexivization is the affixation process:
(4) a. Filip
Filip
se
REFL-CL
jes?te?
still
nehol??.
not-shaves
?Filip doesn?t shave yet.? [Cz]
b. Filip
Filip
esc?e
still
ne
not
breet+sja.
shaves+REFL.SUFFIX
?Filip doesn?t shave yet.? [Ru]
Even though auxiliaries and the copula are the forms
of the same verb byt? ?to be?, both in Russian and in
Czech, the use of this verb is different in the two
languages. For example, Russian does not use an
auxiliary to form past tense:
(5) a. Ja?
I
jsem
aux1sg
psal.
wrote
?I was writing/I wrote.? [Cz]
b. Ja
I
pisal.
wrote
?I was writing/I wrote.? [Ru]
It also does not use the present tense copula, except
for emphasis; but it uses forms of the verb byt? in
some other constructions like past passive.
We implemented a number of simple ?russifica-
tions?. The combination of random omission of the
verb byt?, omission of the reflexive clitics, and nega-
tion transformation gave us the best results on the
development corpus. Their combination improves
the overall result from 68.0% to 69.4%. We admit
we expected a larger improvement.
5.5 Sub-taggers
One of the problems when tagging with a large
tagset is data sparsity; with 1000 tags there are
10003 potential trigrams. It is very unlikely that a
naturally occurring corpus will contain all the ac-
ceptable tag combinations with sufficient frequency
to reliably distinguish them from the unacceptable
combinations. However, not all morphological at-
tributes are useful for predicting the attributes of the
succeeding word (e.g., tense is not really useful for
case). We therefore tried to train the tagger on indi-
vidual components of the full tag, in the hope that
each sub-tagger would be able to learn what it needs
for prediction. This move has the additional bene-
fit of making the tag set of each such tagger smaller
and reducing data sparsity. We focused on the first 5
positions ? POS (P), SubPOS (S), gender (g), num-
ber (n), case (c) and person (p). The selection of
the slots is based on our linguistic intuition ? for
example it is reasonable to assume that the infor-
mation about part-of-speech and the agreement fea-
tures (gnc) of previous words should help in pre-
diction of the same slots of the current word; or
information about part-of-speech, case and person
should assist in determining person. On the other
hand, the combination of tense and case is prima fa-
cie unlikely to be much use for prediction. Indeed,
most of our expectations have been met. The perfor-
mance of some of the models on the development
corpus is summarized in Table 5. The bold num-
bers indicate that the tagger outperforms the full-tag
tagger. As can be seen, the taggers trained on indi-
vidual positions are worse than the full-tag tagger
on these positions. This proves that a smaller tagset
does not necessarily imply that tagging is easier ?
see (Elworthy, 1995) for more discussion of this in-
teresting relation. Similarly, there is no improve-
ment from the combination of unrelated slots ? case
and tense (ct) or gender and negation (ga). How-
ever, the combinations of (detailed) part-of-speech
with various agreement features (e.g., Snc) outper-
form the full-tag tagger on at least some of the slots.
full-tag P S g n c
1 (P) 89.0 87.2 ? ? ? ?
2 (S) 86.6 ? 84.5 ? ? ?
3 (g) 81.4 ? ? 78.8 ? ?
4 (n) 92.4 ? ? ? 91.2 ?
5 (c) 80.9 ? ? ? ? 78.4
full-tag Pc gc ga nc cp ct
1 (P) 89.0 87.5 ? ? ? ? ?
2 (S) 86.6 ? ? ? ? ? ?
3 (g) 81.4 ? 80.4 78.7 ? ? ?
4 (n) 92.4 ? ? ? 91.8 ? ?
5 (c) 80.9 80.6 81.1 ? 81.5 79.3 79.5
8 (p) 98.3 ? ? ? ? 96.9 ?
9 (t) 97.0 ? ? ? ? ? 96.1
11 (a) 97.0 ? ? 95.4 ? ? ?
full-tag Pgc Pnc Sgc Snc Sgnc
1 (P) 89.0 87.9 87.5 ? ? ?
2 (S) 86.6 ? ? 86.1 86.4 87.1
3 (g) 81.4 80.3 ? 81.4 ? 82.7
4 (n) 92.4 ? 92.4 ? 93.0 92.8
5 (c) 80.9 81.8 81.4 80.9 82.9 82.3
Table 5: Performance of the TnT tagger trained on
various subtags (development data)
5.6 Combining Sub-taggers
We now need to put the sub-tags back together to
produce estimates of the correct full tags. We can-
not simply combine the values offered by the best
taggers for each slot, because that could yield ille-
gal tags (e.g., nouns in past tense). Instead we select
the best tag from those offered by our morphologi-
cal analyzer using the following formula:
(6) bestTag = argmaxt?TMAval(t)
TMA ? the set of tags offered by MA
val(t) =?14k=0 Nk(t)/Nk
Nk(t) ? # of taggers voting for k-th slot of t
Nk ? the total # of taggers on slot k
That means, that the best tag is the tag that received
the highest average percentage of votes for each of
full-tag all best 1 best 3
overall 69.5 70.3 70.7 71.1
1 (P) 89.0 88.9 89.1 89.2
2 (S) 86.6 86.5 86.9 86.9
3 (g) 81.4 81.8 83.0 83.2
4 (n) 92.4 92.6 93.1 93.2
5 (c) 80.9 82.1 83.0 83.2
6 (G) 98.5 98.5 98.7 98.7
7 (N) 99.6 99.7 99.8 99.8
8 (p) 98.3 98.2 98.4 98.3
9 (t) 97.0 97.0 97.0 97.0
10 (G) 96.0 96.0 96.0 96.0
11 (a) 97.0 97.0 96.9 97.0
12 (v) 97.4 97.3 97.5 97.4
15 (V) 99.1 99.1 99.0 99.0
Table 6: Combining sub-taggers (development data)
Baseline Direct Russified Russified
Tagger random full-tag full-tag voting
Accuracy
Tags 33.6 69.4 72.6 73.5
1 (POS) 63.2 88.5 90.1 90.4
2 (SubPOS) 57.0 86.8 88.1 88.6
3 (Gender) 59.2 82.5 84.5 85.0
4 (Number) 75.9 91.2 92.6 93.4
5 (Case) 47.3 80.4 84.1 85.3
6 (PossGen) 83.4 98.4 98.8 99.0
7 (PossNr) 99.6 99.6 99.6 99.8
8 (Person) 97.1 99.3 98.9 98.9
9 (Tense) 86.6 96.5 97.6 97.6
10 (Grade) 90.1 95.9 96.6 96.6
11 (Neg) 81.4 95.3 95.5 95.5
12 (Voice) 86.4 97.2 97.9 97.9
15 (Variant) 97.0 99.1 99.5 99.5
Table 7: Tagging with various parameters (test data)
its slots. If we cared about certain slots more than
about others we could weight the slots in the val
function.
We ran several experiments, the results of three of
them are summarized in Table 6. All of them work
better than the full-tag tagger. One (?all?) uses all
available subtaggers, other (?best 1?) uses the best
tagger for each slot (therefore voting in Formula 6
reduces to finding a closest legal tag). The best re-
sult is obtained by the third tagger (?best 3?) which
uses the three best taggers for each of the Pgcp slots
and the best tagger for the rest. We selected this tag-
ger to tag the test corpus, for which the results are
summarized in Table 7.
Russian Gloss Correct Xerox Ours
?Clen member noun nom gen
partii party noun gen obl
po prep prep obl acc
vozmoz?nosti possibility noun obl acc
staralsja tried vfin
nje not ptcl
govorit? to-speak vinf
ni nor ptcl
o about prep obl
Bratstvje Brotherhood noun obl
, cm
ni nor ptcl
o about prep obl
knigje book noun obl
Errors 3 1
?Neither the Brotherhood nor the book was a subject
that any ordinary Party member would mention if
there was a way of avoiding it.? [Orwell: ?1984?]
Table 8: Tagging with Xerox & our tagger
5.7 Comparison with Xerox tagger
A tagger for Russian is part of the Xerox language
tools. We could not perform a detailed evaluation
since the tool is not freely available. We used the
online demo version of Xerox?s Disambiguator6 to
tag a few sentences and compared the results with
the results of our tagger. The Xerox tagset is much
smaller than ours, it uses 63 tags, collapsing some
cases, not distinguishing gender, number, person,
tense etc. (However, it uses different tags for dif-
ferent punctuation, while we have one tag for all
punctuation). For the comparison, we translated our
tagset to theirs. On 201 tokens of the testing cor-
pus, the Xerox tagger achieved an accuracy of 82%,
while our tagger obtained 88%; i.e., a 33% reduc-
tion in error rate. A sample analysis is in Table 8.
5.8 Comparison with Czech taggers
The numbers we obtain are significantly worse than
the numbers reported for Czech (Hajic? et al, 2001)
(95.16% accuracy); however, they use an extensive
manually created morphological lexicon (200K+
entries) which gives 100.0% recall on their testing
data. Moreover, they train and test their taggers on
the same language.
6 Ongoing Research
We are currently working on improving both the
morphological analysis and tagging. We would like
6http://www.xrce.xerox.com/
competencies/content-analysis/demos/
russian
to improve the recall of filters following morpholog-
ical analysis, e.g., using n maximal values instead
of 1, using some basic knowledge of derivational
morphology, etc. We are incorporating phonological
conditions on stems into the guesser module as well
as trying to deal with different morphological phe-
nomena specific to Russian, e.g., verb reflexiviza-
tion. However, we try to stay language independent
(at least within Slavic languages) as much as possi-
ble and limit the language dependent components to
a minimum.
Currently, we are working on more sophisticated
russifications that would be still easily portable to
other languages. For example, instead of omitting
auxiliaries randomly, we want to use the syntac-
tic information present in Prague Dependency Tree-
bank to omit only the ?right? ones.
If possible, we would like to avoid entirely throw-
ing away the Czech emission probabilities, because
our intuition tells us that there are useful lexical
similarities between Russian and Czech, and that
some suitable process of cognate detection will al-
low us to transfer information from the Czech to
the Russian emission probabilities. Just as a knowl-
edge of English words is sometimes helpful (mod-
ulo sound changes) when reading German, a knowl-
edge of the Czech lexicon should be helpful (mod-
ulo character set issues) when reading Russian. We
are seeking the right way to operationalize this in-
tuition in our system, bearing in mind that we want
a sufficiently general algorithm to make the method
portable to other languages, for which we assume
we have neither the time nor the expertise to under-
take knowledge-intensive work. A potentially suit-
able cognate algorithm is described by (Kondrak,
2001).
Finally, we would like to extend our work to Slavic
languages for which there are even fewer available
resources than Russian, such as Belarusian, since
this was the original motivation for undertaking the
work in the first place.
Acknowledgements
We thank Erhard Hinrichs and Eric Fosler-Lussier
for giving us feedback on previous versions of the
paper and providing useful suggestions for subtag-
gers and voting; Jan Hajic? for the help with the
Czech tag system and the morphological analyzer;
to the Clippers discussion group for allowing us to
interview ourselves in front of them, and for ensuing
discussion, and to two anonymous EMNLP review-
ers for extremely constructive feedback.
References
Alena Be?mova?, Jan Hajic?, Barbora Hladka?, and
Jarmila Panevova?. 1999. Morphological and syn-
tactic tagging of the prague dependency treebank.
In Proceedings of ATALA Workshop, pages 21?29.
Paris, France.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of ANLP-NAACL,
pages 224?231.
Sas?o Dz?eroski, Tomaz? Erjavec, and Jakub
Zavrel. 2000. Morphosyntactic Tagging of
Slovene:Evaluating Taggers and Tagsets. In Pro-
ceedings of the Second International Conference
on Language Resources and Evaluation, pages
1099?1104.
David Elworthy. 1995. Tagset design and inflected
languages. In EACL SIGDAT workshop ?From
Texts to Tags: Issues in Multilingual Language
Analysis?, pages 1?10, Dublin, April.
John Goldsmith. 2001. Unsupervised Learning of
the Morphology of a Natural Language. Computa-
tional Linguistics, 27(2):153?198.
Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva,
and Vladim??r Petkevic?. 2001. Serial Combination
of Rules and Statistics: A Case Study in Czech Tag-
ging. In Proceedings of ACL Conference, Toulouse,
France.
Jan Hajic?. 2000. Morphological Tagging: Data
vs. Dictionaries. In Proceedings of ANLP-NAACL
Conference, pages 94?101, Seattle, Washington,
USA.
Jiri Hana and Anna Feldman. to appear. Portable
Language Technology: The case of Czech and Rus-
sian. In Proceedings from the Midwest Computa-
tional Linguistics Colloquium, June 25-26, 2004,
Bloomington, Indiana.
Greg Kondrak. 2001. Identifying cognates by pho-
netic and semantic similarity. In Proceedings of
the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-2001), pages 103?110, June.
Andrey Kovalev. 2002. A Probabilistic Mor-
phological Analyzer for Russian and Ukranian.
http://linguist.nm.ru/stemka/stemka.html.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Andrei Mikheev and Liubov Liubushkina. 1995.
Russian Morphology: An Engineering Approach.
Natural Language Engineering, 3(1):235?260.
Ilya Segalovich and Michail Maslov. 1989.
Dictionary-based Russian morphological analy-
sis and synthesis with generation of morpho-
logical models of unknown words (in Russian).
http://company.yandex.ru/articles/article1.html.
Ilya Segalovich and Vitaly Titov. 2000. Au-
tomatic morphological annotation MYSTEM.
http://corpora.narod.ru/article.html.
Ilya Segalovich. 2003. A fast morpholog-
ical algorithm with unknown word guessing
induced by a dictionary for a web search
engine. http://company.yandex.ru/articles/iseg-las-
vegas.html.
Jean Ve?ronis. 1996. MULTEXT-
EAST (Copernicus 106).
http://www.lpl.univaix.fr/projects/multext-east.
Terence Wade. 1992. A Comprehensive Russian
Grammar. Blackwell. 582 pp.
Serge A. Yablonsky. 1999. Russian Morphological
Analysis. In Proceedings VEXTAL.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of the 38th
Meeting of the Association for Computational Lin-
guistics, pages 207?216.
Tagging Portuguese with a Spanish Tagger Using Cognates
Jirka Hana
Department of Linguistics
The Ohio State University
hana.1@osu.edu
Anna Feldman
Department of Linguistics
The Ohio State University
afeldman@ling.osu.edu
Luiz Amaral
Department of Spanish and Portuguese
The Ohio State University
amaral.1@osu.edu
Chris Brew
Department of Linguistics
The Ohio State University
cbrew@acm.org
Abstract
We describe a knowledge and resource
light system for an automatic morpholog-
ical analysis and tagging of Brazilian Por-
tuguese.1 We avoid the use of labor in-
tensive resources; particularly, large anno-
tated corpora and lexicons. Instead, we
use (i) an annotated corpus of Peninsular
Spanish, a language related to Portuguese,
(ii) an unannotated corpus of Portuguese,
(iii) a description of Portuguese morphol-
ogy on the level of a basic grammar book.
We extend the similar work that we have
done (Hana et al, 2004; Feldman et al,
2006) by proposing an alternative algo-
rithm for cognate transfer that effectively
projects the Spanish emission probabili-
ties into Portuguese. Our experiments use
minimal new human effort and show 21%
error reduction over even emissions on a
fine-grained tagset.
1 Introduction
Part of speech (POS) tagging is an important step
in natural language processing. Corpora that have
been POS-tagged are very useful both for linguis-
tic research, e.g. finding instances or frequencies
of particular constructions (Meurers, 2004) and for
further computational processing, such as syntac-
tic parsing, speech recognition, stemming, word-
sense disambiguation. Morphological tagging is
the process of assigning POS, case, number, gen-
der and other morphological information to each
word in a corpus. Despite the importance of mor-
phological tagging, there are many languages that
1We thank the anonymous reviewers for their constructive
comments on an earlier version of the paper.
lack annotated resources of this kind, mainly due
to the lack of training corpora which are usually
required for applying standard statistical taggers.
Applications of taggers include syntactic pars-
ing, stemming, text-to-speech synthesis, word-
sense disambiguation, information extraction. For
some of these getting all the tags right is inessen-
tial, e.g. the input to noun phrase chunking does
not necessarily require high accuracy fine-grained
tag resolution.
Cross-language information transfer is not new;
however, most of the existing work relies on par-
allel corpora (e.g. Hwa et al, 2004; Yarowsky
and Ngai, 2001) which are difficult to find, es-
pecially for lesser studied languages. In this pa-
per, we describe a cross-language method that re-
quires neither training data of the target language
nor bilingual lexicons or parallel corpora. We re-
port the results of the experiments done on Brazil-
ian Portuguese and Peninsular Spanish, however,
our system is not tied to these particular languages.
The method is easily portable to other (inflected)
languages. Our method assumes that an anno-
tated corpus exists for the source language (here,
Spanish) and that a text book with basic linguis-
tic facts about the source language is available
(here, Portuguese). We want to test the generality
and specificity of the method. Can the systematic
commonalities and differences between two ge-
netically related languages be exploited for cross-
language applications? Is the processing of Por-
tuguese via Spanish different from the processing
of Russian via Czech (Hana et al, 2004; Feldman
et al, 2006)?
33
Spanish Portuguese
1. sg. canto canto
2. sg. cantas cantas
3. sg. canta canta
1. pl. catamos cantamos
2. pl. cantais cantais
3. pl. cantan cantam
Table 1: Verb conjugation present indicative: -ar
regular verb: cantar ?to sing?
2 Brazilian Portuguese (BP)
vs. Peninsular Spanish (PS)
Portuguese and Spanish are both Romance lan-
guages from the Iberian Peninsula, and share
many morpho-syntactic characteristics. Both lan-
guages have a similar verb system with three main
conjugations (-ar, -er, -ir), nouns and adjectives
may vary in number and gender, and adverbs are
invariable. Both are pro-drop languages, they have
a similar pronominal system, and certain phenom-
ena, such as clitic climbing, are prevalent in both
languages. They also allow rather free constituent
order; and in both cases there is considerable de-
bate in the literature about the appropriate char-
acterization of their predominant word order (the
candidates being SVO and VSO).
Sometimes the languages exhibit near-complete
parallelism in their morphological patterns, as
shown in Table 1.
The languages are also similar in their lexicon and
syntactic word order:
(1) Os
Los
The
estudantes
estudiantes
students
ja?
ya
already
comparam
compraron
bought
os
los
the
livros.
libros.
books
[BP]
[PS]
?The students have already bought the
books.?
One of the main differences is the fact that
Brazilian Portuguese (BP) accepts object drop-
ping, while Peninsular Spanish (PS) doesn?t. In
addition, subjects in BP tend to be overt while in
PS they tend to be omitted.
(2) a. A: O que
What
voce?
you
fez
did
com
with
o
the
livro?
book?
[BP]
A: ?What did you do with the book??
B: Eu
I
dei
gave
para
to
Maria.
Mary
B: ?I gave it to Mary.?
b. A: ?Que?
What
hiciste
did
con
with
el
the
libro?
book?
[PS]
A: ?What did you do with the book??
B: Se
Her.dat
lo
it.acc
di
gave
a
to
Mar??a.
Mary.
B: ?I gave it to Mary.?
Notice also that in the Spanish example (2b) the
dative pronoun se ?her? is obligatory even when
the prepositional phrase a Mar??a ?to Mary? is
present.
3 Resources
3.1 Tagset
For both Spanish and Portuguese, we used posi-
tional tagsets developed on the basis of Spanish
CLiC-TALP tagset (Torruella, 2002). Every tag is
a string of 11 symbols each corresponding to one
morphological category. For example, the Por-
tuguese word partires ?you leave? is assigned the
tag VM0S---2PI-, because it is a verb (V), main
(M), gender is not applicable to this verb form (0),
singular (S), case, possesor?s number and form are
not applicable to this category(-), 2nd person (2),
present (P), indicative (I) and participle type is not
applicable (-).
A comparison of the two tagsets is in Table 2.2
When possible the Spanish and Portuguese tagsets
use the same values, however some differences are
unavoidable. For instance, the pluperfect is a com-
pound verb tense in Spanish, but a separate word
that needs a tag of its own in Portuguese. In ad-
dition, we added a tag for ?treatment? Portuguese
pronouns.
The Spanish tagset has 282 tags, while that for
Portuguese has 259 tags.
3.2 Training corpora
Spanish training corpus. The Spanish corpus
we use for training the transition probabilities as
well as for obtaining Spanish-Portuguese cognate
pairs is a fragment (106,124 tokens, 18,629 types)
of the Spanish section of CLiC-TALP (Torruella,
2Notice that we have 6 possible values for the gender po-
sition: M (masc.), F (fem.), N (neutr., for certain pronouns), C
(common, either M or F), 0 (unspecified for this form within
the category), - (the category does not distinguish gender)
34
No. Description No. of values
Sp Po
1 POS 14 11
2 SubPOS ? detailed POS 30 29
3 Gender 6 6
4 Number 5 5
5 Case 6 6
6 Possessor?s Number 4 4
7 Form 3 3
8 Person 5 5
9 Tense 7 9
10 Mood 8 9
11 Participle 3 3
Table 2: Overview and comparison of the tagsets
2002). CLiC-TALP is a balanced corpus, contain-
ing texts of various genres and styles. We automat-
ically translated the CLiC-TALP tagset into our
system (see Sect. 3.1) for easier detailed evalua-
tion and for comparison with our previous work
that used a similar approach for tagging (Hana
et al, 2004; Feldman et al, 2006).
Raw Portuguese corpus. For automatic lexi-
con acquisition, we use NILC corpus,3 containing
1.2M tokens.
3.3 Evaluation corpus
For evaluation purposes, we selected and manually
annotated a small portion (1,800 tokens) of NILC
corpus.
4 Morphological Analysis
Our morphological analyzer (Hana, 2005) is an
open and modular system. It allows us to com-
bine modules with different levels of manual in-
put ? from a module using a small manually pro-
vided lexicon, through a module using a large lex-
icon automatically acquired from a raw corpus, to
a guesser using a list of paradigms, as the only
resource provided manually. The general strat-
egy is to run modules that make fewer errors and
less overgenerate before modules that make more
errors and overgenerate more. This, for exam-
ple, means that modules with manually created
resources are used before modules with resources
3Nu?cleo Interdisciplinar de Lingu???stica Computacional;
available at http://nilc.icmc.sc.usp.br/nilc/,
we used the version with POS tags assigned by PALAVRAS.
We ignored the POS tags.
automatically acquired. In the experiments below,
we used the following modules ? lookup in a list
of (mainly) closed-class words, a paradigm-based
guesser and an automatically acquired lexicon.
4.1 Portuguese closed class words
We created a list of the most common preposi-
tions, conjunctions, and pronouns, and a number
of the most common irregular verbs. The list con-
tains about 460 items and it required about 6 hours
of work. In general, the closed class words can be
derived either from a reference grammar book, or
can be elicited from a native speaker. This does
not require native-speaker expertise or intensive
linguistic training. The reason why the creation
of such a list took 6 hours is that the words were
annotated with detailed morphological tags used
by our system.
4.2 Portuguese paradigms
We also created a list of morphological paradigms.
Our database contains 38 paradigms. We just en-
coded basic facts about the Portuguese morphol-
ogy from a standard grammar textbook (Cunha
and Cintra, 2001). The paradigms include all three
regular verb conjugations (-ar, -er, -ir), the most
common adjective and nouns paradigms and a rule
for adverbs of manner that end with -mente (anal-
ogous to the English -ly). We ignore majority of
exceptions. The creation of the paradigms took
about 8 h of work.
4.3 Lexicon Acquisition
The morphological analyzer supports a module or
modules employing a lexicon containing informa-
tion about lemmas, stems and paradigms. There is
always the possibility to provide this information
manually. That, however, is very costly. Instead,
we created such a lexicon automatically.
Usually, automatically acquired lexicons and
similar systems are used as a backup for large
high-precision high-cost manually created lexi-
cons (e.g. Mikheev, 1997; Hlava?c?ova?, 2001). Such
systems extrapolate the information about the
words known by the lexicon (e.g. distributional
properties of endings) to unknown words. Since
our approach is resource light, we do not have any
such large lexicon to extrapolate from.
The general idea of our system is very sim-
ple. The paradigm-based Guesser, provides all the
possible analyses of a word consistent with Por-
tuguese paradigms. Obviously, this approach mas-
35
sively overgenerates. Part of the ambiguity is usu-
ally real but most of it is spurious. We use a large
corpus to weed the spurious analyses out of the
real ones. In such corpus, open-class lemmas are
likely to occur in more than one form. Therefore,
if a lemma+paradigm candidate suggested by the
Guesser occurs in other forms in other parts of the
corpus, it increases the likelihood that the candi-
date is real and vice versa. If we encounter the
word cantamos ?we sing? in a Portuguese corpus,
using the information about the paradigms we can
analyze it in two ways, either as being a noun in
the plural with the ending -s, or as being a verb in
the 1st person plural with the ending -amos. Based
on this single form we cannot say more. However
if we also encounter the forms canto, canta, can-
tam the verb analysis becomes much more prob-
able; and therefore, it will be chosen for the lex-
icon. If the only forms that we encounter in our
Portuguese corpus were cantamos and (the non-
existing) cantamo (such as the existing word ramo
and ramos) then we would analyze it as a noun and
not as a verb.
With such an approach, and assuming that the
corpus contains the forms of the verb matar ?to
kill?, mato1sg matas2sg, mata3sg, etc., we would
not discover that there is also a noun mata ?forest?
with a plural form matas ? the set of the 2 noun
forms is a proper subset of the verb forms. A sim-
ple solution is to consider not the number of form
types covered in a corpus, but the coverage of the
possible forms of the particular paradigm. How-
ever this brings other problems (e.g. it penalizes
paradigms with large number of forms, paradigms
with some obsolete forms, etc.). We combine both
of these measures in Hana (2005).
Lexicon Acquisition consists of three steps:
1. A large raw corpus is analyzed with a
lexicon-less MA (an MA using a list of
mainly closed-class words and a paradigm
based guesser);
2. All possible hypothetical lexical entries over
these analyses are created.
3. Hypothetical entries are filtered with aim to
discard as many nonexisting entries as possi-
ble, without discarding real entries.
Obviously, morphological analysis based on
such a lexicon still overgenerates, but it overgener-
ates much less than if based on the endings alone.
Lexicon no yes
recall 99.0 98.1
avg ambig (tag/word) 4.3 3.5
Tagging (cognates) ? accuracy 79.1 82.1
Table 3: Evaluation of Morphological analysis
Consider for example, the form func?o?es ?func-
tions? of the feminine noun func?a?o. The analyzer
without a lexicon provides 11 analyses (6 lemmas,
each with 1 to 3 tags); only one of them is cor-
rect. In contrast, the analyzer with an automati-
cally acquired lexicon provides only two analyses:
the correct one (noun fem. pl.) and an incorrect
one (noun masc. pl., note that POS and number
are still correct). Of course, not all cases are so
persuasive.
The evaluation of the system is in Table 3. The
98.1% recall is equivalent to the upper bound for
the task. It is calculated assuming an oracle-
Portuguese tagger that is always able to select the
correct POS tag if it is in the set of options given
by the morphological analyzer. Notice also that
for the tagging accuracy, the drop of recall is less
important than the drop of ambiguity.
5 Tagging
We used the TnT tagger (Brants, 2000), an im-
plementation of the Viterbi algorithm for second-
order Markov model. In the traditional approach,
we would train the tagger?s transitional and emis-
sion probabilities on a large annotated corpus of
Portuguese. However, our resource-light approach
means that such corpus is not available to us and
we need to use different ways to obtain this infor-
mation.
We assume that syntactic properties of Spanish
and Portuguese are similar enough to be able to
use the transitional probabilities trained on Span-
ish (after a simple tagset mapping).
The situation with the lexical properties as cap-
tured by emission probabilities is more complex.
Below we present three different ways how to ob-
tains emissions, assuming:
1. they are the same: we use the Spanish emis-
sions directly (?5.1).
2. they are different: we ignore the Spanish
emissions and instead uniformly distribute
36
the results of our morphological analyzer.
(?5.2)
3. they are similar: we map the Spanish emis-
sions onto the result of morphological analy-
sis using automatically acquired cognates.
(?5.3)
5.1 Tagging ? Baseline
Our lowerbound measurement consists of training
the TnT tagger on the Spanish corpus and apply-
ing this model directly to Portuguese.4 The overall
performance of such a tagger is 56.8% (see the the
min column in Table 4). That means that half of
the information needed for tagging of Portuguese
is already provided by the Spanish model. This
tagger has seen no Portuguese whatsoever, and is
still much better than nothing.
5.2 Tagging ? Approximating Emissions I
The opposite extreme to the baseline, is to assume
that Spanish emissions are useless for tagging Por-
tuguese. Instead we use the morphological an-
alyzer to limit the number of possibilities, treat-
ing them all equally ? The emission probabilities
would then form a uniform distribution of the tags
given by the analyzer. The results are summarized
in Table 4 (the e-even column) ? accuracy 77.2%
on full tags, or 47% relative error reduction against
the baseline.
5.3 Tagging ? Approximating Emissions II
Although it is true that forms and distributions of
Portuguese and Spanish words are not the same,
they are also not completely unrelated. As any
Spanish speaker would agree, the knowledge of
Spanish words is useful when trying to understand
a text in Portuguese.
Many of the corresponding Portuguese and
Spanish words are cognates, i.e. historically they
descend from the same ancestor root or they are
mere translations. We assume two things: (i) cog-
nate pairs have usually similar morphological and
distributional properties, (ii) cognate words are
similar in form.
Obviously both of these assumptions are ap-
proximations:
1. Cognates could have departed in their mean-
ings, and thus probably also have dif-
4Before training, we translated the Spanish tagset into the
Portuguese one.
ferent distributions. For example, Span-
ish embarazada ?pregnant? vs. Portuguese
embarac?ada ?embarrassed?.
2. Cognates could have departed in their mor-
phological properties. For example, Span-
ish cerca ?near?.adverb vs. Portuguese cerca
?fence?.noun (from Latin circa, circus ?cir-
cle?).
3. There are false cognates ? unrelated,
but similar or even identical words. For
example, Spanish salada ?salty?.adj vs. Por-
tuguese salada ?salad?.noun, Spanish doce
?twelve?.numeral vs. Portuguese doce
?candy?.noun
Nevertheless, we believe that these examples
are true exceptions from the rule and that in major-
ity of cases, the cognates would look and behave
similarly. The borrowings, counter-borrowings
and parallel developments of the various Romance
languages have of course been extensively studied,
and we have no space for a detailed discussion.
Identifying cognates. For the present work,
however, we do not assume access to philologi-
cal erudition, or to accurate Spanish-Portuguese
translations or even a sentence-aligned corpus. All
of these are resources that we could not expect to
obtain in a resource poor setting. In the absence
of this knowledge, we automatically identify cog-
nates, using the edit distance measure (normalized
by word length).
Unlike in the standard edit distance, the cost of
operations is dependent on the arguments. Simi-
larly as Yarowsky and Wicentowski (2000), we as-
sume that, in any language, vowels are more muta-
ble in inflection than consonants, thus for example
replacing a for i is cheaper that replacing s by r.
In addition, costs are refined based on some well
known and common phonetic-orthographic regu-
larities, e.g. replacing a q with c is less costly than
replacing m with, say s. However, we do not want
to do a detailed contrastive morpho-phonological
analysis, since we want our system to be portable
to other languages. So, some facts from a simple
grammar reference book should be enough.
Using cognates. Having a list of Spanish-
Portuguese cognate pairs, we can use these to
map the emission probabilities acquired on Span-
ish corpus to Portuguese.
37
Let?s assume Spanish word ws and Portuguese
word wp are cognates. Let Ts denote the tags that
ws occurs within the Spanish corpus, and let ps(t)
be the emission probability of a tag t (t 6? Ts ?
ps(t) = 0). Let Tp denote tags assigned to the
Portuguese word wp by our morphological ana-
lyzer, and the pp(t) is the even emission proba-
bility: pp(t) = 1|Tp| . Then we can assign the new
emission probability p?p(t) to every tag t ? Tp in
the following way (followed by normalization):
p?p(t) =
ps(t) + pp(t)
2
(1)
Results. This method provides the best results.
The full-tag accuracy is 82.1%, compared to
56.9% for baseline (58% error rate reduction) and
77.2% for even-emissions (21% reduction). The
accuracy for POS is 87.6%. Detailed results are in
column e-cognates of Table 4.
6 Evaluation & Comparison
The best way to evaluate our results would be to
compare it against the TnT tagger used the usual
way ? trained on Portuguese and applied on Por-
tuguese. We do not have access to a large Por-
tuguese corpus annotated with detailed tags. How-
ever, we believe that Spanish and Portuguese are
similar enough (see Sect. 2) to justify our assump-
tion that the TnT tagger would be equally success-
ful (or unsuccessful) on them. The accuracy of
TnT trained on 90K tokens of the CLiC-TALP cor-
pus is 94.2% (tested on 16K tokens). The accuracy
of our best tagger is 82.1%. Thus the error-rate is
more than 3 times bigger (17.9% vs. 5.4%).
Branco and Silva (2003) report 97.2% tagging
accuracy on 23K testing corpus. This is clearly
better than our results, on the other hand they
needed a large Portuguese corpus of 207K tokens.
The details of the tagset used in the experiments
are not provided, so precise comparison with our
results is difficult.
7 Related work
Previous research in resource-light language
learning has defined resource-light in different
ways. Some have assumed only partially tagged
training corpora (Merialdo, 1994); some have be-
gun with small tagged seed wordlists (Cucerzan
and Yarowsky, 1999) for named-entity tagging,
while others have exploited the automatic trans-
fer of an already existing annotated resource in a
min e-even e-cognates
Tag: 56.9 77.2 82.1
POS: 65.3 84.2 87.6
SubPOS: 61.7 83.3 86.9
gender: 70.4 87.3 90.2
number: 78.3 95.3 96.0
case: 93.8 96.8 97.2
possessor?s num: 85.4 96.7 97.0
form: 92.9 99.2 99.2
person: 74.5 91.2 92.7
tense: 90.7 95.1 96.1
mood: 91.5 95.0 96.0
participle: 99.9 100.0 100.0
Table 4: Tagging Brazilian Portuguese
different genres or a different language (e.g. cross-
language projection of morphological and syn-
tactic information in (Yarowsky et al, 2001;
Yarowsky and Ngai, 2001), requiring no direct su-
pervision in the target language).
Ngai and Yarowsky (2000) observe that the to-
tal weighted human and resource costs is the most
practical measure of the degree of supervision.
Cucerzan and Yarowsky (2002) observe that an-
other useful measure of minimal supervision is the
additional cost of obtaining a desired functional-
ity from existing commonly available knowledge
sources. They note that for a remarkably wide
range of languages, there exist a plenty of refer-
ence grammar books and dictionaries which is an
invaluable linguistic resource.
7.1 Resource-light approaches to Romance
languages
Cucerzan and Yarowsky (2002) present a method
for bootstrapping a fine-grained, broad coverage
POS tagger in a new language using only one
person-day of data acquisition effort. Similarly
to us, they use a basic library reference gram-
mar book, and access to an existing monolingual
text corpus in the language, but they also use a
medium-sized bilingual dictionary.
In our work, we use a paradigm-based mor-
phology, including only the basic paradigms from
a standard grammar textbook. Cucerzan and
Yarowsky (2002) create a dictionary of regular in-
flectional affix changes and their associated POS
and on the basis of it, generate hypothesized in-
flected forms following the regular paradigms.
38
Clearly, these hypothesized forms are inaccurate
and overgenerated. Therefore, the authors perform
a probabilistic match from all lexical tokens actu-
ally observed in a monolingual corpus and the hy-
pothesized forms. They combine these two mod-
els, a model created on the basis of dictionary in-
formation and the one produced by the morpho-
logical analysis. This approach relies heavily on
two assumptions: (i) words of the same POS tend
to have similar tag sequence behavior; and (ii)
there are sufficient instances of each POS tag la-
beled by either the morphology models or closed-
class entries. For richly inflectional languages,
however, there is no guarantee that the latter as-
sumption would always hold.
The accuracy of their model is comparable to
ours. On a fine-grained (up to 5-feature) POS
space, they achieve 86.5% for Spanish and 75.5%
for Romanian. With a tagset of a similar size (11
features) we obtain the accuracy of 82.1% for Por-
tuguese.
Carreras et al (2003) present work on develop-
ing low-cost Named Entity recognizers (NER) for
a language with no available annotated resources,
using as a starting point existing resources for a
similar language. They devise and evaluate several
strategies to build a Catalan NER system using
only annotated Spanish data and unlabeled Cata-
lan text, and compare their approach with a classi-
cal bootstrapping setting where a small initial cor-
pus in the target language is hand tagged. It turns
out that the hand translation of a Spanish model is
better than a model directly learned from a small
hand annotated training corpus of Catalan. The
best result is achieved using cross-linguistic fea-
tures. Solorio and Lo?pez (2005) follow their ap-
proach; however, they apply the NER system for
Spanish directly to Portuguese and train a classi-
fier using the output and the real classes.
7.2 Cognates
Mann and Yarowsky (2001) present a method for
inducing translation lexicons based on trasduction
modules of cognate pairs via bridge languages.
Bilingual lexicons within language families are in-
duced using probabilistic string edit distance mod-
els. Translation lexicons for abitrary distant lan-
guage pairs are then generated by a combination
of these intra-family translation models and one
or more cross-family online dictionaries. Simi-
larly to Mann and Yarowsky (2001), we show that
languages are often close enough to others within
their language family so that cognate pairs be-
tween the two are common, and significant por-
tions of the translation lexicon can be induced with
high accuracy where no bilingual dictionary or
parallel corpora may exist.
8 Conclusion
We have shown that a tagging system with a small
amount of manually created resources can be suc-
cessful. We have previously shown that this ap-
proach can work for Czech and Russian (Hana
et al, 2004; Feldman et al, 2006). Here we have
shown its applicability to a new language pair.
This can be done in a fraction of the time needed
for systems with extensive manually created re-
sources: days instead of years. Three resources
are required: (i) a reference grammar (for infor-
mation about paradigms and closed class words);
(ii) a large amount of text (for learning a lexicon;
e.g. newspapers from the internet); (iii) a limited
access to a native speaker ? reference grammars
are often too vague and a quick glance at results
can provide feedback leading to a significant in-
crease of accuracy; however both of these require
only limited linguistic knowledge.
In this paper we proposed an algorithm for cog-
nate transfer that effectively projects the source
language emission probabilities into the target lan-
guage. Our experiments use minimal new human
effort and show 21% error reduction over even
emissions on a fine-grained tagset.
In the near future, we plan to compare the ef-
fectiveness (time and price) of our approach with
that of the standard resource-intensive approach to
annotating a medium-size corpus (on a corpus of
around 100K tokens). A resource-intensive sys-
tem will be more accurate in the labels which it of-
fers to the annotator, so annotator can work faster
(there are fewer choices to make, fewer keystrokes
required). On the other hand, creation of the in-
frastructure for such a system is very time con-
suming and may not be justified by the intended
application.
The experiments that we are running right now
are supposed to answer the question of whether
training the system on a small corpus of a closely
related language is better than training on a larger
corpus of a less related language. Some prelim-
inary results (Feldman et al, 2006) suggest that
using cross-linguistic features leads to higher pre-
39
cision, especially for the source languages which
have target-like properties complementary to each
other.
9 Acknowledgments
We would like to thank Maria das Grac?as
Volpe Nunes, Sandra Maria Alu??sio, and Ricardo
Hasegawa for giving us access to the NILC cor-
pus annotated with PALAVRAS and to Carlos
Rodr??guez Penagos for letting us use the Spanish
part of the CLiC-TALP corpus.
References
Branco, A. and J. Silva (2003). Portuguese-
specific Issues in the Rapid Development of
State-of-the-art Taggers. In Workshop on Tag-
ging and Shallow Processing of Portuguese:
TASHA?2000.
Brants, T. (2000). TnT ? A Statistical Part-
of-Speech Tagger. In Proceedings of ANLP-
NAACL, pp. 224?231.
Carreras, X., L. Ma`rquez, and L. Padro? (2003).
Named Entity Recognition for Catalan Using
Only Spanish Resources and Unlabelled Data.
In Proceedings of EACL-2003.
Cucerzan, S. and D. Yarowsky (1999). Lan-
guage Independent Named Entity Recognition
Combining Morphological and Contextual Ev-
idence. In Proceedings of the 1999 Joint SIG-
DAT Conference on EMNLP and VLC, pp. 90?
99.
Cucerzan, S. and D. Yarowsky (2002). Boot-
strapping a Multilingual Part-of-speech Tagger
in One Person-day. In Proceedings of CoNLL
2002, pp. 132?138.
Cunha, C. and L. F. L. Cintra (2001). Nova
Grama?tica do Portugue?s Contempora?neo. Rio
de Janeiro, Brazil: Nova Fronteira.
Feldman, A., J. Hana, and C. Brew (2006). Experi-
ments in Morphological Annotation Transfer. In
Proceedings of Computational Linguistics and
Intelligent Text Processing (CICLing).
Hana, J. (2005). Knowledge and labor light mor-
phological analysis. Unpublished manuscript.
Hana, J., A. Feldman, and C. Brew (2004). A
Resource-light Approach to Russian Morphol-
ogy: Tagging Russian using Czech resources.
In Proceedings of EMNLP 2004, Barcelona,
Spain.
Hlava?c?ova?, J. (2001). Morphological Guesser
or Czech Words. In V. Matous?ek (Ed.), Text,
Speech and Dialogue, Lecture Notes in Com-
puter Science, pp. 70?75. Berlin: Springer-
Verlag.
Hwa, R., P. Resnik, A. Weinberg, C. Cabezas,
and O. Kolak (2004). Bootstrapping Parsers via
Syntactic Projection across Parallel Texts. Nat-
ural Language Engineering 1(1), 1?15.
Mann, G. S. and D. Yarowsky (2001). Multipath
Translation Lexicon via Bridge Languages. In
Proceedings of NAACL 2001.
Merialdo, B. (1994). Tagging English Text with
a Probabilistic Model. Computational Linguis-
tics 20(2), 155?172.
Meurers, D. (2004). On the Use of Electronic Cor-
pora for Theoretical Linguistics. Case Studies
from the Syntax of German. Lingua.
Mikheev, A. (1997). Automatic Rule Induction
for Unknown Word Guessing. Computational
Linguistics 23(3), 405?423.
Ngai, G. and D. Yarowsky (2000). Rule Writing or
Annotation: Cost-efficient Resource Usage for
Base Noun Phrase Chunking. In Proceedings of
the 38th Meeting of ACL, pp. 117?125.
Solorio, T. and A. L. Lo?pez (2005). Learning
named entity recognition in Portuguese from
Spanish. In Proceedings of Computational Lin-
guistics and Intelligent Text Processing (CI-
CLing).
Torruella, M. (2002). Gu??a para la anotacio?n mor-
folo?gica del corpus CLiC-TALP (Versio?n 3).
Technical Report WP-00/06, X-Tract Working
Paper.
Yarowsky, D. and G. Ngai (2001). Inducing Mul-
tilingual POS Taggers and NP Bracketers via
Robust Projection Across Aligned Corpora. In
Proceedings of NAACL-2001, pp. 200?207.
Yarowsky, D., G. Ngai, and R. Wicentowski
(2001). Inducing Multilingual Text Analy-
sis Tools via Robust Projection across Aligned
Corpora. In Proceedings of HLT 2001, First
International Conference on Human Language
Technology Research.
Yarowsky, D. and R. Wicentowski (2000). Min-
imally supervised morphological analysis by
multimodal alignment. In Proceedings of the
38th Meeting of the Association for Computa-
tional Linguistics, pp. 207?216.
40
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2019?2027,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Classifying Idiomatic and Literal Expressions Using Topic Models and
Intensity of Emotions
Jing Peng & Anna Feldman
Computer Science/Linguistics
Montclair State University
Montclair, New Jersey, USA
{pengj,feldmana}@mail.montclair.edu
Ekaterina Vylomova
Computer Science
Bauman State Technical University
Moscow, Russia
evylomova@gmail.com
Abstract
We describe an algorithm for automatic clas-
sification of idiomatic and literal expressions.
Our starting point is that words in a given text
segment, such as a paragraph, that are high-
ranking representatives of a common topic of
discussion are less likely to be a part of an id-
iomatic expression. Our additional hypothesis
is that contexts in which idioms occur, typi-
cally, are more affective and therefore, we in-
corporate a simple analysis of the intensity of
the emotions expressed by the contexts. We
investigate the bag of words topic represen-
tation of one to three paragraphs containing
an expression that should be classified as id-
iomatic or literal (a target phrase). We ex-
tract topics from paragraphs containing idioms
and from paragraphs containing literals us-
ing an unsupervised clustering method, Latent
Dirichlet Allocation (LDA) (Blei et al., 2003).
Since idiomatic expressions exhibit the prop-
erty of non-compositionality, we assume that
they usually present different semantics than
the words used in the local topic. We treat
idioms as semantic outliers, and the identifi-
cation of a semantic shift as outlier detection.
Thus, this topic representation allows us to dif-
ferentiate idioms from literals using local se-
mantic contexts. Our results are encouraging.
1 Introduction
The definition of what is literal and figurative is still
object of debate. Ariel (2002) demonstrates that lit-
eral and non-literal meanings cannot always be distin-
guished from each other. Literal meaning is originally
assumed to be conventional, compositional, relatively
context independent, and truth conditional. The prob-
lem is that the boundary is not clear-cut, some figu-
rative expressions are compositional ? metaphors and
many idioms; others are conventional ? most of the id-
ioms. Idioms present great challenges for many Natu-
ral Language Processing (NLP) applications. They can
violate selection restrictions (Sporleder and Li, 2009)
as in push one?s luck under the assumption that only
concrete things can normally be pushed. Idioms can
disobey typical subcategorization constraints (e.g., in
line without a determiner before line), or change the
default assignments of semantic roles to syntactic cate-
gories (e.g., in X breaks something with Y, Y typically
is an instrument but for the idiom break the ice, it is
more likely to fill a patient role as in How to break the
ice with a stranger). In addition, many potentially id-
iomatic expressions can be used either literally or fig-
uratively, depending on the context. This presents a
great challenge for machine translation. For example,
a machine translation system must translate held fire
differently in Now, now, hold your fire until I?ve had a
chance to explain. Hold your fire, Bill. You?re too quick
to complain. and The sergeant told the soldiers to hold
their fire. Please hold your fire until I get out of the
way. In fact, we tested the last two examples using the
Google Translate engine and we got proper translations
of the two neither into Russian nor into Hebrew, Span-
ish, or Chinese. Most current translation systems rely
on large repositories of idioms. Unfortunately, these
systems are not capable to tell apart literal from figura-
tive usage of the same expression in context. Despite
the common perception that phrases that can be idioms
are mainly used in their idiomatic sense, Fazly et al.
(2009)?s analysis of 60 idioms has shown that close to
half of these also have a clear literal meaning; and of
those with a literal meaning, on average around 40% of
their usages are literal.
In this paper we describe an algorithm for automatic
classification of idiomatic and literal expressions. Our
starting point is that words in a given text segment,
such as a paragraph, that are high-ranking representa-
tives of a common topic of discussion are less likely
to be a part of an idiomatic expression. Our additional
hypothesis is that contexts in which idioms occur, typ-
ically, are more affective and therefore, we incorpo-
rate a simple analysis of the intensity of the emotions
expressed by the contexts. We investigate the bag of
words topic representation of one to three paragraphs
containing an expression that should be classified as
idiomatic or literal (a target phrase). We extract top-
ics from paragraphs containing idioms and from para-
graphs containing literals using an unsupervised clus-
tering method, Latent Dirichlet Allocation (LDA) (Blei
et al., 2003). Since idiomatic expressions exhibit the
property of non-compositionality, we assume that they
usually present different semantics than the words used
2019
in the local topic. We treat idioms as semantic outliers,
and the identification of semantic shift as outlier detec-
tion. Thus, this topic representation allows us to differ-
entiate idioms from literals using the local semantics.
The paper is organized as follows. Section 2 briefly
describes previous approaches to idiom recognition or
classification. In Section 3 we describe our approach in
detail, including the hypothesis, the topic space repre-
sentation, and the proposed algorithm. After describing
the preprocessing procedure in Section 4, we turn to the
actual experiments in Sections 5 and 6. We then com-
pare our approach to other approaches (Section 7) and
discuss the results (Section 8).
2 Previous Work
Previous approaches to idiom detection can be classi-
fied into two groups: 1) Type-based extraction, i.e., de-
tecting idioms at the type level; 2) token-based detec-
tion, i.e., detecting idioms in context. Type-based ex-
traction is based on the idea that idiomatic expressions
exhibit certain linguistic properties that can distinguish
them from literal expressions (Sag et al. (2002); Fa-
zly et al. (2009)), among many others, discuss various
properties of idioms. Some examples of such proper-
ties include 1) lexical fixedness: e.g., neither ?shoot
the wind? nor ?hit the breeze? are valid variations of
the idiom shoot the breeze and 2) syntactic fixedness:
e.g., The guy kicked the bucket is potentially idiomatic
whereas The bucket was kicked is not idiomatic any-
more; and of course, 3) non-compositionality. Thus,
some approaches look at the tendency for words to oc-
cur in one particular order, or a fixed pattern. Hearst
(1992) identifies lexico-syntactic patterns that occur
frequently, are recognizable with little or no precoded
knowledge, and indicate the lexical relation of interest.
Widdows and Dorow (2005) use Hearst?s concept of
lexicosyntactic patterns to extract idioms that consist
of fixed patterns between two nouns. Basically, their
technique works by finding patterns such as ?thrills and
spills?, whose reversals (such as ?spills and thrills?) are
never encountered.
While many idioms do have these properties, many
idioms fall on the continuum from being composi-
tional to being partly unanalyzable to completely non-
compositional (Cook et al. (2007)). Fazly et al. (2009);
Li and Sporleder (2010), among others, notice that
type-based approaches do not work on expressions that
can be interpreted idiomatically or literally depending
on the context and thus, an approach that considers to-
kens in context is more appropriate for the task of idiom
recognition.
A number of token-based approaches have been
discussed in the literature, both supervised (Katz
and Giesbrech (2006)), weakly supervised (Birke and
Sarkar (2006)) and unsupervised (Sporleder and Li
(2009); Fazly et al. (2009)). Fazly et al. (2009) de-
velop statistical measures for each linguistic property
of idiomatic expressions and use them both in a type-
based classification task and in a token identification
task, in which they distinguish idiomatic and literal us-
ages of potentially idiomatic expressions in context.
Sporleder and Li (2009) present a graph-based model
for representing the lexical cohesion of a discourse.
Nodes represent tokens in the discourse, which are con-
nected by edges whose value is determined by a seman-
tic relatedness function. They experiment with two dif-
ferent approaches to semantic relatedness: 1) Depen-
dency vectors, as described in Pado and Lapata (2007);
2) Normalized Google Distance (Cilibrasi and Vit?anyi
(2007)). Sporleder and Li (2009) show that this method
works better for larger contexts (greater than five para-
graphs). Li and Sporleder (2010) assume that literal
and figurative data are generated by two different Gaus-
sians, literal and non-literal and the detection is done by
comparing which Gaussian model has a higher prob-
ability to generate a specific instance. The approach
assumes that the target expressions are already known
and the goal is to determine whether this expression is
literal or figurative in a particular context. The impor-
tant insight of this method is that figurative language
in general exhibits less semantic cohesive ties with the
context than literal language.
Feldman and Peng (2013) describe several ap-
proaches to automatic idiom identification. One of
them is idiom recognition as outlier detection. They
apply principal component analysis for outlier detec-
tion ? an approach that does not rely on costly an-
notated training data and is not limited to a specific
type of a syntactic construction, and is generally lan-
guage independent. The quantitative analysis provided
in their work shows that the outlier detection algorithm
performs better and seems promising. The qualitative
analysis also shows that their algorithm has to incor-
porate several important properties of the idioms: (1)
Idioms are relatively non-compositional, comparing to
literal expressions or other types of collocations. (2)
Idioms violate local cohesive ties, as a result, they are
semantically distant from the local topics. (3) While
not all semantic outliers are idioms, non-compositional
semantic outliers are likely to be idiomatic. (4) Id-
iomaticity is not a binary property. Idioms fall on the
continuum from being compositional to being partly
unanalyzable to completely non-compositional.
The approach described below is taking Feldman
and Peng (2013)?s original idea and is trying to address
(2) directly and (1) indirectly. Our approach is also
somewhat similar to Li and Sporleder (2010) because it
also relies on a list of potentially idiomatic expressions.
3 Our Hypothesis
Similarly to Feldman and Peng (2013), out starting
point is that idioms are semantic outliers that violate
cohesive structure, especially in local contexts. How-
ever, our task is framed as supervised classification and
we rely on data annotated for idiomatic and literal ex-
pressions. We hypothesize that words in a given text
2020
segment, such as a paragraph, that are high-ranking
representatives of a common topic of discussion are
less likely to be a part of an idiomatic expression in
the document.
3.1 Topic Space Representation
Instead of the simple bag of words representation of a
target document (segment of three paragraphs that con-
tains a target phrase), we investigate the bag of words
topic representation for target documents. That is, we
extract topics from paragraphs containing idioms and
from paragraphs containing literals using an unsuper-
vised clustering method, Latent Dirichlet Allocation
(LDA) (Blei et al., 2003). The idea is that if the LDA
model is able to capture the semantics of a target docu-
ment, an idiomatic phrase will be a ?semantic? outlier
of the themes. Thus, this topic representation will al-
low us to differentiate idioms from literals using the
semantics of the local context.
Let d = {w
1
, ? ? ? , w
N
}
t
be a segment (document)
containing a target phrase, where N denotes the num-
ber of terms in a given corpus, and t represents trans-
pose. We first compute a set of m topics from d. We
denote this set by
T (d) = {t
1
, ? ? ? , t
m
},
where t
i
= (w
1
, ? ? ? , w
k
)
t
. Here w
j
represents a word
from a vocabulary of W words. Thus, we have two
representations for d: (1) d, represented by its original
terms, and (2)
?
d, represented by its topic terms. Two
corresponding term by document matrices will be de-
noted by M
D
and M
?
D
, respectively, where D denotes
a set of documents. That is, M
D
represents the original
?text? term by document matrix, while M
?
D
represents
the ?topic? term by document matrix.
Figure 1 shows the potential benefit of topic space
representation. In the figure, text segments containing
target phrase ?blow whistle? are projected on a two di-
mensional subspace. The left figure shows the projec-
tion in the ?text? space, represented by the term by doc-
ument matrixM
D
. The middle figure shows the projec-
tion in the topic space, represented by M
?
D
. The topic
space representation seems to provide a better separa-
tion.
We note that when learning topics from a small data
sample, learned topics can be less coherent and inter-
pretable, thus less useful. To address this issue, regu-
larized LDA has been proposed in the literature (New-
man et al., 2011). A key feature is to favor words that
exhibit short range dependencies for a given topic. We
can achieve a similar effect by placing restrictions on
the vocabulary. For example, when extracting topics
from segments containing idioms, we may restrict the
vocabulary to contain words from these segments only.
The middle and right figures in Figure 1 illustrate a case
in point. The middle figure shows a projection onto the
topic space that is computed with a restricted vocabu-
lary, while the right figure shows a projection when we
place no restriction on the vocabulary. That is, the vo-
cabulary includes terms from documents that contain
both idioms and literals.
Note that by computing M
?
D
, the topic term by doc-
ument matrix, from the training data, we have created
a vocabulary, or a set of ?features? (i.e., topic terms)
that is used to directly describe a query or test segment.
The main advantage is that topics are more accurate
when computed by LDA from a large collection of id-
iomatic or literal contexts. Thus, these topics capture
more accurately the semantic contexts in which the tar-
get idiomatic and literal expressions typically occur. If
a target query appears in a similar semantic context, the
topics will be able to describe this query as well. On the
other hand, one might similarly apply LDA to a given
query to extract query topics, and create the query vec-
tor from the query topics. The main disadvantage is
that LDA may not be able to extract topic terms that
match well with those in the training corpus, when ap-
plied to the query in isolation.
3.2 Algorithm
The main steps of the proposed algorithm, called
TopSpace, are shown below.
Input: D = {d
1
, ? ? ? , d
k
, d
k+1
, ? ? ? , d
n
}: training
documents of k idioms and n? k literals.
Q = {q
1
, ? ? ? , q
l
}: l query documents.
1. Let DicI be the vocabulary determined solely
from idioms {d
1
, ? ? ? , d
k
}. Similarly, let DicL
be the vocabulary obtained from literals
{d
k+1
, ? ? ? , d
n
}.
2. For a document d
i
in {d
1
, ? ? ? , d
k
}, apply LDA
to extract a set of m topics T (d
i
) = {t
1
, ? ? ? , t
m
}
using DicI . For d
i
? {d
k+1
, ? ? ? , d
n
}, DicL is
used.
3. Let
?
D = {
?
d
1
, ? ? ? ,
?
d
k
,
?
d
k+1
, ? ? ? ,
?
d
n
} be the
resulting topic representation of D.
4. Compute the term by document matrix M
?
D
from
?
D, and let DicT and gw be the resulting
dictionary and global weight (idf ), respectively.
5. Compute the term by document matrix M
Q
from
Q, using DicT and gw from the previous step.
Output: M
?
D
and M
Q
To summarize, after splitting our corpus (see section
4) into paragraphs and preprocessing it, we extract top-
ics from paragraphs containing idioms and from para-
graphs containing literals. We then compute a term by
document matrix, where terms are topic terms and doc-
uments are topics extracted from the paragraphs. Our
test data are represented as a term-by-document matrix
as well (See the details in section 5).
2021
?100 ?80 ?60 ?40 ?20 0 20?20
0
20
40
60
80
100 2D Text Space: Blow Whistle
 
 
IdiomsLiterals
?20 ?15 ?10 ?5 0 5 10 15?5
0
5
10
15
20 2D Topic Space: Blow Whistle
 
 
IdiomsLiterals
?12 ?10 ?8 ?6 ?4 ?2 0 2 4 6 8?10
?5
0
5
10
15
20
25 2D Topic Space: Blow Whistle
 
 IdiomsLiterals
Figure 1: 2D projection of text segments containing ?blow whistle.? Left panel: Original text space. Middle panel:
Topic space with restricted vocabulary. Right panel: Topic space with enlarged vocabulary.
3.3 Fisher Linear Discriminant Analysis
Once M
?
D
and M
Q
are obtained, a classification rule
can be applied to predict idioms vs. literals. The ap-
proach we are taking in this work for classifying id-
ioms vs. literals is based on Fisher?s discriminant anal-
ysis (FDA) (Fukunaga, 1990). FDA often significantly
simplifies tasks such as regression and classification by
computing low-dimensional subspaces having statisti-
cally uncorrelated or discriminant variables. In lan-
guage analysis, statistically uncorrelate or discriminant
variables are extracted and utilized for description, de-
tection, and classification. Woods et al. (1986), for ex-
ample, use statistically uncorrelated variables for lan-
guage test scores. A group of subjects is scored on a
battery of language tests, where the subtests measure
different abilities such as vocabulary, grammar or read-
ing comprehension. Horvath (1985) analyzes speech
samples of Sydney speakers to determine the relative
occurrence of five different variants of each of five
vowels sounds. Using this data, the speakers cluster
according to such factors as gender, age, ethnicity and
socio-economic class.
A similar approach has been discussed in Peng et al.
(2010). FDA is a class of methods used in machine
learning to find the linear combination of features that
best separate two classes of events. FDA is closely
related to principal component analysis (PCA), where
a linear combination of features that best explains the
data. Discriminant analysis explicitly exploits class in-
formation in the data, while PCA does not.
Idiom classification based on discriminant analysis
has several advantages. First, as has been mentioned,
it does not make any assumption regarding data distri-
butions. Many statistical detection methods assume a
Gaussian distribution of normal data, which is far from
reality. Second, by using a few discriminants to de-
scribe data, discriminant analysis provides a compact
representation of the data, resulting in increased com-
putational efficiency and real time performance.
In FDA, within-class, between-class, and mixture
scatter matrices are used to formulate the criteria of
class separability. Consider a J class problem, where
m
0
is the mean vector of all data, and m
j
is the mean
vector of jth class data. A within-class scatter ma-
trix characterizes the scatter of samples around their
respective class mean vector, and it is expressed by
S
w
=
J
?
j=1
p
j
l
j
?
i=1
(x
j
i
?m
j
)(x
j
i
?m
j
)
t
, (1)
where l
j
is the size of the data in the jth class, p
j
(
?
j
p
j
= 1) represents the proportion of the jth class
contribution, and t denotes the transpose operator. A
between-class scatter matrix characterizes the scatter of
the class means around the mixture mean m
0
. It is ex-
pressed by
S
b
=
J
?
j=1
p
j
(m
j
?m
0
)(m
j
?m
0
)
t
. (2)
The mixture scatter matrix is the covariance matrix of
all samples, regardless of their class assignment, and it
is given by
S
m
=
l
?
i=1
(x
i
?m
0
)(x
i
?m
0
)
t
= S
w
+ S
b
. (3)
The Fisher criterion is used to find a projection matrix
W ? <
q?d
that maximizes
J(W ) =
|W
t
S
b
W |
|W
t
S
w
W |
. (4)
In order to determine the matrix W that maximizes
J(W ), one can solve the generalized eigenvalue prob-
lem: S
b
w
i
= ?
i
S
w
w
i
. The eigenvectors corresponding
to the largest eigenvalues form the columns ofW . For a
two class problem, it can be written in a simpler form:
S
w
w = m = m
1
? m
2
, where m
1
and m
2
are the
means of the two classes.
4 Data preprocessing
4.1 Verb-noun constructions
For our experiments we use the British National Cor-
pus (BNC, Burnard (2000)) and a list of verb-noun con-
structions (VNCs) extracted from BNC by Fazly et al.
2022
(2009); Cook et al. (2008) and labeled as L (Literal),
I (Idioms), or Q (Unknown). The list contains only
those VNCs whose frequency was greater than 20 and
that occurred at least in one of two idiom dictionaries
(Cowie et al., 1983; Seaton and Macaulay, 2002). The
dataset consists of 2,984 VNC tokens. For our experi-
ments we only use VNCs that are annotated as I or L.
4.2 Lemmatization
Instead of dealing with various forms of the same root,
we use lemmas provided by the BNC XML annotation,
so our corpus is lemmatized. We also apply the (modi-
fied) Google stop list before extracting the topics. The
reason we modified the stop list is that some function
words can potentially be idiom components (e.g., cer-
tain prepositions).
4.3 Paragraphs
We use the original SGML annotation to extract para-
graghs from BNC. We only kept the paragraphs that
contained VNCs for our experiments. We experi-
mented with texts of one paragraph length (single para-
graph contexts) and of three-paragraph length (multi-
paragraph contexts). An example of multi-paragraph
contexts is shown below:
So, reluctantly, I joined Jack Hobbs in not rocking
the boat, reporting the play and the general uproar with
perhaps too much impartiality. My reports went to all
British newspapers, with special direct services by me
to India, South Africa and West Indies; even to King
George V in Buckingham Palace, who loved his cricket.
In other words, I was to some extent leading the British
public astray.
I regret I can shed little new light on the mystery of
who blew the whistle on the celebrated dressing-room
scene after Woodfull was hit. while he was lying on the
massage table after his innings waiting for a doctor,
Warner and Palairet called to express sympathy.
Most versions of Woodfull?s reply seem to agree that
he said. There are two teams out there on the oval.
One is playing cricket, the other is not. This game is
too good to be spoilt. It is time some people got out of
it. Warner and Palairet were too taken aback to reply.
They left the room in embarrassment.
Single paragraph contexts simply consist of the mid-
dle paragraph.
5 Experiments
5.1 Methods
We have carried out an empirical study evaluating the
performance of the proposed algorithm. For compar-
ison, the following methods are evaluated. (1) The
proposed algorithm TopSpace (1), where the data are
represented in topic space. (2) TexSpace algorithm,
where the data are represented in original text space.
For each representation, two classification schemes are
applied: a) FDA (Eq. 4), followed by the nearest neigh-
bor rule. b) SVMs with Gaussian kernels (Cristianini
and Shawe-Taylor (2000)). For the nearest neighbor
rule, the number of nearest neighbors is set to dn/5e,
where n denotes the number of training examples. For
SVMs, kernel width and soft margin parameters are set
to default values.
5.2 Data Sets
The following data sets are used to evaluate the perfor-
mance of the proposed technique. These data sets have
enough examples from both idioms and literals to make
our results meaningful. On average, the training data is
6K word tokens. Our test data is of a similar size.
BlowWhistle: This data set has 78 examples, 27 of
which are idioms and the remaining 51 are literals. The
training data for BlowWhistle consist of 40 randomly
chosen examples (20 paragraphs containing idioms and
20 paragraphs containing literals). The remaining 38
examples (7 idiomatic and 31 literals) are used as test
data.
MakeScene: This data set has 50 examples, 30 of
which are paragraphs containing idioms and the re-
maining 20 are paragraphs containing literals. The
training data for MakeScene consist of 30 randomly
chosen examples, 15 of which are paragraphs contain-
ing make scene as an idiom and the rest 15 are para-
graphs containing make scene as a literal. The remain-
ing 20 examples (15 idiomatic paragraphs and 5 liter-
als) are used as test data.
LoseHead: This data set has 40 examples, 21 of
which are idioms and the remaining 19 are literals.
The training data for LoseHead consist of 30 randomly
chosen examples (15 idiomatic and 15 literal). The
remaining 10 examples (6 idiomatic and 4 literal) are
used as test data.
TakeHeart: This data set has 81 examples, 61 of
which are idioms and the remaining 20 are literals. The
training data for TakeHeart consist of 30 randomly
chosen examples (15 idiomatic and 15 literals). The
remaining 51 examples (46 idiomatic and 5 literals) are
used as test data.
5.3 Adding affect
Nunberg et al. (1994) notice that ?idioms are typically
used to imply a certain evaluation or affective stance
toward the things they denote?. Language users usu-
ally choose an idiom in non-neutral contexts. The situ-
ations that idioms describe can be positive or negative;
however, the polarity of the context is not as impor-
tant as the strength of the emotion expressed. So, we
decided to incorporate the knowledge about the emo-
tion strength into our algorithm. We use a database of
word norms collected by Warriner et al. (2013). This
database contains almost 14,000 English lemmas an-
notated with three components of emotions: valence
(the pleasantness of a stimulus), arousal (the intensity
of emotion provoked by a stimulus), and dominance
2023
Table 1: Average accuracy of competing methods on four datasets in single paragraph contexts: A = Arousal
Model BlowWhistle LoseHead MakeScene TakeHeart
Prec Recall Acc Prec Recall Acc Prec Recall Acc Prec Recall Acc
FDA-Topics 0.44 0.40 0.79 0.70 0.90 0.70 0.82 0.97 0.81 0.91 0.97 0.89
FDA-Topics+A 0.51 0.51 0.75 0.78 0.68 0.66 0.80 0.99 0.80 0.93 0.84 0.80
FDA-Text 0.37 0.81 0.63 0.60 0.88 0.58 0.82 0.89 0.77 0.36 0.38 0.41
FDA-Text+A 0.42 0.49 0.76 0.64 0.92 0.63 0.83 0.95 0.82 0.75 0.53 0.53
SVMs-Topics 0.08 0.39 0.59 0.28 0.25 0.45 0.59 0.74 0.61 0.91 1.00 0.91
SVMs-Topics+A 0.06 0.21 0.69 0.38 0.18 0.44 0.53 0.40 0.44 0.91 1.00 0.91
SVMs-Text 0.08 0.39 0.59 0.36 0.60 0.52 0.23 0.30 0.40 0.42 0.16 0.22
SVMs-Text+A 0.15 0.51 0.60 0.31 0.38 0.48 0.37 0.40 0.45 0.95 0.48 0.50
(the degree of control exerted by a stimulus). These
components were elicited from human subjects via an
Amazon Mechanical Turk crowdsourced experiment.
We only used the arousal feature in our experiments
because we were interested in the intensity of the emo-
tion rather than its valence.
For a document d = {w
1
, ? ? ? , w
N
}
t
, we calculate
the corresponding arousal value a
i
for each w
i
, ob-
taining d
A
= {a
1
, ? ? ? , a
N
}
t
. Let m
A
be the aver-
age arousal value calculated over the entire training
data. The centered arousal value for a training docu-
ment is obtained by subtractingm
A
from d
A
, i.e.,
?
d
A
=
d
A
?m
A
= {a
1
?m
A
, ? ? ? , a
N
?m
A
}
t
. Similarly, the
centered arousal value for a query is computed accord-
ing to q?
A
= q
A
?m
A
= {q
1
?m
A
, ? ? ? , q
N
?m
A
}
t
.
That is, the training arousal mean is used to center both
training and query arousal values. The corresponding
arousal matrices for D,
?
D, and Q are A
D
, A
?
D
, A
Q
, re-
spectively. To incorporate the arousal feature, we sim-
ply compute
?
D
= M
D
+A
D
, (5)
and
?
?
D
= M
?
D
+A
?
D
. (6)
The arousal feature can be similarly incoporated into
query ?
Q
= M
Q
+A
Q
.
6 Results
Table 1 shows the average precision, recall, and ac-
curacy of the competing methods on the four data
sets over 10 runs in simple paragraph contexts. Table
2 shows the results for the multi-paragraph contexts.
Note that for single paragraph contexts, we chose two
topics, each having 10 terms. For multi-paragrah con-
texts, we had four topics, with 10 terms per topic. No
optimization was made for selecting the number of top-
ics as well as the number of terms per topic. In the
tables, the best performance in terms of the sum of pre-
cision, recall and accuracy is given in boldface.
The results show that the topic representation
achieved the best performance in 6 out of 8 cases. Fig-
ure 2 plots the overall aggregated performance in terms
of topic vs text representations across the entire data
sets, regardless of the classifiers used. Everything else
being equal, this clearly shows the advantage of topics
over simple text representation.
Precision Recall Accuracy0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
 
 TopicsText
Figure 2: Aggregated performance: Topic vs text rep-
resentations.
The arousal feature (Eqs 5 and 6) also improved the
overall performance, particularly in text representation
(Eq. 5). This can be seen in the top panel in Figure 3.
In fact, in 2/8 cases, text representation coupled with
the arousal feature achieved the best performance. One
possible explanation is that the LDA model already per-
formed ?feature? selection (choosing topic terms), to
the extent possible. Thus, any additional information
such as arousal only provides marginal improvement
at the best (bottom panel in Figure 3). On the other
hand, original text represents ?raw? features, whereby
arousal information helps provide better contexts, thus
improving overall performance.
Figure 4 shows a case in point: the average (sorted)
arousal values of idioms and literals of the target phrase
?lose head.? The upper panel plots arousal values in
the text space, while lower panel plots arousal values
in the topic space. The plot supports the results shown
in Tables 1 and 2, where the arousal feature generally
improves text representation.
7 Comparisons with other approaches
Even though we used Fazly et al. (2009)?s dataset for
these experiments, the direct comparison with their
method is impossible here because our task is formu-
lated differently and we do not use the full dataset for
the experiments. Fazly et al. (2009)?s unsupervised
2024
Table 2: Average accuracy of competing methods on four datasets in multiple paragraph contexts: A = Arousal
Model BlowWhistle LoseHead MakeScene TakeHeart
Prec Recall Acc Prec Recall Acc Prec Recall Acc Prec Recall Acc
FDA-Topics 0.62 0.60 0.83 0.76 0.97 0.78 0.79 0.95 0.77 0.93 0.99 0.92
FDA-Topics+A 0.47 0.44 0.79 0.74 0.93 0.74 0.82 0.69 0.65 0.92 0.98 0.91
FDA-Text 0.65 0.43 0.84 0.72 0.73 0.65 0.79 0.95 0.77 0.46 0.40 0.42
FDA-Text+A 0.45 0.49 0.78 0.67 0.88 0.65 0.80 0.99 0.80 0.47 0.29 0.33
SVMs-Topics 0.07 0.40 0.56 0.60 0.83 0.61 0.46 0.57 0.55 0.90 1.00 0.90
SVMs-Topics+A 0.21 0.54 0.55 0.66 0.77 0.64 0.42 0.29 0.41 0.91 1.00 0.91
SVMs-Text 0.17 0.90 0.25 0.30 0.50 0.50 0.10 0.01 0.26 0.65 0.21 0.26
SVMs-Text+A 0.24 0.87 0.41 0.66 0.85 0.61 0.07 0.01 0.26 0.74 0.13 0.20
Precision Recall Accuracy0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
 
 TextText+A
Precision Recall Accuracy0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
 
 
TopicsTopics+A
Figure 3: Aggregated performance: Text
vs. text+Arousal representations (top) and Top-
ics vs. Topics+Arousal representations (bottom).
model that relies on the so-called canonical forms gives
72.4% (macro-)accuracy on the extraction of idiomatic
tokens when evaluated on their test data.
We cannot compare our method directly with the
other methods discussed in section 2 either because
each uses a different dataset or formulates the task
differently (detection vs. recognition vs. identifica-
tion). However, we can compare the method presented
here with Feldman and Peng (2013) who also experi-
ment with LDA, use similar data, and frame the prob-
lem as classification. Their goal, however, is to clas-
sify sentences as either idiomatic or literal. To obtain
a discriminant subspace, they train their model on a
small number of randomly selected idiomatic and non-
idiomatic sentences. They then project both the train-
ing and the test data on the chosen subspace and use
the three nearest neighbor (3NN) classifier to obtain
accuracy. The average accuracy they report is 80%.
0 5 10 15?1.4
?1.2
?1
?0.8
?0.6
?0.4
?0.2
0
0.2
Text Terms
Arou
sal V
alue
s
Text Representation
 
 
IdiomsLiterals
0 5 10 15?1.5
?1
?0.5
0
0.5
1
Arou
sal V
alue
s
Topic Terms
Topic Representation
 
 IdiomsLiterals
Figure 4: Average arousal values?Upper panel: Text
space. Lower panel: Topic space.
Our method clearly outperforms the Feldman and Peng
(2013) approach (at least on the dataset we use).
8 Discussion and Conclusion
We have described an algorithm for automatic classi-
fication of idiomatic and literal expressions. We have
investigated the bag of words topic representation for
target documents (segments of one or three paragraphs
that contains a target phrase). The approach definitely
outperforms the baseline model that is based on the
simple bag of words representation, but it also outper-
forms approaches previously discussed in the literature.
Our model captures the local semantics and thus is ca-
pable to identify semantic outliers (=idioms).
While we realize that the data set we use is small, the
results are encouraging. We notice that using 3 para-
graphs for local contexts improves the performance of
the classifiers. The reason is that some paragraphs are
2025
relatively short. A larger context provides more related
terms, which gives LDA more opportunities to sample
these terms.
Idioms are also relatively non-compositional. While
we do not measure their non-compositionality in this
approach, we indirectly touch upon this property by hy-
pothesizing that non-compositional idiomatic expres-
sions are likely to be far from the local topics.
We feel that incorporating the intensity of emotion
expressed by the context into our model improves per-
formance, in particular, in text representation. When
we performed a qualitative analysis of the results try-
ing to determine the causes of false positives and neg-
atives, we noticed that there were quite a number of
cases that improved after incorporating the arousal fea-
ture into the model. For example, the FDA:topic classi-
fier labels ?blow the whistle? as literal in the following
context, but FDA:topics+A marks this expression as id-
iomatic (italicized words indicate words with relatively
high arousal values):
Peter thought it all out very carefully. He decided the wis-
est course was to pool all he had made over the last two years,
enabling Julian to purchase the lease of a high street property.
This would enable them to set up a business on a more set-
tled and permanent trading basis. Before long they opened a
grocery-cum-delicatessen in a good position as far as passing
trade was concerned. Peter?s investment was not misplaced.
The business did very well with the two lads greatly appreci-
ated locally for their hard work and quality of service. The
range of goods they were able to carry was welcomed in the
area, as well as lunchtime sandwich facilities which had pre-
viously been missing in the neighbourhood.
Success was the fruit of some three years? strenuous work.
But it was more than a shock when Julian admitted to Pe-
ter that he had been running up huge debts with their bank.
Peter knew that Julian gambled, but he hadn?t expected him
to gamble to that level, and certainly not to use the shop as
security. With continual borrowing over two years, the bank
had blown the whistle. Everything was gone. Julian was
bankrupt. Even if they?d had a formal partnership, which
they didn?t, it would have made no difference. Peter lost all
he?d made, and with it his chance to help his parents and his
younger brother and sister, Toby and Laura.
Peter was heartbroken. His father had said all along: nei-
ther a lender nor a borrower. Peter had found out the hard
way. But as his mother observed, he was the same Peter, he?d
pick himself up somehow. Once again, Peter was resolute. He
made up his mind he?d never make the same mistake twice. It
wasn?t just the money or the hard work, though the waste of
that was difficult enough to accept. Peter had been working
a debt of love. He?d done all this for his parents, particularly
for his father, whose dedication to his children had always
impressed Peter and moved him deeply. And now it had all
come to nothing.
Therefore, we think that idioms have the tendency to
appear in more affective contexts; and we think that in-
corporating more sophisticated sentiment analysis into
our model will improve the results.
Acknowledgments
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 1319846.
We also thank the anonymous reviewers for useful
comments. The third author thanks the Fulbright Foun-
dation for giving her an opportunity to conduct this re-
search at Montclair State University (MSU).
References
Ariel, M. (2002). The demise of unique concept of
literal meaning. Journal of Pragmatics 34, 361?402.
Birke, J. and A. Sarkar (2006). A clustering approach
to the nearly unsupervised recognition of nonliteral
language. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL?06), Trento, Italy, pp.
329?226.
Blei, D., A. Ng, and M. Jordan (2003). Latent Dirich-
let Allocation. Journal of Machine Learning Re-
search 3, 993?1022.
Burnard, L. (2000). The British National Corpus Users
Reference Guide. Oxford University Computing Ser-
vices.
Cilibrasi, R. and P. M. B. Vit?anyi (2007). The
google similarity distance. IEEE Trans. Knowl. Data
Eng. 19(3), 370?383.
Cook, P., A. Fazly, and S. Stevenson (2007). Pulling
their weight: Exploiting syntactic forms for the auto-
matic identification of idiomatic expressions in con-
text. In Proceedings of the ACL 07 Workshop on A
Broader Perspective on Multiword Expressions, pp.
41?48.
Cook, P., A. Fazly, and S. Stevenson (2008, June). The
VNC-Tokens Dataset. In Proceedings of the LREC
Workshop: Towards a Shared Task for Multiword
Expressions (MWE 2008), Marrakech, Morocco.
Cowie, A. P., R. Mackin, and I. R. McCaig (1983). Ox-
ford Dictionary of Current Idiomatic English, Vol-
ume 2. Oxford University Press.
Cristianini, N. and J. Shawe-Taylor (2000). An In-
troduction to Support Vector Machines and other
kernel-based learning methods. Cambridge, UK:
Cambridge University Press.
Fazly, A., P. Cook, and S. Stevenson (2009). Unsu-
pervised Type and Token Identification of Idiomatic
Expressions. Computational Linguistics 35(1), 61?
103.
Feldman, A. and J. Peng (2013). Automatic detec-
tion of idiomatic clauses. In Computational Linguis-
tics and Intelligent Text Processing, pp. 435?446.
Springer.
Fukunaga, K. (1990). Introduction to statistical pattern
recognition. Academic Press.
Hearst, M. A. (1992). Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th Conference on Computational Linguistics
- Volume 2, COLING ?92, Stroudsburg, PA, USA,
pp. 539?545. Association for Computational Lin-
guistics.
2026
Horvath, B. M. (1985). Variation in Australian English.
Cambridge: Cambridge University PRess.
Katz, G. and E. Giesbrech (2006). Automatic Iden-
tification of Non-compositional Multiword Expres-
sions using Latent Semantic Analysis. In Proceed-
ings of the ACL/COLING-06 Workshop on Multi-
word Expressions: Identifying and Exploiting Un-
derlying Properties, pp. 12?19.
Li, L. and C. Sporleder (2010). Using Gaussian Mix-
ture Models to Detect Figurative Language in Con-
text. In Proceedings of NAACL/HLT 2010.
Newman, D., E. V. Bonilla, and W. L. Buntine (2011).
Improving topic coherence with regularized topic
models. In NIPS, pp. 496?504.
Nunberg, G., I. A. Sag, and T. Wasow (1994). Idioms.
Language 70(3), 491?538.
Pado, S. and M. Lapata (2007). Dependency-based
construction of semantic space models. Computa-
tional Linguistics 33(2), 161?199.
Peng, J., A. Feldman, and L. Street (2010). Comput-
ing linear discriminants for idiomatic sentence de-
tection. Research in Computing Science, Special is-
sue: Natural Language Processing and its Applica-
tions 46, 17?28.
Sag, I. A., T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger (2002). Multiword expressions: A
Pain in the Neck for NLP. In Proceedings of the
3rd International Conference on Intelligence Text
Processing and Computational Linguistics (CICLing
2002), Mexico City, Mexico, pp. 1?15.
Seaton, M. and A. Macaulay (Eds.) (2002). Collins
COBUILD Idioms Dictionary (second ed.). Harper-
Collins Publishers.
Sporleder, C. and L. Li (2009). Unsupervised Recogni-
tion of Literal and Non-literal Use of Idiomatic Ex-
pressions. In EACL ?09: Proceedings of the 12th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, Morristown, NJ,
USA, pp. 754?762. Association for Computational
Linguistics.
Warriner, A. B., V. Kuperman, and M. Brysbaert
(2013). Norms of valence, arousal, and dominance
for 13,915 english lemmas. Behavior Research
Methods 44(4).
Widdows, D. and B. Dorow (2005). Automatic extrac-
tion of idioms using graph analysis and asymmet-
ric lexicosyntactic patterns. In Proceedings of the
ACL-SIGLEX Workshop on Deep Lexical Acquisi-
tion, DeepLA ?05, Stroudsburg, PA, USA, pp. 48?
56. Association for Computational Linguistics.
Woods, A., P. Fletcher, and A. Hughes (1986). Statis-
tics in Language Studies. Cambridge: Cambridge
University Press.
2027
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 197?201,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Challenges of Cheap Resource Creation for Morphological Tagging
Jirka Hana
Charles University
Prague, Czech Republic
first.last@gmail.com
Anna Feldman
Montclair State University
Montclair, New Jersey, USA
first.last@montclair.edu
Abstract
We describe the challenges of resource
creation for a resource-light system for
morphological tagging of fusional lan-
guages (Feldman and Hana, 2010). The
constraints on resources (time, expertise,
and money) introduce challenges that are
not present in development of morphologi-
cal tools and corpora in the usual, resource
intensive way.
1 Introduction
Morphological analysis, tagging and lemmatiza-
tion are essential for many Natural Language Pro-
cessing (NLP) applications of both practical and
theoretical nature. Modern taggers and analyz-
ers are very accurate. However, the standard
way to create them for a particular language re-
quires substantial amount of expertise, time and
money. A tagger is usually trained on a large cor-
pus (around 100,000+ words) annotated with the
correct tags. Morphological analyzers usually rely
on large manually created lexicons. For exam-
ple, the Czech analyzer (Hajic?, 2004) uses a lex-
icon with 300,000+ entries. As a result, most of
the world languages and dialects have no realis-
tic prospect for morphological taggers or analyz-
ers created in this way.
We have been developing a method for creat-
ing morphological taggers and analyzers of fu-
sional languages1 without the need for large-scale
knowledge- and labor-intensive resources (Hana et
al., 2004; Hana et al, 2006; Feldman and Hana,
2010) for the target language. Instead, we rely
on (i) resources available for a related language
and (ii) a limited amount of high-impact, low-
1Fusional languages are languages in which several fea-
ture values are realized in one morpheme. For example Indo-
European languages, including Czech, German, Romanian
and Farsi, are predominantly fusional.
cost manually created resources. This greatly re-
duces cost, time requirements and the need for
(language-specific) linguistic expertise.
The focus of our paper is on the creation of re-
sources for the system we developed. Even though
we have reduced the manual resource creation to
the minimum, we have encountered a number of
problems, including training language annotators,
documenting the reasoning behind the tagset de-
sign and morphological paradigms for a specific
language as well as creating support tools to facil-
itate and speed up the manual work. While these
problems are analogous to those that arise with
standard resource creation, the approach to their
solution is often different as we discuss in the fol-
lowing sections.
2 Resource-light Morphology
The details of our system are provided in (Feld-
man and Hana, 2010). Our main assumption is
that a model for the target language can be approx-
imated by language models from one or more re-
lated source languages and that inclusion of a lim-
ited amount of high-impact and/or low-cost man-
ual resources is greatly beneficial and desirable.
We use TnT (Brants, 2000), a second order
Markov Model tagger. We approximate the target-
language emissions by combining the emissions
from the (modified) source language corpus with
information from the output of our resource-light
analyzer (Hana, 2008). The target-language tran-
sitions are approximated by the source language
(Feldman and Hana, 2010).
3 Resource creation
In this section we address the problem of collec-
tion, selection and creation of resources needed
by our system. The following resources must be
available:
? a reference grammar book for information
197
about paradigms and closed class words,
? a large amount of plain text for learning a lex-
icon, e.g. newspapers from the Internet,
? a large annotated training corpus of a related
language,
? optionally, a dictionary (or a native speaker)
to provide analyses of the most frequent
words,
? a non-expert (not a linguist and not a native
speaker) to create the resources listed below,
? limited access to a linguist (to make non-
obvious decisions in the design of the re-
sources),
? limited access to a native speaker (to anno-
tate a development corpus, to answer a lim-
ited number of language specific questions).
and these resources must be created:
? a list of morphological paradigms,
? a list of closed class words with their analy-
ses,
? optionally, a list of the most frequent forms,
? a small annotated development corpus.
For evaluation, an annotated test corpus must
be also created. As this corpus is not part of the
resource-light system per se, it can (and should)
be as large as possible.
3.1 Restrictions
Since our goal is to create resources cheaply and
fast, we intentionally limit (but not completely ex-
clude) the inclusion of any linguist and of anybody
knowing the target language. We also limit the
time of training and encoding of the basic target-
language linguistic information to a minimum.
3.2 Tagset
In traditional settings, a tagset is usually designed
by a linguist, moreover a native speaker. The con-
straints of a resource-light system preclude both of
these qualifications. Instead, we have standardized
the process as much as possible to make it possible
to have the tagset designed by a non-expert.
3.2.1 Positional Tagset
All languages we work with are morphologically
rich. Naturally, such languages require a large
number of tags to capture their morphological
properties. An obvious way to make it manageable
is to use a structured system. In such a system, a
tag is a composition of tags each coming from a
much smaller and simpler atomic tagset tagging a
particular morpho-syntactic property (e.g. gender
or tense). This system has many benefits, includ-
ing the 1) relative easiness for a human annotator
to remember individual positions rather than sev-
eral thousands of atomic symbols; 2) systematic
morphological description; 3) tag decomposabil-
ity; and 4) systematic evaluation.
3.2.2 Tagset Design: Procedure
Instead of starting from scratch each time a tagset
for a new language is created, we have provided
an annotated tagset template. A particular tagset
can deviate from this template, but only if there is
a linguistic reason. The tagset template includes
the following items:
? order of categories (POS, SubPOS, gender,
animacy, number, case, ...) ? not all might
be present in that language; additional cate-
gories might be needed;
? values for each category (N ? nouns, C ? nu-
merals, M ? masculine);
? which categories we do not distinguish, even
though we could (proper vs. common nouns);
? a fully worked out commented example (as
mentioned above).
Such a template not only provides a general
guidance, but also saves a lot of time, because
many of rather arbitrary decisions involved in any
tagset creation are done just once (e.g. symbols de-
noting basic POS categories, should numerals be
included as separate POS, etc.). As stated, a tagset
may deviate from such a template, but only if there
is a specific reason for it.
3.3 Resources for the morphological analyzer
Our morphological analyzer relies on a small set
of morphological paradigms and a list of closed
class and/or most frequent words.
198
3.3.1 Morphological paradigms
For each target language, we create a list of
morphological paradigms. We just encode basic
facts about the target language morphology from
a standard grammar textbook. On average, the
basic morphology of highly inflected languages,
such as Slavic languages, are captured in 70-80
paradigms. The choices on what to cover involve
a balance between precision, coverage and effort.
3.3.2 A list of frequent forms
Entering a lexicon entry is very costly, both in
terms of time and knowledge needed. While it is
usually easy (for a native speaker) to assign a word
to one of the major paradigm groups, it takes con-
siderably more time to select the exact paradigm
variant differing only in one or two forms (in fact,
this may be even idiolect-dependent). For exam-
ple, in Czech, it is easy to see that the word atom
?atom? does not decline according to the neuter
paradigm me?sto ?town?, but it takes more time to
decide to which of the hard masculine inanimate
paradigms it belongs. On the other hand, enter-
ing possible analyses for individual word forms is
usually very straightforward. Therefore, our sys-
tem uses a list of manually provided analyses for
the most common forms.
Note that the process of providing the list of
forms is not completely manual ? the correct anal-
yses are selected from those suggested on the ba-
sis of the words? endings. This can be done rel-
atively quickly by a native speaker or by a non-
native speaker with the help of a basic grammar
book and a dictionary.
3.4 Documentation
Since the main idea of the project is to create
resources quickly for an arbitrarily selected fu-
sional language, we cannot possibly create anno-
tation and language encoding manuals for each
language. So, we created a manual that explains
the annotation and paradigm encoding procedure
in general and describes the main attributes and
possible values that a language consultant needs
to consider when working on a specific language.
The manual has five parts:
1. How to summarize the basic facts about the
morphosyntax of a language;
2. How to create a tagset
3. How to encode morphosyntactic properties of
the target language in paradigms;
4. How to create a list of closed class words.
5. Corpus annotation manual
The instructions are mostly language indepen-
dent (with some bias toward Indo-European lan-
guages), but contain a lot of examples from lan-
guages we have processed so far. These include
suggestions how to analyze personal pronouns,
what to do with clitics or numerals.
3.5 Procedure
The resource creation procedure involves at least
two people: a native speaker who can annotate
a development corpus, and a non-native speaker
who is responsible for the tagset design, morpho-
logical paradigms, and a list of closed class words
or frequent forms. Below we describe our proce-
dure in more detail.
3.5.1 Tagset and MA resources creation
We have realized that even though we do not need
a native speaker, some understanding of at least
basic morphological categories the language uses
is helpful. So, based on our experience, it is bet-
ter to hire a person who speaks (natively or not) a
language with some features in common. For ex-
ample, for Polish, somebody knowing Russian is
ideal, but even somebody speaking German (it has
genders and cases) is much better than a person
speaking only English. In addition, a person who
had created resources for one language performs
much better on the next target language. Knowl-
edge comes with practice.
The order of work is as follows:
1. The annotator is given basic training that usu-
ally includes the following: 1) brief explana-
tion of the purpose of the project; 2) tagset
design; 3) paradigm creation.
2. The annotator summarizes the basic facts
about the morphosyntax of a language,
3. The first version of the tagset is created.
4. The list of paradigms and closed-class words
is compiled. During this process, the tagset is
further adjusted.
199
3.5.2 Corpus annotation
The annotators do not annotate from scratch.
We first run our morphological analyzer on
the selected corpus; the annotators then dis-
ambiguate the output. We have created a
support tool (http://ufal.mff.cuni.cz/
?hana/law.html) that displays the word to be
annotated, its context, the lemma and possible tags
suggested by the morphological analyzer. There is
an option to insert a new lemma and a new tag if
none of the suggested items is suitable. The tags
are displayed together with their natural language
translation.
4 Case studies
Our case studies include Russian via Czech, Rus-
sian via Polish, Russian via Czech and Polish, Por-
tuguese via Spanish, and Catalan via Spanish.
We use these languages to test our hypotheses
and we do not suggest that morphological tagging
of these languages should be designed in the way
we do. Actually, high precision systems that use
manually created resources already exist for these
languages. The main reason for working with
them is that we can easily evaluate our system on
existing corpora.
We experimented with the direct transfer of
transition probabilities, cognates, modifying tran-
sitions to make them more target-like, training a
battery of subtaggers and combining the results
(Reference omitted). Our best result on Russian
is 81.3% precision (on the full 15-slot tag, on all
POSs), and 92.2% (on the detailed POS). We have
also noticed that the most difficult categories are
nouns and adjectives. If we improve on these in-
dividual categories, we will improve significantly
the overall result. The precision of our model
on Catalan is 87.1% and 91.1% on the full tag
and SubPOS, respectively. The Portuguese perfor-
mance is comparable as well.
The resources our experiments have relied upon
include the following:
1. Russian
? Tagset, paradigms, word-list: speaker of
Czech and linguist, some knowledge of
Russian
? Dev corpus: a native speaker & linguist
2. Catalan
? Tagset: modified existing tagset (de-
signed by native speaking linguists)
? paradigms, word-list: linguist speaking
Russian and English
? Dev corpus: a native speaking linguists
3. Portuguese
? Tagset: modified Spanish tagset (de-
signed by native speaking linguists) by
us
? paradigms, word-list: a native speaking
linguist
? Dev corpus: a native speaking linguist
4. Romanian
? Tagset, paradigms, word-list: designed
by a non-linguist, speaker of English
? Dev corpus ? a native speaker
Naturally, we cannot expect the tagging accu-
racy to be 100%. There are many factors that con-
tribute to the performance of the model:
1. target language morphosyntactic complexity,
2. source-language?target-language proximity,
3. quality of the paradigms,
4. quality of the cognate pairs (that are used for
approximating emissions),
5. time spent on language analysis,
6. expertise of language consultants,
7. supporting tools.
5 Summary
We have described challenges of resource creation
for resource-light morphological tagging. These
include creating clear guidelines for tagset design
that can be reusable for an arbitrarily selected lan-
guage; precise formatting instructions; providing
basic linguistic training with the emphasis on mor-
phosyntactic properties of fusional languages; cre-
ating an annotation support tool; and giving timely
and constructive feedback on intermediate results.
6 Acknowledgement
The development of the tagset was supported by
the GAC?R grant P406/10/P328 and by the U.S.
NSF grant # 0916280.
200
References
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of 6th Applied Nat-
ural Language Processing Conference and North
American chapter of the Association for Computa-
tional Linguistics annual meeting (ANLP-NAACL),
pages 224?231.
Anna Feldman and Jirka Hana. 2010. A Resource-light
Approach to Morpho-syntactic Tagging, volume 70
of Language and Computers: Studies in Practical
Linguistics. Rodopi, Amsterdam/New York.
Jan Hajic?. 2004. Disambiguation of Rich Inflection:
Computational Morphology of Czech. Karolinum,
Charles University Press, Prague, Czech Republic.
Jirka Hana, Anna Feldman, and Chris Brew. 2004.
A Resource-light Approach to Russian Morphol-
ogy: Tagging Russian Using Czech Resources.
In Proceedings of Empirical Methods for Natural
Language Processing (EMNLP), pages 222?229,
Barcelona, Spain.
Jirka Hana, Anna Feldman, Luiz Amaral, and Chris
Brew. 2006. Tagging Portuguese with a Span-
ish Tagger Using Cognates. In Proceedings of the
Workshop on Cross-language Knowledge Induction
hosted in conjunction with the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 33?40, Trento,
Italy.
Jirka Hana. 2008. Knowledge- and labor-light mor-
phological analysis. OSUWPL, 58:52?84.
201
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 10?18,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
A Low-budget Tagger for Old Czech
Jirka Hana
Charles University, MFF
Czech Republic
first.last@gmail.com
Anna Feldman
Montclair State University
USA
first.last@montclair.edu
Katsiaryna Aharodnik
Montclair State University
USA
ogorodnichek@gmail.com
Abstract
The paper describes a tagger for Old Czech
(1200-1500 AD), a fusional language with
rich morphology. The practical restrictions
(no native speakers, limited corpora and lex-
icons, limited funding) make Old Czech an
ideal candidate for a resource-light cross-
lingual method that we have been developing
(e.g. Hana et al, 2004; Feldman and Hana,
2010).
We use a traditional supervised tagger. How-
ever, instead of spending years of effort to cre-
ate a large annotated corpus of Old Czech, we
approximate it by a corpus of Modern Czech.
We perform a series of simple transformations
to make a modern text look more like a text
in Old Czech and vice versa. We also use a
resource-light morphological analyzer to pro-
vide candidate tags. The results are worse
than the results of traditional taggers, but the
amount of language-specific work needed is
minimal.
1 Introduction
This paper describes a series of experiments in an
attempt to create morphosyntactic resources for Old
Czech (OC) on the basis of Modern Czech (MC) re-
sources. The purpose of this work is two-fold. The
practical goal is to create a morphologically anno-
tated corpus of OC which will help in investigation
of various morphosyntactic patterns underpinning
the evolution of Czech. Our second goal is more
theoretical in nature. We wanted to test the resource-
light cross-lingual method that we have been devel-
oping (e.g. Hana et al, 2004; Feldman and Hana,
2010) on a source-target language pair that is di-
vided by time instead of space. The practical restric-
tions (no native speakers, limited corpora and lexi-
cons, limited funding) make OC an ideal candidate
for a resource-light approach.
We understand that the task we chose is hard
given the 500+ years of language evolution. We are
aware of the fact that all layers of the language have
changed, including phonology and graphemics, syn-
tax and vocabulary. Even words that are still used in
MC are often used with different distributions, with
different declensions, with different gender, etc.
Our paper is structured as follows. We first briefly
describe related work and motivate our approach.
Then we outline the relevant aspects of the Czech
language and compare its Modern and Old forms.
Then we describe the corpora and tagsets used in our
experiments. The rest of the paper describes the ac-
tual experiments, the performance of various models
and concludes with a discussion of the results.
2 Related Work
Since there are no morphological taggers devel-
oped specifically for OC, we compare our work
with those for MC. Morc?e (http://ufal.mff.
cuni.cz/morce/) is currently the best tagger,
with accuracy slightly above 95%. It is based on
a statistical (averaged perceptron) algorithm which
relies on a large morphological lexicon containing
around 300K entries. The tool has been trained and
tuned on data from the Prague Dependency Tree-
bank (PDT; Be?mova et al, 1999; Bo?hmova? et al,
2001). The best set of features was selected af-
ter hundreds of experiments were performed. In
10
contrast, the resource-light system we developed is
not as accurate, but the amount of language-specific
work needed is incomparable to that of the state-
of-the-art systems. Language specific work on our
OC tagger, for example, was completed in about 20
hours, instead of several years.
Research in resource-light learning of mor-
phosyntactic properties of languages is not new.
Some have assumed only partially tagged train-
ing corpora (Merialdo, 1994); some have begun
with small tagged seed wordlists (Cucerzan and
Yarowsky, 2002) for named-entity tagging, while
others have exploited the automatic transfer of an
already existing annotated resource in a different
genre or a different language (e.g. cross-language
projection of morphological and syntactic informa-
tion as in (Cucerzan and Yarowsky, 2000; Yarowsky
et al, 2001), requiring no direct supervision in the
target language). The performance of our system is
comparable to the results cited by these researchers.
In our work we wanted to connect to pre-
existing knowledge that has been acquired and sys-
tematized by traditional linguists, e.g. morpholog-
ical paradigms, sound changes, and other well-
established facts about MC and OC.
3 Czech Language
Czech is a West Slavic language with significant in-
fluences from German, Latin and (in modern times)
English. It is a fusional (flective) language with rich
morphology and a high degree of homonymy of end-
ings.
3.1 Old Czech
As a separate language, Czech forms between 1000-
1150 AD; there are very few written documents
from that time. The term Old Czech usually refers
to Czech roughly between 1150 and 1500. It is fol-
lowed by Humanistic Czech (1500-1650), Baroque
Czech (1650-1780) and then Czech of the so-called
National Revival. Old Czech was significantly in-
fluenced by Old Church Slavonic, Latin and Ger-
man. Spelling during this period was not standard-
ized, therefore the same word can have many dif-
ferent spelling variants. However, our corpus was
transliterated ? its pronunciation was recorded using
the rules of the Modern Czech spelling (see Lehec?ka
change example
u? > ou non-init. mu?ka > mouka ?flour?
se? > se se?no > seno ?hay?
o? > uo > u? ko?n? > kuon? > ku?n? ?horse?
s?c? > s?t? s?c???r > s?t??r ?scorpion?
c?s > c c?so > co ?what?
Table 1: Examples of sound/spelling changes from OC to
MC
and Volekova?, 2011, for more details).
3.2 Modern Czech
Modern Czech is spoken by roughly 10 million
speakers, mostly in the Czech Republic. For a more
detailed discussion, see for example (Naughton,
2005; Short, 1993; Janda and Townsend, 2002;
Karl??k et al, 1996). For historical reasons, there
are two variants of Czech: Official (Literary, Stan-
dard) Czech and Common (Colloquial) Czech. The
official variant is based on the 19th-century resur-
rection of the 16th-century Czech. Sometimes it is
claimed, with some exaggeration, that it is the first
foreign language the Czechs learn. The differences
are mainly in phonology, morphology and lexicon.
The two variants are influencing each other, result-
ing in a significant amount of irregularity, especially
in morphology. The Czech writing system is mostly
phonological.
3.3 Differences
Providing a systematic description of differences be-
tween Old and Modern Czech is well beyond the
scope of this paper. Therefore, we just briefly men-
tion a few illustrative examples. For a more detailed
description see (Va?z?ny?, 1964; Dosta?l, 1967; Mann,
1977).
3.3.1 Phonology and Spelling
Examples of some of the more regular changes be-
tween OC and MC spelling can be found in Table 1
(Mann (1977), Boris Lehec?ka p.c.).
3.3.2 Nominal Morphology
The nouns of OC have three genders: feminine,
masculine, and neuter. In declension they distin-
guish three numbers: singular, plural, and dual,
and seven cases: nominative, genitive, dative, ac-
cusative, vocative, locative and instrumental. Voca-
11
category Old Czech Modern Czech
infinitive pe?c-i pe?c-t ?bake?
present 1sg pek-u pec?-u
1du pec?-eve? ?
1pl pec?-em(e/y) pec?-eme
:
imperfect 1sg pec?-iech ?
1du pec?-iechove? ?
1pl pec?-iechom(e/y) ?
:
imperative 2sg pec-i pec?
2du pec-ta ?
2pl pec-te pec?-te
:
verbal noun pec?-enie pec?-en??
Table 2: A fragment of the conjugation of the verb
pe?ci/pe?ct ?bake? (OC based on (Dosta?l, 1967, 74-77))
tive is distinct only for some nouns and only in sin-
gular.
MC nouns preserved most of the features of OC,
but the dual number survives only in a few paired
names of parts of the body, in the declensions of
the words ?two? and ?both? and in the word for
?two hundred?. In Common Czech the dual plural
distinction is completely neutralized. On the other
hand, MC distinguishes animacy in masculine gen-
der, while this distinction is only emerging in late
OC.
3.3.3 Verbal Morphology
The system of verbal forms and constructions was
far more elaborate in OC than in MC. Many forms
disappeared all together (three simple past tenses,
supinum), and some are archaic (verbal adverbs,
plusquamperfectum). Obviously, all dual forms are
no longer in MC. See Table 2 for an example.
4 Corpora
4.1 Modern Czech Corpus
Our MC training corpus is a portion (700K tokens)
of PDT. The corpus contains texts from daily news-
papers, business and popular scientific magazines. It
is manually morphologically annotated.
The tagset (Hajic? (2004)) has more than 4200
tags encoding detailed morphological information.
It is a positional tagset, meaning the tags are se-
quences of values encoding individual morpholog-
ical features and all tags have the same length, en-
coding all the features distinguished by the tagset.
Features not applicable for a particular word have a
N/A value. For example, when a word is annotated
as AAFS4----2A---- it is an adjective (A), long
form (A), feminine (F), singular (S), accusative (4),
comparative (2), not-negated (A).
4.2 Old Czech Corpora
Several steps (e.g., lexicon acquisition) of our
method require a plain text corpus. We used texts
from the Old-Czech Text Bank (STB, http://
vokabular.ujc.cas.cz/banka.aspx), in
total about 740K tokens. This is significantly less
than we have used in other experiments (e.g., 39M
tokens for Czech or 63M tokens for Catalan (Feld-
man and Hana, 2010)).
A small portion (about 1000 words) of the corpus
was manually annotated for testing purposes. Again
this is much less than what we would like to have,
and we plan to increase the size in the near future.
The tagset is a modification of the modern tagset us-
ing the same categories.
5 Method
The main assumption of our method (Feldman and
Hana, 2010) is that a model for the target language
can be approximated by language models from one
or more related source languages and that inclusion
of a limited amount of high-impact and/or low-cost
manual resources is greatly beneficial and desirable.
We use TnT (Brants, 2000), a second order
Markov Model tagger. The language model of such
a tagger consists of emission probabilities (corre-
sponding to a lexicon with usage frequency infor-
mation) and transition probabilities (roughly corre-
sponding to syntax rules with strong emphasis on lo-
cal word-order). We approximate the emission and
transition probabilities by those trained on a mod-
ified corpus of a related language. Below, we de-
scribe our approach in more detail.
12
6 Experiments
We describe three different taggers:
1. a TnT tagger using modified MC corpus as a
source of both transition and emission proba-
bilities (section 6.1);
2. a TnT tagger using modern transitions but
approximating emissions by a uniformly dis-
tributed output of a morphological analyzer
(MA) (sections 6.2 and 6.3); and
3. a combination of both (section 6.4).
6.1 Translation Model
6.1.1 Modernizing OC and Aging MC
Theoretically, we can take the MC corpus, translate
it to OC and then train a tagger, which would proba-
bly be a good OC tagger. However, we do not need
this sophisticated, costly translation because we only
deal with morphology.
A more plausible idea is to modify the MC corpus
so that it looks more like the OC just in the aspects
relevant for morphological tagging. In this case, the
translation would include the tagset, reverse phono-
logical/graphemic changes, etc. Unfortunately, even
this is not always possible or practical. For exam-
ple, historical linguists usually describe phonologi-
cal changes from old to new, not from new to old.1
In addition, it is not possible to deterministically
translate the modern tagset to the older one. So, we
modify the MC training corpus to look more like the
OC corpus (the process we call ?aging?) and also the
target OC corpus to look more like the MC corpus
(?modernizing?).
6.1.2 Creating the Translation Tagger
Below we describe the process of creating a tagger.
As an example we discuss the details for the Trans-
lation tagger. Figure 1 summarizes the discussion.
1. Aging the MC training (annotated) corpus:
? MC to OC tag translation:
Dropping animacy distinction (OC did not
distinguish animacy).
1Note that one cannot simply reverse the rules, as in general,
the function is not a bijection.
? Simple MC to OC form transformations:
E.g., modern infinitives end in -t, OC in-
finitives ended in -ti;
(we implemented 3 transformations)
2. Training an MC tagger. The tagger is trained
on the result of the previous step.
3. Modernizing an OC plain corpus. In this
step we modernize OC forms by applying
sound/graphemic changes such as those in Ta-
ble 1. Obviously, these transformations are not
without problems. First, the OC-to-MC transla-
tions do not always result in correct MC forms;
even worse, they do not always provide forms
that ever existed. Sometimes these transforma-
tions lead to forms that do exist in MC, but are
unrelated to the source form. Nevertheless, we
think that these cases are true exceptions from
the rule and that in the majority of cases, these
OC translated forms will result in existing MC
words and have a similar distribution.
4. Tagging. The modernized corpus is tagged
with the aged tagger.
5. Reverting modernizations. Modernized words
are replaced with their original forms. This
gives us a tagged OC corpus, which can be used
for training.
6. Training an OC tagger. The tagger is trained on
the result of the previous step. The result of this
training is an OC tagger.
The results of the translation model are provided
in Tables 3 (for each individual tag position) and
4 (across various POS categories). What is evident
from these numbers is that the Translation tagger is
already quite good at predicting the POS, subPOS
and number categories. The most challenging POS
category is the category of verbs and the most diffi-
cult feature is case. Based on our previous experi-
ence with other fusional languages, getting the case
feature right is always challenging. Even though
case participates in syntactic agreement in both OC
and MC, this category is more idiosyncratic than,
say, person or tense. Therefore, the MC syntactic
and lexical information provided by the translation
13
STB
old plain
O2M: form translation
tag & form
(back) translation
STB'
plain
STB'
tagged
STB
tagged
tagging
train
Old Czech
HMM tagger
PDT corpus
modern 
annotated
M2O: tag & form translation
PDT ' corpus
annotated
train
HMM tagger
1
2
3
4
5
6
Figure 1: Schema of the Translation Tagger
model might not be sufficient to compute case cor-
rectly. One of the solutions that we explore in this
paper is approximating the OC lexical distribution
by the resource-light morphological analyzer (see
section 6.3).
While most nominal forms and their morpholog-
ical categories (apart from dual) survived in MC,
OC and MC departed in verbs significantly. Thus,
for example, three OC tenses disappeared in MC
and other tenses replaced them. These include the
OC two aorists, supinum and imperfectum. The
transgressive forms are almost not used in MC any-
more either. Instead MC has periphrastic past, pe-
riphrastic conditional and also future. In addition,
these OC verbal forms that disappeared in MC are
unique and non-ambiguous, which makes it even
more difficult to guess if the model is trained on the
MC data. The tagger, in fact, has no way of provid-
ing the right answer. In the subsequent sections we
use a morphological analyzer to address this prob-
lem. Our morphological analyzer uses very basic
hand-encoded facts about the target language.
6.2 Resource-light Morphological Analysis
The Even tagger described in the following section
relies on a morphological analyzer. While it can
use any analyzer, to stay within a resource light
paradigm, we have used our resource-light analyzer
(Hana, 2008; Feldman and Hana, 2010). Our ap-
proach to morphological analysis (Hana, 2008) takes
the middle road between completely unsupervised
systems on the one hand and systems with exten-
sive manually-created resources on the other. It ex-
ploits Zipf?s law (Zipf, 1935, 1949): not all words
and morphemes matter equally. A small number of
words are extremely frequent, while most words are
rare. For example, in PDT, 10% most frequent noun
lemmas cover about 75% of all noun tokens in the
corpus. On the other hand, the less frequent 50% of
noun lemmas cover only 5% of all noun tokens.
Therefore, in our approach, those resources that
are easy to provide and that matter most are created
14
Tags: 70.6
Position 0 (POS ): 91.5
Position 1 (SubPOS ): 88.9
Position 2 (Gender ): 87.4
Position 3 (Number ): 91.0
Position 4 (case ): 82.6
Position 5 (PossGen): 99.5
Position 6 (PossNr ): 99.5
Position 7 (person ): 93.2
Position 8 (tense ): 94.4
Position 9 (grade ): 98.0
Position 10 (negation): 94.4
Position 11 (voice ): 95.9
Table 3: Accuracy of the Translation Model on individual
positions (in %).
All Full: 70.6
SubPOS 88.9
Nouns Full 63.1
SubPOS 99.3
Adjs Full: 60.3
SubPos 93.7
Verbs Full 47.8
SubPOS 62.2
Table 4: Performance of the Translation Model on major
POS categories (in %).
manually or semi-automatically and the rest is ac-
quired automatically. For more discussion see (Feld-
man and Hana, 2010).
Structure The system uses a cascade of modules.
The general strategy is to run ?sure thing? modules
(ones that make fewer errors and that overgener-
ate less) before ?guessing? modules that are more
error-prone and given to overgeneration. Simplify-
ing somewhat the current system for OC contains the
following three levels:
1. Word list ? a list of 250 most frequent OC
words accompanied with their possible analy-
ses. Most of these words are closed class.
2. Lexicon-based analyzer ? the lexicon has been
automatically acquired from a plain corpus us-
ing the knowledge of manually provided infor-
mation about paradigms (see below).
3a. Guesser ? this module analyzes words relying
purely on the analysis of possible endings and
their relations to the known paradigms. Thus
the English word goes would be analyzed not
only as a verb, but also as plural of the po-
tential noun goe, as a singular noun (with the
presumed plural goeses), etc. In Slavic lan-
guages the situation is complicated by high in-
cidence of homonymous endings. For exam-
ple, the Modern Czech ending a has 14 differ-
ent analyses (and that assumes one knows the
morpheme boundary).
Obviously, the guesser has low precision, and
fails to use all kinds of knowledge that it po-
tentially could use. Crucially, however, it has
high recall, so it can be used as a safety net
when the more precise modules fail. It is also
used during lexicon acquisition, another con-
text where its low precision turns out not to be
a major problem.
3b. Modern Czech word list ? a simple analyzer
of Modern Czech; for some words this module
gives the correct answer (e.g., sva?tek ?holiday?,
some proper names).
The total amount of language-specific work needed
to provide OC data for the analyzer (information
about paradigms, analyses of frequent forms) is
about 12 hours and was done by a non-linguist on
the basis of (Va?z?ny?, 1964; Dosta?l, 1967).
The results of the analyzer are summarized in Ta-
ble 5. They show a similar pattern to the results we
have obtained for other fusional languages. As can
be seen, morphological analysis without any filters
(the first two columns) gives good recall but also
very high average ambiguity. When the automat-
ically acquired lexicon and the longest-ending fil-
ter (analyses involving the longest endings are pre-
ferred) are used, the ambiguity is reduced signifi-
cantly but recall drops as well. As with other lan-
guages, even for OC, it turns out that the drop in
recall is worth the ambiguity reduction when the re-
sults are used by our MA-based taggers. Moreover,
as we mentioned in the previous section, the tag-
ger based purely on the MC corpus has no chance
on verbal forms that disappeared from the language
completely.
15
Old Czech te[t
M$
anal\zed Old 
Cz te[t
tagged 
Old Cz te[t
3
tag translation
4
record of the 
original tags
compiling tnt 
emissions
5
tnt
tag back 
translation
eYen OCz 
emissions
Old Czech te[t
tag translation
1
tag translation
2
Cz 
transitions
Cz 
emissions
M$ Creation
)reTuent forms
/e[icon  Paradigms
(nding based *uesser
Modern Czech )orms
Figure 2: Schema of the MA Based Even Tagger
Lexicon & leo no yes
Recall Ambi Recall Ambi
Overall 96.9 14.8 91.5 5.7
Nouns 99.9 26.1 83.9 10.1
Adjectives 96.8 26.5 96.8 8.8
Verbs 97.8 22.1 95.6 6.2
Table 5: Evaluation of the morphological analyzer on Old
Czech
6.3 Even Tagger
The Even tagger (see Figure 2) approximates emis-
sions by using the output of the morphological ana-
lyzer described in the previous section.
The transition probabilities are based on the Aged
Modern Czech corpus (result of step 2 of Figure 1).
This means that the transitions are produced during
the training phase and are independent of the tagged
text. However, the emissions are produced by the
morphological analyzer on the basis of the tagged
text during tagging. The reason why the model
is called Even is that the emissions are distributed
evenly (uniformly; which is a crude approximation
of reality).
The overall performance of the Even tagger drops
down, but it improves on verbs significantly. Intu-
All Full: 67.7
SubPOS 87.0
Nouns Full 44.3
SubPOS 88.6
Adjs Full: 50.8
SubPos 87.3
Verbs Full 74.4
SubPOS 78.9
Table 6: Performance of the Even Tagger on major POS
categories (in %)
itively, this seems natural, because there is a rel-
atively small homonymy among many OC verbal
endings (see Table 2 for an example) so they are
predicted by the morphological analyzer with low
or even no ambiguity.
6.4 Combining the Translation and Even
Taggers
The TranslEven tagger is a combination of the
Translation and Even models. The Even model
clearly performs better on the verbs, while the Trans-
lation model predicts other categories much better.
So, we decided to combine the two models in the fol-
lowing way. The Even model predicts verbs, while
16
the Translation model predicts the other categories.
The TranslEven Tagger gives us a better overall per-
formance and improves the prediction on each indi-
vidual position of the tag. Unfortunately, it slightly
reduces the performance on nouns (see Tables 7 and
8).
All Full: 74.1
SubPOS 90.6
Nouns Full 57.0
SubPOS 91.3
Adjs Full: 60.3
SubPos 93.7
Verbs Full 80.0
SubPOS 86.7
Table 7: Performance of the TranslEven tagger on major
POS categories (in %)
Full tags: 74.1
Position 0 (POS ): 93.0
Position 1 (SubPOS ): 90.6
Position 2 (Gender ): 89.6
Position 3 (Number ): 92.5
Position 4 (case ): 83.6
Position 5 (PossGen): 99.5
Position 6 (PossNr ): 94.9
Position 7 (person ): 94.9
Position 8 (tense ): 95.6
Position 9 (grade ): 98.6
Position 10 (negation): 96.1
Position 11 (voice ): 96.4
Table 8: Performance of the TranslEven tagger on indi-
vidual positions (in %).
7 Discussion
We have described a series of experiments to cre-
ate a tagger for OC. Traditional statistical taggers
rely on large amounts of training (annotated) data.
There is no realistic prospect of annotation for OC.
The practical restrictions (no native speakers, lim-
ited corpora and lexicons, limited funding) make OC
an ideal candidate for a resource-light cross-lingual
method that we have been developing. OC and MC
departed significantly over the 500+ years, at all lan-
guage layers, including phonology, syntax and vo-
cabulary. Words that are still used in MC are often
used with different distributions and have different
morphological forms from OC.
Additional difficulty of this task arises from the
fact that our MC and OC corpora belong to different
genres. While the OC corpus includes poetry, cook-
books, medical and liturgical texts, the MC corpus
is mainly comprised of newspaper texts. We can-
not possibly expect a significant overlap in lexicon
or syntactic constructions. For example, the cook-
books contain a lot of imperatives and second per-
son pronouns which are rare or non-existent in the
newspaper texts.
Even though our tagger does not perform as the
state-of-the-art tagger for Czech, the results are al-
ready useful. Remember that the tag is a combina-
tion of 12 morphological features and if only one of
them is incorrect, the whole positional tag is marked
as incorrect. So, the performance of the tagger
(74%) on the whole tag is not as low in reality. For
example, if one is only interested in detailed POS
information (the tagset that roughly corresponds to
the English Penn Treebank tagset in size), the per-
formance of our system is over 90%.
Acknowledgments
This research was generously supported by
the Grant Agency Czech Republic (project ID:
P406/10/P328) and by the U.S. NSF grants
#0916280, #1033275, and #1048406. We would
like to thank Alena M. C?erna? and Boris Lehec?ka
for annotating the testing corpus and for answering
questions about Old Czech. We also thank Institute
of Czech Language of the Academy of Sciences of
the Czech Republic for the plain text corpus of Old
Czech. Finally, we thank anonymous reviewers for
their insightful comments. All mistakes are ours.
References
Be?mova, A., J. Hajic, B. Hladka?, and J. Panevova?
(1999). Morphological and Syntactic Tagging of
the Prague Dependency Treebank. In Proceedings
of ATALA Workshop, pp. 21?29. Paris, France.
Bo?hmova?, A., J. Hajic, E. Hajic?ova?, and B. Hladka?
(2001). The Prague Dependency Treebank:
Three-Level Annotation Scenario. In A. Abeille?
(Ed.), Treebanks: Building and Using Syntacti-
17
cally Annotated Corpora. Kluwer Academic Pub-
lishers.
Brants, T. (2000). TnT ? A Statistical Part-of-
Speech Tagger. In Proceedings of ANLP-NAACL,
pp. 224?231.
Cucerzan, S. and D. Yarowsky (2000). Language
Independent Minimally Supervised Induction of
Lexical Probabilities. In Proceedings of the 38th
Meeting of the Association for Computational
Linguistics (ACL), Hong Kong, pp. 270?277.
Cucerzan, S. and D. Yarowsky (2002). Bootstrap-
ping a Multilingual Part-of-speech Tagger in One
Person-day. In Proceedings of the 6th Confer-
ence on Natural Language Learning (CoNLL),
pp. 132?138. Taipei, Taiwan.
Dosta?l, A. (1967). Historicka? mluvnice c?eska? II ?
Tvaroslov??. 2. C?asova?n?? [Historical Czech Gram-
mar II - Morphology. 2. Conjugation]. Prague:
SPN.
Feldman, A. and J. Hana (2010). A resource-light
approach to morpho-syntactic tagging. Amster-
dam/New York, NY: Rodopi.
Hajic?, J. (2004). Disambiguation of Rich Inflection:
Computational Morphology of Czech. Praha:
Karolinum, Charles University Press.
Hana, J. (2008). Knowledge- and labor-light mor-
phological analysis. OSUWPL 58, 52?84.
Hana, J., A. Feldman, and C. Brew (2004, July). A
resource-light approach to Russian morphology:
Tagging Russian using Czech resources. In D. Lin
and D. Wu (Eds.), Proceedings of EMNLP 2004,
Barcelona, Spain, pp. 222?229. Association for
Computational Linguistics.
Janda, L. A. and C. E. Townsend (2002). Czech.
Karl??k, P., M. Nekula, and Z. Rus??nova? (1996).
Pr???ruc?n?? mluvnice c?es?tiny [Concise Grammar of
Czech]. Praha: Nakladatelstv?? Lidove? Noviny.
Lehec?ka, B. and K. Volekova? (2011).
(polo)automaticka? poc???tac?ova? transkripce
[(semi)automatic computational transcription].
In Proceedings of the Conference De?jiny c?eske?ho
pravopisu (do r. 1902) [History of the Czech
spelling (before 1902)]. in press.
Mann, S. E. (1977). Czech Historical Grammar.
Hamburg: Buske.
Merialdo, B. (1994). Tagging English Text with
a Probabilistic Model. Computational Linguis-
tics 20(2), 155?171.
Naughton, J. (2005). Czech: An Essential Gram-
mar. Oxon, Great Britain and New York, NY,
USA: Routledge.
Short, D. (1993). Czech. In B. Comrie and G. G.
Corbett (Eds.), The Slavonic Languages, Rout-
ledge Language Family Descriptions, pp. 455?
532. Routledge.
Va?z?ny?, V. (1964). Historicka? mluvnice c?eska? II
? Tvaroslov??. 1. Sklon?ova?n?? [Historical Czech
Grammar II - Morphology. 1. Declension].
Prague: SPN.
Yarowsky, D., G. Ngai, and R. Wicentowski (2001).
Inducing Multilingual Text Analysis via Robust
Projection across Aligned Corpora. In Proceed-
ings of the First International Conference on Hu-
man Language Technology Research (HLT), pp.
161?168.
Zipf, G. K. (1935). The Psychobiology of Language.
Houghton-Mifflin.
Zipf, G. K. (1949). Human Behavior and the Prin-
ciple of Least-Effort. Addison-Wesley.
18
