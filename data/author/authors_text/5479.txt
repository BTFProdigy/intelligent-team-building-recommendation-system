Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 652?659, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Methodology for Extrinsically Evaluating Information Extraction  
Performance 
 
Michael Crystal, Alex Baron, Katherine Godfrey, Linnea Micciulla, Yvette Tenney, and 
Ralph Weischedel  
BBN Technologies 
10 Moulton St. 
Cambridge, MA 02138-1119 
mcrystal@bbn.com 
 
Abstract 
This paper reports a preliminary study 
addressing two challenges in measuring 
the effectiveness of information extrac-
tion (IE) technology: 
? Developing a methodology for ex-
trinsic evaluation of IE; and, 
? Estimating the impact of improving 
IE technology on the ability to per-
form an application task. 
The methodology described can be em-
ployed for further controlled experi-
ments regarding information extraction. 
1 Introduction 
Intrinsic evaluations of information extraction 
(IE) have a history dating back to the Third Mes-
sage Understanding Conference1 (MUC-3) and 
continuing today in the Automatic Content Ex-
traction (ACE) evaluations.2  Extrinsic evalua-
tions of IE, measuring the utility of IE in a task, 
are lacking and needed (Jones, 2005).   
In this paper, we investigate an extrinsic 
evaluation of IE where the task is question an-
swering (QA) given extracted information.  In 
addition, we propose a novel method for explor-
ing hypothetical performance questions, e.g., if 
IE accuracy were x% closer to human accuracy, 
how would speed and accuracy in a task, e.g., 
QA, improve? 
                                                          
1 For more information on the MUC conferences, see 
http://www.itl.nist.gov/iad/894.02/related_projects/muc/.   
2 For an overview of ACE evaluations see 
http://www.itl.nist.gov/iad/894.01/tests/ace/.  
We plot QA accuracy and time-to-complete 
given eight extracted data accuracy levels rang-
ing from the output of SERIF, BBN?s state-of-
the-art IE system, to manually extracted data. 
2 Methodology 
Figure 1 gives an overview of the methodol-
ogy. The left portion of the figure shows source 
documents provided both to a system and a hu-
man to produce two extraction databases, one 
corresponding to SERIF?s automated perform-
ance and one corresponding to double-
annotated, human accuracy.  By merging por-
tions of those two sources in varying degrees 
(?blends?), one can derive several extracted da-
tabases ranging from machine quality, through 
varying percentages of improved performance, 
up to human accuracy. This method of blending 
databases provides a means of answering hypo-
thetical questions, i.e., what if the state-of-the-
art were x% closer to human accuracy, with a 
single set of answer keys. 
A person using a given extraction database 
performs a task, in our case, QA.  The measures 
of effectiveness in our study were time to com-
plete the task and percent of questions answered 
correctly.  An extrinsic measure of the value of 
improved IE technology performance is realized 
by rotating users through different extraction 
databases and questions sets.   
In our preliminary study, databases of fully 
automated IE and manual annotation (the gold 
standard) were populated with entities, relation-
ships, and co-reference links from 946 docu-
ments. The two initial databases representing 
machine extraction and human extraction re-
spectively were then blended to produce a con-
tinuum of database qualities from machine to 
652
human performance. ACE Value Scores3 were 
measured for each database. Pilot studies were 
conducted to develop questions for a QA task. 
Each participant answered four sets of questions, 
each with a different extraction database repre-
senting a different level of IE accuracy. An an-
swer capture tool recorded the time to answer 
each question and additional data to confirm that 
the participant followed the study protocol. The 
answers were then evaluated for accuracy and 
the relationship between QA performance and 
IE quality was established.  
Each experiment used four databases. The first ex-
periment used databases spanning the range from 
solely machine extraction to solely human extraction. 
Based on the results of this experiment, two further 
experiments focused on smaller ranges in database 
quality to study the relationship between IE and QA 
performance.  
2.1 Source Document Selection, Annota-
tion, and Extraction 
Source documents were selected based on the 
availability of manual annotation.  We identified 
946 broadcast news and newswire articles from 
recent ACE efforts, all annotated by the LDC 
according to the ACE guidelines for the relevant 
year (2002, 2003, 2004). Entities, relations, and 
within-document co-reference were marked.  
Inter-document co-reference annotation was 
added by BBN.  The 946 news articles com-
prised 363 articles (187,720 words) from news-
wire and 583 (122,216 words) from broadcast 
news. With some corrections to deal with errors 
and changes in guidelines, the annotations were 
loaded as the human (DB-quality 100) database. 
                                                          
3 The 2004 ACE evaluation plan, available at 
http://www.nist.gov/speech/tests/ace/ace04/doc/ace04-evalplan-
v7.pdf, contains a full description of the scoring metric used in the 
evaluation.  Entity type weights were 1 and the level weights were 
NAM=1.0, NOM=0.5, and PRO=0.1. 
SERIF, BBN?s automatic IE system based on its 
predecessor, SIFT (Miller, 2000), was run on the 
946 ACE documents to create the machine (DB-
quality 0) database. SERIF is a statistically 
trained software system that automatically per-
forms entity, co-reference, and relationship in-
formation extraction. 
Intermediate IE performance was simulated 
by blending the human and automatically gener-
ated databases in various degrees using an inter-
polation algorithm developed specifically for 
this study. To create a blended database, DB-
quality n, all of the entities, relationships, and 
co-reference links common to the human and 
automatically generated databases are copied 
into a new one. Then, n% of the entity mentions 
in the human database (100), but not in the 
automatic IE system output (0), are copied; and, 
(100 ? n)% of the entity mentions in the auto-
matically generated database, but not in the hu-
man database, are copied. Next, the relationships 
for which both of the constituent entity mentions 
have been copied are also copied to the blended 
database. Finally, co-reference links and entities 
for the already copied entity mentions are copied 
into the blended database. 
For the first experiment, two intermediate ex-
traction databases were created: DB-qualities 33 
and 67. For the second experiment, two addi-
tional databases were created: 16.5 and 50. The 
first intermediate databases were both created 
using the 0 and 100 databases as seeds. The 16.5 
database was created by mixing the 0 and the 33 
databases in a 50% blend. The 50 database was 
created by doing the same with the 33 and 67 
databases.  For Experiment 3, 41 and 58 data-
bases were created by mixing the 33 and 50, and 
50 and 67 databases respectively.  
100
0
100
67
33
0<<IE Tool>>
Annotators
Source
docs
++
+
0
-
Blended
Extraction
Accuracy
Measure QA
Accuracy &
Speed
QA Task
Study establishes 
curve
++
+
0
-
10067330
Imputed Accuracy 
Requirement
A Priori Utility 
Threshold
Q
A
 P
er
fo
rm
an
ce
IE Accuracy (% human annotation)
Figure 1: Study Overview 
653
 
  DB Blend 
  
0 
(Machine) 16.5 33 41 50 58 67 
100 
(Human) 
  Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel 
Recall 64 33 70 40 74 45 76 48 79 54 82 58 86 65 100 100 
Pre. 74 50 77 62 79 67 80 70 83 75 85 78 89 82 100 100 
Value 60 29 67 37 71 42 73 45 77 51 80 56 84 63 100 100 
Table 1: Precision, Recall and Value Scores for Entities and Relations for each DB Blend 
 
  0 
(Machine) 16.5 33 41 50 58 67 
100 
(Human)
Entities 17,117 18,269 18,942 19,398 19,594 19,589 19,440 18,687 
Relations 6,684 6,675 6,905 7,091 7,435 7,808 8,406 11,032 
Descriptions 18,666 18,817 19,135 19,350 19,475 19,639 19,752 20,376 
Table 2: Entity, Relation and Description Counts for each DB Blend 
 
To validate the interpolation algorithm and 
blending procedure, we applied NIST?s 2004 
ACE Scorer to the eight extraction databases. 
Polynomial approximations were fitted against 
both the entity and relation extraction curves. 
Entity performance was found to vary linearly 
with DB blend (R2 = .9853) and relation per-
formance was found to vary with the square of 
DB blend (R2 = .9961). Table 1 shows the scores 
for each blend, and Table 2 shows the counts of 
entities, relationships, and descriptions. 
2.2 Question Answering Task 
Extraction effectiveness was measured by how 
well a person could answer questions given a 
database of facts, entities, and documents. Par-
ticipants answered four sets of questions using 
four databases. They accessed the database using 
BBN?s FactBrowser (Miller, 2001) and recorded 
their answers and source citations in a separate 
tool developed for this study, AnswerPad. 
Each database represented a different data-
base quality. In some databases, facts were miss-
ing, or incorrect facts were recorded. 
Consequently, answers were more accessible in 
some databases than in others, and participants 
had to vary their question answering strategy 
depending on the database. 
Participants were given five minutes to an-
swer each question. To ensure that they had ac-
tually located the answer rather than relied on 
world knowledge, they were required to provide 
source citations for every answer. The instruc-
tions emphasized that the investigation was a 
test of the system, and not of their world knowl-
edge or web search skills. Compliance with 
these instructions was high. Users resorted to 
knowledge-based proper noun searches only one 
percent of the time. In addition, keyword search 
was disabled to force participants to rely on the 
database features. 
2.3 Participants 
Study participants were recruited through local 
web lists and at local colleges and universities.  
Participants were restricted to college students 
and recent graduates with PC (not Mac) experi-
ence, without reading disabilities, for whom 
English was their native language. No other 
screening was necessary because the design 
called for each participant to serve as his or her 
own control, and because opportunities to use 
world knowledge in answering the questions 
were minimized through the interface and pro-
cedures. 
During the first two months of the study 23 
participants were used to help develop questions, 
participant criteria, and the overall test proce-
dure. Then, experiments were conducted com-
paring the 0, 33, 67, and 100 database blends 
(Experiment 1, 20 subjects); the 0, 16.5, 33, and 
50 database blends (Experiment 2, 20 subjects), 
and the 33, 41, 50, and 58 database blends (Ex-
periment 3, 24 subjects). 
654
2.4 Question Selection and Validation 
Questions were developed over two months of 
pilot studies. The goal was to find a set of ques-
tions that would be differentially supported by 
the 0, 33, 67, and 100 databases. We explored 
both ?random? and ?engineered? approaches. 
The random approach called for creating ques-
tions using only the documents, without refer-
ence to the kind of information extracted. Using 
a list of keywords, one person generated 86 
questions involving relationships and entities 
pertaining to politics and the military by scan-
ning the 946 ACE documents to find references 
to each keyword and devising questions based 
on the information she found.  
The alternative, engineered approach involved 
eliminating questions that were not supported by 
the types of information extracted by SERIF, 
and generating additional questions to fit the 
desired pattern of increasing support with in-
creased human annotation. This approach en-
sured that the question sets reflected the 
structural differences that are assumed to exist in 
the database, and produced psychophysical data 
that link degree of QA support to human per-
formance parameters. The IE results from four 
of the databases (0, 33, 67 and 100) were used to 
develop questions that received differential sup-
port from the different quality databases. For 
example, such a question could be answered us-
ing the automatically extracted results, but might 
be more straightforwardly answered given hu-
man annotation. 
Sixty-four questions, plus an additional ten 
practice questions, were created using the engi-
neering approach. Additional criteria that were 
followed in creating the question sets were: 1) 
Questions had to contain at least one reasonable 
entry hook into all four databases, e.g., the terms 
U.S. and America were considered too broad to 
be reasonable; and, 2) For ease of scoring, list-
type questions had to specify the number of an-
swers required. Alternative criteria were consid-
ered but rejected because they correlated with 
the aforementioned set.  The following are ex-
amples of engineered questions. 
? Identify eight current or former U.S. State 
Department workers. 
? In what two West Bank towns does Fatah 
have an office? 
? Name two countries where Osama bin 
Laden has been. 
? Were Lebanese women allowed to vote in 
municipal elections between two Shiite 
groups in the year 1998? 
Two question lists, one with 86 questions 
generated by the random procedure and one with 
64 questions generated by the engineered proce-
dure, were analyzed with respect to the degree of 
support afforded by each of the four databases as 
viewed through FactBrowser. Four a priori cri-
teria were established to assess degree of support 
? or its opposite, the degree of expected diffi-
culty ? for each question in each of the four da-
tabases. Ranked from easiest to hardest, they are 
listed in Table 3. 
The question can be answered? 
1. Directly with fact or description (answer 
is highlighted in FactBrowser citation) 
2. Indirectly with fact or description (an-
swer is not highlighted) 
3. With name mentioned in question (long 
list of mentions without context) 
4. Via database crawling 
Table 3: A Priori Question Difficulty Character-
istics, listed from easiest to hardest 
Table 4 shows the question difficulty levels 
for both question types, for each of four data-
bases. Analysis of the engineered set was done 
on all 64 questions.  Analysis for randomly gen-
erated questions was done on a random sample 
of 44 of the 86 questions.  Fifteen questions did 
not meet the question criteria, leaving 29.  
The randomly generated questions showed a 
statistically significant, but small, variation in 
expected difficulty, in part due to the number of 
unanswerable questions. While the questions 
were made up with respect to information found 
in the documents, the process did not consider 
the types of extracted entities and relations. This 
problem might have been mitigated by limiting 
the search to questions involving entities and 
relations that were part of the extraction task. 
By contrast, the engineered question set 
showed a highly significant decrease in expected 
difficulty as the percentage of human annotation 
in the database increased (P < 0.0001 for chi-
square analysis). This result is not surprising, 
given that the questions were constructed with 
reference to the list of entities in the four data-
655
bases. The analysis confirms that the experimen-
tal manipulation of different degrees of support 
provided by the four databases was achieved for 
this question set. 
Random Question Generation 
Difficulty 
Level        
(easiest to 
hardest) 
0% 
Human 
33% 
Human 
67% 
Human 
100% 
Human 
1 Fact-
Highlight 
7 10 13 15
2 Fact-
Indirect 
14 10 8 10
3 Mention 3 5 2 1
4 Web Crawl 5 4 6 3
Total 29 29 29 29
     
Engineered Question Generation 
Difficulty 
Level               
(from easiest 
to hardest) 
0% 
Human 
33% 
Human 
67 
Human 
100% 
Human 
1 Fact-
Highlight 
16 25 35 49
2 Fact-
Indirect 
23 20 18 14
3 Mention 7 14 11 1
4 Web Crawl 18 5 0 0
Total 64 64 64 64
Table 4: Anticipated Difficulty of Questions as a 
Function of Database Quality 
Preliminary human testing with both question 
sets suggested that the a priori difficulty indica-
tors predict human question answering perform-
ance. Experiments with the randomly generated 
questions, therefore, were unlikely to reveal 
much about the databases or about human ques-
tion answering performance. On the other hand, 
an examination of how different levels of data-
base quality affect human performance, in a psy-
chophysical experiment where structure is varied 
systematically, promised to address the question 
of how much support is needed for good per-
formance. 
Based on the question difficulties, and pilot 
study timing and performance results, the 64 
questions were grouped into four, 16-question 
balanced sets. 
2.5 Procedure 
Participants were tested individually at our site, 
in sessions lasting roughly four hours. Training 
prior to the test lasted for approximately a half 
hour. Training consisted of a walk-through of 
the interface features followed by guided prac-
tice with sample questions. The test consisted of 
four question sets, each with a different data-
base.  Participants were informed that they 
would be using a different database for each 
question set and that some might be easier to use 
than others. 
Questions were automatically presented and 
responses were captured in AnswerPad, a soft-
ware tool designed for the study. AnswerPad is 
shown in Figure 2.  
Key features of the tool include: 
? Limiting view to current question set ? 
disallowing participants to view previous 
question sets 
? Automatically connecting to correct db 
? Logging time spent on each question 
? Enforcing five-minute limit per question 
? Enforcing requirement that all answers in-
clude a citation 
 
Figure 2: AnswerPad Question Presentation and 
Answer Capture Interface 
Participants were given written documenta-
tion as part of their training. The participants 
were instructed to cut-and-paste question an-
swers and document citations from source 
documents into AnswerPad. 
Extracted facts and entities, and source docu-
ments were accessed through FactBrowser. 
FactBrowser, shown in Figure 3, is web-browser 
based and is invoked via a button in AnswerPad. 
FactBrowser allows one to enter a string, which 
656
is matched against the database of entity men-
tions. The list of entities that have at least one 
mention partially matching the string are re-
turned (e.g., ?Laura Bush?) along with an icon 
indicating the type of the entity and the number 
of documents in which the entity appears.  
Clicking on the entity in the left panel causes the 
top right panel to display all of the descriptions, 
facts, and mentions for the entity. Selecting one 
of these displays citations in which the descrip-
tion, fact, or mention occurs. Clicking on the 
citation opens up a document view in the lower 
right corner of the screen and highlights the ex-
tracted information in the text. When a docu-
ment is displayed, all of the entities detected in 
the document are listed down the left side of the 
document viewer.  
 
 
Figure 3: Browsing Tool Interface 
The browsing tool was instrumented to record 
command invocations so that the path a partici-
pant took to answer a question could be recre-
ated, and the participant?s adherence to protocol 
could be verified. Furthermore, the find function 
(Ctrl-F) was disabled to prevent users from per-
forming ad hoc searches of the documents in-
stead of using the extracted data. 
The order of question sets and the order of da-
tabase conditions were counterbalanced across 
participants, so that, for every four participants, 
every question set and database appeared once in 
every ordinal position, and every question set 
was paired once with every database. This 
avoided carryover effects from question order. 
2.6 Data Collected 
Based on the initial results from Experiment 1, a 
70% target effectiveness threshold was identi-
fied to occur between the 33 and 67 database 
blends. To refine and verify this finding, Ex-
periment 2 examined the 0, 16.5, 33, and 50 da-
tabase blends. Experiment 3 examined the 33, 
41, 50, and 58 database blends. 
AnswerPad collected participant-provided an-
swers to questions and the corresponding cita-
tions. In addition, AnswerPad recorded the time 
spent answering the questions. A limit of five 
minutes was imposed based on pilot study re-
sults. The browsing tool logged commands in-
voked while the user searched the fact-base for 
question answers. Questions were manually 
scored based on the answers in the provided 
corpus. No partial credit was given. The maxi-
mum score, for each database condition, was 16, 
for a total maximum score of 64. 
3 Results 
Figure 4 shows the question answer scores 
and times for each of the three individual ex-
periments, and for Experiments 1 and 2 com-
bined. Database quality affects both task speed 
(downward-sloping line) and task accuracy (up-
ward-sloping line) in the expected direction. A 
logistic fit, as for a binary-response curve, was 
used to fit the relationship between blend per-
centage and accuracy in each experiment. The 
logistic fit Goodman-Theil quasi-R2 was .9973 
for Experiment 1, .9594 for Experiment 2, .8936 
for Experiment 3, and .9959 for Experiments 1 
and 2 combined. 
For the target accuracy of 70%, the 95% con-
fidence interval for the required blend is (35,56) 
around a predicted 46% blend for Experiment 1, 
and (41,56) around a predicted 49% for Experi-
ments 1 and 2 combined. 
 
657
Experiment 1 Performance and Time vs DB Blend
56
68
75
82
202
174
152
140
50
55
60
65
70
75
80
85
90
95
100
0.0 16.7 33.3 50.0 66.7 83.3 100.0
DB Blend (% Human)
P
er
fo
rm
an
ce
 (%
 C
or
re
ct
)
100
125
150
175
200
225
Ti
m
e 
(s
ec
on
ds
)
Experiment 2 Performance and Time vs DB Blend
52
61
64
70
210
187
183
166
50
55
60
65
70
75
80
85
90
95
100
0.0 16.7 33.3 50.0 66.7 83.3 100.0
DB Blend (% Human)
P
er
fo
rm
an
ce
 (%
 C
o
rr
ec
t)
100
125
150
175
200
225
T
im
e 
(s
ec
o
n
ds
)
Experiment 3 Performance and Time vs DB Blend
61
63
68 67
173
171
164
178
50
55
60
65
70
75
80
85
90
95
100
0.0 16.7 33.3 50.0 66.7 83.3 100.0
DB Blend (% Human)
P
er
fo
rm
an
ce
 (%
 C
or
re
ct
)
100
125
150
175
200
T
im
e 
(s
ec
o
n
d
s)
Experiments  1 & 2 Performance and Time vs DB Blend
54
61
66
70
75
82
206
187
179
166
152
140
50
55
60
65
70
75
80
85
90
95
100
0.0 16.7 33.3 50.0 66.7 83.3 100.0
DB Blend (% Human)
P
er
fo
rm
an
ce
 (%
 C
or
re
ct
)
100
125
150
175
200
225
Ti
m
e 
(s
ec
on
ds
)
% Correct Blend Lower Bound Logistic Fit for Blend
Blend Upper Bound Time  
Figure 4 QA Performance (upward-sloping) and QA Time (downward-sloping) vs. Extraction Blend 
Error Bars are Plus/Minus Standard Error of Mean (SEM) Within Each Blend 
Upper and Lower Bounds Are Approximate 95% Confidence Intervals Based on the Logistic Fit 
For the Blend (X) to Produce a Given Performance (Y) 
(Read these bounds horizontally, as bounds on X, with the upper bound to the right of the lower bound.) 
 
The downward-sloping line in each graph 
displays the average time to answer a question 
as a function of the extraction blend. For this 
analysis we used strict time, the time it took the 
participant to answer the question if he or she 
answered correctly, or the full 5 minutes allowed 
for any incorrectly answered question. This ad-
dresses the situation where a person quickly an-
swers all of the questions incorrectly.  The 
average question-answer time drops 32% as one 
moves from a machine generated extraction da-
tabase to a human generated database. A 
straight-line fit to the Experiment 1 and 2 com-
bined data predicts a drop of 6.5 seconds as the 
human proportion of the database increases by 
10 percentage points. 
A one-way repeated measures analysis of 
variance (ANOVA) was performed for Experi-
ment 1 (0-33-67-100), Experiment 2 (0-16.5-33-
50), and Experiment 3 (33-41-50-58). Table 5 
summarizes the results. In Experiments 1 and 2 
the impact of database quality on QA perform-
ance and on QA time were highly significant (P 
< 0.0001), but not for the narrower range of da-
tabases in Experiment 3. Other ANOVAs 
showed that the impact of trial order and ques-
tion set on QA performance were both non-
significant (P > 0.05). 
 
658
Experiment QA 
Performance 
Strict Time 
1 F(3,57) = 30.98, 
P < .0001  
F(3, 57) = 28.36 
P < .0001 
2 F(3,57)= 19.32, 
P < .0001  
F(3, 57) =  15.37,
P < .0001 
3 F(3,69)= 2.023, 
P = .1187 
F(3,69)= 1.053, 
P = .3747 
Table 5: ANOVA Analyses for QA Performance 
Expt. 1 used db blends of 0, 33, 67, and 100% 
Expt. 2 used db blends of 0, 16.5, 33, and 50% 
Expt. 3 used db blends of 33, 41, 50, and 58% 
In Experiment 1, Newman-Keuls contrasts 
indicate that the 0, 33, 67, and 100 databases 
differ significantly (P < .05) on their impact on 
QA quality. For Experiment 2, however, the 
16.5 and 33 database qualities were not shown to 
be different, nor were any of the database blends 
in Experiment 3. The data suggest that nearly 
half the improvement in QA quality from 0 to 
100 occurs by the 33 database blend, and more 
than half the improvement in QA quality from 0 
to 50 occurs by the 16.5 blend: a little ?human? 
goes a long way. Experiment 3 suggests that 
small differences in data blends make no practi-
cal difference in the results.  Alternatively, there 
might be real differences that are small enough 
such that a larger number of participants would 
be required to detect them. Experiment 3 also 
had two participants with atypical patterns of 
QA against blend, which might account for the 
failure to detect a difference between the 33 and 
50 or 58 blends as suggested by the results from 
Experiment 2. Furthermore, larger experiments 
could reveal whether the atypical participants 
were representatives of a subpopulation, or sim-
ply outliers. Bearing the possibility of outliers in 
mind, we used the combination of Experiments 
1 and 2 for the combined logistic analysis. 
4 Conclusions 
We presented a methodology for assessing in-
formation extraction effectiveness using an ex-
trinsic study. In addition, we demonstrated how 
a novel database blending (merging) strategy 
allows interpolating extraction quality from 
automated performance up through human accu-
racy, thereby decreasing the resources required 
to conduct effectiveness evaluations. 
Experiments showed QA accuracy and speed 
increased with higher IE performance, and that 
the database blend percentage was a good proxy 
for ACE value scores.  We emphasize that the 
study was not to show that IE supports QA bet-
ter than other technologies, rather to isolate util-
ity gains due to IE performance improvements. 
QA performance was plotted against human-
machine IE blend and, for example, 70% QA 
performance was achieved with a database blend 
between 41% and 46% machine extraction.  This 
corresponded to entity and relationship value 
scores of roughly 74 and 47 respectively. 
The logistic dose-response model provided a 
good fit and allowed for computation of confi-
dence bounds for the IE associated with a par-
ticular level of performance. The constraints 
imposed by AnswerPad and FactBrowser en-
sured that world knowledge was neutralized, and 
the repeated-measures design (using participants 
as their own controls across multiple levels of 
database quality) excluded inter-participant vari-
ability from experimental error, increasing the 
ability to detect differences with relatively small 
sample sizes. 
Acknowledgement 
This material is based upon work supported in 
part by the Department of the Interior under 
Contract No. NBCHC030014.  Any opinions, 
findings and conclusions or recommendations 
expressed in this material are those of the au-
thors and do not necessarily reflect the views of 
the Department of the Interior. 
References  
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel, 
"A Novel Use of Statistical Parsing to Extract In-
formation from Text", in Proceedings of 1st Meet-
ing of the North American Chapter of the ACL, 
Seattle, WA., pp.226-233, 2000.  
S. Miller, S. Bratus, L. Ramshaw, R. Weischedel, and 
A. Zamanian. "FactBrowser Demonstration", Hu-
man Language Technology Conference, San 
Diego, 2001. 
D. Jones and E. Walton, ?Measuring the Utility of 
Human Language Technology for Intelligence 
Analysis,? 2005 International Conference on Intel-
ligence Applications, McLean, VA May, 2005. 
659
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 513?519, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
teragram:
Rule-based detection of sentiment phrases using SAS Sentiment Analysis
Hilke Reckman, Cheyanne Baird, Jean Crawford, Richard Crowell,
Linnea Micciulla, Saratendu Sethi, and Fruzsina Veress
SAS Institute
10 Fawcett Street
Cambridge, MA 02138, USA
hilke.reckman@sas.com
Abstract
For SemEval-2013 Task 2, A and B (Sen-
timent Analysis in Twitter), we use a rule-
based pattern matching system that is based on
an existing ?Domain Independent? sentiment
taxonomy for English, essentially a highly
phrasal sentiment lexicon. We have made
some modifications to our set of rules, based
on what we found in the annotated training
data that was made available for the task. The
resulting system scores competitively, espe-
cially on task B.
1 Introduction
SAS taxonomies for sentiment analysis are primar-
ily topic-focused. They are designed to track sen-
timent around brands, entities, or other topics and
subtopics in a domain (Lange and Sethi, 2011;
Lakkaraju and Sethi, 2012; Albright and Lakkaraju,
2011). Domain-independent taxonomies have a
second function. In addition to performing topic-
focused tasks, they can be set up to perform senti-
ment analysis at the document level, classifying the
whole document as positive, negative, or neutral. In
this task all sentiment expressions are taken into ac-
count, rather than only those which are related to
the tracked topic. This second function is becom-
ing increasingly important. It allows for a broader
perspective that is complementary to topic-focused
opinion mining.
We participated in both subtask A and B of
SemEval-2013 Task 2: Sentiment Analysis in Twit-
ter (Wilson et al, 2013) with an adaptation of our
existing system. For task B, identifying the overall
sentiment of a tweet, our taxonomy mainly needed
some fine-tuning to specifically accommodate Twit-
ter data. (Normally tweets only make up a small
part of the data we work with.) We also made a
few adaptations to focus entirely on document level
sentiment, whereas originally the main focus of our
system was on tracking sentiment around products.
For task A, identifying the sentiment of ambiguous
phrases in a tweet, a few more modifications were
needed.
Our system is entirely rule-based, and the rules
are hand-written. In some cases, statistical text min-
ing approaches are used for the discovery of topics
and terms to facilitate rule writing. Our sentiment
analysis software does offer a statistical component,
but our experience is that purely rule-based models
work better for our typical sentiment analysis tasks.
Advantages of rules are that problems observed
in the output can be targeted directly, and the model
can become more and more refined over time. Also,
they allow for simple customization. In our brand-
centered work, we customize our taxonomies for
one or more brands that we want to track. When
we build a taxonomy for a new domain, we build
upon work we have done before in other domains.
The assignment of sentiment to certain phrases can
be sensitive to context where it needs to be. The
canceled task C, identifying sentiment related to a
topic, could have been approached successfully with
a rule-based approach, as our rules are specifically
designed to connect sentiment to targeted topics.
Section 2 describes the basic architecture of our
system, followed by a section on related work. Then
sections 4 and 5 describe the adaptations made for
513
each subtask and present the results. This is fol-
lowed by a more general discussion of our approach
in the light of these results in section 6, and the con-
clusion in section 7.
2 The base system
The datasets we normally use for the development
of our taxonomies include blogs, forums, news, and
Twitter. When developing a domain-specific taxon-
omy, we collect data for that particular domain, e.g.
Banking, Retail, Hospitality. We build the taxonomy
with the terms we encounter in those documents,
and test on a new set of documents. The Domain
Independent taxonomy started out as the common
base derived from several of these taxonomies, and
was then built out and tested using a wider range of
English-language documents. Since we used some
other tweets in the development of the original sys-
tem, our submission is considered unconstrained.
Our rules are patterns that match words or se-
quences of words, which makes our approach essen-
tially lexicon-based. Matching occurs left-to-right
and longer matches take precedence over shorter
ones. The top level rules in our sentiment taxonomy
are set up to recognize positive and negative word-
sequences. There is also a set of ?neutral? rules at
that level that block the assignment of positive or
negative sentiment in certain cases.
A positive or negative sequence can consist of a
single word from the positive or negative word-lists,
or a spelled out phrase from the positive or nega-
tive phrase-lists. Alternatively, it can be built up out
of multiple components, for example an emphatic
modifier and a sentiment term, or a negation and a
sentiment term. We call these sequences Positive
and Negative ?Contexts?, since they are contexts for
the topic-terms that we normally track.
Documents are preprocessed by an in-house POS-
tagger. Rules can require a word to have a particular
part of speech.
The words in the word-list, or in any of the other
rules, can be marked with an ?@?-sign to enable
morphological expansion, and in that case they will
match any of the forms in their paradigm. For ex-
ample ?love@? will match love, loves, loved, and
loving. This functionality is supported by a mor-
phological dictionary that links these forms to their
stem.
The rules are organized into lists that represent
useful concepts, which can be referred to in other
rules as a means of abstraction. For example the
rule:
def{Negation} def{PositiveAdjectives}
matches phrases that are composed of a negation (as
defined in the list named Negation) and a positive
adjective (as defined in the list named PositiveAd-
jectives). Negation includes rules like ?hasn?t been?,
?doesnt?[sic], ?not exactly the most?, etc., and Posi-
tiveAdjectives contains a rule that matches words in
PositiveWords if they are also tagged as adjectives.
For efficiency reasons the dependencies cannot be
circular, hence not allowing for recursion.
Distance rules can be used to capture a longer
span, matching a specified pattern at the beginning
and at the end, including arbitrary intervening words
up to a specified number. They can also be used to
make matching a term dependent on specified terms
in the context. For example,
(SENT, (DIST 4, ? a{ def{HigherIsBetter}}?,
? a{ def{Lowering}}?))
will capture phrases that say a company?s profit
(HigherIsBetter) went down (Lowering). The
SENT-operator prevents matching across sentence
boundaries.
(ORDDIST 7, ? def{PositiveContext}?,
? a{ def{PositiveAmbig}}?)
will capture ambiguous positive expressions when
they follow an unambiguously positive sequence
within a distance of 7 words.
This ensemble of lists and rules has grown rela-
tively organically, and is motivated by the data we
encounter. We introduce new distinctions when we
feel it will make a difference in terms of results,
or sometimes for ease of development and mainte-
nance.
Usually each sentiment expression has the same
weight, and one positive and one negative expres-
sion cancel each other out. However at the top level
we can introduce weights, and we have done so in
this model. We have created lists of weak positive
and negative expressions, and we gave those very
514
Positive:
? (ORDDIST 2, ? a{exceed@}?, ? a{expectation@}?)
? :Pro could not be happier
? blown away by
? def{Negation} want@ it to end
? above and beyond
? break@ down barriers
? can?t go wrong with
? dying to def{Consume}
? save@ me def{Money}
? (ALIGNED, ? c{treat@}?, ?:N?)
Negative:
? def{Negation} find def{NounPhrases}
def{PositivePhrases}
? (SENT, (ORDDIST 7, ? a{disappointed that}?,
? a{ def{PositivePhrases}}?))
? I would have loved
? def{Negation} accept@
? breach of def{PositiveWords}
? def{Money} magically disappears
? lack of training
? make@ no sense
? subject@ me to
? fun dealing with
Figure 1: Examples of rules for positive and negative
phrases and patterns.
low weights, so that they would only matter if there
were no regular-strength expressions present. We
limited some of those weak sentiment rules to sub-
task A only, but they clearly helped with recall there.
Negations in the default case turn positives into
negatives and negatives into neutrals. In addition to
negations we also have sentiment reversers, which
turn negatives into positives. Simple negations nor-
mally scope over a right-adjacent word or phrase, for
example a noun phrase or a verb. A special class of
clausal negations (I don?t think that) by approxima-
tion take scope over a clause.
This system contains roughly 2500 positive words
and 2000 positive phrases, and roughly 7500 neg-
ative words and 3000 negative phrases. Some ex-
amples are given in Figure 1. The neutral list also
contains about 2000 rules. Other helper lists such as
Negation, EmphaticModifiers, and Money typically
contain about a hundred rules each.
A system like this takes about six to eight weeks
to build for a new language. This requires a deve-
loper who is already familiar with the methodology,
and assumes existing support for the language, in-
cluding a morphological dictionary and a part-of-
speech tagger.
3 Related work
In tasks that are not topic-related, purely rule-based
models are rare, although the winning system of
SemEval-2010 Task 18 (Wu and Jin, 2010), some-
what similar to task A, was rule-based (Yang and
Liu, 2010). Liu (2010) suggests that more rule-
based work may be called for. However, there are
many other systems with a substantial rule-based
component (Nasukawa and Yi, 2003; Choi and
Cardie, 2008; Prabowo and Thelwall, 2009; Wilson
et al, 2005). Systems commonly have some rules
in place that account for the effect of negation (Wie-
gand et al, 2010) and modifiers. Sentiment lexicons
are widely used, but mainly contain single words
(Baccianella et al, 2010; Taboada et al, 2011). For
topic-related tasks, rule-based systems are a bit more
common (Ding et al, 2008).
4 Task A
Task A was to assign sentiment to a target in context.
The target in isolation would often be ambiguous. It
was a novel challenge to adapt our model for this
subtask.
Since we normally track sentiment around spe-
cific topics, we can usually afford to ignore highly
ambiguous phrases. Typical examples of this are
ambiguous emoticons and comments like no joke at
the end a sentence, or directly following it. When
these are used and could be disambiguated, usually
there is a less ambiguous term available that occurs
closer to the topic-term that we are interested in. (In
some cases we do use the topic as disambiguating
context.)
Also, we generally place slightly more empha-
sis on precision than on recall, assuming that with
enough data the important trends will emerge, even
if we ignore some of the unclear cases and outliers.
This makes the output cleaner and more pleasant to
515
work with for follow-up analysis.
4.1 Model adaptations and processing
We adapted our model to task A by introducing lists
of ambiguous positive and negative terms that were
then disambiguated in context, e.g. if there was an-
other sentiment term of a specified polarity nearby.
We also added some larger patterns that included an
ambiguous term, but as a whole had a much clearer
polarity. Below are some examples of rules for the
word like, which is highly ambiguous in English.
1. (ALIGNED, ? c{like@}?, ?:V?) (pos)
2. likes (pos)
3. I like (pos)
4. like magic (pos)
5. give it a ?like? (pos)
6. kinda like it (weakpos)
7. doesn?t seem like (hypothetical)
8. How can you like (neg)
9. don?t like (neg)
10. like to pretend (neg)
11. treated like a number (neg)
12. Is it like (neutral)
13. a bit like (neutral)
14. the likes of (neutral)
A seemingly obvious rule for like is (1), restrict-
ing it to usage as a verb. However, disambiguating
like is a difficult task for the tagger too, and the re-
sult is not always correct. Therefore this rule is a
fall-back case, when none of the longer rules apply.
Inflected forms such as (2) are pretty safe, with a
few exceptions, which can be caught by neutralizing
rules, such as (14). The hypothetical case, (7), is not
used in task A, but it is in task B.
A potential issue for our results on this task is that
our system only returns the longest match. So in a
sentence such as ?I didn?t like it?, if you ask people
to annotate like, they may say it is positive, whereas
the longer phrase didn?t like is negative. In the out-
put of our system, like will only be part of a negative
sequence. The information that it was originally rec-
ognized as a positive word cannot be retrieved at the
output level.
We found that the annotators for task A were in
general much more liberal in assigning sentiment
than we normally are. We made major gains by re-
moving some of our neutralizing rules, for example
those that neutralize sentiment in hypothetical con-
texts, and by classifying negations that were not part
of a larger recognized phrase as weak negatives.
The annotations in the development data were
sometimes confusing (see also section 6). We had
some difficulty in figuring out when certain terms
such as hope or miss you should be considered
positive and when negative. The verb apologize
turned out to be annotated sometimes positive and
sometimes negative in near identical tweets.
The test items were processed as follows:
1. run the sentiment model on the text (tweet/SMS)
2. identify the target phrase as a character span
3. collect detected sentiment that overlaps with the tar-
get phrase
(a) if there is no overlapping sentiment expres-
sion, the sentiment is neutral
(b) if there is exactly one overlapping sentiment
expression, that expression determines the
sentiment
(c) if there is more than one sentiment expression
that overlaps with the target, compute which
sentiment has more weight (and in case of a
draw, assign neutral)
4.2 Results
We get a higher precision for positive and negative
sentiment on task A than any of the other teams,
but we generally under-predict sentiment. Precision
on neutral sentiment is very low. Detecting neutral
phrases did not seem to be a very important goal in
the final version of this task, though. The results of
our predictions on the Twitter portion of the data are
shown in Figure 2.
These results are slightly different from what we
submitted, as we did not realize at the time of sub-
mission that the encoding of the text was different
in the test data than it had been in the previously re-
leased data. The submitted results are included in
the summarizing Table 1 at the end of the discussion
section.
Some targets are easily missed. We do not have
a good coverage of hashtags yet, for example. We
incorporate frequent misspellings that are common
in Twitter and SMS. However, we have no general
strategy in place to systematically recognize uncon-
ventionally spelled words (Eisenstein, 2013). For
516
gs \ pred positive negative neutral
positive 1821 77 888 2734
negative 47 1091 403 1541
neutral 11 6 143 160
1879 990 1382 4435
class precision recall f-score
positive 0.9691 0.6661 0.7895
negative 0.9293 0.7080 0.8037
neutral 0.1035 0.8938 0.1855
average(pos and neg) 0.7966
Figure 2: Confusion table and scores on task A, tweets
a project that processes Twitter data it would also
make sense to periodically scan for new hashtags
and add them to the rules if they carry sentiment.
However, a sentiment lexicon is never quite com-
plete.
Therefore we experimented with a guessing com-
ponent. If we do not detect any sentiment in the tar-
get sequence, we let our model make a guess, based
on the overall sentiment it assigns to the document,
assuming that an ambiguous target overall is more
likely to be positive in a positive context and neg-
ative in a negative context. (Note that this is differ-
ent from our disambiguation rules, which only apply
to explicitly listed items.) This gives us substantial
gains on this subtask (Figure 3). However, this may
not hold up in a similar task where there are more
neutral instances than there were here, as we see a
decrease in precision on positive and negative.
gs \ pred positive negative neutral
positive 2147 230 357 2734
negative 137 1249 155 1541
neutral 50 33 77 160
2334 1512 589 4435
class precision recall f-score
positive 0.9199 0.7853 0.8473
negative 0.8261 0.8105 0.8182
neutral 0.1307 0.4813 0.2056
average(pos and neg) 0.8327
Figure 3: Confusion table and scores on task A, tweets,
with guessing
5 Task B
Task B was to predict the overall sentiment of a
tweet. This was much closer to the task our tax-
onomy is designed for, and yet it turned out to be
different in subtle ways.
5.1 Model adaptations and processing
We quickly found that running the model as we had
adapted it for subtask A over-predicted sentiment
on subtask B. We therefore put most of our neu-
tralizing rules back in place for this subtask, and
restricted a subset of the weak sentiment terms to
subtask A only. We disabled the mechanism that
helped us catch ambiguous terms in subtask A (see
section 4.1).
For processing we used our standard method,
comparing the added weights of the positive and of
the negative sequences found. The highest score
wins. In case of a draw, the document is classified as
neutral. ?Unclassified? (no sentiment terms found)
also maps to neutral for this task. A confidence score
is computed, but not used here.
5.2 Results
Our system compares positively to those of the other
teams. Originally we were in 3rd place as a team
on the Twitter data. After correcting for the encod-
ing problem we rise to second (assuming the other
teams did not have the same problem). Among un-
constrained systems only, we are first on tweets and
second on SMS. The results, after the correction, are
shown in Figure 4. As for task A, the original results
are included in the final summarizing Table 1.
gs \ pred positive negative neutral
positive 1188 88 296 1572
negative 66 373 162 601
neutral 408 202 1030 1640
1662 663 1488 3813
class precision recall f-score
positive 0.7148 0.7557 0.7347
negative 0.5626 0.6206 0.5902
neutral 0.6922 0.6280 0.6586
average(pos and neg) 0.6624
Figure 4: Confusion table and scores on task B, tweets
517
6 Discussion
We modified an existing rule-based system for Sem-
Eval Task 2. While the development of this exist-
ing system was a considerable time investment, the
modifications for the two SemEval subtasks took
no more than about 2 person-weeks in total. The
models used in task A and B have a large common
base, and our rule-based approach measures up well
against other systems. This shows that if the work is
done once, it can be re-used, modified, and refined.
As mentioned in section 4.1, the annotations did
not always seem consistent. The guidelines did not
ask the annotators to keep in mind a particular task
or purpose for their annotations. However, the cor-
rect annotation of a tweet or fragment can vary de-
pending on the purpose of the annotation. Non-
arbitrary choices have to be made as to what counts
as sentiment: Do you try to identify cases of im-
plicit sentiment? Do you count cases of quoted or
reported ?3rd-party?-sentiment? . . . Ultimately it
depends on what you are interested in: Do you want
to: -track sentiment around certain topics? -know
how authors are feeling? -assess the general mood?
-track distressing versus optimistic messages in the
news? . . . While manual rule writing allows us to
choose a consistent strategy, it was not obvious what
the optimal strategy was in this SemEval task.
There were considerable differences in annotation
strategy between task A and task B, which shared the
same tweets. The threshold for detecting sentiment
appeared to be considerably lower in task A than in
task B. This suggests that different choices had been
made. These choices probably reflect how the anno-
tators perceived the tasks.
In our core business, we primarily track sentiment
around brands. One of the choices we made was
to also include good and bad news about the brand
(such as that the company?s stock went up or down)
where no explicit sentiment is expressed, because
the circulation of such messages reflects on the rep-
utation of the brand. (Liu (2010) points out that a
lot of sentiment is implicit.) In task B, we noticed
that ?newsy? tweets had a tendency to be annotated
as neutral. We did not have the time to thoroughly
adapt our model for that interpretation.
Both manually annotating training data for super-
vised machine learning and using training data for
manual rule writing require a lot of work. Both
can be crowd-sourced to a large extent if the pro-
cess is made simple enough, and the instructions
are clear enough. All methods that use lists of sen-
timent terms benefit from automatically extracting
such terms from a corpus (Qiu et al, 2009; Wiebe
and Riloff, 2005). As those methods become more
sophisticated, the work of rule writers becomes eas-
ier. Since the correct annotation depends on the task
at hand, and there are many different choices that
can be made, annotated data can be hard to reuse for
a slightly different task than the one for which it was
created. In rule-based models it is easier to leverage
earlier work and to slightly modify the model for a
new task. Both the rules and the model?s decision-
making process are human-interpretable.
Table 1 (next page) summarizes our results on the
various portions of the task, and under different con-
ditions. The results on SMS-data are consistently
lower than their counterparts on tweets, but they fol-
low the same pattern. We conclude that the model
generalizes to SMS, but not perfectly. This is not
surprising, since we have never looked at SMS-data
before, and the genre does appear to have some id-
iosyncrasies.
7 Conclusion
Our model is essentially a highly phrasal sentiment
lexicon. Ways of defining slightly more abstract pat-
terns keep the amount of work and the number of
rules manageable. The model is applied through pat-
tern matching on text, and returns a sentiment pre-
diction based on the number of positive and nega-
tive expressions found, based on the sum of their
weights. This is not mediated by any machine learn-
ing.
Slightly different versions of this system were em-
ployed in subtasks A and B. It turned out to be a
strong competitor in Task 2 of SemEval-2013, espe-
cially on subtask B, where it scored in the top three.
References
Russell Albright and Praveen Lakkaraju. 2011. Com-
bining knowledge and data mining to understand sen-
timent: A practical assessment of approaches. Techni-
cal report, SAS White Paper, January.
518
Task A Twitter Task A SMS Task B Twitter Task B SMS
F-score rank F-score rank F-score rank F-score rank
Submitted 0.7489 3of7 0.7283 4of7 0.6486 1of15 0.5910 2of15
13of23 11of19 3of34 5of29
After fixing encoding 0.7966 3of7 0.7454 3of7 0.6624 1of15 0.6014 1of15
11of23 8of19 2of34 4of29
With guessing 0.8327 (2of7) 0.7840 (2of7) NA NA
(8of23) (7of19)
Table 1: Summary of results. The first rank indication is relative to the other systems in the unconstrained category.
The second is relative to the total number of participating teams (by highest scoring system).
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th conference on International
Language Resources and Evaluation (LREC10), Val-
letta, Malta, May.
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 793?801. Association for Compu-
tational Linguistics.
Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the international conference on Web
search and web data mining, pages 231?240. ACM.
Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proc. of NAACL.
Praveen Lakkaraju and Saratendu Sethi. 2012. Corre-
lating the analysis of opinionated texts using sas text
analytics with application of sabermetrics to cricket
statistics. In Proceedings of SAS Global Forum 2012,
number 136.
Kathy Lange and Saratendu Sethi. 2011. What are peo-
ple saying about your company, your products, or your
brand? In Proceedings of SAS Global Forum 2011,
number 158.
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of natural language processing, 2:568.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: Capturing favorability using natural
language processing. In Proceedings of the 2nd in-
ternational conference on Knowledge capture, pages
70?77. ACM.
Rudy Prabowo and Mike Thelwall. 2009. Sentiment
analysis: A combined approach. Journal of Informet-
rics, 3(2):143?157.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international
jont conference on Artifical intelligence, pages 1199?
1204.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational linguistics,
37(2):267?307.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Computational Linguistics and Intel-
ligent Text Processing, pages 486?497. Springer.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the workshop on negation and specu-
lation in natural language processing, pages 60?68.
Association for Computational Linguistics.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: A system for subjectivity analysis. In
Proceedings of HLT/EMNLP on Interactive Demon-
strations, pages 34?35. Association for Computational
Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
Yunfang Wu and Peng Jin. 2010. Semeval-2010 task
18: Disambiguating sentiment ambiguous adjectives.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 81?85. Association for
Computational Linguistics.
Shi-Cai Yang and Mei-Juan Liu. 2010. Ysc-dsaa: An
approach to disambiguate sentiment ambiguous adjec-
tives based on saaol. In Proceedings of the 5th In-
ternational Workshop on Semantic Evaluation, pages
440?443. Association for Computational Linguistics.
519
