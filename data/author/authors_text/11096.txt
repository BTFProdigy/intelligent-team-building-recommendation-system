Coling 2008: Companion volume ? Posters and Demonstrations, pages 123?126
Manchester, August 2008
Experiments in Base-NP Chunking and Its Role in
Dependency Parsing for Thai
Shisanu Tongchim, Virach Sornlertlamvanich
Thai Computational Linguistics Laboratory
NICT Asia Research Center
112 Paholyothin Road
Klong 1, Klong Luang
Pathumthani 12120, Thailand
{shisanu,virach}@tcllab.org
Hitoshi Isahara
NICT
3-5, Hikari-dai, Seika-cho
Soraku-gun, Kyoto, 619-0289, Japan
isahara@nict.go.jp
Abstract
This paper studies the role of base-NP in-
formation in dependency parsing for Thai.
The baseline performance reveals that the
base-NP chunking task for Thai is much
more difficult than those of some lan-
guages (like English). The results show
that the parsing performance can be im-
proved (from 60.30% to 63.74%) with the
use of base-NP chunk information, al-
though the best chunker is still far from
perfect (F?=1 = 83.06%).
1 Introduction
Many NLP applications require syntactic informa-
tion and tools for syntactic analysis. However,
these linguistic resources are only available for
some languages. In case of Thai, the research in
developing tools for syntactic analysis and syntac-
tically annotated corpora is still limited. Most re-
search in the past has focused on morphological
analysis (i.e. word segmentation, part-of-speech
(POS) tagging). This can be viewed as a bottle-
neck for developing NLP applications that require
a deeper understanding of the language.
We have an ongoing project in developing a syn-
tactically annotated corpus. To accelerate the cor-
pus annotation, some syntactic analysis tools can
be applied in a preprocessing step before correct-
ing the results by human annotators. In this pa-
per, we use the first portion of completely anno-
tated corpus to examine the dependency parsing
and base-NP chunking. The findings will provide
c? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
some guidelines in selecting a parser and a base-
NP chunker for our corpus annotation workflow.
2 Dependency Parsing for Thai
The dependency structure for Thai is more flexible
than some languages like Japanese (Sekine et al,
2000), Turkish (Eryigit and Oflazer, 2006), while
it is close to Chinese (Cheng et al, 2005) and En-
glish (Nivre and Scholz, 2004). An example of
a Thai sentence with dependency relations is out-
lined in Fig. 1. Note that the dependency links are
drawn from the dependents to their heads. The de-
pendency relations of Thai are bidirectional in na-
ture and the root node can be found in arbitrary
positions. Some languages (e.g. Japanese) have
more constrained dependency structures, for ex-
ample, the dependency relations are only from left
to right and the root node is at the rightmost. Due
to the lack of structural constraints and larger num-
ber of possible candidates, finding the correct de-
pendency structure for Thai is more difficult.
??? ??????? ??? ????? ?? ???? ???????
Teacher assign for(to) each person read book
?The teacher assigns each person to read a book?
Figure 1: An example of a Thai sentence with de-
pendency relations.
There are only few studies investigating the de-
pendency parsing for Thai. To our knowledge, the
first research regarding dependency analysis was
done in (Aroonmanakun, 1989). However, this re-
search is based on a very small corpus (50 sen-
tences). The lack of syntactically annotated cor-
123
pora may be a possible explanation why not much
research has been done in this area. Some have
been developed, but they are relatively small or not
public, for example, a treebank of 400 sentences
used in (Satayamas et al, 2005).
To overcome the shortage of corpora, we initiate
the development of a syntactically annotated cor-
pus. This corpus will be used as a fundamental lin-
guistic resource for various projects. To improve
the annotation workflow, we use the first portion
of completely annotated corpus in experimenting
with dependency parsing and base-NP chunking.
The results will be used to improve the preprocess-
ing step of annotation.
Two dependency parsers are included in our ex-
periments. Both are data-driven.
? Model 1 : The first model has been widely
studied in parsing Japanese text. Some ma-
chine learning techniques are used to estimate
the probability that word wi modifies word
wj . Thus, the probability matrix of binary de-
pendency relations can be derived from this
estimation. Some search algorithms are then
used to find the most probable dependency
structure. In this study, we use support vec-
tor machines (SVMs) to estimate the proba-
bility values and use a beam search algorithm
to find the most likely dependency structure.
In parsing Japanese text, the root position is
not an issue. For Thai, however, we have to
identify the root position before finding the
complete dependency relations. Thus, we in-
corporate an additional module to identify the
root node of the sentence. This root finding
module is also based on an SVM.
The root finding module selects the word with
highest probability of being the root node.
The following features are used in the root
model: 1. POS, 2. position, 3. number of
verbs, 4. number of equivalent POS in front
of this word, 5. number of equivalent POS af-
ter this word, 6. number of equivalent major
POS in front of this word and 7. number of
equivalent major POS after this word.
For building the dependency model (e.g. re-
lation between wi and wj), the following fea-
tures are used: 1. POS of wi and wj , 2. de-
pendency direction, 3. distance, 4. major cat-
egory of wi and wj , 5. major POS of wi and
wj and 6. positions of wi and wj .
Table 1: Performance of dependency parsing
RA DA CSA
Model 1? 85.4% 76.0% 44.8%
Model 1? 86.2% 77.5% 47.9%
Model 2? 89.31% 83.53% 60.30%
Model 2? 91.22% 86.03% 65.27%
Note: ? (without chunk), ? (with chunk)
After identifying the root node and creating
the probability matrix, the beam search (beam
width=3) is performed.
? Model 2 : For the second model, we adopt
MaltParser 1.0.4 (Nivre et al, 2007) which is
a shift-reduce parser. Machine learning algo-
rithms are used to predict the sequence of ac-
tions for parsing. In this study, we use the
default setting that utilizes an SVM for pre-
dicting parsing actions.
Assuming that {i0, i1, i2, i4} are the first four
tokens in the remaining input and {s0, s1} are
the two topmost tokens on the stack, we use
the default features including: 1. POS of {i0,
i1, i2, i3, s0, s1}, 2. word form of {s0, i0,
i1, head(s0)}, 3. dependency type of s0 and
its leftmost and rightmost dependent and the
leftmost dependent of i0.
To examine the role of base-NP chunk infor-
mation in dependency parsing, we include chunk
labels in the feature sets of both parsers. Base-
NP chunks are represented by using the IOB2 for-
mat (Sang and Veenstra, 1999). In the first parsing
model, the chunk label of the current word is added
as a feature of the root model, while the chunk la-
bels of both considered words are added in the de-
pendency model. We also add a feature showing
that both words reside in the same chunk or not to
the dependency model. In the second model, we
include chunk labels of {s0, s1, i0, i1, i2, i3} as its
feature set.
We use a section of completely annotated corpus
consisting of 2616 sentences to experiment with
dependency parsing. The sentence length ranges
between 2 words to 20 words with an average of
5.68. These Thai sentences are part of our Thai-
Japanese parallel corpus developed for the MT
project. Since our MT project aims for the con-
versation domain, the source sentences are adopted
mainly from dialogues and conversation books. A
morphological analyzer is applied to these Thai
124
sentences for word segmentation and POS tagging,
and the results are revised manually by our annota-
tors. The sentences are then assigned chunk labels
with IOB2 representation and syntactic structure
respectively.
The corpus is divided into 2355 sentences as
the training set and 261 sentences as the test set.
The experiment is done with gold-standard POS
tags and chunk labels. Three performance met-
rics are used: 1. Root accuracy (RA): a portion
of sentences with correctly identified roots, 2. De-
pendency accuracy (DA): a ratio of correct de-
pendency relations to all dependency links and 3.
Complete sentence accuracy (CSA): a portion of
sentences with correct roots and dependency pat-
terns.
Table 1 shows the accuracy of two parsers with
and without using chunk information. The results
show that chunk information helps in improving
the performance of both parsers, especially in the
number of completely correct sentences. Malt-
Parser (Model 2) which is a shift-reduce parser
performs better in parsing Thai sentences. This
conforms with previously published literature that
shift-reduce parsers have been widely applied to
languages with dependency structure close to Thai
(e.g. English and Chinese), while variants of
Model 1 are applied to languages with more con-
straints in dependency structure (like Japanese).
Although the parsing accuracy can be improved
by chunk information, the results are based on
gold-standard chunk labels. To examine the possi-
bility for deriving chunk labels automatically, we
implement and evaluate base-NP chunkers in the
next section.
3 Base-NP Chunking
We implement a simplified version of Kudo?s
chunker (Kudo and Matsumoto, 2001). Kudo?s
chunker obtained very promising results on
standard English chunking tasks (e.g. preci-
sion=94.2%, recall=94.3%, F?=1 = 94.2%). We
use forward parsing method and employ an SVM
for identifying chunk labels. The original feature
set of Kudo?s chunker consists of: word form, POS
and previous chunk labels. Specifically, the fol-
lowing features are used in identifying the chunk
label of the word wi: word form and POS of
{wi?2, wi?1, wi, wi+1, wi+2}, chunk labels of
{wi?2, wi?1}. However, some preliminary results
show that the original feature set does not work
well with our problem. The obtained model suffers
from overfitting and lack of generalization. Thus,
we modify the feature set as: POS of {wi?2, wi?1,
wi, wi+1}, chunk labels of {wi?2, wi?1} and the
current size of NP chunk in front of wi. An SVM
is trained to estimate the probability of the current
word being each of three chunk labels (B, I, O). A
beam search strategy is used to find the most prob-
able chunk sequence.
In additional to the SVM-based chunkers, we
also examine chunkers based on conditional ran-
dom fields (CRFs). We use the implementation
of CRF++ (Kudo, 2008). CRFs outperform sev-
eral methods on this task (Sha and Pereira, 2003).
Three CRF-based chunkers are included in the ex-
periment: the first one uses word form and POS as
its feature set, the second one includes word class
(function word, content word) as an additional fea-
ture, the third one uses the previous three features
and the major POS category.
We use the training set and test set from pre-
vious section to experiment with chunking. Ta-
ble 2 shows the performance of all chunkers. A
baseline algorithm selects the chunk label which is
most frequently associated with POS of the cur-
rent word. From the results, all chunkers out-
perform the baseline algorithm. The best per-
formance can be obtained by one of CRF-based
chunkers (F?=1 = 83.06%). The inclusion of
more features for CRF-based chunkers helps in
improving the performance. In contrast, SVM-
based chunkers tend to suffer from overfitting
when adding more features. The results also con-
firm the findings of (Sha and Pereira, 2003) that
CRF-based chunkers can beat any single model.
However, the results are still lower than the re-
sults found in English experiments. A reason
may be that Thai NPs are more ambiguous than
English NPs. This is confirmed by a compari-
son between our baseline result (F?=1=55.4%) and
some baseline results of English base-NP chunk-
ing task (e.g. precision=81.9%, recall=78.2%,
F?=1=80.0% (Ramshaw and Marcus, 1995)).
Since the baseline algorithms work exactly in the
same way, the results imply that the Thai chunking
task is more difficult.
We also examine the use of the best chunker as
a preprocessing step of dependency parsing. Us-
ing the parser Model 2, the results are as follows:
RA=90.84%, DA=84.99%, CSA=63.74%. Over-
all, the accuracy of using predicted chunk labels is
125
Table 2: Performance of base-NP chunking
Pr. R. F?=1
Baseline 48.5% 64.5% 55.4%
SVM+beam search
beam width=1 70.1% 65.5% 67.7%
beam width=3 70.6% 66.6% 68.5%
beam width=5 69.6% 65.5% 67.5%
beam width=10 71.0% 66.9% 68.9%
beam width=20 71.0% 66.9% 68.9%
CRF
word+POS 84.79% 78.52% 81.54%
word+POS+class 85.34% 79.93% 82.54%
word+POS+class+main POS 86.04% 80.28% 83.06%
lower than the use of gold-standard chunk labels,
but still better than without any chunk information.
Although the chunking accuracy is not high as in
the reported results of English chunking tasks, the
results show that the dependency parsing still ben-
efits from the predicted chunk information.
4 Conclusions
The results from the chunking task show that the
chunk identification for Thai is not trivial due to
ambiguities in Thai NPs. The CRF-based chunkers
(best:F?=1 = 83.06%) are found to be more effec-
tive than the SVM-based chunkers (best:F?=1 =
68.9%).
Using the predicted chunk labels from the best
chunker in dependency parsing, the performance
of the best dependency parser can be improved
from CSA:60.30% to CSA:63.74%. This accu-
racy may further be improved if the performance
of chunker can be increased (as is shown in parsing
accuracy when using gold-standard chunk labels).
References
Aroonmanakun, Wirote. 1989. A dependency analy-
sis of thai sentences for a computerized parsing sys-
tem. Master thesis, Department of Linguistics, Chu-
lalongkorn University.
Cheng, Yuchang, Masayuki Asahara, and Yuji Mat-
sumoto. 2005. Chinese deterministic dependency
analyzer: Examining effects of global features and
root node finder. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pages 17?24.
Eryigit, Gu?lsen and Kemal Oflazer. 2006. Statistical
dependency parsing for turkish. In EACL. The As-
sociation for Computer Linguistics.
Kudo, Taku and Yuji Matsumoto. 2001. Chunking with
support vector machines. In NAACL.
Kudo, Taku. 2008. CRF++: Yet another CRF toolkit.
http://crfpp.sourceforge.net/.
Nivre, Joakim and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of Coling 2004, pages 64?70, Geneva, Switzerland,
Aug 23?Aug 27. COLING.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?ls?en Eryig?it, Sandra Ku?bler, Stetoslav
Marinov, and Erwin Marsi. 2007. Maltparser: A
language-independent system for data-driven depen-
dency parsing. Natural Language Engineering Jour-
nal, 13(2):99?135.
Ramshaw, Lance A. and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning.
In Proceedings of the Third Workshop on Very Large
Corpora, pages 82?94.
Sang, Erik F. Tjong Kim and Jorn Veenstra. 1999. Rep-
resenting text chunks. In Proceedings of the ninth
conference on European chapter of the Association
for Computational Linguistics, pages 173?179, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Satayamas, Vee, Chalatip Thumkanon, and Asanee
Kawtrakul. 2005. Bootstrap cleaning and quality
control for Thai tree bank construction. In The 9th
National Computer Science and Engineering Con-
ference, Bangkok, Thailand, Oct 27?Oct 28. (In
Thai).
Sekine, Satoshi, Kiyotaka Uchimoto, and Hitoshi Isa-
hara. 2000. Backward beam search algorithm for
dependency analysis of japanese. In COLING, pages
754?760. Morgan Kaufmann.
Sha, Fei and Fernando C. N. Pereira. 2003. Shal-
low parsing with conditional random fields. In HLT-
NAACL.
126
Analysis of an Iterative Algorithm for
Term-Based Ontology Alignment
Shisanu Tongchim, Canasai Kruengkrai, Virach Sornlertlamvanich,
Prapass Srichaivattana, and Hitoshi Isahara
Thai Computational Linguistics Laboratory,
National Institute of Information and Communications Technology,
112,Paholyothin Road, Klong 1, Klong Luang, Pathumthani 12120, Thailand
{shisanu, canasai, virach, prapass}@tcllab.org, isahara@nict.go.jp
Abstract. This paper analyzes the results of automatic concept align-
ment between two ontologies. We use an iterative algorithm to perform
concept alignment. The algorithm uses the similarity of shared terms in
order to find the most appropriate target concept for a particular source
concept. The results show that the proposed algorithm not only finds
the relation between the target concepts and the source concepts, but
the algorithm also shows some flaws in the ontologies. These results can
be used to improve the correctness of the ontologies.
1 Introduction
To date, several linguistic ontologies in different languages have been developed
independently. The integration of these existing ontologies is useful for many
applications. Aligning concepts between ontologies is often done by humans,
which is an expensive and time-consuming process. This motivates us to find an
automatic method to perform such task. However, the hierarchical structures of
ontologies are quite different. The structural inconsistency is a common problem
[1]. Developing a practical algorithm that is able to deal with this problem is a
challenging issue.
The objective of this research is to investigate an automated technique for
ontology alignment. The proposed algorithm links concepts between two ontolo-
gies, namely the MMT semantic hierarchy and the EDR concept dictionary. The
algorithm finds the most appropriate target concept for a given source concept
in the top-down manner. The experimental results show that the algorithm can
find reasonable concept mapping between these ontologies. Moreover, the results
also suggest that this algorithm is able to detect flaws and inconsistency in the
ontologies. These results can be used for developing and improving the ontologies
by lexicographers.
The rest of this paper is organized as follows: Section 2 discusses related
work. Section 3 provides the description of the proposed algorithm. Section
4 presents experimental results and discussion. Finally, Section 5 concludes
our work.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 346?356, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 347
2 Related Work
Daude? et al [2] used a relaxation labeling algorithm ? a constraint satisfaction
algorithm ? to map the verbal, adjectival and adverbial parts between two dif-
ferent WordNet versions, namely WordNet 1.5 and WordNet 1.6. The structural
constraints are used by the algorithm to adjust the weights for the connections
between WN1.5 and WN1.6. Later, some non-structural constraints are included
in order to improve the performance [3].
Asanoma [4] presented an alignment technique between the noun part of
WordNet and Goi-Taikei ?s Ontology. The proposed technique utilizes sets of
Japanese and/or English words and semantic classes from dictionaries in an MT
system, namely ALT-J/E.
Chen and Fung [5] proposed an automatic technique to associate the English
FrameNet lexical entries to the appropriate Chinese word senses. Each FrameNet
lexical entry is linked to Chinese word senses of a Chinese ontology database
called HowNet. In the beginning, each FrameNet lexical entry is associated with
Chinese word senses whose part-of-speech is the same and Chinese word/phrase
is one of the translations. In the second stage of the algorithm, some links are
pruned out by analyzing contextual lexical entries from the same semantic frame.
In the last stage, some pruned links are recovered if their scores are greater than
the calculated threshold value.
Ngai et al [6] also conducted some experiments by using HowNet. They
presented a method for performing alignment between HowNet and WordNet.
They used a word-vector based method which was adopted from techniques
used in machine translation and information retrieval. Recently, Yeh et al [7]
constructed a bilingual ontology by aligning Chinese words in HowNet with
corresponding synsets defined in WordNet. Their alignment approach utilized
the co-occurrence of words in a parallel bilingual corpus.
Khan and Hovy [8] presented an algorithm to combine an Arabic-English
dictionary with WordNet. Their algorithm also tries to find links from Arabic
words to WordNet first. Then, the algorithm prunes out some links by trying to
find a generalization concept.
Doan et al [9] proposed a three steps approach for mapping between ontologies
on the semantic web. The first step used machine learning techniques to determine
the joint distribution of any concept pair. Then, a user-supplied similarity function
is used to compute similarity of concept pairs based on the joint distribution from
the first step. In the final step, a relaxation labeling algorithm is used to find the
mapping configuration based on the similarity from the previous step.
3 Proposed Algorithm
In this section, we describe an approach for ontology alignment based on term
distribution. To alleviate the structural computation problem, we assume that
the considered ontology structure has only the hierarchical (or taxonomic) rela-
tion. One may simply think of this ontology structure as a general tree, where
each node of the tree is equivalent to a concept.
348 S. Tongchim et al
Given two ontologies called the source ontology Ts and the target ontology
Tt, our objective is to align all concepts (or semantic classes) between these
two ontologies. Each ontology consists of the concepts, denoted by C1, . . . , Ck. In
general, the concepts and their corresponding relations of each ontology can be
significantly different due to the theoretical background used in the construction
process. However, for the lexical ontologies such as the MMT semantic hierarchy
and the EDR concept dictionary, it is possible that the concepts may contain
shared members in terms of English words. Thus, we can match the concepts
between two ontologies using the similarity of the shared words.
In order to compute the similarity between two concepts, we must also con-
sider their related child concepts. Given a root concept Ci, if we flatten the
hierarchy starting from Ci, we obtain a nested cluster, whose largest cluster
dominates all sub-clusters. As a result, we can represent the nested cluster with
a feature vector ci = (w1, . . . , w
|V|
)T , where features are the set of unique En-
glish words V extracted from both ontologies, and wj is the number of the word
j occurring the nested cluster i. We note that a word can occur more than once,
since it may be placed in several concepts on the lexical ontology according to
its sense.
After concepts are represented with the feature vectors, the similarity be-
tween any two concepts can be easily computed. A variety of standard similarity
measures exists, such as the Dice coefficient, the Jaccard coefficient, and the co-
sine similarity [10]. In our work, we require a similarity measure that can reflect
the degree of the overlap between two concepts. Thus, the Jaccard coefficient is
suitable for our task. Recently, Strehl and Ghosh [11] have proposed a version
of the Jaccard coefficient called the extended Jaccard similarity that can work
with continuous or discrete non-negative features. Let ?xi? be the L2 norm of a
given vector xi. The extended Jaccard similarity can be calculated as follows:
JaccardSim(xi,xj) =
xTi xj
?xi?2 + ?xj?2 ? xTi xj
. (1)
We now describe an iterative algorithm for term-based ontology alignment.
As mentioned earlier, we formulate that the ontology structure is in the form of
the general tree. Our algorithm aligns the concepts on the source ontology Ts to
the concepts on the target ontology Tt by performing search and comparison in
the top-down manner.
Given a concept Ci ? Ts, the algorithm attempts to find the most appro-
priate concept B? ? Tt, which is located on an arbitrary level of the hierar-
chy. The algorithm starts by constructing the feature vectors for the current
root concept on the level l and its child concepts on the level l + 1. It then
calculates the similarity scores between a given source concept and candidate
target concepts. If the similarity scores of the child concepts are not greater
than the root concept, then the algorithm terminates. Otherwise, it selects a
child concept having the maximum score to be the new root concept, and it-
erates the same searching procedure. Algorithms 1 and 2 outline our ontology
alignment process.
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 349
Algorithm 1. OntologyAlignment
input : The source ontology Ts and the target ontology Tt.
output : The set of the aligned concepts A.
begin
Set the starting level, l ? 0;
while Ts?l? ? Ts?max? do
Find all child concepts on this level, {Ci}ki=1 ? Ts?l?;
Flatten {Ci}ki=1 and build their corresponding feature vectors, {ci}ki=1;
For each ci, find the best matched concepts on Tt,
B ? FindBestMatched(ci);
A ? A ? {B, Ci};
Set l ? l + 1;
end
end
Algorithm 2. FindBestMatched(ci)
begin
Set the starting level, l ? 0;
BestConcept ? Tt(root concept);
repeat
stmp ? JaccardSim(ci, BestConcept);
if Tt?l? > Tt?max? then
return BestConcept;
Find all child concepts on this level, {B}hj=1 ? Tt?l?;
Flatten {Bj}hj=1 and build corresponding feature vectors, {bj}hi=1;
sj? ? argmaxjJaccardSim(ci, {bj}hj=1);
if sj? > stmp then
BestConcept ? Bj? ;
Set l ? l + 1;
until BestConcept does not change;
return BestConcept;
end
Figure 1 shows a simple example that describes how the algorithm works.
It begins with finding the most appropriate concept on Tt for the root concept
1 ? Ts. By flattening the hierarchy starting from given concepts (?1? on Ts,
and ?a?, ?a-b?, ?a-c? for Tt), we can represent them with the feature vectors and
measure their similarities. On the first iteration, the child concept ?a-c? obtains
the maximum score, so it becomes the new root concept. Since the algorithm
cannot find improvement on any child concepts in the second iteration, it stops
the loop and the target concept ?a-c? is aligned with the source concept ?1?. The
algorithm proceeds with the same steps by finding the most appropriate concepts
on Tt for the concepts ?1-1? and ?1-2?. It finally obtains the resulting concepts
?a-c-f? and ?a-c-g?, respectively.
350 S. Tongchim et al
Fig. 1. An example of finding the most appropriate concept on Tt for the root concept
1 ? Ts
4 Experiments and Evaluation
4.1 Data Sets
Two dictionaries are used in our experiments. The first one is the EDR Elec-
tronic Dictionary [12]. The second one is the electronic dictionary of Multilingual
Machine Translation (MMT) project [13].
The EDR Electronic Dictionary consists of lexical knowledge of Japanese
and English divided into several sub-dictionaries (e.g., the word dictionary, the
bilingual dictionary, the concept dictionary, and the co-occurrence dictionary)
and the EDR corpus. In the revised version (version 1.5), the Japanese word
dictionary contains 250,000 words, while the English word dictionary contains
190,000 words. The concept dictionary holds information on the 400,000 concepts
that are listed in the word dictionary. Each concept is marked with a unique
hexadecimal number.
For the MMT dictionary, we use the Thai-English Bilingual Dictionary that
contains around 60,000 lexical entries. The Thai-English Bilingual Dictionary
also contains semantic information about the case relations and the word con-
cepts. The word concepts are organized in a manner of semantic hierarchy. Each
word concept is a group of lexical entries classified and ordered in a hierarchical
level of meanings. The MMT semantic hierarchy is composed of 160 concepts.
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 351
In our experiments, we used a portion of the MMT semantic hierarchy and the
EDR concept dictionary as the source and the target ontologies, respectively. We
considered the ?animal? concept as the root concepts and extracted its related con-
cepts. In the EDR concept dictionary, however, the relations among concepts are
very complex and organized in the form of the semantic network. Thus, we pruned
some links to transform the network to a tree structure. Starting from the ?animal?
concept, there are more than 200 sub-concepts (containing about 7,600 words) in
the EDR concept dictionary, and 14 sub-concepts (containing about 400 words) in
the MMT semantic hierarchy. It is important to note that these two ontologies are
considerably different in terms of the number of concepts and words.
4.2 Experimental Results
The proposed algorithm is used to find appropriate EDR concepts for each one of
14 MMT concepts. The results are shown in Table 1. From the table, there are 6 re-
lations (marked with the symbol ?*?) that aremanually classified as exact mapping.
This classification is done by inspecting the structures of both ontologies by hand.
If the definition of a given MMT concept appears in the EDR concept and the algo-
rithm seems to correctly match the most suitable EDR concept, this mapping will
be classified as exact mapping. The remaining 8 MMT concepts, e.g. ?cold-blood?
and ?amphibian?, are mapped to closely related EDR concepts, although they are
not considered to be exact mapping. The EDR concepts found by our algorithm for
these 8 MMT concepts are considered to be only the subset of the source concepts.
For example, the ?amphibian? concept of the MMT is mapped to the ?toad? concept
of the EDR. The analysis in the later section will explain why some MMT concepts
are mapped to specific sub-concepts.
Our algorithm works by flattening the hierarchy starting from the consid-
ered concept in order to construct a word list represented that concept. The
word lists are then compared to match the concepts. In practice, only a por-
tion of word list is intersected. Figure 2 illustrates what happens in general.
Note that the EDR concept dictionary is much larger than the MMT semantic
MMT
321
EDR
Fig. 2. A schematic of aligned concepts
352 S. Tongchim et al
Table 1. Results of aligned concepts between the MMT and the EDR
MMT concept EDR concept
vertebrate vertebrate ?
| ? warm-blood mammal
| | ? mammal mammal ?
| | ? bird bird ?
|
| ? cold-blood reptile
| ? fish fish ?
| ? amphibian toad
| ? reptile reptile ?
| ? snake snake ?
invertebrate squid
| ? worm leech
| ? insect hornet
| ? shellfish crab
| ? other sea creature squid
? These concepts are manually classified as exact mapping.
hierarchy. Thus, it always has EDR words that are not matched with any MMT
words. These words are located in the section 3 of the figure 2. The words
in the section 1 are more important since they affects the performance of the
algorithm. We assume that the EDR is much larger than the MMT. There-
fore, most MMT words should be found in the EDR. The MMT words that
cannot found any related EDR words may be results of incorrect spellings, spe-
cific words (i.e. only found in Thai language). In case of incorrect spelling and
other similar problems, the results of the algorithm can be used to improve the
MMT ontology.
By analyzing the results, we can classify the MMT words that cannot find
any associated EDR words into 4 categories.
1. Incorrect spelling or wrong grammar : Some English words in the MMT
semantic hierarchy are simply incorrect spelling, or they are written with
wrong grammar. For example, one description of a tiger species is written as
?KIND A TIGER?. Actually, this instance should be ?KIND OF A TIGER?.
The algorithm can be used to find words that possible have such a problem.
Then, the words can be corrected by lexicographers.
2. Inconsistency : The English translation of Thai words in the MMT semantic
hierarchy was performed by several lexicographers. When dealing with Thai
words that do not have exact English words, lexicographers usually enter
phrases as descriptions of these words. Since there is no standard of writing
the descriptions, these is incompatibility between descriptions that explain
the same concept. For example, the following phrases are used to describe
fishes that their English names are not known.
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 353
? Species of fish
? A kind of fish
? Species of fresh water fish
3. Thai specific words : The words that we used in our experiments are animals.
Several animals are region specific species. Therefore, they may not have any
associated English words. In this case, some words are translated by using
short phrases as English descriptions of these Thai words. Another way to
translate these words is to use scientific names of species.
The problems mentioned earlier make it more difficult to match concepts by
the algorithm. However, we can use the algorithm to identify where the problems
occur. Then, we can use these results to improve the MMT ontology.
The proposed algorithm works in the top-down manner. That is, the algo-
rithm attempts to find the most appropriate concept from the top level, and
it will move down if the lower concepts yield better scores. In order to analyze
the algorithm, we trace the algorithm during moving through the EDR concepts.
The first example of the bird concept alignment is shown in Table 2. The concept
alignment of this example is considered to be exact mapping. The first column
indicates the level of EDR concepts. The second and third columns indicate the
number of MMT words and the number of EDR words after flattening respec-
tively. The fourth column shows the number of intersected words between the
MMT and the EDR. From the table, the algorithm moves through the EDR con-
cepts in order to find the most specific concept that still maintains shared terms.
This example shows that the algorithm passes through 3 concepts until it stops
at the ?bird? concept of the EDR. At the final step, the algorithm decides to trade
few shared terms for a more specific EDR concept. Note that the MMT is not
completely cleaned. When moving down to the EDR bird concept, three shared
terms are lost. Our analysis shows that these terms are bat species. They are
all wrongly classified to the MMT bird concept by some lexicographers. Thus,
these shared terms will not intersect with any words in the EDR bird concept
when the algorithm proceeds to the lower step. This result suggests that our
algorithm is quite robust. The algorithm still finds an appropriate concept even
the MMT ontology has some flaws.
Another analysis of exact mapping is shown in Table 3. The algorithm moves
through 4 concepts until matching the EDR snake concept with the MMT snake
concept. In this example, the number of members in the MMT snake concept is
quite small. However, the number of shared terms is sufficient to correctly locate
the EDR snake concept.
Table 2. Concept alignment for the ?bird? concept
Level MMT words EDR words Intersected words
1 67 2112 26
2 67 1288 26
3 67 373 23
354 S. Tongchim et al
Table 3. Concept alignment for the ?snake? concept
Level MMT words EDR words Intersected words
1 17 2112 8
2 17 1288 8
3 17 71 8
4 17 26 8
The third example shown in Table 4 illustrates the case that is considered to
be subset mapping. That is, the EDR concept selected by the algorithm is sub-
concept of the MMT concept. This case happens several times since the EDR
is more fine-grained than the MMT. If the members of MMT concept do not
cover enough, the algorithm tends to return only sub-concepts. From the table,
the MMT amphibian concept covers only toad and frog species (3 members).
Thus, the algorithm moves down to a very specific concept, namely the EDR
toad concept. Another example of subset mapping is shown in Table 5. This
example also shows that the members of MMT concept do not cover enough.
These results can be used to improve the MMT ontology. If the MMT con-
cepts are extended enough, we expect that the correctness of alignment should
be improved.
Table 4. Concept alignment for the ?amphibian? concept
Level MMT words EDR words Intersected words
1 3 2112 2
2 3 1288 2
3 3 23 2
4 3 16 2
5 3 2 1
Table 5. Concept alignment for the ?other sea creature? concept
Level MMT words EDR words Intersected words
1 17 2112 5
2 17 746 5
3 17 78 3
4 17 3 2
5 Conclusion
We have proposed an iterative algorithm to deal with the problem of automated
ontology alignment. This algorithm works in the top-down manner by using the
similarity of the terms from each ontology. We use two dictionaries in our exper-
iment, namely the MMT semantic hierarchy and the EDR concept dictionary.
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 355
The results show that the algorithm can find reasonable EDR concepts for given
MMT concepts. Moreover, the results also suggest that the algorithm can be
used as a tool to locate flaws in the MMT ontology. These results can be used
to improve the ontology.
There are several possible extensions to this study. The first one is to examine
this algorithm with larger data sets or other ontologies. The second one is to
improve and correct the ontologies by using the results from the algorithm.
Then, we plan to apply this algorithm to the corrected ontologies, and examine
the correctness of the results. The third one is to use structural information of
ontologies in order to improve the correctness.
References
1. Ide, N. and Ve?ronis, J.: Machine Readable Dictionaries: What have we learned,
where do we go?. Proceedings of the International Workshop on the Future of
Lexical Research, Beijing, China (1994) 137?146
2. Daude?, J., Padro?, L. and Rigau, G.: Mapping WordNets Using Structural Informa-
tion. Proceedings of the 38th Annual Meeting of the Association for Computational
Linguistics, Hong Kong, (2000)
3. Daude?, J., Padro?, L. and Rigau, G.: A Complete WN1.5 to WN1.6 Mapping.
Proceedings of NAACL Workshop ?WordNet and Other Lexical Resources: Appli-
cations, Extensions and Customizations?, Pittsburg, PA, United States, (2001)
4. Asanoma, N.: Alignment of Ontologies: WordNet and Goi-Taikei. Proceedings of
NAACL Workshop ?WordNet and Other Lexical Resources: Applications, Exten-
sions and Customizations?, Pittsburg, PA, United States, (2001) 89?94
5. Chen, B. and Fung, P.: Automatic Construction of an English-Chinese Bilingual
FrameNet. Proceedings of Human Language Technology conference, Boston, MA
(2004) 29?32
6. Ngai, G., Carpuat , M. and Fung, P.: Identifying Concepts Across Languages: A
First Step towards a Corpus-based Approach to Automatic Ontology Alignment.
Proceedings of the 19th International Conference on Computational Linguistics,
Taipei, Taiwan (2002)
7. Yeh, J.-F., Wu, C.-H., Chen, M.-J. and Yu, L.-C.: Automated Alignment and
Extraction of a Bilingual Ontology for Cross-Language Domain-Specific Applica-
tions. International Journal of Computational Linguistics and Chinese Language
Processing. 10 (2005) 35?52
8. Khan, L. and Hovy, E.: Improving the Precision of Lexicon-to-Ontology Alignment
Algorithms. Proceedings of AMTA/SIG-IL First Workshop on Interlinguas, San
Diego, CA (1997)
9. Doan, A., Madhavan, J., Domingos, P., and Halevy, A.: Learning to Map Between
Ontologies on the Semantic Web. Proceedings of the 11th international conference
on World Wide Web, ACM Press (2002) 662?673
10. Manning, C. D., and Schu?tze, H.: Foundations of Statistical Natural Language
Processing. MIT Press. Cambridge, MA (1999)
356 S. Tongchim et al
11. Strehl, A., Ghosh, J., and Mooney, R. J.: Impact of Similarity Measures on Web-
page Clustering. Proceedings of AAAI Workshop on AI for Web Search (2000)
58?64
12. Miyoshi, H., Sugiyama, K., Kobayashi, M. and Ogino, T.: An Overview of the EDR
Electronic Dictionary and the Current Status of Its Utilization. Proceedings of the
16th International Conference on Computational Linguistics (1996) 1090?1093
13. CICC: Thai Basic Dictionary. Center of the International Cooperation for Com-
puterization, Technical Report 6-CICC-MT55 (1995)
