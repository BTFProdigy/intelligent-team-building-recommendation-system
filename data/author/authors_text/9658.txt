Proceedings of NAACL HLT 2009: Short Papers, pages 145?148,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Evaluating the Syntactic Transformations in Gold Standard Corpora  for Statistical Sentence Compression   Naman K. Gupta, Sourish Chaudhuri, Carolyn P. Ros? Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {nkgupta,sourishc,cprose}@cs.cmu.edu   Abstract 
We present a policy-based error analysis approach that demonstrates a limitation to the current commonly adopted paradigm for sentence compression.  We demon-strate that these limitations arise from the strong assumption of locality of the deci-sion making process in the search for an acceptable derivation in this paradigm. 1 Introduction In this paper we present a policy-based error analy-sis approach that demonstrates a limitation to the current commonly adopted paradigm for sentence compression (Knight and Marcu, 2000; Turner and Charniak, 2005; McDonald, 2006; Clark and La-pata 2006).       Specifically, in typical statistical compression approaches, a simplifying assumption is made that compression is accomplished strictly by means of word deletion. Furthermore, each sequence of con-tiguous words that are dropped from a source sen-tence is considered independently of other sequences of words dropped from other portions of the sentence, so that the features that predict whether deleting a sequence of words is preferred or not is based solely on local considerations.  This simplistic approach allows all possible derivations to be modeled and decoded efficiently within the search space, using a dynamic programming algo-rithm.       In theory, it should be possible to learn how to generate effective compressions using a corpus of source-target sentence pairs, given enough exam-ples and sufficiently expressive features.  How-ever, our analysis casts doubt that this framework 
with its strong assumptions of locality is suffi-ciently powerful to learn the types of example compressions frequently found in corpora of hu-man generated gold standard compressions regard-less of how expressive the features are.     Work in sentence compression has been some-what hampered by the tremendous cost involved in producing a gold standard corpus.  Because of this tremendous cost, the same gold standard corpora are used in many different published studies almost as a black box.  This is done with little scrutiny of the limitations on the learnability of the desired target systems. These limitations result from in-consistencies due to the subtleties in the process by which humans generate the gold standard compres-sions from the source sentences, and from the strong locality assumptions inherent in the frame-works.    Typically, the humans who have participated in the construction of these corpora have been in-structed to preserve grammaticality and to produce compressions by deletion.  Human ratings of the gold standard compressions by separate judges confirm that the human developers have literally followed the instructions, and have produced com-pressions that are themselves largely grammatical.  Nevertheless, what we demonstrate with our error analysis is that they have used meaning preserving transformation that didn't consistently preserve the grammatical relations from the source sentence while transforming source sentences into target sentences.  This places limitations on how well the preferred patterns of compression can be learned using the current paradigm and existing corpora.     In the remainder of the paper, we discuss rele-vant work in sentence compression.  We then in-troduce our policy-based error analysis technique.  Next we discuss the error analysis itself and the conclusions we draw from it.  Finally, we conclude 
145
with future directions for broader application of this error analysis technique. 2 Related Work  Knight and Marcu (2000) present two approaches to the sentence compression problem- one using a noisy channel model and the other using a deci-sion-based model. Subsequent work (McDonald, 2006) has demonstrated an advantage for a soft constraint approach, where a discriminative model learns to make local decisions about dropping a sequence of words from the source sentence in or-der to produce the target compression.  Features in this system are defined over pairs of words in the source sentence, with the idea that the pair of words would appear adjacent in the resulting com-pression, with all intervening words dropped.  Thus, the features represent this transformation, and the feature weights are meant to indicate whether the transformation is associated with good compressions or not.      We use McDonald?s (2006) proposed model as a foundation for our work because its soft constraint approach allows for natural integration of a variety of classes of features, even overlapping features.  In our prior work we have explored the potential for improving the performance of a compression system by including additional, more sophisticated syntactically motivated features than those in-cluded in previously published models.  In this pa-per, we evaluate the gold standard corpus itself using similar syntactic grammar policies. 3 Grammar Policy Extraction In the domain of Sentence Compression, the cor-pus consists of source sentences each paired with a gold standard compressed sentence. Most of the above related work has been evaluated using the following 2 corpora, namely the Ziff-Davis (ZD) set (Knight and Marcu, 2002) consisting of 1055 sentences, and a partial Broadcast News Corpus (CL Corpus) (Clarke and Lapata, 2006) originally consisting of 1619 sentences, of which we used 1070 as the training set in our development work as well as in the error analysis below. Hence, we use these two popular corpora to present our work. We hypothesize certain grammar policies that in-tuitively should be followed while deriving the target-compressed sentence from the source sen-
tence if the mapping between source and target sentences is produced via grammatical transforma-tions. The basic idea behind these policies grows out of the same ideas motivating the syntactic fea-tures used in McDonald (2006). These policies, extracted using the MST (McDonald, 2005) de-pendency parse structure of the source sentence, are as follows:  1. The syntactic root word of a sentence should be retained in the compressed sen-tence. 2.  If a verb is retained in the compressed sentence, then the dependent subject of that verb should also be retained. 3. If a verb is retained in the compressed sen-tence, then the dependent object of that verb should also be retained. 4. If the verb is dropped in the compressed sentence then its arguments, namely sub-ject, object, prepositional phrases etc., should also be dropped. 5. If the Preposition in a Prepositional phrase (PP) is retained in the compressed sen-tence, then the dependent Noun Phrase (NP) of that Preposition should also be re-tained. 6. If the head noun of a Noun phrase (NP) within a Prepositional phrase is retained in the compressed sentence, then the syntac-tic parent Preposition of the NP should also be retained. 7. If a Preposition, the syntactic head of a Prepositional phrase (PP), is dropped in the compressed sentence, then the whole PP, including dependent Noun phrase in that PP, should also be dropped. 8. If the head noun of a Noun phrase within a Prepositional phrase (PP) is dropped in the compressed sentence, then the syntactic parent Preposition of the PP should also be dropped.  These grammar policies make predictions about where, in the phrase structure, constituents are likely to be dropped or retained in the compres-sion.  Thus, these policies have similar motivation to the syntactic features in the McDonald (2006) model. However, there is a fundamental difference in the way these policies are computed. In the McDonald (2006) model, the features are com-
146
puted locally over adjacent words yi-1 & yi in the compression and the words dropped from the original sentence between that word range yi-1 & yi. In cases where the syntactic structure of the in-volved words extends beyond this range, the ex-tracted features are not able to capture all of the relevant syntactic dependencies. On the other hand, in our analysis the policies are computed globally over the complete sentence without specifying any range of words. As an illustrative example, let us consider the following sentence from the CL Cor-pus (bold represents dropped words):  1. The1 leaflet2 given3 to4 Labour5 activists6 mentions7 none8 of9 these10 things11.  According to Policy 2, since the verb 'mentions' is retained, the subject of the verb ?the leaflet? should also be retained. In the McDonald (2006) model, by looking at the local range yi-1 = 5 and yi = 7 for the verb 'mentions', we will not be able to compute whether the subject(1,2) was retained in the compression or not. So this policy can be cap-tured only if the global context is taken into ac-count while evaluating the verb 'mentions'. Now we evaluate each sentence in the corpus to determine whether a particular policy was applica-ble and if applicable then whether it was violated. Table 1 shows the summary of the evaluation of all the sentences in the two corpora. Column 2 in the table shows the percentage of sentences in the ZD Corpus where the respective policies were applica-ble. And column 3 shows the percentage of sen-tences where the respective policies were violated, whenever applicable. Columns 4 and 5 show re-spective percentages for the CL corpus. 4 Evaluation In this section we discuss the results from evaluat-ing the 8 grammar policies discussed in Section 3 over the ZD and CL corpora, as discussed above.   The policies were evaluated with respect to whether they applied in a sentence, i.e., whether the premise of the ?if ? then? rule is true in the sentence, and whether the policy was broken when applied, i.e., if the premise is true but the conse-quent is false.  The striking finding is that for every one of the policies discussed in the previous sec-tion, they are violated for at least 10% of the sen-tences where they applied, and sometimes as much as 72%.  For most policies, the proportion of sen-tences where the policy is violated when applied is 
a minority of cases.  Thus, based on this, we can expect that grammar oriented features motivated by these policies and derived from a syntactic analysis of the source and/or target sentences in the gold standard could be used to improve the per-formance of compression systems that don?t make use of syntactic information to that extent.  How-ever, the noticeable proportion of violations with respect to some of the policies indicate that there is a limited extent to which these types of features can contribute towards improved performance. One observation we make from Table 1 is that while the proportion of sentences where the poli-cies (Columns 2 and 4) apply as well as the propor-tion of sentences where the policies are broken when applied (Columns 3 and 5) are highly corre-lated between the two corpora.  Nevertheless, the distributions are not identical. Thus, again, while we predict that using this style of dependency syn-tax features might improve performance of com-pression systems within a single corpus, we would not expect trained models that rely on these syntac-tic dependency features to generalize in an ideal way between corpora.   ZD (%  Appli-cable) ZD (% Viola-tions when Appli-cable) 
CL (%  Appli-cable) CL (%  Viola-tions when Appli-cable) Policy1 100% 34% 100% 14% Policy2 66% 18% 84% 18% Policy3 50% 10% 61% 24% Policy4 59% 59% 46% 72% Policy5 62% 17% 77% 27% Policy6 65% 22% 79% 29% Policy7 57% 25% 58% 40% Policy8 55% 16% 58% 36% Table 1: Summary of evaluation of grammar policies over the Ziff-Davis (ZD) training set and Clark-Lapata (CL) training set.  Beyond the above evaluation illustrating the extent to which grammar inspired policies are violated in human generated gold standard corpora, interesting insights into challenges that must be addressed in order to improve performance can be obtained by taking a close look at typical examples from the CL corpus where the policies are broken in the 
147
gold standard corpora (bold represents dropped words).  1. The attempt to put flesh and blood on the skeleton structure of a possible united Europe emerged. 2. Annely has used the gallery ?s three floors to divide the exhibits into three dis-tinct groups. 3. Labor has said it will scrap the system. 4. Montenegro ?s sudden rehabilitation of Nicholas ?s memory is a popular move.  In Sentence 1, retaining the dependent Noun struc-ture of the dropped Preposition on in the PP vio-lates Policy 7. Such a NP to Infinitive Phrase transformation changes the syntactic structure of the sentence. Sentence 2 also breaks several poli-cies, namely Policies 1, 4 and 7. The syntactic root has is dropped. Also the main verb has used is dropped while retaining the Subject Annely. In Sentence 3, breaking Policies 1, 2 and 4, the hu-man annotators replaced the pronoun it with the noun Labor, the subject of a dropped verb ?has said?. Such anaphora resolution cannot be done without relevant context, which is not available in strictly local paradigms of sentence compression. In Sentence 4, policies 3. 5 and 8 are violated. Transformations like substituting Nicholas?s mem-ory by the metonym Nicholas and popular move by popular need to be identified and analyzed. Such varied transformations, made in the syntactic struc-ture of the sentences by human annotators, are counter-intuitive, making them hard to be captured in the linear models learned in association with the syntactic features in current compression systems. 5 Conclusions and Current Directions In this paper we have introduced a policy-based error analysis technique that was used to investi-gate the potential impact and limitations of adding a particular style of dependency parse features to typical statistical compression systems.  We have argued that the reason for the limitation arises from the strong assumption of the local nature of the decisions that are made in obtaining the system-generated compression from a source sentence.       Other related technologies such as statistical machine translation and statistical paraphrase are based on similar paradigms with similar assump-
tions of the local nature of decisions that are made in the search for an acceptable derivation.  We con-jecture both that it is likely that the same issues related to the construction of the gold standard corpora likely apply and that a similar policy-based error analysis approach could be used in order to assess the extent to which this is true and identify possible directions for improving performance.  In our ongoing work, we plan to conduct a similar error analysis for these problems in order to evalu-ate the generality of the findings reported here.   Acknowledgments This work was funded in part by the Office of Na-val Research grant number N00014510043. References  James Clarke and Mirella Lapata. 2006. Constraint-Based Sentence Compression: An Integer Program-ming Approach. Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions (ACL-2006), pages 144-151, 2006. James Clarke and Mirella Lapata. 2006. Models for Sen-tence Compression: A Comparison across Domains, Training Requirements and Evaluation Measures.  Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, 377-384. Sydney, Australia. Kevin Knight and Daniel Marcu. 2000. Statistics-Based Summarization ? Step One: Sentence Compression. Proceedings of AAAI-2000, Austin, TX, USA. Knight, Kevin and Daniel Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artificial Intelligence 139(1):91?107. Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of de-pendency parsers. Proc. ACL. Ryan Mcdonald, 2006. Discriminative sentence com-pression with soft syntactic constraints. Proceedings of the 11th EACL. Trento, Italy, pages 297--304. Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. Proc. ACL. 
148
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 24?27,
Columbus, June 2008. c?2008 Association for Computational Linguistics
SIDE: The Summarization Integrated Development Environment 
 
Moonyoung Kang, Sourish Chaudhuri, Mahesh Joshi, Carolyn P. Ros? 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA 15213 USA 
moonyoun,schaudhu,maheshj,cprose@cs.cmu.edu 
 
Abstract 
In this type-II demo, we introduce SIDE1 (the 
Summarization Integrated Development Envi-
ronment), an infrastructure that facilitates 
construction of summaries tailored to the 
needs of the user. It aims to address the issue 
that there is no such thing as the perfect sum-
mary for all purposes. Rather, the quality of a 
summary is subjective, task dependent, and 
possibly specific to a user. The SIDE frame-
work allows users flexibility in determining 
what they find more useful in a summary, 
both in terms of structure and content. As an 
educational tool, it has been successfully user 
tested by a class of 21 students in a graduate 
course on Summarization and Personal Infor-
mation Management. 
1 Introduction 
A wide range of summarization systems have 
been developed in the past 40 years, beginning 
with early work in the Library sciences field. To 
this day, a great deal of research in summarization 
focuses on alternative methods for selecting sub-
sets of text segments based on a variety of forms of 
rhetorical analysis and relevance rankings.  Never-
theless, while there is much in common between 
approaches used for summarization in a variety of 
contexts, each new summarization project tends to 
include a new system development effort, because 
a general purpose, extensible framework for sum-
                                                            
1
 The working system can be downloaded from 
http://www.cs.cmu.edu/~cprose/SIDE.html, and a video 
of an example of SIDE use can be found at 
http://ankara.lti.cs.cmu.edu/side/video.swf. 
This project is supported by ONR Cognitive and Neural 
Sciences Division, Grant number N000140510043 
 
 
marization has not been made available. As an ex-
ample, Teufel and Moens? (2002) argue that the 
summarization strategy for scientific articles must 
be different from news articles because the former 
focus on novelty of information, are much longer 
and very different in structure. 
A large proportion of summarization systems do 
not allow users to intervene in the summarization 
process so that the form of the summary could be 
tailored to the individual user?s needs (Mieskes, M., 
M?ller, C., & Strube, M., 2007). From the same 
document, many summaries can potentially be 
generated, and the most preferable one for one user 
will not, in general, be the same as what is pre-
ferred by a different user. The fact that users with 
similar backgrounds can have vastly differing in-
formation needs is highlighted by Paice and Jones? 
(1993) study where an informal sentence selection 
experiment had to be abandoned because the par-
ticipants, who were agriculture experts, were too 
influenced by their research interests to agree with 
each other. However, summarization systems tend 
to appear as black boxes from the user?s perspec-
tive and the users cannot specify what they would 
want in the summary.  
SIDE is motivated by the two scenarios men-
tioned above - the absence of a common tool for 
generating summaries from different contexts, as 
well as the fact that different users might have dif-
ferent information needs from the same document. 
Bellotti (2005) discusses the problem of informa-
tion overload in communication media such as e-
mail and online discussion boards. The rapid 
growth of weblogs, wikis and dedicated informa-
tion sources makes the problem of information 
overload more acute. It also means that summari-
zation systems have the responsibility of taking 
into account the kind of information that its user 
would be interested in. 
With SIDE, we attempt to give the user a greater 
say in deciding what kind of information and how 
much of it the user wants as part of his summary.  
24
In the following sections, we elaborate on the 
features of SIDE and its technical details.   
2 Functionality 
The design of SIDE is aimed at allowing the user 
as much involvement at every stage of the sum-
mary generation process as the user wishes. SIDE 
allows the user to select a set of documents to train 
the system upon, and to decide what aspects of 
input documents should be detected and used for 
making choices, particularly at the stage of select-
ing a subset of segments to preserve from the 
source documents. The other key feature of the 
development environment is that it allows devel-
opers to plug in custom modules using the Plugin 
Manager in the GUI. In this way, advanced users 
can extend the capabilities of SIDE for meeting 
their specific needs while still taking advantage of 
the existing, general purpose aspects of SIDE. 
     The subsequent sub-sections discuss individual 
parts of system behavior in greater detail at a con-
ceptual level. Screen shots and more step by step 
discussion of how to use the GUI are given with 
the case study that outlines the demo script. 
2.1 Filters 
To train the system and create a model, the user 
has to define a filter. Defining a filter has 4 steps ? 
creating annotated files with user-defined annota-
tions, choosing feature sets to train (unigrams, bi-
grams etc), choosing evaluation metrics (Word 
Token Counter, TF-IDF) and choosing a classifier 
to train the system. 
   Annotating Files: The GUI allows the user to 
create a set of unstructured documents. The user 
can create folders and import sets of documents or 
individual documents. The GUI allows the user to 
view the documents in their original form; alterna-
tively, the user can add it to the filter and segment 
it by sentence, paragraph, or by own definition. 
The user can define a set of annotations for each 
filter, and use those to annotate segments of the file. 
The system has sentence and paragraph segmenters 
built into it. The user can also define a segmenter 
and plug it in. 
   Feature Sets: The feature set panel allows the 
user to decide which features the user wants to use 
in training the model. It is built on top of TagHel-
per Tools (Donmez et al, 2005) and uses it to ex-
tract the features chosen by the user. The system 
has options for using unigrams, bigrams, Part-Of-
Speech bigrams and punctuation built into it, and 
the user can specify whether they wish to apply 
stemming and/or stop word removal. Like the 
segmenters, if the user wants to use a specific fea-
ture to train, the user can plug in the feature extrac-
tor for the same through the GUI. 
   Evaluation Metrics: The evaluation metric de-
cides how to order the sentences that are chosen to 
be part of the summary. In keeping with the plug-
in architecture of the system, the user can define 
own metric and plug it into the system using the 
Plugin Manager. 
   Classifier: The user can decide which classifier 
to train the model with. This functionality is built 
on top of TagHelper Tools, which uses the Weka 
toolkit (Witten & Frank, 2005) to give users a set 
of classifiers to choose from. Once the system has 
been trained, the user can see the training results in 
a panel which provides a performance summary - 
including the kappa scores computed through 10-
fold cross validation and the confusion matrix, the 
sets of features extracted from the text, and the 
settings that were used for training the model. 
    The user can choose the model for classifying 
segments in the target document. The user also can 
plug-in a machine learning algorithm to the system 
if necessary. 
2.2 Summaries 
Summaries are defined by Recipes that specify 
what types of segments should be included in the 
resulting summary, and how a subset of the ones 
that meet those requirements should be selected 
and then arranged. Earlier we discussed how filters 
are defined.  One or more filters can be applied to a 
text so that each segment has one or more labels.  
These labels can then be used to index into a text. 
For example, a Recipe might specify using a logi-
cal expression such that only a subset of segments 
whose labels meet some specified set of constraints 
should be selected. The selected subset is then op-
tionally ranked using a specified Evaluation metric. 
Finally, from this ranked list, some number or 
some percentage of segments will then finally be 
selected to be included in the resulting summary.  
The segments are then optionally re-ordered to the 
original document order before including them in 
the summary, which is then displayed to the user. 
25
3 Case Study  
The following subsections describe an example 
where the user starts with some unstructured doc-
uments and uses the system to generate a specifica-
tion for a summary, which can then be applied to 
other similar documents. 
    We illustrate a script outline of our demo pres-
entation. The demo shows how simple it is to move 
through the steps of configuring SIDE for a type of 
summary that a user would like to be able to gen-
erate.  In order to demonstrate this, we will lead the 
user through an annotation task where we assign 
dialogue acts to turns in some tutoring dialogues.  
From this annotated data, we can generate summa-
ries that pull out key actions of particular types.  
For example, perhaps we would like to look at all 
the instructions that the tutor has given to a student 
or all the questions the student has asked the tutor.  
The summarizing process consists of annotating 
training documents to define filters, deciding 
which features to use along with what machine 
learning algorithm to train the filters, training the 
actual filters, defining a summary in terms of the 
structured annotation that is accomplished by the 
defined filters, and finally, summarizing target files 
using the resulting configuration. The purpose of 
SIDE is to provide both an easy GUI interface for 
people who are not familiar with programming, 
and extensible, plug-and-play code for those who 
want to program and change SIDE into a more so-
phisticated and specialized type of summarizer. 
The demo will provide options for both novice us-
ers primarily interested in working with SIDE 
through its GUI interface and for more experienced 
users who would like to work with the code.   
3.1 Using the GUI 
The summarization process begins with loading 
unstructured training and testing documents. Next, 
filters are defined by adding training documents, 
segmenting each by choosing an automatic seg-
menter, and assigning annotations to the segments. 
   After a document is segmented, the segments are 
annotated with labels that classify segments using 
a user-defined coding scheme (Figure 1). Unanno-
tated segments are later ignored during the training 
phase. Next, a set of feature types, such as uni-
grams, bigrams, part of speech bigrams, etc., are 
selected, which together will be used to build the 
feature space that will be input to a selected ma-
chine learning algorithms, or ensemble of algo-
rithms. In this example, ?Punctuation? Feature 
Class Extractor, which can distinguish interroga-
tive sentence, is selected and for ?Evaluation Met-
rics?, ?Word Token Counter? is selected. Now, we 
train this model with an appropriate machine learn-
ing algorithm. In this example, J48 which is
 
 
Figure 1: The interface where segments are annotated. 
26
Boolean
Expression
Tree
Ranker
Limiter
 
Figure 2: The interface for defining how to build a summary from the annotated data. 
 
one of Weka?s (Witten & Frank, 2005) decision 
tree learners is chosen as the learning algorithm. 
Users can explore different ensembles of machine 
learning algorithms, compare performance over the 
training data using cross-validation, and select the 
best performing one to use for summarization. 
    Once one or more filters have been defined, we 
must define how summaries are built from the 
structured representation that is built by the filters.  
Figure 2 shows the main interface for doing this.  
Recipes consist of four parts, namely ?Selecting?, 
?Ranking?, ?Limiting?, ?Sequencing?. Selection is 
done using a boolean expression tree consisting of 
?and?, ?or?, and ?is? nodes. By doing selection, only 
those segments with proper annotations will be 
selected for inclusion in the resulting summary. 
Ranking is done by the Evaluation Metric selected 
when defining the Recipe. The size of a summary 
can be limited by limiting the number of segments 
you want in your summary. Finally, the summary 
can be reordered as you wish and displayed. 
4 Current Directions 
Currently, most of the functionality in SIDE fo-
cuses on the content selection problem.  We ac-
knowledge that to move beyond extractive forms 
of summarization, additional functionality at the 
summary generation stage is necessary.  Our cur-
rent work focuses on addressing these issues. 
References 
Bellotti, V., Ducheneaut, N., Howard, M., Smith, I., & 
Grinter, R. (2005). Quality versus Quantity: E-Mail 
Centric Task Management and Its Relation with 
Overload, Human-Computer Interaction, Volume 20,  
Donmez, P., Ros?, C. P., Stegmann, K., Weinberger, A., 
and Fischer, F. (2005). Supporting CSCL with Auto-
matic Corpus Analysis Technology , Proceedings of 
Computer Supported Collaborative Learning. 
Mieskes, M., M?ller, C., & Strube, M. (2007) Improv-
ing extractive dialogue summarization by utilizing 
human feedback, Proceedings of the 25th IASTED 
International Multi-Conference: artificial intelligence 
and applications, p.627-632 
Paice, Chris D. & Jones, Paul A. (1993) The identifica-
tion of important concepts in highly structured tech-
nical papers. In Proceedings of the 16th ACM-SIGIR 
Conference, pages 69?78 
Teufel, S. & Moens, M. (2002). Summarizing Scientific 
Articles: Experiments with Relevance and Rhetorical 
Status, Computational Linguistics, Vol 28, No. 1. 
Witten, Ian H.; Frank, Eibe (2005). Data Mining: Prac-
tical machine learning tools and techniques, 2nd Edi-
tion. Morgan Kaufmann, San Francisco.  
27
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 101?104,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Leveraging Structural Relations for Fluent Compressions                    
at Multiple Compression Rates 
 
Sourish Chaudhuri, Naman K. Gupta, Noah A. Smith, Carolyn P. Ros? 
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA-15213, USA. 
{sourishc, nkgupta, nasmith, cprose}@cs.cmu.edu 
 
Abstract 
Prior approaches to sentence compression 
have taken low level syntactic constraints into 
account in order to maintain grammaticality. 
We propose and successfully evaluate a more 
comprehensive, generalizable feature set that 
takes syntactic and structural relationships into 
account in order to sustain variable compres-
sion rates while making compressed sentences 
more coherent, grammatical and readable.  
1 Introduction 
We present an evaluation of the effect of syntac-
tic and structural constraints at multiple levels of 
granularity on the robustness of sentence com-
pression at varying compression rates.  Our eval-
uation demonstrates that the new feature set pro-
duces significantly improved compressions 
across a range of compression rates compared to 
existing state-of-the-art approaches. Thus, we 
name our system for generating compressions the 
Adjustable Rate Compressor (ARC).   
Knight and Marcu (2000) (K&M, henceforth) 
presented two approaches to the sentence com-
pression problem: one using a noisy channel 
model, the other using a decision-based model. 
The performances of the two models were com-
parable though their experiments suggested that 
the noisy channel model degraded more smooth-
ly than the decision-based model when tested on 
out-of-domain data. Riezler et al (2003) applied 
linguistically rich LFG grammars to a sentence 
compression system. Turner and Charniak (2005) 
achieved similar performance to K&M using an 
unsupervised approach that induced rules from 
the Penn Treebank.  
A variety of feature encodings have previous-
ly been explored for the problem of sentence 
compression.  Clarke and Lapata (2007) included 
discourse level features in their framework to 
leverage context for enhancing coherence. 
McDonald?s (2006) model (M06, henceforth) is 
similar to K&M except that it uses discriminative 
online learning to train feature weights. A key 
aspect of the M06 approach is a decoding algo-
rithm that searches the entire space of compres-
sions using dynamic programming to choose the 
best compression (details in Section 2). We use 
M06 as a foundation for this work because its 
soft constraint approach allows for natural inte-
gration of additional classes of features. Similar 
to most previous approaches, our approach com-
presses sentences by deleting words only. 
The remainder of the paper is organized as 
follows. Section 2 discusses the architectural 
framework.  Section 3 describes the innovations 
in the proposed model. We conclude after pre-
senting the results of our evaluation in Section 4. 
2 Experimental Paradigm 
Supervised approaches to sentence compression 
typically use parallel corpora consisting of origi-
nal and compressed sentences (paired corpus, 
henceforth). In this paper, we will refer to these 
pairs as a 2-tuple <x, y>, where x is the original 
sentence and y is the compressed sentence. 
We implemented the M06 system as an expe-
rimental framework in which to conduct our in-
vestigation. The system uses as input the paired 
corpus, the corresponding POS tagged corpus, 
the paired corpus parsed using the Charniak 
parser (Charniak, 2000), and dependency parses 
from the MST parser (McDonald et al, 2005). 
Features are extracted over adjacent pairs of 
words in the compressed sentence and weights 
are learnt at training time using the MIRA algo-
rithm (Crammer and Singer, 2003). We decode 
as follows to find the best compression:  
Let the score of a compression y for a sen-
tence x be s(x, y). This score is factored using a 
first-order Markov assumption over the words in 
the compressed sentence, and is defined by the 
dot product between a high dimensional feature 
representation and a corresponding weight vector 
(for details, refer to McDonald, 2006). The equa-
tions for decoding are as follows: 
 
1),,,(][max][
0.0]1[
iijxsjCiC
C
ij
 
101
where C is the dynamic programming table and 
C[i] represents the highest score for compres-
sions ending at word i for the sentence x. 
The M06 system takes the best scoring com-
pression from the set of all possible compres-
sions.  In the ARC system, the model determines 
the compression rate and enforces a target com-
pression length by altering the dynamic pro-
gramming algorithm as suggested by M06: 
 
1,]][1[
0.0]1][1[
rrC
C  
,1i  
),,(]1][[max]][[ ijxsrjCriC ij
 
 
where C is the dynamic programming table as 
before and C[i][r] is the score for the best com-
pression of length r that ends at position i in the 
sentence x. This algorithm runs in O (n2r) time.  
We define the rate of human generated com-
pressions in the training corpus as the gold stan-
dard compression rate (GSCR). We train a linear 
regression model over the training data to predict 
the GSCR for a sentence based on the ratio be-
tween the lengths of each compressed-original 
sentence pair in the training set. The predicted 
compression rate is used to force the system to 
compress sentences in the test set to a specific 
target length. Based on the computed regression, 
the formula for computing the Predicted Com-
pression Rate (PCR) from the Original Sentence 
Length (OSL) is as follows: 
 
OSLPCR 004.086.0  
 
In our work, enforcing specific compression 
rates serves two purposes. First, it allows us to 
make a more controlled comparison across ap-
proaches, since variation in compression rate 
across approaches confounds comparison of oth-
er aspects of performance.  Second, it allows us 
to investigate how alternative models work at 
higher compression rates. Here our primary con-
tribution is of robustness of the approach with 
respect to alternative feature spaces and com-
pression rates. 
3 Extended Feature Set 
A major focus of our work is the inclusion of 
new types of features derived from syntactic ana-
lyses in order to make the resulting compressions 
more grammatical and thus increase the versatili-
ty of the resulting compression models.   
The M06 system uses features extracted from 
the POS tagged paired corpus: POS bigrams, 
POS context of the words added to or dropped 
from the compression, and other information 
about the dropped words. For a more detailed 
description, please refer to McDonald, 2006.   
From the phrase structure trees, M06 extracts 
context information about nodes that subsume 
dropped words. These features attempt to ap-
proximately encode changes in the grammar 
rules between source and target sentences. De-
pendency features include information about the 
dropped words? parents as well as conjunction 
features of the word and the parent. 
Our extensions to the M06 feature set are in-
spired by an analysis of the compressions gener-
ated by it, and allow for a richer encoding of 
dropped words and phrases using properties of 
the words and their syntactic relations to the rest 
of the sentence. Consider this example (dropped 
words are marked as such):  
 
* 68000 Sweden AB of Uppsala , Sweden , intro-
duced the TeleServe , an integrated answering 
machine and voice-message handler that links a 
Macintosh to Touch-Tone phones . 
  
Note in the above example that the syntactic 
head of the sentence introduced has been 
dropped. Using the dependency parse, we add a 
class of features to be learned during training that 
lets the system decide when to drop the syntactic 
head of the sentence. Also note that answering 
machine in the original sentence was preceded 
by an while the word the was used with Tele-
serve (dropped in the compression). While POS 
information helps the system to learn that the 
answering machine is a good POS sequence, we 
do not have information that links the correct 
article to the noun. Information from the depen-
dency parse allows us to learn when we can drop 
words whose heads are retained and when we 
can drop a head and still retain the dependent.  
Now, consider the following example: 
 
Examples for editors are applicable to awk pat-
terns , grep and egrep .  
 
    Here, Examples has been dropped, while for 
editors which has Examples as a head is retained. 
Besides, in the sequence, editors are applica-
ble?, the word editors behaves as the subject of 
are although the correct compression would have 
examples as its subject. A change in the argu-
ments of the verbs will distort the meaning of the 
sentence. We augmented the feature set to in-
clude a class of features about structural informa-
tion that tells us when the subject (or object) of a 
verb can be dropped while the verb itself is re-
tained. Thus, now if the system does retain the 
102
are, it is more likely to retain the correct argu-
ments of the word from the original sentence. 
    The new classes of features use only the de-
pendency labels generated by the parser and are 
not lexicalized. Intuitively, these features help 
create units within the sentences that are tightly 
bound together, e.g., a subject and an object with 
its parent verb. We notice, as one would expect, 
that some dependency bindings are less strong 
than others. For instance, when faced with a 
choice, our system drops a relative pronoun thus 
breaking the dependency between the retained 
noun and the relative pronoun, rather than drop 
the noun, which was the retained subject. 
Below is a summary of the information that 
the new features in our system encode: 
[Parent-Child]- When a word is dropped, is its 
parent retained in the compression?  
[Dependent]- When a word is dropped, are 
other words dependent on it (its children) 
also dropped or are they retained?  
[Verb-Arg]- Information from the dependency 
parse about the subjects and objects of 
verbs can be used to encode more specific 
features (similar to the above) that say 
whether or not the subject (or object) was 
retained when the verb was dropped.  
[Sent-Head-Dep]- Is the syntactic head of a 
sentence dropped? 
4 Evaluation 
We evaluate our model in comparison with M06. 
At training time, compression rates were not en-
forced on the ARC or M06 model. Our evalua-
tion demonstrates that the proposed feature set 
produces more grammatical sentences across 
varying compression rates.  In this section, 
GSCR denotes gold standard compression rate 
(i.e., the compression rate found in training data), 
CR denotes compression rate.   
4.1 Corpora 
Sentence compression systems have been tested 
on product review data from the Ziff-Davis (ZD, 
henceforth) Corpus by Knight and Marcu (2000), 
general news articles by Clarke and Lapata (CL, 
henceforth) corpus (2007) and biomedical ar-
ticles (Lin and Wilbur, 2007). To evaluate our 
system, we used 2 test sets: Set 1 contained 50 
sentences; all 32 sentences from the ZD test set 
and 18 additional sentences chosen randomly 
from the CL test set; Set 2 contained 40 sen-
tences selected from the CL corpus, 20 of which 
were compressed at 75% of GSCR and 20 at 
50% of GSCR (the percentages denote the en-
forced compression rates). 
Three examples comparing compressed sen-
tences are given below:  
 
 
Original: Like FaceLift, much of ATM 's screen 
performance depends on the underlying applica-
tion. 
Human: Much of ATM 's performance depends 
on the underlying application . 
M06: 's screen performance depends on applica-
tion  
ARC: ATM 's screen performance depends on 
the underlying application . 
 
Original: The discounted package for the Sparc-
server 470 is priced at $89,900 , down from the 
regular $107,795 . 
Human: The Sparcserver 470 is priced at 
$89,900 , down from the regular $107,795 . 
M06: Sparcserver 470 is $89,900 regular 
$107,795 
ARC: The discounted package is priced at 
$89,900 , regular $107,795 .  
 
 
The example below has compressions at 50% 
compression rate for M06 and ARC systems: 
 
 
Original: Cutbacks in local defence establish-
ments is also a factor in some constituencies . 
M06: establishments is a factor in some consti-
tuencies . 
ARC: Cutbacks is a factor in some constituen-
cies .  
 
 
Note that the subject of is is correctly retained 
in the ARC system. 
4.2 User Study 
In order to evaluate the effect of the features that 
we added to create the ARC model, we con-
ducted a user study, adopting an experimental 
methodology similar to that used by K&M and 
M06.  Each of four human judges, who were na-
tive speakers of English and not involved in the 
research we report in this paper, were instructed 
to rate two different sets of compressions along 
two dimensions, namely Grammaticality and 
Completeness, on a scale of 1 to 5. We chose to 
replace Importance (used by K&M), which is a 
task specific and possibly user specific notion, 
with the more general notion of Completeness, 
defined as the extent to which the compressed 
sentence is a complete sentence and communi-
cates the main idea of the original sentence.  
For Set 1, raters were given the original sen-
tence and 4 compressed versions (presented in 
103
random order as in the M06 evaluation): the hu-
man compression, the compression produced by 
the original M06 system, the compression from 
the M06 system with GSCR, and the ARC sys-
tem with GSCR. For Set 2, raters were given the 
original sentence, this time with two compressed 
versions, one from the M06 system and one from 
the ARC system, which were presented in a ran-
dom order.  Table 1 presents all the results in 
terms of human ratings of Grammaticality and 
Completeness as well as automatically computed 
ROUGE F1 scores (Lin and Hovy, 2003). The 
scores in parentheses denote standard deviations. 
 
 Grammati-
cality 
(Human 
Scores) 
Com-
pleteness 
(Human 
Scores) 
 
ROUGE 
F1 
Gold 
Standard 
4.60 (0.69) 3.80(.99) 1.00 (0) 
ARC 
(GSCR) 
3.70 (1.10) 3.50(1.10) .72 (.18) 
M06 3.50 (1.30) 3.10(1.30) .70 (.20) 
M06 
(GSCR) 
3.10 (1.10) 3.10(1.10) .71 (.18) 
ARC 
(75%CR) 
2.60 (1.10) 2.60(1.10) .72 (.14) 
M06 
(75%CR) 
2.20 (1.20) 2.00(1.00) .67 (.20) 
ARC 
(50%CR) 
2.30 (1.30) 1.90(1.00) .54 (.22) 
M06 
(50%CR) 
1.90 (1.10) 1.80(1.00) .58 (.22) 
Table 1: Results of human judgments and ROUGE F1 
 
 ROUGE scores were determined to have a 
significant positive correlation both with Gram-
maticality (R = .46, p < .0001) and Completeness 
(R = .39, p < .0001) when averaging across the 4 
judges? ratings.  On Set 1, a 2-tailed paired t-test 
reveals similar patterns for Grammaticality and 
Completeness: the human compressions are sig-
nificantly better than any of the systems.  ARC is 
significantly better than M06, both with enforced 
GSCR and without. M06 without GSCR is sig-
nificantly better than M06 with GSCR.  In Set 2 
(with 75% and 50% GSCR enforced), the quality 
of compressions degrade as compression rate is 
made more severe; however, the ARC model 
consistently outperforms the M06 model with a 
statistically significant margin across compres-
sion rates on both evaluation criteria. 
5 Conclusions and Future Work 
In this paper, we designed a set of new classes of 
features to generate better compressions, and 
they were found to produce statistically signifi-
cant improvements over the state-of-the-art. 
However, although the user study demonstrates 
the expected positive impact of grammatical fea-
tures, an error analysis (Gupta et al, 2009) re-
veals some limitations to improvements that can 
be obtained using grammatical features that refer 
only to the source sentence structure, since the 
syntax of the source sentence is frequently not 
preserved in the gold standard compression. In 
our future work, we hope to explore alternative 
approaches that allow reordering or paraphrasing 
along with deleting words to make compressed 
sentences more grammatical and coherent. 
 
Acknowledgments 
The authors thank Kevin Knight and Daniel 
Marcu for sharing the Ziff-Davis corpus as well 
as the output of their systems, and the anonym-
ous reviewers for their comments. This work was 
supported by the Cognitive and Neural Sciences 
Division, grant number N00014-00-1-0600. 
References  
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proc. of  NAACL. 
James Clarke and Mirella Lapata, 2007. Modelling 
Compression With Discourse Constraints. In Proc. 
of EMNLP-CoNLL. 
Koby Crammer and Y. Singer. 2003. Ultraconserva-
tive online algorithms for multi-class problems. 
JMLR. 
Naman K. Gupta, Sourish Chaudhuri and Carolyn P. 
Ros?, 2009. Evaluating the Syntactic Transforma-
tions in Gold Standard Corpora for Statistical Sen-
tence Compression . In Proc. of HLT-NAACL. 
Kevin Knight and Daniel Marcu. 2000. Statistics-
Based Summarization ? Step One: Sentence Com-
pression. In Proc. of AAAI. 
Jimmy Lin and W. John Wilbur. 2007. Syntactic sen-
tence compression in the biomedical domain: faci-
litating access to related articles. Information Re-
trieval, 10(4):393-414. 
Chin-Yew Lin and Eduard H. Hovy 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proc. of HLT-NAACL. 
Ryan McDonald, 2006. Discriminative sentence com-
pression with soft syntactic constraints. In Proc. of 
EACL.  
Ryan McDonald, Koby Crammer, and Fernando Pe-
reira. 2005. Online large-margin training of depen-
dency parsers. In Proc.of ACL. 
S. Riezler, T. H. King, R. Crouch, and A. Zaenen.  
2003. Statistical sentence condensation using am-
biguity packing and stochastic disambiguation me-
thods for lexical-functional grammar. In Proc. of 
HLT-NAACL. 
104
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 30?38,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
A Comparison of Latent Variable Models For Conversation Analysis
Sourish Chaudhuri and Bhiksha Raj
Language Technologies Institute,
School of Computer Science,
Carnegie Mellon University,
Pittsburgh, PA - 15213.
{sourishc, bhiksha} @ cs.cmu.edu
Abstract
With the evolution of online communication
methods, conversations are increasingly han-
dled via email, internet forums and other such
methods. In this paper, we attempt to model
lexical information in a context sensitive man-
ner, encoding our belief that the use of lan-
guage depends on the participants in the con-
versation. We model the discourse as a com-
bination of the speaker, the addressee and
other participants in the conversation as well
as a context specific language model. In or-
der to do this, we introduce a novel method
based on an HMM with an exponential state
space to capture speaker-addressee context.
We also study the performance of topic model-
ing frameworks in conversational settings. We
evaluate the models on the tasks of identify-
ing the set of people present in any conver-
sation, as well as identifying the speaker for
every utterance in the conversation, and they
show significant improvement over the base-
line models.
1 Introduction
In this paper, we experiment with different methods
of automatically analyzing discourse. We present
and validate hypotheses on how conversations can
be better analyzed using information about the
speakers, as well as other participants in the con-
versation. We present a novel method of modeling
discourse using an exponential state Hidden Markov
Model where states are based on speakers and ad-
dressees. We also cast the problem into the popular
topic modeling frameworks, and compare the vari-
ous approaches.
Consider a small group of people that a person
knows well. Given a transcript of a discussion on
a topic of mutual interest, that person would likely
be able to identify who is likely to have said what,
based on his knowledge of the speakers and their in-
clinations on various topics. We would like to be
able to encode similar intelligence into a system that
could automatically learn about speakers based on
transcripts of prior conversations, and use that infor-
mation to analyze new conversations.
The scenario we consider in this work is as fol-
lows: we have a known set of characters, any subset
of whom could be present in a conversation. Given
the transcript of a conversation only, without speaker
annotations, we would like to : 1. Predict the set of
participants in the conversation from the character-
istics of the entire conversation, and 2. Identify the
individual speakers at each conversation turn.
In order to do this, we model each utterance in
a conversation as dependent on the speaker, the ad-
dressee and the other people present. As we shall
describe, our models encode the belief that people
speak/behave differently depending on other partic-
ipants in the conversation. This has a two-fold ben-
efit: first, it can help us discover social (or even,
professional) relationship structures; second, it can
help us understand how to respond to different peo-
ple, and incorporate that information into automated
conversational agents which can then behave in a
more context sensitive manner. The ability to auto-
matically model discourse as context specific in this
manner is also useful for other tasks such as directed
advertising and duplicity detection.
In Section 2, we describe relevant related work.
30
Section 3 describes the dataset for our experiments,
Section 4 describes the problem, our use of topic
models, and the novel HMM based method, while
Section 5 summarizes the results and we conclude
in Section 6.
2 Related Work
The task of automatically segmenting speech and
then identifying speakers from audio (Reynolds
and Torres-Carrasquillo, 2005) is referred to as di-
arization and has been well-studied (Tranter and
Reynolds, 2006). More recently, approaches have
been developed to fuse information from both the
audio and video modalities (Noulas et al, 2011)
to improve diarization systems when video informa-
tion is available. In this paper, we attempt to under-
stand just how much information is available in the
text alone. Systems that can work with text only can
be used to improve audio-based systems which can
provide speech recognition output to a text-based
system. They can also be used to work with closed
caption streams, or on human-generated transcrip-
tions of meeting recordings.
Research on identifying speakers from text or lex-
ical information is limited in comparison to work
with audio data. However, efforts have been made
to use discourse level information to automatically
identify speakers to calibrate idiolectal differences
between speakers (Doddington, 2001). (Canseco et
al., 2005, ) investigated the use of lexical features
to automatically diarize (but not actually identify)
transcripts to determine if a current speaker contin-
ued or a previous speaker spoke or the next speaker
spoke. Lei and Mirghafori (2 007) attempted to in-
corporate idiolect based speaker information by us-
ing word conditioning of phone N -grams to recog-
nize speakers in dialogs with 2 speakers.
In our work, the models we use to identify speak-
ers are powerful enough to predict the addressee as
well. In this context, we note that several attempts
have been made recently to automatically identify
addressees in dialog settings. These approaches
have used information about the context and con-
tent of the utterance, using dialog acts and informa-
tion about the speaker?s gaze to aid classifier per-
formance (Jovanovic et al, 2006). Den Akker and
Traum (2009) proposed rule-based methods for ad-
dressee classification. Unlike in these works, we
attempt to jointly model both the speaker and the
addressee as one of our proposed approaches. This
is similar to the approach employed by (Otsuka et
al., 2005, ), who proposed a Dynamic Bayesian Net-
work model to understand multiparty conversation
structure using non-verbal cues only? eye gaze, fa-
cial expression, gesticulations and posture.
3 Data
The data for our experiments consists of fan-
sourced transcripts of the episodes of the sitcom
F.R.I.E.N.D.S. The structure of the data is as fol-
lows: we have a set of conversations as training data.
Each conversation contains a sequence of turns, with
each turn annotated with its speaker. We do not
have any information about the addressee from the
dataset. We do, however, have implicit informa-
tion of the set of speakers within a conversation seg-
ment (we make the assumption here that if a char-
acter doesn?t speak in a segment, he is not present).
Annotator notes appear periodically to indicate that
the scene changed or that new characters entered the
scene or that some characters left the scene. We
treat these annotator notes as conversation bound-
aries and the segment of turns between two such
boundaries constitutes one conversation instance.
The set of characters used for our experiments is
finite. The 6 primary characters in the sitcom (Chan-
dler, Joey, Monica, Phoebe, Rachel and Ross) are
retained. In addition to these 6 primary characters,
there are a number of supporting characters who ap-
pear occasionally. We use Other to denote all other
characters, as the amount of data for a number of the
supporting characters is quite small and would not
result in learning useful patterns regarding their be-
havior. As a result, we treat all of these characters
as one character that can be thought of as a univer-
sal supporting character. Hence, we have a total of
7 possible characters. Any subset of these 7 char-
acters could be part of a conversation. Below is an
example of a pair of conversations from our dataset:
[EVENT]
Paul: thank you! thank you so much!
Monica: stop!
Paul: no, i?m telling you last night was like umm,
all my birthdays, both graduations, plus the barn
31
raising scene in witness.
Monica: we?ll talk later.
Paul: yeah. thank you.
[EVENT]
Joey: that wasn?t a real date?! what the hell do
you do on a real date?
Monica: shut up, and put my table back.
All: okayyy!
[EVENT]
The event markers are tags inserted at pre-
processing time, to denote transcriber annotations
such as characters entering or leaving scenes. The
sequence of turns between two event markers are
treated as one conversation. Also, note the character
Paul in the first conversation in the example above
? when training the system, the content of Paul?s ut-
terances are used to train the model for Other, since
Paul is not one of the primary characters that we
track. At test time, the input looks similar to the
above, except that the turns are not annotated by
speaker.
The transcripts used in our experiments are seg-
mented by speaker turns, so that consecutive turns
are uttered by different speakers. The entire set of
230 episodes was split randomly into training, de-
velopment and test splits. Sequential information
for the individual conversations were not used. Each
episode was further divided into conversations based
on the scene boundaries denoted by the transcribers.
For training, overall, we used 195 episodes from
F.R.I.E.N.D.S, with a total of 9,171 conversations
and a total 52,516 turns. The average length in num-
ber of turns for each conversation was 5.73. The
test set consisted of a total of 20 episodes with 855
conversations and 4,981 turns. The average length
of a conversation in the test set was 5.83. The re-
maining 15 episodes were used as development data
to tune hyperparameters ? this set consisted of 529
conversations and 2,984 turns in total. The distribu-
tion of the number of utterances by speakers across
the training, test and development set are shown in
Figure 1. As one can observe, the distribution is not
particularly skewed for any of the speakers across
the splits of the dataset.
Figure 1: Distribution of #utterances for each speaker in
the dataset.
4 Conversation Models
Previous work in analyzing participants in a conver-
sation have used meeting data, with a fixed number
of participants. In our task, the total number of pos-
sible participants is finite, but we do not have in-
formation on how many of them are present at any
particular instant. Thus, our model first attempts to
detect the participants in a segment of conversation,
and then attempts to attribute speaker turns to indi-
viduals.
Our model for discourse structure is based on two
premises. First, we believe that what a person says
will depend on who he or she is speaking to. Intu-
itively, consider a person trying to make the same
point to his boss and (at a different time and place)
to his friend. It is likely that he will be more formal
with his boss than his friend. Second, if the speaker
addresses someone specifically in a group of people,
knowing who he addressed would likely help us pre-
dict better who would speak next. We assume that
the first hypothesis above also holds for groups of
people in conversations, where the topics and their
distribution in discussions (and words that affect the
tone of the discussion) depend on the participants.
As described earlier, we evaluate our models on
two tasks. First, we would like to identify the set of
characters present in any conversation. Given seg-
ments of conversation, we attempt to understand the
distribution of topics for specific subsets of charac-
ters present in that segment. To do this, we cast
this problem into a topic modeling framework ? we
experiment with the Author-Topic model (Rosen-
32
Zvi et al, 2004), described in Section 4.1, for this
task. We use the Author Topic Model to link the co-
occurrence information of characters with the words
in the conversation.
Second, we attempt to attribute speakers to ut-
terances, described in Section 4.2. We introduce a
novel approach using an HMM with an exponen-
tial state space to model speakers and addressees,
described in Section 4.2.1. We also use the Author
Topic Model and the Author-Recipient Topic Model
(McCallum et al, 2007), described in Section 4.2.2
for this task. The key difference between the HMM-
based model and the topic model based approaches
is that the former explicitly takes sequence informa-
tion into account.
4.1 Identifying Character Subset Presence
The premise behind attempting to model subsets of
characters is that the nature of the conversation de-
pends on the group of people participating. For in-
stance, it seems intuitively likely that the content of
a conversation between two friends would be differ-
ent if they were the only ones present than it would
be if their families were also present. To extend this
hypothesis to a general scenario, the content of each
speaker?s turn depends not only on the speaker, but
also on the person being spoken to as well as the
other people present. To model this, we require a
model that captures the distribution of the text for
entire conversation, for each possible subset of char-
acters. In this section, we describe the training of a
generic model for conversations, and use it to pro-
duce features for a discriminative classifier.
Let there be N characters who could participate
in a conversation. We assume a general scenario,
where any subset of these characters may be present.
Thus, there are 2N?1 character subsets that are pos-
sible. We can model this as a multi-class classifica-
tion problem (we will refer to this as subset model-
ing, henceforth).
The generative model for this task is as follows:
Each conversation segment is associated with a set
of utterances, and a set of characters. For each such
set of characters, we associate a distribution over
topics. For each word that is present in the seg-
ment, we select a topic from the subset-specific topic
distribution, and then we select the word from that
topic. Figure 2 shows the graphical model for this in
plate notation.
Figure 2: Graphical representation of the subset model in
plate notation
In the plate notation, the observed variables are
shaded and the latent variables are unshaded. Plates
encapsulate a set of variables which are repeatedly
sampled a fixed number of times, and the number at
the bottom right indicates this fixed number.
Sc represents a subset of the characters who were
present in the conversation segment. We have C
such conversations, and each conversation contains
Nc words. z represents the latent topic variable, and
? represents the multinomial topic distribution for
each subset of characters (there are 2N such sub-
sets). The multinomial distribution of topics has a
prior distribution characterized by ?. Similarly, ev-
ery topic (there are a set of T topics) has a multino-
mial distribution ? over the words in the vocabulary,
and ? has a prior distribution characterized by ?.
For every conversation in the training corpus, the
set of characters present is known. The content of
the conversation is treated as a bag of words. From
the topic distribution for the subset of characters
present, we sample a topic. Based on the word dis-
tributions for this topic, we sample a word. This
process is repeated Nc times corresponding to the
number of words in the conversation. The entire
process of generating a conversation is repeated C
times, corresponding to the number of conversations
in the training corpus.
Depending on the value of N , the number of pos-
33
sible classes may be very high. Training a large
number of models may lead to a data scarcity, es-
pecially given the high dimensionality of language
data. We therefore slightly modify the model, so that
instead of topic distributions for each possible sub-
set, we have a topic distribution for each character,
and the distribution of topics in the conversation is
a mixture of the topic distributions for each charac-
ter. This leads us to a graphical model that has been
well-studied in the past ? the Author-Topic model
(ATM, henceforth) and is shown in Figure 3.
Figure 3: Graphical representation of the simplified sub-
set model in plate notation
Thus, given the set of characters present, we sam-
ple one of them (x) from a uniform distribution.
Then we generate a topic by sampling from the dis-
tribution of topics for that speaker. The rest of the
process remains the same.We use this model to help
us predict which subset of characters was present in
a given conversation.
We learn speaker-specific topic distributions us-
ing the ATM. In order to predict characters present
in a test conversation, we train binary SVM (Shawe-
Taylor and Cristianini, 2000) classifiers for each
speaker in the following manner: we compute the
distribution of the speaker-specific topics in each
conversation, and use these as the features of the
data point. If the speaker was present in the con-
versation, the data point corresponding to the con-
versation has a class label of +1, else -1. A linear
SVM classifier is trained over the data. At test time,
we compute the distribution of the speaker?s topics
in the conversation, and use the SVM to predict if
the speaker was present or not.
4.2 Identifying Speakers From Utterances
In this section, we describe our approach to identi-
fying speakers from the text of the utterance. The
ATM (as described above) treats all the participants
in the conversation as being potential contributors to
each turn. However, we can also use the ATM to
predict speakers directly. In this case, we will use
each turn as analogous to a document. Each such
document has only one author and the author topic
model can be used to learn models for each author.
The plate notation for this would look very similar
to the one in Figure 2, except that instead of a sub-
set of characters being observed, only one would be
observed, and the number of possible topic distribu-
tions would be equal to the number of characters.
The ATM for this task does not take any context
information into account. In the following subsec-
tion, we introduce a novel HMM based approach
that seeks to leverage information from the sequence
of turns.
4.2.1 Exponential State Hidden Markov Model
In this model, we assign a state to each speaker-
addressee combination possible. If our data consists
ofN characters, only one of theN characters will be
speaking at any given point. He/She may be speak-
ing to any combination of the remainingN?1 char-
acters. Thus, the number of states in this model is
N ?2(N?1). Note that the addressee is not observed
directly from the data.
The sequence of turns in a conversation is mod-
eled by a Hidden Markov Model (Rabiner, 1989).
At each time instant, the speaker corresponding to
the state speaks a turn, which is the observed emis-
sion, before transitioning to another state at the next
time instant. The state at the next time instant is con-
strained to have a different speaker.
The model is trained using the standard Baum-
Welch training. The emission probabilities are cap-
tured by a trigram language model, trained using
the SRILM toolkit (Stolcke, 2002). The parameters
of the model are initialized as follows: for emis-
sion probabilities, we take all the utterances by a
speaker and distributing them uniformly among the
34
states that have that speaker, since we do not have
direct information about the addressees. For tran-
sition probabilities, we initialize with a bias instead
of uniformly. Given a conversation, for a state with
speaker A and set of addressees (R, say ? Note
that R may have multiple characters), we give equal
probabilities of transitioning to all states that have
one of the characters in R as the speaker. Now, we
pick the set of speakers (call it M ) that uttered the
next three turns (essentially, we look ahead in the
data stream to see who the next 3 speakers are while
training). We add a bias to every state with A as
the speaker, and every possible combination of the
speakers in M , to encode the hypothesis that the ad-
dressee would be likely to speak pretty soon, if not
directly after.
The large state space in this model makes compu-
tation extremely expensive. However, an examina-
tion of the posterior probabilities show that a number
of states are rarely, or never, entered. We prune away
such states after every 5 iterations in the following
manner ? we use the current parameters of the model
after each iteration to identify the speakers of each
turn on the development set. Decoding of a sequence
of turns at test time is done using the Viterbi algo-
rithm. However, instead of using the best path only,
we keep track of the top 10 best paths. Thus, after
an iteration of training, we test on the development
data, and obtain 10 possible sequences of speakers
for each conversation. Over 5 iterations, we have
the 50 best paths for each conversation. We then
compute the average number of states entered in all
the decoded paths obtained. If the average number
of times a state was entered is ?, then any state that
was entered less than k ? ? times (k = 0.02, for
our experiments), according to the posterior proba-
bilities was pruned out. In order to set the value of
k, the development set was split into 2 halves, with
one half being used to compute the average number
of times a state is entered across the 10 best decodes
for data in that half. For different values of k, accu-
racy of speaker identification on the 1-best decode
was computed on the other half of the development
set, for values of k from 0.005 to 0.1.
The optimal state sequence at test time also con-
tains information about the addressee. For the tasks
we evaluate, this information is not directly used.
However, in other applications, such as those in-
volving automated agents, this information could be
valuable in triggering the agent.
4.2.2 Author-Recipient Topic Model
The Author Recipient Topic Model (McCallum et
al., 2007) (ARTM, henceforth) was used for discov-
ering topics and roles in social networks. It is built
over the Author-Topic Model discussed previously,
with the exception that messages are conditioned on
the sender as well as the receivers. The graphical
model in plate notation is shown in Figure 4.
Figure 4: Graphical representation of the Author-
Recipient Topic model in plate notation
Here, we model each turn as having a set of Nt
words. Each turn has one speaker S, and a set of
addressees At. The generative model works as fol-
lows: For each word in a turn, sample an addressee
a from the set of addressees. Topic distributions are
now conditioned over speaker-addressee pairs, in-
stead of only the speaker as we saw in the ATM.
A topic is now sampled from the speaker-addressee
specific topic distribution. A word is now sampled
from this topic using the topic specific word distri-
butions. The parameters ?, ?, and z have the same
meaning as in the ATM described earlier.
Note that the set of addressees in our setting is
not explicitly observed. We know the participants in
the conversation at training time, and we know the
speaker, but we do not know who was addressed.
Since we do not have information to make a better
choice of addressee, we model the entire set of par-
35
ticipants without the speaker as the set of addressees,
in this model.
For the task of identifying the speaker who uttered
the turn, we employ an approach, similar to the one
used for ATM. We train speaker-addressee-specific
models. The feature set for this task includes fea-
tures not only from the turn itself, but also from
the context. Thus, we have the distribution of the
topics in the turn for every speaker-addressee pair
with the right speaker, the speakers of the previ-
ous two turns, and the distribution of topics of the
speaker of the current turn over the previous two
turns. (Thus, while the model does not explicitly
model sequence, as an HMM does, it utilizes con-
text information in its feature space.) Using these
features, we train a linear SVM to predict whether
or not the speaker uttered the turn. In this case, we
could potentially have multiple speakers (or none of
them) predicted to have uttered the same turn. In
that case, we choose the speaker with the maximum
distance from the margin.
4.3 Baseline Models
In this section, we set up simple baseline models to
evaluate our performance against. We describe how
we set up a random baseline, a Naive Bayes baseline
and an HMM baseline model.
4.3.1 Random Baseline
For the task of identifying the set of charac-
ters present in a conversation, the random baseline
would work as follows: it knows that the number of
characters present in any conversation lies between
1 and N (N = 7, in this case). (Note that monologues,
with only 1 person being present, are possible. Typ-
ically, in our data, they happen at the beginning or
end of scenes.) Thus, it randomly decides if each
of these characters are present or not in any given
conversation.
Suppose that the total number of characters are n
and r of them are actually present in the conversa-
tion. Let us say the random guess system predicts
t of the characters to be present. If we use the uni-
form distribution for picking t, then P (t) = 17 , ?t ?
[1, 2, ..., 7], in this case. For any given t, the proba-
bility that we get k correct is given by:
P (k|t) =
(r
k
)
?
(n?r
t?k
)
(n
t
) (1)
To compute the probability of getting k right, we
marginalize out the number of characters guessed to
be present, t:
P (k) =
?
t
P (k, t) =
?
t
P (k|t).P (t) (2)
Now we can compute the probability of getting k
correct by randomly guessing, for all k from 0 to r.
Using these, we can compute the expected number
of correct guesses, which turns out to be 0.571.r for
an average recall would be 57.1%.
For the task of identifying the characters, every
turn could have been uttered by one of the n charac-
ters (n = 7, for our case). Thus, the average accu-
racy at identifying turns would be 17 or 14.29%.
4.3.2 Naive Bayes Classifier
For the task of predicting the subset of speakers,
we set up a Naive Bayes using words as features.
We build up a term-document matrix, with each con-
versation treated as a document. For each charac-
ter, we train a binary classifier using the training
data- conversations where the character was present
were marked as a positive instance for that charac-
ter, and ones where he was not present were marked
as negative instances. We experimented both with
using priors based on the empirical distribution in
the training data and with using uniform prior (i.e.
P (character) = 0.5). Given a test conversation,
we use individual classifiers for each of the charac-
ters to determine whether he/she was present or not.
For the task of identifying speakers, given an ut-
terance, the Naive Bayes classifier is set up as fol-
lows: Again, we create term-document matrices for
each of the speakers, where a document is a turn ut-
tered by the speaker. Turns uttered by that speaker
are positive instances and those uttered by someone
else are negative instances. For each speaker, we
compute the Naive Bayes probability ratio (odds) of
him uttering the turn and not uttering the turn, in or-
der to decide. If multiple speakers are classified as
having uttered the turn, or no speaker is classified
as having uttered the turn, the speaker with the best
odds of having uttered the turn is selected.
36
System Precision Recall
Author Topic Model 63.22% 74.71%
NB 52.33% 44.19%
NB-prior 68.31% 36.25%
Random Baseline 28.05% 57.1%
Table 1: Results for predicting subset of characters
present
4.3.3 Single Speaker HMM
This model is only used to attribute speakers to
turns. Section 4.2.1 described an HMM model
that captures speaker-addressee information. In the
single- speaker HMM, we have a state for each
speaker. Emission probabilities are given by a tri-
gram language model that is trained on the speaker?s
utterances in the training data. The transition proba-
bilities are initialized as per the empirical transitions
between speakers in the data. This model does not
capture any kind of addressee information.
5 Results
In this section, we present results of our experi-
ments with the models we described earlier, on the
two tasks, identifying the set of speakers in any
given conversation and identifying individual speak-
ers who uttered each turn in a conversation.
For the task of identifying the set of speakers in
any given conversation, we evaluate performance
using precision and recall, which are defined as fol-
lows: If the conversation actually contained r char-
acters, the system predicted that it contained t char-
acters, and got k right, then:
Precision =
k
t
;Recall =
k
r
(3)
The results are summarized in Table 1. In the ta-
ble, NB-prior indicates that the prior for the binary
classifier was determined based on the number of
conversations each character appeared in, while NB
indicates that the prior was uniform (i.e., for each
character, P (present) = P (absent) = 0.5). We
find that the results obtained using the author-topic
model are significantly better than each of the other
three models.
On average, the number of speakers in each con-
versation in the test data was 2.44 (the correspond-
System Accuracy
ESHMM 27.13%
Speaker-LM HMM 25.04%
ARTM 23.64%
Author Topic Model 26.2%
NB 23.41%
NB-prior 21.39%
Random Baseline 14.29%
Table 2: Results for predicting speakers of utterances
ing number in the training data appears to be some-
what higher at 2.65). Our attempts to restrict the set
of characters in a real setting plays a significant role
here as we shall discuss later.
The Naive Bayes classifier with empirical priors
on average predicted that there were 1.3 characters
present per conversation, while the version with uni-
form priors predicted 2.2 characters to be present per
conversation on average. The author-topic model,
on average, over-estimated the number of characters
at 2.86 characters per conversation.
For the task of predicting the speaker, given an ut-
terance, we have two kinds of Hidden Markov Mod-
els, the Exponential State HMM (ESHMM) and and
HMM with emission probabilities based on individ-
ual speaker language models (Speaker LM HMM).
We also have the topic model based systems- the
ARTM and the ATM. Finally, we have the baseline
models- the Naive Bayes with empirical priors and
with uniform priors, and the random baseline. Table
2 summarizes their performance. In this case, we
only report accuracy. Since each turn has only one
speaker, we can constrain each of the models to pro-
duce one speaker, in order to calculate the accuracy.
The HMM and topic based models all incorporate
sequence information in some form. In the case of
the HMM based models, state transitions are condi-
tioned on the previous speaker. In the case of the
topic model based systems, the feature vectors con-
tain context, although the task is modeled as a dis-
criminative classification task. The ESHMM model
worked the best on this dataset. With the exception
of the ATM and the speaker LM HMM (p < 0.10),
the improvements obtained by using the ESHMM
over all other models were statistically significant
(p < 0.05). Surprisingly, the single speaker LM
37
HMM and the ATM both outperform the ARTM on
this task. One of the reasons for this could be that
the ARTM does not suitably capture what we hoped
it would, perhaps because of the fact that the recipi-
ents (addressees) are not observed.
6 Conclusion
In this paper, we presented a set of latent variable
model based approaches to analyzing conversation
structure using the text transcript of the conversa-
tions only. The initial set of experiments show
promising improvements over simple baseline meth-
ods, though the overall results leave considerable
room for improvement. Conversations are a dy-
namic process, with the content varying significantly
with time, and the use of formulations such as dy-
namic topic modeling (Blei and Lafferty, 2006) may
help.
We believe that the concept of modeling speak-
ers and addressees would be a powerful one in mod-
eling conversation structure and useful in applica-
tions such as those involving automated agents, or
in understanding discourse on discussion forums, as
well as understanding development of authority in
such forums. The state sequences predicted by the
ESHMM implicitly predict addressees for each turn.
This is not directly used in our tasks, but could be
useful for automated agents, in understanding appro-
priate moments to take its turn.
The dataset used in this case introduced some
noise. We decided to subsume everyone aside from
the 6 main characters under the moniker other, in or-
der to keep the state space manageable. In reality, it
was a collection of a few dozen characters, some of
whom appeared intermittently through the episodes.
As a result, the emission model for this state was not
a stable one. The system rarely predicted this class,
and had very low accuracy when it did.
Further, development of datasets with annotations
specifying the addressees explicitly would probably
accelerate development of methods that work well
in such settings.
References
Andrew McCallum, Xuerui Wang and Andres Corrada-
Emmanuel. 2007. Topic and Role Discovery in Social
Networks with Experiments on Enron and Academic
Email. In Journal of Artificial Intelligence Research..
Andreas Stolcke. 2002. SRILM an Extensible Language
Modeling Toolkit. In ICSLP.
D. A. Reynolds and P. Torres-Carrasquillo. 2005. Ap-
proaches and applications of audio diarization. In
Proc. of ICASSP.
David M. Blei and John D. Lafferty. 2006. Dynamic
Topic Models. In Proceedings of ICML.
George Doddington. 2001. Speaker Recognition based
on Idiolectal Differences between Speakers. In Eu-
rospeech..
S.E. Tranter and D.A. Reynolds. 2006. An overview of
automatic speaker diarization systems. In IEEE Trans-
actions on Audio, Speech and Language Processing..
Athanasios Noulas, Gwenn Englebienne, Ben J.A. Krse
2011. Multimodal Speaker Diarization. In IEEE
Transactions on Pattern Analysis and Machine Intel-
ligence..
Howard Lei and Nikki Mirghafori. 2007. Word-
Conditioned Phone n-grams For Speaker Recognition.
In Proceedings of ICASSP
John Shawe Taylor and Nello Cristianini. 2000. Sup-
port Vector Machines and other Kernel Based Learn-
ing Methods. Cambridge University Press..
Kazuhiro Otsuka, Yoshinao Takemae and Junji Yam-
ato. 2005. A probabilistic inference of multiparty-
conversation structure based on Markov-switching
models of gaze patterns, head directions, and utter-
ances. In Proceedings of the 7th international con-
ference on Multimodal interfaces.
L.R. Rabiner. 1989. A tutorial on Hidden Markov Mod-
els and selected applications in speech recognition. In
Proceedings of IEEE.
Leonardo Canseco, Lori Lamel and Jean-Luc Gauvain
2005. A Comparative Study using Manual and Auto-
matic Transcriptions for Diarization. In Proceedings
of ASRU.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers and
Padhraic Smyth. 2004. The Author-Topic Model for
Authors and Documents. In 20th Conference on Un-
certainty in Artificial Intelligence.
Natasa Jovanovic, Rieks op den Akker and Anton Nijholt.
2006. Addressee Identification in Face-to-Face Meet-
ings In Proc. of EACL.
Rieks op den Akker and David Traum. 2009. A Compar-
ison of Addressee Detection Methods for Multiparty
Conversations. In Proc. of Diaholmia 2009.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc of UAI..
Xavier Anguera, Chuck Wooters and Javier Hernando
2005. Speaker Diarization for Multi-Party Meetings
using Acoustic Fusion In IEEE Workshop on Auto-
matic Speech Recognition and Understanding..
38
