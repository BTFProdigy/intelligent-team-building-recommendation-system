Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1579?1590, Dublin, Ireland, August 23-29 2014.
Common Space Embedding of Primal-Dual Relation Semantic Spaces
Hidekazu Oiwa
?
The University of Tokyo
Tokyo, Japan
hidekazu.oiwa@gmail.com
Jun?ichi Tsujii
Microsoft Research
Beijing, China
jtsujii@microsoft.com
Abstract
Explicit continuous vector representation such as vector representation of words, phrases, etc. has
been proven effective for various NLP tasks. This paper proposes a novel method of constructing
such vector representation for both entity-pairs and relation expressions which link them in text.
Based on the insight of the duality of relations, the representation is constructed by embedding
of two separately constructed semantic spaces, one for entity-pairs and the other for relation
expressions, into a common semantic space. By representing the two different types of objects
(i.e. entity-pairs and relation expressions) in the same semantic space, we can treat the two tasks,
relation mining and relation expression mining (a.k.a. pattern mining), systematically and in a
unified manner. The approach is the first attempt to construct a continuous vector representation
for expressions whose validity can be explicitly checked by their proximities to known sets of
entity-pairs. We also experimentally validate the effectiveness of the common space for relation
mining and relation expression mining.
1 Introduction
Learning continuous vector representation for expressions which consist of more than one word has
gained attention in recent years. Various representations have been constructed and used to measure
semantic similarities between expressions in various tasks, such as analogical reasoning (Turney et al.,
2003; Mikolov et al., 2013) and sentiment analysis (Turney and Littman, 2003; Socher et al., 2012).
Many algorithms have been proposed to construct such continuous representations, depending on specific
tasks in mind. In this paper, we propose a method for constructing a vector representation for binary
relations, i.e., relations with two arguments. We demonstrate the effectiveness of the representation for
relation mining and relation expression mining.
The method exploits the duality of a relation (Bollegala et al., 2010). While Bollegala et al. (2010) uses
the duality in their co-clustering algorithm, we construct an explicit semantic space which reflects the
two aspects of a given relation. We first construct two separate semantic spaces, one for pairs of named
entities and another for relation expressions in text which link an entity-pair. A relation is supposed to
correspond to a subset in each of these two spaces. The subset of entity-pairs is a set of pairs between
which the relation holds. The subset is called the extension set of the relation. The subset of relation
expressions consists a set of expressions which are used to link entity-pairs in the extension set.
The two semantic spaces are then embedded into a single common space. Figure 1 illustrates a brief
summary of constructing a common semantic space. While the subsets which correspond to a specific
relation are supposed to constitute natural clusters in the two original spaces, objects in the two spaces
exchange useful information to each other and form a tighter cluster in the common space. Exchange of
information takes place through common space embedding.
Since both entity-pairs and relation expressions have their vector representations in the common se-
mantic space, one can easily enumerate relation expressions specific to a certain set of entity-pairs (re-
?
This project was conducted while the first author stayed at Microsoft Research Asia.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1579
<iPad, Apple> <Stave Ballmer, Microsoft>{is product of} {is CEO of}{of}{is developed by} {become CEO of}
Entity-Pair Semantic Space
<iPad, Apple> <Stave Ballmer,Microsoft>
Relation  Semantic Space
{is developed by}{is product of}Co-occurring links
{is CEO of}{become CEO of}{of}
Common Semantic SpaceCommon Space Embedding
<Tim-Cook, Apple>
<Surface, Windows>
<Surface, Windows>
<Tim-Cook, Apple>
R: PRODUCT-OF R: CEO
Figure 1: Overview of our framework to construct common semantic space.
lation expression mining). Furthermore, unlike the conventional pattern-based relation mining, one can
perform relation mining in the common space without explicit reference to relation expressions.
2 Basic Framework
2.1 Duality of Relation and a Common Space
A binary relation is defined either extensionally by a set of pairs in the relation or intensionally by a set
of conditions which a pair in the relation should satisfy. However, in actual applications of text mining,
either of these definitions is given in a complete form. We are only given a subset of the whole set of pairs
and have to complete the set (i.e. relation mining). Instead of an explicit intensional definition, we only
have a set of observations in text where pairs in a relation are linked by certain linguistic expressions.
Based on such observations, we have to judge whether a given pair holds the relation or not. Though
some observed expressions are non-ambiguous and explicit for a relation (for example, ?the birth place
of A is B?), most of expressions are not (such as?A comes from B?).
We call a set of pairs which define a relation as Extension set of a relation, while we call their ob-
served expressions in text as Manifestation set. While these two sets are only partially given, they define
relations which we are interested in. Such duality of a relation has been recognized by many previous
work and has been exploited in relation mining and relation expression mining. (Bollegala et al., 2010),
for example, used the duality in their work on co-clustering of entity-pairs and relation expressions. (Ba-
roni and Lenci, 2010) presented a more general approach which defines a tensor associating a triplet
< e
1
, l, e
2
> with a weight. e
1
and e
2
are entity pairs, while l is a linking expression in text. By project-
ing the tensor to matrices, they showed that diverse concepts used in distributional semantics could be
captured in a unified manner. In particular, their tensors capture directly the duality of entity pairs and
their linking expressions (i.e. relation expressions).
These previous works implicitly assume that the semantic space of entity pairs and that of relation ex-
1580
pressions are tightly coupled. That is, the space of entity pairs is defined in terms of their co-occurrences
with linking expressions (or the weights in a tensor between them) and vice versa. However, such tight
coupling between the semantic spaces of entity pairs and relation expressions is not a logical necessity,
and harmful in the sense that it restricts available information only to their co-occurrences.
An entity pair and a linking expression are complex objects by themselves, and their semantic spaces
can be defined independently of each other. Two entities in a sentence, for example, are linked not only
by single verbs or predicates but by a long sequence of words. This means that we can define a semantic
space of linking expressions independently of entity pairs which they link. For example, one can use
sequence similarities of words among relation expressions. Since knowledge resources of large scale
have become available of late, we can define a semantic space of entity pairs by using paths in these
knowledge graphs, regardless of their textual occurrences with relation expressions.
In this paper, we first define two separate semantic spaces (i.e. dual primal spaces) for entity pairs and
relation expressions, and then use their textual co-occurrences to construct a common space consistent
with the two primal spaces. In this approach, the co-occurrences of entity pairs with relation expressions
play only an auxiliary role to project the two spaces into a common space.
The approach allows us to integrate information richer than mere co-occurrences of two objects (i.e.
entity pairs and relation expressions). Furthermore, the common space provides us with direct means by
which one can grasp finer grained relationships between two objects. Given a set of seed pairs of entities,
one can gather a set of relation expressions in their nearest neighbor in the common space. Another set
of seed pairs, even though conceptually they belong to the same relation, one may get a different set of
relation expressions. The previous approaches, in which the semantics of the two objects are captured
in two separate spaces, can capture only indirectly the hierarchical nature of natural relations, and how
such a hierarchy is mapped on association of extension sets with manifestation sets.
2.2 Extension set and Manifestation Set
Let E be a set of named entities. Let ?e
i
, e
j
? denote a pair of entities (e
i
, e
j
? E) and E
2
a set of all
entity-pairs. Then, a relation, R, is extensionally defined as a set of entity-pairs E
R
? E
2
, such as
CEO = { ?Tim-Cook, Apple?, ?Ballmer, Microsoft?, . . .}, COMPETE = {?Apple, Samsung?, ?Google,
Microsoft?, . . .} between which the relation holds. We call such a set of entity-pairs the extension set of
a relation R.
On the other hand, a relation R is manifested in text in various forms of expressions. For example, ?is
the CEO of? in ?Tim-Cook is the CEO of Apple? is a direct manifestation of the relation CEO. While
?overtook? in ?Samsung overtook Apple in the smartphone market in China? can be a manifestation of
the relation COMPETE, this manifestation is rather indirect, based on inference. We denote a relation
expression by r
i
and the whole set of relation expressions by D. We call a subset of relation expressions
which manifest, directly or indirectly, a relation R, as the manifestation set of R.
2.3 Primal-dual semantic spaces
A relation, R, is characterized by the two sets, the extension set and the manifestation set. In other words,
the two sets are implicitly associated with each other via the relation R. This association between the
two sets constitutes the foundation of the common semantic space to be constructed in this paper.
We first construct primal-dual semantic spaces, one for entity-pairs and another for relation expres-
sions. A sentence where two entities appear can be seen from two different perspectives. One view is to
see the sentence as characterization of the entity-pair, while the other takes the sentence as characteriza-
tion of the relation expression which links the two entities. Based on these two views, we construct two
semantic spaces from a given set of sentences (corpus). One space is for a set of entity-pairs (E
2
) and
the other for a set of relation expressions (D). e
2
? E
2
and r ? D are represented by vectors e
2
? E
2
and r ? D in the corresponding spaces. We assume that the two spaces are vector spaces, i.e., E
2
and D
are an n-dimensional vector space and an m-dimensional one, respectively.
1581
PRODUCT-OF CEO{is product of} <Stave Ballmer,Microsoft><iPad, Apple> {is CEO of}{of} dissimilardissimilar
{is developed by}
{become CEO of}Common Semantic Space
Ambiguous
Figure 2: Illustration of common semantic space defined by our approach.
2.4 Triplets
The two objects, entity-pairs and relation expressions, whose spaces are separate, are linked through
their co-occurrences in text. Co-occurrence of a relation expression (r) and an entity-pair (e
2
= ?e
2
1
, e
2
2
?)
means that r links in a sentence the entities of e
2
1
and e
2
2
. A triplet represents such a co-occurrence with
its frequency (f ? R) in text. An instance of triplets is denoted as ?e
2
, r, f? ? T . T indicates a set
of triplets. These co-occurrence frequencies between entity-pairs and relation expressions play a critical
role in common space embedding as linkage clues.
2.5 Common space embedding from E
2
and D
We use Multi-View Partial Least Squares (MVPLS) (Wu et al., 2013) as the basic framework to construct
a common space from E
2
? R
m
and D ? R
n
. MVPLS was originally developed for web search and
has been proven to be effective for embedding the semantic space of queries and that of documents into
a common space. This framework is an extension of the conventional well-used approach, Partial Least
Square. The framework is general enough to be used for our purpose.
Let k be the dimension of common latent space such that k ? m and k ? n. e
2
i
? E
2
is a i-th
entity-pair feature vector in the entity-pair space and r
i
? D is a i-th phrase feature vector. L
e
, L
r
are
linear projection matrices for embedding the original feature vector space into the common latent space.
L
e
is m? k and L
r
is n? k size matrices.
MVPLS learns these two projection matrices for generating a well-constructed common space from
the two separated spaces. Construction of latent common space can be formulated as an optimization
problem which maximizes the sum of the similarities between entity-pairs and relation expressions in
the common space when they co-occur. This optimization problem is as follows:
argmax
L
e
,L
r
?
(e
2
i
,r
i
,f
i
)?T
log(f
i
)r
T
i
L
r
L
T
e
e
2
i
s.t. L
T
e
L
e
= I, L
T
r
L
r
= I . (1)
Note that the similarity score is weighted by the logarithmic scale of the co-occurrence counts. The
outputs of this optimization problem are L
e
and L
r
which maximize the objective value where the or-
thogonal constraints on these matrices are satisfied. We do not necessarily solve (1) again when the
system receives a new instance because the derived matrices can be applied not only for the existing
entity-pairs and relation expressions but new ones. The problem is not convex, but Wu et al. (2013)
proved that the global optimal solution can be obtained by SVD of
?
T
log(f
i
)e
2
i
r
T
i
. L
e
corresponds to
left singular vectors and L
r
consists of right singular vectors.
2.6 Ambiguity of Relation Expressions in the Common Space
Due to the ambiguity of relation expressions, the assumption that the manifestation set of the same R
cluster around in proximity does not hold in reality. ?of? in ?Steve Ballmer of Microsoft? belongs to
1582
the manifestation set of CEO, while ?of? in ?iPad of Apple? belongs to the set of a different relation,
PRODUCT-OF. Indirect manifestation such as ?overtake? is another cause of ambiguity. Inference in-
volved here is abductive in nature and not always valid. We may be able to infer COMPETE relation from
?X overtake Y?, but ?X overtake Y? can be a consequence of another relation such as COOPERATE.
Such an ambiguous expression belongs to the manifestation sets of more than one relation and thus
would be located in a rather neutral position in the space. Since the common space reflects how frequently
certain expressions are used to link entity-pairs, their positions in the space reflect the relative specificity
to each relation cluster. Figure 2 illustrates how the ambiguity of a relation expression captured in the
common space.
3 Relation Mining and Relation Expression Mining
In an actual situation, both the extension set and the manifestation set of a relation R are only partially
known. To produce more comprehensive sets of these objects from large corpora is generally called
mining. Two mining tasks have been studied so far, which are different, though mutually related.
We define relation mining as a task which, given a relation R, enumerates entity-pairs in the extension
set. Another mining task (i.e. relation expression mining which is often performed as an auxiliary task
of relation mining) is to gather a set of relation expressions which are manifestations of a given R.
3.1 Relation Mining
Relation mining is the task to enumerate entity-pairs of a relation R from a small given set of objects
of a relation R. For example, if a set of relation expressions as the manifestation set of a relation R are
given, one can produce a set of entity-pairs simply by identifying occurrences of relation expressions in
text and producing the entity-pairs which are linked by them. Alternatively, if a small set of entity-pairs
as a subset of the extension set of a relation R are given, one can produce a set of entity-pairs simply by
gathering similar entity-pairs measured by relation expression co-occurrence vectors. These ideas have
been shared by many mining systems called pattern-based relation mining systems.
The recall and precision of such a system are determined by the quality and quantity of the given set.
If the given set is small, a system suffers low recall. On the other hand, if the set is large but contains
many ?ambiguous? or ?weak? objects, a system suffers low precision.
Therefore, one of the keys for success of relation mining is how to gather a large initial set, which are
effective, i.e. objects less ambiguous with high frequency. The common semantic space can be used not
only to generate a comprehensive set but to measure the specificity of objects in terms of a given R, it
also provides refined semantic measures between entity-pairs.
3.2 Relation Expression Mining
We have discussed semantic spaces of relation expressions and the common semantic space as if to
define what constitutes a relation expression is straightforward. However, it is not trivial to define what
constitutes a relation expression.
In the previous section, we treat ?overtake? in ?Apple overtook Samsung in the smart phone market? as
a relation expression which manifests the relation ?COMPETE?. However, one may argue that a pattern
such as ?X overtake Y in . . . market? should be treated as a basic unit of manifestation of the relation
COMPETE. This longer expression is less ambiguous and thus more effective than the shorter pattern of
?overtake?. On the other hand, the frequency of this pattern would be much less and thus less effective,
compared with the shorter version. Mining of effective relation expressions (sometimes called ?pattern
mining?) has to address the problem of balancing the specificity and generality of relation expressions.
Furthermore, one would like to identify the same relation expression in ?Apple announced yesterday that
it had overtaken Samsung which . . .? as in ?Apple overtook Samsung in the smart phone market?.
In the experiments, we do not treat the process of pattern mining seriously. Instead, we used two con-
ventional methods. The first method is to enumerate subsequences of words in the intervening part in a
sentence between two entities, and use them as relation expressions. We expect less effective expressions
as manifestation to be recognized in the common space. Another method is to use the shortest paths in
1583
dependency structures of sentences as relation expressions. Shortest paths can generalize surface variants
of essentially the same relation expressions and reduce unnecessary proliferation of relation expressions.
4 Experiments
This section empirically evaluates our approach of embedding the two original spaces into a common
space. We show that the common space provides a continuous vector space for relation expressions, in
which not only similarities among expressions but also their ambiguities are properly captured.
4.1 Experiment Settings
4.1.1 Dataset
Entity-pair Relation Triplet
Enumeration 12, 174 12, 185 521, 454
Shortest Path 10, 251 92, 797 130, 897
Table 1: The specifications of the ENT
dataset: Sizes of distinct entity-pairs, relation
expressions, and triplets. ?Enumeration? in-
dicates the results of pattern mining based on
word subsequences. ?Shortest Path? shows
that of shortest path extraction.
We use the ENT benchmark dataset (Bollegala et al.,
2009) for our experiments. The dataset consists of
661,502 snippets, which are brief summaries provided
by Web search engines. Most web search engines
provide links to webpages and snippets as search re-
sults and snippets contains a subset of texts including
the query words derived from the webpages. Table
1 shows how many distinct entity pairs, relation ex-
pressions and triplets were extracted as results of NER
and expression extraction (See Section 4.1.2 and 4.1.3).
The dataset is accompanied with 100 entity-pairs that
are classified into five semantic categories: ACQUISI-
TION, HEADQUARTERS, FIELD, CEO, and BIRTHPLACE. We use the ENT dataset not only for
evaluation of relation mining but also for examining the characteristics of the common space for rela-
tion expression mining. Note that, due to the nature of snippets, the dataset is very noisy. It contains
many non-sentences and even non-English texts, which may adversely affect the performance of mining
systems.
4.1.2 Entity and Entity-Pair Extraction
We first extracted entities from the ENT dataset. After splitting snippets into sentences, we applied
named entity recognizer (NER) (Finkel et al., 2005) to recognize entities in sentences. We used Stanford
Core NLP tools 2
1
for sentence splitting and NER. As relevant semantic classes for the ENT dataset,
entities which are recognized as ORGANIZATION, LOCATION, or PERSON are treated as entities in
the further process. We only used sentences in which at least two entities of these three classes appear.
4.1.3 Extraction of Relation Expressions
The definition of relation expressions which link two entities in text is not trivial. We adopt two methods
of extracting candidates of relation expressions, and compare them in experiments.
The first method is to use, as relation expressions, subsequences of words which appear between two
entities. We assume that two entities which appear apart in a sentence by more than 10 words are not
explicitly linked in the sentence. From the word sequence whose length is less than 10, we enumerate all
possible subsequences whose length is less than 6 words. Since a set of such subsequences include many
noises as relation expressions, we use only subsequences the frequency of which is higher than 100.
This shallow approach can be run very fast, thanks to the advances of sequential pattern mining (Pei
et al., 2004). Although the method is similar to that used in Bollegala et al. (2010) , we do not use any
further constraints based on part-of-speech tags, lexical-syntactic information, etc. Our contention is that
such ad-hoc constraints unnecessarily restrict a set of relation expressions. Our method treats ambiguous
expressions (e.g. ?of?, ?in?, ?with?, etc.) as relation expressions. Instead, the effectiveness or the degree
of ambiguities of a relation expression is captured in the common space after embedding.
1
http://nlp.stanford.edu/software/corenlp.shtml
1584
The second method is based on dependency parsing. We obtain the dependency tree of a sentence by a
publicly available deep parser, Enju3
2
(Miyao and Tsujii, 2005; Miyao and Tsujii, 2008), and then extract
shortest paths between two entities. Unlike the first method, this method uses linguistic information to
extract the skeleton of a relation expression.
Each node in shortest paths consists of a base form (e.g., ?like?, ?player?), syntactic category (e.g.,
?verb?, ?noun?), and predicate-argument links. The length of shortest paths was restricted to the range
from 1 to 6. Compared with the first method, a set of shortest paths contains much less noises, so that
we do not filter out those with low frequency. In the same way as the first method, a set of shortest paths
contains highly ambiguous paths (e.g. the path of ?of?).
4.1.4 Generation of the Space for Entity-Pairs
The primal semantic space for entity pairs can be constructed in several ways. The co-training method
constructed a space of entity pairs based on their co-occurrences with relation expressions. Their method
requires the two spaces of entity pairs and relation expressions have to be tightly coupled.
On the other hand, our approach allows us to design the two spaces independently. In addition to
the tightly coupled spaces, we design a new space for entity pairs based on the distributional hypothesis
(Harris, 1954). We used the point-wise mutual information (PMI) score of each word with an entity-pair.
PMI score is defined as PMI = log
e
p(w
a
|?e
i
, e
j
?)/p(w
a
) where p(w
a
) is an occurrence probability of a
word w
a
and p(w
a
|?e
i
, e
j
?) is a conditional probability with respect to an entity-pair ?e
i
, e
j
?. We filtered
words whose PMI scores were below 1.0 and all the rest were used as the features.
To maximize the effectiveness of the space, we performed preliminary experiments by changing pa-
rameters in the definition of context in the distributional hypothesis, such as how the context around
entities is distinguished, whether the whole of a sentence or limited windows around entities are used
as context, etc. As a result, we chose the settings in which right, left, and intervening contexts are dis-
tinguished. We used three different window sizes as the context (e.g. 4, 5 and 6 words). That is, when
we set the window size to 4, we used the four words in the left side of the first entity as the left context,
those in the right side of the second entity as the right context, and the words in the intervening part
as the intervening context. If the intervening part consists of more than 8 words, the four words in the
neighborhood of the two entities are used as the intervening context.
4.1.5 Generation of the Space for Relation Expressions
Following the work (Lin and Pantel, 2001), we constructed a simple space, in which a relation expression
is characterized by the entities which it links. We counted the entities in the left-hand side and the right
hand side of a relation expression. The same as the vector of an entity-pair, we used the PMI score as the
feature value. As for feature selection, we chose the entities whose PMI scores are no less than 1.0
3
.
4.1.6 Dimension Reduction
After generating vectors for entity-pairs and relation expressions, we applied a dimension reduction.
Since both of the primary semantic spaces use surface words or entities, their vectors tend to have a very
large dimension (i.e. about 100, 000 for entity pairs and about 2, 500 for relation expressions). Since
the cardinalities of the two sets of distinct entity pairs and relations expressions are also very high (See
Table 1 of the specification of the ENT dataset), the high dimensions of the two spaces would make the
computation cost of MVPLS embedding in terms of time and space prohibitively high.
To take advantage of the sparseness of both spaces, we used Randomized SVD (Halko et al., 2011)
which can produce low-dimensional feature vectors from a large-scale sparse feature matrix efficiently.
We produced spaces with 3, 000-dimensions for entity-pairs and 1, 000 for relation expressions.
4.1.7 Common Space Embedding
Lastly, we applied MVPLS (1) to construct common space projection matrices. We set the dimension
of common space as 1, 000. We verified that the dimension does not affect much the evaluation results,
2
http://www.nactem.ac.uk/enju/
3
Other than context-based characterization methods, we have applied path kernel method (Reichartz et al., 2009; Reichartz
et al., 2010) to shortest path relations as preliminary works, however, their performances were definitely worse.
1585
Window Size 4 5 6
VSM (Turney, 2005) 0.68
LRA (Turney, 2005) 0.68
(Bollegala et al., 2010) 0.76
Relation (1, 000) 0.82
Original (1, 000) 0.88 0.88 0.88
Embedded (1, 000) 0.90 0.89 0.90
Table 2: Entity-Pair space evaluation results (Enu-
meration) : Each figure shows the average pre-
cision. The best figures in each window size are
written in bold. Figures in parentheses denote the
number of dimensions.
Window Size 4 5 6
VSM (Turney, 2005) 0.68
LRA (Turney, 2005) 0.68
(Bollegala et al., 2010) 0.76
Relation (1, 000) 0.62
Original (1, 000) 0.91 0.90 0.91
Embedded (1, 000) 0.91 0.91 0.91
Table 3: Entity-Pair space evaluation results
(Shortest Path). Each figure shows the average
precision. The best figures in each window size
are written in bold. Figures in parentheses denote
the number of dimensions.
when we set it to larger than 300. So we used a common space with 1, 000 dimensions for the sake of
comparison with the original spaces.
4.2 Relation Mining Evaluation
We evaluated the embedding approach by a quantitative analysis on the relation mining task used in
(Bollegala et al., 2010). The experimental setting is the same as the previous work. The objective is
to assess whether the derived common semantic space provides a good space for measuring semantic
distances among entity-pairs. We expected that in a good semantic space, entity-pairs which belong to
the same semantic category would be clustered in proximity.
We used the ENT dataset (Bollegala et al., 2009). We used the same evaluation measures used in
(Bollegala et al., 2010). The measure assumes that a semantic space would be judged as appropriate if it
assigned higher similarity scores to entity-pairs the relationships of which belong to the same category.
Therefore, the measure evaluated the top 10 similar pairs to each entity-pair and calculated average
precision defined as
?
10
t=1
Rel(t) ? Pre(t)/10. Here, Rel(t) is a binary valued function that returns 1 if
the entity-pair at rank t and ?e
i
, e
j
? have the same semantic category. Pre(t) is the precision at rank t,
which is defined by the percentage of correct objects in top t pairs.
For the sake of comparison, we prepared several models, which used different semantic spaces for
entity pairs. One space (called Relation) is to characterize an entity pair by the relation expressions
which it co-occur. Another space (called Original) is to characterize an entity pair by the context vector
discussed in Section 4.1.4. There are three Original spaces which use different window sizes (4, 5 and 6
words). Then, the final space is the common space obtained by embedding (called Embedded).
Table 2 and 3 correspond to the experiment results using the two definitions of relation expressions,
one by enumerated word sequences and the other by shortest paths. We note that the previous works
only use co-occurrences information and cannot use any context information. The previous work and
Relation have no ways of changing the size of windows. Therefore, these results are independent of
the window size. These tables show the limitation of co-training which can only use tightly coupled
vector spaces for entity pairs and relation expressions. Both the original and the common embedded
space outperform significantly the performance obtained by previous works, regardless of the definitions
of relation expressions (i.e. enumerated subsequence and shortest paths). Since the space for relation
expressions is simple and poor, we expected that it would hardly add extra information to the space of
entity pairs. However, the common space embedded from the two spaces improve the performance.
4.3 Relation Expression Mining
While the primary space for relation expressions is rather poor, vector representations of relation ex-
pressions are much richer in the common space. This is because they receive extra information from
the rich space of entity-pairs through their co-occurrences. For evaluation, we first chose representative
relation expressions, and then gathered relations that are close to them in the primary space of relation
1586
{announce acquisition} {president ,}
Embedded Original Embedded Original
{announce that have acquire} {announce that have acquire} {chairman ,} {?s president be}
{complete acquisition} {acquire} {, ceo &} {would say}
{say have it buy} {pay} {?s president ,} {would that say}
{acquire} {buy} {ceo &} {?s blue and}
{pay} {compra} {chief ,} {?s chairman ,}
{?s acquisition} {buy company} {, ceo )} {chairman ,}
{?s out of} {say that it buy} {chief ,} {palmisano}
{?s purchase} {nor} {would that say} {,}
{acquisition} {do} {executive ,} {, reader ,}
{?s takeover} {announce be buy} {ceo become} {palmisano include door ?}
Table 4: Evaluation of similarity measure between relation expressions. This table shows the top-10
ranked relation expressions that are closest to two representative relation expressions.
expressions and in the common space. If our expectation was correct, the list of expressions close to the
chosen expression in the common space should be more appropriate than that in the primary space.
We show the result of the experiment in which we use shortest paths as relation expressions. We used
the same dataset as the previous experiment. We removed shortest paths with frequency less than 10. As
for the primary space for entity pairs, we use the one with the window size of 5. We used {announce
acquisition} and {president ,} as two representatives.
Table 4 shows the lists of relation expressions closest to the chosen representatives in the common
space and the primary space. For the ease of interpretation, we do not show syntactic categories and
predicates attached to the shortest paths. One can easily see that the common space successfully moved
down many ambiguous expressions such as {compra} and {nor} in {announce acquisition}, and {would
say} and{,} in {president ,}. On the other hand, some relation expressions which are specific and se-
mantically similar to the chosen ones moved up in the rank, for example {?s purchase} and {chief ,}.
We have also conducted the same experiment for relation expressions produced by the enumeration
method. While the enumeration method improves the relation mining which gathering similar entity-
pairs, it gave much poorer results to expressing mining than the shortest paths. This is because the
enumeration method generated a large amount of non-meaningful relation expressions. For example, to
generate a complex relation expression such as {say have it buy} appeared in Table 4, the enumeration
method has to generate a large variety of noisy ones that co-occur with a complex relation expression.
4.4 Similarity measure between entity-pair and relation expressions
The major advantage of embedding over co-training is that it produces where the two different types of
objects, entity-pairs and relation expressions, are treated in the exactly the same vector space. Therefore,
we can easily gather a set of relation expressions relevant to a given prototype entity pair of a relation. In
this experiment, instead of representative relation expressions, we gave entity pairs which are prototyp-
ical examples of certain relations. As in the previous experiment, we used the shortest paths as relation
expressions, and ignored relation expressions with frequency under 10.
Table 5 shows the list of relation expressions for two prototypical entity-pairs used in the ENT dataset,
?charlie chaplin, london? as a representative entity-pair for BIRTHPLACE and ?facebook inc, mark
zuckerberg? as CEO relation semantics. The table shows that the top-10 frequently co-occurring relation
expressions. While many noisy relation expressions (i.e. ambiguous expressions) appear by extracting
expressions based on their co-occurrence frequency with ?charlie chaplin, london?, these ambiguous
expressions disappear in the proximity of the entity-pair in the common space. Moreover, the result of
?facebook inc. mark zuckerberg? shows that some relation expressions that do not co-occur with the
prototype entity-pair were successfully extracted, such as {?s executive ,}.
5 Related Work
Bollegala et al. (2010) proposed a simple sequential co-clustering framework of entity-pairs and relation
expressions for objects sharing the same semantic relation to be clustered. Our definition of primal-dual
1587
?charlie chaplin, london? ?facebook inc, mark zuckerberg?
Embedded Co-occurrence Embedded Co-occurrence
{bear walworth} {bear} {?s executive ,} {, ceo}
{bear april} {?s ? arrangement while lay orchestra} {ceo be} {, ceo (}
{play} {,} {ceo} {founder and}
{bear} {reception} {,} {everything , ceo}
{bear} {?s} {?s president ,} {andceo}
{bear woolsthorpe ,} {?s} {have say} {ceo ,}
{bear woolthrope} {be when} {, ceo ,} {-}
{be member parliament} {and} {, ceo} N/A
{bear woolsthorpe} {bear april street , walworth ,} {ceo become} N/A
{?s} {walk ,} {buy} N/A
Table 5: Relation expressions gathered by prototype entity-pairs on the ENT dataset. This table shows
the top-10 ranked relation expressions that are closest to the representative entity-pairs ?charlie chap-
lin, london? as BIRTHPLACE and ?facebook inc, mark zuckerberg? as CEO. ?facebook inc, mark
zuckerberg? co-occurred with only seven discrete relation expressions.
semantic space and common space embedding approach can be viewed as extensions of their work by
introducing feature spaces as characterizations. This extension enables to utilize each space?s character-
izations and calculate similarity between different types of objects. Baroni and Lenci (2010) proposed a
framework that analyze triplets as a third-order tensor, called ?distributional memory?. By matricizing
the tensor to second-order tensors, that is matrices, this framework can utilize the relationship between
entity-pairs and relation expressions. They also propose the procedure for generating continuous vec-
tor representations of entities and relation expressions through the tensor decomposition techniques.
However, this framework cannot use semantic spaces independently defined, therefore it is difficult to
incorporate the similarity information between entity-pairs or similarities between relation expressions
into the decomposition procedure in contract to our framework based on MVPLS. Lin and Pantel (2001)
proposed a weakly supervised framework of mining paraphrases based on shortest paths as basic units
to be mined. Our work can be viewed as an extension by mixing entity-pair characterizations with the
extended distributional hypothesis by embedding.
Many other previous work have been proposed to construct a knowledge base, including relation
expressions (Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2012). However, they cannot
interactively predict semantic meanings of objects through labeled objects of the other space.
As for treatment of ambiguity, some previous work has focused on triplet clustering to disambiguate
each triplet object known as relation extraction. Unlike other mining tasks, this task requires a system
to disambiguate the meaning of a relation expression r in ?r, e
1
, e
2
? which appears in a specific context.
We did not treat this task in this paper, however, our framework would discharge the burden by showing
the insight of ambiguities of each relation expression and entity-pair. Yao et al. (2011; 2012) proposed
a new triplet clustering method through a generative probabilistic model. The model used surrounding
contexts as features in both a sentence and document level to identify the meaning of each triplet. They
demonstrated the effectiveness of their models compared with USP (Poon and Domingos, 2009) or DIRT
(Lin and Pantel, 2001). Min et al. (2012) provided a simple and scalable triplet clustering algorithm in
an unsupervised way and enables to incorporate various resources about entity and relation expressions.
Chen et al. (2006) proposed a label propagation algorithm for relation extraction as a semi-supervised
learning method by utilizing the information of parsing.
6 Conclusion
We propose a common space embedding framework which constructs a semantic space in which both
entity-pairs and relation expressions are represented. We showed that our framework is effective to con-
struct the extension set and the manifestation set of a relation R in this space. The results of experiments
showed that the common space is further refined for tasks such as relation and relation expression min-
ing, compared with the original two spaces. Moreover, we showed relation expressions collected from a
small set of entity-pairs through the common space, which share the same semantics as being relevant.
1588
There are several interesting future topics:
? how to iteratively collect objects from a dual object, like bootstrapping
? how to reduce surface diversities of relation expressions which are not abstracted away by simple
method or shortest paths (by using methods such as SOL Pattern Model (Nakashole et al., 2012))
? How to combine a ground truth and non-textual knowledge stored in knowledge bases for charac-
terizing entity-pairs with our framework
? How to extend the framework in order to deal with n-ary relations
References
Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673?721.
Danushka T. Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2009. Measuring the similarity between implicit
semantic relations from the web. In Proc. of WWW.
Danushka T. Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2010. Relational duality: unsupervised extraction
of semantic relations between entities on the web. In Proc. of WWW.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell.
2010. Toward an architecture for never-ending language learning. In Proc. of AAAI.
Jinxiu Chen, Donghong Ji, Chew Lim Tan, and Zhengyu Niu. 2006. Relation extraction using label propagation
based semi-supervised learning. In Proc. of ACL.
Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction.
In Proc. of EMNLP.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In Proc. of ACL.
Nathan Halko, Per G. Martinsson, and Joel A. Tropp. 2011. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217?288.
Zellig Harris. 1954. Distributional structure. Word, 10(23):146?162.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of inference rules from text. In Proc. of KDD.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeffrey Dean. 2013. Distributed representations
of words and phrases and their compositionality. In Proceedings of NIPS.
Bonan Min, Shuming Shi, Ralph Grishman, and Chin-Yew Lin. 2012. Ensemble semantics for large-scale unsu-
pervised relation extraction. In Proc. of EMNLP-CoNLL.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage hpsg parsing. In
Proc. of ACL.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35?80.
Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. Patty: A taxonomy of relational patterns
with semantic types. In Proc. of EMNLP-CoNLL.
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Jianyong Wang, Helen Pinto, Qiming Chen, Umeshwar Dayal, and
Mei-Chun Hsu. 2004. Mining sequential patterns by pattern-growth: The prefixspan approach. IEEE Transac-
tions on Knowledge and Data Engineering, 16(11):1424?1440.
Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proc. of EMNLP.
Frank Reichartz, Hannes Korte, and Gerhard Paass. 2009. Dependency tree kernels for relation extraction from
natural language text. In Proc. of ECML/PKDD (2).
1589
Frank Reichartz, Hannes Korte, and Gerhard Paass. 2010. Semantic relation extraction with kernels over typed
dependency trees. In Proc. of KDD.
Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of EMNLP-CoNLL, pages 1201?1211.
Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information Systems, 21(4):315?346.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. 2003. Combining independent mod-
ules to solve multiple-choice synonym and analogy problems. In RANLP, pages 482?489.
Peter D. Turney. 2005. Measuring semantic similarity by latent relational analysis. In IJCAI, pages 1136?1141.
Wei Wu, Hang Li, and Jun Xu. 2013. Learning query and document similarities from click-through bipartite graph
with metadata. In Proc. of WSDM.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew McCallum. 2011. Structured relation discovery using
generative models. In Proc. of EMNLP.
Limin Yao, Sebastian Riedel, and Andrew McCallum. 2012. Unsupervised relation discovery with sense disam-
biguation. In Proc. of ACL.
1590
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1374?1384,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Formalizing Word Sampling for Vocabulary Prediction
as Graph-based Active Learning
Yo Ehara
?
National Institute of
Information and Communications
Technology
ehara@nict.go.jp
Yusuke Miyao
National Institute of
Informatics
yusuke@nii.ac.jp
Hidekazu Oiwa
Issei Sato
Hiroshi Nakagawa
The University of Tokyo
{oiwa,sato}@r.dl.itc.u-tokyo.ac.jp
nakagawa@dl.itc.u-tokyo.ac.jp
Abstract
Predicting vocabulary of second language
learners is essential to support their lan-
guage learning; however, because of the
large size of language vocabularies, we
cannot collect information on the entire
vocabulary. For practical measurements,
we need to sample a small portion of
words from the entire vocabulary and pre-
dict the rest of the words. In this study, we
propose a novel framework for this sam-
pling method. Current methods rely on
simple heuristic techniques involving in-
flexible manual tuning by educational ex-
perts. We formalize these heuristic tech-
niques as a graph-based non-interactive
active learning method as applied to a spe-
cial graph. We show that by extending the
graph, we can support additional function-
ality such as incorporating domain speci-
ficity and sampling from multiple corpora.
In our experiments, we show that our ex-
tended methods outperform other methods
in terms of vocabulary prediction accuracy
when the number of samples is small.
1 Introduction
Predicting the vocabulary of second language
learners is essential to support them when they are
reading. Educational experts have been continu-
ously studying methods for measuring the size of
a learner?s vocabulary, i.e., the number of words
?
The main body of this work was done when the first
author was a Ph.D. candidate in the University of Tokyo and
the paper was later greatly revised when the first author was
a JSPS (Japan Society for the Promotion of Science) research
fellow (PD) at National Institute of Informatics. See http:
//yoehara.com/ for details.
the learner knows, over the decades (Meara and
Buxton, 1987; Laufer and Nation, 1999). Ehara
et al. (2012) formalized a more fine-grained mea-
surement task called vocabulary prediction. The
goal of this task is to predict whether a learner
knows a given word based on only a relatively
small portion of his/her vocabulary. This vocabu-
lary prediction task can be further used for predict-
ing the readability of texts. By predicting vocab-
ulary unknown to readers and showing the mean-
ing of those specific words to readers, Ehara et al.
(2013) showed that the number of documents that
learners can read increases.
Word sampling is essential for vocabulary pre-
diction. Because of the large size of language vo-
cabularies, we usually cannot collect information
on the entire vocabulary. For practical measure-
ments, we inevitably need to sample a small por-
tion of words from the entire vocabulary and then
predict the rest. We refer to this sampling tech-
nique as word sampling.
Word sampling can greatly affect the perfor-
mance of vocabulary prediction. For example, if
we consider only short everyday general domain
words such as ?cat? and ?dog? as samples, the rest
of the vocabulary is difficult to predict since learn-
ers likely know most of these words. To more ac-
curately measure a learner?s vocabulary, we ide-
ally must sample words that are representative of
the entire set of words. More specifically, we wish
to sample words such that if a learner knows these
words, he/she is likely to know the rest of the
words in the given vocabulary, and vice versa.
To our knowledge, however, all current studies
have relied on a simple heuristic method. In this
heuristic method, educational experts first some-
how create groups of words with the aim that the
words in a group are of similar difficulty for learn-
1374
ers. To create groups of words, the experts typi-
cally make use of word frequencies and sometimes
manually reclassify words based on experience.
Next, a fixed number of words are randomly sam-
pled from each group via a uniform distribution.
We call this approach heuristic word sampling.
In this study, we propose a novel framework
that formalizes word sampling as non-interactive
graph-based active learning based on weighted
graphs. In our approach, nodes of a graph corre-
spond to words, whereas the edge weights show
how similar the difficulty levels of a word pair
are. Unlike interactive active learning algorithms
used in the NLP community, which use expert an-
notators? human labels for sampling nodes, non-
interactive active learning algorithms exclude ex-
pert annotators? human labels from the protocol
(Ji and Han, 2012; Gu and Han, 2012). Given
a weighted graph and using only its structure,
without human labels, these algorithms sample
nodes that are important for classification with al-
gorithms called label propagation. Excluding an-
notators? human labels from the protocol is bene-
ficial for educational purposes since learners can
share the same set of sampled words via, for ex-
ample, printed handouts.
Formalizing the current methods as non-
interactive graph-based active learning enables us
to extend the sampling methods with additional
functionality that current methods cannot han-
dle without applying burdensome manual heuris-
tics because we can flexibly design the weighted
graphs fed to the active learning algorithms. In our
framework, this extension is achieved by extend-
ing the graph, namely, our framework can handle
domain specificity and multiple corpora.
Domains are important when one wants to mea-
sure the vocabulary of learners. For example, con-
sider measuring non-native English speakers tak-
ing computer science graduate courses. We may
want to measure their English vocabulary with an
emphasis on computer science rather than their
general English vocabulary. However, such an
extension is impossible via current methods, and
thus it is desirable to sample algorithms to be able
to handle domain specificity. Our framework can
incorporate domain specificity between words in
the form of edges between such words.
Handling multiple corpora is important when
we cannot single out which corpus we should rely
on. The current technique used by educational
experts to handle multiple corpora is to heuristi-
cally integrate multiple frequency lists from mul-
tiple corpora into a single list of words; however,
such manual integration is burdensome. Thus, au-
tomatic integration is desirable. Our framework
converts multiple corpora into graphs, merges
these graphs together, and then samples from the
merged graph.
Our contributions as presented in this paper are
summarized as follows:
1. We formalize word sampling for vocabulary
prediction as graph-based active learning.
2. Based on this formalization, we can perform
more flexible word sampling that can handle
domain specificity and multiple corpora.
The remaining parts of this paper are orga-
nized as follows. In ?2, we explain the problem
setting in detail. We first explain how existing
heuristic word sampling works and how it relies
on the cluster assumption from the viewpoint of
graphs. Then, we introduce existing graph-based
non-interactive active learning methods. In ?3,
we show that the existing heuristic word sampling
is merely a special case of a non-interactive ac-
tive learning method (Gu and Han, 2012). Pre-
cisely, the existing sampling is identical to the case
where a special graph called a ?multi-complete
graph? is fed to a non-interactive active learning
method. Since this method can take any weighted
graphs other than this special graph, this imme-
diately leads to a way of devising new sampling
methods by modifying graphs. ?4 explains exactly
how we can modify graphs for improving active
learning. ?5 evaluates the proposed method both
quantitatively and qualitatively, and ?6 concludes
our paper.
2 Problem Setting
2.1 Heuristic Word Sampling
A simple vocabulary estimation technique intro-
duced by educational experts is to use the fre-
quency rank of words in a corpus based on the
assumption that learners using words with similar
frequency ranks have a similar vocabulary (Laufer
and Nation, 1999). In accordance with this as-
sumption, they first group words by frequency
ranks in a corpus and then assume that words in
each group have a similar vocabulary status. For
example, they sampled words as follows:
1375
1. Rank words by frequency in a corpus.
2. Group words with frequency ranks from 1 to
1, 000 as Level 1000, words with frequency
ranks from 1, 001 to 2, 000 as Level 2000,
and so on.
3. Take 18 samples from Level 1000, another 18
samples from Level 2000, and so on.
The rationale behind this method is to treat
high-ranked and low-ranked words separately
rather than sample words from the entire vocabu-
lary. After sampling words, this sampling method
can be used for various measurements; for exam-
ple, Laufer and Nation (1999) used this method
to estimate the size of the learners? vocabulary
by simply adding 1, 000 ?
Correctly answered words
18
for
each level.
2.2 Cluster Assumption
In the previous subsection, we noted that existing
word sampling methods rely on the assumption
that words with similar frequency ranks are known
to learners whose familiar words are similar each
other. This assumption is known as the cluster as-
sumption in the field of graph studies (Zhou et al.,
2004).
To further describe the cluster assumption, we
first define graphs. A graph G = (V, E) consists
of a set of nodes (vertices) V and a set of edges E .
Here, each node has a label, and each edge has a
weight. A label denotes the category of its corre-
sponding node. For example, in binary classifica-
tion, a label is taken from {+1,?1}. A weight is
a real value; when the weight of an edge is large,
we describe the edge as being heavy.
The cluster assumption is an assumption that
heavily connected nodes in a graph should have
similar labels. In other words, the cluster as-
sumption states that weights of edges and labels
of nodes should be consistent.
We explain how the cluster assumption relates
to our task. In our application, each node corre-
sponds to a word. Labels of the nodes in a graph
denote the vocabulary of a learner. If he/she knows
a word, the label of the node corresponding to the
word is +1; if not, the label is ?1. The cluster
assumption in our application is that the heavier
the edge, the higher the similarity between users
familiar with the two words.
In this manner, existing word sampling meth-
ods implicitly assume cluster assumption. This
is therefore the underlying approach for reducing
the word sampling problem into graph-based ac-
tive learning. Since graphs allow for more flexible
modeling by changing the weights of edges, we
expect that more flexible word sampling will be
enabled by graph-based active learning.
2.3 Label Propagation
Since the graph-based active learning algorithms
are based on label propagation algorithms, we will
explain them first. Basically, given a weighted
graph, label propagation algorithms classify their
nodes in a weakly supervised manner. While the
graph-based active learning algorithm that we are
trying to use (Gu and Han, 2012) does not use la-
bel propagation algorithms? outputs directly, it is
tuned to be used with a state-of-the-art label prop-
agation method called Learning with Local and
Global Consistency (LLGC) (Zhou et al., 2004).
Label propagation algorithms predict the labels
of nodes from a few manually supervised labels
and graph weights. To this end, label propaga-
tion algorithms follow the following steps. First,
humans label a small subset of the nodes in the
graph. This subset of nodes is called the set of la-
beled nodes, and the remaining nodes are called
unlabeled nodes. Second, label propagation al-
gorithms propagate labels to the unlabeled nodes
based on edge weights. The rationale behind la-
bel propagation algorithms lies in cluster assump-
tion; as label propagation algorithms assume that
two nodes connected by a heavily weighted edge
should have similar labels, more heavily weighted
edges should propagate more labels.
We formalize Learning with Local and Global
Consistency (LLGC) (Zhou et al., 2004), one
of the state-of-the-art label propagation methods.
Here, for simplicity, suppose that we want to per-
form binary classification of nodes. Let N be the
total number of nodes in a graph. Then, we de-
note labels of each node by y
def
= (y
1
, . . . , y
N
)
>
.
For unlabeled nodes, y
i
is set to 0. For labeled
nodes, y
i
is set to +1 if the learner knows a word,
?1 if not. We also introduce a label propagation
(LP) score vector f = (f
1
, . . . , f
N
)
>
. This LP
score vector is the output of label propagation and
is real-valued. To obtain the classification result
from this real-valued LP score vector for an un-
labeled node (word) i, the learner is predicted to
know the word i if f
i
> 0, and he/she is predicted
to be unfamiliar with the word if f
i
? 0.
1376
Next, we formally define a normalized graph-
Laplacian matrix, which is used for penalization
based on the cluster assumption. Let an N ? N
-sized square matrix W be a weighted adjacency
matrix of G. W is symmetric and non-negative
definite; its diagonal elements W
i,i
= 0 and
all other elements are non-negative
1
. The graph
Laplacian of a normalized graph, known as a nor-
malized graph Laplacian matrix, is defined as
L
norm
W
def
= I?D
?
1
2
W
WD
?
1
2
W
. Here, D
W
is defined
as a diagonal matrix whose diagonal element is
(D
W
)
i,i
def
=
?
|V|
j=1
W
i,j
, and I denotes the iden-
tity matrix of the appropriate size. Note that a
normalized graph Laplacian L
norm
W
depends on the
weighted adjacency matrix W.
Then, LLGC can be formalized as a simple op-
timization problem as shown in Equation 1.
min
f
?f ? y?
2
2
+ ?f
>
L
norm
W
f (1)
Equation 1 consists of two terms. Intuitively,
the first term tries to make the LP score vector, the
final output f , as close as possible to the given la-
bels y. The second term is designed to meet the
cluster assumption: it penalizes the case where
two nodes with heavy edges have very different
LP scores. ? > 0 is the only hyper-parameter of
LLGC: it determines how strong the penalization
based on the cluster assumption should be. Thus,
in total, Equation 1 outputs an LP score vector f
considering both the labeled input y and the clus-
ter assumption of the given graph W: the heav-
ier an edge, the closer the scores of the two nodes
connected by the edge becomes.
2.4 Graph-based active learning algorithms
An important categorization of graph-based active
learning for applications is whether it is interactive
or non-interactive. Here, interactive approaches
use human labels during the learning process; they
present a node for humans to label, and based on
this label, the algorithms compute the next node to
be presented to the humans. Thus, in interactive
algorithms, human labeling and computations of
the next node must run concurrently.
Non-interactive algorithms do not use human
labels during the learning process. Given the
entire graph, these algorithms sample important
1
While all elements of a non-negative definite matrix are
not necessarily non-negative, we define all elements of W
as non-negative here, following the definition of Zhou et al.
(2004).
nodes for label propagation algorithms. Here, im-
portant nodes are the ones that minimize estimated
classification error of label propagation when the
nodes are labeled. Note that, unlike active learning
used in the NLP community, non-interactive active
learning algorithms exclude expert annotators? hu-
man labels from the protocol. While they exclude
expert annotators, they are still regarded as active
learning methods in the machine learning commu-
nity since they try to choose such nodes that are
beneficial for classification (Ji and Han, 2012; Gu
and Han, 2012).
For educational purposes, non-interactive algo-
rithms are preferred over interactive algorithms.
The main drawback of interactive algorithms is
that they must run concurrently with the hu-
man labeling. For our applications, this means
that the vocabulary tests for vocabulary prediction
must always be computerized. In contrast, non-
interactive algorithms allow us to have vocabulary
tests printed in the form of handouts, so we focus
on non-interactive algorithms throughout this pa-
per.
Compared with interactive algorithm studies,
such as Zhu et al. (2003), graph-based non-
interactive active learning algorithms have been
introduced in recent years. There has been a sem-
inal paper on non-interactive algorithms (Ji and
Han, 2012). We used Gu and Han?s algorithm be-
cause it reports higher accuracy for many tasks
with competitive computation times over Ji and
Han?s algorithm (Gu and Han, 2012).
These active learning methods share two basic
rules although their objective functions are dif-
ferent. First, these methods tend to select glob-
ally important nodes, also known as hubs. A no-
table example of global importance is the num-
ber of edges. Second, these methods tend to
avoid sampling nodes that are heavily connected
to previously sampled nodes. This is due to clus-
ter assumption, the assumption that similar nodes
should have similar labels, which suggests that it is
redundant to select nodes close to previously sam-
pled nodes; the labels of such nodes should be reli-
ably predicted from the previously sampled nodes.
Gu and Han?s algorithm, which is the algorithm
we used, also follows these rules. In this algo-
rithm, when considering the k-th sample, for every
node i in the current set of not-yet-chosen nodes, a
score score(k, i) is calculated, and the node with
the highest score is chosen. First, the score is de-
1377
signed to be large if the i-th node is globally im-
portant. In the algorithm, the global importance
of a node is measured by an eigenvalue decompo-
sition of the normalized graph-Laplacian, L
norm
.
Transformed from the graph?s adjacency matrix,
this matrix stores the graph?s global information.
Second, the score is designed to be smaller if the
i-th node is close to one of the previously sampled
nodes.
Score score (k, i) is defined as follows. We
perform eigenvalue decomposition beforehand.
L
norm
W
= U?U
>
, u
i
is the transpose of the i-th
row of U, and ?
i
is its corresponding eigenvalue.
score (k, i)
def
=
(
H
?1
k
u
i
)
>
?
?1
(
H
?1
k
u
i
)
1 + u
>
i
H
?1
k
u
i
(2)
In Equation 2, H
k
preserves information of the
previous k ? 1 samples. First, H
0
is a diag-
onal matrix whose i-th diagonal element is de-
fined as
1
(??
i
+1)
2
?1
where ? is a hyper-parameter.
H
0
weighs the score of globally important nodes
through the eigenvalue decomposition. Second,
H
k
is updated such that the scores of the nodes
distant from the previously taken samples are
higher. The precise update formula of H
k
follows.
i
k+1
is the index of the node sampled at k + 1-th
round. For the derivation of this formula, see Gu
and Han (2012).
H
?1
k+1
= H
?1
k
?
(
H
?1
k
u
i
k+1
) (
H
?1
k
u
i
k+1
)
>
1 + u
>
i
k+1
H
?1
k
u
i
k+1
(3)
Hyper-parameter ? determines how strong the
cluster assumption should be; the larger the value,
the more strongly the algorithm avoids selecting
nodes near previously selected samples over the
graph. Note that ? is inherited from the LLGC
2
algorithm (Zhou et al., 2004), i.e., the label prop-
agation algorithm that Gu and Han?s algorithm is
based on. From the optimization viewpoint, ? de-
termines the degree of penalization.
Remember that the score has nothing to do with
the LP scores described in ?2.3. score is used
to choose nodes used for training in the graph-
based non-interactive active learning. LP scores
are later used for classification by label propaga-
tion algorithms that use the chosen training nodes.
Throughout this paper, when we mean LP scores,
we explicitly write ?LP scores?. All the other
scores mean score.
2
Learning with Local and Global Consistency.
Figure 1: Converting frequency list into multiple-
complete graph.
3 Formalizing heuristic word sampling
as graph-based active learning
Figure 1 shows how to formalize a word frequency
list into a multiple complete graph. The word fre-
quency list is split into clusters, and each cluster
forms a complete graph. Each node in a graph cor-
responds to a word. By gathering all the complete
graphs, a multiple complete graph can be formed.
Multiple complete graph G
T,n
is defined as a
graph of T complete graphs, each of which con-
sists of n nodes fully connected within the n
nodes. An example of a multiple complete graph
can be seen in Figure 2. We can define the
Tn ? Tn adjacency matrix for multiple com-
plete graphs. W
complete
all
is defined as follows:
W
complete
all
def
=
?
?
?
?
?
?
W
complete
0 ? ? ? 0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0 ? ? ? 0 W
complete
?
?
?
?
?
?
(4)
W
complete
def
=
?
?
?
?
?
?
?
?
0 1 ? ? ? 1 1
1 0 1 ? ? ? 1
.
.
. 1
.
.
.
.
.
.
1
.
.
.
.
.
.
1
1 1 ? ? ? 1 0
?
?
?
?
?
?
?
?
(5)
We can see that W
complete
all
is a block-diagonal
matrix where each block is a n ? n matrix,
W
complete
.
Heuristic word sampling can be rewritten into
non-interactive active learning on graphs. Suppose
there are T groups, each of which has n words,
and we want to sample n
0
words from each. In
1378
Figure 2: Example of multi-complete graph, where Theorem 3.1 holds true. Here, T = 4, n = 5, and
k = 10; 10 light blue (light) nodes have already been sampled, and 10 blue (dark) nodes remain; the
11-th node is sampled uniformly randomly from the nodes within the red rectangles.
heuristic word sampling, for each group from T
groups, n
0
words are sampled from the n words
in the group uniformly randomly. Thus, there are
Tn
0
words in total.
Since heuristic word sampling takes a node
from each of the T groups, T concurrent sampling
processes are involved. For simplicity, we further
express the same sampling using only one sam-
pling process from the entire graph as follows:
? For every round, we sample words uniformly
randomly from the remaining words of the
groups where the number of samples selected
in previous rounds is least.
Figure 2 shows an example of this sampling
process. Here, the second and third groups from
the left are the groups in which the number of pre-
viously selected nodes is the least. This is because
they have only two previously selected nodes,
while the others have three. Thus, in the figure, the
remaining words of the groups are the nodes with
red rectangles. Randomly sampling one node from
the nodes with red rectangles means sampling a
node from the second or third group. We call the
set of nodes in a graph from which samples will be
taken in the next round a seed pool. Thus, in Fig-
ure 2, the set of nodes with red rectangles is the
seed pool. Nodes that have already been sampled
are taken out of the current seed pool.
Next, we more formally explain the seed pool
concept. We start sampling nodes from a multiple
complete graph via the algorithm presented by Gu
and Han. The initial seed pool is set to all nodes
in the graph, i.e., V . We sample one node in each
round; thus, k ? |V| nodes are selected by the k-
th round. Let t ? T be the index of the complete
graph in the multiple complete graph. Then, the
following theorem holds with  being a small pos-
itive value that substitutes the 0 eigenvalues in the
eigen decomposition.
Theorem 3.1 Let 0 <  < 1 and n ?
{2, 3, 4, . . .}. Then, among T complete graphs,
k mod T complete graphs have b
k
T
c + 1 sam-
ples, and the remaining graphs have b
k
T
c sam-
ples
3
. Moreover, the (k + 1)-th sample is taken
uniformly randomly from the remaining complete
graphs.
In Theorem 3.1,  > 0 is a substitute for the
0 eigenvalue of L
W
4
. Since  is a substitute for
the 0 eigenvalue, it is rational to assume 1 > .
Also, remember that n is the number of nodes in
one complete graph. The algorithm stops when
k = Tn
0
+ 1, i.e., at the Tn
0
+ 1-th round when
there are no remaining nodes to sample. Figure 2
shows an example of Theorem 3.1.
A proof of this theorem is presented in the sup-
plementary material. Briefly, in a multiple com-
plete graph, the score of a node depends only on
the complete graph or the cluster that the node
belongs to. Thus, we only have to consider one
complete graph in which k is the number of nodes
that have been already chosen. Then, mathe-
matical induction proves that, within one com-
plete graph, all the not-yet-chosen nodes have the
same score(k, i). Second, we have to show that
the score always decreases by taking a sample,
i.e., score(k, i) > score(k + 1, i). By a long
but straightforward calculation, we can express
score(k, i) by using only ?, , n, and k. Then, by
substituting the formula to score(k, i), we obtain
score(k, i)? score(k + 1, i) > 0.
4 Extending Graphs
In the previous section, we explained how to for-
malize heuristic word sampling as active learn-
ing on multiple complete graphs. This formaliza-
3
Here, both k and T are non-negative integers. Thus,
k%T denotes the remainder of the division of k by T , and
b
k
T
c is the quotient of the division.
4
In Gu and Han?s algorithm, they substitute the 0 eigen-
value with a small positive value , and they set  = 10
?6
.
1379
Figure 3: Example of merging two graphs.
tion can lead to better active learning by extend-
ing these graphs. In this section, we describe such
graph extensions.
We extend graphs by merging graphs. Figure 3
shows how to merge graphs. We define ?merging?
two weighted graphs as creating a weighted graph
whose adjacency matrix is the sum of the two ad-
jacency matrices of the two weighted graphs. This
suggests that an edge of the merged graph is sim-
ply the sum of the corresponding edges of the two
weighted graphs.
The merged graph is expected to inherit the
characteristics of its original graphs. Thus, ap-
plying graph-based active learning to the merged
graph is expected to sample nodes in accordance
with the characteristics of its original graphs.
For example, if we merge a graph representing
domain-specific relations and a multiple complete
graph representing difficulty grouping of words,
active learning from the resulting merged graph
is expected to sample words considering both do-
main specificity and difficulty grouping of words.
For another example, suppose we merge two
multiple complete graphs created from frequency
lists from two different corpora. Then, active
learning from the resulting merged graph is ex-
pected to sample words taking into account fre-
quency lists from both corpora.
5 Evaluation
We evaluate our proposed method both quantita-
tively and qualitatively. In the quantitative eval-
uation, we measure the prediction accuracy of
graphs. Note that the heuristic word sampling
method is identical to using Gu and Han?s algo-
rithm with a multiple complete graph; however,
our proposed graphs have enriched relations be-
tween words. In the qualitative evaluation, we ex-
plain in detail what words are appropriate as train-
ing examples for vocabulary prediction by pre-
senting sampled examples.
5.1 Quantitative evaluation
To evaluate the accuracy of vocabulary prediction,
we used the dataset that Ehara et al. (2010) and
Ehara et al. (2012) used. This dataset was gleaned
from questionnaires answered by 15 English as a
second language (ESL) learners. Every learner
was asked to answer how well he/she knew 11,999
English words. The data was collected in January
2009. One learner was unpaid, whereas the other
15 learners were paid. We used the data from the
15 paid learners since the data from the unpaid
learner was noisy. Most of the learners were na-
tive Japanese speakers and graduate students. Be-
cause most of the learners in this dataset were na-
tive Japanese speakers, words from SVL 12,000
(SPACE ALC Inc., 1998) were used for the learn-
ers in this dataset. Note that SVL 12,000 is a col-
lection of 12,000 words that are deemed important
for Japanese learners of English, as judged by na-
tive English teachers.
Next, we required frequency lists for the words
that appeared in the dataset. To create frequency
lists, lemmatization is important because the num-
ber of word types depends on the method used
to lemmatize the words. Note that in the field of
vocabulary measurement, lemmatization is mainly
performed by ignoring conjugation (Nation and
Beglar, 2007). Lemmatizing the dataset resulted
in a word list of 8,463 words. We adjusted the size
of the word list to a round 8,000 by removing 463
randomly chosen words. Note that all constituent
words were labeled by the 15 ESL learners.
We created the following four graphs by span-
ning edges among the 8, 000 words.
BNC multi-complete This graph corresponds to
heuristic word sampling and served as our
baseline. It is a multiple complete graph
comprising eight complete graphs, each of
which consisted of 1,000 words based on the
sorted frequency list from the British Na-
tional Corpus (BNC). We chose the BNC be-
cause the method presented by Nation and
Beglar was based on it (Nation and Beglar,
2007). Note that all edge weights are set to 1.
BNC+domain To form this graph, edges rep-
resenting domain specificity are added to
the ?BNC multi-complete? graph. For do-
main specificity, we used domain information
1380
from WordNet 3.0.
5
First, we extracted 102
domain-specific words under the ?computer?
domain among the 8,000 words and created
a complete graph consisting of these domain-
specific words. The edge weights of the com-
plete graph were set to 1. Next, we simply
merged
6
the complete graph consisting of the
domain-specific words with the ?BNC multi-
complete? graph.
BNC+COCA In addition to the ?BNC multi-
complete? graph, edges based on another cor-
pus, the Corpus of Contemporary American
English (COCA), were introduced. We first
created the COCA multi-complete graph, a
multiple complete graph consisting of eight
complete graphs, each of which consisted of
1,000 words based on the sorted frequency
list using COCA. The edge weights of the
COCA multi-complete graph were set to 1.
Next, we merged the BNC multi-complete
and COCA multi-complete graphs to form
the ?BNC + COCA graph?.
BNC+domain+COCA This graph is the graph
produced by merging the ?BNC + domain?
and ?BNC + COCA? graphs.
Note that our experiment setting differed from
the usual label propagation setting used for semi-
supervised learning because the purpose of our
task differed. In the usual label propagation set-
ting, the ?test? nodes (data) are prepared sepa-
rately from the training nodes to determine how
accurately the algorithm can classify forthcoming
or unseen nodes. However, in our setting, there
were no such forthcoming words. Of course, there
will always be words that do not emerge, even in a
large corpus; however, such rare words are too dif-
ficult for language learners to identify, and many
are proper nouns, which are not helpful for mea-
suring the vocabulary of second language learners.
Therefore, our focus here is to measure how
well the learners know a fixed set of words, that
is, the given 8,000 words. Even if an algorithm
can achieve high accuracy for words outside this
fixed set, we have no way of evaluating it using
the pooled annotations. Here, we want to measure,
from a fixed number of samples (e.g., 50), how ac-
curately an algorithm can predict a learner?s vo-
5
We used the NLTK toolkit http://nltk.org/ to extract the
domain information.
6
Definition of how to merge two graphs is in ?4.
Figure 4: Results of our quantitative experiments.
Vertical axis denotes accuracy, and horizontal axis
shows number of samples, i.e., training words.
cabulary for the entire 8,000 words. Thus, we
define accuracy to be the number of words that
each algorithm finds correctly divided by the vo-
cabulary size. We set hyper-parameter ? to 0.01
as Gu and Han (2012) did. Note that this hyper-
parameter is reportedly not sensitive to accuracy
(Zhou et al., 2011).
Figure 4 and Table 1 show the results of the
experiment over the different datasets. The ver-
tical axis in the figure denotes accuracy, whereas
the horizontal axis denotes the number of samples,
i.e., training words. Note that the accuracy is av-
eraged over 15 learners and that LLGC is used for
classification unless otherwise specified. For ex-
ample, ?BNC multi-complete? indicates that sam-
ples taken from the BNC multi-complete graph are
used for training, and LLGC is used for classifica-
tion. Note that ?BNC + domain + COCA (SVM)?
uses a support vector machine (SVM) for classifi-
cation, and ?BNC + domain + COCA (LR)? uses
logistic regression (LR) for classification. Among
many supervised machine learning methods, we
chose SVM and LR because SVM is widely used
in the NLP community, and LR was used for the-
oretical reasons (Ehara et al., 2012; Ehara et al.,
2013).
SVM and LR require features of a word
for classification while LLGC requires a
weighted graph of words. Since the graph
?BNC+domain+COCA? is made from three
features, namely the word frequencies of BNC
1381
Table 1: Results of our quantitative experiments. LLGC is used for classification unless otherwise spec-
ified. Bold letters indicate top accuracy. Asterisks (*) indicate that values are statistically significant
against baseline, heuristic sampling, i.e., ?BNC multi-complete? (using sign test p < 0.01).
10 15 20 30 40 50
BNC multi-complete 64.15 (%) 67.54 73.73 73.66 74.92 74.82
BNC+domain 65.27 71.88 72.88 75.02 76.03 * 75.95
BNC+COCA 73.45 74.10 74.57 74.90 74.96 75.29
BNC+domain+COCA 75.23 * 75.71 * 75.18 * 75.35 * 75.47 76.44 *
BNC+domain+COCA (SVM) 58.99 57.74 60.44 70.79 69.29 74.46
BNC+domain+COCA (LR) 60.29 61.74 59.27 69.17 70.63 73.42
and COCA corpora and whether a word is in the
computer domain, we used these features for the
features of SVM and LR in this experiment for a
fair comparison. When using word frequencies for
features, we used the logarithm of raw frequencies
since it is reported to work well (Ehara et al.,
2013). SVM and LR are also known to heavily
depend on a hyper-parameter called C, which
determines the strength of regularization. We
tried C = 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0,
and 100.0 for each of SVM and LR where the size
of training data is 50 and chose the C value that
performs best. As a result, we set C = 5.0 for
SVM and C = 50.0 for LR. Note that this setting
is advantageous for SVM and LR compared to
LLGC because the hyper-parameters of SVM
and LR are tuned while LLGC?s hyper-parameter
remains untuned. For the implementation of SVM
and LR, we used the ?scikit-learn? package in
Python
7
.
We first observed that our proposed methods
constantly outperform the baseline, heuristic word
sampling, i.e., ?BNC multi-complete? in Table 1.
This indicates that we successfully obtained bet-
ter accuracy by formalizing heuristic word sam-
pling as active learning and extending graphs. In
Table 1, the accuracy of the top-ranked methods
(shown using bold letters) is statistically signif-
icantly better than the accuracy of ?BNC multi-
complete? (using the sign test p < 0.01).
We then observed that ?BNC multi-complete?
and ?BNC + domain? show competitive accuracy
with sample sizes from 10 to 20; furthermore,
?BNC + domain? is slightly better than ?BNC
multi-complete? with sample sizes ranging from
30 to 50 (statistically significant p < 0.01 using
sign test). Next, we note that there is a trade-off
between domain and word frequency when choos-
7
http://scikit-learn.org/stable/
ing samples. More specifically, if we select too
many words from the domain, the measurement of
the general English ability of learners can be in-
accurate; conversely, if we select too many words
from the corpus-based word frequency list, while
the general English ability of learners is accu-
rately measured, we may obtain no information
on the learner?s vocabulary for the targeted do-
main. The competitive or slightly better accuracy
of ?BNC + domain? over ?BNC multi-complete?
shows that ?BNC + domain? could successfully
integrate domain information into the frequency-
based groups without deteriorating measurements
of general English ability.
We also observe that ?BNC + COCA? greatly
outperforms ?BNC multi-complete? when the
number of samples is 10. This shows that the inte-
gration of the two corpora, BNC and COCA (i.e.,
?BNC + COCA?), successfully increases the accu-
racy when there are only a small number of sam-
ples.
?BNC + domain + COCA? achieves the best ac-
curacy of all the graphs except when the number
of samples is 40. This indicates that the domain
information and the information from the COCA
corpus helped one another to improve the accuracy
because ?BNC + domain? and ?BNC + COCA? in-
troduce different types of domain information into
?BNC multi-complete.?
Finally, we observe that ?BNC + domain +
COCA (SVM)? and ?BNC + domain + COCA
(LR)? perform worse than LLGC over the same
dataset for all sample sizes, particularly when the
size of the training data is small. Since LLGC is
a semi-supervised classifier while SVM and LR
are not, SVM and LR perform poorly for small
amounts of training data. This result shows that
LLGC is appropriate for this task compared to
SVM because, in this task, an increase in the size
1382
Table 2: Computer-related samples in top 30 sam-
ples.
Name Num. of
Samples
Examples
BNC multi-
complete
0 -
BNC+domain 5 input, client, field,
background, regis-
ter
BNC+COCA 0 -
BNC+domain
+COCA
3 drive, client, com-
mand
of training data directly leads to an increased bur-
den on the human learners.
5.2 Qualitative evaluation
In this subsection, we qualitatively evaluate our
results to determine the types of nodes that are
sampled when domain specificity is introduced.
Specifically, we evaluate what words are selected
as samples in the ?BNC + domain? graph.
As noted above, in the ?BNC + domain? graph,
the computer science domain is introduced into
?BNC multi-complete? to measure learners? vo-
cabulary with a specific emphasis on the computer
science domain. Thus, it is desirable that some
words in the computer science domain are sam-
pled from the ?BNC + domain? graph; otherwise,
we need to predict the learners? vocabulary for
the computer science domain from general words
rather than those in the computer science domain,
which is extremely difficult.
Table 2 shows the number of words in the com-
puter science domain sampled in the first 30 sam-
ples. Note that only ?BNC + domain? and ?BNC
+ domain + COCA? select samples from the com-
puter science domain. This indicates that in the
other two methods, to measure vocabulary with
an emphasis on the computer science domain, we
need to predict learners? vocabulary from the gen-
eral words, which is almost impossible with only
30 samples. Furthermore, it is interesting to note
that ?BNC + domain? and ?BNC + domain +
COCA? select different samples from the com-
puter science domain, except for the word ?client,?
although originally the same computer science do-
main wordlist was introduced to both graphs.
Since ?BNC + domain? achieves competitive
or slightly better accuracy than ?BNC multi-
complete? in the quantitative analysis and the
qualitative analysis, we conclude that our method
can successfully introduce domain specificity into
the sampling methodology without reducing accu-
racy.
6 Conclusion
In this study, we propose a novel sampling frame-
work that measures the vocabulary of second lan-
guage learners. We call existing sampling meth-
ods heuristic sampling. This approach to sampling
ranks words from a single corpus by frequency and
creates groups of 1,000 words. Next, tens of words
are sampled from each group. This method as-
sumes that the relative difficulty of all 1,000 words
is the same.
In this paper, we introduce a novel sampling
method by showing that the existing heuristic sam-
pling approach is simply a special case of a graph-
based active learning algorithm by Gu and Han
(2012) applied to a special graph. We also pro-
pose a method to extend this graph to enable us to
handle domain specificity of words and multiple
corpora, which are difficult or impossible to han-
dle using current methods.
We evaluate our method both quantitatively and
qualitatively. In our quantitative evaluation, the
proposed method achieves higher prediction accu-
racy compared with the current approach to vo-
cabulary prediction. This suggests that our pro-
posed method can successfully make use of do-
main specificity and multiple corpora for pre-
dicting vocabulary. In our qualitative evaluation,
we examine the words sampled by our proposed
method and observe that targeted domain-specific
words are successfully sampled.
For our future work, because the graph used
in this paper was constructed manually, we plan
to automatically create a graph suitable for active
learning and classification. There are several algo-
rithms that create graphs from feature-based rep-
resentations of words, but these have never been
used for active learning of this task.
Acknowledgments
This work was supported by the Grant-in-Aid for
JSPS Fellows (JSPS KAKENHI Grant Number
12J09575).
1383
References
Yo Ehara, Nobuyuki Shimizu, Takashi Ninomiya, and
Hiroshi Nakagawa. 2010. Personalized reading
support for second-language web documents by col-
lective intelligence. In Proceedings of the 15th in-
ternational conference on Intelligent user interfaces
(IUI 2010), pages 51?60, Hong Kong, China. ACM.
Yo Ehara, Issei Sato, Hidekazu Oiwa, and Hiroshi Nak-
agawa. 2012. Mining words in the minds of second
language learners: learner-specific word difficulty.
In Proceedings of the 24th International Confer-
ence on Computational Linguistics (COLING 2012),
Mumbai, India, December.
Yo Ehara, Nobuyuki Shimizu, Takashi Ninomiya, and
Hiroshi Nakagawa. 2013. Personalized reading
support for second-language web documents. ACM
Transactions on Intelligent Systems and Technology,
4(2).
Quanquan Gu and Jiawei Han. 2012. Towards active
learning on graphs: An error bound minimization
approach. In Proceedings of the IEEE International
Conference on Data Mining (ICDM) 2012.
Ming Ji and Jiawei Han. 2012. A variance minimiza-
tion criterion to active learning on graphs. In Pro-
ceedings of the 15th international conference on Ar-
tificial Intelligence and Statistics (AISTATS).
Batia Laufer and Paul Nation. 1999. A vocabulary-
size test of controlled productive ability. Language
testing, 16(1):33?51.
Paul Meara and Barbara Buxton. 1987. An alterna-
tive to multiple choice vocabulary tests. Language
Testing, 4(2):142?154.
Paul Nation and David Beglar. 2007. A vocabulary
size test. The Language Teacher, 31(7):9?13.
SPACE ALC Inc. 1998. Standard vocabulary list
12,000.
Dengyong Zhou, Oliver Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Sch?olkopf. 2004.
Learning with local and global consistency. In Pro-
ceedings in 18th Annual Conference on Neural In-
formation Processing Systems (NIPS), pages 321?
328.
Xueyuan Zhou, Mikhail Belkin, and Nathan Srebro.
2011. An iterated graph laplacian approach for rank-
ing on manifolds. In Proceedings of 17th ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD), pages 877?885.
Xiaojin Zhu, John Lafferty, and Zoubin Ghahra-
mani. 2003. Combining active learning and semi-
supervised learning using gaussian fields and har-
monic functions. In Proceedings of ICML 2003
workshop on The Continuum from Labeled to Unla-
beled Data in Machine Learning and Data Mining.
1384
Proceedings of the TextGraphs-8 Workshop, pages 44?52,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Understanding seed selection in bootstrapping
Yo Ehara?? Issei Sato?
? Graduate School of Information Science and Technology ? Information Technology Center
The University of Tokyo / 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan
? JSPS Research Fellow
Kojimachi Business Center Building, 5-3-1 Kojimachi, Chiyoda-ku, Tokyo, Japan
{ehara@r., sato@r., oiwa@r., nakagawa@}dl.itc.u-tokyo.ac.jp
Hidekazu Oiwa?? Hiroshi Nakagawa?
Abstract
Bootstrapping has recently become the focus
of much attention in natural language process-
ing to reduce labeling cost. In bootstrapping,
unlabeled instances can be harvested from the
initial labeled ?seed? set. The selected seed set
affects accuracy, but how to select a good seed
set is not yet clear. Thus, an ?iterative seed-
ing? framework is proposed for bootstrapping
to reduce its labeling cost. Our framework
iteratively selects the unlabeled instance that
has the best ?goodness of seed? and labels the
unlabeled instance in the seed set. Our frame-
work deepens understanding of this seeding
process in bootstrapping by deriving the dual
problem. We propose a method called ex-
pected model rotation (EMR) that works well
on not well-separated data which frequently
occur as realistic data. Experimental results
show that EMR can select seed sets that pro-
vide significantly higher mean reciprocal rank
on realistic data than existing naive selection
methods or random seed sets.
1 Introduction
Bootstrapping has recently drawn a great deal of
attention in natural language processing (NLP) re-
search. We define bootstrapping as a method for
harvesting ?instances? similar to given ?seeds? by
recursively harvesting ?instances? and ?patterns? by
turns over corpora using the distributional hypothe-
sis (Harris, 1954). This definition follows the def-
initions of bootstrapping in existing NLP papers
(Komachi et al, 2008; Talukdar and Pereira, 2010;
Kozareva et al, 2011). Bootstrapping can greatly
reduce the cost of labeling instances, which is espe-
cially needed for tasks with high labeling costs.
The performance of bootstrapping algorithms,
however, depends on the selection of seeds. Al-
though various bootstrapping algorithms have been
proposed, randomly chosen seeds are usually used
instead. Kozareva and Hovy (2010) recently reports
that the performance of bootstrapping algorithms
depends on the selection of seeds, which sheds light
on the importance of selecting a good seed set. Es-
pecially a method to select a seed set considering
the characteristics of the dataset remains largely un-
addressed. To this end, we propose an ?iterative
seeding? framework, where the algorithm iteratively
ranks the goodness of seeds in response to current
human labeling and the characteristics of the dataset.
For iterative seeding, we added the following two
properties to the bootstrapping;
? criteria that support iterative updates of good-
ness of seeds for seed candidate unlabeled in-
stances.
? iterative update of similarity ?score? to the
seeds.
To invent a ?criterion? that captures the character-
istics of a dataset, we need to measure the influence
of the unlabeled instances to the model. This model,
however, is not explicit in usual bootstrapping algo-
rithms? notations. Thus, we need to reveal the model
parameters of bootstrapping algorithms for explicit
model notations.
To this end, we first reduced bootstrapping al-
gorithms to label propagation using Komachi et al
44
(2008)?s theorization. Komachi et al (2008) shows
that simple bootstrapping algorithms can be inter-
preted as label propagation on graphs (Komachi
et al, 2008). This accords with the fact that
many papers such as (Talukdar and Pereira, 2010;
Kozareva et al, 2011) suggest that graph-based
semi-supervised learning, or label propagation, is
another effective method for this harvesting task.
Their theorization starts from a simple bootstrap-
ping scheme that can model many bootstrapping al-
gorithms so far proposed, including the ?Espresso?
algorithm (Pantel and Pennacchiotti, 2006), which
was the most cited among the Association for Com-
putational Linguistics (ACL) 2006 papers.
After reducing bootstrapping algorithms to label
propagation, next, we will reveal the model param-
eters of a bootstrapping algorithm by taking the
dual problem of bootstrapping formalization of (Ko-
machi et al, 2008). By revealing the model param-
eters, we can obtain an interpretation of selecting
seeds which helps us to create criteria for the iter-
ative seeding framework. Namely, we propose ex-
pected model rotation (EMR) criterion that works
well on realistic, and not well-separated data.
The contributions of this paper are summarized as
follows.
? The iterative seeding framework, where seeds
are selected by certain criteria and labeled iter-
atively.
? To measure the influence of the unlabeled in-
stances to the model, we revealed the model
parameters through the dual problem of boot-
strapping.
? The revealed model parameters provides an in-
terpretation of selecting seeds focusing on how
well the dataset is separated.
? ?EMR? criterion that works well on not well-
separated data which frequently occur as real-
istic data. .
2 Related Work
Kozareva and Hovy (2010) recently shed light on
the problem of improving the seed set for bootstrap-
ping. They defined several goodness of seeds and
proposed a method to predict these measures using
support vector regression (SVR) for their doubly an-
chored pattern (DAP) system. However, Kozareva
and Hovy (2010) does not show how effective the
seed set selected by the goodness of seeds that they
defined was for the bootstrapping process while they
show how accurately they could predict the good-
ness of seeds.
Early work on bootstrapping includes that of
(Hearst, 1992) and that of (Yarowsky, 1995). Abney
(2004) extended self-training algorithms including
that of (Yarowsky, 1995), forming a theory different
from that of (Komachi et al, 2008). We chose to ex-
tend the theory of (Komachi et al, 2008) because it
can actually explain recent graph-based algorithms
including that of (Pantel and Pennacchiotti, 2006).
The theory of Komachi et al (2008) is also newer
and simpler than that of (Abney, 2004).
The iterative seeding framework can be regarded
as an example of active learning on graph-based
semi-supervised learning. Selecting seed sets cor-
responds to sampling a data point in active learn-
ing. In active learning on supervised learning, the
active learning survey (Settles, 2012) includes a
method called expected model change, after which
this paper?s expected model rotation (EMR) is
named. They share a basic concept: the data
point that surprises the classifier the most is selected
next. Expected model change mentioned by (Settles,
2012), however, is for supervised setting, not semi-
supervised setting, with which this paper deals. It
also does not aim to provide intuitive understanding
of the dataset. Note that our method is for semi-
supervised learning and we also made the calcula-
tion of EMR practical.
Another idea relevant to our EMR is an ?an-
gle diversity? method for support vector machines
(Brinker, 2003). Unlike our method, the angle diver-
sity method interprets each data point as data ?lines?
in a version space. The weight vector is expressed
as a point in a version space. Then, it samples a data
?line? whose angle formed with existing data lines
is large. Again, our method builds upon different
settings in that this method is only for supervised
learning, while ours is for semi-supervised learning.
45
3 Theorization of Bootstrapping
This section introduces a theorization of bootstrap-
ping by (Komachi et al, 2008).
3.1 Simple bootstrapping
Let D = {(y1,x1), . . . , (yl,xl),xl+1, . . . ,xl+u}
be a dataset. The first l data are labeled, and the
following u data are unlabeled. We let n = l + u
for simplicity. Each xi ? Rm is an m-dimensional
input feature vector, and yi ? C is its corresponding
label where C is the set of semantic classes. To han-
dle |C| classes, for k ? C, we call an n-sized 0-1
vector yk = (y1k, . . . , ynk)? a ?seed vector?, where
yik = 1 if the i-th instance is labeled and its label is
k, otherwise yik = 0.
Note that this multi-class formalization includes
typical ranking settings for harvesting tasks as its
special case. For example, if the task is to har-
vest animal names from all given instances, such
as ?elephant? and ?zebra?, C is set to be binary as
C = {animal, not animal}. The ranking is obtained
by the score vector resulting from the seed vector
yanimal ? ynot animal due to the linearity.
By stacking row vectors xi, we denote X =
(x1, . . . ,xn)?. Let X be an instance-pattern (fea-
ture) matrix where (X)ij stores the value of the
jth feature in the ith datum. Note that we can al-
most always assume the matrix X to be sparse for
bootstrapping purposes due to the language sparsity.
This sparsity enables the fast computation.
The simple bootstrapping (Komachi et al, 2008)
is a simple model of bootstrapping using matrix rep-
resentation. The algorithm starts from f0
def= y and
repeats the following steps until f c converges.
1. ac+1 = X?f c. Then, normalize ac+1?
2. f c+1 = Xac+1. Then, normalize f c+1.
The score vector after c iterations of the simple
bootstrapping is obtained by the following equation.
f =
(
1
m
1
n
XX?
)c
y (1)
?Simplified Espresso? is a special version of the
simple bootstrapping where Xij = pmi(i,j)max pmi and we
normalize score vectors uniformly: f c ? f c/n,
ac ? ac/m. Here, pmi(i, j)
def= log p(i,j)p(i)p(j) .
Komachi et al (2008) pointed out that, although
the scores f c are normalized during the iterations in
the simple bootstrapping, when c ? ?, f c con-
verges to a score vector that does not depend on
the seed vector y as the principal eigenvector of
( 1
m
1
nXX
?) becomes dominant. For bootstrapping
purposes, however, it is appropriate for the resulting
score vector f c to depend on the seed vector y.
3.2 Laplacian label propagation
To make f seed dependent, Komachi et al (2008)
noted that we should use a power series of a ma-
trix rather than a simple power of a matrix. As
the following equation incorporates the score vec-
tors ((?L)cy) with both low and high c values, it
provides a seed dependent score vector with taking
higher c into account.
?
?
c=0
?c ((?L)cy) = (I + ?L)?1 y (2)
Instead of using
( 1
m
1
nXX
?), Komachi et al
(2008) used L def= I ? D?1/2XX?D?1/2, a nor-
malized graph Laplacian for graph theoretical rea-
sons. D is a diagonal matrix defined as Dii
def=
?
j(XX?)ij . This infinite summation of the ma-
trix can be expressed by inverting the matrix under
the condition that 0 < ? < 1?(L) , where ?(L) be the
spectral radius of L.
Komachi et al (2008)?s Laplacian label propaga-
tion is simply expressed as (3). Given y, it outputs
the score vector f to rank unlabeled instances. They
reports that the resulting score vector f constantly
achieves better results than those by Espresso (Pan-
tel and Pennacchiotti, 2006).
f = (I + ?L)?1 y. (3)
4 Proposal: criteria for iterative seeding
This section describes our iterative seeding frame-
work. The entire framework is shown in Algo-
rithm 1.
Let gi be the goodness of seed for an unlabeled
instance i. We want to select the instance with the
highest goodness of seed as the next seed added in
the next iteration.
i? = argmax
i
gi (4)
46
Algorithm 1 Iterative seeding framework
Require: y, X , the set of unlabeled instances U ,
the set of classes C.
Initialize gk,i? ; ?k ? C, ?i? ? U
repeat
Select instance i? by (4).
Label i?. Let k? be i??s class.
U ? U\{?i}
for all i? ? U do
Recalculate gk?,i?
end for
until A sufficient number of seeds are collected.
Each seed selection criterion defines each good-
ness of seed gi. To measure the goodness of seeds,
we want to measure how an unlabeled instance will
affect the model underlying Eq. (3). That is, we
want to choose the unlabeled instance that would
most influence the model. However, as the model
parameters are not explicitly shown in Eq. (3), we
first need to reveal them before measuring the influ-
ence of the unlabeled instances.
4.1 Scores as margins
This section reveals the model parameters through
the dual problem of bootstrapping. We show that
the score obtained by Eq. (3) can be regarded as
the ?margin? between each unlabeled data point and
the hyperplane obtained by ridge regression; specif-
ically, we can show that the i-th element of the re-
sulting score vector obtained using Eq. (3) can be
written as fi = ? (yi ? ?w?, ? (xi)?), where w? is
the optimal model parameter that we need to reveal
(Figure 1). ? is a feature function mapping xi to a
feature space and is set to make this relation hold.
Note that, for unlabeled instances, yi = 0 holds, and
thus fi is simply fi = ?? ?w?, ? (xi)?. Therefore,
|fi| ? ? ?w?, ? (xi)? ? denotes the ?margin? between
each unlabeled data point and the underlying hyper-
plane.
Let ? be defined as ? def= (? (x1) , . . . , ? (xn))?.
The score vector f can be written using ? as in (6).
If we set ? as Eq. (6), Eq. (5) is equivalent to Eq.
(3).
f =
(
I + ????
)?1
y (5)
Figure 1: Scores as margins. The absolute values of the
scores of the unlabeled instances are shown as the mar-
gin between the unlabeled instances and the underlying
hyperplane in the feature space.
??? = L = I ?D?
1
2XXTD?
1
2 (6)
By taking the diagonal of ??? in Eq. (6), it is
easy to see that ?? (xi) ?2 = ?? (xi) , ? (xi)? ? 1.
Thus, the data points mapped into the feature space
are within a unit circle in the feature space shown
as the dashed circles in Figure 1-3. The weight vec-
tor is then represented by the classifying hyperplane
that goes through the origin in the feature space.
The classifying hyperplane views all the points posi-
tioned left of this hyperplane as the green class, and
all the points positioned right of this hyperplane as
the blue gray-stroked class. Note that all the points
shown in Figure 1 are unlabeled, and thus the clas-
sifying hyperplane does not know the true classes of
the data points. Due to the lack of space, the proof
is shown in the appendix.
4.2 Margin criterion
Section 4.1 uncovered the latent weight vector for
the bootstrapping model Eq. (3). A weight vector
specifies a hyperplane that classifies instances into
semantic classes. Thus, weight vector interpretation
easily leads to an iterative seeding criterion: an unla-
beled instance closer to the classifying hyperplane is
more uncertain, and therefore obtains higher good-
ness of seed. We call this criterion the ?margin cri-
terion? (Figure 2).
First, we define gk,i?
def= |(fk)i? |/sk as the good-
ness of an instance i? to be labeled as k. sk is the
number of seeds labeled as class k in the current
seed set. In the margin criterion, the goodness of the
seed i? is then obtained by the difference between
47
Figure 2: Margin criterion in binary setting. The instance
closest to the underlying hyperplane, the red-and-black-
stroked point, is selected. The part within the large gray
dotted circle is not well separated. Margin criterion con-
tinues to select seeds from this part only in this example,
and fails to sample from the left-bottom blue gray-stroked
points. Note that all the points are unlabeled and thus the
true classes of data points cannot be seen by the underly-
ing hyperplane in this figure.
the largest and second largest gk,i? among all classes
as follows:
gMargini
def= ?
(
max
k
gMargink,i? ? 2
ndlargestkg
Margin
k,i?
)
.
(7)
The shortcoming of Margin criterion is that it can
be ?stuck?, or jammed, or trapped, when the data are
not well separated and the underlying hyperplanes
goes right through the not well-separated part. In
Figure 2, the part within the large gray dotted cir-
cle is not well separated. Margin criterion continues
to select seeds from this part only in this example,
and fails to sample from the left-bottom blue gray-
stroked points.
4.3 Expected Model Rotation
To avoid Margin criterion from being stuck in the
part where the data are not well separated, we pro-
pose another more promising criterion: the ?Ex-
pected Model Rotation (EMR)?. EMR measures the
expected rotation of the classifying hyperplane (Fig-
ure 3) and selects the data point that rotates the un-
Figure 3: EMR criterion in binary setting. The instance
that would rotate the underlying hyperplane the most is
selected. The amount denoted by the purple brace ?{? is
the goodness of seeds in the EMR criterion. This criterion
successfully samples from the left bottom blue points.
derlying hyperplane ?the most? is selected. This se-
lection method prevents EMR from being stuck in
the area where the data points are not well sepa-
rated. Another way of viewing EMR is that it selects
the data point that surprises the current classifier the
most. This makes the data points influential to the
classification selected in early iteration in the itera-
tive seeding framework. A simple rationale of EMR
is that important information must be made available
earlier.
To obtain the ?expected? model rotation, in EMR,
we define the goodness of seeds for an instance i?,
gi? as the sum of each per-class goodness of seeds
gk,i? weighted by the probability that i? is labeled
as k. Intuitively, gk,i? measures how the classifying
hyperplane would rotate if the instance i? were la-
beled as k. Then, gk,i? is weighted by the probability
that i? is labeled as k and summed. The probability
for i? to be labeled as k can be obtained from the
i?-th element of the current normalized score vector
pi? (k)
def= |(fk)i?/sk|?
k?C|(fk)i?/sk|
, where sk is the number
of seeds labeled as class k in the current seed set.
gEMRi?
def=
?
k?C
pi? (k) gEMRk,i? (8)
The per-class goodness of seeds gk,i? can be cal-
culated as follows:
gEMRk,i?
def= 1?
?
?
?
?
w?k
||wk||
wk,+i?
||wk,+i? ||
?
?
?
?
. (9)
48
From Eq. (17) in the proof, w = ??f . Here, ei?
is a unit vector whose i?-th element is 1 and all other
elements are 0.
wk = ??fk = ?? (I + ?L)?1 yk (10)
wk,+i? = ??fk,+i? = ?? (I + ?L)?1 (yk + ei?) (11)
Although Eqs. (10) and (11) use ?, we do not
need to directly calculate ?. Instead, we can use Eq.
(6) to calculate these weight vectors as follows:
w?k wk,+i? = f?k
(
I ?D?
1
2XXTD?
1
2
)
fk,+i? (12)
||w|| =
?
f?
(
I ?D?
1
2XXTD?
1
2
)
f . (13)
For more efficient computation, we cached
(I + ?L) ei? to boost the calculation in Eqs. (10)
and (11) by exploiting the fact that yk can be writ-
ten as the sum of ei for all the instances in class k.
5 Evaluation
We evaluated our method for two bootstrapping
tasks with high labeling costs. Due to the nature
of bootstrapping, previous papers have commonly
evaluated each method by using running search en-
gines. While this is useful and practical, it also re-
duces the reproducibility of the evaluation. We in-
stead used openly available resources for our evalu-
ation.
First, we want to focus on the separatedness of the
dataset. To this end, we prepared two datasets: one
is ?Freebase 1?, a not well-separated dataset, and
another is ?sb-8-1?, a well-separated dataset. We
fixed ? = 0.01 as Zhou et al (2011) reports that
? = 0.01 generally provides good performance on
various datasets and the performance is not keen to ?
except extreme settings such as 0 or 1. In all exper-
iments, each class initially has 1 seed and the seeds
are selected and increased iteratively according to
each criterion. The meaning of each curve is shared
by all experiments and is explained in the caption of
Figure 4.
?Freebase 1? is an experiment for information ex-
traction, a common application target of bootstrap-
ping methods. Based on (Talukdar and Pereira,
2010), the experiment setting is basically the same
as that of the experiment Section 3.1 in their paper1.
1Freebase-1 with Pantel Classes, http://www.
talukdar.net/datasets/class_inst/
As 39 instances have multiple correct labels, how-
ever, we removed these instances from the exper-
iment to perform the experiment under multi-class
setting. Eventually, we had 31, 143 instances with
1, 529 features in 23 classes. The task of ?Freebase
1? is bootstrapping instances of a certain semantic
class. For example, to harvest the names of stars,
given {Vega, Altair} as a seed set, the bootstrap-
ping ranks Sirius high among other instances (proper
nouns) in the dataset. Following the experiment set-
ting of (Talukdar and Pereira, 2010), we used mean
reciprocal rank (MRR) throughout our evaluation 2.
?sb-8-1? is manually designed to be well-
separated and taken from 20 Newsgroup subsets3.
It has 4, 000 instances with 16, 282 features in 8
classes.
Figure 4 and Figure 5 shows the results. We can
easily see that ?EMR? wins in ?Freebase 1?, a not
well-separated dataset, and ?Margin? wins in ?sb-8-
1?, a well-separated dataset. This result can be re-
garded as showing that ?EMR? successfully avoids
being ?stuck? in the area where the data are not
well separated. In fact, in Figure 4, ?Random? wins
?Margin?. This implies that the not well-separated
part of this dataset causes the classifying hyperplane
in ?Margin? criterion to be stuck and make it lose
against even simple ?Random? criterion.
In contrast, in the ?sb-8-1?, a well-separated bal-
anced dataset, ?Margin? beats the other remaining
two. This implies the following: When the dataset
is well separated, uncertainty of a data point is the
next important factor to select a seed set. As ?Mar-
gin? exactly takes the data point that is the most un-
certain to the current hyperplane, ?Margin? works
quite well in this example.
Note that all figures in all the experiments show
the average of 30 random trials and win-and-lose re-
lationships mentioned are statistically tested using
Mann-Whitney test.
While ?sb-8-1? is a balanced dataset, realistic data
like ?freebase 1? is not only not-well-separated, but
also imbalanced . Therefore, we performed ex-
periments ?sb-8-1?, an imbalanced well-separated
dataset, and ?ol-8-1?, an imbalanced not-well sepa-
2MRR is defined as MRR def= 1|Q|
?
i?Q
1
ri
, where Q is
the test set, i ? Q denotes an instance in the test set Q, and ri
is the rank of the correct class among all |C| classes.
3http://mlg.ucd.ie/datasets/20ng.html
49
Figure 4: Freebase 1, a NOT well-separated dataset. Av-
erage of 30 random trials. ?Random? and ?Margin? are
baselines. ?Random? is the case that the seeds are se-
lected randomly. ?Margin? is the case that the seeds
are selected using the margin criterion described in ?4.2.
?EMR? is proposed and is the case that the seeds are se-
lected using the EMR criterion described in ?4.3. At the
rightmost point, all the curves meet because all the in-
stances in the seed pool were labeled and used as seeds
by this point. The MRR achieved by this point is shown
as the line ?All used?. If a curve of each method crosses
?All used?, this can be intepretted as that iterative seeding
of the curve?s criterion can reduce the cost of labeling all
the instances to the crossing point of the x-axis. ?EMR?
significantly beats ?Random? and ?Margin? where x-axis
is 46 and 460 with p-value < 0.01.
rated dataset under the same experiment setting used
for ?sb-8-1?. ?sl-8-1? have 2, 586 instances with
10, 764 features. ?ol-8-1? have 2, 388 instances with
9, 971 features. Both ?sl-8-1? and ?ol-8-1? have 8
classes.
Results are shown in Figure 6 and Figure 7. In
Figure 6, ?EMR? beats the other remaining two even
though this is a well-separated data set. This im-
plies that ?EMR? can also be robust to the imbal-
ancedness as well. In Figure 7, although the MRR
of ?Margin? eventually is the highest, the MRR of
?EMR? rises far earlier than that of ?Margin?. This
result can be explained as follows: ?Margin? gets
?stuck? in early iterations as this dataset is not well
separated though ?Margin? achieves best once it gets
out of being stuck. In contrast, as ?EMR? can avoid
being stuck, it rises early achieving high perfor-
mance with small number of seeds, or labeling. This
result suggests that ?EMR? is pereferable for reduc-
Figure 5: sb-8-1. A dataset manually designed to be well
separated. Average of 30 random trials. Legends are the
same as those in Figure 4. ?Margin? beats ?Random? and
?EMR? where x-axis is 500 with p-value < 0.01.
Figure 6: sl-8-1. An imbalanced well separated dataset.
Average of 30 random trials. Legends are the same as
those in Figure 4. ?EMR? significantly beats ?Random?
and ?Margin? where x-axis is 100 with p-value < 0.01.
ing labeling cost while ?Margin? can sometimes be
preferable for higher performance.
6 Conclusion
Little is known about how best to select seed sets
in bootstrapping. We thus introduced the itera-
tive seeding framework, which provides criteria for
selecting seeds. To introduce the iterative seed-
ing framework, we deepened the understanding of
the seeding process in bootstrapping through the
dual problem by further extending the interpretation
of bootstrapping as graph-based semi-supervised
learning (Komachi et al, 2008), which generalizes
50
Figure 7: ol-8-1. An imbalanced NOT well separated
dataset. Average of 30 random trials. Legends are the
same as those in Figure 4. ?EMR? significantly beats
?Random? and ?Margin? where x-axis is 100 with p-
value < 0.01. ?Margin? significantly beats ?EMR? and
?Random? where x-axis is 1, 000 with p-value < 0.01.
and improves Espresso-like algorithms.
Our method shows that existing simple ?Margin?
criterion can be ?stuck? at the area when the data
points are not well separated. Note that many real-
istic data are not well separated. To deal with this
problem, we proposed ?EMR? criterion that is not
stuck in the area where the data points are not well
separated.
We also contributed to make the calculation of
?EMR? practical. In particular, we reduced the num-
ber of matrix inversions for calculating the goodness
of seeds for ?EMR?.We also showed that the param-
eters for bootstrapping also affect the convergence
speed of each matrix inversion and that the typical
parameters used in other work are fairly efficient and
practical.
Through experiments, we showed that the pro-
posed ?EMR? significantly beats ?Margin? and
?Random? baselines where the dataset are not well
separated. We also showed that the iterative seed-
ing framework with the proposed measures for the
goodness of seeds can reduce labeling cost.
Appendix: Proof Consider a simple ridge regres-
sion of the following form where 0 < ? < 1 is a
positive constant.
minw
?
2
n
?
i=1
?yi ? ?w, ? (xi)??2 + ?w?2 . (14)
We define ?i = yi ? ?w, ? (xi)?. By using ?i, we
can rewrite Eq. (14) into an optimization problem
with equality constraints as follows:
minw
?
2
n
?
i=1
?2i + ?w?
2 (15)
s.t.?i ? {1, . . . , n} ; yi = w?? (xi) + ?i. (16)
Because of the equality constraints of Eq. (16),
we obtain the following Lagrange function h. Here,
each bootstrapping score fi occurs as Lagrange mul-
tipliers: h (w, ?, f) def= 12 ?w?
2 + ?2
?n
i=1 ?2i ?
?n
i=1 (?w, ? (xi)?+ ?i ? yi) fi.
By taking derivatives of h, we can derive w? by
expressing it with the sum of each fi and ? (xi).
?h
?w = 0? w? =
n
?
i=1
fi? (xi) (17)
?h
??i
= 0? fi = ? (?i = ?yi ? ?w?, ? (xi)?) (18)
Substituting the relations derived in Eqs. (17) and
(18) to the equation ?h?fi = 0 results in Eq. (19).
?h
?fi
= 0?
n
?
j=1
fj? (xi)? ? (xj)+
1
?
fi = yi (19)
Equation (19) can be written as a matrix equation
using ? defined as ? def= (? (x1) , . . . , ? (xn))?.
From Eq. (20), we can easily derive the form of Eq.
(3) as
(
??? + 1? I
)?1
y ?
(
I + ????
)?1 y.
(
??? + 1
?
I
)
f = y (20)
2
References
Steven Abney. 2004. Understanding the yarowsky algo-
rithm. Computational Linguistics, 30(3):365?395.
Klaus Brinker. 2003. Incorporating diversity in active
learning with support vector machines. In Proc. of
ICML, pages 59?66, Washington D.C.
Zelling S. Harris. 1954. Distributional structure. Word.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of COLING,
pages 539?545.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in Espresso-like bootstrapping algorithms.
In Proc. of EMNLP, pages 1011?1020, Honolulu,
Hawaii.
51
Zornitsa Kozareva and Eduard Hovy. 2010. Not all seeds
are equal: Measuring the quality of text mining seeds.
In Proc. of NAACL-HLT, pages 618?626, Los Angeles,
California.
Zornitsa Kozareva, Konstantin Voevodski, and Shanghua
Teng. 2011. Class label enhancement via related in-
stances. In Proc. of EMNLP, pages 118?128, Edin-
burgh, Scotland, UK.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proc. of ACL-COLING,
pages 113?120, Sydney, Australia.
Burr Settles. 2012. Active Learning. Synthesis Lectures
on Artificial Intelligence and Machine Learning. Mor-
gan & Claypool Publishers.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proc. of
ACL, pages 1473?1481, Uppsala, Sweden.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proc. of
ACL, pages 189?196, Cambridge, Massachusetts.
Xueyuan Zhou, Mikhail Belkin, and Nathan Srebro.
2011. An iterated graph laplacian approach for rank-
ing on manifolds. In Proc. of KDD, pages 877?885.
52
