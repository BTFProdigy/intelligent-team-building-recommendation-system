Improving Chinese Word Segmentation by Adopting  
Self-Organized Maps of Character N-gram 
Chongyang Zhang 
iFLYTEK Research 
cyzhang@iflytek.com 
Zhigang Chen
iFLYTEK Research?
zgchen@iflytek.com 
Guoping Hu?
iFLYTEK Research 
gphu@iflytek.com 
?
?
?
Abstract 
Character-based tagging method has 
achieved great success in Chinese Word 
Segmentation (CWS). This paper 
proposes a new approach to improve the 
CWS tagging accuracy by combining 
Self-Organizing Map (SOM) with 
structured support vector machine 
(SVM) for utilization of enormous 
unlabeled text corpus. First, character 
N-grams are clustered and mapped into 
a low-dimensional space by adopting 
SOM algorithm. Two different maps are 
built based on the N-gram?s preceding 
and succeeding context respectively. 
Then new features are extracted from 
these maps and integrated into the 
structured SVM methods for CWS. 
Experimental results on Bakeoff-2005 
database show that SOM-based features 
can contribute more than 7% relative 
error reduction, and the structured SVM 
method for CWS proposed in this paper 
also outperforms traditional conditional 
random field (CRF) method. 
1 Introduction 
It is well known that there is no space or any 
other separators to indicate the word boundary 
in Chinese. But word is the basic unit for most 
of Chinese natural language process tasks, such 
as Machine Translation, Information Extraction, 
Text Categorization and so on. As a result, 
Chinese word segmentation (CWS) becomes 
one of the most fundamental technologies in 
Chinese natural language process. 
In the last decade, many statistics-based 
methods for automatic CWS have been 
proposed with development of machine learning 
and statistical method (Huang and Zhao, 2007). 
Especially, the character-based tagging method 
which was proposed by Nianwen Xue (2003) 
achieves great success in the second 
International Chinese word segmentation 
Bakeoff in 2005 (Low et al, 2005). The 
character-based tagging method formulates the 
CWS problem as a task of predicting a tag for 
each character in the sentence, i.e. every 
character is considered as one of four different 
types in 4-tag set: B (begin of word), M (middle 
of word), E (end of word), and S (single-
character word). 
Most of these works train tagging models 
only on limited labeled training sets, without 
using any unsupervised learning outcomes from 
innumerous unlabeled text. But in recent years, 
researchers begin to exploit the value of 
enormous unlabeled corpus for CWS. Some 
statistics information on co-occurrence of sub-
sequences in the whole text has been extracted 
from unlabeled data and been employed as input 
features for tagging model training (Zhao and  
Kit , 2007).  
Word clustering is a common method to 
utilize unlabeled corpus in language processing 
research to enhance the generalization ability, 
such as part-of-speech clustering and semantic 
clustering (Lee et al, 1999 and B Wang and H 
Wang 2006). Character-based tagging method 
usually employs N-gram features, where an N-
gram is an N-character segment of a string. We 
believe that there are also semantic or 
grammatical relationships between most of N-
grams and these relationships will be useful in 
CWS. Intuitively, assuming the training data 
contains the bigram ?? /? ?(The last two 
characters of the word ?Israel? in Chinese), not 
contain the bigram ?? /? ?(The last two 
characters of the word ?Turkey? in Chinese), if 
we could cluster the two bigrams together 
according to unlabeled corpus and employ it as 
a feature for supervised training of tagging 
model, maybe we will know that there should 
be a word boundary after ??/?? though we 
only find the existence of word boundary after 
??/?? in the training data. So we investigate 
how to apply clustering method onto unlabeled 
data for the purpose of improving CWS 
accuracy in this paper. 
This paper proposes a novel method of using 
unlabeled data for CWS, which employs Self-
Organizing Map (SOM) (Kohonen 1982) to 
organize Chinese character N-grams on a two-
dimensional array, named as ?N-gram cluster 
map? (NGCM), in which the character N-grams 
similar in grammatical structure and semantic 
meaning are organized in the same or adjacent 
position. Two different arrays are built based on 
the N-gram?s preceding context and succeeding 
context respectively because sometimes N-gram 
is just a part of Chinese word and does not share 
similar preceding and succeeding context in the 
same time. Then NGCM-based features are 
extracted and applied to tagging model of CWS. 
Two tagging models are investigated, which are 
structured support vector machine (SVM) 
(Tsochantaridis et al, 2005) model and 
Confidential Random Fields (CRF) (Lafferty et 
al., 2001). The experimental results show that 
NGCM is really helpful to CWS. In addition, 
we find that the structured SVM achieves better 
performance than CRF. 
The rest of this paper is organized as follows: 
Section 2 presents self-organizing map and the 
idea of N-gram cluster maps. Section 3 
describes structured SVM and how to use the 
NGCMs based features in CWS. Section 4 
shows experimental results on Bakeoff-2005 
database and Section 5 gives our conclusion. 
 
2 N-gram cluster maps 
Supervised learning method for CWS needs 
enough pre-labeled corpus with word boundary 
information for training. The final CWS 
performance relies heavily on the quality of the 
training data. The training data is limited and 
cannot cover completely the linguistic 
phenomenon. But unlabeled corpus can be 
obtained easily from internet. One intuitive 
method is to extract information from 
unsupervised learning results from enormous 
unlabeled data to enhance supervised learning.  
 
2.1 Self-Organizing Map 
The Self-Organizing Map (SOM) (Kohonen 
1982), sometimes called Kohonen map, was 
developed by Teuvo Kohonen in the early 
1980s. Different from other clustering method, 
SOM is a type of artificial neural network on 
the basis of competitive learning to visualize 
higher dimensional data in a low-dimensional 
space (usually 1D or 2D) while preserving the 
topological properties of the input space. Figure 
1 displays a 2D SOM. 
Best matching unit
Input
Two-dimensional 
array of neurous
 
Figure 1: SOM model 
 
In SOM, the input is a lot of data samples, 
and each sample is represented as a vector 
, 1,2,...,ix i M= , where M is the number of the 
input vectors. SOM will cluster all these 
samples into L neurons, and each neuron is 
associated with a weight vector , 1,2,...,iw i L= , 
where L  is the total number of the neurons.? jw ?
is? of the same dimensions as the input data 
vectors ix . The learning algorithm of SOM is 
as follows: 
1. Randomize every neuron?s weight vector 
iw  ; 
2. Randomly select an input vector tx  ; 
3. Find the winning neuron j ?, whose 
associate weight vector jw  has  the 
minimal distance to tx  ; 
4. Update the weight vector of all the neurons 
according to the following formula: 
( , )( )i i t iw w i j x w??? + ?  
Where ?  is the learning-rate and ( , )i j?  
is the neighborhood function. A simple 
choice defines ( , ) 1i j? =  for all neuron i  
in a neighborhood of radius r of neuron 
j and ( , ) 0i j? =  for all other neurons. ?  
and ( , )i j?  usually varied dynamically 
during learning for best results; 
5. Continue step 2 until maximum number of 
iterations has been reached or no noticeable 
changes are observed. 
2.2 SOM-based N-gram cluster maps 
Self-organizing semantic maps (Ritter and 
Kohonen 1989, 1990) are SOMs that have been 
organized according to word similarities, 
measured by the similarity of the short contexts 
of the words. Our algorithm of building N-gram 
cluster maps is similar to self-organizing 
semantic maps. Because sometimes N-gram is 
just part of Chinese word and do not share 
similar preceding and succeeding context in the 
same time, so we build two different maps 
according to the preceding context and the 
succeeding context of N-gram individually. In 
the end we build two NGCMs: NGCMP 
(NGCM according to preceding context) and 
NGCMS (NGCM according to succeeding 
context). 
In this paper we only consider bigram cluster 
maps. So our purpose is to acquire a 2GCMP 
and a 2GCMS. The large-scale unlabeled corpus 
we used for training NGCMs is about 3.5G in 
size. It was obtained easily from websites like 
Sohu, Netease, Sina and People Daily. When 
the cut-off threshold is set to 5, we got about 9K 
different characters and 380K different bigrams 
by counting the corpus. For each bigram, a 9K-
dimensional sparse vector can be derived from 
the preceding character of the bigram. Therefore 
a collection of 380K vector samples are 
generated, which is denoted as P. Another 
vector collection S which considers succeeding 
character was obtained using the same method. 
Our implementation used SOM-PAK 
package Version 1.0 (Kohonen et al, 1996). We 
set the topology type to rectangular and the map 
size to15 15? . In the training process, we used 
P and S as input data respectively. After the 
training we acquired a 2GCMP and a 2GCMS, 
meanwhile each bigram was mapped to one 
neuron. Because the number of neurons is much 
smaller than the number of bigrams, each 
neuron in the map was labeled with multiple 
bigrams. The 2GCMP and 2GCMS are shown 
in Figure 2 and Figure 3 respectively. The 
comment boxes in the figures show some 
samples of bigrams mapped in the same neuron.  
 
Figure 2: 2GCMP 
0,0
0,1 1,1 2,1
0,2 1,2 2,2
1,0 2,0
14,14
14,2
14,0
0,14 1,14 2,14
14,1
?/?
?/?
?/?
?/?
?/?
?
?/?
?/?
?/?
?/?
?
?/?
?/?
?/?
?/?
...
?/?
?/?
?/?
?/?
...
 
Figure 3: 2GCMS 
 After checking the results, we find that most 
of the meaningless bigrams that contain 
characters from more than one word, such as the 
bigram "?? " in "...???? ..." , are 
organized into the same neurons in the map, and 
most of the first or last bigrams of the country 
names are organized into a few adjacent 
neurons, such as ??/??, ??/??, ??/?? and 
??/??in 2GCMS , ??/??, ??/??, ??/??, 
??/?? , and ??/?? in 2GCMP. We also tried 
to use the preceding and the succeeding context 
together in NGCM training just like the method 
used in the self-organizing semantic maps. We 
found that the bigrams of ??/??, ??/?? and 
?? /?? will never be assigned to the same 
neuron again, which indicates that we need to 
build two NGCMs according to preceding and 
succeeding context separately. 
3 Integrate NGCM into Structured 
SVM for CWS 
3.1 Structured support vector machine 
The structured support vector machine can learn 
to predict structured y , such as trees sequences 
or sets, from x  based on large-margin approach. 
We employ a structured SVM that can predict a 
sequence of labels 1( ,..., )Ty y y=  for a given 
observation sequence 1( ,..., )Tx x x= , where 
ty ?? ,?  is the label set for y. 
There are two types of features in the 
structured SVM: transition features (interactions 
between neighboring labels along the chain), 
emission features (interactions between 
attributes of the observation vectors and a 
specific label).we can represent the input-output 
pairs via joint feature map (JFM) 
1
1
1
1
( ) ( )
( , )
( ) ( )
T
t c t
t
T
c t c t
t
x y
x y
y y
?
?
?
=
? +
=
? ???? ?? ?= ? ?? ??? ?? ?
?
?
 
where
{
1 2
1 2
,
1,
0,
( ) ( ( , ), ( , ),..., ( , )) '
{0,1} ,  { , ,..., }
Kronecker delta  ,
c
K
K
K
i j
i j
i j
y y y y y y y
y y y y
? ? ?
? ? =?
?
? ?
? ? =
=
 
( )x?  denotes an arbitrary feature representation 
of the inputs. The sign " "?  expresses tensor 
product defined as ? : d kR R? dkR? , 
[ ] ( 1)i j da b + ?? [ ] [ ]i ja b= .T  is the length of an 
observation sequence. 0? ?  is a scaling factor 
which balances the two types of contributions. 
Note that both transition features and 
emission features can be extended by including 
higher-order interdependencies of labels (e.g. 
1 2( ) ( ) ( )c t c t c ty y y+ +? ?? ?? ),by including 
input features from a window centered at the 
current position (e.g. replacing ( )tx?  with 
( ,..., ,... )t r t t rx x x? ? + )or by combining higher-
order output features with input features (e.g. 
1( ) ( ) ( )t c t c t
t
x y y? +?? ??? ) 
The w-parametrized discriminant function 
:F X Y R? ? interpreted as measuring the 
compatibility of x and y is defined as: 
( , ; ) , ( , )F x y w w x y?=  
So we can maximize this function over the 
response variable to make a prediction 
( ) arg max ( , , )
y Y
f x F x y w
?
=  
Training the parameters can be formulated as 
the following optimization problem. 
,
1
1
min ,
2
. . , :
, ( , ) ( , ) ( , )
n
iw
i
i i i i i i i
C
w w
n
s t i y Y
w x y x y y y
? ?
? ? ?
=
+
? ? ?
? ? ? ?
?
where n  is the number of the training samples, 
i?  is a slack variable , 0C ?  is a constant 
controlling the tradeoff between training error 
minimization and margin maximization, 
1( , )y y?  is the loss function ,usually the 
number of misclassified tags in the sentence.  
3.2 Features set for tagging model 
For a training sample denoted as 
1( ,..., )Tx x x=  and 1( ,..., )Ty y y= . We chose 
first-order interdependencies of labels to be 
transition features, and dependencies between 
labels and N-grams (n=1, 2, 3) at current 
position in observed input sequence to be 
emission features.  
So our JFM is the concatenation of the follow 
vectors 
1
1
1
( ) ( )
T
c t c t
t
y y
? +
=
? ??? , 
1
( ) ( ), { 1,0,1}
T
t m c t
t
x y m? +
=
?? ? ??   
1
1
( , ) ( ), { 2, 1,0,1}
T
t m t m c t
t
x x y m? + + +
=
?? ? ? ??
1 1
1
( , , ) ( ), { 1,0,1}
T
t m t m t m c t
t
x x x y m? + ? + + +
=
?? ? ??
Figure 4 shows the transition features and the 
emission features of N-grams (n=1, 2) at 3y . 
The emission features of 3-grams are not shown 
here because of the large number of the 
interactions. 
1y 2y 3y 4y
1x 2x 3x 4x
5y
5x
 
Figure 4: the transition features and the 
emission features at 3y  for structured SVM 
3.3 Using NGCM in CWS 
Two methods can be used for extracting the 
features from NGCMs to expend features 
definition in section 3.2.  
One method is to treat NGCM just as a 
clustering tool and do not take into account the 
similarity between adjacent neurons. So a new 
feature with L dimensions can be generated, 
where L is the number of the neurons or classes. 
Only one value of the L dimension equals to 1 
and others equal to 0. We call it NGCM 
clustering feature.  
Another way of using the NGCM is to adopt 
the position of the neurons which current N-
gram mapped in the NGCM as a new feature. 
So every feature has D dimensions (D equals to 
the dimension of the NGCM, every dimension 
is corresponding to the coordinate value in the 
NGCM). In this way, N-gram which is 
originally represented as a high dimensional 
vector based on its context is mapped into a 
very low-dimensional space. We call it NGCM 
mapping feature. 
In this paper, we only consider the NGCM 
clustering or mapping features related to the 
current label iy . We also extract features from 
the quantization error of current N-gram 
because the result of the NGCM is very noisy. 
Then our previous JFM in section 3.2 is 
concatenated with the following features: 
2GCMS 1
1
( , ) ( ), { 2, 1}
T
t m t m c t
t
x x y m? + + +
=
?? ? ? ??  
2GCMP 1
1
( , ) ( ), {0,1}
T
t m t m c t
t
x x y m? + + +
=
?? ??
2GCMS 1
1
( , ) ( ), { 2, 1}
T
t m t m c t
t
x x y m? + + +
=
?? ? ? ??
2GCMP 1
1
( ) ( ), {0,1}
T
t m t m c t
t
x x y m? + + +
=
?? ?? ?
where 2GCMS( )x?  denotes the NGCM feature 
from 2GCMS, 2GCMP ( )x?  denotes the NGCM 
feature from 2GCMP.? NGCM ( )x?  denotes the 
quantization error  of  current N-gram x on its 
NGCM.?
In 15 15? size NGCM, when we use the 
NGCM clustering feature 2GCMS( )x?  and 
2GCMP ( )x?  15 15{0,1} ?? . When we use the 
NGCM mapping feature 2GCMS( )x?  and 
2GCMP ( )x?  2{0,1,...,14}? .? Notice that the 
dimension of the NGCM clustering feature is 
much higher than the NGCM mapping feature. 
As an example, the process of import features 
from NGCMs at 3y  is presented in Figure 5. 
 
1y 2y 3y 4y
1x 2x 3x 4x
5y
5x
2GCMS 2GCMP
 
Figure 5: Using 2GCMS and 2GCMP as input 
to structured SVM 
4 Applications and Experiments 
4.1 Corpus 
We use the data adopted by the second 
International Chinese Word Segmentation 
Bakeoff (Bakeoff-2005). The corpus size 
information is listed in Table 1. 
 
Corpus As CityU MSRA PKU
Training(M) 5.45 1.46 2.37 1.1 
Test(K) 122 41 107 104 
Table 1: Corpus size of Bakeoff-2005 in 
number of words 
4.2 Text Preprocessing 
Text is usually mixed up with numerical or 
alphabetic characters in Chinese natural 
language, such as ??? office ????? 9
??. These numerical or alphabetic characters 
are barely segmented in CWS. Hence, we treat 
these symbols as a whole ?character? according 
to the following two preprocessing steps. First 
replace one alphabetic character to four 
continuous alphabetic characters with E1 to E4 
respectively, five or more alphabetic characters 
with E5. Then replace one numerical number to 
four numerical numbers with N1 to N4 and five 
or more numerical numbers with N5. After text 
preprocessing, the above examples will be ??
? E5????? N1??. 
4.3 Character-based tagging method 
for CWS 
Previous works show that 6-tag set achieved 
a better CWS performance (Zhao et al, 
2006). Thus, we opt for this tag set. This 6-
tag set adds ?B2? and ?B3? to 4-tag set 
which stand for the type of the second and 
the third character in a Chinese word 
respectively. For example, the tag sequence 
for the sentence ??????/?/??/?
?(Shanghai World Expo / will / last / six 
months)? will be ?B B2 B3 M E S B E B E?. 
4.4 Experiments 
 
The F-measure is employed for evaluation, 
which is defined as follows: 
num of correctly segmented words
Precision: P
num of the system output words
=  
num of correctly segmented words
Recall: R
num of total words in test data
=  
2 P R
F-measure: F
P R
? ?= +   
To compare with other discriminative 
learning methods we first developed a baseline 
system using conditional random field (CRF) 
without using NGCM feature and then we 
developed another CRF system: CFCRF (using 
NGCM clustering features). In the end we 
developed three structured SVM CWS systems: 
SVM (without using NGCM features), CFSVM 
(using NGCM clustering features), and 
MFSVM (using NGCM mapping features). The 
features for the baseline CRF system are the 
same with the SVM system. The features for 
CFCRF are the same with CFSVM. The result 
of the CRF system using NGCM mapping 
features cannot be given here, because it is 
difficult to support continuous-value features 
for CRF method which is based on the 
Maximum Entropy Model. 
We use CRF++ version 0.5 (Kudu, 2009) to 
build our CRF models. The cut-off threshold is 
set to 2(using the features that occurs no less 
than 2 times in the given training data) and the 
hyper-parameter is set to 4.5. We use hmmsvm  
version 3.1 to build our structured SVM models. 
The cut-off threshold is set to 2. The precision 
parameter is set to 0.1. The tradeoff between 
training error minimization and margin 
maximization is set to 1000. 
The comparisons between CRF, CFCRF, SVM, 
CFSVM and MFSVM are shown in Table 2.  
Corpus  As CityU MSRA PKU
CRF 
baseline 
P 0.945 0.943 0.971 0.953
R 0.955 0.942 0.970 0.946
F 0.950 0.942 0.971 0.950
CFCRF P 0.948 0.956 0.973 0.959
R 0.959 0.961 0.972 0.952
F 0.953 0.958 0.973 0.955
SVM P 0.949 0.957 0.972 0.953
R 0.959 0.959 0.972 0.946
F 0.954 0.958 0.972 0.950
CFSVM P 0.952 0.959 0.974 0.958
R 0.960 0.964 0.974 0.952
F 0.956 0.961 0.974 0.955
MFSVM
 
P 0.950 0.957 0.974 0.958
R 0.961 0.963 0.974 0.951
F 0.956 0.960 0.974 0.954
Table 2: The results of our systems 
4.5 Discussion 
From Table 2, we can see that: 
1) The NGCM feature is useful for CWS. The 
feature achieves 13.9% relative error 
reduction on CRF method and 7.2% relative 
error reduction on structured SVM method;  
2) CFSVM and MFSVM achieve similar 
performance, differ from the expectation of 
MFSVM should be better than CFSVM. We 
think that this is because the size of 2GCMs 
is too small. Due to the limitation of our 
computer and time we only get two 15 15?  
size 2GCMs, similarity between adjacent 
neurons on the two small 2GCMs is very 
week, NGCM cluster feature performs as 
good as NGCM mapping feature on CWS. 
But due to the dimensions of the NGCM 
cluster feature is much larger than the 
NGCM mapping feature, the training time 
of the CFSVM is much longer than the 
MFSVM; 
3) It is obvious that structured SVM performs 
better than CRF, demonstrating the benefit 
of large margin approach. 
5 Conclusion 
This paper proposes an approach to improve 
CWS tagging accuracy by combining SOM with 
structured SVM. We use SOM to organize 
Chinese character N-grams on a two-
dimensional array, so that the N-grams similar 
in grammatical structure and semantic meaning 
are organized in the same or adjacent position. 
Two different maps are built based on the N-
gram?s preceding and succeeding context 
respectively. Then new features are extracted 
from these maps and integrated into the 
structured SVM methods for CWS. 
Experimental results on Bakeoff-2005 database 
show that SOM-based features can contribute 
more than 7% relative error reduction, and the 
structured SVM method for CWS,? to our 
knowledge, first proposed in this paper also 
outperforms traditional CRF method.  
In future work, we will try to organizing all 
the N-grams on a much larger array, so that 
every neuron will be labeled by a single N-gram. 
Our ultimate objective is to reduce the 
dimension of input features for supervised CWS 
learning , such as structured SVM , by replacing 
N-gram features with two-dimensional NGCM 
mapping features in most of Chinese natural 
language process tasks. 
References 
B.Wang, H.Wang 2006.A Comparative Study on 
Chinese Word Clustering. Computer Processing 
of Oriental Languages. Beyond the Orient: The 
Research Challenges Ahead, pages 157-164 
Chang-Ning Huang and Hai Zhao. 2007. Chinese 
word segmentation: A decade review. Journal of 
Chinese Information Processing, 21(3):8?20. 
Chung-Hong Lee & Hsin-Chang Yang.1999, A Web 
Text Mining Approach Based on Self-Organizing 
Map, ACM-library 
G.Bakir, T.Hofmann, B.Scholkopf, A.Smola, B. 
Taskar, and S. V. N. Vishwanathan, editors. 2007 
Predicting Structured Data. MIT Press, 
Cambridge, Massachusetts. 
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-
Liang Lu. 2006. Effective tag set selection 
inChinese word segmentation via conditional 
random field modeling. In Proceedings of 
PACLIC-20, pages 87?94. Wuhan, China. 
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006.An 
improved Chinese word segmentation system with 
conditional random field. In SIGHAN-5, pages 
162?165, Sydney, Australia, July 22-23. 
Hai Zhao and Chunyu Kit. 2007. Incorporating 
global information into supervised learning for 
Chinese word segmentation. In PACLING-2007, 
pages 66?74, Melbourne,Australia, September 19-
21. 
H.Ritter, and T.Kohonen, 1989. Self-organizing 
semantic maps. Biological Cybernetics, vol. 61, 
no. 4, pp. 241-254. 
I.Tsochantaridis,T.Joachims,T.Hofmann,and Y.Altun. 
2005. Large Margin Methods for Structured and 
Interdependent Output Variables, Journal of 
Machine Learning Research (JMLR), 
6(Sep):1453-1484. 
Jin Kiat Low, Hwee Tou Ng, and Wenyuan 
Guo.2005. A maximum entropy approach to 
Chinese word segmentation. In Proceedings of the 
Fourth SIGHAN Workshop on Chinese Language 
Processing, pages 161?164. Jeju Island,Korea. 
J.Lafferty,A.McCallum, F.Pereira. 2001. Conditional 
random fields: Probabilistic models for 
segmenting and labeling sequence data. In 
Proceedings of the International Conference on 
Machine Learning (ICML). San Francisco: 
Morgan Kaufmann Publishers, 282?289. 
Nianwen Xue and Susan P. Converse., 2002, 
Combining Classifiers for Chinese Word 
Segmentation, In Proceedings of First SIGHAN 
Workshop on Chinese Language Processing. 
Nianwen Xue. 2003. Chinese word segmentation as 
character tagging. Computational Linguistics and 
Chinese Language Processing, 8(1):29?48. 
R.Sproat and T.Emerson. 2003.The first 
international Chinese word segmentation bakeoff. 
In The Second SIGHAN Workshop on Chinese 
Language Processing, pages 133?143.Sapporo, 
Japan. 
S.Haykin, 1994. Neural Networks: A Comprehensive 
Foundation. NewYork: MacMillan. 
T.Joachims, T.Finley, Chun-Nam Yu. 2009, Cutting-
Plane Training of Structural SVMs, Machine 
Learning Journal,77(1):27-59. 
T.Joachims. 2008 . 
hmmsvm  Sequence Tagging with 
Structural Support Vector Machines, 
http://www.cs.cornell.edu/People/tj/svm_light/sv
m_hmm.html 
T.Honkela, 1997. Self-Organizing Maps in Natural 
Language Processing. PhD thesis, Helsinki 
University of Technology, Department of 
Computer Science and Engineering, Laboratory of 
Computer and Information Science. 
T.Kohonen. 1982.Self-organized formation of 
topologically correct feature maps. Biological 
Cybernetics, 43, pp. 59-69. 
T.Kohonen., J.Hynninen, J.Kangas, J.Laaksonen, 
1996 ,SOM_PAK: The Self-Organizing Map 
Program Package,Technical Report A31, 
Helsinki University of Technology , 
http://www.cis.hut.fi/nnrc/nnrc-programs.html 
T.Kudu.2009. CRF++: Yet another CRF 
toolkit.:http://crfpp.sourceforge.net/. 
Y.Altun, I.Tsochantaridis, T.Hofmann. 2003. Hidden 
Markov Support Vector Machines. In Proceedings 
of International Conference on Machine Learning 
(ICML). 
 
An Double Hidden HMM and an CRF for Segmentation Tasks with
Pinyin?s Finals
Huixing Jiang Zhe Dong
Center for Intelligence Science and Technology
Beijing University of Posts and Telecommunications
Beijing, China
jhx0129@163.com jimmybupt@gmail.com
Abstract
We have participated in the open tracks
and closed tracks on four corpora of Chi-
nese word segmentation tasks in CIPS-
SIGHAN-2010 Bake-offs. In our experi-
ments, we used the Chinese inner phonol-
ogy information in all tracks. For open
tracks, we proposed a double hidden lay-
ers? HMM (DHHMM) in which Chinese
inner phonology information was used as
one hidden layer and the BIO tags as an-
other hidden layer. N-best results were
firstly generated by using DHHMM, then
the best one was selected by using a new
lexical statistic measure. For close tracks,
we used CRF model in which the Chinese
inner phonology information was used as
features.
1 Introduction
Chinese language has many characteristics not
possessed by other languages. One obvious is
that the written Chinese text does not have explicit
word boundaries like western languages. So word
segmentation became very significative for Chi-
nese information processing, and is usually con-
sidered as the first step of any further processing.
Identifying words has been a basic task for many
researchers who have devoted themselves on Chi-
nese text processing.
The biggest characteristic of Chinese language
is its trinity of sound, form and meaning (Pan,
2002). Hanyu Pinyin is the form of sound for
Chinese text and the Chinese phonology informa-
tion is explicit expressed by Pinyin which is the
inner features of Chinese Characters. And it nat-
urally contributes to the identification of Out-Of-
Vacabulary words (OOV).
In our work, Chinese phonology information is
used as basic features of Chinese characters in all
models. For open tracks, we propose a new dou-
ble hidden layers HMM in which a new phonol-
ogy information is built in as a hidden layer, a
new lexical association is proposed to deal with
the OOV questions and domains? adaptation ques-
tions. And for closed tracks, CRF model has been
used , combined with Chinese inner phonology in-
formation. We used the CRF++ package Version
0.43 by Taku Kudo1.
In the rest sections of this paper, we firstly in-
troduce the Chinese phonology in Section 2. Then
in the Section 3, the models used in our tasks are
presented. And the experiments and results are
described in Section 4. Finally, we give the con-
clusions and make prospect on future work.
2 Chinese Phonology
Hanyu Pinyin is the form of sound for Chi-
nese text and the Chinese phonology informa-
tion is explicit expressed by Pinyin. It is cur-
rently the most commonly used romanization sys-
tem for Standard Mandarin. Hanyu means the
Chinese language, and Pinyin means ?phonetics?,
or more literally, ?spelling sound? or ?spelled
sound? (wikipedia, 2010). The system has been
employed to teach Mandarin as home language
or as second language by China, Malaysia, Sin-
gapore et.al. Pinyin has been the most Chinese
character?s input method for computers and other
devices.
1http://crfpp.sourceforge.net/
The romanization system was developed by a
government committee in the People?s Repub-
lic of China, and approved by the Chinese gov-
ernment on February 11, 1958. The Interna-
tional Organization for Standardization adopted
pinyin as the international standard in 1982, and
since then it has been adopted by many other
organizations(wikipedia, 2010). In this system,
pinyin is composed by initials(pinyin: shengmu),
finals(pinyin: yunmu) and tones(pinyin: sheng-
diao) instead of consonants and vowels used in
European language. For example, the Pinyin of
??? is ?zhong1? composed by ?zh?, ?ong? and
?1?. In which ?zh? is initial, ?ong? is final and
?1? is the tone.
Every language has its rhythm and rhyme, so
Chinese is no exception. The rhythm system are
the driving force from the unconscious habit of
language(Edward, 1921). And the Pinyin?s finals
contribute the Chinese rhythm system, Which is
the basic assumption our research based on.
3 Algorithms
Generally the task of segmentation can be viewed
as a sequence labeling problem. We first define a
tag set as TS = {B, I, E, S}, shown in Table 1.
Table 1: The tag set used in this paper.
Label Explanation
B beginning character of a word
I inner character of a word
E end character of a word
S a single character as a word
For the piece ??????????? of
the example described in the experiments section,
firstly, the TS tags are labeled to it. And its re-
sult is ??/S?/B?/E?/S?/B?/E?/B?/I
?/E?. Then the tags are combined sequentially to
get the finally result ?? ?? ? ?? ????.
In this section, A novel HMM solution is pre-
sented firstly for open tracks. Then the CRF solu-
tion for closed tracks is introduced.
3.1 Double hidden layers? HMM
For a given piece of Chinese sentence, X =
x1x2 . . . xT , where xi, i = 1, . . . , T is a Chinese
character. Suppose that we can give each Chinese
character xi a Pinyin?s final yi. And suppose the
label sequence of X is S = s1s2 . . . sT , where
si ? TS is the tag of xi. Then what we want to
find is an optimal tag sequence S? which is de-
fined in (1).
S? = argmax
S
P (S, Y |X)
= argmax
S
P (X|S, Y )P (S, Y ) (1)
The model is described in Fig. 1. For a given
piece of Chinese character strings, One hidden
layer is label sequence S. Another hidden layer is
Pinyin?s finals sequence Y . The observation layer
is the given piece of Chinese characters X .
Figure 1: Double Hidden Markov Model
For transition probability, second-order Markov
model is used to estimate probability of the double
hidden sequences as described in (2).
P (S, Y ) =
?
t
p(st, yt|st?1, yt?1) (2)
For emission probability, we keep the first-
order Markov assumption as shown in (5).
P (X|S, Y ) =
?
t
p(xt|st, yt) (3)
3.1.1 Nbest results
Based on the work of (Jiang, 2010), a word lat-
tice is also built firstly, then in the second step, the
backward A? algorithm is used to find the top N
results instead of using the backward viterbi al-
gorithm to find the top one. The backward A?
search algorithm is described as follow (Wang,
2002; Och, 2001).
3.1.2 Reranking with a new lexical statistic
measure
Given two random Chinese charactersX and Y
and assume that they appears in an aligned region
of the corpus. The distribution of the two random
Chinese characters could be depicted by a 2 by 2
contingency table shown in Fig. 2(Chang, 2002).
Figure 2: A 2 by 2 contingency table
In Fig. 2, a is the counts ofX and Y co-occur; b
is the counts of the cases thatX occurs but Y does
not; c is the counts of the cases that X does not
occur but Y does; d is the counts of the cases that
both X and Y do not occur. The Log-likelihood
rate is calculated by (4).
LLR(x, y) = 2(a ? log a ? N
(a + b) ? (a + c)
+ b ? log b ? N
(a + b) ? (b + d)
+ c ? log c ? N
(c + d) ? (a + c)
+ d ? log d ? N
(c + d) ? (b + d)
) (4)
For the N-best result described in sec. 3.1.1,
they can be re-ranked by (5).
S? = argmin
S
(scoreh(S)+
?
K
K
?
k=1
LLR(xk, yk))
(5)
where scoreh is the negative log value of
P (S, Y |X). K is the number of breaks in X and
xk is the left Chinese character of the k break
and yk is the right Chinese character of the k
break. ? is the regulatory factor(in our experi-
ments ? = 0.45).
Bigger value of LLR(xk, yk) means stronger
ability in combining of the two characters xk and
yk, then they should not be segmented.
3.2 CRF model for closed tracks
Conditional random field, as statistical sequence
labeling model, has been used widely in segmen-
tation(Lafferty, 2001; Zhao, 2006). In the closed
tracks of the paper, we also use it.
3.2.1 Feature templates
We adopted two main kinds of features: n-gram
features and Pinyin?s finals features. The n-gram
feature set is quite orthodox, they are, namely, C-
2, C-1, C0, C1, C2, C-2C-1, C-1C0, C0C1, C1C2.
The Pinyin?s finals feature set is the same as n-
gram feature set. They are described in Table. 2.
Table 2: Feature templates
Templates Category
C-2, C-1, C0, C1, C2 N-gram: Unigram
C-2C-1, C-1C0, C0C1, C1C2 N-gram: Bigram
P-2, P-1, P0, P1, P2 Phonetic: Unigram
P-2P-1, P-1P0, P0P1, P1P2 Phonetic: Bigram
4 Experiments and Results
4.1 Dataset
We build a basic words dictionary for DHHMM
and a Pinyin?s finals dictionary for both DHHMM
and CRF from The Grammatical Knowledge-base
of Contemporary Chinese(Yu, 2001). For the fi-
nals dictionary, we give each Chinese character a
final extracted from its Pinyin. When it comes to
a polyphone, we just combine its all finals simply
to one. For example, ??{ong}?, ??{a&ai&i}?.
The training corpus (5,769 KB) we used is the
Labeled Corpus provided by the organizer. We
firstly add the Pinyin?s finals to each Chinese
character of it, then we train the parameters of
DHHMM and CRF model on it.
And the test corpus contains four domains: Lit-
erature (A), Computer (B), Medicine (C) and Fi-
nance(D).
The LLR function?s parameters{a, b, c, d} are
counted from the current test corpus A, B, C, or
D. It?s means that for segmenting A, the LLR pa-
rameters are counted from A, so the same for seg-
menting B, C and D.
4.2 Preprocessing
The date, time, numbers and symbols information
are easily identified by rules. We propose four
regular expressions? processes, in which the reg-
ular expressions? processes are handled one after
another in order of date, time, numbers and sym-
bols. By now, a rough segmentation can be done.
For a character stream, the date, time, numbers
and symbols are firstly identified, then the whole
stream can be divided by these units to some
pieces of character strings which will be segment
by the models described in sec. 3. For example,
a character stream ?2009??8?31?????
??????12?????? will be divided
to ?2009? ? 8? 31? ? ????????
? 12 ???? ??. Then the pieces ???, ??
?????????, ?????? will be seg-
mented sequentially by the models described in
Section 3.
4.3 Results on DHHMM
We evaluate our system by Precision Rate(6), Re-
call Rate(7), F1 measure(8) and OOV(Out-Of-
Vocabulary) Recall rate(9).
P = C(correct words in segmented result)
C(words in segmented result)
(6)
R = C(correct words in segmented result)
C(words in standard result)
(7)
F1 = 2 ? P ? R
P + R
(8)
OR = C(correct OOV in segmented result)
C(OOV in standard result)
(9)
In (6-9), C(? ? ?) is the count of (? ? ?).
Table 3 are the results of the DHHMM on open
tracks.
In Table 3, OOV RR is the recall rate of OOV,
IV RR is the recall rate of IV(In Vocabulary).
4.4 Postprocessing for CRF and Results on It
Since the CRF segmenter will not always return
a valid tag sequence that can be translated into
segmentation result, some corrections should be
made if such error occurs. We devised a dynamic
programming routine to tackle this problem: first
we compute the valid tag sequence that closest to
Table 3: Results of open tracks using DHHMM:
Literature (A), Computer (B), Medicine (C) and
Finance(D)
A B C D
R 0.893 0.918 0.917 0.928
P 0.918 0.896 0.907 0.934
F1 0.905 0.907 0.912 0.931
OOV RR 0.803 0.771 0.704 0.808
IV RR 0.899 0.945 0.943 0.939
the output of CRF segmenter (by term closest, we
mean least hamming distance), if there is a tie, we
choose the one has the least ?S? tags, if the tie still
exists, we choose the one that comes lexicograph-
ically earlier (B < I < E < S, described in
Table. 1). Table 4 are the results of the CRF on
closed tracks.
Table 4: Results of closed tracks using CRF: Lit-
erature (A), Computer (B), Medicine (C) and Fi-
nance(D)
A B C D
R 0.945 0.946 0.94 0.956
P 0.946 0.914 0.928 0.952
F1 0.946 0.93 0.934 0.954
OOV RR 0.816 0.808 0.761 0.849
IV RR 0.954 0.971 0.962 0.966
From the results of Table 3 and Table 4, we
can observe that the CRF model outperforms the
DHHMM by average 2.72% in F1 measure. In the
other hand, from Table 5, we can see that the com-
putation cost in DHHMM is less than half of the
time cost and lower one-fifth memory cost than
CRF model.
Table 5: The computation cost in DHHMM and
CRF
Time cost(ms) Memory cost(MB)
DHHMM 34398 16.3
CRF 43415 35
5 Conclusions and Future works
This paper has presented a double hidden lawyers
HMM for Chinese word segmentation task in
SIGHAN bakeoff 2010. It firstly created N top
results and then select the best one from it by a
new lexical association.
Chinese phonology (specially by Pinyin?s final
in text) is very useful inner information of Chinese
language, which is the first time used in our mod-
els. We have used it in both DHHMM and CRF
model.
In future work, there are lots of improvements
can be done. Firstly, which polyphone?s finals
should be used in a given context is a visible ques-
tion. And the strategy to train the parameter ? de-
scribed in 3.1.2 can also be improved.
Acknowledgments
This research has been partially supported by
the National Science Foundation of China (NO.
NSFC90920006). We also thank Caixia Yuan for
leading our discuss, Li Sun, Peng Zhang, Yaojing
Chen, Zhixu Lin, Gan Lin, Guannan Fang for their
useful helps in this work.
References
Wenguo Pan. 2002. zibenwei yu hanyu yanjiu:120?
141. East China Normal University Press.
Sapir Edward 1921. Language: An introduction to the
study of speech:230. New York: Harcourt, Brace
and company.
wikipedia. 2010. Pinyin. http://en.wikipedia.org/wiki
/Pinyin#cite note-6.
Baobao Chang, Pernilla Danielsson, and Wolfgang
Teubert. 2002. Extraction of translation unit from
chinese-english parallel corpora, Proceedings of
the first SIGHAN workshop on Chinese language
processing:1?5.
Huixing Jiang, Xiaojie Wang, Jilei Tian. 2010.
Second-order HMM for Event Extraction from Short
Message, 15th International Conference on Ap-
plications of Natural Language to Information Sys-
tems, Cardiff, Wales, UK.
Franz Josef Och, Nicola Ueffing, Hermann Ney. 2001.
An EfficientA? Search Algorithm for Statistical Ma-
chine Translation, Proceedings of the ACL Work-
shop on Data-Driven methods in Machine Transla-
tion 14(Toulouse, France): 1-8.
Ye-Yi Wang, Alex Waibel. 2002. Decoding Algo-
rithm in Statistical Machine Translation, Proceed-
ings of the 35th Annual Meeting of the Association
for Computational Linguistics and Eighth Confer-
ence of the European Chapter of the Association for
Computational Linguistics: 366-372.
Yu Shiwen, Zhu Xuefeng, Wang Hui. 2001. New
Progress of the Grammatical Knowledge-base of
Contemporary Chinese, ZHONGWEN XINXI
XUEBAO, 2001Vol. 01.
John Lafferty, A.Mccallum, F.Pereira. 2001. Condi-
tional Random Field: Probabilitic Models for Seg-
menting and Labeling Sequence Data., Proceedings
of the Eighteenth International Conference on Ma-
chine Learning: 282?289.
Hai Zhao, Changning Huang, Mu Li. 2006. An
Improved Chinese Word Segmentation System with
Conditional Random Field, Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing (SIGHAN-5)(Sydney, Australia):162-165.
