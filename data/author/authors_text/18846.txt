Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 409?420, Dublin, Ireland, August 23-29 2014.
Machine Translation Quality Estimation Across Domains
Jos
?
e G. C. de Souza
University of Trento
Fondazione Bruno Kessler
Trento, Italy
desouza@fbk.eu
Marco Turchi
Fondazione Bruno Kessler
Trento, Italy
turchi@fbk.eu
Matteo Negri
Fondazione Bruno Kessler
Trento, Italy
negri@fbk.eu
Abstract
Machine Translation (MT) Quality Estimation (QE) aims to automatically measure the quality
of MT system output without reference translations. In spite of the progress achieved in re-
cent years, current MT QE systems are not capable of dealing with data coming from different
train/test distributions or domains, and scenarios in which training data is scarce. We investigate
different multitask learning methods that can cope with such limitations and show that they over-
come current state-of-the-art methods in real-world conditions where training and test data come
from different domains.
1 Introduction
Machine Translation (MT) Quality Estimation (QE) aims to automatically predict the quality of MT
output without using reference translations (Blatz et al., 2003; Specia et al., 2009). QE systems usually
employ supervised machine learning models that use different information extracted from (source, target)
sentence pairs as features along with quality scores as labels. The notion of quality that these models
measure can be indicated by different scores. Some examples are the average number of edits required
to post-edit the MT output, i.e., human translation edit rate
1
(HTER (Snover et al., 2006)), and the time
(in seconds) required to post-edit a translation produced by an MT system (Specia, 2011).
Research on QE has received a strong boost in recent years due to the increase in the usage of MT
systems in real-world applications. Automatic and reference-free MT quality prediction demonstrated
to be useful for different applications, such as: deciding whether the translation output can be published
without post-editing (Soricut and Echihabi, 2010), filtering out low-quality translation suggestions that
should be rewritten from scratch (Specia et al., 2009), selecting the best translation output from a pool
of MT systems (Specia et al., 2010), and informing readers of the translation whether it is reliable or not
(Turchi et al., 2012). Another example is the computer-assisted translation (CAT) scenario, in which it
might be necessary to predict the quality of translation suggestions generated by different MT systems
to support the activity of post editors working with different genres of text.
The dominant QE framework presents some characteristics that can limit models? applicability in
such real-world scenarios. First, the scores used as training labels (HTER, time) are costly to obtain
because they are derived from manual post-editions of MT output. Such requirement makes it difficult
to develop models for domains in which there is a limited amount of labeled data. Second, the learning
methods currently used (for instance in the framework of QE shared evaluation campaigns)
2
assume that
training and test data are sampled from the same distribution. Though reasonable as a first evaluation
setting to promote research in the field, this controlled scenario is not realistic as different data in real-
world applications might be post-edited by different translators, the translations might be generated by
different MT systems and the documents being translated might belong to different domains or genres. To
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Edit distance is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by the
number of words in the reference. Lower HTER values indicate better translations.
2
In the last two editions of the yearly Workshop on Machine Translation, several QE shared tasks have been proposed
(Callison-Burch et al., 2012; Bojar et al., 2013).
409
overcome these limitations a plausible research objective is to exploit techniques that: (i) allow domains
and distributions of features to be different between training and test data, and (ii) that cope with the
scarce amount of training labels by sharing information across domains, a common scenario for transfer
learning.
In this paper we investigate the use of techniques that can exploit the training instances from different
domains to learn a QE model for a specific target domain for which there is a small amount of labeled
data. In particular, we are interested in approaches that allow not only learning from one single source
domain but also from multiple source domains simultaneously, by leveraging the labels from all available
data to improve results in a target domain.
Given these requirements, we experiment with different multitask learning techniques that perform
transfer learning via a common task structure (domain relatedness). Furthermore, we employ an approach
based on feature augmentation that has been successfully used in other natural language processing tasks.
We present a series of experiments over three domains with increasing amounts of training data, showing
that our adaptive approaches outperform competitive baselines.
The contributions of our work are: (i) a first exploration of techniques that overcome the limitation
of current QE learning methods when dealing with data with different training and test distributions and
domains, and (ii) an empirical verification of the amount of training data required by such techniques to
outperform competitive baselines on different target domains. To the best of our knowledge, this is the
first work addressing the challenges posed by domain adaptation in MT QE.
2 Related Work
Quality estimation has recently gained increasing attention, also boosted by two evaluation campaigns
organized within the Workshop on Machine Translation (WMT) (Callison-Burch et al., 2012; Bojar et
al., 2013). The bulk of work done so far has focused on the controlled WMT evaluation framework and,
in particular, on two major aspects of the problem: feature engineering and machine learning methods.
Feature engineering accounts for linguistically-based predictors that aim to model different perspec-
tives of the quality estimation problem. The research ranges from identifying indicators that approximate
the complexity of translating the source sentence and designing features that model the fluency of the
automatically generated translation, to linguistically motivated measures that estimate how adequate the
translation is in comparison to the source sentence in terms of meaning (Blatz et al., 2003; Mehdad et
al., 2012; Hardmeier et al., 2012; Rubino et al., 2012; Specia et al., 2012; de Souza et al., 2013a).
State-of-the-art QE explores different supervised linear or non-linear learning methods for regression
or classification such as Support Vector Machines (SVM), different types of Decision Trees, Neural
Networks, Elastic-Net, Gaussian Processes, Naive Bayes, among others (Specia et al., 2009; Buck,
2012; Beck et al., 2013; Souza et al., 2014). Another aspect related to the learning methods that has
received attention is the optimal selection of features in order to overcome issues related with the high-
dimensionality of the feature space (Soricut et al., 2012; de Souza et al., 2013a; Beck et al., 2013; de
Souza et al., 2013b).
Despite constant improvements, such learning methods have limitations. The main one is that they
assume that both training and test data are independently and identically distributed. As a consequence,
when they are applied to data from a different distribution or domain they show poor performance. This
limitation harms the performance of QE systems for several real-world applications, such as CAT envi-
ronments. Advanced CAT systems currently integrate suggestions obtained from MT engines with those
derived from translation memories (TMs). In such framework, the compelling need to speed up the trans-
lation process and reduce its costs by presenting human translators with good-quality suggestions raises
interesting research challenges for the QE community. In such environments, translation jobs come from
different domains that might be translated by different MT systems and are routed to professional transla-
tors with different idiolect, background and quality standards (Turchi et al., 2013). Such variability calls
for flexible and adaptive QE solutions by investigating two directions: (i) modeling translator behaviour
(Turchi et al., 2014) and (ii) maximize the learning capabilities from all the available data. The second
research objective motivates our investigation on methods that allow the training and test domains and
410
the distributions to be different.
Recent work in QE focused on aspects that are problematic even in the controlled WMT scenario, and
are closely related to the flexibility/adaptability issue. Focusing on the first of the two aforementioned
directions (i.e. modeling translators? behaviour), Cohn and Specia (2013) propose a Multitask Gaussian
Process method that jointly learns a series of annotator-specific models and that outperforms models
trained for each annotator. Our work differs from theirs in that we are interested in the latter research
direction (i.e. coping with domain and distribution diversity) and we use in and out-of-domain data to
learn robust in-domain models. Our scenario represents a more challenging setting than the one tackled
in (Cohn and Specia, 2013), which does not consider different domains.
In transfer learning there are many techniques suitable to fulfill our requirements. The aim of transfer
learning is to extract the knowledge from one or more source tasks and apply it to a target task (Pan
and Yang, 2010). One type of transfer learning is multitask learning (MTL), which uses domain-specific
training signals of related tasks to improve model generalization (Caruana, 1997). Although it was not
originally thought for transferring knowledge to a new task, MTL can be used to achieve this objective
due to its capability to capture task relatedness, which is important knowledge that can be applied to a
new task (Jiang, 2009).
Domain adaptation is a kind of transfer learning in which source and target domains (i.e. training and
test) are different but the tasks are the same (Pan and Yang, 2010). The domain adaptation techniques
that inspire our work have been successfully applied to a variety of NLP tasks (Blitzer et al., 2006;
Jiang and Zhai, 2007). For instance, an effective solution for supervised domain adaptation, EasyAdapt
(SVR FEDA henceforth), was proposed in (Daum?e III, 2007) and applied to named entity recognition,
part-of-speech tagging and shallow parsing. The approach transforms the domain adaptation problem
into a standard learning problem by augmenting the source and target feature set. The feature space is
transformed to be a cross-product of the features of the source and target domains augmented with the
original target domain features. In supervised domain adaptation one has access to out-of-domain labels
and wants to leverage a small amount of available in-domain labeled data to train a model (Daum?e III,
2007), the case of this study. This is different from the semi-supervised case in which in-domain labels
are not available.
3 Adaptation for QE
An important assumption in MTL is that different tasks (domains in our case) are correlated via a certain
structure. Examples of such structures are the hidden layers in a neural network (Caruana, 1997) and
shared feature representation (Argyriou et al., 2007) among others. This common structure allows for
knowledge transfer among tasks and has been demonstrated to improve model generalization over single
task learning (STL) for different problems in different areas. Under this scenario, several assumptions
can be made about the relatedness among the tasks, leading to different transfer structures. We explore
three approaches to MTL that deal with task relatedness in different ways. These are the ?Dirty? approach
to MTL (Jalali et al., 2010), Sparse Trace MTL (Chen et al., 2012) and Robust MTL (Chen et al., 2011).
The three approaches use different regularization techniques that capture task relatedness using norms
over the weights of the features.
Before describing the three approaches, we introduce some basic notation similar to (Chen
et al., 2011). In MTL there are T tasks and each task t ? T has m training samples
{(x
(t)
1
, y
(t)
1
), . . . , (x
(t)
m
, y
(t)
m
)}, with x
(t)
i
? R
d
where d is the number of features and y
(t)
i
? R is the
output (the response variable or label). The input features and labels are stacked together to form two
different matrices X
(t)
= [x
(t)
1
, . . . , x
(t)
m
] and Y
(t)
= [x
(t)
1
, . . . , x
(t)
m
], respectively. The weights of the
features for each task are represented by W , where each column corresponds to a task and each row
corresponds to a feature.
The ?Dirty? approach to MTL follows the idea that different tasks may share the same discriminative
features (Argyriou et al., 2007). However, it also considers that different tasks might have different
discriminative features that are inherent to each task. Therefore, the method encourages shared-sparsity
among tasks and among features in each task. It decomposes W into two components, one is a row-
411
sparsed matrix that corresponds to the features shared among the tasks and the other is an element-wise
sparse matrix that corresponds to the non-shared features that are important for each task independently.
More formally, the ?Dirty? approach is explained by Equation 1.
min
W
T
?
t=1
||(W
(t)
X
(t)
? Y
(t)
)||
2
2
+ ?
s
||S||
1
+ ?
b
||B||
1,?
subject to: W = S +B (1)
where ||(W
(t)
X
(t)
? Y
(t)
)||
2
2
is the least squares loss function, S is the regularization term that en-
courages element-wise sparsity and B is the block-structured row-sparsity regularizer. The ||.||
2
is the
l
2
-norm (Euclidean distance), ||.||
1
is the l
1
-norm (given by
?
i=1
|x
i
|) and ||.||
1,?
is the row grouped l
1
-
norm. The ?
s
and ?
b
are non-negative trade-off parameters that control the amount of regularization
applied to S and B, respectively.
Sparse Trace MTL considers the problem of learning incoherent sparse and low-rank patterns from
multiple related tasks. This approach captures task relationship via a shared low-rank structure of the
weight matrix W . As computing the low-rank structure of a matrix leads to a NP-hard optimization
problem, Chen et al. (2012) proposed to compute the trace norm as a surrogate, making the optimization
problem tractable. In addition to learning the low-rank patterns, this method also considers the fact that
different tasks may have different inherent discriminative features. It decomposes W into two compo-
nents: S, which models element-wise sparsity, and Q, which captures task relationship via the trace
norm. The convex problem minimized by Sparse Trace is given in Equation 2.
min
W
T
?
t=1
||(W
(t)
X
(t)
? Y
(t)
)||
2
2
+ ?
s
||S||
1
subject to: W = S +Q, ||Q||
?
< ?
p
(2)
where ||.||
?
is the trace norm, given by the sum of the singular values ?
i
of W , i.e., ||W ||
?
=
?
i=1
?
i
(W ). Here, ?
p
controls the rank of Q and ?
s
controls the sparsity of S.
The key assumption in MTL is that tasks are related in some way. However, this assumption might not
hold for a series of real-world problems. In situations in which tasks are not related a negative transfer
of information among tasks might occur, harming the generalization of the model. One way to deal
with this problem is to: (i) group related tasks in one structure and share knowledge among them, and
(ii) identify irrelevant tasks maintaining them in a different group that does not share information with
the first group. This is the idea of Robust MTL (RMTL henceforth). The algorithm approximates task
relatedness via a low-rank structure like Sparse Trace and identifies outlier tasks using a group-sparse
structure (column-sparse, at task level). Robust MTL is described by Equation 3. It employs a non-
negative linear combination of the trace norm (the task relatedness component L) and a column-sparse
structure induced by the l
1,2
-norm (the outlier task detection component S). If a task is an outlier it will
have non-zero entries in S.
min
W
T
?
t=1
||(W
(t)
X
(t)
? Y
(t)
)||
2
2
+ ?
l
||L||
?
+ ?
s
||S||
1,2
subject to: W = L+ S (3)
where ||S||
1,2
is the group regularizer that induces sparsity on the tasks.
4 Experimental Setting
In this section we describe the data used for our experiments, the features extracted, the set up of the
learning methods, the baselines used for comparison and the evaluation of the models. The goal of our
experiments is to show that the methods presented in Section 3 outperform competitive baselines and
standard QE learning methods that are not capable of adapting to different domains. We experiment with
three different domains of comparable size and evaluate the performance of the adaptive methods and the
standard techniques with different amounts of training data. The MTL models described in section 3 are
trained with the Malsar toolkit implementation (Zhou et al., 2012). The hyper-parameters are optimized
412
using 5-fold cross-validation in a grid search procedure. The parameter values are searched in an interval
ranging from 10
?3
to 10
3
.
4.1 Data
Our experiments focus on the English-French language pair and encompass three very different domains:
newswire text (henceforth News), transcriptions of Technology Entertainment Design talks (TED) and
Information Technology manuals (IT). Such domains are a challenging combination for adaptive systems
since they come from very different sources spanning speech and written discourse (TED and News/IT,
respectively) as well as a very well defined and controlled vocabulary in the case of IT.
Each domain is composed of 363 tuples formed by the source sentence in English, the French trans-
lation produced by an MT system and a human post-edition of the translated sentence. For each pair
(translation, post-edition) we use as labels the HTER score computed with TERCpp
3
. For the three do-
mains we use half of the data for training (181 instances) and half of the data for testing (182 instances).
The limited amount of instances for training contrasts with the 800 or more instances of the WMT evalu-
ation campaigns and is closer to real-world applications where the availability of large and representative
training sets is far from being guaranteed (e.g. the CAT scenario).
The sentence tuples for the first two domains are randomly sampled from the Trace corpus
4
. The
translations were generated by two different MT systems, a state-of-the-art phrase-based statistical MT
system and a commercial rule-based system. Furthermore, the translations were post-edited by up to four
different translators, as described in (Wisniewski et al., 2013).
Domain No. of tokens Vocab. size Avg. sent. length
TED source 6858 1659 19
TED target 7016 1828 19
IT source 3310 1004 9
IT target 3134 1049 8
News source 7605 2273 21
News target 8230 2346 23
Table 1: Datasets statistics for each domain.
The TED talks domain is formed by subtitles of several talks in a range of topics presented in the TED
conferences. The complete dataset has been used for MT and automatic speech recognition systems
evaluation within the International Workshop on Spoken Language Translation (IWSLT). The News
domain is formed by newswire text used in WMT translation campaigns and covers different topics. The
IT texts come from a software user manual translated by a statistical MT system based on the state-of-
the-art phrase-based Moses toolkit (Koehn et al., 2007) trained on about 2M parallel sentences. The
post-editions were collected from one professional translator operating on the Matecat
5
CAT tool in
real working conditions. Table 1 provides macro-indicators (number of tokens, vocabulary size, average
sentence length) that evidence the large difference between the domains addressed by our experiments
and give an idea of the difficulty of the task.
A peculiarity of the TED domain is that it is formed by manual transcriptions of speech translated by
different MT systems, configuring a different type of discourse than News and IT. In TED, the vocabulary
size in the source and target sentences is lower than that of the News domain but higher than IT. News
presents the most varied vocabulary, which is an evidence of the more varied lexical choice represented
by the several topics that compose the domain. Moreover, News has the highest average sentence length,
a characteristic of non-technical written discourse, which tends to have longer sentences than spoken
discourse and domains dominated by technical jargon. Such a characteristic is exactly what differentiates
IT from the other two domains. IT sentences are technical and present a reduced average number of
3
http://sourceforge.net/projects/tercpp/
4
http://anrtrace.limsi.fr/trace_postedit.tar.bz2
5
www.matecat.com
413
words, as evidenced by the vocabulary size (the smallest among the three domains). These numbers
suggest a divergence between IT and the other two domains, possibly making adaptation more difficult.
4.2 Features
For all the experiments we use the same feature set composed of seventeen features proposed in (Specia et
al., 2009). The set is formed by features that model the complexity of translating the source sentence (e.g.
the average source token length or the number of tokens in the source sentence), and the fluency of the
translated sentence produced by the MT system (e.g. the language model probability of the translation).
The decision to use this feature set is motivated by the fact that it demonstrated to be robust across
language pairs, MT systems and text domains (Specia et al., 2009). The 17 features are:
? number of tokens in the source sentence and in the generated translation;
? average source token length;
? average number of occurences of the target word within the generated translation;
? language model probability of the source sentence and generated translation;
? average number of translations per source word in the sentence: as given by IBM 1 model thresh-
olded so that P (t|s) > 0.2 weighted by the inverse frequency of each word in the source side of the
SMT training corpus?;
? average number of translations per source word in the sentence: as given by IBM 1 model thresh-
olded so that P (t|s) > 0.01 weighted by the inverse frequency of each word in the source side of
the SMT training corpus;
? percentage of unigrams?, bigrams and trigrams? in the first quartile of frequency (lower fre-
quency words) in a corpus of the source language;
? percentage of unigrams?, bigrams and trigrams in the fourth quartile of frequency (higher fre-
quency words) in a corpus of the source language;
? percentage of unigrams in the source sentence seen in the source side of the SMT training corpus;
? number of punctuation marks in the source sentence and in the hypothesis translation;
4.3 Baselines
As a term of comparison, we consider these baselines in our experiments. A simple to implement but
difficult to beat baseline when dealing with regression on tasks with different distributions is to compute
the mean of the training labels and use it as the prediction for each testing point (Rubino et al., 2013).
Hereafter we refer to this baseline as ?. Since supervised domain adaptation techniques should outper-
form models that are trained only on the available in-domain data, we also use as baseline the regressor
built only on the available in-domain data (SVR in-domain). Furthermore, as a third baseline, we train a
regressor by pooling together training data of all domains, combining source and target data without any
kind of task relationship mechanism (SVR Pooling).
The baselines are trained on the feature set described earlier in Section 4.2 with an SVM regression
(SVR) method using the implementation of Scikit-learn (Pedregosa et al., 2011). The radial basis func-
tion (RBF) kernel is used for all baselines. The hyper-parameters of the model are optimized using
randomized search optimization process with 50 iterations as described in (Bergstra and Bengio, 2012)
and used previously for QE in (de Souza et al., 2013a). The best parameters are found using 5-fold
cross-validation on the training data and , ? and C are sampled from exponential distributions scaled at
0.1 for the first two parameters and scaled at 100 for the last one. It is important to notice that the SVR
with RBF kernel methods learn non-linear models that have been shown to perform better than linear
models on the set of features used for predicting HTER. On the contrary, the MTL methods presented in
Section 3 are methods that do not explore kernels or any other kind of non-linear learning method.
414
Source / Target IT
tgt
News
tgt
TED
tgt
IT
src
0.2081 0.2341 0.2232
News
src
0.2368 0.1690 0.2130
TED
src
0.2183 0.2263 0.1928
Table 2: Results of the SVR in-domain baseline trained and evaluated in each domain (average of 50
different shuffles). Rows represent the domain data used to train the model and columns represent the
domain data used to evaluate the model. Scores are MAE.
4.4 Evaluation
The accuracy of the models is evaluated with the mean absolute error (MAE), which was also used in
previous work and in the WMT QE shared tasks (Bojar et al., 2013). MAE is the average of the absolute
difference between the prediction y?
i
of a model and the gold standard response y
i
(Equation 4). As it is
an error measure, lower values mean better performance.
MAE =
1
m
m
?
i=1
|y?
i
? y
i
| (4)
To test the statistical significance of our results we need to perform comparisons of multiple models.
In addition, we would like to test the significance over different training amounts. Given these require-
ments we need to perform multiple hypothesis tests instead of paired tests. It has been shown that for
comparisons of multiple machine learning models, the recommended approach is to use a non-parametric
multiple hypothesis test followed by a post-hoc analysis that compares each pair of hypothesis (Dem?sar,
2006). In our experiments we use the Friedman test (Friedman, 1937; Friedman, 1940) followed by a
post-hoc analysis of the pairs of regressors using Holm?s procedure (Holm, 1979) to perform the pairwise
comparisons when the null hypothesis is rejected. All tests for both Friedman and post-hoc analysis are
run with ? = 0.05. For more details about these methods, we refer the reader to (Dem?sar, 2006; Garcia
and Herrera, 2008) which provide a complete review about the application of multiple hypothesis testing
to machine learning methods.
5 Results and Discussion
Our experiments are organized as follows. First, we evaluate the performance of single task learning
methods on different cross-domain experiments. Then, we report the evaluation for the multitask learning
methods and discuss the results.
5.1 Single Task Learning
With the objective of having an insight about the difference between the domains, we train the SVR
in-domain baseline with all available training data for each domain and evaluate its performance on the
same domain and in the two remaining domains.
Results are reported in Table 2, where the diagonal shows the figures for the in-domain evaluation.
These numbers suggest that the IT domain configures a more difficult challenge for the learning algo-
rithm. The IT in-domain model (IT
src
-IT
tgt
) presents a performance 21% inferior to News and 8%
inferior to TED. For all models trained on a source domain different than the target domain there is a
drop in performance, as it is expected from a system that assumes that training and test data are sampled
from the same distribution. In addition, when predicting IT using the model trained on News, we have a
perfomance drop of 13% whereas using the model trained on TED the performance drops up to 4%.
5.2 Multitask learning
We run the baselines described in Section 4.3 and the methods described in Section 3 on different
amounts of training data, ranging from 18 to 181 instances (10% and 100%, respectively). The mo-
tivation is to verify how much training data is required by the MTL methods to outperform the baselines
for a target domain. Table 3 presents the results for the three domains with models trained on 30, 50 and
415
100% of the training data (54, 90 and 181 instances, respectively). Each method was run on 50 different
train/test splits of the data in order to account for the variability of points in each split.
Method TED News IT
30 % of training data (54 instances)
mean 0.1951 0.1711 0.2174
SVR In-Domain 0.2013 0.1753 0.2235
SVR Pooling 0.1962 0.1899 0.2201
SVR FEDA 0.1952 0.1839 0.2193
MTL Dirty 0.1954 0.1708 0.2193
MTL SparseTrace 0.1976 0.1743 0.2222
MTL RMTL 0.1946 0.1685 0.2162
50% of training data (90 instances)
mean 0.1943 0.1707 0.2170
SVR In-Domain 0.1976 0.1711 0.2183
SVR Pooling 0.1951 0.1865 0.2191
SVR FEDA 0.1937 0.1806 0.2161
MTL Dirty 0.1927 0.1678 0.2148
MTL SparseTrace 0.1922 0.1672 0.2157
MTL RMTL 0.1878 0.1653 0.2119
100% of training data (181 instances)
mean 0.1936 0.1690 0.2162
SVR In-Domain 0.1928 0.1690 0.2081
SVR Pooling 0.1927 0.1849 0.2203
SVR FEDA 0.1908 0.1757 0.2107
MTL Dirty 0.1878 0.1666 0.2083
MTL SparseTrace 0.1881 0.1661 0.2094
MTL RMTL 0.1846 0.1653 0.2075
Table 3: Average performance of fifty runs
of the models on different train and test splits
with 30, 50 and 100 percent of training data.
The average scores reported are the MAE.
Figure 1: Visualization of the RMTL task outlier
model when trained on all the 181 instances of
training data. Cells with darker shades are closer
to zero. Cells with lighter shades are closer to one.
Columns with only black entries are considered in-
lier tasks (domains). From left to right, columns
correspond to News, TED and IT domains. The
first 17 rows correspond to the features used to
train the model and the last row in corresponds to
the bias term.
For all three domains, a general trend is that MTL RMTL is the method that reaches the lowest MAE
when compared to all the other models. Given the difference among the domains, it is very likely that
MTL Dirty and MTL SparseTrace suffer from the negative transfer problem (the assumption that all
tasks are similar does not hold). MTL RMTL is the only method among the methods presented here that
copes with negative transfer among tasks. The significance tests indicate that MTL RMTL improvements
are statistically significant with respect to all baselines depending on the range of training data used to
compute the test.
? For TED, the Friedman test rejects the null hypothesis with p = 4.62
?5
. Post-hoc analysis indicates
that there are differences statistically significant between MTL RMTL and all the three baselines
with p ? 0.002.
? For News, the Friedman test measures significant differences with p = 1.14
?9
and the post-hoc
analysis indicates that MTL RMTL is statistically significant with respect to SVR in-domain and
SVR Pooling with p = 0.002 for varying amounts of training data from 10 to 100%. As can be seen
in Figure 2, MTL RMTL starts with a very high MAE using 10% of the data (approximately 0.21
MAE) but improves dramatically with 20% of the data. Calculating the significance test with 20 to
100% of training data, MTL RMTL is significantly better than all baselines with p ? 2.89
?10
.
? For IT, in a similar situation to the News domain, RMTL is significantly better than all baselines
416
trained on 30% to 100% of the training data (Friedman test?s p = 2.86
?4
and post-hoc analysis?
p ? 3.73
?7
).
Another observed trend is that the MTL models benefit from increasing amounts of training data.
MTL RMTL has an improvement in performance of 5.13% for TED, 4% for News and 1.85% for IT
when trained on 100% of the training data in comparison with the model trained on 30% of training data.
18 36 54 72 90 108 126 144 162 181
Training data points
0.16
0.17
0.18
0.19
0.20
0.21
0.22
M
A
E
news as target domain
Mean
SVR RBF in-domain
SVR Pooling
SVR FEDA
MTL Dirty
MTL SparseTrace
MTL RMTL
Figure 2: Learning curves for the News domain.
The results for the IT domain are in line with the in-domain experiments in which we observed that
IT is a more challenging domain in comparison to TED and News. The MAE of IT is always higher
than for the other domains on in-domain and MTL experiments. Another evidence of this is the model
learned by the RMTL method when using all training data and run on one of the 50 training/test splits. A
graphic representation of the RMTL outlier task detection component (described in Section 3) is shown
in Figure 1.
From left to right, each column represents News, TED, and IT domains, respectively, while each row is
the instantiation of a feature in the corresponding task. Columns with non-black entries represent outlier
tasks. The highest number of entries with lighter shades is in the third column, IT. Several features in
this task are considered outliers with respect to the same features in the other tasks. Consequently, the
learning method takes the weights into consideration to a greater extent when learned with the outlier
model for the IT domain. Entries with the lightest shades in the IT domain correspond to the features
marked with? in Section 4.2. These outlier features are directly affected by the length of the sentences
on which they are computed (source or target) given that the number of tokens influences the final value
of the feature. This outcome goes in the same direction of our analysis of the three domains (Section 4.1)
that indicates a very different vocabulary size and average sentence length for IT when compared to the
other two domains.
To a lesser extent than IT, News and TED domains also present a few lighter-shaded entries in the
outlier component (1st and 2nd column). This suggests that MTL RMTL was capable of transfering
information among the domains in a more efficient way than the other MTL methods analyzed.
417
Overall the experiments presented show encouraging results in the direction of coping with QE data
coming from different domains/genres, translated by different MT systems and post-edited by different
translators. Results show that even in such difficult conditions, the methods investigated are capable of
outperforming competitive baselines based on non-linear models on different domains. As a rationale,
models that consider not only similarity between the domains but also deal with some sort of dissimilarity
should be considered. This is the case of the best performing method, MTL RMTL, which identifies
outlier tasks in order to avoid negative transfer among tasks.
6 Conclusion
In this work we presented an investigation of methods that overcome limitations presented by current
MT QE state-of-the-art systems when applied to real world conditions. In such scenarios (e.g. CAT
environment) the requirements are two-fold: (i) learning in the presence of different train/test feature
and label distributions and across different domains/genres, and (ii) the capability of learning with scarce
training data. In our experiments, we explored transfer learning methods, in particular multitask learning,
and we showed that such methods can cope with the needs of real-world scenarios.
We showed that multitask learning methods are capable to learn robust models for three different
domains that perform better than three strong baselines trained on the same amount of data. The methods
explored here benefit from increasing amounts of training data but also perform well when operating
with very limited amounts of data. We believe that the results obtained in this first exploration of model
adaptation for the problem can encourage the MT QE community to shift the focus from controlled
scenarios to more applicable, real-world contexts that require more robust methods.
Acknowledgements
This work has been partially supported by the EC-funded project MateCat (ICT-2011.4.2-287688).
References
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. 2007. Multi-task feature learning. In Advances
in neural information processing systems, volume 19.
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. SHEF-Lite: When less is more for translation
quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 337?342.
James Bergstra and Yoshua Bengio. 2012. Random Search for Hyper-Parameter Optimization. Journal of Ma-
chine Learning Research, 13:281?305.
Joseph John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto San-
chis, and Nicola Ueffing. 2003. Confidence estimation for machine translation. In 20th COLING, pages
315?321.
John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence
learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
120?128, Morristown, NJ, USA. Association for Computational Linguistics.
Ondej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Eighth Workshop on Statistical Machine Translation, pages 1?44.
Christian Buck. 2012. Black Box Features for the WMT 2012 Quality Estimation Shared Task. In Proceedings of
the 7th Workshop on Statistical Machine Translation, pages 91?95.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings
of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the 7th Workshop on Statistical
Machine Translation, pages 10?51, Montr{?e}al, Canada, June. Association for Computational Linguistics.
Rich Caruana. 1997. Multitask Learning. Machine learning, 28(1):41?75.
418
Jianhui Chen, Jiayu Zhou, and Jieping Ye. 2011. Integrating low-rank and group-sparse structures for robust multi-
task learning. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and
data mining - KDD ?11, page 42, New York, New York, USA. ACM Press.
Jianhui Chen, Ji Liu, and Jieping Ye. 2012. Learning incoherent sparse and low-rank patterns from multiple tasks.
ACM Transactions on Knowledge Discovery from Data, 5(4):22, February.
Trevor Cohn and Lucia Specia. 2013. Modelling Annotator Bias with Multi-task Gaussian Processes: An applica-
tion to Machine Translation Quality Estimation. In Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 32?42.
Hal Daum?e III. 2007. Frustratingly Easy Domain Ddaptation. In Conference of the Association for Computational
Linguistics (ACL).
Jos?e G. C. de Souza, Christian Buck, Marco Turchi, and Matteo Negri. 2013a. FBK-UEdin participation to
the WMT13 Quality Estimation shared-task. In Proceedings of the Eighth Workshop on Statistical Machine
Translation, pages 352?358.
Jos?e G.C. de Souza, Miquel Espl`a-Gomis, Marco Turchi, and Matteo Negri. 2013b. Exploiting qualitative infor-
mation from automatic word alignment for cross-lingual nlp tasks. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Volume 2: Short Papers), pages 771?776, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Janez Dem?sar. 2006. Statistical Comparisons of Classifiers over Multiple Data Sets. The Journal of Machine
Learning Research, 7:1?30, December.
Milton Friedman. 1937. The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of
Variance. Journal of the American Statistical Association, 32(200):675?701.
Milton Friedman. 1940. A Comparison of Alternative Tests of Significance for the Problem of m Rankings. The
Annals of Mathematical Statistics, 11(1):86?92.
Salvador Garcia and Francisco Herrera. 2008. An Extension on ?Statistical Comparisons of Classifiers over
Multiple Data Sets? for all Pairwise Comparisons. Journal of Machine Learning Research, 9:2677?2694.
Christian Hardmeier, Joakim Nivre, and Jorg Tiedemann. 2012. Tree Kernels for Machine Translation Quality
Estimation. In Proceedings of the 7th Workshop on Statistical Machine Translation, number 2011, pages 109?
113.
Sture Holm. 1979. A Simple Sequentially Rejective Multiple Test Procedure. Scandinavian Journal of Statistics,
6(2):pp. 65?70.
Ali Jalali, PD Ravikumar, S Sanghavi, and C Ruan. 2010. A Dirty Model for Multi-task Learning. In Advances in
Neural Information Processing Systems (NIPS) 23.
Jing Jiang and Chengxiang Zhai. 2007. Instance Weighting for Domain Adaptation in NLP. In Proceedings of the
45th Annual Meeting of the Association for Computational Linguistics, number June, pages 264?271.
Jing Jiang. 2009. Multi-Task Transfer Learning for Weakly-Supervised Relation Extraction. In ACL ?09 Proceed-
ings of the Joint Conference of the 47th Annual Meeting of the ACL, number August, pages 1012?1020.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zenz, Chris Dyer, Ondej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL 2007 Demo and Poster
Sessions, number June, pages 177?180.
Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012. Match without a Referee : Evaluating MT Adequacy
without Reference Translations. In Proceedings of the 7th Workshop on Statistical Machine Translation, pages
171?180.
Sinno Jialin Pan and Qiang Yang. 2010. A Survey on Transfer Learning. IEEE Transactions on Knowledge and
Data Engineering, 22(10):1345?1359, October.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Mathieu Brucher, Mathieu Perrot, and
?
Edouard Duchesnay. 2011. Scikit-learn : Machine Learning in
Python. Journal of Machine Learning Research, 12:2825?2830.
419
Raphael Rubino, Jennifer Foster, Joachim Wagner, Johann Roturier, Rasul Samad Zadeh Kaljahi, and Fred Hol-
lowood. 2012. DCU-Symantec Submission for the WMT 2012 Quality Estimation Task. In Proceedings of the
Seventh Workshop on Statistical Machine Translation, pages 138?144, Montr{?e}al, Canada, June. Association
for Computational Linguistics.
Raphael Rubino, Jos?e G. C. de Souza, Jennifer Foster, and Lucia Specia. 2013. Topic Models for Translation
Quality Estimation for Gisting Purposes. In Machine Translation Summit (MT Summit) XIV, pages 295?302.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Trans-
lation Edit Rate with Targeted Human Annotation. In Association for Machine Translation in the Americas.
Radu Soricut and A Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In Proceed-
ings of the 48th Annual Meeting of the Association for Computational Linguistics, number July, pages 612?621.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012. The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of the 7th Workshop on Statistical Machine Translation, pages
145?151.
Jos?e G. C. de Souza, Jes?us Gonz?alez-Rubio, Christian Buck, Marco Turchi, and Matteo Negri. 2014. FBK-UPV-
UEdin participation in the WMT14 Quality Estimation shared-task. In Proceedings of the Ninth Workshop on
Statistical Machine Translation, Baltimore, MD, USA, June.
Lucia Specia, Marco Turchi, Nello Cristianini, Nicola Cancedda, and Marc Dymetman. 2009. Estimating the
Sentence-Level Quality of Machine Translation Systems. In Proceedings of the 13th Annual Conference of the
EAMT, number May, pages 28?35.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Machine translation evaluation versus quality estimation.
Machine Translation, 24(1):39?50, May.
Lucia Specia, Stafford Street, Regent Court, and Mariano Felice. 2012. Linguistic Features for Quality Estimation.
In Proceedings of the 7th Workshop on Statistical Machine Translation, pages 96?103.
Lucia Specia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In Proceedings
of the European Association for Machine Translation, number May, pages 73?80.
Marco Turchi, Josef Steinberger, and Lucia Specia. 2012. Relevance ranking for translated texts. In Proceedings
of the 16th Annual Conference of the European Association for Machine Translation, number May, pages 153?
160.
Marco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the Subjectivity of Human Judgements
in MT Quality Estimation. In Proceedings of the 8th Workshop on Statistical Machine Translation (WMT?13),
Sofia, Bulgaria, August.
Marco Turchi, Antonios Anastasopoulos, Jos?e G. C. de Souza, and Matteo Negri. 2014. Adaptive Quality Estima-
tion for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics.
Guillaume Wisniewski, Anil Kumar Singh, Natalia Segal, and Franc?ois Yvon. 2013. Design and Analysis of
a Large Corpus of Post-Edited Translations: Quality Estimation, Failure Analysis and the Variability of Post-
Edition. In Machine Translation Summit XIV, pages 117?124.
Jiayu Zhou, Jianhui Chen, and Jieping Ye. 2012. MALSAR: Multi-tAsk Learning via StructurAl Regularization.
420
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1813?1823, Dublin, Ireland, August 23-29 2014.
Quality Estimation for Automatic Speech Recognition
Matteo Negri
(1)
Marco Turchi
(1)
Jos
?
e G. C. de Souza
(1,2)
Daniele Falavigna
(1)
(1)
FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
(2)
University of Trento, Italy
{negri,turchi,desouza,falavi}@fbk.eu
Abstract
We address the problem of estimating the quality of Automatic Speech Recognition (ASR) out-
put at utterance level, without recourse to manual reference transcriptions and when information
about system?s confidence is not accessible. Given a source signal and its automatic transcription,
we approach this problem as a regression task where the word error rate of the transcribed utter-
ance has to be predicted. To this aim, we explore the contribution of different feature sets and
the potential of different algorithms in testing conditions of increasing complexity. Results show
that our automatic quality estimates closely approximate the word error rate scores calculated
over reference transcripts, outperforming a strong baseline in all the testing conditions.
1 Introduction
In recent years, the increasing usage of large vocabulary continuous speech recognition (LVCSR) systems
to transcribe audio recordings from different sources (e.g. Youtube videos, TV programs, DVD movies,
meetings, etc) has sparked the need of accurate, fast and cost-effective methods to estimate the quality
of ASR output. This need contrasts with the fact that, after decades of progress in ASR research, the
established evaluation protocol is based on computing word error rate scores (WER)
1
over large test
sets of hand-crafted reference transcriptions. Indeed, despite its reliability, reference-based performance
assessment has an evident drawback represented by the cost of acquiring manual transcripts. Besides
increasing the cost-effectiveness of ASR evaluation routines, bypassing this bottleneck has several other
motivations. From an application perspective, for instance, reference-free quality estimation methods
could be used to: i) decide at run-time whether a given input signal has been properly recognized (e.g.
if a user spoken utterance needs to be repeated in a dialogue application), ii) decide if an automatic
transcription is acceptable as is (e.g. if manual revision is needed in an automatic subtitling application),
or iii) select the best transcription among options from multiple ASR systems.
When information about the inner workings of the system used to produce the transcriptions is acces-
sible, current reference-free confidence estimation methods can supply ASR applications with reliable
indicators about output reliability. This condition, however, does not always hold in the aforementioned
scenarios. A clear motivating example is provided by the exponential growth of captioned TED Talks
and Youtube videos,
2
for which no information is available about how transcriptions have been pro-
duced. In this case, neither reference-based methods, nor standard confidence measures can be applied
to obtain useful quality estimates. Nevertheless, in this scenario, supplying reliable indicators of tran-
scription quality has a huge market potential (e.g. to reduce the costs of manual revision/translation)
which motivates our research.
Focusing on these compelling needs, this paper investigates the automatic prediction of ASR out-
put quality when: i) manual reference transcripts are not available and ii) information about the
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
The word error rate is the minimum edit distance between an hypothesis and the reference transcription. Edit distance is
calculated as the number of edits (word insertions, deletions, substitutions) divided by the number of words in the reference.
2
Since 2009, Youtube videos in English can be automatically captioned. In 2012, for the 72 hours of video uploaded per
minute, such functionality was already available for 10 languages. Currently, more than 200 million Youtube videos have either
automatic or human-created captions (source: http://goo.gl/9swYSS).
1813
inner workings of the ASR system is not accessible. Casting the problem as a supervised regression
task, we experiment in a range of testing conditions on a well-known LVCSR setting (i.e. the automatic
transcription of TED talks). In this framework, we analyse the performance of various models (i.e. their
capability to predict utterance-level WER scores) as a function of the different learning algorithms used,
the proposed features, and the amount of training data available.
Our features are categorized according to the type of information they aim to capture. Since the na-
ture of the proposed features is a relevant aspect for the applicability of our approach, an important
distinction is made between ?glass-box? and ?black-box? features, which are respectively informed and
agnostic about systems? internal decoding strategies. The former can play an important role when all the
intermediate processing steps are accessible (e.g. in the selection of the best possible transcription hy-
pothesis). In contrast, black-box features have a wider applicability to situations where such information
is not available (e.g. to estimate the quality of online video subtitles).
Another important aspect relevant to our study is the relation between the accuracy of utterance-level
quality predictions and the degree of homogeneity of training and test data. Indeed, as in any supervised
learning framework, the similarity between training and test data has a direct impact on (classification
and regression) results. In order to fully understand the potential of our approach, we hence measure
performance variations under different levels of similarity between the data used to train the regressor
and the data used for evaluation. To this aim, our experiments account for a range of possible conditions.
These vary from the situation in which training and test are fully homogeneous (i.e. same dataset, with
training instances produced by the same ASR system) to the more challenging situation where training
and test are not homogeneous (i.e. different datasets, with training instances produced by different ASR
systems). Our results, obtained with two different state-of-the-art algorithms for regression, demonstrate
that in all such variable conditions our ASR quality estimation models lead to accurate predictions (i.e.
close the word error rate scores calculated over reference transcripts).
To the best of our knowledge, this paper represents the first extensive investigation on reference-
free and system-agnostic automatic estimation of ASR output quality. Along this direction, our main
contributions can be summarized as follows:
1. We propose a supervised, application-oriented approach to ASR quality estimation that bypasses
the need of manual reference transcriptions and is system-independent.
2. We evaluate our method with different learning algorithms and in different conditions, showing that
its estimates closely approximate the WER scores calculated over reference transcripts.
3. We perform feature analysis, isolating the contribution of each feature set in all the testing condi-
tions.
4. We analyse the learning curves of our best models, investigating the relation between performance
results and the amount of data needed for training.
Overall, these contributions provide useful insights about the feasibility of automatic ASR quality esti-
mation, opening interesting research avenues relevant for system development and for ASR applications.
2 Related Work
As a reference-free automatic evaluation method, our work introduces a valid application-oriented alter-
native to the standard evaluation protocols used within current ASR evaluation campaigns such as IWSLT
(Federico et al., 2011; Federico et al., 2012; Cettolo et al., 2013).
3
Besides that, our approach to ASR
quality estimation (QE) also differs from the well-established confidence estimation (CE) techniques
proposed in previous ASR literature (Sukkar and Lee, 1996; Evermann and Woodland, 2000; Wessel et
al., 2001; Sanchis et al., 2012; Seigel, 2013, inter alia). Such difference firstly relies in the fact that,
while in CE is the system itself that provides an indicator of the reliability of its output transcriptions,
QE aims to provide an external and more objective measure of goodness through WER predictions. A
3
See http://www.iwslt2013.org/ for details about the last edition of the IWSLT Workshop held in 2013.
1814
second (related) difference is that, in contrast with previous CE methods that heavily rely on information
about the internal behaviour of the ASR system, our technique does not necessarily depend on the access
to such information. This extends its applicability to scenarios (out of the scope of CE research) where
the quality of transcriptions produced by (possibly unknown) ASR systems has to be evaluated/compared
solely based on information about the input audio signals and the output transcriptions.
An interesting approach exploiting ASR word accuracy estimates to automatically score the profi-
ciency of non-native English speakers has been proposed by Yoon et al. (2010). To our knowledge this
work is the most similar to the one presented here, although it differs in the application domain and sev-
eral other aspects. First of all, similar to CE methods, it makes some use of glass-box features derived
from knowledge about the ASR internal workings (e.g. word confidence and acoustic/language model
probabilities). Secondly, the domain addressed is constrained to responses to prompted utterances, while
in this paper we address a large unconstrained domain, namely the automatic transcription of lectures
(TED talks) covering different topics. Finally, (Yoon et al., 2010) is based on a rather simple model
whose performance is not carefully analysed from the learning point of view (e.g. by comparing the
contribution different state-of-the-art algorithms) as we do here.
The problem of automating system evaluation without a gold standard has been addressed also in other
NLP areas. For instance, (Louis and Nenkova, 2013) recently addressed the assessment of machine-
generated summaries without model summaries. The strongest parallelism with our work, however,
can be found in the Machine Translation (MT) evaluation field, where the goal of bypassing the need
of manually-created reference translations has motivated a large body of research.
4
Quality estimation
for MT and ASR have a number of commonalities. First, they both deal with a ?source? (respectively
a sentence in a language L and an acoustic utterance) and an ?hypothesis? whose quality has to be
estimated without references (respectively a translation in a language L1 and an automatic transcription
of the audio signal). Second, they can be addressed at various granularities. Indeed, ASR output quality
estimation is similar to its MT counterpart where research focused on quality predictions at word level
(Ueffing and Ney, 2007; Bach et al., 2011), sentence level (Specia et al., 2009; Mehdad et al., 2012)
and document level (Soricut and Echihabi, 2010). Third, both tasks are suitable for supervised machine
learning methods, either for classification (Blatz et al., 2003; Quirk, 2004) or for regression (Specia et
al., 2010; Specia, 2011). Finally, both tasks motivate efforts in designing features capable to capture
the difficulty to process the source, the plausibility of the output hypothesis and (but not necessarily) the
confidence of the decoding process (Felice, 2012; Rubino et al., 2013b).
3 Approach
We approach the automatic estimation of ASR output quality as a supervised regression problem. Given
a training set of (signal, transcription, WER) instances, the task is to predict the WER of each instance
in a test set of unseen (signal, transcription) pairs.
Features. As shown in Table 1, the features used in our experiments (68 in total) can be categorized in
four main groups. The first group (ASR features) includes several glass-box features proposed in previous
literature on ASR confidence estimation (Litman et al., 2000; Gabsdil and Lemon, 2004; Goldwater et al.,
2010; Higgins et al., 2011). These features are suitable only for the ideal situation in which information
about systems? internal decoding strategies is available (as in the experiments discussed in ?4.1). We use
them as a term of comparison to evaluate the usefulness of the other three groups (signal, hybrid and
textual), which belong to the black-box type. These features, which are totally uninformed about the
decoding process, have wider applicability to the system-independent ASR quality estimation tasks that
represent our target scenario (see Sections 4.2 and 4.3). More in detail:
? ASR features aim to capture the confidence of the speech recognizer and the reliability of the whole
decoding process. In our experiments, as we do not have access to decoders of other systems, they
are computed only for the ASR system developed in our labs (Falavigna et al., 2013). These features
4
For a complete overview of the current approaches to MT quality estimation we refer the reader to the WMT12 and WMT13
shared task reports (Callison-Burch et al., 2012; Bojar et al., 2013).
1815
are extracted both from word graphs (WGs) and n-best lists (n=100). In Table 1 ?Total probabil-
ity? is the weighted sum of log Language Model (LM) and log Acoustic Model (AM) probabilities.
LM probability is computed with a 4-gram backoff LM, trained over about 5 billion words using
the IRSTLM toolkit (Federico et al., 2008) and the modified shift-beta smoothing method. AM
probability is computed using a set of tied-state triphone Hidden Markov Models having, as output
state density, a mixture of Gaussian probability densities with diagonal covariance matrices. ?Mean
probability? is obtained dividing the total probability by the number of hypothesized ASR output
items (words + silences). Confidence scores are computed averaging time posterior word proba-
bilities (Evermann and Woodland, 2000). ?Proportion of low confidence words? is the fraction of
words having confidence values ? 0.5. The remaining ASR features are directly extracted from
word graphs and n-best lists scores.
? Signal features aim to capture the difficulty to transcribe a given input looking at the signal as a
whole. They are computed from raw vectors extracted through frame analysis (we employ 20ms
analysis window and 10ms analysis step). For each analysed window, 12 Mel Frequency Cepstral
Coefficients (MFCCs) are evaluated plus log energy. Then, for each given segment, minimum,
maximum and mean values of raw energy, as well as the mean MFCCs values and total segment
duration, are computed to form the signal feature vector.
? Hybrid features provide a more fine-grained way to capture the difficulty of transcribing the signal.
This is done by considering information about word and silence/noise regions, as well as their
respective duration. These features are computed after having performed forced alignment between
the input audio signal and the corresponding automatic hypotheses. Forced alignment is carried
out with our ASR system (Falavigna et al., 2013), in order to detect audio segments related to
words, hesitations and silences in the hypothesis. Pitch features have been computed with the Praat
software tool (Boersma and Weenink, 2005).
? Textual features aim to capture the plausibility (i.e. the fluency) of an output transcription. To
this aim, we consider surface information (such as the number of words and the percentage of
numbers/content-words/nouns/verbs in the hypothesis) as well as information about LM perplexity
and probability of the hypothesis (both at the level of words and parts of speech)
5
.
Feature selection is performed throughout all our experiments to maximize results and, at the same
time, analyse the contribution of the proposed features. To this aim, we use Randomized Lasso, or
stability selection (Meinshausen and B?uhlmann, 2010), which re-samples the training data several times
and fits a Lasso regression model on each sample. Features that appear in a given number of samples are
considered more informative for the task at hand, and hence retained (those marked in bold in Table 1
are the most informative ones based on the experiments described in Sections 4.2 and 4.3).
Learning algorithms. To build our regression models we experimented with two non-parametric learn-
ing approaches: Support Vector Machines (SVMs) (Shawe-Taylor and Cristianini, 2004) and Extremely
Randomized Trees (XT) (Geurts et al., 2006). SVMs are non-parametric deterministic algorithms that
have been widely used in several fields, in particular in NLP where they are the state-of-the-art for various
tasks. Extra-Trees are a tree-based ensemble method for supervised classification and regression that
were also successfully used for MT quality estimation (de Souza et al., 2013; de Souza et al., 2014a). In
XTs each tree can be parametrized differently. When a tree is built, the node splitting step is done at ran-
dom by picking the best split among a random subset of the input features. The results of the individual
trees are combined by averaging their predictions. Hyper-parameter optimization of the SVM (with Ra-
dial basis function kernel ? RBF) and XT models was performed using randomized search (Bergstra and
Bengio, 2012). We used both learning methods as implemented in the Scikit-learn package (Pedregosa
et al., 2011).
5
The PoS LM has been obtained by processing with the TreeTagger (Schmid, 1995) the same data used for the word LM.
6
Hesitations, such as ?uhm?, ?eh? and ?ah? are found through matches with a predefined list. Consecutive repeated words
in the same utterance are also considered as hesitations.
1816
ASR (16)
Total probability of ASR output (w ? logP
LM
+ logP
AM
), mean probability, total
acoustic probability, mean acoustic probability, mean confidence score, Std of confi-
dence scores, confidence scores per second, proportion of low-confidence words, WG
node density, WG transition density, Mean/Std/Min n-best probability, Mean/Std/Min
n-best acoustic probability.
Signal (16)
Total segment duration (sec), Mean/Min/Max raw energy (dB), mean MFCC[1, 2,
3, 4, 5, 6, 7, 8, 9, 10, 11,12].
Hybrid (26)
SNR (dB), mean noise energy (dB), Mean/Min/Max word energy (dB), Min/Max
noise energy (dB), (max word - min noise) energy (dB), # silences, ratio of silences
and words, # words per second, # silences per second, total duration of words
(sec), total duration of silences (sec), mean duration of words (sec), mean duration
of silences (sec), ratio of (tot duration silences) and (tot duration words), Std of word
duration (sec), Std of silence duration (sec), (tot duration words) - (tot duration
silences), Mean/Std/Min./Max. pitch (Hz), # hesitations,
6
frequency of hesitations.
Textual (10)
Number of words, LM log probability of the hypothesis, LM log probability of
POS of the hypothesis, LM log perplexity of POS of the hypothesis, Perplexity of
the hypothesis, % of numbers in the hypothesis, % of tokens in the hypothesis which
do not contain only a-z, % of content words in the hypothesis, % of nouns in the
hypothesis, % of verbs in the hypothesis
Table 1: Full list of the 68 features used in our experiments, divided into four groups. The most predictive
black-box features (resulting from feature selection in the ?4.3 experiments) are marked in bold.
4 Experiments
To evaluate our approach we carried out three sets of experiments. In each set our feature groups are
analysed: i) with the two learning algorithms, ii) in combination/isolation, iii) with/without feature se-
lection. The three sets differ in terms of the difficulty of the quality estimation task from the learning
point of view. To experiment with situations of increasing complexity, we alternate conditions in which
all the features (glass-box and black-box) can be used, training and test sets are non-/homogeneous, the
quality estimator is trained on transcriptions generated by the same/different ASR systems.
Data. The data used in the experiments consists of the audio recordings delivered for the IWSLT 2013
evaluation campaign (Cettolo et al., 2013). One of the tasks of IWSLT 2013 is the automatic tran-
scription of English TED talks, a global set of conferences whose audio/video recordings are publicly
available. The main challenges for ASR in these talks include: the large variability of topics (hence
a large, unconstrained vocabulary), the presence of non-native speakers and a rather informal speaking
style. Each IWSLT participant submitted one primary ASR output run for each of the talks included in
the test set plus some optional contrastive ASR outputs. In addition, participants sent submissions for
the ASR tracks delivered for the 2012 evaluation campaign. Our experiments have been carried out on
the primary submissions, sent by 8 participants, related to the 2012 (consisting in 11 different talks) and
2013 (28 different talks) test sets. The 2012 test set has a total duration of around 1h45sec, it contains
1,118 reference sentences and 18,613 running words. On such dataset, participants? primary submissions
achieved a mean utterance WER ranging from 10.5% to 18.4% (in this work a WER score is computed
for each reference sentence, and mean utterance WER represents the average of sentence WERs). The
2013 test set has a total duration of around 3h55sec, it contains 2,238 reference sentences and 41,545 run-
ning words. On this dataset, primary participants? submissions achieve a mean utterance WER ranging
from 15.9% to 30.8%.
In our experiments, we always use 1,118 utterances for training the regressor and 1,120 for testing. To
this aim, the IWSLT 2013 data is randomly sampled three times in training and test sets of such dimen-
sions. While for the 2012 test set manual utterance segmentation has been provided by the organizers, for
the 2013 data the participants had to employ their own automatic segmentation systems before decoding
the audio tracks (thus resulting in a different number of ASR sentence hypotheses for each team). Hence,
1817
to ensure that each participant has the same number of ASR sentence hypotheses, an alignment with the
reference manual segmentation has been performed in our experiments.
Evaluation. Our evaluation is carried out in terms of Mean Absolute Error (MAE), a standard metric
for regression problems. The MAE is the average of the absolute errors e
i
= |f
i
? y
i
|, where f
i
is
the prediction of the model and y
i
is the actual WER for the i
th
test instance. WER is calculated with
the NIST SCLITE Scoring Package.
7
As it is a measure of error, lower MAE scores indicate that our
predictions are closer to the real WER calculated for each test instance against the reference transcripts.
For each experiment, we report the mean and the standard deviation of the MAE achieved by the best
performing QE models on the IWSLT 2013 test sets.
Baseline. Besides measuring performance in terms of global MAE, each model is compared against a
common baseline for regression tasks. This baseline, which is particularly relevant in settings featuring
different data distributions between training and test sets, is calculated by labelling each test instance
with the mean WER score calculated on the training set. Previous works, also in MT quality estimation,
demonstrated that its results can be particularly hard to beat (Rubino et al., 2013a).
4.1 Experiment 1
In the first set of experiments we consider the easiest situation from the learning perspective. In this
setting we predict the WER of transcriptions produced by our ASR system (denoted by X), whose inner
workings are known (thus enabling the use of glass-box features). To investigate the relation between
prediction accuracy and the degree of homogeneity of training and test data, we experiment both with
similar datasets (disjoint training and test sampled from IWSLT13) and different datasets (IWSLT12
for training and samples from IWSLT13 for test). Results are reported in Table 2, where the notation
?LetterYear - LetterYear? indicates the systems and the datasets used for training and test (respectively
our system X, and data from IWSLT12 and/or IWSLT13).
Train - Test ALL (glass-box + BB COMB) ASR (glass-box) BB COMB (Signal+Hybrid+Textual) Baseline
X13 - X13 11.56?0.29 SVR 12.11?0.29 XT 15.17?0.06 XT 19.84?0.06
X12 - X13 12.61?0.13 XT 13.78?0.16 XT 16.78?0.18 XT 19.06?0.12
Train - Test Signal Hybrid Textual Baseline
X13 - X13 16.42?0.1 XT 17.61?0.12 XT 17.42?0.15 SVR 19.84?0.06
X12 - X13 18.85?0.09
?
XT 18.39?0.22 XT 17.58?0.15 XT 19.06?0.12
Table 2: MAE results using the same system on different datasets, with and without glass-box features.
As can be seen from the table, the two models using ALL the features achieve the largest improvements
over the strong baseline used for comparison (up to 8.2 MAE points in the X13 - X13 setting). This is
not surprising if we consider the high predictive power of ASR (glass-box) features that, when used in
isolation, lead to a considerably lower MAE with respect to the other three groups. However, it?s worth
observing that also the combination of only the black-box features (BB COMB) allows the QE predictors
to significantly outperform the baseline (up to 4.67 MAE points in X13 - X13). Such improvements come
from the joint contribution of each of the three groups, which achieve good results also in isolation.
Indeed, except in one case where the gain over the baseline is not significant
8
(X12 - X13 with the Signal
features), their MAE reduction ranges between 0.67 (X12 - X13 Hybrid) and 3.42 MAE points (X13 -
X13 Signal). The good prediction capability of the black-box features is also shown by the fact that, when
combined with the glass-box features, they lead to improvements between 0.55 and 1.17 MAE points
over the ASR features alone. Considering the privileged condition of the (system-informed) glass-box
features, this is a remarkable result that suggests some complementarity between the two groups.
In general, our supervised approach is sensitive to the similarity between training and test. This is
evidenced by higher MAE results when non-homogeneous datasets (i.e. X12 - X13) are processed. In
7
http://www1.icsi.berkeley.edu/Speech/docs/sctk-1.2/sclite.htm
8
Statistical significance is measured by considering the overlap of confidence intervals defined by the standard deviation
range around the mean. In our tables, the results marked with the ??? symbol are not significantly better than the baseline.
1818
terms of algorithms, XT generally performs better than SVR, in particular when the QE model is trained
and tested on non-homogeneous data. This can be explained by their higher generalization capability
due to variance reductions as explained in (Hastie et al., 2009, Chapter 15).
4.2 Experiment 2
In this set of experiments we consider a situation of intermediate difficulty from the learning perspective.
Our objective is to evaluate, on homogeneous datasets (sampled from IWSLT13), the output of ASR
systems whose inner workings are not known (hence only black-box features can be used). To make
our analysis more complete, we also evaluate the performance of models trained on a given ASR system
to predict the WER of hypotheses produced by a different one. This situation is closer to application
scenarios in which the evaluated ASR system is unknown and different from the one used to train the
quality estimator. Two systems with very different performance are considered for this purpose: the best
and the worst according to the official IWSLT 2013 ranking (respectively denoted by A and Z).
Train - Test BB COMB Signal Hybrid Textual Baseline
A13 - A13 11.18?0.22 SVR 11.91?0.23 SVR 12.76?0.18 SVR 12.57?0.13 SVR 14.35?0.1
Z13 - A13 16.01?0.23 SVR 18.04?0.22 SVR 17.24?0.22 SVR 18.01?0.2 XT 21.58?0.15
Z13 - Z13 15.52?0.6 XT 16.94?0.41 XT 17.04?0.56 SVR 17.84?0.4 XT 19.65?0.43
A13 - Z13 17.36?0.43 XT 18.7?0.53 XT 18.21?0.45 XT 19.38?0.45 XT 21.03?0.51
Table 3: MAE results using different systems on the same dataset, without glass-box features.
The results reported in Table 3 confirm that: i) the combination of black-box features (BB COMB)
always leads to the best QE models, which significantly outperform the baseline, ii) the same holds
also when each single group is used in isolation, iii) with less homogeneous training and test data, XT
performs generally better than SVR.
In addition, it?s worth noting that when a QE model is trained and tested on data transcribed by
the same ASR system the results are significantly better (the MAE is always about 1.0 - 6.0 points
lower). Indeed, as also shown by the same behaviour of our baseline, this condition is simpler and more
suitable for supervised learning methods. This depends on the fact that each ASR system has its own
coherent behaviour, which results in transcriptions with similar characteristics that supervised models
are able to learn (e.g. recurring errors, similar WER distributions). In contrast, when training and test
data are produced by different ASR systems, supervised learning becomes more difficult and the output
predictions less reliable. Each feature group is affected by this situation, but it is interesting to note that
the Hybrid features are more robust than the other two groups to less homogeneous datasets. This can be
explained by the fact that they are extracted after applying forced alignment by means of a third system,
which is likely to normalise and reduce the difference between training and test data. Overall, also in
this more complex scenario where the glass-box features cannot be used, our results demonstrate a good
prediction capability of the QE models, which are still able to beat a strong baseline.
4.3 Experiment 3
In the third set of experiments we consider the hardest case from the learning point of view. In this setting
the evaluated ASR systems are unknown and training/test data are non homogeneous (i.e. training from
IWSLT12, test from samples of IWSLT13). Results are reported in Table 4.
Train - Test BB COMB Signal Hybrid Textual Baseline
A12 - A13 12.81?0.08 XT 13.57?0.13
?
XT 12.85?0.1 XT 13.25?0.23
?
XT 13.65?0.17
Z12 - A13 14.78?0.1 SVR 15.66?0.09
?
XT 13.56?0.09 SVR 13.63?0.24 SVR 15.51?0.35
Z12 - Z13 17.16?0.4 XT 19.34?0.32
?
XT 17.68?0.3 XT 19.59?0.11
?
XT 19.98?0.29
A12 - Z13 19.83?0.23 XT 21.85?0.2 XT 20.68?0.13 XT 22.62?0.08 XT 23.04?0.18
Table 4: MAE results using different systems on different dataset, without glass-box features.
Also in the most challenging scenario our results substantially confirm the previous findings. Indeed,
except in one case (Z12 - A13), the following observations still hold: i) when used in combination, the
1819
0 10 20 30 40 50 60 70 80 90 10012
14
16
18
20
22
24
26
% of Training Data
MA
E
 
 
A12 ? A13
A12 ? Z13
Z12 ? Z13
Z12 ? A13
Figure 1: Learning curves for the best systems of ?Experiment 3? (using BB COMB features).
black-box features (BB COMB) lead to the best QE models, which significantly outperform the baseline,
ii) this holds also when each single group is used in isolation (although not significantly in 5 out of 12
settings), iii) with less homogeneous training and test data, XT performs generally better than SVR.
Unsurprisingly, as also observed in the previous set of experiments, the low homogeneity of training
and test data has an impact on the accuracy of the predictions. The effect of training and testing on
less homogeneous data produced by different systems is now clearly visible. Except for the more robust
Hybrid features, which in the Z12 - A13 setting produce the best model, the results obtained with the two
other groups decreased to the point that their improvement over the baseline is often not significant. Nev-
ertheless, even under the challenging conditions posed by this realistic and application-oriented scenario,
reference-free and system-agnostic ASR evaluation remains a feasible task.
5 Feature Analysis and Learning Curves
In order to gain additional insights about the effectiveness of our method, we performed a further analysis
of the ?Experiment 3? results. In such challenging scenario, the most interesting from the application
perspective, we first identified the most predictive features among those in the BB COMB set. To this
aim, we collected the features that are always chosen by the feature selection algorithm proposed in ?3.
The resulting list contains features from all the three black-box groups (marked in bold in Table 1). This
confirms their complementarity in predicting the quality of a transcribed utterance.
In the same setting, we also investigated the relation between the amount of data used to train our
models and the accuracy of their predictions. To this aim, we measured performance variations when
the same models (i.e. those obtained with the BB COMB set) are trained on different amounts of data.
For each training set, nine subsets were created (with 10%, 20%,..., 90% of the data) by sub-sampling
sentences from a uniform distribution. The process was iterated 5 times. Each subset was used to build
the relative QE regressor, which was then evaluated on our test sets. Figure 1 shows the resulting learning
curves (each point is the average result of the 5 runs on each test set; the error bars show ?1std). As
can be seen from all the curves, after an initial fluctuation of the MAE, performance results with 40% of
the training data are comparable with those obtained using the whole training set. Moreover, it?s worth
remarking that in three out of four cases the models trained with such amount of data already outperform
the baseline (for Z12 - A13 the MAE is only 0.01 point higher). This suggests that reference-free, system-
independent models for ASR quality estimation are able to provide informative predictions even with a
limited amount (?400 manual transcripts) of training instances.
1820
6 Conclusion
We investigated the problem of automatically predicting the word error rate of an automatically-
transcribed utterance in a large vocabulary continuous speech recognition setting. In such scenario,
we proposed a supervised regression approach that bypasses the need of manual reference transcriptions
and does not necessarily depend on information about system?s confidence (first contribution of the pa-
per). Then, by evaluating models obtained with different state-of-the-art learning algorithms, we showed
that our automatic predictions outperform a strong baseline and closely approximate the WER scores
calculated over reference transcripts (second contribution). Different feature groups have been proposed
and their contribution has been analysed in a range of testing conditions of increasing difficulty (third
contribution). This made possible to isolate informative features that significantly contribute to the per-
formance of our quality estimation models, and to get useful insights about the potential of our approach
when different sources of information (glass-box, black box features) are available. Finally, analysing
the relation between prediction performance and the size of the training set, we showed that the results
obtained with 40% of the data are already comparable to our best MAE (fourth contribution).
Our analysis revealed a dependency between the performance of the quality estimation models and
the degree of homogeneity between training and test data. This aspect is particularly relevant from the
application perspective since in real working conditions the availability of large amounts of representa-
tive training instances is far from being guaranteed. In quality estimation for machine translation (a task
featuring strong similarities with ours), these issues have recently motivated studies on domain adapta-
tion and online learning techniques (de Souza et al., 2014b; Turchi et al., 2014). This suggests, as a first
direction for future work, the investigation of approaches capable to better exploit the available training
data and mitigate the impact of large differences between training and test instances.
Acknowledgements
This work has been partially funded by the European project EU-BRIDGE (FP7-287658) and by the
Autonomous Province of Trento, Italy, under the project Wikivoice (L.P. 6/1999).
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011. Goodness: a Method for Measuring Machine Translation
Confidence. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 211?219. The
Association for Computer Linguistics.
James Bergstra and Yoshua Bengio. 2012. Random Search for Hyper-Parameter Optimization. Journal of Ma-
chine Learning Research, 13:281?305.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence Estimation for Machine Translation. Summer workshop final report,
JHU/CLSP.
Paul Boersma and David Weenink. 2005. Praat: Doing Phonetics by Computer (Version 4.3.01). Retrieved from
http://www.praat.org/.
Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Eighth Workshop on Statistical Machine Translation, WMT-2013, pages 1?44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings
of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical
Machine Translation (WMT?12), pages 10?51, Montr?eal, Canada.
Mauro Cettolo, Jan Niehues, Sebastian St?uker, Luisa Bentivogli, and Marcello Federico. 2013. Report on the 10th
IWSLT Evaluation Campaign. Proceedings of the International Workshop on Spoken Language Translation,
Heidelberg, Germany.
1821
Jos?e G. C. de Souza, Christian Buck, Marco Turchi, and Matteo Negri. 2013. FBK-UEdin participation to the
WMT13 quality estimation shared task. In Proceedings of the Eighth Workshop on Statistical Machine Trans-
lation, pages 352?358, Sofia, Bulgaria, August. Association for Computational Linguistics.
Jos?e G. C. de Souza, Jes?us Gonz?alez-Rubio, Christian Buck, Marco Turchi, and Matteo Negri. 2014a. FBK-UPV-
UEdin participation in the WMT14 Quality Estimation shared-task. In Proceedings of the Ninth Workshop on
Statistical Machine Translation, Baltimore, MD, USA, June.
Jos?e G. C. de Souza, Marco Turchi, and Matteo Negri. 2014b. Predicting Machine Translation Quality Esti-
mation Across Domains. In Proceedings of the 25th International Conference on Computational Linguistics,
COLING?14, Dublin, Ireland.
Gunnar Evermann and Philip C. Woodland. 2000. Large Vocabulary Decoding and Confidence Estimation Using
Word Posterior Probabilities. In Proc. of ICASSP, pages 2366?2369, Istanbul, Turkey, June.
Daniele Falavigna, Roberto Gretter, Fabio Brugnara, Diego Giuliani, and Romain Serizel. 2013. FBK@IWSLT
2013 - ASR Tracks. In Proceedings of the IWSLT 2013 workshop, Heidelberg, Germany.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an Open Source Toolkit for Handling
Large Scale Language Models. pages 1618?1621, Brisbane, Australia, September.
Marcello Federico, Luisa Bentivogli, Michael Paul, and Sebastian St?uker. 2011. Overview of the IWSLT 2011
Evaluation Campaign. In International Workshop on Spoken Language Translation, pages 11?27.
Marcello Federico, Mauro Cettolo, Luisa Bentivogli, Michael Paul, and Sebastian St?uker. 2012. Overview of the
IWSLT 2012 Evaluation Campaign. In Proc. of the International Workshop on Spoken Language Translation,
Hong Kong, HK, December.
Mariano Felice. 2012. Linguistic Indicators for Quality Estimation of Machine Translations. Master?s thesis,
University of Wolverhampton, UK.
Malte Gabsdil and Oliver Lemon. 2004. Combining acoustic and pragmatic features to predict recognition perfor-
mance in spoken dialogue systems. pages 344?351.
Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized trees. Mach. Learn., 63(1):3?42,
April.
Sharon Goldwater, Dan Jurafsky, and Christopher Manning. 2010. Which words are hard to recognize? Prosodic,
lexical, and disfluency factors that increase speech recognition error rates. 52(3):181?200.
Trevor Hastie, Robert Tibshirani, Jerome Friedman, T Hastie, J Friedman, and R Tibshirani. 2009. The elements
of statistical learning, volume 2. Springer.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and David Williamson. 2011. A three-stage approach for auto-
mated scoring of spontaneous responses. (25):282?306.
Diane J. Litman, Julia B. Hirschberg, and Marc Swerts. 2000. Predicting Automatic Speech Recognition Perfor-
mance Using Prosodic Cues. In Proceedings of NAACL, pages 218?225.
Annie Louis and Ani Nenkova. 2013. Automatically assessing machine summary content without a gold standard.
Computational Linguistics, 39(2):267?300.
Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012. Match without a Referee: Evaluating MT Adequacy
without Reference Translations. In Proceedings of the Machine Translation Workshop (WMT2012).
Nicolai Meinshausen and Peter B?uhlmann. 2010. Stability selection. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 72(4):417?473.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-
learn : Machine Learning in Python. Journal of Machine Learning Research, 12:2825?2830.
Christopher B. Quirk. 2004. Training a Sentence-Level Machine Translation Confidence Measure. In Proceedings
of LREC 2004.
Raphael Rubino, Jos?e GC de Souza, Jennifer Foster, and Lucia Specia. 2013a. Topic Models for Translation
Quality Estimation for Gisting Purposes. In Proceedings of the Machine Translation Summit XIV.
1822
Raphael Rubino, Joachim Wagner, Jennifer Foster, Johann Roturier, Rasoul Samad Zadeh Kaljahi, and Fred Hol-
lowood. 2013b. DCU-Symantec at the WMT 2013 Quality Estimation Shared Task. In Proceedings of the
Eighth Workshop on Statistical Machine Translation, pages 392?397.
Alberto Sanchis, Alfons Juan, and Enrique Vidal. 2012. A Word-Based Naive Bayes Classifier for Confidence
Estimation in Speech Recognition. 20(12):565?574.
Helmut Schmid. 1995. Improvements in Part-of-Speech Tagging with an Application to German. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50, Dublin, Ireland.
Matthew Stephen Seigel. 2013. Condence Estimation for Automatic Speech Recognition Hypotheses. University
of Cambridge. PhD Thesis.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel methods for pattern analysis. Cambridge university press.
Radu Soricut and Abdessamad Echihabi. 2010. TrustRank: Inducing Trust in Automatic Translations via Ranking.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ?10, pages
612?621, Stroudsburg, PA, USA. Association for Computational Linguistics.
Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the
Sentence-Level Quality of Machine Translation Systems. In Proceedings of the 13th Annual Conference of the
European Association for Machine Translation (EAMT?09), pages 28?35, Barcelona, Spain.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Machine Translation Evaluation versus Quality Estimation.
Machine translation, 24(1):39?50.
Lucia Specia. 2011. Exploiting Objective Annotations for Measuring Translation Post-editing Effort. Proceedings
of the 15th Conference of the European Association for Machine Translation, pages 73?80.
Rafic Antoon Sukkar and Chin-Hui Lee. 1996. Vocabulary Independent Discriminative Utterance Verification for
Nonkeyword Rejection in Subword Based Speech Recognition. 6(6):420?429.
Marco Turchi, Antonios Anastasopoulos, Jos?e G. C. de Souza, and Matteo Negri. 2014. Adaptive Quality Estima-
tion for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics, ACL?14, Baltimore, MD, USA. Association for Computational Linguistics.
Nicola Ueffing and Hermann Ney. 2007. Word-Level Confidence Estimation for Machine Translation. Comput.
Linguist., 33(1):9?40, March.
Frank Wessel, Ralf Schl?uter, Klaus Macherey, and Hermann Ney. 2001. Confidence Measures for Large Vocabu-
lary Continuous Speech Recognition. 9(3):288?298.
Su-Youn Yoon, Lei Chen, and Klaus Zechner. 2010. Predicting word accuracy for the automatic speech recogni-
tion of non-native speech. In Proc. of INTERSPEECH, pages 773?776, Makuhari,Chiba, Japan.
1823
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 710?720,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Adaptive Quality Estimation for Machine Translation
Marco Turchi
(1)
Antonios Anastasopoulos
(3)
Jos
?
e G. C. de Souza
(1,2)
Matteo Negri
(1)
(1)
FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
(2)
University of Trento, Italy
(3)
National Technical University of Athens, Greece
{turchi,desouza,negri}@fbk.eu
anastasopoulos.ant@gmail.com
Abstract
The automatic estimation of machine
translation (MT) output quality is a hard
task in which the selection of the appro-
priate algorithm and the most predictive
features over reasonably sized training sets
plays a crucial role. When moving from
controlled lab evaluations to real-life sce-
narios the task becomes even harder. For
current MT quality estimation (QE) sys-
tems, additional complexity comes from
the difficulty to model user and domain
changes. Indeed, the instability of the sys-
tems with respect to data coming from dif-
ferent distributions calls for adaptive so-
lutions that react to new operating con-
ditions. To tackle this issue we propose
an online framework for adaptive QE that
targets reactivity and robustness to user
and domain changes. Contrastive exper-
iments in different testing conditions in-
volving user and domain changes demon-
strate the effectiveness of our approach.
1 Introduction
After two decades of steady progress, research
in statistical machine translation (SMT) started to
cross its path with translation industry with tan-
gible mutual benefit. On one side, SMT research
brings to the industry improved output quality and
a number of appealing solutions useful to increase
translators? productivity. On the other side, the
market needs suggest concrete problems to solve,
providing real-life scenarios to develop and eval-
uate new ideas with rapid turnaround. The evolu-
tion of computer-assisted translation (CAT) envi-
ronments is an evidence of this trend, shown by
the increasing interest towards the integration of
suggestions obtained from MT engines with those
derived from translation memories (TMs).
The possibility to speed up the translation pro-
cess and reduce its costs by post-editing good-
quality MT output raises interesting research chal-
lenges. Among others, these include deciding
what to present as a suggestion, and how to do it
in the most effective way.
In recent years, these issues motivated research
on automatic QE, which addresses the problem
of estimating the quality of a translated sentence
given the source and without access to reference
translations (Blatz et al, 2003; Specia et al, 2009;
Mehdad et al, 2012). Despite the substantial
progress done so far in the field and in success-
ful evaluation campaigns (Callison-Burch et al,
2012; Bojar et al, 2013), focusing on concrete
market needs makes possible to further define the
scope of research on QE. For instance, moving
from controlled lab testing scenarios to real work-
ing environments poses additional constraints in
terms of adaptability of the QE models to the vari-
able conditions of a translation job. Such variabil-
ity is due to two main reasons:
1. The notion of MT output quality is highly
subjective (Koponen, 2012; Turchi et al,
2013; Turchi and Negri, 2014). Since the
quality standards of individual users may
vary considerably (e.g. according to their
knowledge of the source and target lan-
guages), the estimates of a static QE model
trained with data collected from a group of
post-editors might not fit with the actual
judgements of a new user;
2. Each translation job has its own specifici-
ties (domain, complexity of the source text,
average target quality). Since data from a
new job may differ from those used to train
the QE model, its estimates on the new in-
stances might result to be biased or uninfor-
mative.
The ability of a system to self-adapt to the be-
710
haviour of specific users and domain changes is
a facet of the QE problem that so far has been
disregarded. To cope with these issues and deal
with the erratic conditions of real-world trans-
lation workflows, we propose an adaptive ap-
proach to QE that is sensitive and robust to dif-
ferences between training and test data. Along this
direction, our main contribution is a framework in
which QE models can be trained and can continu-
ously evolve over time accounting for knowledge
acquired from post editors? work.
Our approach is based on the online learning
paradigm and exploits a key difference between
such framework and the batch learning methods
currently used. On one side, the QE models ob-
tained with batch methods are learned exclusively
from a predefined set of training examples under
the assumption that they have similar characteris-
tics with respect to the test data. This makes them
suitable for controlled evaluation scenarios where
such condition holds. On the other side, online
learning techniques are designed to learn in a step-
wise manner (either from scratch, or by refining an
existing model) from new, unseen test instances
by taking advantage of external feedback. This
makes them suitable for real-life scenarios where
the new instances to be labelled can considerably
differ from the data used to train the QE model.
To develop our approach, different online algo-
rithms have been embedded in the backbone of
a QE system. This required the adaptation of its
standard batch learning workflow to:
1. Perform online feature extraction from a
source?target pair (i.e. one instance at a time
instead of processing an entire training set);
2. Emit a prediction for the input instance;
3. Gather user feedback for the instance (i.e.
calculating a ?true label? based on the
amount of user post-editions);
4. Send the true label back to the model to up-
date its predictions for future instances.
Focusing on the adaptability to user and domain
changes, we report the results of comparative ex-
periments with two online algorithms and the stan-
dard batch approach. The evaluation is carried out
by measuring the global error of each algorithm
on test sets featuring different degrees of similar-
ity with the data used for training. Our results
show that the sensitivity of online QE models to
different distributions of training and test instances
makes them more suitable than batch methods for
integration in a CAT framework.
Our adaptive QE infrastructure has been re-
leased as open source. Its C++ implementation is
available at http://hlt.fbk.eu/technologies/
aqet.
2 Related work
QE is generally cast as a supervised machine
learning task, where a model trained from a col-
lection of (source, target, label) instances is used
to predict labels
1
for new, unseen test items (Spe-
cia et al, 2010).
In the last couple of years, research in the field
received a strong boost by the shared tasks orga-
nized within the WMT workshop on SMT,
2
which
is also the framework of our first experiment in
?5. Current approaches to the tasks proposed at
WMT have mainly focused on three main direc-
tions, namely: i) feature engineering, as in (Hard-
meier et al, 2012; de Souza et al, 2013a; de Souza
et al, 2013b; Rubino et al, 2013b), ii) model
learning with a variety of classification and regres-
sion algorithms, as in (Bicici, 2013; Beck et al,
2013; Soricut et al, 2012), and iii) feature selec-
tion as a way to overcome sparsity and overfitting
issues, as in (Soricut et al, 2012).
Being optimized to perform well on specific
WMT sub-tasks and datasets, current systems re-
flect variations along these directions but leave im-
portant aspects of the QE problem still partially
investigated or totally unexplored.
3
Among these,
the necessity to model the diversity of human qual-
ity judgements and correction strategies (Kopo-
nen, 2012; Koponen et al, 2012) calls for solu-
tions that: i) account for annotator-specific be-
haviour, thus being capable of learning from inher-
ently noisy datasets produced by multiple annota-
tors, and ii) self-adapt to changes in data distribu-
tion, learning from user feedback on new, unseen
test items.
1
Possible label types include post-editing effort scores
(e.g. 1-5 Likert scores indicating the estimated percentage
of MT output that has to be corrected), HTER values (Snover
et al, 2006), and post-editing time (e.g. seconds per word).
2
http://www.statmt.org/wmt13/
3
For a comprehensive overview of the QE approaches
proposed so far we refer the reader to the WMT12 and
WMT13 QE shared task reports (Callison-Burch et al, 2012;
Bojar et al, 2013).
711
These interconnected issues are particularly rel-
evant in the CAT framework, where translation
jobs from different domains are routed to pro-
fessional translators with different idiolect, back-
ground and quality standards.
The first aspect, modelling annotators? individ-
ual behaviour and interdependences, has been ad-
dressed by Cohn and Specia (2013), who explored
multi-task Gaussian Processes as a way to jointly
learn from the output of multiple annotations. This
technique is suitable to cope with the unbalanced
distribution of training instances and yields better
models when heterogeneous training datasets are
available.
The second problem, the adaptability of QE
models, has not been explored yet. A common
trait of all current approaches, in fact, is the re-
liance on batch learning techniques, which assume
a ?static? nature of the world where new unseen
instances that will be encountered will be similar
to the training data.
4
However, similarly to trans-
lation memories that incrementally store translated
segments and evolve over time incorporating users
style and terminology, all components of a CAT
tool (the MT engine and the mechanisms to assign
quality scores to the suggested translations) should
take advantage of translators feedback.
On the MT system side, research on adaptive
approaches tailored to interactive SMT and CAT
scenarios explored the online learning protocol
(Littlestone, 1988) to improve various aspects of
the decoding process (Cesa-Bianchi et al, 2008;
Ortiz-Mart??nez et al, 2010; Mart??nez-G?omez et
al., 2011; Mart??nez-G?omez et al, 2012; Mathur
et al, 2013; Bertoldi et al, 2013).
As regards QE models, our work represents the
first investigation on incremental adaptation by ex-
ploiting users feedback to provide targeted (sys-
tem, user, or project specific) quality judgements.
3 Online QE for CAT environments
When operating with advanced CAT tools, transla-
tors are presented with suggestions (either match-
ing fragments from a translation memory or auto-
matic translations produced by an MT system) for
each sentence of a source document. Before being
approved and published, translation suggestions
may require different amounts of post-editing op-
erations depending on their quality.
4
This assumption holds in the WMT evaluation scenario,
but it is not necessarily valid in real operating conditions.
Each post-edition brings a wealth of dynamic
knowledge about the whole translation process
and the involved actors. For instance, adaptive QE
components could exploit information about the
distance between automatically assigned scores
and the quality standards of individual translators
(inferred from the amount of their corrections) to
?profile? their behaviour.
The online learning paradigm fits well with this
research objective. In the online framework, dif-
ferently from the batch mode, the learning al-
gorithm sequentially processes an unknown se-
quence of instances X = x
1
, x
2
, ..., x
n
, returning
a prediction p(x
i
) as output at each step. Differ-
ences between p(x
i
) and the true label p?(x
i
) ob-
tained as feedback are used by the learner to refine
the next prediction p(x
i+1
).
In our experiments on adaptive QE we aim to
predict the quality of the suggested translations
in terms of HTER, which measures the minimum
edit distance between the MT output and its man-
ually post-edited version in the [0,1] interval.
5
In
this scenario:
? The set of instances X is represented by
(source, target) pairs;
? The prediction p(x
i
) is the automatically es-
timated HTER score;
? The true label p?(x
i
) is the actual HTER score
calculated over the target and its post-edition.
At each step of the process, the goal of the learner
is to exploit user post-editions to reduce the differ-
ence between the predicted HTER values and the
true labels for the following (source, target) pairs.
As depicted in Figure 1, this is done as follows:
1. At step i, an unlabelled (source, target) pair
x
i
is sent to a feature extraction component.
To this aim, we used an adapted version
(Shah et al, 2014) of the open-source QuEst
6
tool (Specia et al, 2013). The tool, which im-
plements a large number of features proposed
by participants in the WMT QE shared tasks,
has been modified to process one sentence at
a time as requested for integration in a CAT
environment;
5
Edit distance is calculated as the number of edits (word
insertions, deletions, substitutions, and shifts) divided by the
number of words in the reference. Lower HTER values indi-
cate better translations.
6
http://www.quest.dcs.shef.ac.uk/
712
Figure 1: Online QE workflow. <src>, <trg> and <pe> respectively stand for the source sentence, the
target translation and the post-edited target.
2. The extracted features are sent to an on-
line regressor, which returns a QE prediction
score p(x
i
) in the [0,1] interval (set to 0 at the
first round of the iteration);
3. Based on the post-edition done by the user,
the true HTER label p?(x
i
) is calculated by
means of the TERCpp
7
open source tool;
4. The true label is sent back to the online al-
gorithm for a stepwise model improvement.
The updated model is then ready to process
the following instance x
i+1
.
This new paradigm for QE makes it possible
to: i) let the QE system learn from one point at
a time without complete re-training from scratch,
ii) customize the predictions of an existing QE
model with respect to a specific situation (post-
editor or domain), or even iii) build a QE model
from scratch when training data is not available.
For the sake of clarity it is worth observing that,
at least in principle, a model built in a batch fash-
ion could also be adapted to new test data. For in-
stance, this could be done by running periodic re-
training routines once a certain amount of new la-
belled instances has been collected (de facto mim-
icking an online process). Such periodic updates,
however, would not represent a viable solution in
the CAT framework where post-editors? work can-
not be slowed by time-consuming procedures to
re-train core system components from scratch.
7
goo.gl/nkh2rE
4 Evaluation framework
To measure the adaptation capability of different
QE models, we experiment with a range of condi-
tions defined by variable degrees of similarity be-
tween training and test data.
The degree of similarity depends on several fac-
tors: the MT engine used, the domain of the docu-
ments to be translated, and the post-editing style of
individual translators. In our experiments, the de-
gree of similarity is measured in terms of ?HTER,
which is computed as the absolute value of the dif-
ference between the average HTER of the training
and test sets. Large values indicate a low simi-
larity between training and test data and a more
challenging scenario for the learning algorithms.
4.1 Experimental setup
In the range of possible evaluation scenarios, our
experiments cover:
? One artificial setting (?5) obtained from the
WMT12 QE shared task data, in which train-
ing/test instances are arranged to reflect ho-
mogeneous distributions of the HTER labels.
? Two settings obtained from data collected
with a CAT tool in real working condi-
tions, in which different facets of the adap-
tive QE problem interact with each other.
In the first (user change, ?6.1), train-
ing and test data from the same domain are
obtained from different users. In the sec-
713
ond (user+domain change, ?6.2), train-
ing and test data are obtained from different
users and domains.
For each setting, we compare an adaptive and
an empty model against a system trained in batch
mode. The adaptive model is built on top of an
existing model created from the training data and
exploits the new test instances to refine its predic-
tions in a stepwise manner. The empty model only
learns from the test set, simulating the worst con-
dition where training data is not available. The
batch model is built by learning only from the
training data and is evaluated on the test set with-
out exploiting information from the test instances.
Each model is also compared against a common
baseline for regression tasks, which is particularly
relevant in settings featuring different data distri-
butions between training and test sets. This base-
line (? henceforth) is calculated by labelling each
instance of the test set with the mean HTER score
of the training set. Previous works (Rubino et al,
2013a) demonstrated that its results can be partic-
ularly hard to beat.
4.2 Performance indicator and feature set
To measure the adaptability of our model to a
given test set we compute the Mean Absolute Er-
ror (MAE), a metric for regression problems also
used in the WMT QE shared tasks. The MAE is
the average of the absolute errors e
i
= |f
i
? y
i
|,
where f
i
is the prediction of the model and y
i
is
the true value for the i
th
instance.
As our focus is on the algorithmic aspect, in all
experiments we use the same feature set, which
consists of the seventeen features proposed in
(Specia et al, 2009). This feature set, fully de-
scribed in (Callison-Burch et al, 2012), takes into
account the complexity of the source sentence
(e.g. number of tokens, number of translations per
source word) and the fluency of the target trans-
lation (e.g. language model probabilities). The
results of previous WMT QE shared tasks have
shown that these baseline features are particularly
competitive in the regression task (with only few
systems able to beat them at WMT12).
4.3 Online algorithms
In our experiments we evaluate two online algo-
rithms, OnlineSVR (Parrella, 2007)
8
and Passive-
8
http://www2.imperial.ac.uk/
?
gmontana/
onlinesvr.htm
Aggressive Perceptron (Crammer et al, 2006),
9
by
comparing their performance with a batch learning
strategy based on the Scikit-learn implementation
of Support Vector Regression (SVR).
10
The choice of the OnlineSVR and Passive-
Aggressive (OSVR and PA henceforth) is moti-
vated by different considerations. From a perfor-
mance point of view, as an adaptation of -SVR
which proved to be one of the top performing algo-
rithms in the regression QE tasks at WMT, OSVR
seems to be the best candidate. For this reason,
we use the online adaptation of -SVR proposed
by (Ma et al, 2003). The goal of OnlineSVR is to
find a way to add each new sample to one of three
sets (support, empty, error) maintaining the con-
sistency of a set of conditions known as Karush-
Kuhn Tucker (KKT) conditions. For each new
point, OSVR starts a cycle where the samples are
moved across the three sets until the KKT condi-
tions are verified and the new point is assigned to
one of the sets. If the point is identified as a sup-
port vector, the parameters of the model are up-
dated. This allows OSVR to benefit from the pre-
diction capability of -SVR in an online setting.
From a practical point of view, providing the
best trade off between accuracy and computational
time (He and Wang, 2012), PA represents a good
solution to meet the demand of efficiency posed
by the CAT framework. For each instance i, after
emitting a prediction and receiving the true label,
PA computes the -insensitive hinge loss function.
If its value is larger than the tolerance parameter
(), the weights of the model are updated as much
as the aggressiveness parameter C allows. In con-
trast with OSVR, which keeps track of the most
important points seen in the past (support vectors),
the update of the weights is done without consid-
ering the previously processed i-1 instances. Al-
though it makes PA faster than OSVR, this is a
riskier strategy because it may lead the algorithm
to change the model to adapt to outlier points.
5 Experiments with WMT12 data
The motivations for experiments with training and
test data featuring homogeneous label distribu-
tions are twofold. First, since in this artificial sce-
nario adaptation capabilities are not required for
the QE component, batch methods operate in the
ideal conditions (as training and test are indepen-
9
https://code.google.com/p/sofia-ml/
10
http://scikit-learn.org/
714
WMT Dataset
Train Test
? ? Batch Adaptive Empty
HTER MAE MAE MAE Alg. MAE Alg.
200 754 0.39 13.7 13.2 13.2
?
OSVR 13.5
?
OSVR
600 754 1.32 13.8 12.7 12.9
?
OSVR 13.5
?
OSVR
1500 754 1.22 13.8 12.7 12.8
?
OSVR 13.5
?
OSVR
Table 1: MAE of the best performing batch, adaptive and empty models on WMT12 data. Training sets
of different size and the test set have been arranged to reflect homogeneous label distributions.
dent and identically distributed). This makes pos-
sible to obtain from batch models the best possible
performance to compare with. Second, this sce-
nario provides the fairest conditions for such com-
parison because, in principle, online algorithms
are not favoured by the possibility to learn from
the diversity of the test instances.
For our controlled experiments we use the
WMT12 English-Spanish corpus, which consists
of 2,254 source-target pairs (1,832 for training,
422 for test). The HTER labels for our regression
task are calculated from the post-edited version
and the target sentences provided in the dataset.
To avoid biases in the label distribution, the
WMT12 training and test data have been merged,
shuffled, and eventually separated to generate
three training sets of different size (200, 600, and
1500 instances), and one test set with 754 in-
stances. For each algorithm, the training sets are
used for learning the QE models, optimizing pa-
rameters (i.e. C, , the kernel and its parame-
ters for SVR and OSVR; tolerance and aggressive-
ness for PA) through grid search in 10-fold cross-
validation.
Evaluation is carried out by measuring the per-
formance of the batch (learning only from the
training set), the adaptive (learning from the train-
ing set and adapting to the test set), and the empty
(learning from scratch from the test set) models in
terms of global MAE scores on the test set.
Table 1 reports the results achieved by the
best performing algorithm for each type of model
(batch, adaptive, empty). As can be seen, close
MAE values show a similar behaviour for the three
types of models.
11
With the same amount of train-
ing data, the performance of the batch and the
adaptive models (in this case always obtained with
OSVR) is almost identical. This demonstrates
that, as expected, the online algorithms do not take
11
Results marked with the ?
?
? symbol are NOT statisti-
cally significant compared to the corresponding batch model.
The others are always statistically significant at p?0.005, cal-
culated with approximate randomization (Yeh, 2000).
advantage of test data with a label distribution sim-
ilar to the training set. All the models outper-
form the baseline, even if the minimal differences
confirm the competitiveness of such a simple ap-
proach.
Overall, these results bring some interesting in-
dications about the behaviour of the different on-
line algorithms. First, the good results achieved
by the empty models (less than one MAE point
separates them from the best ones built on the
largest training set) suggest their high potential
when training data are not available. Second,
our results show that OSVR is always the best
performing algorithm for the adaptive and empty
models. This suggests a lower capability of PA to
learn from instances similar to the training data.
6 Experiments with CAT data
To experiment with adaptive QE in more realis-
tic conditions we used a CAT tool
12
to collect
two datasets of (source, target, post edited tar-
get) English-Italian tuples.The source sentences in
the datasets come from two documents from dif-
ferent domains, respectively legal (L) and infor-
mation technology (IT). The L document, which
was extracted from a European Parliament resolu-
tion published on the EUR-Lex platform,
13
con-
tains 164 sentences. The IT document, which was
taken from a software user manual, contains 280
sentences. The source sentences were translated
with two SMT systems built by training the Moses
toolkit (Koehn et al, 2007) on parallel data from
the two domains (about 2M sentences for IT and
1.5M for L). Post-editions were collected from
eight professional translators (four for each docu-
ment) operating with the CAT tool in real working
conditions.
According to the way they are created, the two
datasets allow us to evaluate the adaptability of
different QE models with respect to user changes
12
MateCat ? http://www.matecat.com/
13
http://eur-lex.europa.eu/
715
user change
Legal Domain
Train Test
? ? Batch Adaptive Empty
HTER MAE MAE MAE Alg. MAE Alg.
rad cons 20.5 21.4 20.6 14.5 PA 12.5 OSVR
cons rad 19.4 21.2 21.3 16.1 PA 11.3 OSVR
sim1 sim2 3.3 14.7 12.2 12.6
?
OSVR 12.9
?
OSVR
sim2 sim1 3.2 13.4 13.3 13.9
?
OSVR 15.2
?
OSVR
IT Domain
Train Test
? ? Batch Adaptive Empty
HTER MAE MAE MAE Alg MAE Alg
cons rad 12.8 19.2 19.8 17.5
?
OSVR 16.6 OSVR
rad cons 9.6 16.8 16.6 15.6 PA 15.5 OSVR
sim2 sim1 3.3 14.7 14.4 15
?
OSVR 15.5
?
OSVR
sim1 sim2 1.1 15 13.9 14.4
?
OSVR 16.1
?
OSVR
Table 2: MAE of the best performing batch, adaptive and empty models on CAT data collected from
different users in the same domain.
within the same domain (?6.1), as well as user and
domain changes at the same time (?6.2).
For each document D (L or IT), these two sce-
narios are obtained by dividing D into two parts
of equal size (80 instances for L and 140 for IT).
The result is one training set and one test set for
each post-editor within the same domain. For the
user change experiments, training and test sets
are selected from different post-editors within the
same domain. For the user+domain change
experiments, training and test sets are selected
from different post-editors in different domains.
On each combination of training and test sets,
the batch, adaptive, and empty models are trained
and evaluated in terms of global MAE scores on
the test set.
6.1 Dealing with user changes
Among the possible combinations of training and
test data from different post-editors in the same
domain, Table 2 refers to two opposite scenarios.
For each domain, these respectively involve the
most dissimilar and the most similar post-editors
according to the ?HTER. Also in this case, for
each model (batch, adaptive and empty) we only
report the MAE of the best performing algorithm.
The first scenario defines a challenging situation
where two post-editors (rad and cons) are charac-
terized by opposite behaviour. As evidenced by
the high ?HTER values, one of them (rad) is the
most ?radical? post-editor (performing more cor-
rections) while the other (cons) is the most ?con-
servative? one. As shown in Table 2, global MAE
scores for the online algorithms (both adaptive and
empty) indicate their good adaptation capabilities.
This is evident from the significant improvements
both over the baseline (?) and the batch models.
Interestingly, the best results are always achieved
by the empty models (with MAE reductions up to
10 points when tested on rad in the L domain,
and 3.2 points when tested on rad in the IT do-
main). These results (MAE reductions are always
statistically significant) suggest that, when deal-
ing with datasets with very different label distri-
butions, the evident limitations of batch methods
are more easily overcome by learning from scratch
from the feedback of a new post-editor. This also
holds when the amount of test points to learn from
is limited, as in the L domain where the test set
contains only 80 instances. From the application-
oriented perspective that motivates our work, con-
sidering the high costs of acquiring large and rep-
resentative QE training data, this is an important
finding.
The second scenario defines a less challeng-
ing situation where the two post-editors (sim1 and
sim2) are characterized by the most similar be-
haviour (small ?HTER). This scenario is closer to
the situation described in Section ?5. Also in this
case MAE results for the adaptive and empty mod-
els are slightly worse, but not significantly, than
those of the batch models and the baseline. How-
ever, considering the very small amount of ?unin-
formative? instances to learn from (especially for
the empty models), these lower results are not sur-
prising.
A closer look at the behaviour of the online al-
gorithms in the two domains leads to other obser-
vations. First, OSVR always outperforms PA for
the empty models and when post-editors have sim-
716
user+domain change
Train Test
? ? Batch Adaptive Empty
HTER MAE MAE MAE Alg MAE Alg
L cons IT rad 24.5 26.4 27 18.2 OSVR 16.6 OSVR
IT rad L cons 24.0 24.9 25.4 19.7 OSVR 12.5 OSVR
L rad L cons 20.5 21.4 20.6 14.5 PA 12.5 OSVR
L cons L rad 19.4 21.2 21.3 16.1 PA 11.3 OSVR
IT cons L cons 13.5 17.3 17.5 15.7 OSVR 12.5 OSVR
IT cons IT rad 12.8 19.2 19.8 17.5 OSVR 16.6 OSVR
L cons IT cons 12.7 17.6 17.6 15.1 OSVR 15.5 OSVR
IT rad IT cons 9.6 16.8 16.6 15.6 PA 15.5 OSVR
IT cons L rad 8.3 12.3 13 10.7 OSVR 11.3 OSVR
L rad IT rad 6.8 17 16.9 16.2 OSVR 16.6 OSVR
L rad IT cons 5.0 15.4 16.2 14.7 OSVR 15.5 OSVR
IT rad L rad 2.2 10.6 10.8 10.5 OSVR 11.3 OSVR
Table 3: MAE of the best performing batch, adaptive and empty models on CAT data collected from
different users and domains.
ilar behaviour, which are situations where the al-
gorithm does not have to quickly adapt or react to
sudden changes.
Second, PA seems to perform better for the
adaptive models when the post-editors have sig-
nificantly different behaviour and a quick adapta-
tion to the incoming points is required. This can
be motivated by the fact that PA relies on a simpler
and less robust learning strategy that does not keep
track of all the information coming from the previ-
ously processed instances, and can easily modify
its weights taking into consideration the last seen
point (see Section ?3). For OSVR the addition of
new points to the support set may have a limited
effect on the whole model, in particular if the num-
ber of points in the set is large. This also results
in a different processing time for the two algo-
rithms.
14
For instance, in the empty configurations
on IT data, OSVR devotes 6.0 ms per instance to
update the model, while PA devotes 4.8ms, which
comes at the cost of lower performance.
6.2 Dealing with user and domain changes
In the last round of experiments we evaluate the
reactivity of different online models to simultane-
ous user and domain changes. To this aim, our
QE models are created using a training set coming
from one domain (L or IT), and then used to pre-
dict the HTER labels for the test instances coming
from the other domain (e.g. training on L, testing
on IT).
Among the possible combinations of training
14
Their complexity depends on the number of features (f )
and the number of previously seen instances (n). While for
PA it is linear in f, i.e. O(f), for OSVR it is quadratic in n, i.e.
O(n
2
*f).
and test data, Table 3 refers to scenarios involv-
ing the most conservative and radical post-editors
in each domain (previously identified with cons
and rad)
15
. In the table, results are ordered ac-
cording to the ?HTER computed between the se-
lected post-editor in the training domain (e.g. L
cons) and the selected post-editor in the test do-
main (e.g. IT rad). For the sake of comparison,
we also report (grey rows) the results of the ex-
periments within the same domain presented in
?6.1. For each type of model (batch, adaptive and
empty) we only show the MAE obtained by the
best performing algorithm.
Intuitively, dealing with simultaneous user and
domain changes represents a more challenging
problem compared to the previous setting where
only post-editors changes were considered. Such
intuition is confirmed by the results of the adaptive
models that outperform both the baseline (?) and
the batch models even for low ?HTER values. Al-
though in these cases the distance between train-
ing and test data is comparable to the experiments
with similar post-editors working in the same do-
main (sim1 and sim2), here the predictive power
of the batch models seems in fact to be lower. The
same holds also for the empty models except in
two cases where the ?HTER is the smallest (2.2
and 5.0). This is a strong evidence of the fact that,
in case of domain changes, online models can still
learn from new test instances even if they have a
label distribution similar to the training set.
When the distance between training and test in-
creases, our results confirm our previous findings
15
For brevity, we omit the results for the other post-editors
which, however, show similar trends with respect to the pre-
vious experiments.
717
about the potential of the empty models. The ob-
served MAE reductions range in fact from 10.4
to 12.9 points for the two combinations with the
highest ?HTER.
From the algorithmic point of view, our results
indicate that OSVR achieves the best performance
for all the combinations involving user and domain
changes. This contrasts with the results of most of
the combinations involving only user changes with
post-editors characterized by opposite behaviour
(grey rows in Table 3). However, it has to be re-
marked that in the case of heterogeneous datasets
the difference between the two algorithms is al-
ways very high. In our experiments, when PA out-
performs OSVR, its MAE results are significantly
lower and vice-versa (respectively up to 1.5 and
1.7 MAE points). This suggests that, although PA
is potentially capable of achieving higher results
and better adapt to the new test points, its instabil-
ity makes it less reliable for practical use.
As a final analysis of our results, we investi-
gated how the performance of the different types
of models (batch, adaptive, empty) relates to the
distance between training and test sets. To this
aim, we computed the Pearson correlation be-
tween the ?HTER (column 3 in Table 3) and the
MAE of each model (columns 5, 6 and 8), which
respectively resulted in 0.9 for the batch, 0.63 for
the adaptive and -0.07 for the empty model. These
values confirm that batch models are heavily af-
fected by the dissimilarity between training and
test data: large differences in the label distribution
imply higher MAE results and vice-versa. This
is in line with our previous findings about batch
models that, learning only from the training set,
cannot leverage possible dissimilarities of the test
set. The lower correlation observed for the adap-
tive models also confirms our intuitions: adapting
to the new test points, these models are in fact
more robust to differences with the training data.
As expected, the results of the empty models are
completely uncorrelated with the ?HTER since
they only use the test set.
This analysis confirms that, even when dealing
with different domains, the similarity between the
training and test data is one of the main factors that
should drive the choice of the QE model. When
this distance is minimal, batch models can be a
reasonable option, but when the gap between train-
ing and test data increases, adaptive or empty mod-
els are a preferable choice to achieve good results.
7 Conclusion
In the CAT scenario, each translation job can be
seen as a complex situation where the user (his
personal style and background), the source doc-
ument (the language and the domain) and the un-
derlying technology (the translation memory and
the MT engine that generate translation sugges-
tions) contribute to make the task unique. So far,
the adaptability to such specificities (a major chal-
lenge for CAT technology) has been mainly sup-
ported by the evolution of translation memories,
which incrementally store translated segments in-
corporating the user style. The wide adoption of
translation memories demonstrates the importance
of capitalizing on such information to increase
translators productivity.
While this lesson recently motivated research
on adaptive MT decoders that learn from user cor-
rections, nothing has been done to develop adap-
tive QE components. In the first attempt to ad-
dress this problem, we proposed the application
of the online learning protocol to leverage users
feedback and to tailor QE predictions to their qual-
ity standards. Besides highlighting the limitations
of current batch methods to adapt to user and
domain changes, we performed an application-
oriented analysis of different online algorithms fo-
cusing on specific aspects relevant to the CAT sce-
nario. Our results show that the wealth of dynamic
knowledge brought by user corrections can be ex-
ploited to refine in a stepwise fashion the qual-
ity judgements in different testing conditions (user
changes as well as simultaneous user and domain
changes).
As an additional contribution, to spark further
research on this facet of the QE problem, our adap-
tive QE infrastructure (integrating all the compo-
nents and the algorithms described in this paper)
has been released as open source. Its C++ im-
plementation is available at http://hlt.fbk.eu/
technologies/aqet.
Acknowledgements
This work has been partially supported by the EC-
funded project MateCat (ICT-2011.4.2-287688).
References
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite: When less is more for
translation quality estimation. In Proceedings of the
718
8th
Workshop on Statistical Machine Translation,
Sofia, Bulgaria, August.
Nicola Bertoldi, Mauro Cettolo, and Federico Mar-
cello. 2013. Cache-based Online Adaptation
for Machine Translation Enhanced Computer As-
sisted Translation. In Proceedings of the XIV Ma-
chine Translation Summit, pages 1147?1162, Nice,
France.
Ergun Bicici. 2013. Feature decay algorithms for fast
deployment of accurate statistical machine transla-
tion systems. In Proceedings of the 8
th
Workshop
on Statistical Machine Translation, Sofia, Bulgaria,
August.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence Es-
timation for Machine Translation. Summer work-
shop final report, JHU/CLSP.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proceedings
of the 8
th
Workshop on Statistical Machine Transla-
tion, WMT-2013, pages 1?44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the 7
th
Work-
shop on Statistical Machine Translation (WMT?12),
pages 10?51, Montr?eal, Canada.
Nicol`o Cesa-Bianchi, Gabriel Reverberi, and Sandor
Szedmak. 2008. Online Learning Algorithms for
Computer-Assisted Translation. Deliverable D4.2,
SMART: Statistical Multilingual Analysis for Re-
trieval and Translation.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of the 51
st
Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL-2013, pages 32?42, Sofia, Bulgaria.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
Passive-Aggressive Algorithms. J. Mach. Learn.
Res., 7:551?585, December.
Jos?e G.C. de Souza, Christian Buck, Marco Turchi, and
Matteo Negri. 2013a. FBK-UEdin participation to
the WMT13 quality estimation shared task. In Pro-
ceedings of the 8
th
Workshop on Statistical Machine
Translation, Sofia, Bulgaria, August.
Jos?e G.C. de Souza, Miquel Espl`a-Gomis, Marco
Turchi, and Matteo Negri. 2013b. Exploiting Quali-
tative Information from Automatic Word Alignment
for Cross-lingual NLP Tasks. In Proceedings of the
51
st
Annual Meeting of the Association for Compu-
tational Linguistics - Short Papers, pages 771?776,
Sofia, Bulgaria.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Tree Kernels for Machine Transla-
tion Quality Estimation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT?12), pages 109?113, Montr?eal, Canada.
Zhengyan He and Houfeng Wang. 2012. A Com-
parison and Improvement of Online Learning Al-
gorithms for Sequence Labeling. In Proceedings
of the 24th International Conference on Compu-
tational Linguistics (COLING 2012), pages 1147?
1162, Mumbai, India.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45
th
Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180.
Maarit Koponen, Wilker Aziz, Luciana Ramos, and
Lucia Specia. 2012. Post-editing Time as a Mea-
sure of Cognitive Effort. In Proceedings of the
AMTA 2012 Workshop on Post-editing Technology
and Practice (WPTP 2012), San Diego, California.
Maarit Koponen. 2012. Comparing Human Percep-
tions of Post-editing Effort with Post-editing Op-
erations. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 181?190,
Montr?eal, Canada.
Nick Littlestone. 1988. Learning Quickly when Irrel-
evant Attributes Abound: A New Linear-Threshold
Algorithm. In Machine Learning, pages 285?318.
Junshui Ma, James Theiler, and Simon Perkins. 2003.
Accurate Online Support Vector Regression. Neural
Computation, 15:2683?2703.
Pascual Mart??nez-G?omez, Germ?an Sanchis-Trilles, and
Francisco Casacuberta. 2011. Online Learning via
Dynamic Reranking for Computer Assisted Transla-
tion. In Proceedings of the 12th international con-
ference on Computational linguistics and intelligent
text processing - Volume Part II, CICLing?11.
Pascual Mart??nez-G?omez, Germ?an Sanchis-Trilles, and
Francisco Casacuberta. 2012. Online adaptation
strategies for statistical machine translation in post-
editing scenarios. Pattern Recognition, 45(9):3193?
3203, September.
Prashant Mathur, Mauro Cettolo, and Marcello Fed-
erico. 2013. Online Learning Approaches in Com-
puter Assisted Translation. In Proceedings of the
8
th
Workshop on Statistical Machine Translation,
Sofia, Bulgaria.
719
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Match without a Referee: Evaluating MT
Adequacy without Reference Translations. In Pro-
ceedings of the 7
th
Workshop on Statistical Machine
Translation, pages 171?180, Montr?eal, Canada.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for in-
teractive statistical machine translation. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT ?10, pages
546?554, Stroudsburg, PA, USA.
Francesco Parrella. 2007. Online support vector re-
gression. Master?s Thesis, Department of Informa-
tion Science, University of Genoa, Italy.
Raphael Rubino, Jos?e G.C. de Souza, Jennifer Fos-
ter, and Lucia Specia. 2013a. Topic Models for
Translation Quality Estimation for Gisting Purposes.
In Proceedings of the Machine Translation Summit
XIV, Nice, France.
Raphael Rubino, Antonio Toral, S Cort?es Va??llo, Jun
Xie, Xiaofeng Wu, Stephen Doherty, and Qun Liu.
2013b. The CNGL-DCU-Prompsit translation sys-
tems for WMT13. In Proceedings of the 8
th
Work-
shop on Statistical Machine Translation, pages 211?
216, Sofia, Bulgaria.
Kashif Shah, Marco Turchi, and Lucia Specia. 2014.
An Efficient and User-friendly Tool for Machine
Translation Quality Estimation. In Proceedings of
the 9
t
h International Conference on Language Re-
sources and Evaluation, Reykjavik, Iceland.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Trans-
lation in the Americas, pages 223?231, Cambridge,
Massachusetts, USA.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
the 7
th
Workshop on Statistical Machine Translation
(WMT?12), pages 145?151, Montr?eal, Canada.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13
th
Annual Con-
ference of the European Association for Machine
Translation (EAMT?09), pages 28?35, Barcelona,
Spain.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation versus Quality Es-
timation. Machine translation, 24(1):39?50.
Lucia Specia, Kashif Shah, Jos?e G.C. de Souza, and
Trevor Cohn. 2013. QuEst - A Translation Qual-
ity Estimation Framework. In Proceedings of the
51
st
Annual Meeting of the Association for Compu-
tational Linguistics: System Demonstrations, ACL-
2013, pages 79?84, Sofia, Bulgaria.
Marco Turchi and Matteo Negri. 2014. Automatic An-
notation of Machine Translation Datasets with Bi-
nary Quality Judgements. In Proceedings of the 9
th
International Conference on Language Resources
and Evaluation, Reykjavik, Iceland.
Marco Turchi, Matteo Negri, and Marcello Federico.
2013. Coping with the Subjectivity of Human
Judgements in MT Quality Estimation. In Proceed-
ings of the 8
th
Workshop on Statistical Machine
Translation, pages 240?251, Sofia, Bulgaria.
Alexander Yeh. 2000. More Accurate Tests for the
Statistical Significance of Result Differences. In
Proceedings of the 18th conference on Computa-
tional linguistics (COLING 2000) - Volume 2, pages
947?953, Saarbrucken, Germany.
720
