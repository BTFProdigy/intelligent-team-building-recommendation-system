Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 49?57,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
A Hierarchical Bayesian Model for
Unsupervised Induction of Script Knowledge
Lea Frermann
1
l.frermann@ed.ac.uk
Ivan Titov
2
titov@uva.nl
1
ILCC, School of Informatics, University of Edinburgh, United Kingdom
2
ILLC, University of Amsterdam, Netherlands
3
Department of Computational Linguistics, Saarland University, Germany
Manfred Pinkal
3
pinkal@coli.uni-sb.de
Abstract
Scripts representing common sense
knowledge about stereotyped sequences
of events have been shown to be a valu-
able resource for NLP applications. We
present a hierarchical Bayesian model for
unsupervised learning of script knowledge
from crowdsourced descriptions of human
activities. Events and constraints on event
ordering are induced jointly in one unified
framework. We use a statistical model
over permutations which captures event
ordering constraints in a more flexible
way than previous approaches. In order
to alleviate the sparsity problem caused
by using relatively small datasets, we
incorporate in our hierarchical model an
informed prior on word distributions. The
resulting model substantially outperforms
a state-of-the-art method on the event
ordering task.
1 Introduction
A script is a ?predetermined, stereotyped se-
quence of actions that define a well-known sit-
uation? (Schank and Abelson, 1975). While
humans acquire such common-sense knowledge
over their lifetime, it constitutes a bottleneck for
many NLP systems. Effective question answer-
ing and summarization are impossible without a
form of story understanding, which in turn has
been shown to benefit from access to databases of
script knowledge (Mueller, 2004; Miikkulainen,
1995). Knowledge about the typical ordering of
events can further help assessing document co-
herence and generating coherent text. Here, we
present a general method for acquiring data bases
of script knowledge.
Our work may be regarded as complementary to
existing work on learning script knowledge from
natural text (cf. (Chambers and Jurafsky, 2008)),
as not all types of scripts are elaborated in natural
text ? being left implicit because of assumed read-
ers? world knowledge. Our model, operating on
data obtained in a cheap way by crowdsourcing,
is applicable to any kind of script and can fill this
gap. We follow work in inducing script knowl-
edge from explicit instantiations of scripts, so-
called event sequence descriptions (ESDs) (Reg-
neri et al., 2010). Our data consists of sets of
ESDs, each set describing a well-known situation
we will call scenario (e.g., ?washing laundry?).
An ESD consists of a sequence of events, each
describing an action defining part of the scenario
(e.g., ?place the laundry in the washing machine?).
We refer to descriptions of the same event across
ESDs as event types. We refer to entities involved
in a scenario as participants (e.g., a ?washing ma-
chine? or a ?detergent?), and to sets of participant
descriptions describing the same entity as partici-
pant types.
For each type of scenario, our model clusters
descriptions which refer to the same type of event,
and infers constraints on the temporal order in
which the events types occur in a particular sce-
nario. Common characteristics of ESDs such as
event optionality and varying degrees of temporal
flexibility of event types make this task nontrivial.
We propose a model which, in contrast to previ-
ous approaches, explicitly targets these character-
istics. We develop a Bayesian formulation of the
script learning problem, and present a generative
model for joint learning of event types and order-
ing constraints, arguing that the temporal position
of an event in an ESD provides a strong cue for its
type, and vice versa. Our model is unsupervised
in that no event- or participant labels are required
for training.
We model constraints on the order of event
types using a statistical model over permutations,
the Generalized Mallows Model (GMM; Fligner
49
and Verducci (1986)). With the GMM we can flex-
ibly model apparent characteristics of scripts, such
as event type-specific temporal flexibility. Assum-
ing that types of participants provide a strong cue
for the type of event they are observed in, we use
participant types as a latent variable in our model.
Finally, by modeling event type occurrence using
Binomial distributions, we can model event op-
tionality, a characteristic of scripts that previous
approaches did not capture.
We evaluate our model on a data set of ESDs
collected via web experiments from non-expert
annotators by Regneri et al. (2010) and compare
our model against their approach. Our model
achieves an absolute average improvement of 7%
over the model of Regneri et al. on the task of
event ordering.
For our unsupervised Bayesian model the lim-
ited size of this training set constitutes an ad-
ditional challenge. In order to alleviate this
problem, we use an informed prior on the word
distributions. Instead of using Dirichlet priors
which do not encode a-priori correlations between
words, we incorporate a logistic normal distri-
bution with the covariance matrix derived from
WordNet. While we will show that prior knowl-
edge as defined above enables the application of
our model to small data sets, we emphasize that
the model is generally widely applicable for two
reasons. First, the data, collected using crowd-
sourcing, is comparatively easy and cheap to ex-
tend. Secondly, our model is domain independent
and can be applied to scenario descriptions from
any domain without any modification. Note that
parameters were tuned on held-out scenarios, and
no scenario-specific tuning was performed.
2 Related Work
In the 1970s, scripts were introduced as a way to
equip AI systems with world knowledge (Schank
and Abelson, 1975; Barr and Feigenbaum, 1986).
Task-specific script databases were developed
manually. FrameNet (Baker et al., 1998) follows a
similar idea, in defining verb frames together with
argument types that can fill the verbs? argument
slots. Frames can then be combined into ?scenario
frames?. Manual composition of such databases,
is arguably expensive and does not scale well.
This paper follows a series of more recent work
which aims to infer script knowledge automati-
cally from data. Chambers and Jurafsky (2008)
present a system which learns narrative chains
from newswire texts. Relevant phrases are iden-
tified based on shared protagonists. The phrases
are clustered into equivalence classes and tempo-
rally ordered using a pipeline of methods. We
work with explicit event sequence descriptions of
a specific scenario, arguing that large-scale com-
mon sense knowledge is hard to acquire from nat-
ural text, since it is often left implicit. Regneri
et al. (2010) induce script knowledge from ex-
plicit ESDs using a graph-based method. Event
types and ordering constraints are induced by
aligning descriptions of equivalent events using
WordNet-based semantic similarity. On this basis
an abstract graph-representation (Temporal Script
Graph; TSG) of the scenario is computed, us-
ing Multiple Sequence Alignment (MSA). Our
work follows the work of Regneri et al. (2010),
in that we use the same data and aim to focus on
the same task. However, the two approaches de-
scribed above employ a pipeline architecture and
treat event learning and learning ordering con-
straints as separate problems. In contrast, we pro-
pose to learn both tasks jointly. We incorporate
both tasks in a hierarchical Bayesian model, thus
using one unified framework.
A related task, unsupervised frame induction,
has also been considered in the past (Titov and
Klementiev, 2011; Modi et al., 2012; O?Connor,
2012); the frame representations encode events
and participants but ignore the temporal aspect of
script knowledge.
We model temporal constraints on event type
orderings with the Generalized Mallows Model
(GMM; Mallows (1957); Fligner and Verducci
(1986); Klementiev et al. (2008)), a statistical
model over permutations. The GMM is a flexi-
ble model which can specify item-specific sensi-
tivity to perturbation from the item?s position in
the canonical permutation. With the GMM we are
thus able to model event type-specific temporal
flexibility ? a feature of scripts that MSA cannot
capture.
The GMM has been successfully applied to
modeling ordering constraints in NLP tasks. Chen
et al. (2009) augment classical topic models with
a GMM, under the assumption that topics in struc-
tured domains (e.g., biographies in Wikipedia)
tend to follow an underlying canonical ordering,
an assumption which matches well our data (the
annotators were asked to follow the temporal or-
50
der of events in their descriptions (Regneri et al.,
2010)). Chen et al. show that for these domains
their approach significantly outperforms Marko-
vian modeling of topics. This is expected as
Markov models (MMs) are not very appropriate
for representing linear structure with potentially
missing topics (e.g., they cannot encode that ev-
ery topic is assigned to at most one continuous
fragment of text). Also GMMs are preferable for
smaller collections such as ours, as the parameter
number is linear in the number of topics (i.e., for
us, event types) rather than quadratic as in Markov
models. We are not aware of previous work on
modeling events with GMMs. Conversely, MMs
were considered in the very recent work of Che-
ung et al. (2013) in the context of script induction
from news corpora where the Markovian assump-
tion is much more natural.
There exists a body of work for learning par-
ticipant types involved in scripts. Regneri et al.
(2011) extend their work by inducing participant
types on the basis of the TSG, using structural in-
formation about participant mentions in the TSG
as well as WordNet similarity, which they then
combine into an Integer Linear Program. Simi-
larly, Chambers and Jurafsky (2009) extend their
work on narrative chains, presenting a system with
which they jointly learn event types and semantic
roles of the participants involved, but do not con-
sider event orderings. We include participant types
as a latent feature in our model, assuming that par-
ticipant mentions in an event description are a pre-
dictive feature for the corresponding event type.
One way of alleviating the problem of small
data sets is incorporating informed prior knowl-
edge. Raina et al. (2006) encode word correlations
in a variance-covariance matrix of a multivariate
normal distribution (MVN), and sample prior pa-
rameter vectors from it, thus introducing depen-
dencies among the parameters. They induce the
covariances from supervised learning tasks in the
transfer learning set-up. We use the same idea, but
obtain word covariances from WordNet relations.
In a slightly different setting, covariance matrices
of MVNs have been used in topic models to induce
correlation between topics in documents (Blei and
Lafferty, 2006).
3 Problem Formulation
Our input consists of a corpus of scenario-specific
ESDs, and our goal is to label each event descrip-
tion in an ESD with one event type e. We specify
the number of possible event types E a priori as a
number exceeding the number of event types in all
the scripts considered. The model will select an
effective subset of those types.
Assume a scenario-specific corpus c, consist-
ing of D ESDs, c = {d
1
, ..., d
D
}. Each
ESD d
i
consists of N
d
event descriptions d
i
=
{d
i,1
, ..., d
i,N
i
}. Boundaries between descriptions
of single events are marked in the data. For each
event description d
i,n
a bag of participant descrip-
tions is extracted. Each participant description
corresponds to one noun phrase as identified au-
tomatically by a dependency parser (cf. Regneri
et al. (2011)). We also associate participant types
with participant descriptions, these types are latent
and induced at the inference stage.
Given such a corpus of ESDs, our model assigns
each event description d
i,n
in an ESD d
i
one event
type z
d
i,n
= e, where e ? {1, ..., E}. Assuming
that all ESDs are generated from the same under-
lying set of event types, our objective is to assign
the same event type to equivalent event descrip-
tions across all ESDs in the corpus.
We furthermore assume that there exists a
canonical temporal ordering of event types for
each scenario type, and that events in observed
scenarios tend to follow this ordering, but allowing
for some flexibility. The event labeling sequence
z
d
i
of an entire ESD should reflect this canonical
ordering. This allows us to use global structural
patterns of ESDs in the event type assignments,
and thus introducing dependence between event
types through their position in the sequence.
4 The Model
Before we describe our model, we briefly explain
the Generalized Mallows Model (GMM) which
we use to encode a preference for linear ordering
of events in a script.
4.1 The (Generalized) Mallows Model
The Mallows Model (MM) is a statistical model
over orderings (Mallows, 1957). It takes two pa-
rameters ?, the canonical ordering, and ? > 0,
a dispersion parameter. The dispersion parame-
ter is a penalty for the divergence d(pi,?) of an
observed ordering pi from the canonical ordering
?. The divergence can be any distance metric but
Kendall?s tau distance (?bubble-sort? distance), a
number of swaps needed to bring pi in the order ?,
51
is arguably the most common choice. The proba-
bility of an observed ordering pi is defined as
P (pi|?,?) =
e
?? d(pi,?)
?(?)
,
where ?(?) is a normalization factor. The distri-
bution is centered around the canonical ordering
(as d(?,?) = 0), and the probability decreases
exponentially with an increasing distance. For our
purposes, without loss of generality, we can as-
sume that ? is the identity permutation, that is
? = [1, . . . , n], where n is the number of items.
The Mallows model has been generalized to
take as a parameter a vector of item-specific
dispersion parameters ? (Fligner and Verducci,
1986). In order to introduce this extension, we
first need to reformulate Kendall?s tau in a way
that captures item-specific distance. An ordering
pi of n items can be equivalently represented by
a vector of inversion counts v of length n ? 1,
where each component v
i
equals the number of
items j > i that occur before item i in pi. For
example, for an observed ordering pi = [2,1,0] the
inversion vector v = (2, 1).
1
Then the generalized
Mallows model (GMM) is defined as
GMM(pi|?) ?
?
i
e
??
i
v
i
.
The GMM can be factorized into item-specific
components, which allows for efficient inference:
GMM
i
(v
i
|?
i
) ? e
??
i
v
i
. (1)
Intuitively, we will be able to induce event type-
specific penalty parameters, and will thus be able
to model individual degrees of temporal flexibility
among the event types.
Since the GMM is member of the exponential
family, a conjugate prior can be defined, which
allows for efficient learning of the parameters ?
(Fligner and Verducci, 1990). Like the GMM, its
prior distribution GMM
0
can be factorized into
independent components for each item i:
GMM
0
(?
i
|v
i,0
, ?
0
) ? e
??
i
v
i,0
?log(?
i
(?
i
))?
0
. (2)
The parameters v
i,0
and ?
0
represent our prior
beliefs about flexibility for each item i, and the
strength of these beliefs, respectively.
1
Trivially, the inversion count for the last element in the
canonical ordering is always 0.
4.2 The Generative Story
Our model encodes two fundamental assumptions,
based on characteristics observed in the data: (1)
We assume that each event type can occur at most
once per ESD; (2) Each participant type is as-
sumed to occur at most once per event type.
The formalized generative story is given in Fig-
ure 1. For each document (ESD) d, we decide in-
dependently for each event type e whether to re-
alize it or not by drawing from Binomial(?
e
).
2
We obtain a binary event vector t where t
e
= 1 if
event type e is realized and t
e
= 0 otherwise. We
draw an event ordering pi from GMM(?), repre-
sented as a vector of inversion counts.
Now, we pass event types in the order defined
by pi. For each realized event type i (i.e., i :
t
i
= 1), we first generate a word (normally a
predicate) from the corresponding language model
Mult(?
i
). Then we independently decide for each
participant type p whether to realize it or not with
the probability Binomial(?
i
p
). If realized, the
participant word (its syntactic head) is generated
from the participant language model Mult($
p
).
Note that though the distribution controlling
frequency of participant generation (?
i
j
) is event
type-specific, the language model associated with
the participant (Mult($
j
)) is shared across
events, thus, ensuring that participant types are de-
fined across events.
The learnt binary realization parameters ? and
?
e
should ensure that an appropriate number of
events and participants is generated (e.g. the real-
ization probability for obligatory events, observed
in almost every ESD for a particular scenario,
should be close to 1).
Priors We draw the parameters for the binomial
distributions from the Beta distribution, which al-
lows us to model a global preference for using
only few event types and only few participant
types for each event type. We draw the parame-
ters of the multinomials from the Dirichlet distri-
bution, and can thus model a preference towards
sparsity. The GMM parameter vector ? is drawn
from GMM
0
(c.f. Equation (2)).
4.3 Adding Prior Knowledge
Since we are faced with a limited amount of train-
ing data, we augment the model described above
2
We slightly abuse the notation by dropping the super-
script d for ESD-specific variables.
52
Generation of parameters
for event type e = 1, . . . , E do
?
e
? Beta(?
+
, ?
?
) [ freq of event ]
?
e
? Dirichlet(?) [event lang mod]
for participant type p = 1, . . . , P do
?
e
p
? Beta(?
+
, ?
?
) [ freq of ptcpt ]
for participant type p = 1, . . . , P do
$
p
? Dirichlet(?) [ ptcpt lang mod ]
for event type e = 1, . . . , E ? 1 do
?
e
? GMM
0
(?
0
,?
0
) [ ordering params]
Generation of ESD d
for event type e = 1, . . . , E do
t
e
? Binomial(?
e
) [ realized events ]
pi ? GMM(?,?) [ event ordering ]
for event i from pi s.th. t
i
=1 do
w
i
?Mult(?
i
) [ event lexical unit ]
for participant type p = 1, . . . , P do
u
p
? Binomial(?
e
p
) [ realized ptcpts ]
if u
p
= 1 then
w
p
?Mult($
p
) [ ptcpt lexical unit]
Figure 1: The generative story of the basic model.
to encode correlations between semantically simi-
lar words in the priors for language models. We
describe our approach by first introducing the
model extension allowing for injecting prior cor-
relations between words, and then explaining how
the word correlations are derived from WordNet
(Fellbaum, 1998). Since the event vocabulary
and the participant vocabulary are separate in our
model, the following procedure is carried out sep-
arately, but equivalently, for the two vocabularies.
4.3.1 Modeling Word Correlation
Dirichlet distributions do not provide a way to en-
code correlations between words. To tackle this
problem we add another level in the model hier-
archy: instead of specifying priors Dirichlet(?)
and Dirichlet(?) directly, we generate them for
each event type e and participant type p using mul-
tivariate normal distributions.
The modification for the generative story is
shown in Figure 2. In this extension, each event
type e and participant type p has a different associ-
ated (nonsymmetric) Dirichlet prior, ?
e
and ?
p
, re-
spectively. The generative story for choosing ?
e
is
the following: A vector ?
e
is drawn from the zero-
mean normal distribution N(?
?
,0), where ?
?
is
Generation of parameters ?
e
and $
p
for event type e = 1, . . . , E do
?
e
? N(?
?
, 0)
for all words w do
?
e
w
=exp(?
e
w
)/
?
w
?
exp(?
e
w
?
) [ Dir prior]
?
e
? Dirichlet(?
e
) [event lang mod]
for participant type p = 1, . . . , P do
?
p
? N(?
?
, 0)
for all words w do
?
p
w
=exp(?
p
w
)/
?
w
?
exp(?
p
w
?
) [ Dir prior]
$
p
? Dirichlet(?
p
) [ ptcpt lang mod ]
Figure 2: The modified parameter generation pro-
cedure for ?
e
and $
p
to encode word correlations.
the covariance matrix encoding the semantic relat-
edness of words (see Section 4.3.2). The vector?s
dimensionality corresponds to size of the vocab-
ulary of event words. Then, the vector is expo-
nentiated and normalized to yield ?
e
.
3
The same
procedure is used to choose ?
p
as shown in Figure
2.
4.3.2 Defining Semantic Similarity
We use WordNet to obtain semantic similarity
scores for each pair of words in our vocabulary.
Since we work on limited domains, we define a
subset of WordNet as all synsets that any word in
our vocabulary is a member of, plus the hypernym
sets of all these synsets. We then create a feature
vector for each word f(w
i
) as follows:
f(w
i
)
n
=
{
1 any sense of w
i
? synset n
0 otherwise
The similarity of two words w
i
and w
j
is de-
fined as the dot product f(w
i
) ?f(w
j
). We use this
similarity to define the covariance matrices ?
?
and
?
?
. Each component (i, j) stores the similarity
between words w
i
and w
j
as defined above. Note
that the matrices are guaranteed to be valid covari-
ance matrices, as they are positive semidefinite by
construction.
5 Inference
Our goal is to infer the set of labelings z of our
corpus of ESDs. A labeling z consists of event
3
In fact, Dirichlet concentration parameters do not need
to sum to one. We experimented with normalizing them to
yield a different constant, thus regulating the influence of the
prior, but have not observed much of improvement from this
extension.
53
types t, participant types u and event ordering pi.
Additionally, we induce parameters of our model:
ordering dispersion parameters (?) and the lan-
guage model parameters ? and ?. We induce these
variables conditioned on all the observable words
in the data setw. Since direct joint sampling from
the posterior distributions is intractable, we use
Gibbs sampling for approximate inference. Since
we chose conjugate prior distributions over the pa-
rameter distributions, we can ?collapse? the Gibbs
sampler by integrating out all parameters (Grif-
fiths and Steyvers, 2004), except for the ones listed
above. The unnormalized posterior can be written
as the following product of terms:
P (z,?,?, ?|w) ?
?
e
DCM
e
?
p
DCM
p
?
e
BBM
e
?
p
BBM
ep
?
e
GMM
e
MN
e
?
p
MN
p
.
The terms DCM
e
and DCM
p
are Dirichlet com-
pound multinomials associated with event-specific
and participant-specific language models:
DCM
e
=
?(
?
v
?
e
v
)
?(
?
v
N
e
v
+ ?
e
v
)
?
v
?(N
e
v
+ ?
e
v
)
?(?
e
v
)
DCM
p
=
?(
?
v
?
p
v
)
?(
?
v
N
p
v
+ ?
p
v
)
?
v
?(N
p
v
+ ?
p
v
)
?(?
p
v
)
,
where N
e
v
and N
p
v
is the number of times word
type v is assigned to event e and participant p,
respectively. The terms BBM
e
and BBM
ep
are
the Beta-Binomial distributions associated with
generating event types and generating participant
types for each event type (i.e. encoding optionality
of events and participants):
BBM
e
?
?(N
+
e
+ ?
+
)?(N
?
e
+ ?
?
)
?(N
+
e
+N
?
e
+ ?
+
+ ?
?
)
BBM
ep
?
?
e
?
p
?(N
+
ep
+ ?
+
)?(N
?
ep
+ ?
?
)
?(N
+
ep
+N
?
ep
+ ?
+
+ ?
?
)
,
where N
+
e
and N
?
e
is the number of ESDs where
event type is generated and the number of ESD
where it is not generated, respectively. N
+
ep
and
N
?
ep
are analogously defined for participant types
(for each event type e). The term GMM
e
is as-
sociated with the inversion count distribution for
event type e and has the form
GMM
e
? GMM
0
(?
e
;
?
d
v
d
e
+ v
e,0
?
0
N + ?
0
, N + ?
0
),
where GMM
0
is defined in expression (2) and v
d
e
is the inversion count for event e in ESD d. N is
the cumulative number of event occurrences in the
data set.
Finally, MN
e
and MN
p
correspond to the
probability of drawing ?
e
and ?
p
from the cor-
responding normal distributions, as discussed in
Section 4.3.1.
Though, at each step of Gibbs sampling, com-
ponents of z could potentially be sampled by
considering the full unnormalized posterior, this
clearly can be made much more efficient by ob-
serving that only a fraction of terms affect the cor-
responding conditional probability. For example,
when sampling an event type for a given event
in a ESD d, only the terms DCM
e
, BBM
ep
and
BBM
e
for all e and p are affected. For DCMs it
can be simplified further as only a few word types
are affected. Due to space constraints, we cannot
describe the entire sampling algorithms but it natu-
rally follows from the above equations and is sim-
ilar to the one described in Chen et al. (2009).
For sampling the other parameters of our model,
ranking dispersion parameters ? and the language
model parameters ? and ?, we use slice sampling
(MacKay, 2002). For each event type e we draw
its dispersion parameter ?
e
independently from the
slice sampler.
After every n
th
iteration we resample ? and
? for all language models to capture the corre-
lations. However, to improve mixing time, we
also resample components ?
k
i
and ?
l
i
when word
i has changed event membership from type k to
type l. In addition we define classes of closely
related words (heuristically based on the covari-
ance matrix) by classifying words as related when
their similarity exceeds an empirically determined
threshold. We also resample all components ?
k
j
and ?
l
j
for each word j that related to word i. We
re-normalize ?
m
and ?
n
after resampling to up-
date the Dirichlet concentration parameters. The
same procedure is used for participant language
models (parameters ?).
6 Evaluation
In our evaluation, we evaluate the quality of the
event clusters induced by the model and the ex-
tent to which the clusters capture the global event
ordering underlying the script, as well as the bene-
fit of the GMM and the informed prior knowledge.
We start by describing data and evaluation metrics.
54
Scenario Name ]ESDs Avg len
OMICS corpus
Cook in microwave 59 5.03
Answer the telephone 55 4.47
Buy from vending machine 32 4.53
Make coffee 38 5.00
R10 corpus
Iron clothes 19 8.79
Make scrambled eggs 20 10.3
Eat in fast food restaurant 15 8.93
Return food (in a restaurant) 15 5.93
Take a shower 21 11.29
Take the bus 19 8.53
Table 1: Test scenarios used in experiments (left),
the size of the corresponding corpus (middle), and
the average length of an ESD in events (right).
6.1 Data
We use the data sets presented in Regneri et al.
(2010) (henceforth R10) for development and test-
ing. The data is comprised of ESDs from two cor-
pora. R10 collected a corpus, consisting of sets of
ESDs for a variety of scenarios, via a web exper-
iment from non-expert annotators. In addition we
use ESDs from the OMICS corpus
4
(Kochender-
fer and Gupta, 2003), which consists of instantia-
tions of descriptions of several ?stories?, but is re-
stricted to indoor activities. The details of our data
are displayed in Table 1. For each event descrip-
tion we extract all noun phrases, as automatically
identified by Regneri et al. (2011), separating par-
ticipant descriptions from action descriptions. We
remove articles and pronouns, and reduce NPs to
their head words.
6.2 Gold Standard and Evaluation Metrics
We follow R10 in evaluating induced event types
and orderings in a binary classification setting.
R10 collected a gold standard by classifying pairs
of event descriptions w.r.t. whether or not they are
paraphrases. Our model classifies two event de-
scriptions as equivalent whenever z
e
1
= z
e
2
.
Equivalently, R10 classify ordered pairs of
event descriptions as to whether they are presented
in their natural order. Assuming the identity order-
ing as canonical ordering in the Generalized Mal-
lows Model, event types tending to occur earlier
in the script should be assigned lower cluster IDs
than event types occurring later. Thus, whenever
z
e
1
< z
e
2
, our the model predicts that two event
descriptions occur in their natural order.
4
http://csc.media.mit.edu/
Event Paraphrase Evt. Ordering
P R F P R F
Ret. Food 0.92 0.52 0.67 0.87 0.72 0.79
-GMM 0.70 0.30 0.42 0.46 0.44 0.45
-COVAR 0.92 0.52 0.67 0.77 0.67 0.71
Vending 0.76 0.78 0.77 0.90 0.74 0.81
-GMM 0.74 0.39 0.51 0.64 0.47 0.54
-COVAR 0.74 0.87 0.80 0.85 0.73 0.78
Shower 0.68 0.67 0.67 0.85 0.84 0.85
-GMM 0.36 0.17 0.23 0.42 0.38 0.40
-COVAR 0.64 0.44 0.52 0.77 0.73 0.75
Microwave 0.85 0.80 0.82 0.91 0.74 0.82
-GMM 0.88 0.30 0.45 0.67 0.62 0.64
-COVAR 0.89 0.81 0.85 0.92 0.82 0.87
Table 2: Comparison of model variants: For each
scenario: The full model (top), a version without
the GMM (-GMM), and a version with a uniform
Dirichlet prior over language models (-COVAR).
We evaluate the output of our model against the
described gold standard, using Precision, Recall
and F1 as evaluation metrics, so that our results are
directly comparable to R10. We tune our parame-
ters on a development set of 5 scenarios which are
not used in testing.
6.3 Results
Table 3 presents the results of our two evaluation
tasks. While on the event paraphrase task the R10
system performs slightly better, our model out-
performs the R10 system on the event ordering
task by a substantial margin of 7 points average
F-score. While both systems perform similarly on
the task of event type induction, we induce a joint
model for both objectives. The results show that,
despite the limited amount of data, and the more
complex learning objective, our model succeeds in
inducing event types and ordering constraints.
In order to demonstrate the benefit of the GMM,
we compare the performance of our model to a
variant which excludes this component (-GMM),
cf. Table 2. The results confirm our expectation
that biasing the model towards encouraging a lin-
ear ordering on the event types provides a strong
cue for event cluster inference.
As an example of a clustering learnt by our
model, consider the following event chain:
{get} ? {open,take} ? {put,place} ?
{close} ? {set,select,enter,turn} ? {start}
? {wait} ? {remove,take,open} ?
{push,press,turn}
We display the most frequent words in the clusters
55
Scenario Event Paraphrase Task Event Ordering Task
Precision Recall F1 Precision Recall F1
R10 BS R10 BS R10 BS R10 BS R10 BS R10 BS
Coffee 0.50 0.47 0.94 0.58 0.65 0.52 0.70 0.68 0.78 0.57 0.74 0.62
Telephone 0.93 0.92 0.85 0.72 0.89 0.81 0.83 0.92 0.86 0.87 0.84 0.89
Bus 0.65 0.52 0.87 0.43 0.74 0.47 0.80 0.76 0.80 0.76 0.80 0.76
Iron 0.52 0.65 0.94 0.56 0.67 0.60 0.78 0.87 0.72 0.69 0.75 0.77
Scr. Eggs 0.58 0.92 0.86 0.65 0.69 0.76 0.67 0.77 0.64 0.59 0.66 0.67
Vending 0.59 0.76 0.83 0.78 0.69 0.77 0.84 0.90 0.85 0.74 0.84 0.81
Microwave? 0.75 0.85 0.75 0.80 0.75 0.82 0.47 0.91 0.83 0.74 0.60 0.82
Shower? 0.70 0.68 0.88 0.67 0.78 0.67 0.48 0.85 0.82 0.84 0.61 0.85
Fastfood? 0.50 0.74 0.73 0.87 0.59 0.80 0.53 0.97 0.81 0.65 0.64 0.78
Ret. Food? 0.73 0.92 0.68 0.52 0.71 0.67 0.48 0.87 0.75 0.72 0.58 0.79
Average 0.645 0.743 0.833 0.658 0.716 0.689 0.658 0.850 0.786 0.717 0.706 0.776
Table 3: Results of our model for the event paraphrase task (left) and event type ordering task (right).
Our system (BS) is compared to the system in Regneri et al. (2010) (R10). We were able to obtain the
R10 system from the authors and evaluate on additional scenarios for which no results are reported in
the paper. These additional scenarios are marked with a dot (?).
inferred for the ?Microwave? scenario. Clusters
are sorted by event type ID. Note that the word
?open? is assigned to two event types in the se-
quence, which is intuitively reasonable. This illus-
trates why assuming a deterministic mapping from
predicates to events (as in Chambers and Jurafsky
(2008)) is limiting for our dataset.
We finally examined the influence of the in-
formed prior component, comparing to a model
variant which uses uniform Dirichlet parameters
(-COVAR; see Table 2). As expected, using an in-
formed prior component leads to improved perfor-
mance on scenario types with fewer training ESDs
available (?Take a shower? and ?Return food?; cf.
Table 1). For scenarios with a larger set of training
documents no reliable benefit from the informed
prior is observable. We did not optimize this com-
ponent, e.g. by testing more sophisticated meth-
ods for construction of the covariance matrix, but
expect to be able to improve its reliability.
7 Discussion
The evaluation shows that our model is able to
create meaningful event type clusters, which re-
semble the underlying event ordering imposed by
the scenario. We achieve an absolute average im-
provement of 7% over a state-of-the-art model. In
contrast to previous approaches to script induc-
tion, our model does not include specifically cus-
tomized components, and is thus flexibly applica-
ble without additional engineering effort.
Our model provides a clean, statistical formula-
tion of the problem of jointly inducing event types
and their ordering. Using a Bayesian model al-
lows for flexible enhancement of the model. One
straightforward next step would be to explore the
influence of participants, and try to jointly infer
them with our current set of latent variables.
Statistical models highly rely on a sufficient
amount of training data in order to be able to
induce latent structures. The limited amount of
training data in our case is a bottleneck for the per-
formance. The model performs best on the two
scenarios with the most training data (?Telephone?
and ?Microwave?), which supports this assump-
tion. We showed, however, that our model can be
applied to small data sets through incorporation of
informed prior knowledge without supervision.
8 Conclusion
We presented a hierarchical Bayesian model for
joint induction of event clusters and constraints on
their orderings from sets of ESDs. We incorporate
the Generalized Mallows Model over orderings.
The evaluation shows that our model successfully
induces event clusters and ordering constraints.
We compare our joint, statistical model to a
pipeline based model using MSA for event clus-
tering. Our system outperforms the system on the
task of event ordering induction by a substantial
margin, while achieving comparable results in the
event induction task. We could further explicitly
show the benefit of modeling global ESD struc-
ture, using the GMM.
In future work we plan to apply our model to
larger data sets, and to examine the role of par-
ticipants in our model, exploring the potential of
inferring them jointly with our current objectives.
56
Acknowledgments
We thank Michaela Regneri for substantial support
with the script data, and Mirella Lapata for helpful
comments.
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The berkeley framenet project.
In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on Compu-
tational Linguistics, pages 86?90.
A. Barr and E.A. Feigenbaum. 1986. The hand-
book of artificial intelligence. 1 (1981). The
Handbook of Artificial Intelligence. Addison-
Wesley.
David Blei and John Lafferty. 2006. Correlated
topic models. In Advances in Neural Informa-
tion Processing Systems 18, pages 147?154.
Nathanael Chambers and Dan Jurafsky. 2008. Un-
supervised learning of narrative event chains. In
Proceedings of ACL-08: HLT, pages 789?797.
Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and
their participants. In Proceedings of the 47th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 602?610.
H. Chen, S. R. K. Branavan, R. Barzilay, and D. R.
Karger. 2009. Content modeling using latent
permutations. Journal of Artificial Intelligence
Research, 36(1):129?163.
Christiane Fellbaum, editor. 1998. WordNet: an
electronic lexical database. MIT Press.
M. Fligner and J. Verducci. 1986. Distance based
ranking models. Journal of the Royal Statistical
Society, Series B, 48:359?369.
M. Fligner and J. Verducci. 1990. Posterior prob-
abilities for a consensus ordering. Psychome-
trika, 55:53?63.
T. L. Griffiths and M. Steyvers. 2004. Finding
scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?
5235.
Alexandre Klementiev, Dan Roth, and Kevin
Small. 2008. Unsupervised rank aggregation
with distance-based models. In Proceedings of
the 25th International Conference on Machine
Learning, pages 472?479.
Mykel J. Kochenderfer and Rakesh Gupta. 2003.
Common sense data acquisition for indoor mo-
bile robots. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence
(AAAI-04), pages 605?610.
D. J. C. MacKay. 2002. Information Theory, Infer-
ence & Learning Algorithms. Cambridge Uni-
versity Press, New York, NY, USA.
C. L. Mallows. 1957. Non-null ranking models.
Biometrika, 44:114?130.
Risto Miikkulainen. 1995. Script-based inference
and memory retrieval in subsymbolic story pro-
cessing. Applied Intelligence, pages 137?163.
Ashutosh Modi, Ivan Titov, and Alexandre Kle-
mentiev. 2012. Unsupervised induction of
frame-semantic representations. In Proceedings
of the NAACL-HLT Workshop on the Induction
of Linguistic Structure, pages 1?7.
Erik T. Mueller. 2004. Understanding script-based
stories using commonsense reasoning. Cogni-
tive Systems Research, 5(4):307?340.
Brendan O?Connor. 2012. Bayesian unsupervised
frame learning from text. Technical report,
Carnegie Mellon University.
Rajat Raina, Andrew Y. Ng, and Daphne Koller.
2006. Constructing informative priors using
transfer learning. In Proceedings of the 23rd In-
ternational Conference on Machine Learning,
pages 713?720.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with
web experiments. In Proceedings of the 48th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 979?988.
Michaela Regneri, Alexander Koller, Josef Rup-
penhofer, and Manfred Pinkal. 2011. Learning
script participants from unlabeled data. In Pro-
ceedings of RANLP 2011, pages 463?470.
Roger C. Schank and Robert P. Abelson. 1975.
Scripts, plans, and knowledge. In Proceedings
of the 4th International Joint Conference on Ar-
tificial Intelligence, IJCAI?75, pages 151?157.
Ivan Titov and Alexandre Klementiev. 2011. A
bayesian model for unsupervised semantic pars-
ing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Lin-
guistics: Human Language Technologies, pages
1445?1455.
57
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 249?258,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Incremental Bayesian Learning of Semantic Categories
Lea Frermann and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
l.frermann@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Models of category learning have been ex-
tensively studied in cognitive science and
primarily tested on perceptual abstractions
or artificial stimuli. In this paper we focus
on categories acquired from natural lan-
guage stimuli, that is words (e.g., chair is
a member of the FURNITURE category).
We present a Bayesian model which, un-
like previous work, learns both categories
and their features in a single process. Our
model employs particle filters, a sequential
Monte Carlo method commonly used for
approximate probabilistic inference in an
incremental setting. Comparison against
a state-of-the-art graph-based approach re-
veals that our model learns qualitatively
better categories and demonstrates cogni-
tive plausibility during learning.
1 Introduction
Considerable psychological research has shown
that people reason about novel objects they en-
counter by identifying the category to which these
objects belong and extrapolating from their past
experiences with other members of that category
(Smith and Medin, 1981). Categorization is a clas-
sic problem in cognitive science, underlying a va-
riety of common mental tasks including percep-
tion, learning, and the use of language.
Given its fundamental nature, categorization
has been extensively studied both experimentally
and in simulations. Indeed, numerous models ex-
ist as to how humans categorize objects ranging
from strict prototypes (categories are represented
by a single idealized member which embodies
their core properties; e.g., Reed 1972) to full ex-
emplar models (categories are represented by a list
of previously encountered members; e.g., Nosof-
sky 1988) and combinations of the two (e.g., Grif-
fiths et al. 2007). A common feature across dif-
ferent studies is the use of stimuli involving real-
world objects (e.g., children?s toys; Starkey 1981),
perceptual abstractions (e.g., photographs of ani-
mals; Quinn and Eimas 1996), or artificial ones
(e.g., binary strings, dot patterns or geometric
shapes; Medin and Schaffer 1978; Posner and
Keele 1968; Bomba and Siqueland 1983). Most
existing models focus on adult categorization, in
which it is assumed that a large number of cate-
gories have already been learnt (but see Anderson
1991 and Griffiths et al. 2007 for exceptions).
In this work we focus on categories acquired
from natural language stimuli (i.e., words) and
investigate how the statistics of the linguistic en-
vironment (as approximated by large corpora) in-
fluence category formation (e.g., chair and ta-
ble are FURNITURE whereas peach and apple are
FRUIT
1
). The idea of modeling categories using
words as a stand-in for their referents has been
previously used to explore categorization-related
phenomena such as semantic priming (Cree et al.,
1999) and typicality rating (Voorspoels et al.,
2008), to evaluate prototype and exemplar mod-
els (Storms et al., 2000), and to simulate early lan-
guage category acquisition (Fountain and Lapata,
2011). The idea of using naturalistic corpora has
received little attention. Most existing studies use
feature norms as a proxy for people?s representa-
tion of semantic concepts. In a typical procedure,
participants are presented with a word and asked to
generate the most relevant features or attributes for
its referent concept. The most notable collection
of feature norms is probably the multi-year project
of McRae et al. (2005), which obtained features
for a set of 541 common English nouns.
Our approach replaces feature norms with rep-
resentations derived from words? contexts in cor-
pora. While this is an impoverished view of how
categories are acquired ? it is clear that they are
learnt through exposure to the linguistic environ-
ment and the physical world ? perceptual infor-
1
Throughout this paper we will use small caps to denote
CATEGORIES and italics for their members.
249
mation relevant for extracting semantic categories
is to a large extent redundantly encoded in linguis-
tic experience (Riordan and Jones, 2011). Besides,
there are known difficulties with feature norms
such as the small number of words for which these
can be obtained, the quality of the attributes, and
variability in the way people generate them (see
Zeigenfuse and Lee 2010 for details). Focusing
on natural language categories allows us to build
categorization models with theoretically unlimited
scope.
To this end, we present a probabilistic Bayesian
model of category acquisition based on the key
idea that learners can adaptively form category
representations that capture the structure ex-
pressed in the observed data. We model category
induction as two interrelated sub-problems: (a) the
acquisition of features that discriminate among
categories, and (b) the grouping of concepts into
categories based on those features. An important
modeling question concerns the exact mechanism
with which categories are learned. To maintain
cognitive plausibility, we develop an incremental
learning algorithm. Incrementality is a central as-
pect of human learning which takes place sequen-
tially and over time. Humans are capable of deal-
ing with a situation even if only partial information
is available. They adaptively learn as new infor-
mation is presented and locally update their inter-
nal knowledge state without systematically revis-
ing everything known about the situation at hand.
Memory and processing limitations also explain
why humans must learn incrementally. It is not
possible to store and have easy access to all the in-
formation one has been exposed to. It seems likely
that people store the most prominent facts and gen-
eralizations, which they modify on they fly when
new facts become available.
Our model learns categories using a particle fil-
ter, a Markov Chain Monte Carlo (MCMC) in-
ference mechanism which sequentially integrates
newly observed data and can be thus viewed as a
plausible proxy for human learning. Experimental
results show that the incremental learner obtains
meaningful categories which outperform the state
of the art whilst at the same time acquiring seman-
tic representations of words and their features.
2 Related Work
The problem of category induction has achieved
much attention in the cognitive science literature.
Incremental category learning was pioneered by
Anderson (1991) who develops a non-parametric
model able to induce categories from abstract
stimuli represented by binary features. Sanborn
et al. (2006) present a fully Bayesian adaptation of
Anderson?s original model, which yields a better
fit with behavioral data. A separate line of work
examines the cognitive characteristics of category
acquisition as well as the processes of generalizing
and generating new categories and exemplars (Jern
and Kemp, 2013; Kemp et al., 2012). The above
models are conceptually similar to ours. How-
ever, they were developed with adult categoriza-
tion in mind, and use rather simplistic categories
representing toy-domains. It is therefore not clear
whether they generalize to arbitrary stimuli and
data sizes. We aim to show that it is possible to ac-
quire natural language categories on a larger scale
purely from linguistic context.
Our model is loosely related to Bayesian mod-
els of word sense induction (Brody and Lapata,
2009; Yao and Durme, 2011). We also assume
that local linguistic context can provide important
cues for word meaning and by extension category
membership. However, the above models focus
on performance optimization and learn in an ideal
batch mode, while incorporating various kinds of
additional features such as part of speech tags or
dependencies. In contrast, we develop a cogni-
tively plausible (early) language learning model
and show that categories can be acquired purely
from context, as well as in an incremental fashion.
From a modeling perspective, we learn cate-
gories incrementally using a particle filtering al-
gorithm (Doucet et al., 2001). Particle filters are
a family of sequential Monte Carlo algorithms
which update the state space of a probabilistic
model with newly encountered information. They
have been successfully applied to natural lan-
guage acquisition tasks such as word segmentation
(Borschinger and Johnson, 2011), or sentence pro-
cessing (Levy et al., 2009). Sanborn et al. (2006)
also use particle filters for small-scale categoriza-
tion experiments with artificial stimuli. To the best
of our knowledge, we present the first particle fil-
tering algorithm for large-scale category acquisi-
tion from natural text.
Our work is closest to Fountain and Lapata
(2011) who also develop a model for inducing nat-
ural language categories. Specifically, they pro-
pose an incremental version of Chinese Whispers
(Biemann, 2006), a randomized graph-clustering
algorithm. The latter takes as input a graph which
is constructed from corpus-based co-occurrence
statistics and produces a hard clustering over the
nodes in the graph. Contrary to our model, they
treat the tasks of inferring a semantic representa-
250
wt
w
c
z
?
?
?
?
?
?
Mult Mult
Mult
Dir Dir
Dir
n
d
k
k
Figure 1: Plate diagram representation of the
BayesCat model.
tion for concepts and their class membership as
two separate processes. This allows to experi-
ment with different ways of initializing the co-
occurrence matrix (e.g., from bags of words or
a dependency parsed corpus), however at the ex-
pense of cognitive plausibility. It is unlikely that
humans have two entirely separate mechanisms
for learning the meaning of words and their cat-
egories. We formulate a more expressive model
within a probabilistic framework which captures
the meaning of words, their similarity, and the pre-
dictive power of their linguistic contexts.
3 The BayesCat Model
In this section we present our Bayesian model of
category induction (BayesCat for short). The input
to the model is natural language text, and its final
output is a set of clusters representing categories
of semantic concepts found in the input data. Like
many other semantic models, BayesCat is inspired
by the distributional hypothesis which states that
a word?s meaning is predictable from its context
(Harris, 1954). By extension, we also assume that
contextual information can be used to character-
ize general semantic categories. Accordingly, the
input to our model is a corpus of documents, each
defined as a target word t centered in a fixed-length
context window:
[c
?n
... c
?1
t c
1
... c
n
] (1)
We assume that there exists one global distribu-
tion over categories from which all documents are
generated. Each document is assigned a category
label, based on two types of features: the docu-
ment?s target word and its context words, which
are modeled through separate category-specific
distributions. We argue that it is important to dis-
tinguish between these features, since words be-
longing to the same category do not necessarily
co-occur, but tend to occur in the same contexts.
For example, the words polar bear and anteater
Draw distribution over categories ?? Dir(?)
for category k do
Draw target word distribution ?
k
?Dir(?)
Draw context word distribution ?
k
?
Dir(?)
for Document d do
Draw category z
d
?Mult(?)
Draw target word w
d
t
?Mult(?
z
d
)
for context position n = {1..N} do
Draw context word w
d,n
c
?Mult(?
z
d
)
Figure 2: The generative process of the BayesCat
model.
are both members of the category ANIMAL. How-
ever, they rarely co-occur (in fact, a cursory search
using Google yields only three matches for the
query ?polar bear * anteater?). Nevertheless, we
would expect to observe both words in similar
contexts since both animals eat, sleep, hunt, have
fur, four legs, and so on. This distinction con-
trasts our category acquisition task from the clas-
sical task of topic inference.
Figure 1 presents a plate diagram of the
BayesCat model; an overview of the generative
process is given in Figure 2. We first draw a global
category distribution ? from the Dirichlet distribu-
tion with parameter ?. Next, for each category k,
we draw a distribution over target words ?
k
from a
Dirichlet with parameter ? and a distribution over
context words ?
k
from a Dirichlet with parame-
ter ?. For each document d, we draw a category z
d
,
then a target word, and N context words from the
category-specific distributions ?
z
d
and ?
z
d
, respec-
tively.
4 Learning
Our goal is to infer the joint distribution of
all hidden model parameters, and observable
data W . Since we use conjugate prior distributions
throughout the model, this joint distribution can be
simplified to:
P(W,Z,?,?,?;?,?,?) ?
?
k
?(N
k
+?
k
)
?(
?
k
N
k
+?
k
)
?
K
?
k=1
?
r
?(N
k
r
+?
r
)
?(
?
r
N
k
r
+?
r
)
?
K
?
k=1
?
s
?(N
k
s
+ ?
s
)
?(
?
s
N
k
s
+ ?
s
)
, (2)
where r and s iterate over the target and context
word vocabulary, respectively, and the distribu-
251
tions ?,?, and ? are integrated out and implic-
itly captured by the corresponding co-occurrence
counts N
?
?
. ?() denotes the Gamma function, a
generalization of the factorial to real numbers.
Since exact inference of the parameters of the
BayesCat model is intractable, we use sampling-
based approximate inference. Specifically, we
present two learning algorithms, namely a Gibbs
sampler and a particle filter.
The Gibbs Sampler Gibbs sampling is a well-
established approximate learning algorithm, based
on Markov Chain Monte Carlo methods (Geman
and Geman, 1984). It operates in batch-mode by
repeatedly iterating through all data points (doc-
uments in our case) and assigning the currently
sampled document d a category z
d
conditioned on
the current labelings of all other documents z
?d
:
z
d
? P(z
d
|z
?d
,W
?d
;?,?,?), (3)
using equation (2) but ignoring information
from the currently sampled document in all co-
occurrence counts.
The Gibbs sampler can be seen as an ideal
learner, which can view and revise any relevant
information at any time during learning. From a
cognitive perspective, this setting is implausible,
since a human language learner encounters train-
ing data incrementally and does not systematically
revisit previous learning decisions. Particle filters
are a class of incremental, or sequential, Monte
Carlo methods which can be used to model aspects
of the language learning process more naturally.
The Particle Filter Intuitively, a particle fil-
ter (henceforth PF) entertains a fixed set of
N weighted hypotheses (particles) based on pre-
vious training examples. Figure 3 shows an
overview of the particle filtering learning proce-
dure. At first, every particle of the PF is initialized
from a base distribution P
0
(Initialization). Then a
single iteration over the input data y is performed,
during which the posterior distribution of each
data point y
t
under all current particles is com-
puted given information from all previously en-
countered data points y
t?1
(Sampling/Prediction).
Crucially, each update is conditioned only on the
previous model state z
t?1
, which results in a con-
stant state space despite an increasing amount of
available data. A common problem with PF al-
gorithms is weight degeneration, i.e., one particle
tends to accumulate most of the weight. To avoid
this problem, at regular intervals the set of parti-
cles is resampled in order to discard particles with
for particle p do . Initialization
Initialize randomly or from z
0
p
? p
0
(z)
for observation t do
for particle n do . Sampling/Prediction
P
n
(z
t
n
|y
t
)? p(z
t
n
|z
t?1
n
,?)P(y
t
|z
t
n
,y
t?1
)
z
t
?Mult({P
n
(z
t
n
)}
N
i=1
) . Resampling
Figure 3: The particle filtering procedure.
low probability and to ensure that the sample is
representative of the state space at any time (Re-
sampling).
This general algorithm can be straightforwardly
adapted to our learning problem (Griffiths et al.,
2011; Fearnhead, 2004). Each observation corre-
sponds to a document, which needs to be assigned
a category. To begin with, we assign the first ob-
served document to category 0 in all particles (Ini-
tialization). Then, we iterate once over the remain-
ing documents. For each particle n, we compute
a probability distribution over K categories based
on the simplified posterior distribution as defined
in equation (2) (Sampling/Prediction), with co-
occurrence counts based on the information from
all previously encountered documents. Thus, we
obtain a distribution over N ?K possible assign-
ments. From this distribution we sample with
replacement N new particles, assign the current
document to the corresponding category (Resam-
pling), and proceed to the next input document.
5 Experimental Setup
The goal of our experimental evaluation is to as-
sess the quality of the inferred clusters by compar-
ison to a gold standard and an existing graph-based
model of category acquisition. In addition, we are
interested in the incremental version of the model,
whether it is able to learn meaningful categories
and how these change over time. In the following,
we give details on the corpora we used, describe
how model parameters were selected, and explain
our evaluation procedure.
5.1 Data
All our experiments were conducted on a lem-
matized version of the British National Corpus
(BNC). The corpus was further preprocessed by
removing stopwords and infrequent words (occur-
ring less than 800 times in the BNC).
The model output was evaluated against a gold
standard set of categories which was created by
collating the resources developed by Fountain and
252
Lapata (2010) and Vinson and Vigliocco (2008).
Both datasets contain a classification of nouns into
(possibly multiple) semantic categories produced
by human participants. We therefore assume that
they represent psychologically salient categories
which the cognitive system is in principle capable
of acquiring. After merging the two resources, and
removing duplicates we obtained 42 semantic cat-
egories for 555 nouns. We split this gold standard
into a development (41 categories, 492 nouns) and
a test set (16 categories, 196 nouns).
2
The input to our model consists of short chunks
of text, namely a target word centered in a sym-
metric context window of five words (see (1)).
In our experiments, the set of target words corre-
sponds to the set of nouns in the evaluation dataset.
Target word mentions and their context are ex-
tracted from the BNC.
5.2 Parameters for the BayesCat Model
We optimized the hyperparameters of the
BayesCat model on the development set.
For the particle filter, the optimal values are
? = 0.7,? = 0.1,? = 0.1. We used the same
values for the Gibbs Sampler since it proved
insensitive to hyperparameter variations. We run
the Gibbs sampler for 200 iterations
3
and report
results averaged over 10 runs. For the PF, we set
the number of particles to 500, and report final
scores averaged over 10 runs. For evaluation,
we take the clustering from the particle with the
highest weight
4
.
5.3 Model Comparison
Chinese Whispers We compared our approach
with Fountain and Lapata (2011) who present a
non-parametric graph-based model for category
acquisition. Their algorithm incrementally con-
structs a graph from co-occurrence counts of tar-
get words and their contexts (they use a symmetric
context window of five words). Target words con-
stitute the nodes of the graph, their co-occurrences
are transformed into a vector of positive PMI val-
ues, and graph edges correspond to the cosine sim-
ilarity between the PMI-vectors representing any
two nodes. They use Chinese Whispers (Biemann,
2006) to partition a graph into categories.
2
The dataset is available from www.frermann.de/data.
3
We checked for convergence on the development set.
4
While in theory particles should be averaged, we found
that eventually they became highly similar ? a common
problem known as sample impoverishment, which we plan to
tackle in the future. Nevertheless, diversity among particles
is present in the initial learning phase, when uncertainty is
greatest, so the model still benefits from multiple hypotheses.
We replicated the bag-of-words model pre-
sented in Fountain and Lapata (2011) and assessed
its performance on our training corpora and test
sets. The scores we report are averaged over 10
runs.
Chinese Whispers can only make hard cluster-
ing decisions, whereas the BayesCat model re-
turns a soft clustering of target nouns. In order
to be able to compare the two models, we con-
vert the soft clusters to hard clusters by assign-
ing each target word w to category c such that
cat(w) = max
c
P(w|c) ?P(c|w).
LDA We also compared our model to a standard
topic model, namely Latent Dirichlet Allocation
(LDA; Blei et al. 2003). LDA assumes that a docu-
ment is generated from an individual mixture over
topics, and each topic is associated with one word
distribution. We trained a batch version of LDA
using input identical to our model and the Mallet
toolkit (McCallum, 2002).
Chinese Whispers is a parameter-free algorithm
and thus determines the number of clusters auto-
matically. While the Bayesian models presented
here are parametric in that an upper bound for the
potential number of categories needs to be speci-
fied, the models themselves decide on the specific
value of this number. We set the upper bound of
categories to 100 for LDA as well as the batch and
incremental version of the BayesCat model.
5.4 Evaluation Metrics
Our aim is to learn a set of clusters each of which
corresponds to one gold category, i.e., it contains
all and only members of that gold category. We
report evaluation scores based on three metrics
which measure this tradeoff. Since in unsuper-
vised clustering the cluster IDs are meaningless,
all evaluation metrics involve a mapping from in-
duced clusters to gold categories. The first two
metrics described below perform a cluster-based
mapping and are thus not ideal for assessing the
output of soft clustering algorithms. The third
metric performs an item-based mapping and can
be directly used to evaluate soft clusters.
Purity/Collocation are based on member over-
lap between induced clusters and gold classes
(Lang and Lapata, 2011). Purity measures the de-
gree to which each cluster contains instances that
share the same gold class, while collocation mea-
sures the degree to which instances with the same
gold class are assigned to a single cluster. We re-
port the harmonic mean of purity and collocation
253
as a single measure of clustering quality.
V-Measure is the harmonic mean between
homogeneity and collocation (Rosenberg and
Hirschberg, 2007). Like purity, V-Measure
performs cluster-based comparisons but is an
entropy-based method. It measures the condi-
tional entropy of a cluster given a class, and vice
versa.
Cluster-F1 is an item-based evaluation metric
which we propose drawing inspiration from the
supervised metric presented in Agirre and Soroa
(2007). Cluster-F1 maps each target word type to
a gold cluster based on its soft class membership,
and is thus appropriate for evaluation of soft clus-
tering output. We first create a K?G soft map-
ping matrix M from each induced category k
i
to
gold classes g
j
from P(g
j
|k
i
). We then map each
target word type to a gold class by multiplying
its probability distribution over soft clusters with
the mapping matrix M , and taking the maximum
value. Finally, we compute standard precision, re-
call and F1 between the mapped system categories
and the gold classes.
6 Results
Our experiments are designed to answer three
questions: (1) How do the induced categories
fare against gold standard categories? (2) Are
there performance differences between BayesCat
and Chinese Whispers, given that the two models
adopt distinct mechanisms for representing lexical
meaning and learning semantic categories? (3) Is
our incremental learning mechanism cognitively
plausible? In other words, does the quality of the
induced clusters improve over time and how do the
learnt categories differ from the output of an ideal
batch learner?
Clustering performance for the batch BayesCat
model (BC-Batch), its incremental version
(BC-Inc), Chinese Whispers (CW), and LDA
is shown in Table 1. Comparison of the two
incremental models, namely BC-Inc and CW,
shows that our model outperforms CW under
all evaluation metrics both on the test and the
development set. Our BC models perform at
least as well as LDA, despite the more complex
learning objective. Recall that LDA does not learn
category specific features. BC-Batch performs
best overall, however this is not surprising. The
BayesCat model learnt in batch mode uses a Gibbs
sampler which can be viewed as an ideal learner
with access to the entire training data at any time,
and the ability to systematically revise previous
decisions. This puts the incremental variant at a
disadvantage since the particle filter encounters
the data incrementally and never resamples
previously seen documents. Nevertheless, as
shown in Table 1 BC-Inc?s performance is very
close to BC-Batch. BC-Inc outperforms the Gibbs
sampler in the PC-F1 metric, because it achieves
higher collocation scores. Inspection of the output
reveals that the Gibbs sampler induces larger clus-
ters compared to the particle filter (as well as less
distinct clusters). Although the general pattern of
results is the same on the development and test
sets, absolute scores for all systems are higher on
the test set. This is expected, since the test set
contains less categories with a smaller number of
exemplars and more accurate clusterings can be
thus achieved (on average) more easily.
Figure 4 displays the learning curves produced
by CW and BC-Inc under the PC-F1 (left) and
Cluster-F1 (right) evaluation metrics. Under
PC-F1, CW produces a very steep initial learning
curve which quickly flattens off, whereas no learn-
ing curve emerges for CW under Cluster-F1. The
BayesCat model exhibits more discernible learn-
ing curves under both metrics. We also observe
that learning curves for CW indicate much more
variance during learning compared to BC-Inc, ir-
respectively of the evaluation metric being used.
Figure 4b shows learning curves for BC-Inc when
its output classes are interpreted in two ways,
i.e., as soft or hard clusters. Interestingly, the two
curves have a similar shape which points to the
usefulness of Cluster-F1 as an evaluation metric
for both types of clusters.
In order to better understand the differences in
the learning process between CW and BC-Inc we
tracked the evolution of clusterings over time, as
well as the variance across cluster sizes at each
point in time. The results are plotted in Figure 5.
The top part of the figure compares the number
of clusters learnt by the two models. We see that
the number of clusters inferred by CW drops over
time, but is closer to the number of clusters present
in the gold standard. The final number of clus-
ters inferred by CW is 26, whereas PF-Inc infers
90 clusters (there are 41 gold classes). The mid-
dle plot shows the variance in cluster size induced
at any time by CW which is by orders of magni-
tude higher than the variance observed in the out-
put of BayesCat (bottom plot). More importantly,
the variance in BayesCat resembles the variance
present in the gold standard much more closely.
The clusterings learnt by CW tend to consist of
254
Development Set Test Set
Metric LDA CW BC-Inc BC-Batch LDA CW BC-Inc BC-Batch
PC-F1 (Hard) 0.283 0.211 0.283 0.261 0.446 0.380 0.503 0.413
V-Measure (Hard) 0.399 0.143 0.383 0.428 0.572 0.220 0.567 0.606
Cluster-F1 (Hard) 0.416 0.301 0.386 0.447 0.521 0.443 0.671 0.693
Cluster-F1 (Soft) 0.387 ? 0.484 0.523 0.665 ? 0.644 0.689
Table 1: Evaluation of model output against a gold standard. Results are reported for the BayesCat model
trained incrementally (BC-Inc) and in batch mode (BC-Batch), and Chinese Whispers (CW). The type
of clusters being evaluated is shown within parentheses.
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
PC
-F1
Number of encountered Documents
CW (Hard)
BC-Inc (Hard)
(a)
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
Cl
ust
er-
F1
Number of encountered Documents
CW (Hard)
BC-Inc (Hard)
BC-Inc (Soft)
(b)
Figure 4: Learning curves for BC-Inc and CW based on PC-F1 (left), and Cluster-F1 (right). The type
of clusters being evaluated is shown within parentheses. Results are reported on the development set.
few very large clusters and a large number of very
small (mostly singleton) clusters. Although some
of the bigger clusters are meaningful, the overall
structure of clusterings does not faithfully repre-
sent the gold standard.
Finally, note that in contrast to CW and LDA,
the BayesCat model learns not only how to in-
duce clusters of target words, but also informa-
tion about their category-specific contexts. Table 2
presents examples of the learnt categories together
with their most likely contexts. For example, one
of the categories our model discovers corresponds
to BUILDINGS. Some of the context words or fea-
tures relating to buildings refer to their location
(e.g., city, road, hill, north, park), architectural
style (e.g., modern, period, estate), and material
(e.g., stone).
7 Discussion
In this paper we have presented a Bayesian model
of category acquisition. Our model learns to group
concepts into categories as well as their features
(i.e., context words associated with them). Cat-
egory learning is performed incrementally, using
a particle filtering algorithm which is a natural
choice for modeling sequential aspects of lan-
guage learning.
We now return to our initial questions and sum-
marize our findings. Firstly, we observe that
our incremental model learns plausible linguistic
categories when compared against the gold stan-
dard. Secondly, these categories are qualitatively
better when evaluated against Chinese Whispers,
a closely related graph-based incremental algo-
rithm. Thirdly, analysis of the model?s output
shows that it simulates category learning in two
important ways, it consistently improves over time
and can additionally acquire category features.
Overall, our model has a more cognitively plau-
sible learning mechanism compared to CW, and
is more expressive, as it can simulate both cat-
egory and feature learning. Although CW ulti-
mately yields some meaningful categories, it does
not acquire any knowledge pertaining to their fea-
tures. This is somewhat unrealistic given that hu-
mans are good at inferring missing features for
255
 0
 20
 40
 60
 80
 100
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
Nu
mb
er o
f C
lus
ters
Number of encountered Documents
BC-IncCWGold
 0
 100
 200
 300
 400
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
Va
rian
ce
Number of encountered Documents
CWGold
 3
 6
 9
 12
 15
 18
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
Va
rian
ce
Number of encountered Documents
BC-IncGold
Figure 5: Number of clusters over time (top).
Cluster size variance for CW (middle) and BC-Inc
(bottom). Results shown on the development set.
unknown categories (Anderson, 1991). It is also
symptomatic of the nature of the algorithm which
does not have an explicit learning mechanism.
Each node in the graph iteratively adopts (in ran-
dom order) the strongest class in its neighborhood
(i.e., the set of nodes with which it shares an edge).
We also showed that LDA is less appropriate for
the category learning task on account of its for-
mulation which does not allow to simultaneously
acquire clusters and their features.
There are several options for improving our
model. The learning mechanism presented here
is the most basic of particle methods. A common
problem in particle filtering is sample impoverish-
ment, i.e., particles become highly similar after a
few iterations, and do not optimally represent the
sample space. More involved resampling methods
such as stratified sampling or residual resampling,
have been shown to alleviate this problem (Douc,
2005).
From a cognitive perspective, the most obvious
weakness of our algorithm is its strict incremen-
tality. While our model simulates human mem-
BUILDINGS
wall, bridge, building, cottage, gate, house, train,
bus, stone, chapel, brick, cathedral
plan, include, park, city, stone, building, ho-
tel, lead, road, hill, north, modern, visit, main,
period, cathedral, estate, complete, site, owner,
parish
WEAPONS
shotgun, pistol, knife, crowbar, gun, sledgeham-
mer, baton, bullet, motorcycle, van, ambulance
injure, ira, jail, yesterday, arrest, stolen, fire, of-
ficer, gun, police victim, hospital, steal, crash,
murder, incident, driver, accident, hit
INSTRUMENTS
tuba, drum, harmonica, bagpipe, harp, violin,
saxophone, rock, piano, banjo, guitar, flute, harp-
sichord, trumpet, rocker, clarinet, stereo, cello,
accordion
amp, orchestra, sound, electric, string, sing,
song, drum, piano, condition, album, instrument,
guitar, band, bass, music
Table 2: Examples of categories induced by the in-
cremental BayesCat model (upper row), together
with their most likely context words (lower row).
ory restrictions and uncertainty by learning based
on a limited number of current knowledge states
(i.e., particles), it never reconsiders past catego-
rization decisions. In many linguistic tasks, how-
ever, learners revisit past decisions (Frazier and
Rayner, 1982) and intuitively we would expect
categories to change based on novel evidence, es-
pecially in the early learning phase. In fixed-lag
smoothing, a particle smoothing variant, model
updates include systematic revision of a fixed set
of previous observations in the light of newly en-
countered evidence (Briers et al., 2010). Based
on this framework, we will investigate different
schemes for informed sequential learning.
Finally, we would like to compare the model?s
predictions against behavioral data, and exam-
ine more thoroughly how categories and features
evolve over time.
Acknowledgments We would like to thank
Charles Sutton and members of the ILCC at the
School of Informatics for their valuable feedback.
We acknowledge the support of EPSRC through
project grant EP/I037415/1.
256
References
Agirre, Eneko and Aitor Soroa. 2007. Semeval-
2007 task 02: Evaluating word sense induc-
tion and discrimination systems. In Proceedings
of the 4th International Workshop on Semantic
Evaluations. Prague, Czech Republic, pages 7?
12.
Anderson, John R. 1991. The adaptive nature of
human categorization. Psychological Review
98:409?429.
Biemann, Chris. 2006. Chinese Whispers - an effi-
cient graph clustering algorithm and its applica-
tion to natural language processing problems. In
Proceedings of TextGraphs: the 1st Workshop
on Graph Based Methods for Natural Language
Processing. New York City, pages 73?80.
Blei, David M., Andrew Y. Ng, and Michael I. Jor-
dan. 2003. Latent Dirichlet allocation. Journal
of Machine Learning Research 3:993?1022.
Bomba, Paul C. and Eimas R. Siqueland. 1983.
The nature and structure of infant form cate-
gories. Journal of Experimental Child Psychol-
ogy 35:294?328.
Borschinger, Benjamin and Mark Johnson. 2011.
A particle filter algorithm for Bayesian word
segmentation. In Proceedings of the Aus-
tralasian Language Technology Association
Workshop. Canberra, Australia, pages 10?18.
Briers, Mark, Arnaud Doucet, and Simon Maskell.
2010. Smoothing algorithms for state-space
models. Annals of the Institute of Statistical
Mathematics 62(1):61?89.
Brody, Samuel and Mirella Lapata. 2009.
Bayesian word sense induction. In Proceedings
of the 12th Conference of the European Chapter
of the ACL. Athens, Greece, pages 103?111.
Cree, George S., Ken McRae, and Chris McNor-
gan. 1999. An attractor model of lexical con-
ceptual processing: Simulating semantic prim-
ing. Cognitive Science 23(3):371?414.
Douc, Randal. 2005. Comparison of resampling
schemes for particle filtering. In 4th Interna-
tional Symposium on Image and Signal Pro-
cessing and Analysis. Zagreb, Croatia, pages
64?69.
Doucet, Arnaud, Nando de Freitas, and Neil Gor-
don. 2001. Sequential Monte Carlo Methods in
Practice. Springer, New York.
Fearnhead, Paul. 2004. Particle filters for mix-
ture models with an unknown number of com-
ponents. Statistics and Computing 14(1):11?21.
Fountain, Trevor and Mirella Lapata. 2010. Mean-
ing representation in natural language catego-
rization. In Proceedings of the 32nd Annual
Conference of the Cognitive Science Society.
Portland, Oregon, pages 1916?1921.
Fountain, Trevor and Mirella Lapata. 2011. In-
cremental models of natural language category
acquisition. In Proceedings of the 33nd An-
nual Conference of the Cognitive Science Soci-
ety. Boston, Massachusetts, pages 255?260.
Frazier, Lyn and Keith Rayner. 1982. Making and
correcting errors during sentence comprehen-
sion: Eye movements in the analysis of struc-
turally ambiguous sentences. Cognitive Psy-
chology 14(2):178?210.
Geman, Stuart and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions and the
Bayesian restoration of images. IEEE Trans-
actions on Pattern Analysis and Machine Intel-
ligence 6(6):721?741.
Griffiths, Thomas L., Kevin R. Canini, Adam N.
Sanborn, and Daniel J. Navarro. 2007. Unifying
rational models of categorization via the hierar-
chical Dirichlet process. In Proceedings of the
29th Annual Conference of the Cognitive Sci-
ence Society. Nashville, Tennessee, pages 323?
328.
Griffiths, Thomas L., Adam N. Sanborn, Kevin R.
Canini, John D. Navarro, and Joshua B. Tenen-
baum. 2011. Nonparametric Bayesian mod-
els of categorization. In Emmanuel M. Pothos
and Andy J. Wills, editors, Formal Approaches
in Categorization, Cambridge University Press,
pages 173?198.
Harris, Zellig. 1954. Distributional structure.
Word 10(23):146?162.
Jern, Alan and Charles Kemp. 2013. A proba-
bilistic account of exemplar and category gen-
eration. Cognitive Psychology 66:85?125.
Kemp, Charles, Patrick Shafto, and Joshua B.
Tenenbaum. 2012. An integrated account of
generalization across objects and features. Cog-
nitive Psychology 64:35?75.
Lang, Joel and Mirella Lapata. 2011. Unsuper-
vised semantic role induction with graph par-
titioning. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Edinburgh, Scotland, UK.,
pages 1320?1331.
Levy, Roger P., Florencia Reali, and Thomas L.
Griffiths. 2009. Modeling the effects of mem-
257
ory on human online sentence processing with
particle filters. In D. Koller, D. Schuurmans,
Y. Bengio, and L. Bottou, editors, Advances
in Neural Information Processing Systems 21,
pages 937?944.
McCallum, Andrew Kachites. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
McRae, Ken, George S. Cree, Mark S. Seidenberg,
and Chris McNorgan. 2005. Semantic feature
production norms for a large set of living and
nonliving things. Behavioral Research Methods
37(4):547?59.
Medin, Douglas L. and Marguerite M. Schaffer.
1978. Context theory of classification learning.
Psychological Review 85(3):207?238.
Nosofsky, Robert M. 1988. Exemplar-based
accounts of relations between classification,
recognition, and typicality. Journal of Exper-
imental Psychology: Learning, Memory, and
Cognition 14:700?708.
Posner, Michael I. and Steven W. Keele. 1968. On
the genesis of abstract ideas. Journal of Exper-
imental Psychology 21:367?379.
Quinn, Paul C. and Peter D. Eimas. 1996. Percep-
tual cues that permit categorical differentiation
of animal species by infants. Journal of Exper-
imental Child Psychology 63:189?211.
Reed, Stephen K. 1972. Pattern recognition and
categorization. Cognitive psychology 3(3):382?
407.
Riordan, Brian and Michael N. Jones. 2011. Re-
dundancy in perceptual and linguistic experi-
ence: Comparing feature-based and distribu-
tional models of semantic representation. Top-
ics in Cognitive Science 3(2):303?345.
Rosenberg, Andrew and Julia Hirschberg. 2007.
V-measure: A conditional entropy-based ex-
ternal cluster evaluation measure. In Proceed-
ings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing
and Computational Natural Language Learn-
ing. Prague, Czech Republic, pages 410?420.
Sanborn, Adam N., Thomas L. Griffiths, and
Daniel J. Navarro. 2006. A more rational model
of categorization. In Proceedings of the 28th
Annual Conference of the Cognitive Science So-
ciety. Vancouver, Canada, pages 726?731.
Smith, Edward E. and Douglas L. Medin. 1981.
Categories and Concepts. Harvard University
Press, Cambridge, MA, USA.
Starkey, David. 1981. The origins of concept for-
mation: Object sorting and object preference in
early infancy. Child Development pages 489?
497.
Storms, Gert, Paul De Boeck, and Wim Ruts.
2000. Prototype and exemplar-based informa-
tion in natural language categories. Journal of
Memory and Language 42:51?73.
Vinson, David and Gabriella Vigliocco. 2008. Se-
mantic feature production norms for a large set
of objects and events. Behavior Research Meth-
ods 40(1):183?190.
Voorspoels, Wouter, Wolf Vanpaemel, and Gert
Storms. 2008. Exemplars and prototypes in
natural language concepts: A typicality-based
evaluation. Psychonomic Bulletin & Review
15(3):630?637.
Yao, Xuchen and Benjamin Van Durme. 2011.
Nonparametric Bayesian word sense induc-
tion. In Proceedings of TextGraphs-6: Graph-
based Methods for Natural Language Process-
ing. Portland, Oregon, pages 10?14.
Zeigenfuse, Matthew D. and Michael D. Lee.
2010. Finding the features that represent stim-
uli. Acta Psychologica 133(3):283?295.
258
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 125?129,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Cross-lingual Parse Disambiguation based on Semantic Correspondence
Lea Frermann
Department of Computational Linguistics
Saarland University
frermann@coli.uni-saarland.de
Francis Bond
Linguistics and Multilingual Studies
Nanyang Technological University
bond@ieee.org
Abstract
We present a system for cross-lingual parse
disambiguation, exploiting the assumption
that the meaning of a sentence remains un-
changed during translation and the fact that
different languages have different ambiguities.
We simultaneously reduce ambiguity in multi-
ple languages in a fully automatic way. Eval-
uation shows that the system reliably discards
dispreferred parses from the raw parser output,
which results in a pre-selection that can speed
up manual treebanking.
1 Introduction
Treebanks, sets of parsed sentences annotated with a
sytactic structure, are an important resource in NLP.
The manual construction of treebanks, where a hu-
man annotator selects a gold parse from all parses
returned by a parser, is a tedious and error prone pro-
cess. We present a system for simultaneous and ac-
curate partial parse disambiguation of multiple lan-
guages. Using the pre-selected set of parses returned
by the system, the treebanking process for multiple
languages can be sped up.
The system operates on an aligned parallel cor-
pus. The languages of the parallel corpus are con-
sidered as mutual semantic tags: As the meaning of
a sentence stays constant during translation, we are
able to resolve ambiguities which exist in only one
of the langauges by only accepting those interpreta-
tions which are licensed by the other language.
In particular, we select one language as the tar-
get language, translate the other language?s seman-
tics for every parse into the target language and thus
align maximally similar semantic representations.
The parses with the most overlapping semantics are
selected as preferred parses.
As an example consider the English sentence They
closed the shop at five, which has the following two
interpretations due to PP attachment ambiguity:1
(1) ?At five, they closed the shop?
close(they, shop); at(close, 5)
(2) ?The shop at five was closed by them?
close(they, shop); at(shop, 5)
The Japanese translation is also ambiguous, but in
a completely different way: it has the possibility of
a zero pronoun (we show the translated semantics).
(3) ?
kare
he
?
ra
PL
?
wa
TOP
?
5
5
?
ji
hour
?
ni
at
?
mise
shop
?
wo
ACC
??
shime
close
?
ta
PAST
?At 5 o?clock, they closed the shop.?
close(they, shop); at(close, 5)
(4) ?At 5 o?clock, as for them, someone closed the shop.?
close(?, shop); at(close, 5)
topic(they,close)
We show the semantic representation of the ambi-
guity with each sentence. Both languages are disam-
biguated by the other language as only the English
interpretation (1) is supported in Japanese, and only
the Japanese interpretation (3) leads to a grammati-
cal English sentence.
2 Related Work
There is no group using exactly the same approach
as ours: automated parallel parse disambiguation
on the basis of semantic analyses. Zhechev and
1In fact it has four, as they can be either plural or the androg-
ynous singular, this is also disambiguated by the Japanese.
125
Way (2008) automatically generate parallel tree-
banks for training of statistical machine translation
(SMT) systems through sub-tree alignment. We do
not aim to carry out the complete treebanking pro-
cess, but to optimize speed and precision of manual
creation of high-quality treebanks.
Wu (1997) and others have tried to simultane-
ously learn grammars from bilingual texts. Burkett
and Klein (2008) induce node-alignments of syntac-
tic trees with a log-linear model, in order to guide
bilingual parsing. Chen et al (2011) translate an
existing treebank using an SMT system and then
project parse results from the treebank to the other
language. This results in a very noisy treebank, that
they then clean. These approaches align at the syn-
tactic level (using CFGs and dependencies respec-
tively).
In contrast to the above approaches, we assume
the existence of grammars and use a semantic rep-
resentation as the appropriate level for cross-lingual
processing. We compare semantic sub-structures, as
those are more straightforwardly comparable across
different languages. As a consequence, our system
is applicable to any combination of languages. The
input is plain parallel text, neither side needs to be
treebanked.
3 Materials and Methods
We use grammars within the grammatical frame-
work of head-driven phrase-structure grammar
(HPSG Pollard and Sag (1994)), with the seman-
tic representation of minimal recursion semantics
(MRS; Copestake et al (2005)). We use two large-
scale HPSG grammars and a Japanese-English ma-
chine translation system, all of which were de-
veloped in the DELPH-IN framework:2 The En-
glish Resource Grammar (ERG; Flickinger (2000))
is used for English parsing, and Jacy (Bender and
Siegel, 2004) for parsing Japanese. For Japanese
to English translation we use Jaen, a semantic-
transfer based machine translation system (Bond
et al, 2011).
3.1 Semantic Interface and Alignment
For the alignment, we convert the MRS struc-
tures into simplified elementary dependency graphs
2http://www.delph-in.net/
x4:pronoun_q[]
e2:_close_v_c[ARG1 x4:pron, ARG2 x9:_shop_n_of]
x9:_the_q[]
e8:_at_p_temp[ARG1 e2, ARG2 x16:_num_hour(5)]
x16:_def_implicit_q[]
Figure 1: EDG for They closed the shop at five.
(EDGs), which abstract away information about
grammatical properties of relations and scopal in-
formation. Preliminary experiments showed that the
former kind of information did not contribute to dis-
ambiguation performance, as number is typically
underspecified in Japanese. As we only consider lo-
cal information in the alignment, scopal information
can be ignored as well. An example EDG is dis-
played in Figure 1.
An EDG consists of a bag of elementary predi-
cates (EPs) which are themselves composed of re-
lations. Each line in Figure 1 corresponds to one
EP. Relations are the elementary building blocks of
the EDG, and loosely correspond to words of the
surface string. EPs consist either of atomic rela-
tions (corresponding to quantifiers), or a predicate-
argument structure which is composed of several re-
lations. During alignment, we only consider non-
atomic EPs, as quantifiers should be considered as
grammatical properties of (lexical) relations, which
we chose to ignore.
Given the EDG representations of the translated
Japanese sentence, and the original target language
EDGs, we can straightforwardly align by matching
substructures of different granularity.
Currently, we align at the predicate level. We are
experimenting with aligning further dependency re-
lation based tuples, which would allow us to resolve
more structural ambiguities.
3.2 The Disambiguation System
Ambiguity in the analyses for both languages is re-
duced on the basis of the semantic analyses returned
for each sentence-pair, and a reduced set of pre-
ferred analyses is returned for both languages. For
each sentence-pair, we (1) parse the English and
the Japanese sentence (MRSE and MRSJ ) (2) trans-
fer the Japanese MRS analyses to English MRSs
(MRSJE) (3) convert the top 11 translated MRSs
126
and the original English MRSs to EDGs3 (EDGE
and EDGJE) (4) align every possible E and JE EDG
combination and determine the set of best aligning
analyses (5) from those, create language specific sets
of preferred parses.
We are comparing semantic representations of the
same language, the English text from the bilingual
corpus and the English machine translation of the
Japanese text. In order to increase robustness of
our alignment system we not only consider com-
plete translations, but also accept partially translated
MRSs in case no complete translation could be pro-
duced. This step significantly increases the recall,
while the partial MRSs proved to be informative
enough for parse disambiguation.
4 Evaluation and Results
We evaluate our model on the task of parse disam-
biguation. We use full sentence match as evaluation
metric, a challenging target.
The Tanaka corpus is used for training and testing
(Tanaka, 2001). It is an open corpus of Japanese-
English sentence pairs. We use version (2008-11)
which contains 147,190 sentence pairs. We hold out
4,500 sentence pairs each for development and test.
For each sentence, we compare the number of the-
oretically possible alignments with the number of
preferred alignments returned by our system. On
average, ambiguity is reduced down to 30%. For
English 3.76 and for Japanese 3.87 parses out of
(at most) 11 analyses remain in the partially disam-
biguated list: both languages benefit equally from
the disambiguation.
We evaluate disambiguation accuracy by counting
the number of times the gold parse was present in the
partially disambiguated set (full sentence match).
Table 1 shows the alignment accuracy results.
The correct parse is included in the reduced set
in 80% of the cases for Japanese, and for 82% of
the cases in English. We match atomic relations
when aligning the semantic structures, which is a
very generic method applicable to the vast major-
ity of sentence pairs. This leads to a recall score of
3These are ranked with a model trained on a hand-
treebanked set. The cutoff was determined empirically: For
both languages the gold parse is included in the top 11 parses in
more than 97% of the cases.
English Japanese
Prec F Prec F
Included 0.820 0.897 0.804 0.887
First Rank 0.659 0.791 0.676 0.803
MRR 0.713 0.829 0.725 0.837
Table 1: Accuracy and F-scores for disambiguation per-
formance of our system. Recall was 99% in every case.
?Included?: inclusion of the gold parse in the reduced set
of parses or not. ?First Rank?: ranking of the preferred
parse as top in the reduced list. ?MRR?: mean reciprocal
rank of the gold parse in the list.
99%, and an F-Score of 89.7% and 88.7% for En-
glish and Japanese, respectively.
The reduced list of parser analyses can be further
ranked by the parse ranking model which is included
in the parsers of the respective languages (the same
models with which we determined the top 11 analy-
ses). Given this ranking, we can evaluate how often
the preferred parse is ranked top in our partially dis-
ambiguated list; results are shown in the two bottom
lines of Table 1.
A ranked list of possible preferred parses whose
top rank corresponds with a high probability to the
gold parse should further speed up the manual tree-
banking process.
Performance in the context of the whole pipeline
The performance of parsers and MT system
strongly influences the end-to-end results of the pre-
sented system. In the results given above, this in-
fluence is ignored. We lose around 29% of our data
because no parse could be produced in one or both
languages, or no translation could be produced. and
a further 5% of the sentences did not have the gold
parse in the original set of analyses (before align-
ment): our system could not possibly select the cor-
rect parse in those cases.
5 Discussion
Our system builds on the output of two parsers and
a machine translation system. We reduce ambiguity
for all sentence pairs where a parse could be cre-
ated for both languages, and for which there was at
least a partial translation. For these sentences, the
cross-lingual alignment component achieves a recall
of above 99%, such that we do not lose any addi-
127
tional data. The parsers and the MT system include
a parse ranking system trained on human gold anno-
tations. We use these models in parsing and transla-
tion to select the top 11 analyses. Our system thus
depends on a range of existing technologies. How-
ever, these technologies are available for a range of
languages, and we use them for efficient extension
of linguistic resources.
The effectiveness of cross-lingual parse disam-
biguation on the basis of semantic alignment highly
depends on the languages of choice. Given that we
exploit the differences between languages, pairs of
less related languages should lead to better disam-
biguation performance. Furthermore, disambiguat-
ing with more than two languages should improve
performance. Some ambiguities may be shared be-
tween languages. 4
One weakness when considering the disam-
biguated sentences as training for a parse ranking
model is that the translation fails on similar kinds of
sentences, so there are some phenomena which we
get no examples of ? the automatically trained tree-
bank does not have a uniform coverage of phenom-
ena. Our models may not discriminate some phe-
nomena at all.
Our system provides large amounts of automati-
cally annotated data at the only cost of CPU time:
so far we have disambiguated 25,000 sentences: 10
times more than the existing hand annotated gold
data. Using the parser output for speeding up man-
ual treebanking is most effective if the gold parse is
reliably included in the reduced set of parses. In-
creasing precision by accepting more than only the
most overlapping parses may lead to more effective
manual treebanking.
The alignment method we propose does not make
any language-specific assumptions, nor is it limited
to align two languages only. The algorithm is very
flexible, and allows for straightforward exploration
of different numbers and combinations of languages.
6 Conclusion and Future Work
Translating a sentence into a different language
changes its surface form, but not its meaning. In
4For example the PP attachment ambiguity in John said that
he went on Tuesday where either the saying or the going could
have happened on Tuesday holds in both English and Japanese.
parallel corpora, one language can be viewed as a
semantic tag of the other language and vice versa,
which allows for disambiguation of phenomena
which are ambiguous in only one of the languages.
We use the above observations for cross-lingual
parse disambiguation. We experimented with the
language pair of English and Japanese, and were
able to accurately reduce ambiguity in parser anal-
yses simultaneously for both languages to 30% of
the starting ambiguity. The remaining parses can be
used as a pre-selection to speed up the manual tree-
banking process.
We started working on an extrinsic evaluation of
the presented system by training a discriminative
parse ranking model on the output of our alignment
process. Augmenting the Gold training data with
our data improves the model. Our next step will
be to evaluate the system as part of the treebanking
process, and optimize the parameters such as disam-
biguation precision vs. amount of disambiguation.
As no language-specific assumptions are hard
coded in our disambiguation system, it would be
very interesting to apply the system to different lan-
guage pairs as well as groups of more than two lan-
guages. Using a group of languages for disambigua-
tion will likely lead to increased and more accurate
disambiguation, as more constraints are imposed on
the data.
Probably the most important goal for future work
is improving the recall achieved in the complete dis-
ambiguation pipeline. Many sentence-pairs cannot
be disambiguated because either no parse can be
generated for one or both languages, or no (par-
tial) translation can be produced. Following the
idea of partial translations, partial parses may be a
valid backoff. For purposes of cross-lingual align-
ment, partial structures may contribute enough in-
formation for disambiguation. There has been work
regarding partial parsing in the HPSG community
(Zhang and Kordoni, 2008), which we would like to
explore. There is also current work on learning more
types and instances of transfer rules (Haugereid and
Bond, 2011).
Finally, we would like to investigate more align-
ment methods, such as dependency relation based
alignment which we started experimenting with, or
EDM-based metrics as presented in (Dridan and
Oepen, 2011).
128
Acknowledgments
This research was supported in part by the Erasmus
Mundus Action 2 program MULTI of the European
Union, grant agreement number 2009-5259-5 and
the the joint JSPS/NTU grant on Revealing Meaning
Using Multiple Languages. We would like to thank
Takayuki Kuribayashi and Dan Flickinger for their
help with the treebanking.
References
Emily M. Bender and Melanie Siegel. 2004. Im-
plementing the syntax of Japanese numeral clas-
sifiers. In Proceedings of the IJC-NLP-2004.
Francis Bond, Stephan Oepen, Eric Nichols, Dan
Flickinger, Erik Velldal, and Petter Haugereid.
2011. Deep open-source machine translation.
Machine Translation, 25(2):87?105.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Pro-
ceedings of EMNLP, 2008.
Wenliang Chen, Jun?ichi Kazama, Min Zhang,
Yoshimasa Tsuruoka, Yujie Zhang, Yiou Wang,
Kentaro Torisawa, and Haizhou Li. 2011. SMT
helps bitext dependency parsing. In Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP2011), pages 73?83. Edinburgh.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics ?
an introduction. Research on Language and Com-
putation, 3:281?332.
Rebecca Dridan and Stephan Oepen. 2011. Parser
evaluation using elementary dependency match-
ing. In Proceedings of IWPT.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28. (Special Issue on Effi-
cient Processing with HPSG).
Petter Haugereid and Francis Bond. 2011. Extract-
ing transfer rules for multiword expressions from
parallel corpora. In Proceedings of the Work-
shop on Multiword Expressions: from Parsing
and Generation to the Real World.
Carl Pollard and Ivan A. Sag. 1994. Head
Driven Phrase Structure Grammar. University of
Chicago Press, Chicago.
Yasuhito Tanaka. 2001. Compilation of a multilin-
gual parallel corpus. In Proceedings of PACLING
2001.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?403.
Yi Zhang and Valia Kordoni. 2008. Robust parsing
with a large HPSG grammar. In Proceedings of
the Sixth International Conference on Language
Resources and Evaluation (LREC?08).
Ventsislav Zhechev and Andy Way. 2008. Auto-
matic generation of parallel treebanks. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008).
129
