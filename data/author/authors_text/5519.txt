A Methodology for Terminology-based 
Knowledge Acquisition and Integration 
 
Hideki Mima1
?
, Sophia Ananiadou2, Goran Nenadic2 and Junichi Tsujii1 
 
1Dept. of Information Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan 
{mima, tsujii}@is.s.u-tokyo.ac.jp 
2Computer Science, University of Salford 
Newton Building, Salford M5 4WT, UK 
{S.Ananiadou, G.Nenadic}@salford.ac.uk 
 
                                                
? Current affiliation: Dept. of Engineering, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113- 8656, Japan 
Abstract  
In this paper we propose an integrated knowledge 
management system in which terminology-based 
knowledge acquisition, knowledge integration, 
and XML-based knowledge retrieval are 
combined using tag information and ontology 
management tools. The main objective of the 
system is to facilitate knowledge acquisition 
through query answering against XML-based 
documents in the domain of molecular biology. 
Our system integrates automatic term recognition, 
term variation management, context-based 
automatic term clustering, ontology-based 
inference, and intelligent tag information retrieval. 
Tag-based retrieval is implemented through 
interval operations, which prove to be a powerful 
means for textual mining and knowledge 
acquisition. The aim is to provide efficient access 
to heterogeneous biological textual data and 
databases, enabling users to integrate a wide 
range of textual and non-textual resources 
effortlessly. 
Introduction 
With the recent increasing importance of 
electronic communication and data sharing over 
the Internet, there exist an increasingly growing 
number of publicly accessible knowledge sources, 
both in the form of documents and factual 
databases. These knowledge sources (KSs) are 
intrinsically heterogeneous and dynamic. They 
are heterogeneous since they are autonomously 
developed and maintained by independent 
organizations for different purposes. They are 
dynamic since constantly new information is 
being revised, added and removed. Such an 
heterogeneous and dynamic nature of KSs 
imposes challenges on systems that help users to 
locate and integrate knowledge relevant to their 
needs. 
   Knowledge, encoded in textual documents, is 
organised around sets of specialised (technical) 
terms (e.g. names of proteins, genes, acids). 
Therefore, knowledge acquisition relies heavily 
on the recognition of terms. However, the main 
problems that make term recognition difficult are 
the lack of clear naming conventions and 
terminology variation (cf. Jacquemin and 
Tzoukermann (1999)), especially in the domain 
of molecular biology. Therefore, we need a 
scheme to integrate terminology management as 
a key prerequisite for knowledge acquisition and 
integration. 
   However, automatic term extraction is not the 
ultimate goal itself, since the large number of 
new terms calls for a systematic way to access 
and retrieve the knowledge represented through 
them. Therefore, the extracted terms need to be 
placed in an appropriate framework by 
discovering relations between them, and by 
establishing the links between the terms and 
different factual databases. 
   In order to solve the problem, several 
approaches have been proposed. MeSH Term in 
MEDLINE (2002) and Gene Ontology (2002) 
provide a top-down controlled ontology 
framework, which aims to describe and constrain 
the terminology in the domain of molecular 
biology. On the other hand, automatic term 
acquisition approaches have been developed in 
order to address a dynamic and corpus-driven 
knowledge acquisition methodology (Mima et al, 
1999; 2001a).  
   Different approaches to linking relevant 
resources have also been suggested. The 
Semantic Web framework (Berners-Lee (1998)) 
aims to link relevant Web resources in bottom-up 
manner using the Resource Description 
Framework (RDF) (Bricklet and Guha, 2000) and 
an ontology. However, although the Semantic 
Web framework is powerful to express content of 
resources to be semantically retrieved, some 
manual description is expected using the 
RDF/ontology. Since no solution to the 
well-known difficulties in manual ontology 
development, such as the ontology 
conflictions/mismatches (Visser et al, 1997) is 
provided, an automated ontology management is 
required for the efficient and consistent 
knowledge acquisition and integration. TAMBIS 
(Baker et al, 1998) tried to provide a filter from 
biological information services by building a 
homogenising layer on top of the different 
sources using the classical mediator/wrapper 
architecture. It intended to provide source 
transparency using a mapping from terms placed 
in a conceptual knowledge base of molecular 
biology onto terms in external sources.  
   In this paper we introduce TIMS, an integrated 
knowledge management system in the domain of 
molecular biology, where terminology-based 
knowledge acquisition (KA), knowledge 
integration (KI), and XML-based knowledge 
retrieval are combined using tag information and 
ontology management tools. The management of 
knowledge resources, similarly to the Semantic 
Web, is based on XML, RDF, and 
ontology-based inference. However, our aim is to 
facilitate the KA and KI tasks not only by using 
manually defined resource descriptions, but also 
by exploit ing NLP techniques such as automatic 
term recognition (ATR) and automatic term 
clustering (ATC), which are used for automatic 
and systematic ontology population.  
    
   The paper is organised as follows: in section 1 
we present the overall TIMS architecture and 
briefly describe the components incorporated in 
the system, while section 2 gives the details of the 
proposed method for KA and KI. In the last 
section we present results, evaluation and 
discussion. 
1 TIMS ? system architecture 
XML-based Tag Information Management 
System (TIMS) is a core machinery for managing 
XML tag information obtained from sub 
functional components. Its main aim is to 
facilitate an efficient mechanism for KA and KI 
through a query answering system for 
XML-based documents in the domain of 
molecular biology, by using a tag information 
database.  
   Figure 1 shows the system architecture of 
TIMS. It integrates the following modules via  
XML-based data exchange: JTAG ? an 
annotation tool, ATRACT ? an automatic term 
recognition and clustering workbench, and the 
LiLFeS abstract machine, which we briefly 
describe in this section. ATRACT and LiLFeS 
play a central role in the knowledge acquisition 
process, which includes term recognition, 
ontology population, and ontology-based 
inference. In addition to these modules, TIMS 
implements an XML-data manager and a TIQL 
query processor (see Section 2).  
1.1 JTAG 
JTAG is an XML-based manual annotation and 
resource description aid tool. Its purpose is to 
support manual annotation (e.g. semantic 
tagging), adjusting term recognition results, 
developing RDF logic, etc. In addition, ontology 
information described in XML can also be 
developed and modified using the tool. All the 
annotations can be managed via a GUI.  
1.2 ATRACT 
In the domain of molecular biology, there is an 
increasing amount of new terms that represent 
newly created concepts. Since existing term 
Figure 1: System architecture of TIMS 
 
XML Data 
Retrieval 
TIMS
Tag Information Database  
XML Data 
Management 
ATRACT 
Automatic Term 
Recognition 
and Term 
Clustering 
XML data
XML data
XML data
Document/
Database
Retriever
L iLFeS 
Syntactic and 
Semantic Parser / 
RDF and Ontology 
Manager 
JTAG 
Manual Resource 
Description 
Aid Interface XML data
dictionaries cannot cover the needs of specialists, 
automatic term extraction tools are important for 
consistent term discovery. ATRACT (Mima et al, 
2001a) is a terminology management workbench 
that integrates ATR and ATC. Its main aim is to 
help biologists to gather and manage terminology 
in the domain. The module retrieves and 
classifies terms on the fly and sends the results as 
XML tag information to TIMS.  
   The ATR method is based on the C/NC-value 
method (Frantzi et al, 2000). The original 
method has been augmented with acronym 
acquisition and term variation management 
(Nenadic et al 2002), in order to link different 
terms that denote the same concept. Term 
variation management is based on term 
normalisation as an integral part of the ATR 
process. All orthographic, morphological and 
syntactic term variations and acronym variants (if 
any) are conflated prior to the statistical analysis, 
so that term candidates comprise all variants that 
appear in a corpus. 
   Besides term recognition, term clustering is an 
indispensable component in a knowledge 
management process (see figure 2). Since 
terminological opacity and polysemy are very 
common in molecular biology, term clustering is 
essential for the semantic integration of terms, 
the construction of domain ontology and for 
choosing the appropriate semantic information.  
   The ATC method is based on Ushioda?s AMI 
(Average Mutual Information)-hierarchical 
clustering method (Ushioda, 1996). Our 
implementation uses parallel symmetric 
processing for high speed clustering and is built 
on the C/NC-value results. As input, we use 
co-occurrences of automatically recognised 
terms and their contexts, and the output is a 
dendrogram of hierarchical term clusters (like a 
thesaurus). The calculated term cluster 
information is stored in LiLFeS (see below) and 
combined with a predefined ontology according 
to the term classes automatically assigned. 
1.3 LiLFeS 
LiLFeS (Miyao et al, 2000) is a Prolog-like 
programming language and language processor 
used for defining definite clause programs with 
typed feature structures. Since typed feature 
structures can be used like first order terms in 
Prolog, the LiLFeS language can describe 
various kinds of applications based on feature 
structures. Examples include HPSG parsers, 
HPSG-based grammars and compilers from 
HPSG to CFG. Furthermore, other NLP modules 
can be easily developed because feature structure 
processing can be directly written in the LiLFeS 
language. Within TIMS, LiLFeS is used to: 1) 
infer similarity between terms using hierarchical 
matching, and 2) parse sentences using 
HPSG-based parsers and convert the results into 
an XML-based formalism. 
 
2 Knowledge Integration and Management  
 
Knowledge integration and management in 
TIMS is organised by integrating XML-data 
management (section 2.1) and tag- and 
ontology-based information extraction (section 
2.2). Figure 3 illustrates a model of the 
knowledge management based on the knowledge 
integration and question-answering process 
within TIMS. In this scenario, a user formulates a 
query, which is processed by a query manager. 
The tag data manager retrieves the relevant data 
from the collection of documents via a tag 
database and ontology-based inference (such as 
POS Tagger 
Acronym Recognition 
C-value ATR 
Orthographic Variants 
Morphological Variants 
Syntactic Variants 
NC-value Analyzer 
Term Clustering  
(Semantic Analyzer) 
XML Documents Including 
Term Tags and Term 
Variation/Class Information 
Input Documents 
Figure 2. Term Ontology Development 
Recognition of Term 
Variations (synonyms) 
Recognition of Term 
Classes (Similar Terms) 
hierarchical matching of term classes).  
2.1 XML-tag data management 
Communication within TIMS is based on 
XML-data exchange.  TIMS initially parses the 
XML documents (which contain relevant 
terminology information generated automatically 
by ATRACT) and ?de-tags? them. Then, like in 
the TIPSTER architecture (Grishman, 1995), 
every tag information is stored separately from 
the original documents and managed by an 
external database software. This facility allows, 
as shown in figure 4, different types of tags (POS, 
syntactic, semantic, etc.) for the same document 
to be supported. 
2.2 Tag- and ontology-based IE 
The key feature of KA and KI within TIMS is a 
facility to logically retrieve data that is 
represented by different tags. This feature is 
implemented via interval operations. The main 
assumption is that the XML tags specify certain 
intervals within documents. Interval operations 
are XML specific text/data retrieval operations, 
which operate on such textual intervals. Each 
interval operation takes two sets of intervals as 
input and returns a set of intervals according to 
the specified logical operations. Currently, we 
define four types of logical operations: 
? Intersection ??? returns intersected intervals 
of all the intervals given. 
? Union ??? returns merged intervals of all the 
intersected intervals. 
? Subtraction ?y? returns differences in 
intervals of all the intersected intervals. 
? Concatenation ?+? returns concatenated 
intervals of all the continuous intervals. 
 
For example, the interval operation 1 
<VP>?(<V>?<term>) describes all verb 
(<V>)-term (<term>) pairs within a verb phrase 
(<VP>). Similarly, suppose X denotes a set of 
intervals of manually annotated tags for a 
document and Y denotes a set of intervals of 
automatically annotated tags for the same 
document. The interval operation ((X?Y) 
?{X?Y}) results in the differences between 
human and machine annotations (see figure 5). 
Interval operations are powerful means for 
textual mining from different sources using tag 
information.  In addition, LiLFeS enables tag 
(interval) retrieval to process not only regular 
                                                
1 ??? denotes a merged set of all the elements. 
 
Figure 3: Question-answering process in TIMS 
Database 
A
  
A
  
A
  
A
  
A
  
A
  
XML / HTML 
Knowledge 
Sources 
Tag Data 
Language 
Analyzer 
 
 
Tag Data 
Manager 
TIQL 
Processor
NLP Components 
TIMS  
Query to TIQL
Translator 
Query 
ATRACT 
Ontology 
Data 
A
  
A
  
A
  
A
  
A
  
A
  
 
LiLFeS 
Knowledge Acquisition 
Knowledge Integration 
 ? 
 
26 15 VERB 
? 
 
110 100 ADJ 
? 150 140 DNA 
? ? ? ?.. 
? 209 203 RNA 
? 10 5 NOUN
. . . end start Tag 
? 
 
35 15 VP 
? 
 
100 35 PP 
? 
 
150 100 VP 
? ? ? ?.. 
? 209 203 NP 
? 
 
10 5 NP 
. . . end start Tag 
Figure 4: Tag data management 
Part-of-speech tags 
? 
 
80 40 PROTEIN 
? 
 
180 160 DNA 
? 
 
220 200 DNA 
? ? ? ?.. 
? 
 
260 240 RNA 
? 
 
18 5 DNA 
. . . end start Tag 
Semantic tags 
Syntactic tags 
 
X = {                                                           }
Y = {                                                            }
 
X?Y  = {                                                              } 
?{X?Y}={                                                        
} 
 
(X?Y) ?{X?Y}={                                             
                                                                                               }
Figure 5. (X?Y) ?{X ?Y} 
pattern/string matching using tag information, 
but also the ontological hierarchy matching to 
subordinate classes using either predefined or 
automatically derived term ontology. Thus, 
semantically-based tag information retrieval can 
be achieved. For example, the interval operation2 
<VP>?<nucleic_acid*> will retrieve all 
subordinate terms/classes of nucleic acid, which 
are contained within a VP. 
   The interval operations can be performed over 
the specified documents and/or tag sets (e.g. 
syntactic, semantic tags, etc.) simultaneously or 
in batch mode, by selecting the documents/tag 
sets from a list. This accelerates the process of 
KA, as users are able to retrieve information from 
multiple KSs simultaneously. 
2.3 TIQL - Tag Information Query Language  
In order to integrate and expand the above 
components, we have developed a tag 
information query language (TIQL). Using this 
language, a user can specify the interval 
operations to be performed on selected 
documents (including the ontology inference to 
expand queries). The basic expression in TIQL 
has the following form: 
 
SELECT [n-tuple variables]  
FROM [XML document(s)] 
WHERE [interval operation] 
      FROM [XML document(s)] 
WHERE [interval operation] 
                     ?? 
where, [n-tuple variables] specifies the 
table output format, [XML document(s)] 
denotes the document(s) to be processed, and 
[interval operation] denotes an interval 
operation to be performed over the corresponding 
document with variables of each interval to be 
bound. 
For example, the following expression: 
 
SELECT   x1, x2  
 FROM   ?paper-1.xml? 
    WHERE  
<VP>?{x1:<EVENT*>?x2:<nucleic_acid*>} 
  FROM   ?paper-2.xml? 
 WHERE  
<VP>?{x1:<EVENT*>?x2:<nucleic_acid*>} 
 
                                                
2 ?*? denotes hierarchical matching. 
extracts all the hierarchically subordinate classes 
matched to (<EVENT>, <nucleic_acid>) pair 
within a VP from the specified XML-documents,  
and then automatically builds a table to display 
the results (see figure 6).  
   Since formulating an appropriate TIQL 
expression using interval operations might be 
cumbersome, in particular for novice users, 
TIMS was augmented with a capability of 
?recycling? predefined queries and macros. 
3 Evaluation and discussion 
We have conducted preliminary experiments 
using the proposed framework. In this paper we 
briefly present the quality of automatic term 
recognition and similarity measure calculation 
via automatically clustered terms. After that, we 
discuss the practical performance of tag 
manipulation in TIMS compared to string-based 
XML tag manipulation to show the advantage of 
the tag information management scheme.  
   The term recognition evaluation was performed 
on the NACSIS AI-domain corpus (Koyama et 
al., 1998), which includes 1800 abstracts and on a 
set of MEDLINE abstracts. Table 1 shows a 
sample of extracted terms and term variants. The 
ATR precisions of the top 100 intervals range 
from 93% to 98% (see figure 7; for detailed 
evaluation, see Mima et al (2001b) and Nenadic 
et al (2002)).  
 
 Title 
Background 
                                      
                  
........<DNA>androgen 
receptor gene</DNA>   
............... 
                       
         
                      
paper-2.xml 
Title 
Background 
                                      
                  
........<RNA>HB-EGF 
mRNA</RNA>   
............... 
                       
         
                            
paper-1.xml 
nucleic_acid 
 nucleic acid EVENT 
EVENT 
androgen receptor 
gene acid HB-EGF mRNA 
... 
activate 
bind 
... 
Figure 6. Ontology-based Tagged 
Information Retrieval 
 
   terms (and term variants) term-hood 
retinoic acid receptor                                              
     retinoic acid receptor 
     retinoic acid receptors 
     RAR, RARs 
6.33 
nuclear receptor  
     nuclear receptor 
     nuclear receptors 
     NR, NRs 
6.00 
all-trans retionic acid 
     all trans retionic acid 
     all-trans-retinoic acids 
     ATRA, at-RA, atRA 
4.75 
9-cis-retinoic acid 
     9-cis retinoic acid 
     9cRA, 9-c-RA 
4.25 
 
Table 1: Sample of recognised terms  
85
90
95
100
2.65-3.99 4.00-5.99 6.00-Top
C-value
pr
ec
is
io
n
 
Figure 7: ATR interval precision 
 
   For term clustering and tag manipulation 
performance we used the GENIA resources 
(GENIA corpus, 2002), which include 1,000 
MEDLINE abstracts (MEDLINE, 2002), with 
overall 40,000 (16,000 distinct) semantic tags 
annotated for terms in the domain of nuclear 
receptors. We used the similarity measure 
calculation as the central computing mechanism 
for inferring the relevance between the XML tags 
and tags specified in the TIQL/interval operation,  
determining the most relevant tags in the 
XML-based KS(s). As a gold standard, we used 
similarities between the terms that were 
calculated according to the hierarchy of the 
clustered terms according to the GENIA 
ontology. In this experiment, we have adopted a 
semantic similarity calculation method for 
measuring the similarity between terms described 
in (Oi et al, 1997). The three major sets of 
classes (namely, nucleic_acid, amino_acid, 
SOURCE) of manually classified terms from 
GENIA ontology (GENIA corpus, 2002) were 
used to calculate the average similarities (AS) of 
the elements. ASs of the elements within the 
same classes were greater than the ASs between 
elements from different classes, which proves 
that the terms were clustered reliably according 
to their semantic features. 
   In order to examine the tag manipulation 
performance of TIMS, we measured the 
processing times consumed for executing an 
interval operation in TIMS compared to the time 
needed by using string-based regular expression 
matching (REM). We focused on measuring the 
interval operation ??? with intervals (tags) 
<title> and <term> (i.e. extracting all terms 
within titles).   In the evaluation process, we used 
5 different samples to examine IE performances 
according to their size (namely the number of 
tags and file size in Kb).  
 
 Sample1 Sample2 Sample3 Sample4 Sample5 
TIMS 
(millisec.) 16 28 40 44 62 
REM 
(millisec.) 24 38 58 80 104 
# of tags 1146 2383 3730 4799 5876 
Size  
(K bytes) 92 191 298 382 470 
 
Table 2: TIMS - practical performance 
 
Table 2 and Figure 8 show the results: the 
processing times of TIMS were about 1.4-1.8 
times faster (depending on number of tags and 
corpus length) than those of REM. Therefore, we 
assume that the TIMS tag information 
management scheme can be considered as an 
efficient mechanism to facilitate knowledge 
acquisition and information extraction process. 
0
20
40
60
80
100
120
0 2000 4000 6000
# of tags
tim
e 
(m
ill
i s
ec
.)
TIMS
REM
Figure 8. IE performance (TIMS vs. REM) 
Conclusion 
In this paper, we presented a methodology for 
KA and KI over large KSs. We described TIMS, 
an XML-based integrated KA aid system, in 
which we have integrated automatic term 
recognition, term clustering, tagged data 
management and ontology-based knowledge  
retrieval. TIMS allows users to search and 
combine information from various sources. An 
important source of information in the system is 
derived from terminological knowledge, which is 
provided automatically in the XML format. 
Tag-based retrieval is implemented through 
interval operations, which ? in combination with 
hierarchical matching ? prove to be powerful 
means for textual mining and knowledge 
acquisition. 
   The system has been tested in the domain of 
molecular biology. The preliminary experiments 
show that the TIMS tag information management 
scheme is an efficient methodology to facilitate 
KA and IE in specialised fields. 
   Important areas of future research will involve 
expanding the scalability of the system to real 
WWW knowledge acquisition tasks and 
experiments with fine-grained term 
classification. 
References  
Baker P. G., Brass A., Bechhofer S., Goble C., Paton 
N. and Stevens R. (1998) TAMBIS: Transparent 
Access to Multiple Bioinformatics Information 
Sources. An Overview in Proc. of the Sixth 
International Conference on Intelligent Systems for 
Molecular Biology, ISMB98, Montreal. 
Berners-Lee, T. (1998) The Semantic Web as a 
longuage of logic, available at: http://www.w3.org/ 
DesignIssues/Logic.html 
Brickle, D. and Guha R. (2000) Resource Description 
Framework (RDF) Schema Specification 1.0, W3C 
Candidate Recommendation, available at 
http://www.w3.org/TR/rdf-schema 
Frantzi K. T., Ananiadou S. and Mima H. (2000) 
Automatic Recognition of Multi-Word Terms: the 
C-value/NC-value method, in International Journal 
on Digital Libraries, Vol. 3, No. 2, 115?130. 
Gene Ontology Consortium (2002) GO ontology. 
available at  http:// www.geneontology.org/ 
GENIA corpus (2002) GENIA project home page. 
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/ 
Grishman R (1995) TIPSTER Phase II Architecture 
Design Document. New York University, available 
at http://www.tipster.org/arch.htm 
Jacquemin C. and Tzoukermann E. (1999) NLP for 
Term Variant Extraction: A Synergy of Morphology, 
Lexicon and Syntax. In T. Strzalkowski (editor), 
Natural Language Information Retrieval, Kluwer, 
Boston, pp. 25-74. 
Koyama T., Yoshioka M. and Kageura K. (1998) The 
Construction of a Lexically Motivated Corpus - The 
Problem with Defining Lexical Unit. In Proceedings 
of LREC 1998, Granada, Spain, pp. 1015?1019. 
MEDLINE (2002) National Library of Medicine, 
http://www.ncbi.nlm.nih.gov/PubMed/ 
Mima H., Ananiadou S. and Nenadic G. (2001a) 
ATRACT Workbench: An Automatic Term 
Recognition and Clustering of Te rms, in Text, 
Speech and Dialogue - TSD2001, Lecture Notes in 
AI 2166, Springer Verlag 
Mima H. and Ananiadou S. (2001b) An Application 
and Evaluation of the C/NC-value Approach for the 
Automatic term Recognition of Multi-Word units in 
Japanese, in International Journal on Terminology, 
Vol. 6(2), pp 175-194. 
Mima H., Ananiadou S. and Tsujii J. (1999) A 
Web-based integrated knowledge mining aid 
system using term-oriented natural language 
processing, in Proceedings of The 5th Natural 
Language Processing Pacific Rim Symposium, 
NLPRS'99, pp. 13?18. 
Miyao Y., Makino T., Torisawa K. and Tsujii J. 
(2000) The LiLFeS abstract machine and its 
evaluation with the LinGO grammar. Journal of 
Natural Language Engineering, Cambridge 
University Press, Vol. 6(1), pp.47-62. 
Nenadic G., Spasic I. and Ananiadou S. (2002) 
Automatic Acronym Acquisition and Term 
Variation Management within Domain Specific 
Texts, in Proc. of LREC 2002, Las Palmas, Spain, 
pp. 2155-2162. 
Oi K., Sumita E. and Iida H. (1997) Document 
Retrieval Method Using Semantic Similarity and 
Word Sense Disambiguation (in Japanese), in 
Journal of Natural Language Processing, Vol.4, 
No.3, pp.51-70. 
Visser P.R.S., Jones D.M., Bench-Capon T.J.M. and 
Shave M.J.R. (1997) An Analysis of Ontology 
Mismatches; Heterogeneity versus Interoperability. 
In AAAI 1997 Spring Symposium on Ontological 
Engineering, Stanford University, California, USA. 
Ushioda A. (1996) Hierarchical Clustering of Words. 
In Proc. of COLING ?96, Copenhagen 
A Novel Method for Content Consistency and Efficient Full-text 
Search for P2P Content Sharing Systems 
 
Hideki Mima 
University of Tokyo 
7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan
mima@biz-model.t.u-tokyo.ac.jp 
Hideto Tomabechi 
Cognitive Research Lab 
7-8-25 Roppongi, Minato-ku, Tokyo, Japan 
Hideto_Tomabechi@crl.co.jp 
Abstract 
A problem associated with current P2P (peer-to-peer) 
systems is that the consistency between copied contents 
is not guaranteed. Additionally, the limitation of full-
text search capability in most of the popular P2P 
systems hinders the scalability of P2P-based content 
sharing systems. We proposed a new P2P content 
sharing system in which the consistency of contents in 
the network is maintained after updates or modifications 
have been made to the contents. Links to the 
downloaded contents are maintained on a server. As a 
result, the updates and modifications to the contents can 
be instantly detected and hence get reflected in future 
P2P downloads. Natural language processing including 
morphological analysis is performed distributedly by the 
P2P clients and the update of the inverted index on the 
server is conducted concurrently to provide an efficient 
full-text search. The scheme and a preliminary 
experimental result have been mentioned 
1   Introduction 
P2P content sharing systems can distribute large 
amounts of contents with limited resources. By utiliz-
ing this exceptional feature, the P2P content sharing 
model is expected to be one of the major means for 
exchanging contents. 
However, the presently available P2P content shar-
ing systems are mainly used to illegally copy movies 
and music contents. In some cases, the service provid-
ers are accused of such illegal data exchange. 
We have recognized that the following technical 
problems may result in the above mentioned misuse 
of P2P.  
First, the presently available commercial P2P con-
tent sharing systems do not provide sufficient func-
tions to track the exchange of contents among users. 
Due to this, service providers cannot monitor the 
illegal exchange or tampering of shared contents 
among users. 
Second, the presently available commercial P2P 
content sharing systems only provide simple search 
functions, such as keyword search; therefore, they are 
unsuitable for contents that are either frequently up-
dated or have text. In practice, the current P2P content 
sharing systems are mainly used to only share movies 
and music contents because these are not frequently 
updated. The development of an appropriate search 
method for the P2P content sharing system is required 
in order to apply them to search text contents and the 
latest version of contents. 
In order to solve these technical problems, we are 
developing a content consistency maintenance method 
and an information search technique for P2P content 
sharing systems. Our content consistency maintenance 
method consists of a technique that prevents the tam-
pering of contents and a method that maintains consis-
tency between the following: 
1. how users exchange contents on a P2P contents 
sharing system and 
2. how the service provider recognizes the 
exchange of contents. 
Finally, we aim to standardize the result of previ-
ous research  [10].  
In order to handle the updates of contents, the P2P 
content sharing system that we are developing main-
tains digital signs for each version of the content. Our 
system uses a download protocol based on asymmet-
ric key encryption to maintain content consistency. In 
order to obtain the latest version of contents, even for 
updated contents, this method employs links to the 
original and the downloaded contents. These links are 
managed on a central server. 
In order to efficiently implement a full-text search, 
clients connected to our system perform morphologi-
cal analysis and summarization of the text to generate 
text information that is necessary for building a re-
verse index on a central server. The text information 
is stored on a central server when the content is up-
dated. To reduce the load of full-text search, the 
search results are cached on clients. By these tech-
niques, we can distribute the load of natural language 
processing among clients and rapidly search text con-
tents with content updates. 
In this paper, we briefly describe the P2P content 
sharing system that we are developing and the tech-
niques used in it, namely, a content consistency main-
tenance method and a full-text search method. We 
also report the result of a preliminary experiment on 
load balancing of full-text search by our technique. 
This paper is structured as follows: Section 2 de-
scribes related work. Section 3 briefly describes the 
25
P2P content sharing system that we are developing. 
Sections 4 and 5 describe techniques for content con-
sistency maintenance and full-text search, respectively. 
Finally, Section 6 presents the conclusion and future 
work. 
2   Related Work 
The two kinds of researches related to our work are 
researches on content consistency maintenance and 
those on information search in a P2P environment. 
In this paper, we refer to a hybrid P2P system, such 
as Napster that uses a central server, as a P2P system, 
although it is not entirely decentralized. This is be-
cause, even a hybrid P2P system has an important 
advantage in terms of content sharing; it can distribute 
large amounts of contents with less bandwidth con-
sumption on the service providers side. 
2.1   Contents Consistency Maintenance  
Since the contents are stored on clients in a P2P 
content sharing system, malicious clients can tamper 
with the contents if no protection method against 
tampering is provided. 
The MD5 hash function in the protocol of Napster 
 [4] enables a content publisher to send the hash value 
of a content to a central server when it publishes the 
content. Freenet  [2] prevents tampering with the con-
tent by using the hash value of a content as its key. 
This technique is effective in preventing the tam-
pering of static content such as a movie or music 
content. However, when this technique is applied to 
frequently updated contents, each version is treated as 
a separate content because different versions have 
different keys. To handle such frequently updated 
contents, Freenet introduced indirect files in which the 
hash values of the contents are stored. By retrieving 
an indirect file, a user can retrieve the last updated 
content in two steps. In order to share frequently up-
dated contents, we need to provide a mechanism that 
associates the content ID with the hash value of a 
particular version of the content, as in the case of 
Freenet. 
Another problem of P2P content sharing systems is 
that the provider of a content sharing service cannot 
trace the exchange of contents among users. 
Napster, which is a centralized P2P content sharing 
system similar to our system, uses a download proto-
col by which the clients send a download request to 
the central server before they download the content 
from another client. After this, the central server does 
not participate in the download process of the content. 
Using this protocol, the central server cannot identify 
whether a download has been carried out successfully 
or not. A malicious client can send the same informa-
tion to the central server and pretend that a download 
request has been made by another client. It is also 
possible to send tampered content to another client 
without being detected by the central server. 
2.2   Information Search in P2P Environment 
The two types of search techniques that are widely 
used in P2P content sharing systems include using a 
central search server  [4] and flooding of search re-
quests  [6]. 
The problems of using a central server, such as 
poor scalability of a central search server and vulner-
ability that arises from a single point of failure, are 
widely known. The flooding of search requests also 
has scalability problems. As the number of nodes in a 
network increases, more search requests are flooded 
that consume a major part of the bandwidth. In order 
to reduce search requests, many systems use flooding 
techniques that often limit the search range with heu-
ristic methods. As a result, it cannot be assured that all 
existing contents in a network can be found in these 
systems. 
In order to solve the problems associated with the 
above mentioned techniques, several search methods 
based on distributed hash tables (DHT) have been 
proposed  [5] [7]. These methods are scalable to a con-
siderable extent. A characteristic of these methods is 
that exact match key search can be done with O (log 
n) or O (na) hops. 
Reynolds and Vahdat proposed a method for im-
plementing full-text search by distributing the reverse 
index on a DHT. In this method, a key in a hash table 
corresponds to a particular keyword in a document, 
and a value in a hash table corresponds to a document 
that contains a keyword. A client that publishes a 
document notifies the nodes that correspond to the 
keywords contained in the document and updates the 
reverse indexes on these nodes. In this method, the 
load of the full-text search can be distributed among 
the nodes. We can also expect that the reverse indexes 
on the nodes can be updated rapidly by pushing the 
latest keywords in the contents from a client. 
On the other hand, this method has several limita-
tions. For example, when an AND search is per-
formed by this method, the search results must be 
transferred between the nodes. Li estimated the 
amount of resources that is necessary to implement a 
full-text search engine based on this method and 
pointed out that it is difficult to implement a large-
scale search engine, such as Google, by this method 
 [8]. 
Furthermore, if this method were applied to a P2P 
content sharing system, the problem of low availabil-
ity of nodes would arise because the users? PCs would 
be used as nodes in such a system. In order to store 
reverse indexes on the nodes, we have to replicate 
them to ensure the availability of indexes. This would 
require more resources than that estimated by Li. 
26
Based on the above mentioned reasons, we believe 
that a full-text search technique using a central search 
server that manages reverse indexes is more feasible 
than a distributed reverse index technique for imple-
menting a full-text search engine in a P2P environ-
ment. 
3   System Architecture 
Figure 1 shows the architecture of our system. As 
described earlier, we chose a central server architec-
ture to provide a full-text search of the contents. 
The public keys of the clients are stored on a cen-
tral server. By sending a request to the server, a client 
can obtain a public key of another client that is con-
nected with the central server. The central server also 
has private and public keys. Its public key is available 
to all the clients. 
Each client has a unique ID. When a client connects 
to the central server, it sends its own IP address. An-
other client can obtain the IP address of a client by 
querying to the server using its client ID. The central 
server provides a content consistency maintenance 
mechanism and a full-text search engine. These 
mechanisms are described in the following sections.   
4 Content Consistency Maintenance 
4.1   Data Structure for Content Management 
In this system, a publisher of a document digitally 
signs a document with its private key and registers its 
sign to the central search server with its unique ID. 
When a document is a text document, a client per-
forms morphological analysis to generate search key-
words from a document. 
The ID of contents and digital signs corresponding 
to different versions are managed on the central 
search server. Using the ID and version, a client can 
obtain a digital sign for a document by querying to the 
central server using its ID and version. Using a digital 
sign ensures that a malicious client does not tamper 
with a document. 
A search result obtained from the central server is 
also digitally signed to ensure that a client does not 
tamper with it. As described in detail in section 5, a 
search result is cached on a client and can be modified. 
To prevent this, a search result comprises the ID of 
contents and a digital sign. 
In this system, a client can obtain the latest version 
of a document when a document is updated, by query-
ing its ID to the central server. However, a limitation 
associated with this method is that only the latest 
version of documents can be obtained. For example, 
by using indirect files and hash values of contents as 
in Freenet, we can obtain previous versions of a 
document by directly specifying a hash value of an 
earlier version. However, neither does Freenet assure 
that the latest version is always obtained nor does it 
assure that a particular earlier version is obtained 
because a previous version may be deleted if there is 
no request for it in a certain period. In our system, we 
consider only the latest version of a document which 
can be obtained at any time. Thus, we define our 
document query protocol in order to obtain the latest 
version. 
In order to prevent the concentration of download 
requests on a certain client, our system manages a list 
of clients that have downloaded the latest version of a 
document and distributes download sources to these 
clients using this list. 
In this method, the ID of a client that downloads 
the latest version of a document is added to a list; this 
ID corresponds with the ID of the document. When a 
client sends a request to the central server to 
download a document, the central server selects an 
appropriate client from a downloader?s list and returns 
its ID to the client. When the publisher updates a 
document, the list corresponding to that document is 
emptied. 
We describe this procedure by the following 
pseudo codes, where download is a function that 
requests the download of a document, nodeId is the 
ID of a client that requests the download, update is 
a function that requests the update of a document, and 
getNodeId is a function that gets the ID of a client 
that downloads a document whose ID is docId.  
  nodeIdList: document ID x node ID list  
 
  download(docId, nodeId) { 
    nodeIdList[docId].add(nodeId); } 
 
  update(docId, nodeId) { 
    nodeIdList[docId] = {nodeId}; } 
 
  getNodeId(docId) { 
    index = rand() * nodeIdList[docId].length; 
    return nodeIdList[docId][index]; } 
4.2 Tracing How Contents are Exchanged 
In a P2P content sharing system that uses a simple 
download protocol, such as Napster, when a service 
- Client public keys 
- Contents certificate 
- Links to contents 
- Full-text search index 
- Contents 
Figure 1. System Architecture
Central
server
Client
Client Client 
27
provider keeps records about how contents are trans-
ferred among clients, there exist possibilities of a 
client tampering with such records by sending false 
information about downloading content to the central 
server. 
For example, by Napster protocol, a request to start 
a download that is sent when a client begins to 
download from another client is the information that 
the central server receives from the client. Therefore, 
the central server can obtain the same information in 
the case when a download source does not transfer a 
document as well as in the case when a download 
source transfers a document successfully. 
To avoid this problem, our system uses a download 
protocol that employs the public keys of clients man-
aged on the central server. The protocol is described 
as follows wherein a download destination client is 
denoted as client A and a download source client as 
client B. 
1 Client A sends a download request to the central 
server. The central server generates a common 
encryption key and sends it to client B. 
2 Client B encrypts the requested content with a 
common encryption key, signs it digitally, and 
sends the encrypted content to client A. 
3 Client A confirms that the downloaded content 
has been signed by client B. Client A then sends a 
request for the common encryption key to the 
central server. The central server records that the 
content is downloaded. 
4 Client A decrypts the downloaded content with 
the common encryption key. Client A then veri-
fies that the downloaded content is not altered us-
ing digital sign on the central server.  
5 If the downloaded content is altered, client A 
sends the downloaded data with a sign of client B 
to the central server. The central server can then 
confirm that it is signed by client B and is altered. 
The central server can then cancel the download 
record created in step 3. 
By this protocol, the following properties are satis-
fied: 
? A content download is recorded on the central 
server as long as a download source client fol-
lows the above protocol. 
? The central server does not create a record when 
a document is not downloaded by a client.  
When a download source client encrypts a docu-
ment with a common encryption key following the 
protocol, a download destination client has to send a 
request for a common key to the central server. Thus, 
the central server can record a download. As a result, 
the first property is satisfied. 
Further, when a client that downloaded a document 
sends a request for a common key, it obtains a sign of 
a download source client for the document. When the 
downloaded content is altered or different from the 
requested one, a download record can be cancelled by 
sending the downloaded data to the central server. 
Thus, the second property is satisfied. 
However, even with this protocol, in the case when 
both a download source client and a destination client 
do not follow this protocol, the central server cannot 
record downloads of contents, for example, in the 
case where a download source client does not encrypt 
the content with a common key. Currently, our system 
does not handle such situations. We would like to 
consider this problem in our future work. In order to 
handle such situations, we evaluated the credibility of 
clients from download histories and selected a credi-
ble client as a download source. 
5   Full-text Search 
5.1 Load Balancing of Full-text Search 
To reduce the load of full-text search on the central 
server, our system uses a caching technique to cache 
the search results of clients. It has been reported that 
approximately 30% to 40% of search requests are 
repeated on a full-text search engine [9]. Therefore, a 
caching technique is expected to considerably reduce 
the full-text search load. We employed an applied 
form of a hash-based caching method, as described in 
 [3]. In this section, we describe the manner in which a 
full-text search is performed and search results are 
cached. 
The central server selects a fixed number of clients 
as caches for the search results that connect for a long 
duration. The central server assigns them to the 
equally divided range of a hash function. A client 
obtains a list of caches when it connects to the central 
server. When a client performs a search, it calculates 
the hash value of a search keyword and sends a re-
quest to the cache assigned to a section of the range 
containing the hash value of the keyword. 
In an experiment described later, SHA1 is used as a 
hash function for clients IDs and search keywords. By 
comparing several upper bits of hash values, we im-
plement equally divided range of a hash function. 
If a cache does not have a search result for a search 
keyword, it forwards the search request to the central 
server. The central server then returns the result to the 
cache with a search keyword, search time, and digital 
sign. The search time and digital sign that a client 
receives along with the search result from a cache can 
confirm that the result is not stale and not tampered 
with by a cache. 
When a client sends a search request to a cache and 
detects that a cache is not available because it is con-
nected to the network or is overloaded, the request 
sending client marks that it is not available on a list of 
caches. It then sends the search request to a cache 
assigned to the next section of the hash range. When 
the number of unavailable caches exceeds a certain 
28
fixed number, it requests the central server for the 
most recent list of caches and updates it.  
5.2   A Load Balancing Experiment 
It is reported that the number of search requests for 
each keyword follows Zipf distribution  [9]. Therefore, 
when we increase the number of caches by sub-
dividing the range of a hash function, it is possible 
that search requests are concentrated to a certain 
cache. 
On the other hand, if we increase the number of 
caches by maintaining the number of sections of the 
range of a hash function and increasing the number of 
clients assigned to each section, the cache hit ratio 
would decrease. 
In this research, a preliminary experiment is per-
formed in order to verify the advantages and disad-
vantages of these alternatives. 
First, we generate a list of search keywords so that 
the number of requests for each keyword follows Zipf 
distribution. In this list, 40% of the queries are re-
peated and requests of the most frequently repeated 
word correspond to 0.2% of the entire range of re-
quests. These numbers follow the search trace of 
Excite, as reported in  [9]. 
If all search results corresponding to keywords in 
this list can be cached, the cache hit ratio would be 
40%. 
Using this list, we measure the cache hit ratio, re-
quired cache capacity, and the number of search re-
quest counts for each cache client in the following 
three cases: (1) the range of a hash function is divided 
into 256 sections, a single client is assigned to each 
section, and 100,000 requests are sent; (2) the range 
of a hash function is divided into 1024 sections, a 
single client is assigned to each section, and 400,000 
requests are sent; (3) the range of a hash function is 
divided into 256 sections, 4 clients are assigned to 
each section, and 400,000 requests are sent. We as-
sume that the same capacity is required to cache a 
search result for any keyword. Therefore, the number 
of keywords that are requested more than twice are 
counted as the required cache capacity.  
Figure 2 shows the experimental results. This 
graph shows the number of requests to each cache that 
is sorted in a descending order. In order to compare 
these three cases, the scale of the x-axis of the result 
of case (1) is expanded by four times. 
In case (1), although the range of the hash function 
is divided equally, the number of requests to the most 
frequently requested cache is less than twice that of 
the least frequently requested one. This shows that 
both the hash value of a frequently searched keyword 
and a not so frequently searched one are likely to be 
contained in one interval when the range is divided 
into relatively large intervals. Thus, the search re-
quests for each interval are balanced. This result also 
shows that the cache hit ratio in case (1) is relatively 
high. 
When the number of caches and search requests are 
increased from that in case (1) and when the range is 
divided into smaller intervals, as in case (2), search 
requests are concentrated to a certain cache, as shown 
in Table 1. This shows that the search load becomes 
unbalanced among the caches. However, in compari-
son to case (1), the cache hit ratio is improved be-
cause the number of search requests is increased in 
this case. As shown in Figure 1, the search loads are 
well balanced between most of the caches except a 
few caches where the search requests are concentrated 
in case (2). In order to balance the search load, we 
employed certain techniques to forward the search 
requests from overloaded caches to others. Currently, 
the above results only measure the number of search 
requests and do not consider the search load on caches. 
As a future work, we intend to perform a quantitative 
experiment of search load balancing under the condi-
tion when our system would forward search requests 
from overloaded caches to others. 
When the number of clients assigned to each sec-
tion increases, as in case (3), the search load is well 
balanced as shown in Figure 2, which is similar to that 
of case (1). However, the cache hit ratio decreases 
remarkably. In order to increase the cache hit ratio in 
this situation, other techniques such as hierarchical 
cache would have to be used. 
In this experiment, we assume that the appearance 
ratio of repeated queries is the same.  However, due to 
Figure 2. Distribution of Search Requests 
0
200
400
600
800
1000
1200
1400
(1) 256 clients 100,000 requests
(2) 1024 clients 400,000 requests
(3) 256x4 clients 400,000 requests
Table 1: Result of Experiment 
  max searchrequests 
cache hit 
ratio 
av. cache
capacity 
(1) 256 557 20.6% 75.7 
(2) 1024 1193 24.3% 61.4 
(3) 256?4 578 10.6% 42.1 
 
29
limited vocabulary of the users, the ratio of repeated 
queries in the entire range of queries is expected to 
increase with the number of queries. Owing to this, 
we can expect that the cache hit ratio does not de-
crease when the number of queries increases. 
6   Conclusion 
In this paper, some problems regarding currently 
available P2P content sharing systems such as content 
consistency maintenance and information search have 
been pointed out. We proposed techniques in order to 
solve these problems and described the outline of the 
P2P content sharing system that we are developing. 
This system uses a central server on which the digi-
tal signs of contents publishers are maintained to 
prevent the tampering of contents. Further, this sys-
tem adopts a content transfer protocol that employs 
asymmetric encryption keys. This protocol enables us 
to record content exchanges between clients on the 
central server. On the contrary, since most other P2P 
content sharing systems do not employ sufficient 
techniques to maintain records of content exchanges, 
such records would be unreliable. In particular, it is 
difficult to maintain such records in decentralized P2P 
content sharing systems. 
In order to solve the problem regarding information 
search, we propose full-text search techniques for P2P 
content sharing systems. First, morphological analysis 
and summarizing of documents are performed for 
each client and the results are sent to the central server 
to generate reverse indexes. We can implement a 
relatively efficient full-text search with this technique. 
More efficient and scalable full-text search techniques 
based on DHT are currently being researched. It is 
difficult to implement partial matching or AND search 
by these techniques; however, they can be easily im-
plemented with our method. In order to reduce the 
search load on the central search server, we propose a 
load balance technique that caches search results on 
clients and uses a hash function to distribute search 
requests to clients. In one of the experiments carried 
out, it was seen that the search requests were satisfac-
torily balanced, and the cache hit ratio was relatively 
high for a considerably large set of search requests 
that follow Zipf distribution. 
We believe that content consistency maintenance 
and efficient full-text search on P2P content sharing 
systems can be implemented using our techniques. 
As a future work, we would like to consider novel 
techniques to handle cases where multiple clients do 
not follow our content transfer protocol. In addition, 
we believe it is necessary to quantitatively evaluate 
our implementation to confirm that our system func-
tions well in a practically large-scale environment. 
With respect to the caching technique, we have to 
improve our load balancing technique to avoid some 
clients from being overloaded. 
Acknowledgements 
Hideki Mima and Hideto Tomabechi express their 
gratitude to the Ministry of Internal Affairs and 
Communications for promoting the study in part un-
der the SCOPE R&D grant scheme. 
References 
[1] P. Reynolds and A. Vahdat. Efficient Peer-to-Peer 
Keyword Searching. Middleware 2003. 
[2] T. Hong. Freenet: A distributed anonymous infor-
mation storage and retrieval system. In ICSI Work-
shop on Design Issues in Anonymity and Unob-
servability, 2000. 
[3] David R. Karger, Eric Lehman, Frank Thomson 
Leighton, Rina Panigrahy, Matthew S. Levine, and 
Daniel Lewin. Consistent hashing and random 
trees: Distributed caching protocols for relieving 
hot spots on the World Wide Web. In ACM Sympo-
sium on Theory of Computing, pages 654-663, 1997. 
[4] Napster. http://www.napster.com/,  
http://opennap.sourceforge.net/. 
[5] Ion Stoica, Robert Morris, David Karger, M. Frans 
Kaashoek, and Hari Balakrishnan. Chord: A scal-
able peer-to-peer lookup service for Internet appli-
cations. In Proceedings of ACM SIGCOMM?01, 
2001. 
[6] Gnutella. http://gnutella.wego.com/. 
[7] Tylvia Ratnasamy, Paul Francis, Mark Handley, 
Richard Karp, and Scott Shenker. A scalable con-
tent-addressable network. In Proceedings of ACM 
SIGCOMM?01, 2001. 
[8] Jinyang Li, Boon Thau Loo, Joseph M. Hellerstein, 
M. Frans Kaashoek, David R. Karger, Robert Mor-
ris. On the feasibility of peer-to-peer web indexing 
and search. In 2nd International Workshop on 
Peer-to-Peer Systems, 2003. 
[9] Yinglian Xie and David O'Hallaron. Locality in 
search engine queries and its implications for cach-
ing. IEEE Infocom 2002, 2002. 
[10] Yasuaki Takebe, Hideki Mima, Hideto Tomabechi. 
A next-generation P2P contents sharing system?
implementing content consistency maintenance and 
full-text search. In 11th DPS Workshop, 2003. 
30
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 21?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
MIMA Search: A Structuring Knowledge System  
towards Innovation for Engineering Education 
Hideki Mima 
School of Engineering 
University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan  
mima@t-adm.t.u-tokyo.ac.jp 
Abstract 
The main aim of the MIMA (Mining In-
formation for Management and Acquisi-
tion) Search System is to achieve ?struc-
turing knowledge? to accelerate knowl-
edge exploitation in the domains of sci-
ence and technology. This system inte-
grates natural language processing includ-
ing ontology development, information 
retrieval, visualization, and database tech-
nology. The ?structuring knowledge? that 
we define indicates 1) knowledge storage, 
2) (hierarchical) classification of knowl-
edge, 3) analysis of knowledge, 4) visu-
alization of knowledge. We aim at inte-
grating different types of databases (pa-
pers and patents, technologies and innova-
tions) and knowledge domains, and simul-
taneously retrieving different types of 
knowledge. Applications for the several 
targets such as syllabus structuring will 
also be mentioned. 
1 Introduction 
The growing number of electronically available 
knowledge sources (KSs) emphasizes the impor-
tance of developing flexible and efficient tools for 
automatic knowledge acquisition and structuring 
in terms of knowledge integration. Different text 
and literature mining techniques have been de-
veloped recently in order to facilitate efficient 
discovery of knowledge contained in large textual 
collections. The main goal of literature mining is 
to retrieve knowledge that is ?buried? in a text 
and to present the distilled knowledge to users in 
a concise form. Its advantage, compared to ?man-
ual? knowledge discovery, is based on the as-
sumption that automatic methods are able to 
process an enormous amount of text. It is doubt-
ful that any researcher could process such a huge 
amount of information, especially if the knowl-
edge spans across domains. For these reasons, 
literature mining aims at helping scientists in col-
lecting, maintaining, interpreting and curating 
information. 
In this paper, we introduce a knowledge struc-
turing system (KSS) we designed, in which ter-
minology-driven knowledge acquisition (KA), 
knowledge retrieval (KR) and knowledge visuali-
zation (KV) are combined using automatic term 
recognition, automatic term clustering and termi-
nology-based similarity calculation is explained. 
The system incorporates our proposed automatic 
term recognition / clustering and a visualization 
of retrieved knowledge based on the terminology, 
which allow users to access KSs visually though 
sophisticated GUIs. 
2 Overview of the system 
The main purpose of the knowledge structuring 
system is 1) accumulating knowledge in order to 
develop huge knowledge bases, 2) exploiting the 
accumulated knowledge efficiently. Our approach 
to structuring knowledge is based on: 
? automatic term recognition (ATR) 
? automatic term clustering (ATC) as an ontol-
ogy1 development 
? ontology-based similarity calculation 
? visualization of relationships among docu-
ments (KSs) 
One of our definitions to structuring knowledge is 
discovery of relevance between documents (KSs) 
and its visualization. In order to achieve real time 
processing for structuring knowledge, we adopt 
terminology / ontology-based similarity calcula-
tion, because knowledge  can also be represented 
as textual documents or passages (e.g. sentences, 
subsections) which are efficiently characterized 
by sets of specialized (technical) terms. Further 
details of our visualization scheme will be men-
tioned in Section 4. 
                                                 
1  Although, definition of ontology is domain-
specific, our definition of ontology is the collection 
and classification of (technical) terms to recognize 
their semantic relevance. 
21
The system architecture is modular, and it inte-
grates the following components (Figure 1):  
- Ontology Development Engine(s) (ODE) ? 
components that carry out the automatic ontol-
ogy development which includes recognition 
and structuring of domain terminology; 
- Knowledge Data Manager (KDM) ? stores in-
dex of KSs and ontology in a ontology informa-
tion database (OID) and provides the corre-
sponding interface; 
- Knowledge Retriever (KR) ? retrieves KSs from 
TID and calculates similarities between key-
words and KSs. Currently, we adopt tf*idf 
based similarity calculation; 
- Similarity Calculation Engine(s) (SCE) ? calcu-
late similarities between KSs provided from KR 
component using ontology developed by ODE 
in order to show semantic similarities between 
each KSs. We adopt Vector Space Model 
(VSM) based similarity calculation and use 
terms as features of VSM. Semantic clusters of 
KSs are also provided. 
- Graph Visualizer ? visualizes knowledge struc-
tures based on graph expression in which rele-
vance links between provided keywords and 
KSs, and relevance links between the KSs 
themselves can be shown. 
3 Terminological processing as an ontol-
ogy development 
The lack of clear naming standards in a domain 
(e.g. biomedicine) makes ATR a non-trivial prob-
lem (Fukuda et al, 1998). Also, it typically gives 
rise to many-to-many relationships between terms 
and concepts. In practice, two problems stem 
from this fact: 1) there are terms that have multi-
ple meanings (term ambiguity), and, conversely, 
2) there are terms that refer to the same concept 
(term variation). Generally, term ambiguity has 
negative effects on IE precision, while term varia-
tion decreases IE recall. These problems show the 
difficulty of using simple keyword-based IE 
techniques. Obviously, more sophisticated tech-
niques, identifying groups of different 
terms referring to the same (or similar) 
concept(s), and, therefore, could benefit 
from relying on efficient and consistent 
ATR/ATC and term variation manage-
ment methods are required. These meth-
ods are also important for organising do-
main specific knowledge, as terms should 
not be treated isolated from other terms. 
They should rather be related to one an-
other so that the relations existing between 
the corresponding concepts are at least 
partly reflected in a terminology. 
3.1 Term recognition 
The ATR method used in the system is based on 
the C / NC-value methods (Mima et al, 2001; 
Mima and Ananiadou, 2001). The C-value 
method recognizes terms by combining linguistic 
knowledge and statistical analysis. The method 
extracts multi-word terms2 and is not limited to a 
specific class of concepts. It is implemented as a 
two-step procedure. In the first step, term candi-
dates are extracted by using a set of linguistic fil-
ters which describe general term formation pat-
terns. In the second step, the term candidates are 
assigned termhood scores (referred to as C-
values) according to a statistical measure. The 
measure amalgamates four numerical corpus-
based characteristics of a candidate term, namely 
the frequency of occurrence, the frequency of 
occurrence as a substring of other candidate terms, 
the number of candidate terms containing the 
given candidate term as a substring, and the num-
ber of words contained in the candidate term. 
The NC-value method further improves the C-
value results by taking into account the context of 
candidate terms. The relevant context words are 
extracted and assigned weights based on how fre-
quently they appear with top-ranked term candi-
dates extracted by the C-value method. Subse-
quently, context factors are assigned to candidate 
terms according to their co-occurrence with top-
ranked context words. Finally, new termhood es-
timations, referred to as NC-values, are calculated 
as a linear combination of the C-values and con-
text factors for the respective terms. Evaluation of 
the C/NC-methods (Mima and Ananiadou, 2001) 
has shown that contextual information improves 
term distribution in the extracted list by placing 
real terms closer to the top of the list. 
                                                 
2 More than 85% of domain-specific terms are multi-word 
terms (Mima and Ananiadou, 2001). 
Figure 1: The system architecture 
 B
r
o
w
s
e
r 
 
G
U
I 
KSs 
PDF, Word, HTML, 
XML, CSV 
 Data ReaderDocument 
Viewer  
Ontology Data 
Manager 
 Knowledge 
Retriever 
 Similarity
Manager
   ????? 
???? 
 SimilarityCalculation
Engine 
Similarity 
Graph 
Visualizer 
     Ontology
Development
Engine 
 Summarizer 
Browser 
Interface 
 Knowledge Data
Manager 
Ontology Information 
Database Database 
Similarity Processing Ontology Development 
22
3.2 Term variation management 
Term variation and ambiguity are causing prob-
lems not only for ATR but for human experts as 
well. Several methods for term variation man-
agement have been developed. For example, the 
BLAST system Krauthammer et al, 2000) used 
approximate text string matching techniques and 
dictionaries to recognize spelling variations in 
gene and protein names. FASTR (Jacquemin, 
2001) handles morphological and syntactic varia-
tions by means of meta-rules used to describe 
term normalization, while semantic variants are 
handled via WordNet. 
The basic C-value method has been enhanced 
by term variation management (Mima and 
Ananiadou, 2001). We consider a variety of 
sources from which term variation problems 
originate. In particular, we deal with orthographi-
cal, morphological, syntactic, lexico-semantic and 
pragmatic phenomena. Our approach to term 
variation management is based on term normali-
zation as an integral part of the ATR process. 
Term variants  (i.e. synonymous terms) are dealt 
with in the initial phase of ATR when term can-
didates are singled out, as opposed to other ap-
proaches (e.g. FASTR handles variants subse-
quently by applying transformation rules to ex-
tracted terms). Each term variant is normalized 
(see table 1 as an example) and term variants hav-
ing the same normalized form are then grouped 
into classes in order to link each term candidate to 
all of its variants. This way, a list of normalized 
term candidate classes, rather than a list of single 
terms is statistically processed. The termhood is 
then calculated for a whole class of term variants, 
not for each term variant separately. 
Table 1: Automatic term normalization 
Term variants  Normalised term 
human cancers 
cancer in humans 
human?s cancer 
human carcinoma 
}?  human cancer 
3.3 Term clustering 
Beside term recognition, term clustering is an 
indispensable component of the literature mining 
process. Since terminological opacity and 
polysemy are very common in molecular biology 
and biomedicine, term clustering is essential for 
the semantic integration of terms, the construction 
of domain ontologies and semantic tagging.  
ATC in our system is performed using a hierar-
chical clustering method in which clusters are 
merged based on average mutual information 
measuring how strongly terms are related to one 
another (Ushioda, 1996). Terms automatically 
recognized by the NC-value method and their co-
occurrences are used as input, and a dendrogram 
of terms is produced as output. Parallel symmet-
ric processing is used for high-speed clustering. 
The calculated term cluster information is en-
coded and used for calculating semantic similari-
ties in SCE component. More precisely, the simi-
larity between two individual terms is determined 
according to their position in a dendrogram. Also 
a commonality measure is defined as the number 
of shared ancestors between two terms in the 
dendrogram, and a positional measure as a sum of 
their distances from the root. Similarity between 
two terms corresponds to a ratio between com-
monality and positional measure.   
Further details of the methods and their evalua-
tions can be referred in (Mima et al, 2001; Mima 
and Ananiadou, 2001). 
4 Structuring knowledge 
Structuring knowledge can be regarded as a 
broader approach to IE/KA. IE and KA in our 
system are implemented through the integration 
of ATR, ATC, and ontology-based semantic simi-
larity calculation. Graph-based visualization for 
globally structuring knowledge is also provided 
to facilitate KR and KA from documents. Addi-
tionally, the system supports combining different 
databases (papers and patents, technologies and 
innovations) and retrieves different types of 
knowledge simultaneously and crossly. This fea-
ture can accelerate knowledge discovery by com-
bining existing knowledge. For example, discov-
ering new knowledge on industrial innovation by 
structuring knowledge of trendy scientific paper 
database and past industrial innovation report da-
tabase can be expected. Figure 3 shows an exam-
ple of visualization of knowledge structures in the 
 
P O S  ta g g e r  
A c r o n y m r e c o g n i t io n
C - v a lu e  A T R
O r th o g r a p h i c  v a r ia n t s
M o r p h o l o g i c a l  v a r i a n t s  
S y n ta c t i c  v a r ia n t s
N C - v a lu e  A T R
T e r m  c lu s t e r in g  
X M L  d o c u m e n t s  in c l u d in g  
t e r m  t a g s  a n d  t e r m  
v a r ia t io n / c l a s s  in fo r m a t io n  
In p u t  d o c u m e n t s
R e c o g n i t io n   
o f  t e r m s  
S t r u c t u r i n g   
o f  t e r m s  
 
Figure 2: Ontology development 
23
domain of engineering. In order to structure 
knowledge, the system draws a graph in which 
nodes indicate relevant KSs to keywords given 
and each links between KSs indicates semantic 
similarities dynamically calculated using ontol-
ogy information developed by our ATR / ATC 
components. 
 
Figure 3: Visualization 
5 Conclusion 
In this paper, we presented a system for structur-
ing knowledge over large KSs. The system is a 
terminology-based integrated KA system, in 
which we have integrated ATR, ATC, IR, simi-
larity calculation, and visualization for structuring 
knowledge. It allows users to search and combine 
information from various sources. KA within the 
system is terminology-driven, with terminology 
information provided automatically. Similarity 
based knowledge retrieval is implemented 
through various semantic similarity calculations, 
which, in combination with hierarchical, ontol-
ogy- based matching, offers powerful means for 
KA through visualization-based literature mining. 
We have applied the system to syllabus re-
trieval for The University of Tokyo`s Open 
Course Ware (UT-OCW)3 site and syllabus struc-
turing (SS) site4 for school / department of engi-
neering at University of Tokyo, and they are both 
available in public over the Internet. The UT-
OCW?s MIMA Search system is designed to 
search the syllabuses of courses posted on the 
UT-OCW site and the Massachusetts Institute of 
Technology's OCW site (MIT-OCW).  Also, the 
SS site?s MIMA Search is designed to search the 
syllabuses of lectures from more than 1,600 lec-
tures in school / department of engineering at 
University of Tokyo. Both systems show search 
results in terms of relations among the syllabuses 
as a structural graphic (figure 3). Based on the 
automatically extracted terms from the syllabuses 
and similarities calculated using those terms, 
MIMA Search displays the search results in a 
network format, using dots and lines. Namely, 
                                                 
3 http://ocw.u-tokyo.ac.jp/. 
4 http://ciee.t.u-tokyo.ac.jp/. 
MIMA Search extracts the contents from the 
listed syllabuses, rearrange these syllabuses ac-
cording to semantic relations of the contents and 
display the results graphically, whereas conven-
tional search engines simply list the syllabuses 
that are related to the keywords. Thanks to this 
process, we believe users are able to search for 
key information and obtain results in minimal 
time. In graphic displays, as already mentioned, 
the searched syllabuses are shown in a structural 
graphic with dots and lines. The stronger the se-
mantic relations of the syllabuses, the closer they 
are placed on the graphic. This structure will help 
users find a group of courses / lectures that are 
closely related in contents, or take courses / lec-
tures in a logical order, for example, beginning 
with fundamental mathematics and going on to 
applied mathematics. Furthermore, because of the 
structural graphic display, users will be able to 
instinctively find the relations among syllabuses 
of other universities.  
Currently, we obtain more than 2,000 hits per 
day in average from all over the world, and have 
provided more then 50,000 page views during last 
three months. On the other hand, we are in a 
process of system evaluation using more than 40 
students to evaluate usability as a next generation 
information retrieval.  
The other experiments we conducted also show 
that the system?s knowledge structuring scheme 
is an efficient methodology to facilitate KA and 
new knowledge discovery in the field of genome 
and nano-technology (Mima et al, 2001). 
References 
K. Fukuda, T. Tsunoda, A. Tamura, T. Takagi, 1998. 
Toward information extraction: identifying protein 
names from biological papers, Proc. of PSB-98, 
Hawaii, pp. 3:705-716. 
H. Mima, S. Ananiadou, G. Nenadic, 2001. ATRACT 
workbench: an automatic term recognition and clus-
tering of terms, in: V. Matou?ek, P. Mautner, R. 
Mou?ek, K. Tau?er (Eds.) Text, Speech and Dia-
logue, LNAI 2166, Springer Verlag, pp. 126-133. 
H. Mima, S. Ananiadou, 2001. An application and 
evaluation of the C/NC-value approach for the 
automatic term recognition of multi-word units in 
Japanese, Int. J. on Terminology 6/2, pp. 175-194. 
M. Krauthammer, A. Rzhetsky, P. Morozov, C. 
Friedman, 2000. Using BLAST for identifying gene 
and protein names in journal articles, in: Gene 259, 
pp. 245-252. 
C. Jacquemin, 2001. Spotting and discovering terms 
through NLP, MIT Press, Cambridge MA, p. 378. 
A. Ushioda, 1996. Hierarchical clustering of words, 
Proc. of COLING ?96, Copenhagen, Denmark, pp. 
1159-1162. 
24
Design and Implementation of a Terminology-based 
Literature Mining and Knowledge Structuring System 
 
Hideki Mima 
School of Engineering 
University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 
113-0033, Japan  
mima@biz-model.t.u-tokyo.ac.jp 
Sophia Ananiadou 
School of Computing, Science and 
Engineering, University of Salford, 
Salford M5 4WT, UK 
National Centre for Text Mining 
S.Ananiadou@salford.ac.uk 
Katsumori Matsushima 
School of Engineering 
University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, 
Tokyo 113-0033, Japan  
matsushima@naoe.t.u-tokyo.ac.jp
 
 
Abstract 
The purpose of the study is to develop an 
integrated knowledge management system for the 
domains of genome and nano-technology, in 
which terminology-based literature mining, 
knowledge acquisition, knowledge structuring, 
and knowledge retrieval are combined. The system 
supports integrating different databases (papers 
and patents, technologies and innovations) and 
retrieving different types of knowledge 
simultaneously. The main objective of the system 
is to facilitate knowledge acquisition from 
documents and new knowledge discovery through 
a terminology-based similarity calculation and a 
visualization of automatically structured 
knowledge. Implementation issues of the system 
are also mentioned. 
Key Words: Structuring knowledge, knowledge 
acquisition, information extraction, natural 
language processing, automatic term recognition, 
terminology 
1. Introduction 
The growing number of electronically 
available knowledge sources (KSs) 
emphasizes the importance of developing 
flexible and efficient tools for automatic 
knowledge acquisition and structuring in 
terms of knowledge integration. Different 
text and literature mining techniques have 
been developed recently in order to facilitate 
efficient discovery of knowledge contained 
in large textual collections. The main goal of 
literature mining is to retrieve knowledge 
that is ?buried? in a text and to present the 
distilled knowledge to users in a concise 
form. Its advantage, compared to ?manual? 
knowledge discovery, is based on the 
assumption that automatic methods are 
able to process an enormous amount of 
texts. It is doubtful that any researcher 
could process such huge amount of 
information, especially if the knowledge 
spans across domains. For these reasons, 
literature mining aims at helping scientists 
in collecting, maintaining, interpreting and 
curating information. 
In this paper, we introduce a knowledge 
integration and structuring system (KISS) 
we designed, in which terminology-driven 
knowledge acquisition (KA), knowledge 
retrieval (KR) and knowledge visualization 
(KV) are combined using automatic term 
recognition, automatic term clustering and 
terminology-based similarity calculation is 
explained. The system incorporates our 
proposed automatic term recognition / 
clustering and a visualization of retrieved 
knowledge based on the terminology, which 
allow users to access KSs visually though 
sophisticated GUIs. 
2. Overview of the system  
The main purpose of the knowledge 
structuring system is 1) accumulating 
knowledge in order to develop huge 
knowledge bases, 2) exploiting the 
accumulated knowledge efficiently. Our 
approach to structuring knowledge is based 
on: 
z automatic term recognition (ATR) 
z automatic term clustering (ATC) as an 
ontology1 development 
z ontology-based similarity calculation 
z visualization of relationships among 
documents (KSs) 
One of our definitions to structuring 
knowledge is discovery of relevance between 
documents (KSs) and its visualization. In 
order to achieve real time processing for 
structuring knowledge, we adopt 
terminology / ontology-based similarity 
calculation, because knowledge  can also be 
represented as textual documents or 
passages (e.g. sentences, subsections) which 
are efficiently characterized by sets of 
specialized (technical) terms. Further details 
of our visualization scheme will be 
mentioned in Section 4. 
                                                   
1  Although, definition of ontology is domain-
specific, our definition of ontology is the 
collection and classification of (technical) terms 
to recognize their semantic relevance. 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 83
The system architecture is modular, and 
it integrates the following components 
(Figure 1):  
- Ontology Development Engine(s) (ODE) ? 
components that carry out the automatic 
ontology development which includes 
recognition and structuring of domain 
terminology; 
- Knowledge Data Manager (KDM) ? stores 
index of KSs and ontology in a ontology 
information database (OID) and provides 
the corresponding interface; 
- Knowledge Retriever (KR) ? retrieves KSs 
from TID and calculates similarities 
between keywords and KSs. Currently, we 
adopt tf*idf based similarity calculation; 
- Similarity Calculation Engine(s) (SCE) ? 
calculate similarities between KSs 
provided from KR component using 
ontology developed by ODE in order to 
show semantic similarities between each 
KSs. Semantic clusters of KSs are also 
provided. 
- Graph Visualizer ? visualizes knowledge 
structures based on graph expression in 
which relevance links between provided 
keywords and KSs, and relevance links 
between the KSs themselves can be 
shown. 
Linguistic pre-processing within the 
system is performed in two steps. In the 
first step, POS tagging2, i.e. the assignment 
of basic parts of speech (e.g. noun, verb, 
etc.) to words, is performed. In the second 
step, an ontology development engine is 
used to perform ATR and ATC. We also 
used feature structure-based parsing for 
English and Japanese for linguistic filter of 
the ATR. 
 
                                                   
2 We use EngCG tagger[4] in English and 
JUMAN / Chasen morphological analyzers in 
Japanese. 
3. Terminological processing 
as an ontology development 
The lack of clear naming 
standards in a domain (e.g. 
biomedicine) makes ATR a non-
trivial problem [1]. Also, it typically 
gives rise to many-to-many 
relationships between terms and 
concepts. In practice, two problems 
stem from this fact: 1) there are 
terms that have multiple meanings 
(term ambiguity), and, conversely, 2) 
there are terms that refer to the 
same concept (term variation). Generally, 
term ambiguity has negative effects on IE 
precision, while term variation decreases IE 
recall. These problems point out the 
impropriety of using simple keyword-based 
IE techniques. Obviously, more 
sophisticated techniques, identifying groups 
of different terms referring to the same (or 
similar) concept(s), and, therefore, could 
benefit from relying on efficient and 
consistent ATR/ATC and term variation 
management methods are required. These 
methods are also important for organising 
domain specific knowledge, as terms should 
not be treated isolated from other terms. 
They should rather be related to one another 
so that the relations existing between the 
corresponding concepts are at least partly 
reflected in a terminology. 
Terminological processing in our system 
is carried out based on C / NC-value method 
[2,3] for ATR, and average mutual 
information based ATC (Figure 2). 
3.1. Term recognition 
The ATR method used in the system is 
based on the C / NC-value methods [2,3]. 
The C-value method recognizes terms by 
combining linguistic knowledge and 
statistical analysis. The method extracts 
multi-word terms3 and is not limited to a 
specific class of concepts. It is implemented 
as a two-step procedure. In the first step, 
term candidates are extracted by using a set 
of linguistic filters, implemented using a 
LFG-based GLR parser, which describe 
general term formation patterns. In the 
second step, the term candidates are 
assigned termhoods (referred to as C-values) 
according to a statistical measure. The 
measure amalgamates four numerical 
corpus-based characteristics of a candidate 
                                                   
3 More than 85% of domain-specific terms are 
multi-word terms [3]. 
Figure 1: The system architecture 
 Br
o
w
s
e
r 
G
U
I 
KSs 
PDF, Word, HTML, 
XML, CSV 
Data ReaderDocument 
Viewer 
Ontology Data 
Manager 
Knowledge 
Retriever 
Similarity 
Manager 
 
 ?????  
 
Similarity 
Calculation 
Engine 
Similarity 
Graph 
Visualizer 
     
Ontology
Development
Engine 
Summarizer 
Browser 
Interface 
Knowledge Data
Manager 
Ontology Information 
Database Database 
Similarity Processing Ontology Development 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology84
term, namely the frequency of occurrence, 
the frequency of occurrence as a substring 
of other candidate terms, the number of 
candidate terms containing the given 
candidate term as a substring, and the 
number of words contained in the 
candidate term. 
The NC-value method further improves 
the C-value results by taking into account 
the context of candidate terms. The relevant 
context words are extracted and assigned 
weights based on how frequently they 
appear with top-ranked term candidates 
extracted by the C-value method. 
Subsequently, context factors are assigned 
to candidate terms according to their co-
occurrence with top-ranked context words. 
Finally, new termhood estimations, referred 
to as NC-values, are calculated as a linear 
combination of the C-values and context 
factors for the respective terms. Evaluation 
of the C/NC-methods [3] has shown that 
contextual information improves term 
distribution in the extracted list by placing 
real terms closer to the top of the list. 
3.2. Term variation management 
Term variation and ambiguity are 
causing problems not only for ATR but for 
human experts as well. Several methods for 
term variation management have been 
developed. For example, the BLAST system 
[5] used approximate text string matching 
techniques and dictionaries to recognize 
spelling variations in gene and protein 
names. FASTR [6] handles morphological 
and syntactic variations by means of meta-
rules used to describe term normalization, 
while semantic variants are handled via 
WordNet. 
The basic C-value method has been 
enhanced by term variation management 
[2]. We consider a variety of sources from 
which term variation problems originate. In 
particular, we deal with orthographical, 
morphological, syntactic, lexico-semantic 
and pragmatic phenomena. Our approach 
to term variation management is based on 
term normalization as an integral part of 
the ATR process. Term variants  (i.e. 
synonymous terms) are dealt with in the 
initial phase of ATR when term candidates 
are singled out, as opposed to other 
approaches (e.g. FASTR handles variants 
subsequently by applying transformation 
rules to extracted terms). Each term variant 
is normalized (see table 1 as an example) 
and term variants having the same 
normalized form are then grouped into 
classes in order to link each term candidate 
to all of its variants. This way, a list of 
normalized term candidate classes, rather 
than a list of single terms is statistically 
processed. The termhood is then calculated 
for a whole class of term variants, not for 
each term variant separately. 
Table 1: Automatic term normalization 
Term variants  Normalised term 
human cancers 
cancer in humans 
human?s cancer 
human carcinoma 
} ?  human cancer 
3.3. Term clustering 
Beside term recognition, term clustering 
is an indispensable component of the 
literature mining process. Since 
terminological opacity and polysemy are 
very common in molecular biology and 
biomedicine, term clustering is essential for 
the semantic integration of terms, the 
construction of domain ontologies and 
semantic tagging.  
ATC in our system is performed using a 
hierarchical clustering method in which 
clusters are merged based on average 
mutual information measuring how strongly 
terms are related to one another [7]. Terms 
automatically recognized by the NC-value 
method and their co-occurrences are used 
as input, and a dendrogram of terms is 
produced as output. Parallel symmetric 
processing is used for high-speed clustering. 
The calculated term cluster information is 
encoded and used for calculating semantic 
similarities in SCE component. More 
precisely, the similarity between two 
individual terms is determined according to 
their position in a dendrogram. Also a 
commonality measure is defined as the 
number of shared ancestors between two 
terms in the dendrogram, and a positional 
 
P O S  ta g g e r  
A c r o n y m  r e c o g n i t io n  
C - v a lu e  A T R  
O r th o g r a p h ic  v a r i a n t s  
M o r p h o lo g ic a l  v a r i a n t s  
S y n ta c t i c  v a r i a n t s  
N C - v a lu e  A T R  
T e r m  c lu s te r in g   
X M L  d o c u m e n ts  i n c lu d in g  
t e r m  ta g s  a n d  t e r m  
v a r i a t io n /c l a s s  in f o r m a t io n  
I n p u t  d o c u m e n ts  
R e c o g n i t i o n  
o f  t e r m s  
S t r u c t u r in g  
o f  t e r m s  
Figure 2: Ontology development 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 85
measure as a sum of their distances from 
the root. Similarity between two terms 
corresponds to a ratio between 
commonality and positional measure.   
Further details of the methods and their 
evaluations can be referred in [2,3]. 
4. Structuring knowledge 
Literature mining can be regarded as a 
broader approach to IE/KA. IE and KA in 
our system are implemented through the 
integration of ATR, ATC, and ontology-
based semantic similarity calculation. 
Graph-based visualization for globally 
structuring knowledge is also provided to 
facilitate KR and KA from documents. 
Additionally, the system supports 
combining different databases (papers and 
patents, technologies and innovations) and 
retrieves different types of knowledge 
simultaneously and crossly. This feature 
can accelerate knowledge discovery by 
combining existing knowledge. For example, 
discovering new knowledge on industrial 
innovation by structuring knowledge of 
trendy scientific paper database and past 
industrial innovation report database can 
be expected. Figure 3 shows an example of 
visualization of knowledge structures in the 
domain of innovation and engineering. In 
order to structure knowledge, the system 
draws a graph in which nodes indicate 
relevant KSs to keywords given and each 
links between KSs indicates semantic 
similarities dynamically calculated using 
ontology information developed by our ATR 
/ ATC components. Since characterization 
for KSs using terminology is thought to be 
the most efficient and ultimate 
summarization to KSs, achieving a fast and 
just-in-time processing for structuring 
knowledge can be expected.  
5. Conclusion 
In this paper, we presented a system for 
literature mining and knowledge 
structuring over large KSs. The system is a 
terminology-based integrated KA system, in 
which we have integrated ATR, ATC, IR, 
similarity calculation, and visualization for 
structuring knowledge. It allows users to 
search and combine information from 
various sources. KA within the system is 
terminology-driven, with terminology 
information provided automatically. 
Similarity based knowledge retrieval is 
implemented through various semantic 
similarity calculations, which, in 
combination with hierarchical, ontology- 
 
Figure 3: Visualization 
based matching, offers powerful means for 
KA through visualization-based literature 
mining. 
Preliminary experiments we conducted 
show that the system?s knowledge 
management scheme is an efficient 
methodology to facilitate KA and new 
knowledge discovery in the field of genome 
and nano-technology[2]. 
Important areas of future research will 
involve integration of a manually curated 
ontology with the results of automatically 
performed term clustering. Further, we will 
investigate the possibility of using a term 
classification system as an alternative 
structuring model for knowledge deduction 
and inference (instead of an ontology). 
References 
[1] K. Fukuda, T. Tsunoda, A. Tamura, T. Takagi, 
Toward information extraction: identifying 
protein names from biological papers, Proc. 
of PSB-98, Hawaii, 1998, pp. 3:705-716. 
[2] H. Mima, S. Ananiadou, G. Nenadic, ATRACT 
workbench: an automatic term recognition 
and clustering of terms, in: V. Matou?ek, P. 
Mautner, R. Mou?ek, K. Tau?er (Eds.) Text, 
Speech and Dialogue, LNAI 2166, Springer 
Verlag, 2001, pp. 126-133. 
[3] H. Mima, S. Ananiadou, An application and 
evaluation of the C/NC-value approach for 
the automatic term recognition of multi-word 
units in Japanese, Int. J. on Terminology 6/2 
(2001), pp. 175-194. 
[4] A. Voutilainen, J. Heikkila, An English 
Constraint Grammar (ENGCG) a surface-
syntactic parser of English, in: U. Fries et al 
(Eds.) Creating and Using English language 
corpora, Rodopi, Amsterdam, Atlanta, 1993, 
pp. 189-199. 
[5] M. Krauthammer, A. Rzhetsky, P. Morozov, 
C. Friedman, Using BLAST for identifying 
gene and protein names in journal articles, 
in: Gene 259 (2000), pp. 245-252. 
[6] C. Jacquemin, Spotting and discovering 
terms through NLP, MIT Press, Cambridge 
MA, 2001, p. 378. 
[7] A. Ushioda, Hierarchical clustering of words, 
Proc. of COLING ?96, Copenhagen, Denmark, 
1996, pp. 1159-1162. 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology86
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1042?1051,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Integrating Multiple Dependency Corpora
for Inducing Wide-coverage Japanese CCG Resources
Sumire Uematsu?
uematsu@cks.u-tokyo.ac.jp
Takuya Matsuzaki?
takuya-matsuzaki@nii.ac.jp
Hiroki Hanaoka?
hanaoka@nii.ac.jp
Yusuke Miyao?
yusuke@nii.ac.jp
Hideki Mima?
mima@t-adm.t.u-tokyo.ac.jp
?The University of Tokyo
Hongo 7-3-1, Bunkyo, Tokyo, Japan
?National Institute of Infomatics
Hitotsubashi 2-1-2, Chiyoda, Tokyo, Japan
Abstract
This paper describes a method of in-
ducing wide-coverage CCG resources for
Japanese. While deep parsers with corpus-
induced grammars have been emerging
for some languages, those for Japanese
have not been widely studied, mainly be-
cause most Japanese syntactic resources
are dependency-based. Our method first
integrates multiple dependency-based cor-
pora into phrase structure trees and then
converts the trees into CCG derivations.
The method is empirically evaluated in
terms of the coverage of the obtained lexi-
con and the accuracy of parsing.
1 Introduction
Syntactic parsing for Japanese has been domi-
nated by a dependency-based pipeline in which
chunk-based dependency parsing is applied and
then semantic role labeling is performed on the de-
pendencies (Sasano and Kurohashi, 2011; Kawa-
hara and Kurohashi, 2011; Kudo and Matsumoto,
2002; Iida and Poesio, 2011; Hayashibe et al,
2011). This dominance is mainly because chunk-
based dependency analysis looks most appropriate
for Japanese syntax due to its morphosyntactic ty-
pology, which includes agglutination and scram-
bling (Bekki, 2010). However, it is also true that
this type of analysis has prevented us from deeper
syntactic analysis such as deep parsing (Clark and
Curran, 2007) and logical inference (Bos et al,
2004; Bos, 2007), both of which have been sur-
passing shallow parsing-based approaches in lan-
guages like English.
In this paper, we present our work on induc-
ing wide-coverage Japanese resources based on
combinatory categorial grammar (CCG) (Steed-
man, 2001). Our work is basically an extension of
a seminal work on CCGbank (Hockenmaier and
Steedman, 2007), in which the phrase structure
trees of the Penn Treebank (PTB) (Marcus et al,
1993) are converted into CCG derivations and a
wide-coverage CCG lexicon is then extracted from
these derivations. As CCGbank has enabled a va-
riety of outstanding works on wide-coverage deep
parsing for English, our resources are expected to
significantly contribute to Japanese deep parsing.
The application of the CCGbank method to
Japanese is not trivial, as resources like PTB are
not available in Japanese. The widely used re-
sources for parsing research are the Kyoto corpus
(Kawahara et al, 2002) and the NAIST text corpus
(Iida et al, 2007), both of which are based on the
dependency structures of chunks. Moreover, the
relation between chunk-based dependency struc-
tures and CCG derivations is not obvious.
In this work, we propose a method to integrate
multiple dependency-based corpora into phrase
structure trees augmented with predicate argument
relations. We can then convert the phrase structure
trees into CCG derivations. In the following, we
describe the details of the integration method as
well as Japanese-specific issues in the conversion
into CCG derivations. The method is empirically
evaluated in terms of the quality of the corpus con-
version, the coverage of the obtained lexicon, and
the accuracy of parsing with the obtained gram-
mar. Additionally, we discuss problems that re-
main in Japanese resources from the viewpoint of
developing CCG derivations.
There are three primary contributions of this pa-
per: 1) we show the first comprehensive results for
Japanese CCG parsing, 2) we present a methodol-
ogy for integrating multiple dependency-based re-
1042
I
NP: I?
NPy: I?
NP: I?
NP: I?
give
S\NP/NP/NP :
?x?y?z.give?yxz
them
NP :them?
NP :ythem? >S\NP/NP :?y?z.give?y them?z
money
NP :money?
NP :money?
NP :money? >S\NP :?z.give?money?them?z <S :give?money?them?I ?
??
ambassador
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPni
??
participation
Sstem\NPga\NPni
?
do-CONT
Scont\Sstem <BScont\NPga\NPni
?
PAST-BASE
Sbase\Scont
Sbase\Scont <BSbase\NPga\NPni <Sbase\NPga <Sbase
??
government
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
ambassador
NPnc
?
ACC
NPwo\NPnc <NPwo
NPwo
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
???
participation
Svo s\NPga\NPni
?
CAUSE
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
government
NP
?
NOM
NPga\NP <NPga
NPga
NPga
???
ambassador-ACC
NPwo
NPga
NPga
???
negotiation-DAT
NPni
NPga
???
join
Svo s\NPga\NPni
?
cause
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPga
NPga
??
participation
Sstem\NPni
?
do
Svo s\Sstem <BSvo s\NPni
?
CAUSE
Scont\Svo s
Scont\Svo s <BScont\NPni
?
PAST
Sbase\Scont
Scont\Svo s
Scont <BSbase\NPni <Sbase
1
Figure 1: A CCG derivation.
X/Y : f Y : a ? X : fa (>)
Y : a X\Y : a ? X : fa (<)
X/Y : f Y/Z : g ? X/Z : ?x.f(gx) (> B)
Y\Z : g X\Y : f ? X\Z : ?x.f(gx) (< B)
Figure 2: Combinatory rules (used in the current
implementation).
sources to induce CCG derivations, and 3) we in-
vestigate the possibility of further improving CCG
analysis by additional resources.
2 Background
2.1 Combinatory Categorial Grammar
CCG is a syntactic theory widely accepted in the
NLP field. A grammar based on CCG theory con-
sists of categories, which represent syntactic cat-
egories of words and phrases, and combinatory
rules, which are rules to combine the categories.
Categories are either ground categories like S and
NP or complex categories in the form of X/Y or
X\Y , where X and Y are the categories. Cate-
gory X/Y intuitively means that it becomes cat-
egory X when it is combined with another cat-
egory Y to its right, and X\Y means it takes a
category Y to its left. Categories are combined
by applying combinatory rules (Fig. 2) to form
categories for larger phrases. Figure 1 shows a
CCG analysis of a simple English sentence, which
is called a derivation. The verb give is assigned
category S\NP/NP/NP , which indicates that it
takes two NPs to its right, one NP to its left, and fi-
nally becomes S. Starting from lexical categories
assigned to words, we can obtain categories for
phrases by applying the rules recursively.
An important property of CCG is a clear inter-
face between syntax and semantics. As shown in
Fig. 1, each category is associated with a lambda
term of semantic representations, and each com-
binatory rule is associated with rules for semantic
composition. Since these rules are universal, we
can obtain different semantic representations by
switching the semantic representations of lexical
categories. This means that we can plug in a vari-
Sentence S Verb S\$ (e.g. S\NPga)
Noun phrase NP Post particle NPga|o|ni|to\NP
Auxiliary verb S\S
Table 1: Typical categories for Japanese syntax.
Cat. Feature Value Interpretation
NP case ga nominal
o accusative
ni dative
to comitative, complementizer, etc.
nc none
S form stem stem
base base
neg imperfect or negative
cont continuative
vo s causative
Table 2: Features for Japanese syntax (those used
in the examples in this paper).
ety of semantic theories with CCG-based syntactic
parsing (Bos et al, 2004).
2.2 CCG-based syntactic theory for Japanese
Bekki (2010) proposed a comprehensive theory
for Japanese syntax based on CCG. While the the-
ory is based on Steedman (2001), it provides con-
crete explanations for a variety of constructions of
Japanese, such as agglutination, scrambling, long-
distance dependencies, etc. (Fig. 3).
The ground categories in his theory are S, NP,
and CONJ (for conjunctions). Table 1 presents
typical lexical categories. While most of them
are obvious from the theory of CCG, categories
for auxiliary verbs require an explanation. In
Japanese, auxiliary verbs are extensively used to
express various semantic information, such as
tense and modality. They agglutinate to the main
verb in a sequential order. This is explained in
Bekki?s theory by the category S\S combined with
a main verb via the function composition rule
(<B). Syntactic features are assigned to categories
NP and S (Table 2). The feature case represents a
syntactic case of a noun phrase. The feature form
denotes an inflection form, and is necessary for
constraining the grammaticality of agglutination.
Our implementation of the grammar basically
follows Bekki (2010)?s theory. However, as a first
step in implementing a wide-coverage Japanese
parser, we focused on the frequent syntactic con-
structions that are necessary for computing pred-
icate argument relations, including agglutination,
inflection, scrambling, case alternation, etc. Other
details of the theory are largely simplified (Fig. 3),
1043
I
NP: I?
NPy: I?
NP: I?
NP: I?
give
S\NP/NP/NP :
?x?y?z.give?yxz
them
NP :them?
NP :ythem? >S\NP/NP :?y?z.give?y them?z
money
NP :money?
NP :money?
NP :money? >S\NP :?z.give?money?them?z <S :give?money?them?I ?
??
ambassador
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPni
??
participation
Sstem\NPga\NPni
?
do-CONT
Scont\Sstem <BScont\NPga\NPni
?
PAST-BASE
Sbase\Scont
Sbase\Scont <BSbase\NPga\NPni <Sbase\NPga <Sbase
??
government
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
ambassador
NPnc
?
ACC
NPwo\NPnc <NPwo
NPwo
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
???
participation
Svo s\NPga\NPni
?
CAUSE
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
government
NP
?
NOM
NPga\NP <NPga
NPga
NPga
???
ambassador-ACC
NPwo
NPga
NPga
???
negotiation-DAT
NPni
NPga
???
join
Svo s\NPga\NPni
?
cause
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPga
NPga
??
participation
Sstem\NPni
?
do
Svo s\Sstem <BSvo s\NPni
?
CAUSE
Scont\Svo s
Scont\Svo s <BScont\NPni
?
PAST
Sbase\Scont
Scont\Svo s
Scont <BSbase\NPni <Sbase
1
Figure 3: A simplified CCG analysis of the sentence ?The ambassador participated in the negotiation.?.
S ? NP/NP (RelExt)
S\NP1 ? NP1/ P1 (RelIn)
S ? S1/S1 (Con)
S\$1\NP1 ? (S1\$1\NP1)/(S1\$1\NP1) (ConCoord)
Figure 4: Type changing rules. The upper two are
for relative clauses and the others for continuous
clauses.
coordination and semantic representation in par-
ticular. The current implementation recognizes
coordinated verbs in continuous clauses (e.g., ??
???????????/he played the pia o and
sang?), but the treatment of other types of coor-
dination is largely simplified. For semantic repre-
sentation, we define predicate argument structures
(PASs) rather than the theory?s formal representa-
tion based on dynamic logic. Sophisticating our
semantic representation is left for future work.
For parsing efficiency, we modified the treat-
ment of some constructions so that empty el-
ements are excluded from the implementation.
First, we define type changing rules to produce
relative and continuous clauses (shown in Fig. 4).
The rules produce almost the same results as the
theory?s treatment, but without using empty ele-
ments (pro, etc.). We also used lexical rules to
treat pro-drop and scrambling. For the sentence in
Fig. 3, the deletion of the nominal phrase (??
?), the dative phrase (???), or both results in
valid sentences, and shuffling the two phrases does
so as well. Lexical entries with the scrambled or
dropped arguments are produced by lexical rules
in our implementation.
2.3 Linguistic resources for Japanese parsing
As described in Sec. 1, dependency-based analysis
has been accepted for Japanese syntax. Research
on Japanese parsing also relies on dependency-
based corpora. Among them, we used the follow-
ing resources in this work.
Kyoto corpus A news text corpus annotated
with morphological information, chunk bound-
Kyoto Corpus 
Chunk 
?? ? government NOM ?? ? ambassador ACC ?? ? negotiation DAT ?? ? ? ? participation do cause PAST 
NAIST Corpus 
Dep. 
Causer ARG-ga ARG-ni 
Figure 5: The Kyoto and NAIST annotations for
?The government had the ambassador participate
in the negotiation.?. Accusatives are labeled as
ARG-ga in causative (see Sec. 3.2).
aries, and dependency relations among chunks
(Fig. 5). The dependencies are classified into four
types: Para (coordination), A (apposition), I (ar-
gument cluster), and Dep (default). Most of the
dependencies are annotated as Dep.
NAIST text corpus A corpus annotated with
anaphora and coreference relations. The same set
as the Kyoto corpus is annotated.1 The corpus
only focuses on three cases: ?ga? (subject), ?o?
(direct object), and ?ni? (indirect object) (Fig. 5).
Japanese particle corpus (JP) (Hanaoka et al,
2010) A corpus annotated with distinct gram-
matical functions of the Japanese particle (postpo-
sition) ?to?. In Japanese, ?to? has many functions,
including a complementizer (similar to ?that?), a
subordinate conjunction (similar to ?then?), a co-
ordination conjunction (similar to ?and?), and a
case marker (similar to ?with?).
2.4 Related work
Research on Japanese deep parsing is fairly lim-
ited. Formal theories of Japanese syntax were
presented by Gunji (1987) based on Head-driven
Phrase Structure Grammar (HPSG) (Sag et al,
2003) and by Komagata (1999) based on CCG, al-
though their implementations in real-world pars-
ing have not been very successful. JACY (Siegel
1In fact, the NAIST text corpus includes additional texts,
but in this work we only use the news text section.
1044
and Bender, 2002) is a large-scale Japanese gram-
mar based on HPSG, but its semantics is tightly
embedded in the grammar and it is not as easy
to systematically switch them as it is in CCG.
Yoshida (2005) proposed methods for extracting
a wide-coverage lexicon based on HPSG from a
phrase structure treebank of Japanese. We largely
extended their work by exploiting the standard
chunk-based Japanese corpora and demonstrated
the first results for Japanese deep parsing with
grammar induced from large corpora.
Corpus-based acquisition of wide-coverage
CCG resources has enjoyed great success for En-
glish (Hockenmaier and Steedman, 2007). In
that method, PTB was converted into CCG-based
derivations from which a wide-coverage CCG lex-
icon was extracted. CCGbank has been used for
the development of wide-coverage CCG parsers
(Clark and Curran, 2007). The same methodology
has been applied to German (Hockenmaier, 2006),
Italian (Bos et al, 2009), and Turkish (C?ak?c?,
2005). Their treebanks are annotated with depen-
dencies of words, the conversion of which into
phrase structures is not a big concern. A notable
contribution of the present work is a method for in-
ducing CCG grammars from chunk-based depen-
dency structures, which is not obvious, as we dis-
cuss later in this paper.
CCG parsing provides not only predicate argu-
ment relations but also CCG derivations, which
can be used for various semantic processing tasks
(Bos et al, 2004; Bos, 2007). Our work consti-
tutes a starting point for such deep linguistic pro-
cessing for languages like Japanese.
3 Corpus integration and conversion
For wide-coverage CCG parsing, we need a)
a wide-coverage CCG lexicon, b) combinatory
rules, c) training data for parse disambiguation,
and d) a parser (e.g., a CKY parser). Since d) is
grammar- and language-independent, all we have
to develop for a new language is a)?c).
As we have adopted the method of CCGbank,
which relies on a source treebank to be converted
into CCG derivations, a critical issue to address is
the absence of a Japanese counterpart to PTB. We
only have chunk-based dependency corpora, and
their relationship to CCG analysis is not clear.
Our solution is to first integrate multiple
dependency-based resources and convert them
into a phrase structure treebank that is independent
ProperNoun 
????? Yeltsin 
NP 
ProperNoun 
??? Russia 
Noun 
???president 
PostP 
? DAT 
PP 
NP 
Aux 
??? not 
VP 
Verb 
?? forgive 
VerbSuffix 
? PASSIVE 
VP Aux 
? PAST 
VP 
?to Russian president Yeltsin? ?(one) was not forgiven? 
Figure 6: Internal structures of a nominal chunk
(left) and a verbal chunk (right).
of CCG analysis (Step 1). Next, we translate the
treebank into CCG derivations (Step 2). The idea
of Step 2 is similar to what has been done with
the English CCGbank, but obviously we have to
address language-specific issues.
3.1 Dependencies to phrase structure trees
We first integrate and convert available Japanese
corpora?namely, the Kyoto corpus, the NAIST
text corpus, and the JP corpus ?into a phrase
structure treebank, which is similar in spirit to
PTB. Our approach is to convert the depen-
dency structures of the Kyoto corpus into phrase
structures and then augment them with syntac-
tic/semantic roles from the other two corpora.
The conversion involves two steps: 1) recogniz-
ing the chunk-internal structures, and (2) convert-
ing inter-chunk dependencies into phrase struc-
tures. For 1), we don?t have any explicit infor-
mation in the Kyoto corpus although, in princi-
ple, each chunk has internal structures (Vadas and
Curran, 2007; Yamada et al, 2010). The lack of
a chunk-internal structure makes the dependency-
to-constituency conversion more complex than a
similar procedure by Bos et al (2009) that con-
verts an Italian dependency treebank into con-
stituency trees since their dependency trees are an-
notated down to the level of each word. For the
current implementation, we abandon the idea of
identifying exact structures and instead basically
rely on the following generic rules (Fig. 6):
Nominal chunks Compound nouns are first
formed as a right-branching phrase and
post-positions are then attached to it.
Verbal chunks Verbal chunks are analyzed as
left-branching structures.
The rules amount to assume that all but the last
word in a compound noun modify the head noun
(i.e., the last word) and that a verbal chunk is typ-
ically in a form V A1 . . . An, where V is a verb
1045
PP 
Noun 
?? birth  
PostPcm  
??from  
PP 
Noun 
? death  
PostPcm  
?? to 
PostPadnom  
? adnominal 
PP 
PP 
Noun 
?? process  
PostPcm  
? ACC  PP 
NP 
PP 
Noun 
?? birth  
PostPcm  
??from  
PP 
Noun 
? death  
PostPcm  
?? to 
PostPadnom  
? adnominal 
PP PP 
Noun 
?? process  
PostPcm  
? ACC  
Para Dep 
?$ proFess froP EirtK to deatK? 
Figure 7: From inter-chunk dependencies to a tree.
(or other predicative word) and Ais are auxiliaries
(see Fig. 6). We chose the left-branching structure
as default for verbal chunks because the semantic
scopes of the auxiliaries are generally in that or-
der (i.e., A1 has the narrowest scope). For both
cases, phrase symbols are percolated upward from
the right-most daughters of the branches (except
for a few cases like punctuation) because in almost
all cases the syntactic head of a Japanese phrase is
the right-most element.
In practice, we have found several patterns of
exceptions for the above rules. We implemented
exceptional patterns as a small CFG and deter-
mined the chunk-internal structures by determin-
istic parsing with the generic rules and the CFG.
For example, two of the rules we came up with are
rule A: Number ? PrefixOfNumber Number
rule B: ClassifierPhrase ? Number Classifier
in the precedence: rule A > B > generic rules.
Using the above, we bracket a compound noun
? ? ? ??
approximately thousand people death
PrefixOfNumber Number Classifier CommonNoun
?death of approximately one thousand people?
as in
(((? ?) ?) ??)
(((approximately thousand) people) death)
We can improve chunk-internal structures to some
extent by refining the CFG rules. A complete solu-
tion like the manual annotation by Vadas and Cur-
ran (2007) is left for future work.
The conversion of inter-chunk dependencies
into phrase structures may sound trivial, but it is
not necessarily easy when combined with chunk-
internal structures. The problem is to which node
in the internal structure of the head the dependent
dep modifier-type precedence
Para ??/PostPcm ??/PostPcm, */(Verb|Aux), ...
Dep */PostPcm */(Verb|Aux), */Noun, ...
Dep */PostPadnom */Noun, */(Verb|Aux), ...
Table 3: Rules to determine adjoin position.
PP 
Noun 
? dog 
PostP 
? DAT 
VP 
NP 
Adj  
?? 
white  
NP 
VP 
Noun 
? 
cat  
Verb 
?? say  
Aux 
? PAST 
VP PP 
Verb 
?? go!  
PostP 
? 
CMP  
ARG - to 
ARG - ni 
ARG - ga 
ARG - ga 
ARG - ga 
ARG - ga ARG - ni 
ARG - CLS  
NAIST 
JP 
Figure 8: Overlay of pred-arg structure annotation
(?The white cat who said ?Go!? to the dog.?).
tree is adjoined (Fig. 7). In the case shown in the
figure, three chunks are in the dependency relation
indicated by arrows on the top. The dotted arrows
show the nodes to which the subtrees are adjoined.
Without any human-created resources, we can-
not always determine the adjoin positions cor-
rectly. Therefore, as a compromise, we wound up
implementing approximate heuristic rules to deter-
mine the adjoin positions. Table 3 shows examples
of such rules. A rule specifies a precedence of the
possible adjoin nodes as an ordered list of patterns
on the lexical head of the subtree under an ad-
join position. The precedence is defined for each
combination of the type of the dependent phrase,
which is determined by its lexical head, and the
dependency type in the Kyoto corpus.
To select the adjoin position for the left-most
subtree in Fig. 7, for instance, we look up the
rule table using the dependency type, ?Para?, and
the lexical head of the modifier subtree, ? ??
/PostPcm?, as the key, and find the precedence ??
?/PostPcm, */(Verb|Aux), ...?. We thus select the
PP-node on the middle subtree indicated by the
dotted arrow because its lexical head (the right-
most word), ? ??/PostPcm?, matches the first
pattern in the precedence list. In general, we seek
for an adjoin node for each pattern p in the prece-
dence list, until we find a first match.
The semantic annotation given in the NAIST
corpus and the JP corpus is overlaid on the phrase
structure trees with slight modifications (Fig. 8).
1046
PP 
Noun 
?? negotiation 
PostPcm  
? DAT VP Noun 
?? participation  
Verb 
? do 
VerbSuffix 
? CAUSE  
Aux 
? PAST 
VP 
VP 
S 
NPni 
NP 
?? negotiation 
T1 
? DAT T4 
T5 
?? participation  
S?S 
? do 
S?S 
? CAUSE  
S?S 
? PAST 
T3 
T2 
S ? 
? 
?  or   ?B   
?  or   ?B   
?  or   ?B   
NPni 
NPnc 
?? negotiation 
NPni?NPnc 
? DAT 
Svo_s?NPni 
Svo_s?NPni 
?? participation  
Svo_s?Svo_s 
? do 
Scont?Svo_s 
? CAUSE  
Sbase?Scont 
? PAST 
Scont?NPni 
Sbase?NPni 
Sbase 
Step 2 - 1  
Step 2 - 2, 2 - 3  
Figure 9: A phrase structure into a CCG deriva-
tion.
In the figure, the annotation given in the two cor-
pora is shown inside the dotted box at the bottom.
We converted the predicate-argument annotations
given as labeled word-to-word dependencies into
the relations between the predicate words and their
argument phrases. The results are thus similar to
the annotation style of PropBank (Palmer et al,
2005). In the NAIST corpus, each pred-arg re-
lation is labeled with the argument-type (ga/o/ni)
and a flag indicating that the relation is medi-
ated by either a syntactic dependency or a zero
anaphora. For a relation of a predicate wp and its
argument wa in the NAIST corpus, the boundary
of the argument phrase is determined as follows:
1. If wa precedes wp and the relation is medi-
ated by a syntactic dep., select the maximum
PP that is formed by attaching one or more
postpositions to the NP headed by wa.
2. If wp precedes wa or the relation is mediated
by a zero anaphora, select the maximum NP
headed by wa that does not include wp.
In the figure, ??/dog?/DAT? is marked as the ni-
argument of the predicate ???/say? (Case 1), and
???/white ?/cat? is marked as its ga-argument
(Case 2). Case 1 is for the most basic construction,
where an argument PP precedes its predicate. Case
VP 
??    ? 
friend- DAT 
PP VP 
?? 
meet - BASE  
NPni ? 
VP 
10 ?    ? 
10 o?clock - TIME  
PP VP 
?? 
meet - BASE  
T/T ? 
X  
S 
??    ? 
friend- DAT 
NPni S?NPni 
?? 
meet - BASE  
S 
10 ?    ? 
10 o?clock - TIME  
S?S S 
?? 
meet - BASE  
?(to) Peet at ten? 
?(to) Peet a friend? 
Figure 10: An argument post particle phrase (PP)
(upper) and an adjunct PP (lower).
2 covers the relative clause construction, where a
relative clause precedes the head NP, the modifi-
cation of a noun by an adjective, and the relations
mediated by zero anaphora.
The JP corpus provides only the function label
to each particle ?to? in the text. We determined
the argument phrases marked by the ?to? particles
labeled as (nominal or clausal) argument-markers
in a similar way to Case 1 above and identified the
predicate words as the lexical heads of the phrases
to which the PPto phrases attach.
3.2 Phrase structures to CCG derivations
This step consists of three procedures (Fig. 9):
1. Add constraints on categories and features
to tree nodes as far as possible and assign a
combinatory rule to each branching.
2. Apply combinatory rules to all branching and
obtain CCG derivations.
3. Add feature constraints to terminal nodes.
3.2.1 Local constraint on derivations
According to the phrase structures, the first proce-
dure in Step 2 imposes restrictions on the resulting
CCG derivations. To describe the restrictions, we
focus on some of the notable constructions and il-
lustrate the restrictions for each of them.
Phrases headed by case marker particles A
phrase of this type must be either an argument
(Fig. 10, upper) or a modifier (Fig. 10, lower) of a
predicative. Distinction between the two is made
based on the pred-arg annotation of the predica-
tive. If a phrase is found to be an argument, 1) cat-
egory NP is assigned to the corresponding node,
2) the case feature of the category is given accord-
ing to the particle (in the case of Fig. 10 (upper),
1047
VP 
Verb 
?? 
Speak - NEG  
Aux 
??? 
not- CONT  
Aux 
? 
PAST- BASE  
VP 
S cont ?S  
S ba s e ?S  
?did not speak? 
? or ?B  
? or ?B  S cont ?NPg a  
Sneg ?NPg a  
?? 
Speak - NEG  
S cont ?Sneg  
??? 
not- CONT  
S b ase ?S cont  
? 
PAST- BASE  
S b ase ?NPg a  
Figure 11: An auxiliary verb and its conversion.
VP 
Verb 
?? inquire - NEG  
VerbSuffix 
??? cause - BASE  
??    ? her - DAT 
PP 
VP 
ARG - ga  
?(to) Kave Ker inTuire? 
? 
S?NPni[1 ] 
S?S 
??? cause - BASE  
??    ? her - DAT 
NPni[1 ] 
S 
S?NPni[1 ] 
?? inquire - NEG  
ga         [1]  
NPni[1 ] 
ga: [1 ] 
Figure 12: A causative construction.
ni for dative), and 3) the combinatory rule that
combines the particle phrase and the predicative
phrase is assigned backward function application
rule (<). Otherwise, a category T/T is assigned to
the corresponding modifier node and the rule will
be forward function application (>).
Auxiliary verbs As described in Sec. 2.2, an
auxiliary verb is always given the category S\S
and is combined with a verbal phrase via < or <B
(Fig. 11). Furthermore, we assign the form feature
value of the returning category S according to the
inflection form of the auxiliary. In the case shown
in the figure, Sbase\S is assigned for ??/PAST-
BASE? and Scont\S for ????/not-CONT?. As
a result of this restriction, we can obtain condi-
tions for every auxiliary agglutination because the
two form values in S\S are both restricted after
applying combinatory rules (Sec. 3.2.2).
Case alternations In addition to the argu-
ment/adjunct distinction illustrated above, a pro-
cess is needed for argument phrases of predicates
involving case alternation. Such predicates are
either causative (see Fig. 12) or passive verbs
and can be detected by voice annotation from the
NAIST corpus. For an argument of that type of
verb, its deep case (ga for Fig. 12) must be used
to construct the semantic representation, namely
the PAS. As well as assigning the shallow case
value (ni in Fig. 12) to the argument?s category
NP, as usual, we assign a restriction to the PAS
S?NPo[1] 
S?NPo 
?? 
buy-CONT 
S?S 
? 
PAST-ATTR 
NP 
? 
book 
NP 
NP[1]?NP[1] 
VP 
Verb 
?? 
buy-CONT 
Aux 
? 
PAST-ATTR 
Noun 
? 
book 
NP 
S?NP[1] 
NP[1]?NP[1] 
Noun 
? 
store 
NP 
?   ? 
book-ACC 
PP VP 
Verb 
?? 
buy-CONT 
Aux 
? 
PAST-ATTR 
VP X 
S 
NP?NP 
NP 
? 
store 
NP 
NP?NP 
?    ? 
book-ACC 
NPo 
S 
S?NPo 
S?NPo 
?? 
buy-CONT 
S?S 
? 
PAST-ATTR 
?a store wKere (,) EougKt tKe EooN? 
?a EooN wKiFK (,) EougKt? 
Figure 13: A relative clause with/without argu-
ment extraction (upper/lower, respectively).
of the verb so that the semantic argument corre-
sponding to the deep case is co-indexed with the
argument NP. These restrictions are then utilized
for PAS construction in Sec. 3.2.3.
Relative clauses A relative clause can be de-
tected as a subtree that has a VP as its left child
and an NP as its right child, as shown in Fig. 13.
The conversion of the subtree consists of 1) in-
serting a node on the top of the left VP (see the
right-hand side of Fig. 13), and 2) assigning the
appropriate unary rule to make the new node. The
difference between candidate rules RelExt and Re-
lIn (see Fig. 4) is whether the right-hand NP is
an obligatory argument of the VP or not, which
can be determined by the pred-arg annotation on
the predicate in the VP. In the upper example in
Fig. 13, RelIn is assigned because the right NP
?book? is annotated as an accusative argument of
the predicate ?buy?. In contrast, RelExt is as-
signed in the lower side in the figure because the
right NP ?store? is not annotated as an argument.
Continuous clauses A continuous clause can be
detected as a subtree with a VP of continuous form
as its left child and a VP as its right child. Its
conversion is similar to that of a relative clause,
and only differs in that the candidate rules are Con
and ConCoord. ConCoord generates a continu-
ous clause that shares arguments with the main
clause while Con produces one without shared ar-
guments. Rule assignment is done by comparing
the pred-arg annotations of the two phrases.
1048
Training Develop. Test
#Sentences 24,283 4,833 9,284
#Chunks 234,685 47,571 89,874
#Words 664,898 136,585 255,624
Table 4: Statistics of input linguistic resources.
3.2.2 Inverse application of rules
The second procedure in Step 2 begins with as-
signing a category S to the root node. A combi-
natory rule assigned to each branching is then ?in-
versely? applied so that the constraint assigned to
the parent transfers to the children.
3.2.3 Constraints on terminal nodes
The final process consists of a) imposing restric-
tions on the terminal category in order to instan-
tiate all the feature values, and b) constructing a
PAS for each verbal terminal. An example of pro-
cess a) includes setting the form features in the
verb category, such as S\NPni, according to the
inflection form of the verb. As for b), arguments
in a PAS are given according to the category and
the partial restriction. For instance, if a category
S\NPni is obtained for ???/inquire? (Fig. 12),
the PAS for ?inquire? is unary because the cate-
gory has one argument category (NPni), and the
category is co-indexed with the semantic argument
ga in the PAS due to the partial restriction depicted
in Sec. 3.2.1. As a result, a lexical entry is ob-
tained as?? ` S\NPni[1]: inquire([1]).
3.3 Lexical entries
Finally, lexical rules are applied to each of the ob-
tained lexical entries in order to reduce them to
the canonical form. Since words in the corpus (es-
pecially verbs) often involve pro-drop and scram-
bling, there are a lot of obtained entries that have
slightly varied categories yet share a PAS. We as-
sume that an obtained entry is a variation of the
canonical one and register the canonical entries in
the lexicon. We treat only subject deletion for pro-
drop because there is not sufficient information to
judge the deletion of other arguments. Scrambling
is simply treated as permutation of arguments.
4 Evaluation
We used the following for the implementation of
our resources: Kyoto corpus ver. 4.02, NAIST text
2http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?
Kyoto\%20University\%20Text\%20Corpus
Training Develop. Test
St.1 St.2 St.1 St.2 St.1 St.2
Sent. 24,283 24,116 4,833 4,803 9,284 9,245
Converted 24,116 22,820 4,803 4,559 9,245 8,769
Con. rate 99.3 94.6 99.4 94.9 99.6 94.9
Table 5: Statistics of corpus conversion.
Sentential Coverage
Covered Uncovered Cov. (%)
Devel. 3,920 639 85.99
Test 7,610 1,159 86.78
Lexical Coverage
Word Known Unknown
combi. cat. word
Devel. 127,144 126,383 682 79 0
Test 238,083 236,651 1,242 145 0
Table 6: Sentential and lexical coverage.
corpus ver. 1.53, and JP corpus ver. 1.04. The
integrated corpus is divided into training, devel-
opment, and final test sets following the standard
data split in previous works on Japanese depen-
dency parsing (Kudo and Matsumoto, 2002). The
details of these resources are shown in Table 4.
4.1 Corpus conversion and lexicon extraction
Table 5 shows the number of successful conver-
sions performed by our method. In total, we ob-
tained 22,820 CCG derivations from 24,283 sen-
tences (in the training set), resulting in the to-
tal conversion rate of 93.98%. The table shows
we lost more sentences in Step 2 than in Step 1.
This is natural because Step 2 imposed more re-
strictions on resulting structures and therefore de-
tected more discrepancies including compounding
errors. Our conversion rate is about 5.5 points
lower than the English counterpart (Hockenmaier
and Steedman, 2007). Manual investigation of the
sampled derivations would be beneficial for the
conversion improvement.
For the lexicon extraction from the CCGbank,
we obtained 699 types of lexical categories from
616,305 word tokens. After lexical reduction, the
number of categories decreased to 454, which in
turn may produce 5,342 categories by lexical ex-
pansion. The average number of categories for a
word type was 11.68 as a result.
4.2 Evaluation of coverage
Following the evaluation criteria in (Hockenmaier
and Steedman, 2007), we measured the coverage
3http://cl.naist.jp/nldata/corpus/
4https://alaginrc.nict.go.jp/resources/tocorpus/
tocorpusabstract.html
1049
of the grammar on unseen texts. First, we obtained
CCG derivations for evaluation sets by applying
our conversion method and then used these deriva-
tions as gold standard. Lexical coverage indicates
the number of words to which the grammar assigns
a gold standard category. Sentential coverage indi-
cates the number of sentences in which all words
are assigned gold standard categories 5.
Table 6 shows the evaluation results. Lexical
coverage was 99.40% with rare word treatment,
which is in the same level as the case of the En-
glish CCG parser C&C (Clark and Curran, 2007).
We also measured coverage in a ?weak? sense,
which means the number of sentences that are
given at least one analysis (not necessarily cor-
rect) by the obtained grammar. This number was
99.12 % and 99.06 % for the development and the
test set, respectively, which is sufficiently high for
wide-coverage parsing of real-world texts.
4.3 Evaluation of parsing accuracy
Finally, we evaluated the parsing accuracy. We
employed the parser and the supertagger of
(Miyao and Tsujii, 2008), specifically, its gen-
eralized modules for lexicalized grammars. We
trained log-linear models in the same way as
(Clark and Curran, 2007) using the training set as
training data. Feature sets were simply borrowed
from an English parser; no tuning was performed.
Following conventions in research on Japanese de-
pendency parsing, gold morphological analysis re-
sults were input to a parser. Following C&C, the
evaluation measure was precision and recall over
dependencies, where a dependency is defined as a
4-tuple: a head of a functor, a functor category, an
argument slot, and a head of an argument.
Table 7 shows the parsing accuracy on the de-
velopment and the test sets. The supertagging ac-
curacy is presented in the upper table. While our
coverage was almost the same as C&C, the perfor-
mance of our supertagger and parser was lower.
To improve the performance, tuning disambigua-
tion models for Japanese is a possible approach.
Comparing the parser?s performance with previ-
ous works on Japanese dependency parsing is dif-
ficult as our figures are not directly comparable
to theirs. Sassano and Kurohashi (2009) reported
the accuracy of their parser as 88.48 and 95.09
5Since a gold derivation can logically be obtained if gold
categories are assigned to all words in a sentence, sentential
coverage means that the obtained lexicon has the ability to
produce exactly correct derivations for those sentences.
Supertagging accuracy
Lex. Cov. Cat. Acc.
Devel. 99.40 90.86
Test 99.40 90.69
C&C 99.63 94.32
Overall performance
LP LR LF UP UR UF
Devel. 82.55 82.73 82.64 90.02 90.22 90.12
Test 82.40 82.59 82.50 89.95 90.15 90.05
C&C 88.34 86.96 87.64 93.74 92.28 93.00
Table 7: Parsing accuracy. LP, LR and LF refer to
labeled precision, recall, and F-score respectively.
UP, UR, and UF are for unlabeled.
in unlabeled chunk-based and word-based F1 re-
spectively. While our score of 90.05 in unlabeled
category dependency seems to be lower than their
word-based score, this is reasonable because our
category dependency includes more difficult prob-
lems, such as whether a subject PP is shared by
coordinated verbs. Thus, our parser is expected to
be capable of real-world Japanese text analysis as
well as dependency parsers.
5 Conclusion
In this paper, we proposed a method to induce
wide-coverage Japanese resources based on CCG
that will lead to deeper syntactic analysis for
Japanese and presented empirical evaluation in
terms of the quality of the obtained lexicon and
the parsing accuracy. Although our work is basi-
cally in line with CCGbank, the application of the
method to Japanese is not trivial due to the fact that
the relationship between chunk-based dependency
structures and CCG derivations is not obvious.
Our method integrates multiple dependency-
based resources to convert them into an integrated
phrase structure treebank. The obtained treebank
is then transformed into CCG derivations. The
empirical evaluation in Sec. 4 shows that our cor-
pus conversion successfully converts 94 % of the
corpus sentences and the coverage of the lexicon
is 99.4 %, which is sufficiently high for analyz-
ing real-world texts. A comparison of the parsing
accuracy with previous works on Japanese depen-
dency parsing and English CCG parsing indicates
that our parser can analyze real-world Japanese
texts fairly well and that there is room for improve-
ment in disambiguation models.
1050
References
Daisuke Bekki. 2010. Formal Theory of Japanese Syn-
tax. Kuroshio Shuppan. (In Japanese).
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING 2004, pages
1240?1246.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cate-
gorial grammar treebank for Italian. In Proceedings
of the Eighth International Workshop on Treebanks
and Linguistic Theories (TLT8), pages 27?38.
Johan Bos. 2007. Recognising textual entailment and
computational semantics. In Proceedings of Seventh
International Workshop on Computational Seman-
tics IWCS-7, page 1.
Ruken C?ak?c?. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of ACL Stu-
dent Research Workshop, pages 73?78.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4).
Takao Gunji. 1987. Japanese Phrase Structure Gram-
mar: A Unification-based Approach. D. Reidel.
Hiroki Hanaoka, Hideki Mima, and Jun?ichi Tsujii.
2010. A Japanese particle corpus built by example-
based annotation. In Proceedings of LREC 2010.
Yuta Hayashibe, Mamoru Komachi, and Yuji Mat-
sumoto. 2011. Japanese predicate argument struc-
ture analysis exploiting argument position and type.
In Proceedings of IJCNLP 2011, pages 201?209.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2006. Creating a CCGbank and
a wide-coverage CCG lexicon for German. In Pro-
ceedings of the Joint Conference of COLING/ACL
2006.
Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ILP solution to zero anaphora resolution. In Pro-
ceedings of ACL-HLT 2011, pages 804?813.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text
corpus with predicate-argument and coreference re-
lations. In Proceedings of Linguistic Annotation
Workshop, pages 132?139.
Daisuke Kawahara and Sadao Kurohashi. 2011. Gen-
erative modeling of coordination by factoring paral-
lelism and selectional preferences. In Proceedings
of IJCNLP 2011.
Daisuke Kawahara, Sadao Kurohashi, and Koiti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of the 8th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 495?498. (In Japanese).
Nobo Komagata. 1999. Information Structure in Texts:
A Computational Analysis of Contextual Appropri-
ateness in English and Japanese. Ph.D. thesis, Uni-
versity of Pennsylvania.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analyisis using cascaded chunking. In
Proceedings of CoNLL 2002.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction, 2nd
Edition. CSLI Publications.
Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to Japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of IJCNLP 2011.
Manabu Sassano and Sadao Kurohashi. 2009. A uni-
fied single scan algorithm for Japanese base phrase
chunking and dependency parsing. In Proceedings
of ACL-IJCNLP 2009.
Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of Japanese. In Proceedings of the
3rd Workshop on Asian Language Resources and In-
ternational Standardization.
Mark Steedman. 2001. The Syntactic Process. MIT
Press.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of ACL 2007, pages 240?247.
Emiko Yamada, Eiji Aramaki, Takeshi Imai, and
Kazuhiko Ohe. 2010. Internal structure of a disease
name and its application for ICD coding. Studies
in health technology and informatics, 160(2):1010?
1014.
Kazuhiro Yoshida. 2005. Corpus-oriented develop-
ment of Japanese HPSG parsers. In Proceedings of
the ACL Student Research Workshop.
1051
