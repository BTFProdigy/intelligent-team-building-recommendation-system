Interleaved semantic interpretation in environment-based parsing

William Schuler
Computer and Information Science Dept.
University of Pennsylvania
Philadelphia, PA 19103
schuler@linc.cis.upenn.edu
Abstract
This paper extends a polynomial-time parsing al-
gorithm that resolves structural ambiguity in input
sentences by calculating and comparing the deno-
tations of rival constituents, given some model of
the application environment (Schuler, 2001). The
algorithm is extended to incorporate a full set of
logical operators, including quantiers and conjunc-
tions, into this calculation without increasing the
complexity of the overall algorithm beyond polyno-
mial time, both in terms of the length of the in-
put and the number of entities in the environment
model.
1 Introduction
The development of speaker-independent mixed-
initiative speech interfaces, in which users not only
answer questions but also ask questions and give in-
structions, is currently limited by the inadequacy
of existing corpus-based disambiguation techniques.
This paper explores the use of semantic and prag-
matic information, in the form of the entities and
relations in the interfaced application's run-time en-
vironment, as an additional source of information to
guide disambiguation.
In particular, this paper extends an existing pars-
ing algorithm that calculates and compares the de-
notations of rival parse tree constituents in order
to resolve structural ambiguity in input sentences
(Schuler, 2001). The algorithm is extended to incor-
porate a full set of logical operators into this calcu-
lation so as to improve the accuracy of the resulting
denotations { and thereby improve the accuracy of
parsing { without increasing the complexity of the
overall algorithm beyond polynomial time (both in
terms of the length of the input and the number of
entities in the environment model). This parsimony
is achieved by localizing certain kinds of semantic
relations during parsing, particularly those between
quantiers and their restrictor and body arguments

The author would like to thank David Chiang, Karin Kip-
per, and Alexander Koller, as well as the anonymous reviewers
for comments on this material. This work was partially sup-
ported by NSF IIS-9900297 and DARPA N66001-00-1-8915.
(similar to the way dependencies between predicate
and argument head words are localized in lexicalized
formalisms such as tree adjoining grammars), in or-
der to avoid calculating exponential higher-order de-
notations for expressions like generalized quantiers.
2 Basic algorithm
This section describes the basic environment-based
parser (Schuler, 2001) which will be extended in Sec-
tion 3. Because it will crucially rely on the denota-
tions (or interpretations) of proposed constituents
in order to guide disambiguation, the parser will be
dened on categorial grammars (Ajdukiewicz, 1935;
Bar-Hillel, 1953), whose categories all have well de-
ned types and worst-case denotations. These cat-
egories are drawn from a minimal set of symbols C
such that:
NP 2 C and S 2 C;
if ; ? 2 C then =? 2 C and n? 2 C:
Intuitively, the category NP describes a noun phrase
and the category S describes a sentence, and the
complex categories =? and n? describe `a  lacking
a ? to the right' and `a  lacking a ? to the left'
respectively; so for example SnNP would describe a
declarative verb phrase lacking an NP subject to its
left in the input.
The type T and worst-case (most general) denota-
tion W of each possible category are dened below,
given a set of entities E as an environment:
T (S) = t : truth value W (S) = fTRUE;FALSEg
T (NP) = e : entity W (NP) = E
T (=?) = hT (?); T ()i W (=?) = W (?) W ()
T (n?) = hT (?); T ()i W (n?) = W (?) W ()
The denotation D of any proposed constituent is
constrained to be a subset of the worst-case deno-
tation W of the constituent's category; so a con-
stituent of category NP would denote a set of en-
tities, fe
1
; e
2
; : : : g, and a constituent of category
SnNP would denote a set of entity  truth value
pairs, fhe
1
;TRUEi; he
2
;FALSEi; : : : g. Note that no
denotation of a constituent can contain more than
O(jEj
v
) dierent elements, where v is a valency mea-
sure of the number of NP symbols occurring within
the constituent's category.
This paper will use the following denition of a
categorial grammar (CG):
Denition A categorial grammar G is a formal
grammar (N;; P ) such that:
  is a nite set of words w;
 P is a nite set of productions containing:
 ! w for all w2, with  2 C,
 ! =? ? for every rule =? ! : : : in P ,
 ! ? n? for every rule n? ! : : : in P ,
and nothing else;
 N is the nonterminal set f j  ! : : : 2 Pg.
and the following deductive parser,
1
which will be
extended later to handle a richer set of semantic op-
erations. The parser is dened with:
 constituent chart items [i; j; ] drawn from I
n
0

I
n
0
N , indicating that positions i through j in
the input can be characterized by category ;
 a lexical item [i; j; ] for every rule  ! w 2 P if
w occurs between positions i and j in the input;
 a set of rules of the form:
[i;k;=?] [k;j;?]
[i;j;]
for all  ! =? ? 2 P; i; j; k 2 I
n
0
,
[k;j;n?] [i;k;?]
[i;j;]
for all  ! ? n? 2 P; i; j; k 2 I
n
0
.
and can recognize an n-length input as a constituent
of category  (for example, as an S) if it can deduce
the chart item [0; n; ].
This parser can be implemented in a dynamic pro-
gramming algorithm, using the recursive function:
F (x) =
_
a
1
::: a
k
s:t:
a
1
::: a
k
x
k
^
i=1
F (a
i
)
(where x; a
1
: : : a
k
are proposed constituents drawn
from I
n
0
I
n
0
N ,
W
;
= FALSE, and
V
;
= TRUE),
by recording the result of every recursive sub-call to
F (x) in a chart, then consulting this chart on sub-
sequent calls to F (x) for the same x constituent.
2
Since the indices in every rule's antecedent con-
stituents a
1
: : : a
k
each cover smaller spans than
those in the consequent x, the algorithm will not
enter into an innite recursion; and since there are
only n
2
jN j dierent values of x, and only 2n dier-
ent rules that could prove any consequent x (two rule
forms for = and n, each with n dierent values of k),
the algorithm runs in polynomial time: O(n
3
jN j).
The resulting chart can then be annotated with back
pointers to produce a polynomial-sized shared forest
1
Following Shieber et al (1995).
2
Following Goodman (1999).
representation of all possible grammatical trees (Bil-
lot and Lang, 1989).
Traditional corpus-based parsers select preferred
trees from such forests by calculating Viterbi scores
for each proposed constituent, according to the re-
cursive function:
S
V
(x) = max
a
1
::: a
k
s:t:
a
1
::: a
k
x
 
k
Y
i=1
S
V
(a
i
)
!
 P(a
1
::: a
k
j x)
These scores can be calculated in polynomial time,
using the same dynamic programming algorithm as
that described for parsing. A tree can then be se-
lected, from the top down, by expanding the highest-
scoring rule application for each constituent.
The environment-based parser described here uses
a similar mechanism to select preferred trees, but the
scores are based on the presence or absence of enti-
ties in the denotation (interpretation) of each pro-
posed constituent:
3
S
D
(x) = max
a
1
::: a
k
s:t:
a
1
::: a
k
x
 
k
X
i=1
S
D
(a
i
)
!
+
(
1 if D(x) 6=;
0 otherwise
where the denotation D(x) of a proposed constituent
x is calculated using another recursive function:
D(x) =
[
a
1
::: a
k
s:t:
a
1
::: a
k
x
 

k
on
i=1
D(a
i
)
!
on
(
R(x) if k = 0
fhig otherwise
in which R(x) is a lexical relation dened for each
axiom x of category  equal to some subset of 's
worst-case denotation W (), as dened above.
4
The
operator on is natural (relational) join on the elds
of its operands:
AonB = fhe
1
:::e
max(a;b)
i j he
1
:::e
a
i2A; he
1
:::e
b
i2Bg
where a; b  0; and  is a projection that removes
the rst element of the result (corresponding the
most recently discharged argument of the head or
functor category):
A = fhe
2
:::e
a
i j he
1
:::e
a
i2Ag
This interleaving of semantic evaluation and pars-
ing for the purpose of disambiguation has much in
common with that of Dowding et al (1994), except
3
Here, the score is simply equal to the number of non-
empty constituents in an analysis, but other metrics are pos-
sible.
4
So a lexical relation for the constituent `lemon' of
category NP would contain all and only the lemons in
the environment, and a lexical relation for the con-
stituent `falling' of category SnNP would contain a map-
ping from every entity in the environment to some truth
value (TRUE if that entity is falling, FALSE otherwise):
e.g. fhlemon
1
; TRUEi; hlemon
2
; FALSEi; : : : g.
NP[lemon]
fl
1
; l
2
; l
3
; l
4
g
P:NPnNP/NP[in]
fhb
1
; hl
1
; l
1
ii; hm
1
; hl
2
; l
2
iig
NP[bin]
fb
1
; b
2
g
P:NPnNP/NP[by]
fhm
1
; hb
1
; b
1
ii; hm
2
; hb
2
; b
2
iig
NP[machine]
fm
1
;m
2
;m
3
g
PP:NPnNP[in]
fhl
1
; l
1
ig
PP:NPnNP[by]
fhb
1
; b
1
i; hb
2
; b
2
ig
NP[lemon]
fl
1
g
NP[bin]
fb
1
; b
2
g
PP:NPnNP[in]
fhl
1
; l
1
ig
NP[lemon]
fl
1
g [ ;
Figure 1: Denotation-annotated forest for `lemon in bin by machine.'
that in this case, constituents are not only seman-
tically type-checked, but are also fully interpreted
each time they are proposed.
Figure 1 shows a sample denotation-annotated
forest for the phrase `the lemon in the bin by the
machine', using the lexicalized grammar:
lemon, bin, machine : NP
the : NP=NP
in, by : NPnNP=NP
in which the denotation of each constituent (the set
in each rectangle) is calculated using a join on the
denotations of each pair of constituents that combine
to produce it. In this example, the right-branching
tree would be preferred because the denotation re-
sulting from the composition at the root of the other
tree would be empty.
Since this use of the join operation is linear on the
sum of the cardinalities of its operands, and since
the denotations of the categories in a grammar G
are bounded in cardinality by O(jEj
v
) where v is the
maximum valency of the categories in G, the total
complexity of the above algorithm can be shown to
be O(n
3
jEj
v
): polynomial not only on the length of
the input n, but also on the size of the environment E
(Schuler, 2001).
3 Extended algorithm
The above algorithm works well for attaching ordi-
nary complements and modiers, but as a semantic
theory it is not su?ciently expressive to produce cor-
rect denotations in all cases. For example, the lexical
relations dened above are insu?cient to represent
quantiers like `no' (using category NP=NP) in the
phrase `the boy with no backpack.'
5
A similar prob-
lem occurs with conjunctions; for example, the word
`and' (using category NPnNP=NP) in the phrase `the
child wearing glasses and blue pants', also cannot
be properly represented as a lexical relation.
6
This
raises the question: how much expressivity can be
allowed in a shared semantic interpretation without
exceeding the tractable parsing complexity neces-
sary for practical environment-based parsing?
In traditional categorial semantics (Montague,
1973; Barwise and Cooper, 1981; Keenan and Stavi,
1986) quantiers and noun phrase conjunctions de-
note higher-order relations: that is, relations be-
tween whole sets of entities instead of just be-
tween individuals. Under this interpretation, a
quantier like `no' would denote a set of pairs
fhA
1
; B
1
i; hA
2
; B
2
i; : : : g where each A
i
and B
i
are
disjoint subsets of E , corresponding to an accept-
able pair of restrictor and body sets satisfying the
quantier `no'. Unfortunately, since the cardinalities
of these higher-order denotations can be exponential
on the size of the environment E (there are 2
jEj
pos-
sible subsets of E and 2
2jEj
possible combinations of
two such subsets), such an approach would destroy
the polynomial complexity of the environment-based
parsing algorithm.
5
Assigning the identity relation fhe
1
; e
1
i; he
2
; e
2
i; : : : g to
the quantier would incorrectly yield the set of boys with a
backpack as a denotation for the full noun phrase; and assign-
ing the converse relation (from each entity in the environment
to every other entity fhe
1
; e
2
i; he
1
; e
3
i; : : : g) would incorrectly
yield the set of boys with anything that is not a backpack.
6
The identity relation fhe
1
; e
1
; e
1
i; he
2
; e
2
; e
2
i; : : : g, which
yields a correct interpretation in verb phrase conjunction,
would yield an incorrect denotation for the noun phrase
`glasses and blue pants,' containing only entities which are
at once both glasses and pants.
However, if the number of possible higher-order
functions is restricted to a nite set (say, to some
subset of words in a lexicon), it becomes tractable
to store them by name rather than by denotation
(i.e. as sets). Such function can then discharge all
their rst-order arguments in a single derivational
step to produce a rst-order result, in order to avoid
generating or evaluating any higher-order partial re-
sults. Syntactically, this would be analogous to com-
posing a quantier with both a noun phrase restric-
tor and a body predicate (e.g. a verb or verb phrase)
at the same time, to produce another rst-order
predicate (e.g. a verb phrase or sentence). Since
a generalized quantier function merely counts and
compares the cardinalities of its arguments in a lin-
ear time operation, this analysis provides a tractable
shortcut to the exponential calculations required in
the conventional analysis.
Note that this analysis by itself does not admit
productive modication of quantiers (because their
functions are drawn from some nite set) or of quan-
tied noun phrases (because they are no longer de-
rived as a partial result). This causes no disruption
to the attachment of non-conjunctive modiers, be-
cause ordinary syntactic modiers of quantier con-
stituents are seldom productive (in the sense that
their composition does not yield functions outside
some nite set), and syntactic modiers of NP con-
stituents usually only modify the restrictor set of the
quantier rather than the entire quantied function,
and can therefore safely be taken to attach below
the quantier, to the unquantied NP.
But this is not true in cases involving conjunc-
tion. Conjoined quantiers, like `some but not all,'
cannot always be dened using a single standard lex-
ical function; and conjunctions of quantied noun
phrases, like `one orange and one lemon', cannot
be applied to unquantied subconstituents (syntac-
tically, because this would fail to subsume the sec-
ond quantier, and semantically, because it is not
the restrictor sets which are conjoined). Keenan
and Stavi (1986) model conjunctions of quantiers
and quantied noun phrases using lattice operations
on higher-order sets, but as previously stated, these
higher-order sets preclude tractable interleaving of
semantic interpretation with parsing.
The solution proposed here is to treat each quan-
tier or quantied noun phrase conjunction as an el-
liptical conjunction of two complete rst-order pred-
icates (e.g. verb phrases or sentences), each subsum-
ing a dierent quantier and noun phrase restrictor
(in the case of NP conjunction), but sharing or du-
plicating a common body predicate. This analysis
requires multiple components to keep track of the
duplicated material above the conjunction, but as
long as the number of components is bounded, the
polynomial complexity of the parsing algorithm is
containing
(duplicated)
one orange
(unduplicated)
and one lemon
(unduplicated)
Figure 2: Duplicated verb in NP conjunction.
retained.
7
Figure 2 shows a duplicated verb predicate in the
derivation of an NP conjunction. The conjoined
constituents (the shaded regions in the gure) are
each composed of two components: one for the NP
itself, containing the quantier and the restrictor
predicate, and one for the verb which supplies the
body predicate of the quantier. Since the conjoined
constituents both correspond to complete quanti-
er expressions with no unsatised rst-order argu-
ments, their categories are that of simple rst-order
predicates (they are each complete verb phrases in
essence: `containing one orange' and `containing one
lemon'). The conjunction then forms a larger con-
stituent of the same form (the unshaded outline
in the gure), with a lower component containing
the conjoined constituents' NP components concate-
nated in the usual way, and an upper component in
which the conjoined constituents' non-NP compo-
nents are identied or overlapped. If the duplicated
components do not cover the same string yield, the
conjunction does not apply.
Note that, since they are only applied to ordinary
rst-order predicates (e.g. sentences or verb phrases)
in this analysis, conjunctions can now safely be as-
signed the familiar truth-functional denotations in
every case.
8
Also, since the resulting constituent
has the same number of components as the conjoined
constituents, there is nothing to prevent its use as
an argument in subsequent conjunction operations.
A sample multi-component analysis for quantiers
is shown below, allowing material to be duplicated
both to the left and to the right of a conjoined NP:
some,all,no,etc. : XnNP
q
 NP
q
nNP
q
NP
q
=NP

X=NP
q
 NP
q
nNP
q
NP
q
=NP

The lexical entry for a quantier can be split in this
7
Dahl and McCord (1983) propose a similar duplication
mechanism to produce appropriate semantic representations
for NP and other conjunctions, but for dierent reasons.
8
e.g. for the word `and': fh:::TRUE; :::TRUE; :::TRUEi;
h::TRUE; ::FALSE; ::FALSEi; h::FALSE; ::TRUE; ::FALSEi;
h::FALSE; ::FALSE; ::FALSEig
way into a number of components, the last (or low-
est) of which is not duplicated in conjunction while
others may or may not be. These include a com-
ponent for the quantier NP
q
=NP

(which will ulti-
mately also contain a noun phrase restrictor of cate-
gory NP

), a component for restrictor PPs and rela-
tive clauses of category NP
q
nNP
q
that are attached
above the quantier and duplicated in the conjunc-
tion, and a component for the body (a verb or verb
phrase or other predicate) of category XnNP
q
or
X=NP
q
. The subscript q species one of a nite
set of quantiers, and the subscript  indicates an
unquantied NP.
The deductive parser presented in Section 2 can
now be extended by incorporating sequences of rec-
ognized and unrecognized components into the con-
stituent chart items. As constituents are com-
posed, components are shifted from the unrecog-
nized sequence 
1
   
c
to the recognized sequence
hi
1
; j
1
; 
1
i    hi
c
; j
c
; 
c
i, until the unrecognized se-
quence is empty.
The extended parser is dened with:
 chart items of the form [i; j;;], where  is
a sequence of unrecognized components ,  is
a sequence of recognized components ha; b; i,
and i; j; k; a; b; c are indices in the input. Each
item [i; j;; hi
1
; j
1
; 
1
i    hi
c
; j
c
; 
c
i] indicates
that the span from i to j in the input can be
characterized by the categories 
1
through 
c
at
positions i
1
to j
1
through i
c
to j
c
respectively, so
that if these spans are concatenated in whatever
order they occur in the input string, they form
a grammatical constituent of category  with
unrecognized components .
 a lexical item [i; j; ; hi; j; i] for every rule  !
w 2 P if w occurs between positions i and j in
the input;
 a set of rules for all i; j; k; a; b; c 2 I
n
0
as below.
Two rules to invoke left and right function ap-
plication to an existing component:
[i;k;=?;hi;k;=?i] [k;j;?;hk;b;?="i]
[i;j;;hi;b;="i]
!=? ?2P ,
[k;j;n?;hk;j;n?i] [i;k;?;ha;k;?n"i]
[i;j;;ha;j;n"i]
!? n?2P ,
Two rules to invoke left and right function ap-
plication to a fresh component:
[i;k;=?;hi;k;=?i] [k;j;=??;]
[i;j;;hi;k;=?i]
!=? ?2P ,
[k;j;n?;hk;j;n?i] [i;k;n??;]
[i;j;;hk;j;n?i]
!? n?2P ,
Two rules to discharge empty components:
[i;j;=??;]
[i;j;;]
[i;j;n??;]
[i;j;;]
Three rules to skip conjunctions, by adding a
gap between the components in a constituent
(the rst rule consumes the conjunction to cre-
ate a partial result of category Conj
0
?
, and the
latter two use this to skip the opposing NP):
[k;j;?;]
[i;j;Conj
0
?
;]
[i;k;Conj;hi;k;Conji]
[k;j;Conj
0
?
;]
[i;j;?;]
[i;k;?; ]
[i;k;?;]
[i;j;?;]
[k;j;Conj
0
?
; ]
Two rules to reassemble discontinuous con-
stituents (again, using a partial result Conj
0
?
to
reduce the number of ranging variables):
[a;c;Conj;ha;c;Conji] [i;j;;hc;b;?i]
[i;j;;ha;b;Conj
0
?
i]
[i;j;;hc;b;Conj
0
?
i] [i;j;;ha;c;?i]
[i;j;;ha;b;?i]
Two rules to combine adjacent components:
[i;j;;ha;c;?="ihc;b;"i]
[i;j;;ha;b;?i]
[i;j;;hc;b;?n"iha;c;"i]
[i;j;;ha;b;?i]
And one rule to apply quantier functions:
[i;j;;ha;b;?
q
i]
[i;j;;ha;b;?

i]
The parsing and scoring functions remain identi-
cal to those in Section 2, but an additional k = 1
case containing a modied projection function  is
now added to the interpretation function, in order
to make the denotations of quantied constituents
depend on their associated quantiers:
D(x) =
[
a
1
::: a
k
s:t:
a
1
::: a
k
x
8
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
R(x) if k = 0

q
D(a
1
) if k = 1 and
a
1
x
=
[:::h:::?
q
i]
[:::h:::?

i]
k
on
i=1
D(a
i
) otherwise
The modied projection function evaluates a quan-
tier function q on some argument denotation A,
comparing the cardinality of the image of the re-
strictor set in A with the the cardinality of image of
the intersected restrictor and body sets in A:
9

q
A = fhe
2
:::e
a
; ti j h ; e
2
:::e
a
; i2A; t = q(jRj; jSj)
R = Aonfh ; e
2
:::e
a
; ig;
S = Aonfh ; e
2
:::e
a
;TRUEig g
This algorithm parses a categorial grammar in the
usual way { constituents are initially added to the
chart as single components covering a certain yield
in the input string (the indices of the component
are the same as the indices of the constituent itself),
and they are combined by concatenating the yields
of smaller constituents to make larger ones { until a
conjunction is encountered. When a conjunction is
9
Following Keenan and Stavi (1986).
0 1 2 3 4
containing one orange and one lemon
[0; 1; SnNP
q
=NP
q
0
; [1; 2; XnNP
9
 NP
9
nNP
9
 NP
9
; [2; 3;Conj; [3; 4; XnNP
9
 NP
9
nNP
9
 NP
9
;
h0; 1; SnNP
q
=NP
q
0
i] h1; 2;NP
9
i] fo
1
; o
2
; o
3
; o
4
g h2; 3;Conji] h3; 4;NP
9
i] fl
1
; l
2
; l
3
g
fho
1
; x
1
i; hl
2
; x
1
i; hl
3
; x
3
ig
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (1)
[1; 4; XnNP
9
 NP
9
nNP
9
 NP
9
; h1; 2; NP
9
i] fo
1
; o
2
; o
3
; o
4
g
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (2)
[1; 4; XnNP
9
 NP
9
nNP
9
 NP
9
; h3; 4; NP
9
i] fl
1
; l
2
; l
3
g
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (3)
[1; 4; XnNP
9
 NP
9
; h1; 2; NP
9
i] fo
1
; o
2
; o
3
; o
4
g
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (4)
[1; 4; XnNP
9
 NP
9
; h3; 4; NP
9
i] fl
1
; l
2
; l
3
g
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (5)
[0; 4; SnNP
q
; h0; 1; SnNP
q
=NP
9
i  h1; 2;NP
9
i] fho
1
; x
1
ig
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (6)
[0; 4; SnNP
q
; h0; 1; SnNP
q
=NP
9
i  h3; 4;NP
9
i] fhl
2
; x
1
i; hl
3
; x
3
ig
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (7)
[0; 4; SnNP
q
; h0; 1; SnNP
q
=NP

i  h1; 2;NP

i] fx
1
g
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (8)
[0; 4; SnNP
q
; h0; 1; SnNP
q
=NP

i  h3; 4;NP

i] fx
1
; x
3
g
(9)
[0; 4; SnNP
q
; h0; 1; SnNP
q
=NP

i  h1; 4;NP

i] fx
1
g
(10)
[0; 4; SnNP
q
; h0; 4; SnNP
q
i] fx
1
g
Figure 3: Sample derivation of conjoined NP.
encountered immediately to the left or right of a rec-
ognized constituent constituent x, and another con-
stituent of the same category is found immediately
beyond that conjunction, the parser creates a new
constituent that has the combined yield of both con-
stituents, but copies x's component yield (the string
indices of x's original components) with no change.
This has the eect of creating two new constituents
every time two existing constituents are conjoined:
each with a dierent component yield, but both with
the same (combined) constituent yield. These new
discontinuous constituents (with component yields
that do not exhaust their constituent yields) are still
treated as ordinary constituents by the parser, which
combines them with arguments and modiers until
all of their argument positions have been success-
fully discharged, at which point pairs of discontinu-
ous constituents with the same constituent yield can
be reassembled into whole { or at least less discon-
tinuous { constituents again.
A sample derivation for the verb phrase `con-
taining one orange and one lemon,' involving con-
junction of existentially quantied noun phrases, is
shown in Figure 3, using the above parse rules and
the lexicalized grammar:
containing : SnNP
q
=NP
q
0
one : XnNP
q
 NP
q
nNP
q
 NP
q
=NP

X=NP
q
 NP
q
nNP
q
NP
q
=NP

orange, lemon : NP

and : Conj
First the parser applies the skip conjunction rules
to obtain the discontinuous constituents shown af-
ter steps (1) and (2), and a component is discharged
from each of the resulting constituents using the
empty component rule in steps (3) and (4). The
constituents resulting from (3) and (4) are then com-
posed with the verb constituent for `containing' in
steps (5) and (6), using the left attachment rule for
fresh components. The quantiers are then applied
in steps (7) and (8), and the resulting constituents
are reassembled using the conjunction rules in step
(9). The adjacent components in the constituent
resulting from step (9) are then merged using the
combination rule in step (10), producing a complete
gapless constituent for the entire input.
Since the parser rules are xed, and the number of
components in any chart constituent is bounded by
the maximum number of components in a category
(inasmuch as the rules can only add a component
to the recognized list by subtracting one from the
unrecognized list), the algorithm must run in poly-
nomial space and time on the length of the input
sentence. Since the cardinality of each constituent's
denotation is bounded by jEj
v
(where E is the set
of entities in the environment and v is the maxi-
mum valency of any category), the algorithm runs in
worst-case polynomial space on jEj; and since there
is no more than one set composition operation per-
formed when a rule is applied, and each composition
operation runs in worst-case quadratic time on the
size of its composed sets (due to the quantier oper-
ation), the algorithm runs in worst-case polynomial
time on jEj as well.
4 Evaluation
The extended parser described above has been im-
plemented and evaluated on a corpus of 340 spo-
ken instructions to simulated human-like agents in
a controlled 3-D environment (that of children run-
ning a lemonade stand, which was deemed suitably
familiar to undergraduate student subjects). The
parser was run on the word lattice output of an
o-the-shelf speech recognizer (CMU Sphinx II) and
the parser chart was seeded with every hypothesized
word. The parser was also compared with the rec-
ognizer by itself, in order to determine the degree to
which an environment-based approach could com-
plement corpus-based disambiguation. The systems
were evaluated as word recognizers (i.e. ignoring the
brackets in the parser output) on the rst 100 sen-
tences of the corpus (corresponding to the rst seven
of 33 subjects); the latter 240 sentences were re-
served for training the recognizer and for developing
the grammar and semantic lexicon.
The average utterance length was approximately
three seconds (subsuming about 300 frames or posi-
tions in the parser chart), containing an average of
nine words. Parsing time averaged under 40 seconds
per sentence on a P4-1500MHz, most of which was
spent in forest construction rather than denotation
calculation.
Accuracy results show that the parser was able to
correctly identify a signicant number of words that
the recognizer missed (and vice versa), such that a
perfect synthesis of the two (choosing the correct
word if it is recognized by either system) would pro-
duce an average of 8 percentage points more recall
than the recognizer by itself on successful parses,
and as much as 19 percentage points more for some
subjects:
10
recognizer parser joint
subject prec recall fail prec recall recall
0 76 79 18 72 74 92
1 77 75 28 63 55 83
2 70 71 33 49 54 69
3 71 67 43 49 45 69
4 66 54 37 44 39 67
5 53 52 54 36 31 72
6 84 84 50 56 63 83
all 68 67 37 53 50 75
which indicates that the environment may oer a
useful additional source of information for disam-
biguation. Though it may not be possible to imple-
ment a perfect synthesis of the environment-based
10
Successful parses are those that result in one or more
complete analyses of the input, even if the correct tree is not
among them.
and corpus-based approaches, if even half of the
above gains can be realized, it would mark a sig-
nicant advance.
5 Conclusion
This paper has described an extension to an
environment-based parsing algorithm, increasing its
semantic coverage to include quantier and conjunc-
tion operations without destroying its polynomial
worst-case complexity. Experiments using an imple-
mentation of this algorithm on a corpus of spoken
instructions indicate that 1) the observed complex-
ity of the algorithm is suitable for practical user in-
terface applications, and 2) the ability to draw on
this kind of environment information in an inter-
faced application has the potential to greatly im-
prove recognition accuracy in speaker-independent
mixed-initiative interfaces.
References
Kazimierz Ajdukiewicz. 1935. Die syntaktische konnex-
itat. In S. McCall, editor, Polish Logic 1920-1939,
pages 207{231. Oxford University Press. Translated
from Studia Philosophica 1: 1{27.
Yehoshua Bar-Hillel. 1953. A quasi-arithmetical nota-
tion for syntactic description. Language, 29:47{58.
Jon Barwise and Robin Cooper. 1981. Generalized
quantiers and natural language. Linguistics and Phi-
losophy, 4.
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Proceedings of
the 27
th
Annual Meeting of the Association for Com-
putational Linguistics (ACL '89), pages 143{151.
Veronica Dahl and Michael C. McCord. 1983. Treating
coordination in logic grammars. American Journal of
Computational Linguistics, 9(2):69{91.
John Dowding, Robert Moore, Francois Andery, and
Douglas Moran. 1994. Interleaving syntax and seman-
tics in an e?cient bottom-up parser. In Proceedings of
the 32nd Annual Meeting of the Association for Com-
putational Linguistics (ACL'94).
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573{605.
E. Keenan and J. Stavi. 1986. A semantic characteriza-
tion of natural language determiners. Linguistics and
Philosophy, 9:253{326.
Richard Montague. 1973. The proper treatment of quan-
tication in ordinary English. In J. Hintikka, J.M.E.
Moravcsik, and P. Suppes, editors, Approaches to Nat-
ural Langauge, pages 221{242. D. Riedel, Dordrecht.
Reprinted in R. H. Thomason ed., Formal Philosophy,
Yale University Press, 1994.
William Schuler. 2001. Computational properties of
environment-based disambiguation. In Proceedings of
the 39th Annual Meeting of the Association for Com-
putational Linguistics (ACL '01), Toulouse, France.
Stuart M. Shieber, Yves Schabes, and Fernando C.N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. Journal of Logic Programming, 24:3{
36.
Multi-Component TAG and Notions of Formal Power
William Schuler, David Chiang
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
fschuler,dchiangg@linc.cis.upenn.edu
Mark Dras
Inst. for Research in Cognitive Science
University of Pennsylvania
Suite 400A, 3401 Walnut Street
Philadelphia, PA 19104-6228
madras@linc.cis.upenn.edu
Abstract
This paper presents a restricted version
of Set-Local Multi-Component TAGs
(Weir, 1988) which retains the strong
generative capacity of Tree-Local Multi-
Component TAG (i.e. produces the
same derived structures) but has a
greater derivational generative capacity
(i.e. can derive those structures in more
ways). This formalism is then applied as
a framework for integrating dependency
and constituency based linguistic repre-
sentations.
1 Introduction
An aim of one strand of research in gener-
ative grammar is to nd a formalism that
has a restricted descriptive capacity sucient
to describe natural language, but no more
powerful than necessary, so that the reasons
some constructions are not legal in any nat-
ural language is explained by the formalism
rather than stipulations in the linguistic the-
ory. Several mildly context-sensitive grammar
formalisms, all characterizing the same string
languages, are currently possible candidates
for adequately describing natural language;
however, they dier in their capacities to as-
sign appropriate linguistic structural descrip-
tions to these string languages. The work in
this paper is in the vein of other work (Joshi,
2000) in extracting as much structural de-
scriptive power given a xed ability to de-
scribe strings, and uses this to model depen-
dency as well as constituency correctly.
One way to characterize a formalism's de-
scriptive power is by the the set of string lan-
guages it can generate, called its weak gener-
ative capacity. For example, Tree Adjoining
Grammars (TAGs) (Joshi et al, 1975) can
generate the language a
n
b
n
c
n
d
n
and Context-
Free Grammars (CFGs) cannot (Joshi, 1985).
S
a  b
S
a S
a  b
b
S
a S
a S
a  b
b
b
: : :
Figure 1: CFG-generable tree set for a
n
b
n
.
S
a S
b 
S
a S
a S
b S
b 
S
a S
a S
a S
b S
b S
b 
: : :
Figure 2: TAG-generable tree set for a
n
b
n
.
However, weak generative capacity ignores
the capacity of a grammar formalism to gener-
ate derived trees. This is known as its strong
generative capacity. For example, CFGs and
TAGs can both generate the language a
n
b
n
,
but CFGs can only associate the a's and b's
by making them siblings in the derived tree,
as shown in Figure 1, whereas a TAG can gen-
erate the innite set of trees for the language
a
n
b
n
that have a's and b's as siblings, as well
as the innite set of trees where the a's dom-
inate the b's in each tree, shown in Figure 2
(Joshi, 1985); thus TAGs have more strong
generative capacity than CFGs.
In addition to the tree sets and string lan-
guages a formalism can generate, there may
also be linguistic reasons to care about how
these structures are derived. For this reason,
multi-component TAGs (MCTAGs) (Weir,
1988) have been adopted to model some
linguistic phenomena. In multi-component
TAG, elementary trees are grouped into tree
sets, and at each step of the derivation all the
trees of a set adjoin simultaneously. In tree-
local MCTAG (TL-MCTAG) all the trees of
a set are required to adjoin into the same
elementary tree; in set-local MCTAG (SL-
MCTAG) all the trees of a set are required
to adjoin into the same elementary tree set.
TL-MCTAGs can generate the same string
languages and derived tree sets as ordinary
TAGs, so they have the same weak and strong
generative capacities, but TL-MCTAGs can
derive these same strings and trees in more
than TAGs can. One motivation for TL-
MCTAG as a linguistic formalism (Frank,
1992) is that it can generate a functional head
(such as does) in the same derivational step
as the lexical head with which it is associated
(see Figure 3) without violating any assump-
tions about the derived phrase structure tree
{ something TAGs cannot do in every case.

seem
:
S
does
S
.
.
.
VP
seem VP

sleep
:
S
John
VP
to sleep

sleep

seem
S
does S
John VP
seem VP
to sleep
Figure 3: TL-MCTAG generable derivation
This notion of the derivations of a gram-
mar formalism as they relate to the struc-
tures they derive has been called the deriva-
tional generative capacity (1992). Somewhat
more formally (for a precise denition, see
Becker et al (1992)): we annotate each ele-
ment of a derived structure with a code indi-
cating which step of the derivation produced
that element. This code is simply the address
of the corresponding node in the derivation
tree.
1
Then a formalism's derivational gener-
ative capacity is the sets of derived structures,
thus annotated, that it can generate.
1
In Becker et al (1992) the derived structures were
always strings, and the codes were not addresses but
unordered identiers. We trust that our denition is
in the spirit of theirs.
The derivational generative capacity of a
formalism also describes what parts of a de-
rived structure combine with each other. Thus
if we consider each derivation step to corre-
spond to a semantic dependency, then deriva-
tional generative capacity describes what
other elements a semantic element may de-
pend on. That is, if we interpret the derivation
trees of TAG as dependency structures and
the derived trees as phrase structures, then
the derivational generative capacity of TAG
limits the possible dependency structures that
can be assigned to a given phrase structure.
1.1 Dependency and Constituency
We have seen that TL-MCTAGs can gener-
ate some derivations for \Does John seem
to sleep" that TAG cannot, but even TL-
MCTAG cannot generate the string, \Does
John seem likely to sleep" with a derived tree
that matches some linguistic notion of correct
constituency and a derivation that matches
some notion of correct dependency. This is
because the components for `does' and `seem'
would have to adjoin into dierent compo-
nents of the elementary tree set for `likely'
(see Figure 4), which would require a set-local
multi-component TAG instead of tree-local.

seem
:
S
does
S
.
.
.
VP
seem VP

likely
:
S
.
.
.
VP
likely VP

sleep
:
S
John VP
to sleep

sleep

likely

seem
Figure 4: SL-MCTAG generable derivation
Unfortunately, unrestricted set-local multi-
component TAGs not only have more deriva-
tional generative capacity than TAGs, but
they also have more weak generative capac-
ity: SL-MCTAGs can generate the quadru-
ple copy language wwww, for example, which
does not correspond to any known linguis-
tic phenomenon. Other formalisms aiming to
model dependency correctly similarly expand
weak generative capacity, notably D-tree Sub-
stitution Grammar (Rambow et al, 1995),
and consequently end up with much greater
parsing complexity.
The work in this paper follows another
Figure 5: Set-local adjunction.
line of research which has focused on squeez-
ing as much strong generative capacity as
possible out of weakly TAG-equivalent for-
malisms. Tree-local multicomponent TAG
(Weir, 1988), nondirectional composition
(Joshi and Vijay-Shanker, 1999), and seg-
mented adjunction (Kulick, 2000) are exam-
ples of this approach, wherein the constraint
on weak generative capacity naturally limits
the expressivity of these systems. We discuss
the relation of the formalism of this paper,
Restricted MCTAG (R-MCTAG) with some
of these in Section 5.
2 Formalism
2.1 Restricting set-local MCTAG
The way we propose to deal with multi-
component adjunction is rst to limit the
number of components to two, and then,
roughly speaking, to treat two-component
adjunction as one-component adjunction by
temporarily removing the material between
the two adjunction sites. The reasons behind
this scheme will be explained in subsequent
sections, but we mention it now because it
motivates the somewhat complicated restric-
tions on possible adjunction sites:
 One adjunction site must dominate the
other. If the two sites are 
h
and 
l
, call
the set of nodes dominated by one node
but not strictly dominated by the other
the site-segment h
h
; 
l
i.
 Removing a site-segment must not de-
prive a tree of its foot node. That is, no
site-segment h
h
; 
l
i may contain a foot
node unless 
l
is itself the foot node.
 If two tree sets adjoin into the same tree,
the two site-segments must be simulta-
neously removable. That is, the two site-
segments must be disjoint, or one must
contain the other.
Because of the rst restriction, we depict
tree sets with the components connected by
a dominance link (dotted line), in the man-
ner of (Becker et al, 1991). As written, the
above rules only allow tree-local adjunction;
we can generalize them to allow set-local ad-
junction by treating this dominance link like
an ordinary arc. But this would increase the
weak generative capacity of the system. For
present purposes it is sucient just to allow
one type of set-local adjunction: adjoin the
upper tree to the upper foot, and the lower
tree to the lower root (see Figure 5).
This does not increase the weak generative
capacity, as will be shown in Section 2.3. Ob-
serve that the set-local TAG given in Figure 5
obeys the above restrictions.
2.2 2LTAG
For the following section, it is useful to think
of TAG in a manner other than the usual.
Instead of it being a tree-rewriting system
whose derivation history is recorded in a
derivation tree, it can be thought of as a set
of trees (the `derivation' trees) with a yield
function (here, reading o the node labels of
derivation trees, and composing correspond-
ing elementary trees by adjunction or sub-
stitution as appropriate) applied to get the
TAG trees. Weir (1988) observed that several
TAGs could be daisy-chained into a multi-
level TAG whose yield function is the com-
position of the individual yield functions.
More precisely: a 2LTAG is a pair of
TAGs hG;G
0
i = hh;NT ; I; A; Si; hI [ A; I [
A; I
0
; A
0
; S
0
ii.
We call G the object-level grammar, and
G
0
the meta-level grammar. The object-level
grammar is a standard TAG:  and NT are
its terminal and nonterminal alphabets, I and
A are its initial and auxiliary trees, and S 2 I
contains the trees which derivations may start
with.
The meta-level grammar G
0
is dened so
that it derives trees that look like derivation
trees of G:
 Nodes are labeled with (the names of)
elementary trees of G.
 Foot nodes have no labels.
 Arcs are labeled with Gorn addresses.
2
2
The Gorn address of a root node is ; if a node has
Gorn address , then its ith child has Gorn address


Figure 6: Adjoining into  by removing 

.
 An auxiliary tree may adjoin anywhere.
 When a tree  is adjoined at a node ,  is
rewritten as , and the foot of  inherits
the label of .
The tree set of hG;G
0
i, T (hG;G
0
i), is
f
G
[T (G
0
)], where f
G
is the yield function of
G and T (G
0
) is the tree set of G
0
. Thus, the
elementary trees of G
0
are combined to form
a derived tree, which is then interpreted as a
derivation tree for G, which gives instructions
for combining elementary trees of G into the
nal derived tree.
It was shown in Dras (1999) that when the
meta-level grammar is in the regular form of
Rogers (1994) the formalism is weakly equiv-
alent to TAG.
2.3 Reducing restricted R-MCTAG
to RF-2LTAG
Consider the case of a multicomponent tree
set f
1
; 
2
g adjoining into an initial tree 
(Figure 6). Recall that we dened a site-
segment of a pair of adjunction sites to be all
the nodes which are dominated by the upper
site but not the lower site. Imagine that the
site-segment 

is excised from , and that 
1
and 
2
are fused into a single elementary tree.
Now we can simulate the multi-component
adjunction by ordinary adjunction: adjoin the
fused 
1
and 
2
into what is left of ; then
replace 

by adjoining it between 
1
and 
2
.
The replacement of 

can be postponed
indenitely: some other (fused) tree set
f
1
0
; 
2
0
g can adjoin between 
1
and 
2
, and
so on, and then 

adjoins between the last
pair of trees. This will produce the same re-
sult as a series of set-local adjunctions.
More formally:
1. Fuse all the elementary tree sets of the
grammar by identifying the upper foot
  i.
with the lower root. Designate this fused
node the meta-foot.
2. For each tree, and for every possible com-
bination of site-segments, excise all the
site-segments and add all the trees thus
produced (the excised auxiliary trees and
the remainders) to the grammar.
Now that our grammar has been smashed
to pieces, we must make sure that the right
pieces go back in the right places. We could do
this using features, but the resulting grammar
would only be strongly equivalent, not deriva-
tionally equivalent, to the original. Therefore
we use a meta-level grammar instead:
1. For each initial tree, and for every pos-
sible combination of site-segments, con-
struct the derivation tree that will re-
assemble the pieces created in step (2)
above and add it to the meta-level gram-
mar.
2. For each auxiliary tree, and for every pos-
sible combination of site-segments, con-
struct a derivation tree as above, and for
the node which corresponds to the piece
containing the meta-foot, add a child, la-
bel its arc with the meta-foot's address
(within the piece), and mark it a foot
node. Add the resulting (meta-level) aux-
iliary tree to the meta-level grammar.
Observe that set-local adjunction corre-
sponds to meta-level adjunction along the
(meta-level) spine. Recall that we restricted
set-local adjunction so that a tree set can
only adjoin at the foot of the upper tree and
the root of the lower tree. Since this pair of
nodes corresponds to the meta-foot, we can
restate our restriction in terms of the con-
verted grammar: no meta-level adjunction is
allowed along the spine of a (meta-level) aux-
iliary tree except at the (meta-level) foot.
Then all meta-level adjunction is regular
adjunction in the sense of (Rogers, 1994).
Therefore this converted 2LTAG produces
derivation tree sets which are recognizable,
and therefore our formalism is strongly equiv-
alent to TAG.
Note that this restriction is much stronger
than Rogers' regular form restriction. This
was done for two reasons. First, the deni-
tion of our restriction would have been more
complicated otherwise; second, this restric-
tion overcomes some computational dicul-
ties with RF-TAG which we discuss below.
3 Linguistic Applications
In cases where TAG models dependencies cor-
rectly, the use of R-MCTAG is straightfor-
ward: when an auxiliary tree adjoins at a
site pair which is just a single node, it looks
just like conventional adjunction. However, in
problematic cases we can use the extra expres-
sive power of R-MCTAG to model dependen-
cies correctly. Two such cases are discussed
below.
3.1 Bridge and Raising Verbs
S
NP
John
VP
V
thinks
S
.
.
.
S
S
C
that
S
.
.
.
VP
V
seems
VP
S
NP
Mary
VP
V
to sleep
Figure 7: Trees for (1)
Consider the case of sentences which con-
tain both bridge and raising verbs, noted
by Rambow et al (1995). In most TAG-based
analyses, bridge verbs adjoin at S (or C
0
), and
raising verbs adjoin at VP (or I
0
). Thus the
derivation for a sentence like
(1) John thinks that Mary seems to
sleep.
will have the trees for thinks and seems si-
multaneously adjoining into the tree for like,
which, when interpreted, gives an incorrect
dependency structure.
But under the present view we can ana-
lyze sentences like (1) with derivations mir-
roring dependencies. The desired trees for (1)
are shown in Figure 7. Since the tree for that
seems can meta-adjoin around the subject,
the tree for thinks correctly adjoins into the
tree for seems rather than eat.
Also, although the above analysis produces
the correct dependency links, the directions
are inverted in some cases. This is a disad-
vantage compared to, for example, DSG; but
since the directions are consistently inverted,
for applications like translation or statistical
modeling, the particular choice of direction is
usually immaterial.
3.2 More on Raising Verbs
Tree-local MCTAG is able to derive (2a), but
unable to derive (2b) except by adjoining the
auxiliary tree for to be likely at the foot of the
auxiliary tree for seem (Frank et al, 1999).
(2) a. Does John seem to sleep?
b. Does John seem to be likely to
sleep?
The derivation structure of this analysis does
not match the dependencies, however|seem
adjoins into to sleep.
DSG can derive this sentence with a deriva-
tion matching the dependencies, but it loses
some of the advantage of TAG in that, for
example, cases of super-raising (where the
verb is raised out of two clauses) must be ex-
plicitly ruled out by subsertion-insertion con-
straints. Frank et al (1999) and Kulick (2000)
give analyses of raising which assign the de-
sired derivation structures without running
into this problem. It turns out that the anal-
ysis of raising from the previous section, de-
signed for a translation problem, has both
of these properties as well. The grammar is
shown back in Figure 4.
4 A Parser
Figure 8 shows a CKY-style parser for our
restriction of MCTAG as a system of inference
rules. It is limited to grammars whose trees
are at most binary-branching.
The parser consists of rules over items of
one of the following forms, where w
1
  w
n
is
the input; , 
h
, and 
l
specify nodes of the
grammar; i, j, k, and l are integers between 0
and n inclusive; and code is either + or  :
 [; code ; i; ; ; l; ; ] and
[; code ; i; j; k; l; ; ] function as in
a CKY-style parser for standard TAG
(Vijay-Shanker, 1987): the subtree
rooted by  2 T derives a tree whose
fringe is w
i
  w
l
if T is initial, or
w
i
  w
j
Fw
k
  w
l
if T is the lower
auxiliary tree of a set and F is the label
of its foot node. In all four item forms,
code = + i adjunction has taken place
at .
 [; code ; i; j; k; l; ; 
l
] species that the
segment h; 
l
i derives a tree whose
fringe is w
i
  w
j
Lw
k
  w
l
, where L is
the label of 
l
. Intuitively, it means that
a potential site-segment has been recog-
nized.
 [; code ; i; j; k; l; 
h
; 
l
] species, if  be-
longs to the upper tree of a set, that
the subtree rooted by , the segment
h
h
; 
l
i, and the lower tree concatenated
together derive a tree whose fringe is
w
i
  w
j
Fw
k
  w
l
, where F is the la-
bel of the lower foot node. Intuitively, it
means that a tree set has been partially
recognized, with a site-segment inserted
between the two components.
The rules which require dier from a TAG
parser and hence explanation are Pseudopod,
Push, Pop, and Pop-push. Pseudopod applies
to any potential lower adjunction site and is
so called because the parser essentially views
every potential site-segment as an auxiliary
tree (see Section 2.3), and the Pseudopod ax-
iom recognizes the feet of these false auxiliary
trees.
The Push rule performs the adjunction of
one of these false auxiliary trees|that is, it
places a site-segment between the two trees of
an elementary tree set. It is so called because
the site-segment is saved in a \stack" so that
the rest of its elementary tree can be recog-
nized later. Of course, in our case the \stack"
has at most one element.
The Pop rule does the reverse: every com-
pleted elementary tree set must contain a
site-segment, and the Pop rule places it back
where the site-segment came from, emptying
the \stack." The Pop-push rule performs set-
local adjunction: a completed elementary tree
set is placed between the two trees of yet an-
other elementary tree set, and the \stack" is
unchanged.
Pop-push is computationally the most ex-
pensive rule; since it involves six indices and
three dierent elementary trees, its running
time is O(n
6
G
3
).
It was noted in (Chiang et al, 2000) that
for synchronous RF-2LTAG, parse forests
could not be transferred in time O(n
6
). This
fact turns out to be connected to several prop-
erties of RF-TAG (Rogers, 1994).
3
3
Thanks to Anoop Sarkar for pointing out the rst
The CKY-style parser for regular form
TAG described in (Rogers, 1994) essentially
keeps track of adjunctions using stacks, and
the regular form constraint ensures that the
stack depth is bounded. The only kinds of ad-
junction that can occur to arbitrary depth are
root and foot adjunction, which are treated
similarly to substitution and do not aect the
stacks. The reader will note that our parser
works in exactly the same way.
A problem arises if we allow both root
and foot adjunction, however. It is well-known
that allowing both types of adjunction creates
derivational ambiguity (Vijay-Shanker, 1987):
adjoining 
1
at the foot of 
2
produces the
same derived tree that adjoining 
1
at the
root of 
2
would. The problem is not the am-
biguity per se, but that the regular form TAG
parser, unlike a standard TAG parser, does
not always distinguish these multiple deriva-
tions, because root and foot adjunction are
both performed by the same rule (analogous
to our Pop-push). Thus for a given application
of this rule, it is not possible to say which tree
is adjoining into which without examining the
rest of the derivation.
But this knowledge is necessary to per-
form certain tasks online: for example, enforc-
ing adjoining constraints, computing proba-
bilities (and pruning based on them), or per-
forming synchronous mappings. Therefore we
arbitrarily forbid one of the two possibilities.
4
The parser given in Section 4 already takes
this into account.
5 Discussion
Our version of MCTAG follows other
work in incorporating dependency into a
constituency-based approach to modeling
natural language. One such early integra-
tion involved work by Gaifman (1965), which
showed that projective dependency grammars
could be represented by CFGs. However, it
is known that there are common phenom-
ena which require non-projective dependency
grammars, so looking only at projective de-
such connection.
4
Against tradition, we forbid root adjunction, be-
cause adjunction at the foot ensures that a bottom-up
traversal of the derived tree will encounter elementary
trees in the same order as they appear in a bottom-up
traversal of the derivation tree, simplifying the calcu-
lation of derivations.
Goal: [
r
; ; 0; ; ; n; ; ] 
r
an initial root
(Leaf) [;+; i; ; ; j; ; ]  a leaf
(Foot) [;+; i; i; j; j; ; ]  a lower foot
(Pseudopod) [;+; i; i; j; j; ; ]
(Unary)
[
1
;+; i; p; q; j; 
h
; 
l
]
[; ; i; p; q; j; 
h
; 
l
]


1
(Binary 1)
[
1
;+; i; p; q; j; 
h
; 
l
] [
2
;+; j; ; ; k; ; ]
[; ; i; p; q; k; 
h
; 
l
]


1

2
(Binary 2)
[
1
;+; i; ; ; j; ; ] [
2
;+; j; p; q; k; 
h
; 
l
]
[; ; i; p; q; k; 
h
; 
l
]


1

2
(No adjunction)
[; ; i; p; q; j; 
h
; 
l
]
[;+; i; p; q; j; 
h
; 
l
]
(Push)
[
1
;+; j; p; q; k; ; ] [
h
; ; i; j; k; l; ; 
l
]
[;+; i; p; q; l; 
h
; 
l
]

.
.
.

1
(i.e.  is an upper foot
and 
1
is a lower root)
(Pop)
[
l
; ; j; p; q; k; 
h
0
; 
l
0
] [
r
;+; i; j; k; l; 
h
; 
l
]
[
h
;+; i; p; q; l; 
h
0
; 
l
0
]

r
a root of an upper tree
adjoinable at h
h
; 
l
i
(Pop-push)
[
1
;+; j; p; q; k; ; ] [
r
;+; i; j; k; l; 
h
; 
l
]
[;+; i; p; q; l; 
h
; 
l
]

.
.
.

1
, 
r
a root of an upper
tree adjoinable at
h; 
1
i
Figure 8: Parser
pendency grammars is inadequate. Follow-
ing the observation of TAG derivations' sim-
ilarity to dependency relations, other for-
malisms have also looked at relating depen-
dency and constituency approaches to gram-
mar formalisms.
A more recent instance is D-Tree Substi-
tution Grammars (DSG) (Rambow et al,
1995), where the derivations are also inter-
preted as dependency relations. Thought of
in the terms of this paper, there is a clear
parallel with R-MCTAG, with a local set
ultimately representing dependencies having
some yield function applied to it; the idea
of non-immediate dominance also appears in
both formalisms. The dierence between the
two is in the kinds of languages that they are
able to describe: DSG is both less and more
restrictive than R-MCTAG. DSG can gener-
ate the language count-k for some arbitrary
k (that is, fa
1
n
a
2
n
: : : a
k
n
g), which makes
it extremely powerful, whereas R-MCTAG
can only generate count-4. However, DSG
cannot generate the copy language (that is,
fww j w 2 

g with  some terminal al-
phabet), whereas R-MCTAG can; this may
be problematic for a formalism modeling nat-
ural language, given the key role of the copy
language in demonstrating that natural lan-
guage is not context-free (Shieber, 1985). R-
MCTAG is thus a more constrained relaxation
of the notion of immediate dominance in fa-
vor of non-immediate dominance than is the
case for DSG.
Another formalism of particular interest
here is the Segmented Adjoining Grammar of
(Kulick, 2000). This generalization of TAG is
characterized by an extension of the adjoining
operation, motivated by evidence in scram-
bling, clitic climbing and subject-to-subject
raising. Most interestingly, this extension to
TAG, proposed on empirical grounds, is de-
ned by a composition operation with con-
strained non-immediate dominance links that
looks quite similar to the formalism described
in this paper, which began from formal con-
siderations and was then applied to data. This
conuence suggests that the ideas described
here concerning combining dependency and
constituency might be reaching towards some
deeper connection.
6 Conclusion
From a theoretical perspective, extracting
more derivational generative capacity and
thereby integrating dependency and con-
stituency into a common framework is an in-
teresting exercise. It also, however, proves to
be useful in modeling otherwise problematic
constructions, such as subject-auxiliary inver-
sion and bridge and raising verb interleaving.
Moreover, the formalism developed from the-
oretical considerations, presented in this pa-
per, has similar properties to work developed
on empirical grounds, suggesting that this is
worth further exploration.
References
Tilman Becker, Aravind Joshi, and Owen Ram-
bow. 1991. Long distance scrambling and tree
adjoining grammars. In Fifth Conference of the
European Chapter of the Association for Com-
putational Linguistics (EACL'91), pages 21{26.
Tilman Becker, Owen Rambow, and Michael Niv.
1992. The derivational generative power of for-
mal systems, or, Scrambling is beyond LCFRS.
Technical Report IRCS-92-38, Institute for Re-
search in Cognitive Science, University of Penn-
sylvania.
David Chiang, William Schuler, and Mark Dras.
2000. Some Remarks on an Extension of Syn-
chronous TAG. In Proceedings of TAG+5,
Paris, France.
Mark Dras. 1999. A meta-level grammar: re-
dening synchronous TAG for translation and
paraphrase. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL '99).
Robert Frank, Seth Kulick, and K. Vijay-Shanker.
1999. C-command and extraction in tree-
adjoining grammar. Proceedings of the Sixth
Meeting on the Mathematics of Language
(MOL6).
Robert Frank. 1992. Syntactic locality and
tree adjoining grammar: grammatical acquisi-
tion and processing perspectives. Ph.D. the-
sis, Computer Science Department, University
of Pennsylvania.
Haim Gaifman. 1965. Dependency Systems and
Phrase-Structure Systems. Information and
Control, 8:304{337.
Gerald Gazdar. 1988. Applicability of indexed
grammars to natural languages. In Uwe Reyle
and Christian Rohrer, editors, Natural Lan-
guage Parsin and Linguistic Theories. D. Reidel
Publishing Company, Dordrecht, Holland.
Aravind Joshi and K. Vijay-Shanker. 1999. Com-
positional Semantics with Lexicalized Tree-
Adjoining Grammar (LTAG): How Much Un-
derspecication is Necessary? In Proceedings of
the 2nd International Workshop on Computa-
tional Semantics.
Aravind K. Joshi, Leon S. Levy, and M. Taka-
hashi. 1975. Tree adjunct grammars. Journal
of computer and system sciences, 10:136{163.
Aravind K. Joshi. 1985. How much context sen-
sitivity is necessary for characterizing struc-
tural descriptions: Tree adjoining grammars. In
L. Karttunen D. Dowty and A. Zwicky, editors,
Natural language parsing: Psychological, com-
putational and theoretical perspectives, pages
206{250. Cambridge University Press, Cam-
bridge, U.K.
Aravind Joshi. 2000. Relationship between strong
and weak generative power of formal systems.
In Proceedings of TAG+5, pages 107{114, Paris,
France.
Seth Kulick. 2000. A uniform account of locality
constraints for clitic climbing and long scram-
bling. In Proceedings of the Penn Linguistics
Colloquium.
Owen Rambow, David Weir, and K. Vijay-
Shanker. 1995. D-tree grammars. In Proceed-
ings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL '95).
James Rogers. 1994. Capturing CFLs with tree
adjoining grammars. In Proceedings of the 32nd
Annual Meeting of the Association for Compu-
tational Linguistics (ACL '94).
Stuart Shieber. 1985. Evidence against the
context-freeness of natural language. Linguis-
tics and Philosophy, 8:333{343.
K. Vijay-Shanker. 1987. A study of tree adjoining
grammars. Ph.D. thesis, Department of Com-
puter and Information Science, University of
Pennsylvania.
David Weir. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms.
Ph.D. thesis, Department of Computer and In-
formation Science, University of Pennsylvania.
Computational properties of environment-based disambiguation
William Schuler
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19103
schuler@linc.cis.upenn.edu
Abstract
The standard pipeline approach to se-
mantic processing, in which sentences
are morphologically and syntactically
resolved to a single tree before they
are interpreted, is a poor fit for ap-
plications such as natural language in-
terfaces. This is because the environ-
ment information, in the form of the ob-
jects and events in the application?s run-
time environment, cannot be used to in-
form parsing decisions unless the in-
put sentence is semantically analyzed,
but this does not occur until after pars-
ing in the single-tree semantic architec-
ture. This paper describes the compu-
tational properties of an alternative ar-
chitecture, in which semantic analysis
is performed on all possible interpre-
tations during parsing, in polynomial
time.
1 Introduction
Shallow semantic processing applications, com-
paring argument structures to search patterns
or filling in simple templates, can achieve re-
spectable results using the standard ?pipeline? ap-
proach to semantics, in which sentences are mor-
phologically and syntactically resolved to a single
tree before being interpreted. Putting disambigua-
tion ahead of semantic evaluation is reasonable in
these applications because they are primarily run
on content like newspaper text or dictated speech,
where no machine-readable contextual informa-
tion is readily available to provide semantic guid-
ance for disambiguation.
This single-tree semantic architecture is a poor
fit for applications such as natural language inter-
faces however, in which a large amount of contex-
tual information is available in the form of the ob-
jects and events in the application?s run-time en-
vironment. This is because the environment infor-
mation cannot be used to inform parsing and dis-
ambiguation decisions unless the input sentence
is semantically analyzed, but this does not occur
until after parsing in the single-tree architecture.
Assuming that no current statistical disambigua-
tion technique is so accurate that it could not ben-
efit from this kind of environment-based informa-
tion (if available), then it is important that the se-
mantic analysis in an interface architecture be ef-
ficiently performed during parsing.
This paper describes the computational prop-
erties of one such architecture, embedded within
a system for giving various kinds of conditional
instructions and behavioral constraints to virtual
human agents in a 3-D simulated environment
(Bindiganavale et al, 2000). In one application
of this system, users direct simulated maintenance
personnel to repair a jet engine, in order to ensure
that the maintenance procedures do not risk the
safety of the people performing them. Since it is
expected to process a broad range of maintenance
instructions, the parser is run on a large subset
of the Xtag English grammar (XTAG Research
Group, 1998), which has been annotated with lex-
ical semantic classes (Kipper et al, 2000) associ-
ated with the objects, states, and processes in the
maintenance simulation. Since the grammar has
several thousand lexical entries, the parser is ex-
posed to considerable lexical and structural ambi-
guity as a matter of course.
The environment-based disambiguation archi-
tecture described in this paper has much in
common with very early environment-based ap-
proaches, such as those described by Winograd
(Winograd, 1972), in that it uses the actual en-
tities in an environment database to resolve am-
biguity in the input. This research explores two
extensions to the basic approach however:
1. It incorporates ideas from type theory to rep-
resent a broad range of linguistic phenomena
in a manner for which their extensions or po-
tential referents in the environment are well-
defined in every case. This is elaborated in
Section 2.
2. It adapts the concept of structure sharing,
taken from the study of parsing, not only to
translate the many possible interpretations of
ambiguous sentences into shared logical ex-
pressions, but also to evaluate these sets of
potential referents, over all possible interpre-
tations, in polynomial time. This is elabo-
rated in Section 3.
Taken together, these extensions allow interfaced
systems to evaluate a broad range of natural lan-
guage inputs ? including those containing NP/VP
attachment ambiguity and verb sense ambiguity
? in a principled way, simply based on the ob-
jects and events in the systems? environments.
For example, such a system would be able to cor-
rectly answer ?Did someone stop the test at 3:00??
and resolve the ambiguity in the attachment of ?at
3:00? just from the fact that there aren?t any 3:00
tests in the environment, only an event where one
stops at 3:00.1 Because it evaluates instructions
before attempting to choose a single interpreta-
tion, the interpreter can avoid getting ?stranded?
by disambiguation errors in earlier phases of anal-
ysis.
The main challenge of this approach is that it
requires the efficient calculation of the set of ob-
jects, states, or processes in the environment that
each possible sub-derivation of an input sentence
could refer to. A semantic interpreter could al-
ways be run on an (exponential) enumerated set
of possible parse trees as a post-process, to fil-
ter out those interpretations which have no en-
vironment referents, but recomputing the poten-
tial environment referents for every tree would re-
quire an enormous amount of time (particularly
for broad coverage grammars such as the one em-
ployed here). The primary result of this paper is
therefore a method of containing the time com-
plexity of these calculations to lie within the com-
plexity of parsing (i.e. within   for a context-
free grammar, where

is the number of words
1It is important to make a distinction between this envi-
ronment information, which just describes the set of objects
and events that exist in the interfaced application, and what
is often called domain information, which describes (usually
via hand-written rules) the kinds of objects and events can
exist in the interfaced application. The former comes for free
with the application, while the latter can be very expensive
to create and port between domains.
in the input sentence), without sacrificing logi-
cal correctness, in order to make environment-
based interpretation tractable for interactive appli-
cations.
2 Representation of referents
Existing environment-based methods (such as
those proposed by Winograd) only calculate the
referents of noun phrases, so they only consult the
objects in an environment database when inter-
preting input sentences. But the evaluation of am-
biguous sentences will be incomplete if the refer-
ents for verb phrases and other predicates are not
calculated. In order to evaluate the possible inter-
pretations of a sentence, as described in the previ-
ous section, an interface needs to define referent
sets for every possible constituent.2
The proposed solution draws on a theory of
constituent types from formal linguistic seman-
tics, in which constituents such as nouns and verb
phrases are represented as composeable functions
that take entitiess or situations as inputs and ulti-
mately return a truth value for the sentence. Fol-
lowing a straightforward adaptation of standard
type theory, common nouns (functions from en-
tities to truth values) define potential referent sets
of simple environment entities: 	


   ,
and sentences (functions from situations or world
states to truth values) define potential referent sets
of situations in which those sentences hold true:
	Using model-theoretic semantic interpretation to guide statistical
parsing and word recognition in a spoken language interface

William Schuler
Department of Computer and Information Science
University of Pennsylvania
200 S. 33rd Street
Philadelphia, PA 19104
schuler@linc.cis.upenn.edu
Abstract
This paper describes an extension of the se-
mantic grammars used in conventional sta-
tistical spoken language interfaces to allow
the probabilities of derived analyses to be
conditioned on the meanings or denotations
of input utterances in the context of an
interface's underlying application environ-
ment or world model. Since these denota-
tions will be used to guide disambiguation
in interactive applications, they must be ef-
ciently shared among the many possible
analyses that may be assigned to an input
utterance. This paper therefore presents a
formal restriction on the scope of variables
in a semantic grammar which guarantees
that the denotations of all possible analy-
ses of an input utterance can be calculated
in polynomial time, without undue con-
straints on the expressivity of the derived
semantics. Empirical tests show that this
model-theoretic interpretation yields a sta-
tistically signicant improvement on stan-
dard measures of parsing accuracy over a
baseline grammar not conditioned on deno-
tations.
1 Introduction
The development of speaker-independent mixed-
initiative speech interfaces, in which users not only
answer questions but also ask questions and give
instructions, is currently limited by the perfor-
mance of language models based largely on word co-
occurrences. Even under ideal circumstances, with
large application-specic corpora on which to train,

The author would like to thank David Chiang, Karin
Kipper, and three anonymous reviewers for particularly
helpful comments on this material. This work was sup-
ported by NSF grant EIA 0224417.
conventional language models are not su?ciently
predictive to correctly analyze a wide variety of in-
puts from a wide variety of speakers, such as might
be encountered in a general-purpose interface for di-
recting robots, o?ce assistants, or other agents with
complex capabilities. Such tasks may involve unla-
beled objects that must be precisely described, and a
wider range of actions than a standard database in-
terface would require (which also must be precisely
described), introducing a great deal of ambiguity into
input processing.
This paper therefore explores the use of a statis-
tical model of language conditioned on the mean-
ings or denotations of input utterances in the context
of an interface's underlying application environment
or world model. This use of model-theoretic inter-
pretation represents an important extension to the
`semantic grammars' used in existing statistical spo-
ken language interfaces, which rely on co-occurrences
among lexically-determined semantic classes and slot
llers (Miller et al, 1996), in that the probability
of an analysis is now also conditioned on the exis-
tence of denoted entities and relations in the world
model. The advantage of the interpretation-based
disambiguation advanced here is that the probabil-
ity of generating, for example, the noun phrase `the
lemon next to the safe' can be more reliably esti-
mated from the frequency with which noun phrases
have non-empty denotations { given the fact that
`the lemon next to the safe' does indeed denote some-
thing in the world model { than it can from the rel-
atively sparse co-occurrences of frame labels such as
lemon and next-to, or of next-to and safe.
Since there are exponentially many word strings
attributable to any utterance, and an exponential
(Catalan-order) number of possible parse tree anal-
yses attributable to any string of words, this use
of model-theoretic interpretation for disambiguation
must involve some kind of sharing of partial results
between competing analyses if interpretation is to be
performed on large numbers of possible analyses in a
practical interactive application. This paper there-
fore also presents a formal restriction on the scope of
variables in a semantic grammar (without untoward
constraints on the expressivity of the derived seman-
tics) which guarantees that the denotations of all
possible analyses of an input utterance can be calcu-
lated in polynomial time. Empirical tests show that
this use of model-theoretic interpretation in disam-
biguation yields a statistically signicant improve-
ment on standard measures of parsing accuracy over
a baseline grammar not conditioned on denotations.
2 Model-theoretic interpretation
In order to determine whether a user's directions de-
note entities and relations that exist in the world
model { and of course, in order to execute those
directions once they are disambiguated { it is nec-
essary to precisely represent the meanings of input
utterances.
Semantic grammars of the sort employed in cur-
rent spoken language interfaces for ight reservation
tasks (Miller et al, 1996; Sene et al, 1998) asso-
ciate fragments of logical { typically relational alge-
bra { expressions with recursive transition networks
encoding lexicalized rules in a context-free grammar
(the independent probabilities of these rules can then
be estimated from a training corpus and multiplied
together to give a probability for any given analysis).
In ight reservation systems, these associated seman-
tic expressions usually designate entities through a
xed set of constant symbols used as proper names
(e.g. for cities and numbered ights); but in applica-
tions with unlabeled (perhaps visually-represented)
environments, entities must be described by pred-
icating one or more modiers over some variable,
narrowing the set of potential referents by specify-
ing colors, spatial locations, etc., until only the de-
sired entity or entities remain. A semantic grammar
for interacting with this kind of unlabeled environ-
ment might contain the following rules, using vari-
ables x
1
::: x
n
(over entities in the world model) in
the associated logical expressions:
VP ! VP PP : x
1
::: x
n
: $1(x
1
::: x
m
)^
$2(x
1
; x
m+1
::: x
n
)
VP ! hold NP : x
1
:Hold (Agent; x
1
) ^ $2(x
1
)
NP ! a glass : x
1
:Glass(x
1
)
PP ! under NP : x
1
x
2
:Under(x
1
; x
2
) ^ $2(x
2
)
NP ! the faucet : x
1
:Faucet(x
1
)
in which m and n are integers and 0  m  n. Each
lambda expression x
1
::: x
n
:  indicates a function
from a tuple of entities he
1
::: e
n
i to a truth value
dened by the remainder of the expression  (sub-
VP ! VP PP
x
1
::: x
n=2
: $1(x
1
::: x
m=1
) ^ $2(x
1
; x
m+1
::: x
n
)
fhf
1
; g
1
i; hf
2
; g
2
i; : : : g
VP ! hold NP
x
1
: H(A;x
1
) ^ $2(x
1
)
fg
1
; g
2
; : : : g
hold NP ! . . .
x
1
: G(x
1
)
fg
1
; g
2
; : : : g
a glass
PP ! under NP
x
1
x
2
: U(x
1
; x
2
) ^ $2(x
2
)
fhf
1
; g
1
i; hf
2
; g
2
i; : : : g
under NP ! . . .
x
1
: F (x
1
)
ff
1
; f
2
; : : : g
the faucet
Figure 1: Semantic grammar derivation showing the
associated semantics and denotation of each con-
stituent. Entire rules are shown at each step in the
derivation in order to make the semantic associations
explicit.
stituting e
1
::: e
n
for x
1
::: x
n
), which denotes a set of
tuples satisfying , drawn from E
n
(where E is the
set of entities in the world model).
The pseudo-variables $1; $2; : : : in this notation in-
dicate the sites at which the semantic expressions
associated with each rule's nonterminal symbols are
to compose (the numbers correspond to the relative
positions of the symbols on the right hand side of
each rule, numbered from left to right). Semantic ex-
pressions for complete sentences are then formed by
composing the sub-expressions associated with each
rule at the appropriate sites.
1
Figure 1 shows the above rules assembled in a
derivation of the sentence `hold a glass under the
faucet.' The denotation annotated beneath each con-
stituent is simply the set of variable assignments
(for each free variable) that satisfy the constituent's
semantics. These denotations exactly capture the
meaning (in a given world model) of the assem-
bled semantic expressions dominated by each con-
stituent, regardless of how many sub-expressions are
subsumed by that constituent, and can therefore be
shared among competing analyses in lieu of the se-
mantic expression itself, as a partial result in model-
theoretic interpretation.
2.1 Variable scope
Note, however, that the adjunction of the preposi-
tional phrase modier `under the faucet' adds an-
other free variable (x
2
) to the semantics of the verb
1
This use of pseudo-variables is intended to resemble
that of the unix program `yacc,' which has a similar pur-
pose (associating syntax with semantics in constructing
compilers for programming languages).
VP ! VP PP
x
1
::: x
n=1
: $1(x
1
::: x
m=0
) ^ $2(x
1
; x
m+1
::: x
n
)
VP ! hold NP
Q
x
1
: H(A;x
1
) ^ $2(x
1
)
hold NP ! . . .
PP ! under NP
x
1
: Q
x
2
: U(x
1
; x
2
) ^ $2(x
2
)
under NP ! . . .
Figure 2: Derivation with minimal scoping. The
variable x
1
in the semantic expression associated
with the prepositional phrase `under the faucet' can-
not be identied with the variable in the verb phrase.
phrase, and therefore another factor of jEj to the car-
dinality of its denotation. Moreover, under this kind
of global scoping, if additional prepositional phrases
are adjoined, they would each contribute yet an-
other free variable, increasing the complexity of the
denotation by an additional factor of jEj, making
the shared interpretation of such structures poten-
tially exponential on the length of the input. This
proliferation of free variables means that the vari-
ables introduced by the noun phrases in an utter-
ance, such as `hold a glass under the faucet,' can-
not all be given global scope, as in Figure 1. On
the other hand, the variables introduced by quanti-
ed noun phrases cannot be bound as soon as the
noun phrases are composed, as in Figure 2, because
these variables may need to be used in modiers
composed in subsequent (higher) rule applications.
Fortunately, if these non-immediate variable scop-
ing arrangements are expressed structurally, as dom-
inance relationships in the elementary tree struc-
tures of some grammar, then a structural restriction
on this grammar can be enforced that preserves as
many non-immediate scoping arrangements as possi-
ble while still preventing an unbounded proliferation
of free variables.
The correct scoping arrangements (e.g. for the sen-
tence `hold a glass under the faucet,' shown Fig-
ure 3) can be expressed using ordered sets of parse
rules grouped together in such a way as to allow
other structural material to intervene. In this case,
a group would include a rule for composing a verb
and a noun phrase with some associated predicate,
and one or more rules for binding each of the pred-
icate's variables in a quantier somewhere above it
(thereby ensuring that these rules always occur to-
gether with the quantier rules dominating the pred-
icate rule), while still allowing rules adjoining prepo-
sitional phrase modiers to apply in between them
(so that variables in their associated predicates can
VP ! VP
x
2
::: x
n=1
: Q
x
1
: $1(x
1
::: x
n
)
fhig
VP ! VP PP
x
1
::: x
n=1
: $1(x
1
::: x
m=1
) ^ $2(x
1
; x
m+1
::: x
n
)
fg
1
; g
2
; : : : g
VP ! hold NP
x
1
: H(A;x
1
) ^ $2(x
1
)
fg
1
; g
2
; : : : g
hold NP ! . . .
PP ! PP
x
2
::: x
n
: Q
x
1
: $1(x
1
::: x
n
)
fg
1
; g
2
; : : : g
PP ! under NP
x
1
x
2
: U(x
2
; x
1
) ^ $2(x
1
)
fhf
1
; g
1
i; hf
2
; g
2
i; : : : g
under NP ! . . .
Figure 3: Derivation with desired scoping.
be bound by the same quantiers).
These `grouped rules' can be formalized using a
tree-rewriting system whose elementary trees can
subsume several ordered CFG rule applications (or
steps in a context-free derivation), as shown in Fig-
ure 4. Each such elementary tree contains a rule
(node) associated with a logical predicate and rules
(nodes) associated with quantiers binding each of
the predicate's variables. These trees are then com-
posed by rewriting operations (dotted lines), which
split them up and either insert them between or iden-
tify them with (if demarcated with dashed lines) the
rules in another elementary tree { in this case, the
elementary tree anchored by the word `under.' These
trees are considered elementary in order to exclude
the possibility of generating derivations that contain
unbound variables or quantiers over unused vari-
ables, which would have no intuitive meaning. The
composition operations will be presented in further
detail in Section 2.2.
2.2 Semantic composition as tree-rewriting
A general class of rewriting systems can be dened
using sets of allowable expansions of some type of
object to incorporate zero or more other instances
of the same type of object, each of which is simi-
larly expandable. Such a system can generate ar-
bitrarily complex structure by recursively expand-
ing or `rewriting' each new object, concluding with a
set of zero-expansions at the frontier. For example,
a context-free grammar may be cast as a rewriting
system whose objects are strings, and whose allow-
able expansions are its grammar productions, each
of which expands or rewrites a certain string as a set
VP ! VP
x
2
::: x
n
: Q
x
1
: $1(x
1
::: x
n
)
VP ! hold NP
x
1
::: x
n
: $1(x
1
::: x
n
) ^ $2(x
1
)
V ! hold
x
1
:Hold(A; x
1
)
NP ! . . .
. . .
1
VP ! VP
x
2
::: x
n
: Q
x
1
: $1(x
1
::: x
n
)
VP ! VP PP
x
1
::: x
n
: $1(x
1
::: x
m
) ^ $2(x
1
; x
m+1
::: x
n
)
VP
1
PP ! PP
x
2
::: x
n
: Q
x
1
: $1(x
1
::: x
n
)
PP ! P NP
x
1
::: x
n
: $1(x
1
::: x
n
) ^ $2(x
1
)
P ! under
x
1
x
2
:Under(x
2
; x
1
)
NP
2
1
2
PP ! PP
x
2
::: x
n
: Q
x
1
: $1(x
1
::: x
n
)
NP ! Q N
$2
Q ! a N ! faucet
x
1
:Faucet(x
1
)
Figure 4: Complete elementary tree for `under' showing argument insertion sites.
of zero or more sub-strings arranged around certain
`elementary' strings contributing terminal symbols.
A class of tree-rewriting systems can similarly
be dened as rewriting systems whose objects are
trees, and whose allowable expansions are produc-
tions (similar to context-free productions), each of
which rewrite a tree A as some function f applied
to zero or more sub-trees A
1
; : : : A
s
; s  0 arranged
around some `elementary' tree structure dened by
f (Pollard, 1984; Weir, 1988):
A ! f(A
1
; : : : A
s
) (1)
This elementary tree structure can be used to ex-
press the dominance relationship between a logical
predicate and the quantiers that bind its variables
(which must be preserved in any meaningful derived
structure); but in order to allow the same instance
of a quantier to bind variables in more than one
predicate, the rewriting productions of such a se-
mantic tree-rewriting system must allow expanded
subtrees to identify parts of their structure (speci-
cally, the parts containing quantiers) with parts of
each other's structure, and with that of their host
elementary tree.
In particular, a rewriting production in such a sys-
tem would rewrite a tree A as an elementary tree 
0
with a set of sub-trees A
1
; : : : A
s
inserted into it, each
of which is rst partitioned into a set of contiguous
components (in order to isolate particular quantier
nodes and other kinds of sub-structure) using a tree
partition function g at some sequence of split points
h#
i1
,...#
ic
i
i, which are node addresses in A
i
(the rst
of which simply species the root).
2
The resulting se-
quence of partitioned components of each expanded
2
The node addresses encode a path from the root of
sub-tree are then inserted into 
0
at a correspond-
ing sequence of insertion site addresses h
i1
,... 
ic
i
i
dened by the rewriting function f :
f(A
1
; : : : A
s
) =

0
[h
11
,... 
1c
1
i; g
#
11
,...#
1c
1
(A
1
)] : : :
[h
s1
,... 
sc
s
i; g
#
s1
,...#
sc
s
(A
s
)] (2)
Since each address can only host a single inserted
component, any components from dierent sub-tree
arguments of f that are assigned to the same inser-
tion site address are constrained to be identical in or-
der for the production to apply. Additionally, some
addresses may be `pre-lled' as part of the elemen-
tary structure dened in f , and therefore may also
be identied with components of sub-tree arguments
of f that are inserted at the same address.
Figure 4 shows the set of insertion sites (designated
with boxed indices) for each argument of an elemen-
tary tree anchored by `under.' The sites labeled 1 ,
associated with the rst argument sub-tree (in this
case, the tree anchored by `hold'), indicate that it is
composed by partitioning it into three components,
each dominating or dominated by the others, the low-
est of which is inserted at the terminal node labeled
`VP,' the middle of which is identied with a pre-
lled component (delimited by dashed lines), con-
taining the quantier node labeled `VP ! VP,' and
the uppermost of which (empty in the gure) is in-
serted at the root, while preserving the relative dom-
inance relationships among the nodes in both trees.
Similarly, sites labeled 2 , associated with the sec-
ond argument sub-tree (for the noun phrase comple-
the tree in which every address i species the i
th
child
of the node at the end of path .
ment to the preposition), indicate that it is composed
by partitioning it into two components { again, one
dominating the other { the lowest of which is inserted
at the terminal node labeled `NP,' and the uppermost
of which is identied with another pre-lled compo-
nent containing the quantier node labeled `PP !
PP,' again preserving the relative dominance rela-
tionships among the nodes in both trees.
2.3 Shared interpretation
Recall the problem of unbounded variable prolif-
eration described in Section 2.1. The advantage
of using a tree-rewriting system to model semantic
composition is that such systems allow the appli-
cation of well-studied restrictions to limit their re-
cursive capacity to generate structural descriptions
(in this case, to limit the unbounded overlapping
of quantier-variable dependencies that can produce
unlimited numbers of free variables at certain steps in
a derivation), without limiting the multi-level struc-
ture of their elementary trees, used here for captur-
ing the well-formedness constraint that a predicate
be dominated by its variables' quantiers.
One such restriction, based on the regular form
restriction dened for tree adjoining grammars
(Rogers, 1994), prohibits a grammar from allowing
any cycle of elementary trees, each intervening inside
a spine (a path connecting the insertion sites of any
argument) of the next. This restriction is dened
below:
Denition 2.1 Let a spine in an elementary tree be
the path of nodes (or object-level rule applications)
connecting all insertion site addresses of the same
argument.
Denition 2.2 A grammar G is in regular form if a
directed acyclic graph hV;Ei can be drawn with ver-
tices v
H
; v
A
2 V corresponding to partitioned ele-
mentary trees of G (partitioned as described above),
and directed edges hv
H
; v
A
i 2 E  V  V from each
vertex v
H
, corresponding to a partitioned elementary
tree that can host an argument, to each vertex v
A
,
corresponding to a partitioned elementary tree that
can function as its argument, whose partition inter-
sects its spine at any place other than the top node
in the spine.
This restriction ensures that there will be no un-
bounded `pumping' of intervening tree structure in
any derivation, so there will never be an unbounded
amount of unrecognized tree structure to keep track
of at any step in a bottom-up parse, so the number
of possible descriptions of each sub-span of the in-
put will be bounded by some constant. It is called a
`regular form' restriction because it ensures that the
set of root-to-leaf paths in any derived structure will
form a regular language.
A CKY-style parser can now be built that rec-
ognizes each context-free rule in an elementary tree
from the bottom up, storing in order the unrecog-
nized rules that lie above it in the elementary tree
(as well as any remaining rules from any composed
sub-trees) as a kind of promissory note. The fact
that any regular-form grammar has a regular path
set means that only a nite number of states will be
required to keep track of this promised, unrecognized
structure in a bottom-up traversal, so the parser will
have the usual O(n
3
) complexity (times a constant
equal to the nite number of possible unrecognized
structures).
Moreover, since the parser can recognize any string
derivable by such a grammar, it can create a shared
forest representation of every possible analysis of a
given input by annotating every possible applica-
tion of parse rules that could be used in the deriva-
tion of each constituent (Billot and Lang, 1989).
This polynomial-sized shared forest representation
can then be interpreted determine which constituents
denote entities and relations in the world model, in
order to allow model-theoretic semantic information
to guide disambiguation decisions in parsing.
Finally, the regular form restriction also has the
important eect of ensuring that the number of un-
recognized quantier nodes at any step in a bottom-
up analysis { and therefore the number of free vari-
ables in any word or phrase constituent of a parse { is
also bounded by some constant, which limits the size
of any constituent's denotation to a polynomial or-
der of E , the number of entities in the environment.
The interpretation of any shared forest derived by
this kind of regular-form tree-rewriting system can
therefore be calculated in worst-case polynomial time
on E .
A denotation-annotated shared forest for the noun
phrase `the girl with the hat behind the counter' is
shown in Figure 5, using the noun and preposition
trees from Figure 4, with alternative applications of
parse rules represented as circles below each derived
constituent. This shared structure subsumes two
competing analyses: one containing the noun phrase
`the girl with the hat', denoting the entity g
1
, and
the other containing the noun phrase `the hat be-
hind the counter', which does not denote anything
in the world model. Assuming that noun phrases
rarely occur with empty denotations in the training
data, the parse containing the phrase `the girl with
the hat' will be preferred, because there is indeed a
girl with a hat in the world model.
This formalism has similarities with two ex-
NP ! girl
x
1
:Girl(x
1
)
fg
1
; g
2
; g
3
g
P ! with
x
1
x
2
:With(x
2
; x
1
)
fhh
1
; g
1
i; hh
2
; b
1
ig
NP ! hat
x
1
:Hat(x
1
)
fh
1
; h
2
; h
3
; h
4
g
P ! behind
x
1
x
2
:Behind(x
2
; x
1
)
fhc
1
; g
1
ig
NP ! counter
x
1
:Counter(x
1
)
fc
1
; c
2
g
PP ! P NP
x
1
::: x
n=2
: $1(x
1
::: x
n
) ^ $2(x
1
)
fhh
1
; g
1
i; hh
2
; b
1
ig
PP ! PP
x
2
::: x
n=2
: Q
x
1
: $1(x
1
::: x
n
)
fg
1
; b
1
g
PP ! P NP
x
1
::: x
n=2
: $1(x
1
::: x
n
) ^ $2(x
1
)
fhc
1
; g
1
ig
PP ! PP
x
2
::: x
n=2
: Q
x
1
: $1(x
1
::: x
n
)
fg
1
g
NP ! NP PP
x
1
::: x
n=1
: $1(x
1
::: x
m=1
) ^ $2(x
1
; x
m+1
::: x
n
)
fg
1
g
NP ! NP PP
x
1
::: x
n=1
: $1(x
1
::: x
m=1
) ^ $2(x
1
; x
m+1
::: x
n
)
;
PP ! P NP
x
1
::: x
n=2
: $1(x
1
::: x
n
) ^ $2(x
1
)
;
PP ! PP
x
2
::: x
n=2
: Q
x
1
: $1(x
1
::: x
n
)
;
NP ! NP PP
x
1
::: x
n=1
: $1(x
1
::: x
m=1
) ^ $2(x
1
; x
m+1
::: x
n
)
; or fg
1
g
Figure 5: Shared forest for `the girl with the hat behind the counter.'
tensions of tree-adjoining grammar (Joshi, 1985),
namely multi-component tree adjoining grammar
(Becker et al, 1991) and description tree substitu-
tion grammar (Rambow et al, 1995), and indeed
represents something of a combination of the two:
1. Like description tree substitution grammars,
but unlike multi-component TAGs, it allows
trees to be partitioned into any desired set of
contiguous components during composition,
2. Like multi-component TAGs, but unlike descrip-
tion tree substitution grammars, it allows the
specication of particular insertion sites within
elementary trees, and
3. Unlike both, it allow duplication of structure
(which is used for merging quantiers from dif-
ferent elementary trees).
The use of lambda calculus functions to dene de-
composable meanings for input sentences draws on
traditions of Church (1940) and Montague (1973),
but this approach diers from the Montagovian sys-
tem by introducing explicit limits on computational
complexity (in order to allow tractable disambigua-
tion).
This approach to semantics is very similar to that
described by Shieber (1994), in which syntactic and
semantic expressions are assembled synchronously
using paired tree-adjoining grammars with isomor-
phic derivations, except that in this approach the
derived structures are isomorphic as well, hence the
reduction of synchronous tree pairs to semantically-
annotated syntax trees. This isomorphism restric-
tion on derived trees reduces the number of quantier
scoping congurations that can be assigned to any
given input (most of which are unlikely to be used
in a practical application), but its relative parsimony
allows syntactically ambiguous inputs to be seman-
tically interpreted in a shared forest representation
in worst-case polynomial time. The interleaving of
semantic evaluation and parsing for the purpose of
disambiguation also has much in common with that
of Dowding et al (1994), except that in this case,
constituents are not only semantically type-checked,
but are also fully interpreted each time they are pro-
posed. There are also commonalities between the un-
derspecied semantic representation of structurally-
ambiguous elementary tree constituents in a shared
forest and the underspecied semantic representa-
tion of (e.g. quantier) scope ambiguity described
by Reyle (1993).
3
3 Evaluation
The contribution of this model-theoretic semantic in-
formation toward disambiguation was evaluated on
a set of directions to animated agents collected in a
controlled but spatially complex 3-D simulated en-
vironment (of children running a lemonade stand).
In order to avoid priming them towards particu-
lar linguistic constructions, subjects were shown un-
narrated animations of computer-simulated agents
performing dierent tasks in this environment (pick-
ing fruit, operating a juicer, and exchanging lemon-
ade for money), which were described only as the `de-
sired behavior' of each agent. The subjects were then
asked to direct the agents, using their own words, to
perform the desired behaviors as shown.
340 utterances were collected and annotated with
brackets and elementary tree node addresses as de-
scribed in Section 2.2, for use as training data and
as gold standard data in testing. Some sample direc-
tions are shown in Figure 6. Most elementary trees
were extracted, with some simplications for parsing
e?ciency, from an existing broad-coverage grammar
resource (XTAG Research Group, 1998), but some
elementary trees for multi-word expressions had to
be created anew. In all, a complete annotation of this
corpus required a grammar of 68 elementary trees
and a lexicon of 288 lexicalizations (that is, words
or sets of words with indivisible semantics, forming
the anchors of a given elementary tree). Each lex-
icalization was then assigned a semantic expression
describing the intended geometric relation or class of
objects in the simulated 3-D environment.
4
The interface was tested on the rst 100 collected
utterances, and the parsing model was trained on
the remaining utterances. The presence or absence
of a denotation of each constituent was added to the
label of each constituent in the denotation-sensitive
parsing model (for example, statistics were collected
for the frequency of `NP:  ! NP:+ PP:+' events,
meaning a noun phrase that does not denote any-
3
Denotation of competing applications of parse rules
can be unioned (though this eectively treats ambiguity
as a form of disjunction), or stored separately to some
nitie beam (though some globally preferable but locally
dispreferred structures would be lost).
4
Here it was assumed that the intention of the user
was to direct the agent to perform the actions shown in
the `desired behavior' animation.
Walk towards the tree where you see a yellow lemon
on the ground.
Pick up the lemon.
Place the lemon in the pool.
Take the dollar bill from the person in front of you.
Walk to the left towards the big black cube.
Figure 6: Sample utterances from collected corpus.
thing in the environment expands to a noun phrase
and a prepositional phrase that do have a denota-
tion in the environment), whereas the baseline sys-
tem used a parsing model conditioned on only con-
stituent labels (for example, `NP ! NP PP' events).
The entire word lattice output of the speech recog-
nizer was fed directly into the parser, so as to al-
low the model-theoretic semantic information to be
brought to bear on word recognition ambiguity as
well as on structural ambiguity in parsing.
Since any derivation of elementary trees uniquely
denes a semantic expression at each node, the task
of evaluating this kind of semantic analysis is reduced
to the familiar task of evaluating a the accuracy of
a labeled bracketing (labeled with elementary tree
names and node addresses). Here, the standard mea-
sures of labeled precision and recall are used. Note
that there may be multiple possible bracketings for
each gold standard tree in a given word lattice that
dier only in the start and end frames of the com-
ponent words. Since neither the baseline nor test
parsing models are sensitive to the start and end
frames of the component words, the gold standard
bracketing is simply assumed to use the most likely
frame segmentation in the word lattice that yields
the correct word sequence.
The results of the experiment are summarized
below. The environment-based model shows a
statistically signicant (p<.05) improvement of 3
points in labeled recall, a 12% reduction in error.
Most of the improvement can be attributed to the
denotation-sensitive parser dispreferring noun phrase
constituents with mis-attached modiers, which do
not denote anything in the world model.
Model LR LP
baseline model 82% 78%
baseline + denotation bit 85% 81%
4 Conclusion
This paper has described an extension of the seman-
tic grammars used in conventional spoken language
interfaces to allow the probabilities of derived anal-
yses to be conditioned on the results of a model-
theoretic interpretation. In particular, a formal re-
striction was presented on the scope of variables in a
semantic grammar which guarantees that the deno-
tations of all possible analyses of an input utterance
can be calculated in polynomial time, without un-
due constraints on the expressivity of the derived
semantics. Empirical tests show that this model-
theoretic interpretation yields a statistically signif-
icant improvement on standard measures of parsing
accuracy over a baseline grammar not conditioned
on denotations.
References
Tilman Becker, Aravind Joshi, and Owen Rambow.
1991. Long distance scrambling and tree adjoining
grammars. In Fifth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL'91), pages 21{26.
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27
th
Annual Meeting of the Association
for Computational Linguistics (ACL '89), pages
143{151.
Alonzo Church. 1940. A formulation of the sim-
ple theory of types. Journal of Symbolic Logic,
5(2):56{68.
John Dowding, Robert Moore, Francois Andery, and
Douglas Moran. 1994. Interleaving syntax and se-
mantics in an e?cient bottom-up parser. In Pro-
ceedings of the 32nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL'94).
Aravind K. Joshi. 1985. How much context sensi-
tivity is necessary for characterizing structural de-
scriptions: Tree adjoining grammars. In L. Kart-
tunen D. Dowty and A. Zwicky, editors, Natu-
ral language parsing: Psychological, computational
and theoretical perspectives, pages 206{250. Cam-
bridge University Press, Cambridge, U.K.
Scott Miller, David Stallard, Robert Bobrow, and
Richard Schwartz. 1996. A fully statistical ap-
proach to natural language interfaces. In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL'96),
pages 55{61.
Richard Montague. 1973. The proper treatment
of quantication in ordinary English. In J. Hin-
tikka, J.M.E. Moravcsik, and P. Suppes, editors,
Approaches to Natural Langauge, pages 221{242.
D. Riedel, Dordrecht. Reprinted in R. H. Thoma-
son ed., Formal Philosophy, Yale University Press,
1994.
Carl Pollard. 1984. Generalized phrase structure
grammars, head grammars and natural langauge.
Ph.D. thesis, Stanford University.
Owen Rambow, David Weir, and K. Vijay-Shanker.
1995. D-tree grammars. In Proceedings of the 33rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL '95).
Uwe Reyle. 1993. Dealing with ambiguities by un-
derspecication: Construction, representation and
deduction. Journal of Semantics, 10:123{179.
James Rogers. 1994. Capturing CFLs with tree ad-
joining grammars. In Proceedings of the 32nd An-
nual Meeting of the Association for Computational
Linguistics (ACL '94).
Stephanie Sene, Ed Hurley, Raymond Lau, Chris-
tine Pao, Philipp Schmid, and Victor Zue. 1998.
Galaxy-II: a reference architecture for conversa-
tional system development. In Proceedings of the
5th International Conference on Spoken Language
Processing (ICSLP '98), Sydney, Australia.
Stuart M. Shieber. 1994. Restricting the weak-
generative capability of synchronous tree adjoining
grammars. Computational Intelligence, 10(4).
David Weir. 1988. Characterizing mildly context-
sensitive grammar formalisms. Ph.D. thesis, De-
partment of Computer and Information Science,
University of Pennsylvania.
XTAG Research Group. 1998. A lexicalized tree
adjoining grammar for english. Technical report,
IRCS, University of Pennsylvania.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 569?576
Manchester, August 2008
A Syntactic Time-Series Model for Parsing Fluent and Disfluent Speech ?
Tim Miller
Department of Computer Science
and Engineering
University of Minnesota
tmill@cs.umn.edu
William Schuler
Department of Computer Science
and Engineering
University of Minnesota
schuler@cs.umn.edu
Abstract
This paper describes an incremental ap-
proach to parsing transcribed spontaneous
speech containing disfluencies with a Hier-
archical Hidden Markov Model (HHMM).
This model makes use of the right-corner
transform, which has been shown to in-
crease non-incremental parsing accuracy
on transcribed spontaneous speech (Miller
and Schuler, 2008), using trees trans-
formed in this manner to train the HHMM
parser. Not only do the representations
used in this model align with structure in
speech repairs, but as an HMM-like time-
series model, it can be directly integrated
into conventional speech recognition sys-
tems run on continuous streams of audio.
A system implementing this model is eval-
uated on the standard task of parsing the
Switchboard corpus, and achieves an im-
provement over the standard baseline prob-
abilistic CYK parser.
1 Introduction
Disfluency is one obstacle preventing speech
recognition systems from being able to recog-
nize spontaneous speech. Perhaps the most chal-
lenging aspect of disfluency recognition is the
phenomenon of speech repair, which involves a
speaker realizing a mistake, cutting off the flow
of speech, and then continuing on, possibly re-
tracing and replacing part of the utterance to that
point. This paper will describe a system which ap-
plies a syntactic model of speech repair to a time-
?The authors would like to thank the anonymous review-
ers for their input. This research was supported by National
Science Foundation CAREER/PECASE award 0447685. The
views expressed are not necessarily endorsed by the sponsors.
?c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
series parsing model, and evaluate that system on
the standard Switchboard corpus parsing task.
The speech repair terminology used here fol-
lows that of Shriberg (1994). A speech repair con-
sists of a reparandum, an interruption point, and
the alteration. The reparandum contains the words
that the speaker means to replace, including both
words that are in error and words that will be re-
traced. The interruption point is the point in time
where the stream of speech is actually stopped, and
the repairing of the mistake can begin. The alter-
ation contains the words that are meant to replace
the words in the reparandum.
2 Background
Historically, research in speech repair has focused
on acoustic cues such as pauses and prosodic con-
tours for detecting repairs, which could then be ex-
cised from the text for improved transcription. Re-
cent work has also looked at the possible contribu-
tion of higher-level cues, including syntactic struc-
ture, in the detection of speech repair. Some of this
work is inspired by Levelt?s (1983) investigation of
the syntactic and semantic variables in speech re-
pairs, particularly his well-formedness rule, which
states that the reparandum and alteration of a repair
typically have the same consitituent label, similar
to coordination.
Hale et al (2006) use Levelt?s well-formedness
rule to justify an annotation scheme where unfin-
ished categories (marked X-UNF) have the UNF
label appended to all ancestral category labels
all the way up to the top-most constituent be-
neath an EDITED label (EDITED labels denoting
a reparandum). They reason that this should pre-
vent grammar rules of finished constituents in the
corpus from corrupting the grammar of unfinished
constituents. While this annotation proves helpful,
it also leads to the unfortunate result that a large
reparandum requires several special repair rules to
be applied, even though the actual error is only
569
happening at one point.
Intuitively, though, it seems that an error is only
occurring at the end of the reparandum, and that
until that point only fluent grammar rules are be-
ing applied by the speaker. This intuition has also
been confirmed by empirical studies (Nakatani and
Hirschberg, 1994), which show that there is no ob-
vious error signal in speech up until the moment of
interruption. Although speakers may retrace much
of the reparandum for clarity or other reasons, ide-
ally the reparandum contains nothing but standard
grammatical rules until the speech is interrupted.1
Another recent approach to this problem (John-
son and Charniak, 2004) uses a tree-adjoining
grammar (TAG) approach to define a mapping be-
tween a source sentence possibly containing a re-
pair, and a target, fluent sentence. The use of
the TAG channel model is justified by the putative
crossing dependencies seen in repairs like . . . a
flight to Boston, uh, I mean, to Denver. . . where
there is repetition from the reparandum to the re-
pair. Essentially, this model is building up the
reparandum and alteration in tandem, based on
these crossing dependencies. While this is an inter-
esting model, it focuses on detection and removal
of EDITED sections, and subsequent parsing of
cleaned up speech. As such, it introduces chal-
lenges for integrating the system into a real-time
speech recognizer.
Recent work by Miller and Schuler (2008)
showed how a probabilistic grammar trained on
trees modified by use of the right-corner transform
can improve parsing accuracy over an unmodified
grammar when tested on the Switchboard corpus.
The approach described here builds on that work in
using right-corner transformed trees, and extends it
by mapping them to a time-series model to do pars-
ing directly in a model of the sort used in speech
recognition. This system is shown to be more ac-
curate than a baseline CYK parser when used to
parse the Switchboard corpus. The remainder of
this section will review the right-corner transform,
followed by Section 3, which will step through an
extended example giving details about the trans-
form process and its applicability to the problem
of processing speech repairs.
1One objection to this claim is the case of multiple nested
repairs. In this case, though, we presume that all reparanda
were originally intended by the speaker to be fluent at the time
of generation.
2.1 Right-corner transform
The right-corner transform rewrites syntax trees,
turning all right branching structure into left
branching structure, and leaving left branching
structure as is. As a result, constituent structure
can be explicitly built from the bottom up dur-
ing incremental recognition. This arrangement is
well-suited to recognition of speech with errors,
because it allows constituent structure to be built
up using fluent speech rules until the moment of
interruption, at which point a special repair rule
may be applied.
Before transforming the trees in the gram-
mar into right-corner trees, trees are binarized in
the same manner as Johnson (1998b) and Klein
and Manning (2003).2 Binarized trees are then
transformed into right-corner trees using trans-
form rules similar to those described by John-
son (1998a). In this transform, all right branch-
ing sequences in each tree are transformed into
left branching sequences of symbols of the form
A
1
/A
2
, denoting an incomplete instance of cate-
gory A
1
lacking an instance of category A
2
to the
right.3
Rewrite rules for the right-corner transform are
shown below, first flattening right-branching struc-
ture:
A
1
?
1
A
2
?
2
A
3
a
3
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
A
3
a
3
A
1
?
1
A
2
A
2
/A
3
?
2
. . .
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
. . .
then replacing it with left-branching structure:
A
1
A
1
/A
2
:?
1
A
2
/A
3
?
2
. . .
?
A
1
A
1
/A
3
A
1
/A
2
:?
1
?
2
. . .
Here, the first two rewrite rules are applied iter-
atively (bottom-up on the tree) to flatten all right
branching structure, using incomplete constituents
2For the details of the particular binarization process used
here, see Miller and Schuler (2008).
3Here, all A
i
denote nonterminal symbols, and all ?
i
de-
note subtrees; the notation A
1
:?
1
indicates a subtree ?
1
with
label A
1
; and all rewrites are applied recursively, from leaves
to root.
570
to record the original nonterminal ordering. The
third rule is then applied to generate left branching
structure, preserving this ordering.
Because this process turns right expansion into
left expansion (leaving center expansion as the
only stack consumer), right-corner transformed
trees also require less stack memory than ordi-
nary phrase structure trees. This key property of
the right-corner transform is exploited in the map-
ping of transformed training trees to a time-series
model. This property will be examined further in
Section 5.
3 Speech Repair Example
A substantial example of a speech repair from the
Switchboard corpus can be seen in Figures 1 and 2,
in which the same repair and surrounding context
is shown after the preliminary binarization pro-
cess, and after the right-corner transform. Fig-
ure 1 also shows, in brackets, the augmented an-
notation described above from Hale et al (2006).
This scheme consisted of adding -X to an EDITED
label which produced a category X (daughter an-
notation), as well as propagating the -UNF label at
the right corner of the tree up through every parent
below the EDITED root.
3.1 Re-annotation of speech repair
Before applying the right-corner transform, some
corpus pre-processing steps are applied ? Fig-
ure 1 shows an example tree after these changes.
These steps aim to improve on the default annota-
tion of repair in the Switchboard corpus by mak-
ing the representation more in line with linguistic
as well as processing desiderata.
The standard annotation practice of having the
EDITED label as the category at the root of a
reparandum does not represent any proposed lin-
guistic phenomenon, and it conflates speech re-
pairs of different categories. As a result, the parser
is unable to make use of information about which
syntactic category the reparandum was originally
intended to be. This information would be useful
in the example discussed here, since an unfinished
NP is followed by a completed NP. The daughter
annotation used by Hale et al fixes this annota-
tion to allow the parser to have access to this infor-
mation. The annotation scheme in this paper also
makes use of the daughter annotation.
In addition, trees are modified to reduce the ar-
ity of speech repair rules, and to change the model
of how repairs occur. The default Switchboard
has, e.g. an NP reparandum (headed by EDITED)
and the NP alteration represented as siblings in the
syntax tree. This annotation seems to implicitly
model all repairs as stopping the process of pro-
ducing the current constituent, then starting a new
one. In contrast, the representation here models re-
pairs of this type as a speaker generating a partial
noun phrase, realizing an error, and then continu-
ing to generate the same noun phrase. This rep-
resentation scheme thus represents the beginning
of an effort to distinguish between repairs that are
changing the direction of the utterance and those
that are just fixing a local mistake.
3.2 Right-corner transform model of speech
repair
The right corner transform is then applied to this
example in Figure 2. The resulting tree represen-
tation is especially valuable because it models hu-
man production of speech repairs well, by not ap-
plying any special rule until the moment of inter-
ruption.
In the example in Figure 1, there is an unfinished
constituent (PP-UNF) at the end of the reparan-
dum. This standard annotation is deficient because
even if an unfinished consituent like PP-UNF is
correctly recognized, and the speaker is essentially
in an error state, there may be several partially
completed constituents above ? in Figure 1, the
NP, PP, and NP above the PP-UNF. These con-
stituents need to be completed, but using the stan-
dard annotation there is only one chance to make
use of the information about the error that has oc-
curred ? the ?NP ? NP PP-UNF? rule. Thus, by
the time the error section is completed, there is no
information by which a parsing algorithm could
choose to reduce the topmost NP to EDITED (or
EDITED-NP) other than independent rule proba-
bilities.
The approach used by Hale et al (2006) works
because the information about the transition to an
?error state? is propagated up the tree, in the form
of the -UNF tags. As the parsing chart is filled
in from the bottom up, each rule applied is essen-
tially coming out of a special repair rule set, and
so at the top of the tree the EDITED hypothesis is
much more likely. However, this requires that sev-
eral fluent speech rules from the data set be modi-
fied for use in a special repair grammar, which not
only reduces the amount of available training data,
571
SNP
CC
and
NP
EDITED-NP
NP
DT
the
NN
JJ
first
NN
kind
PP[-UNF]
IN
of
NP[-UNF]
NP
invasion
PP-UNF
of
NP
NP
DT
the
NN
JJ
first
NN
type
PP
IN
of
NP
privacy
VP
. . .
Figure 1: Binarized tree repair structure, with the -UNF propagation as in Hale et al (2006) shown in
brackets.
S/VP
S/VP
S/S
CC
and
NP
NP/NP
NP/PP
NP/NP
EDITED-NP
NP/PP
NP/NP
NP/PP
NP
NP/NN
NP/NN
DT
the
JJ
first
NN
kind
IN
of
NP
invasion
PP-UNF
of
NP
NP/NN
NP/NN
DT
the
JJ
first
NN
type
IN
of
NP
privacy
VBD
. . .
Figure 2: Right-corner transformed tree with repair structure
but violates our intuition that reparanda are usually
fluent up until the actual edit occurs.
The right corner transform works in a different
way, by building up constituent structure from left
to right. In Figure 2, the same repair is shown
as it appears in the training data for this system.
With this representation, the problem noticed by
Hale and colleagues has been solved in a different
way, by incrementally building up left-branching
rather than right-branching structure, so that only
a single special error rule is required at the end of
the constituent. As seen in the figure, all of the
structure beneath the EDITED-NP label is built us-
ing rules from the fluent grammar. It is only at
one point, when the PP-UNF is found, that a re-
pair rule is applied and the EDITED-NP section is
found. The next step in the process is that the NP
essentially restarts (the NP/NP label), and the sub-
sequent words start to build up what will be the NP
alteration in a fluent manner.
To summarize, while the -UNF propagation
scheme often requires the entire reparandum to
be generated from a speech repair rule set, this
scheme only requires one special rule, where the
moment of interruption actually occurred. This re-
duces the number of special speech repair rules
that need to be learned and saves more potential
examples of fluent speech rules, and therefore po-
tentially makes better use of limited data.
4 Mapping to an HHMM
This section describes how a corpus of trees trans-
formed as above can be mapped to a time-series
model called a Hierarchical Hidden Markov Model
(HHMM) in order to incorporate parsing into
speech decoding. This suggests that this approach
can be used in applications using streaming speech
input, unlike other parsing approaches which are
cubic time on input length at best, and require in-
put to be pre-segmented.
This section will begin by showing how HH-
MMs can model linguistic structure by extending
standard Hidden Markov Models (HMMs) used in
speech recognition, and will follow with a descrip-
tion of how right-corner transformed trees can be
mapped to this model topology.
572
4.1 Hierarchical HMMs
In general, the hidden state in an HMM can be as
simple or complex as necessary. This can include
factorizing the hidden state into any number of in-
terdependent random variables modeling the sub-
states of the complex hidden state. A Hierarchi-
cal Hidden Markov Model is essentially an HMM
with a specific factorization that is useful in many
domains ? the hidden state at each time step is
factored into d random variables which function as
a stack, and d additional boolean random variables
which regulate the operations of the stack through
time. The boolean random variables are typically
marginalized out when performing inference on a
sequence.
While the vertical direction of the hidden sub-
states (at a fixed t) represents a stack at a sin-
gle point in time, the horizontal direction of the
hidden sub-states (at a fixed d) can be viewed as
simple HMMs at depth d, taking direction from
the HMM above them and controlling those be-
low them. This interpretation will be useful when
formally defining the transitions between the stack
elements at different time steps below.
Formally, HMMs characterize speech or text as
a sequence of hidden states q
t
(which may con-
sist of speech sounds, words, and/or other hypoth-
esized syntactic or semantic information), and ob-
served states o
t
at corresponding time steps t (typ-
ically short, overlapping frames of an audio sig-
nal, or words or characters in a text processing
application). A most likely sequence of hidden
states q?
1..T
can then be hypothesized given any se-
quence of observed states o
1..T
, using Bayes? Law
(Equation 2) and Markov independence assump-
tions (Equation 3) to define a full P(q
1..T
| o
1..T
)
probability as the product of a Language Model
(?
L
) prior probability and an Observation Model
(?
O
) likelihood probability:
q?
1..T
= argmax
q
1..T
P(q
1..T
| o
1..T
) (1)
= argmax
q
1..T
P(q
1..T
) ? P(o
1..T
| q
1..T
) (2)
def
= argmax
q
1..T
T
?
t=1
P
?
L
(q
t
| q
t-1
)?P
?
O
(o
t
| q
t
) (3)
Language model transitions P
?
L
(q
t
| q
t?1
) over
complex hidden states q
t
can be modeled us-
ing synchronized levels of stacked-up compo-
nent HMMs in a Hierarchic Hidden Markov
Model (HHMM) (Murphy and Paskin, 2001).
HHMM transition probabilities are calculated in
two phases: a ?reduce? phase (resulting in an in-
termediate, marginalized state f
t
), in which com-
ponent HMMs may terminate; and a ?shift? phase
(resulting in a modeled state q
t
), in which untermi-
nated HMMs transition, and terminated HMMs are
re-initialized from their parent HMMs. Variables
over intermediate f
t
and modeled q
t
states are fac-
tored into sequences of depth-specific variables ?
one for each of D levels in the HMM hierarchy:
f
t
= ?f
1
t
. . . f
D
t
? (4)
q
t
= ?q
1
t
. . . q
D
t
? (5)
Transition probabilities are then calculated as a
product of transition probabilities at each level, us-
ing level-specific ?reduce? ?F and ?shift? ?Q mod-
els:
P
?
L
(q
t
|q
t-1
) =
?
f
t
P(f
t
|q
t-1
)?P(q
t
|f
t
q
t-1
) (6)
def
=
?
f
1..D
t
D
?
d=1
P
?F(f
d
t
| f
d+1
t
q
d
t-1
q
d-1
t-1
)?
P
?Q(q
d
t
|f
d+1
t
f
d
t
q
d
t-1
q
d-1
t
) (7)
with fD+1
t
and q0
t
defined as constants.
Shift and reduce probabilities are now defined
in terms of finitely recursive FSAs with probabil-
ity distributions over transition, recursive expan-
sion, and final-state status of states at each hier-
archy level. In simple HHMMs, each interme-
diate state variable is a boolean switching vari-
able fd
t
? {0,1} and each modeled state variable
is a syntactic, lexical, or phonetic state qd
t
. The
intermediate variable fd
t
is true (equal to 1) with
probability 1 if there is a transition at the level im-
mediately below d and the stack element qd
t?1
is a
final state, and false (equal to 0) with probability 1
otherwise:4
P
?F(f
d
t
| f
d+1
t
q
d
t?1
q
d?1
t?1
)
def
=
{
if fd+1
t
=0 : [f
d
t
=0]
if fd+1
t
=1 : P
?F-Reduce(f
d
t
| q
d
t?1
, q
d?1
t?1
)
(8)
where fD+1 = 1 and q0
t
= ROOT.
Shift probabilities at each level are defined us-
ing level-specific transition ?Q-Trans and expan-
4Here [?] is an indicator function: [?] = 1 if ? is true, 0
otherwise.
573
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8 t=9 t=10 t=11 t=12 t=13
and the
first
kind of
invasion
of the
first
type of
privacy
. . .
? ? ? ? ? ? ? ?
NP/NN
NP/NN
? ?
? ?
NP/NN
NP/NN
NP/PP
NP/NP
NP/PP
NP/NP
NP/NP
NP/NP
NP/PP
NP/NP
?
S/S S/S S/S S/S S/S S/S S/S S/S S/S S/S S/S
S/VP
Figure 3: Sample tree from Figure 2 mapped to qd
t
variable positions of an HHMM at each stack depth d
(vertical) and time step t (horizontal). Values for final-state variables fd
t
are not shown. Note that some
nonterminal labels have been omitted; labels for these nodes can be reconstructed from their children.
This includes the EDITED-NP nonterminal that occurs as a child of the NP/NP at t=8, d=2, indicated in
boldface.
sion ?Q-Expand models:
P
?Q(q
d
t
| f
d+1
t
f
d
t
q
d
t?1
q
d?1
t
)
def
=
?
?
?
if fd+1
t
=0, f
d
t
=0 : [q
d
t
= q
d
t?1
]
if fd+1
t
=1, f
d
t
=0 : P
?Q-Trans(q
d
t
| q
d
t?1
q
d?1
t
)
if fd+1
t
=1, f
d
t
=1 : P
?Q-Expand(q
d
t
| q
d?1
t
)
(9)
where fD+1 = 1 and q0
t
= ROOT. This model
is conditioned on final-state switching variables at
and immediately below the current HHMM level.
If there is no final state immediately below the
current level (the first case above), it determinis-
tically copies the current HHMM state forward to
the next time step. If there is a final state imme-
diately below the current level (the second case
above), it transitions the HHMM state at the cur-
rent level, according to the distribution ?Q-Trans.
And if the state at the current level is final (the
third case above), it re-initializes this state given
the state at the level above, according to the distri-
bution ?Q-Expand. The overall effect is that higher-
level HMMs are allowed to transition only when
lower-level HMMs terminate. An HHMM there-
fore behaves like a probabilistic implementation of
a pushdown automaton (or ?shift-reduce? parser)
with a finite stack, where the maximum stack depth
is equal to the number of levels in the HHMM hi-
erarchy.
4.2 Mapping trees to HHMM derivations
Any tree can now be mapped to an HHMM deriva-
tion by aligning the nonterminals with qd
t
cate-
gories. First, it is necessary to define rightward
depth d, right index position t, and final (right)
child status f , for every nonterminal node A in a
tree, where:
? d is defined to be the number of right branches
between node A and the root,
? t is defined to be the number of words beneath
or to the left of node A, and
? f is defined to be 0 if node A is a left (or
unary) child, 1 otherwise.
Any binary-branching tree can then be annotated
with these values and rewritten to define labels and
final-state values for every combination of d and t
covered by the tree. This process simply copies
stacked up constituents over multiple time steps,
while other constituents are being recognized. Co-
ordinates d, t ? D,T that are not covered by the
tree are assigned label ???, and f = 1. The result-
ing label and final-state values at each node now
define a value of qd
t
and fd
t+1
for each depth d and
time step t of the HHMM (see Figure 3). Prob-
abilities for HHMM models ?Q-Expand, ?Q-Trans,
and ?F-Reduce can then be estimated from these val-
ues directly. Like the right-corner transform, this
mapping is reversible, so q and f values can be
taken from a hypothesized most likely sequence
and mapped back to trees (which can then undergo
the reverse of the right-corner transform to become
ordinary phrase structure trees).
5 HHMM Application to Speech Repair
While the HHMM parser described above can pro-
duce the same output as a standard probablistic
CYK parser (the most likely parse tree), the differ-
ent parsing strategy of an HHMM parser and the
close connection of this system with a probabilis-
tic model of semantics present potential benefits to
the recognition of disfluent speech.
574
First, by using a depth-limited stack, this model
better adheres to psycholinguistically observed
short term memory limits that the human parser
and generator are likely to obey (Cowan, 2001;
Miller, 1956). The use of a depth-limited stack
is enabled by the right-corner transform?s prop-
erty of transforming right expansions to left ex-
pansions, which minimizes stack usage. Corpus
studies (Schuler et al, 2008) suggest that broad
coverage parsing can be achieved via this trans-
form using only four stack elements. In practical
terms this means that the model is less likely than
a standard CYK parser to spend time and probabil-
ity mass on analyses that conflict with the memory
limits humans appear to be constrained by when
generating and understanding speech.
Second, this model is part of a more general
framework that incorporates a model of referential
semantics into the parsing model of the HHMM
(Schuler et al, in press). While the framework
evaluated in this paper models only the syntactic
contribution to speech repair, there are some cases
where syntactic cues are not sufficient to distin-
guish disfluent from fluent utterances. In many
of these cases, semantic information is the only
way to tell that an utterance contains a repair.5 A
recognition system that incorporates referential se-
mantics with syntax should improve the accuracy
of speech repair recognition as it has been shown
to increase recognition of entities in fluent speech
recognition.
Finally, the semantic model just described, as
well as the mechanics of the HHMM parser on
a right-corner transformed grammar, combine to
form a model that accounts for two previously
distant aspects of speech processing: referential
semantics and speech repair. From the genera-
tive view of language processing, the model starts
with a desired referent, and based on that refer-
ent selects the appropriate syntactic structures, and
within those it selects the appropriate lexical items
to unambiguously describe the referent. In the se-
mantic sense, then, the model is operating in a
top-down fashion, with the referent being the driv-
ing force for the generation of syntax and words.
However, since the model is also working in a left-
5For example, the sentence ?The red. . . uh. . . blue box? is
more likely to be considered a repair in a context with sin-
gle colored boxes, whereas the sentence ?The big. . . uh. . . blue
box? is less likely to be considered a repair in the same con-
text, although the two sentences have the same syntactic struc-
ture.
to-right fashion on a highly left-branching gram-
mar, there is also a bottom-up composition of con-
stituents, which models the phenomenon of speech
repair naturally and accurately.
6 Evaluation
The evaluation of this system was performed on
the Switchboard corpus of transcribed conversa-
tional speech, using the mrg annotations in directo-
ries 2 and 3 for training, and the files sw4004.mrg
to sw4153.mrg in directory 4 for evaluation, fol-
lowing Johnson and Charniak (2004). In addition
to testing the HHMM parser on the Switchboard
corpus, the experiment testing a CYK parser from
Miller and Schuler (2008) was replicated, with
slightly better results due to a change in the evalu-
ation script6 and small changes in the binarization
process (both of these changes affect the baseline
and test systems).
The input to the system consists of the terminal
symbols from the trees in the corpus section men-
tioned above. The terminal symbol strings are first
pre-processed by stripping punctuation and empty
categories, which could not be expected from the
output of a speech recognizer. In addition, any in-
formation about repair is stripped from the input,
including partial words, repair symbols,7 and in-
terruption point information. While an integrated
system for processing and parsing speech may use
both acoustic and syntactic information to find re-
pairs, and thus may have access to some of this
information about where interruptions occur, this
testing paradigm is intended to evaluate the use of
the right-corner transform in a time-series model
on parsing speech repair. To make a fair compari-
son to the CYK baseline of Hale et al (2006), the
recognizer was given correct part-of-speech tags as
input along with words.
The results presented here use two standard met-
rics for assessing accuracy of transcribed speech
with repairs. The first metric, Parseval F-measure,
takes into account precision and recall of all non-
terminal (and non pre-terminal) constituents in a
hypothesized tree relative to the gold standard. The
second metric, EDIT-finding F, measures precision
and recall of the words tagged as EDITED in the
hypothesized tree relative to those tagged EDITED
6Specifically, we switched to using the evalb tool created
by Sekine and Collins (1997).
7The Switchboard corpus has special terminal symbols in-
dicating e.g. the start and end of the reparandum.
575
in the gold standard. F score is defined as usual,
2pr/(p + r) for precision p and recall r.
Table 1 below shows the results of experiments
using the model of speech repair described in this
paper. The ?Baseline? result shows the accuracy of
the binarized grammar at parsing the Switchboard
test set. The ?RCT? result shows the accuracy
of parsing when the right-corner transform is per-
formed on the trees in the training set prior to train-
ing. Finally, the ?HHMM+RCT? results shows the
accuracy of the HHMM parser system described
in this paper, trained on right-corner trees mapped
to the random variables at each time step. ?CYK?
and ?TAG? lines show relevant results from related
work.
System Parseval F EDIT F
Baseline 63.43 41.82
CYK (H06) 71.16 41.7
RCT 73.21 61.03
HHMM+RCT 77.15 68.03
TAG-based model (JC04) ? 79.7
Table 1: Summary of results.
These results show an improvement over the
standard CYK parsing algorithm, in both overall
parsing accuracy and EDIT-finding accuracy. This
shows that the HHMM parser, which is more ap-
plicable to speech input due to its asymptotic linear
time complexity, does not need to sacrifice any ac-
curacy to do so, and indeed improves on accuracy
for both metrics under consideration.
7 Conclusion
The work described here has extended previous
work for recognizing disfluent speech to an incre-
mental model, moving in a direction that holds
promise for eventual direct implementation in a
speech recognizer.
Extending this model to actual speech adds
some complexity, since disfluency phenomena are
difficult to detect in an audio signal. However,
there are also advantages in this extension, since
the extra phonological variables and acoustic ob-
servations contain information that can be useful
in the recognition of disfluency phenomena.
References
Cowan, Nelson. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87?
185.
Hale, John, Izhak Shafran, Lisa Yung, Bonnie Dorr,
Mary Harper, Anna Krasnyanskaya, Matthew Lease,
Yang Liu, Brian Roark, Matthew Snover, and Robin
Stewart. 2006. PCFGs with syntactic and prosodic
indicators of speech repairs. In Proceedings of the
45th Annual Conference of the Association for Com-
putational Linguistics (COLING-ACL).
Johnson, Mark and Eugene Charniak. 2004. A tag-
based noisy channel model of speech repairs. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL ?04), pages
33?39, Barcelona, Spain.
Johnson, Mark. 1998a. Finite state approximation of
constraint-based grammars using left-corner gram-
mar transforms. In Proceedings of COLING/ACL,
pages 619?623.
Johnson, Mark. 1998b. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Levelt, William J.M. 1983. Monitoring and self-repair
in speech. Cognition, 14:41?104.
Miller, Tim and William Schuler. 2008. A unified syn-
tactic model for parsing fluent and disfluent speech.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL ?08).
Miller, George A. 1956. The magical number seven,
plus or minus two: Some limits on our capacity
for processing information. Psychological Review,
63:81?97.
Murphy, Kevin P. and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833?840.
Nakatani, C. and J. Hirschberg. 1994. A corpus-based
study of repair cues in spontaneous speech. The
Journal of the Acoustic Society of America, 95:1603?
1616.
Schuler, William, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2008. Toward a
psycholinguistically-motivated model of language.
In Proceedings of COLING, Manchester, UK.
Schuler, William, Stephen Wu, and Lane Schwartz. in
press. A framework for fast incremental interpre-
tation during speech decoding. Computational Lin-
guistics.
Sekine, Satoshi and Michael Collins. 1997. Evalb
bracket scoring program.
Shriberg, Elizabeth. 1994. Preliminaries to a Theory
of Speech Disfluencies. Ph.D. thesis, University of
California at Berkeley.
576
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 785?792
Manchester, August 2008
Toward a Psycholinguistically-Motivated Model of Language Processing
William Schuler
Computer Science and Engineering
University of Minnesota
schuler@cs.umn.edu
Samir AbdelRahman
Department of Computer Science
Cairo University
s.abdelrahman@fci-cu.edu.eg
Tim Miller
Computer Science and Engineering
University of Minnesota
tmill@cs.umn.edu
Lane Schwartz
Computer Science and Engineering
University of Minnesota
lschwar@cs.umn.edu
Abstract
Psycholinguistic studies suggest a model
of human language processing that 1) per-
forms incremental interpretation of spo-
ken utterances or written text, 2) preserves
ambiguity by maintaining competing anal-
yses in parallel, and 3) operates within
a severely constrained short-term memory
store ? possibly constrained to as few
as four distinct elements. This paper de-
scribes a relatively simple model of lan-
guage as a factored statistical time-series
process that meets all three of the above
desiderata; and presents corpus evidence
that this model is sufficient to parse natu-
rally occurring sentences using human-like
bounds on memory.
1 Introduction
Psycholinguistic studies suggest a model of human
language processing with three important proper-
ties. First, eye-tracking studies (Tanenhaus et al,
1995; Brown-Schmidt et al, 2002) suggest that hu-
mans analyze sentences incrementally, assembling
and interpreting referential expressions even while
they are still being pronounced. Second, humans
appear to maintain competing analyses in paral-
lel, with eye gaze showing significant attention to
competitors (referents of words with similar pre-
fixes to the correct word), even relatively long af-
ter the end of the word has been encountered, when
attention to other distractor referents has fallen off
(Dahan and Gaskell, 2007). Preserving ambigu-
ity in a parallel, non-deterministic search like this
may account for human robustness to missing, un-
known, mispronounced, or misspelled words. Fi-
nally, studies of short-term memory capacity sug-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
gest human language processing operates within a
severely constrained short-term memory store ?
possibly restricted to as few as four distinct ele-
ments (Miller, 1956; Cowan, 2001).
The first two observations may be taken to
endorse existing probabilistic beam-search mod-
els which maintain multiple competing analyses,
pruned by contextual preferences and dead ends
(e.g. Roark, 2001). But the last observation on
memory bounds imposes a restriction that until
now has not been evaluated in a corpus study. Can
a simple, useful human-like processing model be
defined using these constraints? This paper de-
scribes a relatively simple model of language as a
factored statistical time-series process that meets
all three of the above desiderata; and presents
corpus evidence that this model is sufficient to
parse naturally occurring sentences using human-
like bounds on memory.
The remainder of this paper is organized as fol-
lows: Section 2 describes some current approaches
to incremental parsing; Section 3 describes a statis-
tical framework for parsing using a bounded stack
of explicit constituents; Section 4 describes an ex-
periment to estimate the level of coverage of the
Penn Treebank corpus that can be achieved with
various stack memory limits, using a set of re-
versible tree transforms, and gives accuracy results
of a bounded-memory model trained on this cor-
pus.
2 Background
Much work on cognitive modeling in psycholin-
guistics is centered on modeling the concepts to
which utterances refer. Coarsely, these concepts
may correspond to activation patterns among neu-
rons in specific regions of the brain. In some the-
ories, a short-term memory store of several unre-
lated concepts may be retained by organizing the
activation of these concepts into compatible pat-
terns, only a few of which can be reliably main-
785
tained (Smolensky and Legendre, 2006). Activa-
tion is then theorized to spread through and among
these groups of concepts in proportion to some
learned probability that the concepts will be rel-
evant (Anderson and Reder, 1999), with the most
active concepts corresponding to the most likely
linguistic analyses. Competition between rival ac-
tivated groups of concepts (corresponding to in-
complete linguistic analyses) has even been linked
to reading delays (Hale, 2003).
This competition among mutually-exclusive
variously-activated short term memory stores of
concepts, essentially a weighted disjunction over
conjunctions of concepts, can be modeled in lan-
guage understanding as simple Viterbi decoding of
a factored HMM-like time-series model (Schuler
et al, in press). In this model, concepts (corre-
sponding to vectors of individuals in a first-order
world model) are introduced and composed (via
set operations like intersection) in each hypothe-
sized short-term memory store, using the elements
of the memory store as a stack. These vectors of
individuals can be considered a special case of vec-
tors of concept elements proposed by Smolensky,
with set intersection a special case of tensor prod-
uct in the composition model. Referents in this
kind of incremental model can be constrained by
? but still distinguished from ? higher-level ref-
erents while they are still being recognized.
It is often assumed that this semantic con-
cept composition proceeds isomorphically with
the composition of syntactic constituents (Frege,
1892). This parallel semantic and syntactic com-
position is considered likely to be performed in
short-term memory because it has many of the
characteristics of short-term memory processes,
including nesting limits (Miller and Chomsky,
1963) and susceptibility to degradation due to in-
terruption. Ericsson and Kintch (1995) propose a
theory of long-term working memory that extends
short-term memory, but only for inter-sentential
references, which do seem to be retained across
interruptions in reading. But while the relation-
ship between competing probability distributions
in such a model and experimental reading times
has been evaluated (e.g. by Hale), the relationship
between the syntactic demands on a short-term
memory store and observations of human short-
term memory limits is still largely untested. Sev-
eral models have been proposed to perform syntac-
tic analysis using a bounded memory store.
For example, Marcus (1980) proposed a deter-
ministic parser with an explicit four-element work-
ing memory store in order to model human parsing
limitations. But this model only stores complete
constituents (whereas the model proposed in this
paper stores incompletely recognized constituents,
in keeping with the Tanenhaus et al findings). As
a result, the Marcus model relies on a suite of spe-
cialized memory operations to compose complete
constituents out of complete constituents, which
are not independently cognitively motivated.
Cascaded finite-state automata, as in FASTUS
(Hobbs et al, 1996), also make use of a bounded
stack, but stack levels in such systems are typically
dedicated to particular syntactic operations: e.g.
a word group level, a phrasal level, and a clausal
level. As a result, some amount of constituent
structure may overflow its dedicated level, and be
sacrificed (for example, prepositional phrase at-
tachment may be left underspecified).
Finite-state equivalent parsers (and thus,
bounded-stack parsers) have asymptotically linear
run time. Other parsers (Sagae and Lavie, 2005)
have achieved linear runtime complexity with
unbounded stacks in incremental parsing by
using a greedy strategy, pursuing locally most
probable shift or reduce operations, conditioned
on multiple surrounding words. But without an
explicit bounded stack it is difficult to connect
these models to concepts in a psycholinguistic
model.
Abney and Johnson (1991) explore left-corner
parsing as a memory model, but again only in
terms of (complete) syntactic constituents. The
approach explored here is similar, but the trans-
form is reversed to allow the recognizer to store
recognized structure rather than structures being
sought, and the transform is somewhat simpli-
fied to allow more structure to be introduced into
syntactic constituents, primarily motivated by a
need to keep track of disconnected semantic con-
cepts rather than syntactic categories. Without this
link to disconnected semantic concepts, the syntax
model would be susceptible to criticism that the
separate memory levels could be simply chunked
together through repeated use (Miller, 1956).
Roark?s (2001) top-down parser generates trees
incrementally in a transformed representation re-
lated to that used in this paper, but requires dis-
tributions to be maintained over entire trees rather
than stack configurations. This increases the beam
786
width necessary to avoid parse failure. Moreover,
although the system is conducting a beam search,
the objects in this beam are growing, so the recog-
nition complexity is not linear, and the connection
to a bounded short-term memory store of uncon-
nected concepts becomes somewhat complicated.
The model described in this paper is arguably
simpler than many of the models described above
in that it has no constituent-specific mechanisms,
yet it is able to recognize the rich syntactic struc-
tures found in the Penn Treebank, and is still
compatible with the psycholinguistic notion of a
bounded short-term memory store of conceptual
referents.
3 Bounded-Memory Parsing with a Time
Series Model
This section describes a basic statistical framework
? a factored time-series model ? for recogniz-
ing hierarchic structures using a bounded store of
memory elements, each with a finite number of
states, at each time step. Unlike simple FSA com-
pilation, this model maintains an explicit represen-
tation of active, incomplete phrase structure con-
stituents on a bounded stack, so it can be readily
extended with additional variables that depend on
syntax (e.g. to track hypothesized entities or rela-
tions). These incomplete constituents are related
to ordinary phrase structure annotations through a
series of bidirectional tree transforms. These trans-
forms:
1. binarize phrase structure trees into linguisti-
cally motivated head-modifier branches (de-
scribed in Section 3.1);
2. transform right-branching sequences to left-
branching sequences (described in Sec-
tion 3.2); and
3. align transformed trees to an array of random
variable values at each depth and time step of
a probabilistic time-series model (described
in Section 3.3).
Following these transforms, a model can be trained
from example trees, then run as a parser on unseen
sentences. The transforms can then be reversed to
evaluate the output of the parser. This representa-
tion will ultimately be used to evaluate the cover-
age of a bounded-memory model on a large corpus
of tree-annotated sentences, and to evaluate the ac-
curacy of a basic (unsmoothed, unlexicalized) im-
plementation of this model in Section 4.
It is important to note that these transformations
are not postulated to be part of the human recog-
nition process. In this model, sentences can be
recognized and interpreted entirely in right-corner
form. The transforms only serve to connect this
process to familiar representations of phrase struc-
ture.
3.1 Binary branching structure
This paper will attempt to draw conclusions about
the syntactic complexity of natural language, in
terms of stack memory requirements in incremen-
tal (left-to-right) recognition. These requirements
will be minimized by recognizing trees in a right-
corner form, which accounts partially recognized
phrases and clauses as incomplete constituents,
lacking one instance of another constituent yet to
come.
In particular, this study will use the trees in the
Penn Treebank Wall Street Journal (WSJ) corpus
(Marcus et al, 1994) as a data set. In order to
obtain a linguistically plausible right-corner trans-
form representation of incomplete constituents, the
corpus is subjected to another, pre-process trans-
form to introduce binary-branching nonterminal
projections, and fold empty categories into non-
terminal symbols in a manner similar to that pro-
posed by Johnson (1998b) and Klein and Manning
(2003). This binarization is done in such a way
as to preserve linguistic intuitions of head projec-
tion, so that the depth requirements of right-corner
transformed trees will be reasonable approxima-
tions to the working memory requirements of a hu-
man reader or listener.
3.2 Right-Corner Transform
Phrase structure trees are recognized in this frame-
work in a right-corner form that can be mapped to
and from ordinary phrase structure via reversible
transform rules, similar to those described by
Johnson (1998a). This transformed grammar con-
strains memory usage in left-to-right traversal to a
bound consistent with the psycholinguistic results
described above.
This right-corner transform is simply the left-
right dual of a left-corner transform (Johnson,
1998a). It transforms all right branching sequences
in a phrase structure tree into left branching se-
quences of symbols of the form A
1
/A
2
, denoting
an incomplete instance of category A
1
lacking an
instance of category A
2
to the right. These incom-
plete constituent categories have the same form
787
a) binarized phrase structure tree:
S
NP
NP
JJ
strong
NN
demand
PP
IN
for
NP
NPpos
NNP
NNP
new
NNP
NNP
york
NNP
city
POS
?s
NNS
JJ
general
NNS
NN
obligation
NNS
bonds
VP
VBN
VBN
propped
PRT
up
NP
DT
the
NN
JJ
municipal
NN
market
b) result of right-corner transform:
S
S/NN
S/NN
S/NP
S/VP
NP
NP/NNS
NP/NNS
NP/NNS
NP/NP
NP/PP
NP
NP/NN
JJ
strong
NN
demand
IN
for
NPpos
NPpos/POS
NNP
NNP/NNP
NNP/NNP
NNP
new
NNP
york
NNP
city
POS
?s
JJ
general
NN
obligation
NNS
bonds
VBN
VBN/PRT
VBN
propped
PRT
up
DT
the
JJ
municipal
NN
market
Figure 1: Trees resulting from a) a binarization of a sample phrase structure tree for the sentence Strong
demand for New York City?s general obligations bonds propped up the municipal market, and b) a right-
corner transform of this binarized tree.
and much of the same meaning as non-constituent
categories in a Combinatorial Categorial Grammar
(Steedman, 2000).
Rewrite rules for the right-corner transform are
shown below, first to flatten out right-branching
structure:1
1The tree transforms presented in this paper will be de-
fined in terms of destructive rewrite rules applied iteratively
to each constituent of a source tree, from leaves to root, and
from left to right among siblings, to derive a target tree. These
rewrites are ordered; when multiple rewrite rules apply to the
same constituent, the later rewrites are applied to the results
of the earlier ones. For example, the rewrite:
A
0
. . . A
1
?
2
?
3
. . .
?
A
0
. . . ?
2
?
3
. . .
could be used to iteratively eliminate all binary-branching
nonterminal nodes in a tree, except the root. In the notation
used in this paper, Roman uppercase letters (A
i
) are variables
matching constituent labels, Roman lowercase letters (a
i
) are
variables matching terminal symbols, Greek lowercase letters
A
1
?
1
A
2
?
2
A
3
a
3
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
A
3
a
3
A
1
?
1
A
2
A
2
/A
3
?
2
. . .
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
. . .
then to replace it with left-branching structure:
(?
i
) are variables matching entire subtree structure, Roman
letters followed by colons, followed by Greek letters (A
i
:?
i
)
are variables matching the label and structure, respectively, of
the same subtree, and ellipses (. . . ) are taken to match zero
or more subtree structures, preserving the order of ellipses in
cases where there are more than one (as in the rewrite shown
above).
788
A1
A
1
/A
2
:?
1
A
2
/A
3
?
2
?
3
. . .
?
A
1
A
1
/A
3
A
1
/A
2
:?
1
?
2
?
3
. . .
Here, the first two rewrite rules are applied iter-
atively (bottom-up on the tree) to flatten all right
branching structure, using incomplete constituents
to record the original nonterminal ordering. The
third rule is then applied to generate left-branching
structure, preserving this ordering. Note that the
last rewrite above leaves a unary branch at the left-
most child of each flattened node. This preserves
the nodes at which the original tree was not right-
branching, so the original tree can be reconstructed
when the right-corner transform concatenates mul-
tiple right-branching sequences into a single left-
branching sequence.
An example of a right-corner transformed tree
is shown in Figure 1(b). An important property of
this transform is that it is reversible. Rewrite rules
for reversing a right-corner transform are simply
the converse of those shown above. The correct-
ness of this can be demonstrated by dividing a
tree into maximal sequences of right branches (that
is, maximal sequences of adjacent right children).
The first two ?flattening? rewrites of the right-
corner transform, applied to any such sequence,
will replace the right-branching nonterminal nodes
with a flat sequence of nodes labeled with slash
categories, which preserves the order of the non-
terminal category symbols in the original nodes.
Reversing this rewrite will therefore generate the
original sequence of nonterminal nodes. The final
rewrite similarly preserves the order of these non-
terminal symbols while grouping them from the
left to the right, so reversing this rewrite will re-
produce the original version of the flattened tree.
3.3 Hierarchic Hidden Markov Models
Right-corner transformed phrase structure trees
can then be mapped to random variable positions
in a Hierarchic Hidden Markov Model (Murphy
and Paskin, 2001), essentially a Hidden Markov
Model (HMM) factored into some fixed number of
stack levels at each time step.
HMMs characterize speech or text as a sequence
of hidden states q
t
(in this case, stacked-up syn-
tactic categories) and observed states o
t
(in this
case, words) at corresponding time steps t. A
most likely sequence of hidden states q?
1..T
can
then be hypothesized given any sequence of ob-
served states o
1..T
, using Bayes? Law (Equation 2)
and Markov independence assumptions (Equa-
tion 3) to define a full P(q
1..T
| o
1..T
) probabil-
ity as the product of a Transition Model (?
A
)
prior probability P(q
1..T
)
def
=
?
t
P
?
A
(q
t
| q
t-1
) and
an Observation Model (?
B
) likelihood probability
P(o
1..T
| q
1..T
)
def
=
?
t
P
?
B
(o
t
| q
t
):
q?
1..T
= argmax
q
1..T
P(q
1..T
| o
1..T
) (1)
= argmax
q
1..T
P(q
1..T
)?P(o
1..T
| q
1..T
) (2)
def
= argmax
q
1..T
T
?
t=1
P
?
A
(q
t
| q
t-1
)?P
?
B
(o
t
| q
t
) (3)
Transition probabilities P
?
A
(q
t
| q
t-1
) over com-
plex hidden states q
t
can be modeled using syn-
chronized levels of stacked-up component HMMs
in a Hierarchic Hidden Markov Model (HHMM)
(Murphy and Paskin, 2001). HHMM transition
probabilities are calculated in two phases: a re-
duce phase (resulting in an intermediate, marginal-
ized state f
t
), in which component HMMs may ter-
minate; and a shift phase (resulting in a modeled
state q
t
), in which unterminated HMMs transition,
and terminated HMMs are re-initialized from their
parent HMMs. Variables over intermediate f
t
and
modeled q
t
states are factored into sequences of
depth-specific variables ? one for each of D levels
in the HMM hierarchy:
f
t
= ?f
1
t
. . . f
D
t
? (4)
q
t
= ?q
1
t
. . . q
D
t
? (5)
Transition probabilities are then calculated as a
product of transition probabilities at each level, us-
ing level-specific reduce ?R and shift ?S models:
P
?
A
(q
t
|q
t-1
) =
?
f
t
P(f
t
|q
t-1
)?P(q
t
|f
t
q
t-1
) (6)
def
=
?
f
1..D
t
D
?
d=1
P
?R(f
d
t
|f
d+1
t
q
d
t-1
q
d-1
t-1
)?
P
?S(q
d
t
|f
d+1
t
f
d
t
q
d
t-1
q
d-1
t
) (7)
with fD+1
t
and q0
t
defined as constants. In Viterbi
decoding, the sums are replaced with argmax oper-
ators. This decoding process preserves ambiguity
by maintaining competing analyses of the entire
memory store. A graphical representation of an
HHMM with three levels is shown in Figure 3.
Shift and reduce probabilities can then be de-
fined in terms of finitely recursive Finite State Au-
tomata (FSAs) with probability distributions over
789
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8 t=9 t=10 t=11 t=12 t=13 t=14 t=15
strong
dem
and
for
new
york
city ?s
general
obligations
bonds
propped
up the
m
unicipal
m
arket
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
? ? ? ?
NNP/NNP
NNP/NNP
NPpos/POS
? ? ? ?
VBN/PRT
? ? ?
?
NP/NN
NP/PP
NP/NP
NP/NP
NP/NP
NP/NP
NP/NNS
NP/NNS
NP/NNS
S/VP
S/VP
S/NP
S/NN
S/NN
Figure 2: Sample tree from Figure 1 mapped to qd
t
variable positions of an HHMM at each stack depth
d (vertical) and time step t (horizontal). This tree uses only two levels of stack memory. Values for
final-state variables fd
t
are not shown. Note that some nonterminal labels have been omitted; labels for
these nodes can be reconstructed from their children.
transition, recursive expansion, and final-state sta-
tus of states at each hierarchy level. In simple HH-
MMs, each intermediate variable is a boolean vari-
able over final-state status fd
t
? {0,1} and each
modeled state variable is a syntactic, lexical, or
phonetic state qd
t
. The intermediate variable fd
t
is
true or false (equal to 1 or 0 respectively) accord-
ing to ?F-Reduce if there is a transition at the level
immediately below d, and false (equal to 0) with
probability 1 otherwise:2
P
?R(f
d
t
| f
d+1
t
q
d
t-1
q
d-1
t-1
)
def
=
{
if fd+1
t
=0 : [f
d
t
=0]
if fd+1
t
=1 : P
?F-Reduce(f
d
t
| q
d
t-1
, q
d-1
t-1
)
(8)
where fD+1
t
= 1 and q0
t
= ROOT.
Shift probabilities over the modeled variable qd
t
at each level are defined using level-specific tran-
sition ?Q-Trans and expansion ?Q-Expand models:
P
?S(q
d
t
| f
d+1
t
f
d
t
q
d
t-1
q
d-1
t
)
def
=
?
?
?
if fd+1
t
=0, f
d
t
=0 : [q
d
t
= q
d
t-1
]
if fd+1
t
=1, f
d
t
=0 : P
?Q-Trans(q
d
t
| q
d
t-1
q
d-1
t
)
if fd+1
t
=1, f
d
t
=1 : P
?Q-Expand(q
d
t
| q
d-1
t
)
(9)
where fD+1
t
= 1 and q0
t
= ROOT. This model
is conditioned on final-state switching variables at
and immediately below the current FSA level. If
there is no final state immediately below the cur-
rent level (the first case above), it deterministically
copies the current FSA state forward to the next
time step. If there is a final state immediately be-
low the current level (the second case above), it
2Here [?] is an indicator function: [?] = 1 if ? is true, 0
otherwise.
. . .
. . .
. . .
. . .
f
3
t?1
f
2
t?1
f
1
t?1
q
1
t?1
q
2
t?1
q
3
t?1
o
t?1
f
3
t
f
2
t
f
1
t
q
1
t
q
2
t
q
3
t
o
t
Figure 3: Graphical representation of a Hierarchic
Hidden Markov Model. Circles denote random
variables, and edges denote conditional dependen-
cies. Shaded circles are observations.
transitions the FSA state at the current level, ac-
cording to the distribution ?Q-Trans. And if the state
at the current level is final (the third case above),
it re-initializes this state given the state at the level
above, according to the distribution ?Q-Expand. The
overall effect is that higher-level FSAs are allowed
to transition only when lower-level FSAs termi-
nate. An HHMM therefore behaves like a prob-
abilistic implementation of a pushdown automaton
(or shift?reduce parser) with a finite stack, where
the maximum stack depth is equal to the number
of levels in the HHMM hierarchy.
Figure 2 shows the transformed tree from Fig-
ure 1 aligned to HHMM depth levels and time
steps. Because it uses a bounded stack, recognition
in this model is asymptotically linear (Murphy and
Paskin, 2001).
This model recognizes right-corner transformed
trees constrained to a stack depth corresponding to
observed human short term memory limits. This
790
HHMM depth limit sentences coverage
no memory 127 0.32%
1 memory element 3,496 8.78%
2 memory elements 25,909 65.05%
3 memory elements 38,902 97.67%
4 memory elements 39,816 99.96%
5 memory elements 39,832 100.00%
TOTAL 39,832 100.00%
Table 1: Percent coverage of right-corner trans-
formed treebank sections 2?21 with punctuation
omitted, using HHMMs with depth limits D from
zero to five.
is an attractive model of human language process-
ing because the incomplete syntactic constituents
it stores at each stack depth can be directly associ-
ated with (incomplete) semantic referents, e.g. by
adding random variables over environment or dis-
course referents at each depth and time step. If
these referents are calculated incrementally, recog-
nition decisions can be informed by the values of
these variables in an interactive model of language,
following Tanenhaus et al (1995). The corpus re-
sults described in the next section suggest that a
large majority of naturally occurring sentences can
be recognized using only three or four stack mem-
ory elements via this transform.
4 Empirical Results
In order to evaluate the coverage of this bounded-
memory model, Sections 2?21 of the Penn Tree-
bank WSJ corpus were transformed and mapped
to HHMM variables as described in Section 3.3. In
order to counter possible undesirable effects of an
arbitrary branching analysis of punctuation, punc-
tuation was removed. Coverage results on this cor-
pus are shown in Table 1.
Experiments training on transformed trees from
Sections 2?21 of the WSJ Treebank, evaluating
reversed-transformed output sequences from Sec-
tion 22 (development set) and Section 23 (test set),
show an accuracy (F score) of 82.1% and 80.1%
respectively.3 Although they are lower than those
for state-of-the-art parsers, these results suggest
that the bounded-memory parser described here is
doing a reasonably good job of modeling syntac-
tic dependencies, and therefore may have some
3Using unsmoothed relative frequency estimates from the
training set, a depth limit of D = 3, beam with of 2000, and
no lexicalization.
promise as a psycholinguistic model.
Although recognition in this system is linear, it
essentially works top-down, so it has larger run-
time constants than a bottom-up CKY-style parser.
The experimental system described above runs at
a rate of about 1 sentence per second on a 64-
bit 2.6GHz dual core desktop with a beam width
of 2000. In comparison, the Klein and Manning
(2003) CKY-style parser runs at about 5 sentences
per second on the same machine. On sentences
longer than 40 words, the HHMM and CKY-style
parsers are roughly equivalent, parsing at the rate
of .21 sentences per second, versus .24 for the
Klein and Manning CKY.
But since it is linear, the HHMM parser can be
directly integrated with end-of-sentence detection
(e.g. deciding whether ?.? is a sentence delimiter
based on whether the words preceding it can be
reduced as a sentence), or with n-gram language
models (if words are observations, this is simply
an autoregressive HMM topology). The use of
an explicit constituent structure in a time series
model also allows integration with models of dy-
namic phenomena such as semantics and corefer-
ence which may depend on constituency. Finally,
as a linear model, it can be directly applied to
speech recognition (essentially replacing the hid-
den layer of a conventional word-based HMM lan-
guage model).
5 Conclusion
This paper has described a basic incremental pars-
ing model that achieves worst-case linear time
complexity by enforcing fixed limits on a stack
of explicit (albeit incomplete) constituents. Ini-
tial results show a use of only three to four levels
of stack memory within this framework provides
nearly complete coverage of the large Penn Tree-
bank corpus.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their input. This research was
supported by National Science Foundation CA-
REER/PECASE award 0447685. The views ex-
pressed are not necessarily endorsed by the spon-
sors.
791
References
Abney, Steven P. and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
Anderson, J.R. and L.M. Reder. 1999. The fan effect:
New results and new theories. Journal of Experi-
mental Psychology: General, 128(2):186?197.
Brown-Schmidt, Sarah, Ellen Campana, and
Michael K. Tanenhaus. 2002. Reference res-
olution in the wild: Online circumscription of
referential domains in a natural interactive problem-
solving task. In Proceedings of the 24th Annual
Meeting of the Cognitive Science Society, pages
148?153, Fairfax, VA, August.
Cowan, Nelson. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87?
185.
Dahan, Delphine and M. Gareth Gaskell. 2007. The
temporal dynamics of ambiguity resolution: Evi-
dence from spoken-word recognition. Journal of
Memory and Language, 57(4):483?501.
Ericsson, K. Anders and Walter Kintsch. 1995.
Long-term working memory. Psychological Review,
102:211?245.
Frege, Gottlob. 1892. Uber sinn und bedeutung.
Zeitschrift fur Philosophie und Philosophischekritik,
100:25?50.
Hale, John. 2003. Grammar, Uncertainty and Sen-
tence Processing. Ph.D. thesis, Cognitive Science,
The Johns Hopkins University.
Hobbs, Jerry R., Douglas E. Appelt, John Bear,
David Israel, Megumi Kameyama, Mark Stickel, and
Mabry Tyson. 1996. Fastus: A cascaded finite-state
transducer for extracting information from natural-
language text. In Finite State Devices for Natural
Language Processing, pages 383?406. MIT Press,
Cambridge, MA.
Johnson, Mark. 1998a. Finite state approximation of
constraint-based grammars using left-corner gram-
mar transforms. In Proceedings of COLING/ACL,
pages 619?623.
Johnson, Mark. 1998b. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Marcus, Mitch. 1980. A theory of syntactic recognition
for natural language. MIT Press.
Miller, George and Noam Chomsky. 1963. Finitary
models of language users. In Luce, R., R. Bush, and
E. Galanter, editors, Handbook of Mathematical Psy-
chology, volume 2, pages 419?491. John Wiley.
Miller, George A. 1956. The magical number seven,
plus or minus two: Some limits on our capacity
for processing information. Psychological Review,
63:81?97.
Murphy, Kevin P. and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833?840.
Roark, Brian. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Sagae, Kenji and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies (IWPT?05).
Schuler, William, Stephen Wu, and Lane Schwartz. in
press. A framework for fast incremental interpre-
tation during speech decoding. Computational Lin-
guistics.
Smolensky, Paul and Ge?raldine Legendre. 2006.
The Harmonic Mind: From Neural Computation to
Optimality-Theoretic GrammarVolume I: Cognitive
Architecture. MIT Press.
Steedman, Mark. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
Tanenhaus, Michael K., Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. Inte-
gration of visual and linguistic information in spoken
language comprehension. Science, 268:1632?1634.
792
A Framework for Fast Incremental
Interpretation during Speech Decoding
William Schuler?
University of Minnesota
Stephen Wu?
University of Minnesota
Lane Schwartz?
University of Minnesota
This article describes a framework for incorporating referential semantic information from a
world model or ontology directly into a probabilistic language model of the sort commonly
used in speech recognition, where it can be probabilistically weighted together with phonological
and syntactic factors as an integral part of the decoding process. Introducing world model
referents into the decoding search greatly increases the search space, but by using a single
integrated phonological, syntactic, and referential semantic language model, the decoder is able to
incrementally prune this search based on probabilities associated with these combined contexts.
The result is a single unified referential semantic probability model which brings several kinds
of context to bear in speech decoding, and performs accurate recognition in real time on large
domains in the absence of example in-domain training sentences.
1. Introduction
The capacity to rapidly connect language to referential meaning is an essential aspect
of communication between humans. Eye-tracking studies show that humans listening
to spoken directives are able to actively attend to the entities that the words in these
directives might refer to, even while the words are still being pronounced (Tanenhaus
et al 1995; Brown-Schmidt, Campana, and Tanenhaus 2002). This timely access to
referential information about input utterances may allow listeners to adjust their pref-
erences among likely interpretations of noisy or ambiguous utterances to favor those
that make sense in the current environment or discourse context, before any lower-level
disambiguation decisions have been made. This same capability in a spoken language
interface system could allow reliable human?machine interaction in the idiosyncratic
language of day-to-day life, populated with proper names of co-workers, objects, and
events not found in broad training corpora. When domain-specific training corpora are
? Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455.
E-mail: schuler@cs.umn.edu; swu@cs.umn.edu; lane@cs.umn.edu.
Submission received: 25 April 2007; revised submission received: 4 March 2008; accepted for publication:
2 June 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 3
not available, a referential semantic interface could still exploit its model of the world:
the data to which it is an interface, and patterns characterizing these data.
This article describes a framework for incorporating referential semantic informa-
tion from a world model or ontology directly into a statistical language model of the
sort commonly used in speech recognition, where it can be probabilistically weighted
together with phonological and syntactic factors as an integral part of the decoding
process. Introducing world model referents into the decoding search greatly increases
the search space, but by using a single integrated phonological, syntactic, and referential
semantic language model, the decoder is able to incrementally prune this search based
on probabilities associated with these combined contexts.
Semantic interpretation is defined dynamically in this framework, in terms of transi-
tions over time from less constrained referents to more constrained referents. Because it
is defined dynamically, interpretation in this framework can incorporate dependencies
on referential context?for example, constraining interpretations to a presumed set of
entities, or a presumed setting?which may be fixed prior to recognition, or dynam-
ically hypothesized earlier in the recognition process. This contrasts with other recent
systemswhich interpret constituents only given fixed inter-utterance contexts or explicit
syntactic arguments (Schuler 2001; DeVault and Stone 2003; Gorniak and Roy 2004; Aist
et al 2007). Moreover, because it is defined dynamically, in terms of transitions, this
context-dependent interpretation framework can be directly integrated into a Viterbi
decoding search, like ordinary state transitions in a Hidden Markov Model. The result
is a single unified referential semantic probability model which brings several kinds
of referential semantic context to bear in speech decoding, and performs accurate
recognition in real time on large domains in the absence of example domain-specific
training sentences.
The remainder of this article is organized as follows: Section 2 will describe related
approaches to interleaving semantic interpretation with speech recognition. Section 3
will provide definitions for worldmodels used in semantic interpretation, and language
models used in speech decoding, which will form the basis of a referential semantic
language model, defined in Section 4. Then Section 5 will describe an evaluation of this
model in a sample spoken language interface application.
2. Related Work
Early approaches to incremental interpretation (Mellish 1985; Haddock 1989) apply
semantic constraints associated with each word in a sentence to progressively winnow
the set of individuals that could serve as referents in that sentence. These incrementally
constrained referents are then used to guide the syntactic analysis of the sentence, dis-
preferring analyses with empty interpretations in the current environment or discourse
context. Similar approaches were applied to broad-coverage text processing, querying a
large commonsense knowledge base as a world model (Martin and Riesbeck 1986). But
this winnowing is done deterministically, invoking default assumptions and potentially
exponential backtracking when default assumptions fail.
The idea of basing analysis decisions on constrained sets of referent individuals
was later extended to pursue multiple interpretations at once by exploiting polynomial
structure-sharing in a dynamic programming parser (Schuler 2001; DeVault and Stone
2003; Gorniak and Roy 2004; Aist et al 2007). The resulting shared interpretation is
similar to underspecified semantic representations (Bos 1996), except that the rep-
resentation mainly preserves syntactic ambiguity rather than semantic (e.g., quanti-
314
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
fier scoping) ambiguity, and the size complexity of the parser chart representation is
polynomially bounded. This approach was further extended to support hypothetical
referents (DeVault and Stone 2003), domains with continuous relations (Gorniak and
Roy 2004), and updates to the shared parser chart by components handling other levels
of linguistic analysis in parallel, during real-time recognition (Aist et al 2007).
The advantage of this use of the parser chart is that it allows a straightforward
mapping between syntax and semantics using familiar compositional semantic rep-
resentations. But the standard dynamic programming algorithm for parsing derives
its complexity bounds from the fact that each recognized constituent can be analyzed
independently of every other constituent. These independence assumptions must be
relaxed if dynamic context dependencies are to be applied across sibling constituents
(e.g., in the package data directory, open . . . , where the files to be opened should be
restricted to the contents of the package data directory). More importantly, from an
engineering perspective, the dynamic programming algorithm for parsing runs in cubic
time, not linear, which means this interpretation framework cannot be directly applied
to continuous audio streams. Interface systems therefore typically perform utterance
or sentence segmentation as a stand-alone pre-process, without integrating syntactic or
referential semantic dependencies into this decision.
Finally, some speech recognition systems employ inter-utterance context-dependent
language models that are pre-compiled into word n-grams for particular discourse or
environment states, and swapped out between utterances (Young et al 1989; Lemon
and Gruenstein 2004; Seneff et al 2004). But in some cases accurate interpretation will
require spoken language interfaces to exploit context continuously during utterance
recognition, not just between utterances. For example, the probability distribution over
the next word in the utterance go to the package data directory and get the . . . (or in the
package data directory get the . . . ) will depend crucially on the linguistic and environment
context leading up to this point: the meaning of package data directory in the first part of
this directive, as well as the objects that will be available once this part of the directive
has been carried out. Moreover, in rich environments pre-compilation to word n-grams
can be expensive, since all referents in the world model must be considered to build
accurate n-grams. This will not be practical if environments change frequently.
3. Background
In contrast to the approaches described in Section 2, this article proposes an incremental
interpretation framework which is entirely contained within a single-pass probabilistic
decoding search. Essentially, this approach directly integrates model theoretic seman-
tics, summarized in Section 3.1, with conventional probabilistic time-series models used
in speech recognition, summarized in Section 3.2.
3.1 Referential Semantics
Semantic interpretation requires a framework within which a speaker?s intended mean-
ings can be formalized. Sections 3.1.1 and 3.1.2 describe a model theoretic approach
to semantic interpretation that will later be extended in Section 4.1. The referential
states defined here will then be incorporated into a representation of nested syntactic
constituents in a hierarchic time-series model in Section 4.2. Some of the notation
introduced here is summarized later in Table 1 (Section 4).
315
Computational Linguistics Volume 35, Number 3
Figure 1
A subsumption lattice (laid on its side) over the power set of a domain containing three
individuals: ?1, ?2, and ?3. Subsumption relations are represented as gray arrows from supersets
(or super-concepts) to subsets (or sub-concepts).
3.1.1 Model Theory. The language model described in this article defines semantic ref-
erents in terms of a world model M. In model theory (Tarski 1933; Church 1940), a
world model is defined as a tuple M = ?E , ?? containing a domain of individuals E =
{?1, ?2, . . . } and an interpretation function ? to interpret expressions in terms of those
individuals. This interpretation function accepts expressions ? of various types: logical
statements, of simple type T (for example, the demo file is writable) which may be true
or false; references to individuals, of simple type E (for example, the demo file) which
may refer to any individual in the world model; or functors of complex type ??,??,
which take an argument of type ? and produce output of type ?. Functor expressions ?
of type ??,?? can be applied to other expressions ? of type ? as arguments to yield
expressions ?(?) of type ? (for example, writablemay take the demo file as an argument
and return true). By nesting functors, complex expressions can be defined, denoting
sets or properties of individuals: ?E, T? (for example, writable), relations over individual
pairs: ?E, ?E, T?? (for example, contains), or first-order functors over sets: ??E, T?, ?E, T??
(for example, a comparative adjective like larger).
3.1.2 Ontological Promiscuity. First-order or higher models (in which functors can take
sets as arguments) can be mapped to equivalent zero-order models (with functors
defined only on entities). This is generally motivated by a desire to allow sets of
individuals to be described in much the same way as individuals themselves (Hobbs
1985). Entities in a zero-order model M can be defined from individuals in a higher-
order model M? by mapping or reifying each set S = {?1, ?2, . . . } in P (EM? ) (or each
set of sets in P (P (EM? )), etc.) as an entity eS in a new domain EM.1 Relations l inter-
preted as zero-order functors inM can be defined directly from relations l? interpreted
as higher-order functors (over sets) in M? by mapping each instance of ?S1,S2? in
l?M? : P (EM? )?P (EM? ) to a corresponding instance of ?eS1 , eS2? in lM : EM?EM. Set
subsumption inM? can then be defined on entities made from reified sets inM, similar
to ?ISA? relations over concepts in knowledge representation systems (Brachman and
Schmolze 1985).
These subset or subsumption relations can be represented in a subsumption lattice,
as shown in Figure 1, with supersets to the left connecting to subsets to the right. This
representation will be used in Section 4 to define weighted transitions over first-order
referents in a statistical time-series model of interpretation.
1 Here, P (X) is the power set of X, containing the set of all subsets.
316
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
3.2 Language Modeling for Speech Recognition
The referential semantic language model described in this article is based on Hierar-
chic Hidden Markov Models (HHMMs), an existing extension of the standard Hidden
Markov Model (HMM) language modeling framework used in speech recognition,
which has been factored to represent hierarchic information about language structure
over time. This section will review HMMs (Section 3.2.1) and Hierarchic HMMs (Sec-
tions 3.2.2 and 3.2.3). This underlying framework will then be extended to include
random variables over semantic referents in Section 4.2.
3.2.1 HMMs and Language Models. The model described in this article is a specialization
of the HMM framework commonly used in speech recognition (Baker 1975; Jelinek,
Bahl, and Mercer 1975). HMMs characterize speech as a sequence of hidden states ht
(which may consist of speech sounds, words, or other hypothesized syntactic or se-
mantic information), and observed states ot (typically finite, overlapping frames of an
audio signal) at corresponding time steps t. A most-probable sequence of hidden states
h?1..T can then be hypothesized given any sequence of observed states o1..T, using Bayes?
Law (Equation 2) and Markov independence assumptions (Equation 3) to define the
full probability P(h1..T | o1..T ) as the product of a Language Model (LM) prior proba-
bility P(h1..T )
def
=
?
t P?LM (ht | ht?1) and an Acoustic Model (AM) likelihood probability
P(o1..T | h1..T )
def
=
?
t P?AM (ot | ht):
h?1..T = argmax
h1..T
P(h1..T | o1..T ) (1)
= argmax
h1..T
P(h1..T ) ? P(o1..T | h1..T ) (2)
def
= argmax
h1..T
T
?
t=1
P?LM (ht | ht?1) ? P?AM (ot | ht) (3)
The initial hidden state h0 may be defined as a constant.
2 HMM transitions can be
modeled using Weighted Finite State Automata (WFSAs), corresponding to regular
expressions. An HMM state ht may then be defined as a WFSA state, or a symbol
position in a corresponding regular expression.
3.2.2 Hierarchic HMMs. Language model transitions P?LM (?t |?t?1) over internally
structured hidden states ?t can be modeled using synchronized levels of stacked-
up component HMMs in an HHMM (Murphy and Paskin 2001), generalized here
as an abstract topology over unspecified random variables ? and ?. In this topol-
ogy, HHMM transition probabilities are calculated in two phases: a ?reduce? phase
(resulting in an intermediate, marginalized state ?t at time step t), in which compo-
nent HMMs may terminate; and a ?shift? phase (resulting in a modeled state ?t),
in which unterminated HMMs transition, and terminated HMMs are re-initialized
from their parent HMMs. Variables over intermediate and modeled states are factored
2 It is also common to define a prior distribution over initial states at h0, but this is not necessary here.
317
Computational Linguistics Volume 35, Number 3
into sequences of depth-specific variables?one for each of D levels in the HHMM
hierarchy:
?t = ??1t . . . ?
D
t ? (4)
?t = ??1t . . . ?
D
t ? (5)
Transition probabilities are then calculated as a product of transition probabilities at
each level, using level-specific ?reduce? ?? and ?shift? ?? models:
P?LM (?t |?t?1) =
?
?t
P(?t |?t?1) ? P(?t |?t ?t?1) (6)
def
=
?
?1t ...?
D
t
(
D
?
d=1
P?? (?
d
t |?
d+1
t ?
d
t?1?
d?1
t?1 )
)
?
(
D
?
d=1
P?? (?
d
t |?
d+1
t ?
d
t ?
d
t?1?
d?1
t )
)
(7)
with ?D+1t and ?
0
t defined as constants. In Viterbi (maximum likelihood) decoding, the
marginals (sums) in this equation may be approximated using an argmax operator. A
graphical representation of the dependencies in this model is shown in Figure 2.
3.2.3 Simple Hierarchic HMMs. The previous generalized definition can be considered a
template for factoring HMMs into synchronized levels, using ? and ? as parameters.
The specific Murphy?Paskin definition of HHMMs can then be considered a ?simple?
instantiation of this template using FSA states for ? and switching variables for ?. In
Section 4, this instantiation will be augmented (or further factored) to incorporate addi-
tional variables over semantic referents at each depth and time step, without changing
the overall topology of the model.
Figure 2
Graphical representation of a HHMMwith D = 3 hidden levels. Circles denote random
variables, and edges denote conditional dependencies. Shaded circles denote variables
with observed values.
318
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
In simple HHMMs, each intermediate state variable ?dt is a boolean switching vari-
able f d?,t ? {0, 1} and each modeled state variable ?
d
t is a syntactic, lexical, or phonetic
FSA state qd?,t:
?dt = f
d
?,t (8)
?dt = q
d
?,t (9)
Instantiating ?? as ?Simple-?, f
d is deterministic: true (equal to 1) with probability 1 if
there is a transition at the level immediately below d and the stack element qd?,t?1 is a
final state, and false (equal to 0) with probability 1 otherwise:3
P?Simple-? (?
d
t |?
d+1
t ?
d
t?1?
d?1
t?1 )
def
=
?
?
?
?
?
if f d+1?,t = 0 : [f
d
?,t= 0]
if f d+1?,t = 1, q
d
?,t?1 ?Final : [f
d
?,t= 0]
if f d+1?,t = 1, q
d
?,t?1?Final : [f
d
?,t= 1]
(10)
where f D+1?,t = 1 and q
0
?,t = ROOT.
Shift probabilities at each level (instantiating ?? as ?Simple-?) are defined using
level-specific transition ?Simple-Trans and expansion ?Simple-Init models:
P?Simple-? (?
d
t |?
d+1
t ?
d
t ?
d
t?1?
d?1
t )
def
=
?
?
?
?
?
if f d+1?,t = 0, f
d
?,t= 0 : [q
d
?,t= q
d
?,t?1]
if f d+1?,t = 1, f
d
?,t= 0 : P?Simple-Trans (q
d
?,t | q
d
?,t?1)
if f d+1?,t = 1, f
d
?,t= 1 : P?Simple-Init (q
d
?,t | q
d?1
?,t )
(11)
where f D+1?,t = 1 and q
0
?,t = ROOT. This model is conditioned on final-state switching
variables at and immediately below the current HHMM level: If there is no final state
immediately below the current level (the first case above), it deterministically copies the
current FSA state forward to the next time step; if there is a final state immediately below
the current level (the second case presented), it transitions the FSA state at the current
level, according to the distribution ?Simple-Trans; and if the state at the current level is
final (the third case presented), it re-initializes this state given the state at the level
above, according to the distribution ?Simple-Init. The overall effect is that higher-level
HMMs are allowed to transition only when lower-level HMMs terminate. An HHMM
therefore behaves like a probabilistic implementation of a pushdown automaton (or
?shift?reduce? parser) with a finite stack, where the maximum stack depth is equal to
the number of levels in the HHMM hierarchy.
Like HMM states, the states at each level in a simple HHMM also correspond to
weighted FSA (WFSA) states or symbol positions in regular expressions, except that
some states can be nonterminal states, which introduce corresponding sub-expressions
or sub-WFSAs governing state transitions at the level below. The process of expanding
each nonterminal state qd?1?,t to a sub-expression or WFSA (with start state q
d
?,t) is
modeled in ?Simple-Init. Transitions to adjacent (possibly final) states within each
expression or WFSA are modeled in ?Simple-Trans.
3 Here [?] is an indicator function: [?] = 1 if ? is true, 0 otherwise.
319
Computational Linguistics Volume 35, Number 3
For example, a simple HHMMmay factor a language model into word (q1?,t), phone
(q2?,t), and subphone (q
3
?,t) levels, where a word state may be a single word, a phone state
may be a position in a sequence of phones corresponding to a word, and a subphone
state may be a position in a sequence of subphone states (e.g., onset, middle, and end)
corresponding to a phone. In this case, ?Simple-Init would define a prior model over
words at level 1, a pronunciation model of phone sequences for each word at level 2,
and a state-sequence model of subphone states for each phone at level 3; and?Simple-Trans
would define a word bigram model at level 1, and would deterministically advance
along phone and subphone sequences at levels 2 and 3 (Bilmes and Bartels 2005).
This hierarchy of regular expressions may also be viewed as a probabilistic im-
plementation of a cascaded FSA, used for modeling syntax in information extraction
systems such as FASTUS (Hobbs et al 1996).
4. A Referential Semantic Language Model
A referential semantic language model can now be defined as an instantiation of an
HHMM (as described in Section 3.2), interpreting directives in a reified world model
(as described in Section 3.1). This interpretation framework is novel in that it is defined
dynamically in terms of transitions over referential states?evocations of entity referents
from a (e.g., first-order) world model?stacked up in a Hierarchic HMM. This allows
(1) a straightforward fast implementation of semantic interpretation (as transition) that
is compatible with conventional time-series models used in speech recognition; and (2)
a broader notion of semantic composition that exploits referential context in time order
(from previous constituents to later constituents) as well as bottom-up (from component
constituents to composed constituents).
First, Section 4.1 will describe a definition of semantic constraints as transitions in
a time-series model. Then Section 4.2 will apply these transitions to nested referents
in a Hierarchic HMM. Section 4.3 will introduce a state-based syntactic representa-
tion to link this semantic representation with recognized words. Finally, Section 4.4
will demonstrate the expressive power of this model on some common linguistic
constructions.
Because this section combines notation from different theoretical frameworks (in
particular, from formal semantics and statistical time-series modeling), a notation
summary is provided in Table 1.
4.1 Dynamic Relations
Semantic interpretationmay be easily integrated into a probabilistic time-series model if
it is formulated as a type of transition, from source to destination referents of equivalent
type at adjacent time steps. In other words, while relations in an ordinary Montagovian
interpretation framework (Montague 1973) may be functions from entity referents to
truth value referents, all relations in the world model defined here must be transition
functions from entity referents to entity referents.
One-place properties l may be modeled in this system by defining transitions from
preceding, unconstrained referents to referents constrained by l. The unconstrained
referents can be thought of as context arguments: For example, in the context of the
set of user-writable files, a property like EXECUTABLE evokes the subset of writable
executables. In the subsumption lattice shown in Figure 1, this will define a rightward
transition from each set referent to some subset referent, labeled with the traversed
relation (see Figure 4 in Section 4.4).
320
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Table 1
Summary of notation used in Section 4.
Model theory (see Section 3.1)
M : a world model
EM : the domain of individuals in world modelM
? : an individual
?M : an interpretation function from logical symbols (e.g., relation labels)
to logical functions over individuals, sets of individuals, etc.
variables with asterisks : refer to an initial world model prior to reification
Type theory (see Section 3.1.1)
E : the type of an individual
T : the type of a truth value
??,?? : the type of a function from type ? to type ? (variables over types)
Set theory (see Section 4.1)
S : a set of individuals
R : a relation over tuples of individuals
Random variables (see Sections 3.2 and 4.2)
h : a hidden variable in a time-series model
o : an observed variable in a time-series model,
(in this case, a frame of the acoustical signal)
? : a complex variable occurring in the reduce phase of processing;
for example, composed of ?e?, f??
? : a complex variable occurring in the shift phase of processing;
for example, composed of ?e?, q??
f : a random variable over final state status; for example, with value 1 or 0
q : a random variable over FSA (syntax) states,
in this case compiled from regular expressions; for example, with value q1 or q2
e : a random variable over referent entities; for example, with value e{?1?2?3}
l : a random variable over relation labels; for example, with value EXECUTABLE
(see Section 4.1)
t : a time step, from 1 to the end of the utterance T
d : a depth level, from 1 to the maximum depth level D
? : a probability model mapping variable values to probabilities
(real numbers form 0.0 to 1.0)
L : functions from FSA (syntax) states to relation labels
variables in boldface : instances or values of a random variable
non-bold variables with single subscripts : are specific to a time step; for example, ?t
non-bold variables with double subscripts : are specific to a reduce or shift phase within
a time step; for example, e?,t, q?,t
non-bold variables with superscripts : are specific to a depth level; for example, ?dt , e
d
?,t
General n-ary semantic relations l in this framework are therefore formulated as a
type of multi-source transition, distinguishing one argument of an original, ordinary
relation l? as an output (destination) and leaving the rest as input (source); then intro-
ducing a context referent as an additional input. Instead of defining simple transition
arcs on a subsumption lattice, n-ary relations more accurately define hyperarcs, with
multiple source referents: zero or more conventional arguments and one additional con-
text referent, leading to a destination referent intersectively constrained to this context.
321
Computational Linguistics Volume 35, Number 3
This model of interpretation as transition also allows referential semantic con-
straints to be applied that occur prior to hypothesized constituents, in addition to those
that occur as arguments. For example, in the sentence go to the package data directory
and hide the executable file, the phrase go to the package data directory provides a powerful
constraint on the referent of the executable file, although it does not occur as an argument
sub-constituent of this noun phrase. In this framework, the referent of the package data
directory (as a set of files) can be passed as a context argument to intersectively constrain
the interpretation of the executable file.
Recall the definition in Section 3.1.2 of a zero-order model M with refer-
ents e{?1,?2,...} reified from sets of individuals {?1, ?2, . . . } in some original first- or
higher-order modelM?. The referential semantic language model described in this arti-
cle interacts with this reified world modelM through queries of the form lM(eS1 , eS2 ),
where l is a relation, eS1 is an argument referent, and eS2 is a context referent (or eS1 is a
context referent if there is no argument). Each query returns a destination referent eS
such that S is a subset of the context set in the original world model M?. These
context-dependent relations l inM are then defined in terms of corresponding ordinary
relations l? of various types in the original world modelM? as follows:
lM(eS1 , eS2 ) = eS s.t.
?
?
?
if l?M? is type ?E, T? : S = S1 ? l?M?
if l?M? is type ?E, ?E, T?? : S = S2 ? (S1 ? l?M? )
if l?M? is type ??E, T?, ?E, T?? : S = S2 ? l?M? (S1)
(12)
where relation products are defined to resemble matrix products:
S ? R = {??? | ???S, ???, ?????R} (13)
For example, a property like EXECUTABLE would ordinarily bemodeled as a functor
of type ?E, T?: given an individual, it would return true if the individual can be executed.
The first case in Equation (12) casts this as a transition from an argument set S1 to
the set of individuals within S1 that are executable. On the other hand, a relation like
CONTAINS would ordinarily be modeled as ?E, ?E, T??: given an individual and then
another individual, it would return true if the relation holds over the pair. The second
case in Equation (12) casts this as a transition from a set of containers S1, given a context
set S2, to the subset of this context that are contained by an individual in S1. Finally, a
first-order functor like LARGEST would ordinarily be modeled as ??E, T?, ?E, T??: given
a set of individuals and then another individual, it would return true if the individual
belongs to the (singleton) set of things that are the largest in the argument set. The last
case in Equation (12) casts this as a transition from a set S1, given a context set S2, to
the (singleton) subset of this context that are members of S1 and are larger than all other
individuals in S1. More detailed examples of each relation type in Equation (12) are
provided in Section 4.4.
Relations in this world model have the character of being context-dependent in the
sense that relations like CAPTAIN that are traditionally one-place (denoting a set of enti-
ties with rank captain) are now two-place, dependent on an argument superconcept in
the subsumption lattice. Relations can therefore be given different meanings at different
places in the world model: in the context of a particular football team, CAPTAIN will
refer to a particular player; in the context of a different team, it will refer to someone
else. One-place relations can still be defined using a subsumption lattice root concept
322
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
?? as a context argument of course, but this will increase the perplexity (number of
choices) at the root concept, making recognition less reliable.
In this definition, referents e are similar to the information states in Dynamic Predi-
cate Logic (Groenendijk and Stokhof 1991), except that only limited working memory
for information states is assumed, containing only one referent (or variable binding in
DPL terms) per HHMM level.
4.2 Referential Semantic HHMM
Like the simple HHMM described in Section 3.2.3, the referential semantic language
model described in this article (henceforth RSLM), is defined by instantiating the gen-
eral HHMM ?template? defined in Section 3.2.2. This RSLM instantiation incorporates
both the switching variables f ?{0, 1} and FSA state variables q of the simple HHMM,
and adds variables over semantic referents e to the ?reduce? and ?shift? phases at each
level. Thus, the RSLM decomposes each HHMM reduce variable ?dt into a joint variable
subsuming an intermediate referent ed?,t and a final-state switching variable f
d
?,t; and
decomposes each HHMM shift variable ?dt into a joint variable subsuming a modeled
referent ed?,t and an ordinary FSA state q
d
?,t:
?dt = ?e
d
?,t, f
d
?,t? (14)
?dt = ?e
d
?,t, q
d
?,t? (15)
A graphical representation of this referential semantic language model is shown in
Figure 3.
The intermediate referents ed?,t in this framework correspond to the traditional notion
of compositional semantics (Frege 1892), in which meanings of composed constituents
(at higher levels in the HHMM hierarchy) are derived from meanings of component
constituents (at lower levels in the hierarchy). However, in addition to the referents
Figure 3
A graphical representation of the dependencies in the referential semantic language model
described in this article (compare with Figure 2). Again, circles denote random variables
and edges denote conditional dependencies. Shaded circles denote random variables with
observed values.
323
Computational Linguistics Volume 35, Number 3
of their component constituents, the intermediate referents in this framework are also
constrained by the referents at the same depth in the previous time step?the referen-
tial context described in Section 4.1. The modeled referents ed?,t in this framework then
correspond to a snapshot at each time step of the referential state of the recognizer,
after all completed constituents have been composed (or reduced), and after any new
constituents have been introduced (or shifted).
Both intermediate and modeled referents are constrained by labeled relations l
in ?M associated with ordinary FSA states. Thus, relation labels are defined for ?re-
duce? and ?shift? HHMM operations via label functions L? and L?, respectively, which
map FSA states q to relation labels l.
Entity referents ed? at each reduce phase of this HHMM are constrained by the pre-
vious FSA state qdt-1 using a reduce relation l
d
?,t = L?(q
d
?,t-1), such that e
d
? = l
d
?M(e
d+1
? , e
d
t-1).
Reduce probabilities at each level (instantiating ?? as ?RSLM-?) are therefore:
4
P?RSLM-? (?
d
t |?
d+1
t ?
d
t-1?
d-1
t-1 )
def
=
?
?
?
?
?
?
?
?
?
if f d+1?,t = 0 : [f
d
?,t= 0] ? [e
d
?,t= e
d
?,t]
if f d+1?,t = 1, q
d
?,t-1 ?Final : [f
d
?,t= 0] ? [e
d
?,t= e
d+1
?,t ]
if f d+1?,t = 1, q
d
?,t-1?Final : [f
d
?,t= 1] ?
[ed?,t= l
d
?,tM(e
d+1
?,t , e
d-1
?,t-1)]
(16)
where ?D+1?,t = ?e
D
?,t-1, 1? and ?
0
?,t = ?e,ROOT?. Here, it is assumed that L?(q
d
?,t-1) pro-
vides a non-trivial constraint only when qd?,t is a final state; otherwise it returns an
IDENTITY relation such that IDENTITYM(e, e
?) = e.
Entity referents ed?,t at each shift phase of this HHMM are constrained by the cur-
rent FSA state qd?,t using a shift relation l
d
?,t = L?(q
d
?,t), such that e
d
?,t = l
d
?,tM(e
d-1
?,t, e).
Shift probabilities at each level (instantiating ?? as ?RSLM-?) then generate relation
labels using a ?description? model ?Ref-Init, with referents e
d
?,t and state transitions q
d
?,t
conditioned on (or deterministically dependent on) these labels. The probability distri-
bution over modeled variables is therefore
P?RSLM-? (?
d
t |?
d+1
t ?
d
t ?
d
t-1?
d-1
t )
def
=
?
?
?
?
?
?
?
?
?
?
?
?
?
if f d+1?,t = 0, f
d
?,t= 0 : [e
d
?,t= e
d
?,t] ? [q
d
?,t= q
d
?,t-1]
if f d+1?,t = 1, f
d
?,t= 0 : [e
d
?,t= e
d
?,t] ? P?Syn-Trans (q
d
?,t | q
d
?,t-1)
if f d+1?,t = 1, f
d
?,t= 1 :
?
ld?,t
P?Ref-Init (l
d
?,t | e
d-1
?,t q
d-1
?,t)
?[ed?,t= l
d
?,tM(e
d-1
?,t, e)]
?P?Syn-Init (q
d
?,t | l
d
?,t q
d-1
?,t)
(17)
where ?D+1?,t = ?e
D
?,t-1, 1? and ?
0
?,t = ?e,ROOT?. Here, it is assumed that L?(q
d
?,t) pro-
vides a non-trivial constraint only when qd?,t is an initial state; otherwise it returns an
IDENTITY relation such that IDENTITYM(e, e
?) = e. The probability models?Ref-Init and
?Syn-Init are induced from corpus observations or defined by hand.
The cases in this equation, conditioned on final-state switching variables f d+1?,t
and f d?,t, correspond to those in Equation (11) in Section 3.2.3. In the first case, where
there is no final state immediately below the current level, referents and FSA states are
simply propagated forward. In the second case, where there is a final state immediately
below the current level, referents are propagated forward and the FSA state is advanced
4 Again, [?] is an indicator function: [?] = 1 if ? is true, 0 otherwise.
324
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
according to the distribution ?Syn-Trans. In the third case, where the current FSA state is
final and must be re-initialized, a new referent and FSA state are chosen by:
1. selecting, according to a ?description? model ?Ref-Init, a relation label l
d
?,t
with which to constrain the current referent,
2. deterministically generating a referent ed?,t given this label and the referent
at the level above, and
3. selecting, according to a ?lexicalization? model ?Syn-Init, an FSA state q
d
?,t
that is compatible with this label (i.e., has L?(q
d
?,t) = l
d
?,t).
4.3 Associating Semantic Relations with Syntactic Expressions
In this framework, semantic referents are constrained over time by instances of seman-
tic relations l? and l?. These relations are determined by instances of syntactic FSA
states q1, . . . ,qn, themselves expanded from higher-level FSA states q. These associa-
tions between syntactic and semantic random variable values can be represented in
expansion rules of the form
q  q1 . . . qn; with l? = L?(q1) and l? = L?(qn) (18)
where q1 . . . qn may be any regular expression initiating at state q1 and culminating at
(final) state qn. Note that regular expressions must therefore begin with shift relations
and end with reduce relations. This is in order to keep the syntactic and referential
semantic expansions synchronized.
These hierarchic regular expressions are defined to resemble expansion rules in
a context free grammar (CFG). However, unlike CFGs, HHMMs have memory limits
on nesting, in the form of a maximum depth D beyond which no expansion may take
place. As a result, the expressive power of an HHMM is restricted to the set of regular
languages, whereas CFGs may recognize the set of context-free languages; and HHMM
recognition is worst-case linear on the length of an utterance, whereas CFG recognition
is cubic.5 Similar limits have been proposed on syntax in natural languages, motivated
by limits on short term memory observed in humans (Miller and Chomsky 1963;
Pulman 1986). These have been applied to obtain memory-limited parsers (e.g., Marcus
1980), and depth-limited right-corner grammars that are equivalent to CFGs, except
that they restrict the number of internally recursive expansions allowed in recognition
(Schuler and Miller 2005).
4.4 Expressivity
The language model described herein defines referential semantics purely in terms of
HHMM shift and reduce operations over referent entities, made from reified sets of
individuals in some original world model. This section will show that this basic model
is sufficiently expressive to represent many commonly occurring linguistic phenomena,
5 When expressed as a function of the size of the grammar, HHMM recognition is asymptotically
exponential on D, whereas CFG recognition is cubic regardless of depth. In practice, however, exact
inference using either formalism is impractical, so approximate inference is used instead (e.g.,
maintaining a beam at each time step or at each constituent span in CFG parsing).
325
Computational Linguistics Volume 35, Number 3
Figure 4
A subsumption lattice (laid on its side, in gray) over the power set of a domain containing three
files: f1 (a writable executable), f2 (a read-only executable), and f3 (a read-only data file).
?Reference paths? made up of conjunctions of relations l (directed arcs, in black) traverse the
lattice from left to right toward the empty set, as referents (e{...}, corresponding to sets of files)
are incrementally constrained by intersection with each lM. (Some arcs are omitted for clarity.)
including intersective modifiers (e.g., adjectives like executable), multi-argument rela-
tions (e.g., prepositional phrases or relative clauses, involving trajector and landmark
referents), negation (as in the adverb not), and comparatives over continuous properties
(e.g., larger).
4.4.1 Properties. Properties (traditionally unary relations like EXECUTABLE or WRITABLE)
can be represented in theworldmodel as labeled edges lt from supersets et?1 to subsets et
defined by intersecting the set et?1 with the set ltM satisfying the property lt. Recall that
a reified world model can be cast as a subsumption lattice as described in Section 3.1.2.
The result of conjoining a property l with a context set e can therefore be found by
downward traversal of an edge in this lattice labeled l and departing from e.6
Thus, in Figure 4, the set of executables that are read-only would be reachable by
traversing a READ-ONLY relation from the set of executables, or by traversing an EX-
ECUTABLE relation from the set of read-only objects, or by a composed path READ-
ONLY?EXECUTABLE or EXECUTABLE?READ-ONLY from e. The resulting set may then
serve as context for subsequent traversals. Property relations may also result in self-
traversals (e.g., DATAFILE?READ-ONLY in Figure 4) or traversals to the empty set e?
(e.g., DATAFILE?WRITABLE). Property relations like EXECUTABLE can be defined using
the dynamic relations in the first case of Equation (12) in Section 4.1, which simply
ignore the non-context argument.
A general template for intersective nouns and modifiers can be expressed as a noun
phrase (NP) expansion using the following regular expression (where l? and l? indicate
relation labels constraining referents at the beginning and end of the NP):
NP ? Det
(
Adj
)?
Noun
(
PP |RC
)?
; with l? = IDENTITY and l? = IDENTITY (19)
6 Although properties (and later, n-ary relations) are defined in terms of an exponentially large
subsumption lattice, this lattice need not be an actual data structure. If the world model is queried from a
decoder trellis with a beam filter rather than from a complete search, only those lattice relations that are
phonologically, syntactically, and semantically most likely (in other words, those that are on this beam)
will be explored.
326
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
in which referents are successively constrained by the semantics of relations associated
with adjective and noun expansions:
Adj ? executable; with l? = EXECUTABLE and l? = IDENTITY (20)
Noun ? executable; with l? = EXECUTABLE and l? = IDENTITY (21)
(and are also constrained by the prepositional phrase (PP) and relative clause (RC)
modifiers, as described below). Here the relation EXECUTABLE traverses from refer-
ent e{f1f2f3} to referent e{f1f2}, a subset of e{f1f2f3} satisfying EXECUTABLE
?
M? .
4.4.2 n-ary Relations. Sequences of properties (traditionally unary relations) can be inter-
preted as simple nonbranching paths from referent to referent in a subsumption lattice,
but higher-arity relations define more complex paths that fork and rejoin. For example,
the referent of the directory containing the executable in Figure 5 would be reachable only
by:
1. storing the original set of directories e{d1d2d3} as a top-level referent in the
HHMM hierarchy, then
2. traversing a CONTAIN relation departing e{d1d2d3} to obtain the contents of
those directories e{f2f3}, then
3. traversing an EXECUTABLE relation departing e{f2f3} to constrain this set to
the set of contents that are also executable: e{f2}, then
4. traversing the inverse CONTAIN? of relation CONTAIN to obtain the
containers of these executables, then constraining the original set of
directories e{d1d2d3} by intersection with this resulting set to yield the
directories containing executables: e{d2}.
This ?forking? of referential semantic paths is handled via syntactic recursion: one path is
explored by the recognizer while the other waits on the HHMM hierarchy (essentially
Figure 5
Reference paths for a relation containing in the directory containing the executable file. A reference
path forks to specify referents using a two-place relation CONTAIN in a domain of directories
d1, d2, d3 and files f1, f2, f3. Here, d2 contains f2 and d3 contains f3, and f1 and f2 are executable. The
ellipsis in the referent set indicates the presence of additional individuals that are not directories.
Again, subsumption is represented in gray and relations are represented in black. (Portions of
the complete subsumption lattice and relation graph are omitted for clarity.)
327
Computational Linguistics Volume 35, Number 3
functioning as a stack). A sample template for branching reduced relative clauses (or
prepositional phrases) that exhibit this forking behavior can be expressed as below:
RC ? containing NP; with l? = CONTAIN and l? = CONTAIN? (22)
where the inverse relation CONTAIN? is applied when the NP expansion concludes or
reduces (when the forked paths are re-joined). Relations like CONTAIN are covered in
the second case of Equation (12) in Section 4.1, which define transitions from sets of
individuals associated with one argument of an original relation CONTAIN? to sets of
individuals associated with the other argument of this relation, in the presence of a
context set, which is a superset of the destination. The calculation of semantic tran-
sition probabilities for n-ary relations thus resembles that for properties, except that
the probability term associated with the relation l? and the inverse relation l? would
depend on both context and argument referents (to its left and below it, in the HHMM
hierarchy).
Note that there is ultimately a singleton referent {f2} of the executable file in Figure 5,
even though there are two executable files in the world model used in these examples.
This illustrates an important advantage of a dynamic context-dependent (three referent)
model of semantic composition over the strict compositional (two referent) model. In a
dynamic context model, the executable file is interpreted in the context of the files that are
contained in a directory. In a strict compositional model, the executable file is interpreted
only in the context of fixed constraints covering the entire utterance, and the constraints
related to the relation containing are applied only to the directories. This means that a
generative model based on strict composition will assign some probability to an infi-
nitely recursive description the directories containing executables contained by directories . . .
In generation systems, this problem has been addressed by adding machinery to keep
track of redundancy (Dale and Haddock 1991). But in this framework, a description
model (?Ref-Init) which is sensitive to the sizes of its source referent and destination ref-
erent at the end of each departing labeled transition will be able to disprefer referential
transitions that attempt to constrain already singleton referents, or that provide only
trivial or vacuous (redundant) constraints in general. This solution is therefore more in
line with graph-basedmodels of generation (Krahmer, van Erk, and Verleg 2003), except
that the graphs proposed here are over reified sets rather than individuals, and the goal
is a generative probability model of language rather than generation per se.
4.4.3 Negation. Negation can be modeled in this framework as a relation between sets.
Although it does not require any syntactic memory, negation does require referential
semantic memory, in that the complement of a specified set must be intersected with
some initial context set. Files that are not writable must still be files after all; only the
writable portion of this description should be negated.
A regular expression for negation of adjectives is
Adj ? not Adj; with l? = IDENTITY and l? = NOT (23)
and is applied to a world model in Figure 6. Relations like NOT are covered in the third
case of Equation (12) in Section 4.1, which define transitions between sets in an original
relation NOT?.
4.4.4 Comparatives, Superlatives, and Subsective Modifiers. Comparatives (e.g., larger),
superlatives (e.g., largest), and subsective modifiers (e.g., large, relative to some context
328
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Figure 6
Reference paths for negation in files that are not writable, using a world model with files f1, f2,
and f3 of which only f1 is writable. The recognizer first forks a copy of the set of files {f1, f2, f3}
using the relation IDENTITY, then applies the adjective relation WRITABLE to yield {f1}. The
complement of this set {f2, f3, . . . } is then intersected with the stored top-level referent
set {f1, f2, f3} to produce the set of files that are not writable: {f2, f3}. Ellipses in referent sets
indicate the presence of additional individuals that are not files.
set) define relations from sets to sets, or from sets to individuals (singleton sets). They
can be handled in much the same way as negation. Here the context is provided from
previous words and from sub-structure, in contrast to DeVault and Stone (2003), which
define the context of a comparative either from fixed inter-utterance constraints or as the
referent of the portion of the noun phrase dominated by the comparative (in addition
to inter-utterance constraints). One advantage of dynamic (time-order) constraints is
that implicit comparatives (in the Clark directory, select the file that is larger, with no
complement) can be modeled with no additional machinery. If substructure context is
not needed, then no additional HHMM storage is necessary.
A regular expression for superlative adjectives is
Noun ? largest Noun; with l? = IDENTITY and l? = LARGEST (24)
and is applied to a world model in Figure 7. Relations like LARGEST are also covered
in the third case of Equation (12), which defines transitions between sets in an original
relation LARGEST?.
5. Evaluation in a Spoken Language Interface
Much of the motivation for this approach has been to develop a human-like model of
language processing. But there are practical advantages to this approach as well. One
of the main practical advantages of the referential semantic language model described
Figure 7
Reference paths for a comparative in the largest executable; this forks a copy of the referent set
{f1, f2, f3} using the relation IDENTITY, applies EXECUTABLE to the forked set to obtain {f1, f2},
and returns the referent {f2}with the largest file size using LARGEST.
329
Computational Linguistics Volume 35, Number 3
in this article is that it may allow spoken language interfaces to be applied to content-
creation domains that are substantially developed by individual users themselves. Such
domains may include scheduling or reminder systems (organizing items containing
idiosyncratic person or event names, added by the user), shopping lists (containing
idiosyncratic brand names, added by the user), interactive design tools (containing new
objects designed and named by the user), or programming interfaces for home or small
business automation (containing new actions, defined by the user). Indeed, computers
are frequently used for content creation as well as content browsing; there is every
reason to expect that spoken language interfaces will be used this way as well.
But the critical problem of applying spoken language interfaces to these kinds of
content-creation domains is that the vocabulary of possible proper names that users
may add or invent is vast. Interface vocabularies in such domains must allow new
words to be created, and once they are created, these new words must be incorpo-
rated into the recognizer immediately, so that they can be used in the current context.
The standard tactic of training language models on example sentences prior to use
is not practical in such domains?except for relatively skeletal abstractions, example
sentences will often not be available. Even very large corpora gleaned from Internet
documents are unlikely to provide reliable statistics for users? made-up names with
contextually appropriate usage, as a referential semantic language model provides.
Content-creation applications such as this may have considerable practical value as
a means of improving accessibility to computers for disabled users. These domains also
provide an ideal proving ground for a referential semantic language model, because
directives in these domains mostly refer to a world model that is shared by the user
and the interfaced application, and because the idiosyncratic language used in such
domains makes it more resistant to domain-independent corpus training than other
domains. In contrast, domains such as database query (e.g., of airline reservations),
dictation, or information extraction are less likely to benefit from a referential semantic
language model, because the world model in such domains is not shared by either the
speaker (in database query) or by the interfaced application (in dictation or information
extraction),7 or because these domains are relatively fixed, so the expense ofmaintaining
linguistic training corpora in these domains can often be justified.
This section will describe an evaluation of an implementation of the referential
semantic language model as a spoken language interface in a very basic content-
creation domain: that of a file organizer, similar to a Unix shell.8 The performance of the
model on this domain will be evaluated in large environments containing thousands
of entities; more than will fit on the beam used in the Viterbi decoding search in this
implementation.
The experiments described in Sections 5.1 through 5.8 were conducted to investigate
the effect on recognition time and accuracy of using a referential semantic language
model to recognize common types of queries, generated by an experimenter and read by
several speakers. A thorough evaluation of the possible coverage of this kind of system
on spontaneous input (e.g., in usability experiments) would require a rich syntactic
representation and attention to disfluencies and speech repairs which are beyond the
scope of this article (see Section 6).
7 Techniques based on abductive reasoning may mitigate this problem of incomplete model sharing
(Hobbs et al 1993), but this would require considerable extensions to the proposed model, and is
beyond the scope of this article.
8 This is also similar to a spoken language version of Wilensky?s Unix consultant (Wilensky, Arens, and
Chin 1984).
330
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
5.1 Ontology Navigation Test Domain
To evaluate the contribution to recognition accuracy of referential semantics over that of
syntax and phonology alone, a baseline (syntax only) and test (baseline plus referential
semantics) recognizer were run on sample ontology manipulation directives in a ?stu-
dent activities? domain. This domain has the form of a simple tree-like taxonomy, with
some cross-listings (for example, studentsmay be listed in homerooms and in activities).
Taxonomic ontologies (e.g., for organizing biological classifications or computer file
directories) can be mapped to reified world models of the sort described in Section 3.1.2.
Concepts C in such an ontology define sets of individuals described by that concept:
{?|C(?)}. Subconcepts C? of a concept C then define subsets of individuals: {?|C?(?)} ?
{?|C(?)}. These sets and subsets can be reified as referent entities and arranged on
a subsumption lattice as described in Section 3.1.2. A sample taxonomic ontology is
shown in Figure 8a (tilted on its side tomatch the subsumption lattices shown elsewhere
in this article). Thus defined, such ontologies can be navigated using referent transitions
described in Section 4.1 by entering concept referents via ?downward? (rightward in the
figure) transitions, and leaving concept referents via ?upward? (leftward) transitions.
For example, this ontology can be manipulated using directives such as:
(1) set Crookston campus homeroom two Clark to sports football captain
which are incrementally interpreted by transitioning down the subsumption lattice (e.g.,
from sports to football to captain) or forking to another part of the lattice (e.g., from Clark
to sports).
As an ontology like this is navigated in spoken language, there is a sense in which
other referents e? at the same level of the ontology as the most recently described refer-
ent e, or at higher levels of the ontology than the most recently described entity, should
be semantically accessible without restating the ontological context (the path from the
root concept e) shared by e
? and e. Thus, in the context of having recently referred
to someone in Homeroom 2 at a particular campus in a school activities database,
other students in the same homeroom or other activities at the same campus should
be accessible without giving an explicit back up directive at each branch in the ontology.
To see the value of implicit upward transitions, compare Example (1) to a directive that
makes upward transitions explicit using the keyword back (similar to ?..? in the syntax of
Unix paths) to exit the homeroom two and Clark folders:
(2) set Crookston campus homeroom two Clark to back back sports football captain
or if starting from the Duluth campus sports football directory:
(3) set back back back Crookston campus homeroom two Clark to back back sports
football captain
Instead of requiring explicit back keywords, these upward transitions can be implic-
itly composed with downward transitions, resulting in transitions from source eS1 to
destination eS via some ancestor eS0 :
UP-lM(eS1 , eS2 ) = eS s.t. ?eS0 S0?S1, S0?S, lM(eS0 , e) = eS (25)
The composed transition function finds a referent eS0 which subsumes both eS1 and eS,
then finds an ordinary (downward) transition l connecting eS0 to eS. The result is a UP-l
transition to every immediate child of an ancestor a referent (or in genealogical terms,
331
Computational Linguistics Volume 35, Number 3
Figure 8
Upward and downward transitions in a sample student activities world model. Downward
transitions (a) define basic sub-type relations. Upward transitions (b) relate sibling, ancestor, and
(great-great-...-)aunt/uncle concepts. The entire model is reachable from any given referent via
these two kinds of transitions.
to every sibling, ancestor, and sibling of ancestor), making these contextually salient
concepts immediately accessible without explicit back-stepping (see Figure 8b).
Downward transitions are ordinary properties, as defined in the first case of
Equation (12) in Section 4.1.
5.2 Scaling to Richer Domains
Although navigation in this domain is constrained to tree-like graphs, this domain tests
all of the features of a referential semantic language model that would be required
332
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
in richer domains. As described in Section 4, rich domains (in particular, first-order
domains, in which users can describe sets of individuals as referents) are mapped to
transition edges on a simple graph, similar to the tree-like graphs used in this ontology.
In first-order domains, the size of this graph may be exponential on the number of
individuals in the world model. But once the number of referents exceeds the size of the
decoder beam, the time performance of the recognizer is constrained not by the number
of entities in the world model, but by the beam width and the number of outgoing
relations (labels) that can be traversed from each hypothesis. In a first-order system, just
as in the simple ontology navigation system evaluated here, this number of relations
is constrained to the set of words defined by the user up to that point. In both cases,
although the interface may be used to describe any one of an arbitrarily large set of
referents, the number of referents that can be evoked at the next time step is bounded by
a constant.
When this model is extended to first-order or continuous domains, the time re-
quired to calculate sets of individuals or hypothetical planner states that result from
a transition may be nontrivial, because it may not be possible in such domains to retain
the entire referent transition model in memory. In first-order domains, for example, this
may require evaluating certain binary relations over all pairs of individuals in the world
model, with time complexity proportional to the square of the size of the world model
domain. Fortunately the model described herein, like most generative languagemodels,
hypothesizes words before recognizing them. This means a recognizer based on this
model will be able to compute transitions thatmight follow a hypothesizedword during
the time that word is being recognized. If just the current set of possible transitions
is known (say, these have already been pre-fetched into a cache), the set of outgoing
transitions that will be required at some time following one of these current transitions
can be requested as soon as the beginning of this transition is hypothesized?as soon
as any word associated with this transition makes its way onto the decoder beam.
From this point, the recognizer will have the entire duration of the word to compute
(in parallel, in a separate thread, or on a separate server) the set of outgoing transitions
that may follow this word. In other words, the model described herein may be scaled to
richer domains because it is amenable to parallelization.
5.3 World Model
The student activities ontology used in this evaluation is a taxonomic world model
definedwith upward and downward transitions as described in Section 5.1. It organizes
extracurricular activities under subcategories (e.g., offense ? football ? sports), and
organizes students into homerooms, in which context they can be identified by a single
(first or last) name. Every student or activity is an entity e in the set of entities E , and
relations l are subcategory labels or student names.
5.3.1 World Model M240. In the original student activities world model M240, a total
of 240 entities were created in E : 158 concepts (groups or positions) and 82 instances
(students), each connected via a labeled arc from a parent concept.
Because a world model in this framework is a weighted set of labeled arcs, it is
possible to calculate a meaningful perplexity statistic for transitions in this model,
assuming all referents are equally likely to be a source. The perplexity of this world
model (the average number of departing arcs) is 16.79, after inserting ?UP? arcs as
described in Section 5.1.
333
Computational Linguistics Volume 35, Number 3
5.3.2 WorldModelM4175.An expanded version of the students ontology,M4175, includes
4,175 entities from 717 concepts and 3,458 instances. This model contains M240 as a
subgraph, so that the same directives may be used in either domain; but it expands
M240 from above, with additional campuses and schools, and below, with additional
students in each class. The perplexity of this world model was 37.77, after inserting
?UP? arcs as described in Section 5.1.
5.4 Test Corpus
A corpus of 144 test sentences (no training sentences) was collected from seven native
English speakers (5male, 2 female), whowere asked tomake specific edits to the student
activities ontology described previously. The subjects were all graduate students and
native speakers of English, from various parts of the United States. The edit directives
were recorded as isolated utterances, not as part of an interactive dialogue, and the
target concepts were identified by name in written prompts, so the corpus has much of
the character of read speech. The average sentence length in this collection is 7.17 words.
5.5 Acoustic Model
Baseline and test versions of this system were run using a Recurrent Neural Network
(RNN) acoustic model (Robinson 1994). This acoustic model performs competitively
with multi-state triphone models based on multivariate Gaussian mixtures, but has the
advantage of using only uniphones with single subphone states. As a result, less of the
HMM trellis beam is occupied with subphone variations, so that a larger number of
semantically distinct hypotheses may be considered at each frame.
Each model was evaluated using parameters trained from the TIMIT corpus of
read speech (Fisher et al 1987). This corpus yields several thousand examples for each
of the relatively small set of single-state uniphones used in the RNN model. Read
speech is also appropriate training data for this evaluation, because the test subjects
are constrained to perform fixed edit tasks given written prompts, and the number of
reasonable ways to perform these tasks is limited by the ontology, so hesitations and
disfluencies are relatively rare.
5.6 Phone and Subphone Models
The language model used in these experiments is decomposed into five hierarchic
levels, each with referent e and ordinary FSA state q components, as described in
Section 4.2. The top three levels of this model represent syntactic states as q (derived
from regular expressions defined in Section 4.3) and associated semantic referents as e.
The bottom two levels represent pronunciation and subphone states as q, and ignore e.
Transitions across pronunciation states are defined in terms of sequences of phones
associated with a word via a pronunciation model. The pronunciation model used in
these experiments is taken from the CMU ARPABET dictionary (Weide 1998). Transi-
tions across subphone states are defined in terms of sequences of subphones associated
with a phone. Because this evaluation used an acoustic model trained on the TIMIT
corpus (Fisher et al 1987), the TIMIT phone set was used as subphones. In most cases,
these subphones map directly to ARPABET phones, so each subphone HMM consists of
a single, final state; but in cases of plosive phones (B, D, G, K, P, and T), the subphone
HMM consists of a stop subphone (e.g., bcl) followed by a burst subphone (e.g., b).
334
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Referents are ignored in both the phone and subphone models, and therefore do not
need to be calculated.
State transitions within the phone level P?Pron-Trans (q
4
?,t | q
4
?,t?1) deterministically ad-
vance along a sequence of phones in a pronunciation; and initial phone sequences de-
pend on words in higher-level syntactic states q3?,t, via a pronunciation model ?Pron-Init:
P?RSLM-? (?
4
t |?
5
t ?
4
t ?
4
t?1 ?
3
t )
def
=
?
?
?
if f 5?,t= 0, f
4
?,t= 0 : [q
4
?,t= q
4
?,t?1]
if f 5?,t= 1, f
4
?,t= 0 : P?Pron-Trans (q
4
?,t | q
4
?,t?1)
if f 5?,t= 1, f
4
?,t= 1 : P?Pron-Init (q
4
?,t | q
3
?,t)
(26)
The student activities domain was developed with no synonymy?only one word de-
scribes each semantic relation. Alternate pronunciations are modeled using a uniform
distribution over all listed pronunciations.
Initialization and transition of subphone sequences depend on the phone at the
current time step and the subphone at the previous time step. This model was trained
directly using relative frequency estimation on the TIMIT corpus itself:
P?RSLM-? (?
5
t |?
6
t ?
5
t ?
5
t?1 ?
4
t )
def
= P?(q5?,t | q
4
?,t q
5
?,t?1) (27)
5.7 Syntax and Reference Models
The three upper levels of the HHMM comprise the syntactic and referential portion of
the language model. Concept error rate tests were performed on three baseline and test
versions of this portion of the language model, using the same acoustic, phone, and
subphone models, as described in Sections 5.5 and 5.6.
5.7.1 Language Model ?LM-Sem. First, the syntactic and referential portion of the language
model was implemented as described in Section 4.2. A subset of the regular expres-
sion grammar appears in Figure 9. Any nondeterminism resulting from disjunction or
Kleene-star repetition in the regular expressions was handled in?Syn-Trans using uniform
distributions over all available following states. Distributions over regular expression
expansions in ?Syn-Init were uniform over all available expansions. Distributions over
labels in?Ref-Init were also uniform over all labels departing the entity referent condition
that were compatible with the FSA state category generated by ?Syn-Init.
Figure 9
Sample grammar for student activities domain. Relations l?, l? = IDENTITY unless otherwise
specified.
335
Computational Linguistics Volume 35, Number 3
5.7.2 Language Model ?LM-NoSem. Second, in order to evaluate the contribution of refer-
ential semantics to recognition, a baseline version of the model was tested with all
relations defined to be equivalent to NIL, returning e at each depth and time step,
with all relation labels reachable in M from e. This has the effect of eliminating all
semantic constraints from the recognizer, while preserving the relation labels of the
original model as a resource from which to calculate concept error rate. The decoding
equations and grammar inModel?LM-NoSem are therefore the same as inModel?LM-Sem;
only the domain of possible referents is restricted.
Again, distributions over state transitions, expansions, and outgoing labels in
?Syn-Trans, ?Syn-Init, and ?Ref-Init are uniform over all available options.
5.7.3 Language Model ?LM-Trigram. Finally, the referential semantic language model (Lan-
guage Model ?LM-Sem) was compiled into a word trigram model, in order to test how
well the model would function as a pre-process to a conventional trigram-based speech
recognizer. This was done by iterating over all possible sequences of hidden state
transitions starting from every possible configuration of referents and FSA states on
a stack of depth D (where D = 3):
ht = ?wt?1,wt? (28)
P(ht | ht?1) = P(wt?1 wt |wt?2 wt?1) = P(wt |wt?2 wt?1) (29)
def
=
?
?t?2..t
?
wt?2,wt?1
P?Uniform (?t?2) ? [wt?2=W(q
1..D
?,t?2)]
?P?LM-Sem (?t?1 |?t?2) ? [wt?1=W(q
1..D
?,t?1)]
?P?LM-Sem (?t |?t?1) ? [wt=W(q
1..D
?,t )]
(30)
First, every valid combination of syntactic categories was calculated in a depth-
first search using ?LM-NoSem. Then every combination of three referents from M240 was
hypothesized as a possible referent configuration. A complete set of possible initial
values for ?t?2 was then filled with combinations from the set of syntactic category
configuration crossed with the set of referent configurations. From each possible ?t?2,
?LM-Sem was consulted to give a distribution over ?t?1 (assuming a word-level transition
occurs, with f 4?,t?1 = 1), and then again from each possible configuration of ?t?1 to give
a distribution over ?t (again assuming a word-level transition). The product of these
transition probabilities was then calculated and added to a trigram count, based on the
words wt?2, wt?1, and wt occurring in ?t?2, ?t?1, and ?t. These trigram counts were then
normalized over wt?2 and wt?1 to give P(wt |wt?2 wt?1).
5.8 Results
The following results report Concept Error Rate (CER), as the sum of the percentages of
insertions, deletions, and substitutions required to transform the most likely sequence
of relation labels hypothesized by the system into the hand-annotated transcript, ex-
pressed as a percentage of the total number of labels in the hand-annotated transcript.
Because there are few semantically unconstrained function words in this domain, this is
essentially word error rate, with a few multi-word labels (e.g., first chair, homeroom two)
concatenated together.
5.8.1 Language Model ?LM-Sem and World Model M240. Results using Language Model
?LM-Sem with the 240-entity world model (M240) show an overall 17.1% CER (Table 2).
336
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Table 2
Per-subject results for Language Model ?LM-Sem withM240.
subject % correct % substitute % delete % insert CER %
0 83.8 14.1 2.1 2.8 19.0
1 73.2 20.3 6.5 5.8 32.7
2 90.2 7.8 2.0 0.7 10.5
3 88.1 9.3 2.7 0.7 12.6
4 88.4 10.3 1.4 3.4 15.1
5 90.8 8.5 0.7 7.0 16.2
6 90.6 8.6 0.7 3.6 12.9
all 86.4 11.3 2.3 3.4 17.1
Table 3
Per-subject results for Language Model ?LM-Sem withM4175.
subject % correct % substitute % delete % insert CER %
0 85.2 14.1 0.7 2.1 16.9
1 70.6 25.5 3.9 7.2 36.6
2 86.9 9.2 3.9 3.9 17.0
3 86.8 11.3 2.0 2.0 15.2
4 83.6 14.4 2.1 6.9 23.3
5 89.4 9.9 0.7 3.5 14.1
6 89.9 9.4 0.7 5.0 15.1
all 84.5 13.5 2.1 4.4 19.9
Here the size of the vocabulary was roughly equal to the number of referents in the
world model. The sentence error rate for this experiment was 59.44%.
5.8.2 Language Model ?LM-Sem and World ModelM4175. With the number of entities (and
words) increased to 4,175 (M4175), the CER increases slightly to 19.9% (Table 3). Here
again, the size of the vocabulary was roughly equal to the number of referents in the
world model. The sentence error rate for this experiment was 62.24%. Here, the use of a
world model (Language Model ?LM-Sem) with no linguistic training data is comparable
to that reported for other large-vocabulary systems (Seneff et al 2004; Lemon and
Gruenstein 2004), which were trained on sample sentences.
5.8.3 LanguageModel?LM-NoSem with noWorldModel. In comparison, a baseline using only
the grammar and vocabulary from the students domainM240 without any world model
information and no linguistic training data (Language Model ?LM-NoSem) scores 43.5%
(Table 4).9 The sentence error rate for this experiment was 93.01%.
Ignoring the world model significantly raises error rates compared to Model
?LM-Sem (p < 0.01 using pairwise t-test against Language model ?LM-Sem with M240,
grouping scores by subject), suggesting that syntactic constraints are poor predictors of
9 Ordinarily a syntactic model would be interpolated with word n-gram probabilities derived from corpus
training, but in the absence of training sentences these statistics cannot be included.
337
Computational Linguistics Volume 35, Number 3
Table 4
Per-subject results for Language Model ?LM-NoSem.
subject % correct % substitute % delete % insert CER %
0 57.0 35.9 7.0 12.7 55.6
1 49.0 41.2 9.8 13.7 64.7
2 71.9 18.3 9.8 6.5 34.6
3 69.5 26.5 4.0 9.3 39.7
4 67.8 28.8 3.4 13.7 45.9
5 79.6 19.0 1.4 7.0 27.5
6 75.5 22.3 2.2 10.8 35.3
all 67.1 27.5 5.5 10.5 43.5
concepts without considering reference. But this is not surprising: because the grammar
by itself does not constrain the set of ontology labels that can be used to construct a
path, the perplexity of this model is 240 (reflecting a uniform distribution over nearly
the entire lexicon), whereas the perplexity ofM240 is only 16.79.
5.8.4 Language Model ?LM-Trigram and World Model M240. In order to test how well the
model would function as a pre-process to a conventional trigram-based speech recog-
nizer, the referential semantic languagemodel (LanguageModel?LM-Sem) was compiled
into a word trigram model. This word trigram language model (Language Model
?LM-Trigram), compiled from the referential semantic model (in the 240-entity domain),
shows a concept error rate of 26.6% on the students experiment (Table 5). The sentence
error rate for this experiment was 66.43%.
Using trigram context (Language Model ?LM-Trigram) similarly shows statistically
significant increases in error over Language Model ?LM-Sem with M240 (p = 0.01 using
pairwise t-test, grouping scores by subject), showing that referential context is also
more predictive than word n-grams derived from referential context. Moreover, the
compilation to trigrams required to build Language Model ?LM-Trigram is expensive
(requiring several hours of pre-processing) because it must consider all combinations
of entities in the world model. This would make the pre-compiled model impractical in
mutable domains.
Table 5
Per-subject results for Language Model ?LM-Trigram withM240.
subject % correct % substitute % delete % insert CER %
0 76.1 19.0 4.9 5.6 29.6
1 56.9 24.8 18.3 12.4 44.4
2 81.7 9.2 9.2 0.0 18.3
3 83.4 13.9 2.7 2.0 18.5
4 79.5 13.0 7.5 11.0 31.5
5 86.6 10.6 2.8 0.7 14.1
6 83.5 14.4 2.2 0.7 17.3
all 78.1 15.0 6.9 4.7 26.6
338
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Table 6
Experimental results with four model configurations.
experiment correct substitute delete insert CER
?LM-Sem,M240 86.4 11.3 2.3 3.4 17.1
?LM-Sem,M4175 84.5 13.5 2.1 4.4 19.9
?LM-NoSem 67.1 27.5 5.5 10.5 43.5
?LM-Trigram,M240 78.1 15.0 6.9 4.7 26.6
5.8.5 Summary of Results. Results in Table 6 summarize the results of the four
experiments.
Some of the erroneously hypothesized directives in this domain described im-
plausible edits: for example, making one student a subset of another student. Domain
information or meta-data could eliminate some of these kinds of errors, but in content-
creation applications it is not always possible to provide this information in advance;
and given the subtle nature of the effect of this information on recognition, it is not clear
that users would want to manage it themselves, or allow it to be automatically induced
without supervision.10 In any case, the comparison described in this section to a non-
semantic model?LM-NoSem suggests that the worldmodel by itself is able to apply useful
constraints in the absence of domain knowledge. This suggests that, in an interpolated
approach, direct world model information may relieve some of the burden on authored
or induced domain knowledge to perform robustly, so that this domain knowledge may
be authored more sparsely or induced more conservatively than it otherwise might.
All evaluations ran in real time on a 4-processor dual-core 2.6GHz server, with a
beam width of 1,000 hypotheses per frame. Differences in runtime performance were
minimal, even between the simple trigrammodel and HHMM-based referential seman-
tic language models. This was due to two factors:
1. All recognizers were run with the same beam width. Although it might
be possible to narrow the beam width to produce faster than real-time
performance for some models, widening the beam beyond 1,000 did not
return significant reductions in CER in the experiments described herein.
2. The implementation of the Viterbi decoder used in these experiments was
optimized to skip combinations of joint variable values that would result
in zero probability transitions (which is a reasonable optimization for any
factored time-series model), significantly decreasing runtime for HHMM
recognition.
5.8.6 Statistical Significance vs. Magnitude of Gain. The experiments described in this
article show a statistically significant increase in accuracy due to the incorporation of
referential semantic information into speech decoding. But these results should not be
interpreted to demonstrate any particular magnitude of error reduction (as might be
claimed for the introduction of head words into parsing models, for example).
10 Ehlen et al (2008) provide an example of a user interface for managing imperfect automatically-induced
information about task assignments from meeting transcripts, which is much more concrete than the kind
of domain knowledge inference considered here.
339
Computational Linguistics Volume 35, Number 3
First, this is because the acoustic model used in these experiments was trained on
a relatively small corpus (6,000 utterances), which introduces the possibility that the
acoustic model was under-trained. As a result, the error rates for both baseline and
test systems may be greater here than if a larger training corpus had been used, so the
performance gain due to the introduction of referential semantics may be overstated.
Second, these experiments were designed with relatively strong referential con-
straints (a tree-like ontology, with a perplexity of about 17 for M240) and relatively
weak syntactic constraints (allowing virtually any sequence of relation labels, with a
much higher perplexity of about 240), in order to highlight differences due to referential
semantics. In general use, recognition accuracy gains due to the incorporation of ref-
erential semantic information will depend crucially on the relative perplexity of the
referential constraints combined with syntactic constraints, compared to that of syntac-
tic constraints alone. This paper has argued that in content-creation applications this
difference can be manipulated and exploited?in fact, by reorganizing folders into a
binary branching tree (with perplexity 2), a user could achieve nearly perfect speech
recognition?but in applications involving fixed ontologies and purely hypothetical
directives, as in database query applications, gains may be minimal or nonexistant.
6. Conclusion and Future Work
This article has described a referential semantic language model that achieves recogni-
tion accuracy favorably comparable to a pre-compiled trigram baseline in user-defined
domains with no available domain-specific training corpora, through the use of ex-
plicit hypothesized semantic referents. This architecture requires that the interfaced
application make available a queryable world model, but the combined phonological,
syntactic, and referential semantic decoding process ensures the world model is only
queriedwhen necessary, allowing accurate real time performance even in large domains
containing several thousand entities.
The framework described in this article is defined over first-order sets (of individu-
als), making transition functions over referents equivalent to expressions in first-order
logic. This framework can be extended to model other kinds of references (e.g., to time
intervals or events) by casting them as individuals (Hobbs 1985).
The system as defined herein also has some ability to recognize referents con-
strained by quantifiers: for example, the directory containing two files. Because its referents
are reified sets, the system can naturally model relations that are sensitive to cardinality
(self-transitioning if the set has N or greater individuals, transitioning to e? otherwise).
But a dynamic view of the referential semantics of nested quantifiers requires referents
to be indexed to particular iterations of quantifiers at higher levels of nesting in the
HHMM hierarchy (corresponding to higher-scoping quantifiers). Extending the system
to dynamically interpret nested quantifiers therefore requires that all semantic opera-
tions preserve an ?iteration context? of nested outer-quantified individuals for each
inner-quantified individual. This is left for future work.
Some analyses of phenomena like intensional or non-inherent adjectives?for ex-
ample, toy in toy guns, which are not actually guns; or old in old friends, who are
not necessarily elderly (Peters and Peters 2000)?involve referents corresponding to
second-order sets (this allows these adjectives to be composed before being applied to
a noun: old but casual friend). Unfortunately, extending the framework described in this
article to use a similarly explicit representation of second- or higher-order sets would
be impractical. Not only would the number of possible second- or higher-order sets
be exponentially larger than the number of possible first-order sets (which is already
340
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
exponential on the number of individuals), but the length of the description of each
referent itself would be exponential on the number of individuals (whereas the list of
individuals describing a first-order referent is merely linear).
The definition of semantic interpretation as a transition function does support
interesting extensions to hypothetical reasoning and planning beyond the standard
closed-world model-theoretic framework, however. Recall the sentence go to the package
data directory and hide the executable files, or equivalently, in the package data directory,
hide the executable files, exemplifying the continuous context-sensitivity of the referential
semantic language model. Here, the system focuses on the contents of this directory
because a sequence of transitions resulting from the combined phonological, syntactic,
and referential semantic context of the sentence led it to this state. One may characterize
the referential semantic transitions leading to this state as a hypothetical sequence of
change directory actions moving the active directory of the interface to this directory (for
the purpose of understanding the consequences of the first part of this directive). The
hypothesized context of this directory is then a world state or planning state resulting
from these actions. Thus characterized, the referential semantic decoder is performing
a kind of statistical plan recognition (Blaylock and Allen 2005). By viewing referents
as world states, or as having world-state components, it would then be possible to use
logical conclusions of other types of actions as implicit constraints?e.g., unpack the tar
file and hide the executable [which will result from this unpacking]?without adding extra
functionality to the recognizer implementation. Similarly, referents for hypothetical
objects like the noun phrase a tar file in the directive create a tar file, are not part of the
world model when the user describes them.
Recognizing references to these hypothetical states and objects requires a capacity
to dynamically generate referents not in the current world model. The domain of
referents in this extended system is therefore unbounded. Fortunately, as mentioned
in Section 5.2, the number of referents that can be generated at each time step is still
bounded by a constant, equal to the recognizer?s beam width multiplied by the num-
ber of traversable relation labels. This means that distributions over outgoing relation
labels are still well-defined for each referential state. The only difference is that, when
modeling hypothetical referents, these distributions must be calculated dynamically.
Finally, this article has primarily focused on connecting an explicit representation
of referential semantics to speech recognition decisions. Ordinarily this is thought of
as being mediated by syntax, which is covered in this article only through a rela-
tively simple framework of bounded recursive HHMM state transitions. However, the
bounded HHMM representation used in this paper has been applied (without seman-
tics) to rich syntactic parsing as well, using a transformed grammar to minimize stack
usage to cases of center-expansion (Schuler et al 2008). Coverage experiments with this
transformed grammar demonstrated that over 97% of the large syntactically annotated
Penn Treebank (Marcus, Santorini, andMarcinkiewicz 1994) could be parsed using only
three elements of stack memory, with four elements giving over 99% coverage. This
suggests that the relatively tight bounds on recursion described in this paper might be
expressively adequate if syntactic states are defined using this kind of transform.
This transform model (again, without semantics) was then further applied to pars-
ing speech repairs, in which speakers repeat or edit mistakes in their directives: for
example, select the red, uh, the blue folder (Miller and Schuler 2008). The resulting system
models incomplete disfluent constituents using transitions associated with ordinary
fluent speech until the repair point (the uh in the example), then processes the speech
repair using only a small number of learned repair reductions. Coverage results for
the same transform model on the Penn Treebank Switchboard Corpus of transcribed
341
Computational Linguistics Volume 35, Number 3
spontaneous speech showed a similar three- to four-element memory requirement. If
this HHMM speech repair model were combined with the HHMM model of referen-
tial semantics described in this article, referents associated with ultimately disfluent
constituents could similarly be recognized using referential transitions associated with
ordinary fluent speech until the repair point, then reduced using a repair rule that
discards the referent. These results suggest that an HHMM-based semantic framework
such as the one described in this article may be psycholinguistically plausible.
Acknowledgments
The authors would like to thank the
anonymous reviewers for their input. This
research was supported by National Science
Foundation CAREER/PECASE award
0447685. The views expressed are not
necessarily endorsed by the sponsors.
References
Aist, Gregory, James Allen, Ellen Campana,
Carlos Gallo, Scott Stoness, Mary Swift,
and Michael Tanenhaus. 2007. Incremental
understanding in human?computer
dialogue and experimental evidence
for advantages over nonincremental
methods. In Proceedings of DECALOG,
pages 149?154, Trento.
Baker, James. 1975. The Dragon system: an
overivew. IEEE Transactions on Acoustics,
Speech and Signal Processing, 23(1):24?29.
Bilmes, Jeff and Chris Bartels. 2005.
Graphical model architectures for speech
recognition. IEEE Signal Processing
Magazine, 22(5):89?100.
Blaylock, Nate and James Allen. 2005.
Recognizing instantiated goals using
statistical methods. In IJCAI Workshop
on Modeling Others from Observations
(MOO-2005), pages 79?86, Edinburgh.
Bos, Johan. 1996. Predicate logic unplugged.
In Proceedings of the 10th Amsterdam
Colloquium, pages 133?143, Amsterdam.
Brachman, Ronald J. and James G. Schmolze.
1985. An overview of the kl-one
knowledge representation system.
Cognitive Science, 9(2):171?216.
Brown-Schmidt, Sarah, Ellen Campana, and
Michael K. Tanenhaus. 2002. Reference
resolution in the wild: Online
circumscription of referential domains in a
natural interactive problem-solving task.
In Proceedings of the 24th Annual Meeting of
the Cognitive Science Society, pages 148?153,
Fairfax, VA.
Church, Alonzo. 1940. A formulation of the
simple theory of types. Journal of Symbolic
Logic, 5(2):56?68.
Dale, Robert and Nicholas Haddock. 1991.
Content determination in the generation of
referring expressions. Computational
Intelligence, 7(4):252?265.
DeVault, David and Matthew Stone. 2003.
Domain inference in incremental
interpretation. In Proceedings of ICoS,
pages 73?87, Nancy.
Ehlen, Patrick, Matthew Purver, John
Niekrasz, Stanley Peters, and Kari Lee.
2008. Meeting adjourned: Off-line
learning interfaces for automatic meeting
understanding. In Proceedings of the
International Conference on Intelligent User
Interfaces, pages 276?284, Canary Islands.
Fisher, William M., Victor Zue, Jared
Bernstein, and David S. Pallet. 1987. An
acoustic?phonetic data base. Journal of the
Acoustical Society of America, 81:S92?S93.
Frege, Gottlob. 1892. Uber sinn und
bedeutung. Zeitschrift fur Philosophie und
Philosophischekritik, 100:25?50.
Gorniak, Peter and Deb Roy. 2004. Grounded
semantic composition for visual scenes.
Journal of Artificial Intelligence Research,
21:429?470.
Groenendijk, Jeroen and Martin Stokhof.
1991. Dynamic predicate logic. Linguistics
and Philosophy, 14:39?100.
Haddock, Nicholas. 1989. Computational
models of incremental semantic
interpretation. Language and Cognitive
Processes, 4:337?368.
Hobbs, Jerry R. 1985. Ontological
promiscuity. In Proceedings of ACL,
pages 61?69, Chicago, IL.
Hobbs, Jerry R., Douglas E. Appelt,
John Bear, David Israel, Megumi
Kameyama, Mark Stickel, and
Mabry Tyson. 1996. Fastus: A cascaded
finite-state transducer for extracting
information from natural-language
text. In Yves Schabes, editor, Finite
State Devices for Natural Language
Processing. MIT Press, Cambridge, MA,
pages 383?406.
Hobbs, Jerry R., Mark Stickel, Douglas E.
Appelt, and Paul Martin. 1993.
Interpretation as abduction. Artificial
Intelligence, 63:69?142.
Jelinek, Frederick, Lalit R. Bahl, and Robert L.
Mercer. 1975. Design of a linguistic
342
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
statistical decoder for the recognition of
continuous speech. IEEE Transactions on
Information Theory, 21:250?256.
Krahmer, Emiel, Sebastiaan van Erk, and
Andre Verleg. 2003. Graph-based
generation of referring expressions.
Computational Linguistics, 29(1):53?72.
Lemon, Oliver and Alexander Gruenstein.
2004. Multithreaded context for
robust conversational interfaces:
Context-sensitive speech recognition and
interpretation of corrective fragments.
ACM Transactions on Computer-Human
Interaction, 11(3):241?267.
Marcus, Mitch. 1980. A Theory of Syntactic
Recognition for Natural Language. MIT
Press, Cambridge, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1994. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Martin, Charles and Christopher Riesbeck.
1986. Uniform parsing and inferencing
for learning. In Proceedings of AAAI,
pages 257?261, Philadelphia, PA.
Mellish, Chris. 1985. Computer Interpretation
of Natural Language Descriptions. Wiley,
New York.
Miller, George and Noam Chomsky. 1963.
Finitary models of language users. In
R. Luce, R. Bush, and E. Galanter, editors,
Handbook of Mathematical Psychology,
volume 2. John Wiley, New York,
pages 419?491.
Miller, Tim and William Schuler. 2008.
A unified syntactic model for parsing
fluent and disfluent speech. In
Proceedings of the 46th Annual Meeting
of the Association for Computational
Linguistics (ACL ?08) pages 105?108,
Columbus, OH.
Montague, Richard. 1973. The proper
treatment of quantification in ordinary
English. In J. Hintikka, J. M. E. Moravcsik,
and P. Suppes, editors, Approaches
to Natural Language. D. Riedel,
Dordrecht, pages 221?242. Reprinted in
R. H. Thomason ed., Formal Philosophy,
Yale University Press, New Haven,
CT, 1994.
Murphy, Kevin P. and Mark A. Paskin. 2001.
Linear time inference in hierarchical
HMMs. In Proceedings of NIPS,
pages 833?840, Vancouver.
Peters, Ivonne and Wim Peters. 2000.
The treatment of adjectives in simple:
Theoretical observations. In Proceedings
of LREC, paper # 366, Athens.
Pulman, Steve. 1986. Grammars, parsers
and memory limitations. Language and
Cognitive Processes, 1(3):197?225.
Robinson, Tony. 1994. An application of
recurrent nets to phone probability
estimation. In IEEE Transactions on
Neural Networks, 5:298?305.
Seneff, Stephanie, Chao Wang, Lee
Hetherington, and Grace Chung. 2004.
A dynamic vocabulary spoken dialogue
interface. In Proceedings of ICSLP,
pages 1457?1460, Jeju Island.
Schuler, William. 2001. Computational
properties of environment-based
disambiguation. In Proceedings of ACL,
pages 466?473, Toulouse.
Schuler, William, Samir AbdelRahman,
Tim Miller, and Lane Schwartz. 2008.
Toward a psycholinguistically-motivated
model of language. In Proceedings of
COLING, pages 785?792, Manchester, UK.
Schuler, William and Tim Miller. 2005.
Integrating denotational meaning into a
DBN language model. In Proceedings
of the 9th European Conference on Speech
Communication and Technology /
6th Interspeech Event (Eurospeech/
Interspeech?05), pages 901?904, Lisbon.
Tanenhaus, Michael K., Michael J.
Spivey-Knowlton, Kathy M. Eberhard,
and Julie E. Sedivy. 1995. Integration of
visual and linguistic information in
spoken language comprehension.
Science, 268:1632?1634.
Tarski, Alfred. 1933. Prace Towarzystwa
Naukowego Warszawskiego, Wydzial III Nauk
Matematyczno-Fizycznych, 34. Translated as
?The concept of truth in formalized
languages?, in J. Corcoran, editor, Logic,
Semantics, Metamathematics: Papers from
1923 to 1938. Hackett Publishing Company,
Indianapolis, IN, 1983, pages 152?278.
Weide, R. L. 1998. Carnegie Mellon
University Pronouncing Dictionary v0.6d.
Available at www.speech.cs.cmu.edu/
cgi-bin/cmudict.
Wilensky, Robert, Yigal Arens, and David
Chin. 1984. Talking to UNIX: An overview
of UC. Communications of the ACM,
27(6):574?593.
Young, S. L., A. G. Hauptmann, W. H. Ward,
E. T. Smith, and P. Werner. 1989. High
level knowledge sources in usable speech
recognition systems. Communications
of the ACM, 32(2):183?194.
343

Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 344?352,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Positive Results for Parsing with a Bounded Stack using a Model-Based
Right-Corner Transform
William Schuler
Dept. of Computer Science and Engineering
Minneapolis, MN
schuler@cs.umn.edu
Abstract
Statistical parsing models have recently been
proposed that employ a bounded stack in time-
series (left-to-right) recognition, using a right-
corner transform defined over training trees to
minimize stack use (Schuler et al, 2008). Cor-
pus results have shown that a vast majority
of naturally-occurring sentences can be parsed
in this way using a very small stack bound
of three to four elements. This suggests that
the standard cubic-time CKY chart-parsing
algorithm, which implicitly assumes an un-
bounded stack, may be wasting probability
mass on trees whose complexity is beyond hu-
man recognition or generation capacity. This
paper first describes a version of the right-
corner transform that is defined over entire
probabilistic grammars (cast as infinite sets
of generable trees), in order to ensure a fair
comparison between bounded-stack and un-
bounded PCFG parsing using a common un-
derlying model; then it presents experimental
results that show a bounded-stack right-corner
parser using a transformed version of a gram-
mar significantly outperforms an unbounded-
stack CKY parser using the original grammar.
1 Introduction
Statistical parsing models have recently been pro-
posed that employ a bounded stack in time-series
(left-to-right) recognition, in order to directly and
tractably incorporate incremental phenomena such
as (co-)reference or disfluency into parsing deci-
sions (Schuler et al, 2008; Miller and Schuler,
2008). These models make use of a right-corner
tree transform, based on the left-corner transform
described by Johnson (1998), and are supported by
corpus results suggesting that most sentences (in En-
glish, at least) can be parsed using a very small
stack bound of three to four elements (Schuler et
al., 2008). This raises an interesting question: if
most sentences can be recognized with only three
or four elements of stack memory, is the standard
cubic-time CKY chart-parsing algorithm, which im-
plicitly assumes an unbounded stack, wasting prob-
ability mass on trees whose complexity is beyond
human recognition or generation capacity?
This paper presents parsing accuracy results us-
ing transformed and untransformed versions of a
corpus-trained probabilistic context-free grammar
suggesting that this is indeed the case. Experimental
results show a bounded-memory time-series parser
using a transformed version of a grammar signifi-
cantly outperforms an unbounded-stack CKY parser
using the original grammar.
Unlike the tree-based transforms described previ-
ously, the model-based transform described in this
paper does not introduce additional context from
corpus data beyond that contained in the origi-
nal probabilistic grammar, making it possible to
present a fair comparison between bounded- and
unbounded-stack versions of the same model. Since
this transform takes a probabilistic grammar as in-
put, it can also easily accommodate horizontal and
vertical Markovisation (annotating grammar sym-
bols with parent and sibling categories) as described
by Collins (1997) and subsequently.
The remainder of this paper is organized as fol-
lows: Section 2 describes related approaches to pars-
ing with stack bounds; Section 3 describes an exist-
ing bounded-stack parsing framework using a right-
corner transform defined over individual trees; Sec-
tion 4 describes a redefinition of this transform to ap-
344
ply to entire probabilistic grammars, cast as infinite
sets of generable trees; and Section 5 describes an
evaluation of this transform on the Wall Street Jour-
nal corpus of the Penn Treebank showing improved
results for a transformed bounded-stack version of a
probabilistic grammar over the original unbounded
grammar.
2 Related Work
The model examined here is formally similar to
Combinatorial Categorial Grammar (CCG) (Steed-
man, 2000). But the CCG account is a competence
model as well as a performance model, in that it
seeks to unify category representations used in pro-
cessing with learned generalizations about argument
structure; whereas the model described in this paper
is exclusively a performance model, allowing gen-
eralizations about lexical argument structures to be
learned in some other representation, then combined
with probabilistic information about parsing strate-
gies to yield a set of derived incomplete constituents.
As a result, the model described in this paper has a
freer hand to satisfy strict working memory bounds,
which may not permit some of the alternative com-
position operations proposed in the CCG account,
thought to be associated with available prosody and
quantifier scope analyses.1
Other models (Abney and Johnson, 1991; Gibson,
1991) seek to explain human processing difficulties
as a result of memory capacity limits in parsing or-
dinary phrase structure trees. The Abney-Johnson
and Gibson models adopt a left-corner parsing strat-
egy, of which the right-corner transform described in
this paper is a variant, in order to minimize memory
usage. But the transform-based model described in
this paper exploits a conception of chunking (Miller,
1956) ? in this case, grouping recognized words
into stacked-up incomplete constituents ? to oper-
ate within much stricter estimates of human short-
term memory bounds (Cowan, 2001) than assumed
by Abney and Johnson.
1The lack of support for some of these available scope anal-
yses may not necessarily be problematic for the present model.
The complexity of interpreting nested raised quantifiers may
place them beyond the capability of human interactive incre-
mental interpretation, but not beyond the capability of post-hoc
interpretation (understood after the listener has had time to think
about it).
Several existing incremental systems are orga-
nized around a left-corner parsing strategy (Roark,
2001; Henderson, 2004). But these systems gen-
erally keep large numbers of constituents open for
modifier attachment in each hypothesis. This al-
lows modifiers to be attached as right children of any
such open constituent. But if any number of open
constituents are allowed, then either the assumption
that stored elements have fixed syntactic (and se-
mantic) structure will be violated, or the assump-
tion that syntax operates within a bounded mem-
ory store will be violated, both of which are psy-
cholinguistically attractive as simplifying assump-
tions. The HHMM model examined in this pa-
per upholds both the fixed-element and bounded-
memory assumptions by hypothesizing fixed reduc-
tions of right child constituents into incomplete par-
ents in the same memory element, to make room for
new constituents that may be introduced at a later
time. These in-element reductions are defined natu-
rally on phrase structure trees as the result of align-
ing right-corner transformed constituent structures
to sequences of random variables in a factored time-
series model.
3 Background
The recognition model examined in this paper is a
factored time-series model, based on a Hierarchic
Hidden Markov Model (Murphy and Paskin, 2001),
which probabilistically estimates the contents of a
memory store of three to four partially-completed
constituents over time. Probabilities for expansions,
transitions and reductions in this model can be de-
fined over trees in a training corpus, transformed
and mapped to the random variables in an HHMM
(Schuler et al, 2008). In Section 4 these probabil-
ities will be computed directly from a probabilistic
context-free grammar, in order to evaluate the con-
tribution of stack bounds without introducing addi-
tional corpus context into the model.
3.1 A Bounded-Stack Model
HHMMs are factored HMMs which mimic a
bounded-memory pushdown automaton (PDA), sup-
porting simple push and pop operations on a
bounded stack-like memory store.
HMMs characterize speech or text as a sequence
345
of hidden states qt (in this case, stacked-up syntac-
tic categories) and observed states ot (in this case,
words) at corresponding time steps t. A most likely
sequence of hidden states q?1..T can then be hypothe-
sized given any sequence of observed states o1..T :
q?1..T = argmax
q1..T
P(q1..T | o1..T ) (1)
= argmax
q1..T
P(q1..T )?P(o1..T | q1..T ) (2)
def= argmax
q1..T
T?
t=1
P?A(qt | qt-1)?P?B(ot | qt) (3)
using Bayes? Law (Equation 2) and Markov in-
dependence assumptions (Equation 3) to define
a full P(q1..T | o1..T ) probability as the product
of a Transition Model (?A) prior probability
P(q1..T ) def= ?t P?A(qt | qt-1) and an Observation
Model (?B) likelihood probability P(o1..T | q1..T ) def=?
t P?B(ot | qt).
Transition probabilities P?A(qt | qt-1) over com-
plex hidden states qt can be modeled using synchro-
nized levels of stacked-up component HMMs in an
HHMM. HHMM transition probabilities are calcu-
lated in two phases: a reduce phase (resulting in an
intermediate, marginalized state ft), in which com-
ponent HMMs may terminate; and a shift phase (re-
sulting in a modeled state qt), in which unterminated
HMMs transition, and terminated HMMs are re-
initialized from their parent HMMs. Variables over
intermediate ft and modeled qt states are factored
into sequences of depth-specific variables ? one for
each of D levels in the HHMM hierarchy:
ft = ?f1t . . . fDt ? (4)
qt = ?q1t . . . qDt ? (5)
Transition probabilities are then calculated as a
product of transition probabilities at each level, us-
ing level-specific reduce ?R,d and shift ?S,d models:
P?A(qt|qt-1) =
?
ft
P(ft|qt-1)?P(qt|ft qt-1) (6)
def=?
f1..Dt
D?
d=1
P?R,d(fdt |fd+1t qdt-1qd-1t-1 )?
P?S,d(qdt |fd+1t fdt qdt-1qd-1t ) (7)
with fD+1t and q0t defined as constants. In Viterbi
decoding, the sums are replaced with argmax opera-
tors. This decoding process preserves ambiguity by
. . .
. . .
. . .
. . .
f3t?1
f2t?1
f1t?1
q1t?1
q2t?1
q3t?1
ot?1
f3t
f2t
f1t
q1t
q2t
q3t
ot
Figure 1: Graphical representation of a Hierarchic Hid-
den Markov Model. Circles denote random variables, and
edges denote conditional dependencies. Shaded circles
are observations.
maintaining competing analyses of the entire mem-
ory store. A graphical representation of an HHMM
with three levels is shown in Figure 1.
Shift and reduce probabilities can then be defined
in terms of finitely recursive Finite State Automata
(FSAs) with probability distributions over transition,
recursive expansion, and final-state status of states at
each hierarchy level. In the version of HHMMs used
in this paper, each intermediate variable is a reduc-
tion or non-reduction state fdt ? G?{1,0} (indi-
cating, respectively, a complete reduced constituent
of some grammatical category from domain G, or
a failure to reduce due to an ?active? transition be-
ing performed, or a failure to reduce due to an
?awaited? transition being performed, as defined in
Section 4.3); and each modeled variable is a syn-
tactic state qdt ? G?G (describing an incomplete
constituent consisting of an active grammatical cat-
egory from domain G and an awaited grammatical
category from domain G). An intermediate vari-
able fdt at depth d may indicate reduction or non-
reduction according to ?F-Rd,d if there is a reduction
at the depth level immediately below d, but must in-
dicate non-reduction (0) with probability 1 if there
was no reduction below:2
P?R,d(fdt | fd+1t qdt-1qd-1t-1 ) def={
if fd+1t 6?G : [fdt =0]
if fd+1t ?G : P?F-Rd,d(fdt | qdt-1, qd-1t-1 ) (8)
2Here [?] is an indicator function: [?] = 1 if ? is true, 0
otherwise.
346
where fD+1t ?G and q0t = ROOT.
Shift probabilities over the modeled variable qdt
at each level are defined using level-specific transi-
tion ?Q-Tr,d and expansion ?Q-Ex,d models:
P?S,d(qdt | fd+1t fdt qdt-1qd-1t ) def=?
?
?
if fd+1t 6?G, fdt 6?G : [qdt = qdt-1]
if fd+1t ?G, fdt 6?G : P?Q-Tr,d(qdt | fd+1t fdt qdt-1qd-1t )
if fd+1t ?G, fdt ?G : P?Q-Ex,d(qdt | qd-1t )
(9)
where fD+1t ?G and q0t = ROOT. This model is
conditioned on reduce variables at and immediately
below the current FSA level. If there is no reduc-
tion immediately below the current level (the first
case above), it deterministically copies the current
FSA state forward to the next time step. If there
is a reduction immediately below the current level
but no reduction at the current level (the second case
above), it transitions the FSA state at the current
level, according to the distribution ?Q-Tr,d. And if
there is a reduction at the current level (the third case
above), it re-initializes this state given the state at the
level above, according to the distribution ?Q-Ex,d.
The overall effect is that higher-level FSAs are al-
lowed to transition only when lower-level FSAs ter-
minate. An HHMM therefore behaves like a prob-
abilistic implementation of a pushdown automaton
(or shift?reduce parser) with a finite stack, where the
maximum stack depth is equal to the number of lev-
els in the HHMM hierarchy.
3.2 Tree-Based Transforms
The right-corner transform used in this paper is sim-
ply the left-right dual of a left-corner transform
(Johnson, 1998). It transforms all right branching
sequences in a phrase structure tree into left branch-
ing sequences of symbols of the form A?/A???, de-
noting an incomplete instance of an ?active? category
A? lacking an instance of an ?awaited? category A???
yet to come.3 These incomplete constituent cate-
gories have the same form and much of the same
meaning as non-constituent categories in a Combi-
natorial Categorial Grammar (Steedman, 2000).
3Here ? and ? are node addresses in a binary-branching tree,
defined as paths of left (0) or right (1) branches from the root.
Rewrite rules for the right-corner transform are
shown below:4
? Beginning case: the top of a right-expanding
sequence in an ordinary phrase structure tree is
mapped to the bottom of a left-expanding se-
quence in a right-corner transformed tree:
A?
A??0
?
A??1
?
?
A?
A?/A??1
A??0
?
?
(10)
This case of the right-corner transform may be
considered a constrained version of CCG type
raising.
? Middle case: each subsequent branch in a
right-expanding sequence of an ordinary phrase
structure tree is mapped to a branch in a left-
expanding sequence of the transformed tree:
A?
? A???
A????0
?
A????1
?
?
A?
A?/A????1
A?/A???
?
A????0
?
?
(11)
This case of the right-corner transform may be
considered a constrained version of CCG for-
ward function composition.
? Ending case: the bottom of a right-expanding
sequence in an ordinary phrase structure tree is
mapped to the top of a left-expanding sequence
in a right-corner transformed tree:
A?
? A???
a???
?
A?
A?/A???
?
A???
a???
(12)
This case of the right-corner transform may be
considered a constrained version of CCG for-
ward function application.
4These rules can be applied recursively from bottom up
on a source tree, synchronously associating subtree structures
matched to variables ?, ?, and ? on the left side of each rule
with transformed representations of these subtree structures on
the right.
347
a) binary-branching phrase structure tree:
S
NP
NP
JJ
strong
NN
demand
PP
IN
for
NP
NPpos
NNP
NNP
new
NNP
NNP
york
NNP
city
POS
?s
NNS
JJ
general
NNS
NN
obligation
NNS
bonds
VP
VBN
VBN
propped
PRT
up
NP
DT
the
NN
JJ
municipal
NN
market
b) result of right-corner transform:
S
S/NN
S/NN
S/NP
S/VP
NP
NP/NNS
NP/NNS
NP/NNS
NP/NP
NP/PP
NP
NP/NN
JJ
strong
NN
demand
IN
for
NPpos
NPpos/POS
NNP
NNP/NNP
NNP/NNP
NNP
new
NNP
york
NNP
city
POS
?s
JJ
general
NN
obligation
NNS
bonds
VBN
VBN/PRT
VBN
propped
PRT
up
DT
the
JJ
municipal
NN
market
Figure 2: Trees resulting from a) a sample phrase structure tree for the sentence Strong demand for New York City?s
general obligations bonds propped up the municipal market, and b) a right-corner transform of this tree. Sequences of
left children are recognized from the bottom up through in-element transitions in a Hierarchic Hidden Markov Model.
Right children are recognized by expanding to additional stack elements.
The completeness of the above transform rules can
be demonstrated by the fact that they cover all pos-
sible subtree configurations (with the exception of
bare terminals, which are simply copied). The
soundness of the above transform rules can be
demonstrated by the fact that each rule transforms
a right-branching subtree into a left-branching sub-
tree labeled with an incomplete constituent.
An example of a right-corner transformed tree is
shown in Figure 2(b). An important property of this
transform is that it is reversible. Rewrite rules for re-
versing a right-corner transform are simply the con-
verse of those shown above.
Sequences of left children in the resulting mostly-
left-branching trees are recognized from the bot-
tom up, through transitions at the same stack ele-
ment. Right children, which are much less frequent
in the resulting trees, are recognized through cross-
element expansions in a bounded-stack recognizer.
4 Model-Based Transforms
In order to compare bounded- and unbounded-stack
versions of the same model, the formulation of
the right-corner and bounded-stack transforms in-
troduced in this paper does not map trees to trees,
but rather maps probability models to probability
348
models. This eliminates complications in comparing
models with different numbers of dependent vari-
ables ? and thus different numbers of free parame-
ters ? because the model which ordinarily has more
free parameters (the HHMM, in this case) is derived
from the model that has fewer (the PCFG). Since
they are derived from a simpler underlying model,
the additional parameters of the HHMM are not free.
Mapping probability models from one format to
another can be thought of as mapping the infinite
sets of trees that are defined by these models from
one format to another. Probabilities in the trans-
formed model are therefore defined by calculating
probabilities for the relevant substructures in the
source model, then marginalizing out the values of
nodes in these structures that do not appear in the
desired expression in the target model.
A bounded-stack HHMM ?Q,F can therefore be
derived from an unbounded PCFG ?G by:
1. organizing the rules in the source PCFG
model ?G into direction-specific versions (dis-
tinguishing rules for expanding left and right
children, which occur respectively as active and
awaited constituent categories in incomplete
constituent labels);
2. enforcing depth limits on these direction-
specific rules; and
3. mapping these probabilities to HHMM random
variable positions at the appropriate depth.
4.1 Direction-specific rules
An inspection of the tree-based right-corner trans-
form rewrites defined in Section 3.2 will show two
things: first, that constituents occurring as left chil-
dren in an original tree (with addresses ending in
?0?) always become active constituents (occurring
before the slash, or without a slash) in incomplete
constituent categories, and constituents occurring as
right children in an original tree (with addresses end-
ing in ?1?) always become awaited constituents (oc-
curring after the slash); and second, that left chil-
dren expand locally downward in the transformed
tree (so each A??0/... locally dominates A??0?0/...),
whereas right children expand locally upward (so
each .../A??1 is locally dominated by .../A??1?1).
This means that rules from the original grammar ?
if distinguished into rules applying only to left and
right children (active and awaited constituents) ?
can still be locally modeled following a right-corner
transform. A transformed tree can be generated
in this way by expanding downward along the ac-
tive constituents in a transformed tree, then turning
around and expanding upward to fill in the awaited
constituents, then turning around again to generate
the active constituents at the next depth level, and so
on.
4.2 Depth bounds
The locality of the original grammar rules in a right-
corner transformed tree allows memory limits on in-
complete constituents to be applied directly as depth
bounds in the zig-zag generation traversal defined
above. These depth limits correspond directly to the
depth levels in an HHMM.
In the experiments described in Section 5,
direction-specific and depth-specific versions of the
original grammar rules are implemented in an ordi-
nary CKY-style dynamic-programming parser, and
can therefore simply be cut off at a particular depth
level with no renormalization.
But in an HHMM, this will result in label-bias ef-
fects, in which expanded constituents may have no
valid reduction, forcing the system to define distri-
butions for composing constituents that are not com-
patible. For example, if a constituent is expanded at
depth D, and that constituent has no expansions that
can be completely processed within depth D, it will
not be able to reduce, and will remain incompatible
with the incomplete constituent above it. Probabili-
ties for depth-bounded rules must therefore be renor-
malized to the domain of allowable trees that can be
generated within D depth levels, in order to guaran-
tee consistent probabilities for HHMM recognition.
This is done by determining the (depth- and
direction-specific) probability P?B-L,d(1 |A??0)
or P?B-R,d(1 |A??1) that a tree generated at each
depth d and rooted by a left or right child will fit
within depth D. These probabilities are then esti-
mated using an approximate inference algorithm,
similar to that used in value iteration (Bellman,
1957), which estimates probabilities of infinite trees
by exploiting the fact that increasingly longer trees
contribute exponentially decreasing probability
mass (since each non-terminal expansion must
349
avoid generating a terminal with some probability
at each step from the top down), so a sum over
probabilities of trees with increasing length k is
guaranteed to converge. The algorithm calculates
probabilities of trees with increasing length k until
convergence, or to some arbitrary limit K:
P?B-L,d,k(1 |A??0) def=?
A??1?0,
A??1?1
P?G(A??0  A??0?0 A??0?1)
? P?B-L,d,k?1(1 |A??0?0)
? P?B-R,d,k?1(1 |A??0?1) (13)
P?B-R,d,k(1 |A??1) def=?
A??1?0,
A??1?1
P?G(A??1  A??1?0 A??1?1)
? P?B-L,d+1,k?1(1 |A??1?0)
? P?B-R,d,k?1(1 |A??1?1) (14)
Normalized probability distributions for depth-
bounded expansions ?G-L,d and ?G-R,d can now be
calculated using converged ?B-L,d and ?B-R,d esti-
mates:
P?G-L,d(A??0  A??0?0 A??0?1) def=
P?G(A??0  A??0?0 A??0?1)
? P?B-L,d(1 |A??0?0) ? P?B-R,d(1 |A??0?1) (15)
P?G-R,d(A??1  A??1?0 A??1?1) def=
P?G(A??1  A??1?0 A??1?1)
? P?B-L,d+1(1 |A??1?0) ? P?B-R,d(1 |A??1?1) (16)
4.3 HHMM probabilities
Converting PCFGs to HHMMs requires the calcu-
lation of expected frequencies F?G-L*,d(A? ? A???)
of generating symbols A??? in the left-progeny of a
nonterminal symbol A? (in other words, of A??? be-
ing a left child of A?, or a left child of a left child
of A?, etc.). This is done by summing over sub-
trees of increasing length k using the same approx-
imate inference technique described in Section 4.2,
which guarantees convergence since each subtree of
increasing length contributes exponentially decreas-
ing probability mass to the sum:
F?G-L*,d(A? ? A???) =
??
k=0
F?G-L*,d(A? k A???)
(17)
where:
F?G-L*,d(A? k A??0k) =?
A??0k?1 ,
A??0k?1?1
P?G-L*,d(A? k?1 A??0k?1)
? P?G-L,d(A??0k?1  A??0k A??0k?1?1) (18)
and P?G-L*,d(A? 0 A??) = [A? =A??].
A complete HHMM can now be defined us-
ing depth-bounded right-corner PCFG probabilities.
HHMM probabilities will be defined over syntac-
tic states consisting of incomplete constituent cat-
egories A?/A???.
Expansions depend on only the incomplete con-
stituent category ../A? (for any active category ?..?)
at qd?1t :
P?Q-Ex,d(a??0?? | ../A?) =?
A??0,
A??1
P?G-R,d?1(A?  A??0 A??1)?
F?G-L*,d(A??0 ? a??0??)?
A??0,
A??1,a??0??
P?G-R,d?1(A?  A??0 A??1)?
F?G-L*,d(A??0 ? a??0??)
(19)
Transitions depend on whether an ?active? or
?awaited? transition was performed at the current
level. If an active transition was performed (where
fdt = 1), the transition depends on only the in-
complete constituent category A??0???0/.. (for any
awaited category ?..?) at qdt?1, and the incomplete
constituent category ../A? (for any active category
?..?) at qd?1t?1 :
P?Q-Tr,d(A??0??/A??0???1 |1, A??0???0/.., ../A?) =?
A??0,
A??1
P?G-R,d?1(A?  A??0 A??1)?
F?G-L*,d (A??0
?A??0??)
F?G-L*,d (A?0
?A?0?0)?F?G-L*,d (A?0
0A?0?0)
?
P?G-L,d(A??0??  A??0???0 A??0???1)?
A??0,
A??1,
A??0??,
A??0???1
P?G-R,d?1(A?  A??0 A??1)?
F?G-L*,d (A??0
?A??0??)
F?G-L*,d (A?0
?A?0?0)?F?G-L*,d (A?0
0A?0?0)
?
P?G-L,d(A??0??  A??0???0 A??0???1)
(20)
If an awaited transition was performed (where fdt =
0), the transition depends on only the complete con-
stituent category A????0 at fd+1t , and the incomplete
350
constituent category A?/A??? at qdt?1:
P?Q-Tr,d(A?/A????1 |0, A????0, A?/A???) =
P?G-R,d(A???  A????0 A????1)?
A????1 P?G-R,d(A???  A????0 A????1)
(21)
Reduce probabilities depend on the complete con-
stituent category at fd+1t , and the incomplete con-
stituent category A??0???0/.. (for any awaited cate-
gory ?..?) at qdt?1, and the incomplete constituent cat-
egory ../A? (for any active category ?..?) at qd?1t?1 . If
the complete constituent category at fd+1t does not
match the awaited category of qdt?1, the probability
is [fdt = f0]. If the complete constituent category
at fd+1t does match the awaited category of qdt?1:
P?F-Rd,d(1 |A??0??/.., ../A?) =?
A??0,A??1 P?G-R,d?1(A?  A??0 A??1)?(
F?G-L*,d(A??0 ? A??0??)
?F?G-L*,d(A??0 0 A??0??)
)
?
A??0,A??1 P?G-R,d?1(A?  A??0 A??1)?
F?G-L*,d(A??0 ? A??0??)
(22)
and:
P?F-Rd,d(A??0?? |A??0??/.., ../A?) =?
A??0,A??1 P?G-R,d?1(A?  A??0 A??1)?
F?G-L*,d(A??0 0 A??0??)?
A??0,A??1 P?G-R,d?1(A?  A??0 A??1)?
F?G-L*,d(A??0 ? A??0??)
(23)
The correctness of the above distributions can be
demonstrated by the fact that all terms other than
?G-L,d and ?G-R,d probabilities will cancel out in
any sequence of transitions between an expansion
and a reduction, leaving only those terms that would
appear as factors in an ordinary PCFG parse.5
5 Results
A PCFG model was extracted from sections 2?21
of the Wall Street Journal Treebank. In order to
keep the transform process manageable, punctua-
tion was removed from the corpus, and rules oc-
curring less frequently than 10 times in the corpus
5It is important to note, however, that these probabilities are
not necessarily incrementally balanced, so this correctness only
applies to parsing with an infinite beam.
model (sect 22?24, len>40) F
unbounded PCFG 66.03
bounded PCFG (D=4) 66.08
Table 1: Results of CKY parsing using bounded and un-
bounded PCFG.
were deleted from the PCFG. The right-corner and
bounded-stack transforms described in the previous
section were then applied to the PCFG. The origi-
nal and bounded PCFG models were evaluated in a
CKY recognizer on sections 22?24 of the Treebank,
with results shown in Table 1.6 Results were signif-
icant only for sentences longer than 40 words. On
these sentences, the bounded PCFG model achieves
about a .15% reduction of error over the original
PCFG (p < .1 using one-tailed pairwise t-test). This
suggests that on long sentences the probability mass
wasted due to parsing with an unbounded stack is
substantial enough to impact parsing accuracy.
6 Conclusion
Previous work has explored bounded-stack parsing
using a right-corner transform defined on trees to
minimize stack usage. HHMM parsers trained on
applications of this tree-based transform of train-
ing corpora have shown improvements over ordinary
PCFG models, but this may have been attributable to
the richer dependencies of the HHMM.
This paper has presented an approximate in-
ference algorithm for transforming entire PCFGs,
rather than individual trees, into equivalent right-
corner bounded-stack HHMMs. Moreover, a com-
parison with an untransformed PCFG model sug-
gests that the probability mass wasted due to pars-
ing with an unbounded stack is substantial enough
to impact parsing accuracy.
Acknowledgments
This research was supported by NSF CAREER
award 0447685 and by NASA under award
NNX08AC36A. The views expressed are not nec-
essarily endorsed by the sponsors.
6A CKY recognizer was used in both cases in order to avoid
introducing errors due to model approximation or beam limits
necessary for incremental processing with large grammars.
351
References
Steven P. Abney and Mark Johnson. 1991. Memory re-
quirements and local ambiguities of parsing strategies.
J. Psycholinguistic Research, 20(3):233?250.
Richard Bellman. 1957. Dynamic Programming.
Princeton University Press, Princeton, NJ.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics (ACL ?97).
Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage ca-
pacity. Behavioral and Brain Sciences, 24:87?185.
Edward Gibson. 1991. A computational theory of hu-
man linguistic processing: Memory limitations and
processing breakdown. Ph.D. thesis, Carnegie Mellon.
James Henderson. 2004. Lookahead in deterministic
left-corner parsing. In Proc. Workshop on Incremen-
tal Parsing: Bringing Engineering and Cognition To-
gether, Barcelona, Spain.
Mark Johnson. 1998. Finite state approximation of
constraint-based grammars using left-corner grammar
transforms. In Proceedings of COLING/ACL, pages
619?623.
Tim Miller and William Schuler. 2008. A syntactic time-
series model for parsing fluent and disfluent speech. In
Proceedings of the 22nd International Conference on
Computational Linguistics (COLING?08).
George A. Miller. 1956. The magical number seven, plus
or minus two: Some limits on our capacity for process-
ing information. Psychological Review, 63:81?97.
Kevin P. Murphy and Mark A. Paskin. 2001. Linear time
inference in hierarchical HMMs. In Proc. NIPS, pages
833?840.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2008. Toward a psycholinguistically-
motivated model of language. In Proceedings of COL-
ING, Manchester, UK, August.
Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
352
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 105?108,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Unified Syntactic Model for Parsing Fluent and Disfluent Speech?
Tim Miller
University of Minnesota
tmill@cs.umn.edu
William Schuler
University of Minnesota
schuler@cs.umn.edu
Abstract
This paper describes a syntactic representation
for modeling speech repairs. This representa-
tion makes use of a right corner transform of
syntax trees to produce a tree representation
in which speech repairs require very few spe-
cial syntax rules, making better use of training
data. PCFGs trained on syntax trees using this
model achieve high accuracy on the standard
Switchboard parsing task.
1 Introduction
Speech repairs occur when a speaker makes a mis-
take and decides to partially retrace an utterance in
order to correct it. Speech repairs are common in
spontaneous speech ? one study found 30% of dia-
logue turns contained repairs (Carletta et al, 1993)
and another study found one repair every 4.8 sec-
onds (Blackmer and Mitton, 1991). Because of the
relatively high frequency of this phenomenon, spon-
taneous speech recognition systems will need to be
able to deal with repairs to achieve high levels of
accuracy.
The speech repair terminology used here follows
that of Shriberg (1994). A speech repair consists of
a reparandum, an interruption point, and the alter-
ation. The reparandum contains the words that the
speaker means to replace, including both words that
are in error and words that will be retraced. The in-
terruption point is the point in time where the stream
of speech is actually stopped, and the repairing of
the mistake can begin. The alteration contains the
?This research was supported by NSF CAREER award
0447685. The views expressed are not necessarily endorsed by
the sponsors.
words that are meant to replace the words in the
reparandum.
Recent advances in recognizing spontaneous
speech with repairs (Hale et al, 2006; Johnson and
Charniak, 2004) have used parsing approaches on
transcribed speech to account for the structure in-
herent in speech repairs at the word level and above.
One salient aspect of structure is the fact that there
is often a good deal of overlap in words between
the reparandum and the alteration, as speakers may
trace back several words when restarting after an er-
ror. For instance, in the repair . . . a flight to Boston,
uh, I mean, to Denver on Friday . . . , there is an exact
match of the word ?to? between reparandum and re-
pair, and a part of speech match between the words
?Boston? and ?Denver?.
Another sort of structure in repair is what Lev-
elt (1983) called the well-formedness rule. This
rule states that the constituent started in the reparan-
dum and repair are ultimately of syntactic types that
could be grammatically joined by a conjunction. For
example, in the repair above, the well-formedness
rule says that the repair is well formed if the frag-
ment . . . a flight to Boston and to Denver. . . is gram-
matical. In this case the repair is well formed since
the conjunction is grammatical, if not meaningful.
The approach described here makes use of a trans-
form on a tree-annotated corpus to build a syntactic
model of speech repair which takes advantage of the
structure of speech repairs as described above, while
also providing a representation of repair structure
that more closely adheres to intuitions about what
happens when speakers make repairs.
105
2 Speech repair representation
The representational scheme used for this work
makes use of a right-corner transform, a way of
rewriting syntax trees that turns all right recursion
into left recursion, and leaves left recursion as is.
As a result, constituent structure is built up dur-
ing recognition in a left-to-right fashion, as words
are read in. This arrangement is well-suited to
recognition of speech with repairs, because it al-
lows for constituent structure to be built up using
fluent speech rules up until the moment of interrup-
tion, at which point a special repair rule may be ap-
plied. This property will be examined further in sec-
tion 2.3, following a technical description of the rep-
resentation scheme.
2.1 Binary branching structure
In order to obtain a linguistically plausible right-
corner transform representation of incomplete con-
stituents, the Switchboard corpus is subjected to a
pre-process transform to introduce binary-branching
nonterminal projections, and fold empty categories
into nonterminal symbols in a manner similar to that
proposed by Johnson (1998b) and Klein and Man-
ning (2003). This binarization is done in in such
a way as to preserve linguistic intuitions of head
projection, so that the depth requirements of right-
corner transformed trees will be reasonable approx-
imations to the working memory requirements of a
human reader or listener.
Trees containing speech repairs are reduced in ar-
ity by merging repair structure lower in the tree,
when possible. As seen in the left tree below, 1 re-
pair structure is annotated in a flat manner, which
can lead to high-arity rules which are sparsely repre-
sented in the data set, and thus difficult to learn. This
problem can be mitigated by using the rewrite rule
shown below, which turns an EDITED-X constituent
into the leftmost child of a tree of type X, as long as
the original flat tree had X following an EDITED-
X constituent and possibly some editing term (ET)
categories. The INTJ category (?uh?,?um?,etc.) and
the PRN category (?I mean?, ?that is?, etc.) are con-
sidered to be editing term categories when they lie
1Here, all Ai denote nonterminal symbols, and all ?i denote
subtrees; the notation A1:?1 indicates a subtree ?1 with label
A1; and all rewrites are applied recursively, from leaves to root.
between EDITED-X and X constituents.
A0
EDITED
A1:?1
ET* A1:?2 ?3 ?
A0
A1
EDITED-A1
A1:?1
ET* A1:?2
?3
2.2 Right-corner transform
Binarized trees2 are then transformed into right-
corner trees using transform rules similar to those
described by Johnson(1998a). This right-corner
transform is simply the left-right dual of a left-
corner transform. It transforms all right recursive
sequences in each tree into left recursive sequences
of symbols of the form A1/A2, denoting an incom-
plete instance of category A1 lacking an instance of
category A2 to the right.
Rewrite rules for the right-corner transform are
shown below:
A1
?1 A2
?2 A3:?3
?
A1
A1/A2
?1
A2/A3
?2
A3:?3
A1
A1/A2:?1 A2/A3
?2
?3 . . . ?
A1
A1/A3
A1/A2:?1 ?2
?3 . . .
Here, the first rewrite rule is applied iteratively
(bottom-up on the tree) to flatten all right recursion,
using incomplete constituents to record the original
nonterminal ordering. The second rule is then ap-
plied to generate left recursive structure, preserving
this ordering.
The incomplete constituent categories created by
the right corner transform are similar in form and
meaning to non-constituent categories used in Com-
binatorial Categorial Grammars (CCGs) (Steedman,
2000). Unlike CCGs, however, a right corner trans-
formed grammar does not allow backward function
application, composition, or raising. As a result, it
does not introduce spurious ambiguity between for-
ward and backward operations, but cannot be taken
to explicitly encode argument structure, as CCGs
can.
2All super-binary branches remaining after the above pre-
process are ?nominally? decomposed into right-branching struc-
tures by introducing intermediate nodes with labels concate-
nated from the labels of its children, delimited by underscores
106
EDITED [-NP]
NP [-UNF]
NP
DT
the
JJ
first
NN
kind
PP [-UNF]
IN
of
NP [-UNF]
NN
invasion
PP-UNF
IN
of
Figure 1: Standard tree repair structure, with -UNF prop-
agation as in (Hale et al, 2006) shown in brackets.
EDITED-NP
NP/PP
NP/NP
NP/PP
NP
NP/NN
NP/NN
DT
the
JJ
first
NN
kind
IN
of
NP
invasion
PP-UNF
of
Figure 2: Right-corner transformed tree with repair struc-
ture
2.3 Application to speech repair
An example speech repair from the Switchboard cor-
pus can be seen in Figures 1 and 2, in which the same
repair fragment is shown in a standard state such as
might be used to train a probabilistic context free
grammar, and after the right-corner transform. Fig-
ure 1 also shows, in brackets, the augmented anno-
tation used by Hale et al(2006). This scheme con-
sisted of adding -X to an EDITED label which pro-
duced a category X, as well as propagating the -UNF
label at the right corner of the tree up through every
parent below the EDITED root.
The standard annotation (without -UNF propaga-
tion) is deficient because even if an unfinished con-
stituent like PP-UNF is correctly recognized, and the
speaker is essentially in an error state, there may be
several partially completed constituents above ? in
Figure 1, the NP, PP, and NP above the PP-UNF.
These constituents need to be completed, but using
the standard annotation there is only one chance to
make use of the information about the error that has
occurred ? the NP ? NP PP-UNF rule. Thus, by the
time the error section is completed, there is no infor-
mation by which a parsing algorithm could choose
to reduce the topmost NP to EDITED other than in-
dependent rule probabilities.
The approach used by (Hale et al, 2006) works
because the information about the transition to an er-
ror state is propagated up the tree, in the form of the
-UNF tags. As the parsing chart is filled in bottom
up, each rule applied is essentially coming out of a
special repair rule set, and so at the top of the tree
the EDITED hypothesis is much more likely. How-
ever, this requires that several fluent speech rules
from the data set be modified for use in a special
repair grammar, which not only reduces the amount
of available training data, but violates our intuition
that most reparanda are fluent up until the actual edit
occurs.
The right corner transform model works in a dif-
ferent way, by building up constituent structure from
left to right. In Figure 2, the same fragment is
shown as it appears in the training data for this sys-
tem. With this representation, the problem noticed
by Hale and colleagues (2006) has been solved in
a different way, by incrementally building up left-
branching rather than right-branching structure, so
that only a single special error rule is required at the
end of the constituent. Whereas the -UNF propa-
gation scheme often requires the entire reparandum
to be generated from a speech repair rule set, this
scheme only requires one special rule, where the
moment of interruption actually occurred.
This is not only a pleasing parsimony, but it re-
duces the number of special speech repair rules that
need to be learned and saves more potential exam-
ples of fluent speech rules, and therefore potentially
makes better use of limited data.
3 Evaluation
The evaluation of this system was performed on
the Switchboard corpus, using the mrg annotations
in directories 2 and 3 for training, and the files
sw4004.mrg to sw4153.mrg in directory 4 for evalu-
ation, following Johnson and Charniak(2004).
The input to the system consists of the terminal
symbols from the trees in the corpus section men-
tioned above. The terminal symbol strings are first
pre-processed by stripping punctuation and other
107
System Parseval F EDIT F
Baseline 60.86 42.39
CYK (H06) 71.16 41.7
RCT 68.36 64.41
TAG-based model (JC04) ? 79.7
Table 1: Baseline results are from a standard CYK parser
with binarized grammar. We were unable to find the cor-
rect configuration to match the baseline results from Hale
et al RCT results are on the right-corner transformed
grammar (transformed back to flat treebank-style trees
for scoring purposes). CYK and TAG lines show relevant
results from related work.
non-vocalized terminal symbols, which could not
be expected from the output of a speech recognizer.
Crucially, any information about repair is stripped
from the input, including partial words, repair sym-
bols 3, and interruption point information. While an
integrated system for processing and parsing speech
may use both acoustic and syntactic information to
find repairs, and thus may have access to some of
this information about where interruptions occur,
this experiment is intended to evaluate the use of the
right corner transform and syntactic information on
parsing speech repair. To make a fair comparison to
the CYK baseline of (Hale et al, 2006), the recog-
nizer was given correct part-of-speech tags as input
along with words.
The results presented here use two standard met-
rics for assessing accuracy of transcribed speech
with repairs. The first metric, Parseval F-measure,
takes into account precision and recall of all non-
terminal (and non pre-terminal) constituents in a hy-
pothesized tree relative to the gold standard. The
second metric, EDIT-finding F, measures precision
and recall of the words tagged as EDITED in the
hypothesized tree relative to those tagged EDITED
in the gold standard. F score is defined as usual,
2pr/(p + r) for precision p and recall r.
The results in Table 1 show that this system per-
forms comparably to the state of the art in over-
all parsing accuracy and reasonably well in edit de-
tection. The TAG system (Johnson and Charniak,
2004) achieves a higher EDIT-F score, largely as a
result of its explicit tracking of overlapping words
3The Switchboard corpus has special terminal symbols indi-
cating e.g. the start and end of the reparandum.
between reparanda and alterations. A hybrid system
using the right corner transform and keeping infor-
mation about how a repair started may be able to
improve EDIT-F accuracy over this system.
4 Conclusion
This paper has described a novel method for pars-
ing speech that contains speech repairs. This system
achieves high accuracy in both parsing and detecting
reparanda in text, by making use of transformations
that create incomplete categories, which model the
reparanda of speech repair well.
References
Elizabeth R. Blackmer and Janet L. Mitton. 1991. Theo-
ries of monitoring and the timing of repairs in sponta-
neous speech. Cognition, 39:173?194.
Jean Carletta, Richard Caley, and Stephen Isard. 1993.
A collection of self-repairs from the map task cor-
pus. Technical report, Human Communication Re-
search Centre, University of Edinburgh.
John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr, Mary
Harper, Anna Krasnyanskaya, Matthew Lease, Yang
Liu, Brian Roark, Matthew Snover, and Robin Stew-
art. 2006. PCFGs with syntactic and prosodic indica-
tors of speech repairs. In Proceedings of the 45th An-
nual Conference of the Association for Computational
Linguistics (COLING-ACL).
Mark Johnson and Eugene Charniak. 2004. A tag-based
noisy channel model of speech repairs. In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL ?04), pages 33?
39, Barcelona, Spain.
Mark Johnson. 1998a. Finite state approximation of
constraint-based grammars using left-corner grammar
transforms. In Proceedings of COLING/ACL, pages
619?623.
Mark Johnson. 1998b. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430.
William J.M. Levelt. 1983. Monitoring and self-repair in
speech. Cognition, 14:41?104.
Elizabeth Shriberg. 1994. Preliminaries to a Theory of
Speech Disfluencies. Ph.D. thesis, University of Cali-
fornia at Berkeley.
Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
108
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 277?280,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Parsing Speech Repair without Specialized Grammar Symbols?
Tim Miller
University of Minnesota
tmill@cs.umn.edu
Luan Nguyen
University of Minnesota
lnguyen@cs.umn.edu
William Schuler
University of Minnesota
schuler@cs.umn.edu
Abstract
This paper describes a parsing model for
speech with repairs that makes a clear sep-
aration between linguistically meaningful
symbols in the grammar and operations
specific to speech repair in the operation of
the parser. This system builds a model of
how unfinished constituents in speech re-
pairs are likely to finish, and finishes them
probabilistically with placeholder struc-
ture. These modified repair constituents
and the restarted replacement constituent
are then recognized together in the same
way that two coordinated phrases of the
same type are recognized.
1 Introduction
Speech repair is a phenomenon in spontaneous
spoken language in which a speaker decides to
interrupt the flow of speech, replace some of the
utterance (the ?reparandum?), and continues on
(with the ?alteration?) in a way that makes the
whole sentence as transcribed grammatical only
if the reparandum is ignored. As Ferreira et al
(2004) note, speech repairs1 are the most disrup-
tive type of disfluency, as they seem to require
that a listener first incrementally build up syntac-
tic and semantic structure, then subsequently re-
move it and rebuild when the repair is made. This
difficulty combines with their frequent occurrence
to make speech repair a pressing problem for ma-
chine recognition of spontaneous speech.
This paper introduces a model for dealing with
one part of this problem, constructing a syntac-
tic analysis based on a transcript of spontaneous
spoken language. The model introduced here dif-
fers from other models attempting to solve the
?This research was supported by NSF CAREER award
0447685. The views expressed are not necessarily endorsed
by the sponsors .
1Ferreira et al use the term ?revisions?.
same problem, by completely separating the fluent
grammar from the operations of the parser. The
grammar thus has no representation of disfluency
or speech repair, such as the ?EDITED? category
used to represent a reparandum in the Switchboard
corpus, as such categories are seemingly at odds
with the typical nature of a linguistic constituent.
Rather, the approach presented here uses a
grammar that explicitly represents incomplete
constituents being processed, and repair is rep-
resented by rules which allow incomplete con-
stituents to be prematurely merged with existing
structure. While this model is interesting for its
elegance in representation, there is also reason
to hypothesize improved performance, since this
processing model requires no additional grammar
symbols, and only one additional operation to ac-
count for speech repair, and thus makes better use
of limited data resources.
2 Background
Previous work on parsing of speech with repairs
has shown that syntactic cues can be used to in-
crease accuracy of detection of reparanda, which
can increase overall parsing accuracy. The first
source of structure used to recognize repair is what
Levelt (1983) called the ?Well-formedness Rule.?
This rule essentially states that a speech repair acts
like a conjunction; that is, the reparandum and the
alteration must be of the same syntactic category.
Of course, the reparandum is often unfinished, so
the Well-formedness Rule allows for the reparan-
dum category to be inferred.
This source of structure has been used by two
related approaches, that of Hale et al (2006) and
Miller (2009). Hale and colleagues exploit this
structure by adding contextual information to the
standard reparandum label ?EDITED?. In their
terminology, daughter annotation takes the (pos-
sibly unfinished) constituent label of the reparan-
dum and appends it to the EDITED label. This
277
allows a learned probabilistic context-free gram-
mar to represent the likelihood of a reparandum of
a certain type being a sibling with a finished con-
stituent of the same type.
Miller?s approach exploited the same source of
structure, but changed the representation to use
a REPAIRED label for alterations instead of an
EDITED label for reparanda. The rationale for
that change is the fact that a speech repair does not
really begin until the interruption point, at which
point the alteration is started and the reparandum
is retroactively labelled as such. Thus, the argu-
ment goes, no special syntactic rules or symbols
should be necessary until the alteration begins.
3 Model Description
3.1 Right-corner transform
This work first uses a right-corner transform,
which turns right-branching structure into left-
branching structure, using category labels that use
a ?slash? notation ?/? to represent an incomplete
constituent of type ? ?looking for? a constituent
of type ? in order to complete itself.
This transform first requires that trees be bina-
rized. This binarization is done in a similar way to
Johnson (1998) and Klein and Manning (2003).
Rewrite rules for the right-corner transform are
as follows, first flattening right-branching struc-
ture:2
A
1
?
1
A
2
?
2
A
3
a
3
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
A
3
a
3
A
1
?
1
A
2
A
2
/A
3
?
2
. . .
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
. . .
then replacing it with left-branching structure:
A
1
A
1
/A
2
:?
1
A
2
/A
3
?
2
?
3
. . .
?
A
1
A
1
/A
3
A
1
/ A
2
:?
1
?
2
?
3
. . .
One problem with this notation is the represen-
tation given to unfinished constituents, as seen in
Figures 1 and 2. The standard representation of
2Here, all A
i
denote nonterminal symbols, and ?
i
denote
subtrees; the notationA
1
:?
0
indicates a subtree ?
0
with label
A
1
; and all rewrites are applied recursively, from leaves to
root.
S
. . . EDITED
PP
IN
as
NP-UNF
DT
a
PP
IN
as
NP
NP
DT
a
NN
westerner
PP-LOC
IN
in
NP
NNP
india
. . .
Figure 1: Section of interest of a standard phrase
structure tree containing speech repair with unfin-
ished noun phrase (NP).
PP
PP/NP
PP/PP
PP/NP
PP/PP
EDITEDPP
EDITEDPP/NP-UNF
IN
as
NP-UNF
DT
a
IN
as
NP
NP/NN
DT
a
NN
westerner
IN
in
NP
india
Figure 2: Right-corner transformed version of the
fragment above. This tree requires several special
symbols to represent the reparandum that starts
this fragment.
an unfinished constituent in the Switchboard cor-
pus is to append the -UNF label to the lowest un-
finished constituent (see Figure 1). Since one goal
of this work is separation of linguistic knowledge
from language processing mechanisms, the -UNF
tag should not be an explicit part of the gram-
mar. In theory, the incomplete category notation
induced by the right-corner transform is perfectly
suited to this purpose. For instance, the category
NP-UNF is a stand in category for several incom-
plete constituents, for example NP/NN, NP/NNS,
etc. However, since the sub-trees with -UNF la-
bels in the original corpus are by definition unfin-
ished, the label to the right of the slash (NN in
this case) is not defined. As a result, transformed
trees with unfinished structure have the represen-
tation of Figure 2, which gives away the positive
benefits of the right-corner transform in represent-
ing repair by propagating a special repair symbol
(EDITED) through the grammar.
3.2 Approximating unfinished constituents
It is possible to represent -UNF categories as stan-
dard unfinished constituents, and account for un-
finished constituents by having the parser prema-
278
turely end the processing of a given constituent.
However, in the example given above, this would
require predicting ahead of time that the NP-UNF
was only missing a common noun ? NN (for ex-
ample). This problem is addressed in this work
by probabilistically filling in placeholder final cat-
egories of unfinished constituents in the standard
phrase structure trees, before applying the right-
corner transform.
In order to fill in the placeholder with realistic
items, phrase completions are learned from cor-
pus statistics. First, this algorithm identifies an
unfinished constituent to be finished as well as its
existing children (in the continuing example, NP-
UNF with child labelled DT). Next, the corpus is
searched for fluent subtrees with matching root la-
bels and child labels (NP and DT), and a distri-
bution is computed of the actual completions of
those subtrees. In the model used in this work,
the most common completions are NN, NNS, and
NNP. The original NP-UNF subtree is then given a
placeholder completion by sampling from the dis-
tribution of completions computed above.
After this addition is complete, the UNF and
EDITED labels are removed from the reparandum
subtree, and if a restarted constituent of the same
type is a sibling of the reparandum (e.g. another
NP), the two subtrees are made siblings under a
new subtree with the same category label (NP).
See Figure 3 for a simple visual example of how
this works.
S
. . . EDITED
PP
IN
as
NP
DT
a
NN
eli
PP
IN
as
NP
NP
DT
a
NN
westerner
PP-LOC
IN
in
NP
NNP
india
. . .
Figure 3: Same tree as in Figure 1, with the un-
finished noun phrase now given a placeholder NN
completion (both bolded).
Next, these trees are modified using the right-
corner transform as shown in Figure 4. This tree
still contains placeholder words that will not be
in the text stream of an observed input sentence.
Thus, in the final step of the preprocessing algo-
rithm, the finished category label and the place-
holder right child are removed where found in a
right-corner tree. This results in a right-corner
transformed tree in which a unary child or right
PP
PP/NNP
PP/PP
PP/NP
PP/PP
PP
PP/NN
PP/NP
IN
as
DT
a
NN
eli
IN
as
NP
NP/NN
DT
a
NN
westerner
IN
in
NNP
india
Figure 4: Right-corner transformed tree with
placeholder finished phrase.
PP
PP/NNP
PP/PP
PP/NP
PP/PP
PP/NN
PP/NP
IN
as
DT
a
IN
as
NP
NP/NN
DT
a
NN
westerner
IN
in
NNP
india
Figure 5: Final right-corner transformed state af-
ter excising placeholder completions to unfinished
constituents. The bolded label indicates the signal
of an unfinished category reparandum.
child subtree having an unfinished constituent type
(a slash category, e.g. PP/NN in Figure 5) at its
root represents a reparandum with an unfinished
category. The tree then represents and processes
the rest of the repair in the same way as a coordi-
nation.
4 Evaluation
This model was evaluated on the Switchboard cor-
pus (Godfrey et al, 1992) of conversational tele-
phone speech between two human interlocuters.
The input to this system is the gold standard
word transcriptions, segmented into individual ut-
terances. For comparison to other similar systems,
the system was given the gold standard part of
speech for each input word as well. The standard
train/test breakdown was used, with sections 2 and
3 used for training, and subsections 0 and 1 of sec-
tion 4 used for testing. Several sentences from the
end of section 4 were used during development.
For training, the data set was first standardized
by removing punctuation, empty categories, ty-
pos, all categories representing repair structure,
279
and partial words ? anything that would be diffi-
cult or impossible to obtain reliably with a speech
recognizer.
The two metrics used here are the standard Par-
seval F-measure, and Edit-finding F. The first takes
the F-score of labeled precision and recall of the
non-terminals in a hypothesized tree relative to the
gold standard tree. The second measure marks
words in the gold standard as edited if they are
dominated by a node labeled EDITED, and mea-
sures the F-score of the hypothesized edited words
relative to the gold standard.
System Configuration Parseval-F Edited-F
Baseline CYK 71.05 18.03
Hale et al 68.48 37.94
Plain RC Trees 69.07 30.89
Elided RC Trees 67.91 24.80
Merged RC Trees 68.88 27.63
Table 1: Results
Results of the testing can be seen in Ta-
ble 1. The first line (?Baseline CYK?) indi-
cates the results using a standard probabilistic
CYK parser, trained on the standardized input
trees. The following two lines are results from re-
implementations of the systems from Hale et al
(2006) andMiller (2009). The line marked ?Elided
trees? gives current results. Surprisingly, this re-
sult proves to be lower than the previous results.
Two observations in the output of the parser on
the development set gave hints as to the reasons
for this performance loss.
First, repairs using the slash categories (for un-
finished reparanda) were rare (relative to finished
reparanda). This led to the suspicion that there
was a state-splitting phenomenon, where cate-
gories previously lumped together as EDITED-NP
were divided into several unfinished categories
(NP/NN, NP/NNS, etc.). To test this suspicion, an-
other experiment was performed where all unary
child and right child subtrees with unfinished cat-
egory labels X/Y were replaced with EDITED-X.
This result is shown in line five of Table 1. This
result improves on the elided version, and sug-
gests that the state-splitting effect is most likely
one cause of decreased performance.
The second effect in the parser output was the
presence of several very long reparanda (more
than ten words), which are highly unlikely in nor-
mal speech. This phenomenon does not occur
in the ?Plain RC Trees? condition. One explana-
tion for this effect is that plain RC trees use the
EDITED label in each rule of the reparandum (see
Figure 2 for a short real-world example). This
essentially creates a reparandum rule set, mak-
ing expansion of a reparandum difficult due to the
likelihood of a long chain eventually requiring a
reparandum rule that was not found in the train-
ing data, or was not learned correctly in the much
smaller set of reparandum-specific training data.
5 Conclusion and Future Work
In conclusion, this paper has presented a new
model for speech containing repairs that enforces
a clean separation between linguistic categories
and parsing operations. Performance was below
expectations, but analysis of the interesting rea-
sons for these results suggests future directions. A
model which explicitly represents the distance that
a speaker backtracks when making a repair would
prevent the parser from hypothesizing the unlikely
reparanda of great length.
References
Fernanda Ferreira, Ellen F. Lau, and Karl G.D. Bai-
ley. 2004. Disfluencies, language comprehension,
and Tree Adjoining Grammars. Cognitive Science,
28:721?749.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Proc. ICASSP,
pages 517?520.
John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr,
Mary Harper, Anna Krasnyanskaya, Matthew Lease,
Yang Liu, Brian Roark, Matthew Snover, and Robin
Stewart. 2006. PCFGs with syntactic and prosodic
indicators of speech repairs. In Proceedings of the
45th Annual Conference of the Association for Com-
putational Linguistics (COLING-ACL).
Mark Johnson. 1998. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Willem J.M. Levelt. 1983. Monitoring and self-repair
in speech. Cognition, 14:41?104.
Tim Miller. 2009. Improved syntactic models for pars-
ing speech with repairs. In Proceedings of the North
American Association for Computational Linguis-
tics, Boulder, CO.
280
Broad-Coverage Parsing Using Human-Like
Memory Constraints
William Schuler?
University of Minnesota
Samir AbdelRahman??
Cairo University
Tim Miller?
University of Minnesota
Lane Schwartz?
University of Minnesota
Human syntactic processing shows many signs of taking place within a general-purpose
short-term memory. But this kind of memory is known to have a severely constrained storage
capacity ? possibly constrained to as few as three or four distinct elements. This article describes
a model of syntactic processing that operates successfully within these severe constraints, by
recognizing constituents in a right-corner transformed representation (a variant of left-corner
parsing) and mapping this representation to random variables in a Hierarchical Hidden Markov
Model, a factored time-series model which probabilistically models the contents of a bounded
memory store over time. Evaluations of the coverage of this model on a large syntactically
annotated corpus of English sentences, and the accuracy of a bounded-memory parsing strategy
based on this model, suggest this model may be cognitively plausible.
1. Introduction
It is an interesting possibility that human syntactic processingmay occur entirely within
a general-purpose short-term memory. Like other short-term memory processes, syn-
tactic processing is susceptible to degradation if short-term memory capacity is loaded,
for example, when readers are asked to retain lists of words while reading (Just
and Carpenter 1992); and memory of words and syntax degrades over time within
and across sentences (Sachs 1967; Jarvella 1971), unlike semantics and discourse
information about referents from other sentences (Ericsson and Kintsch 1995). But
short-term memory is known to have severe capacity limitations of perhaps no more
than three to four distinct elements (Miller 1956; Cowan 2001). These limits may seem
? Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455.
E-mail: schuler@cs.umn.edu; tmill@cs.umn.edu; lane@cs.umn.edu.
?? Computer Science Department, Faculty of Computers and Information, 5 Dr. Ahmed Zewail Street,
Postal Code: 12613, Orman, Giza, Egypt. E-mail: s.abdelrahman@fci-cu.edu.eg.
Submission received: 14 February 2008; revised submission received: 31 October 2008; accepted for
publication: 27 May 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 1
too austere to process the rich tree-like phrase structure commonly invoked to explain
word-order regularities in natural language.
This article aims to show that they are not. The article describes a comprehension
model, based on a right-corner transform?a reversible tree transform related to the
left-corner transform of Johnson (1998a)?that associates familiar phrase structure trees
with the contents of a memory store of three to four partially completed constituents
over time. Coverage results on the large syntactically annotated Penn Treebank corpus
show a vast majority of naturally occurring sentences can be recognized using a mem-
ory store containing a maximum of only three incomplete constituents, and nearly all
sentences can be recognized using four, consistent with estimates of human short-term
memory capacity.
This transform reduces memory usage in incremental (left to right) processing
by transforming right-branching constituent structures into left-branching structures,
allowing child constituents to be composed with parent constituents before either have
been completely recognized. But because this composition identifies an incomplete
child as the awaited portion of an incomplete parent, it implicitly predicts that this
child constituent will be the rightmost (i.e., last) child of the parent, before this child
has been completely recognized. Parsing accuracy results on the Penn Treebank
using a Hierarchical Hidden Markov Model (Murphy and Paskin 2001)?essentially a
probabilistic pushdown automaton with a bounded pushdown store?show that this
prediction can be reliably learned from training data.
The remainder of this article is organized as follows: Section 2 describes some
related models of human syntactic processing using a bounded memory store; Section 3
describes a Hierarchical Hidden Markov Model (HHMM) framework for statistical
parsing using this bounded store of incomplete constituents; Section 4 describes the
right-corner transform and how it relates conventional phrase structure to incomplete
constituents in a bounded memory store; Section 5 describes an experiment to estimate
the level of coverage of the Penn Treebank corpus that can be achieved using this
transform with various memory limits, given a linguistically motivated binarization of
this corpus; and Section 6 gives accuracy results of this bounded-memorymodel trained
on this corpus, given that some amount of incremental prediction (as described earlier)
must be involved.
2. Bounded-Memory Parsing
One of the earliest bounded-memory parsing models is that of Marcus (1980). This
model maintains a bounded store of complete but unattached constituents as a buffer,
and operates on them using a variety of specialized memory manipulation operations,
deferring certain attachment decisions until the contents of this buffer indicate it is safe
to do so. (In contrast, the model described in this article maintains a store of incom-
plete constituents using ordinary stack-like push and pop operations, defined to allow
constituents to be composed before being completely recognized.) The Marcus parser
provides a bounded-memory explanation for human difficulties in processing garden
path sentences: for example, the horse raced past the barn fell, with intended interpretation
[NP the horse [RC (which was) raced past the barn]] fell (Bever 1970), in which raced seems like
the main verb of the sentence until the word fell is encountered. But this explanation due
to memory exhaustion is not compatible with observations of unproblematic parsing
of sentences such as these when contextual information is provided in advance: for
example, two men on horseback had a race; one went by the meadow, and the other went by the
barn (Crain and Steedman 1985).
2
Schuler et al Parsing Using Human-Like Memory Constraints
Ades and Steedman (1982) introduce the idea of composing incomplete constituents
to reduce storage demands in incremental processing using Combinatorial Catego-
rial Grammar (CCG), avoiding the need to maintain large buffers of complete but
unattached constituents. The right-corner transform described in this article composes
incomplete constituents in very much the same way, but CCG is essentially a compe-
tence model, in that it seeks to unify lexical category representations used in processing
with learned generalizations about argument structure, whereas the model described
herein is exclusively a performance model, allowing generalizations about lexical ar-
gument structures to be learned in some other representation, then combined with
probabilistic information about parsing strategies to yield a set of derived incomplete
constituents. As a result, the model described in this article has a freer hand to satisfy
strict working memory bounds, which may not permit some of the alternative compo-
sition operations proposed in the CCG account, thought to be associated with available
prosody and quantifier scope analyses.1
Johnson-Laird (1983) and Abney and Johnson (1991) propose a pure processing
account of memory capacity limits in parsing ordinary phrase structure trees. The
Johnson-Laird and Abney and Johnson models adopt a left-corner parsing strategy, of
which the right-corner transform introduced in this article is a variant, in order to bring
memory usage for most parsable sentences to within seven or so active or awaited
phrase structure constituents. This account may be used to explain human processing
difficulties in processing triply center-embedded sentences like the rat that the cat that the
dog chased killed ate the malt, with intended interpretation [NP the rat that [NP the cat that [NP
the dog] chased] killed] ate the malt (Chomsky and Miller 1963). But this explanation does
not account for examples of triply center-embedded sentences that typically do not
cause processing problems: [NP that [NP the food that [NP John] ordered] tasted good] pleased
him (Gibson 1991). Moreover, the apparent competition between comprehension of
center-embedded object relatives and retention of unrelated words in general-purpose
memory (Just and Carpenter 1992) suggests that general-purpose memory is (or at
least, can be) used to store incomplete constituents during comprehension. This would
predict three or four elements of reliable storage, rather than seven (Cowan 2001).
The transform-based model described in this article exploits a conception of chunking
(Miller 1956) to combine pairs of active and awaited constituents from the Abney
and Johnson analysis, connected by recognized structure, in order to operate within
estimates of human short-term memory bounds.
Because of these counterexamples to the memory-exhaustion explanation of garden
path and center-embedding difficulties, recent work has turned to explanations other
than memory exhaustion for these phenomena. Lewis and Vasishth (2005) attribute
processing errors to activation interference among stored constituents that have sim-
ilar syntactic and semantic roles. Hale?s surprisal (2001) and entropic model (2006)
link human processing difficulties to significant changes in the relative probability of
competing hypotheses in incremental parsing, such that if activation is taken to be a
mechanism for probability estimation, processing difficulties may be ascribed to the
relatively slow speed of activation change within the brain (or to collapsing activation
when probabilities grow too small, as in the case of garden path sentences). These
models explain many processing difficulties without invoking memory limits, and are
1 The lack of support for some of these available scope analyses may not necessarily be problematic for the
present model. The complexity of interpreting nested raised quantifiers may place them beyond the
capability of human interactive incremental interpretation, but not beyond the capability of post hoc
interpretation (understood after the listener has had time to think about it).
3
Computational Linguistics Volume 36, Number 1
compatible with brain imaging evidence of increased cortical activity and recruitment
of auxiliary brain areas during periods of increased uncertainty in sentence processing
(Just and Varma 2007). But if interference or changing activation is posited as the source
of processing difficulty, and delays are not linked to memory exhaustion per se, then
these theories do not explain how (or whether) syntactic processing operates within
general-purpose short-term memory.
Toward this end, this article will specifically evaluate the claim that syntactic
processing can be performed entirely within general-purpose short-term memory by
using this memory to store unassimilated incomplete syntactic constituents, derived
through a right-corner transform from basic properties of phrase structure trees. As
a probabilistic incremental parser, the model described in this article is compatible
with surprisal-based explanations of processing difficulties; it is, however, in some
sense orthogonal, because it models a different dimension of resource allocation. The
surprisal framework models allocation of processing resources (in this case, activation)
among disjunctions of competing hypotheses, which are maintained for some amount
of time in parallel, whereas the framework described here can be taken to model the
allocation of processing resources (in this case, memory elements) among conjunctions
of incompletely recognized constituents within each competing hypothesis.2 Thus, in
this view, there are twoways to simultaneously activatemultiple concepts: disjunctively
(sharing activation among competing hypotheses) and conjunctively (sharing activation
among unassimilated constituents within a hypothesis). But only the inner conjunctive
allocation corresponds to the familiar discretely bounded store of short-term memory
as described by Miller (1956); the outer disjunctive allocation treats activation as a
continuous resource in which like-valued pockets expand and contract as they are
reinforced or contradicted by incoming observations. Indeed, it would be surprising
if these two dimensions of resource allocation did not exist: the former, because it
would contradict years of observations about the behavior of short-term memory; and
the latter, because it would require neural activation spreading to be instantaneous
and uniform, contradicting most neuropsychological evidence. Levy (2008) compares
the allocation of activation in this kind of framework to the distributed allocation
of resources in a particle filter (Gordon, Salmond, and Smith 1993), an approximate
inference technique for probabilistic time-series models in which particles in a (typically
fixed) reservoir are assigned randomly sampled hypotheses from learned transition
probabilities, essentially functioning as units of activation. The model described in this
paper qualifies this analogy by positing that each individual particle in this reservoir
endorses a coherent hypothesis about the contents of a three- to four-element memory
store at any given time, rather than about an entire unbounded phrase structure tree.3
2 Probability distributions in entropy-based models like Hale?s are typically assumed to be defined over
sets of hypotheses pursued in parallel, but other interpretations (for example, lookahead-based
deterministic models) are possible. The model described in this article is also compatible with
deterministic parsing frameworks, in which case it models allocation of processing resources among
incompletely-recognized constituents within a single non-competing hypothesis.
3 Pure connectionist models of syntactic processing (Elman 1991; Berg 1992; Rohde 2002) attempt to unify
storage of constituent structure with that of ambiguous alternative analyses, but the memory demands of
systems based on this approach typically do not scale well to broad-coverage parsing. Recent results for
using self-organizing maps as a unified memory resource are encouraging (Mayberry and Miikkulainen
2003), but are still limited to parsing relatively short travel planning queries with limited syntactic
complexity. Hybrid systems that generate explicit alternative hypotheses with explicit stacked-up
constituents, and use connectionist models for probability estimation over these hypotheses (Henderson
2004) typically achieve better performance in practice.
4
Schuler et al Parsing Using Human-Like Memory Constraints
Previous memory-based explanations of problematic sentences (explaining garden
path effects as exceeding a bound of four complete but unattached constituents, or ex-
plaining center embedding difficulties as exceeding a bound of seven active or awaited
constituents) have been shown to underestimate human sentence processing capacity
when equally complex but unproblematic sentences were examined. The hypothesis
advanced in this article, that human sentence processing uses general-purpose short-
term memory to store incomplete constituents as defined by a right-corner transform,
leaves the explanation of several negative examples of unparsable garden path and cen-
ter embedding sentences to orthogonal models of surprisal or interference. But in order
to determine whether this right-corner memory hypothesis still underestimates human
sentence processing capacity, a corpus study was performed on two complementary
corpora of transcribed spontaneous speech and newspaper text, manually annotated
with phrase structure trees (Marcus, Santorini, and Marcinkiewicz 1993). These spon-
taneous speech and newspaper text corpora contain only attested positive examples
of parsable sentences, but they may be considered complementary for this purpose
because the complexity of spontaneous speech may somewhat understate human recog-
nition capacity (potentially limiting it to the cost of spontaneously generating sentences
in an unusual social context), and the complexity of newspaper text may somewhat
overstate human recognition capacity (though it is composed and edited to be readable,
it is still composed and edited off-line), so results from these corpora may be taken
together to suggest generous and conservative upper bounds on human processing
capacity.
3. Bounded-Memory Parsing with a Time Series Model
The framework adopted in this article is a factored HMM-like time series model, which
maintains a probability distribution over the contents of a bounded set of random
variables over time, corresponding to hypothesized stores of memory elements. The
random variables in this store may be understood as simultaneous activations in a cog-
nitive model (similar to the superimposed roles described by Smolensky and Legendre
[2006]), and the probability distribution over these stores may be thought of as compet-
ing pockets of activation, as described in the previous section. Some of these variables
persist as elements of the short-term memory store, and some are transient as results of
hypothesized compositions, which are estimated and immediately discarded or folded
into the persistent store according to the dependencies in the model. The variables
have values or contents (or fillers)?in this case incomplete constituent categories?that
change over time, and although these values may be uncertain, the set of hypothesized
contents of this memory store at any given point in time are collectively constrained to
form a coherent (but possibly incomplete) syntactic analysis of a sentence.
The particular model used here is an HHMM (Murphy and Paskin 2001), which
mimics a bounded-memory pushdown automaton (PDA), supporting simple push and
pop operations on a bounded stack-like memory store. A time-series model is used
here instead of an explicit stack machine, first because the probability model is well
defined on a bounded memory store, and second because the plasticity of the random
variables that mimic stack behavior in this model makes the model cross-linguistically
attractive. By evoking additional random variables and dependencies, the model can
be defined (or presumably, trained) to mimic other types of automata, such as extended
pushdown automata (EPDAs) recognizing tree-adjoining languages with crossed and
nested dependencies, as have been hypothesized for languages like Dutch (Shieber
5
Computational Linguistics Volume 36, Number 1
1985). However, the remainder of this article will only discuss random variables and
dependencies necessary to mimic a bounded stack pushdown automaton.
3.1 Hierarchical HMMs
Hierarchical Hidden Markov Models (Murphy and Paskin 2001) are essentially Hidden
Markov Models factored into some fixed number of stack-like elements at each time
step.
HMMs characterize speech or text as sequences of hidden states qt (which may
consist of phones, words, or other hypothesized syntactic or semantic information), and
observed states ot at corresponding time steps t (typically short, overlapping frames
of an audio signal, or words or characters in a text processing application). A most
likely sequence of hidden states q?1..T can then be hypothesized given any sequence of
observed states o1..T:
q?1..T = argmax
q1..T
P(q1..T | o1..T ) (1)
= argmax
q1..T
P(q1..T ) ? P(o1..T | q1..T ) (2)
def
= argmax
q1..T
T
?
t=1
P?A (qt | qt?1) ? P?B (ot | qt) (3)
using Bayes? Law (Equation 2) and Markov independence assumptions (Equation 3)
to define a full P(q1..T | o1..T ) probability as the product of a Language Model (?A)
prior probability P(q1..T )
def
=
?
t P?A (qt | qt?1) and anObservation Model (?B) likelihood
probability P(o1..T | q1..T )
def
=
?
t P?B (ot | qt) (Baker 1975; Jelinek, Bahl, and Mercer 1975).
Language model transitions P?A (qt | qt?1) over complex hidden states qt can be
modeled using synchronized levels of stacked-up component HMMs in an HHMM,
analogous to a shift-reduce parser or pushdown automaton with a bounded stack.
HHMM transition probabilities are calculated in two phases: a ?reduce? phase (result-
ing in an intermediate, transient final-state variable ft), modeling whether component
HMMs terminate; and a ?shift? phase (resulting in a persistent modeled state qt), in
which unterminated HMMs transition and terminated HMMs are re-initialized from
their parent HMMs. Variables over intermediate ft and modeled qt states are factored
into sequences of depth-specific variables?one for each of D levels in the HHMM
hierarchy:
ft = ? f 1t . . . f
D
t ? (4)
qt = ?q1t . . . q
D
t ? (5)
Transition probabilities are then calculated as a product of transition probabilities at
each level, using level-specific ?reduce? ?F,d and ?shift? ?Q,d models:
P?A (qt | qt?1) =
?
ft
P( ft | qt?1) ? P(qt | ft qt?1) (6)
6
Schuler et al Parsing Using Human-Like Memory Constraints
Figure 1
Graphical representation of a Hierarchical Hidden Markov Model. Circles denote random
variables, and edges denote conditional dependencies. Shaded circles denote variables with
observed values.
def
=
?
f 1t..f
D
t
D
?
d=1
P?F,d ( f
d
t | f
d+1
t q
d
t?1q
d?1
t?1 ) ?
D
?
d=1
P?Q,d (q
d
t | f
d+1
t f
d
t q
d
t?1q
d?1
t ) (7)
with f D+1t and q
0
t defined as constants. In these equations, probabilities are marginalized
or summed over all combinations of intermediate variables f 1t ... f
D
t , so only the memory
store contents q1t ...q
D
t persist across time steps.
4 A graphical representation of anHHMM
with three depth levels is shown in Figure 1.
The independence assumptions in this model can be psycholinguistically moti-
vated. Independence across time points t (Equation 3) arise naturally from causality:
Any change to a memory store configuration to generate a configuration at time step
t+ 1 should depend only on the current memory store configuration at time step t;
memory operations should not be able to peek backward or forward in time to consult
past or future memory stores. Independence across depth levels d (Equation 7) arise
naturally from uncertainty about the structure between incomplete constituent chunks
(this property of right-corner transform categories is elaborated in Section 4).5
Shift and reduce probabilities can now be defined in terms of finitely recursive
HMMs with probability distributions over recursive expansion, transition, and reduc-
tion of states at each depth level. In the version of HHMMs used in this paper, each
modeled variable is a syntactic state qdt ? G?G (describing an incomplete constituent
consisting of an active grammatical category from domain G and an awaited grammat-
ical category from domain G?for example, an incomplete constituent S/NP consisting
of an active sentence S awaiting a noun phrase constituent NP); and each intermediate
4 In Viterbi decoding, probabilities over intermediate variables may be maximized rather than
marginalized, but in any case the intermediate variables do not persist.
5 Also, the fact that this is a generative model, in which observations are conditioned on hypotheses, then
flipped using Bayes? Law (Equation 2)?as opposed to a discriminative or conditional model, in which
hypotheses are conditioned directly on observations?is also appealing as a human model, in that it
allows the same architecture to be used for both recognition and generation. This is a desirable property
for modeling split utterances, in which interlocutors complete one another?s sentences (Lerner 1991;
Helasvuo 2004).
7
Computational Linguistics Volume 36, Number 1
variable is a reduction or non-reduction state f dt ? G?{1, 0} (indicating, respectively, a
reduction of incomplete constituent qdt?1 to a complete right child constituent of some
grammatical category from domainG, or a non-reduction of qdt?1 as a unary or left child,
as defined in Section 4). An intermediate variable f dt at depth d may indicate reduction
or non-reduction according to ?F-Reduction,d if there is a reduction at the depth level
immediately below d, but must indicate non-reduction ( f dt = 0) with probability 1 if
there is no reduction below:6
P?F,d ( f
d
t | f
d+1
t q
d
t?1q
d?1
t?1 )
def
=
{
if f d+1t ?G : [ f
d
t = 0]
if f d+1t ?G : P?F-Reduction,d ( f
d
t | q
d
t?1, q
d?1
t?1 )
(8)
where f D+1t = 1 and q
0
t = ROOT.
Shift probabilities over the modeled variable qdt at each level are defined using level-
specific transition ?Q-Transition,d and expansion ?Q-Expansion,d models:
P?Q,d (q
d
t | f
d+1
t f
d
t q
d
t?1q
d?1
t )
def
=
?
?
?
?
?
if f d+1t ?G, f
d
t ?G : [q
d
t = q
d
t?1]
if f d+1t ?G, f
d
t ?G : P?Q-Transition,d (q
d
t | f
d+1
t f
d
t q
d
t?1q
d?1
t )
if f d+1t ?G, f
d
t ?G : P?Q-Expansion,d (q
d
t | q
d?1
t )
(9)
where f D+1t = 1 and q
0
t = ROOT. This model is conditioned on final-state intermediate
variables f dt and f
d+1
t at and immediately below each HHMM level. If there is no re-
duction immediately below a given level (the first case provided), it deterministically
copies the current HHMM state forward to the next time step. If there is a reduction
immediately below the current level but no reduction at the current level (the second
case provided), it transitions the HHMM state at the current level, according to the
distribution ?Q-Transition,d. And if there is a reduction at the current level (the third case
above), it re-initializes this state given the state at the level above, according to the
distribution ?Q-Expansion,d.
Models ?F-Reduction,d, ?Q-Transition,d, and ?Q-Expansion,d are defined directly from train-
ing examples, for example (in the experiments described in this article), using relative
frequency estimation. The overall effect is that higher-level HMMs are allowed to
transition only when lower-level HMMs terminate. An HHMM therefore behaves like
a probabilistic implementation of a shift?reduce parser or pushdown automaton with a
bounded stack, where the maximum stack depth is equal to the number of depth levels
in the HHMM hierarchy.
4. Right-Corner Transform and Incomplete Constituents
The model described in this article recognizes trees in a right-corner transformed
representation to minimize usage of a bounded short-term memory store. This right-
corner transform is a variant of the left-corner transform described by Johnson (1998a),
but whereas the left-corner transform changes left-branching structure into right-
branching structure, the right-corner transform changes right-branching structure into
6 Here [?] is an indicator function: [?] = 1 if ? is true, 0 otherwise.
8
Schuler et al Parsing Using Human-Like Memory Constraints
left-branching structure. Recognition using this transformed grammar, extracted from
a transformed corpus, is similar to recognition using a left-corner parsing strategy (Aho
and Ullman 1972). This kind of strategy was shown to reduce memory requirements
for parsing sentences with mainly left- or right-recursive phrase structure to fewer than
seven active or awaited constituent categories (Abney and Johnson 1991). This is within
Miller?s (1956) estimate of human short-term memory capacity (if memory elements
store individual categories), whereas parsing heavily center-embedded sentences
(known to be difficult for human readers) would require seven or more elements at the
frontier of this capacity.
But recent research suggests that human memory capacity may be limited to as few
as three or four distinct items (Cowan 2001), with longer estimates of seven or more
possibly due to the human capacity to chunk remembered items into associated groups
(Miller 1956). The right-corner strategy described in this paper therefore assumes
constituent categories can similarly be chunked into incomplete constituents A/B formed
by pairing an active category Awith an awaited category B somewhere along the active
category?s right progeny (so, for example, a transitive verb may become an incomplete
constituent VP/NP consisting of an active verb phrase lacking an awaited noun phrase
yet to come).7 These chunked incomplete constituent categories A and B are naturally
related through fixed contiguous phrase structure between them, established during
the course of parsing prior to the beginning of B, and these incomplete constituents can
be composed with other incomplete constituents B/C to form similarly related category
pairs A/C.
These chunks are not only contiguous sections of phrase structure trees, they also
have contiguous string yields, so they correspond to the familiar notion of text chunks
used in shallow parsing approaches (Hobbs et al 1996). For example, a hypothesized
memory store may contain incomplete constituents S/NP (a sentence without a noun
phrase), followed by NP/NN (a noun phrase lacking a common noun), with cor-
responding string yields demand for bonds propped up and the municipal, respectively,
forming a complete contiguous segmentation of a sentence at any point in processing.
Although these two chunks could be composed into an incomplete constituent S/NN,
doing so at this point would close off the possibility of introducing another constituent
between these two, containing the recognized noun phrase as a left child (e.g., demand
for bonds propped up [NP [NP the municipal bonds]?s prices]).
This conception of chunking applied to right-branching progeny in phrase structure
trees does not have the power to eliminate the bounds of a memory store, however. In a
larger cognitive model, syntactic processing is assumed to occur as part of an interactive
semantic interpretation process, in which referents of constituents are calculated as
these constituents are recognized, and are used to constrain subsequent processing
decisions (Tanenhaus et al 1995; Brown-Schmidt, Campana, and Tanenhaus 2002).8
The chunked category pairs A and B in these incomplete constituents A/B result from
successful compositions of other such constituents earlier in the recognition process,
which means that the relationship between the referents of A and B is known and fixed
7 Incomplete constituents may also be defined through a left-corner transform, but left-corner transformed
categories are incomplete in the other direction?a goal category yet to come lacking an
already-recognized constituent?so stored incomplete constituent categories resulting from a left-corner
transform would have the character of future goal events, rather than remembered past events. This is
discussed in greater detail in Section 4.4.
8 This can be implemented in a time-series model by factoring the model to include additional random
variables over referents, as described in Schuler, Wu, and Schwartz (2009).
9
Computational Linguistics Volume 36, Number 1
in any hypothesized incomplete constituent. But syntactic and semantic relationships
between chunks in a hypothesized memory store are unspecified. Chunking beyond
the level of incomplete constituents would therefore involve grouping referents whose
interrelations have not necessarily been established by the parser. Because the set
of referents is presumably much larger than the set of syntactic categories, one may
assume there are real barriers to reliably chunking them in the absence of these fixed
relationships.
There certainly may be cases where syntactically unconnected referents (belonging
to different incomplete constituents) could be grouped together as chunks. But for
simplicity, this article will assume a very strict condition that only a single incomplete
constituent can be stored in each short-term memory element. Experimental results
described in Section 5 suggest that a vast majority of English sentences can be recog-
nized within these human-like memory bounds, even with this strict condition on
chunking. If parsing can be performed in bounded memory under such strict condi-
tions, it can reasonably be assumed to operate at least as well under more permissive
circumstances, where some amount of syntactically-unrelated referential chunking is
allowed.
Several existing incremental systems are organized around a left-corner parsing
strategy (Roark 2001; Henderson 2004). But these systems generally keep large numbers
of constituents open for modifier attachment in each hypothesis. This allows modifiers
to be attached as right children of any such open constituent. But if any number of
open constituents are allowed, then either the assumption that stored elements have
fixed syntactic (and semantic) internal structure will be violated, or the assumption that
syntax operates within a boundedmemory store will be violated, both of which are psy-
cholinguistically attractive as simplifying assumptions. The HHMM model described
in this article upholds both the fixed-element and bounded-memory assumptions by
hypothesizing fixed reductions of right child constituents into incomplete parents in the
same memory element, to make room for new constituents that may be introduced at a
later time. These in-element reductions are defined naturally on phrase structure trees
as the result of aligning right-corner transformed constituent structures to sequences of
random variables in a factored time-series model. The success of this predictive strategy
in corpus-based coverage and accuracy results described in Sections 5 and 6 suggests
that it may be plausible as a cognitive model.
Other accounts may model reductions in bounded memory as occurring as soon
as possible, by maintaining the option of undoing them when necessary (Stevenson
1998). This seems unattractive in the context of an interactive semantic model, however,
where syntactic constituents and semantic referents are composed in tandem, because
potentially very rich referential constraints introduced by composing a child constituent
into a parent would have to be systematically undone. An interesting possibility might
be that the appearance of syntactic restructuring may arise from a collection of hypoth-
esized stores of syntactically fixed incomplete constituents, pursued in parallel. The
results presented in this article suggest that this mechanism is possible, but these two
possibilities might be very difficult to distinguish empirically.
There is also a tradition of defining incomplete constituents as head-driven?
introduced in parsing only at the point in incremental recognition at which they can
be associated with a head word (Gibson 1991: Pritcher 1991: Gorrell 1995). In typically
head-initial languages such as English, incomplete constituents derived from these
head-driven models resemble those derived from a right-corner transform. But head-
driven incomplete constituents do not appear to obey general-purpose memory bounds
in head-final languages such as Japanese, and do not appear to obey attachment prefer-
10
Schuler et al Parsing Using Human-Like Memory Constraints
ences predicted by a head-driven account (Kamide and Mitchell 1999), favoring a pre-
head attachment account, as a right-corner transform would predict.
4.1 Tree Transforms Using Rewrite Rules
The incomplete constituents used in the present model are defined in terms of tree
transforms, which consist of recursive operations that change tree structures into other
tree structures. These transforms are not cognitive processes?syntax in this model is
learned and used entirely as time-series probabilities over random variable values in
the memory store. The role of these transforms is as a means to associate sequences of
configurations of incomplete constituents in a memory store with linguistically familiar
phrase structure representations, such as those studied in competence models or found
in annotated corpora.
The transforms presented in this article will be defined in terms of destructive rewrite
rules applied iteratively to each constituent of a source tree, from leaves to root, and from
left to right among siblings, to derive a target tree. These rewrites are ordered; when
multiple rewrite rules apply to the same constituent, the later rewrites are applied to
the results of the earlier ones.9 For example, the rewrite
A0
. . . A1
?2 ?3
. . . ?
A0
. . . ?2 ?3 . . .
could be used to iteratively eliminate all binary-branching nonterminal nodes in a tree,
except the root.
In the notation used in this article,
 Roman uppercase letters (Ai) are variables matching constituent labels,
 Roman lowercase letters (ai) are variables matching terminal symbols,
 Greek lowercase letters (?i) are variables matching entire subtree structure,
 Roman letters followed by colons, followed by Greek letters (Ai:?i) are
variables matching the label and structure, respectively, of the same
subtree, and
 ellipses (. . . ) are taken to match zero or more subtree structures,
preserving the order of ellipses in cases where there are more than one (as
in the rewrite shown herein).
Many of the transforms used in this article are reversible, meaning that the result
of applying a transform to a tree, then applying the reverse of that transform to the
resulting tree, will be the original tree itself. In general, a transform can be reversed
if the direction of its rewrite rules is reversed, and if each constituent in a target tree
9 The appropriate analogy here is to a Unix sed script, made sensitive to the beginning and end brackets of
a constituent and those of its children.
11
Computational Linguistics Volume 36, Number 1
matches a unique rewrite rule in the reversed transform. The fact that not all rewrites
can be unambiguously matched to HHMM output means that parse accuracy must be
evaluated on partially-binarized gold-standard trees, in order to remove the effect of
this ambiguous matching from the evaluation. This will be discussed in greater detail
in Section 6.
4.2 Right-Corner Transform Using Rewrite Rules
Rewrite rules for the right-corner transform are shown here, first to flatten out right-
recursive structure,
A1
?1 A2
?2 A3
a3
?
A1
A1/A2
?1
A2/A3
?2
A3
a3
,
A1
?1 A2
A2/A3
?2
. . .
?
A1
A1/A2
?1
A2/A3
?2
. . .
then to replace it with left-recursive structure,
A1
A1/A2:?1 A2/A3
?2
?3 . . . ?
A1
A1/A3
A1/A2:?1 ?2
?3 . . .
Here, the first two rewrite rules are applied iteratively (bottom-up on the tree) to flatten
all right recursion, using incomplete constituents to record the original nonterminal
ordering. The second rule is then applied to generate left-recursive structure, preserving
this ordering. Note that the last rewrite leaves a unary branch at the leftmost child of
each flattened node. This preserves the simple category labels of nodes that correspond
directly to nodes in the original tree, so the original tree can be reconstructed when
the right-corner transform concatenates multiple right-recursive sequences into a single
left-recursive sequence.
An example of a right-corner transformed tree is shown in Figure 2(c). An important
property of this transform is that it is reversible. Rewrite rules for reversing a right-
corner transform are simply the converse of those shown here. The correctness of this
can be demonstrated by dividing a tree into maximal sequences of right-recursive
branches (that is, maximal sequences of adjacent right children). The first two ?flatten-
ing? rewrites of the right-corner transform, applied to any such sequence, will replace
the right-branching nonterminal nodes with a flat sequence of nodes labeled with
slash categories, which preserves the order of the nonterminal category symbols in the
original nodes. Reversing this rewrite will therefore generate the original sequence of
nonterminal nodes. The final rewrite similarly preserves the order of these nonterminal
symbols while grouping them from the left to the right, so reversing this rewrite will
reproduce the flattened tree.
12
Schuler et al Parsing Using Human-Like Memory Constraints
Figure 2
A sample phrase structure tree (a) as it appears in the Penn Treebank, (b) after it has been
binarized, and (c) after it has been right-corner transformed.
4.3 Mapping Trees to HHMMDerivations
Any tree can be mapped to an HHMM derivation by aligning the nonterminals with qdt
categories. First, it is necessary to define rightward depth d, right index position t, and
final (rightmost) child status f dt+1, for every nonterminal node A in a tree, where
 d is defined to be the number of right branches between node A and the
root,
13
Computational Linguistics Volume 36, Number 1
 t is defined to be the number of words beneath or to the left of node A, and
 f dt+1 is defined to be 0 if A is a left child, 1 if A is a unary child, and A if A is
right.
Any right-corner transformed tree can then be annotated with these values and rewrit-
ten to define labels and final-state values for every combination of d and t covered by
the tree. This is done using the rewrite rule
d, t,A0, 0
d, t,A1, 1
? d, t,A1, 1
to replace unary branches with f dt+1 flags, and
d, t,A0, f
d
t+1
d, t?,A1, f
d
t?+1 d+1, t,A2,A2
?
d, t,A0, f
d
t+1
d, t?1,A1, 0
d, t?+1,A1, 0
d, t?,A1, f
d
t?+1
d+1, t,A2,A2
to copy stacked-up left child constituents over multiple time steps, while lower-level
(right child) constituents are being recognized. The dashed line on the right side of the
rewrite rule represents the variable number of time steps for a stacked-up higher-level
constituent (as seen, for example, in time steps 4?7 at depth 1 in Figure 3). Coordinates
d, t ? D, and T that have f dt+1=1 are assigned label ???, and coordinates not covered by
the tree are assigned label ??? and f dt+1=1.
The resulting label and final-state values at each node now define a value of qdt
and f dt for each depth d and time step t of the HHMM (see Figure 3). Probabilities for
HHMMmodels ?Q-Expansion,d, ?Q-Transition,d, and ?F-Reduction,d can then be estimated from
these values directly. Like the right-corner transform, this mapping is reversible, so qdt
and f dt values can be taken from a hypothesized most likely sequence and mapped back
Figure 3
Sample tree from Figure 2 mapped to qdt variable positions of an HHMM at each stack depth d
(vertical) and time step t (horizontal). This tree uses only two levels of stack memory. Values for
final-state variables f dt are not shown. Note that the mapping transform omits some nonterminal
labels; labels for these nodes can be reconstructed from their children.
14
Schuler et al Parsing Using Human-Like Memory Constraints
to trees (which can then undergo the reverse of the right-corner transform to become
ordinary phrase structure trees). Inspection of this rewrite rule will reveal the reverse of
this transform simply involves deleting unary-branching sequences that differ only in
the value of t and restoring unary branches when f dt+1=1.
This alignment of right-corner transformed trees also has the interesting property
that the categories on the stack at any given time step represent a segmentation of the
input up to that time step. For example, in Figure 3 at t = 12 the stack contains a sentence
lacking a verb phrase: S/VP (strong demand for . . . bonds), followed by a verb projection
lacking a particle: VBN/PRT (propped).
4.4 Comparison with Left-Corner Transform
A right-corner transform is used in this study, rather than a left-corner transform,
mainly because the right-corner version coincides with an intuition about how incom-
plete constituents might be stored in human memory. Stacked-up constituents in the
right-corner form correspond to chunks of words that have been encountered, rather
than hypothesized goal constituents. Intuitively, in the right-corner view, after a sen-
tence has been recognized, the stack memory contains a complete sentential constituent
(and some associated referent). In the left corner view, on the other hand, the stackmem-
ory after a sentence has been recognized contains only the lower-rightmost constituent
in the corresponding phrase structure tree (see Figure 4). This is because a time-order
Figure 4
A left-corner transformed version of the tree (a) and memory store (b) from Figures 2 and 3.
15
Computational Linguistics Volume 36, Number 1
alignment of a left-corner tree to elements in a bounded memory store corresponds to
a top-down traversal of the tree, whereas a time-order alignment of a right-corner tree
to elements in a bounded memory store corresponds to a bottom-up traversal of the
tree. If referential semantics are assumed to be calculated in tandem (as suggested by
the Tanenhaus et al [1995] results), a top-down traversal through time requires some
effort to reconcile with the traditional compositional semantic notion that the meanings
of constituents are composed from the meanings of their parts (Frege 1892).
4.5 Comparison with CCG
The incomplete constituent categories generated in the right-corner transform have the
same form and much of the same meaning as non-constituent categories in a CCG
(Steedman 2000).10 Both CCG operations of forward function application:
A1  A1/A2 A2
and forward function composition:
A1/A3  A1/A2 A2/A3
appear in the branching structure of right-corner transformed trees. Nested operations
can also occur in CCG derivations:
A1/A2  A1/A2/A3 A3
as well as in right-corner transformed trees (using underscore delimiters to denote
sequences of constituent categories, described in Section 5.1):
A1/A2  A1/A3 A2 A3
There are also correlates of type-raising (unary branches introduced by the right-corner
transform operations described in Section 4):
A1/A2  A3
But, importantly, the right-corner transform generates no correlates to the CCG
operations of backward function application or composition:
A1  A2 A1\A2
A1\A3  A2\A3 A1\A2
This has two consequences. First, right-corner transform models do not introduce am-
biguity between type-raised forward and backward operations, as CCG derivations do.
Second, because leftward dependencies (as between a verb and its subject in English)
cannot be incorporated into lexical categories, right-corner transform models cannot be
taken to explicitly encode argument structure, as CCGs are. The right-corner transform
model described in this article is therefore perhaps better regarded as a performance
model of processing, given subcategorizations specified in some other grammar (such
as in this case the Treebank grammar), rather than a constraint on grammar itself.
10 In fact, one of the original motivations for CCG as a model of language was to minimize stack usage in
incremental processing (Ades and Steedman 1982).
16
Schuler et al Parsing Using Human-Like Memory Constraints
4.6 Comparison with Cascaded FSAs in Information Extraction
Hierarchies of weighted finite-state automata (FSA)?equivalent HMMs may also be
viewed as probabilistic implementations of cascaded FSAs, used for modeling syntax
in information extraction systems such as FASTUS (Hobbs et al 1996). Indeed, the left-
branching sequences of transformed constituents recognized by this model (as shown
in Figure 3) bear a strong resemblance to the flattened phrase structure representations
recognized by cascaded FSA systems, in that most phrases are consolidated to flat
sequences at one hierarchy level. This flat structure is desirable in cascaded FSA systems
because it allows information to be extracted from noun or verb phrases using straight-
forward pattern matching rules, implemented as FSA-equivalent regular expressions.
Like FASTUS, this system produces layers of flat phrases that can be searched
using regular expression pattern-matching rules. It also has a fixed number of levels
and linear-time recognition complexity. But unlike FASTUS, the model described here
can produce?and can be trained on?complete phrase structure trees (accessible by
reversing the transforms described previously).
5. Coverage
The coverage of this model was evaluated on the large Penn Treebank corpus of
syntactically annotated sentences from the Switchboard corpus of transcribed speech
(Godfrey, Holliman, and McDaniel 1992) and the Wall Street Journal (Marcus, Santorini,
and Marcinkiewicz 1993). These sentences were right-corner transformed and mapped
to a time-aligned bounded memory store as described in Section 4 to determine the
amount of memory each sentence would require.
5.1 Binary Branching Structure
In order to obtain a linguistically plausible right-corner transform representation of
incomplete constituents, the corpus is subjected to another pre-process transform to
introduce binary-branching nonterminal projections, and fold empty categories into
nonterminal symbols in amanner similar to that proposed by Johnson (1998b) and Klein
and Manning (2003). This binarization is done in such a way as to preserve linguistic
intuitions of head projection, so that the depth requirements of right-corner transformed
trees will be reasonable approximations to the working memory requirements of a
human reader or listener.
First, ordinary phrases and clauses are decomposed into head projections, each
consisting of one subordinate head projection and one argument or modifier, for
example:
A0
. . . VB:?1 NP:?2 . . .
?
A0
. . . VB
VB:?1 NP:?2
. . .
The selection of head constituents is done using rewrite rules similar to the Magerman-
Black head rules (Magerman 1995). Any new constituent created by this process is
17
Computational Linguistics Volume 36, Number 1
assigned the label of the subordinate head projection. The subordinate projection may
be the left or complete list of head-projection rewrite rules is provided in Appendix A.11
Conjunctions are decomposed into purely right-branching structures using non-
terminals appended with a ?-LIST? suffix:
A0
. . . A1:?1 CC A1:?2
?
A0
. . . A1-LIST
A1:?1 CC A1:?2
A0
. . . A1:?1 A1-LIST:?2
?
A0
. . . A1-LIST
A1:?1 A1-LIST:?2
This right-branching decomposition of conjoined lists is motivated by the general
preference in English toward right branching structure, and the distinction of right
children as ?-LIST? categories is motivated by the asymmetry of conjunctions such as
and and or generally occurring only between constituents at the end of a list, not at the
beginning. (Thus, in decomposing coffee, tea or milk, the words tea or milk form an NP-
LIST constituent, whereas the words coffee, tea do not.)
Empty constituents are removed outright, along with any unary projections that
may arise from this removal. In the case of empty constituents representing traces, the
extracted category label is annotated onto the lowest nonterminal dominating the trace
using the suffix ?-extrX,? where ?X? is the category of the extracted constituent. To
preserve grammaticality, this annotation is then passed up the tree and eliminated when
awh-, topicalized, or othermoved constituent is encountered, in amanner similar to that
used in Head-driven Phrase Structure Grammar (Pollard and Sag 1994), but this does
not affect branching structure.
Together these rewrites remove about 65% of super-binary branches from the un-
processed Treebank. All remaining super-binary branches are ?nominally? decomposed
into right-branching structures by introducing intermediate nodes, each with a label
concatenated from the labels of its children, delimited by underscores:
A0
. . . A1:?1 A2:?2
?
A0
. . . A1 A2
A1:?1 A2:?2
This decomposition is ?nominal? in that the concatenated labels leave the resulting bi-
nary branches just as complex as the original n-ary branches prior to this decomposition.
It is equivalent to leaving super-binary branches intact and using dot rules in parsing
11 Although it is possible that in some cases these rules may generate counterintuitive branching patterns,
inspection of transformed trees during this experiment showed no unusual branching structure, except in
the case of noun sequences in base noun phrases (e.g. [general obligation] bonds or general [obligation
bonds]), which were left flat in the Treebank. Correct binarization of these structures would require
extensive annotator effort. However, because base noun phrases are often very small, and seldom contain
any sub-structure, it seems safe to assume that structural errors in these base noun phrases would not
drastically alter coverage results reported in this section.
18
Schuler et al Parsing Using Human-Like Memory Constraints
(Earley 1970). This decomposition therefore does nothing to reduce sparse data effects
in statistical parsing.
5.2 Coverage Results
Sections 2 and 3 (the standard training set) of the Penn Treebank Switchboard corpus
were binarized as described in Section 5.1, then right-corner transformed and mapped
to elements in a boundedmemory store as described in Section 4. Punctuation added by
transcribers was removed. Coverage of this corpus, in sentences, for a recognizer using
right-corner transform chunking with one to five levels of stack memory, is shown in
Table 1. These results show that a simple syntax-based chunking into incomplete con-
stituents, using the right-corner transform defined in Section 4 of this article, allows a
vast majority of Switchboard sentences (over 99%) to be recognized using three or fewer
elements of memory, with no sentences requiring more than five elements, essentially
as predicted by studies of human short-term memory.
Although spontaneous speech is arguably more natural test data than prepared
speech or edited text, it is possible that coverage results on these data may under-
estimate processing requirements, due to the preponderance of very short sentences
and sentence fragments in spontaneous speech (for example, nearly 30% of sentences in
the Switchboard corpus are only one word long). It may also be argued that coverage
results on this corpus more accurately reflect the complexity of speech planning under
somewhat awkward social circumstances (being asked to start a conversation with
a stranger), which may be more cognitively demanding than recognition. For these
reasons, the right-corner transform chunking was also evaluated on Sections 2?21 (the
standard training set) of the Penn Treebank Wall Street Journal (WSJ) text corpus (see
Table 2, column 1).
The WSJ text corpus results appear to show substantially higher memory
requirements than Switchboard, with only 93% of sentences recognizable using three or
fewer memory elements. But much of this increase is due to arguably arbitrary treebank
conventions in annotating punctuation (for example, commas between phrases are
attached to the leftmost phrase: ([Pierre Vinken . . . [61 years old] ,] joined . . . ) which
can lead to psycholinguistically implausible analyses in which phrases (in this case
61 years old) are center-embedded by lone punctuation marks on one side or the other.
In general, branching structure for punctuation can be difficult to motivate on linguistic
grounds, because punctuation marks do not have lexical projections or argument
structure in most linguistic theories. In spoken language, punctuation corresponds to
Table 1
Percent coverage of right-corner transformed Switchboard Treebank Sections 2?3.
memory capacity (right-corner, no punct) sentences coverage
no stack memory 26,201 28.38%
1 stack element 53,740 58.21%
2 stack elements 85,068 92.14%
3 stack elements 91,890 99.53%
4 stack elements 92,315 99.99%
5 stack elements 92,328 100.00%
TOTAL 92,328 100.00%
19
Computational Linguistics Volume 36, Number 1
Table 2
Percent coverage of left- and right-corner transformed WSJ Treebank Sections 2?21.
memory capacity right-corner, with punct right-corner, no punct left-corner, no punct
sentences coverage sentences coverage sentences coverage
no stack elements 35 0.09% 127 0.32% 127 0.32%
1 stack elements 3,021 7.57% 3,550 8.90% 4,284 10.74%
2 stack elements 21,916 54.95% 25,948 65.06% 26,750 67.07%
3 stack elements 37,203 93.28% 38,948 97.66% 38,853 97.42%
4 stack elements 39,703 99.54% 39,866 99.96% 39,854 99.93%
5 stack elements 39,873 99.97% 39,883 100.00% 39,883 100.00%
6 stack elements 39,883 100.00% - - - -
TOTAL 39,883 100.00% 39,883 100.00% 39,883 100.00%
pauses or patterns of inflection, distributed throughout an utterance. It therefore seems
questionable to account for punctuation marks in a psycholinguistic model as explicit
composable concepts in a memory store. In order to counter possible undesirable
effects of an arbitrary branching analysis of punctuation, a second evaluation of the
model was performed on a version of the WSJ corpus with punctuation removed.
An analysis (Table 2, column 2) of the Penn Treebank WSJ corpus Sections 2?21
without punctuation, using the right-corner transformed trees just described, shows
that 97.66% of trees can be recognized using three hidden levels, and 99.96% can be
recognized using four, and again (similar to the Switchboard results), no sentences
require more than five remembered incomplete constituents. Table 2, column 3, shows
similar results for a left-corner transformed corpus, using left-right reflections of the
rewrite rules presented in Section 4.
Cowan (2001) documents empirically observed short-term memory limits of about
four elements across awide variety of tasks. It is therefore not surprising to find a similar
limit in the memory required to parse the Treebank, assuming elements corresponding
to right-corner-transformed incomplete constituents.
As the table shows, some quintuply center-embedded constituents were found in
both corpora, suggesting that a three- to four-element limit may be soft, and can be
relaxed for short durations. Indeed, all quintuply embedded constituents were only a
few words long. Interestingly, many of the most heavily embedded words seemed to
strongly co-occur, which may suggest that these words arise from fixed expressions and
are not compositional. For example, Figure 5 shows one of the 13 phrase structure trees
in the Switchboard corpus which require five stack elements in right-corner parsing.
The complete sentence is:
So if there?s no one else around and I have a chance to listen to something I?ll turn that on.
If the construction there ?s NP AP in this sentence is parsed non-compositionally as a
single expression (and thus is rendered left-branching by the right-corner transform as
defined in Section 4), the sentence could be parsed using only four memory elements.
Even constrained to only four center embeddings, the existence of such sentences
confounds explanations of the center-embedding difficulties as directly arising from
stack limits in a left-corner (or right-corner) parser (Abney and Johnson 1991). It is
also interesting to note that three of the incomplete constituents in this example are
recursively nested or self-embedded instances of sentential projections, essentially with
20
Schuler et al Parsing Using Human-Like Memory Constraints
Figure 5
A phrase structure tree requiring five stack elements. Categories in bold will be incomplete at a
point after recognizing so if there?s no . . .
the same category, similar to the center-embedded constructions which human readers
found difficult to process. This suggests that restrictions on self-embedding of identical
constituent categories would also fail to predict readability.
Instead, these data seem to argue in favor of an explanation due to probability:
Although the five-element sentences found in the Treebank use mostly common phrase
structure rules, problematic center-embedded sentences like the salmon the man the dog
chased smoked fell may cause difficulty simply because they are examples of an unusual
construction: a nested object relative clause. The fact that this is an unusual construction
may in turn be a result of the fact that speakers tend to avoid nesting object relative
clauses because they can lead to memory exhaustion, though such constructions may
become readable with practice.
6. In-Element Composition Ambiguity and Parsing Accuracy
The right-corner transform described in Section 4 saves memory because it transforms
any right-branching sequence with left-child subtrees into a left-branching sequence of
incomplete constituents, with the same sequence of subtrees as right children. The left-
branching sequences of siblings resulting from this transform can then be composed
bottom-up through time by replacing each left child category with the category of the
resulting parent, within the same memory element (or depth level). For example, in
Figure 6(a) a left-child category NP/NP at time t = 4 is composed with a noun new of
category NP/NNP (a noun phrase lacking a proper noun yet to come), resulting in a
new parent category NP/NNP at time t = 5 replacing the left child category NP/NP in
the topmost d = 1 memory element.
This in-element composition preserves elements of the bounded memory store for
use in processing descendants of this composed constituent, yielding the human-like
memory demands reported in Section 5. But whenever an in-element composition like
this is hypothesized, it isolates an intermediate constituent (in this example, the noun
phrase new york city) from subsequent composition. Allowing access to this intermediate
constituent?for example, to allow new york city to become a modifier of bonds, which
itself becomes an argument of for?requires an analysis in which the intermediate
21
Computational Linguistics Volume 36, Number 1
Figure 6
Alternative analyses of strong demand for new york city ...: (a) using in-element composition,
compatible with strong demand for new york city is ... (in which the demand is for the city); and (b)
using cross-element (or delayed) composition, compatible with either strong demand for new york
city is ... (in which the demand is for the city) or strong demand for new york city bonds is ... (in
which a forthcoming referent?in this case, bonds?is associated with the city, and is in
demand). In-element composition (a) saves memory but closes off access to the noun phrase
headed by city, and so is not incompatible with the ...bonds completion. Cross-element
composition (b) requires more memory, but allows access to the noun phrase headed by city, so
is compatible with either completion. This ambiguity is introduced at t = 4 and propagated until
at least t = 7. An ordinary, non-right-corner stack machine would exclusively use analysis (b),
avoiding ambiguity.
constituent is stored in a separate memory element, shown in Figure 6(b). This creates
a local ambiguity in the parser (in this case, from time step t = 4) that may have to be
propagated across several words before it can be resolved (in this case, at time step
t = 7). This is essentially an ambiguity between arc-eager (in-element) and arc-standard
(cross-element) composition strategies, as described by Abney and Johnson (1991). In
contrast, an ordinary (purely arc-standard) parser with an unbounded stack would only
hypothesize analysis (b), avoiding this ambiguity.12
The right-corner HHMM approach described in this article relies on a learned
statistical model to predict when in-element (arc-eager) compositions will occur, in
addition to hypothesizing parse trees. The model encodes a mixed strategy: with some
probability arc-eager or arc-standard for each possible expansion. Accuracy results on
a right-corner HHMM model trained on the Penn Wall Street Journal Treebank suggest
that this kind of optionally arc-eager strategy can be reliably statistically learned.
6.1 Evaluation
In order to determinewhether amemory-preserving parsing strategy, like the optionally
arc-eager strategy, can be reliably learned, a baseline Cocke-Kasami-Younger (CKY)
parser and bounded-memory right-corner HHMM parser were evaluated on the stan-
dard Penn Treebank WSJ Section 23 parsing task, using the binarized tree set described
in Section 5.2 (WSJ Sections 2?21) as training data. Training examples requiring more
12 It is important to note that neither the right-corner nor left-corner parsing strategy by itself creates this
ambiguity. The ambiguity arises from the decision to use this optionally arc-eager strategy to reduce
memory store allocation in a bounded memory parser. Implementations of left-corner parsers such as
that of Henderson (2004) adopt an arc-standard strategy, essentially always choosing analysis (b), and
thus do not introduce this kind of local ambiguity. But in adopting this strategy, such parsers must
maintain a stack memory of unbounded size, and thus are not attractive as models of human parsing in
short-term memory (Resnik 1992).
22
Schuler et al Parsing Using Human-Like Memory Constraints
than four stack elements were excluded from training, in order to avoid generating
inconsistent model probabilities (e.g., from expansions that could not be re-composed
within the bounded memory store).
Most likely sequences of HHMM stack configurations are evaluated by reversing
the binarization, right-corner, and time-series mapping transforms described in Sec-
tions 4 and 5. But some of the binarization rewrites cannot be completely reversed,
because they cannot be unambiguously matched to output trees. Automatically derived
lexical projections below the annotated phrase level (e.g., binarizations of base noun
phrases) can be completely reversed, because the derived categories are character-
istically labeled with terminal symbols. So, too, can the conjunction and ?nominal?
binarizations described in Section 5.1, because they can be identified by characteristic
?-LIST? and underscore delimiters. But automatically derived projections above the
annotated phrase level cannot be reliably identified in parser output (for example, an
intermediate projection ?S PP S?may or may not be annotated in the corpus). In order
to isolate the evaluation from the effects of these ambiguous matchings, the evaluation
was performed using trees in a partially binarized format, obtained by reversing only
those rewrites that result in unambiguous matches. Evaluating on this partially bina-
rized data does not seem to unfairly increase parsing performance compared to other
published results?quite the contrary: an evaluation using the state-of-the-art Charniak
(2000) parser scores about half a point worse on labeled F-score (89.3% vs. 89.9%) when
its hypotheses and gold standard trees are converted into this format.13
Both CKY baseline and HHMM test systems were run with a simple part of speech
(POS) model using relative frequency estimates from the training set, backed off to a
discriminative (decision tree) model conditioned on the last five letters of each word,
normalized over unigram POS probabilities. The CKY baseline andHHMMresults were
obtained by training and evaluating on binarized trees, which is a necessary condition
for the right-corner transform. The CKY baseline results appear to be better than those
for a baseline probabilistic context-free grammar (PCFG) system reported by Klein and
Manning (2003) using no modifications to the corpus, and no parent or sibling condi-
tioning (see Table 3, top) because the binarization process allows the parser to avoid
some sparse data effects due to large flat branching structures in the Treebank, resulting
in improved parsing accuracy. Klein and Manning note that applying linguistically
motivated binarization transforms can yield substantial improvements in accuracy?as
much as nine points, in their study (in comparison, binarization only seems to improve
accuracy by about seven points above an unmodified baseline in the present study). But
the Klein and Manning results for binarization are provided only for models already
augmented with Markov dependencies (that is, conditioning on parent and sibling
categories, analogous to HHMM dependencies), so it was not possible to compare to
a binarized and un-Markovized benchmark.
The results for HHMM parsing, training, and evaluating on these same binarized
trees (modulo right-corner and variable-mapping transforms) were substantially bet-
ter than binarized CKY, most likely due to the expanded HHMM dependencies on
previous (qdt?1) and parent (q
d?1
t ) variables at each q
d
t . For example, binarized PCFG
probabilities may be defined in terms of three category symbols A, B, and C: P(A 
B C |A); whereas some of the HHMM probabilities are defined in terms of five category
13 This is presumably because the probability that a human annotator will annotate phrase structure
brackets at a particular projection or not is something existing parsers learn and exploit to improve their
accuracy. But it is not clear that this distinction is linguistically motivated.
23
Computational Linguistics Volume 36, Number 1
Table 3
Labeled recall (LR), labeled precision (LP), weighted average (F-score), and parse failure
(% of sentences yielding no tree output) results for basic CKY parser and HHMM parser on
unmodified and binarized WSJ Sections 22 (sentences 1?393: ?devset?) and 23?24 (all sentences).
Results are shown with and without punctuation, compared to Klein and Manning 2003
(KM?03) using baseline and parent+sibling (par+sib) conditioning, and Roark 2001 (R?01) using
parent+sibling conditioning. Baseline CKY and test (parent+sibling) cases for the HHMM
system start out at a higher accuracy than for the Klein-Manning system because the HHMM
system requires binarization of trees, which removes some data sparsity in the raw Treebank
annotation, whereas the Klein-Manning results are computed prior to binarization. Because it is
incremental, the parser occasionally eliminates all continuable analyses from the beam, and
therefore fails to find a parse. HHMM parse failures are accounted as zeros in the recall statistics,
but are also listed separately, because in principle it might be possible to recover useful syntactic
structure from partial sequences.
with punctuation: (?40 wds) LR LP F-score sentence error
failure reduction
KM?03: unmodified, devset ? ? 72.6 0
KM?03: par+sib, devset ? ? 77.4 0 17.5%
CKY: binarized, devset 80.3 79.9 80.1 0.8
HHMM: par+sib, devset 84.1 83.5 83.8 0.5 18.6%
CKY: binarized, sect 23 78.8 79.4 79.1 0.1
HHMM: par+sib, sect 23 83.4 83.7 83.5 0.1 21.1%
no punctuation: (?120 wds) LR LP F fail
R?01: par+sib, sect 23?24 75.2 77.4 ? 0.1
HHMM: par+sib, sect 23?24 77.2 78.3 77.7 0.0
labels: P(A/B |C/D, E) (transitioning from incomplete constituent C/D to incomplete
constituent A/B in the context of an expanding category E). This increases the number
of free parameters (estimated conditional probabilities) in the model,14 but apparently
not to the point of sparsity; this is similar to the effect of horizontal Markovization (con-
ditioning on the sibling category immediately previous to an expanded category) and
vertical Markovization (conditioning on the parent of an expanded category) commonly
used in PCFG parsing models (Collins 1999).
The improvement due to HHMM parsing over the PCFG baseline (18.6% reduction
in error) is comparable to that reported by Klein and Manning for parent and sibling
dependencies (first-order vertical and horizontal Markovization) over a baseline PCFG
without binarization (17.5% reduction in error). However, because it is not possible
to run the HHMM parser without binarization, and because Klein and Manning do
not report results for binarization transforms in the absence of parent and sibling
Markovization, it is potentially misleading to compare the results directly. For example,
it is possible that the binarization transforms described here may have performance-
optimizing effects that are latent in the binarized PCFG, but are brought out in HHMM
parsing.
Results on Section 23 of this corpus show close to 84% recall and precision, compa-
rable to that reported for state-of-the-art cubic-time parsers (with no constant bounds
14 Without punctuation, the HHMMmodel has 50,429 free parameters (including both Q and F models),
whereas the binarized PCFG has 12,373.
24
Schuler et al Parsing Using Human-Like Memory Constraints
on processing storage) using similar configurations of conditioning information, that is,
without lexicalization or smoothing.
Roark (2001) describes a similar incremental parser based on left-corner trans-
formed grammars, and also reports results for parsing with and without parent and
sibling Markovization. Again the performance is comparable under similar conditions
(Table 3, bottom).
This system was run with a beam width of 2,000 hypotheses. This beam width
was selected in order to compare the performance of the bounded-memory model,
which predicts in-element or cross-element composition, with that of conventional
broad-coverage parsers, which also maintain large beams. With better modeling and
vastly more data from which to learn, it is possible that the human processor may
need to maintain far fewer alternative analyses, or perhaps only one, conditioned on
a lookahead window of observations (Henderson 2004).15
These experiments used a maximum stack depth of four, and conditioned expan-
sion and transition probabilities for each qdt on only the portion of the parent category
following the slash (that is, only A2 of A1/A2), in order to avoid sparse data effects.
Examples requiring more than four stack elements were excluded from training. This
is because in the basic relative frequency estimation used here, training examples are
depth-specific. Because the (unpunctuated) training set contains only about a dozen
sentences requiring more than four depth levels, each occupying that level for only a
few words, the data on which the fifth level of this model would be trained are very
sparse. Models at greater stack depths, and models depending on complete parent cate-
gories (or grandparent categories, etc., as in state-of-the-art parsers) could be developed
using smoothing and backoff techniques or feature-based log-linear models, but this is
left for later work (see Section 7).
7. Conclusion
This article has described a model of human syntactic processing that recognizes com-
plete phrase structure trees using only a small store of memory elements of limited
complexity. Sequences of hypothesized contents of this memory store can be mapped to
and from conventional phrase structure trees using a reversible right-corner transform.
If this syntactic processing model is combined with a bounded-memory interpreter
(Schuler, Wu, and Schwartz 2009), however, allowing the contents of this store to be
incrementally interpreted within the same bounded memory, it stands to reason that
complete, explicit phrase structure trees would not need to be constructed at any time
in processing, in keeping with experimental results showing similar lack of retention of
words and syntactic structure during human processing (Sachs 1967; Jarvella 1971).
Initial results show the use of a memory store consisting of only three to four mem-
ory elements within this framework provides nearly complete coverage of the Penn
Treebank Switchboard and WSJ corpora, consistent with recent estimates of general-
purpose short-term memory capacity. This suggests that, unlike some earlier mod-
els, the hypothesis that human sentence processing uses general-purpose short-term
15 Although, if most competing analyses are unconscious, they would be difficult to detect. Formally, the
competing pockets of activation hypothesized in a parallel-processing version of this model could be
arbitrarily small and numerous, but it seems unlikely that very small pockets of activation would persist
for very long (just as low probability analyses would be unlikely to remain on the HHMM beam). This
possibility is discussed in the particle filter account of Levy (2008).
25
Computational Linguistics Volume 36, Number 1
memory to store incomplete constituents, as defined by a right-corner transform, does
not seem to substantially underestimate human processing capacity. Moreover, despite
additional predictions that must take place within this model to manage parsing in such
close quarters, preliminary accuracy results for an unlexicalized, un-smoothed version
of this model, using only a four-element memory store, show close to 84% recall and
precision on the standard parsing evaluation. This result is comparable to that reported
for state-of-the-art cubic-time parsers (with no constant bounds on processing storage)
using similar configurations of conditioning information, namely, without lexicalization
or smoothing.
This model does not attempt to derive processing difficulties frommemory bounds,
following evidence that garden path and center-embedding processing difficulties are
caused by interference or local probability estimates rather than encounters with mem-
ory capacity limits. But this does not mean that memory store capacity and probabilistic
explanations of processing difficulty are completely independent. Probability estima-
tion seems likely to be dependent on structural information from the memory store (for
example, incomplete object relative clauses seem to be very improbable in the context
of other incomplete object relative clauses). As hypotheses use more elements in the
memory store, the distribution over these hypotheses will tend to become broader,
taxing the reservoir of activation capacity, and making it more likely for low proba-
bility hypotheses to disappear, increasing the incidence of garden path errors. Further
investigations into how the memory store elements are allocated in various syntactic
contexts may allow these apparently disparate dimensions of processing capacity to be
unified.
The model described here may be promising as an engineering tool as well. But
to achieve competitive performance with unconstrained state-of-the-art parsers will
require the development of additional approximation algorithms beyond the scope of
this article. This is because most modern parsers are lexicalized, incorporating head-
word dependencies into parsing decisions, and employing finely tuned smoothing and
backoff techniques to integrate these potentially sparse head-word dependencies with
denser unlexicalized models. The bounded-memory right-corner HHMM described
in this article can also be lexicalized in this way, but because head word dependencies
are most straightforwardly defined in terms of top-down PCFG-like dependency
structures, this lexicalization requires the introduction of additional formal machinery
to transform PCFG probabilities into right-corner form (Schuler 2009). In other
words, rather than transforming a training set of trees and mapping them to a time
series model, it is necessary to transform a consistent probabilistically weighted
grammar (in some sense, an infinite set of trees) into appropriately weighted and
consistent right-corner PCFG and HHMM models. This requires the introduction of
an approximate inference algorithm, similar to that used in value iteration (Bellman
1957), which estimates probabilities of infinite left-recursive or right-recursive chains
by exploiting the fact that increasingly longer chains of events contribute exponentially
decreasing probability mass. On top of this, preserving head-word dependencies in
incremental processing also requires the introduction of a framework for storing head
words of modifier constituents that precede the head word of a parent constituent;
including some mechanism to ensure that probability assignments are fairly distributed
among competing hypotheses (e.g., by marginalizing over possible head words) in
cases where the calculation of accurate dependency probabilities must be deferred
until the head word of the parent constituent is encountered. For these reasons, a
complete lexicalized model is considered beyond the scope of this article, and is left for
future work.
26
Schuler et al Parsing Using Human-Like Memory Constraints
Appendix A: Head Transform Rules
The experiments described in this article used a binarization process that included the
following rewrite rules, designed to binarize flat Treebank constituents into linguisti-
cally motivated head projections:
1. NP: right-binarize basal NPs as much as possible; then left-binarize NPs
after left context reduced to nil:
A0=NP|WHNP
. . . A1=[A-Z]*:?1 A2=NN[A-Z]*:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=NP
A1=NN[A-Z]*|NP:?1 A2=PP|S|VP|WHSBAR:?2 . . .
?
A0
A1
A1:?1 A2:?2
. . .
2. VP: left-binarize basal VPs as much as possible; then right-binarize VPs
after right context reduced to nil:
A0=VP|SQ
. . . A1=VB[A-Z]*|BES:?1 A2=[A-Z]*:?2 . . .
?
A0
. . . A1
A1:?1 A2:?2
. . .
A0=VP
. . . A1=ADVP|RB[A-Z]*|PP:?1 A2=VB[A-Z]*|VP:?2
?
A0
. . . A2
A1:?1 A2:?2
3. ADJP: right-binarize basal ADJPs as much as possible; then left-binarize
ADJPs after left context reduced to nil:
A0=ADJP[A-Z]*
. . . A1=RB[A-Z]*:?1 A2=JJ[A-Z]*:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=ADJP
A1=JJ[A-Z]*|ADJP:?1 A2=PP|S:?2 . . .
?
A0
A1
A1:?1 A2:?2
. . .
4. ADVP: right-binarize basal ADVPs as much as possible; then left-binarize
ADVPs after left context reduced to nil:
A0=ADVP
. . . A1=RB[A-Z]*:?1 A2=RB[A-Z]*:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=ADVP
A1=RB[A-Z]*|ADVP:?1 A2=PP|S:?2 . . .
?
A0
A1
A1:?1 A2:?2
. . .
27
Computational Linguistics Volume 36, Number 1
5. PP: left-binarize PPs as much as possible; then right-binarize PPs after
right context reduced to nil:
A0=PP|SBAR
. . . A1=IN|TO:?1 A2=[A-Z]*:?2 . . .
?
A0
. . . A1
A1:?1 A2:?2
. . .
A0=PP
. . . A1=ADVP|RB|PP:?1 A2=PP:?2
?
A0
. . . A2
A1:?1 A2:?2
6. S: group subject NP and predicate VP of a sentence; then group modifiers
to right and left:
A0=S[A-Z]*
. . . A1=NP:?1 A2=VP:?2 . . .
?
A0
. . . S
A1:?1 A2:?2
. . .
A0=S[A-Z]*
. . . A1=ADVP|RB[A-Z]*|PP:?1 A2=VB[A-Z]*|VP:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=S[A-Z]*
. . . A1=ADVP|RB[A-Z]*|PP:?1 A2=A0:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=S[A-Z]*
. . . A1=A0:?1 A2=ADVP|RB[A-Z]*|PP:?2 . . .
?
A0
. . . A1
A1:?1 A2:?2
. . .
Acknowledgments
The authors would like to thank
the anonymous reviewers for their input.
This research was supported by National
Science Foundation CAREER/PECASE
award 0447685 and by NASA award
NNX08AC36A. The views expressed are not
necessarily endorsed by the sponsors.
References
Abney, Steven P. and Mark Johnson. 1991.
Memory requirements and local
ambiguities of parsing strategies.
J. Psycholinguistic Research, 20(3):233?250.
Ades, Anthony E. and Mark Steedman. 1982.
On the order of words. Linguistics and
Philosophy, 4:517?558.
Aho, Alfred V. and Jeffery D. Ullman. 1972.
The Theory of Parsing, Translation and
Compiling; Volume. I: Parsing. Prentice-Hall,
Englewood Cliffs, NJ.
Baker, James. 1975. The Dragon system: an
overview. IEEE Transactions on Acoustics,
Speech and Signal Processing, 23(1):24?29.
Bellman, Richard. 1957. Dynamic
Programming. Princeton University Press,
Princeton, NJ.
Berg, George. 1992. A connectionist parser
with recursive sentence structure and
lexical disambiguation. In Proceedings of the
Tenth National Conference on Artificial
Intelligence, pages 32?37, San Jose, CA.
Bever, Thomas G.?1970. The cognitive basis
for linguistic structure. In J. R?. Hayes,
editor, Cognition and the Development of
Language. Wiley, New York, pages 279?362.
Brown-Schmidt, Sarah, Ellen Campana, and
Michael K. Tanenhaus. 2002. Reference
resolution in the wild: Online
circumscription of referential domains in a
28
Schuler et al Parsing Using Human-Like Memory Constraints
natural interactive problem-solving task.
In Proceedings of the 24th Annual Meeting of
the Cognitive Science Society, pages 148?153,
Fairfax, VA.
Charniak, Eugene. 2000. A maximum-
entropy inspired parser. In Proceedings
of the First Meeting of the North American
Chapter of the Association for Computational
Linguistics (ANLP-NAACL?00),
pages 132?139, Seattle, WA.
Chomsky, Noam and George A. Miller.
1963. Introduction to the formal
analysis of natural languages.
In Handbook of Mathematical Psychology.
Wiley, New York, pages 269?321.
Collins, Michael. 1999. Head-driven
statistical models for natural language parsing.
Ph.D. thesis, University of Pennsylvania.
Cowan, Nelson. 2001. The magical
number 4 in short-term memory:
A reconsideration of mental storage
capacity. Behavioral and Brain Sciences,
24:87?185.
Crain, Stephen and Mark Steedman.
1985. On not being led up the garden path:
The use of context by the psychological
syntax processor. In D. R. Dowty,
L. Karttunen, and A. M. Zwicky, editors,
Natural Language Parsing: Psychological,
Computational, and Theoretical Perspectives,
number 1 in Studies in Natural Language
Processing. Cambridge University
Press, Cambridge, pages 320?358.
Earley, Jay. 1970. An efficient context-free
parsing algorithm. CACM, 13(2):94?102.
Elman, Jeffrey L. 1991. Distributed
representations, simple recurrent
networks, and grammatical structure.
Machine Learning, 7:195?225.
Ericsson, K. Anders and Walter Kintsch.
1995. Long-term working memory.
Psychological Review, 102:211?245.
Frege, Gottlob. 1892. Uber sinn
und bedeutung. Zeitschrift fur Philosophie
und Philosophischekritik, 100:25?50.
Gibson, Edward. 1991. A computational theory
of human linguistic processing: Memory
limitations and processing breakdown.
Ph.D. thesis, Carnegie Mellon University.
Godfrey, John J., Edward C. Holliman,
and Jane McDaniel. 1992. Switchboard:
Telephone speech corpus for research
and development. In Proceedings of
ICASSP, pages 517?520, San Francisco, CA.
Gordon, N. J., D. J. Salmond, and A. F. M.
Smith. 1993. Novel approach to nonlinear/
non-gaussian bayesian state estimation.
IEE Proceedings F (Radar and Signal
Processing), 140(2):107?113.
Gorrell, Paul. 1995. Syntax and Parsing.
Cambridge University Press, Cambridge.
Hale, John. 2001. A probabilistic earley parser
as a psycholinguistic model. In Proceedings
of the Second Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 159?166, Pittsburgh, PA.
Hale, John. 2006. Uncertainty about the
rest of the sentence. Cognitive Science,
30(4):609?642.
Helasvuo, Marja-Liisa. 2004. Shared
syntax: the grammar of co-constructions.
Journal of Pragmatics, 36:1315?1336.
Henderson, James. 2004. Lookahead
in deterministic left-corner parsing.
In Proceedings Workshop on Incremental
Parsing: Bringing Engineering and
Cognition Together, pages 26?33, Barcelona.
Hobbs, Jerry R., Douglas E. Appelt, John Bear,
David Israel, Megumi Kameyama, Mark
Stickel, and Mabry Tyson. 1996. Fastus:
A cascaded finite-state transducer for
extracting information from natural-
language text. In Yves Schabes, editor,
Finite State Devices for Natural Language
Processing. MIT Press, Cambridge, MA,
pages 383?406.
Jarvella, Robert J. 1971. Syntactic
processing of connected speech. Journal
of Verbal Learning and Verbal Behavior,
10:409?416.
Jelinek, Frederick, Lalit R. Bahl, and Robert L.
Mercer. 1975. Design of a linguistic
statistical decoder for the recognition
of continuous speech. IEEE Transactions
on Information Theory, 21:250?256.
Johnson, Mark. 1998a. Finite state
approximation of constraint-based
grammars using left-corner grammar
transforms. In Proceedings of COLING/ACL,
pages 619?623, Montreal.
Johnson, Mark. 1998b. PCFG models
of linguistic tree representation.
Computational Linguistics, 24:613?632.
Johnson-Laird, P. N. 1983. Mental Models:
Towards a Cognitive Science of Language,
Inference and Consciousness. Harvard
University Press, Cambridge, MA.
Just, Marcel Adam and Patricia A.
Carpenter. 1992. A capacity theory
of comprehension: Individual differences
in working memory. Psychological Review,
99:122?149.
Just, Marcel Adam and Sashank Varma.
2007. The organization of thinking:
What functional brain imaging
reveals about the neuroarchitecture of
complex cognition. Cognitive, Affective,
& Behavioral Neuroscience, 7:153?191.
29
Computational Linguistics Volume 36, Number 1
Kamide, Yuki and Don C. Mitchell. 1999.
Incremental pre-head attachment in
Japanese parsing. Language and
Cognitive Processes, 14:631?662.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo.
Lerner, Gene H. 1991. On the syntax of
sentences in progress. Language in
Society, 20:441?458.
Levy, Roger. 2008. Modeling the effects of
memory on human online sentence
processing with particle filters. In
Proceedings of NIPS, pages 937?944,
Vancouver.
Lewis, Richard L. and Shravan Vasishth.
2005. An activation-based model of
sentence processing as skilled
memory retrieval. Cognitive Science,
29(3):375?419.
Magerman, David. 1995. Statistical decision-
tree models for parsing. In Proceedings of the
33rd Annual Meeting of the Association
for Computational Linguistics (ACL?95),
pages 276?283, Cambridge, MA.
Marcus, Mitch. 1980. Theory of Syntactic
Recognition for Natural Language. MIT Press,
Cambridge, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: the
Penn Treebank. Computational Linguistics,
19(2):313?330.
Mayberry, III, Marshall R. and Risto
Miikkulainen. 2003. Incremental
nonmonotonic parsing through semantic
self-organization. In Proceedings of the
25th Annual Conference of the Cognitive
Science Society, pages 798?803,
Boston, MA.
Miller, George A. 1956. The magical number
seven, plus or minus two: Some limits
on our capacity for processing information.
Psychological Review, 63:81?97.
Murphy, Kevin P. and Mark A. Paskin. 2001.
Linear time inference in hierarchical
HMMs. In Proceedings of NIPS,
pages 833?840, Vancouver.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure
Grammar. Chicago: University of Chicago
Press and Stanford: CSLI Publications.
Pritchett, Bradley L. 1991. Head position
and parsing ambiguity. Journal of
Psycholinguistic Research, 20:251?270.
Resnik, Philip. 1992. Left-corner parsing and
psychological plausibility. In Proceedings
of COLING, pages 191?197, Nantes.
Roark, Brian. 2001. Probabilistic top-down
parsing and language modeling.
Computational Linguistics, 27(2):249?276.
Rohde, Douglas L. T. 2002. A connectionist
model of sentence comprehension and
production. Ph.D. thesis, Computer Science
Department, Carnegie Mellon University.
Sachs, Jacqueline. 1967. Recognition memory
for syntactic and semantic aspects of
connected discourse. Perception and
Psychophysics, 2:437?442.
Schuler, William. 2009. Parsing with a
bounded stack using a model-based
right-corner transform. In Proceedings
of the North American Association for
Computational Linguistics (NAACL ?09),
pages 344?352, Boulder, CO.
Schuler, William, Stephen Wu, and
Lane Schwartz. 2009. A framework for
fast incremental interpretation during
speech decoding. Computational
Linguistics, 35(3):313?343.
Shieber, Stuart. 1985. Evidence
against the context-freeness of natural
language. Linguistics and Philosophy,
8:333?343.
Smolensky, P. and G. Legendre. 2006. The
Harmonic Mind: From Neural Computation
to Optimality-Theoretic Grammar. MIT Press,
Cambridge, MA.
Steedman, Mark. 2000. The Syntactic
Process. MIT Press/Bradford Books,
Cambridge, MA.
Stevenson, Suzanne. 1998. Parsing as
incremental restructuring. In J. D. Fodor
and F. Ferreira, editors, Reanalysis in
Sentence Processing. Kluwer Academic,
Boston, MA, pages 327?363.
Tanenhaus, Michael K., Michael J.
Spivey-Knowlton, Kathy M. Eberhard,
and Julie E. Sedivy. 1995. Integration of
visual and linguistic information in
spoken language comprehension. Science,
268:1632?1634.
30
Proceedings of NAACL-HLT 2013, pages 95?105,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
An Analysis of Frequency- and Memory-Based Processing Costs
Marten van Schijndel
The Ohio State University
vanschm@ling.osu.edu
William Schuler
The Ohio State University
schuler@ling.osu.edu
Abstract
The frequency of words and syntactic con-
structions has been observed to have a sub-
stantial effect on language processing. This
begs the question of what causes certain con-
structions to be more or less frequent. A the-
ory of grounding (Phillips, 2010) would sug-
gest that cognitive limitations might cause lan-
guages to develop frequent constructions in
such a way as to avoid processing costs. This
paper studies how current theories of working
memory fit into theories of language process-
ing and what influence memory limitations
may have over reading times. Measures of
such limitations are evaluated on eye-tracking
data and the results are compared with predic-
tions made by different theories of processing.
1 Introduction
Frequency effects in language have been isolated
and observed in many studies (Trueswell, 1996;
Jurafsky, 1996; Hale, 2001; Demberg and Keller,
2008). These effects are important because they il-
luminate the ontogeny of language (how individual
speakers have acquired language), but they do not
answer questions about the phylogeny of language
(how the language came to its current form).
Phillips (2010) has hypothesized that grammar
rule probabilities may be grounded in memory lim-
itations. Increased delays in processing center-
embedded sentences as the number of embeddings
increases, for example, are often explained in terms
of a complexity cost associated with maintaining in-
complete dependencies in working memory (Gib-
son, 2000; Lewis and Vasishth, 2005). Other stud-
ies have shown a link between processing delays
and the low frequency of center-embedded construc-
tions like object relatives (Hale, 2001), but they
have not explored the source of this low frequency.
A grounding hypothesis would claim that the low
probability of generating such a structure may arise
from an associated memory load. In this account,
while these complexity costs may involve language-
specific concepts such as referent or argument link-
ing, the underlying explanation would be one of
memory limitations (Gibson, 2000) or neural acti-
vation (Lewis and Vasishth, 2005).
This paper seeks to explore the different predic-
tions made by these theories on a broad-coverage
corpus of eye-tracking data (Kennedy et al, 2003).
In addition, the current experiment seeks to isolate
memory effects from frequency effects in the same
task. The results show that memory load measures
are a significant factor even when frequency mea-
sures are residualized out.
The remainder of this paper is organized as fol-
lows: Sections 2 and 3 describe several frequency
and memory measures. Section 4 describes a proba-
bilistic hierarchic sequence model that allows all of
these measures to be directly computed. Section 5
describes how these measures were used to predict
reading time durations on the Dundee eye-tracking
corpus. Sections 6 and 7 present results and discuss.
2 Frequency Measures
2.1 Surprisal
One of the strongest predictors of processing com-
plexity is surprisal (Hale, 2001). It has been shown
in numerous studies to have a strong correlation
with reading time durations in eye-tracking and self-
paced reading studies when calculated with a variety
95
of models (Levy, 2008; Roark et al, 2009; Wu et al,
2010).
Surprisal predicts the integration difficulty that a
word xt at time step t presents given the preceding
context and is calculated as follows:
surprisal(xt) = ? log2
(
?
s?S(x1...xt) P (s)
?
s?S(x1...xt?1) P (s)
)
(1)
where S(x1 . . . xt) is the set of syntactic trees whose
leaves have x1 . . . xt as a prefix.1
In essence, surprisal measures how unexpected
constructions are in a given context. What it does
not provide is an explanation for why certain con-
structions would be less common and thus more sur-
prising.
2.2 Entropy Reduction
Processing difficulty can also be measured in terms
of entropy (Shannon, 1948). A larger entropy over a
random variable corresponds to greater uncertainty
over the observed value it will take. The entropy of
a syntactic derivation over the sequence x1 . . . xt is
calculated as:2
H(x1...t) =
?
s?S(x1...xt)
?P (s) ? log2 P (s) (2)
Reduction in entropy has been found to predict
processing complexity (Hale, 2003; Hale, 2006;
Roark et al, 2009; Wu et al, 2010; Hale, 2011):
?H(x1...t) = max(0, H(x1...t?1)?H(x1...t)) (3)
This measures the change in uncertainty about the
discourse as each new word is processed.
3 Memory Measures
3.1 Dependency Locality
In Dependency Locality Theory (DLT) (Gibson,
2000), complexity arises from intervening referents
introduced between a predicate and its argument.
Under the original formulation of DLT, there is a
1The parser in this study uses a beam. However, given high
parser accuracy, Roark (2001) showed that calculating com-
plexity metrics over a beam should obtain similar results to the
full complexity calculation.
2The incremental formulation used here was first proposed
in Wu et al (2010).
storage cost for each new referent introduced and an
integration cost for each referent intervening in a de-
pendency projection. This is a simplification made
for ease of computation, and subsequent work has
found DLT to be more accurate cross-linguistically
if the intervening elements are structurally defined
rather than defined in terms of referents (Kwon et
al., 2010). That is, simply having a particular ref-
erent intervene in a dependency projection may not
have as great an effect on processing complexity as
the syntactic construction the referent appears in.
Therefore, this work reinterprets the costs of depen-
dency locality to be related to the events of begin-
ning a center embedding (storage) and completing
a center embedding (integration). Note that anti-
locality effects (where longer dependencies are eas-
ier to process) have also been observed in some lan-
guages, and DLT is unable to account for these phe-
nomena (Vasishth and Lewis, 2006).
3.2 ACT-R
Processing complexity has also been attributed to
confusability (Lewis and Vasishth, 2005) as defined
in domain-general cognitive models like ACT-R
(Anderson et al, 2004).
ACT-R is based on theories of neural activation.
Each new word is encoded and stored in working
memory until it is retrieved at a later point for mod-
ification before being re-encoded into the parse. A
newly observed sign (word) associatively activates
any appropriate arguments from working memory,
so multiple similarly appropriate arguments would
slow processing as the parser must choose between
the highly activated hypotheses. Any intervening
signs (words or phrases) that modify a previously
encoded sign re-activate it and raise its resting acti-
vation potential. This can ease later retrieval of that
sign in what is termed an anti-locality effect, con-
tra predictions of DLT. In this way, returning out of
an embedded clause can actually speed processing
by having primed the retrieved sign before it was
needed. ACT-R attributes locality phenomena to fre-
quency effects (e.g. unusual constructions) overrid-
ing such priming and to activation decay if embed-
ded signs do not prime the target sign through mod-
ification (as in parentheticals). Finally, ACT-R pre-
dicts something like DLT?s storage cost due to the
need to differentiate each newly encoded sign from
96
SVP
NP
??NP
ND
the
V
bought
NP
N
studio
D
the
?
?
?
?
?
?
?
S/NP
}
NP/N
Figure 1: Two disjoint connected components of a phrase
structure tree for the sentence The studio bought the pub-
lisher?s rights, shown immediately prior to the word pub-
lisher.
those previously encoded (similarity-based encod-
ing interference) (Lewis et al, 2006).
3.3 Hierarchic Sequential Prediction
Current models of working memory in structured
tasks are defined in terms of hierarchies of sequen-
tial processes, in which superordinate sequences can
be interrupted by subordinate sequences and resume
when the subordinate sequences have concluded
(Botvinick, 2007). These models rely on temporal
cueing as well as content-based cueing to explain
how an interrupted sequence may be recalled for
continuation.
Temporal cueing is based on a context of temporal
features for the current state (Howard and Kahana,
2002). The temporal context in which the subor-
dinate sequence concludes must be similar enough
to the temporal context in which it was initiated to
recall where in the superordinate sequence the sub-
ordinate sequence occurred. For example, the act
of making breakfast may be interrupted by a phone
call. Once the call is complete, the temporal context
is sufficiently similar to when the call began that one
is able to continue preparing breakfast. The associ-
ation between the current temporal context and the
temporal context prior to the interruption is strong
enough to cue the next action.
Temporal cueing is complemented by sequential
(content-based) cueing (Botvinick, 2007) in which
the content of an individual element is associated
with, and thus cues, the following element. For ex-
ample, recalling the 20th note of a song is difficult,
but when playing the song, each note cues the fol-
lowing note, leading one to play the 20th note with-
out difficulty.
Hierarchic sequential prediction may be directly
applicable to processing syntactic center embed-
dings (van Schijndel et al, in press). An ongoing
parse may be viewed graph-theoretically as one or
more connected components of incomplete phrase
structure trees (see Figure 1). Beginning a new sub-
ordinate sequence (a center embedding) introduces
a new connected component, disjoint from that of
the superordinate sequence. As the subordinate se-
quence proceeds, the new component gains asso-
ciated discourse referents, each sequentially cued
from the last, until finally it merges with the super-
ordinate connected component at the end of the em-
bedded clause, forming a single connected compo-
nent representing the parse up to that point. Since
it is not connected to the subordinate connected
component prior to merging, the superordinate con-
nected component must be recalled through tempo-
ral cueing.
McElree (2001; 2006) has found that retrieval
of any non-focused (or in this case, unconnected)
element from memory leads to slower processing.
Therefore, integrating two disjoint connected com-
ponents should be expected to incur a processing
cost due to the need to recall the current state of the
superordinate sequence to continue the parse. Such
a cost would corroborate a DLT-like theory where
integration slows processing.
3.4 Dynamic Recruitment of Additional
Processing Resources
Language processing is typically centered in the left
hemisphere of the brain (for right-handed individ-
uals). Just and Varma (2007) provide fMRI re-
sults suggesting readers dynamically recruit addi-
tional processing resources such as the right-side ho-
mologues of the language processing areas of the
brain when processing center-embedded construc-
tions. Once an embedded construction terminates,
the reader may still have temporary access to these
extra processing resources, which may briefly speed
processing.
This hypothesis would, therefore, predict an en-
coding cost when a center embedding is initiated.
The resulting inhibition would trigger recruitment of
additional processing resources, which would then
97
allow the rest of the embedded structure to be pro-
cessed at the usual speed. Upon completing an em-
bedding, the difficulty arising frommemory retrieval
(McElree, 2001) would be ameliorated by these ex-
tra processing resources, and the reduced process-
ing complexity arising from reduced memory load
would yield a temporary facilitation in processing.
No longer requiring the additional resources to cope
with the increased embedding, the processor would
release them, returning the processor to its usual
speed. Unlike anti-locality, where processing is
facilitated in longer passages due to accumulating
probabilistic evidence, a model of dynamic recruit-
ment of additional processing resources would pre-
dict universal facilitation after a center embedding
of any length, modulo frequency effects.
3.5 Embedding Difference
Wu et al (2010) propose an explicit measure of
the difficulty associated with processing center-
embedded constructions, which is similar to the pre-
dictions of dynamic recruitment and is defined in
terms of changes in memory load. They calcu-
late a probabilistically-weighted average embedding
depth as follows:
?emb(x1 . . . xt) =
?
s?S(x1...xt)
d(s) ? P (s) (4)
where d(s) returns the embedding depth of the
derivation s at xt in a variant of a left-corner pars-
ing process.3 Embedding difference may then be de-
rived as:
EmbDiff (x1 . . . xt) =?emb(x1 . . . xt)? (5)
?emb(x1 . . . xt?1)
This is hypothesized to correlate positively with
processing load: increasing the embedding depth in-
creases processing load and decreasing it reduces
processing load. Note that embedding difference
makes the opposite prediction from DLT in that in-
tegrating an embedded clause is predicted to speed
processing. In fact, the predictions of embedding
3As pointed out by Wu et al (2010), in practice this can be
computed over a beam of potential parses in which case it must
be normalized by the total probability of the beam.
difference are such that it may be viewed as an im-
plementation of the predictions of a hierarchic se-
quential processing model with dynamic recruitment
of additional resources.
4 Model
This paper uses a hierarchic sequence model imple-
mentation of a left-corner parser variant (van Schijn-
del et al, in press), which represents connected com-
ponents of phrase structure trees in hierarchies of
hidden random variables. This requires, at each time
step t:
? a hierarchically-organized set of N connected
component states qnt , each consisting of an ac-
tive sign of category aqnt , and an awaited sign
of category bqnt , separated by a slash ?/?; and
? an observed word xt.
Each connected component state in this model then
represents a contiguous portion of a phrase structure
tree (see Figure 1 on preceding page).
The operations of this parser can be defined as a
deductive system (Shieber et al, 1995) with an input
sequence consisting of a top-level connected com-
ponent state ?/?, corresponding to an existing dis-
course context, followed by a sequence of observed
words x1, x2, . . . 4 If an observation xt can attach as
the awaited sign of the most recent (most subordi-
nate) connected component a/b, it is hypothesized
to do so, turning this incomplete sign into a com-
plete sign a (F?, below); or if the observation can
serve as a lower descendant of this awaited sign, it
is hypothesized to form the first complete sign a? in
a newly initiated connected component (F+):
a/b xt
a b ? xt (F?)
a/b xt
a/b a? b
+? a? ... ; a? ? xt (F+)
Then, if either of these complete signs (a or a?
above, matched to a?? below) can attach as an initial
4A deductive system consists of inferences or productions
of the form:
P
QR, meaning premise P entails conclusion Q ac-
cording to rule R.
98
?/? the
?/?, D F+
?/?, NP/N L? studio
?/?, NP F?
?/?, S/VP L? bought
?/?, S/VP, V F+
?/?, S/NP L+ the
?/?, S/NP, D F+
?/?, S/NP, NP/N L? publisher
?/?, S/NP, NP F?
?/?, S/NP, D/G L? ?s
?/?, S/NP, D F?
?/?, S/N L+ rights
?/?, S F?
?/? L+
Figure 2: Example parse (in the form of a deductive proof) of the sentence The studio bought the publisher?s rights,
using F+, F?, L+, and L? productions. Each pair of deductions combines a context of one or more connected compo-
nent states with a sign (word) observed in that context. By applying the F and L rules to the observed sign and context,
the parser is able to generate a consequent context. Initially, the context corresponds to a connected pre-sentential
dialogue state ?/?. When the is observed, the parser applies F+ to begin a new connected component state D. By
applying L?, the parser determines that this new connected component is unfinished and generates an appropriate
incomplete connected component state NP/N, encoding the superordinate state ?/? for later retrieval. Further on, the
parser observes ?s and uses F? to avoid generating a new connected component, which completes the sign D. The
parser follows this up with L+ to recall the superordinate connected component state S/NP and integrate it into the
most deeply embedded connected component, which results in a less deeply embedded structure.
child of the awaited sign of the immediately superor-
dinate connected component state a/b, it is hypoth-
esized to do so and terminate the subordinate con-
nected component state, with xt as the last observa-
tion of the terminated connected component (L+); or
if the observation can serve as a lower descendant of
this awaited sign, it is hypothesized to remain dis-
joint and form its own connected component (L?):
a/b a??
a/b?? b ? a
?? b?? (L+)
a/b a??
a/b a?/b?? b
+? a? ... ; a? ? a?? b?? (L?)
These operations can be made probabilistic. The
probability ? of a transition at time step t is defined
in terms of (i) a probability ? of initiating a new con-
nected component state with xt as its first observa-
tion, multiplied by (ii) the probability ? of terminat-
ing a connected component state with xt as its last
observation, multiplied by (iii) the probabilities ?
and ? of generating categories for active and awaited
signs aqnt and bqnt in the resulting most subordinate
connected component state qnt . This kind of model
can be defined directly on PCFG probabilities and
trained to produce state-of-the-art accuracy by using
the latent variable annotation of Petrov et al (2006)
(van Schijndel et al, in press).5
An example parse is shown in Figure 2. Since
two binary structural decisions (F and L) must be
made in order to generate each word, there are four
possible structures that may be generated (see Ta-
ble 1). The F+L? transition initiates a new level
of embedding at word xt and so requires the super-
ordinate state to be encoded for later retrieval (e.g.
on observing the in Figure 2). The F?L+ transi-
tion completes the deepest level of embedding and
therefore requires the recall of the current superor-
dinate connected component state with which the
5The model has been shown to achieve an F-score of 87.8,
within .2 points of the Petrov and Klein (2007) parser, which
obtains an F-score of 88.0 on the same task. Because the se-
quence model is defined over binary-branching phrase structure,
both parsers were evaluated on binary-branching phrase struc-
ture trees to provide a fair comparison.
99
F?L? Cue Active Sign
F+L? Initiate/Encode
F?L+ Terminate/Integrate
F+L+ Cue Awaited Sign
Table 1: The hierarchical structure decisions and the op-
erations they represent. F+L? initiates a new connected
component, F?L+ integrates two disjoint connected com-
ponents into a single connected component, and F?L?
and F+L+ sequentially cue, respectively, a new active
sign (along with an associated awaited sign) and a new
awaited sign from the most recent connected component.
subordinate connected component state will be in-
tegrated. For example, in Figure 2, upon observ-
ing ?s, the parser must use temporal cueing to re-
call that it is in the middle of processing an NP (to
complete an S), which sequentially cues a prediction
of N. F?L? transitions complete the awaited sign of
the most subordinate state and so sequentially cue
a following connected component state at the same
tier of the hierarchy. For example, in Figure 2, after
observing studio, the parser uses the completed NP
to sequentially cue the prediction that it has finished
the left child of an S. F+L+ transitions locally ex-
pand the awaited sign of the most subordinate state
and so should also not require any recall or encod-
ing. For example, in Figure 2, observing bought
while awaiting a VP sequentially cues a prediction
of NP.
F+L?, then, loosely corresponds to a storage ac-
tion under DLT as more hierarchic levels must now
be maintained at each future step of the parse. As
stated before, it differs from DLT in that it is sensi-
tive to the depth of embedding rather than a partic-
ular subset of syntactic categories. Wu et al (2010)
found that increasing the embedding depth led to
longer reading times in a self-paced reading experi-
ment. In ACT-R terms, F+L? corresponds to an en-
coding action, potentially causing processing diffi-
culty resulting from the similarity of the current sign
to previously encoded signs.
F?L+, by contrast, is similar to DLT?s integra-
tion action since a subordinate connected compo-
nent is integrated into the rest of the parse structure.
This represents a temporal cueing event in which
the awaited category of the superordinate connected
Theory F+L? F?L+
DLT positive positive
ACT-R positive positive
Hier. Sequential Prediction positive
Dynamic Recruitment positive negative
Embedding Difference positive negative
Table 2: Each theory?s prediction of the direction of
the correlation between each hierachical structure predic-
tor and reading times. Hierarchic sequential prediction
is agnostic about the processing speed of F+L? opera-
tions, and none of the theories make any predictions as to
the sign associated with the within-embedding measures
F?L? and F+L+.
component is recalled. In contrast to DLT, embed-
ding difference and dynamic recruitment would pre-
dict a shorter reading time in the F?L+ case be-
cause of the reduction in memory load. In an ACT-R
framework, reading time durations can increase at
the retrieval site because the retrieval causes compe-
tition among similarly encoded signs in the context
set. While it is possible for reading times to decrease
when completing a center embedding in ACT-R (Va-
sishth and Lewis, 2006), this would be expressed
as a frequency effect due to certain argument types
commonly foreshadowing their predicates (Jaeger et
al., 2008). Since frequency effects are factored sep-
arately from memory effects in this study, ACT-R
would predict longer residual (memory-based) read-
ing times when completing an embedding.
Predicted correlations to reading times for the F
and L transitions are summarized in Table 2.
5 Eye-tracking
Eye-tracking and reading time data are often used to
test complexity measures (Gibson, 2000; Demberg
and Keller, 2008; Roark et al, 2009) under the as-
sumption that readers slow down when reading more
complex passages. Readers saccade over portions of
text and regress back to preceding text in complex
patterns, but studies have correlated certain mea-
sures with certain processing constraints (see Clifton
et al 2007 for a review). For example, the initial
length of time fixated on a single word is correlated
with word identification time; whereas regression
durations after a word is fixated (but prior to a fix-
ation in a new region) are hypothesized to correlate
100
with integration difficulty.
Since this work focuses on incremental process-
ing, all processing that occurs up to a given point in
the sentence is of interest. Therefore, in this study,
predictions will be compared to go-past durations.
Go-past durations are calculated by summing all fix-
ations in a region of text, including regressions, un-
til a new region is fixated, which accounts for addi-
tional processing that may take place after initial lex-
ical access, but before the next region is processed.
For example, if one region ends at word 5 in a sen-
tence, and the next fixation lands on word 8, then the
go-past region consists of words 6-8 and the go-past
duration sums all fixations until a fixation occurs af-
ter word 8.
6 Evaluation
The measures presented in this paper were evaluated
on the Dundee eye-tracking corpus (Kennedy et al,
2003). The corpus consists of 2388 sentences of nat-
urally occurring news text written in standard British
English. The corpus also includes eye-tracking data
from 10 native English speakers, which provides
a test corpus of 260,124 subject-duration pairs of
reading time data. Of this, any fixated words ap-
pearing fewer than 5 times in the training data were
considered unknown and were filtered out to obtain
accurate predictions. Fixations on the first or last
words of a line were also filtered out to avoid any
?wrap-up? effects resulting from preparing to sac-
cade to the beginning of the next line or resulting
from orienting to a new line. Additionally, following
Demberg and Keller (2008), any fixations that skip
more than 4 words were attributed to track loss by
the eyetracker or lack of attention of the reader and
so were excluded from the analysis. This left the fi-
nal evaluation corpus with 151,331 subject-duration
pairs.
The evaluation consisted of fitting a linear mixed-
effects model (Baayen et al, 2008) to reading time
durations using the lmer function of the lme4 R
package (Bates et al, 2011; R Development Core
Team, 2010). This allowed by-subject and by-item
variation to be included in the initial regression as
random intercepts in addition to several baseline pre-
dictors.6 Before fitting, the durations extracted from
6Each fixed effect was centered to reduce collinearity.
the corpus were log-transformed, producing more
normally distributed data to obey the assumptions of
linear mixed effects models.7
Included among the fixed effects were the posi-
tion in the sentence that initiated the go-past region
(SENTPOS) and the number of characters in the ini-
tiating word (NRCHAR). The difficulty of integrat-
ing a word may be seen in whether the immediately
following word was fixated (NEXTISFIX), and sim-
ilarly if the immediately previous word was fixated
(PREVISFIX) the current word probably need not be
fixated for as long. Finally, unigram (LOGPROB)
and bigram probabilities are included. The bigram
probabilities are those of the current word given the
previous word (LOGFWPROB) and the current word
given the following word (LOGBWPROB). Fossum
and Levy (2012) showed that for n-gram probabili-
ties to be effective predictors on the Dundee corpus,
they must be calculated from a wide variety of texts,
so following them, this study used the Brown corpus
(Francis and Kucera, 1979), the WSJ Sections 02-21
(Marcus et al, 1993), the written text portion of the
British National Corpus (BNC Consortium, 2007),
and the Dundee corpus (Kennedy et al, 2003). This
amounted to an n-gram training corpus of roughly
87 million words. These statistics were smoothed
using the SRILM (Stolcke, 2002) implementation of
modified Kneser-Ney smoothing (Chen and Good-
man, 1998). Finally, total surprisal (SURP) was in-
cluded to account for frequency effects in the base-
line.
The preceding measures are commonly used in
baseline models to fit reading time data (Demberg
and Keller, 2008; Frank and Bod, 2011; Fossum and
Levy, 2012) and were calculated from the final word
of each go-past region. The following measures
create a more sophisticated baseline by accumulat-
ing over the entire go-past region to capture what
must be integrated into the discourse to continue the
parse. One factor (CWDELTA) simply counts the
number of words in each go-past region. Cumula-
7In particular, these models assume the noise in the data is
normally distributed. Initial exploratory trials showed that the
residuals of fitting any sensible baseline also become more nor-
mally distributed if the response variable is log-transformed. Fi-
nally, the directions of the effects remain the same whether or
not the reading times are log-transformed, though significance
cannot be ascertained without the transform.
101
tive total surprisal (CUMUSURP) and cumulative en-
tropy reduction (ENTRED) give the surprisal (Hale,
2001) and entropy reduction (Hale, 2003) summed
over the go-past region. To avoid convergence is-
sues, each of the cumulative measures is residual-
ized from the next simpler model in the following
order: CWDELTA from the standard baseline, CU-
MUSURP from the baseline with CWDELTA, and EN-
TRED from the baseline with all other effects.
Residualization was accomplished by using the
simpler mixed-effects model to fit the measure of in-
terest. The residuals from that model fit were then
used in place of the factor of interest. All joint inter-
actions were included in the baseline model as well.
Finally, to account for spillover effects (Just et al,
1982) where processing from a previous region con-
tributes to the following duration, the above baseline
predictors from the previous go-past region were in-
cluded as factors for the current region.
Having SURP as a predictor with CUMUSURP may
seem redundant, but initial analyses showed SURP
was a significant predictor over CUMUSURP when
CWDELTA was a separate factor in the baseline (cur-
rent: p = 2.2 ? 10?16 spillover: p = 2 ? 10?15)
and vice versa (current: p = 2.2 ? 10?16 spillover:
p = 6 ? 10?5). One reason for this could be that
go-past durations conflate complexity experienced
when initially fixating on a region with the difficulty
experienced during regressions. By including both
versions of surprisal, the model is able to account
for frequency effects occurring in both conditions.
This study is only interested in how well the pro-
posed memory-based measures fit the data over the
baseline, so to avoid fitting to the test data or weak-
ening the baseline by overfitting to training data, the
full baseline was used in the final evaluation.
Each measure proposed in this paper was summed
over go-past regions to make it cumulative and
was residualized from all non-spillover factors be-
fore being included on top of the full baseline as a
main effect. Likewise, the spillover version of each
proposed measure was residualized from the other
spillover factors before being included as a main ef-
fect. Only a single proposed measure (or its spillover
corrollary) was included in each model. The results
shown in Table 3 reflect the probability of the full
model fit being obtained by the model lacking each
factor of interest. This was found via posterior sam-
Factor Operation t-score p-value
F?L? Cue Active 0.60 0.55
F+L? Initiate 7.10 2.22?10?14
F?L+ Integrate -5.44 5.23?10?8
F+L+ Cue Awaited -1.55 0.12
Table 3: Significance of each of the structure generation
outcomes at predicting log-transformed durations when
added to the baseline as a main effect after being residu-
alized from it. The sign of the t-score indicates the direc-
tion of the correlation between the residualized factor and
go-past durations. Note that these factors are all based
on the current go-past region; the spillover corollaries of
these were not significant predictors of reading times.
pling of each factor using the Markov chain Monte
Carlo implementation of the languageR R package
(Baayen, 2008).
The results indicate that the F+L? and F?L+ mea-
sures were both significant predictors of duration as
expected. Further, F?L? and F+L+, which both sim-
ply reflect sequential cueing, were not significant
predictors of go-past duration, also as expected.
7 Discussion and Conclusion
The fact that F+L? was strongly predictive over the
baseline is encouraging as it suggests that memory
limitations could provide at least a partial explana-
tion of why certain constructions are less frequent in
corpora and thus yield a high surprisal. Moreover,
it indicates that the model corroborates the shared
prediction of most of the memory-based models that
initiating a new connected component slows pro-
cessing.
The fact that F?L+ is predictive but has a neg-
ative coefficient could be evidence of anti-locality,
or it could be an indication of some sort of pro-
cessing momentum due to dynamic recruitment of
additional processing resources (Just and Varma,
2007). Since anti-locality is an expectation-based
frequency effect, and since this study controlled for
frequency effects with n-grams, surprisal, and en-
tropy reduction, an anti-locality explanation would
rely on either (i) more precise variants of the met-
rics used in this study or (ii) other frequency metrics
altogether. Future work could investigate the possi-
bility of anti-locality by looking at the distance be-
tween an encoding operation and its corresponding
102
integration action to see if the integration facilita-
tion observed in this study is driven by longer em-
beddings or if there is simply a general facilitation
effect when completing embeddings.
The finding of a negative integration cost was pre-
viously observed by Wu et al (2010) as well as
Demberg and Keller (2008), although Demberg and
Keller calculated it using the original referent-based
definitions of Gibson (1998; 2000) and varied which
parts of speech counted for calculating integration
cost. Ultimately, Demberg and Keller (2008) con-
cluded that the negative coefficient was evidence
that integration cost was not a good broad-coverage
predictor of reading times; however, this study has
replicated the effect and showed it to be a very strong
predictor of reading times, albeit one that is corre-
lated with facilitation rather than inhibition.
It is interesting that many studies have found
negative integration cost using naturalistic stimuli
while others have consistently found positive inte-
gration cost when using constructed stimuli with
multiple center embeddings presented without con-
text (Gibson, 2000; Chen et al, 2005; Kwon et al,
2010). It may be the case that any dynamic re-
cruitment is overwhelmed by the memory demands
of multiply center-embedded stimuli. Alternatively,
it may be that the difficulty of processing multiply
center-embedded sentences containing ambiguities
produces anxiety in subjects, which slows process-
ing at implicit prosodic boundaries (Fodor, 2002;
Mitchell et al, 2008). In any case, the source of this
discrepancy presents an attractive target for future
research.
In general, sequential prediction does not seem
to present people with any special ease or difficulty
as evidenced by the lack of significance of F?L?
and F+L+ predictions when frequency effects are
factored out. This supports a theory of sequential,
content-based cueing (Botvinick, 2007) that predicts
that certain states would directly cue other states and
thus avoid recall difficulty. An example of this may
be seen in the case of a transitive verb triggering
the prediction of a direct object. This kind of cue-
ing would show up as a frequency effect predicted
by surprisal rather than as a memory-based cost,
due to frequent occurrences becoming ingrained as
a learned skill. Future work could use these sequen-
tial cueing operations to investigate further claims
of the dynamic recruitment hypothesis. One of the
implications of the hypothesis is that recruitment of
resources alleviates the initial encoding cost, which
allows the parser to continue on as before the em-
bedding. DLT, on the other hand, predicts that there
is a storage cost for maintaining unresolved depen-
dencies during a parse (Gibson, 2000). By weight-
ing each of the sequential cueing operations with the
embedding depth at which it occurs, an experiment
may be able to test these two predictions.
This study has shown that measures based on
working memory operations have strong predictivity
over other previously proposed measures including
those associated with frequency effects. This sug-
gests that memory limitations may provide a partial
explanation of what gives rise to frequency effects.
Lastly, this paper provides evidence that there is a
robust facilitation effect in English that arises from
completing center embeddings.
The hierarchic sequence model, all evaluation
scripts, and regression results for all baseline pre-
dictors used in this paper are freely available at
http://sourceforge.net/projects/modelblocks/.
Acknowledgements
Thanks to Peter Culicover, Micha Elsner, and three
anonymous reviewers for helpful suggestions. This
work was funded by an OSU Department of Lin-
guistics Targeted Investment for Excellence (TIE)
grant for collaborative interdisciplinary projects
conducted during the academic year 2012-13.
References
John R. Anderson, Dan Bothell, Michael D. Byrne,
S. Douglass, Christian Lebiere, and Y. Qin. 2004. An
integrated theory of the mind. Psychological Review,
111(4):1036?1060.
R. Harald Baayen, D. J. Davidson, and Douglas M. Bates.
2008. Mixed-effects modeling with crossed random
effects for subjects and items. Journal of Memory and
Language, 59:390?412.
R. Harald Baayen. 2008. Analyzing Linguistic Data:
A Practical Introduction to Statistics using R. Cam-
bridge University Press, New York, NY.
Douglas Bates, Martin Maechler, and Ben Bolker, 2011.
lme4: Linear mixed-effects models using S4 classes.
BNC Consortium. 2007. The british national corpus.
103
Matthew Botvinick. 2007. Multilevel structure in behav-
ior and in the brain: a computational model of Fuster?s
hierarchy. Philosophical Transactions of the Royal So-
ciety, Series B: Biological Sciences, 362:1615?1626.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University.
Evan Chen, Edward Gibson, and Florian Wolf. 2005.
Online syntactic storage costs in sentence comprehen-
sion. Journal of Memory and Language, 52(1):144?
169.
Charles Clifton, Adrian Staub, and Keith Rayner. 2007.
Eye movements in reading words and sentences. In
Eye movements: A window on mind and brain, pages
341?372. Elsevier.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
Janet Fodor. 2002. Prosodic disambiguation in silent
reading. In M. Hirotani, editor, In Proceedings of
NELS 32.
Victoria Fossum and Roger Levy. 2012. Sequential
vs. hierarchical syntactic models of human incremen-
tal sentence processing. In Proceedings of CMCL-
NAACL 2012. Association for Computational Linguis-
tics.
W. Nelson Francis and Henry Kucera. 1979. The brown
corpus: A standard corpus of present-day edited amer-
ican english.
Stefan Frank and Rens Bod. 2011. Insensitivity of
the human sentence-processing system to hierarchical
structure. Psychological Science.
Edward Gibson. 1998. Linguistic complexity: Locality
of syntactic dependencies. Cognition, 68(1):1?76.
Edward Gibson. 2000. The dependency locality theory:
A distance-based theory of linguistic complexity. In
Image, language, brain: Papers from the first mind ar-
ticulation project symposium, pages 95?126.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the second
meeting of the North American chapter of the Associ-
ation for Computational Linguistics, pages 159?166,
Pittsburgh, PA.
John Hale. 2003. Grammar, Uncertainty and Sen-
tence Processing. Ph.D. thesis, Cognitive Science,
The Johns Hopkins University.
John Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30(4):609?642.
John Hale. 2011. What a rational parser would do. Cog-
nitive Science, 35(3):399?443.
Marc W. Howard and Michael J. Kahana. 2002. A dis-
tributed representation of temporal context. Journal of
Mathematical Psychology, 45:269?299.
F. T. Jaeger, E. Fedorenko, P. Hofmeister, and E. Gib-
son. 2008. Expectation-based syntactic processing:
Antilocality outside of head-final languages. In The
21st CUNY Sentence Processing Conference.
Daniel Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science: A Multidisciplinary Journal, 20(2):137?194.
Marcel Adam Just and Sashank Varma. 2007. The or-
ganization of thinking: What functional brain imaging
reveals about the neuroarchitecture of complex cogni-
tion. Cognitive, Affective, & Behavioral Neuroscience,
7:153?191.
Marcel Adam Just, Patricia A. Carpenter, and Jacque-
line D. Woolley. 1982. Paradigms and processes in
reading comprehension. Journal of Experimental Psy-
chology: General, 111:228?238.
Alan Kennedy, James Pynte, and Robin Hill. 2003. The
Dundee corpus. In Proceedings of the 12th European
conference on eye movement.
Nayoung Kwon, Yoonhyoung Lee, Peter C. Gordon,
Robert Kluender, and Maria Polinsky. 2010. Cog-
nitive and linguistic factors affecting subject/object
asymmetry: An eye-tracking study of pre-nominal rel-
ative clauses in korean. Language, 86(3):561.
Roger Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126?1177.
Richard L. Lewis and Shravan Vasishth. 2005.
An activation-based model of sentence processing
as skilled memory retrieval. Cognitive Science,
29(3):375?419.
Richard L. Lewis, Shravan Vasishth, and Jane A. Van
Dyke. 2006. Computational principles of working
memory in sentence comprehension. Trends in Cog-
nitive Science, 10(10):447?454.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Brian McElree. 2001. Working memory and focal atten-
tion. Journal of Experimental Psychology, Learning
Memory and Cognition, 27(3):817?835.
Brian McElree. 2006. Accessing recent events. The Psy-
chology of Learning and Motivation, 46:155?200.
D. Mitchell, X. Shen, M. Green, and T. Hodgson. 2008.
Accounting for regressive eye-movements in models
of sentence processing: A reappraisal of the selective
reanalysis hypothesis. Journal of Memory and Lan-
guage, 59:266?293.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
NAACL HLT 2007, pages 404?411, Rochester, New
York, April. Association for Computational Linguis-
tics.
104
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics (COLING/ACL?06).
Colin Phillips. 2010. Some arguments and non-
arguments for reductionist accounts of syntactic phe-
nomena. Language and Cognitive Processes, 28:156?
187.
R Development Core Team, 2010. R: A Language and
Environment for Statistical Computing. R Foundation
for Statistical Computing, Vienna, Austria. ISBN 3-
900051-07-0.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and syn-
tactic expectation-based measures for psycholinguistic
modeling via incremental top-down parsing. Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Langauge Processing, pages 324?333.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Claude Shannon. 1948. A mathematical theory of com-
munication. Bell System Technical Journal, 27:379?
423, 623?656.
Stuart M. Shieber, Yves Schabes, and Fernando C.N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. Journal of Logic Programming, 24:3?
36.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Seventh International Conference
on Spoken Language Processing.
John Trueswell. 1996. The role of lexical frequency
in syntactic ambiguity resolution. Journal of Memory
and Language, 35:566?585.
Marten van Schijndel, Andy Exley, and William Schuler.
in press. A model of language processing as hierarchic
sequential prediction. Topics in Cognitive Science.
Shravan Vasishth and Richard L. Lewis. 2006.
Argument-head distance and processing complexity:
Explaining both locality and antilocality effects. Lan-
guage, 82(4):767?794.
Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an in-
cremental right-corner parser. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?10), pages 1189?1198.
105
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1189?1198,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Complexity Metrics in an Incremental Right-corner Parser
Stephen Wu Asaf Bachrach? Carlos Cardenas? William Schuler?
Department of Computer Science, University of Minnesota
? Unit de Neuroimagerie Cognitive INSERM-CEA
? Department of Brain & Cognitive Sciences, Massachussetts Institute of Technology
? University of Minnesota and The Ohio State University
swu@cs.umn.edu ?asaf@mit.edu ?cardenas@mit.edu ?schuler@ling.ohio-state.edu
Abstract
Hierarchical HMM (HHMM) parsers
make promising cognitive models: while
they use a bounded model of working
memory and pursue incremental hypothe-
ses in parallel, they still achieve parsing
accuracies competitive with chart-based
techniques. This paper aims to validate
that a right-corner HHMM parser is also
able to produce complexity metrics, which
quantify a reader?s incremental difficulty
in understanding a sentence. Besides
defining standard metrics in the HHMM
framework, a new metric, embedding
difference, is also proposed, which tests
the hypothesis that HHMM store elements
represents syntactic working memory.
Results show that HHMM surprisal
outperforms all other evaluated metrics
in predicting reading times, and that
embedding difference makes a significant,
independent contribution.
1 Introduction
Since the introduction of a parser-based calcula-
tion for surprisal by Hale (2001), statistical tech-
niques have been become common as models of
reading difficulty and linguistic complexity. Sur-
prisal has received a lot of attention in recent lit-
erature due to nice mathematical properties (Levy,
2008) and predictive ability on eye-tracking move-
ments (Demberg and Keller, 2008; Boston et al,
2008a). Many other complexity metrics have
been suggested as mutually contributing to reading
difficulty; for example, entropy reduction (Hale,
2006), bigram probabilities (McDonald and Shill-
cock, 2003), and split-syntactic/lexical versions of
other metrics (Roark et al, 2009).
A parser-derived complexity metric such as sur-
prisal can only be as good (empirically) as the
model of language from which it derives (Frank,
2009). Ideally, a psychologically-plausible lan-
guage model would produce a surprisal that would
correlate better with linguistic complexity. There-
fore, the specification of how to encode a syntac-
tic language model is of utmost importance to the
quality of the metric.
However, it is difficult to quantify linguis-
tic complexity and reading difficulty. The two
commonly-used empirical quantifications of read-
ing difficulty are eye-tracking measurements and
word-by-word reading times; this paper uses read-
ing times to find the predictiveness of several
parser-derived complexity metrics. Various fac-
tors (i.e., from syntax, semantics, discourse) are
likely necessary for a full accounting of linguis-
tic complexity, so current computational models
(with some exceptions) narrow the scope to syn-
tactic or lexical complexity.
Three complexity metrics will be calculated in
a Hierarchical Hidden Markov Model (HHMM)
parser that recognizes trees in right-corner form
(the left-right dual of left-corner form). This type
of parser performs competitively on standard pars-
ing tasks (Schuler et al, 2010); also, it reflects
plausible accounts of human language processing
as incremental (Tanenhaus et al, 1995; Brants and
Crocker, 2000), as considering hypotheses proba-
bilistically in parallel (Dahan and Gaskell, 2007),
as bounding memory usage to short-term mem-
ory limits (Cowan, 2001), and as requiring more
memory storage for center-embedding structures
than for right- or left-branching ones (Chomsky
and Miller, 1963; Gibson, 1998). Also, unlike
most other parsers, this parser preserves the arc-
eager/arc-standard ambiguity of Abney and John-
1189
son (1991). Typical parsing strategies are arc-
standard, keeping all right-descendants open for
subsequent attachment; but since there can be an
unbounded number of such open constituents, this
assumption is not compatible with simple mod-
els of bounded memory. A consistently arc-eager
strategy acknowledges memory bounds, but yields
dead-end parses. Both analyses are considered in
right-corner HHMM parsing.
The purpose of this paper is to determine
whether the language model defined by the
HHMM parser can also predict reading times ?
it would be strange if a psychologically plausi-
ble model did not also produce viable complex-
ity metrics. In the course of showing that the
HHMM parser does, in fact, predict reading times,
we will define surprisal and entropy reduction in
the HHMM parser, and introduce a third metric
called embedding difference.
Gibson (1998; 2000) hypothesized two types
of syntactic processing costs: integration cost, in
which incremental input is combined with exist-
ing structures; and memory cost, where unfinished
syntactic constructions may incur some short-term
memory usage. HHMM surprisal and entropy
reduction may be considered forms of integra-
tion cost. Though typical PCFG surprisal has
been considered a forward-looking metric (Dem-
berg and Keller, 2008), the incremental nature of
the right-corner transform causes surprisal and en-
tropy reduction in the HHMM parser to measure
the likelihood of grammatical structures that were
hypothesized before evidence was observed for
them. Therefore, these HHMM metrics resemble
an integration cost encompassing both backward-
looking and forward-looking information.
On the other hand, embedding difference is
designed to model the cost of storing center-
embedded structures in working memory. Chen,
Gibson, and Wolf (2005) showed that sentences
requiring more syntactic memory during sen-
tence processing increased reading times, and it
is widely understood that center-embedding incurs
significant syntactic processing costs (Miller and
Chomsky, 1963; Gibson, 1998). Thus, we would
expect for the usage of the center-embedding
memory store in an HHMM parser to correlate
with reading times (and therefore linguistic com-
plexity).
The HHMM parser processes syntactic con-
structs using a bounded number of store states,
defined to represent short-term memory elements;
additional states are utilized whenever center-
embedded syntactic structures are present. Simi-
lar models such as Crocker and Brants (2000) im-
plicitly allow an infinite memory size, but Schuler
et al (2008; 2010) showed that a right-corner
HHMM parser can parse most sentences in En-
glish with 4 or fewer center-embedded-depth lev-
els. This behavior is similar to the hypothesized
size of a human short-term memory store (Cowan,
2001). A positive result in predicting reading
times will lend additional validity to the claim
that the HHMM parser?s bounded memory cor-
responds to bounded memory in human sentence
processing.
The rest of this paper is organized as fol-
lows: Section 2 defines the language model of the
HHMM parser, including definitions of the three
complexity metrics. The methodology for evalu-
ating the complexity metrics is described in Sec-
tion 3, with actual results in Section 4. Further dis-
cussion on results, and comparisons to other work,
are in Section 5.
2 Parsing Model
This section describes an incremental parser in
which surprisal and entropy reduction are sim-
ple calculations (Section 2.1). The parser uses a
Hierarchical Hidden Markov Model (Section 2.2)
and recognizes trees in a right-corner form (Sec-
tion 2.3 and 2.4). The new complexity metric, em-
bedding difference (Section 2.5), is a natural con-
sequence of this HHMM definition. The model
is equivalent to previous HHMM parsers (Schuler,
2009), but reorganized into 5 cases to clarify the
right-corner structure of the parsed sentences.
2.1 Surprisal and Entropy in HMMs
Hidden Markov Models (HMMs) probabilistically
connect sequences of observed states ot and hid-
den states qt at corresponding time steps t. In pars-
ing, observed states are words; hidden states can
be a conglomerate state of linguistic information,
here taken to be syntactic.
The HMM is an incremental, time-series struc-
ture, so one of its by-products is the prefix prob-
ability, which will be used to calculate surprisal.
This is the probability that that words o1..t have
been observed at time t, regardless of which syn-
tactic states q1..t produced them. Bayes? Law and
Markov independence assumptions allow this to
1190
be calculated from two generative probability dis-
tributions.1
Pre(o1..t)=
?
q1..t
P(o1..t q1..t) (1)
def=
?
q1..t
t
?
?=1
P?A(q? | q??1)?P?B(o? | q? ) (2)
Here, probabilities arise from a Transition
Model (?A) between hidden states and an Ob-
servation Model (?B) that generates an observed
state from a hidden state. These models are so
termed for historical reasons (Rabiner, 1990).
Surprisal (Hale, 2001) is then a straightforward
calculation from the prefix probability.
Surprisal(t) = log2
Pre(o1..t?1)
Pre(o1..t)
(3)
This framing of prefix probability and surprisal in
a time-series model is equivalent to Hale?s (2001;
2006), assuming that q1..t ? Dt, i.e., that the syn-
tactic states we are considering form derivations
Dt, or partial trees, consistent with the observed
words. We will see that this is the case for our
parser in Sections 2.2?2.4.
Entropy is a measure of uncertainty, defined as
H(x) = ?P(x) log2 P(x). Now, the entropy Ht
of a t-word string o1..t in an HMM can be written:
Ht =
?
q1..t
P(q1..t o1..t) log2 P(q1..t o1..t) (4)
and entropy reduction (Hale, 2003; Hale, 2006) at
the tth word is then
ER(ot) = max(0, Ht?1 ? Ht) (5)
Both of these metrics fall out naturally from the
time-series representation of the language model.
The third complexity metric, embedding differ-
ence, will be discussed after additional back-
ground in Section 2.5.
In the implementation of an HMM, candidate
states at a given time qt are kept in a trel-
lis, with step-by-step backpointers to the highest-
probability q1..t?1.2 Also, the best qt are often kept
in a beam Bt, discarding low-probability states.
1Technically, a prior distribution over hidden states,
P(q0), is necessary. This q0 is factored and taken to be a de-
terministic constant, and is therefore unimportant as a proba-
bility model.
2Typical tasks in an HMM include finding the most likely
sequence via the Viterbi algorithm, which stores these back-
pointers to maximum-probability previous states and can
uniquely find the most likely sequence.
This mitigates the problems of large state spaces
(e.g., that of all possible grammatical derivations).
Since beams have been shown to perform well
(Brants and Crocker, 2000; Roark, 2001; Boston
et al, 2008b), complexity metrics in this paper
are calculated on a beam rather than over all (un-
bounded) possible derivations Dt. The equations
above, then, will replace the assumption q1..t ?Dt
with qt?Bt.
2.2 Hierarchical Hidden Markov Models
Hidden states q can have internal structure; in Hi-
erarchical HMMs (Fine et al, 1998; Murphy and
Paskin, 2001), this internal structure will be used
to represent syntax trees and looks like several
HMMs stacked on top of each other. As such, qt
is factored into sequences of depth-specific vari-
ables ? one for each of D levels in the HMM hi-
erarchy. In addition, an intermediate variable ft is
introduced to interface between the levels.
qt
def= ?q1t . . . qDt ? (6)
ft
def= ?f1t . . . fDt ? (7)
Transition probabilities P?A(qt | qt?1) over com-
plex hidden states qt are calculated in two phases:
? Reduce phase. Yields an intermediate
state ft, in which component HMMs may ter-
minate. This ft tells ?higher? HMMs to hold
over their information if ?lower? levels are in
operation at any time step t, and tells lower
HMMs to signal when they?re done.
? Shift phase. Yields a modeled hidden state qt,
in which unterminated HMMs transition, and
terminated HMMs are re-initialized from
their parent HMMs.
Each phase is factored according to level-
specific reduce and shift models, ?F and ?Q:
P?A(qt|qt?1) =
?
ft
P(ft|qt?1)?P(qt|ft qt?1) (8)
def=
?
f1..Dt
D
?
d=1
P?F(f
d
t |fd+1t qdt?1qd?1t?1 )
? P?Q(qdt |fd+1t fdt qdt?1qd?1t ) (9)
with fD+1t and q0t defined as constants. Note that
only qt is present at the end of the probability cal-
culation. In step t, ft?1 will be unused, so the
marginalization of Equation 9 does not lose any
information.
1191
. . .
. . .
. . .
. . .
f3t?1
f2t?1
f1t?1
q1t?1
q2t?1
q3t?1
ot?1
f3t
f2t
f1t
q1t
q2t
q3t
ot
(a) Dependency structure in the HHMM
parser. Conditional probabilities at a node are
dependent on incoming arcs.
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8
the
engineers
pulled
off an
engineering
trick
? ? ? ? ? ? ?
? ? vbd
VBD/PRT
? ? ?
dt
NP/NN
S/VP
S/VP
S/NP
S/NN
S/NN S
(b) HHMM parser as a store whose elements at each time step are listed
vertically, showing a good hypothesis on a sample sentence out of many
kept in parallel. Variables corresponding to qdt are shown.
S
NP
DT
the
NN
engineers
VP
VBD
VBD
pulled
PRT
off
NP
DT
an
NN
NN
engineering
NN
trick
(c) A sample sentence in CNF.
S
S/NN
S/NN
S/NP
S/VP
NP
NP/NN
DT
the
NN
engineers
VBD
VBD/PRT
VBD
pulled
PRT
off
DT
an
NN
engineering
NN
trick
(d) The right-corner transformed version of (c).
Figure 1: Various graphical representations of HHMM parser operation. (a) shows probabilistic depen-
dencies. (b) considers the qdt store to be incremental syntactic information. (c)?(d) demonstrate the
right-corner transform, similar to a left-to-right traversal of (c). In ?NP/NN? we say that NP is the active
constituent and NN is the awaited.
The Observation Model ?B is comparatively
much simpler. It is only dependent on the syntac-
tic state at D (or the deepest active HHMM level).
P?B(ot | qt)
def= P(ot | qDt ) (10)
Figure 1(a) gives a schematic of the dependency
structure of Equations 8?10 for D = 3. Evalua-
tions in this paper are done with D = 4, following
the results of Schuler, et al (2008).
2.3 Parsing right-corner trees
In this HHMM formulation, states and dependen-
cies are optimized for parsing right-corner trees
(Schuler et al, 2008; Schuler et al, 2010). A sam-
ple transformation between CNF and right-corner
trees is in Figures 1(c)?1(d).
Figure 1(b) shows the corresponding store-
element interpretation3 of the right corner tree
in 1(d). These can be used as a case study to
see what kind of operations need to occur in an
3This is technically a pushdown automoton (PDA), where
the store is limited to D elements. When referring to direc-
tions (e.g., up, down), PDAs are typically described opposite
of the one in Figure 1(b); here, we push ?up? instead of down.
HHMM when parsing right-corner trees. There
is one unique set of HHMM state values for each
tree, so the operations can be seen on either the
tree or the store elements.
At each time step t, a certain number of el-
ements (maximum D) are kept in memory, i.e.,
in the store. New words are observed input, and
the bottom occupied element (the ?frontier? of the
store) is the context; together, they determine what
the store will look like at t+1. We can characterize
the types of store-element changes by when they
happen in Figures 1(b) and 1(d):
Cross-level Expansion (CLE). Occupies a new
store element at a given time step. For exam-
ple, at t=1, a new store element is occupied
which can interact with the observed word,
?the.? At t = 3, an expansion occupies the
second store element.
In-level Reduction (ILR). Completes an active
constituent that is a unary child in the right-
corner tree; always accompanied by an in-
level expansion. At t= 2, ?engineers? com-
pletes the active NP constituent; however, the
1192
level is not yet complete since the NP is along
the left-branching trunk of the tree.
In-level Expansion (ILE). Starts a new active
constituent at an already-occupied store ele-
ment; always follows an in-level reduction.
With the NP complete in t= 2, a new active
constituent S is produced at t=3.
In-level Transition (ILT). Transitions the store
to a new state in the next time step at the same
level, where the awaited constituent changes
and the active constituent remains the same.
This describes each of the steps from t=4 to
t=8 at d=1 .
Cross-level Reduction (CLR). Vacates a store
element on seeing a complete active con-
stituent. This occurs after t = 4; ?off?
completes the active (at depth 2) VBD con-
stituent, and vacates store element 2. This
is accompanied with an in-level transition at
depth 1, producing the store at t=5. It should
be noted that with some probability, complet-
ing the active constituent does not vacate the
store element, and the in-level reduction case
would have to be invoked.
The in-level/cross-level ambiguity occurs in the
expansion as well as the reduction, similar to Ab-
ney and Johnson?s arc-eager/arc-standard compo-
sition strategies (1991). At t=3, another possible
hypothesis would be to remain on store element
1 using an ILE instead of a CLE. The HHMM
parser, unlike most other parsers, will preserve this
in-level/cross-level ambiguity by considering both
hypotheses in parallel.
2.4 Reduce and Shift Models
With the understanding of what operations need to
occur, a formal definition of the language model is
in order. Let us begin with the relevant variables.
A shift variable qdt at depth d and time step t is
a syntactic state that must represent the active and
awaited constituents of right-corner form:
qdt
def= ?gAqdt , g
W
qdt
? (11)
e.g., in Figure 1(b), q12=?NP,NN?=NP/NN. Each g is
a constituent from the pre-right-corner grammar,
G.
Reduce variables f are then enlisted to ensure
that in-level and cross-level operations are correct.
fdt
def= ?kfdt , gfdt ? (12)
First, kfdt is a switching variable that differenti-
ates between ILT, CLE/CLR, and ILE/ILR. This
switching is the most important aspect of fdt , so
regardless of what gfdt is, we will use:
? fdt ? F0 when kfdt =0, (ILT/no-op)
? fdt ? F1 when kfdt =1, (CLE/CLR)
? fdt ? FG when kfdt ? G. (ILE/ILR)
Then, gfdt is used to keep track of a completely-
recognized constituent whenever a reduction oc-
curs (ILR or CLR). For example, in Figure 1(b),
after time step 2, an NP has been completely rec-
ognized and precipitates an ILR. The NP gets
stored in gf13 for use in the ensuing ILE instead
of appearing in the store-elements.
This leads us to a specification of the reduce and
shift probability models. The reduce step happens
first at each time step. True to its name, the re-
duce step handles in-level and cross-level reduc-
tions (the second and third case below):
P?F(f
d
t | fd+1t qdt?1qd?1t?1 )
def=
{
if fd+1t 6?FG : Jfdt =0K
if fd+1t ?FG, fdt ? F1 : P??F-ILR,d(fdt | qdt?1 qd?1t?1 )
if fd+1t ?FG, fdt ? FG : P??F-CLR,d(fdt | qdt?1 qd?1t?1 )
(13)
with edge cases q0t and fD+1t defined as appropri-
ate constants. The first case is just store-element
maintenance, in which the variable is not on the
?frontier? and therefore inactive.
Examining ?F-ILR,d and ?F-CLR,d, we see that
the produced fdt variables are also used in the ?if?
statement. These models can be thought of as
picking out a fdt first, finding the matching case,
then applying the probability models that matches.
These models are actually two parts of the same
model when learned from trees.
Probabilities in the shift step are also split into
cases based on the reduce variables. More main-
tenance operations (first case) accompany transi-
tions producing new awaited constituents (second
case below) and expansions producing new active
constituents (third and fourth case):
P?Q(q
d
t | fd+1t fdt qdt?1qd?1t )
def=
?
?
?
?
?
if fd+1t 6?FG : Jqdt = qdt?1K
if fd+1t ?FG, fdt ? F0 : P??Q-ILT,d(qdt | fd+1t qdt?1 qd?1t )
if fd+1t ?FG, fdt ? F1 : P??Q-ILE,d(qdt | fdt qdt?1 qd?1t )
if fd+1t ?FG, fdt ?FG : P??Q-CLE,d(qdt | qd?1t )
(14)
1193
FACTOR DESCRIPTION EXPECTED
Word order in
narrative
For each story, words were indexed. Subjects would tend to read faster later in a story. negative
slope
Reciprocal
length
Log of the reciprocal of the number of letters in each word. A decrease in the reciprocal
(increase in length) might mean longer reading times.
positive
slope
Unigram
frequency
A log-transformed empirical count of word occurrences in the Brown Corpus section of
the Penn Treebank. Higher frequency should indicate shorter reading times.
negative
slope
Bigram
probability
A log-transformed empirical count of two-successive-word occurrences, with Good-
Turing smoothing on words occuring less than 10 times.
negative
slope
Embedding
difference
Amount of change in HHMM weighted-average embedding depth. Hypothesized to in-
crease with larger working memory requirements, which predict longer reading times.
positive
slope
Entropy
reduction
Amount of decrease in the HHMM?s uncertainty about the sentence. Larger reductions
in uncertainty are hypothesized to take longer.
positive
slope
Surprisal ?Surprise value? of a word in the HHMM parser; models were trained on the Wall Street
Journal, sections 02?21. More surprising words may take longer to read.
positive
slope
Table 1: A list of factors hypothesized to contribute to reading times. All data was mean-centered.
A final note: the notation P??(? | ?) has been used
to indicate probability models that are empirical,
trained directly from frequency counts of right-
corner transformed trees in a large corpus. Alter-
natively, a standard PCFG could be trained on a
corpus (or hand-specified), and then the grammar
itself can be right-corner transformed (Schuler,
2009).
Taken together, Equations 11?14 define the
probabilistic structure of the HHMM for parsing
right-corner trees.
2.5 Embedding difference in the HHMM
It should be clear from Figure 1 that at any time
step while parsing depth-bounded right-corner
trees, the candidate hidden state qt will have a
?frontier? depth d(qt). At time t, the beam of
possible hidden states qt stores the syntactic state
(and a backpointer) along with its probability,
P(o1..t q1..t). The average embedding depth at a
time step is then
?EMB(o1..t) =
?
qt?Bt
d(qt) ?
P(o1..t q1..t)
?
q?t?Bt P(o1..t q
?
1..t)
(15)
where we have directly used the beam notation.
The embedding difference metric is:
EmbDiff(o1..t) = ?EMB(o1..t) ? ?EMB(o1..t?1)
(16)
There is a strong computational correspondence
between this definition of embedding difference
and the previous definition of surprisal. To see
this, we rewrite Equations 1 and 3:
Pre(o1..t)=
?
qt?Bt
P(o1..t q1..t) (1?)
Surprisal(t) = log2 Pre(o1..t?1) ? log2 Pre(o1..t)
(3?)
Both surprisal and embedding difference include
summations over the elements of the beam, and
are calculated as a difference between previous
and current beam states.
Most differences between these metrics are rel-
atively inconsequential. For example, the dif-
ference in order of subtraction only assures that
a positive correlation with reading times is ex-
pected. Also, the presence of a logarithm is rel-
atively minor. Embedding difference weighs the
probabilities with center-embedding depths and
then normalizes the values; since the measure is
a weighted average of embedding depths rather
than a probability distribution, ?EMB is not always
less than 1 and the correspondence with Kullback-
Leibler divergence (Levy, 2008) does not hold, so
it does not make sense to take the logs.
Therefore, the inclusion of the embedding
depth, d(qt), is the only significant difference
between the two metrics. The result is a met-
ric that, despite numerical correspondence to sur-
prisal, models the HHMM?s hypotheses about
memory cost.
3 Evaluation
Surprisal, entropy reduction, and embedding dif-
ference from the HHMM parser were evaluated
against a full array of factors (Table 1) on a cor-
pus of word-by-word reading times using a linear
mixed-effects model.
1194
The corpus of reading times for 23 native En-
glish speakers was collected on a set of four nar-
ratives (Bachrach et al, 2009), each composed of
sentences that were syntactically complex but con-
structed to appear relatively natural. Using Linger
2.88, words appeared one-by-one on the screen,
and required a button-press in order to advance;
they were displayed in lines with 11.5 words on
average.
Following Roark et al?s (2009) work on the
same corpus, reading times above 1500 ms (for
diverted attention) or below 150 ms (for button
presses planned before the word appeared) were
discarded. In addition, the first and last word of
each line on the screen were removed; this left
2926 words out of 3540 words in the corpus.
For some tests, a division between open- and
closed-class words was made, with 1450 and 1476
words, respectively. Closed-class words (e.g., de-
terminers or auxiliary verbs) usually play some
kind of syntactic function in a sentence; our evalu-
ations used Roark et al?s list of stop words. Open
class words (e.g., nouns and other verbs) more
commonly include new words. Thus, one may ex-
pect reading times to differ for these two types of
words.
Linear mixed-effect regression analysis was
used on this data; this entails a set of fixed effects
and another of random effects. Reading times y
were modeled as a linear combination of factors
x, listed in Table 1 (fixed effects); some random
variation in the corpus might also be explained by
groupings according to subject i, word j, or sen-
tence k (random effects).
yijk = ?0 +
m
X
?=1
??xijk? + bi + bj + bk + ? (17)
This equation is solved for each of m fixed-
effect coefficients ? with a measure of confidence
(t-value = ??/SE(??), where SE is the standard er-
ror). ?0 is the standard intercept to be estimated
along with the rest of the coefficients, to adjust for
affine relationships between the dependent and in-
dependent variables. We report factors as statisti-
cally significant contributors to reading time if the
absolute value of the t-value is greater than 2.
Two more types of comparisons will be made to
see the significance of factors. First, a model of
data with the full list of factors can be compared
to a model with a subset of those factors. This is
done with a likelihood ratio test, producing (for
mixed-effects models) a ?21 value and correspond-
ing probability that the smaller model could have
produced the same estimates as the larger model.
A lower probability indicates that the additional
factors in the larger model are significant.
Second, models with different fixed effects can
be compared to each other through various infor-
mation criteria; these trade off between having
a more explanatory model vs. a simpler model,
and can be calculated on any model. Here, we
use Akaike?s Information Criterion (AIC), where
lower values indicate better models.
All these statistics were calculated in R, using
the lme4 package (Bates et al, 2008).
4 Results
Using the full list of factors in Table 1, fixed-effect
coefficients were estimated in Table 2. Fitting the
best model by AIC would actually prune away
some of the factors as relatively insignificant, but
these smaller models largely accord with the sig-
nificance values in the table and are therefore not
presented.
The first data column shows the regression on
all data; the second and third columns divide the
data into open and closed classes, because an eval-
uation (not reported in detail here) showed statis-
tically significant interactions between word class
and 3 of the predictors. Additionally, this facil-
itates comparison with Roark et al (2009), who
make the same division.
Out of the non-parser-based metrics, word order
and bigram probability are statistically significant
regardless of the data subset; though reciprocal
length and unigram frequency do not reach signif-
icance here, likelihood ratio tests (not shown) con-
firm that they contribute to the model as a whole.
It can be seen that nearly all the slopes have been
estimated with signs as expected, with the excep-
tion of reciprocal length (which is not statistically
significant).
Most notably, HHMM surprisal is seen here to
be a standout predictive measure for reading times
regardless of word class. If the HHMM parser is
a good psycholinguistic model, we would expect
it to at least produce a viable surprisal metric, and
Table 2 attests that this is indeed the case. Though
it seems to be less predictive of open classes, a
surprisal-only model has the best AIC (-7804) out
of any open-class model. Considering the AIC
on the full data, the worst model with surprisal
1195
FULL DATA OPEN CLASS CLOSED CLASS
Coefficient Std. Err. t-value Coefficient Std. Err. t-value Coefficient Std. Err. t-value
(Intcpt) -9.340?10?3 5.347?10?2 -0.175 -1.237?10?2 5.217?10?2 -0.237 -6.295?10?2 7.930?10?2 -0.794
order -3.746?10?5 7.808?10?6 -4.797? -3.697?10?5 8.002?10?6 -4.621? -3.748?10?5 8.854?10?6 -4.232?
rlength -2.002?10?2 1.635?10?2 -1.225 9.849?10?3 1.779?10?2 0.554 -2.839?10?2 3.283?10?2 -0.865
unigrm -8.090?10?2 3.690?10?1 -0.219 -1.047?10?1 2.681?10?1 -0.391 -3.847?10+0 5.976?10+0 -0.644
bigrm -2.074?10+0 8.132?10?1 -2.551? -2.615?10+0 8.050?10?1 -3.248? -5.052?10+1 1.910?10+1 -2.645?
embdiff 9.390?10?3 3.268?10?3 2.873? 2.432?10?3 4.512?10?3 0.539 1.598?10?2 5.185?10?3 3.082?
etrpyrd 2.753?10?2 6.792?10?3 4.052? 6.634?10?4 1.048?10?2 0.063 4.938?10?2 1.017?10?2 4.857?
srprsl 3.950?10?3 3.452?10?4 11.442? 2.892?10?3 4.601?10?4 6.285? 5.201?10?3 5.601?10?4 9.286?
Table 2: Results of linear mixed-effect modeling. Significance (indicated by ?) is reported at p < 0.05.
(Intr) order rlngth ungrm bigrm emdiff entrpy
order .000
rlength -.006 -.003
unigrm .049 .000 -.479
bigrm .001 .005 -.006 -.073
emdiff .000 .009 -.049 -.089 .095
etrpyrd .000 .003 .016 -.014 .020 -.010
srprsl .000 -.008 -.033 -.079 .107 .362 .171
Table 3: Correlations in the full model.
(AIC=-10589) outperformed the best model with-
out it (AIC=-10478), indicating that the HHMM
surprisal is well worth including in the model re-
gardless of the presence of other significant fac-
tors.
HHMM entropy reduction predicts reading
times on the full dataset and on closed-class
words. However, its effect on open-class words is
insignificant; if we compare the model of column
2 against one without entropy reduction, a likeli-
hood ratio test gives ?21 = 0.0022, p = 0.9623
(the smaller model could easily generate the same
data).
The HHMM?s average embedding difference
is also significant except in the case of open-
class words ? removing embedding difference on
open-class data yields ?21 = 0.2739, p = 0.6007.
But what is remarkable is that there is any signifi-
cance for this metric at all. Embedding difference
and surprisal were relatively correlated compared
to other predictors (see Table 3), which is expected
because embedding difference is calculated like
a weighted version of surprisal. Despite this, it
makes an independent contribution to the full-data
and closed-class models. Thus, we can conclude
that the average embedding depth component af-
fects reading times ? i.e., the HHMM?s notion of
working memory behaves as we would expect hu-
man working memory to behave.
5 Discussion
As with previous work on large-scale parser-
derived complexity metrics, the linear mixed-
effect models suggest that sentence-level factors
are effective predictors for reading difficulty ? in
these evaluations, better than commonly-used lex-
ical and near-neighbor predictors (Pollatsek et al,
2006; Engbert et al, 2005). The fact that HHMM
surprisal outperforms even n-gram metrics points
to the importance of including a notion of sentence
structure. This is particularly true when the sen-
tence structure is defined in a language model that
is psycholinguistically plausible (here, bounded-
memory right-corner form).
This accords with an understated result of
Boston et al?s eye-tracking study (2008a): a
richer language model predicts eye movements
during reading better than an oversimplified one.
The comparison there is between phrase struc-
ture surprisal (based on Hale?s (2001) calculation
from an Earley parser), and dependency grammar
surprisal (based on Nivre?s (2007) dependency
parser). Frank (2009) similarly reports improve-
ments in the reading-time predictiveness of unlexi-
calized surprisal when using a language model that
is more plausible than PCFGs.
The difference in predictivity due to word class
is difficult to explain. One theory may be that
closed-class words are less susceptible to random
effects because there is a finite set of them for
any language, making them overall easier to pre-
dict via parser-derived metrics. Or, we could note
that since closed-class words often serve grammat-
ical functions in addition to their lexical content,
they contribute more information to parser-derived
measures than open-class words. Previous work
with complexity metrics on this corpus (Roark et
al., 2009) suggests that these explanations only ac-
count for part of the word-class variation in the
performance of predictors.
1196
Further comparsion to Roark et al will show
other differences, such as the lesser role of word
length and unigram frequency, lower overall cor-
relations between factors, and the greater predic-
tivity of their entropy metric. In addition, their
metrics are different from ours in that they are de-
signed to tease apart lexical and syntactic contri-
butions to reading difficulty. Their notion of en-
tropy, in particular, estimates Hale?s definition of
entropy on whole derivations (2006) by isolating
the predictive entropy; they then proceed to define
separate lexical and syntactic predictive entropies.
Drawing more directly from Hale, our definition
is a whole-derivation metric based on the condi-
tional entropy of the words, given the root. (The
root constituent, though unwritten in our defini-
tions, is always included in the HHMM start state,
q0.)
More generally, the parser used in these evalu-
ations differs from other reported parsers in that
it is not lexicalized. One might expect for this
to be a weakness, allowing distributions of prob-
abilities at each time step in places not licensed
by the observed words, and therefore giving poor
probability-based complexity metrics. However,
we see that this language model performs well
despite its lack of lexicalization. This indicates
that lexicalization is not a requisite part of syntac-
tic parser performance with respect to predicting
linguistic complexity, corroborating the evidence
of Demberg and Keller?s (2008) ?unlexicalized?
(POS-generating, not word-generating) parser.
Another difference is that previous parsers have
produced useful complexity metrics without main-
taining arc-eager/arc-standard ambiguity. Results
show that including this ambiguity in the HHMM
at least does not invalidate (and may in fact im-
prove) surprisal or entropy reduction as reading-
time predictors.
6 Conclusion
The task at hand was to determine whether the
HHMM could consistently be considered a plau-
sible psycholinguistic model, producing viable
complexity metrics while maintaining other char-
acteristics such as bounded memory usage. The
linear mixed-effects models on reading times val-
idate this claim. The HHMM can straightfor-
wardly produce highly-predictive, standard com-
plexity metrics (surprisal and entropy reduction).
HHMM surprisal performs very well in predicting
reading times regardless of word class. Our for-
mulation of entropy reduction is also significant
except in open-class words.
The new metric, embedding difference, uses the
average center-embedding depth of the HHMM
to model syntactic-processing memory cost. This
metric can only be calculated on parsers with an
explicit representation for short-term memory el-
ements like the right-corner HHMM parser. Re-
sults show that embedding difference does predict
reading times except in open-class words, yielding
a significant contribution independent of surprisal
despite the fact that its definition is similar to that
of surprisal.
Acknowledgments
Thanks to Brian Roark for help on the reading
times corpus, Tim Miller for the formulation of
entropy reduction, Mark Holland for statistical in-
sight, and the anonymous reviewers for their input.
This research was supported by National Science
Foundation CAREER/PECASE award 0447685.
The views expressed are not necessarily endorsed
by the sponsors.
References
Steven P. Abney and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
Asaf Bachrach, Brian Roark, Alex Marantz, Susan
Whitfield-Gabrieli, Carlos Cardenas, and John D.E.
Gabrieli. 2009. Incremental prediction in naturalis-
tic language processing: An fMRI study.
Douglas Bates, Martin Maechler, and Bin Dai. 2008.
lme4: Linear mixed-effects models using S4 classes.
R package version 0.999375-31.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
U. Patil, and Shravan Vasishth. 2008a. Parsing costs
as predictors of reading difficulty: An evaluation us-
ing the Potsdam Sentence Corpus. Journal of Eye
Movement Research, 2(1):1?12.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
and Shravan Vasishth. 2008b. Surprising parser ac-
tions and reading difficulty. In Proceedings of ACL-
08: HLT, Short Papers, pages 5?8, Columbus, Ohio,
June. Association for Computational Linguistics.
Thorsten Brants and Matthew Crocker. 2000. Prob-
abilistic parsing and psychological plausibility. In
Proceedings of COLING ?00, pages 111?118.
1197
Evan Chen, Edward Gibson, and Florian Wolf. 2005.
Online syntactic storage costs in sentence com-
prehension. Journal of Memory and Language,
52(1):144?169.
Noam Chomsky and George A. Miller. 1963. Intro-
duction to the formal analysis of natural languages.
In Handbook of Mathematical Psychology, pages
269?321. Wiley.
Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87?
185.
Matthew Crocker and Thorsten Brants. 2000. Wide-
coverage probabilistic sentence processing. Journal
of Psycholinguistic Research, 29(6):647?669.
Delphine Dahan and M. Gareth Gaskell. 2007. The
temporal dynamics of ambiguity resolution: Evi-
dence from spoken-word recognition. Journal of
Memory and Language, 57(4):483?501.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
Ralf Engbert, Antje Nuthmann, Eike M. Richter, and
Reinhold Kliegl. 2005. SWIFT: A dynamical model
of saccade generation during reading. Psychological
Review, 112:777?813.
Shai Fine, Yoram Singer, and Naftali Tishby. 1998.
The hierarchical hidden markov model: Analysis
and applications. Machine Learning, 32(1):41?62.
Stefan L. Frank. 2009. Surprisal-based comparison be-
tween a symbolic and a connectionist model of sen-
tence processing. In Proc. Annual Meeting of the
Cognitive Science Society, pages 1139?1144.
Edward Gibson. 1998. Linguistic complexity: Local-
ity of syntactic dependencies. Cognition, 68(1):1?
76.
Edward Gibson. 2000. The dependency locality the-
ory: A distance-based theory of linguistic complex-
ity. In Image, language, brain: Papers from the first
mind articulation project symposium, pages 95?126.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the
Association for Computational Linguistics, pages
159?166, Pittsburgh, PA.
John Hale. 2003. Grammar, Uncertainty and Sentence
Processing. Ph.D. thesis, Cognitive Science, The
Johns Hopkins University.
John Hale. 2006. Uncertainty about the rest of the
sentence. Cognitive Science, 30(4):609?642.
Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition, 106(3):1126?1177.
Scott A. McDonald and Richard C. Shillcock. 2003.
Low-level predictive inference in reading: The influ-
ence of transitional probabilities on eye movements.
Vision Research, 43(16):1735?1751.
George Miller and Noam Chomsky. 1963. Finitary
models of language users. In R. Luce, R. Bush,
and E. Galanter, editors, Handbook of Mathematical
Psychology, volume 2, pages 419?491. John Wiley.
Kevin P. Murphy and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833?840, Vancouver, BC, Canada.
Joakim Nivre. 2007. Inductive dependency parsing.
Computational Linguistics, 33(2).
Alexander Pollatsek, Erik D. Reichle, and Keith
Rayner. 2006. Tests of the EZ Reader model:
Exploring the interface between cognition and eye-
movement control. Cognitive Psychology, 52(1):1?
56.
Lawrence R. Rabiner. 1990. A tutorial on hid-
den Markov models and selected applications in
speech recognition. Readings in speech recognition,
53(3):267?296.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
Proceedings of the 2009 Conference on Empirical
Methods in Natural Langauge Processing, pages
324?333.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
William Schuler, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2008. Toward a
psycholinguistically-motivated model of language.
In Proceedings of COLING, pages 785?792,
Manchester, UK, August.
William Schuler, Samir AbdelRahman, TimMiller, and
Lane Schwartz. 2010. Broad-coverage incremen-
tal parsing using human-like memory constraints.
Computational Linguistics, 36(1).
William Schuler. 2009. Parsing with a bounded
stack using a model-based right-corner transform.
In Proceedings of the North American Association
for Computational Linguistics (NAACL ?09), pages
344?352, Boulder, Colorado.
Michael K. Tanenhaus, Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. In-
tegration of visual and linguistic information in spo-
ken language comprehension. Science, 268:1632?
1634.
1198
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 620?631,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Incremental Syntactic Language Models for Phrase-based Translation
Lane Schwartz
Air Force Research Laboratory
Wright-Patterson AFB, OH USA
lane.schwartz@wpafb.af.mil
Chris Callison-Burch
Johns Hopkins University
Baltimore, MD USA
ccb@cs.jhu.edu
William Schuler
Ohio State University
Columbus, OH USA
schuler@ling.ohio-state.edu
Stephen Wu
Mayo Clinic
Rochester, MN USA
wu.stephen@mayo.edu
Abstract
This paper describes a novel technique for in-
corporating syntactic knowledge into phrase-
based machine translation through incremen-
tal syntactic parsing. Bottom-up and top-
down parsers typically require a completed
string as input. This requirement makes it dif-
ficult to incorporate them into phrase-based
translation, which generates partial hypothe-
sized translations from left-to-right. Incre-
mental syntactic language models score sen-
tences in a similar left-to-right fashion, and are
therefore a good mechanism for incorporat-
ing syntax into phrase-based translation. We
give a formal definition of one such linear-
time syntactic language model, detail its re-
lation to phrase-based decoding, and integrate
the model with the Moses phrase-based trans-
lation system. We present empirical results
on a constrained Urdu-English translation task
that demonstrate a significant BLEU score im-
provement and a large decrease in perplexity.
1 Introduction
Early work in statistical machine translation viewed
translation as a noisy channel process comprised of
a translation model, which functioned to posit ad-
equate translations of source language words, and
a target language model, which guided the fluency
of generated target language strings (Brown et al,
This research was supported by NSF CAREER/PECASE
award 0447685, NSF grant IIS-0713448, and the European
Commission through the EuroMatrixPlus project. Opinions, in-
terpretations, conclusions, and recommendations are those of
the authors and are not necessarily endorsed by the sponsors or
the United States Air Force. Cleared for public release (Case
Number 88ABW-2010-6489) on 10 Dec 2010.
1990). Drawing on earlier successes in speech
recognition, research in statistical machine trans-
lation has effectively used n-gram word sequence
models as language models.
Modern phrase-based translation using large scale
n-gram language models generally performs well
in terms of lexical choice, but still often produces
ungrammatical output. Syntactic parsing may help
produce more grammatical output by better model-
ing structural relationships and long-distance depen-
dencies. Bottom-up and top-down parsers typically
require a completed string as input; this requirement
makes it difficult to incorporate these parsers into
phrase-based translation, which generates hypothe-
sized translations incrementally, from left-to-right.1
As a workaround, parsers can rerank the translated
output of translation systems (Och et al, 2004).
On the other hand, incremental parsers (Roark,
2001; Henderson, 2004; Schuler et al, 2010; Huang
and Sagae, 2010) process input in a straightforward
left-to-right manner. We observe that incremental
parsers, used as structured language models, pro-
vide an appropriate algorithmic match to incremen-
tal phrase-based decoding. We directly integrate in-
cremental syntactic parsing into phrase-based trans-
lation. This approach re-exerts the role of the lan-
guage model as a mechanism for encouraging syn-
tactically fluent translations.
The contributions of this work are as follows:
? A novel method for integrating syntactic LMs
into phrase-based translation (?3)
? A formal definition of an incremental parser for
1While not all languages are written left-to-right, we will
refer to incremental processing which proceeds from the begin-
ning of a sentence as left-to-right.
620
statistical MT that can run in linear-time (?4)
? Integration with Moses (?5) along with empiri-
cal results for perplexity and significant transla-
tion score improvement on a constrained Urdu-
English task (?6)
2 Related Work
Neither phrase-based (Koehn et al, 2003) nor hierar-
chical phrase-based translation (Chiang, 2005) take
explicit advantage of the syntactic structure of either
source or target language. The translation models in
these techniques define phrases as contiguous word
sequences (with gaps allowed in the case of hierar-
chical phrases) which may or may not correspond
to any linguistic constituent. Early work in statisti-
cal phrase-based translation considered whether re-
stricting translation models to use only syntactically
well-formed constituents might improve translation
quality (Koehn et al, 2003) but found such restric-
tions failed to improve translation quality.
Significant research has examined the extent to
which syntax can be usefully incorporated into sta-
tistical tree-based translation models: string-to-tree
(Yamada and Knight, 2001; Gildea, 2003; Imamura
et al, 2004; Galley et al, 2004; Graehl and Knight,
2004; Melamed, 2004; Galley et al, 2006; Huang
et al, 2006; Shen et al, 2008), tree-to-string (Liu
et al, 2006; Liu et al, 2007; Mi et al, 2008; Mi
and Huang, 2008; Huang and Mi, 2010), tree-to-tree
(Abeille? et al, 1990; Shieber and Schabes, 1990;
Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan
et al, 2006; Nesson et al, 2006; Zhang et al, 2007;
DeNeefe et al, 2007; DeNeefe and Knight, 2009;
Liu et al, 2009; Chiang, 2010), and treelet (Ding
and Palmer, 2005; Quirk et al, 2005) techniques
use syntactic information to inform the translation
model. Recent work has shown that parsing-based
machine translation using syntax-augmented (Zoll-
mann and Venugopal, 2006) hierarchical translation
grammars with rich nonterminal sets can demon-
strate substantial gains over hierarchical grammars
for certain language pairs (Baker et al, 2009). In
contrast to the above tree-based translation models,
our approach maintains a standard (non-syntactic)
phrase-based translation model. Instead, we incor-
porate syntax into the language model.
Traditional approaches to language models in
speech recognition and statistical machine transla-
tion focus on the use of n-grams, which provide a
simple finite-state model approximation of the tar-
get language. Chelba and Jelinek (1998) proposed
that syntactic structure could be used as an alterna-
tive technique in language modeling. This insight
has been explored in the context of speech recogni-
tion (Chelba and Jelinek, 2000; Collins et al, 2005).
Hassan et al (2007) and Birch et al (2007) use
supertag n-gram LMs. Syntactic language models
have also been explored with tree-based translation
models. Charniak et al (2003) use syntactic lan-
guage models to rescore the output of a tree-based
translation system. Post and Gildea (2008) investi-
gate the integration of parsers as syntactic language
models during binary bracketing transduction trans-
lation (Wu, 1997); under these conditions, both syn-
tactic phrase-structure and dependency parsing lan-
guage models were found to improve oracle-best
translations, but did not improve actual translation
results. Post and Gildea (2009) use tree substitution
grammar parsing for language modeling, but do not
use this language model in a translation system. Our
work, in contrast to the above approaches, explores
the use of incremental syntactic language models in
conjunction with phrase-based translation models.
Our syntactic language model fits into the fam-
ily of linear-time dynamic programming parsers de-
scribed in (Huang and Sagae, 2010). Like (Galley
and Manning, 2009) our work implements an in-
cremental syntactic language model; our approach
differs by calculating syntactic LM scores over all
available phrase-structure parses at each hypothesis
instead of the 1-best dependency parse.
The syntax-driven reordering model of Ge (2010)
uses syntax-driven features to influence word order
within standard phrase-based translation. The syn-
tactic cohesion features of Cherry (2008) encour-
ages the use of syntactically well-formed translation
phrases. These approaches are fully orthogonal to
our proposed incremental syntactic language model,
and could be applied in concert with our work.
3 Parser as Syntactic Language Model in
Phrase-Based Translation
Parsing is the task of selecting the representation ??
(typically a tree) that best models the structure of
621
???????
?s?
??0
???????
?s? the
??11
???????
?s? that
??12
???????
?s? president
??13
. . .
???????
the president
??21
???????
that president
??22
???????
president Friday
??23
. . .
???????
president meets
??31
???????
Obama met
??32
. . .
Figure 1: Partial decoding lattice for standard phrase-based decoding stack algorithm translating the German
sentence Der Pra?sident trifft am Freitag den Vorstand. Each node h in decoding stack t represents the
application of a translation option, and includes the source sentence coverage vector, target language n-
gram state, and syntactic language model state ??th . Hypothesis combination is also shown, indicating
where lattice paths with identical n-gram histories converge. We use the English translation The president
meets the board on Friday as a running example throughout all Figures.
sentence e, out of all such possible representations
? . This set of representations may be all phrase
structure trees or all dependency trees allowed by
the parsing model. Typically, tree ?? is taken to be:
?? = argmax
?
P(? | e) (1)
We define a syntactic language model P(e) based
on the total probability mass over all possible trees
for string e. This is shown in Equation 2 and decom-
posed in Equation 3.
P(e) =
?
???
P(?, e) (2)
P(e) =
?
???
P(e | ?)P(?) (3)
3.1 Incremental syntactic language model
An incremental parser processes each token of in-
put sequentially from the beginning of a sentence to
the end, rather than processing input in a top-down
(Earley, 1968) or bottom-up (Cocke and Schwartz,
1970; Kasami, 1965; Younger, 1967) fashion. After
processing the tth token in string e, an incremen-
tal parser has some internal representation of possi-
ble hypothesized (incomplete) trees, ?t. The syntac-
tic language model probability of a partial sentence
e1...et is defined:
P(e1...et) =
?
???t
P(e1...et | ?)P(?) (4)
In practice, a parser may constrain the set of trees
under consideration to ??t, that subset of analyses or
partial analyses that remains after any pruning is per-
formed. An incremental syntactic language model
can then be defined by a probability mass function
(Equation 5) and a transition function ? (Equation
6). The role of ? is explained in ?3.3 below. Any
parser which implements these two functions can
serve as a syntactic language model.
P(e1...et) ? P(?? t) =
?
???? t
P(e1...et | ?)P(?) (5)
?(et, ?? t?1)? ?? t (6)
622
3.2 Decoding in phrase-based translation
Given a source language input sentence f , a trained
source-to-target translation model, and a target lan-
guage model, the task of translation is to find the
maximally probable translation e? using a linear
combination of j feature functions h weighted ac-
cording to tuned parameters ? (Och and Ney, 2002).
e? = argmax
e
exp(
?
j
?jhj(e,f)) (7)
Phrase-based translation constructs a set of trans-
lation options ? hypothesized translations for con-
tiguous portions of the source sentence ? from a
trained phrase table, then incrementally constructs a
lattice of partial target translations (Koehn, 2010).
To prune the search space, lattice nodes are orga-
nized into beam stacks (Jelinek, 1969) according to
the number of source words translated. An n-gram
language model history is also maintained at each
node in the translation lattice. The search space
is further trimmed with hypothesis recombination,
which collapses lattice nodes that share a common
coverage vector and n-gram state.
3.3 Incorporating a Syntactic Language Model
Phrase-based translation produces target language
words in an incremental left-to-right fashion, gen-
erating words at the beginning of a translation first
and words at the end of a translation last. Similarly,
incremental parsers process sentences in an incre-
mental fashion, analyzing words at the beginning of
a sentence first and words at the end of a sentence
last. As such, an incremental parser with transition
function ? can be incorporated into the phrase-based
decoding process in a straightforward manner. Each
node in the translation lattice is augmented with a
syntactic language model state ??t.
The hypothesis at the root of the translation lattice
is initialized with ?? 0, representing the internal state
of the incremental parser before any input words are
processed. The phrase-based translation decoding
process adds nodes to the lattice; each new node
contains one or more target language words. Each
node contains a backpointer to its parent node, in
which ?? t?1 is stored. Given a new target language
word et and ?? t?1, the incremental parser?s transi-
tion function ? calculates ?? t. Figure 1 illustrates
S
NP
DT
The
NN
president
VP
VP
VB
meets
NP
DT
the
NN
board
PP
IN
on
NP
Friday
Figure 2: Sample binarized phrase structure tree.
S
S/NP
S/PP
S/VP
NP
NP/NN
DT
The
NN
president
VP
VP/NN
VP/NP
VB
meets
DT
the
NN
board
IN
on
NP
Friday
Figure 3: Sample binarized phrase structure tree af-
ter application of right-corner transform.
a sample phrase-based decoding lattice where each
translation lattice node is augmented with syntactic
language model state ??t.
In phrase-based translation, many translation lat-
tice nodes represent multi-word target language
phrases. For such translation lattice nodes, ? will
be called once for each newly hypothesized target
language word in the node. Only the final syntac-
tic language model state in such sequences need be
stored in the translation lattice node.
4 Incremental Bounded-Memory Parsing
with a Time Series Model
Having defined the framework by which any in-
cremental parser may be incorporated into phrase-
based translation, we now formally define a specific
incremental parser for use in our experiments.
The parser must process target language words
incrementally as the phrase-based decoder adds hy-
potheses to the translation lattice. To facilitate this
incremental processing, ordinary phrase-structure
trees can be transformed into right-corner recur-
623
r1t?1
r2t?1
r3t?1
s1t?1
s2t?1
s3t?1
r1t
r2t
r3t
s1t
s2t
s3t
et?1 et
. . .
. . .
. . .
. . .
Figure 4: Graphical representation of the depen-
dency structure in a standard Hierarchic Hidden
Markov Model with D = 3 hidden levels that can
be used to parse syntax. Circles denote random vari-
ables, and edges denote conditional dependencies.
Shaded circles denote variables with observed val-
ues.
sive phrase structure trees using the tree transforms
in Schuler et al (2010). Constituent nontermi-
nals in right-corner transformed trees take the form
of incomplete constituents c?/c?? consisting of an
?active? constituent c? lacking an ?awaited? con-
stituent c?? yet to come, similar to non-constituent
categories in a Combinatory Categorial Grammar
(Ades and Steedman, 1982; Steedman, 2000). As
an example, the parser might consider VP/NN as a
possible category for input ?meets the?.
A sample phrase structure tree is shown before
and after the right-corner transform in Figures 2
and 3. Our parser operates over a right-corner trans-
formed probabilistic context-free grammar (PCFG).
Parsing runs in linear time on the length of the input.
This model of incremental parsing is implemented
as a Hierarchical Hidden Markov Model (HHMM)
(Murphy and Paskin, 2001), and is equivalent to a
probabilistic pushdown automaton with a bounded
pushdown store. The parser runs in O(n) time,
where n is the number of words in the input. This
model is shown graphically in Figure 4 and formally
defined in ?4.1 below.
The incremental parser assigns a probability
(Eq. 5) for a partial target language hypothesis, using
a bounded store of incomplete constituents c?/c??.
The phrase-based decoder uses this probability value
as the syntactic language model feature score.
4.1 Formal Parsing Model: Scoring Partial
Translation Hypotheses
This model is essentially an extension of an HHMM,
which obtains a most likely sequence of hidden store
states, s?1..D1..T , of some length T and some maxi-
mum depth D, given a sequence of observed tokens
(e.g. generated target language words), e1..T , using
HHMM state transition model ?A and observation
symbol model ?B (Rabiner, 1990):
s?1..D1..T
def
= argmax
s1..D1..T
T?
t=1
P?A(s
1..D
t | s
1..D
t?1 )?P?B(et | s
1..D
t )
(8)
The HHMM parser is equivalent to a probabilis-
tic pushdown automaton with a bounded push-
down store. The model generates each successive
store (using store model ?S) only after considering
whether each nested sequence of incomplete con-
stituents has completed and reduced (using reduc-
tion model ?R):
P?A(s
1..D
t | s
1..D
t?1 )
def
=
?
r1t ..r
D
t
D?
d=1
P?R(r
d
t | r
d+1
t s
d
t?1s
d?1
t?1 )
? P?S(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t ) (9)
Store elements are defined to contain only the
active (c?) and awaited (c??) constituent categories
necessary to compute an incomplete constituent
probability:
sdt
def
= ?c?, c??? (10)
Reduction states are defined to contain only the
complete constituent category crdt necessary to com-
pute an inside likelihood probability, as well as a
flag frdt indicating whether a reduction has taken
place (to end a sequence of incomplete constituents):
rdt
def
= ?crdt , frdt ? (11)
The model probabilities for these store elements
and reduction states can then be defined (from Mur-
phy and Paskin 2001) to expand a new incomplete
constituent after a reduction has taken place (frdt =
1; using depth-specific store state expansion model
?S-E,d), transition along a sequence of store elements
624
s11
s21
s31
e1
t=1
r12
r22
r32
s12
s22
s32
e2
t=2
r13
r23
r33
s13
s23
s33
e3
t=3
r14
r24
r34
s14
s24
s34
e4
t=4
r15
r25
r35
s15
s25
s35
e5
t=5
r16
r26
r36
s16
s26
s36
e6
t=6
r17
r27
r37
s17
s27
s37
e7
t=7
r18
r28
r38
=DT
=NP/NN
=NP
=NN
=S/VP
=VB
=S/VP
=VP/NP
=DT
=VP/NN
=S/VP
=NN
=VP
=S/PP
=IN
=S/NP
=S
=NP
=The =president =meets =the =board =on =Friday
Figure 5: Graphical representation of the Hierarchic Hidden Markov Model after parsing input sentence The
president meets the board on Friday. The shaded path through the parse lattice illustrates the recognized
right-corner tree structure of Figure 3.
if no reduction has taken place (frdt =0; using depth-
specific store state transition model ?S-T,d): 2
P?S(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t )
def
=
?
?
?
if frd+1t =1, frdt =1 : P?S-E,d(s
d
t | s
d?1
t )
if frd+1t =1, frdt =0 : P?S-T,d(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t )
if frd+1t =0, frdt =0 : Js
d
t = s
d
t?1K
(12)
and possibly reduce a store element (terminate
a sequence) if the store state below it has re-
duced (frd+1t = 1; using depth-specific reduction
model ?R,d):
P?R(r
d
t | r
d+1
t s
d
t?1s
d?1
t?1 )
def
=
{
if frd+1t =0 : Jr
d
t = r?K
if frd+1t =1 : P?R,d(r
d
t | r
d+1
t s
d
t?1 s
d?1
t?1 )
(13)
where r? is a null state resulting from the failure of
an incomplete constituent to complete, and constants
are defined for the edge conditions of s0t and r
D+1
t .
Figure 5 illustrates this model in action.
These pushdown automaton operations are then
refined for right-corner parsing (Schuler, 2009),
distinguishing active transitions (model ?S-T-A,d, in
which an incomplete constituent is completed, but
not reduced, and then immediately expanded to a
2An indicator function J?K is used to denote deterministic
probabilities: J?K = 1 if ? is true, 0 otherwise.
new incomplete constituent in the same store el-
ement) from awaited transitions (model ?S-T-W,d,
which involve no completion):
P?S-T,d(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t )
def
=
{
if rdt 6=r? : P?S-T-A,d(s
d
t | s
d?1
t r
d
t )
if rdt =r? : P?S-T-W,d(s
d
t | s
d
t?1r
d+1
t )
(14)
P?R,d(r
d
t | r
d+1
t s
d
t?1s
d?1
t?1 )
def
=
{
if crd+1t 6=xt : Jr
d
t = r?K
if crd+1t =xt : P?R-R,d(r
d
t | s
d
t?1s
d?1
t?1 )
(15)
These HHMM right-corner parsing operations are
then defined in terms of branch- and depth-specific
PCFG probabilities ?G-R,d and ?G-L,d: 3
3Model probabilities are also defined in terms of left-
progeny probability distribution E?G-RL?,d which is itself defined
in terms of PCFG probabilities:
E?G-RL?,d(c?
0
? c?0 ...)
def
=
?
c?1
P?G-R,d(c? ? c?0 c?1) (16)
E?G-RL?,d(c?
k
? c?0k0 ...)
def
=
?
c
?0k
E?G-RL?,d(c?
k?1
? c?0k ...)
?
?
c
?0k1
P?G-L,d(c?0k ? c?0k0 c?0k1) (17)
E?G-RL?,d(c?
?
? c?? ...)
def
=
??
k=0
E?G-RL?,d(c?
k
? c?? ...) (18)
E?G-RL?,d(c?
+
? c?? ...)
def
= E?G-RL?,d(c?
?
? c?? ...)
? E?G-RL?,d(c?
0
? c?? ...) (19)
625
???????
president meets
??31
. . .
???????
the board
??51
. . .
s13
s23
s33
e3
r14
r24
r34
s14
s24
s34
e4
r15
r25
r35
s15
s25
s35
e5=meets =the =board
Figure 6: A hypothesis in the phrase-based decoding lattice from Figure 1 is expanded using translation op-
tion the board of source phrase den Vorstand. Syntactic language model state ??31 contains random variables
s1..33 ; likewise ??51 contains s
1..3
5 . The intervening random variables r
1..3
4 , s
1..3
4 , and r
1..3
5 are calculated by
transition function ? (Eq. 6, as defined by ?4.1), but are not stored. Observed random variables (e3..e5) are
shown for clarity, but are not explicitly stored in any syntactic language model state.
? for expansions:
P?S-E,d(?c??, c
?
??? | ??, c??)
def
=
E?G-RL?,d(c?
?
? c?? ...) ? Jx?? = c??? = c??K (20)
? for awaited transitions:
P?S-T-W,d(?c?, c??1? | ?c
?
?, c??? c??0)
def
=
Jc? = c??K ?
P?G-R,d(c?? ? c??0 c??1)
E?G-RL?,d(c??
0
? c??0 ...)
(21)
? for active transitions:
P?S-T-A,d(?c??, c??1? | ??, c?? c??0)
def
=
E?G-RL?,d(c?
?
? c?? ...) ? P?G-L,d(c?? ? c??0 c??1)
E?G-RL?,d(c?
+
? c??0 ...)
(22)
? for cross-element reductions:
P?R-R,d(c??,1 | ??, c?? ?c
?
??,??)
def
=
Jc?? = c???K ?
E?G-RL?,d(c?
0
? c?? ...)
E?G-RL?,d(c?
?
? c?? ...)
(23)
? for in-element reductions:
P?R-R,d(c??,0 | ??, c?? ?c
?
??,??)
def
=
Jc?? = c???K ?
E?G-RL?,d(c?
+
? c?? ...)
E?G-RL?,d(c?
?
? c?? ...)
(24)
We use the parser implementation of (Schuler,
2009; Schuler et al, 2010).
5 Phrase Based Translation with an
Incremental Syntactic Language Model
The phrase-based decoder is augmented by adding
additional state data to each hypothesis in the de-
626
coder?s hypothesis stacks. Figure 1 illustrates an ex-
cerpt from a standard phrase-based translation lat-
tice. Within each decoder stack t, each hypothe-
sis h is augmented with a syntactic language model
state ??th . Each syntactic language model state is
a random variable store, containing a slice of ran-
dom variables from the HHMM. Specifically, ??th
contains those random variables s1..Dt that maintain
distributions over syntactic elements.
By maintaining these syntactic random variable
stores, each hypothesis has access to the current
language model probability for the partial transla-
tion ending at that hypothesis, as calculated by an
incremental syntactic language model defined by
the HHMM. Specifically, the random variable store
at hypothesis h provides P(??th) = P(e
h
1..t, s
1..D
1..t ),
where eh1..t is the sequence of words in a partial hy-
pothesis ending at h which contains t target words,
and where there are D syntactic random variables in
each random variable store (Eq. 5).
During stack decoding, the phrase-based decoder
progressively constructs new hypotheses by extend-
ing existing hypotheses. New hypotheses are placed
in appropriate hypothesis stacks. In the simplest
case, a new hypothesis extends an existing hypothe-
sis by exactly one target word. As the new hypothe-
sis is constructed by extending an existing stack ele-
ment, the store and reduction state random variables
are processed, along with the newly hypothesized
word. This results in a new store of syntactic ran-
dom variables (Eq. 6) that are associated with the
new stack element.
When a new hypothesis extends an existing hy-
pothesis by more than one word, this process is first
carried out for the first new word in the hypothe-
sis. It is then repeated for the remaining words in
the hypothesis extension. Once the final word in
the hypothesis has been processed, the resulting ran-
dom variable store is associated with that hypoth-
esis. The random variable stores created for the
non-final words in the extending hypothesis are dis-
carded, and need not be explicitly retained.
Figure 6 illustrates this process, showing how a
syntactic language model state ??51 in a phrase-based
decoding lattice is obtained from a previous syn-
tactic language model state ??31 (from Figure 1) by
parsing the target language words from a phrase-
based translation option.
In-domain Out-of-domain
LM WSJ 23 ppl ur-en dev ppl
WSJ 1-gram 1973.57 3581.72
WSJ 2-gram 349.18 1312.61
WSJ 3-gram 262.04 1264.47
WSJ 4-gram 244.12 1261.37
WSJ 5-gram 232.08 1261.90
WSJ HHMM 384.66 529.41
Interpolated WSJ
5-gram + HHMM 209.13 225.48
Giga 5-gram 258.35 312.28
Interp. Giga 5-gr
+ WSJ HHMM 222.39 123.10
Interp. Giga 5-gr
+ WSJ 5-gram 174.88 321.05
Figure 7: Average per-word perplexity values.
HHMM was run with beam size of 2000. Bold in-
dicates best single-model results for LMs trained on
WSJ sections 2-21. Best overall in italics.
Our syntactic language model is integrated into
the current version of Moses (Koehn et al, 2007).
6 Results
As an initial measure to compare language models,
average per-word perplexity, ppl, reports how sur-
prised a model is by test data. Equation 25 calculates
ppl using log base b for a test set of T tokens.
ppl = b
?logbP(e1...eT )
T (25)
We trained the syntactic language model from
?4 (HHMM) and an interpolated n-gram language
model with modified Kneser-Ney smoothing (Chen
and Goodman, 1998); models were trained on sec-
tions 2-21 of the Wall Street Journal (WSJ) tree-
bank (Marcus et al, 1993). The HHMM outper-
forms the n-gram model in terms of out-of-domain
test set perplexity when trained on the same WSJ
data; the best perplexity results for in-domain and
out-of-domain test sets4 are found by interpolating
4In-domain is WSJ Section 23. Out-of-domain are the En-
glish reference translations of the dev section , set aside in
(Baker et al, 2009) for parameter tuning, of the NIST Open
MT 2008 Urdu-English task.
627
Sentence Moses +HHMM +HHMM
length beam=50 beam=2000
10 0.21 533 1143
20 0.53 1193 2562
30 0.85 1746 3749
40 1.13 2095 4588
Figure 8: Mean per-sentence decoding time (in sec-
onds) for dev set using Moses with and without syn-
tactic language model. HHMM parser beam sizes
are indicated for the syntactic LM.
HHMM and n-gram LMs (Figure 7). To show the
effects of training an LM on more data, we also re-
port perplexity results on the 5-gram LM trained for
the GALE Arabic-English task using the English Gi-
gaword corpus. In all cases, including the HHMM
significantly reduces perplexity.
We trained a phrase-based translation model on
the full NIST Open MT08 Urdu-English translation
model using the full training data. We trained the
HHMM and n-gram LMs on the WSJ data in order
to make them as similar as possible. During tuning,
Moses was first configured to use just the n-gram
LM, then configured to use both the n-gram LM and
the syntactic HHMM LM. MERT consistently as-
signed positive weight to the syntactic LM feature,
typically slightly less than the n-gram LM weight.
In our integration with Moses, incorporating a
syntactic language model dramatically slows the de-
coding process. Figure 8 illustrates a slowdown
around three orders of magnitude. Although speed
remains roughly linear to the size of the source sen-
tence (ruling out exponential behavior), it is with an
extremely large constant time factor. Due to this
slowdown, we tuned the parameters using a con-
strained dev set (only sentences with 1-20 words),
and tested using a constrained devtest set (only sen-
tences with 1-20 words). Figure 9 shows a statis-
tically significant improvement to the BLEU score
when using the HHMM and the n-gram LMs to-
gether on this reduced test set.
7 Discussion
This paper argues that incremental syntactic lan-
guages models are a straightforward and appro-
Moses LM(s) BLEU
n-gram only 18.78
HHMM + n-gram 19.78
Figure 9: Results for Ur-En devtest (only sentences
with 1-20 words) with HHMM beam size of 2000
and Moses settings of distortion limit 10, stack size
200, and ttable limit 20.
priate algorithmic fit for incorporating syntax into
phrase-based statistical machine translation, since
both process sentences in an incremental left-to-
right fashion. This means incremental syntactic LM
scores can be calculated during the decoding pro-
cess, rather than waiting until a complete sentence is
posited, which is typically necessary in top-down or
bottom-up parsing.
We provided a rigorous formal definition of in-
cremental syntactic languages models, and detailed
what steps are necessary to incorporate such LMs
into phrase-based decoding. We integrated an incre-
mental syntactic language model into Moses. The
translation quality significantly improved on a con-
strained task, and the perplexity improvements sug-
gest that interpolating between n-gram and syntactic
LMs may hold promise on larger data sets.
The use of very large n-gram language models is
typically a key ingredient in the best-performing ma-
chine translation systems (Brants et al, 2007). Our
n-gram model trained only on WSJ is admittedly
small. Our future work seeks to incorporate large-
scale n-gram language models in conjunction with
incremental syntactic language models.
The added decoding time cost of our syntactic
language model is very high. By increasing the
beam size and distortion limit of the baseline sys-
tem, future work may examine whether a baseline
system with comparable runtimes can achieve com-
parable translation quality.
A more efficient implementation of the HHMM
parser would speed decoding and make more exten-
sive and conclusive translation experiments possi-
ble. Various additional improvements could include
caching the HHMM LM calculations, and exploiting
properties of the right-corner transform that limit the
number of decisions between successive time steps.
628
References
Anne Abeille?, Yves Schabes, and Aravind K. Joshi.
1990. Using lexicalized tree adjoining grammars for
machine translation. In Proceedings of the 13th Inter-
national Conference on Computational Linguistics.
Anthony E. Ades and Mark Steedman. 1982. On the
order of words. Linguistics and Philosophy, 4:517?
558.
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically informed machine translation
(SIMT). SCALE summer workshop final report, Hu-
man Language Technology Center Of Excellence.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 9?16.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Frederick Jelinek, John Lafferty,
Robert Mercer, and Paul Roossin. 1990. A statisti-
cal approach to machine translation. Computational
Linguistics, 16(2):79?85.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proceedings of the Ninth Ma-
chine Translation Summit of the International Associ-
ation for Machine Translation.
Ciprian Chelba and Frederick Jelinek. 1998. Exploit-
ing syntactic structure for language modeling. In Pro-
ceedings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics, pages 225?
231.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University.
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 72?80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263?270.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452.
John Cocke and Jacob Schwartz. 1970. Program-
ming languages and their compilers. Technical report,
Courant Institute of Mathematical Sciences, New York
University.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 507?514.
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 232?241.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 727?736.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 755?763.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 541?548.
Jay Earley. 1968. An efficient context-free parsing algo-
rithm. Ph.D. thesis, Department of Computer Science,
Carnegie Mellon University.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
205?208.
Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 773?781.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
629
Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 273?
280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 961?
968.
Niyu Ge. 2010. A direct syntax-driven reordering model
for phrase-based machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 849?857.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 80?87.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 105?112.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 288?295.
James Henderson. 2004. Lookahead in deterministic
left-corner parsing. In Proceedings of the Workshop
on Incremental Parsing: Bringing Engineering and
Cognition Together, pages 26?33.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 273?283.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077?1086.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
conference of the Association for Machine Translation
in the Americas.
Kenji Imamura, Hideo Okuma, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Example-based machine transla-
tion based on syntactic transfer with statistical models.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 99?105.
Frederick Jelinek. 1969. Fast sequential decoding al-
gorithm using a stack. IBM Journal of Research and
Development, pages 675?685.
T. Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context free languages. Techni-
cal Report AFCRL-65-758, Air Force Cambridge Re-
search Laboratory.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, pages 177?180.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 609?616.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 704?711.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 558?566.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of the 42nd Meeting of
the Association for Computational Linguistics, pages
653?660.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 206?214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 192?199.
630
Kevin P. Murphy and Mark A. Paskin. 2001. Linear time
inference in hierarchical HMMs. In Proceedings of
Neural Information Processing Systems, pages 833?
840.
Rebecca Nesson, Stuart Shieber, and Alexander Rush.
2006. Induction of probabilistic synchronous tree-
insertion grammars for machine translation. In Pro-
ceedings of the 7th Biennial conference of the Associ-
ation for Machine Translation in the Americas, pages
128?137.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, pages
161?168.
Matt Post and Daniel Gildea. 2008. Parsers as language
models for statistical machine translation. In Proceed-
ings of the Eighth Conference of the Association for
Machine Translation in the Americas, pages 172?181.
Matt Post and Daniel Gildea. 2009. Language modeling
with tree substitution grammars. In NIPS workshop on
Grammar Induction, Representation of Language, and
Language Learning.
Arjen Poutsma. 1998. Data-oriented translation. In
Ninth Conference of Computational Linguistics in the
Netherlands.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 271?279.
Lawrence R. Rabiner. 1990. A tutorial on hid-
den Markov models and selected applications in
speech recognition. Readings in speech recognition,
53(3):267?296.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-coverage incremental
parsing using human-like memory constraints. Com-
putational Linguistics, 36(1):1?30.
William Schuler. 2009. Positive results for parsing with a
bounded stack using a model-based right-corner trans-
form. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 344?352.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 577?585.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree adjoining grammars. In Proceedings of the 13th
International Conference on Computational Linguis-
tics.
Stuart M. Shieber. 2004. Synchronous grammars as tree
transducers. In Proceedings of the Seventh Interna-
tional Workshop on Tree Adjoining Grammar and Re-
lated Formalisms.
Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523?530.
D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n cubed. Information
and Control, 10(2):189?208.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Seng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of the 11th Machine Translation Summit
of the International Association for Machine Transla-
tion, pages 535?542.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141.
631
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1169?1178,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Pronoun Anaphora Resolution System based on
Factorial Hidden Markov Models
Dingcheng Li
University of Minnesota,
Twin Cities, Minnesosta
lixxx345@umn.edu
Tim Miller
University of Wisconsin
Milwaukee, Wisconsin
tmill@cs.umn.edu
William Schuler
The Ohio State University
Columbus, Ohio
schuler@ling.osu.edu
Abstract
This paper presents a supervised pronoun
anaphora resolution system based on factorial
hidden Markov models (FHMMs). The ba-
sic idea is that the hidden states of FHMMs
are an explicit short-term memory with an an-
tecedent buffer containing recently described
referents. Thus an observed pronoun can find
its antecedent from the hidden buffer, or in
terms of a generative model, the entries in the
hidden buffer generate the corresponding pro-
nouns. A system implementing this model is
evaluated on the ACE corpus with promising
performance.
1 Introduction
Pronoun anaphora resolution is the task of find-
ing the correct antecedent for a given pronominal
anaphor in a document. It is a subtask of corefer-
ence resolution, which is the process of determin-
ing whether two or more linguistic expressions in
a document refer to the same entity. Adopting ter-
minology used in the Automatic Context Extraction
(ACE) program (NIST, 2003), these expressions
are called mentions. Each mention is a reference
to some entity in the domain of discourse. Men-
tions usually fall into three categories ? proper men-
tions (proper names), nominal mentions (descrip-
tions), and pronominal mentions (pronouns). There
is a great deal of related work on this subject, so
the descriptions of other systems below are those
which are most related or which the current model
has drawn insight from.
Pairwise models (Yang et al, 2004; Qiu et al,
2004) and graph-partitioning methods (McCallum
and Wellner, 2003) decompose the task into a col-
lection of pairwise or mention set coreference de-
cisions. Decisions for each pair or each group
of mentions are based on probabilities of features
extracted by discriminative learning models. The
aforementioned approaches have proven to be fruit-
ful; however, there are some notable problems. Pair-
wise modeling may fail to produce coherent parti-
tions. That is, if we link results of pairwise deci-
sions to each other, there may be conflicting corefer-
ences. Graph-partitioning methods attempt to recon-
cile pairwise scores into a final coherent clustering,
but they are combinatorially harder to work with in
discriminative approaches.
One line of research aiming at overcoming the
limitation of pairwise models is to learn a mention-
ranking model to rank preceding mentions for a
given anaphor (Denis and Baldridge, 2007) This ap-
proach results in more coherent coreference chains.
Recent years have also seen the revival of in-
terest in generative models in both machine learn-
ing and natural language processing. Haghighi
and Klein (2007), proposed an unsupervised non-
parametric Bayesian model for coreference resolu-
tion. In contrast to pairwise models, this fully gener-
ative model produces each mention from a combina-
tion of global entity properties and local attentional
state. Ng (2008) did similar work using the same un-
supervised generative model, but relaxed head gen-
eration as head-index generation, enforced agree-
ment constraints at the global level, and assigned
salience only to pronouns.
Another unsupervised generative model was re-
cently presented to tackle only pronoun anaphora
1169
resolution (Charniak and Elsner, 2009). The
expectation-maximization algorithm (EM) was ap-
plied to learn parameters automatically from the
parsed version of the North American News Cor-
pus (McClosky et al, 2008). This model generates a
pronoun?s person, number and gender features along
with the governor of the pronoun and the syntactic
relation between the pronoun and the governor. This
inference process allows the system to keep track of
multiple hypotheses through time, including multi-
ple different possible histories of the discourse.
Haghighi and Klein (2010) improved their non-
parametric model by sharing lexical statistics at the
level of abstract entity types. Consequently, their
model substantially reduces semantic compatibility
errors. They report the best results to date on the
complete end-to-end coreference task. Further, this
model functions in an online setting at mention level.
Namely, the system identifies mentions from a parse
tree and resolves resolution with a left-to-right se-
quential beam search. This is similar to Luo (2005)
where a Bell tree is used to score and store the
searching path.
In this paper, we present a supervised pro-
noun resolution system based on Factorial Hidden
Markov Models (FHMMs). This system is moti-
vated by human processing concerns, by operating
incrementally and maintaining a limited short term
memory for holding recently mentioned referents.
According to Clark and Sengul (1979), anaphoric
definite NPs are much faster retrieved if the an-
tecedent of a pronoun is in immediately previous
sentence. Therefore, a limited short term memory
should be good enough for resolving the majority of
pronouns. In order to construct an operable model,
we also measured the average distance between pro-
nouns and their antecedents as discussed in next sec-
tions and used distances as important salience fea-
tures in the model.
Second, like Morton (2000), the current sys-
tem essentially uses prior information as a dis-
course model with a time-series manner, using a
dynamic programming inference algorithm. Third,
the FHMM described here is an integrated system,
in contrast with (Haghighi and Klein, 2010). The
model generates part of speech tags as simple struc-
tural information, as well as related semantic in-
formation at each time step or word-by-word step.
While the framework described here can be ex-
tended to deeper structural information, POS tags
alone are valuable as they can be used to incorpo-
rate the binding features (described below).
Although the system described here is evaluated
for pronoun resolution, the framework we describe
can be extended to more general coreference resolu-
tion in a fairly straightforward manner. Further, as
in other HMM-based systems, the system can be ei-
ther supervised or unsupervised. But extensions to
unsupervised learning are left for future work.
The final results are compared with a few super-
vised systems as the mention-ranking model (De-
nis and Baldridge, 2007) and systems compared in
their paper, and Charniak and Elsner?s (2009) unsu-
pervised system, emPronouns. The FHMM-based
pronoun resolution system does a better job than the
global ranking technique and other approaches. This
is a promising start for this novel FHMM-based pro-
noun resolution system.
2 Model Description
This work is based on a graphical model framework
called Factorial Hidden Markov Models (FHMMs).
Unlike the more commonly known Hidden Markov
Model (HMM), in an FHMM the hidden state at
each time step is expanded to contain more than one
random variable (as shown in Figure 1). This al-
lows for the use of more complex hidden states by
taking advantage of conditional independence be-
tween substates. This conditional independence al-
lows complex hidden states to be learned with lim-
ited training data.
2.1 Factorial Hidden Markov Model
Factorial Hidden Markov Models are an extension
of HMMs (Ghahramani and Jordan, 1997). HMMs
represent sequential data as a sequence of hidden
states generating observation states (words in this
case) at corresponding time steps t. A most likely
sequence of hidden states can then be hypothesized
given any sequence of observed states, using Bayes
Law (Equation 2) and Markov independence as-
sumptions (Equation 3) to define a full probability as
the product of a Transition Model (?T ) prior prob-
ability and an Observation Model (?O) likelihood
1170
probability.
h?1..T
def= argmax
h1..T
P(h1..T | o1..T ) (1)
def= argmax
h1..T
P(h1..T ) ? P(o1..T |h1..T ) (2)
def= argmax
h1..T
T
?
t=1
P?T (ht |ht?1) ? P?O(ot |ht)
(3)
For a simple HMM, the hidden state corresponding
to each observation state only involves one variable.
An FHMM contains more than one hidden variable
in the hidden state. These hidden substates are usu-
ally layered processes that jointly generate the ev-
idence. In the model described here, the substates
are also coupled to allow interaction between the
separate processes. As Figure 1 shows, the hidden
states include three sub-states, op, cr and pos which
are short forms of operation, coreference feature and
part-of-speech. Then, the transition model expands
the left term in (3) to (4).
P?T (ht |ht?1)
def= P(opt | opt?1, post?1)
?P(crt | crt?1, opt?1)
?P(post | opt, post?1)
(4)
The observation model expands from the right
term in (3) to (5).
P?O(ot |ht)
def= P(ot | post, crt) (5)
The observation state depends on more than one hid-
den state at each time step in an FHMM. Each hid-
den variable can be further split into smaller vari-
ables. What these terms stand for and the motiva-
tions behind the above equations will be explained
in the next section.
2.2 Modeling a Coreference Resolver with
FHMMs
FHMMs in our model, like standard HMMs, can-
not represent the hierarchical structure of a syntac-
tic phrase. In order to partially represent this in-
formation, the head word is used to represent the
whole noun phrase. After coreference is resolved,
the coreferring chain can then be expanded to the
whole phrase with NP chunker tools.
In this system, hidden states are composed of
three main variables: a referent operation (OP),
coreference features (CR) and part of speech tags
(POS) as displayed in Figure 1. The transition model
is defined as Equation 4.
opt-1=
copy
post-1=
VBZ
ot-1=loves
et-1=
per,org
gt-1=
neu,fem
crt-1
opt=
old
post=
PRP
ot=them
gt=
fem,neu
crt
ht-1 ht
et=
org,per
nt-1=
plu,sing
nt=
sing,plu
it-1=
-,2
it=
0,2
Figure 1: Factorial HMM CR Model
The starting point for the hidden state at each time
step is the OP variable, which determines which
kind of referent operations will occur at the current
word. Its domain has three possible states: none,
new and old.
The none state indicates that the present state will
not generate a mention. All previous hidden state
values (the list of previous mentions) will be passed
deterministically (with probability 1) to the current
time step without any changes. The new state signi-
fies that there is a new mention in the present time
step. In this event, a new mention will be added to
the entity set, as represented by its set of feature val-
ues and position in the coreference table. The old
state indicates that there is a mention in the present
time state and that this mention refers back to some
antecedent mention. In such a case, the list of enti-
ties in the buffer will be reordered deterministically,
moving the currently mentioned entity to the top of
the list.
Notice that opt is defined to depend on opt?1
and post?1. This is sometimes called a switching
FHMM (Duh, 2005). This dependency can be use-
ful, for example, if opt?1 is new, in which case opt
has a higher probability of being none or old. If
1171
post?1 is a verb or preposition, opt has more proba-
bility of being old or new.
One may wonder why opt generates post, and
not the other way around. This model only roughly
models the process of (new and old) entity genera-
tion, and either direction of causality might be con-
sistent with a model of human entity generation,
but this direction of causality is chosen to represent
the effect of semantics (referents) generating syn-
tax (POS tags). In addition, this is a joint model in
which POS tagging and coreference resolution are
integrated together, so the best combination of those
hidden states will be computed in either case.
2.3 Coreference Features
Coreference features for this model refer to features
that may help to identify co-referring entities.
In this paper, they mainly include index (I),
named entity type (E), number (N) and gender (G).
The index feature represents the order that a men-
tion was encountered relative to the other mentions
in the buffer. The latter three features are well
known and described elsewhere, and are not them-
selves intended as the contribution of this work. The
novel aspect of this part of the model is the fact that
the features are carried forward, updated after ev-
ery word, and essentially act as a discourse model.
The features are just a shorthand way of represent-
ing some well known essential aspects of a referent
(as pertains to anaphora resolution) in a discourse
model.
Features Values
I positive integers from 1. . .n
G male, female, neutral, unknown
N singular, plural, unknown
E person, location, organization,
GPE, vehicle,
company, facility
Table 1: Coreference features stored with each mention.
Unlike discriminative approaches, generative
models like the FHMM described here do not have
access to all observations at once. This model must
then have a mechanism for jointly considering pro-
nouns in tandem with previous mentions, as well as
the features of those mentions that might be used to
find matches between pronouns and antecedents.
Further, higher order HMMs may contain more
accurate information about observation states. This
is especially true for coreference resolution because
pronouns often refer back to mentions that are far
away from the present state. In this case, we would
need to know information about mentions which are
at least two mentions before the present one. In
this sense, a higher order HMM may seem ideal
for coreference resolution. However, higher order
HMMs will quickly become intractable as the order
increases.
In order to overcome these limitations, two strate-
gies which have been discussed in the last section
are taken: First, a switching variable called OP is
designed (as discussed in last section); second, a
memory of recently mentioned entities is maintained
to store features of mentions and pass them forward
incrementally.
OP is intended to model the decision to use the
current word to introduce a new referent (new), refer
to an antecedent (old), or neither (none). The entity
buffer is intended to model the set of ?activated? en-
tities in the discourse ? those which could plausibly
be referred to with a pronoun. These designs allow
similar benefits as longer dependencies of higher-
order HMMs but avoid the problem of intractability.
The number of mentions maintained must be limited
in order for the model to be tractable. Fortunately,
human short term memory faces effectively similar
limitations and thus pronouns usually refer back to
mentions not very far away.
Even so, the impact of the size of the buffer on
decoding time may be a concern. Since the buffer of
our system will carry forward a few previous groups
of coreference features plus op and pos, the compu-
tational complexity will be exorbitantly high if we
keep high beam size and meanwhile if each feature
interacts with others. Luckily, we have successfully
reduced the intractability to a workable system in
both speed and space with following methods. First,
we estimate the size of buffer with a simple count
of average distances between pronouns and their an-
tecedents in the corpus. It is found that about six is
enough for covering 99.2% of all pronouns.
Secondly, the coreference features we have used
have the nice property of being independent from
one another. One might expect English non-person
entities to almost always have neutral gender, and
1172
thus be modeled as follows:
P(et, gt | et?1, gt?1) = P(gt | gt?1, et) ? P(et | et?1)
(6)
However, a few considerations made us reconsider.
First, exceptions are found in the corpus. Personal
pronouns such as she or he are used to refer to coun-
try, regions, states or organizations. Second, existing
model files made by Bergsma (2005) include a large
number of non-neutral gender information for non-
person words. We employ these files for acquiring
gender information of unknown words. If we use
Equation 6, sparsity and complexity will increase.
Further, preliminary experiments have shown mod-
els using an independence assumption between gen-
der and personhood work better. Thus, we treat each
coreference feature as an independent event. Hence,
we can safely split coreference features into sepa-
rate parts. This way dramatically reduces the model
complexity. Thirdly, our HMM decoding uses the
Viterbi algorithm with A-star beam search.
The probability of the new state of the coreference
table P(crt | crt?1, opt) is defined to be the product
of probabilities of the individual feature transitions.
P(crt | crt?1, opt) = P(it | it?1, opt)?
P(et | et?1, it, opt)?
P(gt | gt?1, it, opt)?
P(nt |nt?1, it, opt)
(7)
This supposes that the features are conditionally in-
dependent of each other given the index variable, the
operator and previous instance. Each feature only
depends on the operator and the corresponding fea-
ture at the previous state, with that set of features
re-ordered as specified by the index model.
2.4 Feature Passing
Equation 7 is correct and complete, but in fact the
switching variable for operation type results in three
different cases which simplifies the calculation of
the transition probabilities for the coreference fea-
ture table.
Note the following observations about corefer-
ence features: it only needs a probabilistic model
when opt is old ? in other words, only when the
model must choose between several antecedents to
re-refer to. gt, et and nt are deterministic except
when opt is new, when gender, entity type, and num-
ber information must be generated for the new entity
being introduced.
When opt is none, all coreference variables (en-
tity features) will be copied over from the previous
time step to the current time step, and the probabil-
ity of this transition is 1.0. When opt is new, it is
changed deterministically by adding the new entity
to the first position in the list and moving every other
entity down one position. If the list of entities is
full, the least recently mentioned entity will be dis-
carded. The values for the top of the feature lists
gt, et, and nt will then be generated from feature-
specific probability distributions estimated from the
training data. When opt is old, it will probabilisti-
cally select a value 1 . . . n, for an entity list contain-
ing n items. The selected value will deterministi-
cally order the gt, nt and et lists. This distribution
is also estimated from training data, and takes into
account recency of mention. The shape of this dis-
tribution varies slightly depending on list size and
noise in the training data, but in general the probabil-
ity of a mention being selected is directly correlated
to how recently it was mentioned.
With this understanding, coreference table tran-
sition probabilities can be written in terms of only
their non-deterministic substate distributions:
P(crt | crt?1, old) = Pold(it | it?1)?
Preorder(et | et?1, it)?
Preorder(gt | gt?1, it)?
Preorder(nt |nt?1, it)
(8)
where the old model probabilistically selects the an-
tecedent and moves it to the top of the list as de-
scribed above, thus deciding how the reordering will
take place. The reorder model actually implements
the list reordering for each independent feature by
moving the feature value corresponding to the se-
lected entity in the index model to the top of that
feature?s list. The overall effect is simply the prob-
abilistic reordering of entities in a list, where each
entity is defined as a label and a set of features.
P(crt | crt?1, new) = Pnew(it | it?1)?
Pnew(gt | gt?1)?
Pnew(nt |nt?1)?
Pnew(et | et?1)
(9)
where the new model probabilistically generates a
1173
feature value based on the training data and puts it
at the top of the list, moves every other entity down
one position in the list, and removes the final item if
the list is already full. Each entity in i takes a value
from 1 to n for a list of size n. Each g can be one of
four values ? male, female, neuter and unknown; n
one of three values ? plural, singular and unknown
and e around eight values.
Note that post is used in both hidden states and
observation states. While it is not considered a
coreference feature as such, it can still play an im-
portant role in the resolving process. Basically, the
system tags parts of speech incrementally while si-
multaneously resolving pronoun anaphora. Mean-
while, post?1 and opt?1 will jointly generate opt.
This point has been discussed in Section 2.2.
Importantly, the pos model can help to imple-
ment binding principles (Chomsky, 1981). It is
applied when opt is old. In training, pronouns
are sub-categorised into personal pronouns, reflex-
ive and other-pronoun. We then define a vari-
able loct whose value is how far back in the list
of antecedents the current hypothesis must have
gone to arrive at the current value of it. If we
have the syntax annotations or parsed trees, then,
the part of speech model can be defined when
opt is old as Pbinding(post | loct, sloct). For ex-
ample, if post ? ref lexive, P(post | loct, sloct)
where loct has smaller values (implying closer men-
tions to post) and sloct = subject should have
higher values since reflexive pronouns always re-
fer back to subjects within its governing domains.
This was what (Haghighi and Klein, 2009) did and
we did this in training with the REUTERS cor-
pus (Hasler et al, 2006) in which syntactic roles
are annotated. We finally switched to the ACE
corpus for the purpose of comparison with other
work. In the ACE corpus, no syntactic roles are
annotated. We did use the Stanford parser to ex-
tract syntactic roles from the ACE corpus. But
the result is largely affected by the parsing accu-
racy. Again, for a fair comparison, we extract simi-
lar features to Denis and Baldridge (2007), which is
the model we mainly compare with. They approx-
imate syntactic contexts with POS tags surround-
ing the pronoun. Inspired by this idea, we success-
fully represent binding features with POS tags be-
fore anaphors. Instead of using P(post | loct, sloct),
we train P(post | loct, posloct) which can play
the role of binding. For example, suppose the
buffer size is 6 and loct = 5, posloct = noun.
Then, P(post = ref lexive | loct, posloct) is usu-
ally higher than P(post = pronoun | loct, posloct),
since the reflexive has a higher probability of refer-
ring back to the noun located in position 5 than the
pronoun.
In future work expanding to coreference resolu-
tion between any noun phrases we intend to inte-
grate syntax into this framework as a joint model of
coreference resolution and parsing.
3 Observation Model
The observation model that generates an observed
state is defined as Equation 5. To expand that equa-
tion in detail, the observation state, the word, de-
pends on its part of speech and its coreference fea-
tures as well. Since FHMMs are generative, we can
say part of speech and coreference features generate
the word.
In actual implementation, the observed model will
be very sparse, since crt will be split into more vari-
ables according to how many coreference features it
is composed of. In order to avoid the sparsity, we
transform the equation with Bayes? law as follows.
P?O(ot |ht) =
P (ot) ? P(ht | ot)
?
o? P (o?)P(ht | o?)
(10)
= P (ot) ? P(post, crt | ot)?
o? P (o?)P(post, crt | o?)
(11)
We define pos and cr to be independent of each
other, so we can further split the above equation as:
P?O(ot |ht)
def= P (ot) ? P(post | ot) ? P(crt | ot)?
o? P (o?) ? P(post | o?) ? P(crt | o?)
(12)
where P(crt | ot) = P(gt | ot)P(nt | ot)P(et | ot) and
P(crt | o?) = P(gt | o?)P(nt | o?)P(et | o?).
This change transforms the FHMM to a hybrid
FHMM since the observation model no longer gen-
erates the data. Instead, the observation model gen-
erates hidden states, which is more a combination
of discriminative and generative approaches. This
way facilitates building likelihood model files of fea-
tures for given mentions from the training data. The
1174
hidden state transition model represents prior proba-
bilities of coreference features associated with each
while this observation model factors in the probabil-
ity given a pronoun.
3.1 Unknown Words Processing
If an observed word was not seen in training, the
distribution of its part of speech, gender, number and
entity type will be unknown. In this case, a special
unknown words model is used.
The part of speech of unknown words
P(post |wt = unkword) is estimated using a
decision tree model. This decision tree is built
by splitting letters in words from the end of the
word backward to its beginning. A POS tag is
assigned to the word after comparisons between
the morphological features of words trained from
the corpus and the strings concatenated from the
tree leaves are made. This method is about as
accurate as the approach described by Klein and
Manning (2003).
Next, a similar model is set up for estimating
P(nt |wt = unkword). Most English words have
regular plural forms, and even irregular words have
their patterns. Therefore, the morphological features
of English words can often be used to determine
whether a word is singular or plural.
Gender is irregular in English, so model-based
predictions are problematic. Instead, we follow
Bergsma and Lin (2005) to get the distribution of
gender from their gender/number data and then pre-
dict the gender for unknown words.
4 Evaluation and Discussion
4.1 Experimental Setup
In this research, we used the ACE corpus (Phase 2) 1
for evaluation. The development of this corpus in-
volved two stages. The first stage is called EDT (en-
tity detection and tracking) while the second stage
is called RDC (relation detection and characteriza-
tion). All markables have named entity types such
as FACILITY, GPE (geopolitical entity), PERSON,
LOCATION, ORGANIZATION, PERSON, VEHI-
CLE and WEAPONS, which were annotated in the
first stage. In the second stage, relations between
1See http://projects.ldc.upenn.edu/ace/
annotation/previous/ for details on the corpus.
named entities were annotated. This corpus include
three parts, composed of different genres: newspa-
per texts (NPAPER), newswire texts (NWIRE) and
broadcasted news (BNEWS). Each of these is split
into a train part and a devtest part. For the train
part, there are 76, 130 and 217 articles in NPA-
PER, NWIRE and BNEWS respectively while for
the test part, there are 17, 29 and 51 articles respec-
tively. Though the number of articles are quite dif-
ferent for three genres, the total number of words are
almost the same. Namely, the length of NPAPER
is much longer than BNEWS (about 1200 words,
800 word and 500 words respectively for three gen-
res). The longer articles involve longer coreference
chains. Following the common practice, we used
the devtest material only for testing. Progress during
the development phase was estimated only by using
cross-validation on the training set for the BNEWS
section. In order to make comparisons with publica-
tions which used the same corpus, we make efforts
to set up identical conditions for our experiments.
The main point of comparison is Denis and
Baldridge (2007), which was similar in that it de-
scribed a new type of coreference resolver using
simple features.
Therefore, similar to their practice, we use all
forms of personal and possessive pronouns that were
annotated as ACE ?markables?. Namely, pronouns
associated with named entity types could be used in
this system. In experiments, we also used true ACE
mentions as they did. This means that pleonastics
and references to eventualities or to non-ACE enti-
ties are not included in our experiments either. In
all, 7263 referential pronouns in training data set
and 1866 in testing data set are found in all three
genres. They have results of three different systems:
SCC (single candidate classifier), TCC (twin candi-
date classifier) and RK (ranking). Besides the three
and our own system, we also report results of em-
Pronouns, which is an unsupervised system based
on a recently published paper (Charniak and Elsner,
2009). We select this unsupervised system for two
reasons. Firstly, emPronouns is a publicly available
system with high accuracy in pronoun resolution.
Secondly, it is necessary for us to demonstrate our
system has strong empirical superiority over unsu-
pervised ones. In testing, we also used the OPNLP
Named Entity Recognizer to tag the test corpus.
1175
During training, besides coreference annotation
itself, the part of speech, dependencies between
words and named entities, gender, number and index
are extracted using relative frequency estimation to
train models for the coreference resolution system.
Inputs for testing are the plain text and the trained
model files. The entity buffer used in these exper-
iments kept track of only the six most recent men-
tions. The result of this process is an annotation
of the headword of every noun phrase denoting it
as a mention. In addition, this system does not
do anaphoricity detection, so the antecedent oper-
ation for non-anaphora pronoun it is set to be none.
Finally, the system does not yet model cataphora,
about 10 cataphoric pronouns in the testing data
which are all counted as wrong.
4.2 Results
The performance was evaluated using the ratio of
the number of correctly resolved anaphors over the
number of all anaphors as a success metrics. All the
standards are consistent with those defined in Char-
niak and Elsner (2009).
During development, several preliminary experi-
ments explored the effects of starting from a simple
baseline and adding more features. The BNEWS
corpus was employed in these development exper-
iments. The baseline only includes part of speech
tags, the index feature and and syntactic roles. Syn-
tactic roles are extracted from the parsing results
with Stanford parser. The success rate of this base-
line configuration is 0.48. This low accuracy is par-
tially due to the errors of automatic parsing. With
gender and number features added, the performance
jumped to 0.65. This shows that number and gen-
der agreements play an important role in pronoun
anaphora resolution. For a more standard compari-
son to other work, subsequent tests were performed
on the gold standard ACE corpus (using the model
as described with named entity features instead of
syntactic role features). As shown in Denis and
Baldridge (2007), they employ all features we use
except syntactic roles. In these experiments, the sys-
tem got better results as shown in Table 2.
The result of the first one is obtained by running
the publicly available system emPronouns2. It is a
2the available system in fact only includes the testing part.
Thus, it may be unfair to compare emPronouns this way with
System BNEWS NPAPER NWIRE
emPronouns 58.5 64.5 60.6
SCC 62.2 70.7 68.3
TCC 68.6 74.7 71.1
RK 72.9 76.4 72.4
FHMM 74.9 79.4 74.5
Table 2: Accuracy scores for emPronouns, the single-
candidate classifier (SCC), the twin-candidate classifier
(TCC), the ranker and FHMM
high-accuracy unsupervised system which reported
the best result in Charniak and Elsner (2009).
The results of the other three systems are those
reported by Denis and Baldridge (2007). As Table 2
shows, the FHMM system gets the highest average
results.
The emPronouns system got the lowest results
partially due to the reason that we only directly
run the existing system with its existing model files
without retraining. But the gap between its results
and results of our system is large. Thus, we may
still say that our system probably can do a better job
even if we train new models files for emPronouns
with ACE corpus.
With almost exactly identical settings, why does
our FHMM system get the highest average results?
The convincing reason is that FHMM is strongly in-
fluenced by the sequential dependencies. The rank-
ing approach ranks a set of mentions using a set of
features, and it also maintains the discourse model,
but it is not processing sequentially. The FHMM
system always maintain a set of mentions as well
as a first-order dependencies between part of speech
and operator. Therefore, context can be more fully
taken into consideration. This is the main reason that
the FHMM approach achieved better results than the
ranking approach.
From the result, one point we may notice is that
NPAPER usually obtains higher results than both
BNEWS and NWIRE for all systems while BNEWS
lower than other two genres. In last section, we
mention that articles in NPAPER are longer than
other genres and also have denser coreference chains
while articles in BENEWS are shorter and have
sparer chains. Then, it is not hard to understand
why results of NPAPER are better while those of
other systems.
1176
BNEWS are poorer.
In Denis and Baldridge (2007), they also reported
new results with a window of 10 sentences for RK
model. All three genres obtained higher results than
those when with shorter ones. They are 73.0, 77.6
and 75.0 for BNEWS,NPAPER and NWIRE respec-
tively. We can see that except the one for NWIRE,
the results are still poorer than our system. For
NWIRE, the RK model got 0.5 higher. The average
of the RK is 75.2 while that of the FHMM system is
76.3, which is still the best.
Since the emPronoun system can output sample-
level results, it is possible to do a paired Student?s
t-test. That test shows that the improvement of our
system on all three genres is statistically significant
(p < 0.001). Unfortunately, the other systems only
report overall results so the same comparison was
not so straightforward.
4.3 Error Analysis
After running the system on these documents, we
checked which pronouns fail to catch their an-
tecedents. There are a few general reasons for er-
rors.
First, pronouns which have antecedents very far
away cannot be caught. Long-distance anaphora res-
olution may pose a problem since the buffer size
cannot be too long considering the complexity of
tracking a large number of mentions through time.
During development, estimation of an acceptable
size was attempted using the training data. It was
found that a mention distance of fourteen would ac-
count for every case found in this corpus, though
most cases fall well short of that distance. Future
work will explore optimizations that will allow for
larger or variable buffer sizes so that longer distance
anaphora can be detected.
A second source of error is simple misjudgments
when more than one candidate is waiting for selec-
tion. A simple case is that the system fails to distin-
guish plural personal nouns and non-personal nouns
if both candidates are plural. This is not a problem
for singular pronouns since gender features can tell
whether pronouns are personal or not. Plural nouns
in English do not have such distinctions, however.
Consequently, demands and Israelis have the same
probability of being selected as the antecedents for
they, all else being equal. If demands is closer to
they, demands will be selected as the antecedent.
This may lead to the wrong choice if they in fact
refers to Israelis. This may require better measures
of referent salience than the ?least recently used?
heuristic currently implemented.
Third, these results also show difficulty resolv-
ing coordinate noun phrases due to the simplistic
representation of noun phrases in the input. Con-
sider this sentence: President Barack Obama and
his wife Michelle Obama visited China last week.
They had a meeting with President Hu in Beijing.
In this example, the pronoun they corefers with the
noun phrase President Barack Obama and his wife
Michelle Obama. The present model cannot repre-
sent both the larger noun phrase and its contained
noun phrases. Since the noun phrase is a coordinate
one that includes both noun phrases, the model can-
not find a head word to represent it.
Finally, while the coreference feature annotations
of the ACE are valuable for learning feature mod-
els, the model training may still give some mislead-
ing results. This is brought about by missing fea-
tures in the training corpus and by the data sparsity.
We solved the problem with add-one smoothing and
deleted interpolation in training models besides the
transformation in the generation order of the obser-
vation model.
5 Conclusion and Future Work
This paper has presented a pronoun anaphora resolu-
tion system based on FHMMs. This generative sys-
tem incrementally resolves pronoun anaphora with
an entity buffer carrying forward mention features.
The system performs well and outperforms other
available models. This shows that FHMMs and
other time-series models may be a valuable model
to resolve anaphora.
Acknowledgments
We would like to thank the authors and maintainers
of ranker models and emPronouns. We also would
like to thank the three anonymous reviewers. The
final version is revised based on their valuable com-
ments. Thanks are extended to Shane Bergsma, who
provided us the gender and number data distribution.
In addition, Professor Jeanette Gundel and our lab-
mate Stephen Wu also gave us support in paper edit-
ing and in theoretical discussion.
1177
References
S Bergsma. 2005. Automatic acquisition of gender
information for anaphora resolution. page 342353.
Springer.
Eugene Charniak and Micha Elsner. 2009. Em works
for pronoun anaphora resolution. In Proceedings of
the Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-09),
Athens, Greece.
Noam Chomsky. 1981. Lectures on government and
binding. Foris, Dordercht.
H.H. Clark and CJ Sengul. 1979. In search of refer-
ents for nouns and pronouns. Memory & Cognition,
7(1):35?41.
P. Denis and J. Baldridge. 2007. A ranking approach to
pronoun resolution. In Proc. IJCAI.
Kevin Duh. 2005. Jointly labeling multiple sequences:
a factorial HMM approach. In ACL ?05: Proceedings
of the ACL Student Research Workshop, pages 19?24,
Ann Arbor, Michigan.
Zoubin Ghahramani and Michael I. Jordan. 1997. Facto-
rial hidden markov models. Machine Learning, 29:1?
31.
A. Haghighi and D. Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric bayesian model.
In Proceedings of the 45th annual meeting on Associ-
ation for Computational Linguistics, page 848.
A. Haghighi and D. Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3-
Volume 3, pages 1152?1161. Association for Compu-
tational Linguistics.
A. Haghighi and D. Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393. Associa-
tion for Computational Linguistics.
L. Hasler, C. Orasan, and K. Naumann. 2006. NPs
for events: Experiments in coreference annotation. In
Proceedings of the 5th edition of the International
Conference on Language Resources and Evaluation
(LREC2006), pages 1167?1172. Citeseer.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan.
X Luo. 2005. On coreference resolution performance
metrics. pages 25?32. Association for Computational
Linguistics Morristown, NJ, USA.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In IJCAI Workshop on In-
formation Integration on the Web. Citeseer.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. BLLIP North American News Text, Complete.
Linguistic Data Consortium. LDC2008T13.
T.S. Morton. 2000. Coreference for NLP applications.
In Proceedings of the 38th Annual Meeting on Associ-
ation for Computational Linguistics, pages 173?180.
Association for Computational Linguistics.
V. Ng. 2008. Unsupervised models for coreference reso-
lution. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 640?
649. Association for Computational Linguistics.
US NIST. 2003. The ACE 2003 Evaluation Plan. US Na-
tional Institute for Standards and Technology (NIST),
Gaithersburg, MD.[online, pages 2003?08.
L. Qiu, M.Y. Kan, and T.S. Chua. 2004. A public ref-
erence implementation of the rap anaphora resolution
algorithm. Arxiv preprint cs/0406031.
X. Yang, J. Su, G. Zhou, and C.L. Tan. 2004. Im-
proving pronoun resolution by incorporating corefer-
ential information of candidates. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 127. Association for Com-
putational Linguistics.
1178
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 141?150,
Dublin, Ireland, August 23-24 2014.
Cognitive Compositional Semantics using Continuation Dependencies
William Schuler
Department of Linguistics
The Ohio State University
schuler@ling.osu.edu
Adam Wheeler
Department of Linguistics
The Ohio State University
wheeler@ling.osu.edu
Abstract
This paper describes a graphical semantic
representation based on bottom-up ?con-
tinuation? dependencies which has the im-
portant property that its vertices define a
usable set of discourse referents in work-
ing memory even in contexts involving
conjunction in the scope of quantifiers. An
evaluation on an existing quantifier scope
disambiguation task shows that non-local
continuation dependencies can be as reli-
ably learned from annotated data as repre-
sentations used in a state-of-the-art quanti-
fier scope resolver, suggesting that contin-
uation dependencies may provide a natural
representation for scope information.
1 Introduction
It is now fairly well established that at least shal-
low semantic interpretation informs parsing deci-
sions in human sentence processing (Tanenhaus et
al., 1995; Brown-Schmidt et al., 2002), and re-
cent evidence points to incremental processing of
quantifier implicatures as well (Degen and Tanen-
haus, 2011). This may indicate that inferences
about the meaning of quantifiers are processed di-
rectly in working memory. Human working mem-
ory is widely assumed to store events (includ-
ing linguistic events) as re-usable activation-based
states, connected by a durable but rapidly mutable
weight-based memory of cued associations (Marr,
1971; Anderson et al., 1977; Murdock, 1982; Mc-
Clelland et al., 1995; Howard and Kahana, 2002).
Complex dependency structures can therefore be
stored in this associative memory as graphs, with
states as vertices and cued associations as directed
edges (e.g. Kintsch, 1988). This kind of represen-
tation is necessary to formulate and evaluate algo-
rithmic claims (Marr, 1982) about cued associa-
tions and working memory use in human sentence
processing (e.g. van Schijndel and Schuler, 2013).
But accounting for syntax and semantics in this
way must be done carefully in order to preserve
linguistically important distinctions. For example,
positing spurious local dependencies in filler-gap
constructions can lead to missed integrations of
dependency structure in incremental processing,
resulting in weaker model fitting (van Schijndel et
al., 2013). Similar care may be necessary in cases
of dependencies arising from anaphoric corefer-
ence or quantifier scope.
Unfortunately, most existing theories of compo-
sitional semantics (Montague, 1973; Barwise and
Cooper, 1981; Bos, 1996; Baldridge and Kruijff,
2002; Koller, 2004; Copestake et al., 2005) are
defined at the computational level (Marr, 1982),
employing beta reduction over complete or under-
specified lambda calculus expressions as a precise
description of the language processing task to be
modeled, not at the algorithmic level, as a model
of human language processing itself. The struc-
tured expressions these theories generate are not
intended to represent re-usable referential states
of the sort that could be modeled in current theo-
ries of associative memory. As such, it should not
be surprising that structural adaptations of lambda
calculus expressions as referential states exhibit a
number of apparent deficiencies:
First, representations based on lambda calculus
expressions lack topologically distinguishable ref-
erents for sets defined in the context of outscop-
ing quantifiers. For example, a structural adapta-
tion of a lambda calculus expression for the sen-
tence Every line contains two numbers, shown in
Figure 1a (adapted from Koller, 2004), contains
referents for the set of all document lines (s
L
) and
for the set of all numbers (s
N
) which can be iden-
tified by cued associations to predicate constants
like Number, but it is not clear how a referent for
the set of numbers in document lines can be dis-
tinguished from a referent for the set of numbers
141
a)
p
L
Every
0
s
L
?
0
d
L
1
e
L
Line
0
2
1
s
?
L
?
0
d
?
L
1
p
N
Two
0
s
N
?
0
d
N
1
e
N
Number
0
2
1
s
?
N
?
0
d
?
N
1
e
C
Contain
0
2
2
2
2
1
1
1
2
b)
p
L
Every
0
s
L
?
0
d
L
1
e
L
Line
0
2
1
s
?
L
?
0
d
?
L
1
p
A
p
S
A
0
s
S
?
0
d
S
1
e
S
Space
0
2
1
s
?
S
?
0
d
?
S
1
e
B
BeginsWith
0
2
2
1
And
0
p
N
Two
0
s
N
?
0
d
N
1
e
N
Number
0
2
1
s
?
N
?
0
d
?
N
1
e
C
Contain
0
2
2
2
2
2
1
1
1
2 1
1
2
c)
(Every p
L
s
L
s
?
L
) ? (Set s
L
d
L
e
L
) ? (Line e
L
d
L
) ? (Set s
?
L
d
?
L
p
N
) ?
(Two p
N
s
N
s
?
N
) ? (Set s
N
d
N
e
N
) ? (Number e
N
d
N
) ? (Set s
?
N
d
?
N
e
C
) ? (Contain e
C
d
?
L
d
?
N
)
Figure 1: Semantic dependency graph in a ?direct? (top-down) style, adapted from a disambiguated rep-
resentation of Koller (2004), excluding quantifiers over eventualities. The semantic dependency structure
for the sentence Every line contains two numbers (a), with flat logical form (c), is not a subgraph of the
semantic dependency structure for Every line begins with a space and contains two numbers (b), because
the structure is interrupted by the explicit conjunction predicate ?And?.
in each document line (s
?
N
) using local topological
features of the dependency graph, as would be re-
quired to accurately recall assertions about total or
average quantities of numbers in document lines.
1
Second, graphs based on traditional lambda
calculus representations do not model conjuncts
as subgraphs of conjunctions. For example, the
graphical representation of the sentence Every line
1
This graph matching can be implemented in a vectorial
model of associative memory by comparing the (e.g. cosine)
similarity of superposed vectors resulting from cueing in-
coming and outgoing dependencies with all possible labels
in increasingly longer paths from one or more constant vec-
tor states (e.g. vectors for predicate constants). This graph
matching does not necessarily preclude the introduction of
monotonicity constraints from matched quantifiers. For ex-
ample, More than two perl scripts work, can entail More
than two scripts work, using a subgraph in the first argu-
ment, but Fewer than two scripts work, can entail Fewer than
two perl scripts work, using a supergraph in the first argu-
ment. This consideration is similar to those observed in rep-
resentations based on natural logic (MacCartney and Man-
ning, 2009) which also uses low-level matching to perform
some kinds of inference, but representations based on natural
logic typically exclude other forms of inference, whereas the
present model does not.
This matching also assumes properties of nuclear scope
variables are inherited from associated restrictor variables,
e.g. through a set of dependencies from nuclear scope sets
to restrictor sets not shown in the figure. This assumption
will be revisited in Section 3.
begins with a space and contains two numbers
shown in Figure 1b does not contain the graphical
representation of the sentence Every line contains
two numbers shown in Figure 1a as a connected
subgraph. Although one might expect a query
about a conjunct to be directly answerable from
a knowledge base containing the conjoined repre-
sentation, the pattern of dependencies that make
up the conjunct in a graphical representation of a
lambda calculus expression does not match those
in the larger conjunction.
Finally, representations based on lambda calcu-
lus expressions contain vertices that do not seem
to correspond to viable discourse referents. For
example, following the sentence Every line con-
tains two numbers, using the lambda expression
shown in Figure 1b, d
L
may serve as a referent of
it in but it has only one underscore, s
N
may serve
as a referent of they in but they are not negative,
e
C
may serve as a referent of that in but that was
before it was edited, and p
L
may serve as a ref-
erent of that in but the compiler doesn?t enforce
that, but it is not clear what if anything would nat-
urally refer to the internal conjunction p
A
. Predi-
cations over such conjunctions (e.g. Kim believes
that every line begins with a space and contains
142
two numbers) are usually predicated at the outer
proposition p
L
, and in any case do not have truth
values that are independent of the same predica-
tion at each conjunct. One of the goals of Minimal
Recursion Semantics (Copestake et al., 2005) was
to eliminate similar kinds of superfluous conjunc-
tion structure.
Fortunately, lambda calculus expressions like
those shown in Figure 1 are not the only way to
represent compositional semantics of sentences.
This paper defines a graphical semantic depen-
dency representation that can be translated into
lambda calculus, but has the important property
that its vertices define a usable set of discourse
referents in working memory even in contexts in-
volving conjunction in the scope of quantifiers.
It does this by reversing the direction of de-
pendencies from parent-to-child subsumption in
a lambda-calculus tree to a representation sim-
ilar to the inside-out structure of function def-
initions in a continuation-passing style (Barker,
2002; Shan and Barker, 2006)
2
so that sets are de-
fined in terms of their context, and explicit ?And?
predicates are no longer required, leaving noth-
ing to get in the way of an exact pattern match.
3
The learnability of the non-local continuation de-
pendencies involved in this representation is then
evaluated on an existing quantifier scope disam-
biguation task using a dependency-based statisti-
cal scope resolver, with results comparable to a
state-of-the-art unrestricted graph-based quantifier
scope resolver (Manshadi et al., 2013).
2 Continuation Dependencies
This paper explores the use of a bottom-up depen-
dency representation, inspired by the inside-out
structure of function definitions in a continuation-
passing style (Barker, 2002; Shan and Barker,
2006), which creates discourse referents for sets
that are associated with particular scoping con-
texts. This dependency representation preserves
the propositions, sets, eventualities, and ordinary
2
This representation also has much in common with gen-
eralized Skolem terms of Steedman (2012), which also repre-
sent dependencies to outscoped terms, but here continuation
dependencies are applied to all quantifiers, including univer-
sals.
3
This also holds for explicit disjunction predicates, which
can be cast as conjunction through application of de Morgan?s
law and manipulation of the polarity of adjacent quantifiers.
For example, Every line begins with at least one space or
contains at least two numbers, is equivalent to No line be-
gins with fewer than one space and contains fewer than two
numbers.
discourse referents of a ?direct? representation (the
p, s, e, and d nodes in Figure 1), but replaces the
downward dependencies departing set referents
with upward dependencies to context sets (high-
lighted in Figure 2).
Figures 1c and 2c also show flat logical forms
composed of elementary predications, adapted
from Kruijff (2001) and Copestake et al. (2005),
for the sentence Every line contains two numbers,
which are formed by identifying the function as-
sociated with the predicate constant (e.g. Contain)
that is connected to each proposition or eventual-
ity referent (e.g. e
C
) by a dependency labeled ?0?,
then applying that function to this referent, fol-
lowed by the list of arguments connected to this
referent by functions numbered ?1? and up: e.g.
(Contain e
C
d
?
L
d
?
N
). These dependencies can also
be defined by numbered dependency functions f
n
from source instance j to destination instance i,
notated (f
n
j) = i. This notation will be used
in Section 4 to define constraints in the form of
equations. For example, the subject (first argu-
ment) of a lexical item may be constrained to be
the subject (first argument) of that item?s senten-
tial complement (second argument), as in an in-
stance of subject control, using the dependency
equation (f
1
i) = (f
1
(f
2
i)).
Since continuation dependencies all flow up the
tree, any number of conjuncts can impinge upon a
common outscoping continuation, so there is no
longer any need for explicit conjunction nodes.
The representation is also attractive in that it lo-
cally distinguishes queries about, say, the cardi-
nality of the set of numbers in each document line
(Set s
?
N
d
?
N
s
?
L
) from queries about the cardinal-
ity of the set of numbers in general (Set s
?
N
d
?
N
s
?
?
)
which is crucial for successful inference by pattern
matching. Finally, connected sets of continuation
dependencies form natural ?scope graphs? for use
in graph-based disambiguation algorithms (Man-
shadi and Allen, 2011; Manshadi et al., 2013),
which will be used to evaluate this representation
in Section 6.
3 Mapping to Lambda Calculus
It is important for this representation not only
to have attractive graphical subsumption proper-
ties, but also to be sufficiently expressive to de-
fine corresponding expressions in lambda calcu-
lus. When continuation dependencies are filled in,
the resulting dependency structure can be trans-
143
a)
e
L
Line
p
L
Every
0
s
L
?
0
d
L
1
1
s
?
L
?
0
d
?
L
1
2
e
N
Number
p
N
Two
0
s
N
?
0
d
N
1
1
s
?
N
?
0
d
?
N
1
2
p
C
Some
0
s
C
?
0
e
C
Contain
0
1
1
s
?
C
?
0
e
?
C
1
2
1 1
1
2
2
2
b)
e
L
Line
p
L
Every
0
s
L
?
0
d
L
1
1
s
?
L
?
0
d
?
L
1
2
e
S
Space
p
S
A
0
s
S
?
0
d
S
1
1
s
?
S
?
0
d
?
S
1
2
p
B
Some
0
s
B
?
0
e
B
BeginWith
0
1
1
s
?
B
?
0
e
?
B
1
2
e
N
Number
p
N
Two
0
s
N
?
0
d
N
1
1
s
?
N
?
0
d
?
N
1
2
p
C
Some
0
s
C
?
0
e
C
Contain
0
1
1
s
?
C
?
0
e
?
C
1
2
1 1
1
2
1
1
2
2
2
2
2
c)
(Every p
L
s
L
s
?
L
) ? (Set s
L
d
L
s
?
) ? (Line e
L
d
L
) ? (Set s
?
L
d
?
L
s
?
) ?
(Two p
N
s
N
s
?
N
) ? (Set s
N
d
N
s
?
) ? (Number e
N
d
N
) ? (Set s
?
N
d
?
N
s
?
L
) ? (Contain e
C
d
?
L
d
?
N
)
Figure 2: Semantic dependency graph in a ?continuation-passing? (bottom-up) style, including quantifiers
over eventualities for verbs (in gray). The semantic dependency structure for the sentence Every line
contains two numbers (a), with flat logical form (c), is now contained by the semantic dependency
structure for Every line begins with a space and contains two numbers (b).
lated into a lambda calculus expression by a de-
terministic algorithm which traverses sequences of
continuation dependencies and constructs accord-
ingly nested terms in a manner similar to that de-
fined for DRT (Kamp, 1981). This graphical rep-
resentation can be translated into lambda calculus
by representing the source graph as a set ? of ele-
mentary predications ( f i
0
.. i
N
) and the target as
a set ? of translated lambda calculus expressions,
e.g. (?
i
(h
f
i
0
.. i .. i
N
)). The set ? can then be de-
rived from ? using the following natural deduction
rules:
4
? Initialize ? with lambda terms (sets) that have
no outscoped sets in ?:
?, (Set s i ) ; ?
?, (Set s i ) ; (?
i
True),?
(Set s ) < ?
? Add constraints to appropriate sets in ?:
4
Here, set predications are defined with an additional final
argument position, which is defined to refer in a nuclear scope
set to the restrictor set that is its sibling, and in a restrictor set
to refer to s
?
.
?, ( f i
0
.. i .. i
N
) ; (?
i
o),?
? ; (?
i
o ? (h
f
i
0
.. i .. i
N
)),?
i
0
? E
? Add constraints of supersets as constraints on
subsets in ?:
?, (Set s i ), (Set s
?
i
?
s
??
s) ;
(?
i
o ? (h
f
i
0
.. i .. i
N
)), (?
i
?
o
?
),?
?, (Set s i ), (Set s
?
i
?
s
??
s) ;
(?
i
o ? (h
f
i
0
.. i .. i
N
)),
(?
i
?
o
?
? (h
f
i
0
.. i
?
.. i
N
)),?
? Add quantifiers over completely constrained
sets in ?:
?, (Set s i ), ( f p s
?
s
??
),
(Set s
?
i
?
s ), (Set s
??
i
??
s
?
s
?
) ;
(?
i
o), (?
i
?
o
?
), (?
i
??
o
??
),?
?, (Set s i ) ;
(?
i
o ? (h
f
(?
i
?
o
?
) (?
i
??
o
??
))),?
p ? P,
( f
?
.. i
?
..) < ?,
( f
??
.. i
??
..) < ?.
For example, the graph in Figure 2 can be trans-
lated into the following lambda calculus expres-
sion (including quantifiers over eventualities in the
source graph, to eliminate unbound variables):
144
(Every (?
d
L
Some (?
e
L
Line e
L
d
L
))
(?
d
?
L
Two (?
d
N
Some (?
e
N
Number e
N
d
N
))
(?
d
?
N
Some (?
e
C
Contain e
C
d
?
L
d
?
N
))))
4 Derivation of Syntactic and Semantic
Dependencies
The semantic dependency representation defined
in this paper assumes semantic dependencies other
than those representing continuations are derived
compositionally by a categorial grammar. In par-
ticular, this definition assumes a Generalized Cat-
egorial Grammar (GCG) (Bach, 1981; Oehrle,
1994), because it can be used to distinguish argu-
ment and modifier compositions (from which re-
strictor and nuclear scope sets are derived in a tree-
structured continuation graph), and because large
GCG-annotated corpora defined with this distinc-
tion are readily available (Nguyen et al., 2012).
GCG category types c ? C each consist of a prim-
itive category type u ? U, typically labeled with
the part of speech of the head of a category (e.g.
V, N, A, etc., for phrases or clauses headed by
verbs, nouns, adjectives, etc.), followed by one or
more unsatisfied dependencies, each consisting of
an operator o ? O (-a and -b for adjacent argument
dependencies preceding and succeeding a head, -c
and -d for adjacent conjunct dependencies preced-
ing and succeeding a head, -g for filler-gap depen-
dencies, -r for relative pronoun dependencies, and
some others), each followed by a dependent cate-
gory type from C. For example, the category type
for a transitive verb would be V-aN-bN, since it is
headed by a verb, and has unsatisfied dependen-
cies to satisfied noun-headed categories preced-
ing and succeeding it (for the subject and direct
object noun phrase, respectively). This formula-
tion has the advantage for semantic dependency
calculation that it distinguishes modifier and ar-
gument attachment. Since the semantic represen-
tation described in this paper makes explicit dis-
tinctions between restrictor sets and scope sets
(which is necessary for coherent interpretation of
quantifiers) it is necessary to consistently apply
predicate-argument constraints to discourse refer-
ents in the nuclear scope set of a quantifier and
modifier-modificand constraints to discourse ref-
erents in the restrictor set of a quantifier. For ex-
ample, in Sentence 1:
(1) Everything is [
A-aN
open].
the predicate open constrains the nuclear scope set
of every, but in Sentence 2:
(2) Everything [
A-aN
open] is finished.
the predicate open constrains the restrictor set.
These constraints can be consistently applied in
the argument and modifier attachment rules of a
GCG.
Like a Combinatory Categorial Grammar
(Steedman, 2000), a GCG defines syntactic depen-
dencies for compositions that are determined by
the number and kind of unsatisfied dependencies
of the composed category types. These are similar
to dependencies for subject, direct object, prepo-
sition complement, etc., of Stanford dependencies
(de Marneffe et al., 2006), but are reduced to num-
bers based on the order of the associated depen-
dencies in the category type of the lexical head.
These syntactic dependencies are then associ-
ated with semantic dependencies, with the refer-
ent of a subject associated with the first argument
of an eventuality, the referent of a direct object as-
sociated with the second argument, and so on, for
all verb forms other than passive verbs. In the case
of passive verbs, the referent of a subject is asso-
ciated with the second argument of an eventuality,
the referent of a direct object associated with the
third argument, and so on.
In order to have a consistent treatment of ar-
gument and modifier attachment across all cate-
gory types, and also in order to model referents
of verbs as eventualities which can be quantified
by adverbs like never, once, twice, etc. (Parsons,
1990), it is desirable for eventualities associated
with verbs to also be quantified. Outgoing seman-
tic dependencies to arguments of eventualities are
then applied as constraints to the discourse refer-
ent variable of the restrictor sets of these quanti-
fiers. Incoming dependencies to eventualities and
other discourse referents used as modificands of
modifiers are also applied as constraints to dis-
course referent variables of restrictor sets, but in-
coming dependencies to discourse referents used
as arguments of predicates are applied as con-
straints to discourse referent variables of nuclear
scope sets. This assignment to restrictor or nuclear
scope sets depends on the context of the relevant
(argument or modifier attachment) parser opera-
tion, so associations between syntactic and seman-
tic dependencies must be left partially undefined
in lexical entries. Lexical entries are therefore de-
fined with separate syntactic and semantic depen-
dencies, using even numbers for syntactic depen-
dencies from lexical items, and odd numbers for
145
a)
containing
s
?
d
?
1
i
C
p
C
s
C
e
C
Contain
0
1
1
1
s
??
d
??
1
1
2
3 5
0
b)
i
?
p
?
s
?
2
1
i
3
2
c)
i
?
p
?
s
?
1
1
i
3
2
Figure 3: Example lexical semantic dependencies for the verb containing (a), and dependency equations
for argument attachment (b) and modifier attachment (c) in GCG deduction rules. Lexical dependencies
are shown in gray. Even numbered edges departing lexical items denote lexical syntactic dependen-
cies, and odd numbered edges departing lexical items are lexical semantic dependencies. Argument
attachments constrain semantic arguments to the nuclear scope sets of syntactic arguments, and modifier
attachments constrain semantic arguments to the restrictor sets of syntactic arguments.
semantic dependencies from lexical items. For ex-
ample, a lexical mapping for the finite transitive
verb contains might be associated with the pred-
icate Contain, and have the discourse referent of
its first lexical semantic argument (f
1
(f
3
i)) as-
sociated with the first argument of the eventuality
discourse referent of the restrictor set of its propo-
sition (f
1
(f
1
(f
1
(f
1
i)))), and the discourse referent
of its second lexical semantic argument (f
1
(f
5
i))
associated with the second argument of the even-
tuality discourse referent of the restrictor set of its
proposition (f
2
(f
1
(f
1
(f
1
i)))):
contains ? V-aN-bN : ?
i
(f
0
i)=contains
? (f
0
(f
1
(f
1
(f
1
i))))=Contain
? (f
1
(f
1
(f
1
(f
1
i))))=(f
1
(f
3
i))
? (f
2
(f
1
(f
1
(f
1
i))))=(f
1
(f
5
i))
A graphical representation of these dependencies
is shown in Figure 3a. These lexical semantic con-
straints are then associated with syntactic depen-
dencies by grammar rules for argument and modi-
fier attachment, as described below.
4.1 Inference rules for argument attachment
In GCG, as in other categorial grammars, infer-
ence rules for argument attachment apply functors
of category c-ad or c-bd to preceding or succeed-
ing arguments of category d:
d : g c-ad : h? c : (f
c-ad
g h) (Aa)
c-bd : g d : h? c : (f
c-bd
g h) (Ab)
where f
u?
1
...?
n
are composition functions for u?U
and ??{-a, -b, -c, -d}?C, which connect the lexi-
cal item (f
2n
i) of a preceding child function g as
the 2n
th
argument of lexical item i of a succeeding
child function h, or vice versa:
f
u?
1..n?1
-ad
def
= ?
g h i
(g (f
2n
i)) ? (h i)
? (f
2n+1
i)=(f
2
(f
1
(f
2n
i))) (1a)
f
u?
1..n?1
-bd
def
= ?
g h i
(g i) ? (h (f
2n
i))
? (f
2n+1
i)=(f
2
(f
1
(f
2n
i))) (1b)
as shown in Figure 3b. This associates the lex-
ical semantic argument of the predicate (f
2n+1
i)
with the nuclear scope of the quantifier propo-
sition associated with the syntactic argument
(f
2
(f
1
(f
2n
i))). For example, the following infer-
ence attaches a subject to a verb:
every line
N : ?
i
(f
0
i)=line ..
contains two numbers
V-aN : ?
i
(f
0
i)=contains ..
V : ?
i
(f
0
(f
2
i))=line .. ? (f
0
i)=contains ..
? (f
3
i)=(f
2
(f
1
(f
2
i)))
Aa
4.2 Inference rules for modifier attachment
This grammar also uses distinguished inference
rules for modifier attachment. Inference rules for
modifier attachment apply preceding or succeed-
ing modifiers of category u-ad to modificands of
category c, for u ? U and c, d ? C:
u-ad : g c : h? c : (f
PM
g h) (Ma)
c : g u-ad : h? c : (f
SM
g h) (Mb)
146
eL
Line
0
lines
i
L
p
L
Some
0
s
L
?
0
d
L
1
1
s
?
L
?
0
d
?
L
1
2
1
containing
i
C
p
C
Some
0
s
C
?
0
e
C
Contain
0
1
1
s
?
C
?
0
e
?
C
1
2
1
e
N
Number
0
numbers
i
N
p
N
Some
0
s
N
?
0
d
N
1
1
s
?
N
?
0
d
?
N
1
2
1
0
0
0
2
4
3
5
2
2
1
1
2
1
Figure 4: Compositional analysis of noun phrase lines containing numbers exemplifying both argument
attachment (to numbers) and modifier attachment (to lines). Lexical dependencies are shown in gray, and
continuation dependencies (which do not result from syntactic composition) are highlighted.
where f
PM
and f
SM
are category-independent com-
position functions for preceding and succeeding
modifiers, which return the lexical item of the ar-
gument ( j) rather than of the predicate (i):
f
PM
def
= ?
g h j
?
i
(f
2
i)= j ? (g i) ? (h j)
? (f
3
i)=(f
1
(f
1
(f
2
i))) (2a)
f
SM
def
= ?
g h j
?
i
(f
2
i)= j ? (g j) ? (h i)
? (f
3
i)=(f
1
(f
1
(f
2
i))) (2b)
as shown in Figure 3c. This allows categories
for predicates to be re-used as modifiers. Unlike
argument attachment, modifier attachment asso-
ciates the lexical semantic argument of the mod-
ifier (f
2n+1
i) with the restrictor of the quantifier
proposition of the modificand (f
1
(f
1
(f
2n
i))). For
example, the following inference attaches an ad-
jectival modifier to the quantifier proposition of a
noun phrase:
every line
N:?
i
(f
0
i)=line ..
containing two numbers
A-aN:?
i
(f
0
i)=containing ..
N : ?
i
(f
0
i)=line .. ? ?
j
(f
0
j)=containing ..
? (f
2
j)=i ? (f
3
j)=(f
1
(f
1
(f
2
j)))
Mb
An example of argument and modifier attachment
is shown in Figure 4.
5 Estimation of Scope Dependencies
Semantic dependency graphs obtained from GCG
derivations as described in Section 4 are scopally
underspecified. Scope disambiguations must then
be obtained by specifying continuation dependen-
cies from every set referent to some other set ref-
erent (or to a null context, indicating a top-level
set). In a sentence processing model, these non-
local continuation dependencies would be incre-
mentally calculated in working memory in a man-
ner similar to coreference resolution.
5
However, in
this paper, in order to obtain a reasonable estimate
of the learnability of such a system, continuation
dependencies are assigned post-hoc by a statistical
inference algorithm.
The disambiguation algorithm first defines a
partition of the set of reified set referents into
sets {s, s
?
, s
??
} of reified set referents s whose dis-
course referent variables (f
1
s) are connected by
semantic dependencies. For example, s
L
, s
C
and
s
?
N
in Figure 4 are part of the same partition, but s
?
L
is not.
Scope dependencies are then constructed from
these partitions using a greedy algorithm which
starts with an arbitrary set from this partition in
5
Like any other dependency, a continuation dependency
may be stored during incremental processing when both its
cue (source) and target (destination) referents have been hy-
pothesized. For example, upon processing the word numbers
in the sentence Every line contains two numbers, a continu-
ation dependency may be stored from the nuclear scope set
associated with this word to the nuclear scope set of the sub-
ject every line, forming an in-situ interpretation with some
amount of activation (see Figure 4), and with some (proba-
bly smaller) amount of activation, a continuation dependency
may be stored from the nuclear scope set of this subject to
the nuclear scope set of this word, forming an inverted inter-
pretation. See Schuler (2014) for a model of how sentence
processing in associative memory might incrementally store
dependencies like these as cued associations.
147
the dependency graph, then begins connecting it,
selecting the highest-ranked referent of that par-
tition that is not yet attached and designating it
as the new highest-scoping referent in that parti-
tion, attaching it as the context of the previously
highest-scoping referent in that partition if one ex-
ists. This proceeds until:
1. the algorithm reaches a restrictor or nuclear
scope referent with a sibling (superset or sub-
set) nuclear scope or restrictor referent that
has not yet served as the highest-scoping ref-
erent in its partition, at which point the algo-
rithm switches to the partition of that sibling
referent and begins connecting that; or
2. the algorithm reaches a restrictor or nuclear
scope referent with a sibling nuclear scope or
restrictor referent that is the highest-scoping
referent in its partition, in which case it con-
nects it to its sibling with a continuation de-
pendency from the nuclear scope referent to
the restrictor referent and merges the two sib-
lings? partitions.
In this manner, all set referents in the dependency
graph are eventually assembled into a single tree
of continuation dependencies.
6 Evaluation
This paper defines a graphical semantic represen-
tation with desirable properties for storing sen-
tence meanings as cued associations in associa-
tive memory. In order to determine whether this
representation of continuation dependencies is re-
liably learnable, the set of test sentences from the
QuanText corpus (Manshadi et al., 2011) was au-
tomatically annotated with these continuation de-
pendencies and evaluated against the associated
set of gold-standard quantifier scopes. The sen-
tences in this corpus were collected as descrip-
tions of text editing tasks using unix tools like sed
and awk, collected from online tutorials and from
graduate students asked to write and describe ex-
ample scripts. Gold-standard scoping relations in
this corpus are specified over bracketed sequences
of words in each sentence. For example, the sen-
tence Print every line that starts with a number
might be annotated:
Print [
1
every line] that starts with [
2
a number] .
scoping relations: 1 > 2
meaning that the quantifier over lines, referenced
in constituent 1, outscopes the quantifier over
numbers, referenced in constituent 2. In order to
isolate the learnablility of the continuation depen-
dencies described in this paper, both training and
test sentences of this corpus were annotated with
hand-corrected GCG derivations which are then
used to obtain semantic dependencies as described
in Section 4. Continuation dependencies are then
inferred from these semantic dependencies us-
ing the algorithm described in Section 5. Gold-
standard scoping relations are considered success-
fully recalled if a restrictor (f
1
(f
1
i)) or nuclear
scope (f
2
(f
1
i)) referent of any lexical item i within
the outscoped span is connected by a sequence of
continuation dependencies (in the appropriate di-
rection) to any restrictor or nuclear scope referent
of any lexical item within the outscoping span.
First, the algorithm was run without any lexical-
ization on the 94 non-duplicate sentences of the
QuanText test set. Results of this evaluation are
shown in the third line of Table 1 using the per-
sentence complete recall accuracy (?AR?) defined
by Manshadi et al. (2013).
The algorithm was then run using bilexical
weights based on the frequencies
?
F(h, h
?
) with
which a word h
?
occurs as a head of a category
outscoped by a category headed by word h in the
350-sentence training set of the QuanText corpus.
For example, since quantifiers over lines are often
outscoped by quantifiers over files in the training
data, the system learns to rank continuation de-
pendencies to referents associated with the word
lines ahead of continuation dependencies to ref-
erents associated with the word files in bottom-
up inference. These lexical features may be par-
ticularly helpful because continuation dependen-
cies are generated only between directly adjacent
sets. Results for scope disambiguation using these
rankings are shown in the fourth line of Table 1.
This increase is statistically significant (p = 0.001
by two-tailed McNemar?s test). This significance
for local head-word features on continuation de-
pendencies shows that these dependencies can be
reliably learned from training examples, and sug-
gests that continuation dependencies may be a nat-
ural representation for scope information.
Interestingly, effects of lexical features for
quantifiers (the word each, or definite/indefinite
distinctions) were not substantial or statistically
significant, despite the relatively high frequencies
148
System AR
Manshadi and Allen (2011) baseline 63%
Manshadi et al. (2013) 72%
This system, w/o lexicalized model 61%
This system, w. lexicalized model 72%
Table 1: Per-sentence complete recall accuracy
(?AR?) of tree-based algorithm as compared to
Manshadi and Allen (2011) and Manshadi et al.
(2013) on explicit NP chunks in the QuanText test
set, correcting for use of gold standard trees as de-
scribed in footnote 19 of Manshadi et al. (2013).
of the words each and the in the test corpus (oc-
curring in 16% and 68% of test sentences, respec-
tively), which suggests that these words may often
be redundant with syntactic and head-word con-
straints. Results using preferences that rank refer-
ents quantified by the word each after other refer-
ents achieve a numerical increase in accuracy over
a model with no preferences (up 5 points, to 66%),
but it is not statistically significant (p = .13). Re-
sults using preferences that rank referents quanti-
fied by the word the after other referents achieve a
numerical increase in accuracy over a model with
no preferences (up 1 point, to 62%), but this is
even less significant (p = 1). Results are even
weaker in combination with head-word features
(up 1 point, to 73%, for each; down two points,
to 70%, for the). This suggests that world knowl-
edge (in the form of head-word information) may
be more salient to quantifier scope disambiguation
than many intuitive linguistic preferences.
7 Conclusion
This paper has presented a graphical semantic de-
pendency representation based on bottom-up con-
tinuation dependencies which can be translated
into lambda calculus, but has the important prop-
erty that its vertices define a usable set of discourse
referents in working memory even in contexts in-
volving conjunction in the scope of quantifiers.
An evaluation on an existing quantifier scope dis-
ambiguation task shows that non-local continua-
tion dependencies can be as reliably learned from
annotated data as representations used in a state-
of-the-art quantifier scope resolver. This suggests
that continuation dependencies may be a natural
representation for scope information.
Continuation dependencies as defined in this
paper provide a local representation for quantifi-
cational context. This ensures that graphical repre-
sentations match only when their quantificational
contexts match. When used to guide a statistical
or vectorial representation, it is possible that this
local context will allow certain types of inference
to be defined by simple pattern matching, which
could be implemented in existing working mem-
ory models. Future work will explore the use of
this graph-based semantic representation as a ba-
sis for vectorial semantics in a cognitive model of
inference during sentence processing.
8 Acknowledgements
The authors would like to thank Mehdi Manshadi
for assistance in obtaining the QuanText corpus.
The authors would also like to thank Erhard Hin-
richs, Craige Roberts, the members of the OSU
LLIC Reading Group, and the three anonymous
*SEM reviewers for their helpful comments about
this work.
References
James A. Anderson, Jack W. Silverstein, Stephen A.
Ritz, and Randall S. Jones. 1977. Distinctive fea-
tures, categorical perception and probability learn-
ing: Some applications of a neural model. Psycho-
logical Review, 84:413?451.
Emmon Bach. 1981. Discontinuous constituents in
generalized categorial grammars. Proceedings of
the Annual Meeting of the Northeast Linguistic So-
ciety (NELS), 11:1?12.
Jason Baldridge and Geert-Jan M. Kruijff. 2002. Cou-
pling CCG and hybrid logic dependency seman-
tics. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL
2002), Philadelphia, Pennsylvania.
Chris Barker. 2002. Continuations and the nature
of quantification. Natural Language Semantics,
10:211?242.
Jon Barwise and Robin Cooper. 1981. Generalized
quantifiers and natural language. Linguistics and
Philosophy, 4.
Johan Bos. 1996. Predicate logic unplugged. In Pro-
ceedings of the 10th Amsterdam Colloquium, pages
133?143.
Sarah Brown-Schmidt, Ellen Campana, and Michael K.
Tanenhaus. 2002. Reference resolution in the wild:
Online circumscription of referential domains in a
natural interactive problem-solving task. In Pro-
ceedings of the 24th Annual Meeting of the Cogni-
tive Science Society, pages 148?153, Fairfax, VA,
August.
149
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics: An intro-
duction. Research on Language and Computation,
pages 281?332.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC 2006.
Judith Degen and Michael K. Tanenhaus. 2011. Mak-
ing inferences: The case of scalar implicature pro-
cessing. In Proceedings of the 33rd Annual Confer-
ence of the Cognitive Science Society, pages 3299?
3304.
Marc W. Howard and Michael J. Kahana. 2002. A dis-
tributed representation of temporal context. Journal
of Mathematical Psychology, 45:269?299.
Hans Kamp. 1981. A theory of truth and semantic
representation. In Jeroen A. G. Groenendijk, Theo
M. V. Janssen, and Martin B. J. Stokhof, editors,
Formal Methods in the Study of Language: Math-
ematical Centre Tracts 135, pages 277?322. Mathe-
matical Center, Amsterdam.
Walter Kintsch. 1988. The role of knowledge in dis-
course comprehension: A construction-integration
model. Psychological review, 95(2):163?182.
Alexander Koller. 2004. Constraint-based and graph-
based resolution of ambiguities in natural language.
Ph.D. thesis, Universit?at des Saarlandes.
Geert-Jan M. Kruijff. 2001. A Categorial-Modal
Architecture of Informativity: Dependency Gram-
mar Logic and Information Structure. Ph.D. thesis,
Charles University.
Bill MacCartney and Christopher D. Manning. 2009.
An Extended Model of Natural Logic. In Proceed-
ings of the Eighth International Conference on Com-
putational Semantics, IWCS-8 ?09, pages 140?156.
Association for Computational Linguistics.
Mehdi Manshadi and James F. Allen. 2011. Unre-
stricted quantifier scope disambiguation. In Graph-
based Methods for Natural Language Processing,
pages 51?59.
Mehdi Manshadi, James F. Allen, and Mary Swift.
2011. A corpus of scope-disambiguated english
text. In Proceedings of ACL, pages 141?146.
Mehdi Manshadi, Daniel Gildea, and James F. Allen.
2013. Plurality, negation, and quantification: To-
wards comprehensive quantifier scope disambigua-
tion. In Proceedings of ACL, pages 64?72.
David Marr. 1971. Simple memory: A theory
for archicortex. Philosophical Transactions of the
Royal Society (London) B, 262:23?81.
David Marr. 1982. Vision. A Computational Investiga-
tion into the Human Representation and Processing
of Visual Information. W.H. Freeman and Company.
J. L. McClelland, B. L. McNaughton, and R. C.
O?Reilly. 1995. Why there are complementary
learning systems in the hippocampus and neocortex:
Insights from the successes and failures of connec-
tionist models of learning and memory. Psychologi-
cal Review, 102:419?457.
Richard Montague. 1973. The proper treatment
of quantification in ordinary English. In J. Hin-
tikka, J.M.E. Moravcsik, and P. Suppes, editors,
Approaches to Natural Langauge, pages 221?242.
D. Riedel, Dordrecht. Reprinted in R. H. Thoma-
son ed., Formal Philosophy, Yale University Press,
1994.
B.B. Murdock. 1982. A theory for the storage and
retrieval of item and associative information. Psy-
chological Review, 89:609?626.
Luan Nguyen, Marten van Schijndel, and William
Schuler. 2012. Accurate unbounded dependency re-
covery using generalized categorial grammars. In
Proceedings of the 24th International Conference
on Computational Linguistics (COLING ?12), pages
2125?2140, Mumbai, India.
Richard T. Oehrle. 1994. Term-labeled categorial type
systems. Linguistics and Philosophy, 17(6):633?
678.
Terence Parsons. 1990. Events in the Semantics of
English. MIT Press.
William Schuler. 2014. Sentence processing in a
vectorial model of working memory. In Fifth An-
nual Workshop on Cognitive Modeling and Compu-
tational Linguistics (CMCL 2014).
Chung-chieh Shan and Chris Barker. 2006. Explaining
crossover and superiority as left-to-right evaluation.
Linguistics and Philosophy, 29:91?134.
Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
Mark Steedman. 2012. Taking Scope - The Natural
Semantics of Quantifiers. MIT Press.
Michael K. Tanenhaus, Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. In-
tegration of visual and linguistic information in spo-
ken language comprehension. Science, 268:1632?
1634.
Marten van Schijndel and William Schuler. 2013. An
analysis of frequency- and recency-based processing
costs. In Proceedings of NAACL-HLT 2013. Associ-
ation for Computational Linguistics.
Marten van Schijndel, Luan Nguyen, and William
Schuler. 2013. An analysis of memory-based pro-
cessing costs using incremental deep syntactic de-
pendency parsing. In Proceedings of CMCL 2013.
Association for Computational Linguistics.
150
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 27?35,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
HHMM Parsing with Limited Parallelism
Tim Miller
Department of Computer Science
and Engineering
University of Minnesota, Twin Cities
tmill@cs.umn.edu
William Schuler
University of Minnesota, Twin Cities
and The Ohio State University
schuler@ling.ohio-state.edu
Abstract
Hierarchical Hidden Markov Model
(HHMM) parsers have been proposed as
psycholinguistic models due to their broad
coverage within human-like working
memory limits (Schuler et al, 2008) and
ability to model human reading time
behavior according to various complexity
metrics (Wu et al, 2010). But HHMMs
have been evaluated previously only with
very wide beams of several thousand
parallel hypotheses, weakening claims to
the model?s efficiency and psychological
relevance. This paper examines the effects
of varying beam width on parsing accu-
racy and speed in this model, showing that
parsing accuracy degrades gracefully as
beam width decreases dramatically (to 2%
of the width used to achieve previous top
results), without sacrificing gains over a
baseline CKY parser.
1 Introduction
Probabilistic parsers have been successful at ac-
curately estimating syntactic structure from free
text. Typically, these systems work by consider-
ing entire sentences (or utterances) at once, using
dynamic programming to obtain globally optimal
solutions from locally optimal sub-parses.
However, these methods usually do not attempt
to conform to human-like processing constraints,
e.g. leading to center embedding and garden path
effects (Chomsky and Miller, 1963; Bever, 1970).
For systems prioritizing accurate parsing perfor-
mance, there is little need to produce human-like
errors. But from a human modeling perspective,
the success of globally optimized whole-utterance
models raises the question of how humans can ac-
curately parse linguistic input without access to
this same global optimization. This question cre-
ates a niche in computational research for models
that are able to parse accurately while adhering as
closely as possible to human-like psycholinguistic
constraints.
Recent work on incremental parsers includes
work on Hierarchical Hidden Markov Model
(HHMM) parsers that operate in linear time by
maintaining a bounded store of incomplete con-
stituents (Schuler et al, 2008). Despite this seem-
ing limitation, corpus studies have shown that
through the use of grammar transforms, this parser
is able to cover nearly all sentences contained in
the Penn Treebank (Marcus et al, 1993) using a
small number of unconnected memory elements.
But this bounded-memory parsing comes at a
price. The HHMM parser obtains good coverage
within human-like memory bounds only by pur-
suing an ?optionally arc-eager? parsing strategy,
nondeterministically guessing which constituents
can be kept open for attachment (occupying an ac-
tive memory element), or closed for attachment
(freeing a memory element for subsequent con-
stituents). Although empirically determining the
number of parallel competing hypotheses used in
human sentence processing is difficult, previous
results in computational models have shown that
human-like behavior can be elicited at very low
levels of parallelism (Boston et al, 2008b; Brants
and Crocker, 2000), suggesting that large num-
bers of active hypotheses are not needed. Previ-
ously, the HHMM parser has only been evaluated
on large beam widths, leaving this aspect of its
psycholinguistic plausibility untested.
In this paper, the performance of an HHMM
parser will be evaluated in two experiments that
27
vary the amount of parallelism allowed during
parsing, measuring the degree to which this de-
grades the system?s accuracy. In addition, the
evaluation will compare the HHMM parser to an
off-the-shelf probabilistic CKY parser to evaluate
the actual run time performance at various beam
widths. This serves two purposes, evaluating one
aspect of the plausibility of this parsing frame-
work as a psycholinguistic model, and evaluating
its potential utility as a tool for operating on un-
segmented text or speech.
2 Related Work
There are several criteria a parser must meet
in order to be plausible as a psycholinguistic
model of the human sentence-processing mecha-
nism (HSPM).
Incremental operation is perhaps the most obvi-
ous. The HSPM is able to process sentences in-
crementally, meaning that at each point in time of
processing input, it has some hypothesis of the in-
terpretation of that input, and each subsequent unit
of input serves to update that hypothesis.
The next criterion for psycholinguistic plausi-
bility is processing efficiency. The HSPM not
only operates incrementally, but in standard op-
eration it does not lag behind a speaker, even if,
for example, the speaker continues speaking at ex-
tended length without pause. Standard machine
approaches, such as chart parsers based on the
CKY algorithm, operate in worst-case cubic run
time on the length of input. Without knowing
where an utterance or sentence might end, such an
algorithm will take more time with each succes-
sive word and will eventually fall behind.
The third criterion is a reasonable limiting of
memory resources. This constraint means that the
HSPM, while possibly considering multiple hy-
potheses in parallel, is not limitlessly so, as evi-
denced by the existence of garden path sentences
(Bever, 1970; Lewis, 2000). If this were not the
case, garden-path sentences would not cause prob-
lems, as reaching the disambiguating word would
simply result in a change in the favored hypothe-
sis. In fact, garden path sentences typically cannot
be understood on a first pass and must be reread,
indicating that the correct analysis is attainable
and yet not present in the set of parallel hypotheses
of the first pass.
While parsers meeting these three criteria can
claim to not violate any psycholinguistic con-
straints, there has been much recent work in
testing psycholinguistically-motivated parsers to
make forward predictions about human sentence
processing, in order to provide positive evidence
for certain probabilistic parsing models as valid
psycholinguistic models of sentence processing.
This work has largely focused on correlating mea-
sures of parsing difficulty in computational models
with delays in reading time in human subjects.
Hale (2001) introduced the surprisal metric for
probabilistic parsers, which measures the log ra-
tio of the total probability mass at word t ? 1
and word t. In other words, it measures how
much probability was lost in incorporating the
next word into the current hypotheses. Boston et
al. (2008a) show that surprisal is a significant pre-
dictor of reading time (as measured in self-paced
reading experiments) using a probabilistic depen-
dency parser. Roark et al (2009) dissected parsing
difficulty metrics (including surprisal and entropy)
to separate out the effects of syntactic and lexical
difficulties, and showed that these new metrics are
strong predictors of reading difficulty.
Wu et al (2010) evaluate the same Hierarchical
Hidden Markov Model parser used in this work in
terms of its ability to reproduce human-like results
for various complexity metrics, including some of
those mentioned above, and introduce a new met-
ric called embedding difference. This metric is
based on the idea of embedding depth, which is
the number of elements in the memory store re-
quired to hold a given hypothesis. Using more
memory elements corresponds to center embed-
ding in phrase structure trees, and presumably cor-
relates to some degree with complexity. Average
embedding for a time step is computed by com-
puting the weighted average number of required
memory elements (weighted by probability) for all
hypotheses on the beam. Embedding difference is
simply the change in this value when the next word
is encountered.
Outside of Wu et al, the most similar work
from a modeling perspective is an incremen-
tal parser implemented using Cascaded Hidden
Markov Models (CHMMs) (Crocker and Brants,
2000). This model is superficially similar to the
Hierarchical Hidden Markov Models described
below in that it relies on multiple levels of interde-
pendent HMMs to account for hierarchical struc-
ture in an incremental model. Crocker and Brants
use the system to parse ambiguous sentences (such
28
as the athlete realized his goals were out of reach)
and examine the relative probabilities of two plau-
sible analyses at each time step. They then show
that the shifting of these two probabilities is con-
sistent with empirical evidence about how humans
perceive these sentences word by word.
However, as will be described below, the
HHMM has advantages over the CHMM from
a psycholinguistic modeling perspective. The
HHMM uses a limited memory store contain-
ing only four elements which is consistent with
many estimates of human short term memory lim-
its (Cowan, 2001; Miller, 1956). In addition to
modeling memory limits, the limited store acts as
a fixed-depth stack that ensures linear asymptotic
parsing time, and a grammar transform allows for
wide coverage of speech and newspaper corpora
within that limited memory store (Schuler et al,
2010).
3 Hierarchical Hidden Markov Model
Parser
Hidden Markov Models (HMMs) have long been
used to successfully model sequence data in which
there is a latent (hidden) variable at each time step
that generates the observed evidence at that time
step. These models are used for such applications
as part-of-speech tagging, and speech recognition.
Hierarchical Hidden Markov Models (HH-
MMs) are an extension of HMMs which can rep-
resent sequential data containing hierarchical rela-
tions. In HHMMs, complex hidden variables may
output evidence for several time steps in sequence.
This process may recurse, though a finite depth
is required to make any guarantees about perfor-
mance. Murphy and Paskin (2001) showed that
this model could be framed as a Dynamic Bayes
Network (DBN), so that inference is linear on the
length of the input sequence.
In the HHMM parser used here, the complex
hidden variables are syntactic states that gener-
ate sub-sequences of other syntactic states, even-
tually generating pre-terminals and words. This
section will describe how the trees must be trans-
formed, and then mapped to HHMM states. This
section will then continue with a formal definition
of an HHMM, followed by a description of how
this model can parse natural language, and finally
a discussion of what different aspects of the model
represent in terms of psycholinguistic modeling.
3.1 Right-Corner Transform
In order to parse with an HHMM, phrase struc-
ture trees need to be mapped to a hierarchical se-
quence of states of nested HMMs. Since Mur-
phy and Paskin showed that the run time complex-
ity of the HHMM is exponential on the depth of
the nested HMMs, it is important to minimize the
depth of the model for optimal performance. In
order to do this, a tree transformation known as
a right-corner transform is applied to the phrase
structure trees comprising the training data, to
transform right-expanding sequences of complete
constituents into left-expanding sequences of in-
complete constituents A?/A?, consisting of an in-
stance of an active constituent A? lacking an in-
stance of an awaited constituent A? yet to be rec-
ognized. This transform can be defined as a syn-
chronous grammar that maps every context-free
rule expansion in a source tree (in Chomsky Nor-
mal Form) to a corresponding expansion in a right-
corner transformed tree:1
? Beginning case: the top of a right-expanding
sequence in an ordinary phrase structure tree
is mapped to the bottom of a left-expanding
sequence in a right-corner transformed tree:
A?
A??0
?
A??1
?
?
A?
A?/A??1
A??0
?
?
(1)
? Middle case: each subsequent branch in
a right-expanding sequence of an ordinary
phrase structure tree is mapped to a branch in
a left-expanding sequence of the transformed
tree:
A?
? A???
A????0
?
A????1
?
?
A?
A?/A????1
A?/A???
?
A????0
?
?
(2)
? Ending case: the bottom of a right-expanding
sequence in an ordinary phrase structure tree
1Here, ? and ? are tree node addresses, consisting of se-
quences of zeros, representing left branches, and ones, repre-
senting right branches, on a path from the root of the tree to
any given node.
29
a) A?
A?0
a ?
00 A?01
A?010
a ?
01
00
a ?
01
01
A?011
a ?
01
10
a ?
01
11
A?1
A?10
A?100
a ?
10
00
a ?
10
01
A?101
a ?
10
10
a ?
10
11
a ?
11
b) A?
A?/A?11
A?/A?1
A?0
A?0/A?0111
A?0/A?011
A?0/A?01
a ?
00
A?010
A?010/A?0101
a ?
01
00 a ?
01
01
a ?
01
10 a ?
01
11
A?10
A?10/A?1011
A?10/A?101
A?100
A?100/A?1001
a ?
10
00 a ?
10
01
a ?
10
10 a ?
10
11
a ?
11
Figure 1: Sample right-corner transform of
schematized tree before (a) and after (b) applica-
tion of transform.
is mapped to the top of a left-expanding se-
quence in a right-corner transformed tree:
A?
? A???
a???
?
A?
A?/A???
?
A???
a???
(3)
The application of this transform is exemplified in
Figure 1.
3.2 Hierarchical Hidden Markov Models
Right-corner transformed trees are mapped to ran-
dom variables in a Hierarchical Hidden Markov
Model (Murphy and Paskin, 2001).
A Hierarchical Hidden Markov Model
(HHMM) is essentially a factored version of
a Hidden Markov Model (HMM), configured to
recognize bounded recursive structures (i.e. trees).
Like HMMs, HHMMs use Viterbi decoding to
obtain sequences of hidden states s?1..T given
sequences of observations o1..T (words or audio
features), through independence assumptions
in a transition model ?A and an observation
model ?B (Baker, 1975; Jelinek et al, 1975):
s?1..T
def= argmax
s1..T
T
?
t=1
P?A(st | st?1) ? P?B(ot | st)
(4)
HHMMs then factor the hidden state transition?A
into a reduce and shift phase (Equation 5), then
into a bounded set of depth-specific operations
(Equation 6):
P?A(st|st?1) =
?
rt
P?R(rt|st?1)?P?S(st|rt st?1)
(5)
def=
?
r1..Dt
D
?
d=1
P?R,d(r
d
t | rd+1t sdt?1sd?1t?1 )?
P?S,d(s
d
t |rd+1t rdt sdt?1sd?1t )
(6)
which allow depth-specific variables to reduce
(through ?R-Rdn,d), transition (?S-Trn,d), and ex-
pand (?S-Exp,d) like tape symbols in a pushdown
automaton with a bounded memory store, depend-
ing on whether the variable below has reduced
(rdt ?RG) or not (rdt 6?RG):2
P?R,d(r
d
t | rd+1t sdt?1sd?1t?1 )
def=
{
if rd+1t 6?RG : Jrdt =r?K
if rd+1t ?RG : P?R-Rdn,d(rdt | rd+1t sdt?1 sd?1t?1 )
(7)
P?S,d(s
d
t | rd+1t rdt sdt?1sd?1t )
def=
?
?
?
if rd+1t 6?RG, rdt 6?RG : Jsdt =sdt?1K
if rd+1t ?RG, rdt 6?RG : P?S-Trn,d(sdt | rd+1t rdt sdt?1sd?1t )
if rd+1t ?RG, rdt ?RG : P?S-Exp,d(sdt | sd?1t )
(8)
where s0t = s? and rD+1t = r? for constants s?
(an incomplete root constituent), r? (a complete
lexical constituent) and r? (a null state resulting
from reduction failure) s.t. r??RG and r? 6?RG.
Right-corner transformed trees, as exemplified
in Figure 1(b), can then be aligned to HHMM
states as shown in Figure 2, and used to train an
HHMM as a parser.
Parsing with an HHMM simply involves pro-
cessing the input sequence, and estimating a most
likely hidden state sequence given this observed
input. Since the output is to be the best possible
parse, the Viterbi algorithm is used, which keeps
track of the highest probability state at each time
step, where the state is the store of incomplete syn-
tactic constituents being processed. State transi-
tions are computed using the models above, and
each state at each time step keeps a back pointer to
the state it most probably came from. Extracting
the highest probability parse requires extracting
2Here, J?K is an indicator function: J?K = 1 if ? is true, 0
otherwise.
30
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7
a
?0101
a
?0110
a
?0111
a
?1000
a
?1001
a
?1010
? ? ? ? ? ?
? ? ? ?
A
?100 /A
?1001
A
?10 /A
?101
A
?10 /A
?1011
?
A
?0 /A
?011
A
?0 /A
?0111
A
? /A
?1
A
? /A
?1
A
? /A
?1
A
? /A
?1
Figure 2: Mapping of schematized right-corner
tree into HHMM memory elements.
the most likely sequence, deterministically map-
ping that sequence back to a right-corner tree, and
reversing the right-corner transform to produce an
ordinary phrase structure tree.
Unfortunately exact inference is not tractable
with this model and dataset. The state space is
too large to manage for both space and time rea-
sons, and thus approximate inference is carried
out, through the use of a beam search. At each
time step, only the top N most probable hypoth-
esized states are maintained. Experiments de-
scribed in (Schuler, 2009) suggest that there does
not seem to be much lost in going from exact in-
ference using the CKY algorithm to a beam search
with a relatively large width. However, the op-
posite experiment, examining the effect of going
from a relatively wide beam to a very narrow beam
has not been thoroughly studied in this parsing ar-
chitecture.
4 Optionally Arc-eager Parsing
The right-corner transform described in Sec-
tion 3.1 saves memory because it transforms any
right-expanding sequence with left-child subtrees
into a left-expanding sequence of incomplete con-
stituents, with the same sequence of subtrees as
right children. The left-branching sequences of
siblings resulting from this transform can then be
composed bottom-up through time by replacing
each left child category with the category of the
resulting parent, within the same memory element
(or depth level). For example, in Figure 3(a) a
left-child category NP/NP at time t=4 is composed
with a noun new of category NP/NNP (a noun
phrase lacking a proper noun yet to come), result-
ing in a new parent category NP/NNP at time t=5
replacing the left child category NP/NP in the top-
most d=1 memory element.
This in-element composition preserves ele-
ments of the bounded memory store for use in pro-
cessing descendants of this composed constituent,
yielding the human-like memory demands re-
ported in (Schuler et al, 2008). But whenever
an in-element composition like this is hypothe-
sized, it isolates an intermediate constituent (in
this example, the noun phrase ?new york city?)
from subsequent composition. Allowing access
to this intermediate constituent ? for example,
to allow ?new york city? to become a modifier
of ?bonds?, which itself becomes an argument of
?for? ? requires an analysis in which the interme-
diate constituent is stored in a separate memory
element, shown in Figure 3(b). This creates a lo-
cal ambiguity in the parser (in this case, from time
step t=4) that may have to be propagated across
several words before it can be resolved (in this
case, at time step t=7). This is essentially an am-
biguity between arc-eager (in-element) and arc-
standard (cross-element) composition strategies,
as described by Abney and Johnson (1991). In
contrast, an ordinary (purely arc-standard) parser
with an unbounded stack would only hypothesize
analysis (b), avoiding this ambiguity.3
The right-corner HHMM approach described
in this paper relies on a learned statistical model
to predict when in-element (arc-eager) compo-
sitions will occur, in addition to hypothesizing
parse trees. The model encodes a mixed strategy:
with some probability arc-eager or arc-standard
for each possible expansion. Accuracy results on
a right-corner HHMM model trained on the Penn
Wall Street Journal Treebank suggest that this kind
of optionally arc-eager strategy can be reliably sta-
tistically learned.
By placing firm limits on the number of open
incomplete constituents in working memory, the
Hierarchical HMM parser maintains parallel hy-
potheses on the beam which predict whether each
constituent will host a subsequent attachment or
not. Empirical results described in the next section
3It is important to note that neither the right-corner nor
left-corner parsing strategy by itself creates this ambiguity.
The ambiguity arises from the decision to use this option-
ally arc-eager strategy to reduce memory store allocation in
a bounded memory parser. Implementations of left-corner
parsers such as that of Henderson (2004) adopt a arc-standard
strategy, essentially always choosing analysis (b) above, and
thus do not introduce this kind of local ambiguity. But in
adopting this strategy, such parsers must maintain a stack
memory of unbounded size, and thus are not attractive as
models of human parsing in short-term memory (Resnik,
1992).
31
a)
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7
strong
dem
and
for
new
york
city
? ? ? ? ? ?
? ? ? ? ? ?
?
NP/NN
NP/PP
NP/NP
NP/NNP
NP/NNP
NP(dem
.)
b)
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7
strong
dem
and
for
new
york
city
? ? ? ? ? ?
? ? ? ?
NNP/NNP
NNP/NNP
NP(city)
?
NP/NN
NP/PP
NP/NP
NP/NP
NP/NP
NP(dem
.)/NP(?)
Figure 3: Alternative analyses of ?strong demand for new york city ...?: a) using in-element composition,
compatible with ?strong demand for new york city is ...? (in which the demand is for the city); and b)
using cross-element (or delayed) composition, compatible with either ?strong demand for new york city
is ...? (in which the demand is for the city) or ?strong demand for new york city bonds is ...? (in which a
forthcoming referent ? in this case, bonds ? is associated with the city, and is in demand). In-element
composition (a) saves memory but closes off access to the noun phrase headed by ?city?, and so is not
incompatible with the ?...bonds? completion. Cross-element composition (b) requires more memory,
but allows access to the noun phrase headed by ?city?, so is compatible with either completion. This
ambiguity is introduced at t=4 and propagated until at least t=7. An ordinary, non-right-corner stack
machine would exclusively use analysis (b), avoiding ambiguity.
show that this added demand on parallelism does
not substantially degrade parsing accuracy, even at
very narrow beam widths.
5 Experimental Evaluation
The parsing model described in Section 3 has
previously been evaluated on the standard task
of parsing the Wall Street Journal section of the
Penn Treebank. This evaluation was optimized
for accuracy results, and reported a relatively wide
beam width of 2000 to achieve its best results.
However, most psycholinguistic models of the hu-
man sentence processing mechanism suggest that
if the HSPM does work in parallel, it does so with
a much lower number of concurrent hypotheses
(Boston et al, 2008b). Viewing the HHMM pars-
ing framework as a psycholinguistic model, a nec-
essary (though not sufficient) condition for it being
a valid model is that it be able to maintain rela-
tively accurate parsing capabilities even at much
lower beam widths.
Thus, the first experiments in this paper evalu-
ate the degradation of parsing accuracy depending
on beam width of the HHMM parser. Experiments
were conducted again on the WSJ Penn Treebank,
using sections 02-21 to train, and section 23 as the
test set. Punctuation was included in both train-
ing and testing. A set of varied beam widths were
considered, from a high of 2000 to a low of 15.
This range was meant to roughly correspond to
the range of parallelism used in other similar ex-
periments, using 2000 as a high end due to its us-
age in previous parsing experiments. However, it
should be noted that in fact the highest value of
2000 is already an approximate search ? prelim-
inary experiments showed that exhaustive search
with the HHMM would require more than 100000
elements per time step (exact values may be much
higher but could not be collected because they ex-
hausted system memory).
The HHMM parser was compared to a custom
built (though standard) probabilistic CKY parser
implementation trained on the CNF trees used as
input to the right-corner transform, so that the
CKY parser was able to compete on a fair foot-
ing. The accuracy results of these experiments are
shown in Figure 4.
These results show fairly graceful decline in
parsing accuracy with a beam width starting at
2000 elements down to about 50 beam elements.
This beam width is much less than 1% of the ex-
haustive search, though it is around 1% of what
might be considered the highest reasonable beam
width for efficient parsing. The lowest beam
widths attempted, 15, 20, and 25, result in ac-
curacy below that of the CKY parser. The low-
est beam width attempted, 15, shows the sharpest
decline in accuracy, putting the HHMM system
nearly 8 points below the CKY parser in terms of
accuracy.
This compares reasonably well to results by
32
 70
 72
 74
 76
 78
 80
 82
 84
 0  100  200  300  400  500
La
be
le
d 
F-
Sc
or
e
Beam Width
Figure 4: Plot of parsing accuracy (labeled F-
score) vs. beam widths for an HHMM parser
(curved line). Top line is HHMM accuracy with
beam width of 2000 (upper bound). The bottom
line is CKY parser results. Points correspond to
beam widths of 15, 20, 25, 50, 100, 250, and 500.
Brants and Crocker (2000) showing that an in-
cremental chart-parsing algorithm can parse accu-
rately with pruning down to 1% of normal memory
usage. While that parsing algorithm is difficult to
compare directly to this HHMM parser, the reduc-
tion in beam width in this system to 50 beam el-
ements from an already approximated 2000 beam
elements shows similar robustness to approxima-
tion. Accuracy comparisons should be taken with
a grain of salt due to additional annotations per-
formed to the Treebank before training, but the
HHMM parser with a beam width of 50 obtains
approximately the same accuracy as the Brants
and Crocker incremental CKY parser pruning to
3% of chart size. At 1% pruning, Brants and
Crocker achieved around 75% accuracy, which
falls between the HHMM parser at beam widths
of 20 and 25.
Results by Boston et al (2008b) are also dif-
ficult to compare directly due to a difference in
parsing algorithm and different research priority
(that paper was attempting to correlate parsing dif-
ficulty with reading difficulty). However, that pa-
per showed that a dependency parser using less
than ten beam elements (and as few as one) was
just as capable of predicting reading difficulty as
the parser using 100 beam elements.
A second experiment was conducted to eval-
uate the HHMM for its time efficiency in pars-
ing. This experiment is intended to address two
questions: Whether this framework is efficient
 0
 2
 4
 6
 8
 10
 12
 14
 10  20  30  40  50  60  70
Se
co
nd
s 
pe
r s
en
te
nc
e
Sentence Length
CKY
HHMM
Figure 5: Plot of parsing time vs. sentence length
for HHMM and CKY parsers.
enough to be considered a viable psycholinguis-
tic model, and whether its parsing time and accu-
racy remain competitive with more standard cu-
bic time parsing technologies at low beam widths.
To evaluate this aspect, the HHMM parser was
run at low beam widths on sentences of varying
lengths. The baseline was the widely-used Stan-
ford parser (Klein and Manning, 2003), run in
?vanilla PCFG? mode. This parser was used rather
than the custom-built CKY parser from the pre-
vious experiment, to avoid the possibility that its
implementation was not efficient enough to pro-
vide a realistic test. The HHMMparser was imple-
mented as described in the previous section. These
experiments were run on a machine with a single
2.40 GHz Celeron CPU, with 512 MB of RAM. In
both implementations the parser timing includes
only time spent actually parsing sentences, ignor-
ing the overhead incurred by reading in model files
or training.
Figure 5 shows a plot of parsing time versus
sentence length for the HHMM parser for a beam
width of 20. Sentences shorter than 10 words were
not included for visual clarity (both parsers are ex-
tremely fast at that length). At this beam width,
the performance of the HHMM parser (labeled F-
score) was 74.03%, compared to 71% for a plain
CKY parser. As expected, the HHMM parsing
time increases linearly with sentence length, while
the CKY parsing time increases super-linearly.
(However, due to high constants in the run time
complexity of the HHMM, it was not a priori clear
that the HHMM would be faster for any sentence
of reasonable length.)
33
The results of this experiment show that the
HHMM parser is indeed competitive with a proba-
bilistic CKY parser, in terms of parsing efficiency,
even while parsing with higher accuracy. At sen-
tences longer that 26 words (including punctua-
tion), the HHMM parser is faster than the CKY
parser. This advantage is clear for segmented text
such as the Wall Street Journal corpus. However,
this advantage is compounded when considering
unsegmented or ambiguously segmented text such
as transcribed speech or less formal written text, as
the HHMM parser can also make decisions about
where to put sentence breaks, and do so in linear
time.4
6 Conclusion and Future Work
This paper furthers the case for the HHMM as a
viable psycholinguistic model of the human pars-
ing mechanism by showing that performance de-
grades gracefully as parallelism decreases, provid-
ing reasonably accurate parsing even at very low
beam widths. In addition, this work shows that
an HHMM parser run at low beam widths is com-
petitive in speed with parsers that don?t work in-
crementally, because of its asymptotically linear
runtime.
This is especially surprising given that the
HHMM uses parallel hypotheses on the beam to
predict whether constituents will remain open for
attachment or not. Success at low beam widths
suggests that this optionally arc-eager prediction
is something that is indeed relatively predictable
during parsing, lending credence to claims of psy-
cholinguistic relevance of HHMM parsing.
Future work should explore further directions
in improving parsing performance at low beam
widths. The lowest beam value experiments
presented here generally parsed fairly accurately
when they completed, but were already encounter-
ing problems with unparseable sentences that neg-
atively affected parser accuracy. The large accu-
racy decrease between beam sizes of 20 and 15 is
likely to be mostly due to the lack of any correct
analysis on the beam when the sentence is com-
pleted.
It should be noted, however, that no adjustments
were made to the parser?s syntactic model with
these beam variations. This syntactic model was
optimized for accuracy at the standard beam width
4It does this probabilistically as a side effect of the pars-
ing, by choosing an analysis in which r0t ? RG (for any t).
of 2000, and thus contains some state splittings
that are beneficial at wide beam widths, but at
low beam widths are redundant and prevent oth-
erwise valid hypotheses from being maintained on
the beam. For applications in which speed is a
priority, future research can evaluate tradeoffs in
accuracy that occur at different beam widths with
a coarser-grained syntactic representation that al-
lows for more variation of hypotheses even on
very small beams.
Acknowledgments
This research was supported by National Science
Foundation CAREER/PECASE award 0447685.
The views expressed are not necessarily endorsed
by the sponsors.
References
Steven P. Abney and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
James Baker. 1975. The Dragon system: an overivew.
IEEE Transactions on Acoustics, Speech and Signal
Processing, 23(1):24?29.
Thomas G. Bever. 1970. The cognitive basis for lin-
guistic structure. In J. ?R. Hayes, editor, Cognition
and the Development of Language, pages 279?362.
Wiley, New York.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
Umesh Patil, and Shravan Vasishth. 2008a. Parsing
costs as predictors of reading difficulty: An evalua-
tion using the Potsdam Sentence Corpus. Journal of
Eye Movement Research, 2(1):1?12.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
and Shravan Vasishth. 2008b. Surprising parser ac-
tions and reading difficulty. In Proceedings of ACL-
08: HLT, Short Papers, pages 5?8, Columbus, Ohio,
June. Association for Computational Linguistics.
Thorsten Brants and Matthew Crocker. 2000. Prob-
abilistic parsing and psychological plausibility. In
Proceedings of COLING ?00, pages 111?118.
Noam Chomsky and George A. Miller. 1963. Intro-
duction to the formal analysis of natural languages.
In Handbook of Mathematical Psychology, pages
269?321. Wiley.
Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87?
185.
Matthew Crocker and Thorsten Brants. 2000. Wide-
coverage probabilistic sentence processing. Journal
of Psycholinguistic Research, 29(6):647?669.
34
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the
Association for Computational Linguistics, pages
159?166, Pittsburgh, PA.
James Henderson. 2004. Lookahead in determinis-
tic left-corner parsing. In Proc. Workshop on Incre-
mental Parsing: Bringing Engineering and Cogni-
tion Together, pages 26?33, Barcelona, Spain.
Frederick Jelinek, Lalit R. Bahl, and Robert L. Mercer.
1975. Design of a linguistic statistical decoder for
the recognition of continuous speech. IEEE Trans-
actions on Information Theory, 21:250?256.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
Japan.
Richard L. Lewis. 2000. Falsifying serial and paral-
lel parsing models: Empirical conundrums and an
overlooked paradigm. Journal of Psycholinguistic
Research, 29:241?248.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
George A. Miller. 1956. The magical number seven,
plus or minus two: Some limits on our capacity
for processing information. Psychological Review,
63:81?97.
Kevin P. Murphy and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833?840, Vancouver, BC, Canada.
Philip Resnik. 1992. Left-corner parsing and psy-
chological plausibility. In Proceedings of COLING,
pages 191?197, Nantes, France.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
Proceedings of the 2009 Conference on Empirical
Methods in Natural Langauge Processing, pages
324?333.
William Schuler, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2008. Toward a
psycholinguistically-motivated model of language.
In Proceedings of COLING, pages 785?792,
Manchester, UK, August.
William Schuler, Samir AbdelRahman, TimMiller, and
Lane Schwartz. 2010. Broad-coverage incremen-
tal parsing using human-like memory constraints.
Computational Linguistics, 36(1).
William Schuler. 2009. Parsing with a bounded
stack using a model-based right-corner transform.
In Proceedings of the North American Association
for Computational Linguistics (NAACL ?09), pages
344?352, Boulder, Colorado.
Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an
incremental right-corner parser. In Proceedings of
the 49th Annual Conference of the Association for
Computational Linguistics.
35
Structured Composition of Semantic Vectors
Stephen Wu
Mayo Clinic
wu.stephen@mayo.edu
William Schuler
The Ohio State University
schuler@ling.ohio-state.edu
Abstract
Distributed models of semantics assume that word meanings can be discovered from ?the com-
pany they keep.? Many such approaches learn semantics from large corpora, with each document
considered to be unstructured bags of words, ignoring syntax and compositionality within a docu-
ment. In contrast, this paper proposes a structured vectorial semantic framework, in which semantic
vectors are defined and composed in syntactic context. As such, syntax and semantics are fully
interactive; composition of semantic vectors necessarily produces a hypothetical syntactic parse.
Evaluations show that using relationally-clustered headwords as a semantic space in this framework
improves on a syntax-only model in perplexity and parsing accuracy.
1 Introduction
Distributed semantic representations like Latent Semantic Analysis (Deerwester et al, 1990), probabilis-
tic LSA (Hofmann, 2001), Latent Dirichlet Allocation (Blei et al, 2003), or relational clustering (Taskar
et al, 2001) have garnered widespread interest because of their ability to quantitatively capture ?gist?
semantic content.
Two modeling assumptions underlie most of these models. First, the typical assumption is that words
in the same document are an unstructured bag of words. This means that word order and syntactic struc-
ture are ignored in the resulting vectorial representations of meaning, and the only relevant relationship
between words is the ?same-document? relationship. Second, these semantic models are not composi-
tional in and of themselves. They require some external process to aggregate the meaning representations
of words to form phrasal or sentential meaning; at best, they can jointly represent whole strings of words
without the internal relationships.
This paper introduces structured vectorial semantics (SVS) as a principled response to these weak-
nesses of vector space models. In this framework, the syntax?semantics interface is fully interactive:
semantic vectors exist in syntactic context, and any composition of semantic vectors necessarily pro-
duces a hypothetical syntactic parse. Since semantic information is used in syntactic disambiguation
(MacDonald et al, 1994), we would expect practical improvements in parsing accuracy by accounting
for the interactive interpretation process.
Others have incorporated syntactic information with vector-space semantics, challenging the bag-
of-words assumption. Syntax and semantics may be jointly generated with Bayesian methods (Griffiths
et al, 2005); syntactic structure may be coupled to the basis elements of a semantic space (Pado? and
Lapata, 2007); clustered semantics may be used as a pre-processing step (Koo et al, 2008); or, semantics
may be learned in some defined syntactic context (Lin, 1998). These techniques are interactive, but
their semantic models are not syntactically compositional (Frege, 1892). SVS is a generative model of
sentences that uses a variant of the last strategy to incorporate syntax at preterminal tree nodes, but is
inherently compositional.
Mitchell and Lapata (2008) provide a general framework for semantic vector composition:
p = f(u,v,R,K) (1)
295
where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base,
and p is a resulting composed vector (or tensor). In this initial work of theirs, they leave out any notion
of syntactic context, focusing on additive and multiplicative vector composition (with some variations):
Add: p[i] = u[i] + v[i] Mult: p[i] = u[i] ? v[i] (2)
Since the structured vectorial semantics proposed here may be viewed within this framework, our dis-
cussion will begin from their definition in Section 2.1.
Erk and Pado??s (2008) model also fits inside Mitchell and Lapata?s framework, and like SVS, it in-
cludes syntactic context. Their semantic vectors use syntactic information as relations between multiple
vectors in arriving at a final meaning representation. The emphasis, however, is on selectional prefer-
ences of individual words; resulting representations are similar to word-sense disambiguation output,
and do not construct phrase-level meaning from word meaning. Mitchell and Lapata?s more recent work
(2009) combines syntactic parses with distributional semantics; but the underlying compositional model
requires (as other existing models would) an interpolation of the vector composition results with a sepa-
rate parser. It is thus not fully interactive.
Though the proposed structured vectorial semantics may be defined within Equation 1, the end output
necessarily includes not only a semantic vector, but a full parse hypothesis. This slightly shifts the focus
from the semantically-centered Equation 1 to an accounting of meaning that is necessarily interactive
(between syntax and semantics); vector composition and parsing are then twin lenses by which the pro-
cess may be viewed. Thus, unlike previous models, a unique phrasal vectorial semantic representation
is composed during decoding.
Due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have
chosen to report results on the well-understood dual problem of parsing. The structured vectorial seman-
tic framework subsumes variants of several common parsing algorithms, two of which will be discussed:
lexicalized parsing (Charniak, 1996; Collins, 1997, etc.) and relational clustering (akin to latent annota-
tions (Matsuzaki et al, 2005; Petrov et al, 2006; Gesmundo et al, 2009)). Because previous work has
shown that linguistically-motivated syntactic state-splitting already improves parses (Klein andManning,
2003), syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive
and intransitive verbs). This pessimistically isolates the contribution of semantics on parsing accuracy ?
it will only show parsing gains where semantic information does not overlap with distributional syntactic
information. Evaluations show that interactively considering semantic information with syntax has the
predicted positive impact on parsing accuracy over syntax alone; it also lowers per-word perplexity.
The remainder of this paper is organized as follows: Section 2 describes SVS as both vector compo-
sition and parsing; Section 3 shows how relational-clustering SVS subsumes PCFG-LAs; and Section 4
evaluates modeling assumptions and empirical performance.
2 Structured Vectorial Semantics
2.1 Vector Composition
We begin with some notation. This paper will use boldfaced uppercase letters to indicate matrices
(e.g., L), boldfaced lowercase letters to indicate vectors (e.g., e), and no boldface to indicate any single-
valued variable (e.g. i). Indices of vectors and matrices will be associated with semantic concepts
(e.g., i1, i2, . . .); variables over those indices are single-value (scalar) variables (e.g., i); the contents
of vectors and matrices can be accessed by index (e.g., e[i1] for a constant, e[i] for a variable). We will
also define an operation d(?), which lists the elements of a column vector on the diagonal of a diagonal
matrix, i.e., d(e)[i, i]=e[i]. Often, these variables will technically be functions with arguments written
in parentheses, producing vectors or matrices (e.g., L(l) produces a matrix based on the value of l).
As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate
our vectors for now. We can rewrite their equation (Equation 1) in SVS notation by following several
conventions. All semantic vectors have a fixed dimensionality and are denoted e; source vectors and the
296
a) e
(lMOD )S
e0
(lMOD )NP
e00
(lMOD )DT
the
e01
(lID )NN
engineers
pulled off ...
b)
e00 =
he
a
dw
o
rd
s
i u
i t
i p
truth???????
0
1
.1
???????
e01 =
he
a
dw
o
rd
s
i u
i t
i p
truth???????
0
0
1
???????
aT0 = [ .1 .1 .8 ]
L0?00(lMOD) =
pa
re
n
t0
i u
i t
i p
child 00
ip it iu
???????
0 .2 .8
0 0 1
.1 .4 .5
???????
M(lMOD?NP ? lMOD?DT lID?NN) =
pa
re
n
t0
i u
i t
i p
parent 0
ip it iu
???????
.2 0 0
0 .1 0
0 0 .4
???????
Figure 1: a) Syntax and semantics on a tree during decoding. Semantic vectors e are subscripted with the
node?s address. Relations l and syntactic categories c are constants for the example. b) Example vectors
and matrices needed for the composition of a vector at address 0 (Section 2.2.1).
target vector are differentiated by subscript; instead of context variables R and K we will use M and L:
e? = f(e?,e?,M,L) (3)
Syntactic context is in the form of grammar rules M that are aware of semantic concepts; semantic
knowledge is in the form of labeled dependency relationships between semantic concepts, L. Both of
these are present and explicitly modeled as matrices in SVS?s canonical form of vector composition:
e? = M ? d(L??? ? e?) ? d(L??? ? e?) ? 1 (4)
Here, M is a diagonal matrix that encapsulates probabilistic syntactic information, where the syntactic
probabilities depend on the semantic concept being considered. TheLmatrices are linear transformations
that capture how semantically relevant source vectors are to the resulting vector (e.g., L??? defines the
the relevance of e? to e?), with the intuition that two 1D vectors are under consideration and require
a 2D matrix to relate them. 1 is a vector of ones ? this takes a diagonal matrix and returns a column
vector corresponding to the diagonal elements.
Of note in this definition of f(?) is the presence of matrices that operate on distributed semantic
vectors. While it is widely understood that matrices can represent transformations, relatively few have
used matrices to represent the distributed, dynamic nature of meaning composition (see Rudolph and
Giesbrecht (2010) for a counterexample).
2.2 Syntax?Semantics Interface
This section aims to more thoroughly define the way in which the syntax and semantics interact during
structured vectorial semantic composition. SVS will specify this interface such that the composition of
semantic vectors is probabilistically consistent and subsumes parsing under various frameworks. Parsing
has at times added semantic annotations that unwittingly carry some semantic value: headwords (Collins,
1997) are one-word concepts that subsume the words below them; latent annotations (Matsuzaki et al,
2005) are clustered concepts that touch on both syntactic and semantic information at a node. Of course,
other annotations (Ge and Mooney, 2005) carry more explicit forms of semantics. In this light, semantic
concepts (vector indices i) and relation labels (matrix arguments l) may also be seen as annotations on
grammar trees.
Let us introduce notation to make the connection with parsing and syntax explicit. This paper will
denote syntactic categories as c and string yields as x. The location of these variables in phrase structure
will be identified using subscripts that describe the path from the root to the constituent.1 Paths consist
of left and/or right branches (indicated by ?0?s and ?1?s, respectively, as in Figure 1a). Variables ?, ?,
and ? stand for whole paths; ? is the path of a composed vector; and  is the empty path at the root. The
yield x? is the observed (sub)string that eventually results from the progeny of c? . Multiple trees ?? can
be constructed at ? by stringing together grammar rules that are consistent with observed text.
1For simplicity, trees are assumed to be compiled into strictly binary-branching form.
297
2.2.1 Lexicalized Parsing
To illustrate the definitions and operations presented in this section, we start with the concrete ?semantic?
space of headwords (i.e., bilexical parsing) before moving on to a formal definition. Our example here
corresponds to the best parse of the first two words in Figure 1a. In this example domain, assume that
the semantic space of concept headwords is {ipulled, ithe, iunk}, abbreviated as {ip, it, iu} where the last
concept is a constant for infrequently-observed words. This semantic space becomes the indices of
semantic vectors; complete vectors e at each node of Figure 1a are shown in Figure 1b.
The tree in Figure 1a contains complete concept vectors e at each node, with corresponding indices
i. Values in these vectors (see Figure 1b) are probabilities, indicating the likelihood that a particular
concept summarizes the meaning below a node. For example, consider e00: it produces the yield below
address 00 (?the?) with probability 1, and iu may also produce ?the? with probability 0.1.
Not shown on the tree are the matrices in Figure 1b. In the parametrized matrix
M(lMOD?NP ? lMOD?DT lID?NN), each diagonal element corresponds to the hypothesized grammar rule?s
probability, given a headword. Similarly, the matrix L0?00(lMOD) is parametrized by the semantic context
lMOD ? here, lMOD represents a generalized ?modifier? semantic role. For the semantic concept ip at ad-
dress 0, the left-child modifier (address 00) could be semantic concept it with probability 0.2, or concept
iu with probability 0.8. Finally, by adding an identity matrix for L0?01(lID) (a ?head? semantic role) to
the quantities in Figure 1b, we would have all the components to construct the vector at address 0:
e0 =
???????
.2 0 0
0 .1 0
0 0 .4
????????????????????????????????????????????????
M
?d??
???????
0 .2 .8
0 0 1
.1 .4 .5
????????????????????????????????????????????????
L0?01
???????
0
1
.1
???????dcurly
e00
?
? ? d
?
?
???????
1 0 0
0 1 0
0 0 1
?????????????????????????????????
L0?01
???????
0
0
1
???????dcurly
e01
?
? ?
???????
1
1
1
???????dcurly
1
=
i u
i t
i p
truth???????
0
0
0.036
???????????????????????????????
e0
Since the vector was constructed in syntactic and semantic context, the tree structure shown (including
semantic relationships l) is implied by the context.
2.2.2 Probabilities in vectors and matrices
Formally defining the probabilities in Figure 1, SVS populates vectors and matrices by means of 5
probability models (models are denoted by ?), along with the process of composition:
Syntactic model M(lc?? lc? lc?)[i? , i?] =P?M(lci? ? lc? lc?)
Semantic model L???(l?)[i? , i?] =P?L(i? ? i? , l?)
Preterminal model e?[i?] =P?P-Vit(G)(x? ? lci?), for preterm ? (5)
Root const. model aT [i] =PpiG(lci)
Any const. model aT?[i?] =PpiG(lci?)
These probabilities are encapsulated into vectors and matrices using a convention: column indices of
vectors or matrices represent conditioned semantic variables, row indices represent modeled variables.
As an example, from Figure 1b, elements of L0?00(lMOD) represent the probability P?L(i00 ? i0, l00).
Thus, the conditioned variable i00 is shown in the figure as column indices, and the modeled i0 as
row indices. This convention applies to the M matrix as well. Recall that M is a diagonal matrix
? its rows and columns model the same variable. Thus, we could rewrite P?M(lci? ? lc? lc?) as
P?M(lci? ? lc? lc?, i?) to make a consistent probabilistic interpretation.
We have intentionally left out the probabilistic definition of normal (non-preterminal) nonterminals
P?Vit(G) , and the rationale for aT vectors. These are both best understood in the dual problem of parsing.
2.2.3 Vector Composition for Parsing
The vector composition of Equation 4 can be rewritten with all arguments and syntactic information as:
e? = M(lc? ? lc? lc?) ? d(L???(l?) ? e?) ? d(L???(l?) ? e?) ? 1 (4?)
298
a compact representation that masks the underlying consistent probability operations. This section will
expand the vector composition equation to show its equivalence to standard statistical parsing methods.
Let us say that e?[i?] = P(x? ? lci?), the probability of giving a particular yield given the present
distributed semantics. Recall that in matrix multiplication, there is a summation over the inner dimen-
sions of the multiplied objects; replacing matrices and vectors with their probabilistic interpretations and
summing in the appropriate places, each element of e? is then:
e?[i?] = P?M(lci? ? lc? lc?) ? ?
i?
P?L(i? ? i? , l?) ? P?Vit(G)(x? ? lci?)
? ?
i?
P?L(i? ? i? , l?) ? P?Vit(G)(x? ? lci?) (6)
This can be loosely considered the multiplication of the syntax (?M term), left-child semantics (first
sum), and right-child semantics (second sum). The only summations are between L and e, since all other
multiplications are between diagonal matrices (similar to pointwise multiplication).
We can simplify this probability expression by grouping ?M and ?L into a grammar rule
P?G(lci?? lci? lci?)def= P?M(lci?? lc? lc?) ? P?L(i? ? i? , l?) ? P?L(i? ? i? , l?), since they deal with every-
thing except the yield of the two child nodes. The summations are then pushed to the front:
e?[i?] = ?
i?,i?
P?G(lci? ? lci? lci?)?P?Vit(G)(x? ? lci?) ? P?Vit(G)(x? ? lci?) (7)
Thus, we have a standard chart-parsing probability P(x? ? lci?) ? with distributed semantic concepts ?
in each vector element.
The use of grammar rules necessarily builds a hypothetical subtree ?? . In a typical CKY algorithm,
the tree corresponding to the highest probability would be chosen; however, we have not defined how to
make this choice for vectorial semantics.
We will choose the best tree with probability 1.0, so we define a deterministic Viterbi probability
over candidate vectors (not concepts) and context variables:
P?Vit(G)(x? ? lce?)def= J e? = argmaxlce? (aT? e? ? PpiG(lcaT? ) ? P?Vit(G)(x ? lce?))K (8)
where J?K is an indicator function such that J?K=1 if ? is true, 0 otherwise. Intuitively, the process is as
follows: we construct the vector e? at a node, according to Eqn. 4?; we then weight this vector against
prior knowledge about the context aT? ; the best vector in context will be chosen (the argmax). Also, the
vector at a node comes with assumptions of what structure produced it. Thus, the last two terms in the
parentheses are deterministic models ensuring that the best subtree ?? is indeed the one generated.
Determining the root constituent of the Viterbi tree is the same process as choosing any other Viterbi
constituent, except that prior contextual knowledge gets its own probability model in aT . As before,
the most likely tree ?? is the tree that maximizes the probability at the root, and can be constructed
recursively from the best child trees. Importantly, ?? has an associated, sentential semantic vector which
may be construed as the composed semantic information for the whole parsed sentence. Similar phrasal
semantic vectors can be obtained anywhere on the parse chart.
These equations complete the linear algebraic definition of structured vectorial semantics.
3 SVS with Relational Clusters
3.1 Inducing Relational Clusters
Unlike many vector space models that are based on the frequencies of terms in documents, we may
consider frequencies of terms that occur in similar semantic relations (e.g., head lID or modifier lMOD).
Reducing the dimensionality of terms in a term?context matrix will result in relationally-clustered con-
cepts. From a parsing perspective, this amounts to latent annotations (Matsuzaki et al, 2005) in l-context.
299
Let us re-notate the headword-lexicalized version of SVS (the example in Section 2.2.1) using h
for headword semantics, and reserve i for relationally-clustered concepts. Treebank trees can be deter-
ministically annotated with headwords h and relations l by using head rules (Magerman, 1995). The 5
SVS models ?M, ?L, ?P-Vit(G), piG, and piG can thus be obtained by counting instances and normalizing.
Empirical probabilities of this kind are denoted with a tilde, whereas estimated models have a hat.
Concepts i in a distributed semantic representation, however, cannot be found from annotated trees
(see example concepts in Figure 2). Therefore, we use Expectation Maximization (EM) in a variant of
the inside-outside algorithm (Baker, 1979) to learn distributed-concept behavior. In the M-step, the data-
informed result of the E-step is used to update the estimates of ?M, ?L, and ?H (where ?H is a generlization
of ?P-Vit(G) to any nonterminal). These updated estimates are then plugged back in to the next E-step. The
two steps continually alternate until convergence or a maximum number of iterations.
E-step:
P?(i? , i?, i? ? lc? , lc?, lc?) = P??Out(lci? , lch?lch?) ? P??Ins(lch? ? lci?)P?(lch) (9)
E(lci? ,lci?,lci?) = P?(i? ,i?,i? ?lc? ,lc?,lc?) ?P?(lc? ,lc?,lc?)
M-step:
P??M(lci?  lc?, lc?) =
?i?,i? E(lci? , lci?, lci?)
?lci?,lci? E(lci? , lci?, lci?)
P??L(i? ? i? ; l?) =
?lc? ,c?,lci? E(lci? , lci?, lci?)
?lc? ,ci?,lci? E(lci? , lci?, lci?)
(10)
P??H(h? ? lci?) = E(lci? ,?,?)?h? E(lci? ,?,?)
Inside probabilities can be recursively calculated on training trees from the bottom up. These are
simply probability sums of all subsumed subtrees (Viterbi probabilities with sums instead of maxes).
Outside probabilities can also be recursively calculated from training trees, here from parent proba-
bilities. For a left child (the right-child case is similar):
P??Out(lci?, lch?lch?) =P??Out(lci? , lch?lch?) ? P??M(lci?  lc?, lc?)
? ?i? P??L(i? ? i? , l?) ? P??Ins(lch? ? lci?) ? P??L(i? ? i? , l?) (11)
Since outside probabilities signify everything but what is subsumed by the node, they carry a comple-
mentary set of information to inside probabilities. Thus, inside and outside probabilities together are a
natural way to produce parent and child clustered concepts.
3.2 Relational Semantic Clusters in Parsing
Section 2.2.2 listed the five probability models necessary for SVS. To define SVS with relational clusters,
the estimates in Equation 10 can be used for ?M and ?L.
The preterminal model is based on ?H, but it also includes some backoff for words that have not been
used as headwords. The other two models also fall out nicely from the algorithm, though they are not
explicitly estimated in EM. The prior probability at the root is just the base case for outside probabilities:
P?piG(lci)def= P??Out(lci, lch?lch) (12)
Prior probabilities at non-root constituents are estimated from the empirically-weighted joint probability.
P?piG(lci?)def= ?
lci?,lci?
P?(lci? , lci?, lci?) (13)
With these models, a relationally-clustered SVS parser is now defined.
300
Cluster i1
?announcement?
unk 0.362
was 0.173
reported 0.097
posted 0.036
earned 0.029
filed 0.024
were 0.022
had 0.020
told 0.013
approved 0.013
Cluster i5
?change in value?
rose 0.137
fell 0.124
unk 0.116
gained 0.063
dropped 0.051
attributed 0.051
jumped 0.046
added 0.041
lost 0.039
advanced 0.022
Cluster i7
?change possession?
unk 0.381
had 0.065
was 0.062
took 0.036
bought 0.027
completed 0.025
received 0.024
were 0.023
got 0.018
made 0.018
acquired 0.016
Figure 2: Example ?H clusters from 1,000 head-
words clustered into 10 referents, after 10 EM itera-
tions, for transitive past-tense verbs (VBD-argNP).
0 5 10 15 20 25 30 35 40
0
100
200
300
400
500
Sentence Length
A
ve
ra
ge
 P
ar
si
ng
 T
im
e 
(s)
 
 
Non?vectorized
Vectorized
Figure 3: Speed of relationally-clustered SVS
parsers, with and without vectorization.
4 Evaluation
Sections 02?21 of the Wall Street Journal (WSJ) corpus were used as training data; Section 23 was
used as test data with reported parsing results on sentences greater than length 40. Punctuation was
left in for all reported evaluations. Trees were binarized, and syntactic states were thoroughly split into
subcategorization classes. As previously discussed, unlike tests on state-of-the-art automatically state-
splitting parsers, this isolates the contribution of semantics. The baseline 83.57 F-measure is comparable
to Klein and Manning (2003) before the inclusion of head annotations.
Subsequently, each branch was annotated with a head relation lID or a modifier relation lMOD ac-
cording to a binarized version of headword percolation rules (Magerman, 1995; Collins, 1997), and the
headword was propagated up from its head constituent. The most frequent headwords (e.g., h1, . . . , h50)
were stored, and the rest were assigned a constant, ?unk? headword category.
From counts on the binary rules of these annotated trees, the ?M, ?L, ?P-Vit(G), piG, and piG proba-
bilities for headword-lexicalization SVS were obtained. Modifier relations lMOD were deterministically
augmented with their syntactic context; both c and l symbols appearing fewer than 10 times in the whole
corpus were assigned ?unknown? categories.
These lexicalized models served as a baseline, but the augmented trees from which they were derived
were also inputs to the EM algorithm in Section 3.1. Each parameter in the model or training algorithm
was examined, with ?I ?={1,5,10,15,20} clusters, random initialization from reproducible seeds, and a
varying numbers of EM iterations.
The implemented parser had few adjustments from a plain CKY parser other than these vectors. No
approximate inference was used, with no beam for candidate parses and no re-ranking.
4.1 Interpretable relational clusters
Figure 2 shows example clusters for one of the headword models used, where EM clustered 1,000 head-
words into 10 concepts in 10 iterations. The lists are parts of the P??H(h? ? lci?) model. As such, each of
the 10 clusters will only produce headwords in light of some syntactic constituent. The figure shows how
distributed concepts produce headwords for transitive past-tense verbs. Note that the probability distri-
butions for different headwords are quite uneven, again confirming that some clusters are more specific,
and others are more general.
Each cluster has been given a heading of its approximate meaning ? i5, for example, mostly picks
verbs that are ?change in value? events. With 10 clusters, we might not expect such fine-grained clusters,
since pLSA-related approaches typically use several hundred for such tasks. The syntactic context of
transitive (and therefore state-split) past-tense verbs allows for much finer-grained distinctions, which
are then predominantly semantic in nature.
301
a)
Sec. 23, length < 40 wds LR LP F
syntax-only baseline: 83.32 83.83 83.57
headword-lex. 10hw: 83.10 83.61 83.35
headword-lex. 50hw: 83.09 83.40 83.24
rel?n clust. 50hw10 clust: 83.67 84.13 83.90
b)
Sec. 23, length < 40 wds LR LP F
baseline1 clust 83.34 83.90 83.62
1000 hw5 clust, avg 83.85 84.23 84.04
1000 hw10 clust, avg 84.04 84.40 84.21
1000 hw15 clust, avg 84.15 84.38 84.26
1000 hw20 clust, avg 84.21 84.42 84.31
Table 1: a) Unsmoothed lexicalized CKY parsers versus 10 semantic clusters. Evaluations were run
with EM trained to 10 iterations. b) Average dependence of parsing performance on number of semantic
clusters. Averages are taken over different random seeds, with EM running 4 or 10 iterations.
4.2 Engineering considerations
We should note that relationally-clustered SVS is feasible with respect to random initialization and speed.
Four relationally-clustered SVS models (with 500 headwords clustered into 5 concepts) were trained,
each having a different random initialization. We found that the parsing F-score had a mean of 83.98
and a standard deviation of 0.21 across different initializations of the model. This indicates that though
there are significant difference between the models, they still outperform models without SVS (see next
section).
Also, it may seem slow to consider the set of semantic concepts and relations alongside syntax, at
least with respect to normal parsing. The definition of SVS in terms of vectors actually mitigates this
effect on WSJ Section 23, according to Figure 3. Since SVS is probabilistically consistent, the parser
could be defined without vectors, but this would have the ?non-vectorized? speed curve. The contiguous
storage and access of information in the ?vectorized? version leads to an efficient implementation.
4.3 Comparison to Lexicalization
One important comparison to draw here is between the effectiveness of semantic clusters versus
headword-lexicalization. For fair head-to-head comparison onWSJ Section 23, both models were vector-
ized and included no smoothing or backoff. Neither relational clusters nor lexicalization were optimized
with backoff or smoothing.
Table 1a shows precision, recall, and F-score for lexicalized models and for clustered semantic mod-
els. First, note that the 10-cluster model (in bold) improves on a syntax-only parser (top line), showing
that the semantic model is contributing useful information to the parsing task.
Next, compare the 50-headword, 10-cluster model (in bold) to the line above it. It is natural to
compare this model to the headword-lexicalized model with 50 headwords, since the same information
from the trees is available to both models. The relationally-clustered model outperforms the headword-
lexicalized model, showing that clustering the headwords actually improves their usefulness, despite the
fact that fewer referents are used in the actual vectors.
It is also interesting, then, to compare this 50-headword, 10-cluster model to a headword-lexicalized
model with 10 headwords. In this case, the possible size of the grammar is equal. Again, the relationally-
clustered model outperforms plain lexicalization. This indicates that the 10 clustered referents are much
more meaningful than 10 headword referents for the disambiguating of syntax.
4.4 Effect of Number of clusters
The final experiment on relational-clustering SVS was to determine whether performance would vary
with the number of clusters. Table 1b compares average performance (over different random initializa-
tions) for numbers of clusters from 1 (a syntax-equivalent case) to 20.
First, it should be noted that all of the relationally clustered models improved on the baseline. Ran-
dom initializations did not vary enough for these models to do worse than syntax alone. For each vec-
tor/domain size, in fact, the gains over syntax-only are substantial.
302
In addition, the table shows that average performance increases with the number of clusters. This
loosely positive slope means that EM is still finding useful parts of the semantic space to explore and
cluster, so that the clusters remain meaningful. However, the increase in performance with number of
clusters is likely to eventually plateau.
Maximum-accuracy models were also evaluated, since each model is a full-fledged parser. The best
20-referent model obtained an F score of 84.60%, beating the syntactic baseline by almost a full absolute
point. Thus, finding relationally-clustered semantic output also contributes to some significant parsing
benefit.
4.5 Perplexity
Finally, per-word perplexities were calculated for a syntactic model and for a 5-concept relationally-
clustered model. Specific to this evaluation, following Mitchell and Lapata (2009), only the top 20,000
words in WSJ Sections 02-21 were kept in training or test sentences, and the rest replaced with ?unk?;
numbers were replaced with ?num.?
Sec. 23, ?unk?+?num? Perplexity
syntax only baseline 428.94
rel?n clust. 1khw?005e 371.76
Table 2: Model fit as measured by perplexity.
Table 2 shows that adding semantic information greatly reduces perplexity. Since as much syntactic
information as possible (such as argument structure) has been pre-annotated onto trees, the isolated
contribution of interactive semantics improves on a syntax-only model model.
5 Conclusion
This paper has introduced a structured vectorial semantic (SVS) framework in which vector composition
and syntactic parsing are a single, interactive process. The framework thus fully integrates distributional
semantics with traditional syntactic models of language.
Two standard parsing techniques were defined within SVS and evaluated: headword-lexicalization
SVS (bilexical parsing) and relational-clustering SVS (latent annotations). It was found that relationally-
clustered SVS outperformed the simpler lexicalized model and syntax-only models, and that additional
clusters had a mildly positive effect. Additionally, perplexity results showed that the integration of
distributed semantics in relationally-clustered SVS improved the model over a non-interactive baseline.
It is hoped that this flexible framework will enable new generations of interactive interpretation
models that deal with the syntax?semantics interface in a plausible manner.
References
Baker, J. (1979). Trainable grammars for speech recognition. In D. Klatt and J. Wolf (Eds.), Speech
Communication Papers for the 97th Meeting of the Acoustical Society of America, pp. 547?550.
Blei, D. M., A. Y. Ng, and M. I. Jordan (2003). Latent dirichlet alocation. Journal of Machine Learning
Research 3, 993?1022.
Charniak, E. (1996). Tree-bank grammars. In Proceedings of the National Conference on Artificial
Intelligence, pp. 1031?1036.
Collins, M. (1997). Three generative, lexicalised models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computational Linguistics (ACL ?97).
303
Deerwester, S., S. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman (1990). Indexing by latent
semantic analysis. Journal of the American Society for Information Science 41(6), 391?407.
Erk, K. and S. Pado? (2008). A structured vector space model for word meaning in context. In Proceedings
of EMNLP 2008.
Frege, G. (1892). Uber sinn und bedeutung. Zeitschrift fur Philosophie und Philosophischekritik 100,
25?50.
Ge, R. and R. J. Mooney (2005). A statistical semantic parser that integrates syntax and semantics. In
Ninth Conference on Computational Natural Language Learning, pp. 9?16.
Gesmundo, A., J. Henderson, P. Merlo, and I. Titov (2009). A latent variable model of synchronous
syntactic-semantic parsing for multiple languages. In Proceedings of CoNLL, pp. 37?42. Association
for Computational Linguistics.
Griffiths, T. L., M. Steyvers, D. M. Blei, and J. B. Tenenbaum (2005). Integrating topics and syntax.
Advances in neural information processing systems 17, 537?544.
Hofmann, T. (2001). Unsupervised learning by probabilistic latent semantic analysis. Machine Learn-
ing 42(1), 177?196.
Klein, D. and C. D. Manning (2003). Accurate unlexicalized parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics, Sapporo, Japan, pp. 423?430.
Koo, T., X. Carreras, and M. Collins (2008). Simple semi-supervised dependency parsing. In Proceed-
ings of the 46th Annual Meeting of the ACL, Volume 8. Citeseer.
Lin, D. (1998). An information-theoretic definition of similarity. In Proceedings of the 15th International
Conference on Machine Learning, Volume 296304.
MacDonald, M. C., N. J. Pearlmutter, and M. S. Seidenberg (1994). The lexical nature of syntactic
ambiguity resolution. Psychological Review 101(4), 676?703.
Magerman, D. (1995). Statistical decision-tree models for parsing. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguistics (ACL?95), Cambridge, MA, pp. 276?283.
Matsuzaki, T., Y. Miyao, and J. Tsujii (2005). Probabilistic CFG with latent annotations. In Proceedings
of the 43rd Annual Meeting on Association for Computational Linguistics, pp. 75?82. Association for
Computational Linguistics.
Mitchell, J. and M. Lapata (2008). Vector-based models of semantic composition. In Proceedings of
ACL-08: HLT, Columbus, OH, pp. 236?244.
Mitchell, J. and M. Lapata (2009). Language Models Based on Semantic Composition. In Proceedings
of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 430?439.
Pado?, S. and M. Lapata (2007). Dependency-based construction of semantic space models. Computa-
tional Linguistics 33(2), 161?199.
Petrov, S., L. Barrett, R. Thibaux, and D. Klein (2006). Learning accurate, compact, and interpretable
tree annotation. In Proceedings of the 44th Annual Meeting of the Association for Computational
Linguistics (COLING/ACL?06).
Rudolph, S. and E. Giesbrecht (2010). Compositional matrix-space models of language. In Proceedings
of ACL 2010. Association for Computational Linguistics.
Taskar, B., E. Segal, and D. Koller (2001). Probabilistic classification and clustering in relational data.
In IJCAI?01: Proceedings of the 17th international joint conference on Artificial intelligence, San
Francisco, CA, USA, pp. 870?876. Morgan Kaufmann Publishers Inc.
304
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 25?30,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Tree-Rewriting Models of Multi-Word Expressions
William Schuler
Department of Linguistics
The Ohio State University
schuler@ling.osu.edu
Aravind K. Joshi
Dept. Computer and Information Science
University of Pennsylvania
joshi@linc.cis.upenn.edu
Abstract
Multi-word expressions (MWEs) account for
a large portion of the language used in day-
to-day interactions. A formal system that is
flexible enough to model these large and often
syntactically-rich non-compositional chunks as
single units in naturally occurring text could
considerably simplify large-scale semantic an-
notation projects, in which it would be un-
desirable to have to develop internal compo-
sitional analyses of common technical expres-
sions that have specific idiosyncratic meanings.
This paper will first define a notion of functor-
argument decomposition on phrase structure
trees analogous to graph coloring, in which the
tree is cast as a graph, and the elementary
structures of a grammar formalism are colors.
The paper then presents a formal argument
that tree-rewriting systems, a class of grammar
formalism that includes Tree Adjoining Gram-
mars, are able to produce a proper superset
of the functor-argument decompositions that
string-rewriting systems can produce.
1 Introduction
Multi-word expressions (MWEs), whose structure
and meaning cannot be derived from their compo-
nent words as they occur independently, account for
a large portion of the language used in day-to-day
interactions. Indeed, the relatively low frequency of
comparable single-word paraphrases for elementary
spatial relations like ?in front of? (compare to ?be-
fore?) or ?next to? (compare to ?beside?) suggest a
fundamentality of expressions, as opposed to words,
as a basic unit of meaning in language (Becker, 1975;
Fillmore, 2003). Other examples of MWEs are id-
ioms such as ?kick the bucket? or ?spill the beans?,
which have figurative meanings as expressions that
sometimes even allow modification (?spill some of the
beans?) and variation in sentence forms (?which beans
were spilled??), but are not available when the com-
ponent words of the MWE occur independently. A
formal system that is flexible enough to model these
large and often syntactically-rich non-compositional
chunks as single units in naturally occurring text
could considerably simplify large-scale semantic an-
notation projects, in which it would be undesirable
to have to develop internal compositional analyses
of common technical expressions that have specific
idiosyncratic meanings.
Models have been proposed for MWEs based on
string-rewriting systems such as HPSG (Sag et al,
2002), which model compositionality as string ad-
jacency of a functor and an argument substring.
This string-rewriting model of compositionality es-
sentially treats each projection of a head word as
a functor, each capable of combining with an argu-
ment to yield a higher-level projection or functor.
The set of projections from a lexical head can there-
fore be thought of as a single elementary structure:
an n-ary functor, subsuming the arguments of the
individual functors at each projection. This kind of
approach is intuitive for fully-compositional analy-
ses (e.g. in which a transitive verb like ?hold? is a
functor and a NP complement like ?the basket? is an
argument), but is less natural when applied to sub-
strings of MWEs (e.g. treating pick as a functor and
up as an argument in the verb-particle MWE pick
. . . up), since some of these arguments do not have
any semantic significance (in the pick . . . up exam-
ple , there is no coherent meaning for Up such that
Jpick X upK = Pick(JXK,Up)).
This paper will argue that tree-rewriting systems,
a class of grammar formalisms that includes Tree
Adjoining Grammars (Joshi, 1985; Joshi and Sch-
abes, 1997), are a more natural candidate for mod-
eling MWEs since they can model entire fragments
of phrase structure trees as elementary (locally non-
compositional) semantic building blocks, in addition
to the set of head-projections used in string-rewriting
25
NA
proverbial
N?
S
NP? VP
V
kick
NP
D
the
N
bucket
Figure 1: Composition of elementary trees for idiom
MWE ?kick the bucket? and adjective ?proverbial,? with
the same semantics as an adverb ?proverbially? adjoining
at the VP.
systems. This allows more flexibility in defining the
functor-argument decomposition of a given phrase
structure tree.
This will be demonstrated by reducing the functor-
argument decompositions (compositional accounts of
semantics assigned to portions of phrase structure
trees) of string-rewriting systems to a special case
of functor-argument decompositions of tree-rewriting
systems. Discussion in this paper will focus on
string-rewriting systems augmented with unification
(such as HPSG) because in this framework the issue
of multi-word expressions has been discussed (Sag
et al, 2002). The arguments in this paper also ap-
ply to other string rewriting systems such as catego-
rial grammars (Ajdukiewicz, 1935; Bar-Hillel, 1953;
Steedman, 2000), but in these formalisms the issues
concerning MWEs have not been extensively devel-
oped. Essentially, this paper formalizes the intuition
(Abeille?, 1993) that the extended domain of locality
of tree-rewriting systems allows them to provide a
compositional account of the semantics assigned to
multi-word or idiomatic portions of phrase structure
trees using elementary units that, after composition,
may end up partially discontinuous in these trees.
For example, a portion of a phrase structure tree for
?kick the bucket? with a single interpretation equiv-
alent to ?die? can be modified through adjunction
of the adjective ?proverbial? at the noun constituent
?bucket? without postulating separate semantics for
?kick? (see Figure 1).
2 Definitions
String rewriting systems are sets of rules for re-
placing symbols with other symbols in strings. A
rewriting of some start symbol into a set of lexical
symbols is called a derivation. Rewrite rules in a
string rewriting system can be defined to have des-
ignated functor and argument symbols. Any deriva-
tion ? can therefore yield a functor-argument decom-
position D(?), essentially defining a set of semantic
functor-argument dependencies among structured el-
ementary categories.
For simplicity, a functor-argument decomposition
will be defined as a mapping from the constituent
nodes in a phrase structure tree to the nodes in
the elementary structures used to derive that tree.
This can be thought of as a coloring of phrase struc-
ture nodes, in which colors correspond to elementary
structures in the rewriting system. The elementary
structures used in such a decomposition may then
be considered n-ary functors, which may take sev-
eral arguments, each of a different color.
In string-rewriting systems such as HPSG, these
n-ary functors consist of a head word and its pro-
jections, and the arguments of the functor are the
non-projecting child of each such projection. Fig-
ure 2 shows feature-based and categorial analyses
for the MWE ?. . . to the . . . power? (as in ?raise Y
to the X power?) which is taken here to have unam-
biguous meaning (in a technical context) as Y X or
Pow(Y,X), and is analyzed here to wrap around an
ordinal number argument X and then adjoin onto a
verb phrase ?raise Y ? as a modifier.1 Because their
elementary structures are projected up from individ-
ual head words, these systems prohibit an analysis
of this MWE as a single wrapping functor. Instead,
MWEs like this must be decomposed into individual
functor words (e.g. power) and argument words (e.g.
the, and to).
Tree-rewriting systems, on the other hand, allow
elementary structures to contain nodes which are nei-
ther projections nor argument sites. This permits
an analysis of ?to the . . . power? as a single functor
wrapped around its argument (see Figure 3), with-
out having to specify functor-argument relations be-
tween power, to, and the.
More generally, string-rewriting systems use ele-
mentary structures (n-ary functors) that originate
at the lexical item and exhibit a bottom-up branch-
ing structure, branching to an argument site and a
higher level projection at each step. In contrast, tree-
rewriting systems use elementary structures that
originate at a phrasal or clausal node and exhibit
1We are using the MWE ?. . . to the . . . power? as a sim-
ple example with an unambiguous meaning in the domain
of mathematics to illustrate our main points in the context
of both adjunction and substitution operations. Alternative
analyses are possible (e.g. with ?the? or additional modifiers
adjoining in, to allow variations like ?to every even power un-
der six?), but in any case the words ?to? and ?power? on either
side of the X argument are taken to be idiosyncratic to this
expression of Y X . Since it is analyzed as a modifier, this ex-
ample can be used to demonstrate coindexation of structure
in a tree-rewriting system.
26
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
label : power
left :
[
label : ORD
]
proj :
?
?
?
?
?
?
?
?
?
?
?
?
?
?
label : N1
left :
[
label : the
]
proj :
?
?
?
?
?
?
?
?
?
?
label : NP
left :
[
label : to
]
proj :
?
?
?
?
?
?
label : PP
left :
[ label : VP
1
]
proj :
[ label : VP
1
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
=
power
?
OR l
N1
p
the l
NP
p
to l
PP
p
VP l1
VP
p
1
Figure 2: Elementary structures for a verb-phrase-modifying preposition in a functor-argument analysis derived from
a feature structure grammar. Here, ? indicates the origin node and boxed numbers indicate coindexations.
a top-down branching structure that mirrors that of
a phrase structure tree. As one might expect, there
are tree-rewriting systems (namely those whose el-
ementary structures contain multiple lexical items)
that can produce functor-argument decompositions
(?colorings?) of a phrase structure tree which can-
not be produced by a string-rewriting system. More
surprisingly however, this paper will show that the
converse is not true: in other words, for any string-
rewriting system there always exists a tree-rewriting
system that can produce the same functor-argument
decomposition of a phrase structure tree. Thus, the
set of functor-argument decompositions that can be
produced by tree-rewriting systems is a proper super-
set of those that can be produced by string-rewriting
systems.
This is surprising because, taken as a class,
there is no inherent difference in recognition com-
plexity between string-rewriting systems and tree-
rewriting systems (as may be the case between spe-
cific members of these classes, say between CGs
and TAGs), since both are worst-case exponential
if unconstrained coindexation of structure is allowed
(as in unification grammars). This is also surpris-
ing because, since they branch upward, the ele-
mentary structures of string-rewriting systems can
specify complex functors as arguments, which the
downward-branching elementary structures of tree-
rewriting systems cannot. However, this paper will
show that this ability to specify complex functors
as arguments does not confer any additional flexibil-
ity in calculating functor-argument decompositions
of phrase structure trees, and can be factored out
with no loss in expressivity.
VP
VP? PP
to? NP
the? N1
OR? power?
=
VP
?
1
VP
1
1 PP
2
to
1
? NP
2
the
1
? N1
2
OR
1
power
2
?
Figure 3: Elementary structure for a verb-phrase-
modifying prepositional phrase ?to the . . . power? in a
tree-rewriting system, derived from a tree-adjoining
grammar. Here, ? indicates the origin node, ? indicates a
non-argument node (or lexical ?anchor?), and boxed num-
bers indicate coindexations.
3 Reduction of string-rewriting
systems to tree-rewriting systems
The first step will be to define an n-ary functor in a
string-rewriting system as a kind of elementary struc-
ture ? (a tree in fact), whose nodes ?? branch ?up-
ward? into sub-structure nodes (connected by depart-
ing arcs labeled l, r, or p,) specifying a left or right
argument category (???l or ???r) and a projected
category (???p), rather than branching ?downward?
into left and right child constituents as in an ordi-
nary phrase structure tree.2 In order to extend this
reduction to feature-based systems, these elemen-
tary structures will also be augmented with coindex-
ation sets I of elementary structure nodes that must
be identical (in terms of labels and departing arcs)
in any functor-argument decomposition of a phrase
structure tree.
2Here, a node ?? is defined by the path of concatenated
arcs ? that lead to it from the origin or root ?? .
27
SNP
Cube
VP
VP
raises NP
the sum
PP
to NP
the NP
ORD
third
power
? :
raise
?
NPr
VP
p
NP l
S
p
? :
power
?
OR l
N1
p
the l
NP
p
to l
PP
p
VP l1
VP
p
1
?p?p?DFA(??)
?p?I(?p?p?p?l)
Figure 4: Decomposition (?coloring?) of a phrase structure tree ? for the sentence ?Cube raises the sum to the third
power?, using elementary structures ? and ? shown at right. Dotted lines from phrase structure tree nodes ?? to
elementary structure nodes ?? indicate that ?? generates ?? in the functor-argument decomposition: ?? ?DFA(??).
Dashed lines from elementary structure nodes ?? to other elementary structure nodes ?? indicate that ?? is among
the nodes identified with ?? as arguments of ? in the decomposition. Boxed identifiers indicated coindices between
nodes ?? and ??? in ? such that ?I?? . ?? , ??? ?I.
Figure 4 shows a functor-argument decomposition
(or ?coloring?) of a phrase structure tree using these
upward-branching elements.
The upward-branching elementary structures used
in any such decomposition can then be converted
into a normal form in which all argument nodes are
atomic (have no departing arcs), using the following
transformations of elementary structures to equiva-
lent structures that fit together generate the same
functor-argument decomposition. This is done by si-
multaneously excising ?matched? material from both
the argument branch of an elementary structure and
the top of the elementary structure that is its argu-
ment in the given decomposition.
The use of coindexation sets complicates this
transformation somewhat. Initial configurations of
coindexation sets in upward-branching elementary
structures can be exhaustively partitioned into three
classes, defined with respect to the ?trunk? of the el-
ementary structure, which is the set of nodes con-
nected to the origin by paths containing only p arcs.
These classes are:
1. coindexations with more than one coindexed
node on the trunk,
2. coindexations with fewer than one coindexed
node on the trunk, and
3. coindexations with exactly one coindexed node
on the trunk.
Elementary structures in the first class, with more
than one coindexed node on the trunk, are equivalent
to graphs with directed cycles, and are ordinarily
excluded from feature-based analyses, so they will
be ignored here.
Elementary structures in the second class, with
fewer than one coindexed node on the trunk,
can be converted to equivalent structures with
no coindices (which trivially satisfies the above
argument-atomicity requirement), using the simulta-
neous excision of ?matched? structure in functor and
argument structures described above, by simply ex-
tending this to cover the portion of the argument
elementary structure that extends all the way to the
top of the trunk.
Elementary structures in the third class, with
exactly one coindexed node on the trunk, can
be converted to equivalent structures that sat-
isfy argument-atomicity using a three-step process.
First, the upward-branching sub-structures above
these coindexed nodes (if any) are unified, so the arcs
departing from each coindexed node will be recur-
sively identical (this must be possible in any feature-
based grammar, or the coindexation would be ill-
formed, and should therefore be excluded). The coin-
dexation is then recursively slid up along the p arc
departing from each such node, until the coindexa-
28
tion set contains nothing but atomic categories (with
no departing arcs). Finally, the argument nodes are
made to be atomic using the simultaneous excision of
?matched? structure in functor and argument struc-
tures described above, leaving an (atomic) coindex-
ation at each (atomic) argument position in each af-
fected branch.
Elementary structures with multiple class 3 coin-
dexation sets I and I ? (which cannot be deleted
as described above for class 2 sets) can be trans-
formed into structures with a single coindexation
set I by copying the portion of the trunk between
the (unique) on-trunk members of each initial set I
and I ? onto every other node in the set I ? that con-
tains the lower trunk node (this copy should include
the coindex belonging to I). The coindexation set
I ? containing the lower on-trunk node is then simply
deleted.
The normal-form upward-branching structures re-
sulting from this transformation can now be con-
verted into downward-branching elementary trees in
a tree-rewriting system (with coindexed nodes corre-
sponding to ?root? and ?foot? nodes as defined for tree-
adjoining grammars) by simply replacing each pair
of argument and conclusion arcs with a pair of left-
child and right-child arcs departing the conclusion
node. Since the normal form for upward-branching
elementary structures allows only atomic arguments,
this re-drawing of arcs must result in well-formed
downward-branching elementary trees in every case.3
In particular, this conversion results in a subset of
tree-rewriting systems in which each (binary) branch
of every elementary tree must have exactly one argu-
ment position and one non-argument position among
its two children. This is a special case of a more
general class of tree-rewriting systems, which may
have two argument positions or no argument po-
sitions among the children at each binary branch.
Such trees are not equivalent to trees with a single ar-
gument position per branch, because they will result
in different functor-argument decompositions (?col-
orings?) of a target phrase structure tree. Moreover,
it is precisely these non-string-rewriting-equivalent
elementary trees that are needed to model the lo-
cal non-compositionality of larger multi-word expres-
sions like ?threw X to the lions? (see Figure 5), be-
cause only downward branches with multiple non-
3Recognition and parsing of feature-based grammars, and
of tree-rewriting systems whose elementary trees contain mul-
tiple foot nodes, are both exponential in the worst case. How-
ever, both types of grammars are amenable to regular-from re-
strictions which prohibit recursive adjunction at internal (non-
root, non-foot) tree nodes, and thereby constrain recognition
and parsing complexity to cubic time for most kinds of natural
language grammars (Rogers, 1994).
S
NP? VP
VP
threw? NP?
PP
to? NP
the? lions?
Figure 5: Elementary structure for MWE idiom ?threw
. . . to the lions,? allowing modification to both VP, PP
and NP sub-constituents (e.g. ?threw your friends today
right to the proverbial lions).
argument children can produce the multi-level sub-
trees containing the word ?threw? and the word ?lions?
in the same elementary unit.
4 Conclusion
This paper has shown that tree-rewriting systems
are able to produce a superset of the functor-
argument decompositions that can be produced by
string-rewriting systems such as categorial gram-
mars and feature-structure grammars such as HPSG.
This superset additionally allows elementary units
to contain multiple (lexical) leaves, which a string-
rewriting system cannot. This makes tree-rewriting
systems ideally suited to the analysis of natural lan-
guage texts that contain many multi-word expres-
sions with idiosyncratic (non-compositional) mean-
ings. Although neither the tree-rewriting nor the
string-rewriting analyses defined above can be gen-
erated in guaranteed polynomial time (since they
may require the construction of unbounded stacks
of unrecognized structure during bottom-up recogni-
tion), they can both be made polynomial (indeed, cu-
bic) by the introduction of ?regular form? constraints
(Rogers, 1994), which limit this stack in the same
way in both cases.
In contrast with representations like that of
(Villavicencio et al, 2004), in which concepts are dis-
tributed over several lexical entries, a tree-rewriting
representation such as the one described in this pa-
per allows only a single lexical entry to be listed for
each concept. For example:
... throw ... to the lions:
(s(np0!)(vp(v)(np1!)(pp(p)(np(d)(n)))))
... to the ... power:
(vp(vp0*)(pp(p)(np(d)(n(a1!)(n)))))
(using the notation ?!? and ?*? for substitution sites
and foot nodes, respectively). It is anticipated that
this will simplify the organization of lexical resources
for multi-word expressions.
29
References
Abeille?, Anne. 1993. The flexibility of french idioms:
a representation with lexicalized tree adjoining gram-
mar. In A. Schenk and E. van der Linden, editors,
Idioms. Erlbaum.
Ajdukiewicz, Kazimierz. 1935. Die syntaktische kon-
nexitat. In S. McCall, editor, Polish Logic 1920-1939.
Oxford University Press, pages 207?231. Translated
from Studia Philosophica 1: 1?27.
Bar-Hillel, Yehoshua. 1953. A quasi-arithmetical nota-
tion for syntactic description. Language, 29:47?58.
Becker, Joseph D. 1975. The phrasal lexicon. In Pro-
ceedings of the Workshop on Theoretical Issues in Nat-
ural Language Processing, Workshop in Computational
Linguisitcs, Psychology, and AI, Cambridge, MA.
Fillmore, Charles J. 2003. Multiword expressions,
November. Invited talk at the Institute for Research in
Cognitive Science (IRCS), University of Pennsylvania.
http://www.cis.upenn.edu/?ircs/colloq/2003/fall/fillmore.html.
Joshi, Aravind and Yves Schabes. 1997. Tree-adjoning
grammars. In G. Rozenberg and A. Salomaa, edi-
tors, Handbook of Formal Languages. Springer-Verlag,
Berlin, pages 69?123.
Joshi, Aravind K. 1985. How much context sensitivity
is necessary for characterizing structural descriptions:
Tree adjoining grammars. In L. Karttunen D. Dowty
and A. Zwicky, editors, Natural language parsing: Psy-
chological, computational and theoretical perspectives.
Cambridge University Press, Cambridge, U.K., pages
206?250.
Rogers, James. 1994. Capturing CFLs with tree ad-
joining grammars. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?94).
Sag, Ivan, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: a pain in the neck for nlp. In Proceedings
of the Third International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLING?02), pages 1?15, Mexico City, Mexico.
Steedman, Mark. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
Villavicencio, Aline, Ann Copestake, Benjamin Waldron,
and Fabre Lambeau. 2004. Lexical encoding of
mwes. In Takaaki Tanaka, Aline Villavicencio, Fran-
cis Bond, and Anna Korhonen, editors, Second ACL
Workshop on Multiword Expressions: Integrating Pro-
cessing, pages 80?87, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
30
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 51?60,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Connectionist-Inspired Incremental PCFG Parsing
Marten van Schijndel
The Ohio State University
vanschm@ling.ohio-state.edu
Andy Exley
University of Minnesota
exley@cs.umn.edu
William Schuler
The Ohio State University
schuler@ling.ohio-state.edu
Abstract
Probabilistic context-free grammars (PCFGs)
are a popular cognitive model of syntax (Ju-
rafsky, 1996). These can be formulated to
be sensitive to human working memory con-
straints by application of a right-corner trans-
form (Schuler, 2009). One side-effect of the
transform is that it guarantees at most a sin-
gle expansion (push) and at most a single re-
duction (pop) during a syntactic parse. The
primary finding of this paper is that this prop-
erty of right-corner parsing can be exploited to
obtain a dramatic reduction in the number of
random variables in a probabilistic sequence
model parser. This yields a simpler structure
that more closely resembles existing simple
recurrent network models of sentence compre-
hension.
1 Introduction
There may be a benefit to using insights from human
cognitive modelling in parsing. Evidence for in-
cremental processing can be seen in garden pathing
(Bever, 1970), close shadowing (Marslen-Wilson,
1975), and eyetracking studies (Tanenhaus et al,
1995; Allopenna et al, 1998; Altmann and Kamide,
1999), which show humans begin attempting to pro-
cess a sentence immediately upon receiving lin-
guistic input. In the cognitive science community,
this incremental interaction has often been mod-
elled using recurrent neural networks (Elman, 1991;
Mayberry and Miikkulainen, 2003), which utilize
a hidden context with a severely bounded repre-
sentational capacity (a fixed number of continuous
units or dimensions), similar to models of activation-
based memory in the prefrontal cortex (Botvinick,
2007), with the interesting possibility that the dis-
tributed behavior of neural columns (Horton and
Adams, 2005) may directly implement continuous
dimensions of recurrent hidden units. This paper
presents a refinement of a factored probabilistic se-
quence model of comprehension (Schuler, 2009) in
the direction of a recurrent neural network model
and presents some observed efficiencies due to this
refinement.
This paper will adopt an incremental probabilis-
tic context-free grammar (PCFG) parser (Schuler,
2009) that uses a right-corner variant of the left-
corner parsing strategy (Aho and Ullman, 1972)
coupled with strict memory bounds, as a model of
human-like parsing. Syntax can readily be approxi-
mated using simple PCFGs (Hale, 2001; Levy, 2008;
Demberg and Keller, 2008), which can be easily
tuned (Petrov and Klein, 2007). This paper will
show that this representation can be streamlined to
exploit the fact that a right-corner parse guarantees
at most one expansion and at most one reduction can
take place after each word is seen (see Section 2.2).
The primary finding of this paper is that this prop-
erty of right-corner parsing can be exploited to ob-
tain a dramatic reduction in the number of random
variables in a probabilistic sequence model parser
(Schuler, 2009) yielding a simpler structure that
more closely resembles connectionist models such
as TRACE (McClelland and Elman, 1986), Short-
list (Norris, 1994; Norris and McQueen, 2008), or
recurrent models (Elman, 1991; Mayberry and Mi-
ikkulainen, 2003) which posit functional units only
for cognitively-motivated entities.
The rest of this paper is structured as follows:
Section 2 gives the formal background of the right-
corner parser transform and probabilistic sequence
51
model parsing. The simplification of this model is
described in Section 3. A discussion of the interplay
between cognitive theory and computational mod-
elling in the resulting model may be found in Sec-
tion 4. Finally, Section 5 demonstrates that such
factoring also yields large benefits in the speed of
probabilistic sequence model parsing.
2 Background
2.1 Notation
Throughout this paper, PCFG rules are defined over
syntactic categories subscripted with abstract tree
addresses (c??). These addresses describe a node?s
location as a path from a given ancestor node. A 0
on this path represents a leftward branch and a 1 a
rightward branch. Positions within a tree are repre-
sented by subscripted ? and ? so that c?0 is the left
child of c? and c?1 is the right child of c?. The set of
syntactic categories in the grammar is denoted by C.
Finally, J?K denotes an indicator probability which
is 1 if ? and 0 otherwise.
2.2 Right-Corner Parsing
Parsers such as that of Schuler (2009) model hierar-
chically deferred processes in working memory us-
ing a coarse analogy to a pushdown store indexed
by an embedding depth d (to a maximum depth D).
To make efficient use of this store, a CFG G must
be transformed using a right-corner transform into
another CFG G? with no right recursion. Given an
optionally arc-eager attachment strategy, this allows
the parser to clear completed parse constituents from
the set of incomplete constituents in working mem-
ory much earlier than with a conventional syntactic
structure. The right-corner transform operates de-
terministically over a CFG following three mapping
rules:
c? ? c?0 c?1 ? G
c?/c?1 ? c?0 ? G?
(1)
c?? ? c??0 c??1 ? G, c? ? C
c?/c??1 ? c?/c?? c??0 ? G?
(2)
c?? ? x?? ? G, c? ? C
c? ? c?/c?? c?? ? G?
(3)
A bottom-up incremental parsing strategy com-
bined with the way the right-corner transform pulls
each subtree into a left-expanding hierarchy ensures
at most a single expansion (push) will occur at
any given observation. That is, each new observa-
tion will be the leftmost leaf of a right-expanding
subtree. Additionally, by reducing multiply right-
branching subtrees to single rightward branches, the
transform also ensures that at most a single reduc-
tion (pop) will take place at any given observation.
Schuler et al (2010) show near complete cover-
age of the Wall Street Journal portion of the Penn
Treebank (Marcus et al, 1993) can be achieved with
a right-corner incremental parsing strategy using no
more than four incomplete contituents (deferred pro-
cesses), in line with recent estimates of human work-
ing memory capacity (Cowan, 2001).
Section 3 will show that, in addition to being de-
sirable for bounded working memory restrictions,
the single expansion/reduction guarantee reduces
the search space between words to only two decision
points ? whether to expand and whether to reduce.
This allows rapid processing of each candidate parse
within a sequence modelling framework.
2.3 Model Formulation
This transform is then extended to PCFGs and inte-
grated into a sequence model parser. Training on
an annotated corpus yields the probability of any
given syntactic state executing an expansion (creat-
ing a syntactic subtree) or a reduction (completing
a syntactic subtree) to transition from every suffi-
ciently probable (in this sense active) hypothesis in
the working memory store.
The probability of the most likely sequence of
store states q?1..D1..T can then be defined as the prod-
uct of nonterminal ?Q, preterminal ?P,d, and termi-
nal ?X factors:
q?1..D1..T
def= argmax
q1..D1..T
T
?
t=1
P?Q(q1..Dt | q1..Dt?1 pt?1)
? P?P,d? (pt | b
d?
t ) ? P?X (xt | pt) (4)
where all incomplete constituents qdt are factored
into active adt and awaited bdt components:
qdt
def= adt /bdt (5)
and d? determines the deepest non-empty incomplete
constituent of q1..Dt :
52
d? def= max{d | qdt 6= ???} (6)
The preterminal model ?P,d denotes the expecta-
tion of a subtree containing a given preterminal, ex-
pressed in terms of side- and depth-specific gram-
mar rules P?Gs,d(c? ? c?0 c?1) and expected counts
of left progeny categories E?G?,d(c?
?? c?? ...) (see
Appendix A):
P?P,d(c?? | c?)
def= E?G?,d(c?
?? c?? ...)
?
?
x??
P?GL,d(c?? ? x??) (7)
and the terminal model ?X is simply:
P?X (x? | c?)
def=
P?G(c? ? x?)
?
x? P?G(c? ? x?)
(8)
The Schuler (2009) nonterminal model ?Q is
computed from a depth-specific store element
model ?Q,d and a large final state model ?F,d:
P?Q(q1..Dt | q1..Dt?1 pt?1)
def=
?
f1..Dt
D
?
d=1
P?F,d(fdt | fd+1t qdt?1 qd?1t?1 )
? P?Q,d(qdt | fd+1t fdt qdt?1 qd?1t ) (9)
After each time step t and depth d, ?Q generates
a set of final states to generate a new incomplete
constituent qdt . These final states fdt are factored
into categories cfdt and boolean variables (0 or 1)
encoding whether a reduction has taken place at
depth d and time step t. The depth-specific final state
model ?F,d gives the probability of generating a final
state fdt from the preceding qdt and qd?1t which is the
probability of executing a reduction or consolidation
of those incomplete constituents:
P?F,d(fdt | fd+1t qdt?1 qd?1t?1 )
def=
{
if fd+1t = ??? : Jfdt = 0K
if fd+1t 6= ??? : P?F,d,R(fdt | qdt?1 qd?1t?1 )
(10)
With these depth-specific fdt in hand, the model can
calculate the probabilities of each possible qdt for
each d and t based largely on the probability of tran-
sitions (?Q,d,T ) and expansions (?Q,d,E) from the in-
complete constituents at the previous time step:
P?Q,d(qdt | fd+1t fdt qdt?1 qd?1t?1 )
def=
?
?
?
if fd+1t = ???, fdt = ??? : Jqdt = qdt?1K
if fd+1t 6= ???, fdt = ??? : P?Q,d,T (qdt | fd+1t fdt qdt?1 qd?1t )
if fd+1t 6= ???, fdt 6= ??? : P?Q,d,E (qdt | qd?1t )
(11)
This model is shown graphically in Figure 1.
The probability distributions over reductions
(?F,d,R), transitions (?Q,d,T ) and expansions
(?Q,d,E) are then defined, also in terms of side- and
depth-specific grammar rules P?Gs,d(c? ? c?0 c?1)
and expected counts of left progeny cate-
gories E?G?,d(c?
?? c?? ...) (see Appendix A):
P?Q,d,T (qdt | fd+1t fdt qdt?1qd?1t )
def=
{
if fdt 6= ???: P?Q,d,A(qdt | qd?1t fdt )
if fdt = ???: P?Q,d,B (qdt | qdt?1fd+1t )
(12)
P?F,d,R(fdt | fd+1t qdt?1qd?1t?1 )
def=
{
if cfd+1t 6=xt : Jf
d
t = ???K
if cfd+1t =xt : P?F,d,R(f
d
t | qdt?1qd?1t?1 )
(13)
P?Q,d,E (c??/c??? | /c?)
def=
E?G?,d(c?
?? c?? ...) ? Jx?? = c??? = c??K (14)
The subcomponent models are obtained by ap-
plying the transform rules to all possible trees pro-
portionately to their probabilities and marginalizing
over all constituents that are not used in the models:
? for active transitions (from Transform Rule 1):
P?Q,d,A(c??/c??1 | /c? c??0)
def=
E?G?,d(c?
?? c?? ...) ? P?GL,d(c?? ? c??0 c??1)
E?G?,d(c?
+? c??0 ...)
(15)
53
p1
x1
q11
q21
q31
q41
p2
x2
q12
q22
q32
q42
p3
x3
q13
q23
q33
q43
p4
x4
q14
q24
q34
q44
p5
x5
q15
q25
q35
q45
p6
x6
q16
q26
q36
q46
p7
x7
q17
q27
q37
q47
f12
f22
f32
f42
f13
f23
f33
f43
f14
f24
f34
f44
f15
f25
f35
f45
f16
f26
f36
f46
f17
f27
f37
f47
=
DT
=
the
=
0,D
T
=
NP
/N
N
=
NN
=
fun
d
=
0,N
P
=
S/V
P
=
VB
=
bo
ug
ht
=
0,V
B = S
/V
P
=
VP
/N
P
=
DT
=
tw
o
=
1,D
T
=
S/V
P
=
VP
/N
N
=
JJ
=
re
gio
na
l
=
1,J
J
=
S/V
P
=
VP
/N
N
=
NN
=
ban
ks
=
1,V
P = S
/RB
=
RB
=
tod
ay
Figure 1: Schuler (2009) Sequence Model
? for awaited transitions (Transform Rule 2):
P?Q,d,B (c?/c??1 | c??/c?? c??0)
def=
Jc? = c??K ?
P?GR,d(c?? ? c??0 c??1)
E?G?,d(c??
0? c??0 ...)
(16)
? for reductions (from Transform Rule 3):
P?F,d,R(c??,1 | /c? c???/ )
def=
Jc?? = c???K ?
E?G?,d(c?
0? c?? ...)
E?G?,d(c?
?? c?? ...)
(17)
P?F,d,R(c??,0 | /c? c???/ )
def=
Jc?? = c???K ?
E?G?,d(c?
+? c?? ...)
E?G?,d(c?
?? c?? ...)
(18)
3 Simplified Model
As seen in the previous section, the right-corner
parser of Schuler (2009) makes the center embed-
ding depth explicit and each memory store element
is modelled as a combination of an active and an
awaited component. Each input can therefore either
increase (during an expansion) or decrease (during
a reduction) the store of incomplete constituents or
it can alter the active or awaited component of the
deepest incomplete constituent (the affectable ele-
ment). Alterations of the awaited component of the
affectable element can be thought of as the expan-
sion and immediate reduction of a syntactic con-
stituent. The grammar models transitions in the ac-
tive component implicitly, so these are conceptual-
ized as consisting of neither an expansion nor a re-
duction.
Removing some of the variables in this model re-
sults in one that looks much more like a neural net-
work (McClelland and Elman, 1986; Elman, 1991;
Norris, 1994; Norris and McQueen, 2008) in that
all remaining variables have cognitive correllates ?
in particular, they correspond to incomplete con-
stituents in working memory ? while still maintain-
ing the ability to explicitly represent phrase struc-
ture. This section will demonstrate how it is possi-
ble to exploit this to obtain a large reduction in the
number of modelled random variables.
In the Schuler (2009) sequence model, eight ran-
dom variables are used to model the hidden states
at each time step (see Figure 1). Half of these vari-
ables are joint consisting of two further (active and
awaited) constituent variables, while the other half
are merely over intermediate final states. Although
the entire store is carried from time step to time
step, only one memory element is affectable at any
one time, and this element may be reduced zero or
54
one times (using an intermediate final state), and ex-
panded zero or one times (using an incomplete con-
stituent state), yielding four possible combinations.
This means the model only actually needs one of its
intermediate final states.
The transition model ?Q can therefore be simpli-
fied with terms ?F,d for the probability of expand-
ing the incomplete constituent at d, and terms ?A,d
and ?B,d for reducing the resulting constituent
(defining the active and awaited components of
a new incomplete constituent), along with terms
for copying incomplete constituents above this af-
fectable element, and for emptying the elements be-
low it:
P?Q(q1..Dt | q1..Dt?1 pt?1)
def=P?F,d? (?+? | bd
?
t?1 pt?1) ? P?A,d? (??? | b
d??1
t?1 ad
?
t?1)
? Jad??1t =ad
??1
t?1 K ? P?B,d??1(b
d??1
t | bd
??1
t?1 ad
?
t?1)
? Jq1..d??2t =q1..d
??2
t?1 K ? Jqd
?..D
t = ???K
+P?F,d? (?+? | bd
?
t?1 pt?1) ? P?A,d? (ad
?
t | bd
??1
t?1 ad
?
t?1)
? P?B,d? (b
d?
t | ad
?
t ad
?+1
t?1 )
? Jq1..d??1t =q1..d
??1
t?1 K ? Jqd
?+1..D
t = ???K
+P?F,d? (??? | bd
?
t?1 pt?1) ? P?A,d? (??? | bd
?
t?1 pt?1)
? Jad?t =ad
?
t?1K ? P?B,d? (b
d?
t | bd
?
t?1 pt?1)
? Jq1..d??1t =q1..d
??1
t?1 K ? Jqd
?+1..D
t = ???K
+P?F,d? (??? | bd
?
t?1 pt?1) ? P?A,d? (a
d?+1
t | bd
?
t?1 pt?1)
? P?B,d? (b
d?+1
t | ad
?+1
t pt?1)
? Jq1..d?t =q1..d
?
t?1 K ? Jqd
?+2..D
t = ???K (19)
The first element of the sum in Equation 19 com-
putes the probability of a reduction with no expan-
sion (decreasing d?). The second corresponds to the
probability of a store undergoing neither an expan-
sion nor a reduction (a transition to a new active con-
stituent at the same embedding depth). In the third
is the probability of an expansion and a reduction
(a transition among awaited constituents at the same
embedding depth). Finally, the last term yields the
probability of an expansion without a reduction (in-
creasing d?).
From Equation 19 it may be seen that the unaf-
fected store elements of each time step are main-
tained sans change as guaranteed by the single-
reduction feature of the right-corner parser. This re-
sults in a large representational economy by mak-
ing the majority of store state decisions determinis-
tic. This representational economy will later trans-
late into computational efficiencies (see section 5).
In this sense, cognitive modelling contributes to a
practical speed increase.
Since the bulk of the state remains the same,
the recognizer can access the affectable variable
and operate solely over the transition possibili-
ties from that variable to calculate the distribu-
tion over store states for the next time step to ex-
plore. Reflecting this change, the hidden states
now model a single final-state variable (f) for
results of the expansion decision, and the af-
fectable variable resulting from the reduction de-
cision (both its active (a) and awaited (b) cate-
gories), as well as the preterminal state (p) defined
in the previous section. These models are again ex-
pressed in terms of side- and depth-specific grammar
rules P?Gs,d(c? ? c?0 c?1) and expected counts of
left progeny categories E?G?,d(c?
?? c?? ...) (see
Appendix A).
Expansion probabilities are modelled as a binary
decision depending on whether or not the awaited
component of the affectable variable c? is likely to
expand immediately into an anticipated pretermi-
nal c?? (resulting in a non-empty final state: ?+?) or
if intervening embeddings are necessary given the
affectable active component (yielding no final state:
???):
P?F,d(f | c? c??)
def=
?
?
?
?
?
?
?
?
?
if f= ?+? :
E?G?,d (c?
0
?c?? ...)
E?G?,d (c?
?
?c?? ...)
if f= ??? :
E?G?,d (c?
+
?c?? ...)
E?G?,d (c?
?
?c?? ...)
(20)
The active component category c?? is defined as de-
pending on the category of the awaited component
above it c? and its left-hand child c??0:
P?A,d(c?? | c? c??0)
def=
E?G?,d (c?
1
?c??0 ...)
E?G?,d (c?
+
?c??0 ...)
? Jc??= ???K
+
E?G?,d (c?
+
?c?? ...)?P?GL,d (c???c??0 ...)
E?G?,d (c?
+
?c??0 ...)
(21)
The awaited component category c?1 is defined as
55
depending on the category of its parent c? and the
preceding sibling c?0:
P?B,d(c?1 | c? c?0)
def=
P?GR,d (c??c?0 c?1)
E?G?,d (c?
1
?c?0 ...)
(22)
Both of these make sense given the manner in which
the right-corner parser shifts dependencies to the left
down the tree in order to obtain incremental infor-
mation about upcoming constituents.
3.1 Graphical Representation
In order to be represented graphically, the working
memory store ?Q is factored into a single expansion
term ?F and a product of depth-specific reduction
terms ?Q,d:
P?Q(q1..Dt | q1..Dt?1 pt?1)
def=
?
ft
P?F (ft | q1..Dt?1 )
?
D
?
d=1
P?Q,d(qdt | q1..Dt?1 pt?1 ft qd+1t ) (23)
and the depth-specific reduction model ?Q,d is fac-
tored into individual decisions over each random
variable:
P?Q,d(qdt | q1..Dt?1 pt?1 ft qd+1t )
def=
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
if qd+1t = ???, ft 6= ???, d=d??1 :
Jadt =adt?1K ? P?B,d(bdt | bdt?1 ad+1t?1 )
if qd+1t = ???, ft 6= ???, d=d? :
P?A,d(adt | bd?1t?1 adt?1) ? P?B,d(bdt | adt adt?1)
if qd+1t = ???, ft= ???, d=d? :
Jadt =adt?1K ? P?B,d(bdt | bdt?1 pt?1)
if qd+1t = ???, ft= ???, d=d?+1 :
P?A,d(adt | bd?1t?1 pt?1) ? P?B,d(bdt | adt pt?1)
if qd+1t 6= ??? : Jqdt =qdt?1K
otherwise : Jqdt = ???K
(24)
This dependency structure is represented graphically
in Figure 2.
The first conditional in Equation 24 checks
whether the input causes a reduction but no expan-
sion (completing a subtree parse). In this case, d? is
reduced from the previous t, and the relevant qdt?1 is
copied to qdt except the awaited constituent is altered
to reflect the completion of its preceding awaited
subtree. In the second case, the parser makes an
active transition as it completes a left subtree and
begins exploring the right subtree. The third case
is similar to the first except it transitions between
two like depths (awaited transition), and depends on
the preterminal just seen to contrive a new subtree
to explore. In the fourth case, d? is incremented as
another incomplete constituent opens up in working
memory. The final two cases simply update the un-
affected store states to reflect their previous states at
time t? 1.
4 Discussion
This factoring of redundant hidden states out of the
Schuler (2009) probabilistic sequence model shows
that cognitive modelling can more closely approx-
imate a simple recurrent network model of lan-
guage processing (Elman, 1991). Probabilistic se-
quence model parsers have previously been mod-
elled with random variables over incomplete con-
stituents (Schuler, 2009). In the current implementa-
tion, each variable can be thought of as a bank of ar-
tificial neurons. These artificial neurons inhibit one
another through the process of normalization. Con-
versely, they activate artificial neurons at subsequent
time steps by contributing probability mass through
the transformed grammar. This point was made by
Norris and McQueen (2008) with respect to lexical
access; this model extends it to parsing.
Recurrent networks can parse simple sentences
but run into problems when running over more com-
plex datasets. This limitation comes from the unsu-
pervised methods typically used to train them, which
have difficulty scaling to sufficiently large training
sets for more complex constructions. The approach
described in this paper uses a hidden context simi-
lar to that of a recurrent network to inform the pro-
gression of the parse, except that the context is in
terms of random variables with distributions over a
set of explicit syntactic categories. By framing the
variable domains in a linguistically-motivated fash-
ion, the problem of acquisition can be divested from
the problem of processing. This paper then uses the
semi-supervised grammar training of Petrov et al
(2006) in order to develop a simple, accurate model
for broad-coverage parsing independent of scale.
56
p1
x1
q11
q21
q31
q41
p2
x2
q12
q22
q32
q42
p3
x3
q13
q23
q33
q43
p4
x4
q14
q24
q34
q44
p5
x5
q15
q25
q35
q45
p6
x6
q16
q26
q36
q46
p7
x7
q17
q27
q37
q47
f2 f3 f4 f5 f6 f7 f8=D
T
=the
=NP/
NN
=NN
=fund
=+
=S/V
P
=VB
=bou
ght
=S/V
P
=VP/
NP
=DT
=two
=S/V
P
=VP/
NN
=JJ
=re
giona
l
=S/V
P
=VP/
NN
=NN
=ban
ks
=+
=S/R
B
=RB
=toda
y
=+
Figure 2: Parse using Simplified Model
Like Schuler (2009), the incremental parser dis-
cussed here operates in O(n) time where n is the
length of the input. Further, by its incremental na-
ture, this parser is able to run continuously on a
stream of input, which allows any other processes
dependent on the input (such as discourse integra-
tion) to run in parallel regardless of the length of the
input.
5 Computational Benefit
Due to the decreased number of decisions required
by this simplified model, it is substantially faster
than previous similar models. To test this speed in-
crease, the simplified model was compared with that
of Schuler (2009). Both parsers used a grammar that
had undergone 5 iterations of the Petrov et al (2006)
split-merge-smooth algorithm as found to be opti-
mal by Petrov and Klein (2007), and both used a
beam-width of 500 elements. Sections 02-21 of the
Wall Street Journal Treebank were used in training
the grammar induction for both parsers according to
Petrov et al (2006), and Section 23 was used for
evaluation. No tuning was done as part of the trans-
form to a sequence model. Speed results can be seen
in Table 1. While the speed is not state-of-the-art in
the field of parsing at large, it does break new ground
for factored sequence model parsers.
To test the accuracy of this parser, it was com-
pared using varying beam-widths to the Petrov and
Klein (2007) and Roark (2001) parsers. With the
exception of the Roark (2001) parser, all parsers
used 5 iterations of the Petrov et al (2006) split-
System Sec/Sent
Schuler 2009 74
Current Model 12
Table 1: Speed comparison with an unfactored proba-
bilistic sequence model using a beam-width of 500 ele-
ments
System P R F
Roark 2001 86.6 86.5 86.5
Current Model (500) 86.6 87.3 87.0
Current Model (2000) 87.8 87.8 87.8
Current Model (5000) 87.8 87.8 87.8
Petrov Klein (Binary) 88.1 87.8 88.0
Petrov Klein (+Unary) 88.3 88.6 88.5
Table 2: Accuracy comparison with state-of-the-art mod-
els. Numbers in parentheses are number of parallel acti-
vated hypotheses
merge-smooth algorithm, and the training and test-
ing datasets remained the same. These results may
be seen in Table 2. Note that the Petrov and Klein
(2007) parser allows unary branching within the
phrase structure, which is not well-defined under the
right-corner transform. To obtain a fair comparison,
it was also run with strict binarization. The cur-
rent approach achieves comparable accuracy to the
Petrov and Klein (2007) parser assuming a strictly
binary-branching phrase structure.
57
6 Conclusion
The primary goal of this paper was to demonstrate
that a cognitively-motivated factoring of an exist-
ing probabilistic sequence model parser (Schuler,
2009) is not only more attractive from a modelling
perspective but also more efficient. Such factor-
ing yields a much slimmer model where every vari-
able has cognitive correlates to working memory el-
ements. This also renders several transition prob-
abilities deterministic and the ensuing representa-
tional economy leads to a 5-fold increase in pars-
ing speed. The results shown here suggest cognitive
modelling can lead to computational benefits.
References
Alfred V. Aho and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation and Compiling; Volume. I:
Parsing. Prentice-Hall, Englewood Cliffs, New Jersey.
P. D. Allopenna, J. S. Magnuson, and M. K. Tanenhaus.
1998. Tracking the time course of spoken word recog-
nition using eye movements: evidence for continuous
mapping models. Journal of Memory and Language,
38:419?439.
G. T. M. Altmann and Y. Kamide. 1999. Incremental
interpretation at verbs: restricting the domain of sub-
sequent reference. Cognition, 73:247?264.
Richard Bellman. 1957. Dynamic Programming.
Princeton University Press, Princeton, NJ.
Thomas G. Bever. 1970. The cognitive basis for lin-
guistic structure. In J. ?R. Hayes, editor, Cognition and
the Development of Language, pages 279?362. Wiley,
New York.
Matthew Botvinick. 2007. Multilevel structure in behav-
ior and in the brain: a computational model of fusters
hierarchy. Philosophical Transactions of the Royal So-
ciety, Series B: Biological Sciences, 362:1615?1626.
Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage ca-
pacity. Behavioral and Brain Sciences, 24:87?185.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
Jeffrey L. Elman. 1991. Distributed representations,
simple recurrent networks, and grammatical structure.
Machine Learning, 7:195?225.
John Hale. 2001. A probabilistic earley parser as a psy-
cholinguistic model. In Proceedings of the Second
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 159?166,
Pittsburgh, PA.
Jonathan C Horton and Daniel L Adams. 2005. The cor-
tical column: a structure without a function. Philo-
sophical Transactions of the Royal Society of London
- Series B: Biological Sciences, 360(1456):837?862.
Daniel Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science: A Multidisciplinary Journal, 20(2):137?194.
Roger Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126?1177.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
William D. Marslen-Wilson. 1975. Sentence per-
ception as an interactive parallel process. Science,
189(4198):226?228.
Marshall R. Mayberry, III and Risto Miikkulainen. 2003.
Incremental nonmonotonic parsing through semantic
self-organization. In Proceedings of the 25th Annual
Conference of the Cognitive Science Society, pages
798?803, Boston, MA.
James L. McClelland and Jeffrey L. Elman. 1986. The
trace model of speech perception. Cognitive Psychol-
ogy, 18:1?86.
Dennis Norris and James M. McQueen. 2008. Shortlist
b: A bayesian model of continuous speech recognition.
Psychological Review, 115(2):357?395.
Dennis Norris. 1994. Shortlist: A connectionist model
of continuous speech recognition. Cognition, 52:189?
234.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
NAACL HLT 2007, pages 404?411, Rochester, New
York, April. Association for Computational Linguis-
tics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics (COLING/ACL?06).
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-coverage incremental
parsing using human-like memory constraints. Com-
putational Linguistics, 36(1):1?30.
William Schuler. 2009. Parsing with a bounded stack
using a model-based right-corner transform. In Pro-
ceedings of NAACL/HLT 2009, NAACL ?09, pages
344?352, Boulder, Colorado. Association for Compu-
tational Linguistics.
58
Michael K. Tanenhaus, Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. Inte-
gration of visual and linguistic information in spoken
language comprehension. Science, 268:1632?1634.
A Grammar Formulation
Given D memory elements indexed by d (see Sec-
tion 2.2) and a PCFG ?G, the probability ?(k)Ts,d of a
tree rooted at a left or right sibling s ? {L,R} of
category c? ? C requiring d ? 1..D memory ele-
ments is defined recursively over paths of increasing
length k:
P?(0)Ts,d
(1 | c?) def= 0 (25)
P?(k)TL,d
(1 | c?) def=
?
x?
P?G(c? ? x?)
+
?
c?0,c?1
P?G(c? ? c?0 c?1)
? P?(k?1)TL,d (1 | c?0) ? P?(k?1)TR,d(1 | c?1)
(26)
P?(k)TR,d
(1 | c?) def=
?
x?
P?G(c? ? x?)
+
?
c?0,c?1
P?G(c? ? c?0 c?1)
? P?(k?1)TL,d+1(1 | c?0) ? P?(k?1)TR,d(1 | c?1)
(27)
Note that the center embedding depth d increases
only for left children of right children. This is be-
cause in a binary branching structure, center embed-
dings manifest as zigzags. Since the model is also
sensitive to the depth d of each decomposition, the
side- and depth-specific probabilities of ?GL,d and
?GR,d are defined as follows:
P?GL,d(c? ? x?)
def=
P?G(c? ? x?)
P?(?)TL,d
(1 | c?)
(28)
P?GR,d(c? ? x?)
def=
P?G(c? ? x?)
P?(?)TR,d
(1 | c?)
(29)
P?GL,d(c? ? c?0 c?1)
def= P?G(c? ? c?0 c?1)
? P?(?)TL,d(1 | c?0) ? P?(?)TR,d(1 | c?1)
? P?(?)TL,d(1 | c?)
?1 (30)
P?GR,d(c? ? c?0 c?1)
def= P?G(c? ? c?0 c?1)
? P?(?)TL,d+1(1 | c?0) ? P?(?)TR,d(1 | c?1)
? P?(?)TR,d(1 | c?)
?1 (31)
The model will also need an expected count
E?G?,d(c?
?? c?? ...) of the given child constituent
c?? dominating a prefix of constituent c?. Expected
versions of these counts may later be used to derive
probabilities of memory store state transitions (see
Sections 2.3, 3).
E?G?,d(c?
0? c? ...) def=
?
x?
P?GR,d(c? ? x?)
(32)
E?G?,d(c?
1? c?0 ...) def=
?
c?1
P?GR,d(c? ? c?0 c?1)
(33)
E?G?,d(c?
k? c??0 ...) def=
?
c??
E?G?,d(c?
k?1? c?? ...)
?
?
c??1
P?GL,d(c?? ? c??0 c??1)
(34)
E?G?,d(c?
?? c?? ...) def=
?
?
k=0
E?G?,d(c?
k? c?? ...)
(35)
E?G?,d(c?
+? c?? ...) def=
?
?
k=1
E?G?,d(c?
k? c?? ...)
(36)
Equation 32 gives the probability of a constituent
appearing as an observation, and Equation 33 gives
the probability of a constituent appearing as a left
59
child. Equation 34 extends the previous two equa-
tions to account for a constituent appearing at an ar-
bitrarily deep embedded path of length k. Taking
the sum of all k path lengths (as in Equation 35)
allows the model to account for constituents any-
where in the left progeny of the dominated subtree.
Similarly, Equation 36 gives the expectation that the
constituent is non-immediately dominated by c?. In
practice the infinite sum is estimated to some con-
stant K using value iteration (Bellman, 1957).
60
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 37?46,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
An Analysis of Memory-based Processing Costs using Incremental
Deep Syntactic Dependency Parsing?
Marten van Schijndel
The Ohio State University
vanschm@ling.osu.edu
Luan Nguyen
University of Minnesota
lnguyen@cs.umn.edu
William Schuler
The Ohio State University
schuler@ling.osu.edu
Abstract
Reading experiments using naturalistic
stimuli have shown unanticipated facili-
tations for completing center embeddings
when frequency effects are factored out.
To eliminate possible confounds due to
surface structure, this paper introduces a
processing model based on deep syntac-
tic dependencies. Results on eye-tracking
data indicate that completing deep syntac-
tic embeddings yields significantly more
facilitation than completing surface em-
beddings.
1 Introduction
Self-paced reading and eye-tracking experiments
have often been used to support theories about
inhibitory effects of working memory operations
in sentence processing (Just and Carpenter, 1992;
Gibson, 2000; Lewis and Vasishth, 2005), but it
is possible that many of these effects can be ex-
plained by frequency (Jurafsky, 1996; Hale, 2001;
Karlsson, 2007). Experiments on large naturalis-
tic text corpora (Demberg and Keller, 2008; Wu et
al., 2010; van Schijndel and Schuler, 2013) have
shown significant memory effects at the ends of
center embeddings when frequency measures have
been included as separate factors, but these mem-
ory effects have been facilitatory rather than in-
hibitory.
Some of the memory-based measures that pro-
duce these facilitatory effects (Wu et al, 2010; van
Schijndel and Schuler, 2013) are defined in terms
of initiation and integration of connected compo-
nents of syntactic structure,1 with the presumption
?*Thanks to Micha Elsner and three anonymous review-
ers for their feedback. This work was funded by an Ohio State
University Department of Linguistics Targeted Investment
for Excellence (TIE) grant for collaborative interdisciplinary
projects conducted during the academic year 2012?13.
1Graph theoretically, the set of connected components
that referents that belong to the same connected
component may cue one another using content-
based features, while those that do not must rely
on noisier temporal features that just encode how
recently a referent was accessed. These measures,
based on left-corner parsing processes (Johnson-
Laird, 1983; Abney and Johnson, 1991), abstract
counts of unsatisfied dependencies from noun or
verb referents (Gibson, 2000) to cover all syntactic
dependencies, motivated by observations of Dem-
berg and Keller (2008) and Kwon et al (2010) of
the inadequacies of Gibson?s narrower measure.
But these experiments use naturalistic stimuli
without constrained manipulations and therefore
might be susceptible to confounds. It is possible
that the purely phrase-structure-based connected
components used previously may ignore some in-
tegration costs associated with filler-gap construc-
tions, making them an unsuitable generalization of
Gibson-style dependencies. It is also possible that
the facilitatory effect for integration operations in
naturally-occurring stimuli may be driven by syn-
tactic center embeddings that arise from modifiers
(e.g. The CEO sold [[the shares] of the com-
pany]), which do not require any dependencies
to be deferred, but which might be systematically
under-predicted by frequency measures, produc-
ing a confound with memory measures when fre-
quency measures are residualized out.
In order to eliminate possible confounds due to
exclusion of unbounded dependencies in filler-gap
constructions, this paper evaluates a processing
model that calculates connected components on
deep syntactic dependency structures rather than
surface phrase structure trees. This model ac-
counts unattached fillers and gaps as belonging
to separate connected components, and therefore
performs additional initiation and integration op-
of a graph ?V, E? is the set of maximal subsets of
it {?V1, E1?, ?V2, E2?, ...} such that any pair of vertices in
each Vi can be connected by edges in the corresponding Ei.
37
a) Noun Phrase
Relative Clause
Sentence w. Gap
Verb Phrase w. Gap
Sentence w. Gap
millionsstole
say
officials
who
Noun Phrase
personthe
b)
i1
i2 i3
i4
i5
i6
i7
1
2
1
2
1
1
say
officials
stole
who
person
millionsthe
0
0
0
0
0
00
Figure 1: Graphical representation of (a) a single
connected component of surface syntactic phrase
structure corresponding to (b) two connected com-
ponents of deep syntactic dependency structure for
the noun phrase the person who officials say stole
millions, prior to the word say. Connections es-
tablished prior to the word say are shown in black;
subsequent connections are shown in gray.
erations in filler-gap constructions as hypothesized
by Gibson (2000) and others. Then, in order to
control for possible confounds due to modifier-
induced center embedding, this refined model is
applied to two partitions of an eye-tracking cor-
pus (Kennedy et al, 2003): one consisting of sen-
tences containing only non-modifier center em-
beddings, in which dependencies are deferred, and
the other consisting of sentences containing no
center embeddings or containing center embed-
dings arising from attachment of final modifiers,
in which no dependencies are deferred. Processing
this partitioned corpus with deep syntactic con-
nected components reveals a significant increase
in facilitation in the non-modifier partition, which
lends credibility to the observation of negative
integration cost in processing naturally-occurring
sentences.
2 Connected Components
The experiments described in this paper evalu-
ate whether inhibition and facilitation in reading
correlate with operations in a hierarchic sequen-
tial prediction model that initiate and integrate
connected components of hypothesized syntactic
structure during incremental parsing. The model
used in these experiments refines previous con-
nected component models by allowing fillers and
gaps to occur in separate connected components
of a deep syntactic dependency graph (Mel?c?uk,
1988; Kintsch, 1988), even when they belong to
the same connected component when defined on
surface structure.
For example, the surface syntactic phrase struc-
ture and deep syntactic dependency structure for
the noun phrase the person who officials say stole
millions are shown in Figure 1.2 Notice that af-
ter the word officials, there is only one connected
component of surface syntactic phrase structure
(from the root noun phrase to the verb phrase with
gap), but two disjoint connected components of
deep syntactic dependency structure (one ending
at i3, and another at i5). Only the deep syntactic
dependency structure corresponds to familiar (Just
and Carpenter, 1992; Gibson, 1998) notions of
how memory is used to store deferred dependen-
cies in filler-gap constructions. The next section
will describe a generalized categorial grammar,
which (i) can be viewed as context-free, to seed a
latent-variable probabilistic context-free grammar
to accurately derive parses of filler-gap construc-
tions, and (ii) can be viewed as a deep syntactic
dependency grammar, defining dependencies for
connected components in terms of function appli-
cations.
3 Generalized Categorial Grammar
In order to evaluate memory effects for hypothe-
sizing unbounded dependencies between referents
of fillers and referents of clauses containing gaps,
a memory-based processor must define connected
components in terms of deep syntactic dependen-
cies (including unbounded dependencies) rather
than in terms of surface syntactic phrase structure
trees. To do this, at least some phrase structure
edges must be removed from the set of connec-
tions that define a connected component.
Because these unbounded dependencies are not
represented locally in the original Treebank for-
mat, probabilities for operations on these modified
2Following Mel?c?uk (1988) and Kintsch (1988),
the graphical dependency structure adopted here uses
positionally-defined labels (?0? for the predicate label, ?1?
for the first argument ahead of a predicate, ?2? for the last
argument behind, etc.) but includes unbounded dependen-
cies between referents of fillers and referents of clauses
containing gaps. It is assumed that semantically-labeled
structures would be isomorphic to the structures defined
here, but would generalize across alternations such as active
and passive constructions, for example.
38
connected components are trained on a corpus an-
notated with generalized categorial grammar de-
pendencies for ?gap? arguments at all categories
that subsume a gap (Nguyen et al, 2012). This
representation is similar to the HPSG-like repre-
sentation used by Hale (2001) and Lewis and Va-
sishth (2005), but has a naturally-defined depen-
dency structure on which to calculate connected
components. This generalized categorial grammar
is then used to identify the first sign that introduces
a gap, at which point a deep syntactic connected
component containing the filler can be encoded
(stored), and a separate deep syntactic connected
component for a clause containing a gap can be
initiated.
A generalized categorial grammar (Bach, 1981)
consists of a set U of primitive category types;
a set O of type-constructing operators allowing a
recursive definition of a set of categories C =def
U ? (C ? O ? C); a set X of vocabulary items;
a mapping M from vocabulary items in X to se-
mantic functions with category types in C; and
a set R of inference rules for deriving functions
with category types inC from other functions with
category types in C. Nguyen et al (2012) use
primitive category types for clause types (e.g. V
for finite verb-headed clause, N for noun phrase
or nominal clause, D for determiners and pos-
sessive clauses, etc.), and use the generalized set
of type-constructing operators to characterize not
only function application dependencies between
arguments immediately ahead of and behind a
functor (-a and -b, corresponding to ?\? and ?/? in
Ajdukiewicz-Bar-Hillel categorial grammars), but
also long-distance dependencies between fillers
and categories subsuming gaps (-g), dependencies
between relative pronouns and antecedent modif-
icands of relative clauses (-r), and dependencies
between interrogative pronouns and their argu-
ments (-i), which remain unsatisfied in derivations
but function to distinguish categories for content
and polar questions. A lexicon can then be de-
fined in M to introduce lexical dependencies and
obligatory pronominal dependencies using num-
bered functions for predicates and deep syntactic
arguments, for example:
the ? (?i (0 i)=the) : D
person ? (?i (0 i)=person) : N-aD
who ? (?k i (0 i)=who ? (1 i)=k) : N-rN
officials ? (?i (0 i)=officials) : N
the
D
person
N-aD
N Aa
who
N-rN
officials
N
say
V-aN-bV
stole
V-aN-bN
millions
N
V-aN Ae
V-gN Ga
V-aN-gN
Ag
V-gN Ac
V-rN Fc
N R
Figure 2: Example categorization of the noun
phrase the person who officials say stole millions.
say ? (?i (0 i)=say) : V-aN-bV
stole ? (?i (0 i)=stole) : V-aN-bN
millions ? (?i (0 i)=millions) : N
Inference rules in R are then defined to com-
pose arguments and modifiers and propagate gaps.
Arguments g of type d ahead of functors h of
type c-ad are composed by passing non-local de-
pendencies ? ? {-g, -i, -r} ? C from premises to
conclusion in all combinations:
g:d h: c-ad ? ( fc-ad g h): c (Aa)
g:d? h: c-ad ? ?k ( fc-ad (g k) h): c? (Ab)
g:d h: c-ad? ? ?k ( fc-ad g (h k)): c? (Ac)
g:d? h: c-ad? ? ?k ( fc-ad (g k) (h k)): c? (Ad)
Similar rules compose arguments behind functors:
g: c-bd h:d ? ( fc-bd g h): c (Ae)
g: c-bd? h:d ? ?k ( fc-bd (g k) h): c? (Af)
g: c-bd h:d? ? ?k ( fc-bd g (h k)): c? (Ag)
g: c-bd? h:d? ? ?k ( fc-bd (g k) (h k)): c? (Ah)
These rules use composition functions fc-ad
and fc-bd for initial and final arguments, which de-
fine dependency edges numbered v from referents
of predicate functors i to referents of arguments j,
where v is the number of unsatisfied arguments
?1...?v ? {-a, -b} ?C in a category label:
fu?1..v?1-ac
def= ?g h i ? j (v i)= j ? (g j) ? (h i) (1a)
fu?1..v?1-bc
def= ?g h i ? j (v i)= j ? (g i) ? (h j) (1b)
R also contains inference rules to compose mod-
ifier functors g of type u-ad ahead of modifi-
cands h of type d:
g: u-ad h:c ? ( fIM g h):c (Ma)
g: u-ad? h:c ? ?k ( fIM (g k) h):c? (Mb)
g: u-ad h:c? ? ?k ( fIM g (h k)):c? (Mc)
39
?i1 j1.. i? j? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. i? ... ? ((g? f ):c i?)
xt ? f :d (?Fa)
?i1 j1.. i? j? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. i? j?i?+1 ... ? (g?:c/d { j?} i?) ? ( f :e i?+1)
xt ? f :e (+Fa)
?i1 j1.. i??1 j??1i? ... ? (g?:d i?)
?i1 j1.. i? j? ... ? (( f g?):c/e { j?} i?)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
g:d h:e ? ( f g h):c or
g:d h:e ? ?k( f (g k) h):c or
g:d h:e ? ?k( f g (h k)):c or
g:d h:e ? ?k( f (g k) (h k)):c
(?La)
?i1 j1.. i??1 j??1i? ... ? (g??1:a/c { j??1} i??1) ? (g?:d i?)
?i1 j1.. i??1 j??1 ... ? (g??1 ? ( f g?):a/e { j??1} i??1)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
g:d h:e ? ( f g h):c or
g:d h:e ? ?k( f (g k) h):c or
g:d h:e ? ?k( f g (h k)):c or
g:d h:e ? ?k( f (g k) (h k)):c
(+La)
Figure 3: Basic processing productions of a right-corner parser.
g: u-ad? h:c? ? ?k ( fIM (g k) (h k)):c? (Md)
or for modifier functors behind a modificand:
g:c h: u-ad ? ( fFM g h):c (Me)
g:c? h: u-ad ? ?k ( fFM (g k) h):c? (Mf)
g:c h: u-ad? ? ?k ( fFM g (h k)):c? (Mg)
g:c? h: u-ad? ? ?k ( fFM (g k) (h k)):c? (Mh)
These rules use composition functions fIM and fFM
for initial and final modifiers, which define depen-
dency edges numbered ?1? from referents of mod-
ifier functors i to referents of modificands j:
fIM
def= ?g h j ?i (1 i)= j ? (g i) ? (h j) (2a)
fFM
def= ?g h j ?i (1 i)= j ? (g j) ? (h i) (2b)
R also contains inference rules for hypothesiz-
ing gaps -gd for arguments and modifiers:3
g: c-ad ? ?k ( fc-ad {k} g): c-gd (Ga)
g: c-bd ? ?k ( fc-ad {k} g): c-gd (Gb)
g:c ? ?k ( fIM {k} g):c-gd (Gc)
and for attaching fillers e, d-re, d-ie as gaps -gd:
g:e h: c-gd ? ?i ? j (g i) ? (h i j):e (Fa)
g:d-re h: c-gd ? ?k j ?i (g k i) ? (h i j): c-re (Fb)
g:d-ie h: c-gd ? ?k j ?i (g k i) ? (h i j): c-ie (Fc)
3Since these unary inferences perform no explicit compo-
sition, they are defined to use only initial versions composi-
tion functions fc-ad and fIM.
and for attaching modificands as antecedents of
relative pronouns:
g:e h:c-rd ? ?i ? j (g i) ? (h i j):e (R)
An example derivation of the noun phrase the per-
son who officials say stole millions using these
rules is shown in Figure 2. The semantic expres-
sion produced by this derivation consists of a con-
junction of terms defining the edges in the graph
shown in Figure 1b.
This GCG formulation captures many of the in-
sights of the HPSG-like context-free filler-gap no-
tation used by Hale (2001) or Lewis and Vasishth
(2005): inference rules with adjacent premises can
be cast as context-free grammars and weighted us-
ing probabilities, which allow experiments to cal-
culate frequency measures for syntactic construc-
tions. Applying a latent variable PCFG trainer
(Petrov et al, 2006) to this formulation was shown
to yield state-of-the-art accuracy for recovery of
unbounded dependencies (Nguyen et al, 2012).
Moreover, the functor-argument dependencies in
a GCG define deep syntactic dependency graphs
for all derivations, which can be used in incremen-
tal parsing to calculate connected components for
memory-based measures.
4 Incremental Processing
In order to obtain measures of memory opera-
tions used in incremental processing, these GCG
inference rules are combined into a set of parser
40
?i1 j1.. in jn.. i? j? ... ? (gn:y/z? { jn} in) ? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. in jn.. i? ... ? (gn:y/z? { jn} in) ? ... ? ((g?( f ?{ jn} f )):c i?)
xt ? ?k( f ?{k} f ):d
(?Fb)
?i1 j1.. in jn.. i? j? ... ? (gn:y/z? { jn} in) ? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. in jn.. i? j?i?+1 ... ? (gn:y/z? { jn} in) ? ... ? (g?:c/d { j?} i?) ? (( f ?{ jn} f ):e i?+1)
xt ? ?k( f ?{k} f ):e
(+Fb)
?i1 j1.. in jn.. i??1 j??1i? ... ? (gn:y/z? { jn} in) ? ... ? (g?:d i?)
?i1 j1.. in jn.. i? j? ... ? (gn:y/z? { jn} in) ? ... ? (( f g?) ? ( f ?{ jn}):c?/e { j?} i?)
g:d h:e ? ?k( f g ( f ?{k} h)):c? (?Lb)
?i1 j1.. in jn.. i??1 j??1i? ... ? (gn:y/z? { jn} in) ? ... ? (g??1:a/c? { j??1} i??1) ? (g?:d i?)
?i1 j1.. in jn.. i??1 j??1 ... ? (gn:y/z? { jn} in) ? ... ? (g??1 ? ( f g?) ? ( f ?{ jn}):a/e { j??1} i??1)
g:d h:e ? ?k( f g ( f ?{k} h)):c? (+Lb)
Figure 4: Additional processing productions for attaching a referent of a filler jn as the referent of a gap.
productions, similar to those of the ?right corner?
parser of van Schijndel and Schuler (2013), ex-
cept that instead of recognizing shallow hierarchi-
cal sequences of connected components of surface
structure, the parser recognizes shallow hierarchi-
cal sequences of connected components of deep
syntactic dependencies. This parser exploits the
observation (van Schijndel et al, in press) that left-
corner parsers and their variants do not need to ini-
tiate or integrate more than one connected compo-
nent at each word. These two operations are then
augmented with rules to introduce fillers and at-
tach fillers as gaps.
This parser is defined on incomplete connected
component states which consist of an active sign
(with a semantic referent and syntactic form or
category) lacking an awaited sign (also with a ref-
erent and category) yet to come. Semantic func-
tions of active and awaited signs are simplified to
denote only sets of referents, with gap arguments
(?k) stripped off and handled by separate con-
nected components. Incomplete connected com-
ponents, therefore, always denote semantic func-
tions from sets of referents to sets of referents.
This paper will notate semantic functions of
connected components using variables g and h, in-
complete connected component categories as c/d
(consisting of an active sign of category c and an
awaited sign of category d), and associations be-
tween them as g:c/d. The semantic representa-
tion used here is simply a deep syntactic depen-
dency structure, so a connected component func-
tion is satisfied if it holds for some output ref-
erent i given input referent j. This can be no-
tated ?i j (g:c/d { j} i), where the set { j} is equiva-
lent to (? j? j?= j). Connected component functions
that have a common referent j can then be com-
posed into larger connected components:4
?i jk (g { j} i) ? (h {k} j) ? ?i j (g?h {k} i) (3)
Hierarchies of ? connected compo-
nents can be represented as conjunctions:
?i1 j1... i? j? (g1:c1/d1 { j1} i1) ? ... ? (g?:c?/d? { j?} i?).
This allows constraints such as unbounded depen-
dencies between referents of fillers and referents
of clauses containing gaps to be specified across
connected components by simply plugging vari-
ables for filler referents into argument positions
for gaps.
A nondeterministic incremental parser can now
be defined as a deductive system, given an input
sequence consisting of an initial connected com-
ponent state of category T/T, corresponding to an
existing discourse context, followed by a sequence
of observations x1, x2, . . . , processed in time order.
As each xt is encountered, it is connected to an ex-
isting connected component or it introduces a new
disjoint component using the productions shown
in Figures 3, 4, and 5.
4These are connected components of dependency struc-
ture resulting from one or more composition functions being
composed, with each function?s output as the previous func-
tion?s second argument. This uses a standard definition of
function composition: (( f ? g) x) = ( f (g x)).
41
?i1 j1.. i??1 j??1i? ... ? (g?:d i?)
?i1 j1.. i? j? ... ? (( f g?) ? (?h k i (h k)):a/e? { j?} i?)
g:d h:e? ? ( f g h):c (?Lc)
?i1 j1.. i??1 j??1i? ... ? (g??1:a/c { j??1} i??1) ? (g?:d i?)
?i1 j1.. i??1 j??1 ... ? (g??1 ? ( f g?) ? (?h k i (h k)):a/e? { j??1} i??1)
g:d h:e? ? ( f g h):c (+Lc)
?i1 j1.. i? j? ... ? (g??1:c/d? { j??1} i??1) ? (g?:d?/e { j?} i?)
?i1 j1.. i??1 j??1 ... ? (g??1 ? (?h i? j(h j)) ? g?:c/e { j??1} i??1)
(+N)
Figure 5: Additional processing productions for hypothesizing filler-gap attachment.
Operations on dependencies that can be derived
from surface structure (see Figure 3) are taken
directly from van Schijndel and Schuler (2013).
First, if an observation xt can immediately fill
the awaited sign of the last connected component
g?:c/d, it is hypothesized to do so, turning this
incomplete connected component into a complete
connected component (g? f ):c (Production ?Fa); or
if the observation can serve as an initial sub-sign
of this awaited sign, it is hypothesized to form a
new complete sign f :e in a new component with xt
as its first observation (Production +Fa). Then,
if either of these resulting complete signs g?:d
can immediately attach as an initial child of the
awaited sign of the most recent connected com-
ponent g??1:a/c, it is hypothesized to merge and
extend this connected component, with xt as the
last observation of the completed connected com-
ponent (Production +La); or if it can serve as an
initial sub-sign of this awaited sign, it is hypoth-
esized to remain disjoint and form its own con-
nected component (Production ?La). The side
conditions of La productions are defined to unpack
gap propagation (instances of ?k that distinguish
rules Aa?h and Ma?h) from the inference rules
in Section 3, because this functionality will be re-
placed with direct substitution of referent variables
into subordinate semantic functions, below.
The Nguyen et al (2012) GCG was defined
to pass up unbounded dependencies, but in in-
cremental deep syntactic dependency processing,
unbounded dependencies are accounted as sepa-
rate connected components. When hypothesizing
an unbounded dependency, the processing model
simply cues the active sign of a previous connected
component containing a filler without completing
the current connected component. The four +F,
?F, +L, and ?L operations are therefore combined
with applications of unary rules Ga?c for hypoth-
esizing referents as fillers for gaps (providing f ?
in the equations in Figure 4). Productions ?Fb
and +Fb fill gaps in initial children, and Produc-
tions ?Lb and +Lb fill gaps in final children. Note
that the Fb and Lb productions apply to the same
types of antecedents as Fa and La productions re-
spectively, so members of these two sets of pro-
ductions cannot be applied together.
Applications of rules Fa?c and R for introduc-
ing fillers are applied to store fillers as existentially
quantified variable values in Lc productions (see
Figure 5). These Lc productions apply to the same
type of antecedent as La and Lb productions, so
these also cannot be applied together.
Finally, connected components separated by
gaps which are no longer hypothesized (?) are
reattached by a +N production. This +N pro-
duction may then be paired with a ?N production
which yields its antecedent unchanged as a conse-
quent. These N productions apply to antecedents
and consequents of the same type, so they may be
applied together with one F and one L production,
but since the +N production removes in its conse-
quent a ? argument required in its antecedent, it
may not apply more than once in succession (and
applying the ?N production more than once in suc-
cession has no effect).
An incremental derivation of the noun phrase
the person who officials say stole millions, using
these productions, is shown in Figure 6.
5 Evaluation
The F, L, and N productions defined in the pre-
vious section can be made probabilistic by first
computing a probabilistic context-free grammar
(PCFG) from a tree-annotated corpus, then trans-
forming that PCFG model into a model of prob-
abilities over incremental parsing operations us-
ing a grammar transform (Schuler, 2009). This
allows the intermediate PCFG to be optimized us-
ing an existing PCFG-based latent variable trainer
42
?i0 (.. :T/T {i0} i0) the
?i0 i2 (.. :T/T {i0} i0) ? (.. :N/N-aD {i2} i2)
+Fa,?La,?N
person
?i0 i2 (.. :T/T {i0} i0) ? (.. :N/V-rN {i2} i2)
?Fa,?La,?N
who
?i0 i2 i3 (.. :T/T {i0} i0) ? (.. :N/V-gN {i3} i2)
+Fa,+Lc,?N
officials
?i0 i2 i3 i5 (.. :T/T {i0} i0) ? (.. :N/V-gN {i3} i2) ? (.. :V-gN/V-aN-gN {i5} i5)
+Fa,?La,?N
say
?i0 i2 i6 (.. :T/T {i0} i0) ? (.. :N/V-aN {i6} i2)
+Fb,+La,+N
stole
?i0 i2 i7 (.. :T/T {i0} i0) ? (.. :N/N {i7} i2)
+Fa,+La,?N
millions
?i0 (.. :T/T {i0} i0)
?Fa,+La,?N
Figure 6: Derivation of the person who officials say stole millions, showing connected components with
unique referent variables (calculated according to the equations in Section 4). Semantic functions are
abbreviated to ?..? for readability. This derivation yields the following lexical relations: (0 i1)=the,
(0 i2)=person, (0 i3)=who, (0 i4)=officials, (0 i5)=say, (0 i6)=stole, (0 i7)=millions, and the following
argument relations: (1 i2)=i1, (1 i3)=i2, (1 i5)=i4, (2 i5)=i6, (1 i6)=i3, (2 i6)=i7.
(Petrov et al, 2006). When applied to the output
of this trainer, this transform has been shown to
produce comparable accuracy to that of the origi-
nal Petrov et al (2006) CKY parser (van Schijn-
del et al, 2012). The transform used in these ex-
periments diverges from that of Schuler (2009), in
that the probability associated with introducing a
gap in a filler-gap construction is reallocated from
a ?F?L operation to a +F?L operation (to encode
the previously most subordinate connected com-
ponent with the filler as its awaited sign and be-
gin a new disjoint connected component), and the
probability associated with resolving such a gap is
reallocated from an implicit ?N operation to a +N
operation (to integrate the connected component
containing the gap with that containing the filler).
In order to verify that the modifications to the
transform correctly reallocate probability mass for
gap operations, the goodness of fit to reading
times of a model using this modified transform
is compared against the publicly-available base-
line model from van Schijndel and Schuler (2013),
which uses the original Schuler (2009) transform.5
To ensure a valid comparison, both parsers are
trained on a GCG-reannotated version of the Wall
Street Journal portion of the Penn Treebank (Mar-
cus et al, 1993) before being fit to reading times
using linear mixed-effects models (Baayen et al,
2008).6 This evaluation focuses on the process-
ing that can be done up to a given point in a sen-
tence. In human subjects, this processing includes
both immediate lexical access and regressions that
5The models used here also use random slopes to reduce
their variance, which makes them less anticonservative.
6The models are built using lmer from the lme4R package
(Bates et al, 2011; R Development Core Team, 2010).
aid in the integration of new information, so the
reading times of interest in this evaluation are log-
transformed go-past durations.7
The first and last word of each line in the
Dundee corpus, words not observed at least 5
times in the WSJ training corpus, and fixations af-
ter long saccades (>4 words) are omitted from the
evaluation to filter out wrap-up effects, parser in-
accuracies, and inattention and track loss of the
eyetracker. The following predictors are centered
and used in each baseline model: sentence posi-
tion, word length, whether or not the previous or
next word were fixated upon, and unigram and bi-
gram probabilities.8 Then each of the following
predictors is residualized off each baseline before
being centered and added to it to help residualize
the next factor: length of the go-past region, cumu-
lative total surprisal, total surprisal (Hale, 2001),
and cumulative entropy reduction (Hale, 2003).9
All 2-way interactions between these effects are
7Go-past durations are calculated by summing all fixa-
tions in a region of text, including regressions, until a new
region is fixated, which accounts for additional processing
that may take place after initial lexical access, but before the
next region is processed. For example, if one region ends at
word 5 in a sentence, and the next fixation lands on word 8,
then the go-past region consists of words 6-8 while go-past
duration sums all fixations until a fixation occurs after word
8. Log-transforming eye movements and fixations may make
their distributions more normal (Stephen and Mirman, 2010)
and does not substantially affect the results of this paper.
8For the n-gram model, this study uses the Brown corpus
(Francis and Kucera, 1979), the WSJ Sections 02-21 (Mar-
cus et al, 1993), the written portion of the British National
Corpus (BNC Consortium, 2007), and the Dundee corpus
(Kennedy et al, 2003) smoothed with modified Kneser-Ney
(Chen and Goodman, 1998) in SRILM (Stolcke, 2002).
9Non-cumulative metrics are calculated from the final
word of the go-past region; cumulative metrics are summed
over the go-past region.
43
included as predictors along with the predictors
from the previous go-past region (to account for
spillover effects). Finally, each model has sub-
ject and item random intercepts added in addition
to by-subject random slopes (cumulative total sur-
prisal, whether the previous word was fixated, and
length of the go-past region) and is fit to centered
log-transformed go-past durations.10
The Akaike Information Criterion (AIC)
indicates that the gap-reallocating model
(AIC = 128,605) provides a better fit to reading
times than the original model (AIC = 128,619).11
As described in Section 1, previous findings of
negative integration cost may be due to a confound
whereby center-embedded constructions caused
by modifiers, which do not require deep syntac-
tic dependencies to be deferred, may be driving
the effect. Under this hypothesis, embeddings
that do not arise from final adjunction of mod-
ifiers (henceforth canonical embeddings) should
yield a positive integration cost as found by Gib-
son (2000).
To investigate this potential confound, the
Dundee corpus is partitioned into two parts. First,
the model described in this paper is used to anno-
tate the Dundee corpus. From this annotated cor-
pus, all sentences are collected that contain canon-
ical embeddings and lack modifier-induced em-
beddings.12 This produces two corpora: one con-
sisting entirely of canonical center-embeddings
such as those used in self-paced reading exper-
iments with findings of positive integration cost
(e.g. Gibson 2000), the other consisting of the
remainder of the Dundee corpus, which contains
sentences with canonical embeddings but also in-
cludes modifier-caused embeddings.
The coefficient estimates for integration oper-
ations (?F+L and +N) on each of these corpora
are then calculated using the baseline described
above. To ensure embeddings are driving any ob-
served effect rather than sentence wrap-up effects,
the first and last words of each sentence are ex-
cluded from both data sets. Integration cost is
measured by the amount of probability mass the
parser allocates to ?F+L and +N operations, accu-
10Each fixed effect that has an absolute t-value greater than
10 when included in a random-intercepts only model is added
as a random slope by-subject.
11The relative likelihood of the original model to the gap-
sensitive model is 0.0009 (n = 151,331), which suggests the
improvement is significant.
12Modifier-induced embeddings are found by looking for
embeddings that arise from inference rules Ma-h in Section 3.
Model coeff std err t-score
Canonical -0.040 0.010 -4.05
Other -0.017 0.004 -4.20
Table 1: Fixed effect estimates for integration cost
when used to fit reading times over two partitions
of the Dundee corpus: one containing only canon-
ical center embeddings and the other composed of
the rest of the sentences in the corpus.
mulated over each go-past region, and this cost is
added as a fixed effect and as a random slope by
subject to the mixed model described earlier.13
The fixed effect estimate for cumulative inte-
gration cost from fitting each corpus is shown
in Table 1. Application of Welch?s t-test shows
that the difference between the estimated distri-
butions of these two parameters is highly signif-
icant (p < 0.0001).14 The strong negative corre-
lation of integration cost to reading times in the
purely canonical corpus suggests canonical (non-
modifier) integrations contribute to the finding of
negative integration cost.
6 Conclusion
This paper has introduced an incremental parser
capable of using GCG dependencies to distinguish
between surface syntactic embeddings and deep
syntactic embeddings. This parser was shown to
obtain a better fit to reading times than a surface-
syntactic parser and was used to parse the Dundee
eye-tracking corpus in two partitions: one consist-
ing of canonical embeddings that require deferred
dependencies and the other consisting of sentences
containing no center embeddings or center em-
beddings arising from the attachment of clause-
final modifiers, in which no dependencies are de-
ferred. Using linear mixed effects models, com-
pletion (integration) of canonical center embed-
dings was found to be significantly more nega-
tively correlated with reading times than comple-
tion of non-canonical embeddings. These results
suggest that the negative integration cost observed
in eye-tracking studies is at least partially due to
deep syntactic dependencies and not due to con-
founds related to surface forms.
13Integration cost is residualized off the baseline before be-
ing centered and added as a fixed effect.
14Integration cost is significant as a fixed effect (p = 0.001)
in both partitions: canonical (n = 16,174 durations) and
non-canonical (n = 131,297 durations).
44
References
Steven P. Abney and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
R. Harald Baayen, D. J. Davidson, and Douglas M.
Bates. 2008. Mixed-effects modeling with crossed
random effects for subjects and items. Journal of
Memory and Language, 59:390?412.
Emmon Bach. 1981. Discontinuous constituents in
generalized categorial grammars. Proceedings of
the Annual Meeting of the Northeast Linguistic So-
ciety (NELS), 11:1?12.
Douglas Bates, Martin Maechler, and Ben Bolker,
2011. lme4: Linear mixed-effects models using S4
classes.
BNC Consortium. 2007. The british national corpus.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report, Harvard University.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
W. Nelson Francis and Henry Kucera. 1979. The
brown corpus: A standard corpus of present-day
edited american english.
Edward Gibson. 1998. Linguistic complexity: Local-
ity of syntactic dependencies. Cognition, 68(1):1?
76.
Edward Gibson. 2000. The dependency locality the-
ory: A distance-based theory of linguistic complex-
ity. In Image, language, brain: Papers from the first
mind articulation project symposium, pages 95?126,
Cambridge, MA. MIT Press.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the sec-
ond meeting of the North American chapter of the
Association for Computational Linguistics, pages
159?166, Pittsburgh, PA.
John Hale. 2003. Grammar, Uncertainty and Sentence
Processing. Ph.D. thesis, Cognitive Science, The
Johns Hopkins University.
Philip N. Johnson-Laird. 1983. Mental models: to-
wards a cognitive science of language, inference,
and consciousness. Harvard University Press, Cam-
bridge, MA, USA.
Daniel Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science: A Multidisciplinary Journal, 20(2):137?
194.
Marcel Adam Just and Patricia A. Carpenter. 1992. A
capacity theory of comprehension: Individual differ-
ences in working memory. Psychological Review,
99:122?149.
Fred Karlsson. 2007. Constraints on multiple center-
embedding of clauses. Journal of Linguistics,
43:365?392.
Alan Kennedy, James Pynte, and Robin Hill. 2003.
The Dundee corpus. In Proceedings of the 12th Eu-
ropean conference on eye movement.
Walter Kintsch. 1988. The role of knowledge in dis-
course comprehension: A construction-integration
model. Psychological review, 95(2):163?182.
Nayoung Kwon, Yoonhyoung Lee, Peter C. Gordon,
Robert Kluender, and Maria Polinsky. 2010. Cog-
nitive and linguistic factors affecting subject/object
asymmetry: An eye-tracking study of pre-nominal
relative clauses in korean. Language, 86(3):561.
Richard L. Lewis and Shravan Vasishth. 2005.
An activation-based model of sentence processing
as skilled memory retrieval. Cognitive Science,
29(3):375?419.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Igor Mel?c?uk. 1988. Dependency syntax: theory and
practice. State University of NY Press, Albany.
Luan Nguyen, Marten van Schijndel, and William
Schuler. 2012. Accurate unbounded dependency
recovery using generalized categorial grammars. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING ?12), Mumbai,
India.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
44th Annual Meeting of the Association for Compu-
tational Linguistics (COLING/ACL?06).
R Development Core Team, 2010. R: A Language and
Environment for Statistical Computing. R Foun-
dation for Statistical Computing, Vienna, Austria.
ISBN 3-900051-07-0.
William Schuler. 2009. Parsing with a bounded stack
using a model-based right-corner transform. In Pro-
ceedings of NAACL/HLT 2009, NAACL ?09, pages
344?352, Boulder, Colorado. Association for Com-
putational Linguistics.
Damian G. Stephen and Daniel Mirman. 2010. Inter-
actions dominate the dynamics of visual cognition.
Cognition, 115(1):154?165.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Seventh International Confer-
ence on Spoken Language Processing.
Marten van Schijndel and William Schuler. 2013. An
analysis of frequency- and recency-based processing
costs. In Proceedings of NAACL-HLT 2013. Associ-
ation for Computational Linguistics.
45
Marten van Schijndel, Andy Exley, and William
Schuler. 2012. Connectionist-inspired incremental
PCFG parsing. In Proceedings of CMCL 2012. As-
sociation for Computational Linguistics.
Marten van Schijndel, Andy Exley, and William
Schuler. in press. A model of language processing
as hierarchic sequential prediction. Topics in Cogni-
tive Science.
Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an
incremental right-corner parser. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL?10), pages 1189?1198.
46
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 19?27,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Sentence Processing in a Vectorial Model of Working Memory
William Schuler
Department of Linguistics
The Ohio State University
schuler@ling.osu.edu
Abstract
This paper presents a vectorial incremen-
tal parsing model defined using indepen-
dently posited operations over activation-
based working memory and weight-based
episodic memory. This model has the at-
tractive property that it hypothesizes only
one unary preterminal rule application and
only one binary branching rule applica-
tion per time step, which allows it to be
smoothly integrated into a vector-based
recurrence that propagates structural am-
biguity from one time step to the next.
Predictions of this model are calculated
on a center-embedded sentence processing
task and shown to exhibit decreased pro-
cessing accuracy in center-embedded con-
structions.
1 Introduction
Current models of memory (Marr, 1971; Ander-
son et al., 1977; Murdock, 1982; McClelland et
al., 1995; Howard and Kahana, 2002) involve a
continuous activation-based (or ?working?) mem-
ory, typically modeled as a vector representing the
current firing pattern of neurons or neural clus-
ters in the cortex. This activation-based memory
is then supported by a durable but rapidly mu-
table weight-based (or ?episodic?) memory, typi-
cally modeled as one or more matrices formed by
summed outer-products of cue and target vectors
and cued by simple matrix multiplication, repre-
senting variable synaptic connection strengths be-
tween neurons or neural clusters.
The lack of discrete memory units in such mod-
els makes it difficult to imagine a neural imple-
mentation of a typical e.g. chart-based computa-
tional account of sentence processing. On the
other hand, superposition in vectorial models sug-
gests a natural representation of a parallel incre-
mental processing model. This paper explores
how such an austere model of memory not only
might be used to encode a simple probabilistic in-
cremental parser, but also lends itself to naturally
implement a vectorial interpreter and coreference
resolver. This model is based on the left-corner
parser formulation of van Schijndel et al. (2013a),
which has the attractive property of generating ex-
actly one binary-branching rule application after
processing each word. This property greatly sim-
plifies a vectorial implementation because it al-
lows these single grammar rule applications to be
superposed in cases of attachment ambiguity.
Predictions of the vectorial model described in
this paper are then calculated on a simple center-
embedded sentence processing task, producing a
lower completion accuracy for center-embedded
sentences than for right-branching sentences with
the same number of words. As noted by Levy and
Gibson (2013), this kind of memory effect is not
easily explained by existing information-theoretic
models of frequency effects (Hale, 2001; Levy,
2008).
The model described in this paper also provides
an explanation for the apparent reality of linguistic
objects like categories, grammar rules, discourse
referents and dependency relations, as cognitive
states in activation-based memory (in the case of
categories and discourse referents), or cued asso-
ciations in weight-based memory (in the case of
grammar rules, and dependency relations), with-
out having to posit complex machinery specific
to language processing. In this sense, unlike ex-
isting chart-based parsers or connectionist models
based on recurrent neural networks, this model in-
tegrates familiar notions of grammar and seman-
tic relations with current ideas of activation-based
and weight-based memory. It is also anticipated
that this interface to both linguistic and neurosci-
entific theories will make the model useful as a
basis for more nuanced understanding of linguistic
phenomena such as ambiguity resolution, seman-
19
tic representation, and language acquisition.
2 Related Work
The model described in this paper is based on the
left-corner parser formulation of van Schijndel et
al. (2013a), which is an implementation of a fully
parallel incremental parser. This parser differs
from chart-based fully parallel incremental parsers
used by Hale (2001), Levy (2008) and others in
that it enforces a cognitively-motivated bound on
center-embedding depth. This bound allows the
parser to represent a tractable set of incremental
hypotheses in an explicitly enumerated list as a
factored hidden Markov model, without necessi-
tating the use of a parser chart. This model has the
attractive property that, in any context, it hypoth-
esizes exactly one binary-branching rule applica-
tion at each time step.
The model described in this paper extends the
van Schijndel et al. (2013a) parser by maintain-
ing possible store configurations as superposed
sequence states in a finite-dimensional state vec-
tor. The model then exploits the uniformity of
its parsing operations to integrate probabilistically
weighted grammar rule applications into this su-
perposed state vector. These superposed states
are then used to cue more superordinate sequen-
tial states as ?continuations? whenever subordinate
states conclude. Interference in this cueing pro-
cess is then observed to produce a natural center-
embedding limit.
This model is defined as a recurrence over an
activation vector, similar to the simple recurrent
network of Elman (1991) and others, but unlike an
SRN, which does not encode anything in weight-
based memory during processing, this model en-
codes updates to a processing hierarchy in weight-
based memory at every time step. The model is
also similar to the ACT-R parser of Lewis and Va-
sishth (2005) in that it maintains a single state
which is updated based on content-based cued
association, but unlike the ACT-R parser, which
cues category tokens on category types and there-
fore models memory limits as interference among
grammar rules, this model cues category tokens
on other category tokens, and therefore predicts
memory limits even in cases where grammar rules
do not involve similar category types. Also unlike
Lewis and Vasishth (2005), this model is defined
purely in terms of state vectors and outer-product
associative memory and therefore has the capacity
S
VP
NP
N
N
NN
mud
D
the
V
shook
NP
N
wind
D
the
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
S/N
}
N/N
Figure 1: Example incomplete category during
processing of the sentence The wind shook the
mud room door.
to maintain parallel states in superposition.
3 Background: Non-vectorial
Incremental Parsing
The model defined in this paper is based on the
left-corner parser formulation of van Schijndel et
al. (2013a). This parser maintains a set of incom-
plete categories a/b at each time step, each con-
sisting of an active category a lacking an awaited
category b yet to come. For example, Figure 1
shows an incomplete category S/N consisting of
a sentence lacking a common noun yet to come,
which non-immediately dominates another incom-
plete category N/N consisting of a common noun
lacking another common noun yet to come.
Processing in this model is defined to alternate
between two phases:
1. a ?fork? phase in which a word is either used
to complete an existing incomplete category,
or forked into a new complete category; and
2. a ?join? phase in which one of these complete
categories is used as a left child of a grammar
rule application and then either joined onto
a superordinate incomplete category or kept
disjoint.
In any case, only one grammar rule is applied af-
ter each word. These fork and join operations are
shown graphically and as natural deduction rules
in Figure 2.
An example derivation of the sentence, The
wind shook the mud room door, using the pro-
ductions in Figure 2 is shown in Figure 3, with
corresponding partial parse trees shown in Fig-
ure 4. Van Schijndel et al. (2013a) show that a
20
?F:
a
b
x
t
+F:
a
b
a
?
x
t
a/b x
t
a
b? x
t
(?F)
a/b x
t
a/b a
?
b
+
? a
?
... ; a
?
? x
t
(+F)
+J:
a
b
a
??
b
??
?J:
a
b
a
?
a
??
b
??
a/b a
??
a/b
??
b? a
??
b
??
(+J)
a/b a
??
a/b a
?
/b
??
b
+
? a
?
... ; a
?
? a
??
b
??
(?J)
Figure 2: Fork and join operations from the van
Schijndel et al. (2013a) left-corner parser formu-
lation. During the fork phase, word x either com-
pletes an existing incomplete category a, or forks
into a new complete category a
?
. During the join
phase, complete category a
??
becomes a left child
of a grammar rule application, then either joins
onto a superordinate incomplete category a/b or
remains disjoint.
probabilistic version of this incremental parser can
reproduce the results of a state-of-the-art chart-
based parser (Petrov and Klein, 2007).
4 Vectorial Parsing
This left corner parser can be implemented in a
vectorial model of working memory using vec-
tors as activation-based memory and matrices as
weight-based memory. Following Anderson et al.
(1977) and others, vectors v in activation-based
memory are cued from other vectors u through
weight-based memory matrices M using ordinary
T/T the
T/T, D
+F
T/T, NP/N
?J
wind
T/T, NP
?F
T/T, S/VP
?J
shook
T/T, S/VP, V
+F
T/T, S/NP
+J
the
T/T, S/NP, D
+F
T/T, S/N
+J
mud
T/T, S/N, N
+F
T/T, S/N, N/N
?J
room
T/T, S/N, N
?F
T/T, S/N
+J
door
T/T, S
?F
T/T
+J
Figure 3: Processing steps in parsing the sentence
The wind shook the mud room door.
matrix multiplication:
1
v = M u (1)
This representation has been used to model the in-
fluence of activation in antecedent neurons on ac-
tivation in consequent neurons (Marr, 1971; An-
derson et al., 1977).
Unless they are cued from some other source,
all vectors in this model are initially randomly
generated by sampling from an exponential distri-
bution, denoted here simply by:
v ? Exp (2)
Also following Anderson et al. (1977), weight-
based memory matrices M are themselves defined
and updated by simply adding outer products of
desired cue u and target v vectors:
2
M
t
= M
t?1
+ v ? u (3)
This representation has been used to model rapid
synaptic sensitization in the hippocampus (Marr,
1971; McClelland et al., 1995), in which synapses
of activated antecedent neurons that impinge on
activated consequent neurons are strengthened.
1
That is, multiplication of an associative memory ma-
trix M by a state vector v yields:
(M v)
[i]
def
=
?
J
j=1
M
[i, j]
? v
[ j]
(1
?
)
2
An outer product v ? u defines a matrix by multiplying
each combination of scalars in vectors v and u:
(v ? u)
[ j,i]
def
= v
[ j]
? u
[i]
(2
?
)
21
a) NP
ND
the
b) S
VPNP
N
wind
D
the
c) S
VP
NPV
shook
NP
N
wind
D
the
d) S
VP
NP
ND
the
V
shook
NP
N
wind
D
the
e) S
VP
NP
N
N
NN
mud
D
the
V
shook
NP
N
wind
D
the
f) S
VP
NP
N
NN
N
room
N
mud
D
the
V
shook
NP
N
wind
D
the
Figure 4: Processing steps in parsing the sentence The wind shook the mud room door.
Finally, cued associations can be combined us-
ing pointwise or diagonal products:
3
w = diag(u) v (4)
Unlike a symbolic statistical model, a vectorial
model must explicitly distinguish token represen-
tations from types in order to define structural rela-
tions that would be implicit in the positions of data
structure elements in a symbolic model. Thus, the
active or awaited distinction is applied to category
tokens rather than types, but grammar rule appli-
cations are defined over category types rather than
tokens.
The vectorial left-corner parser described in this
paper is therefore defined on a single category to-
ken vector b
t
which encodes the awaited category
token of the most subordinate incomplete category
at the current time step t. A hierarchy of nested in-
complete category tokens is then encoded in two
?continuation? matrices:
? A
t
, which cues the active category token a
of the same incomplete category as a given
awaited token b; and
3
A diagonal product diag(v) u defines a vector by multi-
plying corresponding scalars in vectors v and u:
(diag(v) u)
[i]
def
= v
[i]
? u
[i]
(3
?
)
? B
t
, which cues the awaited category to-
ken b of the incomplete category that non-
immediately dominates any active category
token a.
Together, the cued associations in these continua-
tion matrices trace a path up from the most sub-
ordinate awaited category token b to the most su-
perordinate category token currently hypothesized
as the root of the syntactic tree. Vectors for cate-
gory types c can then be cued from any category
token a or b through an associative matrix C
t
. All
three of these matrices may be updated from time
step to time step by associating cue and target vec-
tors through outer product addition, as described
above.
The model also defines vectors for binary-
branching grammar rules g, which are associ-
ated with parent, left child, or right child cat-
egory types via ?accessor? matrices G, G
?
, or
G
??
.
4
These accessor matrices are populated
from binary-branching rules in a probabilistic
context-free grammar (PCFG) in Chomsky Nor-
mal Form (CNF). For example, the PCFG rule
P(S ? NP VP) = 0.8 may be encoded using a
4
This use of reification and accessor matrices for gram-
mar rules emulates a tensor model (Smolensky, 1990; beim
Graben et al., 2008) in that in the worst case grammar rules
(composed of multiple categories) would require a space
polynomially larger than that of category types, but since this
space is sparsely inhabited in the expected case, this reified
representation is computationally more tractable.
22
grammar rule vector g
S? NP VP
and category vec-
tors c
S
, c
VP
, c
NP
with the following outer-product
associations:
G
def
= g
S? NP VP
? c
S
? 0.8
G
?
def
= g
S? NP VP
? c
NP
G
??
def
= g
S? NP VP
? c
VP
Grammars with additional rules can then be en-
coded as a sum of outer products of rule and cat-
egory vectors. Grammar rules can then be cued
from category types by matrix multiplication, e.g.:
g
S? NP VP
= G
?
c
NP
and category types can be cued from grammar
rules using transposed versions of accessor matri-
ces:
c
NP
= G
?>
g
S? NP VP
The model also defines:
? vectors x
t
for observation types (i.e. words),
? a matrix P cueing category types from obser-
vation types, populated from unary rules in a
CNF PCFG, and
? a matrix D = D
K
of leftmost descendant cate-
gories cued from ancestor categories, derived
from accessor matrices G and G
?
by K itera-
tions of the following recurrence:
5
D
?
0
def
= diag(1) (5)
D
0
def
= diag(0) (6)
D
?
k
def
= G
?>
G D
?
k?1
(7)
D
k
def
= D
k?1
+ D
?
k
(8)
where each D
?
k
cues a probabilistically-
weighted descendant at distance k from its
cue, and D
k
is the superposition of all such
descendant associations from length 1 to
length K. This produces a superposed set of
category types that may occur as leftmost de-
scendants of a (possibly superposed) ancestor
category type.
In order to exclude active category types C
t
a
t
that are not compatible with awaited category
types C
t
b
t
in the same incomplete category, the
model also defines:
5
Here 1 and 0 denote vectors of ones and zeros, respec-
tively.
? a matrix E = E
K
of rightmost descendant
categories cued from ancestor categories, de-
rived in the same manner as D, except us-
ing G
??
in place of G
?
.
The parser proceeds in two phases, generating a
complete category token vector a
??
t
from b
t?1
dur-
ing the F phase, then generating an awaited cat-
egory token vector b
t
of an incomplete category
during the J phase. Since the parser proceeds in
two phases, this paper will distinguish variables
updated in each phase using a subscript for time
step t? .5 at the end of the first phase and t at the
end of the second phase.
The vectorial parser implements the F phase of
the left-corner parser (the ?fork/no-fork? decision)
by first defining two new category tokens for the
possibly forked or unforked complete category:
a
t?.5
, a
?
t?.5
? Exp
The parser then obtains:
? the category type of the most subordinate
awaited category token at the previous time
step: C
t?1
b
t?1
(which involves no fork), and
? a superposed set of non-immediate descen-
dants of the category type of this most sub-
ordinate awaited category token: D C
t?1
b
t?1
(which involves a fork),
These fork and no-fork categories are then diag-
onally multiplied (intersected) with a superposed
set of preterminal categories for the current obser-
vation (P x
t
):
c
?
t
= diag(P x
t
) C
t?1
b
t?1
c
+
t
= diag(P x
t
) D C
t?1
b
t?1
The B and C continuation and category matrices
are then updated with a superordinate awaited cat-
egory token and category type for a and a
?
:
a
t?1
= A
t?1
b
t?1
B
t?.5
= B
t?1
+ b
t?1
? a
?
t?.5
+ B
t?1
a
t?1
? a
t?.5
C
t?.5
= C
t?1
+ c
+
t
? a
?
t?.5
+ diag(C
t?1
a
t?1
) E
>
c
?
t
? a
t?.5
where the updated category for a
t?.5
results from
an intersection (diagonal product) of the current
category at a
t?1
with the set of categories that can
occur with c
?
t
as a rightmost child, as defined by E.
The intersected fork and no-fork category types
are then used to weight superposed hypotheses for
23
?F:
a
t?1
(= a
??
t
)
b
t?1
x
t
+F:
a
t?1
b
t?1
a
?
t?.5
(= a
??
t
)
x
t
B
Figure 5: Updates to continuation matrices during
the ?fork? phase of a left-corner parser.
the complete category token a
??
t
that will result
from this phase of processing, and the b vector is
updated to encode the category token:
6
a
??
t
=
a
t?1
||c
?
t
|| + a
?
t?.5
||c
+
t
||
||a
t?1
||c
?
t
|| + a
?
t?.5
||c
+
t
||||
b
t?.5
= B
t?.5
a
??
t
These updates can be represented graphically as
shown in Figure 5.
The vectorial parser then similarly implements
the J phase (the ?join/no-join? decision) of the left-
corner parser by first defining a new category to-
ken a
?
for a possible new active category of the
most subordinate incomplete category, and b
??
for
a new awaited category token:
a
?
t
, b
??
t
? Exp
The parser then obtains:
? a superposed set of grammar rules with par-
ent category matching the category of the
most subordinate awaited category token at
the previous time step: G C
t?.5
b
t?.5
(which as-
sumes a join), and
? a superposed set of grammar rules with
parent category non-immediately descended
from the category of this most subordinate
awaited category token: G D C
t?.5
b
t?.5
(which
assumes no join)
These join and no-join grammar rule vectors
are then diagonally multiplied (intersected) with
6
This uses the two norm ||v||, which is the magnitude of
vector v, defined as the square root of the sum of the squares
of its scalar values:
||v||
def
=
?
?
i
(v
[i]
)
2
(4
?
)
Dividing a vector by its two norm has the effect of normaliz-
ing it to unit length.
+J:
a
t?.5
b
t?.5
a
??
t
b
??
t
A
?J:
a
t?.5
b
t?.5
a
?
t
a
??
t
b
??
t
B
A
Figure 6: Updates to continuation matrices during
the ?join? phase of a left-corner parser.
the superposed set of grammar rules whose left
child category type matches the category type
of the most subordinate complete category to-
ken (G
?
C
t?.5
a
??
t
):
g
+
t
= diag(G
?
C
t?.5
a
??
t
) G C
t?.5
b
t?.5
g
?
t
= diag(G
?
C
t?.5
a
??
t
) G D C
t?.5
b
t?.5
These intersected join and no-join grammar rule
vectors are then used to weight superposed hy-
potheses for the incomplete category that will re-
sult from this phase of processing in updates to the
continuation and category matrices A, B, and C:
A
t
= A
t?1
+
A
t?1
b
t?.5
||g
+
t
|| + a
?
t
||g
?
t
||
||A
t?1
b
t?.5
||g
+
t
|| + a
?
t
||g
?
t
||||
? b
??
t
B
t
= B
t?.5
+ b
t?.5
? a
?
t
C
t
= C
t?.5
+ G
>
g
?
t
? a
?
t
+
G
??>
g
+
t
+ G
??>
g
?
t
||G
??>
g
+
t
+ G
??>
g
?
t
||
? b
??
t
These updates can be represented graphically as
shown in Figure 6. Finally the the most subordi-
nate awaited category token is updated for the next
word:
b
t
= b
??
t
5 Predictions
In order to assess the cognitive plausibility of the
memory modeling assumptions in this vectorial
parser, predictions of the implementation defined
in Section 4 were calculated on center-embedding
and right-branching sentences, exemplified by:
(1) If either Kim stays or Kim leaves then Pat
leaves. (center-embedded condition)
(2) If Kim stays then if Kim leaves then Pat
leaves. (right-branching condition)
24
P(T? S T) = 1.0
P(S? NP VP) = 0.5
P(S? IF S THEN S) = 0.25
P(S? EITHER S OR S) = 0.25
P(IF? if) = 1.0
P(THEN? then) = 1.0
P(EITHER? either) = 1.0
P(OR? or) = 1.0
P(NP? kim) = 0.5
P(NP? pat) = 0.5
P(VP? leaves) = 0.5
P(VP? stays) = 0.5
Figure 7: ?If . . . then . . . ? grammar used in sen-
tence processing experiment. Branches with arity
greater than two are decomposed into equivalent
right-branching sequences of binary branches.
both of which contain the same number of words.
These sentences were processed using the gram-
mar shown in Figure 7, which assigns the same
probability to both center-embedding and right-
branching sentences. The if . . . then . . . and ei-
ther . . . or . . . constructions used in these ex-
amples are taken from the original Chomsky and
Miller (1963) paper introducing center-embedding
effects, and are interesting because they do not in-
volve the same grammar rule (as is the case with
familiar nested object relative constructions), and
do not involve filler-gap constructions, which may
introduce overhead processing costs as a possible
confound.
This assessment consisted of 500 trials for each
sentence type. Sentences were input to an imple-
mentation of this model using the Numpy package
in Python, which consists of the equations shown
in Section 4 enclosed in a loop over the words in
each sentence. Each trial initially sampled a, b,
c, and g vectors from random exponential distri-
butions of dimension 100, and the parser initial-
ized b
0
with category type T as shown in Figure 3,
with the active category token at A
0
b
0
also asso-
ciated with category type T.
Accuracy for this assessment was calculated by
finding the category type with the maximum co-
sine similarity for the awaited category b
T
at the
end of the sentence. If this category type was T
sentence correct incorrect
center-embedded 231 269
right-branching 297 203
Table 1: Accuracy of vectorial parser on each sen-
tence type.
(as it is in Figure 3), the parser was awarded a
point of accuracy; otherwise it was not. The re-
sults of this assessment are shown in Table 1. The
parser processes sentences with right-branching
structure substantially more accurately than sen-
tences with center-embedded structure. These re-
sults are strongly significant (p < .001) using a ?
2
test.
These predictions seem to be consistent with
observations by Chomsky and Miller (1963) that
center-embedded structures are more difficult to
parse than right-branching structures, but it is also
important to note how the model arrives at these
predictions. The decreased accuracy of center-
embedded sentences is not a result of an ex-
plicit decay factor, as in ACT-R and other models
(Lewis and Vasishth, 2005), or distance measures
as in DLT (Gibson, 2000), nor is it attributable
to cue interference (as modeled by Lewis and
Vasishth for nested object relative constructions),
since the inner and outer embeddings in these
sentences use different grammar rules. The de-
creased accuracy for center-embedding is also not
attributable to frequency effects of grammar rules
(as modeled by Hale, 2001), since the rules in
this grammar are relatively common and equally
weighted.
Instead, the decrease for center-embedded
structures emerges from this model as a necessary
result of drift due to repeated superposition of tar-
gets encoded in continuation matrices A and B.
This produces a natural decay over time as se-
quences of subordinate category token vectors b
t
introduce noise in updates to A
t
and B
t
. When
these matrices are cued in concert, as happens
when cueing across incomplete categories, the dis-
tortion is magnified. This decay is therefore a
consequence of encoding hierarchic structural in-
formation using cued associations. In contrast,
right-branching parses are not similarly as badly
degraded over time because the flat treatment of
left- and right- branching structures in a left-corner
parser does not cue as often across incomplete cat-
egories using matrix B.
25
6 Extensions
This model is also interesting because it allows se-
mantic relations to be constructed using the same
outer product associations used to define contin-
uation and category matrices in Section 4. First,
discourse referent instances and numbered relation
types are defined as vectors i and n, respectively.
Then relation tokens are reified as vectors r, simi-
lar to the reification of grammar rules described in
Section 4, and connected to relation type vectors n
by cued association R and to source and target dis-
course referents i by cued associations R
?
and R
??
.
Semantic relation types can then be cued from
grammar rules g using associative matrix N, al-
lowing relations of various types to be constructed
in cases of superposed grammar rules. In future
work, it would be interesting to see whether this
representation is consistent with observations of
local syntactic coherence (Tabor et al., 2004).
This model can also constrain relations to dis-
course referents introduced in a previous sentence
or earlier in the same sentence using a vector of
temporal features (Howard and Kahana, 2002).
This is a vector of features z
t
, that has a randomly
chosen selection of features randomly resampled
at each time step, exponentially decreasing the co-
sine similarity of the current version of the z
t
vec-
tor to earlier versions z
t
?
. If discourse referents i
are cued from the current temporal features z
t
in
an outer product associative matrix Z, it will cue
relatively recently mentioned discourse referents
more strongly than less recently mentioned refer-
ents. If discourse referents for eventualities and
propositions j are connected to explicit predicate
type referents k (say, cued by a relation of type
?0?), and if temporal cues are combined in a diag-
onal product with cues by semantic relations from
a common predicate type, the search for a consis-
tent discourse referent can be further constrained
to match the gender of a pronoun or other rela-
tions from a definite reference. In future work, it
would be interesting to compare the predictions of
this kind of model to human coreference resolu-
tion, particularly in the case of parsing conjunc-
tions with reflexive pronouns, which has been used
to argue for fully connected incremental parsing
(Sturt and Lombardo, 2005).
7 Conclusion
This paper has presented a vectorial left-
corner parsing model defined using independently
posited operations over activation-based working
memory and weight-based episodic memory. This
model has the attractive property that it hypoth-
esizes only one unary branching rule application
and only one binary branching rule application per
time step, which allows it to be smoothly inte-
grated into a vector-based recurrence that propa-
gates structural ambiguity from one time step to
the next. Predictions of this model were calcu-
lated on a center-embedded sentence processing
task and the model was shown to exhibit decreased
processing accuracy in center-embedded construc-
tions, as observed by Chomsky and Miller (1963),
even in the absence of repeated grammar rules or
potential confounding overhead costs that may be
associated with filler-gap constructions.
This model is particularly interesting because,
unlike other vectorial or connectionist parsers,
it directly implements a recursive probabilistic
grammar with explicit categories of syntactic con-
text. This explicit implementation of a probabilis-
tic grammar allows variations of this processing
model to be evaluated without having to also posit
a human-like model of acquisition. For example,
the model can simply be defined with a PCFG de-
rived from a syntactically annotated corpus.
The model is also interesting because it serves
as an existence proof that recursive grammar is not
incompatible with current models of human mem-
ory.
Finally, the fact that this model predicts mem-
ory effects at boundaries between incomplete cat-
egories, in line with predictions of fully paral-
lel left-corner parsers (van Schijndel and Schuler,
2013; van Schijndel et al., 2013b), suggests
that measures based on incomplete categories (or
based on connected components of other kinds of
syntactic or semantic structure) are not simply ar-
bitrary but rather may naturally emerge from the
use of associative memory during sentence pro-
cessing.
Although the model may not scale to broad-
coverage parsing evaluations in its present form,
future work will explore hybridization of some of
these methods into a parser with an explicit beam
of parallel hypotheses. It is anticipated that an
algorithmic-level comprehension model such as
this will allow a more nuanced understanding of
human semantic representation and grammar ac-
quisition.
26
References
James A. Anderson, Jack W. Silverstein, Stephen A.
Ritz, and Randall S. Jones. 1977. Distinctive fea-
tures, categorical perception and probability learn-
ing: Some applications of a neural model. Psycho-
logical Review, 84:413?451.
Peter beim Graben, Sabrina Gerth, and Shravan Va-
sishth. 2008. Towards dynamical system models
of language-related brain potentials. Cognitive Neu-
rodynamics, 2(3):229?255.
Noam Chomsky and George A. Miller. 1963. Intro-
duction to the formal analysis of natural languages.
In Handbook of Mathematical Psychology, pages
269?321. Wiley, New York, NY.
Jeffrey L. Elman. 1991. Distributed representations,
simple recurrent networks, and grammatical struc-
ture. Machine Learning, 7:195?225.
Edward Gibson. 2000. The dependency locality the-
ory: A distance-based theory of linguistic complex-
ity. In Image, language, brain: Papers from the first
mind articulation project symposium, pages 95?126,
Cambridge, MA. MIT Press.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the sec-
ond meeting of the North American chapter of the
Association for Computational Linguistics, pages
159?166, Pittsburgh, PA.
Marc W. Howard and Michael J. Kahana. 2002. A dis-
tributed representation of temporal context. Journal
of Mathematical Psychology, 45:269?299.
Roger Levy and Edward Gibson. 2013. Surprisal, the
pdc, and the primary locus of processing difficulty
in relative clauses. Frontiers in Psychology, 4(229).
Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition, 106(3):1126?1177.
Richard L. Lewis and Shravan Vasishth. 2005.
An activation-based model of sentence processing
as skilled memory retrieval. Cognitive Science,
29(3):375?419.
David Marr. 1971. Simple memory: A theory
for archicortex. Philosophical Transactions of the
Royal Society (London) B, 262:23?81.
J. L. McClelland, B. L. McNaughton, and R. C.
O?Reilly. 1995. Why there are complementary
learning systems in the hippocampus and neocortex:
Insights from the successes and failures of connec-
tionist models of learning and memory. Psychologi-
cal Review, 102:419?457.
B.B. Murdock. 1982. A theory for the storage and
retrieval of item and associative information. Psy-
chological Review, 89:609?626.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, New York,
April. Association for Computational Linguistics.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial intelligence, 46(1-
2):159?216.
Patrick Sturt and Vincent Lombardo. 2005. Processing
coordinate structures: Incrementality and connect-
edness. Cognitive Science, 29:291?305.
W. Tabor, B. Galantucci, and D Richardson. 2004.
Effects of merely local syntactic coherence on sen-
tence processing. Journal of Memory and Lan-
guage, 50(4):355?370.
Marten van Schijndel and William Schuler. 2013. An
analysis of frequency- and recency-based processing
costs. In Proceedings of NAACL-HLT 2013. Associ-
ation for Computational Linguistics.
Marten van Schijndel, Andy Exley, and William
Schuler. 2013a. A model of language processing
as hierarchic sequential prediction. Topics in Cogni-
tive Science, 5(3):522?540.
Marten van Schijndel, Luan Nguyen, and William
Schuler. 2013b. An analysis of memory-based pro-
cessing costs using incremental deep syntactic de-
pendency parsing. In Proceedings of CMCL 2013.
Association for Computational Linguistics.
27
