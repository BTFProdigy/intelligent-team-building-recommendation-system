Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 985?992,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Hierarchical Bayesian Language Model based on Pitman-Yor Processes
Yee Whye Teh
School of Computing,
National University of Singapore,
3 Science Drive 2, Singapore 117543.
tehyw@comp.nus.edu.sg
Abstract
We propose a new hierarchical Bayesian
n-gram model of natural languages. Our
model makes use of a generalization of
the commonly used Dirichlet distributions
called Pitman-Yor processes which pro-
duce power-law distributions more closely
resembling those in natural languages. We
show that an approximation to the hier-
archical Pitman-Yor language model re-
covers the exact formulation of interpo-
lated Kneser-Ney, one of the best smooth-
ing methods for n-gram language models.
Experiments verify that our model gives
cross entropy results superior to interpo-
lated Kneser-Ney and comparable to mod-
ified Kneser-Ney.
1 Introduction
Probabilistic language models are used exten-
sively in a variety of linguistic applications, in-
cluding speech recognition, handwriting recogni-
tion, optical character recognition, and machine
translation. Most language models fall into the
class of n-gram models, which approximate the
distribution over sentences using the conditional
distribution of each word given a context consist-
ing of only the previous n? 1 words,
P (sentence) ?
T
?
i=1
P (wordi |wordi?1i?n+1) (1)
with n = 3 (trigram models) being typical. Even
for such a modest value of n the number of param-
eters is still tremendous due to the large vocabu-
lary size. As a result direct maximum-likelihood
parameter fitting severely overfits to the training
data, and smoothing methods are indispensible for
proper training of n-gram models.
A large number of smoothing methods have
been proposed in the literature (see (Chen and
Goodman, 1998; Goodman, 2001; Rosenfeld,
2000) for good overviews). Most methods take a
rather ad hoc approach, where n-gram probabili-
ties for various values of n are combined together,
using either interpolation or back-off schemes.
Though some of these methods are intuitively ap-
pealing, the main justification has always been
empirical?better perplexities or error rates on test
data. Though arguably this should be the only
real justification, it only answers the question of
whether a method performs better, not how nor
why it performs better. This is unavoidable given
that most of these methods are not based on in-
ternally coherent Bayesian probabilistic models,
which have explicitly declared prior assumptions
and whose merits can be argued in terms of how
closely these fit in with the known properties of
natural languages. Bayesian probabilistic mod-
els also have additional advantages?it is rela-
tively straightforward to improve these models by
incorporating additional knowledge sources and
to include them in larger models in a principled
manner. Unfortunately the performance of pre-
viously proposed Bayesian language models had
been dismal compared to other smoothing meth-
ods (Nadas, 1984; MacKay and Peto, 1994).
In this paper, we propose a novel language
model based on a hierarchical Bayesian model
(Gelman et al, 1995) where each hidden variable
is distributed according to a Pitman-Yor process, a
nonparametric generalization of the Dirichlet dis-
tribution that is widely studied in the statistics and
probability theory communities (Pitman and Yor,
1997; Ishwaran and James, 2001; Pitman, 2002).
985
Our model is a direct generalization of the hierar-
chical Dirichlet language model of (MacKay and
Peto, 1994). Inference in our model is however
not as straightforward and we propose an efficient
Markov chain Monte Carlo sampling scheme.
Pitman-Yor processes produce power-law dis-
tributions that more closely resemble those seen
in natural languages, and it has been argued that
as a result they are more suited to applications
in natural language processing (Goldwater et al,
2006). We show experimentally that our hierarchi-
cal Pitman-Yor language model does indeed pro-
duce results superior to interpolated Kneser-Ney
and comparable to modified Kneser-Ney, two of
the currently best performing smoothing methods
(Chen and Goodman, 1998). In fact we show a
stronger result?that interpolated Kneser-Ney can
be interpreted as a particular approximate infer-
ence scheme in the hierarchical Pitman-Yor lan-
guage model. Our interpretation is more useful
than past interpretations involving marginal con-
straints (Kneser and Ney, 1995; Chen and Good-
man, 1998) or maximum-entropy models (Good-
man, 2004) as it can recover the exact formulation
of interpolated Kneser-Ney, and actually produces
superior results. (Goldwater et al, 2006) has inde-
pendently noted the correspondence between the
hierarchical Pitman-Yor language model and in-
terpolated Kneser-Ney, and conjectured improved
performance in the hierarchical Pitman-Yor lan-
guage model, which we verify here.
Thus the contributions of this paper are three-
fold: in proposing a langauge model with excel-
lent performance and the accompanying advan-
tages of Bayesian probabilistic models, in propos-
ing a novel and efficient inference scheme for the
model, and in establishing the direct correspon-
dence between interpolated Kneser-Ney and the
Bayesian approach.
We describe the Pitman-Yor process in Sec-
tion 2, and propose the hierarchical Pitman-Yor
language model in Section 3. In Sections 4 and
5 we give a high level description of our sampling
based inference scheme, leaving the details to a
technical report (Teh, 2006). We also show how
interpolated Kneser-Ney can be interpreted as ap-
proximate inference in the model. We show ex-
perimental comparisons to interpolated and mod-
ified Kneser-Ney, and the hierarchical Dirichlet
language model in Section 6 and conclude in Sec-
tion 7.
2 Pitman-Yor Process
Pitman-Yor processes are examples of nonpara-
metric Bayesian models. Here we give a quick de-
scription of the Pitman-Yor process in the context
of a unigram language model; good tutorials on
such models are provided in (Ghahramani, 2005;
Jordan, 2005). Let W be a fixed and finite vocabu-
lary of V words. For each word w ? W let G(w)
be the (to be estimated) probability of w, and let
G = [G(w)]w?W be the vector of word probabili-
ties. We place a Pitman-Yor process prior on G:
G ? PY(d, ?,G0) (2)
where the three parameters are: a discount param-
eter 0 ? d < 1, a strength parameter ? > ?d and
a mean vector G0 = [G0(w)]w?W . G0(w) is the
a priori probability of word w: before observing
any data, we believe word w should occur with
probability G0(w). In practice this is usually set
uniformly G0(w) = 1/V for all w ? W . Both ?
and d can be understood as controlling the amount
of variability around G0 in different ways. When
d = 0 the Pitman-Yor process reduces to a Dirich-
let distribution with parameters ?G0.
There is in general no known analytic form for
the density of PY(d, ?,G0) when the vocabulary
is finite. However this need not deter us as we
will instead work with the distribution over se-
quences of words induced by the Pitman-Yor pro-
cess, which has a nice tractable form and is suffi-
cient for our purpose of language modelling. To
be precise, notice that we can treat both G and
G0 as distributions over W , where word w ? W
has probability G(w) (respectively G0(w)). Let
x1, x2, . . . be a sequence of words drawn inde-
pendently and identically (i.i.d.) from G. We
shall describe the Pitman-Yor process in terms of
a generative procedure that produces x1, x2, . . . it-
eratively with G marginalized out. This can be
achieved by relating x1, x2, . . . to a separate se-
quence of i.i.d. draws y1, y2, . . . from the mean
distribution G0 as follows. The first word x1 is
assigned the value of the first draw y1 from G0.
Let t be the current number of draws from G0
(currently t = 1), ck be the number of words as-
signed the value of draw yk (currently c1 = 1),
and c? =
?t
k=1 ck be the current number of draws
from G. For each subsequent word xc?+1, we ei-
ther assign it the value of a previous draw yk with
probability ck?d?+c? (increment ck; set xc?+1 ? yk),
or we assign it the value of a new draw from G0
986
100 101 102 103 104 105 106
100
101
102
103
104
105
100 101 102 103 104 105 106
100
101
102
103
104
105
100 101 102 103 104 105 106
0
0.2
0.4
0.6
0.8
1
100 101 102 103 104 105 106
0
0.2
0.4
0.6
0.8
1
Figure 1: First panel: number of unique words as a function of the number of words drawn on a log-log
scale, with d = .5 and ? = 1 (bottom), 10 (middle) and 100 (top). Second panel: same, with ? = 10
and d = 0 (bottom), .5 (middle) and .9 (top). Third panel: proportion of words appearing only once, as
a function of the number of words drawn, with d = .5 and ? = 1 (bottom), 10 (middle), 100 (top). Last
panel: same, with ? = 10 and d = 0 (bottom), .5 (middle) and .9 (top).
with probability ?+dt?+c? (increment t; set ct = 1;
draw yt ? G0; set xc?+1 ? yt).
The above generative procedure produces a se-
quence of words drawn i.i.d. from G, with G
marginalized out. It is informative to study the
Pitman-Yor process in terms of the behaviour it
induces on this sequence of words. Firstly, no-
tice the rich-gets-richer clustering property: the
more words have been assigned to a draw from
G0, the more likely subsequent words will be as-
signed to the draw. Secondly, the more we draw
from G0, the more likely a new word will be as-
signed to a new draw from G0. These two ef-
fects together produce a power-law distribution
where many unique words are observed, most of
them rarely. In particular, for a vocabulary of un-
bounded size and for d > 0, the number of unique
words scales as O(?T d) where T is the total num-
ber of words. For d = 0, we have a Dirichlet dis-
tribution and the number of unique words grows
more slowly at O(? log T ).
Figure 1 demonstrates the power-law behaviour
of the Pitman-Yor process and how this depends
on d and ?. In the first two panels we show the
average number of unique words among 10 se-
quences of T words drawn from G, as a func-
tion of T , for various values of ? and d. We
see that ? controls the overall number of unique
words, while d controls the asymptotic growth of
the number of unique words. In the last two pan-
els, we show the proportion of words appearing
only once among the unique words; this gives an
indication of the proportion of words that occur
rarely. We see that the asymptotic behaviour de-
pends on d but not on ?, with larger d?s producing
more rare words.
This procedure for generating words drawn
from G is often referred to as the Chinese restau-
rant process (Pitman, 2002). The metaphor is as
follows. Consider a sequence of customers (cor-
responding to the words draws from G) visiting a
Chinese restaurant with an unbounded number of
tables (corresponding to the draws from G0), each
of which can accommodate an unbounded number
of customers. The first customer sits at the first ta-
ble, and each subsequent customer either joins an
already occupied table (assign the word to the cor-
responding draw from G0), or sits at a new table
(assign the word to a new draw from G0).
3 Hierarchical Pitman-Yor Language
Models
We describe an n-gram language model based on a
hierarchical extension of the Pitman-Yor process.
An n-gram language model defines probabilities
over the current word given various contexts con-
sisting of up to n ? 1 words. Given a context u,
let Gu(w) be the probability of the current word
taking on value w. We use a Pitman-Yor process
as the prior for Gu[Gu(w)]w?W , in particular,
Gu ? PY(d|u|, ?|u|, Gpi(u)) (3)
where pi(u) is the suffix of u consisting of all but
the earliest word. The strength and discount pa-
rameters are functions of the length |u| of the con-
text, while the mean vector is Gpi(u), the vector
of probabilities of the current word given all but
the earliest word in the context. Since we do not
know Gpi(u) either, We recursively place a prior
over Gpi(u) using (3), but now with parameters
?|pi(u)|, d|pi(u)| and mean vector Gpi(pi(u)) instead.
This is repeated until we get to G?, the vector
of probabilities over the current word given the
987
empty context ?. Finally we place a prior on G?:
G? ? PY(d0, ?0, G0) (4)
where G0 is the global mean vector, given a uni-
form value of G0(w) = 1/V for all w ? W . Fi-
nally, we place a uniform prior on the discount pa-
rameters and a Gamma(1, 1) prior on the strength
parameters. The total number of parameters in the
model is 2n.
The structure of the prior is that of a suffix tree
of depth n, where each node corresponds to a con-
text consisting of up to n?1 words, and each child
corresponds to adding a different word to the be-
ginning of the context. This choice of the prior
structure expresses our belief that words appearing
earlier in a context have (a priori) the least impor-
tance in modelling the probability of the current
word, which is why they are dropped first at suc-
cessively higher levels of the model.
4 Hierarchical Chinese Restaurant
Processes
We describe a generative procedure analogous
to the Chinese restaurant process of Section 2
for drawing words from the hierarchical Pitman-
Yor language model with all Gu?s marginalized
out. This gives us an alternative representation of
the hierarchical Pitman-Yor language model that
is amenable to efficient inference using Markov
chain Monte Carlo sampling and easy computa-
tion of the predictive probabilities for test words.
The correspondence between interpolated Kneser-
Ney and the hierarchical Pitman-Yor language
model is also apparent in this representation.
Again we may treat each Gu as a distribution
over the current word. The basic observation is
that since Gu is Pitman-Yor process distributed,
we can draw words from it using the Chinese
restaurant process given in Section 2. Further, the
only operation we need of its parent distribution
Gpi(u) is to draw words from it too. Since Gpi(u)
is itself distributed according to a Pitman-Yor pro-
cess, we can use another Chinese restaurant pro-
cess to draw words from that. This is recursively
applied until we need draws from the global mean
distribution G0, which is easy since it is just uni-
form. We refer to this as the hierarchical Chinese
restaurant process.
Let us introduce some notations. For each con-
text u we have a sequence of words xu1, xu2, . . .
drawn i.i.d. from Gu and another sequence of
words yu1, yu2, . . . drawn i.i.d. from the parent
distribution Gpi(u). We use l to index draws from
Gu and k to index the draws from Gpi(u). Define
tuwk = 1 if yuk takes on value w, and tuwk = 0
otherwise. Each word xul is assigned to one of
the draws yuk from Gpi(u). If yuk takes on value
w define cuwk as the number of words xul drawn
from Gu assigned to yuk, otherwise let cuwk = 0.
Finally we denote marginal counts by dots. For
example, cu?k is the number of xul?s assigned the
value of yuk, cuw? is the number of xul?s with
value w, and tu?? is the current number of draws
yuk from Gpi(u). Notice that we have the follow-
ing relationships among the cuw??s and tuw?:
{
tuw? = 0 if cuw? = 0;
1 ? tuw? ? cuw? if cuw? > 0;
(5)
cuw? =
?
u?:pi(u?)=u
tu?w? (6)
Pseudo-code for drawing words using the hier-
archical Chinese restaurant process is given as a
recursive function DrawWord(u), while pseudo-
code for computing the probability that the next
word drawn from Gu will be w is given in
WordProb(u,w). The counts are initialized at all
cuwk = tuwk = 0.
Function DrawWord(u):
Returns a new word drawn from Gu.
If u = 0, return w ?W with probability G0(w).
Else with probabilities proportional to:
cuwk ? d|u|tuwk: assign the new word to yuk.
Increment cuwk; return w.
?|u| + d|u|tu??: assign the new word to a new
draw yuknew from Gpi(u).
Let w ? DrawWord(pi(u));
set tuwknew = cuwknew = 1; return w.
Function WordProb(u,w):
Returns the probability that the next word after
context u will be w.
If u = 0, return G0(w). Else return
cuw??d|u|tuw?
?|u|+cu?? +
?|u|+d|u|tu??
?|u|+cu?? WordProb(pi(u),w).
Notice the self-reinforcing property of the hi-
erarchical Pitman-Yor language model: the more
a word w has been drawn in context u, the more
likely will we draw w again in context u. In fact
word w will be reinforced for other contexts that
share a common suffix with u, with the probabil-
ity of drawing w increasing as the length of the
988
common suffix increases. This is because w will
be more likely under the context of the common
suffix as well.
The hierarchical Chinese restaurant process is
equivalent to the hierarchical Pitman-Yor language
model insofar as the distribution induced on words
drawn from them are exactly equal. However, the
probability vectors Gu?s have been marginalized
out in the procedure, replaced instead by the as-
signments of words xul to draws yuk from the
parent distribution, i.e. the seating arrangement of
customers around tables in the Chinese restaurant
process corresponding to Gu. In the next section
we derive tractable inference schemes for the hi-
erarchical Pitman-Yor language model based on
these seating arrangements.
5 Inference Schemes
In this section we give a high level description
of a Markov chain Monte Carlo sampling based
inference scheme for the hierarchical Pitman-
Yor language model. Further details can be ob-
tained at (Teh, 2006). We also relate interpolated
Kneser-Ney to the hierarchical Pitman-Yor lan-
guage model.
Our training data D consists of the number of
occurrences cuw? of each word w after each con-
text u of length exactly n ? 1. This corresponds
to observing word w drawn cuw? times from Gu.
Given the training data D, we are interested in
the posterior distribution over the latent vectors
G = {Gv : all contexts v} and parameters ? =
{?m, dm : 0 ? m ? n? 1}:
p(G,?|D) = p(G,?,D)/p(D) (7)
As mentioned previously, the hierarchical Chinese
restaurant process marginalizes out each Gu, re-
placing it with the seating arrangement in the cor-
responding restaurant, which we shall denote by
Su. Let S = {Sv : all contexts v}. We are thus
interested in the equivalent posterior over seating
arrangements instead:
p(S,?|D) = p(S,?,D)/p(D) (8)
The most important quantities we need for lan-
guage modelling are the predictive probabilities:
what is the probability of a test word w after a con-
text u? This is given by
p(w|u,D) =
?
p(w|u,S,?)p(S,?|D) d(S,?)
(9)
where the first probability on the right is the pre-
dictive probability under a particular setting of
seating arrangements S and parameters ?, and the
overall predictive probability is obtained by aver-
aging this with respect to the posterior over S and
? (second probability on right). We approximate
the integral with samples {S(i),?(i)}Ii=1 drawn
from p(S,?|D):
p(w|u,D) ?
I
?
i=1
p(w|u,S(i),?(i)) (10)
while p(w|u,S,?) is given by the function
WordProb(u,w):
p(w | 0,S,?) = 1/V (11)
p(w |u,S,?) = cuw? ? d|u|tuw??|u| + cu??
+
?|u| + d|u|tu??
?|u| + cu??
p(w |pi(u),S,?) (12)
where the counts are obtained from the seating ar-
rangement Su in the Chinese restaurant process
corresponding to Gu.
We use Gibbs sampling to obtain the posterior
samples {S,?} (Neal, 1993). Gibbs sampling
keeps track of the current state of each variable
of interest in the model, and iteratively resamples
the state of each variable given the current states of
all other variables. It can be shown that the states
of variables will converge to the required samples
from the posterior distribution after a sufficient
number of iterations. Specifically for the hierar-
chical Pitman-Yor language model, the variables
consist of, for each u and each word xul drawn
from Gu, the index kul of the draw from Gpi(u)
assigned xul. In the Chinese restaurant metaphor,
this is the index of the table which the lth customer
sat at in the restaurant corresponding to Gu. If xul
has value w, it can only be assigned to draws from
Gpi(u) that has value w as well. This can either be
a preexisting draw with value w, or it can be a new
draw taking on value w. The relevant probabili-
ties are given in the functions DrawWord(u) and
WordProb(u,w), where we treat xul as the last
word drawn from Gu. This gives:
p(kul = k|S?ul,?) ?
max(0, c?uluxulk ? d)
? + c?ulu??
(13)
p(kul = knew with yuknew = xul|S?ul,?) ?
? + dt?ulu??
? + c?ulu??
p(xul|pi(u),S?ul,?) (14)
989
where the superscript ?ul means the correspond-
ing set of variables or counts with xul excluded.
The parameters ? are sampled using an auxiliary
variable sampler as detailed in (Teh, 2006). The
overall sampling scheme for an n-gram hierarchi-
cal Pitman-Yor language model takes O(nT ) time
and requires O(M) space per iteration, where T is
the number of words in the training set, and M is
the number of unique n-grams. During test time,
the computational cost is O(nI), since the predic-
tive probabilities (12) require O(n) time to calcu-
late for each of I samples.
The hierarchical Pitman-Yor language model
produces discounts that grow gradually as a func-
tion of n-gram counts. Notice that although each
Pitman-Yor process Gu only has one discount pa-
rameter, the predictive probabilities (12) produce
different discount values since tuw? can take on
different values for different words w. In fact tuw?
will on average be larger if cuw? is larger; averaged
over the posterior, the actual amount of discount
will grow slowly as the count cuw? grows. This
is shown in Figure 2 (left), where we see that the
growth of discounts is sublinear.
The correspondence to interpolated Kneser-Ney
is now straightforward. If we restrict tuw? to be at
most 1, that is,
tuw? = min(1, cuw?) (15)
cuw? =
?
u?:pi(u?)=u
tu?w? (16)
we will get the same discount value so long as
cuw? > 0, i.e. absolute discounting. Further sup-
posing that the strength parameters are all ?|u| =
0, the predictive probabilities (12) now directly re-
duces to the predictive probabilities given by inter-
polated Kneser-Ney. Thus we can interpret inter-
polated Kneser-Ney as the approximate inference
scheme (15,16) in the hierarchical Pitman-Yor lan-
guage model.
Modified Kneser-Ney uses the same values for
the counts as in (15,16), but uses a different val-
ued discount for each value of cuw? up to a maxi-
mum of c(max). Since the discounts in a hierarchi-
cal Pitman-Yor language model are limited to be-
tween 0 and 1, we see that modified Kneser-Ney is
not an approximation of the hierarchical Pitman-
Yor language model.
6 Experimental Results
We performed experiments on the hierarchical
Pitman-Yor language model on a 16 million word
corpus derived from APNews. This is the same
dataset as in (Bengio et al, 2003). The training,
validation and test sets consist of about 14 mil-
lion, 1 million and 1 million words respectively,
while the vocabulary size is 17964. For trigrams
with n = 3, we varied the training set size between
approximately 2 million and 14 million words by
six equal increments, while we also experimented
with n = 2 and 4 on the full 14 million word train-
ing set. We compared the hierarchical Pitman-Yor
language model trained using the proposed Gibbs
sampler (HPYLM) against interpolated Kneser-
Ney (IKN), modified Kneser-Ney (MKN) with
maximum discount cut-off c(max) = 3 as recom-
mended in (Chen and Goodman, 1998), and the
hierarchical Dirichlet language model (HDLM).
For the various variants of Kneser-Ney, we first
determined the parameters by conjugate gradient
descent in the cross-entropy on the validation set.
At the optimal values, we folded the validation
set into the training set to obtain the final n-gram
probability estimates. This procedure is as recom-
mended in (Chen and Goodman, 1998), and takes
approximately 10 minutes on the full training set
with n = 3 on a 1.4 Ghz PIII. For HPYLM we
inferred the posterior distribution over the latent
variables and parameters given both the training
and validation sets using the proposed Gibbs sam-
pler. Since the posterior is well-behaved and the
sampler converges quickly, we only used 125 it-
erations for burn-in, and 175 iterations to collect
posterior samples. On the full training set with
n = 3 this took about 1.5 hours.
Perplexities on the test set are given in Table 1.
As expected, HDLM gives the worst performance,
while HPYLM performs better than IKN. Perhaps
surprisingly HPYLM performs slightly worse than
MKN. We believe this is because HPYLM is not a
perfect model for languages and as a result poste-
rior estimates of the parameters are not optimized
for predictive performance. On the other hand
parameters in the Kneser-Ney variants are opti-
mized using cross-validation, so are given opti-
mal values for prediction. To validate this con-
jecture, we also experimented with HPYCV, a hi-
erarchical Pitman-Yor language model where the
parameters are obtained by fitting them in a slight
generalization of IKN where the strength param-
990
T n IKN MKN HPYLM HPYCV HDLM
2e6 3 148.8 144.1 145.7 144.3 191.2
4e6 3 137.1 132.7 134.3 132.7 172.7
6e6 3 130.6 126.7 127.9 126.4 162.3
8e6 3 125.9 122.3 123.2 121.9 154.7
10e6 3 122.0 118.6 119.4 118.2 148.7
12e6 3 119.0 115.8 116.5 115.4 144.0
14e6 3 116.7 113.6 114.3 113.2 140.5
14e6 2 169.9 169.2 169.6 169.3 180.6
14e6 4 106.1 102.4 103.8 101.9 136.6
Table 1: Perplexities of various methods and for
various sizes of training set T and length of n-
grams.
eters ?|u|?s are allowed to be positive and opti-
mized over along with the discount parameters
using cross-validation. Seating arrangements are
Gibbs sampled as in Section 5 with the parame-
ter values fixed. We find that HPYCV performs
better than MKN (except marginally worse on
small problems), and has best performance over-
all. Note that the parameter values in HPYCV are
still not the optimal ones since they are obtained
by cross-validation using IKN, an approximation
to a hierarchical Pitman-Yor language model. Un-
fortunately cross-validation using a hierarchical
Pitman-Yor language model inferred using Gibbs
sampling is currently too costly to be practical.
In Figure 2 (right) we broke down the contribu-
tions to the cross-entropies in terms of how many
times each word appears in the test set. We see
that most of the differences between the methods
appear as differences among rare words, with the
contribution of more common words being neg-
ligible. HPYLM performs worse than MKN on
words that occurred only once (on average) and
better on other words, while HPYCV is reversed
and performs better than MKN on words that oc-
curred only once or twice and worse on other
words.
7 Discussion
We have described using a hierarchical Pitman-
Yor process as a language model and shown that
it gives performance superior to state-of-the-art
methods. In addition, we have shown that the
state-of-the-art method of interpolated Kneser-
Ney can be interpreted as approximate inference
in the hierarchical Pitman-Yor language model.
In the future we plan to study in more detail
the differences between our model and the vari-
ants of Kneser-Ney, to consider other approximate
inference schemes, and to test the model on larger
data sets and on speech recognition. The hierarchi-
cal Pitman-Yor language model is a fully Bayesian
model, thus we can also reap other benefits of the
paradigm, including having a coherent probabilis-
tic model, ease of improvements by building in
prior knowledge, and ease in using as part of more
complex models; we plan to look into these possi-
ble improvements and extensions.
The hierarchical Dirichlet language model of
(MacKay and Peto, 1994) was an inspiration for
our work. Though (MacKay and Peto, 1994) had
the right intuition to look at smoothing techniques
as the outcome of hierarchical Bayesian models,
the use of the Dirichlet distribution as a prior was
shown to lead to non-competitive cross-entropy re-
sults. Our model is a nontrivial but direct gen-
eralization of the hierarchical Dirichlet language
model that gives state-of-the-art performance. We
have shown that with a suitable choice of priors
(namely the Pitman-Yor process), Bayesian meth-
ods can be competitive with the best smoothing
techniques.
The hierarchical Pitman-Yor process is a natural
generalization of the recently proposed hierarchi-
cal Dirichlet process (Teh et al, 2006). The hier-
archical Dirichlet process was proposed to solve
a different problem?that of clustering, and it is
interesting to note that such a direct generaliza-
tion leads us to a good language model. Both the
hierarchical Dirichlet process and the hierarchi-
cal Pitman-Yor process are examples of Bayesian
nonparametric processes. These have recently re-
ceived much attention in the statistics and ma-
chine learning communities because they can re-
lax previously strong assumptions on the paramet-
ric forms of Bayesian models yet retain computa-
tional efficiency, and because of the elegant way
in which they handle the issues of model selection
and structure learning in graphical models.
Acknowledgement
I wish to thank the Lee Kuan Yew Endowment
Fund for funding, Joshua Goodman for answer-
ing many questions regarding interpolated Kneser-
Ney and smoothing techniques, John Blitzer and
Yoshua Bengio for help with datasets, Anoop
Sarkar for interesting discussion, and Hal Daume
III, Min Yen Kan and the anonymous reviewers for
991
0 10 20 30 40 500
1
2
3
4
5
6
Count of n?grams
A
ve
ra
ge
 D
isc
ou
nt
IKN
MKN
HPYLM
2 4 6 8 10?0.01
?0.005
0
0.005
0.01
0.015
0.02
0.025
0.03
Cr
os
s?
En
tro
py
 D
iff
er
en
ce
s f
ro
m
 M
K
N
Count of words in test set
IKN
MKN
HPYLM
HPYCV
Figure 2: Left: Average discounts as a function of n-gram counts in IKN (bottom line), MKN (middle
step function), and HPYLM (top curve). Right: Break down of cross-entropy on test set as a function
of the number of occurrences of test words. Plotted is the sum over test words which occurred c times
of cross-entropies of IKN, MKN, HPYLM and HPYCV, where c is as given on the x-axis and MKN is
used as a baseline. Lower is better. Both panels are for the full training set and n = 3.
helpful comments.
References
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
S.F. Chen and J.T Goodman. 1998. An empirical
study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Computer Science
Group, Harvard University.
A. Gelman, J. Carlin, H. Stern, and D. Rubin. 1995.
Bayesian data analysis. Chapman & Hall, London.
Z. Ghahramani. 2005. Nonparametric Bayesian meth-
ods. Tutorial presentation at the UAI Conference.
S. Goldwater, T.L. Griffiths, and M. Johnson. 2006.
Interpolating between types and tokens by estimat-
ing power-law generators. In Advances in Neural
Information Processing Systems, volume 18.
J.T. Goodman. 2001. A bit of progress in language
modeling. Technical Report MSR-TR-2001-72, Mi-
crosoft Research.
J.T. Goodman. 2004. Exponential priors for maximum
entropy models. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics.
H. Ishwaran and L.F. James. 2001. Gibbs sampling
methods for stick-breaking priors. Journal of the
American Statistical Association, 96(453):161?173.
M.I. Jordan. 2005. Dirichlet processes, Chinese
restaurant processes and all that. Tutorial presen-
tation at the NIPS Conference.
R. Kneser and H. Ney. 1995. Improved backing-
off for m-gram language modeling. In Proceedings
of the IEEE International Conference on Acoustics,
Speech and Signal Processing, volume 1.
D.J.C. MacKay and L.C.B. Peto. 1994. A hierarchical
Dirichlet language model. Natural Language Engi-
neering.
A. Nadas. 1984. Estimation of probabilities in the lan-
guage model of the IBM speach recognition system.
IEEE Transaction on Acoustics, Speech and Signal
Processing, 32(4):859?861.
R.M. Neal. 1993. Probabilistic inference using
Markov chain Monte Carlo methods. Technical Re-
port CRG-TR-93-1, Department of Computer Sci-
ence, University of Toronto.
J. Pitman and M. Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25:855?900.
J. Pitman. 2002. Combinatorial stochastic processes.
Technical Report 621, Department of Statistics, Uni-
versity of California at Berkeley. Lecture notes for
St. Flour Summer School.
R. Rosenfeld. 2000. Two decades of statistical lan-
guage modeling: Where do we go from here? Pro-
ceedings of the IEEE, 88(8).
Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei. 2006.
Hierarchical Dirichlet processes. To appear in Jour-
nal of the American Statistical Association.
Y. W. Teh. 2006. A Bayesian interpretation of in-
terpolated Kneser-Ney. Technical Report TRA2/06,
School of Computing, National University of Singa-
pore.
992
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1015?1023, Prague, June 2007. c?2007 Association for Computational Linguistics
Improving Word Sense Disambiguation Using Topic Features
Jun Fu Cai, Wee Sun Lee
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
{caijunfu, leews}@comp.nus.edu.sg
Yee Whye Teh
Gatsby Computational Neuroscience Unit
University College London
17 Queen Square, London WC1N 3AR, UK
ywteh@gatsby.ucl.ac.uk
Abstract
This paper presents a novel approach for ex-
ploiting the global context for the task of
word sense disambiguation (WSD). This is
done by using topic features constructed us-
ing the latent dirichlet alocation (LDA) al-
gorithm on unlabeled data. The features are
incorporated into a modified na??ve Bayes
network alongside other features such as
part-of-speech of neighboring words, single
words in the surrounding context, local col-
locations, and syntactic patterns. In both the
English all-words task and the English lex-
ical sample task, the method achieved sig-
nificant improvement over the simple na??ve
Bayes classifier and higher accuracy than the
best official scores on Senseval-3 for both
task.
1 Introduction
Natural language tends to be ambiguous. A word
often has more than one meanings depending on the
context. Word sense disambiguation (WSD) is a nat-
ural language processing (NLP) task in which the
correct meaning (sense) of a word in a given context
is to be determined.
Supervised corpus-based approach has been the
most successful in WSD to date. In such an ap-
proach, a corpus in which ambiguous words have
been annotated with correct senses is first collected.
Knowledge sources, or features, from the context of
the annotated word are extracted to form the training
data. A learning algorithm, like the support vector
machine (SVM) or na??ve Bayes, is then applied on
the training data to learn the model. Finally, in test-
ing, the learnt model is applied on the test data to
assign the correct sense to any ambiguous word.
The features used in these systems usually in-
clude local features, such as part-of-speech (POS)
of neighboring words, local collocations , syntac-
tic patterns and global features such as single words
in the surrounding context (bag-of-words) (Lee and
Ng, 2002). However, due to the data scarcity prob-
lem, these features are usually very sparse in the
training data. There are, on average, 11 and 28
training cases per sense in Senseval 2 and 3 lexi-
cal sample task respectively, and 6.5 training cases
per sense in the SemCor corpus. This problem is
especially prominent for the bag-of-words feature;
more than hundreds of bag-of-words are usually ex-
tracted for each training instance and each feature
could be drawn from any English word. A direct
consequence is that the global context information,
which the bag-of-words feature is supposed to cap-
ture, may be poorly represented.
Our approach tries to address this problem by
clustering features to relieve the scarcity problem,
specifically on the bag-of-words feature. In the pro-
cess, we construct topic features, trained using the
latent dirichlet alocation (LDA) algorithm. We train
the topic model (Blei et al, 2003) on unlabeled data,
clustering the words occurring in the corpus to a pre-
defined number of topics. We then use the resulting
topic model to tag the bag-of-words in the labeled
corpus with topic distributions. We incorporate the
distributions, called the topic features, using a sim-
ple Bayesian network, modified from na??ve Bayes
1015
model, alongside other features and train the model
on the labeled corpus. The approach gives good per-
formance on both the lexical sample and all-words
tasks on Senseval data.
The paper makes mainly two contributions. First,
we are able to show that a feature that efficiently
captures the global context information using LDA
algorithm can significantly improve the WSD ac-
curacy. Second, we are able to obtain this feature
from unlabeled data, which spares us from any man-
ual labeling work. We also showcase the potential
strength of Bayesian network in the WSD task, ob-
taining performance that rivals state-of-arts meth-
ods.
2 Related Work
Many WSD systems try to tackle the data scarcity
problem. Unsupervised learning is introduced pri-
marily to deal with the problem, but with limited
success (Snyder and Palmer, 2004). In another ap-
proach, the learning algorithm borrows training in-
stances from other senses and effectively increases
the training data size. In (Kohomban and Lee,
2005), the classifier is trained using grouped senses
for verbs and nouns according to WordNet top-level
synsets and thus effectively pooling training cases
across senses within the same synset. Similarly,
(Ando, 2006) exploits data from related tasks, using
all labeled examples irrespective of target words for
learning each sense using the Alternating Structure
Optimization (ASO) algorithm (Ando and Zhang,
2005a; Ando and Zhang, 2005b). Parallel texts is
proposed in (Resnik and Yarowsky, 1997) as po-
tential training data and (Chan and Ng, 2005) has
shown that using automatically gathered parallel
texts for nouns could significantly increase WSD ac-
curacy, when tested on Senseval-2 English all-words
task.
Our approach is somewhat similar to that of us-
ing generic language features such as POS tags; the
words are tagged with its semantic topic that may be
trained from other corpuses.
3 Feature Construction
We first present the latent dirichlet alocation algo-
rithm and its inference procedures, adapted from the
original paper (Blei et al, 2003).
3.1 Latent Dirichlet Allocation
LDA is a probabilistic model for collections of dis-
crete data and has been used in document model-
ing and text classification. It can be represented
as a three level hierarchical Bayesian model, shown
graphically in Figure 1. Given a corpus consisting of
M documents, LDA models each document using a
mixture over K topics, which are in turn character-
ized as distributions over words.
?
wz??
N
M
Figure 1: Graphical Model for LDA
In the generative process of LDA, for each doc-
ument d we first draw the mixing proportion over
topics ?d from a Dirichlet prior with parameters ?.
Next, for each of the Nd words wdn in document d, a
topic zdn is first drawn from a multinomial distribu-
tion with parameters ?d. Finally wdn is drawn from
the topic specific distribution over words. The prob-
ability of a word token w taking on value i given
that topic z = j was chosen is parameterized using
a matrix ? with ?ij = p(w = i|z = j). Integrating
out ?d?s and zdn?s, the probability p(D|?, ?) of the
corpus is thus:
M?
d=1
?
p(?d|?)
(
Nd?
n=1
?
zdn
p(zdn|?d)p(wdn|zdn, ?)
)
d?d
3.1.1 Inference
Unfortunately, it is intractable to directly solve the
posterior distribution of the hidden variables given a
document, namely p(?, z|w, ?, ?). However, (Blei
et al, 2003) has shown that by introducing a set of
variational parameters, ? and ?, a tight lower bound
on the log likelihood of the probability can be found
using the following optimization procedure:
(??, ??) = argmin
?,?
D(q(?, z|?, ?)?p(?, z|w, ?, ?))
1016
where
q(?, z|?, ?) = q(?|?)
N?
n=1
q(zn|?n),
? is the Dirichlet parameter for ? and the multino-
mial parameters (?1 ? ? ??N ) are the free variational
parameters. Note here ? is document specific in-
stead of corpus specific like ?. Graphically, it is rep-
resented as Figure 2. The optimizing values of ? and
? can be found by minimizing the Kullback-Leibler
(KL) divergence between the variational distribution
and the true posterior.
?
?
?
z
N
M
Figure 2: Graphical Model for Variational Inference
3.2 Baseline Features
For both the lexical sample and all-words tasks,
we use the following standard baseline features for
comparison.
POS Tags For each training or testing word, w,
we include POS tags for P words prior to as well as
after w within the same sentence boundary. We also
include the POS tag of w. If there are fewer than
P words prior or after w in the same sentence, we
denote the corresponding feature as NIL.
Local Collocations Collocation Ci,j refers to the
ordered sequence of tokens (words or punctuations)
surrounding w. The starting and ending position of
the sequence are denoted i and j respectively, where
a negative value refers to the token position prior to
w. We adopt the same 11 collocation features as
(Lee and Ng, 2002), namely C?1,?1, C1,1, C?2,?2,
C2,2, C?2,?1, C?1,1, C1,2, C?3,?1, C?2,1, C?1,2,
and C1,3.
Bag-of-Words For each training or testing word,
w, we get G words prior to as well as after w, within
the same document. These features are position in-
sensitive. The words we extract are converted back
to their morphological root forms.
Syntactic Relations We adopt the same syntactic
relations as (Lee and Ng, 2002). For easy reference,
we summarize the features into Table 1.
POS of w Features
Noun Parent headword h
POS of h
Relative position of h to w
Verb Left nearest child word of w, l
Right nearest child word of w, r
POS of l
POS of r
POS of w
Voice of w
Adjective Parent headword h
POS of h
Table 1: Syntactic Relations Features
The exact values of P and G for each task are set
according to cross validation result.
3.3 Topic Features
We first select an unlabeled corpus, such as 20
Newsgroups, and extract individual words from it
(excluding stopwords). We choose the number of
topics, K, for the unlabeled corpus and we apply the
LDA algorithm to obtain the ? parameters, where
? represents the probability of a word wi given a
topic zj , p(wi|zj) = ?ij . The model essentially
clusters words that occurred in the unlabeled cor-
pus according to K topics. The conditional prob-
ability p(wi|zj) = ?ij is later used to tag the words
in the unseen test example with the probability of
each topic.
For some variants of the classifiers that we con-
struct, we also use the ? parameter, which is doc-
ument specific. For these classifiers, we may need
to run the inference algorithm on the labeled corpus
and possibly on the test documents. The ? param-
eter provides an approximation to the probability of
1017
selecting topic i in the document:
p(zi|?) =
?i
?
K ?k
. (1)
4 Classifier Construction
4.1 Bayesian Network
We construct a variant of the na??ve Bayes network
as shown in Figure 3. Here, w refers to the word.
s refers to the sense of the word. In training, s is
observed while in testing, it is not. The features f1
to fn are baseline features mentioned in Section 3.2
(including bag-of-words) while z refers to the la-
tent topic that we set for clustering unlabeled corpus.
The bag-of-words b are extracted from the neigh-
bours of w and there are L of them. Note that L can
be different from G, which is the number of bag-of-
words in baseline features. Both will be determined
by the validation result.
? ? ?
? ?? ?
baselinefeatures
w
s
fnf1
b
z
L
Figure 3: Graphical Model with LDA feature
The log-likelihood of an instance, `(w, s, F, b)
where F denotes the set of baseline features, can be
written as
= logp(w) + logp(s|w) +
?
F
log(p(f |s))
+
?
L
log
(
?
K
p(zk|s)p(bl|zk)
)
.
The log p(w) term is constant and thus can be ig-
nored. The first portion is normal na??ve Bayes. And
second portion represents the additional LDA plate.
We decouple the training process into three separate
stages. We first extract baseline features from the
task training data, and estimate, using normal na??ve
Bayes, p(s|w) and p(f |s) for all w, s and f . The
parameters associated with p(b|z) are estimated us-
ing LDA from unlabeled data. Finally we estimate
the parameters associated with p(z|s). We experi-
mented with three different ways of both doing the
estimation as well as using the resulting model and
chose one which performed best empirically.
4.1.1 Expectation Maximization Approach
For p(z|s), a reasonable estimation method is to
use maximum likelihood estimation. This can be
done using the expectation maximization (EM) algo-
rithm. In classification, we just choose s? that maxi-
mizes the log-likelihood of the test instance, where:
s? = argmax
s
`(w, s, F, b)
In this approach, ? is never used which means the
LDA inference procedure is not used on any labeled
data at all.
4.1.2 Soft Tagging Approach
Classification in this approach is done using the
full Bayesian network just as in the EM approach.
However we do the estimation of p(z|s) differently.
Essentially, we perform LDA inference on the train-
ing corpus in order to obtain ? for each document.
We then use the ? and ? to obtain p(z|b) for each
word using
p(zi|bl, ?) =
p(bl|zi)p(zi|?)
?
K p(bl|zk)p(zk|?)
,
where equation [1] is used for estimation of p(zi|?).
This effectively transforms b to a topical distri-
bution which we call a soft tag where each soft
tag is probability distribution t1, . . . , tK on topics.
We then use this topical distribution for estimating
p(z|s). Let si be the observed sense of instance i
and tij1 , . . . , t
ij
K be the soft tag of the j-th bag-of-
word feature of instance i. We estimate p(z|s) as
p(zjk|s) =
?
si=s t
ij
k
?
si=s
?
k? t
ij
k?
(2)
This approach requires us to do LDA inference on
the corpus formed by the labeled training data, but
1018
not the testing data. This is because we need ? to
get transformed topical distribution in order to learn
p(z|s) in the training. In the testing, we only apply
the learnt parameters to the model.
4.1.3 Hard Tagging Approach
Hard tagging approach no longer assumes that z is
latent. After p(z|b) is obtained using the same pro-
cedure in Section 4.1.2, the topic zi with the high-
est p(zi|b) among all K topics is picked to represent
z. In this way, b is transformed into a single most
?prominent? topic. This topic label is used in the
same way as baseline features for both training and
testing in a simple na??ve Bayes model.
This approach requires us to perform the transfor-
mation both on the training as well as testing data,
since z becomes an observed variable. LDA infer-
ence is done on two corpora, one formed by the
training data and the other by testing data, in order
to get the respective values of ?.
4.2 Support Vector Machine Approach
In the SVM (Vapnik, 1995) approach, we first form a
training and a testing file using all standard features
for each sense following (Lee and Ng, 2002) (one
classifier per sense). To incorporate LDA feature,
we use the same approach as Section 4.1.2 to trans-
form b into soft tags, p(z|b). As SVM deals with
only observed features, we need to transform b both
in the training data and in the testing data. Compared
to (Lee and Ng, 2002), the only difference is that for
each training and testing case, we have additional
L ?K LDA features, since there are L bag-of-words
and each has a topic distribution represented by K
values.
5 Experimental Setup
We describe here the experimental setup on the En-
glish lexical sample task and all-words task.
We use MXPOST tagger (Adwait, 1996) for POS
tagging, Charniak parser (Charniak, 2000) for ex-
tracting syntactic relations, SVMlight1 for SVM
classifier and David Blei?s version of LDA2 for LDA
training and inference. All default parameters are
used unless mentioned otherwise. For all standard
1http://svmlight.joachims.org
2http://www.cs.princeton.edu/?blei/lda-c/
baseline features, we use Laplace smoothing but for
the soft tag (equation [2]), we use a smoothing pa-
rameter value of 2.
5.1 Development Process
5.1.1 Lexical Sample Task
We use the Senseval-2 lexical sample task for
preliminary investigation of different algorithms,
datasets and other parameters. As the dataset is used
extensively for this purpose, only the Senseval-3 lex-
ical sample task is used for evaluation.
Selecting Bayesian Network The best achievable
result, using the three different Bayesian network
approaches, when validating on Senseval-2 test data
is shown in Table 2. The parameters that are used
are P = 3 and G = 3.
EM 68.0
Hard Tagging 65.6
Soft Tagging 68.9
Table 2: Results on Senseval-2 English lexical sam-
ple using different Bayesian network approaches.
From the results, it appears that both the EM and
the Hard Tagging approaches did not yield as good
results as the Soft Tagging approach did. The EM
approach ignores the LDA inference result, ?, which
we use to get our topic prior. This information is
document specific and can be regarded as global
context information. The Hard Tagging approach
also uses less information, as the original topic dis-
tribution is now represented only by the topic with
the highest probability of occurring. Therefore, both
methods have information loss and are disadvan-
taged against the Soft Tagging approach. We use
the Soft Tagging approach for the Senseval-3 lexical
sample and the all-words tasks.
Unlabeled Corpus Selection The unlabeled cor-
pus we choose to train LDA include 20 News-
groups, Reuters, SemCor, Senseval-2 lexical sam-
ple data and Senseval-3 lexical sample data. Al-
though the last three are labeled corpora, we only
need the words from these corpora and thus they can
be regarded as unlabeled too. For Senseval-2 and
Senseval-3 data, we define the whole passage for
each training and testing instance as one document.
1019
The relative effect using different corpus and com-
binations of them is shown in Table 3, when validat-
ing on Senseval-2 test data using the Soft Tagging
approach.
Corpus |w| K L Senseval-2
20 Newsgroups 1.7M 40 60 67.9
Reuters 1.3M 30 60 65.5
SemCor 0.3M 30 60 66.9
Senseval-2 0.6M 30 40 66.9
Senseval-3 0.6M 50 60 67.6
All 4.5M 60 40 68.9
Table 3: Effect of using different corpus for LDA
training, |w| represents the corpus size in terms of
the number of words in the corpus
The 20 Newsgroups corpus yields the best result
if used individually. It has a relatively larger corpus
size at 1.7 million words in total and also a well bal-
anced topic distribution among its documents, rang-
ing across politics, finance, science, computing, etc.
The Reuters corpus, on the other hand, focuses heav-
ily on finance related articles and has a rather skewed
topic distribution. This probably contributed to its
inferior result. However, we found that the best re-
sult comes from combining all the corpora together
with K = 60 and L = 40.
Results for Optimized Configuration As base-
line for the Bayesian network approaches, we use
na??ve Bayes with all baseline features. For the base-
line SVM approach, we choose P = 3 and include
all the words occurring in the training and testing
passage as bag-of-words feature.
The F-measure result we achieve on Senseval-2
test data is shown in Table 4. Our four systems
are listed as the top four entries in the table. Soft
Tag refers to the soft tagging Bayesian network ap-
proach. Note that we used the Senseval-2 test data
for optimizing the configuration (as is done in the
ASO result). Hence, the result should not be taken
as reliable. Nevertheless, it is worth noting that the
improvement of Bayesian network approach over its
baseline is very significant (+5.5%). On the other
hand, SVM with topic features shows limited im-
provement over its baseline (+0.8%).
Bayes (Soft Tag) 68.9
SVM-Topic 66.0
SVM baseline 65.2
NB baseline 63.4
ASO(best configuration)(Ando, 2006) 68.1
Classifier Combination(Florian, 2002) 66.5
Polynomial KPCA(Wu et al, 2004) 65.8
SVM(Lee and Ng, 2002) 65.4
Senseval-2 Best System 64.2
Table 4: Results (best configuration) compared to
previous best systems on Senseval-2 English lexical
sample task.
5.1.2 All-words Task
In the all-words task, no official training data is
provided with Senseval. We follow the common
practice of using the SemCor corpus as our training
data. However, we did not use SVM approach in this
task as there are too few training instances per sense
for SVM to achieve a reasonably good accuracy.
As there are more training instances in SemCor,
230, 000 in total, we obtain the optimal configura-
tion using 10 fold cross validation on the SemCor
training data. With the optimal configuration, we
test our system on both Senseval-2 and Senseval-3
official test data.
For baseline features, we set P = 3 and B = 1. We
choose a LDA training corpus comprising 20 News-
groups and SemCor data, with number of topics K
= 40 and number of LDA bag-of-words L = 14.
6 Results
We now present the results on both English lexical
sample task and all-words task.
6.1 Lexical Sample Task
With the optimal configurations from Senseval-2,
we tested the systems on Senseval-3 data. Table 5
shows our F-measure result compared to some of the
best reported systems. Although SVM with topic
features shows limited success with only a 0.6%
improvement, the Bayesian network approach has
again demonstrated a good improvement of 3.8%
over its baseline and is better than previous reported
best systems except ASO(Ando, 2006).
1020
Bayes (Soft Tag) 73.6
SVM-topic 73.0
SVM baseline 72.4
NB baseline 69.8
ASO(Ando, 2006) 74.1
SVM-LSA (Strapparava et al, 2004) 73.3
Senseval-3 Best System(Grozea, 2004) 72.9
Table 5: Results compared to previous best systems
on Senseval-3 English lexical sample task.
6.2 All-words Task
The F-measure micro-averaged result for our sys-
tems as well as previous best systems for Senseval-2
and Senseval-3 all-words task are shown in Table 6
and Table 7 respectively. Bayesian network with soft
tagging achieved 2.6% improvement over its base-
line in Senseval-2 and 1.7% in Senseval-3. The re-
sults also rival some previous best systems, except
for SMUaw (Mihalcea, 2002) which used additional
labeled data.
Bayes (Soft Tag) 66.3
NB baseline 63.7
SMUaw (Mihalcea, 2002) 69.0
Simil-Prime (Kohomban and Lee, 2005) 66.4
Senseval-2 Best System 63.6
(CNTS-Antwerp (Hoste et al, 2001))
Table 6: Results compared to previous best systems
on Senseval-2 English all-words task.
Bayes (Soft Tag) 66.1
NB baseline 64.6
Simil-Prime (Kohomban and Lee, 2005) 66.1
Senseval-3 Best System 65.2
(GAMBL-AW-S(Decadt et al, 2004))
Senseval-3 2nd Best System (SenseLearner 64.6
(Mihalcea and Faruque, 2004))
Table 7: Results compared to previous best systems
on Senseval-3 English all-words task.
6.3 Significance of Results
We perform the ?2-test, using the Bayesian network
and its na??ve Bayes baseline (NB baseline) as pairs,
to verify the significance of these results. The result
is reported in Table 8. The results are significant at
90% confidence level, except for the Senseval-3 all-
words task.
Senseval-2 Senseval-3
All-word 0.0527 0.2925
Lexical Sample <0.0001 0.0002
Table 8: P value for ?2-test significance levels of
results.
6.4 SVM with Topic Features
The results on lexical sample task show that SVM
benefits less from the topic feature than the Bayesian
approach. One possible reason is that SVM base-
line is able to use all bag-of-words from surround-
ing context while na??ve Bayes baseline can only use
very few without decreasing its accuracy, due to the
sparse representation. In this sense, SVM baseline
already captures some of the topical information,
leaving a smaller room for improvement. In fact, if
we exclude the bag-of-words feature from the SVM
baseline and add in the topic features, we are able
to achieve almost the same accuracy as we did with
both features included, as shown in Table 9. This
further shows that the topic feature is a better rep-
resentation of global context than the bag-of-words
feature.
SVM baseline 72.4
SVM baseline - BAG + topic 73.5
SVM-topic 73.6
Table 9: Results on Senseval-3 English lexical sam-
ple task
6.5 Results on Different Parts-of-Speech
We analyse the result obtained on Senseval-3 En-
glish lexical sample task (using Senseval-2 optimal
configuration) according to the test instance?s part-
of-speech, which includes noun, verb and adjec-
tive, compared to the na??ve Bayes baseline. Ta-
ble 10 shows the relative improvement on each part-
of-speech. The second column shows the number
of testing instances belonging to the particular part-
of-speech. The third and fourth column shows the
1021
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
0 2 4 6 8 10 12 14 16 18
L
K=10
3
3
3
3 3
3 3
3
3
K=20
+
+
+
+ + +
+
+
+
K=40
2 2
2
2
2
2
2
2
2
K=60
? ?
? ?
? ?
?
?
?
K=80
4 4
4
4
4 4
4 4
4
Figure 4: Accuracy with varing L and K on
Senseval-2 all-words task
accuracy achieved by na??ve Bayes baseline and the
Bayesian network. Adjectives show no improve-
ment while verbs show a moderate +2.2% improve-
ment. Nouns clearly benefit from topical informa-
tion much more than the other two parts-of-speech,
obtaining a +5.7% increase over its baseline.
POS Total NB baseline Bayes (Soft Tag)
Noun 1807 69.5 75.2
Verb 1978 71.1 73.5
Adj 159 57.2 57.2
Total 3944 69.8 73.6
Table 10: Improvement with different POS on
Senseval-3 lexical sample task
6.6 Sensitivity to L and K
We tested on Senseval-2 all-words task using differ-
ent L and K. Figure 4 is the result.
6.7 Results on SemEval-1
We participated in SemEval-1 English coarse-
grained all-words task (task 7), English fine-grained
all-words task (task 17, subtask 3) and English
coarse-grained lexical sample task (task 17, subtask
1), using the method described in this paper. For
all-words task, we use Senseval-2 and Senseval-3
all-words task data as our validation set to fine tune
the parameters. For lexical sample task, we use the
training data provided as the validation set.
We achieved 88.7%, 81.6% and 57.6% for coarse-
grained lexical sample task, coarse-grained all-
words task and fine-grained all-words task respec-
tively. The results ranked first, second and fourth in
the three tasks respectively.
7 Conclusion and Future Work
In this paper, we showed that by using LDA algo-
rithm on bag-of-words feature, one can utilise more
topical information and boost the classifiers accu-
racy on both English lexical sample and all-words
task. Only unlabeled data is needed for this improve-
ment. It would be interesting to see how the feature
can help on WSD of other languages and other nat-
ural language processing tasks such as named-entity
recognition.
References
Y. K. Lee and H. T. Ng. 2002. An Empirical Evaluation
of Knowledge Sources and Learning Algorithms for
Word Sense Disambiguation. In Proc. of EMNLP.
B. Snyder and M. Palmer. 2004. The English All-Words
Task. In Proc. of Senseval-3.
U. S. Kohomban and W. S. Lee 2005. Learning Semantic
Classes for Word Sense Disambiguation. In Proc. of
ACL.
R. K. Ando. 2006. Applying Alternating Structure Op-
timization to Word Sense Disambiguation. In Proc. of
CoNLL.
Y. S. Chan and H. T. Ng 2005. Scaling Up Word Sense
Disambiguation via Parallel Texts. In Proc. of AAAI.
R. K. Ando and T. Zhang. 2005a. A Framework for
Learning Predictive Structures from Multiple Tasks
and Unlabeled Data. Journal of Machine Learning Re-
search.
R. K. Ando and T. Zhang. 2005b. A High-Performance
Semi-Supervised Learning Method for Text Chunking.
In Proc. of ACL.
P. Resnik and D. Yarowsky. 1997. A Perspective on
Word Sense Disambiguation Methods and Their Eval-
uation. In Proc. of ACL.
D. M. Blei and A. Y. Ng and M. I. Jordan. 2003. La-
tent Dirichlet Allocation. Journal of Machine Learn-
ing Research.
1022
A. Ratnaparkhi 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Proc. of EMNLP.
E. Charniak 2000. A Maximum-Entropy-Inspired
Parser. In Proc. of the 1st Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
V. N. Vapnik 1995. The Nature of Statistical Learning
Theory. Springer-Verlag, New York.
R. Florian and D. Yarowsky 2002. Modeling consensus:
Classifier Combination for Word Sense Disambigua-
tion. In Proc. of EMNLP.
D. Wu and W. Su and M. Carpuat. 2004. A Kernel PCA
Method for Superior Word Sense Disambiguation. In
Proc. of ACL.
C. Strapparava and A. Gliozzo and C. Giuliano 2004.
Pattern Abstraction and Term Similarity for Word
Sense Disambiguation: IRST at Senseval-3. In Proc.
of Senseval-3.
C. Grozea 2004. Finding Optimal Parameter Settings
for High Performance Word Sense Disambiguation. In
Proc. of Senseval-3.
R. Mihalcea 2002. Bootstrapping Large Sense Tagged
Corpora. In Proc. of the 3rd International Conference
on Languages Resources and Evaluations.
V. Hoste and A. Kool and W. Daelmans 2001. Classifier
Optimization and Combination in English All Words
Task. In Proc. of Senseval-2.
B. Decadt and V. Hoste and W. Daelmans 2004.
GAMBL, Genetic Algorithm Optimization of
Memory-Based WSD. In Proc. of Senseval-3.
R. Mihalcea and E. Faruque 2004. Sense-learner: Mini-
mally Supervised Word Sense Disambiguation for All
Words in Open Text. In Proc. of Senseval-3.
1023
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 249?252,
Prague, June 2007. c?2007 Association for Computational Linguistics
NUS-ML: Improving Word Sense Disambiguation Using Topic Features
Jun Fu Cai, Wee Sun Lee
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
{caijunfu, leews}@comp.nus.edu.sg
Yee Whye Teh
Gatsby Computational Neuroscience Unit
University College London
17 Queen Square, London WC1N 3AR, UK
ywteh@gatsby.ucl.ac.uk
Abstract
We participated in SemEval-1 English
coarse-grained all-words task (task 7), En-
glish fine-grained all-words task (task 17,
subtask 3) and English coarse-grained lex-
ical sample task (task 17, subtask 1). The
same method with different labeled data is
used for the tasks; SemCor is the labeled
corpus used to train our system for the all-
words tasks while the labeled corpus that
is provided is used for the lexical sam-
ple task. The knowledge sources include
part-of-speech of neighboring words, single
words in the surrounding context, local col-
locations, and syntactic patterns. In addi-
tion, we constructed a topic feature, targeted
to capture the global context information,
using the latent dirichlet alocation (LDA)
algorithm with unlabeled corpus. A modi-
fied na??ve Bayes classifier is constructed to
incorporate all the features. We achieved
81.6%, 57.6%, 88.7% for coarse-grained all-
words task, fine-grained all-words task and
coarse-grained lexical sample task respec-
tively.
1 Introduction
Supervised corpus-based approach has been the
most successful in WSD to date. However, this ap-
proach faces severe data scarcity problem, resulting
features being sparsely represented in the training
data. This problem is especially prominent for the
bag-of-words feature. A direct consequence is that
the global context information, which the bag-of-
words feature is supposed to capture, may be poorly
represented.
Our system tries to address this problem by
clustering features to relieve the scarcity problem,
specifically on the bag-of-words feature. In the pro-
cess, we construct topic features, trained using the
latent dirichlet alocation (LDA) algorithm. We train
the topic model (Blei et al, 2003) on unlabeled data,
clustering the words occurring in the corpus to a pre-
defined number of topics. We then use the resulting
topic model to tag the bag-of-words in the labeled
corpus with topic distributions.
We incorporate the distributions, called the topic
features, using a simple Bayesian network, modified
from na??ve Bayes model, alongside other features
and train the model on the labeled corpus.
2 Feature Construction
2.1 Baseline Features
For both the lexical sample and all-words tasks, we
use the following standard baseline features.
POS Tags For each word instance w, we include
POS tags for P words prior to as well as after w
within the same sentence boundary. We also include
the POS tag of w. If there are fewer than P words
prior or after w in the same sentence, we denote the
corresponding feature as NIL.
Local Collocations We adopt the same 11 col-
location features as (Lee and Ng, 2002), namely
C?1,?1, C1,1, C?2,?2, C2,2, C?2,?1, C?1,1, C1,2,
C?3,?1, C?2,1, C?1,2, and C1,3.
249
Bag-of-Words For each training or testing word,
w, we get G words prior to as well as after w, within
the same document. These features are position in-
sensitive. The words we extract are converted back
to their morphological root forms.
Syntactic Relations We adopt the same syntactic
relations as (Lee and Ng, 2002). For easy reference,
we summarize the features into Table 1.
POS of w Features
Noun Parent headword h
POS of h
Relative position of h to w
Verb Left nearest child word of w, l
Right nearest child word of w, r
POS of l
POS of r
POS of w
Voice of w
Adjective Parent headword h
POS of h
Table 1: Syntactic Relations Features
The exact values of P and G for each task are set
according to validation result.
2.2 Latent Dirichlet Allocation
We present here the latent dirichlet alocation algo-
rithm and its inference procedures, adapted from the
original paper (Blei et al, 2003).
LDA is a probabilistic model for collections of
discrete data and has been used in document mod-
eling and text classification. It can be represented
as a three level hierarchical Bayesian model, shown
graphically in Figure 1. Given a corpus consisting of
M documents, LDA models each document using a
mixture over K topics, which are in turn character-
ized as distributions over words.
In the generative process of LDA, for each doc-
ument d we first draw the mixing proportion over
topics ?d from a Dirichlet prior with parameters ?.
Next, for each of the Nd words wdn in document d, a
topic zdn is first drawn from a multinomial distribu-
tion with parameters ?d. Finally wdn is drawn from
the topic specific distribution over words. The prob-
ability of a word token w taking on value i given
that topic z = j was chosen is parameterized using
?
wz??
N
M
Figure 1: Graphical Model for LDA
a matrix ? with ?ij = p(w = i|z = j). Integrating
out ?d?s and zdn?s, the probability p(D|?, ?) of the
corpus is thus:
M?
d=1
?
p(?d|?)
(
Nd?
n=1
?
zdn
p(zdn|?d)p(wdn|zdn, ?)
)
d?d
In variational inference, the latent variables ?d
and zdn are assumed independent and updates to
the variational posteriors for ?d and zdn are derived
(Blei et al, 2003). It can be shown that the varia-
tional posterior for ?d is a Dirichlet distribution, say
with variational parameters ?d, which we shall use
in the following to construct topic features.
2.3 Topic Features
We first select an unlabeled corpus, such as 20
Newsgroups, and extract individual words from it
(excluding stopwords). We choose the number of
topics, K, for the unlabeled corpus and we apply the
LDA algorithm to obtain the ? parameters, where ?
represents the probability of a word w = i given a
topic z = j, p(w = i|z = j) = ?ij .
The model essentially clusters words that oc-
curred in the unlabeled corpus according to K top-
ics. The conditional probability p(w = i|z = j) =
?ij is later used to tag the words in the unseen test
example with the probability of each topic.
We also use the document-specific ?d parameters.
Specifically, we need to run the inference algorithm
on the labeled corpus to get ?d for each document d
in the corpus. The ?d parameter provides an approx-
imation to the probability of selecting topic i in the
document:
p(zi|?d) =
?di
?
K ?dk
. (1)
250
3 Classifier Construction
We construct a variant of the na??ve Bayes network
as shown in Figure 2. Here, w refers to the word.
s refers to the sense of the word. In training, s is
observed while in testing, it is not. The features f1
to fn are baseline features mentioned in Section 2.1
(including bag-of-words) while z refers to the la-
tent topic that we set for clustering unlabeled corpus.
The bag-of-words b are extracted from the neigh-
bours of w and there are L of them. Note that L can
be different from G, which is the number of bag-of-
words in baseline features. Both will be determined
by the validation result.
? ? ?
? ?? ?
baselinefeatures
w
s
fnf1
b
z
L
Figure 2: Graphical Model with LDA feature
The log-likelihood of an instance, `(w, s, F, b)
where F denotes the set of baseline features, can be
written as
= logp(w) + logp(s|w) +
?
F
log(p(f |s))
+
?
L
log
(
?
K
p(zk|s)p(bl|zk)
)
.
The log p(w) term is constant and thus can be
ignored. The first portion is normal na??ve Bayes.
And second portion represents the additional LDA
plate. We decouple the training process into separate
stages. We first extract baseline features from the
task training data, and estimate, using normal na??ve
Bayes, p(s|w) and p(f |s) for all w, s and f .
Next, the parameters associated with p(b|z) are
estimated using LDA from unlabeled data, which is
?. To estimate p(z|s), we perform LDA inference
on the training corpus in order to obtain ?d for each
document d. We then use the ?d and ? to obtain
p(z|b) for each word using
p(zi|bl, ?d) =
p(bl|zi)p(zi|?d)
?
K p(bl|zk)p(zk|?d)
,
where equation (1) is used for estimation of p(zi|?d).
This effectively transforms b to a topical distri-
bution which we call a soft tag where each soft
tag is probability distribution t1, . . . , tK on topics.
We then use this topical distribution for estimating
p(z|s). Let si be the observed sense of instance i
and tij1 , . . . , t
ij
K be the soft tag of the j-th bag-of-
word feature of instance i. We estimate p(z|s) as
p(zjk|s) =
?
si=s t
ij
k
?
si=s
?
k? t
ij
k?
(2)
This approach requires us to do LDA inference on
the corpus formed by the labeled training data, but
not the testing data. This is because we need ? to
get transformed topical distribution in order to learn
p(z|s) in the training. In the testing, we only apply
the learnt parameters to the model.
4 Experimental Setup
We describe here the experimental setup on the En-
glish lexical sample task and all-words task. Note
that we do not distinguish the two all-words tasks as
the same parameters will be applied.
For lexical sample task, we use 5-fold cross val-
idation on the training data provided to determine
our parameters. For all-words task, we use SemCor
as our training data and validate on Senseval-2 and
Senseval-3 all-words test data.
We use MXPOST tagger (Adwait, 1996) for POS
tagging, Charniak parser (Charniak, 2000) for ex-
tracting syntactic relations, and David Blei?s version
of LDA1 for LDA training and inference. All default
parameters are used unless mentioned otherwise.
For the all-word tasks, we use sense 1 as back-off
for words that have not appeared in SemCor. We use
the same fine-grained system for both the coarse and
fine-grained all-words tasks. We make predictions
1http://www.cs.princeton.edu/?blei/lda-c/
251
for all words for all the systems - precision, recall
and accuracy scores are all the same.
Baseline features For lexical sample task, we
choose P = 3 and G = 3. For all-words task, we
choose P = 3 and G = 1. (G = 1 means only the
nearest word prior and after the test word.)
Smoothing For all standard baseline features, we
use Laplace smoothing but for the soft tag (equation
(2)), we use a smoothing parameter value of 2 for
all-words task and 0.1 for lexical sample task.
Unlabeled Corpus Selection The unlabeled cor-
pus we select from for LDA training include 20
Newsgroups, Reuters, SemCor, Senseval-2 lexical
sample data, Senseval-3 lexical sample data and
SemEval-1 lexical sample data. Although the last
four are labeled corpora, we only need the words
from these corpora and thus they can be regarded as
unlabeled too. For lexical sample data, we define the
whole passage for each training and testing instance
as one document.
For lexical sample task, we use all the unlabeled
corpus mentioned with K = 60 and L = 18. For
all-words task, we use a corpora consisting only 20
Newsgroups and SemCor with K = 40 and L = 14.
Validation Result Table 2 shows the results we
get on the validation sets. We give both the system
accuracy (named as Soft Tag) and the na??ve Bayes
result with only standard features as baseline.
Validation Set Soft Tag NB baseline
SE-2 All-words 66.3 63.7
SE-3 All-words 66.1 64.6
Lexical Sample 89.3 87.9
Table 2: Validation set results (best configuration).
5 Official Results
We now present the official results on all three tasks
we participated in, summarized in Table 3.
The system ranked first, fourth and second in
the lexical sample task, fine-grained all-words task
and coarse-grained all-words task respectively. For
coarse-grained all-words task, we obtained 86.1,
88.3, 81.4, 76.7 and 79.1 for each document, from
d001 to d005.
Task Precision/Recall
Lexical sample(Task 17) 88.7
Fine-grained all-words(Task 17) 57.6
Course-grained all-words(Task 7) 81.6
Table 3: Official Results
5.1 Analysis of Results
For the lexical sample task, we compare the re-
sults to that of our na??ve Bayes baseline and Sup-
port Vector Machine (SVM) (Vapnik, 1995) base-
line. Our SVM classifier (using SVMlight) follows
that of (Lee and Ng, 2002), which ranked the third
in Senseval-3 English lexical sample task. We also
analyse the result according to the test instance?s
part-of-speech and find that the improvements are
consistent for both noun and verb.
System Noun Verb Total
Soft Tag 92.7 84.2 88.7
NB baseline 91.7 83.5 87.8
SVM baseline 91.6 83.1 87.6
Table 4: Analysis on different POS on English lexi-
cal sample task
Our coarse-grained all-words task result outper-
formed the first sense baseline score of 0.7889 by
about 2.7%.
References
Y. K. Lee and H. T. Ng. 2002. An Empirical Evaluation
of Knowledge Sources and Learning Algorithms for
Word Sense Disambiguation. In Proc. of EMNLP.
D. M. Blei and A. Y. Ng and M. I. Jordan. 2003. La-
tent Dirichlet Allocation. Journal of Machine Learn-
ing Research.
A. Ratnaparkhi 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Proc. of EMNLP.
E. Charniak 2000. A Maximum-Entropy-Inspired
Parser. In Proc. of the 1st Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
V. N. Vapnik 1995. The Nature of Statistical Learning
Theory. Springer-Verlag, New York.
252
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, page 219,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Bayesian Tools for Natural Language Learning
Invited talk
Yee Whye Teh
Gatsby Computational Neuroscience Unit, UCL
ywteh@gatsby.ucl.ac.uk
In recent years Bayesian techniques have made good inroads in computational linguistics, due to their pro-
tection against overfitting and expressiveness of the Bayesian modeling language. However most Bayesian
models proposed so far have used pretty simple prior distributions, chosen more for computational conve-
nience than as reflections of real prior knowledge.
In this talk I will propose that prior distributions can be powerful ways to put computational linguis-
tics knowledge into your models, and give two examples from my own work. Firstly, hierarchical priors
can allow you to specify relationships among different components of your model so that the information
learned in one component can be shared with the rest, improving the estimation of parameters for all. Sec-
ondly, newer distributions like Pitman-Yor processes have interesting power-law characteristics that if used
as prior distributions can allow your linguistic models to express Zipf?s Law and Heap?s Law.
I will round up the talk with a discussion of the viability of the Bayesian approach, in a future where
we have too much data, making the natural language learning problem more a computational rather than a
statistical one.
219
