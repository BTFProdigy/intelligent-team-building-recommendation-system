Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 161?170, Prague, June 2007. c?2007 Association for Computational Linguistics
LEDIR: An Unsupervised Algorithm for Learning Directionality of Inference Rules 
Rahul Bhagat, Patrick Pantel, Eduard Hovy Information Sciences Institute University of Southern California Marina del Rey, CA {rahul,pantel,hovy}@isi.edu   Abstract Semantic inference is a core component of many natural language applications. In re-sponse, several researchers have developed algorithms for automatically learning infer-ence rules from textual corpora. However, these rules are often either imprecise or un-derspecified in directionality. In this paper we propose an algorithm called LEDIR that filters incorrect inference rules and identi-fies the directionality of correct ones. Based on an extension to Harris?s distribu-tional hypothesis, we use selectional pref-erences to gather evidence of inference di-rectionality and plausibility. Experiments show empirical evidence that our approach can classify inference rules significantly better than several baselines. 1 Introduction Paraphrases are textual expressions that convey the same meaning using different surface forms. Tex-tual entailment is a similar phenomenon, in which the presence of one expression licenses the validity of another. Paraphrases and inference rules are known to improve performance in various NLP applications like Question Answering (Harabagiu and Hickl 2006), summarization (Barzilay et al 1999) and Information Retrieval (Anick and Tipir-neni 1999).  Paraphrase and entailment involve inference rules that license a conclusion when a premise is given.  Deciding whether a proposed inference rule is fully valid is difficult, however, and most NL systems instead focus on plausible inference.  In this case, one statement has some likelihood of 
being identical in meaning to, or derivable from, the other.  In the rest of this paper we discuss plau-sible inference only.   Given the importance of inference, several re-searchers have developed inference rule collec-tions. While manually built resources like Word-Net (Fellbaum 1998) and Cyc (Lenat 1995) have been around for years, for coverage and domain adaptability reasons many recent approaches have focused on automatic acquisition of paraphrases (Barzilay and McKeown 2001) and inference rules (Lin and Pantel 2001; Szpektor et al 2004). The downside of these approaches is that they often result in incorrect inference rules or in inference rules that are underspecified in directionality (i.e. asymmetric but are wrongly considered symmet-ric). For example, consider an inference rule from DIRT (Lin and Pantel 2001): X eats Y ? X likes Y  (1)   All rules in DIRT are considered symmetric. Though here, one is most likely to infer that ?X eats Y? ? ?X likes Y?, because if someone eats something, he most probably likes it1, but if he likes something he might not necessarily be able to eat it. So for example, given the sentence ?I eat spicy food?, one is mostly likely to infer that ?I like spicy food?. On the other hand, given the sentence ?I like rollerblading?, one cannot infer that ?I eat rollerblading?. In this paper, we propose an algorithm called LEDIR (pronounced ?leader?) for LEarning Di-rectionality of Inference Rules. Our algorithm fil-ters incorrect inference rules and identifies the di-rectionality of the correct ones. Our algorithm                                                 1 There could be certain usages of ?X eats Y? where, one might not be able to infer ?X likes Y? (for example meta-phorical). But, in most cases, this inference holds. 
161
works with any resource that produces inference rules of the form shown in example (1). We use both the distributional hypothesis and selectional preferences as the basis for our algorithm. We pro-vide empirical evidence to validate the following main contribution:  Claim: Relational selectional preferences can be used to automatically determine the plausibility and directionality of an inference rule. 2 Related Work In this section, we describe applications that can benefit by using inference rules and their direc-tionality.  We then talk about some previous work in this area. 2.1 Applications Open domain question answering approaches often cast QA as the problem of finding some kind of semantic inference between a question and its an-swer(s) (Moldovan et al 2003; Echiabi and Marcu 2003). Harabagiu and Hickl (2006) recently dem-onstrated that textual entailment inference informa-tion, which in this system is a set of directional inference relations, improves the performance of a QA system significantly even without using any other form of semantic inference. This evidence supports the idea that learning the directionality of other sets of inference rules may improve QA per-formance.  In Multi-Document Summarization (MDS), paraphrasing is useful for determining sentences that have similar meanings (Barzilay et al 1999). Knowing the directionality between the inference rules here could allow the MDS system to choose either the more specific or general sentence de-pending on the purpose of the summary. In IR, paraphrases have been used for query ex-pansion, which is known to promote effective re-trieval (Anick and Tipirneni 1999). Knowing the directionality of rules here could help in making a query more general or specific depending on the user needs. 2.2 Learning Inference Rules Automatically learning paraphrases and inference rules from text is a topic that has received much attention lately. Barzilay and McKeown (2001) for paraphrases, DIRT (Lin and Pantel 2001) and TEASE (Szpektor et al 2004) for inference rules, 
are recent approaches that have achieved promis-ing results. While all these approaches produce collections of inference rules that have good recall, they suffer from the complementary problem of low precision. They also make no attempt to dis-tinguish between symmetric and asymmetric infer-ence rules. Given the potential positive impact shown in Section 2.1 of learning the directionality of inference rules, there is a need for methods, such as the one we present, to improve existing automatically created resources. 2.3 Learning Directionality There have been a few approaches at learning the directionality of restricted sets of semantic rela-tions, mostly between verbs. Chklovski and Pantel (2004) used lexico-syntactic patterns over the Web to detect certain types of symmetric and asymmet-ric relations between verbs. They manually exam-ined and obtained lexico-syntactic patterns that help identify the types of relations they considered and used these lexico-syntactic patterns over the Web to detect these relations among a set of candi-date verb pairs. Their approach however is limited only to verbs and to specific types of verb-verb relations. Zanzotto et al (2006) explored a selectional preference-based approach to learn asymmetric inference rules between verbs. They used the selec-tional preferences of a single verb, i.e. the semantic types of a verb?s arguments, to infer an asymmetric inference between the verb and the verb form of its argument type. Their approach however applies also only to verbs and is limited to some specific types of verb-argument pairs. Torisawa (2006) presented a method to acquire inference rules with temporal constraints, between verbs. They used co-occurrences between verbs in Japanese coordinated sentences and co-occurrences between verbs and nouns to learn the verb-verb inference rules. Like the previous two methods, their approach too deals only with verbs and is lim-ited to learning inference rules that are temporal in nature. Geffet and Dagan (2005) proposed an extension to the distributional hypothesis to discover entail-ment relation between words. They model the con-text of a word using its syntactic features and com-pare the contexts of two words for strict inclusion to infer lexical entailment. In principle, their work is the most similar to ours. Their method however 
162
is limited to lexical entailment and they show its effectiveness for nouns. Our method on the other hand deals with inference rules between binary relations and includes inference rules between ver-bal relations, non-verbal relations and multi-word relations. Our definition of context and the meth-odology for obtaining context similarity and over-lap is also much different from theirs. 3 Learning Directionality of Inference Rules The aim of this paper is to filter out incorrect infer-ence rules and to identify the directionality of the correct ones. Let pi ? pj be an inference rule where each p is a binary semantic relation between two entities x and y. Let <x, p, y> be an instance of relation p. Formal problem definition: Given the inference rule pi ? pj, we want to conclude which one of the following is more appropriate: 1. pi ? pj 2. pi ? pj 3. pi ? pj 4. No plausible inference Consider the example (1) from section 1. There, it is most plausible to conclude  ?X eats Y? ? ?X likes Y?.  Our algorithm LEDIR uses selectional prefer-ences along the lines of Resnik (1996) and Pantel et al (2007) to determine the plausibility and di-rectionality of inference rules. 3.1 Underlying Assumption Many approaches to modeling lexical semantics have relied on the distributional hypothesis (Harris 1954), which states that words that appear in the same contexts tend to have similar meanings. The idea is that context is a good indicator of a word meaning. Lin and Pantel (2001) proposed an exten-sion to the distributional hypothesis and applied it to paths in dependency trees, where if two paths tend to occur in similar contexts it is hypothesized that the meanings of the paths tend to be similar. In this paper, we assume and propose a further extension to the distributional hypothesis and call it the ?Directionality Hypothesis?. Directionality Hypothesis: If two binary semantic relations tend to occur in similar contexts and the first one occurs in significantly more contexts than 
the second, then the second most likely implies the first and not vice versa. The intuition here is that of generality. The more general a relation, more the types (and number) of contexts in which it is likely to appear. Consider the example (1) from section 1. The fact is that there are many more things that someone might like than those that someone might eat. Hence, by applying the directionality hypothesis, one can in-fer that ?X eats Y? ? ?X likes Y?. The key to applying the distributional hypothe-sis to the problem at hand is to model the contexts appropriately and to introduce a measure for calcu-lating context similarity. Concepts in semantic space, due to their abstractive power, are much richer for reasoning about inferences than simple surface words. Hence, we model the context of a relation p of the form <x, p, y> by using the seman-tic classes C(x) and C(y) of words that can be in-stantiated for x and y respectively. To measure context similarity of two relations, we calculate the overlap coefficient (Manning and Sch?tze, 1999) between their contexts. 3.2 Selectional Preferences The selectional preferences of a predicate is the set of semantic classes that its arguments can belong to (Wilks 1975). Resnik (1996) gave an informa-tion theoretical formulation of the idea. Pantel et al (2007) extended this idea to non-verbal rela-tions by defining the relational selectional prefer-ences (RSPs) of a binary relation p as the set of semantic classes C(x) and C(y) of words that can occur in positions x and y respectively. The set of semantic classes C(x) and C(y) can be obtained either from a manually created taxonomy like WordNet as proposed in the above previous approaches or by using automatically generated classes from the output of a word clustering algo-rithm as proposed in Pantel et al (2007). For ex-ample given a relation like ?X likes Y?, its RSPs from WordNet could be {individual, so-cial_group?} for X and {individual, food, activ-ity?} for Y. In this paper, we deployed both the Joint Rela-tional Model (JRM) and Independent Relational Model (IRM) proposed by Pantel et al (2007) to obtain the selectional preferences for a relation p.   
163
Model 1: Joint Relational Model (JRM) The JRM uses a large corpus to learn the selec-tional preferences of a binary semantic relation by considering its arguments jointly. Given a relation p and large corpus of English text, we first find all occurrences of relation p in the corpus. For every instance <x, p, y> in the cor-pus, we obtain the sets C(x) and C(y) of the seman-tic classes that x and y belong to. We then accumu-late the frequencies of the triples <c(x), p, c(y)> by assuming that every c(x) ? C(x) can co-occur with every  c(y) ? C(y) and vice versa. Every triple <c(x), p, c(y)> obtained in this manner is a candi-date selectional preference for p. Following Pantel et al (2007), we rank these candidates using Pointwise mutual information (Cover and Thomas 1991). The ranking function is defined as the strength of association between two semantic classes, cx and cy2, given the relation p: 
? 
pmi c
x
p; c
y
p
( )
= log
P c
x
,c
y
p
( )
P c
x
p
( )
P c
y
p
( )
                   (3.1) 
Let |cx, p, cy| denote the frequency of observing the instance <c(x), p, c(y)>. We estimate the prob-abilities of Equation 3.1 using maximum likeli-hood estimates over our corpus: 
? 
P c
x
p
( )
=
c
x
, p,?
?, p,?
P c
y
p
( )
=
?, p,c
y
?, p,?
P c
x
,c
y
p
( )
=
c
x
, p,c
y
?, p,?
                 (3.2) 
We estimate the above frequencies using: 
  
? 
c
x
, p,? =
w, p,?
C w
( )
w?c
x
?
?, p,c
y
=
?, p,w
C w
( )
w?c
y
?
c
x
, p,c
y
=
w
1
, p,w
2
C w
1
( )
? C w
2
( )
w
1
?c
x
,w
2
?c
y
?
       (3.3) 
where |x, p, y| denotes the frequency of observing the instance <x, p, y> and |C(w)| denotes the num-ber of classes to which word w belongs. |C(w)| dis-tributes w?s mass equally among all of its senses C(w). Model 2: Independent Relational Model (IRM) Due to sparse data, the JRM is likely to miss some pair(s) of valid relational selectional preferences. Hence we use the IRM, which models the argu-ments of a binary semantic relation independently.                                                 2 cx and cy are shorthand for c(x) and c(y) in our equations. 
Similar to JRM, we find all instances of the form <x, p, y> for a relation p. We then find the sets C(x) and C(y) of the semantic classes that x and y belong to and accumulate the frequencies of the triples <c(x), p, *> and <*, p, c(y)> where c(x) ? C(x) and  c(y) ? C(y). All the tuples <c(x), p, *> and <*, p, c(y)> are the independent candidate RSPs for a relation p and we rank them according to equation 3.3. Once we have the independently learnt RSPs, we need to convert them into a joint representation for use by the inference plausibility and direction-ality model. To do this, we obtain the Cartesian product between the sets <C(x), p, *>  and <*, p, C(y)> for a relation p. The Cartesian product be-tween two sets A and B is given by: 
?
A ? B = a,b
( )
:?a? A and ?b? B
{ }
        (3.4) Similarly we obtain: 
? 
C
x
, p,? ? ?, p,C
y
=
c
x
, p,c
y
: ? c
x
, p,? ? C
x
, p,? and
? ?, p,c
y
? ?, p,C
y
? 
? 
? 
? 
? 
? 
? 
? 
? 
? 
 (3.5) 
The Cartesian product in equation 3.5 gives the joint representation of the RSPs of the relation p learnt using IRM. In the joint representation, the IRM RSPs have the form <c(x), p, c(y)>  which is the same form as the JRM RSPs. 3.3 Inference plausibility and directionality model Our model for determining inference plausibility and directionality is based on the intuition that for an inference to hold between two semantic rela-tions there must exist sufficient overlap between their contexts and the directionality of the infer-ence depends on the quantitative comparison be-tween their contexts. Here we model the context of a relation by the selectional preferences of that relation. We deter-mine the plausibility of an inference based on the overlap coefficient (Manning and Sch?tze, 1999) between the selectional preferences of the two paths. We determine the directionality based on the difference in the number of selectional preferences of the relations when the inference seems plausi-ble.  Given a candidate inference rule pi ? pj, we first obtain the RSPs <C(x), pi, C(y)>  for pi and <C(x), pj, C(y)> for pj.  We then calculate the over-lap coefficient between their respective RSPs. Overlap coefficient is one of the many distribu-
164
tional similarity measures used to calculate the similarity between two vectors A and B: 
? 
sim A,B
( )
=
A? B
min A , B
( )
           (3.6) 
The overlap coefficient between the selectional preferences of pi and pj is calculated as: 
? 
sim p
i
, p
j
( )
=
C
x
, p
i
,C
y
? C
x
, p
j
,C
y
min C
x
, p
i
,C
y
,C
x
, p
j
,C
y
( )
          (3.7) 
If sim(pi,pj) is above a certain empirically de-termined threshold ? (?1), we conclude that the inference is plausible, i.e.: If  
? 
sim p
i
,p
j
( )
??  we conclude the inference is plausible else  we conclude the inference is not plausible For a plausible inference, we then compute the ratio between the number of selectional prefer-ences |C(x), pi, C(y)|  for pi and |C(x), pj, C(y)| for pj and compare it against an empirically determined threshold ? (?1) to determine the direction of in-ference. So the algorithm is: If   
? 
C
x
, p
i
,C
y
C
x
, p
j
,C
y
? ?       we conclude pi ? pj 
else if  
? 
C
x
, p
i
,C
y
C
x
, p
j
,C
y
?
1?     we conclude pi ? pj else                 we conclude pi ? pj 4 Experimental Setup In this section, we describe our experimental setup to validate our claim that LEDIR can be used to determine plausibility and directionality of an in-ference rule. Given an inference rule of the form pi ? pj, we want to use automatically learned relational selec-tional preferences to determine whether the infer-ence rule is valid and if it is valid then what its di-rectionality is.  4.1 Inference Rules LEDIR can work with any set of binary semantic inference rules. For the purpose of this paper, we chose the inference rules from the DIRT resource (Lin and Pantel 2001). DIRT consists of 12 million rules extracted from 1GB of newspaper text (AP Newswire, San Jose Mercury and Wall Street 
Journal). For example, ?X eats Y? ? ?X likes Y? is an inference rule from DIRT. 4.2 Semantic Classes Appropriate choice of semantic classes is crucial for learning relational selectional preferences. The ideal set should have semantic classes that have the right balance between abstraction and discrimina-tion, the two important characteristics that are of-ten conflicting. A very general class has limited discriminative power, while a very specific class has limited abstractive power. Finding the right balance here is a separate research problem of its own. Since the ideal set of universally acceptable se-mantic classes in unavailable, we decided to use the Pantel et al (2007) approach of using two sets of semantic classes. This approach gave us the ad-vantage of being able to experiment with sets of classes that vary a lot in the way they are generated but try to maintain the granularity by obtaining approximately the same number of classes. The first set of semantic classes was obtained by running the CBC clustering algorithm (Pantel and Lin, 2002) on TREC-9 and TREC-2002 newswire collections consisting of over 600 million words. This resulted in 1628 clusters, each representing a semantic class. The second set of semantic classes was obtained by using WordNet 2.1 (Fellbaum 1998). We ob-tained a cut in the WordNet noun hierarchy3 by manual inspection and used all the synsets below a cut point as the semantic class at that node. Our inspection showed that the synsets at depth four formed the most natural semantic classes4. A cut at depth four resulted in a set of 1287 semantic classes, a set that is much coarser grained than WordNet which has an average depth of 12. This seems to be a depth that gives a reasonable abstrac-tion while maintaining good discriminative power. It would however be interesting to experiment with more sophisticated algorithms for extracting se-mantic classes from WordNet and see their effect 
                                                3 Since we are dealing with only noun binary relations, we use only WordNet noun Hierarchy. 4 By natural, here, we simply mean that a manual inspection by the authors showed that, at depth four, the resulting clus-ters had struck a better granularity balance than other cutoff points. We acknowledge that this is a very coarse way of ex-tracting concepts from WordNet. 
165
on the relational selectional preferences, something we do not address this in this paper. 4.3 Implementation We implemented LEDIR with both the JRM and IRM models using inference rules from DIRT and semantic classes from both CBC and WordNet. We parsed the 1999 AP newswire collection consisting of 31 million words with Minipar (Lin 1993) and used this to obtain the probability statistics for the models (as described in section 3.2).  We performed both system-wide evaluations and intrinsic evaluations with different values of ? and ? parameters. Section 5 presents these results and our error analysis. 4.4 Gold Standard Construction In order to evaluate the performance of the differ-ent systems, we compare their outputs against a manually annotated gold standard. To create this gold standard, we randomly sampled 160 inference rules of the form pi ? pj from DIRT. We discarded three rules since they contained nominalizations5.  For every inference rule of the form pi ? pj, the annotation guideline asked annotators (in this pa-per we used two annotators) to choose the most appropriate of the four options: 1. pi ? pj 2. pi ? pj 3. pi ? pj 4. No plausible inference To help the annotators with their decisions, the annotators were provided with 10 randomly chosen instances for each inference rule. These instances, extracted from DIRT, provided the annotators with context where the inference could hold. So for ex-ample, for the inference rule ?X eats Y? ? ?X likes Y?, an example instance would be ?I eat spicy food? ? ?I like spicy food?. The annotation guide-line however gave the annotators the freedom to think of examples other than the ones provided to make their decisions. The annotators found that while some decisions were quite easy to make, the more complex ones                                                 5 For the purpose of simplicity, we in our experiments did not use DIRT rules containing nominalizations. The algo-rithm however can be applied without change to inference rules containing nominalization. In fact, in the resource that we plan to release soon, we have applied the algorithm without change to DIRT rules containing nominalizations. 
often involved the choice between bi-directionality and one of the directions. To minimize disagree-ments and to get a better understanding of the task, the annotators trained themselves by annotating several samples together. We divided the set of 157 inference rules, into a development set of 57 inference rules and a blind test set of 100 inference rules. Our two annotators annotated the development test set together to train themselves. The blind test set was then annotated individually to test whether the task is well de-fined. We used the kappa statistic (Siegel and Castellan Jr. 1988) to calculate the inter-annotator agreement, resulting in ?=0.63. The annotators then looked at the disagreements together to build the final gold standard. All this resulted in a final gold standard of 100 annotated DIRT rules. 4.5 Baselines To get an objective assessment of the quality of the results obtained by using our models, we compared the output of our systems against three baselines: B-random: Randomly assigns one of the four pos-sible tags to each candidate inference rule.  B-frequent: Assigns the most frequently occurring tag in the gold standard to each candidate infer-ence rule. B-DIRT: Assumes each inference rule is bidirec-tional and assigns the bidirectional tag to each candidate inference rule. 5 Experimental Results In this section, we provide empirical evidence to validate our claim that the plausibility and direc-tionality of an inference rule can be determined using LEDIR. 5.1 Evaluation Criterion We want to measure the effectiveness of LEDIR for the task of determining the validity and direc-tionality of a set of inference rules. We follow the standard approach of reporting system accuracy by comparing system outputs on a test set with a manually created gold standard. Using the gold standard described in Section 4.4, we measure the accuracy of our systems using the following for-mula: 
166
erencesinput
erencestaggedcorrectly
Accuracy
inf
inf
=
 
5.2 Result Summary We ran all our algorithms with different parameter combinations on the development set (the 57 DIRT rules described in Section 4.4). This resulted in a total of 420 experiments on the development set. Based on these experiments, we used the accuracy statistic to obtain the best parameter combination for each of our four systems. We then used these parameter values to obtain the corresponding per-centage accuracies on the test set for each of the four systems. Model ?  ? Accuracy (%) B-random - - 25 B-frequent - - 34 B-DIRT - - 25 CBC 0.15 2 38 JRM WN 0.55 2 38 CBC 0.15 3 48 IRM WN 0.45 2 43 Table 1: Summary of results on the test set Table 1 summarizes the results obtained on the test set for the three baselines and for each of the four systems using the best parameter combina-tions obtained as described above. The overall best performing system uses the IRM algorithm with RSPs form CBC. Its performance is found to be significantly better than all the three baselines us-ing the Student?s paired t-test (Manning and Sch?tze, 1999) at p<0.05. However, this system is not statistically significant when compared with the other LEDIR implementations (JRM and IRM with WordNet). 5.3 Performance and Error Analysis The best performing system selected using the de-velopment set is the IRM system using CBC with the parameters ?=0.15 and ?=3. In general, the results obtained on the test set show that the IRM tends to perform better than the JRM. This obser-vation points at the sparseness of data available for learning RSPs for the more restrictive JRM, the reason why we introduced the IRM in the first place. A much larger corpus would be needed to obtain good enough coverage for the JRM. 
GOLD STANDARD  ? ? ? NO ? 16 1 3 7 ? 0 3 1 3 ? 7 4 22 15 SYST
EM 
NO 2 3 4 9 Table 2: Confusion Matrix for the best performing system, IRM using CBC with ?=0.15 and ?=3. Table 2 shows the confusion matrix for the overall best performing system as selected using the development set (results are taken from the test set). The confusion matrix indicates that the system does a very good job of identifying the directional-ity of the correct inference rules, but gets a big per-formance hit from its inability to identify the incor-rect inference rules accurately. We will analyze this observation in more detail below. Figure 1 plots the variation in accuracy of IRM with different RSPs and different values of ? and ?. The figure shows a very interesting trend.  It is clear that for all values of ?, systems for IRM us-ing CBC tend to reach their peak in the range 0.15 ? ? ? 0.25, whereas the systems for IRM using WordNet (WN), tend to reach their peak in the range 0.4 ? ? ? 0.6. This variation indicates the kind of impact the selection of semantic classes could have on the overall performance of the sys-tem. This is not hard evidence, but it does suggest that finding the right set of semantic classes could be one big step towards improving system accu-racy. 
 Figure 1: Accuracy variation for IRM with differ-ent values of ? and ?. Two other factors that have a big impact on the performance of our systems are the values of the system parameters ? and ?, which decide the plau-
167
sibility and directionality of an inference rule, re-spectively. To better study their effect on the sys-tem performances, we studied the two parameters independently. 
 Figure 2: Accuracy variation in predicting correct versus incorrect inference rules for different values of ?. 
 Figure 3: Accuracy variation in predicting direc-tionality of correct inference rules for different values of ?. Figure 2 shows the variation in the accuracy for the task of predicting the correct and incorrect in-ference rules for the different systems when vary-ing the value of ?. To obtain this graph, we classi-fied the inference rules in the test set only as cor-rect and incorrect without further classification based on directionality. All of our four systems obtained accuracy scores in the range of 68-70% showing a good performance on the task of deter-mining plausibility. This however is only a small improvement over the baseline score of 66% ob-tained by assuming every inference to be plausible (as will be shown below, our system has most im-pact not on determining plausibility but on deter-
mining directionality). Manual inspection of some system errors showed that the most common errors were due to the well-known ?problem of an-tonymy? when applying the distributional hypothe-sis. In DIRT, one can learn rules like ?X loves Y? ? ?X hates Y?. Since the plausibility of inference rules is determined by applying the distributional hypothesis and the antonym paths tend to take the same set of classes for X and Y, our models find it difficult to filter out the incorrect inference rules which DIRT ends up learning for this very same reason. To improve our system, one avenue of re-search is to focus specifically on filtering incorrect inference rules involving antonyms (perhaps using methods similar to (Lin et al 2003)). Figure 3 shows the variation in the accuracy for the task of predicting the directionality of the cor-rect inference rules for the different systems when varying the value of ?.  To obtain this graph, we separated the correct inference rules form the in-correct ones and ran all the systems on only the correct ones, predicting only the directionality of each rule for different values of ?. Too low a value of ? means that the algorithms tend to predict most things as unidirectional and too high a value means that the algorithms tend to predict everything as bidirectional. It is clear from the figure that the performance of all the systems reach their peak performance in the range 2 ? ? ? 4, which agrees with our intuition of obtaining the best system ac-curacy in a medium range. It is also seen that the best accuracy for each of the models goes up as compared to the corresponding values obtained in the general framework. The best performing sys-tem, IRM using CBC RSPs, reaches a peak accu-racy of 63.64%, a much higher score than its accu-racy score of 48% under the general framework and also a significant improvement over the base-line score of 48.48% for this task. Paired t-test shows that the difference is statistically significant at p<0.05. The baseline score for this task is ob-tained by assigning the most frequently occurring direction to all the correct inference rules. This paints a very encouraging picture about the ability of the algorithm to identify the directionality much more accurately if it can be provided with a cleaner set of inference rules. 
168
6 Conclusion Semantic inferences are fundamental to under-standing natural language and are an integral part of many natural language applications such as question answering, summarization and textual entailment. Given the availability of large amounts of text and with the increase in computation power, learning them automatically from large text cor-pora has become increasingly feasible and popular. We introduced the Directionality Hypothesis, which states that if two paths share a significant number of relational selectional preferences (RSPs) and if the first has many more RSPs than the second, then the second path implies the first. Our experiments show empirical evidence that the Directionality Hypothesis with RSPs can indeed be used to filter incorrect inference rules and find the directionality of correct ones. We believe that this result is one step in the direction of solving the basic problem of semantic inference. Several questions must still be addressed. The models need to be improved in order to address the problem of incorrect inference rules. The distribu-tional hypothesis does not provide a framework to address the issue with antonymy relations like ?X loves Y? ? ?X hates Y? and hence other ideas need to be investigated. Ultimately, our goal is to improve the perform-ance of NLP applications with better inferencing capabilities. Several recent data points, such as  (Harabagiu and Hickl 2006), and others discussed in Section 2.1, give promise that refined inference rules for directionality may indeed improve ques-tion answering, textual entailment and multi-document summarization accuracies. It is our hope that methods such as the one proposed in this paper may one day be used to harness the richness of automatically created inference rule resources within large-scale NLP applications. References Anick, P.G. and Tipirneni, S. 1999. The Paraphrase Search Assistant: Terminology Feedback for Iterative Information Seeking. In Proceedings of SIGIR 1999. pp. 53-159. Berkeley, CA Barzilay, R. and McKeown, K.R. 2001.Extracting Para-phrases from a Parallel Corpus. In Proceedings of ACL 2001. pp. 50?57. Toulose, France. 
Barzilay, R.; McKeown, K.R. and Elhadad, M. 1999. Information Fusion in the Context of Multi-Document Summarization. In Proceedings of ACL 1999. College Park, Maryland. Chklovski, T. and Pantel, P. 2004. VerbOCEAN: Min-ing the Web for Fine-Grained Semantic Verb Rela-tions. In Proceedings of EMNLP 2004. Barcellona Spain. Cover, T.M. and Thomas, J.A. 1991. Elements of Infor-mation Theory. John Wiley & Sons. Echihabi, A. and Marcu. D. 2003. A Noisy-Channel Approach to Question Answering. In Proceedings of ACL 2003. Sapporo, Japan. Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT Press. Geffet, M.; Dagan, I. 2005. The Distributional Inclusion Hypothesis and Lexical Entailment. In Proceedings of ACL 2005. pp. 107-114. Ann Arbor, Michigan. Harabagiu, S.; and Hickl, A. 2006. Methods for Using Textual Entailment in Open-Domain Question An-swering. In Proceedings of ACL 2006.  pp. 905-912. Sydney, Australia. Harris, Z. 1954. Distributional structure. Word. 10(23): 146-162. Lenat, D. 1995. CYC: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11):33?38. Lin, D. 1993. Parsing Without OverGeneration. In Pro-ceedings of  ACL 1993. pp. 112-120. Columbus, OH. Lin, D. and Pantel, P. 2001. Discovery of Inference Rules for Question Answering. Natural Language Engineering 7(4):343-360. Lin, D.; Zhao, S.; Qin, L. and Zhou, M. 2003. Identify-ing Synonyms among Distributionally Similar Words. In Proceedings of IJCAI 2003, pp. 1492-1493. Acapulco, Mexico. Manning, C.D. and Sch?tze, H. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA. Moldovan, D.; Clark, C.; Harabagiu, S. and Maiorano S.  2003. COGEX: A Logic Prover for Question An-swering. In Proceedings of HLT/NAACL 2003. Ed-monton, Canada. Pantel, P.; Bhagat, R.; Coppola, B.; Chklovski, T. and Hovy, E. 2007. ISP: Learning Inferential Selectional Preferences. In Proceedings of HLT/NAACL 2007. Rochester, NY. 
169
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In Proceedings of KDD 2002. pp. 613-619. Edmonton, Canada. Resnik, P. 1996. Selectional Constraints: An Informa-tion-Theoretic Model and its Computational Realiza-tion. Cognition, 61:127?159. Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill. Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. Scaling web-based acquisition of entailment relations. In Proceedings of EMNLP 2004. pp. 41-48. Barce-lona, Spain. Torisawa, K. 2006. Acquiring Inference Rules with Temporal Constraints by Using Japanese Coordi-nated Sentences and Noun-Verb Co-occurances. In Proceedings of HLT/NAACL 2006. pp. 57-64. New York, New York. Wilks, Y. 1975. Preference Semantics.  In E.L. Keenan (ed.), Formal Semantics of Natural Language. Cam-bridge: Cambridge University Press. Zanzotto, F.M.; Pennacchiotti, M.; Pazienza, M.T. 2006. Discovering Asymmetric Entailment Relations between Verbs using Selectional Preferences. In Pro-ceedings of COLING/ACL 2006. pp. 849-856. Syd-ney, Australia.   
170
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 582?590,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Weakly-Supervised Acquisition of Labeled Class Instances using Graph
Random Walks
Partha Pratim Talukdar?
University of Pennsylvania
Philadelphia, PA 19104
partha@cis.upenn.edu
Joseph Reisinger?
University of Texas at Austin
Austin, TX 78712
joeraii@cs.utexas.edu
Marius Pas?ca
Google Inc.
Mountain View, CA 94043
mars@google.com
Deepak Ravichandran
Google Inc.
Mountain View, CA 94043
deepakr@google.com
Rahul Bhagat?
USC Information Sciences Institute
Marina Del Rey, CA 90292
rahul@isi.edu
Fernando Pereira
Google Inc.
Mountain View, CA 94043
pereira@google.com
Abstract
We present a graph-based semi-supervised la-
bel propagation algorithm for acquiring open-
domain labeled classes and their instances
from a combination of unstructured and struc-
tured text sources. This acquisition method
significantly improves coverage compared to
a previous set of labeled classes and instances
derived from free text, while achieving com-
parable precision.
1 Introduction
1.1 Motivation
Users of large document collections can readily ac-
quire information about the instances, classes, and
relationships described in the documents. Such rela-
tions play an important role in both natural language
understanding andWeb search, as illustrated by their
prominence in both Web documents and among the
search queries submitted most frequently by Web
users (Jansen et al, 2000). These observations moti-
vate our work on algorithms to extract instance-class
information from Web documents.
While work on named-entity recognition tradi-
tionally focuses on the acquisition and identifica-
tion of instances within a small set of coarse-grained
classes, the distribution of instances within query
logs indicates that Web search users are interested
in a wider range of more fine-grained classes. De-
pending on prior knowledge, personal interests and
immediate needs, users submit for example medi-
cal queries about the symptoms of leptospirosis or
?Contributions made during internships at Google.
the treatment of monkeypox, both of which are in-
stances of zoonotic diseases, or the risks and benefits
of surgical procedures such as PRK and angioplasty.
Other users may be more interested in African coun-
tries such as Uganda and Angola, or active volca-
noes like Etna and Kilauea. Note that zoonotic dis-
eases, surgical procedures, African countries and
active volcanoes serve as useful class labels that cap-
ture the semantics of the associated sets of class in-
stances. Such interest in a wide variety of specific
domains highlights the utility of constructing large
collections of fine-grained classes.
Comprehensive and accurate class-instance in-
formation is useful not only in search but also
in a variety of other text processing tasks includ-
ing co-reference resolution (McCarthy and Lehn-
ert, 1995), named entity recognition (Stevenson and
Gaizauskas, 2000) and seed-based information ex-
traction (Riloff and Jones, 1999).
1.2 Contributions
We study the acquisition of open-domain, labeled
classes and their instances from both structured
and unstructured textual data sources by combin-
ing and ranking individual extractions in a princi-
pled way with the Adsorption label-propagation al-
gorithm (Baluja et al, 2008), reviewed in Section 3
below.
A collection of labeled classes acquired from
text (Van Durme and Pas?ca, 2008) is extended in two
ways:
1. Class label coverage is increased by identify-
ing additional class labels (such as public agen-
cies and governmental agencies) for existing
582
instances such as Office of War Information),
2. The overall instance coverage is increased by
extracting additional instances (such as Addi-
son Wesley and Zebra Books) for existing class
labels (book publishers).
The WebTables database constructed by Cafarella
et al (2008) is used as the source of additional
instances. Evaluations on gold-standard labeled
classes and instances from existing linguistic re-
sources (Fellbaum, 1998) indicate coverage im-
provements relative to that of Van Durme and Pas?ca
(2008), while retaining similar precision levels.
2 First Phase Extractors
To show Adsorption?s ability to uniformly combine
extractions from multiple sources and methods, we
apply it to: 1) high-precision open-domain extrac-
tions from free Web text (Van Durme and Pas?ca,
2008), and 2) high-recall extractions from WebTa-
bles, a large database of HTML tables mined from
the Web (Cafarella et al, 2008). These two meth-
ods were chosen to be representative of two broad
classes of extraction sources: free text and structured
Web documents.
2.1 Extraction from Free Text
Van Durme and Pas?ca (2008) produce an open-
domain set of instance clusters C ? C that parti-
tions a given set of instances I using distributional
similarity (Lin and Pantel, 2002), and labels using
is-a patterns (Hearst, 1992). By filtering the class
labels using distributional similarity, a large number
of high-precision labeled clusters are extracted. The
algorithm proceeds iteratively: at each step, all clus-
ters are tested for label coherence and all coherent
labels are tested for high cluster specificity. Label
L is coherent if it is shared by at least J% of the
instances in cluster C, and it is specific if the total
number of other clusters C ? ? C, C ? 6= C containing
instances with label L is less thanK. When a cluster
is found to match these criteria, it is removed from
C and added to an output set. The procedure termi-
nates when no new clusters can be removed from C.
Table 1 shows a few randomly chosen classes and
representative instances obtained by this procedure.
2.2 Extraction from Structured Text
To expand the instance sets extracted from free
text, we use a table-based extraction method that
mines structured Web data in the form of HTML
tables. A significant fraction of the HTML ta-
bles in Web pages is assumed to contain coherent
lists of instances suitable for extraction. Identifying
such tables from scratch is hard, but seed instance
lists can be used to identify potentially coherent ta-
ble columns. In this paper we use the WebTables
database of around 154 million tables as our struc-
tured data source (Cafarella et al, 2008).
We employ a simple ranking scheme for candi-
date instances in the WebTables corpus T . Each ta-
ble T ? T consists of one or more columns. Each
column g ? T consists of a set of candidate in-
stances i ? g corresponding to row elements. We
define the set of unique seed matches in g relative to
semantic class C ? C as
MC(g)
def
= {i ? I(C) : i ? g}
where I(C) denotes the set of instances in seed class
C. For each column g, we define its ?-unique class
coverage, that is, the set of classes that have at least
? unique seeds in g,
Q(g;?)
def
= {C ? C : |MC(g)| ? ?}.
Using M and Q we define a method for scoring
columns relative to each class. Intuitively, such a
score should take into account not only the number
of matches from class C, but also the total num-
ber of classes that contribute to Q and their relative
overlap. Towards this end, we introduce the scoring
function
score(C, g;?)
def
= |MC(g)|
? ?? ?
seed matches
?
class coherence
? ?? ?
|MC(g)|
|
?
C??Q(g;?) I(C
?)|
which is the simplest scoring function combining
the number of seed matches with the coherence of
the table column. Coherence is a critical notion
in WebTables extraction, as some tables contain in-
stances across many diverse seed classes, contribut-
ing to extraction noise. The class coherence intro-
duced here also takes into account class overlap; that
583
Class Size Examples of Instances
Book Publishers 70 crown publishing, kluwer academic, prentice hall, puffin
Federal Agencies 161 catsa, dhs, dod, ex-im bank, fsis, iema, mema, nipc, nmfs, tdh, usdot
Mammals 956 armadillo, elephant shrews, long-tailed weasel, river otter, weddell seals, wild goat
NFL Players 180 aikman, deion sanders, fred taylor, jamal lewis, raghib ismail, troy vincent
Scientific Journals 265 biometrika, european economic review, nature genetics, neuroscience
Social Issues 210 gender inequality, lack of education, substandard housing, welfare dependency
Writers 5089 bronte sisters, hemingway, kipling, proust, torquato tasso, ungaretti, yeats
Table 1: A sample of the open-domain classes and associated instances from (Van Durme and Pas?ca, 2008).
is, a column containing many semantically similar
classes is penalized less than one containing diverse
classes.1 Finally, an extracted instance i is assigned
a score relative to class C equal to the sum of all its
column scores,
score(i, C;?)
def
=
1
ZC
?
g?T,T?T
score(C, g;?)
where ZC is a normalizing constant set to the max-
imum score of any instance in class C. This scor-
ing function assigns high rank to instances that oc-
cur frequently in columns with many seed matches
and high class specificity.
The ranked list of extracted instances is post-
filtered by removing all instances that occur in less
than d unique Internet domains.
3 Graph-Based Extraction
To combine the extractions from both free and struc-
tured text, we need a representation capable of en-
coding efficiently all the available information. We
chose a graph representation for the following rea-
sons:
? Graphs can represent complicated relationships
between classes and instances. For example,
an ambiguous instance such as Michael Jor-
dan could belong to the class of both Profes-
sors and NBA players. Similarly, an instance
may belong to multiple nodes in the hierarchy
of classes. For example, Blue Whales could be-
long to both classes Vertebrates and Mammals,
because Mammals are a subset of Vertebrates.
1Note that this scoring function does not take into account
class containment: if all seeds are both wind Instruments and
instruments, then the column should assign higher score to the
more specific class.
? Extractions frommultiple sources, such asWeb
queries, Web tables, and text patterns can be
represented in a single graph.
? Graphs make explicit the potential paths of in-
formation propagation that are implicit in the
more common local heuristics used for weakly-
supervised information extraction. For exam-
ple, if we know that the instance Bill Clinton
belongs to both classes President and Politician
then this should be treated as evidence that the
class of President and Politician are related.
Each instance-class pair (i, C) extracted in the
first phase (Section 2) is represented as a weighted
edge in a graph G = (V,E,W ), where V is the set
of nodes, E is the set of edges and W : E ? R+
is the weight function which assigns positive weight
to each edge. In particular, for each (i, C,w) triple
from the set of base extractions, i and C are added
to V and (i, C) is added to E, 2 with W (i, C) = w.
The weight w represents the total score of all extrac-
tions with that instance and class. Figure 1 illustrates
a portion of a sample graph. This simple graph rep-
resentation could be refined with additional types of
nodes and edges, as we discuss in Section 7.
In what follows, all nodes are treated in the same
way, regardless of whether they represent instances
or classes. In particular, all nodes can be assigned
class labels. For an instance node, that means that
the instance is hypothesized to belong to the class;
for a class node, that means that the node?s class is
hypothesized to be semantically similar to the label?s
class (Section 5).
We now formulate the task of assigning labels to
nodes as graph label propagation. We are given a
2In practice, we use two directed edges, from i to C and
from C to i, both with weight w.
584
bob dylan
musician
0.95
johnny cash
0.87
singer
0.73
billy joel
0.82
0.75
Figure 1: Section of a graph used as input into Adsorp-
tion. Though the nodes do not have any type associated
with them, for readability, instance nodes are marked in
pink while class nodes are shown in green.
set of instances I and a set of classes C represented
as nodes in the graph, with connecting edges as de-
scribed above. We annotate a few instance nodes
with labels drawn from C. That is, classes are used
both as nodes in the graph and as labels for nodes.
There is no necessary alignment between a class
node and any of the (class) labels, as the final labels
will be assigned by the Adsorption algorithm.
The Adsorption label propagation algo-
rithm (Baluja et al, 2008) is now applied to
the given graph. Adsorption is a general framework
for label propagation, consisting of a few nodes
annotated with labels and a rich graph structure
containing the universe of all labeled and unlabeled
nodes. Adsorption proceeds to label all nodes
based on the graph structure, ultimately producing a
probability distribution over labels for each node.
More specifically, Adsorption works on a graph
G = (V,E,W ) and computes for each node v a la-
bel distribution Lv that represents which labels are
more or less appropriate for that node. Several in-
terpretations of Adsorption-type algorithms have ap-
peared in various fields (Azran, 2007; Zhu et al,
2003; Szummer and Jaakkola, 2002; Indyk and Ma-
tousek, 2004). For details, the reader is referred to
(Baluja et al, 2008). We use two interpretations
here:
Adsorption through Random Walks: Let Gr =
(V,Er,Wr) be the edge-reversed version of the
original graph G = (V,E,W ) where (a, b) ?
Er iff (b, a) ? E; and Wr(a, b) = W (b, a).
Now, choose a node of interest q ? V . To es-
timate Lq for q, we perform a random walk on
Gr starting from q to generate values for a ran-
dom label variable L. After reaching a node v
during the walk, we have three choices:
1. With probability pcontv , continue the ran-
dom walk to a neighbor of v.
2. With probability pabndv , abandon the ran-
dom walk. This abandonment proba-
bility makes the random walk stay rela-
tively close to its source when the graph
has high-degree nodes. When the ran-
dom walk passes through such a node,
it is likely that further transitions will be
into regions of the graph unrelated to the
source. The abandonment probability mit-
igates that effect.
3. With probability pinjv , stop the random
walk and emit a label L from Iv.
Lq is set to the expectation of all labels L emit-
ted from random walks initiated from node q.
Adsorption through Averaging: For this interpre-
tation we make some changes to the original
graph structure and label set. We extend the la-
bel distributions Lv to assign a probability not
only to each label in C but also to the dummy
label ?, which represents lack of information
about the actual label(s). We represent the ini-
tial knowledge we have about some node labels
in an augmented graph G? = (V ?, E?,W ?) as
follows. For each v ? V , we define an ini-
tial distribution Iv = L?, where L? is the
dummy distribution with L?(?) = 1, repre-
senting lack of label information for v. In addi-
tion, let Vs ? V be the set of nodes for which
we have some actual label knowledge, and let
V ? = V ? {v? : v ? Vs}, E? = E ? {(v?, v) :
v ? Vs}, and W ?(v?, v) = 1 for v ? Vs,
W ?(u, v) = W (u, v) for u, v ? V . Finally,
let Iv? (seed labels) specify the knowledge about
possible labels for v ? Vs. Less formally, the
v? nodes in G? serve to inject into the graph the
prior label distributions for each v ? Vs.
The algorithm proceeds as follows: For each
node use a fixed-point computation to find label
585
distributions that are weighted averages of the
label distributions for all their neighbors. This
causes the non-dummy initial distribution of Vs
nodes to be propagated across the graph.
Baluja et al (2008) show that those two views are
equivalent. Algorithm 1 combines the two views:
instead of a random walk, for each node v, it itera-
tively computes the weighted average of label distri-
butions from neighboring nodes, and then uses the
random walk probabilities to estimate a new label
distribution for v.
For the experiments reported in Section 4, we
used the following heuristics from Baluja et al
(2008) to set the random walk probabilities:
? Let cv =
log ?
log(? + expH(v)) where H(v) =
?
?
u puv ? log(puv) with puv =
W (u,v)
P
u
? W (u
? ,v)
.
H(v) can be interpreted as the entropy of v?s
neighborhood. Thus, cv is lower if v has many
neighbors. We set ? = 2.
? jv = (1 ? cv) ?
?
H(v) if Iv 6= L> and 0
otherwise.
? Then let
zv = max(cv + jv, 1)
pcontv = cv/zv
pinjv = jv/zv
pabndv = 1? p
cont
v ? p
abnd
v
Thus, abandonment occurs only when the con-
tinuation and injection probabilities are low
enough.
The algorithm is run until convergence which is
achieved when the label distribution on each node
ceases to change within some tolerance value. Alter-
natively, the algorithm can be run for a fixed number
of iterations which is what we used in practice3.
Finally, since Adsorption is memoryless, it eas-
ily scales to tens of millions of nodes with dense
edges and can be easily parallelized, as described
by Baluja et al (2008).
3The number of iterations was set to 10 in the experiments
reported in this paper.
Algorithm 1 Adsorption Algorithm.
Input: G? = (V
?
, E
?
,W ?), Iv (?v ? V ?).
Output: Distributions {Lv : v ? V }.
1: Lv = Iv ?v ? V
?
2:
3: repeat
4: Nv =
?
u W (u, v)
5: Dv = 1Nv
?
u W (u, v)Lu ?v ? V
?
6: for all v ? V
?
do
7: Lv = pcontv ?Dv +p
inj
v ? Iv +pabndv ?L
>
8: end for
9: until convergence
4 Experiments
4.1 Data
As mentioned in Section 3, one of the benefits of
using Adsorption is that we can combine extrac-
tions by different methods from diverse sources into
a single framework. To demonstrate this capabil-
ity, we combine extractions from free-text patterns
and from Web tables. To the best of our knowl-
edge, this is one of the first attempts in the area of
minimally-supervised extraction algorithms where
unstructured and structured text are used in a prin-
cipled way within a single system.
Open-domain (instance, class) pairs were ex-
tracted by applying the method described by Van
Durme and Pas?ca (2008) on a corpus of over 100M
English web documents. A total of 924K (instance,
class) pairs were extracted, containing 263K unique
instances in 9081 classes. We refer to this dataset as
A8.
Using A8, an additional 74M unique (in-
stance,class) pairs are extracted from a random 10%
of the WebTables data, using the method outlined in
Section 2.2. For maximum coverage we set ? = 2
and d = 2, resulting in a large, but somewhat noisy
collection. We refer to this data set as WT.
4.2 Graph Creation
We applied the graph construction scheme described
in Section 3 on the A8 and WT data combined, re-
sulting in a graph with 1.4M nodes and 75M edges.
Since extractions in A8 are not scored, weight of all
586
Seed Class Seed Instances
Book Publishers millbrook press, academic press, springer verlag, chronicle books, shambhala publications
Federal Agencies dod, nsf, office of war information, tsa, fema
Mammals african wild dog, hyaena, hippopotamus, sperm whale, tiger
NFL Players ike hilliard, isaac bruce, torry holt, jon kitna, jamal lewis
Scientific Journals american journal of roentgenology, pnas, journal of bacteriology, american economic review,
ibm systems journal
Table 2: Classes and seeds used to initialize Adsorption.
edges originating from A8 were set at 14. This graph
is used in all subsequent experiments.
5 Evaluation
We evaluated the Adsorption algorithm under two
experimental settings. First, we evaluate Adsorp-
tion?s extraction precision on (instance, class) pairs
obtained by Adsorption but not present in A8 (Sec-
tion 5.1). This measures whether Adsorption can
add to the A8 extractions at fairly high precision.
Second, we measured Adsorption?s ability to assign
labels to a fixed set of gold instances drawn from
various classes (Section 5.2).
Book Publishers Federal Agencies NFL Players Scientific Journals Mammals20
40
60
80
100
 
 Adsorption A8
Book
Publishers
Federal
Agencies
NFL
Players
Scientific
Journals
Mammals
A8 Adsorption
Figure 2: Precision at 100 comparisons for A8 and Ad-
sorption.
5.1 Instance Precision
First we manually evaluated precision across five
randomly selected classes from A8: Book Publish-
ers, Federal Agencies, NFL Players, Scientific Jour-
nals and Mammals. For each class, 5 seed in-
stances were chosen manually to initialize Adsorp-
tion. These classes and seeds are shown in Table 2.
Adsorption was run for each class separately and the
4A8 extractions are assumed to be high-precision and hence
we assign them the highest possible weight.
resulting ranked extractions were manually evalu-
ated.
Since the A8 system does not produce ranked lists
of instances, we chose 100 random instances from
the A8 results to compare to the top 100 instances
produced by Adsorption. Each of the resulting 500
instance-class pairs (i, C) was presented to two hu-
man evaluators, who were asked to evaluate whether
the relation ?i is a C? was correct or incorrect. The
user was also presented with Web search link to ver-
ify the results against actual documents. Results
from these experiments are presented in Figure 2
and Table 4. The results in Figure 2 show that the
A8 system has higher precision than the Adsorption
system. This is not surprising since the A8 system is
tuned for high precision. When considering individ-
ual evaluation classes, changes in precision scores
between the A8 system and the Adsorption system
vary from a small increase from 87% to 89% for the
class Book Publishers, to a significant decrease from
52% to 34% for the class Federal Agencies, with a
decrease of 10% as an average over the 5 evaluation
classes.
Class Precision at 100
(non-A8 extractions)
Book Publishers 87.36
Federal Agencies 29.89
NFL Players 94.95
Scientific Journals 90.82
Mammal Species 84.27
Table 4: Precision of top 100 Adsorption extractions (for
five classes) which were not present in A8.
Table 4 shows the precision of the Adsorption sys-
tem for instances not extracted by the A8 system.
587
Seed Class Non-Seed Class Labels Discovered by Adsorption
Book Publishers small presses, journal publishers, educational publishers, academic publishers,
commercial publishers
Federal Agencies public agencies, governmental agencies, modulation schemes, private sources,
technical societies
NFL Players sports figures, football greats, football players, backs, quarterbacks
Scientific Journals prestigious journals, peer-reviewed journals, refereed journals, scholarly journals,
academic journals
Mammal Species marine mammal species, whale species, larger mammals, common animals, sea mammals
Table 3: Top class labels ranked by their similarity to a given seed class in Adsorption.
Seed Class Sample of Top Ranked Instances Discovered by Adsorption
Book Publishers small night shade books, house of anansi press, highwater books,
distributed art publishers, copper canyon press
NFL Players tony gonzales, thabiti davis, taylor stubblefield, ron dixon, rodney hannah
Scientific Journals journal of physics, nature structural and molecular biology,
sciences sociales et sante?, kidney and blood pressure research,
american journal of physiology?cell physiology
Table 5: Random examples of top ranked extractions (for three classes) found by Adsorption which were not present
in A8.
Such an evaluation is important as one of the main
motivations of the current work is to increase cov-
erage (recall) of existing high-precision extractors
without significantly affecting precision. Results in
Table 4 show that Adsorption is indeed able to ex-
traction with high precision (in 4 out of 5 cases)
new instance-class pairs which were not extracted
by the original high-precision extraction set (in this
case A8). Examples of a few such pairs are shown
in Table 5. This is promising as almost all state-
of-the-art extraction methods are high-precision and
low-recall. The proposed method shows a way to
overcome that limitation.
As noted in Section 3, Adsorption ignores node
type and hence the final ranked extraction may also
contain classes along with instances. Thus, in ad-
dition to finding new instances for classes, it also
finds additional class labels similar to the seed class
labels with which Adsorption was run, at no extra
cost. Some of the top ranked class labels extracted
by Adsorption for the corresponding seed class la-
bels are shown in Table 3. To the best of our knowl-
edge, there are no other systems which perform both
tasks simultaneously.
5.2 Class Label Recall
Next we evaluated each extraction method on its rel-
ative ability to assign labels to class instances. For
each test instance, the five most probably class la-
bels are collected using each method and the Mean
Reciprocal Rank (MRR) is computed relative to a
gold standard target set. This target set, WN-gold,
consists of the 38 classes in Wordnet containing 100
or more instances.
In order to extract meaningful output from Ad-
sorption, it is provided with a number of labeled seed
instances (1, 5, 10 or 25) from each of the 38 test
classes. Regardless of the actual number of seeds
used as input, all 25 seed instances from each class
are removed from the output set from all methods,
in order to ensure fair comparison.
The results from this evaluation are summarized
in Table 6; AD x refers to the adsorption run with x
seed instances. Overall, Adsorption exhibits higher
MRR than either of the baseline methods, with MRR
increasing as the amount of supervision is increased.
Due to its high coverage, WT assigns labels to
a larger number of the instance in WN-gold than
any other method. However, the average rank of
the correct class assignment is lower, resulting is
588
MRR MRR # found
Method (full) (found only)
A8 0.16 0.47 2718
WT 0.15 0.21 5747
AD 1 0.26 0.45 4687
AD 5 0.29 0.48 4687
AD 10 0.30 0.51 4687
AD 25 0.32 0.55 4687
Table 6: Mean-Reciprocal Rank scores of instance class
labels over 38 Wordnet classes (WN-gold). MRR (full)
refers to evaluation across the entire gold instance set.
MRR (found only) computes MRR only on recalled in-
stances.
lower MRR scores compared to Adsorption. This
result highlights Adsorption?s ability to effectively
combine high-precision, low-recall (A8) extractions
with low-precision, high-recall extractions (WT) in
a manner that improves both precision and coverage.
6 Related Work
Graph based algorithms for minimally supervised
information extraction methods have recently been
proposed. For example, Wang and Cohen (2007)
use a random walk on a graph built from entities and
relations extracted from semi-structured text. Our
work differs both conceptually, in terms of its focus
on open-domain extraction, as well as methodologi-
cally, as we incorporate both unstructured and struc-
tured text. The re-ranking algorithm of Bellare et al
(2007) also constructs a graph whose nodes are in-
stances and attributes, as opposed to instances and
classes here. Adsorption can be seen as a general-
ization of the method proposed in that paper.
7 Conclusion
The field of open-domain information extraction has
been driven by the growth of Web-accessible data.
We have staggering amounts of data from various
structured and unstructured sources such as general
Web text, online encyclopedias, query logs, web ta-
bles, or link anchor texts. Any proposed algorithm
to extract information needs to harness several data
sources and do it in a robust and scalable manner.
Our work in this paper represents a first step towards
that goal. In doing so, we achieved the following:
1. Improved coverage relative to a high accuracy
instance-class extraction system while main-
taining adequate precision.
2. Combined information from two different
sources: free text and web tables.
3. Demonstrated a graph-based label propagation
algorithm that given as little as five seeds per
class achieved good results on a graph with
more than a million nodes and 70 million
edges.
In this paper, we started off with a simple graph.
For future work, we plan to proceed along the fol-
lowing lines:
1. Encode richer relationships between nodes,
for example instance-instance associations and
other types of nodes.
2. Combine information from more data sources
to answer the question of whether more data or
diverse sources are more effective in increasing
precision and coverage.
3. Apply similar ideas to other information extrac-
tion tasks such as relation extraction.
Acknowledgments
We would like to thank D. Sivakumar for useful dis-
cussions and the anonymous reviewers for helpful
comments.
References
A. Azran. 2007. The rendezvous algorithm: multiclass
semi-supervised learning with markov random walks.
Proceedings of the 24th international conference on
Machine learning, pages 49?56.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008. Video
suggestion and discovery for youtube: taking random
walks through the view graph.
K. Bellare, P. Talukdar, G. Kumaran, F. Pereira, M. Liber-
man, A. McCallum, and M. Dredze. 2007. Lightly-
Supervised Attribute Extraction. NIPS 2007Workshop
on Machine Learning for Web Search.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. Webtables: Exploring the power of tables on the
web. VLDB.
589
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539?545, Nantes, France.
P. Indyk and J. Matousek. 2004. Low-distortion embed-
dings of finite metric spaces. Handbook of Discrete
and Computational Geometry.
B. Jansen, A. Spink, and T. Saracevic. 2000. Real life,
real users, and real needs: a study and analysis of user
queries on the Web. Information Processing and Man-
agement, 36(2):207?227.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference
on Computational linguistics (COLING-02), pages 1?
7.
K. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the
14th International Joint Conference on Artificial Intel-
ligence (IJCAI-95), pages 1050?1055, Montreal, Que-
bec.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the 16th National Conference on
Artificial Intelligence (AAAI-99), pages 474?479, Or-
lando, Florida.
M. Stevenson and R. Gaizauskas. 2000. Using corpus-
derived name lists for named entity recognition. In
Proceedings of the 6th Conference on Applied Natu-
ral Language Processing (ANLP-00), Seattle, Wash-
ington.
M. Szummer and T. Jaakkola. 2002. Partially labeled
classification with markov random walks. Advances in
Neural Information Processing Systems 14: Proceed-
ings of the 2002 NIPS Conference.
B. Van Durme and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. Twenty-Third AAAI Conference on Artificial In-
telligence.
R. Wang and W. Cohen. 2007. Language-Independent
Set Expansion of Named Entities Using theWeb. Data
Mining, 2007. ICDM 2007. Seventh IEEE Interna-
tional Conference on, pages 342?350.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. ICML-03, 20th International Con-
ference on Machine Learning.
590
Proceedings of NAACL HLT 2007, pages 564?571,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
ISP: Learning Inferential Selectional Preferences 
 
Patrick Pantel?, Rahul Bhagat?, Bonaventura Coppola?, 
Timothy Chklovski?, Eduard Hovy? 
?Information Sciences Institute 
University of Southern California 
Marina del Rey, CA 
{pantel,rahul,timc,hovy}@isi.edu
?ITC-Irst and University of Trento 
Via Sommarive, 18 ? Povo 38050  
Trento, Italy 
coppolab@itc.it 
  
Abstract 
Semantic inference is a key component 
for advanced natural language under-
standing. However, existing collections of 
automatically acquired inference rules 
have shown disappointing results when 
used in applications such as textual en-
tailment and question answering. This pa-
per presents ISP, a collection of methods 
for automatically learning admissible ar-
gument values to which an inference rule 
can be applied, which we call inferential 
selectional preferences, and methods for 
filtering out incorrect inferences. We 
evaluate ISP and present empirical evi-
dence of its effectiveness. 
1 Introduction 
Semantic inference is a key component for ad-
vanced natural language understanding. Several 
important applications are already relying heavily 
on inference, including question answering 
(Moldovan et al 2003; Harabagiu and Hickl 2006), 
information extraction (Romano et al 2006), and 
textual entailment (Szpektor et al 2004). 
In response, several researchers have created re-
sources for enabling semantic inference. Among 
manual resources used for this task are WordNet 
(Fellbaum 1998) and Cyc (Lenat 1995). Although 
important and useful, these resources primarily 
contain prescriptive inference rules such as ?X di-
vorces Y ? X married Y?. In practical NLP appli-
cations, however, plausible inference rules such as 
?X married Y? ? ?X dated Y? are very useful. This, 
along with the difficulty and labor-intensiveness of 
generating exhaustive lists of rules, has led re-
searchers to focus on automatic methods for build-
ing inference resources such as inference rule 
collections (Lin and Pantel 2001; Szpektor et al 
2004) and paraphrase collections (Barzilay and 
McKeown 2001). 
Using these resources in applications has been 
hindered by the large amount of incorrect infer-
ences they generate, either because of altogether 
incorrect rules or because of blind application of 
plausible rules without considering the context of 
the relations or the senses of the words. For exam-
ple, consider the following sentence: 
Terry Nichols was charged by federal prosecutors for murder 
and conspiracy in the Oklahoma City bombing. 
and an inference rule such as: 
 X is charged by Y ? Y announced the arrest of X (1) 
Using this rule, we can infer that ?federal prosecu-
tors announced the arrest of Terry Nichols?. How-
ever, given the sentence: 
Fraud was suspected when accounts were charged by CCM 
telemarketers without obtaining consumer authorization. 
the plausible inference rule (1) would incorrectly 
infer that ?CCM telemarketers announced the ar-
rest of accounts?. 
This example depicts a major obstacle to the ef-
fective use of automatically learned inference 
rules. What is missing is knowledge about the ad-
missible argument values for which an inference 
rule holds, which we call Inferential Selectional 
Preferences. For example, inference rule (1) 
should only be applied if X is a Person and Y is a 
Law Enforcement Agent or a Law Enforcement 
Agency. This knowledge does not guarantee that 
the inference rule will hold, but, as we show in this 
paper, goes a long way toward filtering out errone-
ous applications of rules. 
In this paper, we propose ISP, a collection of 
methods for learning inferential selectional prefer-
ences and filtering out incorrect inferences. The 
564
presented algorithms apply to any collection of 
inference rules between binary semantic relations, 
such as example (1). ISP derives inferential selec-
tional preferences by aggregating statistics of in-
ference rule instantiations over a large corpus of 
text. Within ISP, we explore different probabilistic 
models of selectional preference to accept or reject 
specific inferences. We present empirical evidence 
to support the following main contribution: 
Claim: Inferential selectional preferences can be 
automatically learned and used for effectively fil-
tering out incorrect inferences. 
2 Previous Work 
Selectional preference (SP) as a foundation for 
computational semantics is one of the earliest top-
ics in AI and NLP, and has its roots in (Katz and 
Fodor 1963).  Overviews of NLP research on this 
theme are (Wilks and Fass 1992), which includes 
the influential theory of Preference Semantics by 
Wilks, and more recently (Light and Greiff 2002). 
Rather than venture into learning inferential 
SPs, much previous work has focused on learning 
SPs for simpler structures. Resnik (1996), the 
seminal paper on this topic, introduced a statistical 
model for learning SPs for predicates using an un-
supervised method. 
Learning SPs often relies on an underlying set of 
semantic classes, as in both Resnik?s and our ap-
proach. Semantic classes can be specified manu-
ally or derived automatically. Manual collections 
of semantic classes include the hierarchies of 
WordNet (Fellbaum 1998), Levin verb classes 
(Levin 1993), and FrameNet (Baker et al 1998). 
Automatic derivation of semantic classes can take 
a variety of approaches, but often uses corpus 
methods and the Distributional Hypothesis (Harris 
1964) to automatically cluster similar entities into 
classes, e.g. CBC (Pantel and Lin 2002). In this 
paper, we experiment with two sets of semantic 
classes, one from WordNet and one from CBC. 
Another thread related to our work includes ex-
tracting from text corpora paraphrases (Barzilay 
and McKeown 2001) and inference rules, e.g. 
TEASE1 (Szpektor et al 2004) and DIRT (Lin and 
Pantel 2001). While these systems differ in their 
approaches, neither provides for the extracted in-
                                                     
1 Some systems refer to inferences they extract as entail-
ments; the two terms are sometimes used interchangeably. 
ference rules to hold or fail based on SPs. Zanzotto 
et al (2006) recently explored a different interplay 
between SPs and inferences. Rather than examine 
the role of SPs in inferences, they use SPs of a par-
ticular type to derive inferences.  For instance the 
preference of win for the subject player, a nomi-
nalization of play, is used to derive that ?win ? 
play?. Our work can be viewed as complementary 
to the work on extracting semantic inferences and 
paraphrases, since we seek to refine when a given 
inference applies, filtering out incorrect inferences. 
3 Selectional Preference Models 
The aim of this paper is to learn inferential selec-
tional preferences for filtering inference rules. 
Let pi ? pj be an inference rule where p is a bi-
nary semantic relation between two entities x and 
y. Let ?x, p, y? be an instance of relation p. 
Formal task definition: Given an inference rule 
 pi ? pj and the instance ?x, pi, y?, our task is to 
determine if ?x, pj, y? is valid. 
Consider the example in Section 1 where we 
have the inference rule ?X is charged by Y? ? ?Y 
announced the arrest of X?. Our task is to auto-
matically determine that ?federal prosecutors an-
nounced the arrest of Terry Nichols? (i.e., 
?Terry Nichols, pj, federal prosecutors?) is valid 
but that ?CCM telemarketers announced the arrest 
of accounts? is invalid. 
Because the semantic relations p are binary, the 
selectional preferences on their two arguments may 
be either considered jointly or independently. For 
example, the relation p = ?X is charged by Y? 
could have joint SPs: 
 ?Person, Law Enforcement Agent? 
 ?Person, Law Enforcement Agency?  (2) 
 ?Bank Account, Organization? 
or independent SPs: 
 ?Person, *? 
 ?*, Organization? (3) 
 ?*, Law Enforcement Agent? 
This distinction between joint and independent 
selectional preferences constitutes the difference 
between the two models we present in this section. 
The remainder of this section describes the ISP 
approach. In Section 3.1, we describe methods for 
automatically determining the semantic contexts of 
each single relation?s selectional preferences. Sec-
tion 3.2 uses these for developing our inferential 
565
selectional preference models. Finally, we propose 
inference filtering algorithms in Section 3.3. 
3.1 Relational Selectional Preferences 
Resnik (1996) defined the selectional preferences 
of a predicate as the semantic classes of the words 
that appear as its arguments. Similarly, we define 
the relational selectional preferences of a binary 
semantic relation pi as the semantic classes C(x) of 
the words that can be instantiated for x and as the 
semantic classes C(y) of the words that can be in-
stantiated for y. 
The semantic classes C(x) and C(y) can be ob-
tained from a conceptual taxonomy as proposed in 
(Resnik 1996), such as WordNet, or from the 
classes extracted from a word clustering algorithm 
such as CBC (Pantel and Lin 2002). For example, 
given the relation ?X is charged by Y?, its rela-
tional selection preferences from WordNet could 
be {social_group, organism, state?} for X and 
{authority, state, section?} for Y. 
Below we propose joint and independent mod-
els, based on a corpus analysis, for automatically 
determining relational selectional preferences. 
Model 1: Joint Relational Model (JRM) 
Our joint model uses a corpus analysis to learn SPs 
for binary semantic relations by considering their 
arguments jointly, as in example (2). 
Given a large corpus of English text, we first 
find the occurrences of each semantic relation p. 
For each instance ?x, p, y?, we retrieve the sets C(x) 
and C(y) of the semantic classes that x and y be-
long to and accumulate the frequencies of the tri-
ples ?c(x), p, c(y)?, where c(x) ? C(x) and  
c(y) ? C(y)2. 
Each triple ?c(x), p, c(y)? is a candidate selec-
tional preference for p. Candidates can be incorrect 
when: a) they were generated from the incorrect 
sense of a polysemous word; or b) p does not hold 
for the other words in the semantic class. 
Intuitively, we have more confidence in a par-
ticular candidate if its semantic classes are closely 
associated given the relation p. Pointwise mutual 
information (Cover and Thomas 1991) is a com-
monly used metric for measuring this association 
strength between two events e1 and e2: 
                                                     
2 In this paper, the semantic classes C(x) and C(y) are ex-
tracted from WordNet and CBC (described in Section 4.2).  
 ( )( ) ( )21
21
21
,
log);(
ePeP
eeP
eepmi =  (3.1) 
We define our ranking function as the strength 
of association between two semantic classes, cx and 
cy3, given the relation p: 
 ( ) ( )( ) ( )pcPpcP pccPpcpcpmi yx yxyx
,
log; =  (3.2) 
Let |cx, p, cy| denote the frequency of observing 
the instance ?c(x), p, c(y)?. We estimate the prob-
abilities of Equation 3.2 using maximum likeli-
hood estimates over our corpus: 
( ) ?? ?= ,, ,,ppcpcP xx
 ( ) ???= ,, ,, pcppcP yy  ( ) ??= ,, ,,, p cpcpccP yxyx   (3.3) 
Similarly to (Resnik 1996), we estimate the 
above frequencies using: 
( )??
?=?
xcw
x wC
pw
pc
,,
,,
 
( )??
?=?
ycw
y wC
wp
cp
,,
,,
 
( ) ( )??? ?= yx cwcwyx wCwC
wpw
cpc
21 , 21
21 ,,,,
 
where |x, p, y| denotes the frequency of observing 
the instance ?x, p, y? and |C(w)| denotes the number 
of classes to which word w belongs. |C(w)| distrib-
utes w?s mass equally to all of its senses cw. 
Model 2: Independent Relational Model (IRM) 
Because of sparse data, our joint model can miss 
some correct selectional preference pairs. For ex-
ample, given the relation  
 Y announced the arrest of X 
we may find occurrences from our corpus of the 
particular class ?Money Handler? for X and ?Law-
yer? for Y, however we may never see both of 
these classes co-occurring even though they would 
form a valid relational selectional preference. 
To alleviate this problem, we propose a second 
model that is less strict by considering the argu-
ments of the binary semantic relations independ-
ently, as in example (3). 
Similarly to JRM, we extract each instance  
?x, p, y? of each semantic relation p and retrieve the 
set of semantic classes C(x) and C(y) that x and y 
belong to, accumulating the frequencies of the tri-
ples ?c(x), p, *? and ?*, p, c(y)?, where  
c(x) ? C(x) and c(y) ? C(y). 
All tuples ?c(x), p, *? and ?*, p, c(y)? are candi-
date selectional preferences for p. We rank candi-
dates by the probability of the semantic class given 
the relation p, according to Equations 3.3. 
                                                     
3 cx and cy are shorthand for c(x) and c(y) in our equations. 
566
3.2 Inferential Selectional Preferences 
Whereas in Section 3.1 we learned selectional 
preferences for the arguments of a relation p, in 
this section we learn selectional preferences for the 
arguments of an inference rule pi ? pj. 
Model 1: Joint Inferential Model (JIM) 
Given an inference rule pi ? pj, our joint model 
defines the set of inferential SPs as the intersection 
of the relational SPs for pi and pj, as defined in the 
Joint Relational Model (JRM). For example, sup-
pose relation pi = ?X is charged by Y? gives the 
following SP scores under the JRM: 
 ?Person, pi, Law Enforcement Agent? = 1.45 
 ?Person, pi, Law Enforcement Agency? = 1.21  
 ?Bank Account, pi, Organization? = 0.97 
and that pj = ?Y announced the arrest of X? gives 
the following SP scores under the JRM: 
 ?Law Enforcement Agent, pj, Person? = 2.01 
 ?Reporter, pj, Person? = 1.98  
 ?Law Enforcement Agency, pj, Person? = 1.61 
The intersection of the two sets of SPs forms the 
candidate inferential SPs for the inference pi ? pj: 
 ?Law Enforcement Agent, Person? 
 ?Law Enforcement Agency, Person? 
We rank the candidate inferential SPs according 
to three ways to combine their relational SP scores, 
using the minimum, maximum, and average of the 
SPs. For example, for ?Law Enforcement Agent, 
Person?, the respective scores would be 1.45, 2.01, 
and 1.73. These different ranking strategies pro-
duced nearly identical results in our experiments, 
as discussed in Section 5. 
Model 2: Independent Inferential Model (IIM) 
Our independent model is the same as the joint 
model above except that it computes candidate in-
ferential SPs using the Independent Relational 
Model (IRM) instead of the JRM. Consider the 
same example relations pi and pj from the joint 
model and suppose that the IRM gives the follow-
ing relational SP scores for pi: 
 ?Law Enforcement Agent, pi, *? = 3.43 
 ?*, pi, Person? = 2.17  
 ?*, pi, Organization? = 1.24 
and the following relational SP scores for pj: 
 ?*, pj, Person? = 2.87 
 ?Law Enforcement Agent, pj, *? = 1.92  
 ?Reporter, pj, *? = 0.89 
The intersection of the two sets of SPs forms the 
candidate inferential SPs for the inference pi ? pj: 
 ?Law Enforcement Agent, *? 
 ?*, Person?  
We use the same minimum, maximum, and av-
erage ranking strategies as in JIM. 
3.3 Filtering Inferences 
Given an inference rule pi ? pj and the instance  
?x, pi, y?, the system?s task is to determine whether 
?x, pj, y? is valid. Let C(w) be the set of semantic 
classes c(w) to which word w belongs. Below we 
present three filtering algorithms which range from 
the least to the most permissive: 
? ISP.JIM, accepts the inference ?x, pj, y? if the 
inferential SP ?c(x), pj, c(y)? was admitted by the 
Joint Inferential Model for some c(x) ? C(x) and 
c(y) ? C(y). 
? ISP.IIM.?, accepts the inference ?x, pj, y? if the 
inferential SPs ?c(x), pj, *? AND ?*, pj, c(y)? were 
admitted by the Independent Inferential Model 
for some c(x) ? C(x) and c(y) ? C(y) . 
? ISP.IIM.?, accepts the inference ?x, pj, y? if the 
inferential SP ?c(x), pj, *? OR ?*, pj, c(y)? was 
admitted by the Independent Inferential Model 
for some c(x) ? C(x) and c(y) ? C(y) . 
Since both JIM and IIM use a ranking score in 
their inferential SPs, each filtering algorithm can 
be tuned to be more or less strict by setting an ac-
ceptance threshold on the ranking scores or by se-
lecting only the top ? percent highest ranking SPs. 
In our experiments, reported in Section 5, we 
tested each model using various values of ?. 
4 Experimental Methodology 
This section describes the methodology for testing 
our claim that inferential selectional preferences 
can be learned to filter incorrect inferences. 
Given a collection of inference rules of the form 
pi ? pj, our task is to determine whether a particu-
lar instance ?x, pj, y? holds given that ?x, pi, y? 
holds4. In the next sections, we describe our collec-
tion of inference rules, the semantic classes used 
for forming selectional preferences, and evaluation 
criteria for measuring the filtering quality. 
                                                     
4 Recall that the inference rules we consider in this paper are 
not necessary strict logical inference rules, but plausible in-
ference rules; see Section 3. 
567
4.1 Inference Rules 
Our models for learning inferential selectional 
preferences can be applied to any collection of in-
ference rules between binary semantic relations. In 
this paper, we focus on the inference rules con-
tained in the DIRT resource (Lin and Pantel 2001). 
DIRT consists of over 12 million rules which were 
extracted from a 1GB newspaper corpus (San Jose 
Mercury, Wall Street Journal and AP Newswire 
from the TREC-9 collection). For example, here 
are DIRT?s top 3 inference rules for ?X solves Y?: 
 ?Y is solved by X?, ?X resolves Y?, ?X finds a solution to Y? 
4.2 Semantic Classes 
The choice of semantic classes is of great impor-
tance for selectional preference. One important 
aspect is the granularity of the classes. Too general 
a class will provide no discriminatory power while 
too fine-grained a class will offer little generaliza-
tion and apply in only extremely few cases. 
The absence of an attested high-quality set of 
semantic classes for this task makes discovering 
preferences difficult. Since many of the criteria for 
developing such a set are not even known, we de-
cided to experiment with two very different sets of 
semantic classes, in the hope that in addition to 
learning semantic preferences, we might also un-
cover some clues for the eventual decisions about 
what makes good semantic classes in general. 
Our first set of semantic classes was directly ex-
tracted from the output of the CBC clustering algo-
rithm (Pantel and Lin 2002). We applied CBC to 
the TREC-9 and TREC-2002 (Aquaint) newswire 
collections consisting of over 600 million words. 
CBC generated 1628 noun concepts and these were 
used as our semantic classes for SPs. 
Secondly, we extracted semantic classes from 
WordNet 2.1 (Fellbaum 1998). In the absence of 
any externally motivated distinguishing features 
(for example, the Basic Level categories from Pro-
totype Theory, developed by Eleanor Rosch 
(1978)), we used the simple but effective method 
of manually truncating the noun synset hierarchy5 
and considering all synsets below each cut point as 
part of the semantic class at that node. To select 
the cut points, we inspected several different hier-
archy levels and found the synsets at a depth of 4 
                                                     
5 Only nouns are considered since DIRT semantic relations 
connect only nouns. 
to form the most natural semantic classes. Since 
the noun hierarchy in WordNet has an average 
depth of 12, our truncation created a set of con-
cepts considerably coarser-grained than WordNet 
itself. The cut produced 1287 semantic classes, a 
number similar to the classes in CBC. To properly 
test WordNet as a source of semantic classes for 
our selectional preferences, we would need to ex-
periment with different extraction algorithms. 
4.3 Evaluation Criteria 
The goal of the filtering task is to minimize false 
positives (incorrectly accepted inferences) and 
false negatives (incorrectly rejected inferences). A 
standard methodology for evaluating such tasks is 
to compare system filtering results with a gold 
standard using a confusion matrix. A confusion 
matrix captures the filtering performance on both 
correct and incorrect inferences: 
  
where A represents the number of correct instances 
correctly identified by the system, D represents the 
number of incorrect instances correctly identified 
by the system, B represents the number of false 
positives and C represents the number of false 
negatives. To compare systems, three key meas-
ures are used to summarize confusion matrices: 
? Sensitivity, defined as CA
A
+ , captures a filter?s 
probability of accepting correct inferences; 
? Specificity, defined as DB
D
+ , captures a filter?s 
probability of rejecting incorrect inferences; 
? Accuracy, defined as DCBA
DA
+++
+ , captures the 
probability of a filter being correct. 
5 Experimental Results 
In this section, we provide empirical evidence to 
support the main claim of this paper. 
Given a collection of DIRT inference rules of 
the form pi ? pj, our experiments, using the meth-
odology of Section 4, evaluate the capability of our 
ISP models for determining if ?x, pj, y? holds given 
that ?x, pi, y? holds. 
GOLD STANDARD   
1 0 
1 A B 
SY
ST
E
M
 
0 C D 
568
5.1 Experimental Setup 
Model Implementation 
For each filtering algorithm in Section 3.3, ISP.JIM, 
ISP.IIM.?, and ISP.IIM.?, we trained their probabil-
istic models using corpus statistics extracted from 
the 1999 AP newswire collection (part of the 
TREC-2002 Aquaint collection) consisting of ap-
proximately 31 million words. We used the Mini-
par parser (Lin 1993) to match DIRT patterns in 
the text. This permits exact matches since DIRT 
inference rules are built from Minipar parse trees. 
For each system, we experimented with the dif-
ferent ways of combining relational SP scores: 
minimum, maximum, and average (see Section 
3.2). Also, we experimented with various values 
for the ? parameter described in Section 3.3. 
Gold Standard Construction 
In order to compute the confusion matrices de-
scribed in Section 4.3, we must first construct a 
representative set of inferences and manually anno-
tate them as correct or incorrect. 
We randomly selected 100 inference rules of the 
form pi ? pj from DIRT. For each pattern pi, we 
then extracted its instances from the Aquaint 1999 
AP newswire collection (approximately 22 million 
words), and randomly selected 10 distinct in-
stances, resulting in a total of 1000 instances. For 
each instance of pi, applying DIRT?s inference rule 
would assert the instance ?x, pj, y?. Our evaluation 
tests how well our models can filter these so that 
only correct inferences are made. 
To form the gold standard, two human judges 
were asked to tag each instance ?x, pj, y? as correct 
or incorrect. For example, given a randomly se-
lected inference rule ?X is charged by Y ? Y an-
nounced the arrest of X? and the instance ?Terry 
Nichols was charged by federal prosecutors?, the 
judges must determine if the instance ?federal 
prosecutors, Y announced the arrest of X, Terry 
Nichols? is correct. The judges were asked to con-
sider the following two criteria for their decision: 
? ?x, pj, y? is a semantically meaningful instance; 
? The inference pi ? pj holds for this instance. 
Judges found that annotation decisions can range 
from trivial to difficult. The differences often were 
in the instances for which one of the judges fails to 
see the right context under which the inference 
could hold. To minimize disagreements, the judges 
went through an extensive round of training. 
To that end, the 1000 instances ?x, pj, y? were 
split into DEV and TEST sets, 500 in each. The 
two judges trained themselves by annotating DEV 
together. The TEST set was then annotated sepa-
rately to verify the inter-annotator agreement and 
to verify whether the task is well-defined. The 
kappa statistic (Siegel and Castellan Jr. 1988) was 
? = 0.72. For the 70 disagreements between the 
judges, a third judge acted as an adjudicator. 
Baselines 
We compare our ISP algorithms to the following 
baselines: 
? B0: Rejects all inferences; 
? B1: Accepts all inferences; 
? Rand: Randomly accepts or rejects inferences. 
One alternative to our approach is admit instances 
on the Web using literal search queries. We inves-
tigated this technique but discarded it due to subtle 
yet critical issues with pattern canonicalization that 
resulted in rejecting nearly all inferences. How-
ever, we are investigating other ways of using Web 
corpora for this task. 
Table 1. Filtering quality of best performing systems according to the evaluation criteria defined in Section 4.3 on 
the TEST set ? the reported systems were selected based on the Accuracy criterion on the DEV set. 
PARAMETERS SELECTED FROM DEV SET 
SYSTEM 
RANKING STRATEGY ? (%) 
SENSITIVITY 
(95% CONF) 
SPECIFICITY 
(95% CONF) 
ACCURACY 
(95% CONF) 
B0 - - 0.00?0.00 1.00?0.00 0.50?0.04 
B1 - - 1.00?0.00 0.00?0.00 0.49?0.04 
Random - - 0.50?0.06 0.47?0.07 0.50?0.04 
ISP.JIM maximum 100 0.17?0.04 0.88?0.04 0.53?0.04 
ISP.IIM.? maximum 100 0.24?0.05 0.84?0.04 0.54?0.04 CBC 
ISP.IIM.? maximum 90 0.73?0.05 0.45?0.06 0.59?0.04? 
ISP.JIM minimum 40 0.20?0.06 0.75?0.06 0.47?0.04 
ISP.IIM.? minimum 10 0.33?0.07 0.77?0.06 0.55?0.04 WordNet 
ISP.IIM.? minimum 20 0.87?0.04 0.17?0.05 0.51?0.05 
? Indicates statistically significant results (with 95% confidence) when compared with all baseline systems using pairwise t-test. 
569
5.2 Filtering Quality 
For each ISP algorithm and parameter combina-
tion, we constructed a confusion matrix on the de-
velopment set and computed the system sensitivity, 
specificity and accuracy as described in Section 
4.3. This resulted in 180 experiments on the devel-
opment set. For each ISP algorithm and semantic 
class source, we selected the best parameter com-
binations according to the following criteria: 
? Accuracy: This system has the best overall abil-
ity to correctly accept and reject inferences. 
? 90%-Specificity: Several formal semantics and 
textual entailment researchers have commented 
that inference rule collections like DIRT are dif-
ficult to use due to low precision. Many have 
asked for filtered versions that remove incorrect 
inferences even at the cost of removing correct 
inferences. In response, we show results for the 
system achieving the best sensitivity while main-
taining at least 90% specificity on the DEV set. 
We evaluated the selected systems on the TEST 
set. Table 1 summarizes the quality of the systems 
selected according to the Accuracy criterion. The 
best performing system, ISP.IIM.?, performed  sta-
tistically significantly better than all three base-
lines. The best system according to the 90%-
Specificity criteria was ISP.JIM, which coinciden-
tally has the highest accuracy for that model as 
shown in Table 16. This result is very promising 
for researchers that require highly accurate infer-
ence rules since they can use ISP.JIM and expect to 
recall 17% of the correct inferences by only ac-
cepting false positives 12% of the time. 
Performance and Error Analysis 
Figures 1a) and 1b) present the full confusion ma-
trices for the most accurate and highly specific sys-
tems, with both systems selected on the DEV set. 
The most accurate system was ISP.IIM.?, which is 
the most permissive of the algorithms. This sug-
                                                     
6 The reported sensitivity of ISP.Joint in Table 1 is below 
90%, however it achieved 90.7% on the DEV set. 
gests that a larger corpus for learning SPs may be 
needed to support stronger performance on the 
more restrictive methods. The system in Figure 
1b), selected for maximizing sensitivity while 
maintaining high specificity, was 70% correct in 
predicting correct inferences. 
Figure 2 illustrates the ROC curve for all our 
systems and parameter combinations on the TEST 
set. ROC curves plot the true positive rate against 
the false positive rate. The near-diagonal line plots 
the three baseline systems. 
Several trends can be observed from this figure. 
First, systems using the semantic classes from 
WordNet tend to perform less well than systems 
using CBC classes. As discussed in Section 4.2, we 
used a very simplistic extraction of semantic 
classes from WordNet. The results in Figure 2 
serve as a lower bound on what could be achieved 
with a better extraction from WordNet. Upon in-
spection of instances that WordNet got incorrect 
but CBC got correct, it seemed that CBC had a 
much higher lexical coverage than WordNet. For 
example, several of the instances contained proper 
names as either the X or Y argument (WordNet has 
poor proper name coverage). When an argument is 
not covered by any class, the inference is rejected. 
Figure 2 also illustrates how our three different 
ISP algorithms behave. The strictest filters, ISP.JIM 
and ISP.IIM.?, have the poorest overall perform-
ance but, as expected, have a generally very low 
rate of false positives. ISP.IIM.?, which is a much 
more permissive filter because it does not require 
ROC on the TEST Set
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1-Specificity
S
en
si
tiv
ity
Baselines WordNet CBC ISP.JIM ISP.IIM.AND ISP.IIM.OR
Figure 2. ROC curves for our systems on TEST. 
GOLD STANDARD a)  
1 0 
1 184 139 
SY
ST
E
M
 
0 63 114 
GOLD STANDARD b)  
1 0 
1 42 28 
SY
ST
E
M
 
0 205 225 
Figure 1. Confusion matrices for a) ISP.IIM.? ? best 
Accuracy; and b) ISP.JIM ? best 90%-Specificity.
570
both arguments of a relation to match, has gener-
ally many more false positives but has an overall 
better performance. 
We did not include in Figure 2 an analysis of the 
minimum, maximum, and average ranking strate-
gies presented in Section 3.2 since they generally 
produced nearly identical results. 
For the most accurate system, ISP.IIM.?, we ex-
plored the impact of the cutoff threshold ? on the 
sensitivity, specificity, and accuracy, as shown in 
Figure 3. Rather than step the values by 10% as we 
did on the DEV set, here we stepped the threshold 
value by 2% on the TEST set. The more permis-
sive values of ? increase sensitivity at the expense 
of specificity. Interestingly, the overall accuracy 
remained fairly constant across the entire range of 
?, staying within 0.05 of the maximum of 0.62 
achieved at ?=30%. 
Finally, we manually inspected several incorrect 
inferences that were missed by our filters. A com-
mon source of errors was due to the many incorrect 
?antonymy? inference rules generated by DIRT, 
such as ?X is rejected in Y???X is accepted in Y?. 
This recognized problem in DIRT occurs because 
of the distributional hypothesis assumption used to 
form the inference rules. Our ISP algorithms suffer 
from a similar quandary since, typically, antony-
mous relations take the same sets of arguments for 
X (and Y). For these cases, ISP algorithms learn 
many selectional preferences that accept the same 
types of entities as those that made DIRT learn the 
inference rule in the first place, hence ISP will not 
filter out many incorrect inferences. 
6 Conclusion 
We presented algorithms for learning what we call 
inferential selectional preferences, and presented 
evidence that learning selectional preferences can 
be useful in filtering out incorrect inferences. Fu-
ture work in this direction includes further explora-
tion of the appropriate inventory of semantic 
classes used as SP?s. This work constitutes a step 
towards better understanding of the interaction of 
selectional preferences and inferences, bridging 
these two aspects of semantics. 
References 
Barzilay, R.; and McKeown, K.R. 2001.Extracting Paraphrases from a 
Parallel Corpus. In Proceedings of ACL 2001. pp. 50?57. Toulose, 
France. 
Baker, C.F.; Fillmore, C.J.; and Lowe, J.B. 1998. The Berkeley 
FrameNet Project. In Proceedings of COLING/ACL 1998.  pp. 86-
90. Montreal, Canada. 
Cover, T.M. and Thomas, J.A. 1991. Elements of Information Theory. 
John Wiley & Sons. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT 
Press. 
Harabagiu, S.; and Hickl, A. 2006. Methods for Using Textual 
Entailment in Open-Domain Question Answering. In Proceedings 
of ACL 2006.  pp. 905-912. Sydney, Australia. 
Katz, J.; and Fodor, J.A. 1963. The Structure of a Semantic Theory. 
Language, vol 39. pp.170?210.  
Lenat, D. 1995. CYC: A large-scale investment in knowledge 
infrastructure. Communications of the ACM, 38(11):33?38. 
Levin, B. 1993. English Verb Classes and Alternations: A Preliminary 
Investigation. University of Chicago Press, Chicago, IL. 
Light, M. and Greiff, W.R. 2002. Statistical Models for the Induction 
and Use of Selectional Preferences. Cognitive Science,26:269?281. 
Lin, D. 1993. Parsing Without OverGeneration. In Proceedings of  
ACL-93. pp. 112-120. Columbus, OH. 
Lin, D. and Pantel, P. 2001. Discovery of Inference Rules for 
Question Answering. Natural Language Engineering 7(4):343-360. 
Moldovan, D.I.; Clark, C.; Harabagiu, S.M.; Maiorano, S.J. 2003. 
COGEX: A Logic Prover for Question Answering. In Proceedings 
of HLT-NAACL-03. pp. 87-93. Edmonton, Canada. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In 
Proceedings of KDD-02. pp. 613-619. Edmonton, Canada. 
Resnik, P. 1996. Selectional Constraints: An Information-Theoretic 
Model and its Computational Realization. Cognition, 61:127?159. 
Romano, L.; Kouylekov, M.; Szpektor, I.; Dagan, I.; Lavelli, A. 2006. 
Investigating a Generic Paraphrase-Based Approach for Relation 
Extraction. In EACL-2006. pp. 409-416. Trento, Italy. 
Rosch, E. 1978. Human Categorization. In E. Rosch and B.B. Lloyd 
(eds.) Cognition and Categorization. Hillsdale, NJ: Erlbaum.  
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statistics for 
the Behavioral Sciences. McGraw-Hill. 
Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. Scaling 
web-based acquisition of entailment relations. In Proceedings of 
EMNLP 2004. pp. 41-48. Barcelona,Spain. 
Wilks, Y.; and Fass, D. 1992. Preference Semantics: a family history. 
Computing and Mathematics with Applications, 23(2). A shorter 
version in the second edition of the Encyclopedia of Artificial 
Intelligence, (ed.) S. Shapiro. 
Zanzotto, F.M.; Pennacchiotti, M.; Pazienza, M.T. 2006. Discovering 
Asymmetric Entailment Relations between Verbs using Selectional 
Preferences. In COLING/ACL-06. pp. 849-856. Sydney, Australia. 
Figure 3. ISP.IIM.? (Best System)?s performance 
variation over different values for the ? threshold. 
ISP.IIM.OR (Best System)'s Performance vs. Tau-Thresholds
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 10 20 30 40 50 60 70 80 90 100
Tau-Thresholds
Sensitivity Specificity Accuracy
571
Proceedings of ACL-08: HLT, pages 674?682,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Large Scale Acquisition of Paraphrases for Learning Surface Patterns
Rahul Bhagat?
Information Sciences Institute
University of Southern California
Marina del Rey, CA
rahul@isi.edu
Deepak Ravichandran
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA
deepakr@google.com
Abstract
Paraphrases have proved to be useful in many
applications, including Machine Translation,
Question Answering, Summarization, and In-
formation Retrieval. Paraphrase acquisition
methods that use a single monolingual corpus
often produce only syntactic paraphrases. We
present a method for obtaining surface para-
phrases, using a 150GB (25 billion words)
monolingual corpus. Our method achieves an
accuracy of around 70% on the paraphrase ac-
quisition task. We further show that we can
use these paraphrases to generate surface pat-
terns for relation extraction. Our patterns are
much more precise than those obtained by us-
ing a state of the art baseline and can extract
relations with more than 80% precision for
each of the test relations.
1 Introduction
Paraphrases are textual expressions that convey the
same meaning using different surface words. For ex-
ample consider the following sentences:
Google acquired YouTube. (1)
Google completed the acquisition of YouTube. (2)
Since they convey the same meaning, sentences
(1) and (2) are sentence level paraphrases, and the
phrases ?acquired? and ?completed the acquisition
of ? in (1) and (2) respectively are phrasal para-
phrases.
Paraphrases provide a way to capture the vari-
ability of language and hence play an important
?Work done during an internship at Google Inc.
role in many natural language processing (NLP) ap-
plications. For example, in question answering,
paraphrases have been used to find multiple pat-
terns that pinpoint the same answer (Ravichandran
and Hovy, 2002); in statistical machine transla-
tion, they have been used to find translations for
unseen source language phrases (Callison-Burch et
al., 2006); in multi-document summarization, they
have been used to identify phrases from different
sentences that express the same information (Barzi-
lay et al, 1999); in information retrieval they have
been used for query expansion (Anick and Tipirneni,
1999).
Learning paraphrases requires one to ensure iden-
tity of meaning. Since there are no adequate se-
mantic interpretation systems available today, para-
phrase acquisition techniques use some other mech-
anism as a kind of ?pivot? to (help) ensure semantic
identity. Each pivot mechanism selects phrases with
similar meaning in a different characteristic way. A
popular method, the so-called distributional simi-
larity, is based on the dictum of Zelig Harris ?you
shall know the words by the company they keep?:
given highly discriminating left and right contexts,
only words with very similar meaning will be found
to fit in between them. For paraphrasing, this has
been often used to find syntactic transformations in
parse trees that preserve (semantic) meaning. An-
other method is to use a bilingual dictionary or trans-
lation table as pivot mechanism: all source language
words or phrases that translate to a given foreign
word/phrase are deemed to be paraphrases of one
another. In this paper we call the paraphrases that
contain only words as surface paraphrases and those
674
that contain paths in a syntax tree as syntactic para-
phrases.
We here, present a method to acquire surface
paraphrases from a single monolingual corpus. We
use a large corpus (about 150GB) to overcome the
data sparseness problem. To overcome the scalabil-
ity problem, we pre-process the text with a simple
parts-of-speech (POS) tagger and then apply locality
sensitive hashing (LSH) (Charikar, 2002; Ravichan-
dran et al, 2005) to speed up the remaining compu-
tation for paraphrase acquisition. Our experiments
show results to verify the following main claim:
Claim 1: Highly precise surface paraphrases can be
obtained from a very large monolingual corpus.
With this result, we further show that these para-
phrases can be used to obtain high precision surface
patterns that enable the discovery of relations in a
minimally supervised way. Surface patterns are tem-
plates for extracting information from text. For ex-
ample, if one wanted to extract a list of company ac-
quisitions, ??ACQUIRER? acquired ?ACQUIREE??
would be one surface pattern with ??ACQUIRER??
and ??ACQUIREE?? as the slots to be extracted.
Thus we can claim:
Claim 2: These paraphrases can then be used for
generating high precision surface patterns for rela-
tion extraction.
2 Related Work
Most recent work in paraphrase acquisition is based
on automatic acquisition. Barzilay and McKeown
(2001) used a monolingual parallel corpus to obtain
paraphrases. Bannard and Callison-Burch (2005)
and Zhou et al (2006) both employed a bilingual
parallel corpus in which each foreign language word
or phrase was a pivot to obtain source language para-
phrases. Dolan et al (2004) and Barzilay and Lee
(2003) used comparable news articles to obtain sen-
tence level paraphrases. All these approaches rely
on the presence of parallel or comparable corpora
and are thus limited by their availability and size.
Lin and Pantel (2001) and Szpektor et al (2004)
proposed methods to obtain entailment templates by
using a single monolingual resource. While both dif-
fer in their approaches, they both end up finding syn-
tactic paraphrases. Their methods cannot be used if
we cannot parse the data (either because of scale or
data quality). Our approach on the other hand, finds
surface paraphrases; it is more scalable and robust
due to the use of simple POS tagging. Also, our use
of locality sensitive hashing makes finding similar
phrases in a large corpus feasible.
Another task related to our work is relation extrac-
tion. Its aim is to extract instances of a given rela-
tion. Hearst (1992) the pioneering paper in the field
used a small number of hand selected patterns to ex-
tract instances of hyponymy relation. Berland and
Charniak (1999) used a similar method for extract-
ing instances of meronymy relation. Ravichandran
and Hovy (2002) used seed instances of a relation
to automatically obtain surface patterns by querying
the web. But their method often finds patterns that
are too general (e.g., X and Y), resulting in low pre-
cision extractions. Rosenfeld and Feldman (2006)
present a somewhat similar web based method that
uses a combination of seed instances and seed pat-
terns to learn good quality surface patterns. Both
these methods differ from ours in that they learn
relation patterns on the fly (from the web). Our
method however, pre-computes paraphrases for a
large set of surface patterns using distributional sim-
ilarity over a large corpus and then obtains patterns
for a relation by simply finding paraphrases (offline)
for a few seed patterns. Using distributional simi-
larity avoids the problem of obtaining overly gen-
eral patterns and the pre-computation of paraphrases
means that we can obtain the set of patterns for any
relation instantaneously.
Romano et al (2006) and Sekine (2006) used syn-
tactic paraphrases to obtain patterns for extracting
relations. While procedurally different, both meth-
ods depend heavily on the performance of the syntax
parser and require complex syntax tree matching to
extract the relation instances. Our method on the
other hand acquires surface patterns and thus avoids
the dependence on a parser and syntactic matching.
This also makes the extraction process scalable.
3 Acquiring Paraphrases
This section describes our model for acquiring para-
phrases from text.
675
3.1 Distributional Similarity
Harris?s distributional hypothesis (Harris, 1954) has
played an important role in lexical semantics. It
states that words that appear in similar contexts tend
to have similar meanings. In this paper, we apply
the distributional hypothesis to phrases i.e. word n-
grams.
For example, consider the phrase ?acquired? of
the form ?X acquired Y ?. Considering the con-
text of this phrase, we might find {Google, eBay,
Yahoo,...} in position X and {YouTube, Skype,
Overture,...} in position Y . Now consider another
phrase ?completed the acquisition of ?, again of the
form ?X completed the acquisition of Y ?. For this
phrase, we might find {Google, eBay, Hilton Hotel
corp.,...} in position X and {YouTube, Skype, Bally
Entertainment Corp.,...} in position Y . Since the
contexts of the two phrases are similar, our exten-
sion of the distributional hypothesis would assume
that ?acquired? and ?completed the acquisition of ?
have similar meanings.
3.2 Paraphrase Learning Model
Let p be a phrase (n-gram) of the form X p Y ,
where X and Y are the placeholders for words oc-
curring on either side of p. Our first task is to
find the set of phrases that are similar in meaning
to p. Let P = {p1, p2, p3, ..., pl} be the set of all
phrases of the form X pi Y where pi ? P . Let
Si,X be the set of words that occur in position X of
pi and Si,Y be the set of words that occur in posi-
tion Y of pi. Let Vi be the vector representing pi
such that Vi = Si,X ? Si,Y . Each word f ? Vi
has an associated score that measures the strength
of the association of the word f with phrase pi; as
do many others, we employ pointwise mutual infor-
mation (Cover and Thomas, 1991) to measure this
strength of association.
pmi(pi; f) = log P (pi,f)P (pi)P (f) (1)
The probabilities in equation (1) are calculated by
using the maximum likelihood estimate over our
corpus.
Once we have the vectors for each phrase pi ? P ,
we can find the paraphrases for each pi by finding its
nearest neighbors. We use cosine similarity, which
is a commonly used measure for finding similarity
between two vectors.
If we have two phrases pi ? P and pj ? P with
the corresponding vectors Vi and Vj constructed
as described above, the similarity between the two
phrases is calculated as:
sim(pi; pj) = Vi!Vj|Vi|?|Vj | (2)
Each word in Vi (and Vj) has with it an associated
flag which indicates weather the word came from
Si,X or Si,Y . Hence for each phrase pi of the form
X pi Y , we have a corresponding phrase ?pi that
has the form Y pi X. This is important to find cer-
tain kinds of paraphrases. The following example
will illustrate. Consider the sentences:
Google acquired YouTube. (3)
YouTube was bought by Google. (4)
From sentence (3), we obtain two phrases:
1. pi = acquired which has the form ?X acquired Y ?
where ?X = Google? and ?Y = YouTube?
2. ?pi = ?acquired which has the form ?Y acquired
X? where ?X = YouTube? and ?Y = Google?
Similarly, from sentence (4) we obtain two phrases:
1. pj = was bought by which has the form ?X was
bought by Y ? where ?X = YouTube? and ?Y =
Google?
2. ?pj = ?was bought by which has the form ?Y
was bought by X? where ?X = Google? and ?Y
= YouTube?
The switching of X and Y positions in (3) and (4)
ensures that ?acquired? and ??was bought by? are
found to be paraphrases by the algorithm.
3.3 Locality Sensitive Hashing
As described in Section 3.2, we find paraphrases of
a phrase pi by finding its nearest neighbors based
on cosine similarity between the feature vector of
pi and other phrases. To do this for all the phrases
in the corpus, we?ll have to compute the similarity
between all vector pairs. If n is the number of vec-
tors and d is the dimensionality of the vector space,
finding cosine similarity between each pair of vec-
tors has time complexity O(n2d). This computation
is infeasible for our corpus, since both n and d are
large.
676
To solve this problem, we make use of Local-
ity Sensitive Hashing (LSH). The basic idea behind
LSH is that a LSH function creates a fingerprint
for each vector such that if two vectors are simi-
lar, they are likely to have similar fingerprints. The
LSH function we use here was proposed by Charikar
(2002). It represents a d dimensional vector by a
stream of b bits (b & d) and has the property of pre-
serving the cosine similarity between vectors, which
is exactly what we want. Ravichandran et al (2005)
have shown that by using the LSH nearest neighbors
calculation can be done in O(nd) time.1.
4 Learning Surface Patterns
Let r be a target relation. Our task is to find a set of
surface patterns S = {s1, s2, ..., sn} that express the
target relation. For example, consider the relation r
= ?acquisition?. We want to find the set of patterns
S that express this relation:
S = {?ACQUIRER? acquired ?ACQUIREE?,
?ACQUIRER? bought ?ACQUIREE?, ?ACQUIREE?
was bought by ?ACQUIRER?,...}.
The remainder of the section describes our model
for learning surface patterns for target relations.
4.1 Model Assumption
Paraphrases express the same meaning using differ-
ent surface forms. So if one knew a pattern that ex-
presses a target relation, one could build more pat-
terns for that relation by finding paraphrases for the
surface phrase(s) in that pattern. This is the basic
assumption of our model.
For example, consider the seed pattern
??ACQUIRER? acquired ?ACQUIREE?? for
the target relation ?acquisition?. The surface phrase
in the seed pattern is ?acquired?. Our model then
assumes that we can obtain more surface patterns
for ?acquisition? by replacing ?acquired? in the
seed pattern with its paraphrases i.e. {bought, ?was
bought by2,...}. The resulting surface patterns are:
1The details of the algorithm are omitted, but interested
readers are encouraged to read Charikar (2002) and Ravichan-
dran et al (2005)
2The ??? in ??was bought by? indicates that the
?ACQUIRER? and ?ACQUIREE? arguments of the input
phrase ?acquired? need to be switched for the phrase ?was
bought by?.
{?ACQUIRER? bought ?ACQUIREE?, ?ACQUIREE?
was bought by ?ACQUIRER?,...}
4.2 Surface Pattern Model
Let r be a target relation. Let SEED = {seed1,
seed2,..., seedn} be the set of seed patterns that ex-
press the target relation. For each seedi ? SEED,
we obtain the corresponding set of new patterns
PATi in two steps:
1. We find the surface phrase, pi, using a seed
and find the corresponding set of paraphrases,
Pi = {pi,1, pi,2, ..., pi,m}. Each paraphrase,
pi,j ? Pi, has with it an associated score which
is similarity between pi and pi,j .
2. In seed pattern, seedi, we replace the sur-
face phrase, pi, with its paraphrases and
obtain the set of new patterns PATi =
{pati,1, pati,2, ..., pati,m}. Each pattern has
with it an associated score, which is the same as
the score of the paraphrase from which it was
obtained3 . The patterns are ranked in the de-
creasing order of their scores.
After we obtain PATi for each seedi ? SEED,
we obtain the complete set of patterns, PAT , for
the target relation r as the union of all the individual
pattern sets, i.e., PAT = PAT1 ? PAT2 ? ... ?
PATn.
5 Experimental Methodology
In this section, we describe experiments to validate
the main claims of the paper. We first describe para-
phrase acquisition, we then summarize our method
for learning surface patterns, and finally describe the
use of patterns for extracting relation instances.
5.1 Paraphrases
Finding surface variations in text requires a large
corpus. The corpus needs to be orders of magnitude
larger than that required for learning syntactic varia-
tions, since surface phrases are sparser than syntac-
tic phrases.
For our experiments, we used a corpus of about
150GB (25 billion words) obtained from Google
News4 . It consists of few years worth of news data.
3If a pattern is generated from more than one seed, we assign
it its average score.
4The corpus was cleaned to remove duplicate articles.
677
We POS tagged the corpus using Tnt tagger (Brants,
2000) and collected all phrases (n-grams) in the cor-
pus that contained at least one verb, and had a noun
or a noun-noun compound on either side. We re-
stricted the phrase length to at most five words.
We build a vector for each phrase as described in
Section 3. Tomitigate the problem of sparseness and
co-reference to a certain extent, whenever we have a
noun-noun compound in the X or Y positions, we
treat it as bag of words. For example, in the sen-
tence ?Google Inc. acquired YouTube?, ?Google?
and ?Inc.? will be treated as separate features in the
vector5.
Once we have constructed all the vectors, we find
the paraphrases for every phrase by finding its near-
est neighbors as described in Section 3. For our ex-
periments, we set the number of random bits in the
LSH function to 3000, and the similarity cut-off be-
tween vectors to 0.15. We eventually end up with
a resource containing over 2.5 million phrases such
that each phrase is connected to its paraphrases.
5.2 Surface Patterns
One claim of this paper is that we can find good sur-
face patterns for a target relation by starting with a
seed pattern. To verify this, we study two target re-
lations6:
1. Acquisition: We define this as the relation be-
tween two companies such that one company
acquired the other.
2. Birthplace: We define this as the relation be-
tween a person and his/her birthplace.
For ?acquisition? relation, we start with the sur-
face patterns containing only the words buy and ac-
quire:
1. ??ACQUIRER? bought ?ACQUIREE?? (and its
variants, i.e. buy, buys and buying)
2. ??ACQUIRER? acquired ?ACQUIREE?? (and its
variants, i.e. acquire, acquires and acquiring)
5This adds some noise in the vectors, but we found that this
results in better paraphrases.
6Since we have to do all the annotations for evaluations on
our own, we restricted our experiments to only two commonly
used relations.
This results in a total of eight seed patterns.
For ?birthplace? relation, we start with two seed
patterns:
1. ??PERSON? was born in ?LOCATION??
2. ??PERSON? was born at ?LOCATION??.
We find other surface patterns for each of these
relations by replacing the surface words in the seed
patterns by their paraphrases, as described in Sec-
tion 4.
5.3 Relation Extraction
The purpose of learning surface patterns for a rela-
tion is to extract instances of that relation. We use
the surface patterns obtained for the relations ?ac-
quisition? and ?birthplace? to extract instances of
these relations from the LDC North American News
Corpus. This helps us to extrinsically evaluate the
quality of the surface patterns.
6 Experimental Results
In this section, we present the results of the experi-
ments and analyze them.
6.1 Baselines
It is hard to construct a baseline for comparing the
quality of paraphrases, as there isn?t much work in
extracting surface level paraphrases using a mono-
lingual corpus. To overcome this, we show the effect
of reduction in corpus size on the quality of para-
phrases, and compare the results informally to the
other methods that produce syntactic paraphrases.
To compare the quality of the extraction patterns,
and relation instances, we use the method presented
by Ravichandran and Hovy (2002) as the baseline.
For each of the given relations, ?acquisition? and
?birthplace?, we use 10 seed instances, download
the top 1000 results from the Google search engine
for each instance, extract the sentences that contain
the instances, and learn the set of baseline patterns
for each relation. We then apply these patterns to
the test corpus and extract the corresponding base-
line instances.
6.2 Evaluation Criteria
Here we present the evaluation criteria we used to
evaluate the performance on the different tasks.
678
Paraphrases
We estimate the quality of paraphrases by annotating
a random sample as correct/incorrect and calculating
the accuracy. However, estimating the recall is diffi-
cult given that we do not have a complete set of para-
phrases for the input phrases. Following Szpektor et
al. (2004), instead of measuring recall, we calculate
the average number of correct paraphrases per input
phrase.
Surface Patterns
We can calculate the precision (P ) of learned pat-
terns for each relation by annotating the extracted
patterns as correct/incorrect. However calculating
the recall is a problem for the same reason as above.
But we can calculate the relative recall (RR) of the
system against the baseline and vice versa. The rela-
tive recallRRS|B of system S with respect to system
B can be calculated as:
RRS|B = CS?CBCB
where CS is the number of correct patterns found by
our system and CB is the number of correct patterns
found by the baseline. RRB|S can be found in a sim-
ilar way.
Relation Extraction
We estimate the precision (P ) of the extracted in-
stances by annotating a random sample of instances
as correct/incorrect. While calculating the true re-
call here is not possible, even calculating the true
relative recall of the system against the baseline is
not possible as we can annotate only a small sam-
ple. However, following Pantel et al (2004), we as-
sume that the recall of the baseline is 1 and estimate
the relative recall RRS|B of the system S with re-
spect to the baseline B using their respective pre-
cision scores PS and PB and number of instances
extracted by them |S| and |B| as:
RRS|B = PS?|S|PB?|B|
6.3 Gold Standard
In this section, we describe the creation of gold stan-
dard for the different tasks.
Paraphrases
We created the gold standard paraphrase test set by
randomly selecting 50 phrases and their correspond-
ing paraphrases from our collection of 2.5 million
phrases. For each test phrase, we asked two annota-
tors to annotate its paraphrases as correct/incorrect.
The annotators were instructed to look for strict
paraphrases i.e. equivalent phrases that can be sub-
stituted for each other.
To obtain the inter-annotator agreement, the two
annotators annotated the test set separately. The
kappa statistic (Siegal and Castellan Jr., 1988) was
? = 0.63. The interesting thing is that the anno-
tators got this respectable kappa score without any
prior training, which is hard to achieve when one
annotates for a similar task like textual entailment.
Surface Patterns
For the target relations, we asked two annotators to
annotate the patterns for each relation as either ?pre-
cise? or ?vague?. The annotators annotated the sys-
tem as well as the baseline outputs. We consider the
?precise? patterns as correct and the ?vague? as in-
correct. The intuition is that applying the vague pat-
terns for extracting target relation instances might
find some good instances, but will also find many
bad ones. For example, consider the following two
patterns for the ?acquisition? relation:
?ACQUIRER? acquired ?ACQUIREE? (5)
?ACQUIRER? and ?ACQUIREE? (6)
Example (5) is a precise pattern as it clearly identi-
fies the ?acquisition? relation while example (6) is
a vague pattern because it is too general and says
nothing about the ?acquisition? relation. The kappa
statistic between the two annotators for this task was
? = 0.72.
Relation Extraction
We randomly sampled 50 instances of the ?acquisi-
tion? and ?birthplace? relations from the system and
the baseline outputs. We asked two annotators to an-
notate the instances as correct/incorrect. The anno-
tators marked an instance as correct only if both the
entities and the relation between them were correct.
To make their task easier, the annotators were pro-
vided the context for each instance, and were free
to use any resources at their disposal (including a
web search engine), to verify the correctness of the
instances. The annotators found that the annotation
for this task was much easier than the previous two;
the few disagreements they had were due to ambigu-
ity of some of the instances. The kappa statistic for
this task was ? = 0.91.
679
Annotator Accuracy
Average # correct
paraphrases
Annotator 1 67.31% 4.2
Annotator 2 74.27% 4.28
Table 1: Quality of paraphrases
are being distributed to approved a revision to the
have been distributed to unanimously approved a new
are being handed out to approved an annual
were distributed to will consider adopting a
?are handing out approved a revised
will be distributed to all approved a new
Table 2: Example paraphrases
6.4 Result Summary
Table 1 shows the results of annotating the para-
phrases test set. We do not have a baseline
to compare against but we can analyze them in
light of numbers reported previously for syntac-
tic paraphrases. DIRT (Lin and Pantel, 2001) and
TEASE (Szpektor et al, 2004) report accuracies of
50.1% and 44.3% respectively compared to our av-
erage accuracy across two annotators of 70.79%.
The average number of paraphrases per phrase is
however 10.1 and 5.5 for DIRT and TEASE respec-
tively compared to our 4.2. One reason why this
number is lower is that our test set contains com-
pletely random phrases from our set (2.5 million
phrases): some of these phrases are rare and have
very few paraphrases. Table 2 shows some para-
phrases generated by our system for the phrases ?are
being distributed to? and ?approved a revision to
the?.
Table 3 shows the results on the quality of surface
patterns for the two relations. It can be observed
that our method outperforms the baseline by a wide
margin in both precision and relative recall. Table 4
shows some example patterns learned by our system.
Table 5 shows the results of the quality of ex-
tracted instances. Our system obtains very high pre-
cision scores but suffers in relative recall given that
the baseline with its very general patterns is likely
to find a huge number of instances (though a very
small portion of them are correct). Table 6 shows
some example instances we extracted.
acquisition birthplace
X agreed to buy Y X , who was born in Y
X , which acquired Y X , was born in Y
X completed its acquisition
of Y
X was raised in Y
X has acquired Y X was born in NNNNa in Y
X purchased Y X , born in Y
a
Each ?N? here is a placeholder for a number from 0 to 9.
Table 4: Example extraction templates
acquisition birthplace
1. Huntington Bancshares
Inc. agreed to acquire Re-
liance Bank
1. Cyril Andrew Ponnam-
peruma was born in Galle
2. Sony bought Columbia
Pictures
2. Cook was born in NNNN
in Devonshire
3. Hanson Industries buys
Kidde Inc.
3. Tansey was born in
Cincinnati
4. Casino America inc.
agreed to buy Grand Palais
4. Tsoi was born in NNNN in
Uzbekistan
5. Tidewater inc. acquired
Hornbeck Offshore Services
Inc.
5. Mrs. Totenberg was born
in San Francisco
Table 6: Example instances
6.5 Discussion and Error Analysis
We studied the effect of the decrease in size of the
available raw corpus on the quality of the acquired
paraphrases. We used about 10% of our original cor-
pus to learn the surface paraphrases and evaluated
them. The precision, and the average number of
correct paraphrases are calculated on the same test
set, as described in Section 6.2. The performance
drop on using 10% of the original corpus is signif-
icant (11.41% precision and on an average 1 cor-
rect paraphrase per phrase), which shows that we in-
deed need a large amount of data to learn good qual-
ity surface paraphrases. One reason for this drop
is also that when we use only 10% of the original
data, for some of the phrases from the test set, we do
not find any paraphrases (thus resulting in 0% accu-
racy for them). This is not unexpected, as the larger
resource would have a much larger recall, which
again points at the advantage of using a large data
set. Another reason for this performance drop could
be the parameter settings: We found that the qual-
ity of learned paraphrases depended greatly on the
various cut-offs used. While we adjusted our model
680
Relation Method # Patterns
Annotator 1 Annotator 2
P RR P RR
Acquisition
Baseline 160 55% 13.02% 60% 11.16%
Paraphrase Method 231 83.11% 28.40% 93.07% 25%
Birthplace
Baseline 16 31.35% 15.38% 31.25% 15.38%
Paraphrase Method 16 81.25% 40% 81.25% 40%
Table 3: Quality of Extraction Patterns
Relation Method # Patterns
Annotator 1 Annotator 2
P RR P RR
Acquisition
Baseline 1, 261, 986 6% 100% 2% 100%
Paraphrase Method 3875 88% 4.5% 82% 12.59%
Birthplace
Baseline 979, 607 4% 100% 2% 100%
Paraphrase Method 1811 98% 4.53% 98% 9.06%
Table 5: Quality of instances
parameters for working with smaller sized data, it is
conceivable that we did not find the ideal setting for
them. So we consider these numbers to be a lower
bound. But even then, these numbers clearly indi-
cate the advantage of using more data.
We also manually inspected our paraphrases. We
found that the problem of ?antonyms? was some-
what less pronounced due to our use of a large cor-
pus, but they still were the major source of error.
For example, our system finds the phrase ?sell? as
a paraphrase for ?buy?. We need to deal with this
problem separately in the future (may be as a post-
processing step using a list of antonyms).
Moving to the task of relation extraction, we see
from table 5 that our system has a much lower rel-
ative recall compared to the baseline. This was ex-
pected as the baseline method learns some very gen-
eral patterns, which are likely to extract some good
instances, even though they result in a huge hit to
its precision. However, our system was able to ob-
tain this performance using very few seeds. So an
increase in the number of input seeds, is likely to in-
crease the relative recall of the resource. The ques-
tion however remains as to what good seeds might
be. It is clear that it is much harder to come up with
good seed patterns (that our system needs), than seed
instances (that the baseline needs). But there are
some obvious ways to overcome this problem. One
way is to bootstrap. We can look at the paraphrases
of the seed patterns and use them to obtain more pat-
terns. Our initial experiments with this method using
handpicked seeds showed good promise. However,
we need to investigate automating this approach.
Another method is to use the good patterns from the
baseline system and use them as seeds for our sys-
tem. We plan to investigate this approach as well.
One reason, why we have seen good preliminary re-
sults using these approaches (for improving recall),
we believe, is that the precision of the paraphrases is
good. So either a seed doesn?t produce any new pat-
terns or it produces good patterns, thus keeping the
precision of the system high while increasing rela-
tive recall.
7 Conclusion
Paraphrases are an important technique to handle
variations in language. Given their utility in many
NLP tasks, it is desirable that we come up with
methods that produce good quality paraphrases. We
believe that the paraphrase acquisition method pre-
sented here is a step towards this very goal. We have
shown that high precision surface paraphrases can be
obtained by using distributional similarity on a large
corpus. We made use of some recent advances in
theoretical computer science to make this task scal-
able. We have also shown that these paraphrases
can be used to obtain high precision extraction pat-
terns for information extraction. While we believe
that more work needs to be done to improve the sys-
tem recall (some of which we are investigating), this
seems to be a good first step towards developing a
minimally supervised, easy to implement, and scal-
able relation extraction system.
681
References
P. G. Anick and S. Tipirneni. 1999. The paraphrase
search assistant: terminological feedback for iterative
information seeking. In ACM SIGIR, pages 153?159.
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Association for
Computational Linguistics, pages 597?604.
R. Barzilay and L. Lee. 2003. Learning to paraphrase: an
unsupervised approach using multiple-sequence align-
ment. In In Proceedings North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 16?23.
R. Barzilay and K. R. McKeown. 2001. Extracting para-
phrases from a parallel corpus. In In Proceedings of
Association for Computational Linguistics, pages 50?
57.
R. Barzilay, K. R. McKeown, and M. Elhadad. 1999.
Information fusion in the context of multi-document
summarization. InAssociation for Computational Lin-
guistics, pages 550?557.
M. Berland and E. Charniak. 1999. Finding parts in very
large corpora. In In Proceedings of Association for
Computational Linguistics, pages 57?64.
T. Brants. 2000. Tnt ? a statistical part-of-speech tag-
ger. In In Proceedings of the Applied NLP Conference
(ANLP).
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, pages 17?24.
M. S. Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In In Proceedings of the
thiry-fourth annual ACM symposium on Theory of
computing, pages 380?388.
T.M. Cover and J.A. Thomas. 1991. Elements of Infor-
mation Theory. John Wiley & Sons.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: ex-
ploiting massively parallel news sources. In In Pro-
ceedings of the conference on Computational Linguis-
tics (COLING), pages 350?357.
Z. Harris. 1954. Distributional structure. Word, pages
10(23):146?162.
M. A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the confer-
ence on Computational linguistics, pages 539?545.
D. Lin and P. Pantel. 2001. Dirt: Discovery of infer-
ence rules from text. In ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 323?328.
P. Pantel, D. Ravichandran, and E.H. Hovy. 2004. To-
wards terascale knowledge acquisition. In In Proceed-
ings of the conference on Computational Linguistics
(COLING), pages 771?778.
D. Ravichandran and E.H. Hovy. 2002. Learning sur-
face text for a question answering system. In Associ-
ation for Computational Linguistics (ACL), Philadel-
phia, PA.
D. Ravichandran, P. Pantel, and E.H. Hovy. 2005. Ran-
domized algorithms and nlp: using locality sensitive
hash function for high speed noun clustering. In In
Proceedings of Association for Computational Lin-
guistics, pages 622?629.
L. Romano, M. Kouylekov, I. Szpektor, I. Dagan, and
A. Lavelli. 2006. Investigating a generic paraphrase-
based approach for relation extraction. In In Proceed-
ings of the European Chapter of the Association for
Computational Linguistics (EACL).
B. Rosenfeld and R. Feldman. 2006. Ures: an unsuper-
vised web relation extraction system. In Proceedings
of the COLING/ACL on Main conference poster ses-
sions, pages 667?674.
S. Sekine. 2006. On-demand information extraction. In
In Proceedings of COLING/ACL, pages 731?738.
S. Siegal and N.J. Castellan Jr. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling web-based acquisition of entailment relations.
In In Proceedings of Empirical Methods in Natural
Language Processing, pages 41?48.
L. Zhou, C.Y. Lin, D. Munteanu, and E.H. Hovy. 2006.
Paraeval: using paraphrases to evaluate summaries au-
tomatically. In In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, pages 447?454.
682
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 186?187,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Statistical Shallow Semantic Parsing despite Little Training Data
Rahul Bhagat
Information Sciences
Institute
University of Southern
California
Marina del Rey,
CA, 90292, USA
rahul@isi.edu
Anton Leuski
Institute for Creative
Technologies
University of Southern
California
Marina del Rey,
CA, 90292, USA
leuski@ict.usc.edu
Eduard Hovy
Information Sciences
Institute
University of Southern
California
Marina del Rey,
CA, 90292, USA
hovy@isi.edu
1 Introduction and Related Work
Natural language understanding is an essential mod-
ule in any dialogue system. To obtain satisfac-
tory performance levels, a dialogue system needs
a semantic parser/natural language understanding
system (NLU) that produces accurate and detailed
dialogue oriented semantic output. Recently, a
number of semantic parsers trained using either
the FrameNet (Baker et al, 1998) or the Prop-
Bank (Kingsbury et al, 2002) have been reported.
Despite their reasonable performances on general
tasks, these parsers do not work so well in spe-
cific domains. Also, where these general purpose
parsers tend to provide case-frame structures, that
include the standard core case roles (Agent, Patient,
Instrument, etc.), dialogue oriented domains tend
to require additional information about addressees,
modality, speech acts, etc. Where general-purpose
resources such as PropBank and Framenet provide
invaluable training data for general case, it tends to
be a problem to obtain enough training data in a spe-
cific dialogue oriented domain.
We in this paper propose and compare a num-
ber of approaches for building a statistically trained
domain specific parser/NLU for a dialogue system.
Our NLU is a part of Mission Rehearsal Exercise
(MRE) project (Swartout et al, 2001). MRE is a
large system that is being built to train experts, in
which a trainee interacts with a Virtual Human using
voice input. The purpose of our NLU is to convert
the sentence strings produced by the speech recog-
nizer into internal shallow semantic frames com-
posed of slot-value pairs, for the dialogue module.
2 Parsing Methods
2.1 Voting Model
We use a simple conditional probability model
P (f | W ) for parsing. The model represents the
probability of producing slot-value pair f as an out-
put given that we have seen a particular word or
n-gram W as input. Our two-stage procedure for
generating a frame for a given input sentence is: (1)
Find a set of all slot-value that correspond with each
word/ngram (2) Select the top portion of these can-
didates to form the final frame (Bhagat et al, 2005;
Feng and Hovy, 2003).
2.2 Maximum Entropy
Our next approach is the Maximum Entropy (Berger
et al, 1996) classification approach. Here, we cast
our problem as a problem of ranking using a classi-
fier where each slot-value pair in the training data is
considered a class and feature set consists of the un-
igrams, bigrams and trigrams in the sentences (Bha-
gat et al, 2005).
2.3 Support Vector Machines
We use another commonly used classifier, Support
Vector Machine (Burges, 1998), to perform the
same task (Bhagat et al, 2005). Approach is sim-
ilar to Section 2.2.
2.4 Language Model
As a fourth approach to the problem, we use the Sta-
tistical Language Model (Ponte and Croft, 1997).
We estimate the language model for the slot-value
pairs, then we construct our target interpretation as
186
Method Precison Recall F-score
V oting 0.82 0.78 0.80
ME 0.77 0.80 0.78
SVM 0.79 0.72 0.75
LM1 0.80 0.84 0.82
LM2 0.82 0.84 0.83
Table 1: Performance of different systems on test
data.
a set of the most likely slot-value pairs. We use
unigram-based and trigram-based language mod-
els (Bhagat et al, 2005).
3 Experiments and Results
We train all our systems on a training set of 477
sentence-frame pairs. The systems are then tested on
an unseen test set of 50 sentences. For the test sen-
tences, the system generated frames are compared
against the manually built gold standard frames, and
Precision, Recall and F-scores are calculated for
each frame.
Table 1 shows the average Precision, Recall and
F-scores of the different systems for the 50 test sen-
tences: Voting based (Voting), Maximum Entropy
based (ME), Support Vector Machine based (SVM),
Language Model based with unigrams (LM1) and
Language Model based with trigrams (LM2). The
F-scores show that the LM2 system performs the
best though the system scores in general for all the
systems are very close. To test the statistical signifi-
cance of these scores, we conduct a two-tailed paired
Student?s t test (Manning and Schtze, 1999) on the
F-scores of these systems for the 50 test cases. The
test shows that there is no statistically significant dif-
ference in their performances.
4 Conclusions
This work illustrates that one can achieve fair suc-
cess in building a statistical NLU engine for a re-
stricted domain using relatively little training data
and surprisingly using a rather simple voting model.
The consistently good results obtained from all the
systems on the task clearly indicate the feasibility of
using using only word/ngram level features for pars-
ing.
5 Future Work
Having successfully met the initial challenge of
building a statistical NLU with limited training data,
we have identified multiple avenues for further ex-
ploration. Firstly, we wish to build an hybrid system
that will combine the strengths of all the systems to
produce a much more accurate system. Secondly,
we wish to see the effect that ASR output has on
each of the systems. We want to test the robustness
of systems against an increase in the ASR word er-
ror rate. Thirdly, we want to build a multi-clause
utterance chunker to integrate with our systems. We
have identified that complex multi-clause utterances
have consistently hurt the system performances. To
handle this, we are making efforts along with our
colleagues in the speech community to build a real-
time speech utterance-chunker. We are eager to dis-
cover any performance benefits. Finally, since we
already have a corpus containing sentence and their
corresponding semantic-frames, we want to explore
the possibility of building a Statistical Generator us-
ing the same corpus that would take a frame as input
and produce a sentence as output. This would take
us a step closer to the idea of building a Reversible
System that can act as a parser when used in one
direction and as a generator when used in the other.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley
framenet project. In Proceedings of COLING/ACL, page 8690, Montreal,
Canada.
Adam L. Berger, Stephen Della Pietra, and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural language processing. Computational
Linguistics, 22(1):39?71.
Rahul Bhagat, Anton Leuski, and Eduard Hovy. 2005. Statistical shallow
semantic parsing despite little training data. Technical report available at
http://www.isi.edu/?rahul.
Christopher J. C. Burges. 1998. A tutorial on support vector machines for pattern
recognition. Data Mining and Knowledge Discovery, 2(2):121?167.
Donghui Feng and Eduard Hovy. 2003. Semantics-oriented language understand-
ing with automatic adaptability. In Proceedings of Natural Language Process-
ing and Knowledge Engineering.
Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002. Adding semantic an-
notation to the penn treebank. In Proceedings of HLT Conference.
Christopher D. Manning and Hinrich Schtze. 1999. Foundations of Statistical
Natural Language Processing. The MIT Press, Cambridge, MA.
Jay M. Ponte and W. Bruce Croft. 1997. Text segmentation by topic. In Proceed-
ings of the First European Conference on Research and Advanced Technology
for Digital Libraries, pages 120?129.
W. Swartout, R. Hill, J. Gratch, W. Johnson, C. Kyriakakis, C. LaBore, R. Lind-
heim, S. Marsella, D. Miraglia, B. Moore, J. Morie, J. Rickel, M. Thiebaux,
L. Tuch, R. Whitney, and J. Douglas. 2001. Toward the holodeck: Integrating
graphics, sound, character and story. In Proceedings of Autonomous Agents.
187
Squibs
What Is a Paraphrase?
Rahul Bhagat?
USC Information Sciences Institute
Eduard Hovy??
USC Information Sciences Institute
Paraphrases are sentences or phrases that convey the same meaning using different wording.
Although the logical definition of paraphrases requires strict semantic equivalence, linguistics
accepts a broader, approximate, equivalence?thereby allowing far more examples of ?quasi-
paraphrase.? But approximate equivalence is hard to define. Thus, the phenomenon of para-
phrases, as understood in linguistics, is difficult to characterize. In this article, we list a set
of 25 operations that generate quasi-paraphrases. We then empirically validate the scope and
accuracy of this list by manually analyzing random samples of two publicly available paraphrase
corpora. We provide the distribution of naturally occurring quasi-paraphrases in English text.
1. Introduction
Sentences or phrases that convey the same meaning using different wording are called
paraphrases. For example, consider sentences (1) and (2):
(1) The school said that their buses seat 40 students each.
(2) The school said that their buses accommodate 40 students each.
Paraphrases are of interest for many current NLP tasks, including textual entailment,
machine reading, question answering, information extraction, and machine translation.
Whenever the text contains multiple ways of saying ?the same thing,? but the applica-
tion requires the same treatment of those various alternatives, an automated paraphrase
recognition mechanism would be useful.
One reason why paraphrase recognition systems have been difficult to build is
because paraphrases are hard to define. Although the strict interpretation of the
term ?paraphrase? is quite narrow because it requires exactly identical meaning,
in linguistics literature paraphrases are most often characterized by an approxi-
mate equivalence of meaning across sentences or phrases. De Beaugrande and Dressler
(1981, page 50) define paraphrases as ?approximate conceptual equivalence among
? 24515 SE 46th Terrace Issaquah, WA 98029. E-mail: me@rahulbhagat.net.
?? 24515 SE 46th Terrace Issaquah, WA 98029. E-mail: hovy@isi.edu.
Submission received: 5 July 2012; revised submission received: 21 January 2013; accepted for publication:
6 March 2013.
doi:10.1162/COLI a 00166
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
outwardly different material.? Hirst (2003, slide 9) defines paraphrases as ?talk(ing)
about the same situation in a different way.? He argues that paraphrases aren?t fully
synonymous: There are pragmatic differences in paraphrases, namely, difference of eval-
uation, connotation, viewpoint, and so forth. According to Mel?cuk (2012, page 7) ?An
approximate synonymy of sentences is considered as sufficient for them to be produced
from the same SemS.? He further adds that approximate paraphrases include implica-
tions (not in the logical sense, but in the everyday sense). Taking an extreme view, Clark
(1992, page 172) rejects the idea of absolute synonymy by saying ?Every two forms (in
language) contrast in meaning.? Overall, there is a large body of work in the linguistics
literature that argues that paraphrases are not restricted to strict synonymy.
In this article, we take a broad view of paraphrases. To avoid the conflict between
the notion of strict paraphrases as understood in logic and the broad notion in linguis-
tics, we use the term quasi-paraphrases to refer to the paraphrases that we deal with here.
In the context of this article, the term ?paraphrases? (even without the prefix ?quasi?)
means ?quasi-paraphrases.? We define quasi-paraphrases as ?sentences or phrases that
convey approximately the same meaning using different words.? We ignore the fine
grained distinctions of meaning between sentences and phrases, introduced due to the
speaker?s evaluation of the situation, connotation of the terms used, change of modality,
and so on. For example, consider sentences (3) and (4).
(3) The school said that their buses seat 40 students each.
(4) The school said that their buses cram in 40 students each.
Here, seat and cram in are not synonymous: They carry different evaluations by the
speaker about the same situation. We, however, consider sentences (3) and (4) to be
(quasi) paraphrases. Similarly, consider sentences (5) and (6).
(5) The school said that their buses seat 40 students each.
(6) The school is saying that their buses might accommodate 40 students each.
Here, said and is saying have different tenses. Also, might accommodate and seat are not
synonymous, due to the modal verb might. We consider sentences (5) and (6) to be
quasi-paraphrases, however.
Note that this article focuses on defining quasi-paraphrases. It does not provide
direct implementation/application results of using them. We believe, however, that
this work will allow computation-oriented researchers to focus their future work more
effectively on a subset of paraphrase types without concern for missing important
material, and it will provide linguistics-oriented researchers with a blueprint of the
overall distribution of the types of paraphrase.
2. Paraphrasing Phenomena Classified
Although approximate equivalence is hard to characterize, it is not a completely un-
structured phenomenon. By studying various existing paraphrase theories?Mel?cuk
(2012), Harris (1981), Honeck (1971)?and through an analysis of paraphrases obtained
from two different corpora, we have discovered that one can identify a set of 25 classes
of quasi-paraphrases, with each class having its own specific way of relaxing the re-
quirement of strict semantic equivalence. In this section, we define and describe these
classes.
464
Bhagat and Hovy What Is a Paraphrase?
The classes described here categorize quasi-paraphrases from the lexical perspec-
tive. The lexical perspective defines paraphrases in terms of the kinds of lexical changes
that can take place in a sentence/phrase resulting in the generation of its paraphrases.
1. Synonym substitution: Replacing a word/phrase by a synonymous word/phrase,
in the appropriate context, results in a paraphrase of the original sentence/phrase. This
category covers the special case of genitives, where the clitic ?s is replaced by other
genitive indicators like of, of the, and so forth. This category also covers near-synonymy,
that is, it allows for changes in evaluation, connotation, and so on, of words or phrases
between paraphrases. Example:
(a) Google bought YouTube. ? Google acquired YouTube.
(b) Chris is slim. ? Chris is slender. ? Chris is skinny.
2. Antonym substitution: Replacing a word/phrase by its antonym accompanied by
a negation or by negating some other word, in the appropriate context, results in a
paraphrase of the original sentence/phrase. This substitution may be accompanied by
the addition/deletion of appropriate function words. Example:
(a) Pat ate. ? Pat did not starve.
3. Converse substitution: Replacing a word/phrase with its converse and inverting
the relationship between the constituents of a sentence/phrase, in the appropriate
context, results in a paraphrase of the original sentence/phrase, presenting the sit-
uation from the converse perspective. This substitution may be accompanied by the
addition/deletion of appropriate function words and sentence restructuring. Example:
(a) Google bought YouTube. ? YouTube was sold to Google.
4. Change of voice: Changing a verb from its active to passive form and vice versa re-
sults in a paraphrase of the original sentence/phrase. This change may be accompanied
by the addition/deletion of appropriate function words and sentence restructuring.
This often generates the most strictly meaning-preserving paraphrase. Example:
(a) Pat loves Chris. ? Chris is loved by Pat.
5. Change of person: Changing the grammatical person of a referenced object results in
a paraphrase of the original sentence/phrase. This change may be accompanied by the
addition/deletion of appropriate function words. Example:
(a) Pat said, ?I like football.? ? Pat said that he liked football.
6. Pronoun/Co-referent substitution: Replacing a pronoun by the noun phrase it
co-refers with results in a paraphrase of the original sentence/phrase. This also often
generates the most strictly meaning-preserving paraphrase. Example:
(a) Pat likes Chris, because she is smart. ? Pat likes Chris, because Chris
is smart.
465
Computational Linguistics Volume 39, Number 3
7. Repetition/Ellipsis: Ellipsis or elliptical construction results in a paraphrase of the
original sentence/phrase. Similarly, this often generates the most strictly meaning-
preserving paraphrase. Example:
(a) Pat can run fast and Chris can run fast, too. ? Pat can run fast and Chris
can, too.
8. Function word variations: Changing the function words in a sentence/phrase with-
out affecting its semantics, in the appropriate context, results in a paraphrase of the
original sentence/phrase. This can involve replacing a light verb by another light verb,
replacing a light verb by copula, replacing certain prepositions with other prepositions,
replacing a determiner by another determiner, replacing a determiner by a preposition
and vice versa, and addition/removal of a preposition and/or a determiner. Example:
(a) Results of the competition have been declared. ? Results for the
competition have been declared.
(b) Pat showed a nice demo. ? Pat?s demo was nice.
9. Actor/Action substitution: Replacing the name of an action by a word/phrase denot-
ing the person doing the action (actor) and vice versa, in the appropriate context, results
in a paraphrase of the original sentence/phrase. This substitution may be accompanied
by the addition/deletion of appropriate function words. Example:
(a) I dislike rash drivers. ? I dislike rash driving.
10. Verb/?Semantic-role noun? substitution: Replacing a verb by a noun correspond-
ing to the agent of the action or the patient of the action or the instrument used for
the action or the medium used for the action, in the appropriate context, results in
a paraphrase of the original sentence/phrase. This substitution may be accompanied
by the addition/deletion of appropriate function words and sentence restructuring.
Example:
(a) Pat teaches Chris. ? Pat is Chris?s teacher.
(b) Pat teaches Chris. ? Chris is Pat?s student.
(c) Pat tiled his bathroom floor. ? Pat installed tiles on his bathroom floor.
11. Manipulator/Device substitution: Replacing the name of a device by a word/
phrase denoting the person using the device (manipulator) and vice versa, in the
appropriate context, results in a paraphrase of the original sentence/phrase. This
substitution may be accompanied by the addition/deletion of appropriate function
words. Example:
(a) The pilot took off despite the stormy weather. ? The plane took off despite
the stormy weather.
12. General/Specific substitution: Replacing a word/phrase by a more general or more
specific word/phrase, in the appropriate context, results in a paraphrase of the original
466
Bhagat and Hovy What Is a Paraphrase?
sentence/phrase. This substitution may be accompanied by the addition/deletion of ap-
propriate function words. Hypernym/hyponym substitution is a part of this category.
This often generates a quasi-paraphrase. Example:
(a) I dislike rash drivers. ? I dislike rash motorists.
(b) Pat is flying in this weekend. ? Pat is flying in this Saturday.
13. Metaphor substitution: Replacing a noun by its standard metaphorical use and
vice versa, in the appropriate context, results in a paraphrase of the original sentence/
phrase. This substitution may be accompanied by the addition/deletion of appropriate
function words. Example:
(a) I had to drive through fog today. ? I had to drive through a wall of fog
today.
(b) Immigrants have used this network to send cash. ? Immigrants have used
this network to send stashes of cash.
14. Part/Whole substitution: Replacing a part by its corresponding whole and vice
versa, in the appropriate context, results in a paraphrase of the original sentence/
phrase. This substitution may be accompanied by the addition/deletion of appropriate
function words. Example:
(a) American airplanes pounded the Taliban defenses. ? American airforce
pounded the Taliban defenses.
15. Verb/Noun conversion: Replacing a verb by its corresponding nominalized noun
form and vice versa, in the appropriate context, results in a paraphrase of the original
sentence/phrase. This substitution may be accompanied by the addition/deletion of
appropriate function words and sentence restructuring. Example:
(a) The police interrogated the suspects. ? The police subjected the suspects to
an interrogation.
(b) The virus spread over two weeks. ? Two weeks saw a spreading of the
virus.
16. Verb/Adjective conversion: Replacing a verb by the corresponding adjective form
and vice versa, in the appropriate context, results in a paraphrase of the original
sentence/phrase. This substitution may be accompanied by the addition/deletion of
appropriate function words and sentence restructuring. Example:
(a) Pat loves Chris. ? Chris is lovable to Pat.
17. Verb/Adverb conversion: Replacing a verb by its corresponding adverb form and
vice versa, in the appropriate context, results in a paraphrase of the original sentence/
phrase. This substitution may be accompanied by the addition/deletion of appropriate
function words and sentence restructuring. Example:
(a) Pat boasted about his work. ? Pat spoke boastfully about his work.
467
Computational Linguistics Volume 39, Number 3
18. Noun/Adjective conversion: Replacing a verb by its corresponding adjective form
and vice versa, in the appropriate context, results in a paraphrase of the original
sentence/phrase. This substitution may be accompanied by the addition/deletion of
appropriate function words and sentence restructuring. Example:
(a) I?ll fly by the end of June. ? I?ll fly late June.
19. Verb-preposition/Noun substitution: Replacing a verb and a preposition denoting
location by a noun denoting the location and vice versa, in the appropriate context,
results in a paraphrase of the original sentence/phrase. This substitution may be
accompanied by the addition/deletion of appropriate function words and sentence
restructuring. Example:
(a) The finalists will play in Giants stadium. ? Giants stadium will be the
playground for the finalists.
20. Change of tense: Changing the tense of a verb, in the appropriate context, results
in a paraphrase of the original sentence/phrase. This change may be accompanied
by the addition/deletion of appropriate function words. This often generates a quasi-
paraphrase, although it might be semantically less accurate than many other quasi-
paraphrases. Example:
(a) Pat loved Chris. ? Pat loves Chris.
21. Change of aspect: Changing the aspect of a verb, in the appropriate context, results
in a paraphrase of the original sentence/phrase. This change may be accompanied by
the addition/deletion of appropriate function words. Example:
(a) Pat is flying in today. ? Pat flies in today.
22. Change of modality: Addition/deletion of a modal or substitution of one modal
by another, in the appropriate context, results in a paraphrase of the original sen-
tence/phrase. This change may be accompanied by the addition/deletion of appro-
priate function words. This often generates a quasi-paraphrase, although it might be
semantically less accurate than many other quasi-paraphrases. Example:
(a) Google must buy YouTube. ? Google bought YouTube.
(b) The government wants to boost the economy. ? The government hopes to
boost the economy.
23. Semantic implication: Replacing a word/phrase denoting an action, event, and so
forth, by a word/phrase denoting its possible future effect, in the appropriate context,
results in a paraphrase of the original sentence/phrase. This may be accompanied by
the addition/deletion of appropriate function words and sentence restructuring. This
often generates a quasi-paraphrase. Example:
(a) Google is in talks to buy YouTube. ? Google bought YouTube.
(b) The Marines are fighting the terrorists. ? The Marines are eliminating
the terrorists.
468
Bhagat and Hovy What Is a Paraphrase?
24. Approximate numerical equivalences: Replacing a numerical expression (a word/
phrase denoting a number, often with a unit) by an approximately equivalent nu-
merical expression (even perhaps with change of unit), in the appropriate context,
results in a paraphrase of the original sentence/phrase. This often generates a quasi-
paraphrase. Example:
(a) At least 23 U.S. soldiers were killed in Iraq last month. ? About 25 U.S.
soldiers were killed in Iraq last month.
(b) Disneyland is 32 miles from here. ? Disneyland is around 30 minutes
from here.
25. External knowledge: Replacing a word/phrase by another word/phrase based on
extra-linguistic (world) knowledge, in the appropriate context, results in a paraphrase
of the original sentence/phrase. This may be accompanied by the addition/deletion of
appropriate function words and sentence restructuring. This often generates a quasi-
paraphrase, although in some cases preserves meaning exactly. Example:
(a) We must work hard to win this election. ? The Democrats must work hard
to win this election.
(b) The government declared victory in Iraq. ? Bush declared victory in Iraq.
3. Analysis of Paraphrases
In Section 2, we presented a list of lexical changes that define quasi-paraphrases. In this
section, we seek to validate the scope and accuracy of this list. Our analysis uses two
criteria:
1. Distribution: What is the distribution of each of these lexical changes in a paraphrase
corpus?
2. Human judgment: If one uses each of the lexical changes, on applicable sentences,
how often do each of these changes generate acceptable quasi-paraphrases?
3.1 Distribution
We used the following procedure to measure the distribution of the lexical changes:
1. We downloaded paraphrases from two publicly available data sets containing
sentence-level paraphrases: the Multiple-Translations Corpus (MTC) (Huang, Graff,
and Doddington 2002) and the Microsoft Research (MSR) paraphrase corpus (Dolan,
Quirk, and Brockett 2004). The paraphrase pairs come with their equivalent parts
manually aligned (Cohn, Callison-Burch, and Lapata 2008).
2. We selected 30 sentence-level paraphrase pairs from each of these corpora at random
and extracted the corresponding aligned and unaligned phrases.1 This resulted in 210
phrase pairs for the MTC corpus and 145 phrase pairs for the MSR corpus.
1 We assume that any unaligned phrase is paired with a null phrase and we discard it prior to the analysis.
469
Computational Linguistics Volume 39, Number 3
3. We labeled each of the phrase pairs with the appropriate lexical changes defined in
Section 2. If any phrase pair could not be labeled by a lexical change from Section 2, we
labeled it as unknown.
4. We finally calculated the distribution of each label (lexical change), over all the labels,
for each corpus. Table 1 shows the percentage distribution of the lexical changes in the
MTC (column 3) and MSR corpora (column 4).
3.2 Human Judgment
In this section, we explain the procedure we used to obtain the human judgments of the
changes that define paraphrases from the lexical perspective:
1. We randomly selected two words or phrases from publicly available resources (de-
pending on the lexical change) for each of the lexical operations from Section 2 (except
external knowledge). For example, to obtain words for synonym substitution, we used
WordNet (Fellbaum 1998) (and selected a word, say buy); to obtain implication rules
for semantic implication, we used the DIRT resource (Lin and Pantel 2001); and so on.
Table 1
Distribution and Precision of paraphrases. Distribution may not sum to 100% due to rounding.
# Category % Distribution MTC % Distribution MSR % Precision
1. Synonym substitution 37 19 95
2. Antonym substitution 0 0 65
3. Converse substitution 1 0 75
4. Change of voice 1 1 85
5. Change of person 0 1 80
6. Pronoun/Co-referent
substitution
1 1 70
7. Repetition/Ellipsis 4 4 100
8. Function word variations 37 30 85
9. Actor/Action substitution 0 0 75
10. Verb/?Semantic-role noun?
substitution
1 0 60
11. Manipulator/Device substitution 0 0 30
12. General/Specific substitution 4 3 80
13. Metaphor substitution 0 1 60
14. Part/Whole substitution 0 0 65
15. Verb/Noun conversion 2 3 100
16. Verb/Adjective conversion 1 0 55
17. Verb/Adverb conversion 0 0 65
18. Noun/Adjective conversion 0 0 80
19. Verb-preposition/
Noun substitution
0 0 65
20. Change of tense 4 1 70
21. Change of aspect 1 0 95
22. Change of modality 1 0 80
23. Semantic implication 1 4 70
24. Approximate numerical
equivalences
0 2 95
25. External knowledge 6 32 95
26. Unknown 0 0 NA
470
Bhagat and Hovy What Is a Paraphrase?
2. For each selected word or phrase, we obtained five random sentences from the Giga-
word corpus. These sentences were manually checked to make sure that they contained
the intended sense of the word or phrase. This gave us a total of 10 sentences for each
phenomenon. For example, for the word buy, one of the selected sentences might be:
(a) They want to buy a house.
3. For each sentence selected in step 2, we applied the corresponding lexical changes to
the word or phrase selected in step 1 to generate a potential paraphrase.2 For example,
we might apply synonym substitution to sentence (a) and replace the word buy with its
WordNet synonym purchase. This will result in the following sentence:
(b) They want to purchase a house.
4. For the phenomenon of external knowledge, we randomly sampled a total of 10 sen-
tence pairs from the MTC and MSR corpora, such that the pairs were paraphrases based
on external knowledge.
5. We gave the sentence pairs to two annotators and asked them to annotate them as
either paraphrases or non-paraphrases. For example, the annotator might be given the
sentence pair (a) and (b) and she/he might annotate this pair as paraphrases.
6. We used the annotations from each of the annotators to calculate the precision per-
centage for each lexical change. The final precision score was calculated as the average
of the precision scores obtained from the two annotations. Table 1 shows the percentage
precision (column 5) of lexical changes in this test corpus.
7. We finally calculated the kappa statistic (Siegal and Castellan Jr. 1988) to measure the
inter-annotator agreement. A kappa score of ? = 0.66 was obtained on the annotation
task.
4. Conclusion
A definition of what phenomena constitute paraphrases and what do not has been a
problem in the past. Whereas some people have used a very narrow interpretation
of paraphrases?paraphrases must be exactly logically equivalent?others have taken
broader perspectives that consider even semantic implications to be acceptable para-
phrases. To the best of our knowledge, outside of specific language interpretation frame-
works (like Meaning Text Theory [Mel?cuk 1996]), no one has tried to create a general,
exhaustive list of the transformations that define paraphrases. In this article we provide
such a list. We have also tried to empirically quantify the distribution and accuracy of
the list. It is notable that certain types of quasi-paraphrases dominate whereas others
are very rare. We also observed, however, that the dominating transformations vary
based on the type of paraphrase corpus used, thus indicating the variety of behavior
exhibited by the paraphrases. Based on the large variety of possible transformations that
can generate paraphrases, its seems likely that the kinds of paraphrases that are deemed
useful would depend on the application at hand. This might motivate the creation of
2 The words in the new sentence were allowed to be reordered (permuted) if needed and only function
words (and no content words) were allowed to be added to the new sentence.
471
Computational Linguistics Volume 39, Number 3
application-specific lists of the kinds of allowable paraphrases and the development of
automatic methods to distinguish the different kinds of paraphrases.
Acknowledgments
The authors wish to thank Jerry Hobbs and
anonymous reviewers for valuable
comments and feedback.
References
Clark, E. V. 1992. Conventionality and
contrasts: Pragmatic principles with
lexical consequences. In Andrienne Lehrer
and Eva Feder Kittay, editors, Frame,
Fields, and Contrasts: New Essays in
Semantic Lexical Organization. Lawrence
Erlbaum Associates, Hillsdale, NJ,
pages 171?188.
Cohn, T., C. Callison-Burch, and M. Lapata.
2008. Constructing corpora for the
development and evaluation of
paraphrase systems. Computational
Linguistics, 34(4):597?614.
De Beaugrande, R. and W. V. Dressler.
1981. Introduction to Text Linguistics.
Longman, New York, NY.
Dolan, B., C. Quirk, and C. Brockett.
2004. Unsupervised construction of
large paraphrase corpora: Exploiting
massively parallel news sources.
In Proceedings of the Conference on
Computational Linguistics (COLING),
pages 350?357, Geneva.
Fellbaum, C. 1998. An Electronic Lexical
Database. MIT Press, Cambridge, MA.
Harris, Z. 1981. Co-occurence and
transformation in linguistic structure.
In Henry Hiz, editor, Papers on Syntax.
D. Reidel Publishing Co., Dordrecht,
pages 143?210. First published in 1957.
Hirst, G. 2003. Paraphrasing paraphrased.
Invited talk at the ACL International
Workshop on Paraphrasing, Sapporo.
Honeck, Richard P. 1971. A study of
paraphrases. Journal of Verbal Learning
and Verbal Behavior, 10(4):367?381.
Huang, S., D. Graff, and G. Doddington.
2002. Multiple-translation Chinese
corpus. Linguistic Data Consortium,
Philadelphia, PA.
Lin, D. and P. Pantel. 2001. Dirt: Discovery of
inference rules from text. In ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 323?328,
San Francisco, CA.
Mel?cuk, I. 1996. Lexical functions: A tool for
description of lexical relations in a lexicon.
In Leo Wanner, editor, Lexical Functions
in Lexicography and Natural Language
Processing. John Benjamins Publishing Co.,
Philadelphia, PA, pages 37?102.
Mel?cuk, I., 2012. Semantics: From Meaning
to Text. John Benjamins Publishing Co.,
Philadelphia, PA.
Siegal, S. and N. J. Castellan, Jr. 1988.
Nonparametric Statistics for the Behavioral
Sciences. McGraw-Hill, Columbus, OH.
472
