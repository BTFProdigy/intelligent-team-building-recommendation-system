Automated Generalization of Translation Examples 
Ra l f  D .  Brown 
()a.lnc'g'c Mellon Univclsity, l~a.nguage Technologies 111stil;ul:('. 
l)ittsl)urgh, I)A \]5213-3890 
ralf+ ~,/cs. (;1 n u. cdu 
Abst ract  
l~t:ovious work has shown thai adding gen- 
era.liza.tion of the exa.ml)les in the corpus of 
a.n exa.ml)le-1)ased machine tra.nsla.tion (I'31LMT) 
system ea, n reduce 1;he re(ltfire.d amount  o\[' pre- 
tra.nsla.ted exa.ml)le text l)y as \[iltl(;\]l }is a.ii order 
o\[' magnitude for Spa.nish-l';nglish and l,'rench- 
l~;nglish I+',I~Mrl '. Using word clusto.t:itlg to a.tt- 
toma.ticaJly generalize the example eorl>uS ca.n 
provide the majority o\[' this inlprovement for 
l,'rench-l'hlglish wil;h no nlanuaI illtervelltioll; 
the prior work required a. la.rge I)iliugual dic- 
lionary ta.gged wil;}l 1)a.rls of speech aud the 
manual crea.tion of gl'.%llllll.:ll" rules. /~y seeding 
the clustering with a. small a.mou nt of manually- 
crea.ted iM'orma.tion, even t)el;ter t)erl'ornla.nce 
ea.n be a.chieved. This pa.l)ev descril)es a. method 
whereby bilingual word clustering ca.n 1)e per- 
\[brined using sta.nda.rd 'nto,zoli'n.qttal document 
cl ustering techniques, a,nd its e\[l'ectiveness at re- 
d ucing the. size of the exam l)le corpus ,'eq u ire(I. 
1 h l t roduct ion  
I';xanq)le-I{ased Machine 'l'ranslaLion (I';I{M'I') 
relies on a. collection of textual units (usually 
sentences) and their  tra, nsla, l,ions. New text  |,o 
1)e tra, nsla, ted is nla,tched a,ga, inst the source- 
langua.ge ha.If of the colh'x;tion, and the corre- 
sponding tra.nsla.tions from the ta.rget-langua.ge 
half axe used to generate a. l;ra.nsh~tion of the 
new text. 
l~xperience with several language pairs has 
shown that producing a.n EBMT system which 
provides reasomt.ble t, ra.nsla.tion coverage of un- 
restricted texts using simple textual matching 
requires on the order of two million words of 
pre-translated texts (one million words in each 
l;mguage); if either la.nguage is highly in\[letting, 
polysynthetic, or (worse yet) a.gglu tina.tive, even 
llloro text will be required. It ma.y I)e difficult, 
time-consuming, and expensive to obtain tha.t 
much pa.rallel text, pa.rtieula.rly for lesser-used 
la.nguage pairs. Thus, it' one' wishes to develop 
a. new tr,~nslator ra.pidly a.nd a.t low cost, tech- 
niques are needed which permit the 131~MT sys~ 
tom to 1)erform just as well using substantia.lly 
less example text. 
lk~th the C,a.ijin Ii;I~MT system 1)y Veale and 
~ " r \Va.y (\]997) and 1,he a.uthor's l~\]~h/l I sySteln 
(I999) COllVel't {;he examples in the corpus into 
teml)la.tes against which the new texts ea.n I)e 
ma.tched. (la.ijin va.ria.I)lizes the well-formed 
segment mappings between source a.nd ta.rget 
sentences 1;}ta.t i  is able to find, using a. closed 
set o\[' markers to segment 1.he input into l)hrasos. 
q'he a.utllor'.~ syslem i)er\['orms its generaliza.tioll 
using equix,a.lence classes (both syntactic a.nd se- 
ma.ntic) a.nd a. production-rule grammar. First, 
any occurrences of terms conta,ined in a,n equiv- 
alence class are replaced l)y a. token giving 1.he 
name of the equiwdence (:lass, a.nd then the 
gramma.r ules a~re used to replace l)a.tterns of 
words a.nd tokens I)y more genera.l tokens (such 
as <NI '> for noun phrases). (\]{town, 1999) 
showed t\]la.t one ca.n reduce the corpus size by 
as much as a.\]l order o\[' ma.gnitude in this way. 
(liven l;ha.t, explicit, ma.llua.lly-gom~ra.ted equi- 
va.lence classes red uce the need for exam l)le text, 
an obvious extelmion would l)e I;o ;~tte\]nl)t lo 
gelleral.e tll(~se classes a.ul;olna.tica.l\[y frolll the 
corpus of pre-tra.nslated exanlples. This pa.- 
1)or describes ()lie ~q)l)roa,ch to a.utoma.ted ex- 
1;racl;ioll of equiva.lence classes, using clustering 
teclmiques. 
The rema.inder of this l)aper describes how 
to 1)erform bilingua.1 word clustering using stan- 
dard monoh;ngual document clustering tech- 
niques 1)y converting the problem space; the 
va.rious clustering algorithms which were inves- 
tiga.ted; mid the effectiveness of generaliza.tion 
using the derived clusters a.t reducing the re- 
quired amount of example text. 
2 Conver t ing  the  Prob lem 
The task of clustering words a.ccording to their 
occurrence pa, tterns ca, n 1)e testa,ted as a, sta, n- 
dard document-clustering task by converting 
the l)rol)lem sl)a.ce. For each unique word to be 
clllstered, crea.te a. l)seudo-doculnent conta.ining 
the words of the contexts in which theft word N)- 
125 
pears, and use the word itself as tile document 
identifier. After the pseudo-documents are clus- 
tered, retrieving the identitier for each docu- 
ment in a particular cluster l)roduces tile list of 
words occurring in su\[\[iciently similar contexts 
to be considered equivalent \['or the l)urposes of 
generalizing an EBM(1 ~ system. 
By itself, this approach only produces a 
monolingual clustering, but we require a, bilin- 
gum clustering fox" proper generalization since 
different senses of a word will appear in differing 
contexts. The method of Barrachina and Vilar 
(1999) provides the means for injecting bilingual 
information into the clustering process. 
Using a bilingual dictionary - -  which may be 
created fl'om the corl)us using statistical meth- 
()<Is, such as those of Peter \]~rown el al (71990) or 
the author's own l)r(~viotls? work (Brown, 11997) 
and the parallel text, create a rough ma.pping 
1)etween the words in the source-language half of 
each translation example in tile corpus and tile 
target-language half el'that example. Whenever 
there is exactly one l)ossible translation candi- 
date listed for a word by the mapping, generate 
a bilingual word pair consisting of the word and 
its translation. This word pair will be treated 
as an indivisible token in further processing, 
adding bilingual information to the clustering 
process. \]eorming 1)airs in this manner causes 
each distinct translation of a. word to be treated 
as a separate sense; although translation pairs 
do not exactly correspond to word senses, pairs 
can be formed without any additional knowl- 
edge sonrces and are what tile EBM:I' systern 
requires for its equivalence classes. 
1,'or every unique word pair found in the 1)re- 
vious step, we a.ccurnulate counts for each word 
in the surrounding context of its occurrences. 
The context of ~n occurrence is defined to be 
tile N words immediately prior to and the N 
words immediately following the occurrence; N 
currently is set to 3. Because word order is im- 
portant, counts are accumulated separately for 
each position within the context, i.e. for N = 3, 
a particular context word may contribute to any 
of six different counts, depending on its loca- 
tion relative to the occurrence. Further, as the 
distance ffoln the occurrence increases, the sur- 
rounding words become less likely to be a true 
part of the word-pair's context, so tile counts 
are weighted to give the greatest importance 
to the words immediately adjacent o the word 
pair being examined. Currently, a silnple linear 
decay fl'om 1.0 to -~ is used, but other decay 
functions such as the reciprocal of the distance 
are also possible. Tile resulting weighted set of 
word counts tbrms the above-mentioned I)seudo- 
document which is converted into a term vector 
Ibr cosine similarity computations (a standaM 
measure in information retrieval, defined as the 
dot product of two term vectors normalized to 
unit length), 
If the clustering is seeded with a. set of ini- 
tial equivalence classes (which will be discussed 
below), then the equivalences will be used to 
generalize the contexts as they are added to tile 
overall counts \['or tile word pair. Any words in 
the context for which a unique correspondence 
can be found (and f'or which the word and its 
corresponding translation are one of the pah:s 
in an equivalence class) will be counted as if the 
name of the equivMence class had been l)resent 
in the text rather than the original word. For 
example, if days of the week are an equivalence 
class, then ':(lid he come on Fridas:' and "did 
he leave on Mends3:' will yield identical con- 
text vectors for "come" and "leave", maldng it 
easier \['or those two terms to chlster together. 
To illustrate the conversion process, consider 
tile li'rench word "('inq" in two examl)les where 
it translates into English as ::five" (thus forming 
tile word pair "cinq_fi ve") : 
<NUt> <NI/L> Le ci,zq jours dcpuis la 
<NUL> <NUL> 73e five dags si~zce lhe 
ellcs com'me~,cc~w~,t c~z cinq jours .<NUL> 
they will begin i~), five days .<NUL> 
where <NUt> is used as a placeholder when 
the word pair is too near the beginning or end 
of the sentence for the flfll context o be present. 
Note that the word order on the target-language 
side \]s not considered when building the term 
vector, so it need llOt be the same as on the 
source-language side; the examples were chosen 
with the same word order merely for clarity. 
The resulting ternl vector for "cinqJive" is 
a.s follows, where the numbers in parentheses 
indicate the context word's position relative to 
the word pair under consideration: 
Word Occur Weight 
<NWl.>(-3) 1 0.333 
elles(-3) 1 0.333 
1 0.667 
commenceront(-2) 1 0.667 
Le(q) 1 1.ooo 
en(-1) 1 1.000 
jours(J) 2 2.000 
depuis(2) 1 0.667 
.(2) 1 0.667 
la(3) 1 0.333 
<NUL>(3)  1 0.333 
Term vectors such as tile above are then clus- 
tered to determine equivalent usages among 
words. 
126 
3 Cluster ing  Approaches  
A tota.l of six clustering a.lgoHthms ha.v(~ I)oen 
1.ested; th roe variants of grout)-a.vora.go. ('\]tlsl.('.,'- 
ins a.nd i, hree of agglomera.tive clustering. In- 
cl'omental group-a.vera.ge clustering was ilnple- 
mented tirst, to provide a. proof of concopt, 
borore the COml)uta.tiona.lly more expensive a.g- 
glomerative (bottom-up) clusteril~g was i lnple- 
mented. 
The incremental groul)-a.vera.ge a.lgoril;hms all 
exa.mine each word pair in turn, computing a 
similsu:ity measure to evory existing clustor. If 
th(; 1)(;st siinila.rity measur(; is a l)ov(~ a. l)r(;del;er -
nfin('d threshold, the new word pair is i)laced 
in tile corresponding cluster; otherwis% a now 
(;\]usi;er is crea.ted. The th roe varianl;s diltT, r only 
in tile simila.rity moasure eml)loyed: 
:1. cosin(; s imi lar i ty 1)(;1;w(~(;n 1,h(~ i)s(;u(lo<loc- 
umonl, a.nd the centroid o1" the oxisting clus- 
ter (standard grOUl)-a.vera.ge clusto.rillg;) 
2. a.verage of' i;\]lo cosine similaril;ies l)otwe(;n 
the l)seudo-docuni(;nl; a.nd all nl(;nll)ers o\[' 
the 0xisting (:lust(;,' (a.voragc-link clustor- 
ing) 
3. square root of' 1;h(; a.vcrag(; of 1;lie S(luared 
cosine simila.r\]l;io.s I)ctweon l;he l)seudo- 
( locuinent an(\] all molnl)(~,'s or l he existing 
('hlster (rool.-nloa.n-sqllar(, nlo(lifical.ion of 
average-liNl? clustering) 
Thoso i;hro(~ vnria.tiol,S give hlc,'eas\]ngly IIl()l'(': 
weight to 1,ho nea.rer mcml)ers of' tho oxist.ing 
cl ust;cr. 
Tim t)o(;1;oin-u 1) a.gglomera.tive algoril;hms all 
funcl;ion I)y (;tea.tills a. clustor For each I)Seudo- 
(\[o(:unlenl,, t;hon r(;i)(;a.1;(;(lly ln(u:ging l:li(; two 
clusl;ors wit l i  the \]iighesl; siinila.ril,y score unl,il 
110 (,WO C\]tlS|,orH \]lSt,vo ,% ,q\] i i l i la,r i l ;y .~(:Ol'(~ (~x('.(~(;d- 
ing a l)re(Iol;ornlino(\] 1;hl:eshold. The three vari-- 
;/,IIi;S }/,ga, ill differ ()lily ill 1;lio S\]liiilaril,y lllO}lStll'O 
O llll)loyc(l: 
\]. cosine simila.rity between clustor centroids 
(st~ul(la.rd agglomei:a.tivo clustering) 
2. a.vera.ge of cosine sitnilariLy 1)etween men> 
l)ers of the two clusters (a.vera.ge-tink) 
3. nia.xilnal cosino similarity betweon a.ny pair 
Of ni('.nll)oi:s of l,\]ie i;wo clusl;(',rs (single-lin\]{) 
l"oi: (;acli of the va.i:ia.tions a.bovc, the l)r(~(l(;1,er - 
niincd (;hreshol(I is a. funci;ion of word \['r(xluoncy. 
Two words wliich each a.l)l)ea.r only onc(Y in the 
entire tra.ining text a.nd ha.re a. high simila.rib, 
score a.ro more likely to ha.re a.l)l)ea.red in siniila.r 
contexl;s I)y cohicide.nce l:ha.n 1;wo wor(ls which 
each a,1)pea.r ill 1;he traJli i l/g 1;(;xi; lifty tin-its. 
l,'ro( t UO I / cy  
5 
(J 
7 
8 -  
10 - 
\ ]2  - \] 5 
>16 
Thresho ld -  
1 \] .00 
2 0.85 
3 0.80 
4 0.75 
0.70 
0.65 
0.60 
9 0.55 
1 \] 0.50 
0.45 
0.40 
I,'igure \] : Chtslel ing 'l'hro.shold t unction 
I~br exa.ml)le ~ when using threo words on ei- 
thor side as context, a.nd a. linca.r dcca.y in t;erm 
weights, two singleton words achievo a. sinlitar- 
it; 5, scor(', of ().321 (1.000 is the ma.ximum t)os- 
siblc) if just one o\[" the immodia,tely a(lja,ccnt 
words is the sa.mc for 1)oth, evon if none of' 1;ho 
other five context words axe the sa, mc'. /ks the 
number  o\[' occul 'renc('s increases,  l;ho contr i \ ])u-  
l,ion t,o the simila.rit,y score o\[' hidividua.l words 
decreases, ma.king it less likely 1;o encounter a 
high score by chance. Ilencc, we wish to set 
a. si;ricl;er 1;hres\],ol(l \['or clustering low-frequollcy 
words i;hati higho,'-l'roquelmy words. 
The thr(~shold Function is exI)ressc(l in 1,(~rms 
of tim fr('(lU(mcy o1" occurrence in th(~ 1,ra.il,ing 
1.exl.s. I"or si,,gle, ull('lus(;ere(\[ vord pairs, I, ho 
t'requollcy is sinll)ly 1,11o numb(~r ol' 1;hnos I, he 
wor(I 1)a.ir was (m(:ounl,(u'(,d. When I)e,'\['orn> 
ing groul)-a.\,erag(; ;lu.qlx;ring, the l'requoncy as- 
signod l;() a. ('\]/ml;('.r is tim sum o\[' (;h(; frequencios 
of a.ll the members; for agglomera.l.ive (:lust('.ri)lg, 
the \['re(ltten(;y is the sum when using cent;roids 
and 1,he lnaximunl fre(lucn('y <tnlong the m(;m- 
I)oJ'S wllen using l;he average or lmarest-,,(;ighl)or 
,~imila.rity. The va.lu(~ of' the (;hr(>shold \['or a. given 
pair of ('lusi,('ms is the va.lue of tim thr(~,~hold 
I'unction a.t the lower word frequency. \]:igure 1 
sl,ows l,h(', threshold tunction used in the (,Xl)Cr- 
iments whose results a, rc rel)ortcd here; cluster- 
ins is only allowed if the simila, rity measure is 
a.1)ove the indicated threshold vahm. 
On its own, clustering is quite suc(:essfill for 
generalizing EBMT ('Xaml)les, I)ut the fully- 
a.utomated t)roducl;ion of clusters is not com- 
t)a.tible with adding a, l)roduction-rule gra.mma.r 
as (lcscril)od in (l~rown, \]999). Therel'ore, the 
clustering process may 1)e seeded with a. set of 
m an u a.lly-gc'nera.ted clusters. 
VVhell seed clusters m'e a.va.ilablo., the cluster- 
ins process is moditied in two ways. First, l;he 
grOUl)-avera.ge a.pl)roa.clms a.dd an initiaJ clusl;er 
for o.a,('h soed cluslcr and the a.gglolnera.tive a p- 
127 
proaches add an initial cluster for each word 
pair; these initial clusters are tagged with the 
name of the seed cluster. Second, whenever a
tagged chister is merged with an untagged one 
or another cluster with the same tag, the com- 
bination inherits the tag; further, merging two 
clusters with different ags is disallowed. As a 
result, the initial seed chlsters are expanded by 
adding additional word pairs while preventing 
any of the seed clusters from themselves inerg- 
ing with each other. 
One special case is handled sepa.rately, 
namely numeric strings. If both the source- 
language and target-l~mguage words of a word 
pair are numeric strings, the word pair is treated 
as if it had been specified ill the seed class 
<number>.  Word pairs not containing a digit 
in either word can optionally be prevented fi'om 
being added to the <number> chlster unless 
explicitly seeded in that cluster. The former 
feature eusures that nunibers will apl)ear in a. 
single cluster, rather than in multiple chlsters. 
The latter avoids the inclusion of the many non- 
numeric word pairs (primarily adjectives) which 
would otherwise tend to cluster with numbers, 
because both they and numbers are used as 
modifiers. 
Once clustering is completed, any clusters 
which have inherited the same tag (which is 
possible when using agglomerative clustering) 
are merged. Those clusters which contain more 
than one pseudo-document areoutput,  together 
with any inherited label, a.nd can be used as a 
set of equivalence classes for EBMT. 
Agglomerative chlstering using the maximal 
cosine sinfila.rity (single-link) produced the sub- 
jectively best clusters, and was used for the ex- 
periments described here. 
4 Exper iment  
The Inethod described in the previous two 
sections was tested on French-English EBMT. 
The training corpus was a subset of the 1BM 
Ilansard corpns of Canadian parliamentary pro- 
ceedings (Linguistic Data Consortium, 1997), 
containing a total of slightly more than one 
million words, approximately half in each lan- 
guage. Word-level alignment between French 
and English was pertbrmed using a dictio- 
nary containing entries derived statistically 
from the full Hansard corpus, auglnented by 
the ARTH, French-English did;iona.ry (ARTFL 
Project, 1998). This dictionary was used for all 
EBMT and chlstering runs. 
The efl'ects of varying the amount of train- 
ing texts were determined by further sl)litting 
the training corl)us into smaller seglnents aM 
using differing numbers of segments. For each 
Clust I 
238 
260 
348 
522 
535 
1375 
1386 
1528 
;1563 
;1.652 
2008 
21.82 
2472 
3539 
M e lnbers  
lJ.IS'l'OIl{E HISTOIW 
ECONOMIE ECONOMY 
CERTAINI!~MENT CEI{TAI NLY 
CERTAINEMENT SURELY 
CERTES SURELY 
JAMAIS NEVER 
PAS NOT 
I~EUT-F, TRE MAY 
H~OI~ABLEMENT PROBAI~LY 
QUE ONLY 
l.{lfl';N NOTItING 
S\[JREMENT CERTAINLY 
SUREMENT SURELY 
VRAIMENT REALLY 
CONSERVATEUR CONSEIWATI \q~J 
CQNSERVATEUII TORY 
I)EMOCIi,NI.'IQUE DEMOCtl, ATIC 
I)I~IVl OCRATIQUIE NDP 
LIBI~RAL LII3ERA L 
l)l A{NII, A%LS LAS \] 
\])ERNIIjEI{ES PAST 
I)ERNIIERIDS I{h;CENT 
PI{OCI\]A INF, S NEXT 
Q UELQUES FEW 
QUF, LQUh;S SOME 
AVONS HA\q'; 
SOMMES ARI'~ 
p p t r ,  p 1 ,LLC 10RALL CAMPAIGN 
EM~2CTOIi,A ILF~ EIAECTION 
FI~I)I~RAM:,S-I)I {OXq N C IAL1,;S 
FEI)ERA L-PllOVINCIAI, 
INDUS'FRIEM3~S INI)US'I'IIIAI, 
OUVRIERES LA BOUR 
FA(,J()N h;VENT 
P ? 17' I~VIDLNCL CLEARLY 
EVIDh;NC\]'; OBVIO USIN 
HOMMF, S POIATICIANS 
PRISONNIFJ{S PR/SONEI{S 
RETOUR, BA.CII(, 
REVENIR BACK 
CONVENU AGREED 
SIGNE SIGNEI) 
VU SEEN 
AGRJCOLE AGR1C UL'I'URE 
ENT'IER AROUN\]) 
E N T I ER T Ill RO U G I\] O U T 
OCCIDENTAL WESTERN 
AVIDUGLI~S BI,IND 
CIIA.USSURI'2S SI-IOES 
CONSTRUC;I'EURS BUILDh;RS 
PENSIONN, F,S PENSIONERS 
RISTRAITES PENSIONERS 
VETEMENTS CLOTHING 
POISSON FISI\] 
PORC IK)RK 
Figure 2: Sanli)le Chlsters 
128 
run using clustering, the first K segments of 
the corl)uS a.re cones.Lena.ted into a. single file, 
which is used as inl)ut \['or both the clustering 
l)t:ogra, m a.nd the EI{M:I.' system. The clust;er- 
ltlg 1)rogranl is rtltt (;o deternfine a. set o1" equiv- 
alence classes, a.nd these classes a.re then pro- 
vkled to tile I';I{M:I' systetn a Jest  with the tra.in- 
ing exa, mples to be indexed, lleld-out lla.nsa.rd 
text (a,1)I)roxima.lsely d5,0()O words)is then tra.ns- 
laLed, +tnd tile l)ercenta.ge of tile words in the 
test text for which the I~;I~M~.I ' system could 
lind ma,tches a.nd generate a. tl'a.lasla.tion is de- 
termined. 
To test the efl'ects of adding seed ('lttsters+ 
a set of' initia.1 clusters was generated with 
the \]te.lp of the A I{:I'I"I, dict;iona.ry. First, the 
500 most frequ(:nt words in the milliou-word 
\]\]~msa.rd sul)se.t (excluding pun('.\[;uation) were 
extracted. These terms were then nmtched 
a.gMnst the AI~.TFI, dictionary, removing those 
words which had multi-word transla.tions as 
well a.s severaJ which listed multil)le parts el" 
sl)eech For the same tra,nslation (multil>le l>a, rts 
of speech can only 1)e used i\[' the corresi>on(I- 
ing tra.tlsla.tiolls are distinct f'rom each <)ther). 
The remaining d20 tra.nslal.ion pairs, tagged for 
l)a.rt o\[' speech, were then convert:e(l inl,o se(~(I 
clusters a.nd l)rovided to the clustering t)rogra.nl. 
To fa.cilita.te xperiments using the t)re-existing 
l)roduction-rule grammar, tire a.d(litiona,I tra.ns- 
la?ion I)a,h's from the lna,nually-gelmra, ix~(1 equiv- 
aJe.n(:e ('la.sses were a.dded t;o l)rovide seeds for 
five equiva.\]ence lasses which a.re not, l)resent in 
the dictiona.ry. 
5 Resu l ts  
The nlethod (les('ril>ed i,I this l)a, per does (Sttl) 
jectively) a, very good jol> of clustering like 
words toget\]wx, a lid using the clusters to getl- 
era.lize EI{MT gives a. (;onsidera.I)le boost, to the. 
l)etTVol'ltl~-Lt,ce+ of' the  l<\]\]\]\]\/l~\[ ' SySl;(':lll. 
l"igure 2 shows a, sa.ml)ling of tile sma.ller 
clusters generated from 1.\] million words o\[' 
Hansard text. While the nmmbers of a, clus- 
ter are o f ten semant ica, l ly  l inked (a,s in c luster  
848, which cotltains types of politica.1 paxl;ies, or 
cluster stag), they need not be. Those clusters 
whose members a.re not semantically linked gen- 
eraJly contain words which a.17e all the sa.me l)a.rt 
of sl)eech , numl)er, a.nd gender (a.s in (:luster 
2472, which costa.ins exclusively plural nouns) 
1)ut a.s will be discussed in t;he next section, 
even those chlsters whose ,neml)ers a.re tota.lly 
unrela.ted may 1)e useful a.nd correct.. One J'a.h:ly 
cotl l t l \ ]Otl  occur re l tce  a, l l lOl lg the smaller clusters 
is that various synonymous 1;ra.nslnt;ions o\[ a 
word (from either source or target language) 
will chlster together, as in cluster \]652. This 
is pa.rticula.rly useful when tile ta.rget-language 
word is the sa.me, a.s this a.llows va.rious wa.ys of 
expressing t.he same thing to be tra.nsla.ted when 
~l.lly Of" those  \['OFtlIS ~/l'e present in the tra.ining 
('orpl.t s. 
Figure 3 shows how adding a.utoma.tically- 
generated equiva.lence classes sul)sta.ntially in- 
c reases  the covers,we of the EI3MT system. A1- 
terna.tively, lnuch less text is required to rea.ch 
a. specific level of coverage. The lowest curve in 
the. graph is tile percentage of the d5,000-word 
test text for which the EI{M:J' system was able 
to genera.te tra.nsla.tions when using strict lexi- 
c+d matching against the trahling corpus. The 
lop-most curve shows the best performa.nce, pre- 
viously achieved using 1)oth a, la.rge set of eqttiva- 
lento classes (in t;he fornt of tagged entries from 
the \]\ItYI'II+'I, dicl;iona.rv) a.nd a. production-rule 
gra.nlntar (\]{rows, J999). Of the two center 
curves, the lower is the performs.nee when gen- 
era.lizing the tra.ining corl)us using the equiv- 
alence classes which were autolna.tica.lly goner- 
ated from that same text, a.nd tim upper shows 
the t)erforma.tlce using ('lustering with the d25 
seed pairs. 
/ks can b~, seen in Figure 3, 80% cover- 
age of the test text is achieved with less than 
300,000 words using nta.ntta.lly-crea.te(l gener- 
alizat, ion information a.nd with approxima.te- 
ly 300,000 words wllen using a.utonmtically- 
creaJ;ed genera.liza.tion i forma.tion, but requires 
1.2 million words when not using genera.liza.- 
ties. 90% covers.we is reached with less than 
500,000 words using lna.nua.lly-ereat.ed informa. 
lion a.nd should I>e reached with less t.ha.n 1.2 
tnillion words using a.utonm.tically-crealed gen- 
era.lization informa.tk)n, versus T million words 
without genera.liza.tion. Tiffs reduction I)y a. tim- 
(or of f  our to live in tile amount of text is accom- 
1)lishe(I with lit;tie o)' no degradation in the qual- 
ity of the tra.nsla.tions. Adding a. small amount 
of kt,owle(lge in the f'ornt o1" 425 seed pairs re- 
(lutes the required trahling text; even further; 
this ca.n la.rgely be attril)uted to the merging of 
clusters which would otherwise have rema.ined 
distinct, thus increasing the level of generaliza.- 
ties. 
Adding the production-rule gratnma.r to the 
seeded clustering had little effect. When usirtg 
more than 50,000 words of tra.ining text, the in- 
crease in coverage from adding the gram m a,r was 
negligible, and even with the sma.llest raining 
corl)ora, (,he+ increase wa.s very modest. 
Using the sa.me thresltolds tha.t were used in 
tile fully-~mtonla.tic case, clustering on 1.\] mil- 
lion words expands the initial 425 word pairs 
in 37 clusters to a200 word pairs, a.nd adds a.n 
additions.1 555 word pairs in \]d() further non- 
(;t:ivia,1 clusters. This (:Oral)ares very fa.vorably 
129 
c- 
O o 
(D 
C1) 
CD 
O 
O 
90 
80 
70 
60 
50 
40 
30 
20 
' s 
i 
- -x -  . . . .  : . . . . . .  x . . . . . . . . . . .  x . . . .  i . . . . .  - x  . . . . . . . . . .  * . . . .  . . . . .  
~- .~+" 
, zz 
V3 . . . .  i ~ , (a" / 
, / /  
/ 
/ 
i 
I 
0.2 0.4 0.6 
Corpus Size (millions of words) 
lexical matching only -~- -  
w i th  automatic clustering -4--  
clustering w/425 seeds -D--, 
full manual g~neralization --x--- 
0.8 
l i'igure 3: BI3MT \]~el'formance with and without Generalization 
to the 3506 word 1)airs in 221 clusters tbund 
without seeding. 
'l'he 1)rogram also runs reasonably quickly. 
The step of creating context term vectors con- 
verts approximately 500,000 words of raw text 
per minute on a 300 MHz processor. 1,'or ag- 
glomerative clustering, the processing time is 
roughly quadratic in the number of word \])airs, 
with a theoretical cubic worst case; the 17,527 
distinct word pairs found from the million-word 
training corpus require about 25 minutes to 
cluster. 
6 D iscuss ion  
One statement made earlier deserves cla.rifica- 
tion: l;he members of ~ cluster need not be re- 
lated to each other in any way, either syntacti- 
cally or semantically, for a cluster to be useful 
and correct. This is because (absent a gram- 
mar) we do not care about the features of tile 
words in the cluster, only wh, cthc~" their tr(msla- 
lion,s Jbllow the same pattcrT~. 
An illustration based on actual experience 
is useful here. In early testing of the group- 
average clustering algorithm with seeding, the 
<con junct ion> seed class of "and" and "or" 
was used. Clustering augmented this seed class 
with "," (comma.), "in", and %y". One can eas- 
ily see tha.t the comma is a valid member of the 
class, since it takes the place of "and" in lists 
of items. 13ut wllat about ':in" and "135;", wlfich 
are prepositions rather than conjunctions2 11' 
one considers the tra.nsbttion t)attern 
__  7~ C \[ ? __  Fr'eNl~ I'>cNP2 --+ EW/A 1 1 F-~:/NI):2 
it becomes clear that all of the terms in the 
expanded class give a correct translation when 
placed in the blank in this pattern, lndeed, 
one could imagine a production-rule grammar 
geared toward taking advmltage of such com- 
mon translation patterns regardless of conven- 
tional linguistic features. 
7 Conc lus ion  and  Future  Work  
Using word clustering to automatically gener- 
alize the example corpus of an I;BM'I? system 
can provide the majority of tile improvement 
which can be achieved using both ~ manually- 
generated set of equivalence ('lasses and a pro- 
duct;ion rule grammar. The use of a set of small 
initial equivalence classes produces a substan- 
tial further reduction in training text at a very 
low cost (a few hours) in lal)or. 
130 
An obvious {'~xtension to using st,.e{\] clusl;ors 
iS (;(} 1+180 (,110 I'Osllll; ()\[' a, ClU,'ql;{':l'illg 1"I+111 ;IS l;tl{? 
i\]lil;ia\] seed \['or a second it{;ra,l;io\]t o1' chlsl,er- 
ing, sin('{', th{; additional g{,neralization of lo- 
{;a.i COlll;{!xl;s cnabl(;d 1)y the la.rgcr s{,e(1 clusl,(,J's 
will l)ormit a.(l(litional ex\])allSiOll O\['LIlo clusl,(Brs. 
l:or such itera.tivo {:lustoring, a.II but the last 
rou n(1 shouI(1 l)l'(2Slllllal)ly USe sl;ri(;Ler 1,hresh- 
ol(Is, to avoi(1 adding goo many irr{;l{,A,ant inonl- 
t)ers Lo tim clusLers. I ) rd iminary OXl)erinmnts 
hay{ Bbeen inconclusive --although ihc result o\[' 
a second it{wation {'onta.ins more {,{'.rms ill the 
{;lusl;ers, IBBMT l}erforma.nce {toes not seem to 
lint)rove. 
More sophistica.ted {;hlsl;o.l'illg; a.lg(}rithms such 
as k-lneans and (l('+terlninLqtic a.nnealing l\]lay' 
1)rovi(lo \])etter-qua.lity clust{ws for bcl, ter t)ei't"of 
lllall{;e} :-1+,{; the  (~xi)ens(; of  illCl'Oas(;(\] t)ro{'eHsill~ 
tim{'.. 
This a.i)l}Z:oach to gelWXa.l,ing e(luival('Jw(~ 
cla.sses hould worl( j usl; as well \['or l)h rases as I'or 
single words, simply hy mo(lil~qng {;he conver- 
Si()ll SLOp 1;O el'oat;(; C(}lltOXt VeCl;ors l"or phrases. 
This enhancenmnt would elimi,lal;{'~ i;he current 
limitation t, hat trat,slal;ion \]):q,il:S l,O 1)O. clust(;red 
\]\]lUSt )O single words in 1)oth languages. \Vot:k 
or, this n\]o(lifi{;al;ion is {:urP(~ll|;ly ttn(ler way. 
An inleresting \['ui, ur{~ (;xI)eriment would 1)(~ 
tbr{'going gratnnlar rules based {)n standa.rd 
gl :a l l l l l l : - / , l ; ica l  \['{'.:-1+,l;tll'(~s Sl l{:h as \]).~l,rl, o\[' st){"(':{:\]l , 
and inst{,ad crea,tinp; a gran~ma, r guid(,{I I} 3 ,
{;1~{; ('lusters I'oun(l fully aul,o~tati{'ally (wil, houl, 
sce{liug) fronl th{~ exa.nll}lc re\l,. 'File r{,{:(;nt 
woH{ I)y +\(lcTait and 'l?..iil lo (I 999) {}, OXtl':dcl.,- 
ing tra1~slal, ion t}al;l;{'+rn,q woul(l a.t)poa.r t,o 1}o. a 
l){;rfe{:l; {;oml)lc'nmnt, as 1;h{'5 are it, e\[t'ect lind- 
i ,g {:ont;ext strings wit\], (}l}e. slots, while the 
work descril)ed h('.re lit,(ls {,he fillers I'(}1' tJ~{)s(' 
slots. (liv{;n the al)ility to learn such +1+. gra.mmar 
without l\]\]a.nual interv{mtion, it would \])e(:onl{'. 
I)ossil)l{~ to ere'at{; an I!'I:~MT 8yst{m\] usillg g{:ql- 
era, liz{,(l e,:aml)les from nol, hi\]~g ~n{)r{; than l)ar- 
allel l;ext~ which for n~any hulguag(, pairs could 
also 1)c acquired a hnost fully a, utom~tical ly 1)y 
crawling the World Wide VVel) (Resnil{, :1.998). 
References  
A\]~.TFL Project. :1998. I"re'nch-?'nglish. Dic- 
lionarg. I}roject for American and French 
l{esearch {}i\] the Treasury of {,he l:rench 
l,anguage, University o\[" Chicago. h t tp  : / / -  
human?t i es .  uch?cago, edu/kgTFL, html. 
Sergio Barrachina a.nd Juan Miguel \:ilar. 1999. 
Bilingual Clustering Using Monolingual Algo- 
rithms. Ill 15vcecdings of lhe l@hlh \]'n.lcrna- 
~ional (7onJ?rence on Theoretical and Method- 
ological Issues in Machinc 7)'anslatio'n (~1341- 
99), pages 77 87, Chester, England, August. 
I)ctor l~,roxvH, .l. (;ocke, S. l)ella \])iotra, V. I)olla 
l}h:l, ra, I:. dolinel<, 3. l,afl'erl.y, R. Mercer, and 
I }. I~.oossh,. 1990. A Statistical Approach 
to Machine Translation. COmlmtaHonal Lin- 
.qui.slic% 16:79 85. 
l{alF \]). \]h:own. 1997. Autonlated \]) ictionary 
l';xLracLion lot "l(nowlcdge-l,'ree:' Example- 
1},ascd Translation. In l)~vccedin:l s of ihc 
,5'cvcnlh h,l(rnaiional (.'o,@Tvncc on "IJm- 
orclical and A4clhodolo:lical l~ue.s in Ma- 
chi'~c 7}'an.~lalion (TM1-97), F, ages \ ]  11 1 \]8, 
Sant~t \]:c, New Mexico, July. ht;1;p://- 
www. cs.  cmu. edu/Oral: f /papers,  html.  
Rail" I). lh'own. 1999. Adding l,ingttistic l (nowl-  
edge Co a l,exica\] \]';xamp\]e-\]~ased Tra, nsla- 
tion System. In I)rocccding.s of thc 12iEM, h
hzlcrnational (,'onj)rencc on 7'hcorel, ical mzd 
Mclhodoh::lical fss.uc.~ in Machine 7)'anslation. 
(7341-99), pages 22-32, Chesl:er, I~;ngland, 
August. http ://www. cs.  cmu. edu/~rat f / -  
papers .html. 
I,illguistic I)al, a. Con.~orLium. 1997. lla~>'ard 
(,'ou~u.~ of I)aralld ?'Rqlish (rod t'Yc,~ch. 
IAnguistic l )ata  Consorl;ium, \])ecember. 
http  ://wwu. ldc .  upenn, edu/. 
l(evin McTl'a.it and Arturo Trujil\]o. 1999. A 
l,atlguag{>Neutral Sl)arse-\])ata Algorithnl fbr 
I:x tracti n g 'I'ranslation I}atterns. \] II \])ro(:ccd- 
in:l s of lhc l'/i:lhlh hztcrlzational Co~@rcncc 
on 77~colv:ical and Mcthodoh:gical Is.sues in 
MachMc 7~rmtslatio~t (7'A41-99), l)ages 98 
1()8, Ch{~ster, l'3ngla,(1, August. 
I}hilil } l{(,snik. \]998. I)ara.llel Strait(Is: A 1)re -
l iminary lnvesCiga.tion into Mining the \V(;b 
r 1 { for l~\]lingua.1 e\  . In I)a.vid l'a.rweI1, l,a.urle 
(:left)or, and Edua.rd llovy, editors, Mac.hine 
7}'a~.~laZion a d the hdbrm.alion Soup: 7'hird 
(;'on, fcrcncc of Ihc A.~..s'ociation for A'lachi~c 
'l}.anslation in Ihe Americas (A A4'F4-9S), vol- 
un\]e 1529 o\[' Lccl'mv No*ca in ArZi./icial lnlcl- 
ligc:~zcc, i)ages 72 82, l,anghorne, I}ennsylva - 
ida, ()ct{}l}er. Springer. 
Tony Vcalc and An(ly Wa.y. 1997. Gaijin: A 
'lhml)la.te-I)riven Bootstrapl)ing AI)l)roacll 
to Exa, ml)Ie-Ba.sed Ma,chin(; Tra.nsla.tion. 
:It\] 1}roecedin:p of the NeMNLl~'97. New 
Mel.hods in Natural Langauge 1)rocessessin9, 
Sofia., lhdgaria., Sel)teml)er. h t tp : / / -  
wwu. compapp, dcu. ie / " tonyv /papers / -  
ga i j  in .  html. 
Ellen M. \:oorhees. 1986. lmplel\]mnting Ag- 
glomera.tive ll ierarchical Clustering Algo- 
r ithms for Use in \ ] )ocument l/etrieval. 
I'nJbrmation l~rocessi'ng and Mana.qeme'nt, 
22(6):d65 ,176. 
131 
Adapting an Example-Based Translation System to
Chinese
Ying Zhang, Ralf D. Brown, and Robert E. Frederking
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213-3890 USA
fjoy,ralf,refg@cs.cmu.edu
ABSTRACT
We describe an Example-Based Machine Translation (EBMT) sys-
tem and the adaptations and enhancementsmade to create a Chinese-
English translation system from the Hong Kong legal code and var-
ious other bilingual resources available from the Linguistic Data
Consortium (LDC).
1. BACKGROUND
We describe an Example-Based Machine Translation (EBMT)
system and the adaptations and enhancements made to create a
Chinese-English translation system from the Hong Kong legal code
and various other bilingual resources available from the Linguistic
Data Consortium (LDC).
The EBMT software [1, 3] used for the experiments described
here is a shallow system which can function using nothing more
than sentence-alignedplaintext and a bilingual dictionary; and given
sufficient parallel text, the dictionary can be extracted statistically
from the corpus [2]. To perform a translation, the program looks up
all matching phrases in the source-language half of the parallel cor-
pus and performs a word-level alignment on the entries containing
matches to determine a (usually partial) translation. Portions of the
input for which there are no matches in the corpus do not generate
a translation.
Because the EBMT system does not generate translations for
100% of the text it is given as input, a bilingual dictionary and
phrasal glossary are used to fill any gaps. Selection of a ?best?
translation is guided by a trigram model of the target language [6].
Supporting Chinese required a number of changes to the program
and training procedures; those changes are discussed in the next
section.
2. ENHANCEMENTS
The first change required of the translation software was sup-
port for the two-byte encoding used for the Chinese text (GB-2312,
?GB? for short). Further, the EBMT (as well as dictionary and glos-
sary) approaches are word-based, but Chinese is ordinarily writ-
ten without breaks between words. Thus, Chinese input must be
.
segmented into individual words. The initial baseline system used
the segmenter made available by the LDC. This segmenter uses a
word-frequency list to make segmentation decisions, but although
the list provided by the LDC is large, it did not completely cover
the vocabulary of the EBMT training corpus (described below). As
a result, many sentences had incorrect segmentations or included
long sequences which were not segmented at all or were broken
into single characters. Almost every Chinese character has at least
one meaning, and its meaning may be entirely different from the
meaning of the word containing it. The mis-segmenting of Chinese
words due to the inadequate dictionary makes it very hard to build
a statistical dictionary and properly index the EBMT corpus.
To improve the performance of the Chinese segmenter, we aug-
mented its word list by finding sequences of characters in the train-
ing corpus that belong together, based on their frequency and high
mutual information. We developed a form of term extraction to
find English phrases which should be treated as atomic units for
translation, thus increasing the average length of ?words? in both
source and target languages. Finally, we also created an augmented
bilingual dictionary for use in word-level alignment for EBMT by
applying statistical dictionary extraction techniques to the training
corpus.
As the improved segmenter and the term finder may be produc-
ing excessively long phrases or phrases which are impossible to
match in the other language, we repeat the procedure of segment-
ing/bracketing/dictionary-building several times. On each succes-
sive iteration, the segmenter and bracketer are limited to words and
phrases for which the statistical dictionary from the previous itera-
tion contains translations. Through this iteration, we increased the
size of the statistical dictionary from each step and guaranteed that
all Chinese words generated by the segmenter have translations in
the dictionary. This helps ensure that the EBMT engine can per-
form word-level alignments.
3. EXPERIMENTAL DESIGN
The primary purpose of this experiment was to determine the
effect of each enhancement by operating with various subsets of
the enhancements. Since it rapidly becomes impractical to test all
possible combinations, we opted for the following test conditions:
1. baseline: parallel corpus segmented with the LDC segmenter
and LDC dictionary/glossary
2. baseline plus improved segmenter
3. baseline plus improved segmenter and term finder
4. baseline plus improved segmenter and statistical dictionary
5. baseline plus improved segmenter, term finder, and statistical
dictionary
For training, we had available two parallel Chinese-English cor-
pora distributed by the LDC: the complete Hong Kong legal code
(after cleaning: 47.86 megabytes, 5.5 million English words, 9 mil-
lion Chinese characters) where 85% of the content (by sentence) is
unique, and a collection of Hong Kong news articles (after clean-
ing: 24.58 megabytes, 2.67 million English words, 4.5 million Chi-
nese characters). In addition, LDC distributes a bilingual dictio-
nary/phrasebook, which we also used.
To determine the effects of varying amounts of training data on
overall performance, we divided the bilingual training corpus into
ten nearly equal slices. Each test condition was then run ten times,
each time increasing the number of slices used for training the sys-
tem. After each training pass, the test sentences were translated and
the system?s performance evaluated automatically; selected points
were then manually evaluated for translation quality.
The automatic performance evaluation measured coverage of the
input and average phrase length. Coverage is the percentage of the
input text for which a translation is produced by a particular trans-
lation method (since the EBMT engine does not generally produce
hypotheses that cover every word of input), while average phrase
length is a crude indication of translation quality ? the longer the
phrase that is translated, the more context is incorporated and the
less likely it is that the wrong sense will be used in the translation or
that (for EBMT) the alignment will be incorrect. Since the dictio-
nary and glossary remain constant for a given test condition, only
the EBMT coverage will be presented.
Manual grading of the output was performed using a web-based
system with which the graders could assign one of three scores
(?Good?, ?OK?, ?Bad?) in each of two dimensions: grammatical
correctness and meaning preservation. This type of quality scoring
is commonly used in assessing translation quality, and is used by
other TIDES participants. Fifty-two test sentences were translated
for each of four points from the automated evaluation and these sets
of four alternatives presented to the graders. The four points chosen
were the baseline system with 100% of the training corpus, the full
system with 20% and 100% training, and the full system trained on
a corpus of Hong Kong news text (cross-domain); only four points
were selected due to the difficulty and expense of obtaining large
numbers of manual quality judgements.
To assess the performance of the system in a different domain,
as well as the effect of the trigram language model on the selec-
tion of translated fragments for the final translation, we obtained
manual judgements for 44 sentences on an additional four test con-
ditions, each trained with the entire available parallel text and tested
on Hong Kong news text rather than legal sentences. These points
were the cross-domain case (trained on the legal corpus) and three
different language models for within-domain training: an English
language model derived from the legal corpus, one derived from
the news corpus, and a pre-existing model generated from two gi-
gabytes of newswire and broadcast news transcriptions.
4. RESULTS
We discovered that there is a certain amount of synergy between
some of the improvements, particularly the term finder and statis-
tical dictionary extraction. Applying the term finder modifies the
parallel corpus in such a way that it becomes more difficult for
the EBMT engine to find matches which it can align, while adding
dictionary entries derived from the modified corpus eliminates that
effect. As a result, we will not present the performance results for
Test Condition 3 (improved segmenter plus term finder); further,
the data for Test Conditions 2 (improved segmenter only) and 4
(improved segmenter plus statistical dictionary) may not accurately
reflect the contribution of those two components to the full system
Translating Legal Code
System Baseline Full Full X-Dom
Training 100% 20% 100% 100%
Syntactic 42.31% 54.81% 61.06% 39.42%
Semantic 43.75% 61.54% 64.42% 34.62%
Translating Hong Kong News
Training News News News Legal
LangModel Legal News Prior Legal
Syntactic 45.67% 44.71% 47.60% 34.62%
Semantic 50.00% 50.96% 51.92% 47.12%
Figure 1: Judgements ? Acceptable Translations
used for Test Condition 5.
Figure 2 shows the proportion of the words in the test sentences
for which the EBMT engine was able to produce a translation,
while Figure 3 shows the average number of source-languagewords
per translated fragment. These curves do not increase monotoni-
cally because, for performance reasons, the EBMT engine does not
attempt to align every occurrence of a phrase, only theN (currently
12) most-recently added ones; as a result, adding more text to the
corpus can cause EBMT to ignore matches that successfully align
in favor of newer occurrences which it is unable to align.
Examining Figure 3, it is clear that the fifth slice (from 40 to
50%) is much more like the test data than other slices, resulting in
longer matches. In general, the closer training and test text are to
each other, the longer the phrases they have in common.
Figure 1 summarizes the results of human quality assessments.
The ?Good? and ?OK? judgements were combined into ?Accept-
able? and the the percentage of ?Acceptable? judgements was aver-
aged across sentences and graders. As hoped and expected, the im-
provements do in fact result not only in better coverage by EBMT,
but also in better quality assessments by the human graders. Fur-
ther, the results on Hong Kong news text show that the choice of
language model does have a definite effect on quality. These results
also confirm the adage that there is no such thing as too much train-
ing text for language modeling, since the model generated from the
EBMT corpus was unable to match the performance of the pre-
existing model generated from two orders of magnitude more text.
5. CONCLUSIONS AND FUTURE WORK
As seen in Figure 2, the enhancements described here cumula-
tively provide a 12% absolute improvement in coverage for EBMT
translations without requiring any additional knowledge resources.
Further, the enhanced coverage does, in fact, result in improved
translations, as verified by human judgements. We can also con-
clude that when we combine words into larger chunks on both sides
of the corpus, the possibility of finding larger matches between the
source language and the target language increases, which leads to
the improvement of the translation quality for EBMT.
We will do further research on the interaction between the im-
proved segmenter, term finder and statistical dictionary builder, uti-
lizing the information provided by the statistical dictionary as feed-
back for the segmenter and term finder to modify their results. We
are also investigating the effects of splitting the EBMT training into
multiple sets of topic-specific sentences, automatically separated
using clustering techniques.
The relatively low slope of the coverage curve also indicates that
the training corpus is sufficiently large. Our prior experience with
Spanish (using the UN Multilingual Corpus [5]) and French (using
00.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0 10 20 30 40 50 60 70 80 90 100
Co
ve
ra
ge
 o
f E
BM
T
Percentage of corpus used for training (%)
Improved Segmenter, term finder, statDict
Improved Segmenter, statDict
Improved Segmenter
Baseline system
Trained on News tested On Legalcode
Figure 2: EBMT Coverage with Varying Training
the Hansard corpus [7]) was that the curve flattens out at between
two and three million words of training text, which appears also to
be the case for Chinese (each training slice contains approximately
one million words of total text).
We have not yet taken full advantage of the features of the EBMT
software. In particular, it supports equivalence classes that permit
generalization of the training text into templates for improved cov-
erage. We intend to test automatic creation of equivalence classes
from the training corpus [4] in conjunction with the other improve-
ments reported herein.
6. ACKNOWLEDGEMENTS
We would like to thank Alon Lavie and Lori Levin for their com-
ments on drafts of this paper.
7. REFERENCES
[1] R. D. Brown. Example-Based Machine Translation in the
PANGLOSS System. In Proceedings of the Sixteenth
International Conference on Computational Linguistics, pages
169?174, Copenhagen, Denmark, 1996.
http://www.cs.cmu.edu/?ralf/papers.html.
[2] R. D. Brown. Automated Dictionary Extraction for
?Knowledge-Free? Example-Based Translation. In
Proceedings of the Seventh International Conference on
Theoretical and Methodological Issues in Machine
Translation (TMI-97), pages 111?118, Santa Fe, New Mexico,
July 1997.
http://www.cs.cmu.edu/?ralf/papers.html.
[3] R. D. Brown. Adding Linguistic Knowledge to a Lexical
Example-Based Translation System. In Proceedings of the
Eighth International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI-99),
pages 22?32, Chester, England, August 1999.
http://www.cs.cmu.edu/?ralf/papers.html.
[4] R. D. Brown. Automated Generalization of Translation
Examples. In Proceedings of the Eighteenth International
Conference on Computational Linguistics (COLING-2000),
pages 125?131, 2000.
[5] D. Graff and R. Finch. Multilingual Text Resources at the
Linguistic Data Consortium. In Proceedings of the 1994 ARPA
Human Language Technology Workshop. Morgan Kaufmann,
1994.
00.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
6.5
7
7.5
0 10 20 30 40 50 60 70 80 90 100
Av
er
ag
e 
ph
ra
se
 le
ng
th
 (w
ord
s)
Percentage of corpus used for training (%)
Improved Segmenter, term finder, statDict
Improved Segmenter, statDict
Improved Segmenter
Baseline system
Trained on News tested On Legalcode
Figure 3: Average EBMT Match Lengths
[6] C. Hogan and R. E. Frederking. An Evaluation of the
Multi-engine MT Architecture. In Machine Translation and
the Information Soup: Proceedings of the Third Conference of
the Association for Machine Translation in the Americas
(AMTA ?98), volume 1529 of Lecture Notes in Artificial
Intelligence, pages 113?123. Springer-Verlag, Berlin, October
1998.
[7] Linguistic Data Consortium. Hansard Corpus of Parallel
English and French. Linguistic Data Consortium, December
1997. http://www.ldc.upenn.edu/.
A Server for Real-Time Event Tracking in News
Ralf D. Brown
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213-3890 USA
ralf@cs.cmu.edu
1. INTRODUCTION
 
	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 41?44,
New York, June 2006. c?2006 Association for Computational Linguistics
Spectral Clustering for Example Based Machine Translation
Rashmi Gangadharaiah
LTI
Carnegie Mellon University
Pittsburgh P.A. 15213
rgangadh@andrew.cmu.edu
Ralf Brown
LTI
Carnegie Mellon University
Pittsburgh P.A. 15213
ralf@cs.cmu.edu
Jaime Carbonell
LTI
Carnegie Mellon University
Pittsburgh P.A. 15213
jgc@cs.cmu.edu
Abstract
Prior work has shown that generaliza-
tion of data in an Example Based Ma-
chine Translation (EBMT) system, re-
duces the amount of pre-translated text re-
quired to achieve a certain level of accu-
racy (Brown, 2000). Several word clus-
tering algorithms have been suggested to
perform these generalizations, such as k-
Means clustering or Group Average Clus-
tering. The hypothesis is that better con-
textual clustering can lead to better trans-
lation accuracy with limited training data.
In this paper, we use a form of spectral
clustering to cluster words, and this is
shown to result in as much as 29.08% im-
provement over the baseline EBMT sys-
tem.
1 Introduction
In EBMT, the source sentence to be translated
is matched against the source language sentences
present in a corpus of source-target sentence pairs.
When a partial match is found, the corresponding
target translations are obtained through subsenten-
tial alignment. These partial matches are put to-
gether to obtain the final translation by optimizing
translation and alignment scores and using a statisti-
cal target language model in the decoding process.
Prior work has shown that EBMT requires large
amounts of data (in the order of two to three mil-
lion words) (Brown, 2000) of pre-translated text, to
function reasonably well. Thus, some modification
of the basic EBMT method is required to make it ef-
fective when less data is available. In order to use
the available text efficiently, systems such as, (Veale
and Way, 1997) and (Brown, 1999), convert the ex-
amples in the corpus into templates against which
the new text can be matched. Thus, source-target
sentence pairs are converted to source-target gener-
alized template pairs. An example of such a pair is
shown below:
The session opened at 2p.m
La se?ance est ouverte a? 2 heures
The <event> <verb-past-tense> at <time>
La <event> <verb-past-tense> a <time>
This single template can be used to translate differ-
ent source sentences, including for example,
The session adjourned at 6p.m
The seminar opened at 8a.m
if ?session? and ?seminar? are both generalized to
?<event>?, ?opened? and ?adjourned? are both gen-
eralized to ?<verb-past-tense>? and finally ?6p.m?
and ?8a.m? are both generalized to ?<time>?.
The system used by (Brown, 1999) performs
its generalization using both equivalence classes of
words and a production rule grammar. This paper
describes the use of spectral clustering (Ng. et. al.,
2001; Zelnik-Manor and Perona, 2004), for auto-
mated extraction of equivalence classes. Spectral
clustering is seen to be superior to Group Average
Clustering (GAC) (Brown, 2000) both in terms of
semantic similarity of words falling in a single clus-
ter, and overall BLEU score (Papineni. et. al., 2002)
in a large scale EBMT system.
The next section explains the term vectors ex-
tracted for each word, which are then used to cluster
words into equivalence classes and provides an out-
line of the Standard GAC algorithm. Section 3 de-
scribes the spectral clustering algorithm used. Sec-
41
tion 4 lists results obtained in a full evaluation of the
algorithm. Section 5 concludes and discusses direc-
tions for future work.
2 Term vectors for clustering
Using a bilingual dictionary, usually created using
statistical methods such as those of (Brown et. al.,
1990) or (Brown, 1997), and the parallel text, a
rough mapping between source and target words can
be created. This word pair is then treated as an in-
divisible token for future processing. For each such
word pair we then accumulate counts for each to-
ken in the surrounding context of its occurrences
(N words, currently 3, immediately prior to and
N words immediately following). The counts are
weighted with respect to distance from occurrence,
with a linear decay (from 1 to 1/N) to give great-
est importance to the words immediately adjacent to
the word pair being examined. These counts form a
pseudo-document for each pair, which are then con-
verted into term vectors for clustering.
In this paper, we compare our algorithm against
the incremental GAC algorithm(Brown, 2000). This
method examines each word pair in turn, comput-
ing a similarity measure to every existing cluster.
If the best similarity measure is above a predeter-
mined threshold, the new word is placed in the cor-
responding cluster, otherwise a new cluster is cre-
ated if the maximum number of clusters has not yet
been reached.
3 Spectral clustering
Spectral clustering is a general term used to de-
scribe a group of algorithms that cluster points using
the eigenvalues of ?distance matrices? obtained from
data. In our case, the algorithm described in (Ng.
et. al., 2001) was performed with certain variations
that were proposed by (Zelnik-Manor and Perona,
2004) to compute the scaling factors automatically
and for the k-Means orthogonal treatment (Verma
and Meila, 2003) during the initialization. These
scaling factors help in self-tuning distances between
points according to the local statistics of the neigh-
borhoods of the points. The algorithm is briefly de-
scribed below.
1. Let S =s1, s2, ....sn, denote the term vectors to
be clustered into k classes.
2. Form the affinity matrix A defined by
Aij = exp(?d2(si, sj)/?i?j) for i 6= j
Aii = 1
Where, d(si, sj) = 1/(sim(si, sj) + )
sim(si, sj) is the Cosine similarity between si
and sj ,  is used to prevent the ratio from be-
coming infinity
?i is the set of local scaling parameters for si.
?i = d(si, sT ) where, sT is the T th neighbor of
point si for some fixed T (7 for this paper).
3. Define D to be the diagonal matrix given by,
Dii = ?jAij
4. Compute L = D?1/2AD?1/2
5. Select k eigenvectors corresponding to k
largest eigenvalues (k is presently an externally
set parameter). The eigenvectors are normal-
ized to have unit length. Form matrix U by
stacking all the eigenvectors in columns.
6. Form the matrix Y by normalizing U?s rows,
Yij = Uij/
?
(?jU2ij)
7. Perform k-Means clustering treating each row
of Y as a point in k dimensions. The k-Means
algorithm is initialized either with random cen-
ters or with orthogonal vectors.
8. After clustering, assign the point si to cluster c
if the corresponding row i of the matrix Y was
assigned to cluster c.
9. Sum the distances between the members and
the centroid of each cluster to obtain the classi-
fication cost.
10. Goto step 7, iterate for a fixed number of it-
erations. In this paper, 20 iterations were per-
formed with orthogonal k-Means initialization
and 5 iterations with random k-Means initial-
ization.
11. The clusters obtained from the iteration with
least classification cost are selected as the k
clusters.
4 Preliminary Results
The clusters obtained from the spectral clustering
method are seen by inspection to correspond to more
natural and intuitive word classes than those ob-
tained by GAC. Even though this is subjective and
not guaranteed to lead to improve translation perfor-
mance, it shows that maybe the increased power of
spectral clustering to represent non-convex classes
42
(non-convex in the term vector domain) could be
useful in a real translation experiment. Some ex-
ample classes are shown in Table 1. The first
class in an intuitive sense corresponds to measure-
ment units. We see that in the <units> case,
GAC misses some of the members which are ac-
tually distributed among many different classes and
hence these are not well generalized. In the second
class <months>, spectral clustering has primarily
the months in a single class whereas GAC adds a
number of seemingly unrelated words to the clus-
ter. The classes were all obtained by finding 80
clusters in a 20,000-sentence pair subset of the IBM
Hansard Corpus (Linguistic Data Consortium, 1997)
for spectral clustering. 80 was chosen as the number
of clusters since it gave the highest BLEU score in
the evaluation. For GAC, 300 clusters were used as
this gave the best performance.
To show the effectiveness of the clustering meth-
ods in an actual evaluation, we set up the following
experiment for an English to French translation task
on the Hansard corpus. The training data consists of
three sets of size 10,000 (set1), 20,000 (set2) and
30,000 (set3) sentence pairs chosen from the first
six files of the Hansard Corpus. Only sentences of
length 5 to 21 words were taken. Only words with
frequency of occurrence greater than 9 were chosen
for clustering because more contextual information
would be available when the word occurs frequently
and this would help in obtaining better clusters. The
test data was chosen to be a set of 500 sentences ob-
tained from files 20, 40, 60 and 80 of the Hansard
corpus with 125 sentences from each file. Each of
the methods was run with different number of clus-
ters and results are reported only for the optimal
number of clusters in each case.
The results in Table 2 show that spectral clus-
tering requires moderate amounts of data to get a
large improvement. For small amounts of data it is
slightly worse than GAC, but neither gives much im-
provement over the baseline. For larger amounts of
data, again both methods are very similar, though
spectral clustering is better. Finally, for moderate
amounts of data, when generalization is the most
useful, spectral clustering gives a significant im-
provement over the baseline as well as over GAC.
By looking at the clusters obtained with varying
amounts of data, it can be concluded that high pu-
Table 1: Clusters for <units> and <months>
Spectral clustering GAC
?adjourned? ?hre? ?adjourned? ?hre?
?cent? ?%?
?days? ?jours?
?families? ?familles? ?families? ?familles?
?hours? ?heures?
?million? ?millions? ?million? ?millions?
?minutes? ?minutes?
?o?clock? ?heures? ?o?clock? ?heures?
?p.m.? ?heures? ?p.m.? ?heures?
?p.m.? ?hre?
?people? ?personnes? ?people? ?personnes?
?per? ?%? ?per? ?%?
?times? ?fois? ?times? ?fois?
?years? ?ans?
?august? ?aou?t? ?august? ?aou?t?
?december? ?de?cembre? ?december? ?de?cembre?
?february? ?fe?vrier? ?february? ?fe?vrier?
?january? ?janvier? ?january? ?janvier?
?march? ?mars? ?march? ?mars?
?may? ?mai? ?may? ?mai?
?november? ?novembre? ?november? ?novembre?
?october? ?octobre? ?october? ?octobre?
?only? ?seulement? ?only? ?seulement?
?june? ?juin? ?june? ?juin?
?july? ?juillet? ?july? ?juillet?
?april? ?avril? ?april? ?avril?
?september? ?septembre? ?september? ?septembre?
?page? ?page?
?per? ?$?
?recognize? ?parole?
?recognized? ?parole?
?recorded? ?page?
?section? ?article?
?since? ?depuis? ?since? ?depuis?
?took? ?se?ance?
?under? ?loi?
43
Table 2: % Relative improvement over baseline EBMT
# clus is the number of clusters for best performance
GAC Spectral
% Rel imp #clus % Rel imp #clus
10k 3.33 50 1.37 20
20k 22.47 300 29.08 80
30k 2.88 300 3.88 200
rity clusters can be obtained with even just moderate
amounts of data.
5 Conclusions and future work
From the experimental results we see that spectral
clustering leads to relatively purer and more intu-
itive clusters. These clusters result in an improved
BLEU score in comparison with the clusters ob-
tained through GAC. GAC can only collect clusters
in convex regions in the term vector space, while
spectral clustering is not limited in this regard. The
ability of spectral clustering to represent non-convex
shapes arises due to the projection onto the eigen-
vectors as described in (Ng. et. al., 2001).
As future work, we would like to analyze the
variation in performance as the amount of data in-
creases. It is widely known that increasing the
amount of training data in a generalized EBMT sys-
tem eventually leads to saturation of performance,
where all clustering methods perform about as well
as baseline. Thus, all methods have an operating re-
gion where they are the most useful. We would like
to locate and extend this region for spectral cluster-
ing.
Also, it would be interesting to compare the clus-
ters obtained with spectral clustering and the Part of
Speech tags of the words in the same cluster, espe-
cially for languages such as English where good tag-
gers are available.
Finally, an important direction of research is in
automatically selecting the number of clusters for
the clustering algorithm. To do this, we could use
information from the eigenvalues or the distribution
of points in the clusters.
Acknowledgment
This work was funded by National Business Center
award NBCHC050082.
References
Andrew Ng, Michael Jordan, and Yair Weiss 2001. On
Spectral Clustering: Analysis and an algorithm. In Ad-
vances in Neural Information Processing Systems 14:
Proceeding of the 2001 Conference, pages 849-856,
Vancouver, British Columbia, Canada, December.
Deepak Verma and Marina Meila. 2003. Comparison of
Spectral Clustering Algorithms. http://www.ms.
washington.edu/?spectral/.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
Jing Zhu. 2002. BLEU: a method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL 2002), pages 311-
318,Philadelphia, PA, July. http://acl.ldc.
upenn.edu/P/P02
Linguistic Data Consortium. 1997. Hansard Corpus of
Parallel English and French. Linguistic Data Con-
sortium, December. http://www.ldc.upenn.
edu/
L. Zelnik-Manor and P. Perona 2004 Self-Tuning Spec-
tral Clustering. In Advances in Neural Information
Processing Systems 17: Proceeding of the 2004 Con-
ference.
Peter Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F.
Jelinek, J. Lafferty, R. Mercer and P. Roossin. 1990.
A Statistical Approach to Machine Translation. Com-
putational Linguistics, 16:79-85.
Ralf D. Brown. 1997. Automated Dictionary Extrac-
tion for ?Knowledge-Free? Example-Based Transla-
tion. In Proceedings of the Seventh International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI-97), pages 111-118, Santa
Fe, New Mexico, July. http://www.cs.cmu.
edu/?ralf/papers.html
Ralf D. Brown. 1999. Adding Linguistic Knowledge
to a Lexical Example-Based Translation System. In
Proceedings of the Eighth International Conference
on Theoretical and Methodological Issues in Machine
Translation(TMI-99), pages 22-32, August. http:
//www.cs.cmu.edu/?ralf/papers.html
Ralf. D. Brown. 2000. Automated Generalization of
Translation Examples. In Proceedings of Eighteenth
International Conference on Computational Linguis-
tics (COLING-2000), pages 125-131, Saarbru?cken,
Germany.
Tony Veale and Andy Way. 1997. Gaijin: A Template-
Driven Bootstrapping Approach to Example-Based
Machine Translation. In Proceedings of NeMNLP97,
New Methods in Natural Language Processing, Sofia,
Bulgaria, September. http://www.compapp.
dcu.ie/?tonyv/papers/gaijin.html.
44
Using Similarity Scoring To Improve the Bilingual Dictionary for Word
Alignment
Katharina Probst
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, USA, 15213
kathrin@cs.cmu.edu
Ralf Brown
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, USA, 15213
ralf@cs.cmu.edu
Abstract
We describe an approach to improve the
bilingual cooccurrence dictionary that is
used for word alignment, and evaluate the
improved dictionary using a version of
the Competitive Linking algorithm. We
demonstrate a problem faced by the Com-
petitive Linking algorithm and present an
approach to ameliorate it. In particular, we
rebuild the bilingual dictionary by cluster-
ing similar words in a language and as-
signing them a higher cooccurrence score
with a given word in the other language
than each single word would have other-
wise. Experimental results show a signifi-
cant improvement in precision and recall
for word alignment when the improved
dicitonary is used.
1 Introduction and Related Work
Word alignment is a well-studied problem in Natu-
ral Language Computing. This is hardly surprising
given its significance in many applications: word-
aligned data is crucial for example-based machine
translation, statistical machine translation, but also
other applications such as cross-lingual information
retrieval. Since it is a hard and time-consuming task
to hand-align bilingual data, the automation of this
task receives a fair amount of attention. In this pa-
per, we present an approach to improve the bilin-
gual dictionary that is used by word alignment al-
gorithms. Our method is based on similarity scores
between words, which in effect results in the clus-
tering of morphological variants.
One line of related work is research in clustering
based on word similarities. This problem is an area
of active research in the Information Retrieval com-
munity. For instance, Xu and Croft (1998) present
an algorithm that first clusters what are assumedly
variants of the same word, then further refines the
clusters using a cooccurrence related measure. Word
variants are found via a stemmer or by clustering all
words that begin with the same three letters. An-
other technique uses similarity scores based on N-
grams (e.g. (Kosinov, 2001)). The similarity of two
words is measured using the number of N-grams that
their occurrences have in common. As in our ap-
proach, similar words are then clustered into equiv-
alence classes.
Other related work falls in the category of word
alignment, where much research has been done. A
number of algorithms have been proposed and eval-
uated for the task. As Melamed (2000) points out,
most of these algorithms are based on word cooccur-
rences in sentence-aligned bilingual data. A source
language word   and a target language word   are
said to cooccur if   occurs in a source language sen-
tence and   occurs in the corresponding target lan-
guage sentence. Cooccurrence scores then are then
counts for all word pairs   and  , where   is in
the source language vocabulary and 	 is in the tar-
get language vocabulary. Often, the scores also take
into account the marginal probabilites of each word
and sometimes also the conditional probabilities of
one word given the other.
Aside from the classic statistical approach of
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 409-416.
                         Proceedings of the 40th Annual Meeting of the Association for
(Brown et al, 1990; Brown et al, 1993), a number
of other algorithms have been developed. Ahren-
berg et al (1998) use morphological information on
both the source and the target languages. This infor-
mation serves to build equivalence classes of words
based on suffices. A different approach was pro-
posed by Gaussier (1998). This approach models
word alignments as flow networks. Determining the
word alignments then amounts to solving the net-
work, for which there are known algorithms. Brown
(1998) describes an algorithm that starts with ?an-
chors?, words that are unambiguous translations of
each other. From these anchors, alignments are ex-
panded in both directions, so that entire segments
can be aligned.
The algorithm that this work was based on is the
Competitive Linking algorithm. We used it to test
our improved dictionary. Competitive Linking was
described by Melamed (1997; 1998; 2000). It com-
putes all possible word alignments in parallel data,
and ranks them by their cooccurrence or by a similar
score. Then links between words (i.e. alignments)
are chosen from the top of the list until no more links
can be assigned. There is a limit on the number of
links a word can have. In its basic form the Compet-
itive Linking algorithm (Melamed, 1997) allows for
only up to one link per word. However, this one-to-
one/zero-to-one assumption is relaxed by redefining
the notion of a word.
2 Competitive Linking in our work
We implemented the basic Competitive Linking al-
gorithm as described above. For each pair of paral-
lel sentences, we construct a ranked list of possible
links: each word in the source language is paired
with each word in the target language. Then for
each word pair the score is looked up in the dictio-
nary, and the pairs are ranked from highest to lowest
score. If a word pair does not appear in the dictio-
nary, it is not ranked. The algorithm then recursively
links the word pair with the highest cooccurrence,
then the next one, etc. In our implementation, link-
ing is performed on a sentence basis, i.e. the list of
possible links is constructed only for one sentence
pair at a time.
Our version allows for more than one link per
word, i.e. we do not assume one-to-one or zero-to-
one alignments between words. Furthermore, our
implementation contains a threshold that specifies
how high the cooccurrence score must be for the two
words in order for this pair to be considered for a
link.
3 The baseline dictionary
In our experiments, we used a baseline dictionary,
rebuilt the dictionary with our approach, and com-
pared the performance of the alignment algorithm
between the baseline and the rebuilt dictionary. The
dictionary that was used as a baseline and as a ba-
sis for rebuilding is derived from bilingual sentence-
aligned text using a count-and-filter algorithm:
  Count: for each source word type, count the
number of times each target word type cooc-
curs in the same sentence pair, as well as the
total number of occurrences of each source and
target type.
  Filter: after counting all cooccurrences, re-
tain only those word pairs whose cooccurrence
probability is above a defined threshold. To be
retained, a word pair  ,  must satisfy

	
 

  	
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 87?90,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Symmetric Probabilistic Alignment
Ralf D. Brown Jae Dong Kim Peter J. Jansen Jaime G. Carbonell
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
{ralf,jdkim,pjj,jgc}@cs.cmu.edu
Abstract
We recently decided to develop a new
alignment algorithm for the purpose
of improving our Example-Based Ma-
chine Translation (EBMT) system?s per-
formance, since subsentential alignment is
critical in locating the correct translation
for a matched fragment of the input. Un-
like most algorithms in the literature, this
new Symmetric Probabilistic Alignment
(SPA) algorithm treats the source and tar-
get languages in a symmetric fashion.
In this short paper, we outline our basic
algorithm and some extensions for using
context and positional information, and
compare its alignment accuracy on the
Romanian-English data for the shared task
with IBM Model 4 and the reported results
from the prior workshop.
1 Symmetric Probabilistic Alignment
(SPA)
In subsentential alignment, mappings are produced
from words or phrases in the source language sen-
tence and those words or phrases in the target lan-
guage sentence that best express their meaning.
An alignment algorithm takes as input a bilingual
corpus consisting of corresponding sentence pairs
and strives to find the best possible alignment in the
second for selected n-grams (sequences of n words)
in the first language. The alignments are based on
a number of factors, including a bilingual dictionary
(preferably a probabilistic one), the position of the
words, invariants such as numbers and punctuation,
and so forth.
For our baseline algorithm, we make the follow-
ing simplifying assumptions, each of which we in-
tend to relax in future work, and the last of which
has already been partially relaxed:
1. A fixed bilingual probabilistic dictionary is
available.
2. Fragments (word sequences) are translated in-
dependently of surrounding context.
3. Contiguous fragments of source language text
are translated into contiguous fragments in the
target language text.
Unlike the work of (Marcu and Wong, 2002),
our alignment algorithm is not generative and does
not use the idea of a bag of concepts from which
the phrases in the sentence pair arise. It is, rather,
intended to find the corresponding target-language
phrase given a specific source-language phrase of in-
terest, as required by our EBMT system after find-
ing a match between the input and the training data
(Brown, 2004).
1.1 Baseline Algorithm
Our baseline algorithm is based on maximizing the
probability of bi-directional translations of individ-
ual words between a selected n-gram in the source
language and every possible n-gram in the corre-
sponding paired target language sentence. No posi-
tional preference assumptions are made, nor are any
length preservation assumptions made. That is, an
n-gram may translate to an m-gram, for any val-
ues of n or m bounded by the source and target
sentence lengths, respectively. Finally a smooth-
ing factor is used to avoid singularities (i.e. avoid-
ing zero-probabilities for unknown words, or words
never translated before in a way consistent with the
dictionary).
87
Given a source-language sentence
S1 : s0, s1, ..., si, ..., si+k, ..., sn (1)
in the bilingual corpus, where si, ..., si+k is a phrase
of interest, and the corresponding target language
sentence S2 is
S2 : t0, t1, ..., tj , ..., tj+l, ..., tm (2)
the values of j and l are to be determined.
Then the segment we try to obtain is the target
fragment F?T with the highest probability of all pos-
sible fragments of S2 to be a mutual translation with
the given source fragment, or
F?T = argmax{FT } (p(si, ..., si+k ? tj, ..., tj+l))
(3)
All possible segments can be checked in O(m2)
time, where m is the target language length, because
we will check m 1-word segments, m? 1 two-word
segments, and so on. If we bound the target language
n-grams to a maximal length k, then the complexity
is linear, i.e. O(km).
The score of the best possible alignment is com-
puted as follows: Let LT be the Target Language
Vocabulary, s a source word, ti be target segment
words, and V = {ti ? {LT }|i ? 1} the translation
word set of s,
We define the translation relation probability
p(Tr(s) ? {t0, t1, ..., tk}) as follows:
1. p(Tr(s) ? {t0, t1, ..., tk}) = max(p(ti|s))
for all ti ? {t0, t1, ..., tk} when {ti|ti ?
{t0, t1, ..., tk}} is not empty.
2. p(Tr(s) ? {t0, t1, ..., tk}) = 0 otherwise.
Then the score of the best alignment is
SF?T = max{FT }
SFT (4)
where the score can be written as two components
SFT = P1 ? P2 (5)
which can be further specified as
P1 =
( k
?
m=0
max (p (Tr(si+m) ? {tj...j+l}) , ?)
)
1
k+1
(6)
P2 =
( l
?
n=0
max (p (Tr(tj+n) ? {si...i+k}) , ?)
)
1
l+1
(7)
where ? is a very small probability used as a smooth-
ing value.
1.2 Length Penalty
The ratio between source and target segment (n-
gram) lengths should be comparable to the ratio be-
tween the lengths of the source and target sentences,
though certainly variation is possible. Therefore, we
add a penalty function to the alignment probability
that increases with the discrepancy between the two
ratios.
Let the length of the source language segment be
i and the length of a target language segment under
consideration be j. Given a source language sen-
tence length of n (in the corpus sentence containing
the fragment) and its corresponding target language
length of m. The expected target segment length is
then given by j? = i? mn . Further defining an allow-
able difference AD, our implementation calculates
the length penalty LP as follows, with the value of
the exponent determined empirically:
LPFT = min
?
?
(
|j ? j?|
AD
)4
, 1
?
? (8)
The score for a segment including the penalty func-
tion is then:
SFT ? SFT ? (1? LPFT ) (9)
Note that, as intended, the score is forced to 0 when
the length difference |j ? j?| > AD.
1.3 Distortion Penalty
For closely-related language pairs which tend to
have similar word orders, we introduce a distortion
penalty to penalize the alignment score of any can-
didate target fragment which is out of the expected
position range. First, we calculate CE , the expected
center of the candidate target fragment using CFS ,
the center of the source fragment and the ratio of
target- to source-sentence length.
CE = CFS ?
m
n (10)
88
Then we calculate an allowed distance limit of the
center Dallowed using a constant distance limit value
DL and the ratio of actual target sentence length to
average target sentence length.
Dallowed = DL ?
m
maverage
(11)
Let Dactual be the actual distance difference be-
tween the candidate target fragment?s center and the
expected center, and set
SFT ?
?
?
?
0, ifDactual ? Dallowed
SFT
(Dactual?Dallowed+1)2 , otherwise
(12)
Furthermore, we think that we can apply this
penalty to language pairs which have lower word-
order similarities than e.g. French-English. Because
there might exist certain positional relationships be-
tween such language pairs, if we can calculate the
expected position using each language?s sentence
structure, we can apply a distortion penalty to the
candidate alignments.
1.4 Anchor Context
If the adjacent words of the source fragment and the
candidate target fragment are translations of each
other, we expect that this alignment is more likely
to be correct. We boost SFT with the anchor context
alignment score SACp ,
SACp = P (si?1 ? tj?1) ? P (si+k ? tj+l) (13)
SFT ? (SFT )? ? (SACp)1?? (14)
Empirically, we found this combination gives the
best score for French-English when ? = 0.6 and
for Romanian-English when ? = 0.8, and leads to
better results than the similar formula
SFT ? ? ? SFT + (1? ?) ? SACp (15)
2 Experimental Design
In previous work (Kim et al, 2005), we tested our
alignment method on a set of French-English sen-
tence pairs taken from the Canadian Hansard corpus
and on a set of English-Chinese sentence pairs, and
compared the results to human alignments. For the
present workshop, we chose to use the Romanian-
English data which had been made available.
Due to a lack of time prior to the period of the
shared task, we merely re-used the parameters which
had been tuned for French-English, rather than tun-
ing the alignment parameters specifically for the de-
velopment data.
SPA was run under three experimental conditions.
In the first, labeled ?SPA (c)? in Tables 1 and 2, SPA
was instructed to examine only contiguous target
phrases as potential alignments for a given source
phrase. In the second, labeled ?SPA (n)?, a noncon-
tiguous target algnment consisting of two contigu-
ous segments with a gap between them was permit-
ted in addition to contiguous target algnments. The
third condition (?SPA (h)?) examined the impact of
a small amount of manual alignment information on
the selection of contiguous alignments. Unlike the
first two conditions, the presence of additional data
beyond the training corpus forces SPA(h) into the
Unlimited Resources track.
We had a native Romanian speaker hand-align
204 sentence pairs from the training corpus, and
extracted 732 distinct translation pairs from those
alignments, of which 450 were already present in
the automatically-generated dictionaries. The new
translation pairs were added to the dictionaries for
the SPA(h) condition and the translation probabili-
ties for the existing pairs were increased to reflect
the increased confidence in their correctness. Had
more time been available, we would have investi-
gated more sophisticated means of integrating the
human knowledge into the translation dictionaries.
3 Results and Conclusions
Table 1 compares the performance of SPA on what
is now the development data against the submissions
with the best AER values reported by (Mihalcea
and Pedersen, 2003) for the participants in the 2003
workshop, including CMU, MITRE, RALI, Univer-
sity of Alberta, and XRCE 1. As SPA generates only
SURE alignments, the values in Table 1 are SURE
alignments under the NO-NULL-Align scoring con-
dition for all systems except Fourday, which did not
generate SURE alignments.
Despite the fact that SPA was designed specifi-
cally for phrase-to-phrase alignments rather than the
1Citations for individual participants? papers have been
omitted for space reasons; all appear in the same proceedings.
89
Method Prec% Rec% F1% AER
SPA (c) 64.47 62.68 63.56 36.44
SPA (n) 64.38 62.70 63.53 36.47
SPA (h) 64.61 62.55 63.56 36.44
Fourday 52.83 42.86 47.33 52.67
UMD.RE.2 58.29 49.99 53.82 46.61
BiBr 70.65 55.75 62.32 41.39
Ralign 92.00 45.06 60.49 35.24
XRCEnolm 82.65 62.44 71.14 28.86
Table 1: Romanian-English alignment results (De-
velopment Set, NO-NULL-Align)
word-to-word alignments needed for the shared task
and was not tuned for this corpus, its performance is
competitive with the best of the systems previously
used for the shared task. We thus decided to submit
runs for the official 2005 evaluation, whose resulting
scores are shown in Table 2.
On the development set, noncontiguous align-
ments resulted in slightly lower precision than con-
tiguous alignments, which was not unexpected, but
recall does not increase enough to improve F1 or
AER. The modified dictionaries improved preci-
sion slightly, as anticipated, but lowered recall suffi-
ciently to have no net effect on F1 or AER.
The evaluation set proved to be very similar in dif-
ficulty to the development data, resulting in scores
that were very close to those achieved on the dev-test
set. Noncontiguous alignments again proved to have
a very small negative effect on AER resulting from
reduced precision, but this time the altered dictionar-
ies for SPA(h) resulted in a substantial reduction in
recall, considerably harming overall performance.
After the shared task was complete, we performed
some tuning of the alignment parameters for the
Romanian-English development test set, and found
that the French-English-tuned parameters were close
to optimal in performance. The AER on the develop-
ment test set for the SPA(c) contiguous alignments
condition decreased from 36.44% to 36.11% after
the re-tuning.
4 Future Work
Enhancements in the extraction of word-to-word
alignments from what is fundamentally a phrase-to-
phrase alignment algorithm could probably further
Method Prec% Recall% F1% AER%
SPA (c) 64.96 61.34 63.10 36.90
SPA (n) 64.91 61.34 63.07 36.93
SPA (h) 64.60 60.54 62.50 37.50
Table 2: Evaluation results (NO-NULL-Align)
improve results on the Romanian-English data. We
also intend to investigate principled, seamless inte-
gration of manual alignments and dictionaries with
probabilistic ones, since the ad hoc method proved
detrimental. Finally, a more detailed performance
analysis is in order, to determine whether the close
balance of precision and recall is inherent in the bidi-
rectionality of the algorithm or merely coincidence.
5 Acknowledgements
We would like to thank Lucian Vlad Lita for provid-
ing manual alignments.
References
Ralf D. Brown. 2004. A Modified Burrows-Wheeler
Transform for Highly-Scalable Example-Based Trans-
lation. In Machine Translation: From Real Users
to Research, Proceedings of the 6th Conference of
the Association for Machine Translation in the Amer-
icas (AMTA-2004), volume 3265 of Lecture Notes
in Artificial Intelligence, pages 27?36. Springer Ver-
lag, September-October. http://www.cs.cmu.-
edu/?ralf/papers.html.
Jae Dong Kim, Ralf D. Brown, Peter J. Jansen, and
Jaime G. Carbonell. 2005. Symmetric Probabilistic
Alignment for Example-Based Translation. In Pro-
ceedings of the Tenth Workshop of the European Asso-
cation for Machine Translation (EAMT-05), May. (to
appear).
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Machine
Translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2002), July. http://www.isi.edu/-
?marcu/papers.html.
Rada Mihalcea and Ted Pedersen. 2003. An Evalua-
tion Exercise for Word Alignment. In Proceedings of
the HLT-NAACL 2003 Workshop: Building and Using
Parallel Texts: Data Driven Machine Translation and
Beyond, pages 1?10. Association for Computational
Linguistics, May.
90
Coling 2010: Poster Volume, pages 320?328,
Beijing, August 2010
Monolingual Distributional Profiles for Word Substitution in Machine
Translation
Rashmi Gangadharaiah
rgangadh@cs.cmu.edu
Ralf D. Brown
ralf@cs.cmu.edu
Language Technologies Institute,
Carnegie Mellon University
Jaime Carbonell
jgc@cs.cmu.edu
Abstract
Out-of-vocabulary (OOV) words present a
significant challenge for Machine Trans-
lation. For low-resource languages, lim-
ited training data increases the frequency
of OOV words and this degrades the qual-
ity of the translations. Past approaches
have suggested using stems or synonyms
for OOV words. Unlike the previous
methods, we show how to handle not just
the OOV words but rare words as well
in an Example-based Machine Transla-
tion (EBMT) paradigm. Presence of OOV
words and rare words in the input sentence
prevents the system from finding longer
phrasal matches and produces low qual-
ity translations due to less reliable lan-
guage model estimates. The proposed
method requires only a monolingual cor-
pus of the source language to find can-
didate replacements. A new framework
is introduced to score and rank the re-
placements by efficiently combining fea-
tures extracted for the candidate replace-
ments. A lattice representation scheme al-
lows the decoder to select from a beam
of possible replacement candidates. The
new framework gives statistically signif-
icant improvements in English-Chinese
and English-Haitian translation systems.
1 Introduction
An EBMT system makes use of a parallel corpus
to translate new sentences. Each input sentence
is matched against the source side of a training
corpus. When matches are found, the correspond-
ing translations in the target language are obtained
through sub-sentential alignment. In our EBMT
system, the final translation is obtained by com-
bining the partial target translations using a sta-
tistical target Language Model. EBMT systems,
like other data-driven approaches, require large
amounts of data to function well (Brown, 2000).
Having more training data is beneficial re-
sulting in log-linear improvement in translation
quality for corpus-based methods (EBMT, SMT).
Koehn (2002) shows translation scores for a num-
ber of language pairs with different training sizes
translated using the Pharaoh SMT toolkit (Koehn
et al, 2003). However, obtaining sizable paral-
lel corpora for many languages is time-consuming
and expensive. For rare languages, finding bilin-
gual speakers becomes especially difficult.
One of the main reasons for low quality transla-
tions is the presence of large number of OOV and
rare words (low frequency words in the training
corpus). Variation in domain and errors in spelling
increase the number of OOV words. Many of the
present translation systems either ignore these un-
known words or leave them untranslated in the fi-
nal target translation. When data is limited, the
number of OOV words increases, leading to the
poor performance of the translation models and
the language models due to the absence of longer
sequences of source word matches and less reli-
able language model estimates.
Approaches in the past have suggested using
stems or synonyms for OOV words as replace-
ments (Yang and Kirchhoff, 2006). Similarity
measures have been used to find words that are
closely related (Marton et al, 2009). For morpho-
320
logically rich languages, the OOV word is mor-
phologically analyzed and the stem is used as its
replacement (Popovic? and Ney, 2004).
This paper presents a simpler method inspired
by the Context-based MT approach (Carbonell et
al., 2006) to improve translation quality. The
method requires a large source language mono-
lingual corpus and does not require any other
language dependent resources to obtain replace-
ments. Approaches suggested in the past only
concentrated on finding replacements for the OOV
words and not the rare words. This paper pro-
poses a unified method to find possible replace-
ments for OOV words as well as rare words based
on the context in which these words appear. In
the case of rare words, the translated sentence is
traced back to find the origin of the translations
and the target translations of the replacements are
replaced with the translations of the rare words. In
the case of OOV words, the target translations are
replaced by the OOV word itself. The main idea
for adopting this approach is the belief that the
EBMT system will be able to find longer phrasal
matches and that the language model will be able
to give better probability estimates while decod-
ing if it is not forced to fragment text at OOV and
rare-word boundaries. This method is highly ben-
eficial for low-resource languages that do not have
morphological analysers or Part-of-Speech (POS)
taggers and in cases where the similarity measures
proposed in the past do not find closely related
words for certain OOV words.
The rest of the paper is organized as follows.
The next section (Section 2) discusses related
work in handling OOV words. Section 3 describes
the method adopted in this paper. Section 4 de-
scribes the experimental setup. Section 5 reports
the results obtained with the new framework for
English-Chinese and English-Haitian translation
systems. Section 6 concludes and suggests pos-
sible future work.
2 Related Work
Orthographic and morpho-syntactic techniques
for preprocessing training and test data have been
shown to reduce OOV word rates. Popovic?
and Ney (2004) demonstrated this on rich mor-
phological languages in an SMT system. They
introduced different types of transformations to
the verbs to reduce the number of unseen word
forms. Habash (2008) addresses spelling, name-
transliteration OOVs and morphological OOVs in
an Arabic-English Machine Translation system.
Phrases with the OOV replacements in the phrase
table of a phrase-based SMT system were ?recy-
cled? to create new phrases in which the replace-
ments were replaced by the OOV words.
Yang and Kirchhoff (2006) proposed a back-
off model for phrase-based SMT that translated
word forms in the source language by hierarchi-
cal morphological phrase level abstractions. If
an unknown word was found, the word was first
stemmed and the phrase table entries for words
sharing the same stem were modified by replacing
the words with their stems. If a phrase entry or a
single word phrase was found, the corresponding
translation was used, otherwise the model backed
off to the next level and applied compound split-
ting to the unknown word. The phrase table in-
cluded phrasal entries based on full word forms as
well as stemmed and split counterparts.
Vilar et al (2007) performed the translation
process treating both the source and target sen-
tences as a string of letters. Hence, there are
no unknown words when carrying out the actual
translation of a test corpus. The word-based sys-
tem did most of the translation work and the letter-
based system translated the OOV words.
The method proposed in this work to han-
dle OOV and rare words is very similar to the
method adopted by Carbonell et al (2006) to gen-
erate word and phrasal synonyms in their Context-
based MT system. Context-based MT does not
require parallel text but requires a large monolin-
gual target language corpus and a fullform bilin-
gual dictionary. The main principle is to find those
n-gram candidate translations from a large target
corpus that contain as many potential word and
phrase translations of the source text from the dic-
tionary and fewer spurious content words. The
overlap decoder combines the target n-gram trans-
lation candidates by finding maximal left and right
overlaps with the translation candidates of the pre-
vious and following n-grams. When the overlap
decoder does not find coherent sequences of over-
lapping target n-grams, more candidate transla-
321
tions are obtained by substituting words or phrases
in the target n-grams by their synonyms.
Barzilay and McKeown (2001) and Callison-
Burch et al (2006) extracted paraphrases from
monolingual parallel corpus where multiple trans-
lations were present for the same source. The syn-
onym generation in Carbonell et al (2006) differs
from the above in that it does not require paral-
lel resources containing multiple translations for
the same source language. In Carbonell et al
(2006), a list of paired left and right contexts that
contain the desired word or phrase are extracted
from the monolingual corpus. The same corpus
is used to find other words and phrases that fit the
paired contexts in the list. The idea is based on the
distributional hypothesis which states that words
with similar meanings tend to appear in similar
contexts (Harris, 1954). Hence, their approach
performed synonym generation on the target lan-
guage to find translation candidates that would
provide maximal overlap during decoding.
Marton et al (2009) proposed an approach sim-
ilar to Carbonell et al (2006) to obtain replace-
ments for OOV words, where monolingual dis-
tributional profiles for OOV words were con-
structed. Hence, the approach was applied on the
source language side as opposed to Carbonell et
al. (2006) which worked on the target language.
Only similarity scores and no other features were
used to rank the paraphrases (or replacements)
that occured in similar contexts. The high rank-
ing paraphrases were used to augment the phrase
table of phrase-based SMT.
All of the previously suggested methods only
handle OOV words (except Carbonell et al (2006)
which handles low frequency target phrases) and
no attempt is made to handle rare words. Many of
the methods explained above directly modify the
training corpus (or phrase table in phrase-based
SMT) increasing the size of the corpus. Our
method clusters words and phrases based on their
context as described by Carbonell et al (2006) but
uses the clustered words as replacements for not
just the OOV words but also for the rare words
on the source language side. Our method does
not make use of any morphological analysers,
POS taggers or manually created dictionaries
as they may not be available for many rare or
low-resource languages. The translation of the
replacements in the final decoded target sentence
is replaced by the translation of the original word
(or the source word itself in the OOV case),
hence, we do not specifically look for synonyms.
The only condition for a word to be a candidate
replacement is that its left and right context need
to match with that of the OOV/rare-word. Hence,
the clustered words could have different semantic
relations. For example,
(cluster1):?laugh, giggle, chuckle, cry, weep?
where ?laugh, giggle, chuckle? are synonyms and
?cry, weep? are antonyms of ?laugh?.
Clusters can also contain hypernyms (or hy-
ponyms), meronyms (or holonyms), troponyms
and coordinate terms along with synonyms and
antonyms. For example,
(cluster2):?country, region, place, area, dis-
trict, state, zone, United States, Canada, Korea,
Malaysia?.
where ?country? is a hypernym of ?United
States/Canada/Korea/Malaysia?. ?district? is a
meronym of ?state?. ?United States, Canada,
Korea, Malaysia? are coordinate terms sharing
?country? as their hypernym.
The contributions made by the paper are three-
fold: first, replacements are found for not just the
OOV words but for the rare words as well. Sec-
ond, the framework used allows scoring replace-
ments based on multiple features to permit op-
timization. Third, instead of directly modifying
the training corpus by replacing the candidate re-
placements by the OOV words, a new representa-
tion scheme is used for the test sentences to effi-
ciently handle a beam of possible replacements.
3 Proposed Method
Like Marton et al (2009), only a large monolin-
gual corpus is required to extract candidate re-
placements. To retrieve more replacements, the
monolingual corpus is pre-processed by first gen-
eralizing numbers, months and years by NUM-
BER, MONTH and YEAR tags, respectively.
322
3.1 OOV and Rare words
Words in the test sentence (new source sentence
to be translated) that do not appear in the training
corpus are called OOV words. Words in the test
sentence that appear less thanK times in the train-
ing corpus are considered as rare words (in this
paper K = 3). The method presented in the fol-
lowing sections holds for both OOV as well as rare
words. In the case of rare words, the final transla-
tion is postprocessed (Section 3.7) to include the
translation of the rare word.
The procedure adopted will be explained with
a real example T (the rest of the sentence is
removed for the sake of clarity) encountered in
the test data with ?hawks? as the OOV word,
T :a mobile base , hitting three hawks with
one arrow over the past few years ...
3.2 Context
As the goal is to obtain longer target phrasal trans-
lations for the test sentence before decoding,
only words that fit the left and right context of the
OOV/rare-word in the test sentence are extracted.
Unlike Marton et al (2009) where a context list
for each OOV is generated from the contexts
of their replacements, this paper uses only the
left and right context of the OOV/rare-word.
The default window size for the context is five
words (two words to the left and two words to the
right of the OOV/rare-word). If the windowed
words contain only function words, the window
is incremented until at least one content word is
present in the resulting context. This enables one
to find sensible replacements that fit the context
well. The contexts for T are:
Left-context (L): hitting three
Right-context (R): with one arrow
The above contexts are further processed to
generalize the numbers by a NUMBER tag
to produce more candidate replacements. The
resulting contexts are now:
Left-context (L): hitting NUMBER
Right-context (R): with NUMBER arrow
As a single L ? R context is used, a far
smaller number of replacements are extracted.
3.3 Finding Candidate replacements
The monolingual corpus (ML) of the source lan-
guage is used to find words and phrases (Xk) that
fitLXkR i.e., withL as its left context and/orR as
its right context. The maximum length for Xk is
set to 3 currently. The replacements are further fil-
tered to obtain only those replacements that con-
tain at least one content word. As illustrated ear-
lier, the resulting replacement candidates are not
necessarily synonyms.
3.4 Features
A local context of two to three words to the left
of an OOV/rare-word (wordi) and two to three
words to the right of wordi contain sufficient
clues for the word,wordi. Hence, local contextual
features are used to score each of the replacement
candidates (Xi,k) of wordi. Each Xi,k extracted
in the previous step is converted to a feature vector
containing 11 contextual features. Certainly more
features can be extracted with additional knowl-
edge sources. The framework allows adding more
features, but for the present results, only these 11
features were used.
As our aim is to assist the translation system in
finding longer target phrasal matches, the features
are constructed from the occurrence statistics of
Xi,k from the bilingual training corpus (BL). If a
candidate replacement does not occur in the BL,
then it is removed from the list of possible replace-
ment candidates.
Frequency counts for the features of a partic-
ular replacement, Xi,k, extracted in the context
of Li,?2Li,?1 (two preceding words of wordi)
and Ri,+1Ri,+2 (two following words of wordi)
(the remaining words in the left and right context
of wordi are not used for feature extraction) are
obtained as follows:
f1: frequency of Xi,kRi,+1
f2: frequency of Li,?1Xi,k
f3: frequency of Li,?1Xi,kRi,+1
f4: frequency of Li,?2Li,?1Xi,k
f5: frequency of Xi,kRi,+1Ri,+2
f6: frequency of Li,?2Li,?1Xi,kRi,+1
323
f7: frequency of Li,?1Xi,kRi,+1Ri,+2
f8: frequency of Li,?2Li,?1Xi,kRi,+1Ri,+2
f9: frequency of Xi,k in ML
f10: frequency of Xi,k in BL
f11: number of feature values (f1, ..f10) > 0
f11 is a vote feature which counts the num-
ber of features (f1 ... f10) that have a value
greater than zero. The features are normalized
to fall within [0, 1]. The sentences in ML, BL
and test data are padded with two begin markers
and two end markers for obtaining counts for
OOV/rare-words that appear at the beginning or
end of a test sentence.
3.5 Representation
Before we go on to explaining the lattice repre-
sentation, we would like to make a small clarifica-
tion in the terminalogy used. In the MT commu-
nity, a lattice usually refers to the list of possible
partially-overlapping target translations for each
possible source n?gram phrase in the input sen-
tence. Since we are using the term lattice to also
refer to the possible paths through the input sen-
tence, we will call the lattice used by the decoder,
the ?decoding lattice?. The lattice obtained from
the input sentence representing possible replace-
ment candidates will be called the ?input lattice?.
An input lattice (Figure 1) is constructed with
a beam of replacements for the OOV and rare
words. Each replacement candidate is given a
score (Eqn 1) indicating the confidence that a suit-
able replacement is found. The numbers in Fig-
ure 1 indicate the start and end indices (based
on character counts) of the words in the test sen-
tence. In T , two replacements were found for the
word ?hawks?: ?homers? and ?birds?. However,
?homers? was not found in the BL and hence, it
was removed from the replacement list.
The input lattice also includes the OOV word
with a low score (Eqn 2). This allows the EBMT
system to also include the OOV/rare-word dur-
ing decoding. In the Translation Model of the
EBMT system, this test lattice is matched against
the source sentences in the bilingual training cor-
pus. The matching process would now also look
for phrases with ?birds? and not just ?hawks?.
When a match is found, the corresponding trans-
  
T?:?????a?mobile?base?,?hitting?three?? hawks?with?one?arrow?.....input?lattice:0? 0? (???a???)1? 6? (???mobile???)7? 10? (???base???)11? 11? (??,??)12? 18? (???hitting???)13? 17? (???three???)18? 22? (???hawks?????0.0026)18? 22? (???birds???????0.9974)23? 26? (???with???)27? 29? (???one???)30? 34? (???arrow???)??????????????
Figure 1: Lattice of the input sentence T contain-
ing replacements for OOV words.
  
OOV/Rare word Candidate ReplacementsSpelling errorskrygyzstan kyrgyzstan,...
yusukuni yasukuni,..
kilomaters kilometers, miles, km, ...Coordinate termssomoa india, turkey, germany, russia, japan,...
ear body, arms, hands, feet, mind, car, ...
buyers dealer, inspector, the experts, smuggler,.Synonymsplummet drop, dropped, fell, ....Synonyms and Antonymsoptimal worse, better, minimal,....
Figure 2: Sample English candidate replacements
obtained.
lation in the target language is obtained through
sub-sentential alignment (Section 3.7). The scores
on the input lattice are later used by the decoder
(Section 3.7). Each replacement Xi,k for the
OOV/rare-word (wordi) is scored with a logistic
function (Bishop, 2006) to convert the dot product
of the features and weights (~? ? ~fi,k) to a score be-
tween 0 and 1 (Eqn 1 and Eqn 2).
p?(Xi,k|wordi) =
exp(~?? ~fi,k)
1+
?
j=1...S exp(~?? ~fi,j)
(1)
324
p?(wordi) =
1
1 +
?
j=1...S exp(~? ? ~fi,j)
(2)
where, ~fi,j is the feature vector for the jth replace-
ment candidate of wordi, S is the number of re-
placements, ~? is the weight vector indicating the
importance of the corresponding features.
3.6 Tuning feature weights
We would like to select those feature weights (~?)
which would lead to the least expected loss in
translation quality (Eqn 3). ?log(BLEU) (Pap-
ineni et al, 2002) is used to calculate the expected
loss over a development set. As this objective
function has many local minima and is piecewise
constant, the surface is smoothed using the L2-
norm regularization. Powell?s algorithm (Powell,
1964) with grid-based line optimization is used to
find the best weights. 7 different random guesses
are used to initialize the algorithm.
min
?
E?[L(ttune)] + ? ? ||?||2 (3)
The algorithm assumes that partial derivates of
the function are not available. Approximations of
the weights (?1, ..?N ) are generated successively
along each of the N standard base vectors. The
procedure is iterated with a stopping criteria based
on the amount of change in the weights and the
change in the loss. A cross-validation set (in ad-
dition to the regularization term) is used to pre-
vent overfitting at the end of each iteration of the
Powell?s algorithm. This process is repeated with
different values of ? , as in Deterministic Anneal-
ing (Rose, 1998). ? is initialized with a high value
and is halved after each process.
3.7 System Description
The EBMT system finds phrasal matches for the
test (or input) sentence from the source side of
the bilingual corpus. The corresponding tar-
get phrasal translations are obtained through sub-
sentential alignment. When an input lattice is
given instead of an input sentence, the system per-
forms the same matching process for all possible
phrases obtained from the input lattice. Hence,
the system also finds matches for source phrases
that contain the replacements for the OOV/rare-
word. Only the top C ranking replacement candi-
? ?
??????a????mobile??base???,??hitting???three???hawks????with???????one???????arrow??....????????????????????? ?????birds
       ?? ??   ? ?  ? ?
? ?? ?,?? ???? hawks
???three?birds??
??
Decoding?Lattice
 ? ? ?????three?birds?with?one?arrow??
Figure 3: Lattice containing possible phrasal tar-
get translations for the test sentence T .
dates for every OOV/rare word are used in build-
ing the input lattice. The optimal value of C was
empirically found to be 2. On examining the ob-
tained input lattices, the proposed method found
replacements for at the most 3 OOV/rare words in
each test sentence (Section 4). Hence, the number
of possible paths through the input lattice is not
substantially large.
The target translations of all the source phrases
are placed on a common decoding lattice. An
example of a decoding lattice for example T is
given in Figure 3. The system is now able to find
longer matches (? three birds with one arrow ?
and ? three birds ?) which was not possible earlier
with the OOV word, ?hawks?. The local order-
ing information between the translations of ?three
birds? and ?with one arrow? is well captured due
to the retrieval of the longer source phrasal match,
?three birds with one arrow?. Our ultimate goal
is to obtain translations for such longer n?gram
source phrases boosting the confidence of both the
translation model and the language model.
The decoder used in this paper (Brown, 2003)
works on this decoding lattice of possible
phrasal target translations (or fragments) for
source phrases present in the input lattice to gen-
erate the target translation. Similar to Pharaoh
(Koehn et al, 2003), the decoder uses multi-
level beam search with a priority queue formed
based on the number of source words translated.
Bonuses are given for paths that have overlapping
fragments. The total score (TS) for a path (Eqn
4) through the translation lattice is the arithmetic
average of the scores for each target word in the
325
path. The EBMT engine assigns each candidate
phrasal translation a quality score computed as
a log-linear combination of alignment score and
translation probability. The alignment score indi-
cates the engine?s confidence that the right target
translation has been chosen for a source phrase.
The translation probability is the proportion of
times each distinct alternative translation was en-
countered out of all the translations. If the path
includes a candidate replacement, the log of the
score, p?(wi), given for a candidate replacement
is incorporated into TS as an additional term with
a weight wt5.
TS = 1t
t?
i=1
[wt1 log(bi) + wt2 log(peni)
+wt3 log(qi) + wt4 log(P (wi|wi?2, wi?1))
+1I(wi=replacement)wt5 log(p?(wi)) ] (4)
where, t is the number of target words in the path,
wtj indicates the importance of each score, bi is
the bonus factor given for long phrasal matches,
peni is the penalty factor for source and target
phrasal-length mismatches, qi is the quality score
and P (wi|wi?2, wi?1) is the LM score. The pa-
rameters of the EBMT system (wtj) are tuned on
a development set.
The target translation is postprocessed to in-
clude the translation of the OOV/rare-word with
the help of the best path information from the
decoder. In the case of OOV words, since the
translation is not available, the OOV word is put
back into the final output translation in place of
the translation of its replacement. In the output
translation of the test example T , the translation
of ?birds? is replaced by the word, ?hawks?. For
rare words, knowing that the translation of the rare
word may not be correct (due to poor alignment
statistics), the target translation of the replacement
is replaced by the translation of the rare word
obtained from the dictionary. If the rare word
has multiple translations, the translation with the
highest score is chosen.
4 Experimental Setup
As we are interested in improving the per-
formance of low-resource EBMT, the English-
Haitian (Eng-Hai) newswire data (Haitian Cre-
ole, CMU, 2010) containing 15,136 sentence-
pairs was used. To test the performance in other
languages, we simulated sparsity by choosing less
training data for English-Chinese (Eng-Chi). For
the Eng-Chi experiments, we extracted 30k train-
ing sentence pairs from the FBIS (NIST, 2003)
corpus. The data was segmented using the Stan-
ford segmenter (Tseng et al, 2005). Although
we are only interested in small data sets, we also
performed experiments with a larger data set of
200k. 5-gram Language Models were built from
the target half of the training data with Kneser-
Ney smoothing. For the monolingual English cor-
pus, 9 million sentences were collected from the
Hansard Corpus (LDC, 1997) and FBIS data.
EBMT system without OOV/rare-word han-
dling is chosen as the Baseline system. The pa-
rameters of the EBMT system are tuned with 200
sentence pairs for both Eng-Chi and Eng-Hai. The
tuned EBMT parameters are used for the Base-
line system and the system with OOV/rare-word
handling. The feature weights for the proposed
method are then tuned on a seperate development
set of 200 sentence-pairs with source sentences
containing at least 1 OOV/rare-word. The cross-
validation set for this purpose is made up of 100
sentence-pairs. In the OOV case, 500 sentence
pairs containing at least 1 OOV word are used for
testing. For the rare word handling experiments,
500 sentence pairs containing at least 1 rare word
are used for testing.
To assess the translation quality, 4-gram word-
based BLEU is used for Eng-Hai and 3-gram
word-based BLEU is used for Eng-Chi. Since
BLEU scores have a few limitations, the NIST and
TER metrics are also used. The test data used for
comparing the system handling OOV words and
the Baseline (without OOV word handling) is dif-
ferent from the test data used for comparing the
system handling rare words and the Baseline sys-
tem (without rare word handling). In the former
case, the test data handles only OOV words and
in the latter, the test data only handles rare words.
Hence, the test data for both the cases do not com-
pletely overlap. As we are interested in determin-
ing whether handling rare words in test sentences
is useful, we keep both the test data sets seper-
ate and assess the improvements obtained by only
326
OOV/Rare system TER BLEU NISTOOV Baseline 77.89 18.61 4.8525Handling OOV 76.95 19.32 4.9664Rare Baseline 74.23 22.84 5.3803Handling Rare 74.02 23.12 5.4406
Table 1: Comparison of translation scores of the
Baseline system and system handling OOV and
Rare words for Eng-Hai.
handling OOV words and by only handling rare
words over their corresponding Baselines. As fu-
ture work, it would be interesting to create one test
data set to handle both OOV and rare words to see
the overall gain.
The test set is further split into 5 files and the
Wilcoxon (Wilcoxon, 1945) Signed-Rank test is
used to find the statistical significance.
5 Results
Sample replacements found are given in Figure 2.
For both Eng-Chi and Eng-Hai experiments, only
the top C ranking replacement candidates were
used. The value of C was tuned on the develop-
ment set and the optimal value was found to be
2. Translation quality scores obtained on the test
data with 30k and 200k Eng-Chi training data sets
are given in Table 2. Table 1 shows the results
obtained on Eng-Hai. Statistically significant im-
provements (p < 0.0001) were seen by handling
OOV words as well as rare words over their cor-
responding baselines.
As the goal of the approach was to obtain longer
target phrasal matches, we counted the number of
n-grams for each value of n present on the de-
coding lattice in the 30k Eng-Chi case. The sub-
plots: A and B in Figure 4, shows the frequency
of n-grams for higher values of n (for n > 5)
when handling OOV and rare words. The plots
clearly show the increase in number of longer tar-
get phrases when compared to the phrases ob-
tained by the baseline systems.
Since the BLEU and NIST scores were com-
puted only up to 3-grams, we further found the
number of n-gram matches (for n > 3) in the
final translation of the test data with respect to
the reference translations (subplots: C and D).
As expected, a larger number of longer n?gram
matches were found. For the OOV case, matches
6 7 8 9 10 11 12 13 14 150
20004000
60008000
1000012000
n?gram
#n?gr
ams o
n the 
decod
ing la
ttice
 
 BaselineHandling OOV words
6 7 8 9 10 11 12 13 14 150
5000
10000
15000
n?gram 
 BaselineHandling Rare words
4 5 6 7 8 9 10 110
50
100
150
n?gram 
 BaselineHandling OOV words
4 5 6 7 8 9 10 110
10
20
30
40
n?gram
#corre
ctly tr
ansla
ted n?
grams
 
 BaselineHandling Rare words
A C
B D
Figure 4: A, B: number of n-grams found for in-
creasing values of n on the decoding lattice. C, D:
number of target n-gram matches for increasing
values of n with respect to the reference transla-
tions.
OOV/Rare Training system TER BLEU NISTdata sizeOOV 30k Baseline 82.03 14.12 4.118630k Handling OOV 80.97 14.78 4.1798200k Baseline 79.41 19.90 4.6822200k Handling OOV 77.66 20.50 4.7654Rare 30k Baseline 82.09 15.36 4.362630k Handling Rare 80.02 16.03 4.4314200k Baseline 78.04 20.96 4.9647200k Handling Rare 77.35 21.17 5.0122
Table 2: Comparison of translation scores of the
Baseline system and system handling OOV and
Rare words for Eng-Chi.
up to 9-grams were found where the baseline only
found matches up to 8-grams.
6 Conclusion and Future Work
A simple approach to improve translation quality
by handling both OOV and rare words was pro-
posed. The framework allowed scoring and rank-
ing each replacement candidate efficiently.
The method was tested on two language pairs
and statistically significant improvements were
seen in both cases. The results showed that rare
words also need to be handled to see improve-
ments in translation quality.
In this paper, the proposed method was only ap-
plied on words, as future work we would like to
extend it to OOV and rare-phrases as well.
327
References
R. Barzilay and K. McKeown 2001. Extracting para-
phrases from a parallel corpus. In Proceedings
of the 39th Annual Meeting of the Association for
Computaional Linguistics, pp. 50-57.
C. M. Bishop 2006. Pattern Recognition and Machine
Learning, Springer.
R. D. Brown, R. Hutchinson, P. N. Bennett, J. G. Car-
bonell, P. Jansen. 2003. Reducing Boundary Fric-
tion Using Translation-Fragment Overlap. In Pro-
ceedings of The Ninth Machine Translation Summit,
pp. 24-31.
R. D. Brown. 2000. Automated Generalization of
Translation Examples. In Proceedings of The Inter-
national Conference on Computational Linguistics,
pp. 125-131.
C. Callison-Burch, P. Koehn and M. Osborne. 2006.
Improved Statistical Machine Translation Using
Paraphrases. In Proceedings of The North Ameri-
can Chapter of the Association for Computational
Linguistics, pp. 17-24.
J. Carbonell, S. Klien, D. Miller, M. Steinbaum, T.
Grassiany and J. Frey. 2006. Context-Based Ma-
chine Translation Using Paraphrases. In Proceed-
ings of The Association for Machine Translation in
the Americas, pp. 8-12.
N. Habash. 2008. Four Techniques for On-
line Handling of Out-of-Vocabulary Words in
Arabic-English Statistical Machine Translation. In
Proceedings of Association for Computational
Linguistics-08: HLT, pp. 57-60.
Public release of Haitian Creole lan-
guage data by Carnegie Mellon, 2010.
http://www.speech.cs.cmu.edu/haitian/
Z. Harris. 1954. Distributional structure. Word,
10(23): 146-162.
P. Koehn. 2004. Pharaoh: a Beam Search Decoder for
Phrase-Based Statistical Machine Translation Mod-
els. The Association for Machine Translation.
P. Koehn, F. J. Och and D. Marcu. 2003. Statis-
tical Phrase-Based Translation. In Proceedings of
HLT:The North American Chapter of the Associa-
tion for Computational Linguistics.
P. Koehn 2002 Europarl: A multilingual corpus
for evaluation of machine translation. Unpublished,
http://www.isi.edu/koehn/publications/europarl/
Linguistic Data Consortium. 1997 Hansard Corpus of
Parallel English and French. Linguistic Data Con-
sortium, December. http://www.ldc.upenn.edu/
Y. Marton, C. Callison-Burch and P. Resnik. 2009.
Improved Statistical Machine Translation Using
Monolingually-derived Paraphrases. In Proceed-
ing of The Empirical Methods in Natural Language
Processing, pp. 381-390.
NIST. 2003. Machine translation evaluation.
http://nist.gov/speech/tests/mt/
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of The Associa-
tion for Computational Linguistics. pp. 311-318.
M. Popovic? and H. Ney. 2004. Towards the use of
Word Stems and Suffixes for Statistical Machine
Translation. In Proceedings of The International
Conference on Language Resources and Evalua-
tion.
M. J. D. Powell. 1964. An efficient method for find-
ing the minimum of a function of several variables
without calculating derivatives Computer Journal.
Volume 7, pp. 152-162.
K. Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression, and related
optimization problems. In Proceedings of The In-
stitute of Electrical and Electronics Engineers, pp.
2210-2239.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky and C.
Manning. 2005. A Conditional Random Field
Word Segmenter. Fourth SIGHAN Workshop on
Chinese Language Processing.
D. Vilar, J. Peter, and H. Ney. 2007. Can we translate
letters? In Proceedings of Association Computa-
tional Linguistics Workshop on SMT, pp. 33-39.
M. Yang and K. Kirchhoff. 2006. Phrase-based
back-off models for machine translation of highly
inflected languages. In Proceedings of European
Chapter of the ACL, 41-48.
F. Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics, 1, 80-83. tool:
http://faculty.vassar.edu/lowry/wilcoxon.html
328
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 627?632,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Non-linear Mapping for Improved Identification of 1300+ Languages
Ralf D. Brown
Carnegie Mellon University Language Technologies Institute
5000 Forbes Avenue, Pittsburgh PA 15213 USA
ralf@cs.cmu.edu
Abstract
Non-linear mappings of the form
P (ngram)
?
and
log(1+?P (ngram))
log(1+?)
are applied to the n-gram probabilities
in five trainable open-source language
identifiers. The first mapping reduces
classification errors by 4.0% to 83.9%
over a test set of more than one million
65-character strings in 1366 languages,
and by 2.6% to 76.7% over a subset of 781
languages. The second mapping improves
four of the five identifiers by 10.6% to
83.8% on the larger corpus and 14.4% to
76.7% on the smaller corpus. The subset
corpus and the modified programs are
made freely available for download at
http://www.cs.cmu.edu/?ralf/langid.html.
1 Introduction
Language identification, particularly of short
strings, is a task which is becoming quite impor-
tant as a preliminary step in much automated pro-
cessing of online data streams such as microblogs
(e.g. Twitter). In addition, an increasing num-
ber of languages are represented online, so it is
desireable that performance remain high as more
languages are added to the identifier.
In this paper, we stress-test five open-source
n-gram-based language identifiers by presenting
them with 65-character strings (about one printed
line of text in a book) in up to 1366 languages. We
then apply a simple modification to their scoring
algorithms which improves the classification ac-
curacy of all five of them, three quite dramatically.
2 Method
The selected modification to the scoring algorithm
is to apply a non-linear mapping which spreads
out the lower probability values while compact-
ing the higher ones. This low-end spreading of
values is the opposite of what one sees in a Zip-
fian distribution (Zipf, 1935), where the proba-
bilities of the most common items are the most
spread out while the less frequent items become
ever more crowded as there are increasing num-
bers of them in ever-smaller ranges. The hypoth-
esis is that regularizing the spacing between val-
ues will improve language-identification accuracy
by avoiding over-weighting frequent items (from
having higher probabilities in the training data and
also occurring more frequently in the test string).
Two functions were selected for experiments:
x = P (ngram)
gamma:
y = x
?
loglike:
y =
log(1 + 10
?
x)
log(1 + 10
?
)
The first simply raises the n-gram probabil-
ity to a non-unity power; this exponent is named
?gamma? as in image processing (Poynton, 1998).
The second mapping function is a normalized vari-
ant of the logarithm function; the normalization
provides fixed points at 0 and 1, as is the case for
gamma. Each of the functions gamma and loglike
has one tunable parameter, ? and ? , respectively.
3 Related Work
Although n-gram statistics as a basis for language
identification has been in use for two decades since
Cavnar and Trenkle (1994) and Dunning (1994),
little work has been done on trying to optimize
the values used for those n-gram statistics. Where
some form of frequency mapping is used, it is of-
ten implicit (as in Cavnar and Trenkle?s use of
ranks instead of frequencies) and generally goes
unremarked as such.
Vogel and Tresner-Kirsch (2012) use the log-
arithm of the frequency for some experimental
runs, reporting that it improved accuracy in some
cases. Gebre et al (2013) used logarithmic term-
frequency scaling of words in an English-language
627
essay to classify the native language of the writer,
reporting an improvement from 82.36% accuracy
to 84.55% in conjunction with inverse document
frequency (IDF) weighting, and from 79.18% ac-
curacy to 80.82% without IDF.
4 Programs
4.1 LangDetect
LangDetect, version 2011-09-13 (Shuyo,
2014), uses the Naive Bayes approach. Inputs are
split into a bag of character n-grams of length 1
through 3; each randomly-drawn n-gram?s prob-
ability in each of the trained models is multiplied
by the current score for that model. After 1000
n-grams, or when periodic renormalization into
a probability distribution detects that one model
has accumulated an overwhelming probability
mass, the iteration is terminated. After averaging
seven randomized iterations, each with a random
gaussian offset (mean 5?10
?6
, standard deviation
0.5? 10
?6
) that is added to each probability prior
to multiplication (to avoid multiplication by zero),
the highest-scoring model is declared to be the
language of the input.
The mapping function is applied to the model?s
probability before adding the randomized off-
set. To work around the limitation of one model
per language code, disambiguating digits are ap-
pended to the language code during training and
removed from the output prior to scoring.
4.2 libtextcat
libtextcat, version 2.2-9 (Hugueney, 2011),
is a C reimplementation of the Cavnar and Tren-
kle (1994) method. It compiles ?fingerprints? con-
taining a ranked list of the 400 (by default) most
frequent 1- through 5-grams in the training data.
An unknown text is classified by forming its fin-
gerprint and comparing that fingerprint against the
trained fingerprints. A penalty is assigned based
on the number of positions by which each n-gram
differs between the input and the trained model;
n-grams which appear in only one of the two are
assigned the maximum penalty, equal to the size
of the fingerprints. The model with the lowest
penalty score is selected as the language of the in-
put.
For this work, the libtextcat source code
was modified to remove the hard-coded fingerprint
size of 400 n-grams. While adding the frequency
mapping, the code was discovered to also hard-
code the maximum distortion penalty at 400; this
was corrected to set the maximum penalty equal to
the maximum size of any loaded fingerprint.
1
Score mapping was implemented by dividing
each penalty value by the maximum penalty to
produce a proportion, applying the mapping func-
tion, and then multiplying the result by the maxi-
mum penalty and rounding to an integer (to avoid
other code changes). Because there are only a lim-
ited number of possible penalties, a lookup table is
pre-computed, eliminating the impact on speed.
4.3 mguesser
mguesser, version 0.4 (Barkov, 2008), is part of
the mnoGoSearch search engine. While its doc-
umentation indicates that it implements the Cav-
nar and Trenkle approach, its actual similarity
computation is very different. Each training and
test text is converted into a 4096-element hash ta-
ble by extracting byte n-grams of length 6 (trun-
cated at control characters and multiple consecu-
tive blanks), hashing each n-gram using CRC-32,
and incrementing the count for the corresponding
hash entry. The hash table entries are then nor-
malized to a mean of 0.0 and standard deviation
of 1.0, and the similarity is computed as the inner
(dot) product of the hash tables treated as vectors.
The trained model receiving the highest similarity
score against the input is declared the language of
the input.
Nonlinear mapping was added by inserting a
step just prior to the normalization of the hash ta-
ble. The counts in the table are converted to proba-
bilities by dividing by the sum of counts, the map-
ping is applied to that probability, and the result is
converted back into a count by multiplying by the
original sum of counts.
4.4 whatlang
whatlang, version 1.24 (Brown, 2014a), is
the stand-alone identification program from LA-
Strings (Brown, 2013). It performs identifica-
tion by computing the inner product of byte tri-
grams through k-grams (k=6 by default and in
this work) between the input and the trained mod-
els; for speed, the computation is performed in-
crementally, adding the length-weighted probabil-
1
The behavior observed by (Brown, 2013) of performance
rapidly degrading for fingerprints larger than 500 disappears
with this correction. It was an artifact of an increasing pro-
portion of n-grams present in the model receiving penalties
greater than n-grams absent from the model.
628
ity of each n-gram as it is encountered in the in-
put. Models are formed by finding the highest-
frequency n-grams of the configured lengths, with
some filtering as described in (Brown, 2012).
4.5 YALI
YALI (Yet Another Language Identifier) (Majlis,
2012) is an identifier written in Perl. It performs
minor text normalization by collapsing multiple
blanks into a single blank and removing leading
and trailing blanks from lines. Thereafter, it uses
a sliding window to generate byte n-grams of a
(configurable) fixed length, and sums the proba-
bilities for each n-gram in each trained model. As
with whatlang, this effectively computes the in-
ner products between the input and the models.
Mapping was added by applying the mapping
function to the model probabilities as they are
read in from disk. As with LangDetect, disam-
biguating digits were used to allow multiple mod-
els per language code.
5 Data
The data used for the experiments described in
this paper comes predominantly from Bible trans-
lations, Wikipedia, and the Europarl corpus of Eu-
ropean parliamentary proceedings (Koehn, 2005).
The 1459 files of the training corpus generate 1483
models in 1368 languages. A number of train-
ing files generate models in both UTF-8 and ISO
8859-1, numerous languages have multiple train-
ing files in different writing systems, and several
have multiple files for different regional variants
(e.g. European and Brazilian Portugese).
The text for a language is split into training,
test, and possibly a disjoint development set. The
amount of text per language varies, with quartiles
of 1.19/1.47/2.22 million bytes. In general, ev-
ery thirtieth line of text is reserved for the test set;
some smaller languages reserve a higher propor-
tion. If more than 3.2 million bytes remain af-
ter reserving the test set, every thirtieth line of
the remaining text is reserved as a development
set. There are development sets for 220 languages.
The unreserved test is used for model training.
The test data is word-wrapped to 65 characters
or less, and wrapped lines shorter than 25 bytes
are excluded. Up to the first 1000 lines of wrapped
text are used for testing. One language with fewer
than 50 test strings is excluded from the test set, as
is the constructed language Klingon due to heavy
pollution with English. In total, the test files con-
tain 1,090,571 lines of text in 1366 languages.
Wikipedia text and many of the Bible transla-
tions are redistributable under Creative Commons
licenses, and have been packaged into the LTI
LangID Corpus (Brown, 2014b). This smaller
corpus contains 781 languages, 119 of them with
development sets, and a total of 649,589 lines in
the test files. The languages are a strict subset
of those in the larger corpus, but numerous lan-
guages have had Wikipedia text substituted for
non-redistributable Bible translations.
6 Experiments
Using the data sets described in the previous sec-
tion, we ran a sweep of different gamma and tau
values for each language identifier to determine
their optimal values on both development and test
strings. Step sizes for ? were generally 0.1, while
those for ? were 1.0, with smaller steps near the
minima. Since it does not provide explicit con-
trol over model sizes, LangDetect was trained
on a maximum of 1,000,000 bytes per model, as
reported optimal in (Brown, 2013). The other pro-
grams were trained on a maximum of 2,500,000
bytes per model; libtextcat and whatlang
used default model sizes of 400 and 3500, respec-
tively, while mguesser was set to the previously-
reported 1500 n-grams per model. After some ex-
perimentation, YALI was set to use 5-grams, with
3500 n-grams per model to match whatlang.
7 Results
Tables 1 and 2 show the absolute performance and
relative percentage change in classification errors
for the five programs using the two mapping func-
tions, as well as the values of ? and ? at which the
fewest errors were made on the development set.
Overall, the smaller corpus performed worse due
to the greater percentage of Wikipedia texts, which
are polluted with words and phrases in other lan-
guages. In the test set, this occasionally causes
a correct identification as another language to be
scored as an error.
Figures 2 and 3 graph the classification error
rates (number of incorrectly-labeled strings di-
vided by total number of strings in the test set) in
percent for different values of ?. A gamma of 1.0
is the baseline condition. The dramatic improve-
ments in mguesser, whatlang and YALI are
quite evident, while the smaller but non-trivial im-
629
gamma mapping loglike mapping
Program Error% Error% ?% ? Error% ?% ?
LangDet. 3.233 2.767 -14.4 0.80 2.889 -10.6 1.0
libtextcat 6.787 6.514 -4.0 2.20 ? ? ?
mguesser 15.704 4.330 -72.4 0.39 4.177 -73.4 3.8
whatlang 13.309 2.136 -83.9 0.27 2.146 -83.8 4.5
YALI 9.883 2.313 -76.6 0.20 2.313 -76.6 8.0
Table 1: Language-identification accuracy on the 1366-language corpus. ? and ? were tuned on the
220-language development set; only marginally better results can be achieved by tuning on the test set.
gamma mapping loglike mapping
Program Error% Error% ?% ? Error% ?% ?
LangDet. 3.603 3.093 -14.2 0.68 3.083 -14.4 2.3
libtextcat 6.693 6.521 -2.6 1.70 ? ? ?
mguesser 14.200 4.936 -65.2 0.40 4.779 -66.3 3.7
whatlang 11.879 2.770 -76.7 0.14 2.772 -76.7 5.6
YALI 8.726 2.972 -65.9 0.09 2.989 -65.7 9.0
Table 2: Language-identification accuracy on the 781-language corpus. ? and ? were tuned on the 119-
language development set. libtextcat did not improve with the loglike mapping (see text).
provements in libtextcat are difficult to dis-
cern at this scale. Since libtextcat uses much
smaller models than the others by default, Figure
1 gives a closer look at its performance for larger
model sizes. As the models grow, the absolute
baseline performance improves, but the change
from gamma-correction decreases and the optimal
value of ? also decreases toward 1.0. This hints
that the implicit mapping of ranks either becomes
closer to optimal, or that gamma becomes less ef-
fective at correcting it. At a model size of 3000
n-grams, the baseline error rate is 2.465% while
the best performance is 2.457% at ? = 1.10.
That the best ? for libtextcat is greater
than 1.0 was not entirely unexpected. The power-
law distribution of n-gram frequencies implies
that the conversion from frequencies to ranks is
essentially logarithmic, and log n eventually be-
comes less than n
c
for any c > 0. The implication
of ? > 1 is simply that the conversion to ranks
is too strong a correction, which must be partially
undone by the gamma mapping.
Figures 4 and 5 graph the error rates for differ-
ent values of ? . On the graph, zero is the baseline
condition without mapping for comparison pur-
poses; the mapping function is not the identity for
? = 0. It can clearly be seen that libtextcat is
hurt by the loglike mapping, which never reduces
values, even with negative ? . Using the inverse of
2
3
4
5
6
7
8
9
0.0 0.5 1.0 1.5 2.0
Er
ro
r R
at
e 
(%
)
Gamma
n=400
n=500
n=2000
n=3000
Figure 1: libtextcat performance at different
fingerprint sizes. ? = 1 is the baseline.
the loglike mapping should improve performance,
but has not yet been tried. The other programs
show very similar behavior to their results with
gamma.
8 Conclusions and Future Work
Non-linear mapping is shown to be effective at
improving the accuracy of five different language
630
24
6
8
10
12
14
16
0.0 0.5 1.0 1.5 2.0
Er
ro
r R
at
e 
(%
)
Gamma
mguesser (n=1500)
YALI (5gr, n=3500)
libtextcat (n=400)
LangDetect
whatlang (n=3500)
Figure 2: Performance of the identifiers on the
1366-language corpus using the gamma mapping.
2
4
6
8
10
12
14
16
0.0 0.5 1.0 1.5 2.0
Er
ro
r R
at
e 
(%
)
Gamma
mguesser (n=1500)
YALI (5gr, n=3500)
libtextcat (n=400)
LangDetect
whatlang (n=3500)
Figure 3: Performance of the identifiers on the
781-language corpus using the gamma mapping.
identifier using four highly-divergent algorithms
for computing model scores from n-gram statis-
tics. Improvements range from small ? 2.6% re-
duction in classification errors ? to dramatic for
the three programs with the worst baselines ? 65.2
to 76.7% reduction in errors on the smaller cor-
pus and 72.4 to 83.9% on the larger. While both
mappings have similar performance for four of the
programs, libtextcat only benefits from the
gamma mapping, as it can also reduce n-gram
scores, unlike the loglike mapping.
2
4
6
8
10
12
14
16
0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0
Er
ro
r R
at
e 
(%
)
Tau
mguesser (n=1500)
YALI (5gr, n=3500)
libtextcat (n=400)
LangDetect
whatlang (n=3500)
Figure 4: Performance of the identifiers on the
1366-language corpus using the loglike mapping.
2
4
6
8
10
12
14
16
0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0
Er
ro
r R
at
e 
(%
)
Tau
mguesser (n=1500)
YALI (5gr, n=3500)
libtextcat (n=400)
LangDetect
whatlang (n=3500)
Figure 5: Performance of the identifiers on the
781-language corpus using the loglike mapping.
Training data, source code, and supple-
mentary information may be downloaded from
http://www.cs.cmu.edu/?ralf/langid.html.
Future work includes modifying additional lan-
guage identifiers such as langid.py (Lui and
Baldwin, 2012) and VarClass (Zampieri and
Gebre, 2014), experimenting with other mapping
functions, and investigating the method?s efficacy
on pluricentric languages like those VarClass is
designed to identify.
631
References
Alexander Barkov. 2008. mguesser ver-
sion 0.4. http://www.mnogosearch.org/guesser/-
mguesser-0.4.tar.gz (accessed 2014-08-19).
Ralf D. Brown. 2012. Finding and Identifying Text in
900+ Languages. Digital Investigation, 9:S34?S43.
Ralf D. Brown. 2013. Selecting and Weighting N-
Grams to Identify 1100 Languages. In Proceedings
of Text, Speech, and Discourse 2013, September.
Ralf Brown. 2014a. Language-Aware String Extractor,
August. https://sourceforge.net/projects/la-strings/
(accessed 2014-08-19).
Ralf D. Brown. 2014b. LTI LangID Corpus, Release
1. http://www.cs.cmu.edu/?ralf/langid.html.
William B. Cavnar and John M. Trenkle. 1994. N-
Gram-Based Text Categorization. In Proceedings
of SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, pages 161?175.
UNLV Publications/Reprographics, April.
Ted Dunning. 1994. Statistical Identification of Lan-
guage. Technical Report MCCS 94-273, New Mex-
ico State University.
Binyam Gebrekidan Gebre, Marcos Zampieri, Peter
Wittenburg, and Tom Heskes. 2013. Improving Na-
tive Language Identification with TF-IDF Weight-
ing. In Proceedings of the 8th NAACL Workshop on
Innovative Use of NLP for Building Educational Ap-
plications (BEA8).
Bernard Hugueney. 2011. libtextcat 2.2-
9: Faster Unicode-focused C++ reimplementation
of libtextcat. https://github.com/scientific-coder/-
libtextcat (accessed 2014-08-19).
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the Tenth Machine Translation Summit (MT Summix
X), pages 79?86.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
Off-the-shelf Language Identification Tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2012),
pages 25?30, July.
Martin Majlis. 2012. Yet Another Language Identi-
fier. In Proceedings of the Student Research Work-
shop at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 46?54, Avignon, France, April. Association
for Computational Linguistics.
Charles Poynton. 1998. The Rehabilitation of
Gamma. In Human Vision and Electronic Imag-
ing III, Proceedings of SPIE/IS&T Conference
3299, January. http://www.poynton.com/PDFs/-
Rehabilitation of gamma.pdf.
Nakatani Shuyo. 2014. Language Detection Li-
brary for Java, March. http://code.google.com/p/-
language-detection/ (accessed 2014-08-19).
John Vogel and David Tresner-Kirsch. 2012. Robust
Language Identification in Short, Noisy Texts: Im-
provements to LIGA. In Proceedings of the Third
International Workshop on Mining Ubiquitous and
Social Environments (MUSE 2012), pages 43?50,
September.
Marcos Zampieri and Binyam Gebrekidan Gebre.
2014. VarClass: An Open Source Language Iden-
tification Tool for Language Varieties. In Proceed-
ings of the Ninth International Language Resources
and Evaluation Conference (LREC 2014), Reyk-
javik, Iceland, May.
George Kingsley Zipf. 1935. The Psycho-biology of
Language: An Introduction to Dynamic Philology.
Houghton-Mifflin Co., Boston.
632
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 384?391,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Taming Structured Perceptrons on Wild Feature Vectors
Ralf D. Brown
Carnegie Mellon University Language Technologies Institute
5000 Forbes Avenue, Pittsburgh PA 15213 USA
ralf+@cs.cmu.edu
Abstract
Structured perceptrons are attractive due
to their simplicity and speed, and have
been used successfully for tuning the
weights of binary features in a machine
translation system. In attempting to apply
them to tuning the weights of real-valued
features with highly skewed distributions,
we found that they did not work well. This
paper describes a modification to the up-
date step and compares the performance
of the resulting algorithm to standard min-
imum error-rate training (MERT). In ad-
dition, preliminary results for combining
MERT or structured-perceptron tuning of
the log-linear feature weights with coordi-
nate ascent of other translation system pa-
rameters are presented.
1 Introduction
Structured perceptrons are a relatively recent
(Collins, 2002) update of the classic perceptron
algorithm which permit the prediction of vec-
tors of values. Initially developed for part of
speech taggers, they have been applied to tuning
the weights of the features in the log-linear mod-
els used by statistical machine translation (Arun
and Koehn, 2007), and found to have performance
similar to the Margin-Infused Relaxed Algorithm
(MIRA) by Crammer and Singer (2003; 2006) and
Minimum-Error Rate Training (MERT) by Och
(2003). Parameter tuning is an important aspect of
current data-driven machine translation systems,
as an improper selection of feature weights can
dramatically reduce scores on evaluation metrics
such as BLEU (Papineni et al, 2002) or METEOR
(Banerjee and Lavie, 2005).
When we recently added new features to the
CMU-EBMT translation system (Brown, 1996;
Brown, 2008)1, in addition to splitting a number of
composite features into their components, our pre-
vious method of parameter tuning via coordinate
ascent2 became impractical. With now more than
50 features partaking in the scoring model, MERT
no longer seemed a good choice, as the common
wisdom is that it is not able to reliably optimize
more than about 20 features (Chiang et al, 2008).
We had been using coordinate ascent because of
a need to tune a substantial number of parameters
which are not directly part of the log-linear model
which can be tuned by MERT or similar methods.
Our system generates a translation lattice by run-
time lookup in the training corpus rather than us-
ing a precomputed phrase table, so important pa-
rameters include
? the size of the sample of retrieved training
instances for a given input phrase which are
aligned,
? the weight of source features for ranking
training instances during sampling, and
? the minimum alignment score to accept a
translation instance
Decoder parameters which are important to tune,
but which are generally not mentioned in the liter-
ature include
? how many alternative translations of a phrase
to consider during decoding,
? the size of the reordering window, and
? the rank of the language model (4-gram, 5-
gram, etc.)
In addition, it is desirable to tune parameters such
as beam width to minimize translation time with-
out degrading performance.
1Source code for CMU-EBMT is available from
http://cmu-ebmt.sourceforge.net.
2Coordinate ascent is described in more detail in Sec-
tion 7.
384
As a result of the non-model parameters, a full
system tuning will involve multiple runs of the
tuning algorithm for the feature weights, since the
other parameters will affect the optimal weights.
Thus, speed is an important consideration for any
method to be used in this setting. The structured
perceptron algorithm is ideally suited due to its
speed, provided that it can produce competitive re-
sults.
2 Related Work
The perceptron algorithm (Rosenblatt, 1958) itself
is over 50 years old, but variations such as voted
and averaged perceptrons have gained popularity
in the past ten years. In particular, Collins (2002)
adapted the perceptron algorithm to structured
prediction tasks such as part of speech tagging and
noun phrase chunking. Arun and Koehn (2007)
subsequently applied Collins? structured percep-
tron algorithm to the task of tuning feature weights
in a statistical machine translation system, demon-
strating the extreme scalability of the algorithm by
applying it to vectors containing four to six mil-
lion binary features. However, their work left open
the question of how well structured perceptrons
would deal with continuous-valued features. They
were unable to apply a language model due to the
lack of continuous-valued features and hence had
to compare performance against a standard statis-
tical machine translation (SMT) system which had
been stripped of its language model, with a conse-
quent loss of several BLEU points in performance.
During the same period, Crammer et al(2003;
2006) developed a number of ?ultraconservative?
learning algorithms, including MIRA, the Margin-
Infused Relaxed Algorithm (which was also ap-
plied to large binary feature vectors by Arun and
Koehn) and variations of what they referred to as
Passive-Aggressive algorithms including PA-I and
PA-II. These algorithms have in common the no-
tion of updating a weight vector ?just enough? to
account for a new training instance which is in-
correctly predicted by the existing weight vector.
In contrast, the perceptron algorithm aggressively
updates the weight vector and relies on averaging
effects over the whole of the training set.
3 Structured Perceptrons
The structured perceptron algorithm can be ap-
plied to tasks where the goal is to select the best
among competing hypotheses, where each hypoth-
esis has an associated vector of feature values and
the score for a hypothesis is a linear combination
of its feature values.
Beginning with a zero vector for the feature
weights, the structured perceptron algorithm it-
erates through each element of the training set,
updating the weight vector after processing each
training instance. The training set is processed re-
peatedly (each pass is known as a training epoch)
until convergence. The update step is very sim-
ple: if the best hypothesis according to the prod-
uct of feature vector and weight vector is not the
correct answer, add the difference between the fea-
ture vectors of the correct answer and the model?s
selected answer to the weight vector.
Thus, the entire algorithm may be summarized
with just two equations:
~w ? 0 (1)
~w ? ~w + (?oracle ? ?top1) (2)
where ?x is the feature vector (?1, ?2, ..., ?n) for
hypothesis x.
Repeated application of Equation 2 results in
a weight vector which reflects the relative impor-
tance (on average) of each feature to making the
correct selection. Since selecting the best hypoth-
esis is an arg max operation, the absolute mag-
nitudes of the weights are not important.
4 More Conservative Updates for
Structured Perceptrons
One issue which arises in using learning algo-
rithms for machine translation is that there is no
one correct answer. In addition, it may not even
be possible for the MT system to generate the
reference translation at all. This is commonly
addressed by using the highest-scoring (by some
metric such as BLEU) translation which the sys-
tem can generate as a pseudo-oracle.
Our initial implementation closely followed the
description in (Arun and Koehn, 2007), includ-
ing the refinement of using the objective-function
score of the pseudo-oracle translation from the n-
best list to modulate the learning rate of the update
step, i.e.
~w ? ~w + S?oracle ? (?oracle ? ?top1) (3)
As can be seen, the difference between Equa-
tions 2 and 3 is simply the additional factor of
S?oracle .
385
While we initially used sentence-level
smoothed BLEU as the objective function,
we found it to perform very poorly (the full BLEU
scores on the Haitian Creole tuning set were well
below 0.10), and instead adopted the Rouge-S
(skip bigrams) metric by Lin and Och (2004a)
with a maximum skip distance of four words,
which was found to best correlate with human
quality judgements (Lin and Och, 2004b).
In early testing, we found that both the feature
weights and performance as measured by the av-
erage objective score over the tuning set oscillated
wildly. Analyzing the results, it became appar-
ent that the update function was overly aggres-
sive. Unlike the binary features used in (Arun
and Koehn, 2007), our continuous-valued features
have different operating ranges for each feature,
e.g. the total distance moved as a result of reorder-
ing could reach 100 on a long sentence, while the
proportion of training instances with at least six
words of adjacent context in the bilingual corpus
is unlikely to exceed 0.05, even where sampling
is biased toward training instances with adjacent
context.
The first attempt to address the disparity in op-
erating ranges was to perform feature-wise nor-
malization on the update. Instead of taking the
simple difference in feature vectors between the
n-best entry with the highest log-linear score and
the one with the highest objective score, we con-
struct ?diff such that
?i(diff)?
(?i(oracle)? ?i(top1))
r2
(4)
where
r ? max(0.01,maxj |?i(j)|) (5)
i.e. we estimate the operating range by finding the
n-best entry with the highest magnitude value of
the feature, and then divide by the square of that
magnitude since large feature values also magnify
the effects of weight changes. Normalization is
limited by clipping the normalization factor to be
at least 0.01 so that features whose values are al-
ways very near zero do not dominate the overall
score.
While the feature-wise normalization did
largely control the wild swings in feature weights,
it did not curb the oscillations in the objective
scores and produced only a minor improvement in
tuning results.
We next looked at MIRA and related work
on so-called Passive-Aggressive algorithms, and
in particular at the update functions described in
(Crammer et al, 2006). We decided on their PA-
II update rule (PA-II being akin to 1-best MIRA),
with which the learning step becomes
~w ? ~w + ? ? (?oracle ? ?top1) (6)
where
loss? S?oracle ? S?top1 (7)
? ?
loss
||?oracle ? ?top1||2 + 12C
(8)
with C an ?aggressiveness? parameter.
This version of the update function produced
the desired smooth changes in feature weights
from iteration to iteration, though objective scores
still do not converge. Allowing multiple passes
through the tuning set before re-decoding with up-
dated feature weights now frequently results in
weights where the pseudo-oracle is the top-ranked
translation in 80 to 90 percent of all sentences.
None of our previous experiments had achieved
even a fraction of this level due to the erratic be-
havior of the feature weights. However, as the ex-
treme overfitting necessary to achieve such high
rankings of the oracle translation results in poor
BLEU scores, we have since used only one pass
over the tuning set before re-decoding with up-
dated weights.
5 The Final Algorithm
After the various attempts at taming the behav-
ior of the structured perceptron approach just de-
scribed, the final algorithm used for the experi-
ments described below was
1. Structured perceptron, with
2. passive-aggressive updates,
3. run in semi-batch mode,
4. using sentence-level modified Rouge-S4 as
the objective function
Semi-batch mode here means that while the per-
ceptron algorithm updates the weight vector af-
ter each sentence, those updates are not commu-
nicated to the decoder until the end of a complete
pass through the tuning set. An exception is made
for the very first iteration, as it starts with uniform
weights of 10?9 (rather than the conventional zero,
which would cause problems with decoding). This
386
permits the exact determination of the overall ob-
jective score for the weight vector which is even-
tually returned as the tuned optimal weights, and
permits parallelization of the decoding (though the
latter has not yet been implemented).
We slightly modified the Rouge-S scoring func-
tion to use the generalized F-measure
F? =
(1 + ?2)? precision? recall
?2 ? precision+ recall
(9)
instead of the standard F1, allowing us to give
more weight to recall over precision by increas-
ing ? above 1.0. This change was prompted by
the observation that the tuning process strongly
favored shorter outputs, resulting in substantial
brevity penalties from BLEU.
6 Experiments
We present the results of experiments on three
data sets in the next section. The data sets
are English-to-Haitian, French-to-English, and
Czech-to-English.
The English-to-Haitian system was built using
the data released by Carnegie Mellon University
(2010). It consists of a medical phrasebook, a
glossary, and a modest amount of newswire text,
each available as a set of sentence pairs in En-
glish and Haitian Creole. For training, we used
all of the glossary, all but the last 300 phrase pairs
of the medical phrasebook (these had previously
been used for development and testing of a ?toy?
system), and the first 12,500 sentence pairs of the
newswire text. Tuning was performed using the
next 217 sentence pairs of the newswire text, and
the test set consisted of the final 800 sentence pairs
of the newswire text. The target language model
was built solely from the target half of the training
corpus, as we did not have any additional Haitian
Creole text.
The French-to-English system was built using
the Europarl (Koehn, 2005) version 3 data for
French and English. As is usual practice, text from
the fourth quarter of 2000 was omitted from the
training set. Tuning was performed using 200 sen-
tences from the ?devtest2006? file and all 2000
sentences of ?test2007? were used as the final test
set. Two target language models were built and
interpolated during decoding; the first was trained
on the target half of the bilingal corpus, and the
second was built using the Canadian Hansards text
released by ISI (Natural Language Group, 2001).
The Czech-to-English system was built us-
ing the parallel data made available for the
2010 Workshop on Statistical Machine Transla-
tion (WMT10). The target language model was
built from the target half of the bilingual training
corpus. Tuning was performed on a 200-sentence
subset of the ?news-2008-test? data, and all 2525
sentences of the ?news-2009-test? data were used
as unseen test data. As these experiments were
the very first time that the CMU-EBMT system
was applied to Czech, there are undoubtedly nu-
merous pre-processing and training improvements
which will increase scores above the values pre-
sented here.
Parameter tuning was performed using CMERT
0.5, the reimplemented MERT program included
with recent releases of the MOSES translation
system (specifically, the version included with
the 2010-04-01 release), the annealing-based op-
timizer included with Cunei (Phillips and Brown,
2009; Phillips, 2010), and the Structured Percep-
tron optimizer. Feature weights were initialized
to a uniform value of 1.0 for MERT and 10?9
for annealing and Perceptron (since the usual zero
causes problems for the decoder). Both versions
of MERT were permitted to run for 15 iterations
or until features weights converged and remained
(nearly) unchanged from one iteration to the next,
using merged n-best lists from the current and the
three most recent prior iterations. Annealing was
run with gamma values from 0.25 to 4.0, skipping
the entropy phase. The Structured Perceptron was
allowed to run for 18 iterations and to choose the
weights from the iteration which resulted in the
highest average Rouge-S score for the top trans-
lation in the n-best list. For French-English, this
proved to be the sixth iteration, while for English-
Haitian it was the twelfth. We have found that the
objective score increases for the first six to eight
iterations of SP, after which it fluctuates with no
trend up or down (but occasionally setting a new
high, which is why we decided to run 18 itera-
tions).
For French-English, we determined the best
value of ? for the Rouge-S scoring to be 1.5,
and the best value of the aggressiveness parame-
ter C to be 0.1, using a 40-sentence subset of the
French-English tuning set, and then applied those
value for the full tuning set. For English-Haitian,
we used ? = 1.2 and C = 0.01 (lower values
of C provide more smoothing and overall smaller
387
updates, which is necessary for sparse or noisy
data). Due to limited time prior to submission, the
English-Haitian values for ? and C were re-used
for Czech, with no attempt at tuning.
7 Combining Log-Linear Tuning with
Coordinate Ascent
As noted in the introduction, translation systems
using SMT-style decoders incorporate various fea-
tures that affect performance (and/or speed), but
which do not contribute directly to the log-linear
scoring model. Thus, neither MERT nor the struc-
tured perceptron training presented in this paper is
a complete solution for parameter tuning.
The CMU-EBMT system has long used a coor-
dinate ascent approach to parameter tuning. Each
parameter is varied in turn, with the MT system
performing a translation for each setting, and the
value which produces the best score is retained
while the next parameter is varied. If the best
scoring value is the highest or lowest in the list of
values to be checked, the range is extended; like-
wise, unless the interval between adjacent values
is already very small, the intervals on each side
of the highest-scoring value (which is not one of
the extremes) is divided in half and the two addi-
tional points are evaluated. This process continues
until convergence (cycling through all parameters
without changing any of them) or until a pre-set
maximum number of parameter combinations is
scored. Naturally, the approach becomes slower
as the number of parameters increases, but it was
still (barely) practical with 20 to 25 parameters.
A recent change in the internals of CMU-EBMT
led to a decomposition of multiple composite
scores and the addition of numerous others, bal-
looning the total number of tunable parameters to
more than 60. Fortunately, most of the tunable
parameters are feature weights, which can all be
treated as a unit, leaving only about a dozen fea-
tures for coordinate ascent.
The tuning program operates by calling an eval-
uation script which in turn invokes the machine
translation on a modified configuration file pro-
vided by the tuner and returns the score corre-
sponding to the given parameter settings. When
given an optional flag, the evaluation script first
invokes either MERT or SP to further adjust the
parameters before performing the actual evalua-
tion, and modifies the given configuration file ac-
cordingly. The tuner reads the modified parame-
ters from the configuration file and stores then for
further use.
Both MERT and SP can produce settings which
actually decrease the resulting BLEU score, since
they are optimizing toward a surrogate metric. If
the evaluation score after an invocation of MERT
or SP is less than 0.98 times the previous best
score, the parameter settings are rolled back; oth-
erwise, the best score is set to the evaluation score.
This permits MERT/SP to move the parameters
to a different space if necessary, without allowing
them to substantially degrade overall scores.
There was time for only one experiment involv-
ing complete tuning, as summarized in Table 4.
Starting with the Haitian-Creole feature weights
found for the results in Table 1, the tuner ran-
domly perturbed the non-feature-weight parame-
ters by a small amount (up to 2% relative) twenty
times, then started coordinate ascent from the best-
scoring of those 20 trials. The tuner requested a
MERT/SP run before ascending on the first pa-
rameter, and after every fourth parameter was pro-
cessed thereafter. Because both MERT and SP
started from previously-tuned feature weights, the
number of iterations was reduced from 15 to 4 for
MERT and from 18 to 5 for SP. The maximum
number of parameter combinations for coordinate
ascent was set to 750, which is approximately four
cycles through all parameters (the exact number of
combinations per cycle varies, as the tuner can add
new combinations by extended the range which is
searched or adding intermediate points around a
maximum).
In Table 4, the three different Perceptron en-
tries refer to the results starting from the pre-
vious experiment?s feature weights (?Perceptron
1?), starting from the results of the complete tun-
ing (?Perceptron 2?), and starting from uniform
feature weights (?Perceptron 3?). The third run
was stopped before convergence due to the loom-
ing submission deadline.
8 Results
Tables 1, 2, and 3 present the results of running the
tuning methods on the English-Haitian, French-
English, and Czech-English data sets, respectively.
Performance is shown both in terms of the time re-
quired to perform a tuning run as well as the BLEU
score achieved using the resulting feature weights.
Structured perceptrons are the clear winner for
speed, thanks to the simplicity of the algorithm.
388
Method Run-Time Iter BLEU (dev) BLEU (test) #words / ratio
CMERT 0.5 73m 5 0.0993 ?
new MERT 58m 3 0.0964 ?
CMERT 0.51 138m 15 0.1073 0.0966 22298 / 1.213x
new MERT1 187m 15 0.1516 0.1347 17375 / 0.945x
Perceptron 22m 18 0.1619 0.1534 15565 / 0.847x
1 omitting several unused features, as noted in the text
Table 1: English-to-Haitian tuning performance
Method Run-Time Iter BLEU (dev) BLEU (test) #words / ratio
CMERT 0.5 3h53m 15 0.12952 0.13927 100875 / 1.709x
new MERT 5h52m 15 0.22533 0.23315 60354 / 1.023x
Annealing 6h46m - 0.25017 0.25943 58518 / 0.992x
Perceptron 1h23m 18 0.24214 0.26048 57408 / 0.973x
Table 2: French-to-English tuning performance
While MERT takes two to three times as long to
process ten random starting points as it does to
decode the test set, SP is three orders of magni-
tude faster than decoding. As a result, SP tuning
requires one-third or less of the time that MERT
does, even though we used 18 iterations of SP
compared to 15 for MERT. Note that the time dif-
ference between the two versions of MERT is in
part due to different amounts of time spent decod-
ing as a result of the different feature weights.
MERT unexpectedly has considerable difficulty
with our new feature set, as can be seen by
its much lower BLEU scores, particularly in the
case of CMERT. An analysis of the actual fea-
ture weights produced by MERT shows that it
places nearly all of the mass on a single feature,
and that the feature receiving the bulk of the mass
changes from iteration to iteration. In contrast, SP
produces BLEU scores consistent with those pro-
duced by pure coordinate ascent prior to the pro-
liferation of features.
We believe that the difference in performance
between the two versions of MERT is due pri-
marily to the simple difference in output format:
CMERT 0.5 prints its tuned weights using a fixed-
point format having six digits after the decimal
point, while the new MERT program prints us-
ing scientific notation. Because the tuned weight
vector is highly skewed, most features have low
weights after L1 normalization, and thus CMERT
truncated many weights to zero (and indeed, loses
significant digits for any features assigned weights
less than 0.1), including such critical weights as
length features and language model scores. We
suspect that this preservation of significant digits
contributes substantially to the improved BLEU
scores Bertoldi et al(2009) reported for the new
implementation compared to CMERT.
The features which, at one time or another, re-
ceive the bulk of the mass have one thing in com-
mon: for most translations, they have a default
value, and in a small proportion of cases they have
a value which varies from the default by only a
small amount. Initially, most such features had
a default value of zero in CMU-EBMT, but this
meant that the line optimization in MERT had ab-
solutely no constraint on raising the weight of the
feature, and thus obtaining feature vectors where
one feature has 1018 or even 1020 times the weight
of any other feature. The same problem occurs
with features that are unused but have a small jit-
ter in their values due to rounding errors, for ex-
ample, if there are no document boundaries (as is
the case for the Haitian data described previously),
the document-similarity score may be 1.000000
for 99% of the arcs in the translation lattices and
0.999999 for the remainder. Offsetting the mostly-
zero features so that their default value is 1 or -1
(depending on the sense of the feature) and elim-
inating unused features mitigated but did not en-
tirely solve the problem. In Table 1, two results are
shown for both CMERT and new MERT; the first
includes all 52 features while the second excludes
five features which are not used in a baseline-
trained CMU-EBMT system. In the former case,
both programs placed all the mass on a single fea-
389
Method Run-Time Iter BLEU (dev) BLEU (test)
new MERT 56m 15 0.0584 0.0743
Perceptron 14m 18 0.0830 0.1163
Table 3: Czech-to-English tuning performance
Method Run-Time BLEU (dev) BLEU (test) length ratio
new MERT 48h 0.1821 0.1633 0.942
Perceptron 1 25h 0.1675 0.1547 0.833
Perceptron 2 38h 0.1738 0.1597 0.837
Perceptron 3 12h? 0.1705 0.1647 0.939
? truncated run (see text)
Table 4: English-to-Haitian tuning performance (including coordinate ascent)
ture and left all the others at 10?14 or less (dis-
played as 0.000000 in the case of CMERT).
The full tuning runs summarized in Table 4
show that SP is often competitive with MERT
while running more quickly, but still requires fur-
ther analysis to determine the causes of variability
in its performance. One initial conclusion from
examining the logs of the SP runs is that weight
updates are perhaps too conservative when ap-
plied in conjunction with coordinate ascent. While
MERT frequently shifted settings in response to
changes in the non-feature parameters, SP rarely
does so, typically preferring to retain the exist-
ing feature weights as the best setting encountered
during the five iterations performed at each invo-
cation. The ?Perceptron 3? run starting with small
uniform feature weights resulted from the obser-
vation that a first, buggy attempt at integration
reached tuning-set BLEU scores in excess of 0.18
before early termination. The bug in question was
that many of the feature weights were initially read
in from the configuration file as zero rather than
the correct value.
As shown in the rightmost column of Tables 1,
2 and 4, the Perceptron algorithm tends toward
short output, yielding translations which are about
97% as long as the reference translation in French-
English, a mere 85% as long for English-Haitian,
and even shorter than that in two of three Czech
runs. This tendency towards short translations
prompted the inclusion of the ? parameter ?
the French-English output was originally much
shorter, but ? has little effect on Haitian given the
sparse training data. The extremely long output
for CMERT on French-English is due to a large
number of zero weights, including those for length
features.
9 Conclusion and Future Work
Structured perceptrons with passive-aggressive
updates are a viable alternative to the usual MERT
feature-weight tuning, particularly where the num-
ber of features exceeds that which MERT can
reliably handle, or when some of the features
have characteristics which confuse MERT. Struc-
tured perceptrons are also a good alternative where
speed is important, such as in a hybrid tuning
scheme which alternates between (re-)tuning the
log-linear model and performing coordinate ascent
on parameters which do not directly contribute
weight to the log-linear model.
We have thus far implemented two objective
functions which operate on individual sentences
without regard for choices made on other sen-
tences. When the final evaluation metric incorpo-
rates global statistics, however, an objective func-
tion which takes them into account is desirable.
For example, when using BLEU, it makes a big
difference whether individual sentences are both
longer and shorter than the reference or system-
atically shorter than the reference, but these two
cases can not be distinguished by single-sentence
objective functions. Our plan is to implement a
windowed or moving-average version of BLEU as
in (Chiang et al, 2008).
We also plan to further speed up the tuning pro-
cess by parallelizing the decoding of the sentences
in the tuning set. As we have used a semi-batch
update method which leaves the decoder?s weights
unchanged for an entire pass through the tuning
set, there is no data dependency between individ-
ual sentences, allowing them to be decoded in par-
390
allel. The perceptron algorithm itself remains se-
quential, but as it is three orders of magnitude
faster than the decoding, this will have negligible
impact on overall speedup factors until hundreds
of CPUs are used for simultaneous decoding.
References
Abhishek Arun and Phillip Koehn. 2007. Online
Learning Methods for Discriminative Training of
Phrase Based Statistical Machine Translation. In
Proceedings of the Eleventh Machine Translation
Summit (MT Summit XI).
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgements. In
Proceedings of the Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for MT and/or Summa-
rization at the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2005),
June.
Nicola Bertoldi, Barry Haddow, and Jean-Baptiste
Fouet. 2009. Improved Minimum Error Rate Train-
ing in Moses. The Prague Bulletin of Mathematical
Linguistics, pages 1?11, February.
Ralf D. Brown. 1996. Example-Based Machine
Translation in the PANGLOSS System. In Proceed-
ings of the Sixteenth International Conference on
Computational Linguistics, pages 169?174, Copen-
hagen, Denmark. http://www.cs.cmu.edu-
/?ralf/papers.html.
Ralf D. Brown. 2008. Exploiting Document-
Level Context for Data-Driven Machine Trans-
lation. In Proceedings of the Eighth Con-
ference of the Association for Machine Trans-
lation in the Americas (AMTA-2008), Octo-
ber. http://www.amtaweb.org/papers/-
2.02 Brown.pdf.
Carnegie Mellon University. 2010. Public release of
haitian-creole language data, January. http://-
www.speech.cs.cmu.edu/haitian/text.
David Chiang, Yuval Marton, and Philis Resnik. 2008.
Online Large-Margin Training of Syntactic and
Structural Translation Features. In Proceedings of
the Conference on Empirical Methods in Natural
Langauge Processing (EMNLP-2008), pages 224?
233, October.
Michael Collins. 2002. Discriminative Training
Methods for Hidden Markov Models: Theory and
Experiments with Perceptron Algorithms. In Pro-
ceedings of EMNLP-2002. http://people.-
csail.mit.edu/mcollins/papers/-
tagperc.pdf.
Koby Crammer, Ofer Deke, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
Passive-Aggressive Algorithms. The Journal of Ma-
chine Learning Research, 7:551?585, December.
Koby Cranmer and Yoram Singer. 2003. Ultraconser-
vative Online Algorithms for Multiclass Problems.
The Journal of Machine Learning Research, 3:951?
991, March.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the Tenth Machine Translation Summit (MT Summix
X), pages 79?86.
Chin-Yew Lin and Franz Joseph Och. 2004a. Au-
tomatic Evaluation of Machine Translation Qual-
ity using Longest Common Subsequence and Skip-
Bigram Statistics. In Proceedings of ACL-2004.
Chin-Yew Lin and Franz Joseph Och. 2004b. OR-
ANGE: A Method for Evaluating Automatic Evalu-
ation Metrics for Machine Translation. In Proceed-
ings of the 20th International Conference on Com-
putational Linguistics (COLING 2004).
USC Information Sciences Institute Natural Language
Group. 2001. Aligned Hansards of the 36th Par-
liament of Canada, Release 2001-1a. http://-
www.isi.edu/natural-language/-
download/hansard/.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Meeting of the Association for Computa-
tional Linguistics (ACL-2003), Sapporo, Japan, July
6?7.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a Method for Au-
tomatic Evaluation of Machine Translation. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics, July.
http://acl.ldc.upenn.edu/P/P02/.
Aaron B. Phillips and Ralf D. Brown. 2009. Cunei
Machine Translation Platform: System Descrip-
tion. In Proceedings of the Third Workshop on
Example-Based Machine Translation, Dublin, Ire-
land, November.
Aaron B. Phillips. 2010. The Cunei Machine Trans-
lation Platform for WMT?10. In Proceedings of the
ACL 2010 Joint Fifth Workshop on Statistical Ma-
chine Translation and Metrics MATR, July.
F. Rosenblatt. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in
the Brain. Psychological Review, 65:386?408.
391
