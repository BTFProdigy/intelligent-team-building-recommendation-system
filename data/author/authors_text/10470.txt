Proceedings of ACL-08: HLT, pages 183?191,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Hypertagging: Supertagging for Surface Realization with CCG
Dominic Espinosa and Michael White and Dennis Mehay
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{espinosa,mwhite,mehay}@ling.osu.edu
Abstract
In lexicalized grammatical formalisms, it is
possible to separate lexical category assign-
ment from the combinatory processes that
make use of such categories, such as pars-
ing and realization. We adapt techniques
from supertagging ? a relatively recent tech-
nique that performs complex lexical tagging
before full parsing (Bangalore and Joshi,
1999; Clark, 2002) ? for chart realization
in OpenCCG, an open-source NLP toolkit for
CCG. We call this approach hypertagging, as
it operates at a level ?above? the syntax, tag-
ging semantic representations with syntactic
lexical categories. Our results demonstrate
that a hypertagger-informed chart realizer can
achieve substantial improvements in realiza-
tion speed (being approximately twice as fast)
with superior realization quality.
1 Introduction
In lexicalized grammatical formalisms such as Lex-
icalized Tree Adjoining Grammar (Schabes et al,
1988, LTAG), Combinatory Categorial Grammar
(Steedman, 2000, CCG) and Head-Driven Phrase-
Structure Grammar (Pollard and Sag, 1994, HPSG),
it is possible to separate lexical category assign-
ment ? the assignment of informative syntactic cat-
egories to linguistic objects such as words or lex-
ical predicates ? from the combinatory processes
that make use of such categories ? such as pars-
ing and surface realization. One way of performing
lexical assignment is simply to hypothesize all pos-
sible lexical categories and then search for the best
combination thereof, as in the CCG parser in (Hock-
enmaier, 2003) or the chart realizer in (Carroll and
Oepen, 2005). A relatively recent technique for lex-
ical category assignment is supertagging (Bangalore
and Joshi, 1999), a preprocessing step to parsing that
assigns likely categories based on word and part-of-
speech (POS) contextual information. Supertagging
was dubbed ?almost parsing? by these authors, be-
cause an oracle supertagger left relatively little work
for their parser, while speeding up parse times con-
siderably. Supertagging has been more recently ex-
tended to a multitagging paradigm in CCG (Clark,
2002; Curran et al, 2006), leading to extremely ef-
ficient parsing with state-of-the-art dependency re-
covery (Clark and Curran, 2007).
We have adapted this multitagging approach to
lexical category assignment for realization using the
CCG-based natural language toolkit OpenCCG.1 In-
stead of basing category assignment on linear word
and POS context, however, we predict lexical cat-
egories based on contexts within a directed graph
structure representing the logical form (LF) of a
proposition to be realized. Assigned categories are
instantiated in OpenCCG?s chart realizer where, to-
gether with a treebank-derived syntactic grammar
(Hockenmaier and Steedman, 2007) and a factored
language model (Bilmes and Kirchhoff, 2003), they
constrain the English word-strings that are chosen to
express the LF. We have dubbed this approach hy-
pertagging, as it operates at a level ?above? the syn-
tax, moving from semantic representations to syn-
tactic categories.
We evaluate this hypertagger in two ways: first,
1http://openccg.sourceforge.net.
183
we evaluate it as a tagger, where the hypertagger
achieves high single-best (93.6%) and multitagging
labelling accuracies (95.8?99.4% with category per
lexical predication ratios ranging from 1.1 to 3.9).2
Second, we compare a hypertagger-augmented ver-
sion of OpenCCG?s chart realizer with the pre-
existing chart realizer (White et al, 2007) that sim-
ply instantiates the chart with all possible CCG cat-
egories (subject to frequency cutoffs) for each in-
put LF predicate. The hypertagger-seeded realizer
runs approximately twice as fast as the pre-existing
OpenCCG realizer and finds a larger number of
complete realizations, resorting less to chart frag-
ment assembly in order to produce an output within
a 15 second per-sentence time limit. Moreover, the
overall BLEU (Papineni et al, 2002) and METEOR
(Lavie and Agarwal, 2007) scores, as well as num-
bers of exact string matches (as measured against to
the original sentences in the CCGbank) are higher
for the hypertagger-seeded realizer than for the pre-
existing realizer.
This paper is structured as follows: Section 2 pro-
vides background on chart realization in OpenCCG
using a corpus-derived grammar. Section 3 de-
scribes our hypertagging approach and how it is in-
tegrated into the realizer. Section 4 describes our
results, followed by related work in Section 5 and
our conclusions in Section 6.
2 Background
2.1 Surface Realization with OpenCCG
The OpenCCG surface realizer is based on Steed-
man?s (2000) version of CCG elaborated with
Baldridge and Kruijff?s multi-modal extensions for
lexically specified derivation control (Baldridge,
2002; Baldridge and Kruijff, 2003) and hybrid
logic dependency semantics (Baldridge and Kruijff,
2002). OpenCCG implements a symbolic-statistical
chart realization algorithm (Kay, 1996; Carroll et al,
1999; White, 2006b) combining (1) a theoretically
grounded approach to syntax and semantic composi-
tion with (2) factored language models (Bilmes and
Kirchhoff, 2003) for making choices among the op-
tions left open by the grammar.
In OpenCCG, the search for complete realizations
2Note that the multitagger is ?correct? if the correct tag is
anywhere in the multitag set.
he h2
aa1
heh3
<Det>
<Arg0> <Arg1>
<TENSE>pres
<NUM>sg
<Arg0>
w1 want.01
m1
<Arg1>
<GenRel>
<Arg1>
<TENSE>pres
p1point
h1have.03
make.03
Figure 1: Semantic dependency graph from the CCGbank
for He has a point he wants to make [. . . ]
makes use of n-gram language models over words
represented as vectors of factors, including surface
form, part of speech, supertag and semantic class.
The search proceeds in one of two modes, anytime
or two-stage (packing/unpacking). In the anytime
mode, a best-first search is performed with a con-
figurable time limit: the scores assigned by the n-
gram model determine the order of the edges on
the agenda, and thus have an impact on realization
speed. In the two-stage mode, a packed forest of
all possible realizations is created in the first stage;
in the second stage, the packed representation is un-
packed in bottom-up fashion, with scores assigned
to the edge for each sign as it is unpacked, much
as in (Langkilde, 2000). Edges are grouped into
equivalence classes when they have the same syn-
tactic category and cover the same parts of the in-
put logical form. Pruning takes place within equiv-
alence classes of edges. Additionally, to realize a
wide range of paraphrases, OpenCCG implements
an algorithm for efficiently generating from disjunc-
tive logical forms (White, 2006a).
To illustrate the input to OpenCCG, consider the
semantic dependency graph in Figure 1, which is
taken from section 00 of a Propbank-enhanced ver-
sion of the CCGbank (Boxwell and White, 2008).
In the graph, each node has a lexical predica-
tion (e.g. make.03) and a set of semantic features
(e.g. ?NUM?sg); nodes are connected via depen-
dency relations (e.g. ?ARG0?). Internally, such
184
graphs are represented using Hybrid Logic Depen-
dency Semantics (HLDS), a dependency-based ap-
proach to representing linguistic meaning developed
by Baldridge and Kruijff (2002). In HLDS, hy-
brid logic (Blackburn, 2000) terms are used to de-
scribe dependency graphs. These graphs have been
suggested as representations for discourse structure,
and have their own underlying semantics (White,
2006b).
To more robustly support broad coverage surface
realization, OpenCCG has recently been enhanced
to greedily assemble fragments in the event that the
realizer fails to find a complete realization. The frag-
ment assembly algorithm begins with the edge for
the best partial realization, i.e. the one that covers
the most elementary predications in the input logi-
cal form, with ties broken according to the n-gram
score. (Larger fragments are preferred under the
assumption that they are more likely to be gram-
matical.) Next, the chart and agenda are greedily
searched for the best edge whose semantic coverage
is disjoint from those selected so far; this process re-
peats until no further edges can be added to the set
of selected fragments. In the final step, these frag-
ments are concatenated, again in a greedy fashion,
this time according to the n-gram score of the con-
catenated edges: starting with the original best edge,
the fragment whose concatenation on the left or right
side yields the highest score is chosen as the one to
concatenate next, until all the fragments have been
concatenated into a single output.
2.2 Realization from an Enhanced CCGbank
White et al (2007) describe an ongoing effort to en-
gineer a grammar from the CCGbank (Hockenmaier
and Steedman, 2007) ? a corpus of CCG deriva-
tions derived from the Penn Treebank ? suitable for
realization with OpenCCG. This process involves
converting the corpus to reflect more precise anal-
yses, where feasible, and adding semantic represen-
tations to the lexical categories. In the first step, the
derivations in the CCGbank are revised to reflect the
desired syntactic derivations. Changes to the deriva-
tions are necessary to reflect the lexicalized treat-
ment of coordination and punctuation assumed by
the multi-modal version of CCG that is implemented
in OpenCCG. Further changes are necessary to sup-
port semantic dependencies rather than surface syn-
tactic ones; in particular, the features and unifica-
tion constraints in the categories related to semanti-
cally empty function words such complementizers,
infinitival-to, expletive subjects, and case-marking
prepositions are adjusted to reflect their purely syn-
tactic status.
In the second step, a grammar is extracted from
the converted CCGbank and augmented with logi-
cal forms. Categories and unary type changing rules
(corresponding to zero morphemes) are sorted by
frequency and extracted if they meet the specified
frequency thresholds.
A separate transformation then uses around two
dozen generalized templates to add logical forms
to the categories, in a fashion reminiscent of (Bos,
2005). The effect of this transformation is illustrated
below. Example (1) shows how numbered seman-
tic roles, taken from PropBank (Palmer et al, 2005)
when available, are added to the category of an ac-
tive voice, past tense transitive verb, where *pred*
is a placeholder for the lexical predicate; examples
(2) and (3) show how more specific relations are in-
troduced in the category for determiners and the cat-
egory for the possessive ?s, respectively.
(1) s1 :dcl\np2/np3 =?
s1 :dcl,x1\np2 :x2/np3 :x3 : @x1(*pred* ?
?TENSE?pres ? ?ARG0?x2 ? ?ARG1?x3)
(2) np1/n1 =?
np1 :x1/n1 :x1 : @x1(?DET?(d ? *pred*))
(3) np1/n1\np2 =?
np1 :x1/n1 :x1\np2 :x2 : @x1(?GENOWN?x2)
After logical form insertion, the extracted and
augmented grammar is loaded and used to parse the
sentences in the CCGbank according to the gold-
standard derivation. If the derivation can be success-
fully followed, the parse yields a logical form which
is saved along with the corpus sentence in order to
later test the realizer. The algorithm for following
corpus derivations attempts to continue processing if
it encounters a blocked derivation due to sentence-
internal punctuation. While punctuation has been
partially reanalyzed to use lexical categories, many
problem cases remain due to the CCGbank?s re-
liance on punctuation-specific binary rules that are
not supported in OpenCCG.
185
Currently, the algorithm succeeds in creating log-
ical forms for 97.7% of the sentences in the devel-
opment section (Sect. 00) of the converted CCG-
bank, and 96.1% of the sentences in the test section
(Sect. 23). Of these, 76.6% of the development log-
ical forms are semantic dependency graphs with a
single root, while 76.7% of the test logical forms
have a single root. The remaining cases, with multi-
ple roots, are missing one or more dependencies re-
quired to form a fully connected graph. These miss-
ing dependencies usually reflect inadequacies in the
current logical form templates.
2.3 Factored Language Models
Following White et al (2007), we use factored tri-
gram models over words, part-of-speech tags and
supertags to score partial and complete realiza-
tions. The language models were created using the
SRILM toolkit (Stolcke, 2002) on the standard train-
ing sections (2?21) of the CCGbank, with sentence-
initial words (other than proper names) uncapital-
ized. While these models are considerably smaller
than the ones used in (Langkilde-Geary, 2002; Vell-
dal and Oepen, 2005), the training data does have
the advantage of being in the same domain and
genre (using larger n-gram models remains for fu-
ture investigation). The models employ interpolated
Kneser-Ney smoothing with the default frequency
cutoffs. The best performing model interpolates a
word trigrammodel with a trigrammodel that chains
a POS model with a supertag model, where the POS
model conditions on the previous two POS tags, and
the supertag model conditions on the previous two
POS tags as well as the current one.
Note that the use of supertags in the factored lan-
guage model to score possible realizations is distinct
from the prediction of supertags for lexical category
assignment: the former takes the words in the local
context into account (as in supertagging for parsing),
while the latter takes features of the logical form into
account. It is this latter process which we call hyper-
tagging, and to which we now turn.
3 The Approach
3.1 Lexical Smoothing and Search Errors
In White et al?s (2007) initial investigation of scal-
ing up OpenCCG for broad coverage realization,
test set grammar complete
oracle / best
dev (00) dev 49.1% / 47.8%
train 37.5% / 22.6%
Table 1: Percentage of complete realizations using an or-
acle n-gram model versus the best performing factored
language model.
all categories observed more often than a thresh-
old frequency were instantiated for lexical predi-
cates; for unseen words, a simple smoothing strategy
based on the part of speech was employed, assign-
ing the most frequent categories for the POS. This
approach turned out to suffer from a large number
of search errors, where the realizer failed to find a
complete realization before timing out even in cases
where the grammar supported one. To confirm that
search errors had become a significant issue, White
et al compared the percentage of complete realiza-
tions (versus fragmentary ones) with their top scor-
ing model against an oracle model that uses a simpli-
fied BLEU score based on the target string, which is
useful for regression testing as it guides the best-first
search to the reference sentence. The comparison
involved both a medium-sized (non-blind) grammar
derived from the development section and a large
grammar derived from the training sections (the lat-
ter with slightly higher thresholds). As shown in
Table 1, with the large grammar derived from the
training sections, many fewer complete realizations
are found (before timing out) using the factored lan-
guage model than are possible, as indicated by the
results of using the oracle model. By contrast, the
difference is small with the medium-sized grammar
derived from the development section. This result is
not surprising when one considers that a large num-
ber of common words are observed to have many
possible categories.
In the next section, we show that a supertag-
ger for CCG realization, or hypertagger, can reduce
the problem of search errors by focusing the search
space on the most likely lexical categories.
3.2 Maximum Entropy Hypertagging
As supertagging for parsing involves studying a
given input word and its local context, the concep-
186
tual equivalent for a lexical predicate in the LF is to
study a given node and its local graph structure. Our
implementation makes use of three general types of
features: lexicalized features, which are simply the
names of the parent and child elementary predica-
tion nodes, graph structural features, such as the
total number of edges emanating from a node, the
number of argument and non-argument dependents,
and the names of the relations of the dependent
nodes to the parent node, and syntactico-semantic
attributes of nodes, such as the tense and number.
For example, in the HLDS graph shown in Figure 1,
the node representing want has two dependents, and
the relational type of make with respect to want is
ARG1.
Clark (2002) notes in his parsing experiments that
the POS tags of the surrounding words are highly in-
formative. As discussed below, a significant gain in
hypertagging accuracy resulted from including fea-
tures sensitive to the POS tags of a node?s parent, the
node itself, and all of its arguments and modifiers.
Predicting these tags requires the use of a separate
POS tagger, which operates in a manner similar to
the hypertagger itself, though exploiting a slightly
different set of features (e.g., including features cor-
responding to the four-character prefixes and suf-
fixes of rare logical predication names). Follow-
ing the (word) supertagging experiments of (Cur-
ran et al, 2006) we assigned potentially multiple
POS tags to each elementary predication. The POS
tags assigned are all those that are some factor ?
of the highest ranked tag,3 giving an average of 1.1
POS tags per elementary predication. The values of
the corresponding feature functions are the POS tag
probabilities according to the POS tagger. At this
ambiguity level, the POS tagger is correct ? 92% of
the time.
Features for the hypertagger were extracted from
semantic dependency graphs extracted from sections
2 through 21 of the CCGbank. In total, 37,168
dependency graphs were derived from the corpus,
yielding 468,628 feature parameters.
The resulting contextual features and gold-
standard supertag for each predication were then
used to train a maximum entropy classifier model.
3I.e., all tags t whose probabilities p(t) ? ? ? p?, where p?
is the highest ranked tag?s probability.
Maximum entropy models describe a set of proba-
bility distributions of the form:
p(o | x) =
1
Z(x)
? exp
( n?
i=1
?ifi(o, x)
)
where o is an outcome, x is a context, the fi are
feature functions, the ?i are the respective weights
of the feature functions, and Z(x) is a normalizing
sum over all competing outcomes. More concretely,
given an elementary predication labeled want (as in
Figure 1), a feature function over this node could be:
f(o, x) =
{ 1, if o is (s[dcl]\np)/(s[adj]\np) and
number of LF dependents(x) = 2
0, otherwise.
We used Zhang Le?s maximum entropy toolkit4
for training the hypertagging model, which uses an
implementation of Limited-memory BFGS, an ap-
proximate quasi-Newton optimization method from
the numerical optimization literature (Liu and No-
cedal, 1989). Using L-BFGS allowed us to include
continuous feature function values where appropri-
ate (e.g., the probabilities of automatically-assigned
POS tags). We trained each hypertagging model to
275 iterations and our POS tagging model to 400 it-
erations. We used no feature frequency cut-offs, but
rather employed Gaussian priors with global vari-
ances of 100 and 75, respectively, for the hypertag-
ging and POS tagging models.
3.3 Iterative ?-Best Realization
During realization, the hypertagger serves to prob-
abilistically filter the categories assigned to an ele-
mentary predication, as well as to propose categories
for rare or unseen predicates. Given a predication,
the tagger returns a ?-best list of supertags in order
of decreasing probability. Increasing the number of
categories returned clearly increases the likelihood
that the most-correct supertag is among them, but at
a corresponding cost in chart size. Accordingly, the
hypertagger begins with a highly restrictive value for
?, and backs off to progressively less-restrictive val-
ues if no complete realization could be found using
the set of supertags returned. The search is restarted
4http://homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html.
187
Table 2: Hypertagger accuracy on Sections 00 and 23.
Results (in percentages) are for per-logical-predication
(PR) and per-whole-graph (GRPH) tagging accurcies.
Difference between best-only and baselines (b.l.) is sig-
nificant (p < 2 ? 10?16) by McNemar?s ?2 test.
Sect00 Sect23
? TagsPred PR GRPH PR GRPH
b.l. 1 1 68.7 1.8 68.7 2.3
b.l. 2 2 84.3 9.9 84.4 10.9
1.0 1 93.6 40.4 93.6 38.2
0.16 1.1 95.8 55.7 96.2 56.8
0.05 1.2 96.6 63.8 97.3 66.0
0.0058 1.5 97.9 74.8 98.3 76.9
1.75e-3 1.8 98.4 78.9 98.7 81.8
6.25e-4 2.2 98.7 82.5 99.0 84.3
1.25e-4 3.2 99.0 85.7 99.3 88.5
5.8e-5 3.9 99.1 87.2 99.4 89.9
from scratch with the next ? value, though in prin-
ciple the same chart could be expanded. The iter-
ative, ?-best search for a complete realization uses
the realizer?s packing mode, which can more quickly
determine whether a complete realization is possi-
ble. If the halfway point of the overall time limit
is reached with no complete realization, the search
switches to best-first mode, ultimately assembling
fragments if no complete realization can be found
during the remaining time.
4 Results and Discussion
Several experiments were performed in training and
applying the hypertagger. Three different models
were created using 1) non-lexicalized features only,
2) all features excluding POS tags, 3) all, 3) all
features except syntactico-semantic attributes such
as tense and number and 4) all features available.
Models trained on these feature subsets were tested
against one another on Section 00, and then the best
performing model was run on both Section 00 and
23.
4.1 Feature Ablation Testing
The the whole feature set was found in feature abla-
tion testing on the development set to outperform all
other feature subsets significantly (p < 2.2 ? 10?16).
These results listed in Table 3. As we can see, taking
Table 3: Hypertagger feature ablation testing results on
Section 00. The full feature set outperforms all others sig-
nificantly (p < 2.2 ? 10?16). Results for per-predication
(PR) and per-whole-graph (GRPH) tagging percentage
accuracies are listed. (Key: no-POS=no POS features;
no-attr=no syntactico-semantic attributes such as tense
and number; non-lex=non-lexicalized features only (no
predication names).
FEATURESET PR GRPH
full 93.6 40.37
no-POS 91.3 29.5
no-attr 91.8 31.2
non-lex 91.5 28.7
away any one class of features leads to drop in per-
predication tagging accuracy of at least 1.8% and a
drop per-whole-graph accuracy of at least 9.2%. As
expected from previous work in supertagging (for
parsing), POS features resulted in a large improve-
ment in overall accuracy (1.8%). Although the POS
tagger by itself is only 92% accurate (as a multi-
tagger of 1.1 POSword average ambiguity) ? well be-
low the state-of-the-art for the tagging of words ?
its predictions are still quite valuable to the hyper-
tagger.
4.2 Best Model Hypertagger Accuracy
The results for the full feature set on Sections 00
and 23 are outlined in Table 2. Included in this
table are accuracy data for a baseline dummy tag-
ger which simply assigns the most-frequently-seen
tag(s) for a given predication and backs off to the
overall most frequent tag(s) when confronted with
an unseen predication. The development set (00)
was used to tune the ? parameter to obtain reason-
able hypertag ambiguity levels; the model was not
otherwise tuned to it. The hypertagger achieves high
per-predication and whole-graph accuracies even at
small ambiguity levels.
4.3 Realizer Performance
Tables 4 and 5 show how the hypertagger improves
realization performance on the development and test
sections of the CCGbank. As Table 4 indicates, us-
ing the hypertagger in an iterative beta-best fash-
ion more than doubles the number of grammati-
cally complete realizations found within the time
188
Table 5: Realization quality metrics exact match, BLEU and METEOR, on complete realizations only and overall,
with and without hypertagger, on Sections 00 and 23.
Sec- Hyper- Complete Overall
tion tagger BLEU METEOR Exact BLEU METEOR
00 with 0.8137 0.9153 15.3% 0.6567 0.8494
w/o 0.6864 0.8585 11.3% 0.5902 0.8209
23 with 0.8149 0.9162 16.0% 0.6701 0.8557
w/o 0.6910 0.8606 12.3% 0.6022 0.8273
Table 4: Percentage of grammatically complete realiza-
tions, runtimes for complete realizations and overall run-
times, with and without hypertagger, on Sections 00 and
23.
Sec- Hyper- Percent Complete Overall
tion tagger Complete Time Time
00 with 47.4% 1.2s 4.5s
w/o 22.6% 8.7s 9.5s
23 with 48.5% 1.2s 4.4s
w/o 23.5% 8.9s 9.6s
limit; on the development set, this improvement eli-
mates more than the number of known search errors
(cf. Table 1). Additionally, by reducing the search
space, the hypertagger cuts overall realization times
by more than half, and in the cases where complete
realizations are found, realization times are reduced
by a factor of four, down to 1.2 seconds per sentence
on a desktop Linux PC.
Table 5 shows that increasing the number of com-
plete realizations also yields improved BLEU and
METEOR scores, as well as more exact matches. In
particular, the hypertagger makes possible a more
than 6-point improvement in the overall BLEU score
on both the development and test sections, and a
more than 12-point improvement on the sentences
with complete realizations.
As the effort to engineer a grammar suitable for
realization from the CCGbank proceeds in paral-
lel to our work on hypertagging, we expect the
hypertagger-seeded realizer to continue to improve,
since a more complete and precise extracted gram-
mar should enable more complete realizations to be
found, and richer semantic representations should
simplify the hypertagging task. Even with the cur-
rent incomplete set of semantic templates, the hy-
pertagger brings realizer performance roughly up to
state-of-the-art levels, as our overall test set BLEU
score (0.6701) slightly exceeds that of Cahill and
van Genabith (2006), though at a coverage of 96%
instead of 98%. We caution, however, that it remains
unclear how meaningful it is to directly compare
these scores when the realizer inputs vary consider-
ably in their specificity, as Langkilde-Geary?s (2002)
experiments dramatically illustrate.
5 Related Work
Our approach follows Langkilde-Geary (2002) and
Callaway (2003) in aiming to leverage the Penn
Treebank to develop a broad-coverage surface re-
alizer for English. However, while these earlier,
generation-only approaches made use of converters
for transforming the outputs of Treebank parsers to
inputs for realization, our approach instead employs
a shared bidirectional grammar, so that the input to
realization is guaranteed to be the same logical form
constructed by the parser. In this regard, our ap-
proach is more similar to the ones pursued more re-
cently by Carroll, Oepen and Velldal (2005; 2005;
2006), Nakanishi et al (2005) and Cahill and van
Genabith (2006) with HPSG and LFG grammars.
While we consider our approach to be the first to
employ a supertagger for realization, or hypertagger,
the approach is clearly reminiscent of the LTAG tree
models of Srinivas and Rambow (2000). The main
difference between the approaches is that ours con-
sists of a multitagging step followed by the bottom-
up construction of a realization chart, while theirs
involves the top-down selection of the single most
likely supertag for each node that is grammatically
189
compatible with the parent node, with the proba-
bility conditioned only on the child nodes. Note
that although their approach does involve a subse-
quent lattice construction step, it requires making
non-standard assumptions about the TAG; in con-
trast, ours follows the chart realization tradition of
working with the same operations of grammatical
combination as in parsing, including a well-defined
notion of semantic composition. Additionally, as
our tagger employs maximum entropy modeling, it
is able to take into account a greater variety of con-
textual features, including those derived from parent
nodes.
In comparison to other recent chart realization ap-
proaches, Nakanishi et al?s is similar to ours in that
it employs an iterative beam search, dynamically
changing the beam size in order to cope with the
large search space. However, their log-linear selec-
tion models have been adapted from ones used in
parsing, and do not condition choices based on fea-
tures of the input semantics to the same extent. In
particular, while they employ a baseline maximum
likelihood model that conditions the probability of
a lexical entry upon its predicate argument struc-
ture (PAS) ? that is, the set of elementary predi-
cations introduced by the lexical item ? this prob-
ability does not take into account other elements of
the local context, including parents and modifiers,
and their lexical predicates. Similarly, Cahill and
van Genabith condition the probability of their lex-
ical rules on the set of feature-value pairs linked to
the RHS of the rule, but do not take into account any
additional context. Since their probabilistic mod-
els involve independence assumptions like those in
a PCFG, and since they do not employ n-grams for
scoring alternative realizations, their approach only
keeps the single most likely edge in an equivalence
class, rather than packing them into a forest. Car-
roll, Oepen and Velldal?s approach is like Nakanishi
et al?s in that they adapt log-linear parsing models
to the realization task; however, they employ manu-
ally written grammars on much smaller corpora, and
perhaps for this reason they have not faced the need
to employ an iterative beam search.
6 Conclusion
We have introduced a novel type of supertagger,
which we have dubbed a hypertagger, that assigns
CCG category labels to elementary predications in
a structured semantic representation with high accu-
racy at several levels of tagging ambiguity in a fash-
ion reminiscent of (Bangalore and Rambow, 2000).
To our knowledge, we are the first to report tag-
ging results in the semantic-to-syntactic direction.
We have also shown that, by integrating this hy-
pertagger with a broad-coverage CCG chart real-
izer, considerably faster realization times are possi-
ble (approximately twice as fast as compared with
a realizer that performs simple lexical look-ups)
with higher BLEU, METEOR and exact string match
scores. Moreover, the hypertagger-augmented real-
izer finds more than twice the number of complete
realizations, and further analysis revealed that the
realization quality (as per modified BLEU and ME-
TEOR) is higher in the cases when the realizer finds
a complete realization. This suggests that further
improvements to the hypertagger will lead to more
complete realizations, hence more high-quality re-
alizations. Finally, further efforts to engineer a
grammar suitable for realization from the CCGbank
should provide richer feature sets, which, as our fea-
ture ablation study suggests, are useful for boosting
hypertagging performance, hence for finding better
and more complete realizations.
Acknowledgements
The authors thank the anonymous reviewers, Chris
Brew, Detmar Meurers and Eric Fosler-Lussier for
helpful comments and discussion.
References
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling
CCG and Hybrid Logic Dependency Semantics. In
Proc. ACL-02.
Jason Baldridge and Geert-Jan Kruijff. 2003. Multi-
Modal Combinatory Categorial Grammar. In Proc.
ACL-03.
Jason Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, School of Informatics, University of Edinburgh.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
190
pertagging: An Approach to Almost Parsing. Com-
putational Linguistics, 25(2):237?265.
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proce. COLING-00.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and general parallelized backoff. In
Proc. HLT-03.
Patrick Blackburn. 2000. Representation, reasoning, and
relational structures: a hybrid logic manifesto. Logic
Journal of the IGPL, 8(3):339?625.
Johan Bos. 2005. Towards wide-coverage semantic in-
terpretation. In Proc. IWCS-6.
Stephen Boxwell and Michael White. 2008. Projecting
Propbank roles onto the CCGbank. In Proc. LREC-08.
To appear.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
LFG approximations. In Proc. COLING-ACL ?06.
Charles Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proc. IJCAI-03.
John Carroll and Stefan Oepen. 2005. High efficiency
realization for a wide-coverage unification grammar.
In Proc. IJCNLP-05.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznan?ski. 1999. An efficient chart generator for
(semi-) lexicalist grammars. In Proc. ENLG-99.
Stephen Clark and James Curran. 2007. Wide-coverage
efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4).
Stephen Clark. 2002. Supertagging for combinatory
categorial grammar. In Proceedings of the 6th Inter-
national Workshop on Tree Adjoining Grammars and
Related Frameworks (TAG+6), pages 19?24, Venice,
Italy.
James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar pars-
ing. In Proceedings of the Joint Conference of the
International Committee on Computational Linguis-
tics and the Association for Computational Linguis-
tics (COLING/ACL-06), pages 697?704, Sydney, Aus-
tralia.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2003. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh, Edin-
burgh, Scotland.
Martin Kay. 1996. Chart generation. In Proc. ACL-96.
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG-02.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proc. NAACL-00.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels
of correlation with human judgments. In Proceedings
of Workshop on Statistical Machine Translation at the
45th Annual Meeting of the Association of Computa-
tional Linguistics (ACL-2007), Prague.
D C Liu and Jorge Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathematical
Programming B, 45(3).
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proc. IWPT-05.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Philadelphia, PA.
Carl J Pollard and Ivan A Sag. 1994. Head-Driven
Phrase Structure Grammar. University Of Chicago
Press.
Yves Schabes, Anne Abeille?, and Aravind K. Joshi.
1988. Parsing strategies with ?lexicalized? grammars:
Application to tree adjoining grammars. In Proceed-
ings of the 12th International Conference on Compu-
tational Linguistics (COLING-88), Budapest.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, Massachusetts, USA.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proc. ICSLP-02.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proc. MT
Summit X.
Erik Velldal and Stephan Oepen. 2006. Statistical rank-
ing in tactical generation. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, Sydney, Australia, July.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proc. of the Workshop on Using
Corpora for NLG: Language Generation and Machine
Translation (UCNLG+MT).
Michael White. 2006a. CCG chart realization from dis-
junctive inputs. In Proceedings, INLG 2006.
Michael White. 2006b. Efficient realization of coordi-
nate structures in Combinatory Categorial Grammar.
Research on Language and Computation, 4(1):39?75.
191
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 37?45,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Brutus: A Semantic Role Labeling System Incorporating CCG, CFG, and
Dependency Features
Stephen A. Boxwell, Dennis Mehay, and Chris Brew
Department of Linguistics
The Ohio State University
{boxwe11,mehay,cbrew}@1ing.ohio-state.edu
Abstract
We describe a semantic role labeling system
that makes primary use of CCG-based fea-
tures. Most previously developed systems
are CFG-based and make extensive use of a
treepath feature, which suffers from data spar-
sity due to its use of explicit tree configura-
tions. CCG affords ways to augment treepath-
based features to overcome these data sparsity
issues. By adding features over CCG word-
word dependencies and lexicalized verbal sub-
categorization frames (?supertags?), we can
obtain an F-score that is substantially better
than a previous CCG-based SRL system and
competitive with the current state of the art. A
manual error analysis reveals that parser errors
account for many of the errors of our system.
This analysis also suggests that simultaneous
incremental parsing and semantic role labeling
may lead to performance gains in both tasks.
1 Introduction
Semantic Role Labeling (SRL) is the process of assign-
ing semantic roles to strings of words in a sentence ac-
cording to their relationship to the semantic predicates
expressed in the sentence. The task is difficult because
the relationship between syntactic relations like ?sub-
ject? and ?object? do not always correspond to seman-
tic relations like ?agent? and ?patient?. An effective
semantic role labeling system must recognize the dif-
ferences between different configurations:
(a) [The man]Arg0 opened [the door]Arg1 [for
him]Arg3 [today]ArgM?TMP .
(b) [The door]Arg1 opened.
(c) [The door]Arg1 was opened by [a man]Arg0.
We use Propbank (Palmer et al, 2005), a corpus of
newswire text annotated with verb predicate semantic
role information that is widely used in the SRL litera-
ture (Ma`rquez et al, 2008). Rather than describe se-
mantic roles in terms of ?agent? or ?patient?, Propbank
defines semantic roles on a verb-by-verb basis. For ex-
ample, the verb open encodes the OPENER as Arg0, the
OPENEE as Arg1, and the beneficiary of the OPENING
action as Arg3. Propbank also defines a set of adjunct
roles, denoted by the letter M instead of a number. For
example, ArgM-TMP denotes a temporal role, like ?to-
day?. By using verb-specific roles, Propbank avoids
specific claims about parallels between the roles of dif-
ferent verbs.
We follow the approach in (Punyakanok et al, 2008)
in framing the SRL problem as a two-stage pipeline:
identification followed by labeling. During identifica-
tion, every word in the sentence is labeled either as
bearing some (as yet undetermined) semantic role or
not . This is done for each verb. Next, during label-
ing, the precise verb-specific roles for each word are
determined. In contrast to the approach in (Punyakanok
et al, 2008), which tags constituents directly, we tag
headwords and then associate them with a constituent,
as in a previous CCG-based approach (Gildea and
Hockenmaier, 2003). Another difference is our choice
of parsers. Brutus uses the CCG parser of (Clark and
Curran, 2007, henceforth the C&C parser), Charniak?s
parser (Charniak, 2001) for additional CFG-based fea-
tures, and MALT parser (Nivre et al, 2007) for de-
pendency features, while (Punyakanok et al, 2008)
use results from an ensemble of parses from Char-
niak?s Parser and a Collins parser (Collins, 2003; Bikel,
2004). Finally, the system described in (Punyakanok et
al., 2008) uses a joint inference model to resolve dis-
crepancies between multiple automatic parses. We do
not employ a similar strategy due to the differing no-
tions of constituency represented in our parsers (CCG
having a much more fluid notion of constituency and
the MALT parser using a different approach entirely).
For the identification and labeling steps, we train
a maximum entropy classifier (Berger et al, 1996)
over sections 02-21 of a version of the CCGbank cor-
pus (Hockenmaier and Steedman, 2007) that has been
augmented by projecting the Propbank semantic anno-
tations (Boxwell and White, 2008). We evaluate our
SRL system?s argument predictions at the word string
level, making our results directly comparable for each
argument labeling.1
In the following, we briefly introduce the CCG
grammatical formalism and motivate its use in SRL
(Sections 2?3). Our main contribution is to demon-
strate that CCG ? arguably a more expressive and lin-
1This is guaranteed by our string-to-string mapping from
the original Propbank to the CCGbank.
37
guistically appealing syntactic framework than vanilla
CFGs ? is a viable basis for the SRL task. This is sup-
ported by our experimental results, the setup and details
of which we give in Sections 4?10. In particular, using
CCG enables us to map semantic roles directly onto
verbal categories, an innovation of our approach that
leads to performance gains (Section 7). We conclude
with an error analysis (Section 11), which motivates
our discussion of future research for computational se-
mantics with CCG (Section 12).
2 Combinatory Categorial Grammar
Combinatory Categorial Grammar (Steedman, 2000)
is a grammatical framework that describes syntactic
structure in terms of the combinatory potential of the
lexical (word-level) items. Rather than using standard
part-of-speech tags and grammatical rules, CCG en-
codes much of the combinatory potential of each word
by assigning a syntactically informative category. For
example, the verb loves has the category (s\np)/np,
which could be read ?the kind of word that would be
a sentence if it could combine with a noun phrase on
the right and a noun phrase on the left?. Further, CCG
has the advantage of a transparent interface between the
way the words combine and their dependencies with
other words. Word-word dependencies in the CCG-
bank are encoded using predicate-argument (PARG)
relations. PARG relations are defined by the functor
word, the argument word, the category of the functor
word and which argument slot of the functor category
is being filled. For example, in the sentence John loves
Mary (figure 1), there are two slots on the verbal cat-
egory to be filled by NP arguments. The first argu-
ment (the subject) fills slot 1. This can be encoded
as <loves,john,(s\np)/np,1>, indicating the head of
the functor, the head of the argument, the functor cat-
egory and the argument slot. The second argument
(the direct object) fills slot 2. This can be encoded as
<loves,mary,(s\np)/np,2>. One of the potential ad-
vantages to using CCGbank-style PARG relations is
that they uniformly encode both local and long-range
dependencies ? e.g., the noun phrase the Mary that
John loves expresses the same set of two dependencies.
We will show this to be a valuable tool for semantic
role prediction.
3 Potential Advantages to using CCG
There are many potential advantages to using the CCG
formalism in SRL. One is the uniformity with which
CCG can express equivalence classes of local and long-
range (including unbounded) dependencies. CFG-
based approaches often rely on examining potentially
long sequences of categories (or treepaths) between the
verb and the target word. Because there are a number of
different treepaths that correspond to a single relation
(figure 2), this approach can suffer from data sparsity.
CCG, however, can encode all treepath-distinct expres-
sions of a single grammatical relation into a single
predicate-argument relationship (figure 3). This fea-
ture has been shown (Gildea and Hockenmaier, 2003)
to be an effective substitute for treepath-based features.
But while predicate-argument-based features are very
effective, they are still vulnerable both to parser er-
rors and to cases where the semantics of a sentence
do not correspond directly to syntactic dependencies.
To counteract this, we use both kinds of features with
the expectation that the treepath feature will provide
low-level detail to compensate for missed, incorrect or
syntactically impossible dependencies.
Another advantage of a CCG-based approach (and
lexicalist approaches in general) is the ability to en-
code verb-specific argument mappings. An argument
mapping is a link between the CCG category and the
semantic roles that are likely to go with each of its ar-
guments. The projection of argument mappings onto
CCG verbal categories is explored in (Boxwell and
White, 2008). We describe this feature in more detail
in section 7.
4 Identification and Labeling Models
As in previous approaches to SRL, Brutus uses a two-
stage pipeline of maximum entropy classifiers. In ad-
dition, we train an argument mapping classifier (de-
scribed in more detail below) whose predictions are
used as features for the labeling model. The same
features are extracted for both treebank and automatic
parses. Automatic parses were generated using the
C&C CCG parser (Clark and Curran, 2007) with its
derivation output format converted to resemble that of
the CCGbank. This involved following the derivational
bracketings of the C&C parser?s output and recon-
structing the backpointers to the lexical heads using an
in-house implementation of the basic CCG combina-
tory operations. All classifiers were trained to 500 iter-
ations of L-BFGS training ? a quasi-Newton method
from the numerical optimization literature (Liu and No-
cedal, 1989) ? using Zhang Le?s maxent toolkit.2 To
prevent overfitting we used Gaussian priors with global
variances of 1 and 5 for the identifier and labeler, re-
spectively.3 The Gaussian priors were determined em-
pirically by testing on the development set.
Both the identifier and the labeler use the following
features:
(1) Words. Words drawn from a 3 word window
around the target word,4 with each word asso-
ciated with a binary indicator feature.
(2) Part of Speech. Part of Speech tags drawn
from a 3 word window around the target word,
2Available for download at http://homepages.
inf.ed.ac.uk/s0450736/maxent_toolkit.
html.
3Gaussian priors achieve a smoothing effect (to prevent
overfitting) by penalizing very large feature weights.
4The size of the window was determined experimentally
on the development set ? we use the same window sizes
throughout.
38
John loves Mary
np (s[dcl]\np)/np np
>
s[dcl]\np
<
s[dcl]
Figure 1: This sentence has two depen-
dencies: <loves,mary,(s\np)/np,2> and
<loves,john,(s\np)/np,1>
Saaa!!!
NP
Robin
VPbb""
V
fixed
NP
@ 
Det
the
N
car
NPHHH
Det
the
NHHH
N
car
RCHHH
Rel
that
S
ZZProceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Refining the most frequent sense baseline
Judita Preiss
Department of Linguistics
The Ohio State University
judita@ling.ohio-state.edu
Jon Dehdari
Department of Linguistics
The Ohio State University
jonsafari@ling.ohio-state.edu
Josh King
Computer Science and Engineering
The Ohio State University
kingjo@cse.ohio-state.edu
Dennis Mehay
Department of Linguistics
The Ohio State University
mehay@ling.ohio-state.edu
Abstract
We refine the most frequent sense baseline
for word sense disambiguation using a num-
ber of novel word sense disambiguation tech-
niques. Evaluating on the S???????-3 English
all words task, our combined system focuses
on improving every stage of word sense dis-
ambiguation: starting with the lemmatization
and part of speech tags used, through the ac-
curacy of the most frequent sense baseline, to
highly targeted individual systems. Our super-
vised systems include a ranking algorithm and
a Wikipedia similarity measure.
1 Introduction
The difficulty of outperforming the most frequent
sense baseline, the assignment of the sense which
appears most often in a given annotated corpus, in
word sense disambiguation (WSD) has been brought
to light by the recent S??????? WSD system evalu-
ation exercises. In this work, we present a combi-
nation system, which, rather than designing a single
approach to all words, enriches the most frequent
sense baseline when there is high confidence for an
alternative sense to be chosen.
WSD, the task of assigning a sense to a given
word from a sense inventory is clearly necessary
for other natural language processing tasks. For ex-
ample, when performing machine translation, it is
necessary to distinguish between word senses in the
original language if the different senses have differ-
ent possible translations in the target language (Yn-
gve, 1955). A number of different approaches to
WSD have been explored in recent years, with two
distinct approaches: techniques which require anno-
tated training data (supervised techniques) and tech-
niques which do not (unsupervised methods).
It has long been believed that supervised systems,
which can be tuned to a word?s context, greatly out-
perform unsupervised systems. This theory was sup-
ported in the S??????? WSD system evaluation exer-
cises, where the performance gap between the best
supervised system and the best unsupervised sys-
tem is large. Unsupervised systems were found to
never outperform the most frequent sense (MFS)
baseline (a sense assignment made on the basis of
the most frequent sense in an annotated corpus),
while supervised systems occasionally perform bet-
ter than the MFS baseline, though rarely by more
than 5%. However, recent work by McCarthy et al
(2007) shows that acquiring a predominant sense
from an unannotated corpus can outperform many
supervised systems, and under certain conditions
will also outperform the MFS baseline.
Rather than proposing a new algorithm which will
tackle all words, we focus on improving upon the
MFS baseline system when an alternative system
proposes a high confidence answer. An MFS refin-
ing system can therefore benefit from answers sug-
gested by a very low recall (but high precision) WSD
system. We propose a number of novel approaches
to WSD, but also demonstrate the importance of a
highly accurate lemmatizer and part of speech tag-
ger to the English all words task of S???????-3.1
We present our enriched most frequent sense
1Unless specified otherwise, we use WordNet 1.7.1 (Miller
et al, 1990) and the associated sense annotated SemCor cor-
pus (Miller et al, 1993) (translated to WordNet 1.7.1 by Rada
Mihalcea).
10
baseline in Section 2, which motivates the lemma-
tizer and part of speech tagger refinements presented
in Section 3. Our novel high precision WSD al-
gorithms include a reranking algorithm (Section 4),
and a Wikipedia-based similarity measure (Sec-
tion 5). The individual systems are combined in
Section 6, and we close with our conclusions in Sec-
tion 7.
2 Most frequent sense baseline
The most frequent sense (MFS) baseline assumes
a sense annotated corpus from which the frequen-
cies of individual senses are learnt. For each tar-
get word, a part of speech tagger is used to deter-
mine the word?s part of speech, and the MFS for
that part of speech is selected. Although this is a
fairly naive baseline, it has been shown to be diffi-
cult to beat, with only 5 systems of the 26 submitted
to the S???????-3 English all words task outperform-
ing the reported 62.5% MFS baseline. The success
of the MFS baseline is mainly due to the frequency
distribution of senses, with the shape of the sense
rank versus frequency graph being a Zipfian curve
(i.e., the top-ranked sense being much more likely
than any other sense).
However, two different MFS baseline perfor-
mance results are reported in Snyder and Palmer
(2004), with further implementations being differ-
ent still. The differences in performance of the MFS
baseline can be attributed to a number of factors:
the English all words task is run on natural text and
therefore performance greatly depends on the accu-
racy of the lemmatizer and the part of speech tag-
ger employed.2 If the lemmatizer incorrectly iden-
tifies the stem of the word, the MFS will be looked
up for the wrong word and the resulting sense as-
signment will be incorrect. The performance of the
MFS given the correct lemma and part of speech
information is 66%, while the performance of the
MFS with a Port Stemmer without any POS infor-
mation is 32%. With a TreeTagger (Schmidt, 1994),
and a sophisticated lemma back-off strategy, the per-
formance increases to 56%. It is this difference in
2Other possible factors include: 1) The sense distribution in
the corpus which the MFS baseline is drawn from, 2) If SemCor
is used as the underlying sense annotated corpus, the accuracy
of the mapping from WordNet 1.6 (with which SemCor was
initially annotated) to WordNet 1.7.1 could also have an effect
on the performance).
performance which motivates refining the most fre-
quent sense baseline, and our work on improving
the underlying lemmatizer and part of speech tagger
presented in Section 3.
Our initial investigation refines the SemCor based
MFS baseline using the automatic method of de-
termining the predominant sense presented in Mc-
Carthy et al (2007).
1. For nouns and adjectives which appear in Sem-
Cor fewer than 5 times, we employ the auto-
matically determined predominant sense.
2. For verbs which appear in SemCor fewer than 5
times, we employ subcategorization frame sim-
ilarity rather than Lesk similarity to give us a
verb?s predominant sense.
2.1 Predominant sense
McCarthy et al (2007) demonstrate that it is possi-
ble to acquire the predominant sense for a word in
a corpus without having access to annotated data.
They employ an automatically created thesaurus
(Lin, 1998), and a sense?word similarity metric to
assign to each sense si of a word w a score corre-
sponding to
?
n j?Nw
dss(w, n j) ? sss(si, n j)?
s?i?senses(w) sss(s?i , n j)
where dss(w, n j) reflects the distributional simi-
larity of word w to n j, w?s thesaural neighbour, and
sss(si, n j) = maxsx?senses(n j) sss?(si, sx) is the max-
imum similarity3 between w?s sense si and a sense
sx of w?s thesaural neighbour n j. The authors show
that although this method does not always outper-
form the MFS baseline based on SemCor, it does
outperform it when the word?s SemCor frequency is
below 5. We therefore switch our MFS baseline to
this value for such words. This result is represented
as ?McCarthy? in Table 1, which contains the results
of the techniques presented in this Section evaluated
on the S???????-3 English all words task.
2.2 Verb predominant sense
McCarthy et al (2007) observe that their predom-
inant sense method is not performing as well for
3We use the Lesk (overlap) similarity as implemented by the
WordNet::similarity package (Pedersen et al, 2004).
11
System Precision Recall F-measure
MFS 58.4% 58.4% 58.4%
McCarthy 58.5% 58.5% 58.5%
Verbs 58.5% 58.5% 58.5%
All 58.6% 58.6% 58.6%
Table 1: Refining the MFS baseline with predominant
sense
verbs as it does for nouns and adjectives. We hy-
pothesize that this is due to the thesaural neighbours
obtained from Lin?s thesaurus, and we group verbs
according to the subcategorization frame (SCF) dis-
tributions they present in the ????? (Korhonen et al,
2006) lexicon. A word w1 is grouped with word w2
if the Bhattacharyya coefficient
BC(w1, w2) =
?
x?X
?p(x)q(x)
where p(x) and q(x) represent the probability val-
ues for subcategorization class x, is above a cer-
tain threshold. The BC coefficient then replaces the
dss value in the original formula and the predomi-
nant senses are obtained. Again, this system is only
used for words with frequency lower than 5 in Sem-
Cor. The great advantage of the Bhattacharyya co-
efficient over various entropy based similarity mea-
sures which are usually used to compare SCF distri-
butions (Korhonen and Krymolowski, 2002), is that
it is guaranteed to lie between 0 and 1, unlike the
entropy based measures which are not easily com-
parable between different word pairs. This result is
represented by ?Verbs? in Table 1.
Table 1 displays the results for the MFS, the MFS
combined with the two approaches described above,
and the MFS combining MFS with verbs and Mc-
Carthy.
3 Lemmatization and Part of Speech
Tagging
We made use of several lemmatizers and part-of-
speech taggers, in order to give the other WSD com-
ponents the best starting point possible.
3.1 Lemmatization
Lemmatization, the process of obtaining the canon-
ical form of a word, was the first step for us to
ultimately identify the correct WordNet sense of
a given word in the English all words task. We
found that without any lemmatizing of the test input,
the maximum f -score possible was in the mid-50?s.
Conversely, we found that a basic most-frequent-
sense system that had a perfectly-lemmatized input
achieved an f -score in the mid-60?s. This large dif-
ference in the ceiling of a non-lemmatized system
and the floor of a perfectly-lemmatized system mo-
tivated us to focus on this task.
We looked at three different lemmatizers: the lem-
matizing backend of the XTAG project (XTAG Re-
search Group, 2001)4, Celex (Baayen et al, 1995),
and the lemmatizing component of an enhanced
TBL tagger (Brill, 1992).5 We then employed a vot-
ing system on these three components, taking the
lemma from the most individual lemmatizers. If all
three differ, we take the lemma from the most accu-
rate individual system, namely the TBL tagger.
3.1.1 Lemmatizer Evaluation
We evaluated the lemmatizers against the lem-
mas found in the S???????-3 gold standard.6 Even
the lowest performing system improved accuracy
by 31.74% over the baseline, which baseline sim-
ply equates the given token with the lemma. Ta-
ble 2 shows the results of evaluating the lemmatizers
against the EAW key.
While the simple voting system performed bet-
ter than any of the individual lemmatizers, hyphen-
ated words proved problematic for all of the sys-
tems. Some hyphenated words in the test set re-
mained hyphenated in the gold standard, and some
others were separated. However, evaluation results
show that splitting hyphenated words increases lem-
matizing accuracy by 0.9% .
3.2 Part of Speech Tagging
We also investigated the contribution of part of
speech taggers to the task of word sense disam-
biguation. We considered three taggers: the El-
worthy bigram tagger (Elworthy, 1994) within the
RASP parser (Briscoe et al, 2006), an enhanced
4http://www.cis.upenn.edu/?xtag
5http://gposttl.sourceforge.net
6We removed those lines from both the test input and the
gold standard which were marked U (= unknown, 34 lines), and
we removed the 40 lines from the test input that were missing
from the gold standard. This gave us 2007 words in both the
test set and the gold standard.
12
Lemmatizer Accuracy
Baseline 57.50%
XTAG 89.24%
Celex 91.58%
TBL 92.38%
Voting {XTAG,Celex,TBL} 93.77%
Voting, no hyphen {XTAG,Celex,TBL} 94.67%
Table 2: Accuracy of several lemmatizers on <head>
words of EAW task.
TBL tagger (Brill, 1992)7, and a TnT-style trigram
tagger (Hala?csy et al, 2007).8 The baseline was a
unigram tagger which selects the most frequently-
occurring tag of singletons when dealing with un-
seen words.
All three of the main taggers performed compa-
rably, although only the Elworthy tagger provides
probabilities associated with tags, rather than get-
ting a single tag as output. This additional infor-
mation can be useful, since we can employ differ-
ent strategies for a word with one single tag with a
probability of 1, versus a word with multiple tags,
the most probable of which might only have a prob-
ability of 0.3 for example. For comparative pur-
poses, we mapped the various instantiations of tags
for nouns, verbs, adjectives, and adverbs to these
four basic tags, and evaluated the taggers? results
against the EAW key. Table 3 shows the results of
this evaluation.
The performance of these taggers on the EAW
<head>-words is lower than results reported on
other datasets. This can explained by the lack of
frequently-occurring function words, which are easy
to tag and raise overall accuracy. Also, the words
in the test set are often highly ambiguous not only
with respect to their word sense, but also their part
of speech.
4 Supervised Learning of Sparse Category
Indices for WSD
In this component of our refinement of the base-
line, we train a supervised system that performs
higher-precision classification, only returning an an-
swer when a predictive feature that strongly pre-
dicts a particular sense is observed. To achieve this,
7http://gposttl.sourceforge.net
8http://code.google.com/p/hunpos
POS Tagger Accuracy
Baseline 84.10%
TBL 90.48%
Elworthy 90.58%
TnT 91.13%
Voting {TBL,Elw.,TnT} 91.88%
Table 3: Accuracy of several POS taggers on <head>
words of EAW task.
we implemented a ?feature focus? classifier (sparse
weighted index) as described in (Madani and Con-
nor, 2008, henceforth, MC08). MC08?s methods
for restricting and pruning the number of feature-to-
class associations are useful for finding and retain-
ing only strong predictive features. Moreover, this
allowed us to use a rich feature set (more than 1.6
million features) without an unwieldy explosion in
the number of parameters, as feature-class associa-
tions that are not strong enough are simply dropped.
4.1 Sparse Category Indices
MC08 describe a space and time efficient method
for learning discriminative classifiers that rank large
numbers of output classes using potentially millions
of features for many instances in potentially tera-
scale data sets. The authors describe a method for
learning ?category indices? ? i.e., weighted bipar-
tite graphs G ? F ?W ?C, where F is the set of fea-
tures, C is the set of output classes and all weights
(or ?associations?) w ? W between features and the
output classes they predict are real-valued and in
[0.0, 1.0]. The space and time efficiency of MC08?s
approach stems chiefly from three (parameterisable)
restrictions on category indices and how they are up-
dated. First, at any time in the learning process, only
those edges ( fi, w j, ck) ? G whose associations w j
are a large enough proportion of the sum of all class
associations for fi are retained: that is, only retain
w j s.t. w j ? wmin.9 Second, by setting an upper
bound dmax on the number of associations that a
feature fi is allowed to have, only the largest fea-
ture associations are retained. Setting dmax to a low
number (? 25) makes each feature a high-precision,
low-recall predictor of output classes. Further, the
dmax and wmin restrictions on parameter reten-
9Recall that w j ? W are all between 0.0 and 1.0 and sum to
1.0.
13
tion allow efficient retrieval and update of feature
weights, as only a small number of feature weights
need be consulted for predicting output classes or
learning from prediction mistakes in an online learn-
ing setting.10 Finally, in the online learning algo-
rithm,11 in addition to the small number of features
that need be consulted or updated, an error margin
marg can be set so that parameter update only oc-
curs when the score(c)? score(c?) ? marg, where c
is the correct output class and c? , c is the most con-
fident incorrect prediction of the classifier. Setting
marg = 0.0 leads to purely error-driven learning,
while marg = 1.0 always updates on every learning
instance. Values of marg ? (0.0, 1.0) will bias the
category index learner to update at different levels
of separation of the correct class from the most con-
fident incorrect class, ranging from almost always
error driven (near 0.0) to almost error-insensitive
learning (near 1.0).
4.2 Integration into the WSD Task
Using both the Semcor-3 and English Lexical Sam-
ple training data sets (a total of ?45,000 sentences,
each with one or more labeled instances), we trained
a sparse category index classifier as in MC08 with
the following features: using words, lemmas and
parts of speech (POSs) as tokens, we define fea-
tures for (1) preceding and following unigrams and
bigrams over tokens, as well as (2) the conjunc-
tion of the preceding unigrams (i.e., a 3-word win-
dow minus the current token) and (3) the conjunc-
tion of the preceding and following bigrams (5-
word window minus the current token). Finally
all surrounding lemmas in the sentence are treated
as left- or right-oriented slot-independent features
with an exponentially decaying level of activation
act(li) = 0.5 ? exp
(
0.5 ? ? dist(li , targ wd)
)
? where dist(li, targ wd) is simply the word dis-
tance from the target word to the contextual lemma
li.12 Although WSD is not a many-class, large-
10dmax bounds the number of feature-class associations (pa-
rameters) must be consulted in prediction and updating, but,
because of the wmin restriction, MC08 found that, on aver-
age, many fewer feature associations ? ? 16 ? were ever
touched per training or testing instance in their classification
experiments. See Madani and Connor (2008) for more details.
11Again, see Madani and Connor (2008) for more details.
12The value 0.5 is also a parameter that we have fixed, but it
could in principle be tuned to a particular data set. In the interest
of simplicity, we have not done this.
scale classification task,13 we nevertheless found
MC08?s pruning mechanisms useful for removing
weak feature-word associations. Due to the ag-
gressive pruning of feature-class associations, our
model only has ?1.9M parameters out of a potential
1, 600, 000 ? 200, 000 = 320 billion (the number of
features times the number of WordNet 3.0 senses).
4.3 Individual System Results
To integrate the predictions of the classifier into the
EAW task, we looked up all senses for each lemma-
POS pairing, backing off to looking up the words
themselves by the same POS, and finally resorting
to splitting hyphenated words and rejoining multi-
word units (as marked up in the EAW test set). Be-
ing high precision, the classifier does not return a
valid answer for every lemma, so we report results
with and without backing off to the most frequent
sense baseline to fill in these gaps.
Individual system scores are listed in Table 4. The
classifier on its own returns very few answers (with a
coverage ? as distinct from recall ? of only 10.4%
of the test set items). Although the classifier-only
performance does not have broad enough coverage
for stand-alone use, its predictions are nonetheless
useful in combination with the baseline. Further, we
expect coverage to grow when trained over a larger
corpus (such as the very large web-extracted corpus
of Agirre et al (2004), which this learning method
is well suited for).
5 Wikipedia for Word Sense
Disambiguation
Wikipedia, an online, user-created encyclopedia,
can be considered a collection of articles which link
to each other. While much information exists within
the textual content of Wikipedia that may assist in
WSD, the approach presented here instead uses the
article names and link structure within Wikipedia to
find articles which are most related to a WordNet
sense or context. We use the Green method to find a
relatedness metric for articles from Wikipedia14 (Ol-
13Large-scale data sets are available, but this does not change
the level of polysemy in WordNet, which is not in the thousands
for any given lemma.
14Computations were performed using a January 3rd 2008
download of the English Wikipedia.
14
Back-off Precision Recall Prec. (n-best) Rec. (n-best)
Y?? 0.592 0.589 0.594 0.589
N? 0.622 0.065 0.694 0.070
Table 4: Precision and recall of sparse category index classifier ? both ?soft? scores of standard Senseval script and
scores where any correct answer in list returned by the classifier is counted as a correct answer (?n-best?). ?Back-off?
signals whether the system backs off to the most frequent sense baseline.
livier and Senellart, 2007) based on each sense or
context of interest.
Advantages of this method over alternative meth-
ods that attempt to incorporate Wikipedia into WSD
is that our system is unsupervised and that no man-
ual mapping needs to take place between WordNet
and Wikipedia. Mihalcea (2007) demonstrates that
manual mappings can be created for a small num-
ber of words with relative ease, but for a very large
number of words the effort involved in mapping
would approach presented involves no be consider-
able. The approach presented here involves no map-
ping between WordNet and Wikipedia but human ef-
fort in mapping between WordNet and Wikipedia,
but instead initializes the Green method with a vec-
tor based only on the article names (as described in
Section 5.2).
5.1 Green Method
The Green method (Ollivier and Senellart, 2007) is
used to determine the importance of one node in a
directed graph with respect to other nodes.15 In the
context of Wikipedia the method finds the articles
which are most likely to be frequented if a random
walk were used to traverse the articles, starting with
a specific article and returning to that article if the
random walk either strays too far off topic or to an
article which is generally popular even without the
context of the initial article. One of the features of
the Green method is that it does not simply repro-
duce the global PageRank (Brin and Page, 1998),
instead determining the related pages nearby due to
relevance to the initial node.
The probability that the random walker of
Wikipedia will transfer to an article is defined as a
uniform distribution over the outlinks of the page
where the random walker is currently located. As
an approximation to the method described by Ol-
15In subsequent sections we give a high-level description of
using the Green method with Wikipedia, however see Ollivier
and Senellart (2007) for a much more detailed explanation.
livier and Senellart (2007), we create a subgraph of
Wikipedia for every computation, comprised of the
articles within a distance of 2 outlink traversals from
the initial articles. Since Wikipedia is very highly
connected, this constructed subgraph still contains
a large number of articles and performance of the
Green method on this subgraph is similar to that on
the whole connectivity graph.
5.2 Green Method for Contexts
To use the Green method to find Wikipedia arti-
cles which correspond to a given word to be dis-
ambiguated, articles which may discuss that word
and the context surrounding that word are found in
Wikipedia as an initial set of locations for the ran-
dom walker to start. This is done by looking for the
word itself as the name of an article. If there is not
an article whose name corresponds to the word in
question, then articles with the word as a substring
of the article name are found.
Since the goal of WSD is to choose the best word
sense within the context of other words, we use a
given word?s context to select a set of Wikipedia ar-
ticles which may discuss the content of the word in
question. The expectation is that the context words
will aid in disambiguation and that the context words
will together be associated with an appropriate sense
of the word being disambiguated. For this method
we defined a word?s context as the word itself, the
content words in the sentence the word occurs in,
and those occurring in the sentences before and af-
ter that sentence.
5.3 Green Method for Senses
Every sense of a word to be disambiguated also
needs to be represented as corresponding articles
in Wikipedia before using the Green method. The
words that we search for in the titles of Wikipedia
articles include the word itself, and, for every sense,
the content words of the sense?s WordNet gloss, as
well as the content of the sense?s hypernym gloss
15
and the synonyms of the hypernym. Exploring this
particular aspect of this module ? which informa-
tion about a sense to extract before using the Green
Method ? is a point for further exploration.
5.4 Interpreting Projections
The Green method as described by Ollivier and
Senellart (2007) uses, as the initial set of articles,
the vector containing only one article: that article
for which related articles are being searched. We
use as the initial set of articles the collection of ar-
ticles in Wikipedia corresponding to either the con-
text for the word to be disambiguated or the sense of
a word. The random walker is modeled as starting
in any of the articles in this set with uniform proba-
bility. Within the context of the Green method, this
means that this initial set of articles corresponds to
what would be linked to from a new Wikipedia arti-
cle about the sense or context. Each of the content
words in this new article (which is not in Wikipedia)
would link to one of the articles in the set found by
the methods described above. In this way the results
of the Green method computation can be interpreted
as a relatedness metric for the sense or context itself
and the articles which are in Wikipedia.
5.5 Analysis
The process of finding the sense of a word to be dis-
ambiguated is as follows: the vector output from the
Green method (a relatedness measure between the
initial seed and each article in Wikipedia) for the
context of the word is compared against the vector
output from using the Green method on each sense
that the word could have. The comparison is done
using the cosine of the angle between the two vec-
tors.
To determine for which instances in S??????? this
method may perform well, an analysis was per-
formed on a small development set (15 sentences)
from SemCor. A simple heuristic was formulated,
selecting the sense with the nearest Green method
output to the sentence?s Green method output when
the ratio between the first and second highest ranked
senses? cosine angle scores was above a threshold.
Applying this heuristic to the EAW task yielded
an expectedly low recall of 11% but a precision of
81% on all the words that this heuristic could apply,
but only a precision of 25% (recall 0.5%) for non-
monosemous words (which were the desired targets
MFS Rerank Wiki
MFS ? 94% 97%
Rerank 23% ? 99%
Wiki 45% 98% ?
Table 5: Complementarity between modules
of the method). Of 37 instances where this method
differs from the MFS baseline in the EAW task, 8 in-
stances are correctly disambiguated by this module.
6 Results
Although the individual systems have fairly low re-
call, we can calculate pairwise complementarity be-
tween systems si and s j by evaluating
(
1 ? |wrong in si and s j|
|wrong in si|
)
The results, presented in Table 5, indicate that the
systems complement each other well, and suggest
that a combination system could have a higher per-
formance than the individual systems.
We investigate a number of techniques to combine
the results ? while the integration of the lemma / part
of speech refinement is done by all modules as a pre-
processing step, the method of combination of the
resulting modules is less clear. As shown in Florian
et al (2002), a simple voting mechanism achieves
comparable performance to a stacking mechanism.
We present our results in Table 6, DT gives the re-
sult of a 10-fold cross-validation of WEKA stacked
decision trees and nearest neighbours built from the
individual system results (Witten and Frank, 2000).
Very few decisions are changed with the voting
method of combination, and the overall result does
not outperform the best MFS baseline (presented in
the table as ?All MFS?). This combination method
may be more useful with a greater number of sys-
tems being combined ? our system only combines
three systems (thus only one non-MFS system has to
suggest the MFS for this to be selected), and backs
off to the MFS sense in case all three disagree. The
degree of complementarity between the Wiki system
and the MFS system indicates that these will over-
ride the Rerank system in many cases.
Better results are seen with the simple stacking
result: in this case, systems are ordered and thus
16
System Precision Recall F-measure
All MFS 58.6% 58.6% 58.6%
Voting 58.6% 58.6% 58.6%
Stacking 58.9% 58.9% 58.9%
Stacked DT/NN 58.7% 58.7% 58.7%
Table 6: Resulting refined system (forced-choice)
are not being subjected to overriding by other MFS
skewed systems.
7 Conclusion
We have presented a refinement of the most fre-
quent sense baseline system, which incorporates a
number of novel approaches to word sense disam-
biguation methods. We demonstrate the need for
accurate lemmatization and part of speech tagging,
showing that that is probably the area where the
biggest boost in performance can currently be ob-
tained. We would also argue that examining the ab-
solute performance in a task where the baseline is so
exceedingly variable (ourselves, we have found the
baseline to be as low as 56% with restricted lemma
backoff, 58.4% with a fairly sophisticated lemma /
PoS module, against published baselines of 61.5%
in McCarthy et al, 62.5% reported in Snyder, or the
upper bound baseline of 66% using correct lemmas
and parts of speech), the performance difference be-
tween the baseline used and the resulting system is
interesting in itself.
Acknowledgments
We would like to thank DJ Hovermale for his input
throughout this project.
References
Agirre, E., , and de Lacalle Lekuona, O. L. (2004).
Publicly Available Topic Signatures for all Word-
Net Nominal Senses. In Proceedings of the 4th In-
ternational Conference on Languages Resources
and Evaluations (LREC), Lisbon, Portugal.
Baayen, H., Piepenbrock, R., and Gulikers, L.
(1995). The CELEX lexical database (release 2).
CD-ROM. Centre for Lexical Information, Max
Planck Institute for Psycholinguistics, Nijmegen;
Linguistic Data Consortium, University of Penn-
sylvania.
Brill, E. (1992). A simple rule-based part of speech
tagger. In Proceedings of the Third Conference
on Applied Natural Language Processing, pages
152?155, Trento, Italy.
Brin, S. and Page, L. (1998). The anatomy of a large-
scale hypertextual web search engine. In Com-
puter Networks and ISDN Systems, pages 107?
117.
Briscoe, E., Carroll, J., and Watson, R. (2006). The
second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions, Sydney, Australia.
Elworthy, D. (1994). Does Baum-Welch re-
estimation help taggers? In Proceedings of the
4th ACL Conference on Applied NLP, pages 53?
58, Stuttgart, Germany.
Florian, R., Cucerzan, S., Schafer, C., and
Yarowsky, D. (2002). Combining classifiers for
word sense disambiguation. Journal of Natural
Language Engineering, 8(4):327?342.
Hala?csy, P., Kornai, A., and Oravecz, C. (2007).
HunPos ? an open source trigram tagger. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, pages 209?212, Prague, Czech Republic.
Association for Computational Linguistics.
Korhonen, A., Krymolovski, Y., and Briscoe, T.
(2006). A large subcategorization lexicon for
natural language processing applications. In
Proceedings of the 5th international conference
on Language Resources and Evaluation, pages
1015?1020.
Korhonen, A. and Krymolowski, Y. (2002). On the
robustness of entropy-based similarity measures
in evaluation of subcategorization acquisition sys-
tems. In Proceedings of the 6th Conference on
Natural Language Learning, pages 91?97.
Lin, D. (1998). Automatic retrieval and clustering
of similar words. In Proceedings of the COLING-
ACL?98, pages 768?773.
Madani, O. and Connor, M. (2008). Large-Scale
17
Many-Class Learning. In Proceedins of the SIAM
Conference on Data Mining (SDM-08).
McCarthy, D., Koeling, R., Weeds, J., and Carroll,
J. (2007). Unsupervised acquisition of predom-
inant word senses. Computational Linguistics,
33(4):553?590.
Mihalcea, R. (2007). Using Wikipedia for automatic
word sense disambiguation. In Human Language
Technologies 2007: The Conferece of the North
Americ an Chapter of the Association for Compu-
tational Linguistics, Rochester, New York.
Miller, G., Beckwith, R., Felbaum, C., Gross, D.,
and Miller, K. (1990). Introduction to WordNet:
An on-line lexical database. Journal of Lexicog-
raphy, 3(4):235?244.
Miller, G., Leacock, C., Ranee, T., and Bunker, R.
(1993). A semantic concordance. In Proceedings
of the 3rd DARPA Workshop on Human Language
Technology, pages 232?235.
Ollivier, Y. and Senellart, P. (2007). Finding related
pages using Green measures: An illustration with
Wikipedia. In Association for the Advancement
of Artificial Intelligence Conference on Artificial
Intelligence (AAAI 2007).
Pedersen, T., Patwardhan, S., and Michelizzi, J.
(2004). Wordnet::similarity - measuring the re-
latedness of concepts. In Proceedings of Fifth
Annual Meeting of the North American Chapter
of the Association for Computational Linguistics,
pages 38?41.
Schmidt, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
the International Conference on New Methods in
Language Processing, pages 44?49.
Snyder, B. and Palmer, M. (2004). The english all-
words task. In Mihalcea, R. and Chklowski, T.,
editors, Proceedings of SENSEVAL-3: Third In-
ternational Workshop on Evaluating Word Sense
Disambiguating Systems, pages 41?43.
Witten, I. H. and Frank, E. (2000). Data min-
ing: Practical Machine Learning Tools and Tech-
niques with Java Implementations, chapter 8.
Morgan Kaufmann Publishers.
XTAG Research Group (2001). A lexicalized tree
adjoining grammar for English. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.
Yarowsky, D. (1993). One Sense Per Collocation. In
Proceedings of the Human Language Technology
Conference, Princeton, NJ, USA.
Yngve, V. H. (1955). Syntax and the problem of
multiple meaning. In Locke, W. N. and Booth,
A. D., editors, Machine translation of languages,
pages 208?226. John Wiley and Sons, New York.
18
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 736?744,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
What a Parser can Learn from a Semantic Role Labeler and Vice Versa
Stephen A. Boxwell, Dennis N. Mehay, Chris Brew
The Ohio State University
{boxwell, mehay, cbrew}@ling.ohio-state.edu
Abstract
In many NLP systems, there is a unidirectional flow
of information in which a parser supplies input to a
semantic role labeler. In this paper, we build a sys-
tem that allows information to flow in both direc-
tions. We make use of semantic role predictions in
choosing a single-best parse. This process relies on
an averaged perceptron model to distinguish likely
semantic roles from erroneous ones. Our system pe-
nalizes parses that give rise to low-scoring semantic
roles. To explore the consequences of this we per-
form two experiments. First, we use a baseline gen-
erative model to produce n-best parses, which are
then re-ordered by our semantic model. Second, we
use a modified version of our semantic role labeler
to predict semantic roles at parse time. The perfor-
mance of this modified labeler is weaker than that
of our best full SRL, because it is restricted to fea-
tures that can be computed directly from the parser?s
packed chart. For both experiments, the resulting se-
mantic predictions are then used to select parses. Fi-
nally, we feed the selected parses produced by each
experiment to the full version of our semantic role
labeler. We find that SRL performance can be im-
proved over this baseline by selecting parses with
likely semantic roles.
1 Introduction
In the semantic role labeling task, words or groups of
words are described in terms of their relations to a pred-
icate. For example, the sentence Robin admires Leslie
has two semantic role-bearing words: Robin is the agent
or experiencer of the admire predicate, and Leslie is
the patient. These semantic relations are distinct from
syntactic relations like subject and object ? the proper
nouns in the sentence Leslie is admired by Robin have
the same semantic relationships as Robin admires Leslie,
even though the syntax differs.
Although syntax and semantics do not always align with
each other, they are correlated. Almost all automatic se-
mantic role labeling systems take a syntactic representa-
tion of a sentence (taken from an automatic parser or a
human annotator), and use the syntactic information to
predict semantic roles. When a semantic role labeler pre-
dicts an incorrect role, it is often due to an error in the
parse tree. Consider the erroneously annotated sentence
from the Penn Treebank corpus shown in Figure 1. If a
semantic role labeling system relies heavily upon syntac-
tic attachment decisions, then it will likely predict that
in 1956 describes the time that asbestos was used, rather
than when it ceased to be used.
Errors of this kind are common in treebanks and in au-
tomatic parses. It is telling, though, that while the hand-
annotated Penn Treebank (Marcus et al, 1993), the Char-
niak parser (Charniak, 2001), and the C&C parser (Clark
and Curran, 2004) all produce the erroneous parse from
Figure 1, the hand-annotated Propbank corpus of verbal
semantic roles (Palmer et al, 2005) correctly identifies in
1956 as a temporal modifier of stopped, rather than using.
This demonstrates that while syntactic attachment deci-
sions like these are difficult for humans and for automatic
parsers, a human reader has little difficulty identifying the
correct semantic relationship between the temporal mod-
ifier and the verbs. This is likely due to the fact that the
meaning suggested by the parse in Figure 1 is unlikely ?
the reader instinctively feels that a temporal modifier fits
better with the verb stop than with the verb use.
In this paper, we will use the idea that semantic roles
predicted by correct parses are more natural than seman-
tic roles predicted by erroneous parses. By modifying a
state-of-the-art CCG semantic role labeler to predict se-
mantic roles at parse time, or by using it to select from
an n-best list, we can prefer analyses that yield likely se-
mantic roles. Syntactic analysis is treated not as an au-
tonomous task, but rather as a contributor to the final goal
of semantic role labeling.
2 Related Work
There has been a great deal of work in joint parsing and
semantic role labeling in recent years. Two notable ef-
forts have been the CoNLL 2008 and 2009 shared tasks
736
S


HH
HH
H
NP
the company
VP



HH
H
H
VB
stopped
VP



HH
H
HH
VB
using
NP
asbestos
PP
in 1956
Figure 1: A parse tree based on the treebank parse of wsj 0003.3. Notice that the temporal adjunct is erroneously attached low. In
a syntax-based SRL system, this will likely lead to a role prediction error.
(Surdeanu et al, 2008; Hajic? et al, 2009). Many of these
systems perform joint syntactic and semantic analysis by
generating an n-best list of syntactic parses, labeling se-
mantic roles on all of them, then re-ranking these parses
by some means. Our approach differs from this strategy
by abandoning the preliminary ranking and predicting se-
mantic roles at parse time. By doing this, we effectively
open semantic roles in the entire parse forest to exami-
nation by the ranking model, rather than restricting the
model to an n-best list generated by a baseline parser. The
spirit of this work more closely resembles that of Finkel
and Manning (2009) , which improves both parsing and
named entity recognition by combining the two tasks.
3 Why Predicting Semantic Roles in a
Packed Chart is Difficult
Predicting semantic roles in the environment of a packed
chart is difficult when using an atomic CFG. In order to
achieve the polynomial efficiency appropriate for wide-
coverage parsing, it is necessary to ?pack? the chart ?
that is, to combine distinct analyses of a given span of
words that produce the same category. The only other
widely used option for wide-coverage parsing is to use
beam search with a narrow beam, which runs the risk
of search errors. On methodological grounds we pre-
fer an exhaustive search, since systems that rely heav-
ily on heuristics for their efficiency are difficult to un-
derstand, debug or improve. It is straightforward to read
off the highest scoring parse from a packed chart, and
similarly routine to generate an n-best list containing a
highly-ranked subset of the parses. However, a packed
chart built on an atomic CFG does not make available
all of the features that are important to many CFG-based
SRL systems. In particular, the very useful treepath fea-
ture, which lists the categories touched by walking the
tree from the predicate to the target word, only makes
sense when you have a complete tree, so cannot easily
be computed from the chart (Figure 2). Chart edges can
S




HH
H
HH
H
NPpeople



PP
PP
P
More intelligent people
VPsaw



PP
PP
P
saw kids with telescopes
Figure 2: In the context of a packed chart, it is meaningless to
speak of a treepath between saw and people because multiple
analyses are ?packed? under a single category.
be lexicalized with their headwords, and this information
would be useful in role labeling ? but even this misses
vital subcategorization information that would be avail-
able in the complete parse. An ideal formalism for our
purpose would condense into the category label a wide
range of information about combinatory potential, heads,
and syntactic dependencies. At the same time it should
allow the creation of a packed chart, come with labeled
training data, and have a high-quality parser and semantic
role labeler already available. Fortunately, Combinatory
Categorial Grammar offers these desiderata, so this is our
formalism of choice.
4 Combinatory Categorial Grammar
Combinatory Categorial Grammar (Steedman, 2000) is
a grammar formalism that describes words in terms of
their combinatory potential. For example, determiners
belong to the category np/n, or ?the category of words
that become noun phrases when combined with a noun
to the right?. The rightmost category indicates the argu-
ment that the category is seeking, the leftmost category
indicates the result of combining this category with its
argument, and the slash (/ or \) indicates the direction of
combination. Categories can be nested within each other:
a transitive verb like devoured belongs to the category
737
The man devoured the steak
np/n n (s\np)/npx npx/nx nx
> >np npx
>
s\np
<s
Figure 3: A simple CCG derivation.
The steak that the man devoured
np (npx\npx)/(s/npx) np (s\np)/npx
>T
s/(s\np)
>B
s/npx
>
npx\npx
<npx
Figure 4: An example of CCG?s treatment of relative clauses.
The syntactic dependency between devoured and steak is the
same as it was in figure 3. Co-indexations (the ?xs?) have been
added here and above to aid the eye in following the relevant
[devoured-steak] dependency.
(s\np)/np, or ?the category that would become a sentence
if it could combine with a noun phrase to the right and
another noun phrase to the left?. An example of how cat-
egories combine to make sentences is shown in Figure 3.
CCG has many capabilities that go beyond that of a typ-
ical context-free grammar. First, it has a sophisticated
internal system of managing syntactic heads and depen-
dencies1. These dependencies are used to great effect in
CCG-based semantic role labeling systems (Gildea and
Hockenmaier, 2003; Boxwell et al, 2009), as they do
not suffer the same data-sparsity effects encounted with
treepath features in CFG-based SRL systems. Secondly,
CCG permits these dependencies to be passed through in-
termediary categories in grammatical structures like rel-
ative clauses. In Figure 4, the steak is still in the object
relation to devoured, even though the verb is inside a rel-
ative clause. Finally and most importantly, these depen-
dencies are represented directly on the CCG categories
themselves. This is what makes CCG resistant to the
problem described in Section 3 ? because the dependency
is formed when the two heads combine, it is available to
be used as a local feature by the semantic role labeler.
1A complete explanation of CCG predicate-argument dependencies
can be found in the CCGbank user manual (Hockenmaier and Steed-
man, 2005)
5 Semantic Role Labeling
We use a modified version of the Brutus semantic role
labeling system (Boxwell et al, 2009)2. The original ver-
sion of this system takes complete CCG derivations as in-
put, and predicts semantic roles over them. For our pur-
poses, however, it is necessary to modify the system to
make semantic predictions at parse time, inside a packed
chart, before the complete derivation is available. For
this reason, it is necessary to remove the global features
from the system (that is, features that rely on the com-
plete parse), leaving only local features (features that are
known at the moment that the predicate is attached to the
argument). Crucially, dependency features count as ?lo-
cal? features, even though they have the potential to con-
nect words that are very far apart in the sentence.
Brutus is arranged in a two-stage pipeline. First, a max-
imum entropy classifier3 predicts, for each predicate in
turn, which words in the sentence are likely headwords of
semantic roles. Then, a second maximum entropy classi-
fier assigns role labels to each of these words. The fea-
tures used in the identification model of the local-only
version of Brutus are as follows:
? Words. A three-word window surrounding the can-
didate word. For example, if we were considering
the word steak in Figure 3, the three features would
be represented as word -1=the, word 0=steak, and
word 1=#, with the last feature representing an out-
of-bounds index.
? Predicate. The predicate whose semantic roles the
system is looking for. For example, the sentence in
figure 3 contains one predicate: devour.
? Syntactic Dependency. As with a previous ap-
proach in CCG semantic role labeling (Gildea and
Hockenmaier, 2003), this feature shows the ex-
act nature of the syntactic dependency between the
predicate and the word we are considering, if any
such dependency exists. This feature is represented
by the category of the predicate, the argument slot
that this word fits into, and whether or not the predi-
cate is the head of the resultant category, represented
with a left or right arrow. In the example from fig-
ure 3, the relationship between devoured and steak
would be represented as (s\np)/np.2.?.
The second maximum entropy classifier uses all of the
features from the identifier, plus several more:
2Found at http://www.ling.ohio-state.edu/
?boxwell/software/brutus.html
3Brutus uses Zhang Le?s maxent toolkit, available at
http://homepages.inf.ed.ac.uk/s0450736/maxent_
toolkit.html.
738
Model P R F
Local 89.8% 80.8% 85.1%
Global 89.8% 84.3% 87.0%
Table 1: SRL results for treebank parses, using the local model
described in Section 5 and the full global model.
? Before / After. A binary indicator feature indicat-
ing whether the candidate word is before or after the
predicate.
? Result Category Detail. This indicates the feature
on the result category of the predicate. Possible
values include dcl (for declarative sentences), pss
(for passive sentences), ng (for present-progressive
phrases like ?running the race?), etc. These are read
trivially off of the verbal category.
? Argument Mapping. An argument mapping is a
prediction of a likely set of semantic roles for a
given CCG predicate category. For example, a likely
argument mapping for devoured:(s[dcl]\np)/np is
[Arg0,Arg1]. These are predicted from string-level
features, and are useful for bringing together oth-
erwise independent classification decisions for in-
dividual roles. Boxwell et al (2009) describe this
feature in detail.
The Maximum-Entropy models were trained to 500 it-
erations. To prevent overfitting, we used Gaussian pri-
ors with global variances of 1 and 5 for the identifier
and the labeler, respectively. Table 1 shows SRL perfor-
mance for the local model described above, and the full
global CCG-system described by Boxwell et al (2009).
We use the method for calculating the accuracy of Prop-
bank verbal semantic roles described in the CoNLL-2008
shared task on semantic role labeling (Surdeanu et al,
2008). Because the Brutus SRL system is not designed
to accommodate Nombank roles (Meyers et al, 2004),
we restrict ourselves to predicting Propbank roles in the
present work.
The local system has the same precision as the global
one, but trails it on recall and F-measure. Note that this
performance is achieved with gold standard parses.
6 Performing Semantic Role Predictions at
Parse Time
Recall that the reasoning for using a substantially pared
down version of the Brutus SRL system is to allow it to
predict semantic roles in the context of a packed chart.
Because we predict semantic roles for each constituent
immediately after the constituent is formed and before it
is added to the chart, we can use semantic roles to inform
parsing. We use a CKY parsing algorithm, though this
approach could be easily adapted to other parse strate-
gies.
Whenever two constituents are combined, the SRL sys-
tem checks to see if either of the constituents contains
predicates. The system then attempts to identify seman-
tic roles in the other constituent related to this predicate.
This process repeats at every step, creating a combined
syntax-semantics parse forest. Crucially, this allows us
to use features derived from the semantic roles to rank
parses inside the packed chart. This could result in an
improvement over ranking completed parses, because re-
ranking completed parses requires first generating an n-
best list of parse candidates, potentially preventing the
re-ranker from examining high value parses falling out-
side the n-best list.
In order to train our parse model, it is necessary to first
employ a baseline parse model over the training set. The
baseline model is a PCFG model, where the products of
the probabilities of individual rule applications are used
to rank candidate parses. We use a cross-fold validation
technique to parse the training set (train on sections 02-
20 to parse section 21, train on sections 02-19 and 21 to
parse section 20, and so on). As we parse these sentences,
we use the local SRL model described in Section 5 to
predict semantic roles inside the packed chart. We then
iterate over the packed chart and extract features based
on the semantic roles in it, effectively learning from ev-
ery possible semantic role in the parse forest. Notice that
this does not require enumerating every parse in the for-
est (which would be prohibitively expensive) ? the roles
are labeled at parse time and can therefore be read di-
rectly from the packed chart. For each role in the packed
chart, we label it as a ?good? semantic role if it appears in
the human-judged Propbank annotation for that sentence,
and a ?bad? semantic role if it does not.
The features extracted from the packed chart are as fol-
lows:
? Role. The semantic role itself, concatenated with
the predicate. For example, play.Arg1. This will
represent the intuition described in Section 1 that
certain roles are more semantically appealing than
others.
? Role and Headword. The semantic role concate-
nated with the predicate and the headword of the se-
mantic role. This reflects the idea that certain words
fit with particular roles better than others.
These features are used to train an averaged percep-
tron model to distinguish between likely and unlikely se-
mantic roles. We incorporate the perceptron directly with
the parser using a packed feature forest implementation,
following an approach used by the current state-of-the-
art CCG parser (Clark and Curran, 2004). By prefer-
739
ring sentences with good semantic roles, we hope to pro-
duce parses that give better overall semantic role predic-
tions. The parser prefers spans with better semantic roles,
and breaks ties that would have arisen using the base-
line model alone. Similarly the baseline model can break
ties between equivalent semantic roles; this has the added
benefit of encouraging normal-form derivations in cases
of spurious ambiguity. The result is a single-best com-
plete parse with semantic roles already predicted. Once
the single-best parse is selected, we allow the global SRL
model to predict any additional roles over the parse, to
catch those roles that are difficult to predict from local
features alone.
7 Experiment 1: Choosing a Single-Best
Derivation from an N-best List
Our first experiment demonstrates our model?s perfor-
mance in a ranking task. In this task, a list of candidate
parses are generated by our baseline model. This base-
line model treats rule applications as a PCFG ? each rule
application (say, np + s\np = s) is given a probability in
the standard way. The rule probabilities are unsmoothed
maximum likelihood estimates derived from rule counts
in the training portion of CCGbank. After n-best deriva-
tions are produced by the baseline model, we use the Bru-
tus semantic role labeler to assign roles to each candi-
date derivation. We vary the size of the n-best list from
1 to 10 (note that an n-best list of size 1 is equivalent to
the single-best baseline parse). We then use the seman-
tic model to re-rank the candidate parses and produce a
single-best parse. The outcomes are shown in Table 2.
n P R F
1 85.1 71.7 77.8
2 85.9 74.8 79.9
5 84.5 76.8 80.5
10 83.7 76.8 80.1
C&C 83.6 76.8 80.0
Table 2: SRL performance on the development set (section 00)
for various values of n. The final row indicates SRL perfor-
mance on section 00 parses from the Clark and Curran CCG
parser.
The availability of even two candidate parses yields
a 2.1% boost to the balanced F-measure. This is be-
cause the semantic role labeler is very sensitive to syn-
tactic attachment decisions, and in many cases the set of
rule applications used in the derivation are very similar or
even the same. Consider the simplified version of a phe-
nomenon found in wsj 0001.1 shown in Figures 5 and 6.
The only difference in rule applications in these deriva-
tions is whether the temporal adjunct attaches to s[b]\np
or s[dcl]\np. Because the s[dcl]\np case is slightly more
He will join Nov. 27th
np (s[dcl]\np)/(s[b]\np) s[b]\np (s\np)\(s\np)
>
s[dcl]\np
>
s[dcl]\np
<
s[dcl]
Figure 5: The single-best analysis for He will join Nov 27th
according to the baseline model. Notice that the temporal ad-
junct is attached high, leading the semantic role labeler to fail
to identify ArgM-TMP.
He will join Nov. 27th
np (s[dcl]\np)/(s[b]\np) s[b]\np (s\np)\(s\np)
<
s[b]\np
>
s[dcl]\np
<
s[dcl]
Figure 6: The second-best analysis of He will join Nov 27th.
This analysis correctly predicts Nov 27th as the ArgM-TMP of
join, and the semantic model correctly re-ranks this analysis to
the single-best position.
common in the treebank, the baseline model identifies it
as the single-best parse, and identifies the derivation in
figure 6 as the second-best parse. The semantic model,
however, correctly recognizes that the semantic roles pre-
dicted by the derivation in Figure 6 are superior to those
predicted by the derivation in figure 5. This demonstrates
how a second or third-best parse according to the baseline
model can be greatly superior to the single-best in terms
of semantics.
8 Experiment 2: Choosing a Single-Best
Derivation Directly from the Packed
Chart
One potential weakness with the n-best list approach de-
scribed in Section 7 is choosing the size of the n-best list.
As the length of the sentence grows, the number of can-
didate analyses grows. Because sentences in the treebank
and in real-world applications are of varying length and
complexity, restricting ourselves to an n-best list of a par-
ticular size opens us to considering some badly mangled
derivations on short, simple sentences, and not enough
derivations on long, complicated ones. One possible so-
lution to this is to simply choose a single best derivation
directly from the packed chart using the semantic model,
eschewing the baseline model entirely except for break-
ing ties. In this approach, we use the local SRL model
described in section 6 to predict semantic roles at parse
time, inside the packed chart. This frees us from the
740
need to have a complete derivation (as in the n-best list
approach in Section 7). We use the semantic model to
choose a single-best parse from the packed chart, then we
pass this complete parse through the global SRL model to
give it all the benefits afforded to the parses in the n-best
approach. The results for the semantic model compared
to the baseline model are shown in table 3. Interestingly,
Model P R F
Baseline 85.1 71.7 77.8
Semantic 82.7 70.5 76.1
Table 3: A comparison of the performance of the baseline model
and the semantic model on semantic role labeling. The seman-
tic model, when unrestrained by the baseline model, performs
substantially worse.
the semantic model performs considerably worse than the
baseline model. To understand why, it is necessary to re-
member that the semantic model uses only semantic fea-
tures ? probabilities of rule applications are not consid-
ered. Therefore, the semantic model is perfectly happy to
predict derivations with sequences of highly unlikely rule
applications so long as they predict a role that the model
has been trained to prefer.
Apparently, the reckless pursuit of appealing semantic
roles can ultimately harm semantic role labeling accuracy
as well as parse accuracy. Consider the analysis shown
in Figure 7. Because the averaged perceptron semantic
model is not sensitive to the relationships between differ-
ent semantic roles, and because Arg1 of name is a ?good?
semantic role, the semantic model predicts as many of
them as it can. The very common np-appositive construc-
tion is particularly vulnerable to this kind of error, as it
can be easily mistaken for a three-way coordination (like
carrots, peas and watermelon). Many of the precision
errors generated by the local model are of this nature,
and the global model is unlikely to remove them, given
the presence of strong dependencies between each of the
?subjects? and the predicate.
Coordination errors are also common when dealing with
relative clause attachment. Consider the analysis in Fig-
ure 8. To a PCFG model, there is little difference be-
tween attaching the relative clause to the researchers or
Lorillard nor the researchers. The semantic model, how-
ever, would rather predict two semantic roles than just
one (because study:Arg0 is a highly appealing semantic
role). Once again, the pursuit of appealing semantic roles
has led the system astray.
We have shown in Section 7 that the semantic model
can improve SRL performance when it is constrained to
the most likely PCFG derivations, but enumerating n-best
lists is costly and cumbersome. We can, however, com-
bine the semantic model with the baseline PCFG. Our
method for doing this is designed to avoid the kinds of er-
ror described above. We first identify the highest-scoring
parse according to the PCFG model. This parse will be
used in later processing unless we are able to identify an-
other parse that satisfies the following criteria:
1. It must be closely related to the parse that has the
best score according to the semantic model. To iden-
tify such parses, we ask the chart unpacking algo-
rithm to generate all the parses that can be reached
by making up to five attachment changes to this se-
mantically preferred parse ? no more.
2. It must have a PCFG score that is not much less than
that of the single-best PCFG parse. We do this by
requiring that it has a score that is within a factor of
? of the best available. That is, the single-best parse
from the semantic model must satisfy
logP (sem) > logP (baseline) + log(?)
where the ? value is tuned on the development set.
If no semantically preferred parse meets the above cri-
teria, the single-best PCFG parse is used. We find that
the PCFG-preferred parse is used about 35% of the time
and an alternative used instead about 65% of the time.
The SRL performance for this regime, using a range of
cut-off factors, is shown in table 4. On this basis we se-
lect a cut-off of 0.5 as suitable for use for final testing.
On the development set this method gives the best pre-
cision in extracting dependencies, but is slightly inferior
to the method using a 2-best list on recall and balanced
F-measure.
Factor (?) P R F
0.5 86.3 71.9 78.5
0.1 85.4 72.0 78.1
0.05 85.2 72.0 78.0
0.005 84.3 71.3 77.3
Table 4: SRL accuracy when the semantic model is constrained
by the baseline model
9 Results and Discussion
We use the method for calculating SRL performance de-
scribed in the CoNNL 2008 and 2009 shared tasks. How-
ever, because the semantic role labeler we use was not de-
signed to work with Nombank (and it is difficult to sepa-
rate Nombank and Propbank predicates from the publicly
released shared task output), it is not feasible to compare
results with the candidate systems described there. We
can, however, compare our two experimental models with
our baseline parser and the current state-of-the-art CCG
741
Arg1 Arg1 Arg1 mod rel Arg2
Rudolph Agnew, 61 and the former chairman, was named a nonexecutive director
np np conj np/n n/n n (s\np)/(s\np) (s\np)/np np/n n/n n
> >
n/n n/n
> >np np
<?> >
np s\np
<?> >
np s\np
<s
Figure 7: A parse produced by the unrestricted semantic model. Notice that Rudolph Agnew, 61 and the former chairman is
erroneously treated as a three-way conjunction, assigning semantic roles to all three heads.
Arg0 Arg0 rel Arg1
Neither Lorillard nor the researchers who studied the workers were aware
np/np np conj np (np\np)/(s\np) (s\np)/np np (s\np)/(s\np) s\np
<?> > >
np s\np s\np
>
np\np
<np
>np
>s
Figure 8: Relative clause attachment poses problems when preceded by a conjunction ? the system generally prefers attaching
relative clauses high. In this case, the relative clause should be attached low.
parser (Clark and Curran, 2004). The results on the test
set (WSJ Section 23, <40 words) are shown in Table 5.
There are many areas for potential improvement for the
system. The test set scores of both of our experimental
models are lower than their development set scores,where
the n-best model outperforms even the Clark and Curran
parser in the SRL task. This may be due to vocabulary
issues (we are of course unable to evaluate if the vocab-
ulary of the training set more closely resembles the de-
velopment set or the test set). If there are vocabulary is-
sues, they could be alleviated by experimenting with POS
based lexical features, or perhaps even generalizing a la-
tent semantics over heads of semantic roles (essentially
identifying broad categories of words that appear with
particular semantic roles, rather than counting on having
encountered that particular word in training). Alternately,
this drop in performance could be caused by a mismatch
in the average length of sentences, which would cause our
? factor and the size of our n-best lists (which were tuned
on the development set) to be suboptimal. We anticipate
the opportunity to further explore better ways of deter-
mining n-best list size. We also anticipate the possibility
of integrating the semantic model with a state-of-the-art
CCG parser, potentially freeing the ranker from the limi-
tations of a simple PCFG baseline.
It is also worth noting that the chart-based model seems
heavily skewed towards precision. Because the parser can
dig deeply into the chart, it is capable of choosing a parse
that predicts only semantic roles that it is highly confi-
dent about. By choosing these parses (and not parses with
less attractive semantic roles), the model can maximize
the average score of the semantic roles it predicts. This
tendency towards identifying only the most certain roles
is consistent with high-precision low-recall results. The
n-best parser has a much more restricted set of semantic
roles from parses more closely resembling the single-best
parse, and therefore is less likely to be presented with the
opportunity to choose parses that do away with less likely
(but still reasonable) roles.
10 Conclusions and Future Work
In this paper, we discuss the procedure for identifying se-
mantic roles at parse time, and using these roles to guide
the parse. We demonstrate that using semantic roles to
guide parsing can improve overall SRL performance, but
that these same benefits can be realized by re-ranking an
n-best list with the same model. Regardless, there are
several reasons why it is useful to have the ability to pre-
dict semantic roles inside the chart.
Predicting semantic roles inside the chart could be used
to perform SRL on very long or unstructured passages.
742
SRL Labeled Deps
Model P R F P R F
Baseline 84.7 70.7 77.0 80.0 79.8 79.9
Rank n=5 82.0 73.7 77.7 80.1 80.0 80.0
Chart 90.0 68.4 77.7 82.3 80.2 81.2
C&C 83.3 77.6 80.4 84.9 84.6 84.7
Char 77.1 75.5 76.5 - - -
Table 5: The full system results on the test set of the WSJ
corpus (Section 23). Included are the baseline parser, the n-
best reranking model from Section 7, the single-best chart-
unpacking model from Section 8, and the state-of-the-art C&C
parser. The final row shows the SRL performance obtained by
Punyakanok et al (2008) using the Charniak parser. Unfor-
tunately, their results are evaluated based on spans of words
(rather than headword labels), which interferes with direct com-
parison. The Charniak parser is a CFG-style parser, making la-
beled dependency non-applicable.
Most parsing research on the Penn Treebank (the present
work included) focuses on sentences of 40 words or less,
because parsing longer sentences requires an unaccept-
ably large amount of computing resources. In practice,
however, semantic roles are rarely very distant from their
predicates ? generally they are only a few words away;
often they are adjacent. In long sentences, the prediction
of an entire parse may be unnecessary for the purposes of
SRL.
The CKY parsing algorithm works by first predicting all
constituents spanning two words, then all constituents
spanning three words, then four, and so on until it pre-
dicts constituents covering the whole sentence. By setting
a maximum constituent size (say, ten or fifteen), we could
abandon the goal of completing a spanning analysis in fa-
vor of identifying semantic roles in the neighborhood of
their predicates, eliminating the need to unpack the chart
at all. This could be used to efficiently perform SRL on
poorly structured text or even spoken language transcrip-
tions that are not organized into discrete sentences. Doing
so would also eliminate the potentially noisy step of au-
tomatically separating out individual sentences in a larger
text. Alternately, roles predicted in the chart could even
be incorporated into a low-precision-high-recall informa-
tion retrieval system seeking a particular semantic rela-
tionship by scanning the chart for a particular semantic
role.
Another use for the packed forest of semantic roles could
be to predict complete sets of roles for a given sentence
using a constraint based method like integer linear pro-
gramming. Integer linear programming takes a large
number of candidate results (like semantic roles), and ap-
plies a set of constraints over them (like ?roles may not
overlap? or ?no more than one of each role is allowed in
each sentence?) to find the optimal set. Doing so could
eliminate the need to unpack the chart at all, effectively
producing semantic roles without committing to a single
syntactic analysis.
11 Acknowledgements
We would like to thank Mike White, William Schuler,
Eric Fosler-Lussier, and Matthew Honnibal for their help-
ful feedback.
References
Stephen A. Boxwell, Dennis N. Mehay, and Chris Brew. 2009.
Brutus: A semantic role labeling system incorporating CCG,
CFG, and Dependency features. In Proc. ACL-09.
E. Charniak. 2001. Immediate-head parsing for language mod-
els. In Proc. ACL-01, volume 39, pages 116?123.
Stephen Clark and James R. Curran. 2004. Parsing the WSJ
using CCG and Log-Linear Models. In Proc. ACL-04.
J.R. Finkel and C.D. Manning. 2009. Joint parsing and named
entity recognition. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computational Lin-
guistics, pages 326?334. Association for Computational Lin-
guistics.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying se-
mantic roles using Combinatory Categorial Grammar. In
Proc. EMNLP-03.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara, M.A. Mart??,
L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?, J. S?te?pa?nek, et al
2009. The CoNLL-2009 shared task: Syntactic and seman-
tic dependencies in multiple languages. In Proceedings of
the Thirteenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18. Association for
Computational Linguistics.
J. Hockenmaier and M. Steedman. 2005. CCGbank manual.
Technical report, MS-CIS-05-09, University of Pennsylva-
nia.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska,
B. Young, and R. Grishman. 2004. The nombank project:
An interim report. In A. Meyers, editor, HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation, pages 24?31,
Boston, Massachusetts, USA, May 2 - May 7. Association
for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic Roles.
Computational Linguistics, 31(1):71?106.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008. The
Importance of Syntactic Parsing and Inference in Semantic
Role Labeling. Computational Linguistics, 34(2):257?287.
Mark Steedman. 2000. The Syntactic Process. MIT Press.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. In Proceedings
743
of the Twelfth Conference on Computational Natural Lan-
guage Learning, pages 159?177. Association for Computa-
tional Linguistics.
744
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 54?58,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Lightly-Supervised Word Sense Translation Error Detection for an
Interactive Conversational Spoken Language Translation System
Dennis N. Mehay, Sankaranarayanan Ananthakrishnan and Sanjika Hewavitharana
Speech, Language and Multimedia Processing Unit
Raytheon BBN Technologies
Cambridge, MA, 02138, USA
{dmehay,sanantha,shewavit}@bbn.com
Abstract
Lexical ambiguity can lead to concept
transfer failure in conversational spo-
ken language translation (CSLT) systems.
This paper presents a novel, classification-
based approach to accurately detecting
word sense translation errors (WSTEs) of
ambiguous source words. The approach
requires minimal human annotation effort,
and can be easily scaled to new language
pairs and domains, with only a word-
aligned parallel corpus and a small set of
manual translation judgments. We show
that this approach is highly precise in de-
tecting WSTEs, even in highly skewed
data, making it practical for use in an in-
teractive CSLT system.
1 Introduction
Lexical ambiguity arises when a single word form
can refer to different concepts. Selecting a con-
textually incorrect translation of such a word ?
here referred to as a word sense translation error
(WSTE) ? can lead to a critical failure in a con-
versational spoken language translation (CSLT)
system, where accuracy of concept transfer is
paramount. Interactive CSLT systems are espe-
cially prone to mis-translating less frequent word
senses, when they use phrase-based statistical ma-
chine translation (SMT), due to its limited use of
source context (source phrases) when constructing
translation hypotheses. Figure 1 illustrates a typi-
cal WSTE in a phrase-based English-to-Iraqi Ara-
bic CSLT system, where the English word board
Disclaimer: This paper is based upon work supported by
the DARPA BOLT program. The views expressed here are
those of the authors and do not reflect the official policy or
position of the Department of Defense or the U.S. Govern-
ment.
Distribution Statement A (Approved for Public Release,
Distribution Unlimited)
Figure 1: Example WSTE in English-to-Iraqi SMT.
is mis-translated as mjls (?council?), completely
distorting the intended message.
Interactive CSLT systems can mitigate this
problem by automatically detecting WSTEs in
SMT hypotheses, and engaging the operator in a
clarification dialogue (e.g. requesting an unam-
biguous rephrasing). We propose a novel, two-
level classification approach to accurately detect
WSTEs. In the first level, a bank of word-specific
classifiers predicts, given a rich set of contextual
and syntactic features, a distribution over possi-
ble target translations for each ambiguous source
word in our inventory. A single, second-level clas-
sifier then compares the predicted target words to
those chosen by the decoder and determines the
likelihood that an error was made.
A significant novelty of our approach is that the
first-level classifiers are fully unsupervised with
respect to manual annotation and can easily be
expanded to accommodate new ambiguous words
and additional parallel data. The other innovative
aspect of our solution is the use of a small set of
manual translation judgments to train the second-
level classifier. This classifier uses high-level fea-
tures derived from the output of the first-level clas-
sifiers to produce a binary WSTE prediction, and
can be re-used unchanged even when the first level
of classifiers is expanded.
Our goal departs from the large body of work
devoted to lightly-supervised word sense disam-
biguation (WSD) using monolingual and bilingual
corpora (Yarowsky, 1995; Schutze, 1998; Diab
and Resnik, 2002; Ng et al., 2003; Li and Li, 2002;
Purandare and Pedersen, 2004), which seeks to la-
54
bel and group unlabeled sense instances. Instead,
our approach detects mis-translations of a known
set of ambiguous words.
The proposed method also deviates from ex-
isting work on global lexical selection models
(Mauser et al., 2009) and on integration of WSD
features within SMT systems with the goal of im-
proving offline translation performance (Chan et
al., 2007). Rather, we detect translation errors due
to ambiguous source words with the goal of pro-
viding feedback to and soliciting clarification from
the system operator in real time. Our approach
is partly inspired by Carpuat and Wu?s (2007b;
2007a) unsupervised sense disambiguation mod-
els for offline SMT. More recently, Carpuat et al.
(2013) identify unseen target senses in new do-
mains, but their approach requires the full test cor-
pus upfront, which is unavailable in spontaneous
CSLT. Our approach can, in principle, identify
novel senses when unfamiliar source contexts are
encountered, but this is not our current focus.
2 Baseline SMT System
In this paper, we focus on WSTE detection in
the context of phrase-based English-to-Iraqi Ara-
bic SMT, an integral component of our interac-
tive, two-way CSLT system that mediates con-
versation between monolingual speakers of En-
glish and Iraqi Arabic. The parallel training cor-
pus of approximately 773K sentence pairs (7.3M
English words) was derived from the DARPA
TransTac English-Iraqi two-way spoken dialogue
collection and spans a variety of domains includ-
ing force protection, medical diagnosis and aid,
etc. Phrase pairs were extracted from bidirectional
IBM Model 4 word alignment after applying a
merging heuristic similar to that of Koehn et al.
(2003). A 4-gram target LM was trained on Iraqi
Arabic transcriptions. Our phrase-based decoder,
similar to Moses (Koehn et al., 2007), performs
beam search stack decoding based on a standard
log-linear model, whose parameters were tuned
with MERT (Och, 2003) on a held-out develop-
ment set (3,534 sentence pairs, 45K words). The
BLEU and METEOR scores of this system on a
separate test set (3,138 sentence pairs, 38K words)
were 16.1 and 42.5, respectively.
3 WSTE Detection
The core of the WSTE detector is a novel, two-
level classification pipeline. Our approach avoids
Figure 2: An English?Iraqi training pair.
the need for expensive, sense-labeled training data
based on the observation that knowing the sense of
an ambiguous source word is distinct from know-
ing whether a sense translation error has occurred.
Instead, the target (Iraqi Arabic) words typically
associated with a given sense of an ambiguous
source (English) word serve as implicit sense la-
bels, as the following describes.
3.1 A First Level of Unsupervised Classifiers
The main intuition behind our approach is that
strong disagreement between the expanded con-
text of an ambiguous source word and the corre-
sponding SMT hypothesis indicates an increased
likelihood that a WSTE has occurred. To identify
such disagreement, we train a bank of maximum-
entropy classifiers (Berger et al., 1996), one for
each ambiguous word. The classifiers are trained
on the same word-aligned parallel data used for
training the baseline SMT system, as follows.
For each instance of an ambiguous source word
in the training set, and for each target word it is
aligned to, we emit a training instance associating
that target word and the wider source context of
the ambiguous word. Figure 2 illustrates a typical
training instance for the ambiguous English word
board, which emits a tuple of contextual features
and the aligned Iraqi Arabic word lwHp (?plac-
ard?) as a target label. We use the following con-
textual features similar to those of Carpuat and
Wu (2005), which are in turn based on the clas-
sic WSD features of Yarowsky (1995).
Neighboring Words/Lemmas/POSs. The to-
kens, t, to the left and right of the current ambigu-
ous token, as well as all trigrams of tokens that
span the current token. Separate features for word,
lemma and parts of speech tokens, t.
Lemma/POS Dependencies. The lemma-
lemma and POS-POS labeled and unlabeled
directed syntactic dependencies of the current
ambiguous token.
55
Figure 3: An unsupervised first-level classifier.
Bag-of-words/lemmas. Distance decayed bag-
of-words-style features for each word and lemma
in a seven-word window around the current token.
Figure 3 schematically illustrates how this classi-
fier operates on a sample test sentence. The ex-
ample assumes that the ambiguous English word
board is only ever associated with the Iraqi Arabic
words lwHp (?placard?) and mjls (?council?) in
the training word alignment. We emphasize that
even though the first-level maximum entropy clas-
sifiers are intrinsically supervised, their training
data is derived via unsupervised word alignment.
3.2 A Second-Level Meta-Classifier
The first-level classifiers do not directly predict
the presence of a WSTE, but induce a distribu-
tion over possible target words that could be gen-
erated by the ambiguous source word in that con-
text. In order to make a binary decision, this distri-
bution must be contrasted with the corresponding
target phrase hypothesized by the SMT decoder.
One straightforward approach, which we use as
a baseline, is to threshold the posterior probabil-
ity of the word in the SMT target phrase which is
ranked highest in the classifier-predicted distribu-
tion. However, this approach is not ideal because
each classifier has a different target label set and is
trained on a different number of instances.
To address this issue, we introduce a second
meta-classifier, which is trained on a small number
of hand-annotated translation judgments of SMT
hypotheses of source sentences containing am-
biguous words. The bilingual annotator was sim-
ply asked to label the phrasal translation of source
phrases containing ambiguous words as correct or
incorrect. We obtained translation judgments for
511 instances from the baseline SMT development
and test sets, encompassing 147 pre-defined am-
biguous words obtained heuristically from Word-
Net, public domain homograph lists, etc.
The second-level classifier is trained on a small
Figure 4: The two-level WSTE architecture.
set of meta-features drawn from the predictions of
the first-level classifiers and from simple statistics
of the training corpus. For an ambiguous word
w
a
in source sentence S, with contextual features
f
1
(S), and aligned to target words t ? T (the set
of words in the target phrase) in the SMT hypoth-
esis, we extract the following features:
1. The first-level classifier?s maximum
likelihood of any decoded target word:
max
t?T
p
w
a
(t|f
1
(S))
2. The entropy of the predicted distribution:
?
t
p
w
a
(t|f
1
(S)) ? ln(p
w
a
(t|f
1
(S)))
3. The number of training instances for w
a
4. The inverse of the number of distinct target
labels for w
a
.
5. The product of meta-features (1) and (4)
A high value for feature 1 indicates that the first-
level model and the SMT decoder agree. By con-
trast, a high value for feature 2 indicates uncer-
tainty in the classifier?s prediction, due either to a
novel source context, or inadequate training data.
Feature 3 indicates whether the second scenario of
meta-feature 2 might be at play, and feature 4 can
be thought of as a simple, uniform prior for each
classifier. Finally, feature 5 attenuates feature 1
by this simple, uniform prior. We feed these fea-
tures to a random forest (Breiman, 2001), which
is a committee of decision trees, trained using ran-
domly selected features and data points, using the
implementation in Weka (Hall et al., 2009). The
target labels for training the second-level classifier
are obtained from the binary translation judgments
on the small annotated corpus. Figure 4 illustrates
the interaction of the two levels of classification.
56
3.3 Scalability and Portability
Scalability was an important consideration in de-
signing the proposed WSTE approach. For in-
stance, we may wish to augment the inventory
with new ambiguous words if the vocabulary
grows due to addition of new parallel data or due
to a change in the domain. The primary advan-
tage of the two-level approach is that new ambigu-
ous words can be accommodated by augmenting
the unsupervised first-level classifier set with addi-
tional word-specific classifiers, which can be done
by simply extending the pre-defined list of am-
biguous words. Further, the current classification
stack requires only ?1.5GB of RAM and performs
per-word WSTE inference in only a few millisec-
onds on a commodity, quad-core laptop, which is
critical for real-time, interactive CSLT.
The minimal annotation requirements also al-
low a high level of portability to new language
pairs. Moreover, as our results indicate (below), a
good quality WSTE detector can be bootstrapped
for a new language pair without any annotation ef-
fort by simply leveraging the first-level classifiers.
4 Experimental Results
The 511 WSTE-annotated instances used for train-
ing the second-level classifier doubled as an eval-
uation set using the leave-one-out cross-validation
method. Of these, 115 were labeled as errors by
the bilingual judge, while the remaining 396 were
translated correctly by the baseline SMT system.
The error prediction score from the second-level
classifier was thresholded to obtain the receiver
operating characteristic (ROC) curve shown in the
top (black) curve of Figure 5. We obtain a 43%
error detection rate with only 10% false alarms
and 71% detection with 20% false alarms, in spite
of the highly skewed label distribution. In abso-
lute terms, true positives outnumber false alarms
at both the 10% (49 to 39) and 20% (81 to 79) false
alarm rates. This is important for deployment, as
we do not want to disrupt the flow of conversation
with more false alarms than true positives.
For comparison, the bottom (red) ROC curve
shows the performance of a baseline WSTE pre-
dictor comprised of just meta-feature (1), obtain-
able directly from the first-level classifiers. This
performs slightly worse than the two-level model
at 10% false alarms (40% detection, 46 true pos-
itives, 39 false alarms), and considerably worse
at 20% false alarms (57% detection, 66 true pos-
Figure 5: WST error detection ROC curve.
itives, 78 false alarms). Nevertheless, this result
indicates the possibility of bootstrapping a good
quality baseline WSTE detector in a new language
or domain without any annotation effort.
5 Conclusion
We proposed a novel, lightly-supervised, two-
level classification architecture that identifies pos-
sible mis-translations of pre-defined ambiguous
source words. The WSTE detector pre-empts
communication failure in an interactive CSLT sys-
tem by serving as a trigger for initiating feed-
back and clarification. The first level of our de-
tector comprises of a bank of word-specific classi-
fiers trained on automatic word alignment over the
SMT parallel training corpus. Their predicted dis-
tributions over target words feed into the second-
level meta-classifier, which is trained on a small
set of manual translation judgments. On a 511-
instance test set, the two-level approach exhibits
WSTE detection rates of 43% and 71% at 10%
and 20% false alarm rates, respectively, in spite of
a nearly 1:4 skew against actual WSTE instances.
Because adding new ambiguous words to the in-
ventory only requires augmenting the set of first-
level unsupervised classifiers, our WSTE detec-
tion approach is scalable to new domains and
training data. It is also easily portable to new lan-
guage pairs due to the minimal annotation effort
required for training the second-level classifier. Fi-
nally, we show that it is possible to bootstrap a
good quality WSTE detector in a new language
pair without any annotation effort using only un-
supervised classifiers and a parallel corpus.
57
References
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A Maximum Entropy Ap-
proach to Natural Language Processing. Computa-
tional Linguistics, 22(1):39?71.
Leo Breiman. 2001. Random Forests. Technical re-
port, Statistics Department, University of California,
Berkeley, Berkeley, CA, USA, January.
Marine Carpuat and Dekai Wu. 2005. Word sense dis-
ambiguation vs. statistical machine translation. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 387?394.
Marine Carpuat and Dekai Wu. 2007a. How phrase
sense disambiguation outperforms word sense dis-
ambiguation for statistical machine translation. In
Proceedings of the 11th International Conference on
Theoretical and Methodological Issues in Machine
Translation (TMI 2007), Skovde, Sweden, Septem-
ber.
Marine Carpuat and Dekai Wu. 2007b. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72, Prague,
Czech Republic, June.
Marine Carpuat, Hal Daume? III, Katharine Henry,
Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. Sensespotting: Never let your par-
allel data tie you to an old domain. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1435?1445, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 33?40, Prague, Czech Republic,
June.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
255?262, July.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1).
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Hang Li and Cong Li. 2002. Word translation dis-
ambiguation using bilingual bootstrapping. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 343?351, July.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending statistical machine translation with dis-
criminative and trigger-based lexicon models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
210?218, Singapore, August. Association for Com-
putational Linguistics.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003.
Exploiting parallel texts for word sense disambigua-
tion: An empirical study. In Proceedings of 41st
Annual Meeting on Association for Computational
Linguistics, pages 455?462, July.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Amruta Purandare and Ted Pedersen. 2004. Word
sense discrimination by clustering contexts in vector
and similarity spaces. In Proceedings of the Confer-
ence on Computational Natural Language Learning,
pages 41?48.
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Journal of Computational Linguistics,
24:97?123.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd annual meeting on Association
for Computational Linguistics, ACL ?95, pages 189?
196, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
58
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 697?701,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Incremental Topic-Based Translation Model Adaptation for
Conversational Spoken Language Translation
Sanjika Hewavitharana, Dennis N. Mehay, Sankaranarayanan Ananthakrishnan
and Prem Natarajan
Speech, Language and Multimedia Business Unit
Raytheon BBN Technologies
Cambridge, MA 02138, USA
{shewavit,dmehay,sanantha,pnataraj}@bbn.com
Abstract
We describe a translation model adapta-
tion approach for conversational spoken
language translation (CSLT), which en-
courages the use of contextually appropri-
ate translation options from relevant train-
ing conversations. Our approach employs
a monolingual LDA topic model to de-
rive a similarity measure between the test
conversation and the set of training con-
versations, which is used to bias trans-
lation choices towards the current con-
text. A significant novelty of our adap-
tation technique is its incremental nature;
we continuously update the topic distribu-
tion on the evolving test conversation as
new utterances become available. Thus,
our approach is well-suited to the causal
constraint of spoken conversations. On
an English-to-Iraqi CSLT task, the pro-
posed approach gives significant improve-
ments over a baseline system as measured
by BLEU, TER, and NIST. Interestingly,
the incremental approach outperforms a
non-incremental oracle that has up-front
knowledge of the whole conversation.
1 Introduction
Conversational spoken language translation
(CSLT) systems facilitate communication be-
tween subjects who do not speak the same
language. Current systems are typically used to
achieve a specific task (e.g. vehicle checkpoint
search, medical diagnosis, etc.). These task-driven
Disclaimer: This paper is based upon work supported by the
DARPA BOLT program. The views expressed here are those
of the authors and do not reflect the official policy or position
of the Department of Defense or the U.S. Government.
Distribution Statement A (Approved for Public Release,
Distribution Unlimited)
conversations typically revolve around a set of
central topics, which may not be evident at the
beginning of the interaction. As the conversation
progresses, however, the gradual accumulation of
contextual information can be used to infer the
topic(s) of discussion, and to deploy contextually
appropriate translation phrase pairs. For example,
the word ?drugs? will predominantly translate
into Spanish as ?medicamentos? (medicines) in a
medical scenario, whereas the translation ?drogas?
(illegal drugs) will predominate in a law enforce-
ment scenario. Most CSLT systems do not take
high-level global context into account, and instead
translate each utterance in isolation. This often
results in contextually inappropriate translations,
and is particularly problematic in conversational
speech, which usually exhibits short, spontaneous,
and often ambiguous utterances.
In this paper, we describe a novel topic-based
adaptation technique for phrase-based statistical
machine translation (SMT) of spoken conversa-
tions. We begin by building a monolingual la-
tent Dirichlet alocation (LDA) topic model on the
training conversations (each conversation corre-
sponds to a ?document? in the LDA paradigm).
At run-time, this model is used to infer a topic
distribution over the evolving test conversation up
to and including the current utterance. Transla-
tion phrase pairs that originate in training conver-
sations whose topic distribution is similar to that
of the current conversation are given preference
through a single similarity feature, which aug-
ments the standard phrase-based SMT log-linear
model. The topic distribution for the test conver-
sation is updated incrementally for each new utter-
ance as the available history grows. With this ap-
proach, we demonstrate significant improvements
over a baseline phrase-based SMT system as mea-
sured by BLEU, TER and NIST scores on an
English-to-Iraqi CSLT task.
697
2 Relation to Prior Work
Domain adaptation to improve SMT performance
has attracted considerable attention in recent years
(Foster and Kuhn, 2007; Finch and Sumita, 2008;
Matsoukas et al, 2009). The general theme is to
divide the training data into partitions representing
different domains, and to prefer translation options
for a test sentence from training domains that most
resemble the current document context. Weak-
nesses of this approach include (a) assuming the
existence of discrete, non-overlapping domains;
and (b) the unreliability of models generated by
segments with little training data.
To avoid the need for hard decisions about do-
main membership, some have used topic modeling
to improve SMT performance, e.g., using latent
semantic analysis (Tam et al, 2007) or ?biTAM?
(Zhao and Xing, 2006). In contrast to our source
language approach, these authors use both source
and target information.
Perhaps most relevant are the approaches of
Gong et al (2010) and Eidelman et al (2012),
who both describe adaptation techniques where
monolingual LDA topic models are used to ob-
tain a topic distribution over the training data, fol-
lowed by dynamic adaptation of the phrase table
based on the inferred topic of the test document.
While our proposed approach also employs mono-
lingual LDA topic models, it deviates from the
above methods in the following important ways.
First, the existing approaches are geared towards
batch-mode text translation, and assume that the
full document context of a test sentence is always
available. This assumption is incompatible with
translation of spoken conversations, which are in-
herently causal. Our proposed approach infers
topic distributions incrementally as the conversa-
tion progresses. Thus, it is not only consistent
with the causal requirement, but is also capable
of tracking topical changes during the course of a
conversation.
Second, we do not directly augment the trans-
lation table with the inferred topic distribution.
Rather, we compute a similarity between the cur-
rent conversation history and each of the training
conversations, and use this measure to dynami-
cally score the relevance of candidate translation
phrase pairs during decoding.
3 Corpus Data and Baseline SMT
We use the DARPA TransTac English-Iraqi par-
allel two-way spoken dialogue collection to train
both translation and LDA topic models. This data
set contains a variety of scenarios, including med-
ical diagnosis; force protection (e.g. checkpoint,
reconnaissance, patrol); aid, maintenance and in-
frastructure, etc.; each transcribed from spoken
bilingual conversations and manually translated.
The SMT parallel training corpus contains ap-
proximately 773K sentence pairs (7.3M English
words). We used this corpus to extract transla-
tion phrase pairs from bidirectional IBM Model
4 word alignment (Och and Ney, 2003) based on
the heuristic approach of (Koehn et al, 2003). A
4-gram target LM was trained on all Iraqi Ara-
bic transcriptions. Our phrase-based decoder is
similar to Moses (Koehn et al, 2007) and uses
the phrase pairs and target LM to perform beam
search stack decoding based on a standard log-
linear model, the parameters of which were tuned
with MERT (Och, 2003) on a held-out develop-
ment set (3,534 sentence pairs, 45K words) using
BLEU as the tuning metric. Finally, we evaluated
translation performance on a separate, unseen test
set (3,138 sentence pairs, 38K words).
Of the 773K training sentence pairs, about
100K (corresponding to 1,600 conversations) are
marked with conversation boundaries. We use the
English side of these conversations for training
LDA topic models. All other sentence pairs are
assigned to a ?background conversation?, which
signals the absence of the topic similarity feature
for phrase pairs derived from these instances. All
of the development and test set data were marked
with conversation boundaries. The training, devel-
opment and test sets were partitioned at the con-
versation level, so that we could model a topic
distribution for entire conversations, both during
training and during tuning and testing.
4 Incremental Topic-Based Adaptation
Our approach is based on the premise that biasing
the translation model to favor phrase pairs origi-
nating in training conversations that are contextu-
ally similar to the current conversation will lead
to better translation quality. The topic distribution
is incrementally updated as the conversation his-
tory grows, and we recompute the topic similarity
between the current conversation and the training
conversations for each new source utterance.
698
4.1 Topic modeling with LDA
We use latent Dirichlet alocation, or LDA, (Blei et
al., 2003) to obtain a topic distribution over con-
versations. For each conversation di in the train-
ing collection (1,600 conversations), LDA infers a
topic distribution ?di = p(zk|di) for all latent top-
ics zk = {1, ...,K}, where K is the number of
topics. In this work, we experiment with values
of K ? {20, 30, 40}. The full conversation his-
tory is available for training the topic models and
estimating topic distributions in the training set.
At run-time, however, we construct the con-
versation history for the tuning and test sets in-
crementally, one utterance at a time, mirroring a
real-world scenario where our knowledge is lim-
ited to the utterances that have been spoken up to
that point in time. Thus, each development/test ut-
terance is associated with a different conversation
history d?, for which we infer a topic distribution
?d? = p(zk|d?) using the trained LDA model. We
use Mallet (McCallum, 2002) for training topic
models and inferring topic distributions.
4.2 Topic Similarity Computation
For each test utterance, we are able to infer the
topic distribution ?d? based on the accumulated
history of the current conversation. We use this
to compute a measure of similarity between the
evolving test conversation and each of the train-
ing conversations, for which we already have topic
distributions ?di . Because ?di and ?d? are proba-
bility distributions, we use the Jensen-Shannon di-
vergence (JSD) to evaluate their similarity (Man-
ning and Schu?tze, 1999). The JSD is a smoothed
and symmetric version of Kullback-Leibler diver-
gence, which is typically used to compare two
probability distributions. We define the similar-
ity score as sim(?di , ?d?) = 1? JSD(?di ||?d?).1
Thus, we obtain a vector of similarity scores in-
dexed by the training conversations.
4.3 Integration with the Decoder
We provide the SMT decoder with the similar-
ity vector for each test utterance. Additionally,
the SMT phrase table tracks, for each phrase pair,
the set of parent training conversations (including
the ?background conversation?) from which that
phrase pair originated. Using this information, the
decoder evaluates, for each candidate phrase pair
1JSD(?di ||?d?) ? [0, 1] when defined using log2.
REFERENCE TRANSCRIPTIONS
SYSTEM BLEU? TER? NIST?
Baseline 19.32 58.66 6.22
incr20 19.39 58.44 6.26*
incr30 19.36 58.32* 6.26
incr40 19.68* 58.19* 6.28*
conv20 19.60* 58.36* 6.27*
conv30 19.48 58.38* 6.27*
conv40 19.50 58.33* 6.28*
ASR TRANSCRIPTIONS
SYSTEM BLEU? TER? NIST?
Baseline 16.92 62.57 5.75
incr20 16.99 62.28* 5.77
incr30 16.96 62.33* 5.78
incr40 17.31* 61.97* 5.83*
conv20 17.29* 62.28* 5.81*
conv30 17.12 62.19* 5.80*
conv40 17.00 62.14* 5.79*
Table 1: Stemmed results on 3,138-utterance test
set. Asterisked results are significantly better than
the baseline (p ? 0.05) using 1,000 iterations
of paired bootstrap re-sampling (Koehn, 2004).
(Key: incrN = incremental LDA with N topics;
convN = non-incremental, whole-conversation
LDA with N topics.)
X ? Y added to the search graph, its topic simi-
larity score as follows:
FX?Y = max
i?Par(X?Y )
sim(?di , ?d?) (1)
where Par(X ? Y ) is the set of training con-
versations from which the candidate phrase pair
originated. Phrase pairs from the ?background
conversation? only are assigned a similarity score
FX?Y = 0.00. In this way we distill the in-
ferred topic distributions down to a single feature
for each candidate phrase pair. We add this fea-
ture to the log-linear translation model with its
own weight, which is tuned with MERT. The in-
tuition behind this feature is that the lower bound
of suitability of a candidate phrase pair should be
directly proportional to the similarity between its
most relevant conversational provenance and the
current context. Phrase pairs which only occur in
the background conversation are not directly pe-
nalized, but contribute nothing to the topic simi-
larity score.
699
Figure 1: Rank trajectories of 4 LDA inferred topics, with incremental topic inference. The x-axis
indicates the utterance number. The y-axis indicates a topic?s rank at each utterance.
5 Experimental Setup and Results
The baseline English-to-Iraqi phrase-based SMT
system was built as described in Section 3. This
system translated each utterance independently,
ignoring higher-level conversational context.
For the topic-adapted system, we compared
translation performance with a varying number of
LDA topics. In intuitive agreement with the ap-
proximate number of scenario types known to be
covered by our data set, a range of 20-40 topics
yielded the best results. We compared the pro-
posed incremental topic tracking approach to a
non-causal oracle approach that had up-front ac-
cess to the entire source conversations at run-time.
In all cases, we compared translation perfor-
mance on both clean-text and automatic speech
recognition (ASR) transcriptions of the source ut-
terances. ASR transcriptions were generated using
a high-performance two-pass HMM-based sys-
tem, which delivered a word error rate (WER) of
10.6% on the test set utterances.
Table 1 summarizes test set performance in
BLEU (Papineni et al, 2001), NIST (Doddington,
2002) and TER (Snover et al, 2006). Given the
morphological complexity of Iraqi Arabic, com-
puting string-based metrics on raw output can
be misleadingly low and does not always reflect
whether the core message was conveyed. Since
the primary goal of CSLT is information transfer,
we present automatic results that are computed af-
ter stemming with an Iraqi Arabic stemmer.
We note that in all settings (incremental
and non-causal oracle) our adaptation approach
matches or significantly outperforms the baseline
across multiple evaluation metrics. In particular,
the incremental LDA system with 40 topics is the
top-scoring system in both clean-text and ASR set-
tings. In the ASR setting, which simulates a real-
world deployment scenario, this system achieves
improvements of 0.39 (BLEU), -0.6 (TER) and
0.08 (NIST).
6 Discussion and Future Directions
We have presented a novel, incremental topic-
based translation model adaptation approach that
obeys the causality constraint imposed by spoken
conversations. This approach yields statistically
significant gains in standard MT metric scores.
We have also demonstrated that incremental
adaptation on an evolving conversation performs
better than oracle adaptation based on the com-
plete conversation history. Although this may
seem counter-intuitive, Figure 1 gives clues as to
why this happens. This figure illustrates the rank
trajectory of four LDA topics as the incremen-
tal conversation grows. The accompanying text
shows excerpts from the conversation. We indi-
cate (in superscript) the topic identity of most rele-
vant words in an utterance that are associated with
that topic. At the first utterance, the top-ranked
topic is ?5?, due to the occurrence of ?captain?
in the greeting. As the conversation evolves, we
note that this topic become less prominent. The
conversation shifts to a discussion on ?windows?,
raising the prominence of topic ?4?. Finally, topic
?3? becomes prominent due to the presence of the
700
words ?project? and ?contract?. Thus, the incre-
mental approach is able to track the topic trajecto-
ries in the conversation, and is able to select more
relevant phrase pairs than oracle LDA, which esti-
mates one topic distribution for the entire conver-
sation.
In this work we have used only the source lan-
guage utterance in inferring the topic distribution.
In a two-way CLST system, we also have access
to SMT-generated back-translations in the Iraqi-
English direction. As a next step, we plan to use
SMT-generated English translation of Iraqi utter-
ances to improve topic estimation.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022, March.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, HLT ?02, pages 138?145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic transla-
tion model adaptation. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Short Papers - Volume 2, ACL
?12, pages 115?119, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Andrew Finch and Eiichiro Sumita. 2008. Dynamic
model interpolation for statistical machine transla-
tion. In Proceedings of the Third Workshop on
Statistical Machine Translation, StatMT ?08, pages
208?215, Stroudsburg, PA, USA. Association for
Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, StatMT ?07, pages 128?135, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zhengxian Gong, Yu Zhang, and Guodong Zhou.
2010. Statistical machine translation based on LDA.
In Universal Communication Symposium (IUCS),
2010 4th International, pages 286?290.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395, Barcelona, Spain, July.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume
2, EMNLP ?09, pages 708?717, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311?318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings AMTA, pages 223?231, August.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical ma-
chine translation. Machine Translation, 21(4):187?
207, December.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In In
Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL ?06).
701
OSU-2: Generating Referring Expressions
with a Maximum Entropy Classifier
Emily Jamison
Department of Linguistics
The Ohio State University
Columbus, OH 43210, USA
jamison@ling.osu.edu
Dennis Mehay
Department of Linguistics
The Ohio State University
Columbus, OH 43210, USA
mehay@ling.osu.edu
Abstract
Selection of natural-sounding referring ex-
pressions is useful in text generation and in-
formation summarization (Kan et al, 2001).
We use discourse-level feature predicates in
a maximum entropy classifier (Berger et al,
1996) with binary and n-class classification to
select referring expressions from a list. We
find that while mention-type n-class classifi-
cation produces higher accuracy of type, bi-
nary classification of individual referring ex-
pressions helps to avoid use of awkward refer-
ring expressions.
1 Introduction
Referring expression generation is the task of insert-
ing noun phrases that refer to a mentioned extra-
linguistic entity into a text. REG is helpful for tasks
such as text generation and information summariza-
tion (Kan et al, 2001).
2 Task Description
The Referring Expressions Generation Chal-
lenge (Belz and Gatt, 2008) includes a task based
on the GREC corpus, a collection of introductory
texts from Wikipedia that includes articles about
cities, countries, rivers, people, and mountains.
In this corpus, the main topic of each text (MSR)
has been replaced with a list of possible referring
expressions (REs). The objective of the task is to
identify the most appropriate referring expression
from the list for each mention of the MSR, given
the surrounding text and annotated syntactic and
semantic information.
3 Predicates
We created 13 predicates, in addition to the six pred-
icates available with the corpus. All predicates can
be used with the binary classification method; only
non-RE-level predicates can be used with the n-class
classification method. Predicates describe: string
similarity of the RE and the title of the article, the
mention?s order in the article, distance between pre-
vious mention and current mention, and detection of
a contrastive discourse entity in the text.1
4 Maximum Entropy Classifier
We defined indicator feature functions for a number
of contextual predicates, each describing a pairing of
some potential property of the syntactico-semantic
and discourse context of a RE (a ?predicate?) and a
label. These feature functions fi were used to train
a maximum entropy classifier (Berger et al, 1996)
(Le, 2004) that assigns a probability to a RE re given
context cx as follows:
p(re | cx) = Z(cx) exp
n?
i=1
?ifi(cx, re)
where Z(cx) is a normalizing sum and the ?i are the
parameters (feature weights) learned. Two classifi-
cation systems were used: binary and n-class. With
the binary method, the classifier estimates the like-
lihood of a possible referring expression?s correct
insertion into the text, and inserts the RE with the
highest ?yes? probability. With the n-class method,
1More details at http://www.ling.ohio-state.edu/?jamison
196
Predicates Used Single Combinations
GREC predicates 40.40% 50.91%
all predicates 50.30% 58.54%
no contrasting entities 50.30% 59.30%
all non-RE-level preds 44.82% 51.07%
Table 1: Results with binary classification.
Predicates Used Single Combinations
all non-RE-level preds 61.13% 62.50%
Table 2: Results with n-class classification.
the mention is classified according to type of refer-
ring expression (proper name, common noun, pro-
noun, empty) and a RE of the proper type is chosen.
A predicate combinator was implemented to cre-
ate pairs of predicates for the classifier.
5 Results
Our results are shown in tables 1 and 2; table 3
shows further per-category results. N-class classi-
fication has a higher type accuracy than the binary
method(single: 61.13% versus 44.82%). Added
predicates made a notable difference (single, orig-
inal predicates: 40.40%; with added predicates:
50.30%). However, the predicates that detected con-
trasting discourse entities proved not to be helpful
(combinations: 59.30% declined to 58.54%). Fi-
nally, the predicate combinator improved all results
(binary, all predicates: 50.30% to 58.54%).
6 Discussion
The n-class method does not evaluate characteristics
of each individual referring expression. However,
the accuracy measure is designed to judge appro-
priateness of a referring expression based only on
whether its type is correct. A typical high-accuracy
n-class result is shown in example 1.
System City Ctry Mnt River Pple
b-all 53.54 57.61 49.58 75.00 65.85
b-nonRE 51.52 53.26 45.83 40.00 57.07
n-nonRE 53.54 63.04 61.67 65.00 67.32
Table 3: Challenge-submitted results by category.
Example 1: Albania
The Republic of Albania itself is a
Balkan country in Southeastern Europe.
Which itself borders Montenegro to the
north, the Serbian province of Kosovo to
the northeast, the Republic of Macedonia
in the east, and Greece in the south.
In example 1, both mentions are matched with an RE
that is the proper type (proper name and pronoun,
respectively), yet the result is undesireable.
A different example typical of the binary classifi-
cation method is shown in example 2.
Example 2: Alfred Nobel
Alfred Nobel was a Swedish chemist,
engineer, innovator, armaments manufac-
turer and the inventor of dynamite. [...] In
his last will, Alfred Nobel used his enor-
mous fortune to institute the Nobel Prizes.
In example 2, the use of predicates specific to each
RE besides the type causes use of the RE ?Alfred
Nobel? as a subject, and the RE ?his? as a posses-
sive pronoun. The text, if mildly repetitive, is still
comprehensible.
7 Conclusion
In this study, we used discourse-level predicates and
binary and n-class maximum entropy classifiers to
select referring expressions. Eventually, we plan
to combine these two approaches, first selecting all
REs of the appropriate type and then ranking them.
References
Anya Belz and Albert Gatt. 2008. REG
Challenge 2008: Participants Pack.
http://www.nltg.brighton.ac.uk/research/reg08/.
A. L. Berger, S. D. Pietra, and V. D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistcs, 22(1):39?71.
Min-Yen Kan, Kathleen R. McKeown, and Judith L. Kla-
vans. 2001. Applying natural language generation to
indicative summarization. EWNLG ?01: Proceedings
of the 8th European workshop on Natural Language
Generation.
Zhang Le. 2004. Maximum Entropy
Modeling Toolkit for Python and C++.
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
197
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 210?221,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
CCG Syntactic Reordering Models for Phrase-based Machine Translation
Dennis N. Mehay
The Ohio State University
Columbus, OH, USA
mehay@ling.ohio-state.edu
Chris Brew
Educational Testing Service
Princeton, NJ, USA
cbrew@ets.org
Abstract
Statistical phrase-based machine translation
requires no linguistic information beyond
word-aligned parallel corpora (Zens et al,
2002; Koehn et al, 2003). Unfortunately,
this linguistic agnosticism often produces un-
grammatical translations. Syntax, or sentence
structure, could provide guidance to phrase-
based systems, but the ?non-constituent? word
strings that phrase-based decoders manipu-
late complicate the use of most recursive syn-
tactic tools. We address these issues by
using Combinatory Categorial Grammar, or
CCG, (Steedman, 2000), which has a much
more flexible notion of constituency, thereby
providing more labels for putative non-
constituent multiword translation phrases. Us-
ing CCG parse charts, we train a syntactic
analogue of a lexicalized reordering model by
labelling phrase table entries with multiword
labels and demonstrate significant improve-
ments in translating between Urdu and En-
glish, two language pairs with divergent sen-
tence structure.
1 Introduction
Statistical phrase-based machine translation (PMT)
is attractive, as it requires no linguistic informa-
tion beyond word-aligned parallel corpora (Zens et
al., 2002; Koehn et al, 2003). Unfortunately, this
linguistic agnosticism leaves phrase-based systems
with no precise characterization of the word order
relationships between languages, often leading to
ungrammatical translations. Syntax could provide
guidance to phrase-based systems, by steering them
towards reorderings that reflect the structural rela-
tionships between languages, but using syntax to
guide a phrase-based system is problematic. Phrase-
based systems build the result incrementally from
the beginning of the target string to the end, and
the intermediate strings need not constitute complete
traditional syntactic constituents. It is difficult to
reconcile traditional recursive syntactic processing
with this regime, because not all intermediate strings
considered by the decoder would even have a syntac-
tic category to assess. As a result, most phrase-based
decoders control reordering using simple distance-
based distortion models, which penalize all reorder-
ing equally, and lexicalized reordering models (Till-
mann, 2004; Axelrod et al, 2005), which probabilis-
tically score various reordering configurations con-
ditioned on specific lexical translations. While un-
doubtedly better than nothing, these models perform
poorly when languages diverge considerably in sen-
tence structure. Distance-based distortion models
are too coarse-grained to distinguish correct from
incorrect reordering, while lexical reordering mod-
els suffer from data sparsity and fail to capture more
general patterns. We argue that finding a way to
label translation phrases with syntactic labels will
abstract over the observed reordering configurations
thereby address both all three deficiencies of granu-
larity, data sparsity and lack of generality.
The present work presents a novel syntactic ana-
logue of the lexicalized reordering model that uses
multiword syntactic labels to capture the general re-
ordering patterns between two languages with very
different word order. We accomplish this by using
Combinatory Categorial Grammar, or CCG (Steed-
210
man, 2000), a word-centered syntax that allows a
great deal of flexibility in how sentence analyses
are formed. Syntactic derivations in CCG are mas-
sively spuriously ambiguous, i.e., there are many
ways to derive the same semantic analysis of a sen-
tence, similar to how a mathematical equation can
be reduced by canceling out variables in different
orders. Despite its name, spurious ambiguity is a
benefit to us, as it provides many different labelled
bracketings for the same dependency graph of the
same sentence, thereby increasing the chance that
any substring of that sentence will have a syntactic
label. Our approach exploits this property of CCG
to derive multiword CCG syntactic labels for target
translation strings in a phrase table, thus providing a
firmer basis on which to collect syntactic reordering
statistics. In particular:
? We show how CCG can derive constituent la-
bels for target-side phrase-table entries that
are often lamented as ?non-constituents? or as
?crossing a phrase boundary?.
? Our CCG categories are not limited to single-
word supertags. Rather, as these labels are
drawn from CCG parse charts, they can span
multiple words. Further, the labels are tailored
specifically to each translation constituent?s
boundaries (Section 2.1). As a consequence,
?70% of phrase table entries receive a single
syntactic label (Section 5), largely removing
the terminological inconsistency of calling lex-
ical translation constituents ?phrases?. Now,
more of them actually are syntactic phrases.
? We use these labels to train a target-language
bidirectional reordering model over CCG syn-
tactic sequences (Section 3), which, when
added to the baseline system, is found to be su-
perior to systems that use both lexicalized re-
ordering models and supertag reordering mod-
els (Section 5).
With only minor modifications, we incorporate these
enhancements into a state-of-the-art PMT decoder
(Koehn et al, 2007), achieving significant improve-
ments over two competitive baselines in an Urdu-
English translation task (Sections 5). This language
pair was chosen to highlight the promise of this ap-
proach for languages with considerable, but syntac-
tically governed, word-order differences to one an-
other. Finally, in a small discussion we provide qual-
itative evidence that the improvements in automatic
metric scores correspond to real gains in target lan-
guage fluency.
2 Syntax, Constituency and Phrase-based
MT
Consider the following German-English PMT
phrase pair that we have extracted from a parallel
European parliamentary transcript:1
Ich hoffe, da? ? I hope that
Neither word string is a well-formed constituent in
traditional theories of syntax. But tradition is at odds
with the intuition that that such ?non-constituent?
sequences are still well-formed substrings, governed
by rules of how they can be combined with other
word strings ? e.g., declarative sentence translation
rules like es mo?glich sein wird ? it will be possible
can grammatically extend each, but a noun phrase
rule cannot.
As Figure 1 illustrates, putative non-constituent
word sequences abound in phrase-based MT. Here a
translation ?phrase? is simply any contiguous word
string that is consistent with a word alignment (a
relation between source and target words), usually
produced by a language-independent alignment pro-
cedure (Zens et al, 2002). The figure also high-
lights the need for linguistic syntax in controlling
how translations are assembled; the successful trans-
lation is merely one among many possible reorder-
ings, many of which (despite their ungrammatical-
ity) might score well on a word n-gram model. But
rather than changing the word alignments or PMT
?phrase? boundaries to fit a syntactic theory, we
choose to use a flexible syntax which can produce a
wider range of bracketings to accommodate the re-
sults of alignment-derived translations. To this end,
we use Combinatory Categorial Grammar, or CCG,
(Steedman, 2000). To understand how CCG allows
this, we illustrate its use with some simple examples.
1Throughout this paper, the term ?PMT phrase? refers to an
unbroken sequence of words used by a PMT system, whereas
?phrase? (without context) refers to a syntactic constituent.
211
Wiederaufnahme der Situngsperiode
Resumption of the session
Ich hoffe , da? es mo?glich sein wird
I hope that it will be possible
Ich hoffe, da? das den Weg fu?r eine baldige Wiederaufnahme der Debatte ebnen wird
I hope that this will pave the way for an early resumption of the debate
Figure 1: Two phrase-based MT word groups are extracted from aligned words (the dashed outlines) and then used to form a new
translation (bottom). [Adapted from parallel sentences in the Europarl German-English corpus, v6.]
2.1 CCG, Spurious Ambiguity and PMT:
Turning ?Phrases? into Phrases
CCG is a derivational syntax, where words are as-
signed a lexical category2 and sentence structures
are then recursively built using a small set of de-
ductive rule schemata known as combinators (Steed-
man, 2000). Lexical syntactic categories can be
richly structured in CCG, indicating how words can
combine. A syntactic category of the form X/Y,
e.g., states that a category of type X can be formed if
combined with a Y to its right ? i.e., a function from
rightward Ys to X. This can be accomplished with
the forward function application combinator (>),3
which is written in derivational form as follows:4
X/Y Y
>
X
This derivation of the symbol X is known as the
normal-form derivation (Steedman, 2000), since it
uses function application whenever possible. But
CCG has the ability to construct the same result
by using a different, non-normal-form sequence of
combinatory inferences. For example, by using the
backward type-raising combinator (T<) and then
backward function application (<), we can arrive at
the same result:
2When represented by a strings, lexical categories are called
supertags.
3CCG actually respects the rule-to-rule hypothesis (Bach,
l976), where, for every syntactic term built, there is a corre-
sponding semantic term, but, for simplicity of exposition, we
focus only on syntax here.
4The reader will notice that CCG derivations are in fact
trees, but that they ?grow? in the direction opposite to how parse
trees are often depicted in NLP.
X/Y Y
T<
X\(X/Y)
<
X
This derivation shows how the argument Y to the
functional type X/Y5 can ?raise? its type to be-
come a function that consumes that functional type,
X\(X/Y), only to produce same result as before,
namely X. This property of CCG is often referred
to as ?spurious ambiguity?, because there are many
ways of reaching the same result as the canonical,
normal-form derivation.
Despite the name, this property is useful for our
purposes. Considering the target translation in Fig-
ure 1, we then observe in Figure 2 how CCG can
derive not only a bracketing similar to a more tra-
ditional Penn Treebank-style parse, but also a non-
normal-form variant that gives us a single category
for the English translation string I hope that ?
namely the category S[dcl]/S[dcl] (a declarative sen-
tence lacking a declarative sentence complement to
its right).
We use this fact about CCG to label a wider
range of PMT phrases with genuine syntactic con-
stituent labels. First we parse the English sen-
tences in our training data with the C&C parser, a
state-of-the-art, treebank-trained CCG parser (Clark
and Curran, 2007), producing normal-form CCG
derivations. We then enumerate all non-normal-
form derivations that result in the same top-level
symbol, packing all derivations (normal-form and
non-normal-form) into a parse chart (see Figure 4).
5Also referred to as a functor.
212
SNP
I
VP
VBP
hope
SBAR
WNP
WDT
that
S
it will...
I hope that it will ...
NP (S[dcl]\NP)/S[em] S[em]/S[dcl] S[dcl]
>
S[em]
>
S[dcl]\NP
<
S[dcl]
I hope that it will ...
NP (S[dcl]\NP)/S[em] S[em]/S[dcl] S[dcl]
T>
S[dcl]/(S[dcl]\NP)
B>
S[dcl]/S[em]
B>
S[dcl]/S[dcl]
>
S[dcl]
Figure 2: Left: a traditional syntactic derivation; top right: a normal-form CCG derivation with the same subject+predicate
bracketing; bottom right: one of many non-normal-form variants. Combinator symbol key: >=forward function application,
<=backward function application, T>=forward type-raising, B>=forward composition. Note: the CCG dependencies that are
discharged in different orders are indicated by color-coding (if available in your medium) and underlining the appropriate categories
(type-raising discharges no dependencies). Both CCG derivations lead to the same symbol (S[dcl]), and dependencies.
UR.-EN.
SINGLE-LABEL COVERAGE 69%
AVE. EN. PHRASE LEN. 2.8 wds
AVE. CCG LABEL SPAN 2.3 wds
AVE. CCG LABS/ENTRY 1.4
Table 1: Training data statistics (top to bottom): (1) % of sin-
gle CCG labels spanning entire English translation phrases, (2)
average length of English translation phrase, (3) average CCG
label span and (4) average CCG labels per English translation
phrase. (Maximum translation phrase length is 7 words.)
For the English string of each phrase table entry, we
inspect the chart for the English-side sentence that
it came from and extract a list of labels as in Fig-
ure 3. For each span, this procedure either (lines
5?9) finds the topmost single label, only using type-
raised categories when no others exist,6 or (lines 10?
19) recursively and greedily finds the longest span-
ning labels from left to right, if no single label ex-
ists. The degenerate case is the single-word level
(supertags). In this way we find single labels for
69% of the English-side phrase training instances.
Table 1 gives more details.
6Type-raisings are almost always possible, and will always
be closer to the top-level symbol. Many type-raisings, however,
are superfluous ? i.e., produce no novel bracketings. Therefore
we only use type-raised symbols to derive a label for a span of
words when necessary.
GETLABELS(C,s)
1 B C: a packed chart of derivations of E
2 B s = (el, er): a span in target sentence E
3 B RETURN: a list of labels covering all words
4 B from E in span s
5 if EXISTSSINGLESPANNINGLABEL(C,S)
6 then B Get the topmost label
7 B non-type-raised, if possible
8 lb ? GETTOPMOSTLABEL(C,s)
9 return [ lb ]
10 else B Get the longest label starting at el
11 for i? (er ? 1) to (el + 1)
12 do lbs ? GETLABELS(C,(el, i))
13 if LENGTH(lbs)=1
14 then el? ? i+ 1
15 lb ? HEAD(lbs)
16 BREAK
17 else CONTINUE
18 return
19 CONS(lb,GETLABELS(C,(el? , er)))
Figure 3: Algorithm for labeling English sides of phrase
table instances.
213
0 I 1 hope 2 that 3 it 4 will 5 rain 6
0 Ich 1 hoffe 2 , 3 da? 4 es 5 regnen 6 wird 7
NP
S/(S\NP)
S[dcl]/S[em]
S[dcl]/S[dcl]
(S[dcl]\NP)/S[em]
S[em]/S[dcl]
NP
S/(S\NP)
S[em]/(S[dcl]\NP)
S[dcl]/(S[b]\NP)
S[dcl]
S[em]
S[dcl]\NP
S[em]/(S[b]\NP)
(S[dcl]\NP)/(S[b]\NP) S[b]\NP
(S[dcl]\NP)/(S[b]\NP)
S[dcl]\NP
S[dcl]
Figure 4: A packed CCG parse chart with multiple semantically equivalent derivations and two word-aligned strings. (Not all
derivations are depicted.)
3 Reordering Models: from Words to
Supertags to Parses
In phrase-based MT systems, the standard reorder-
ing model that controls the order in which the
source string is translated is the lexicalized reorder-
ing model (Tillmann, 2004; Axelrod et al, 2005). In
its simplest form, a lexicalized reordering model es-
timates, for each translation phrase pair (fi...j , ek...l)
(where the indices sit ?in-between? words, as in Fig-
ure 4), the probability of p(O | fi...j , ek...l), where
O ? {MONO, SWAP,DISCONTINUOUS} (abbrevi-
ated M, S and D) is the orientation of the phrase pair
(fi...j , ek...l) w.r.t. the previously translated source
phrase fu...v. If v = i, then O = M; if u = j, then
O = S; otherwise O = D. This model, known as
a unidirectional MSD lexicalized reordering model,
can also be enriched with statistics over orientations
to the next source phrase translated (i.e., it can be
a bidirectional model), as well as with more fine-
grained distinctions in the third class D (i.e., whether
it is DLEFT or DRIGHT). All models in the present
work are bidirectional MSD models.
During decoding, orientations are predicted based
on previously translated (or following) phrases in
the decoder?s search state, but, when extracting ori-
entation statistics, there are many different possi-
ble phrasal segmentations of both strings. A sim-
ple solution, known as word-based extraction, is to
look for neighboring alignment points that support
the various orientations. In Figure 4, e.g., a word-
based extraction regime would count the phrase
hoffe ? hope as being in orientation D w.r.t. to
what follows, because its rightmost index, 2, is dis-
contiguous with the next aligned source point, (3,4).
Another approach, known as phrase-based extrac-
tion aims to remedy this situation by conditioning
the extraction of orientations on translation phrases
consistent with the alignment. In Figure 4 there is a
translation phrase that follows the phrase in question
? viz., , da? ? that ? and an orientation of M
is therefore tallied.
Regardless of the method of extraction, lexi-
calized reordering model statistics rely on exact
word-string pairs, (f, e), which can lead problems
with data sparsity. Moreover, even given ample
data, cross-phrasal reordering generalizations will
be missed. E.g., the fact that regnen ? rain has
orientation S w.r.t. the previous phrase pair does not
support the fact that other infinitival German verbs
should also behave similarly in relative clausal envi-
ronments.
To remedy this we might substitute abstract sym-
bols for each word in e, and train a syntactic bidirec-
tional MSD reordering model. For this we use CCG
supertags (cf. the single-word labels in the parse
214
chart in Figure 4), which are richly structured parts
of speech that describe their potential to combine
with other words (cf. Section 2.1). Given the same
phrase from Figure 4, we can estimate the proba-
bility of orientation S, given regnen ? S[b]\NP .
A further level of abstraction is to use CCG parse
charts packed with all derivations. The phrase
da? es ? that it can therefore be abstracted to
da? es ? S[em]/(S[dcl]\NP) (a ?that? clause
lacking a verb phrase to the right).
Except in cases of high ambiguity, the source
phrase effectively encodes the target phrase, mean-
ing that these extensions will suffer from data spar-
sity similarly to the baseline lexicalized model. We
therefore omit the source phrase in our syntactic
reordering models, estimating probability distribu-
tions p(O|LAB(e)) where LAB(e) is the syntactic la-
bel sequence derived from the chart (or supertagged
string, as the case may be) using the algorithm in
Figure 3.7 Orientations are determined using the
phrase-based extraction regime described in (Till-
mann, 2004), but statistics are tallied only for the
syntactic label sequence of the target string. More
precisely, for phrase pair (fi...j , ek...l), if a phrase
(fa...i, eb...k) exists in the alignment grid, an orien-
tation of M is assigned to LAB(ek...l) . Otherwise,
if a phrase (fj...p, el...m) exists in the alignment grid,
an orientation of S is assigned. In all other cases, an
orientation of D is assigned.
Using these statistics, we deploy target-side re-
ordering models, as described below.
4 Related Work
As noted, lexicalized reordering models can be
trained and configured in many different ways. In
addition to the standard word-based extraction (Ax-
elrod et al, 2005) and phrase-based extraction (Till-
mann, 2004) cases, more recent work has explored
using dynamic programming to extract and later
score orientations based on hierarchical configura-
tions of phrases consistent with an alignment (Gal-
ley and Manning, 2008). This means that the re-
ordering model can be conditioned on an unbounded
amount of context and can capture the fact that
7Note that a tagged string can be viewed as a very impover-
ished parse chart, and so the algorithm defined in Figure 3 can
be applied to the supertagging case as well.
many translations are monotonic w.r.t. the previ-
ously translated block, but are mistakenly identified
as having orientation S or D.
Su and colleagues (2010) observe that the space
of phrase pairs consistent with an alignment can
be viewed in its entirety, as a graph of phrases,
thereby collecting reordering statistics w.r.t. the en-
tire space of surrounding phrases. Ling and col-
leagues (2011) extend this approach by weighting
orientation counts with multiple scored alignments.
All of these more sophisticated reordering extrac-
tion approaches are compatible with the current ap-
proach, and could be straightforwardly applied to
our labelled target-side word strings.
Syntax-driven reordering approaches in phrase-
based MT abound, but, perhaps due to the incom-
patibility of phrase table entries and traditional syn-
tactic constituency, most research has avoided using
recursive target-side syntax during decoding. Till-
mann (2008) presents an algorithm that reorders us-
ing part-of-speech based permutation patterns dur-
ing the decoding process. Others have side-stepped
the issue by restructuring the source language be-
fore decoding to resemble the target language using
syntactic rules, either automatically extracted (Xia
and McCord, 2004), or hand-crafted (Collins et al,
2005; Wang et al, 2007; Xu and Seneff, 2008).
The flexibility of CCG syntax is also gaining
recognition as a useful tool for constraining statis-
tical MT decoders. Hassan (2009) describes an in-
cremental CCG parsing language model, although
his model does not beat a supertag factored PMT
approach. Almaghout and colleagues (2010) also
use a CCG chart to improve translation, augment-
ing SCFG rules by consulting the multiple deriva-
tions in the parse chart of Clark and Curran?s (2007)
CCG parser. We note two key differences to our
use of spurious ambiguity. First, they use a chart
packed with multiple dependency analyses, unlike
our spuriously ambiguous reworkings of the parser?s
single-best analysis. Second, the C&C parser re-
strains type-raising to a small number of possi-
bilities, thereby blocking many non-normal-form
derivations that we do not.
Two SCFG approaches that employ catego-
rial syntax that resembles CCG are the syntax-
augmented MT (SAMT) system described in (Venu-
gopal et al, 2007), and the target dependency lan-
215
guage model of of (Shen et al, 2008). (Venu-
gopal et al, 2007) uses a Penn Treebank-trained
CFG parser to label target strings and then re-
works the CFG parse trees, if needed,x to ac-
count for non-traditional constituents. This on-
demand reworking process, however, is bounded by
tree depth, and sometimes produces conjoined cat-
egories, rather than consistently produce the func-
tional ?slash? categories that a full CCG would ?
e.g., a subject + transitive verb string might some-
times be labelled NP+ V and other times S/NP .
The approach in (Shen et al, 2010) uses a simple
categorial grammar with only a single atomic sym-
bol ? i.e., every functional category has the form
C\X or C/X, where X is either C or another slash
category C\X or C/X. In contrast to these two ap-
proaches, the CCG parser we use is trained on a
CCG treebank that is the result of a carefully engi-
neered Penn Treebank-to-CCG conversion (Hocken-
maier and Steedman, 2007) and we impose no limits
on deriving categorial functional categories (X/Y).
We view our reworking of CCG charts as a poten-
tially useful extension to such approaches.
5 Experimental Results
We empirically validate our technique by translat-
ing from Urdu into English. Urdu has a canoni-
cal word order of SOV ? subject, object(s), verb
? whereas English has SVO, leading to indefinitely
long distances between corresponding verbs and ob-
jects. This language pair is therefore a strong test
case for a reordering model.
For decoding we use Moses (Koehn et al, 2007),
a state-of-the-art PMT decoder, with IRST LM (Fed-
erico and Cettolo, 2007) for language model infer-
ence. For Urdu-English parallel data, we use the
OpenMT 2008 training set which consists of 88
thousand sentence-level translations and a transla-
tion dictionary of ?114 thousand word and phrase
translations. We use half of the OpenMT 2008 Urdu-
English evaluation data for development and per-
form development testing on the other half. Both
halves are ?900 sentences long and were balanced
to contain approximately the same number of to-
kens. Our blind test set is the entire OpenMT 2009
Urdu-English evaluation set. All evaluation sets had
4 reference translations for each tuning or testing in-
stance. All system component weights were tuned
using minimum error-rate training (Och, 2003), with
three tuning runs for each condition. The data was
normalized, tokenized and the English sentences
were lowercased,8
As a baseline, we train a standard phrase-based
system with a bidirectional MSD lexicalized re-
ordering model using word-based extraction. Our
CCG-augmented reordering system has all of the
model components of the baseline, as well as a bidi-
rectional orientation reordering model over target-
side multiword syntactic labels. To directly test the
effect of using CCG parse charts ? as opposed to
simply using a CCG supertagger ? we also added a
CCG supertag bidirectional MSD reordering model
to the baseline set-up. All systems were tuned and
tested with distortion limit of 15 words, and test
runs were performed with and without 200-best min-
imum Bayes? risk (MBR) hypothesis selection (Ku-
mar and Byrne, 2004).
To acquire CCG labels for our English parallel
data, we use the C&C CCG toolkit of Clark and
Curran (2007). We build CCG parse charts by re-
working the normal-form derivations from the C&C
parser in all spuriously ambiguous ways, as de-
scribed in Section 2.1. For supertags, we tag with
the C&C supertagger. Rather than training sepa-
rate phrase tables for our CCG systems, however,
we instead decorate the baseline phrase tables with
CCG multiword labels or supertags. To smooth over
parsing and tagging errors, we only use those la-
bels whose relative frequency (rf) is sufficiently high
w.r.t. the most frequent label for that phrase pair
LAB*[f?e]. More precisely, for each phrase pair, we
use the set of labels:9
{LAB[f?e]|rf(LAB[f?e]) ? ? ? rf(LAB*[f?e])}
This is reminiscent of the ?-best tagging approach
of (Clark and Curran, 2004), but performed in a
batch process when creating the syntactic phrase ta-
bles (both supertag and CCG chart-derived). We set
8N.B. We use Penn Treebank III-compatible tokenization for
English and a specially designed tokenization script for Urdu,
cf. (Baker et al, 2010), Appendix C
9Recalling that ?31% of the time, a phrase pair might have
a list of labels, rather than a single label, the word ?label? here
refers to a single token that can be the concatenation of multiple
symbols.
216
DEVTEST (NIST-08) (MBR/NON-MBR) NIST-09 TEST (MBR/NON-MBR)
BLEU-4 METEOR TER LENGTH BLEU-4 METEOR TER LENGTH
LR 25.3/24.7 28.3/28.2 64.2/64.4 98.2/97.6 29.1/28.8 30.0/28.8 60.0/60.1 98.2/97.8
NO-LR 22.5/22.1 27.5/27.3 66.3/66.3 97.6/97.1 26.2/25.8 29.2/29.1 61.9/62.0 97.1/96.6
ST+LR 24.5/24.2 28.4/28.3 64.6/64.5 97.9/97.3 28.5/28.2 30.0/30.0 60.3/60.2 97.9/97.3
CCG+LR 25.6/25.2 28.7/28.5 64.3/64.5 98.7/98.1 29.1/29.2 30.1/30.2 59.5/59.8 97.4/97.9
Table 2: Case-insensitive BLEU-4, METEOR, TER and hypothesis/reference length ratio (LENGTH) for a lexicalized reordering
baseline (LR), a system with only a distance-based distortion model (NO-LR), a system with an additional CCG supertag reordering
model (ST+LR) and our system with an additional CCG chart-derived reordering model (CCG+LR). Systems were run with (left
of slash) and without (right of slash) 200-best-list MBR hypothesis selection. All boldfaced results were found to be significantly
better than the baseline at ? the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning
runs for each system. Non-boldfaced numbers are statistically indistinguishable from (or worse than) the baseline.
? = 0.5 in all of our CCG experiments.
To minimize disruption to the Moses decoder
(which only supports single-word labels in phrase-
based mode), we project multiword labels across the
words they label as single-word factors with book-
keeping characters, similar to the ?microtag? anno-
tations of asynchronous factored translation mod-
els (Cettolo et al, 2008). We modified to the de-
coder to reassemble the multiple single-word fac-
tors into a single label before querying the reorder-
ing model. As an example, we might have the phrase
pair le ve?lo rouge ? the|NP( red|NP+ bike|NP) .
Before querying the reordering model, the fac-
tor sequence NP( NP+ NP) is collapsed into the
single, multiword label ?NP? by the rule schema
X( . . . X+ . . . X) ? X.
We train a language model using all of the WMT
2011 NEWSCRAWL, NEWSCOMENTARY and EU-
ROPARL monolingual data,10 tokenized and lower-
cased as above, but de-duplicated to address the re-
dundancy of the Web-crawled portion of that data
set. We also train a separate language model on the
English portion of the Urdu-English parallel corpus
(minus the dictionary entries), and interpolate the
two models by optimizing perplexity on our tuning
set.
Table 2 lists our results, where we see significant
improvement over both of our baselines, lexicalized
reordering (LR) and supertag reordering plus lexi-
calized reordering (ST+LR). To test the effects of
the lexicalized reordering model itself, we also eval-
uate a system with no lexicalized reordering model
10http://www.statmt.org/wmt11/
translation-task.html
(only a distance-based distortion model). This last
system (a system which almost always prefers not
to reorder) is considerably worse than all other sys-
tems, demonstrating the need for non-monotonic
reordering configurations when accounting for the
Urdu-English data.
6 Analysis and Discussion
Our CCG system (CCG+LR) outperforms both
baseline systems (LR and ST+LR) in a majority of
metrics in both MBR and non-MBR conditions. We
see that, even though MBR decoding closes the per-
formance gap somewhat, our system continues to
match or outperform (if sometimes insignificantly)
in all areas. Note that the CCG+LR non-MBR
configuration outperforms both LR and ST+LR in
MBR and non-MBR decoding conditions in its ME-
TEOR score on the NIST-09 test set. We note also
that, in the NIST-09 test case, the CCG+LR sys-
tem?s poorer performance is perhaps due to a mis-
match in hypothesis length, which could be harming
its scores, particularly the BLEU brevity penalty.
6.1 Poor Performance of CCG Supertag Model
We have no firm explanation for the poor per-
formance of the CCG supertag model (ST-
LR), but it is important to note that the su-
pertag reordering model does not unify statis-
tics across phrases of different lengths, as the
CCG chart-derived model does. E.g., the
phrase pair den Weg fu?r eine ? the way for an
will query the CCG chart-derived reordering
model with the same symbol as the phrase pair
den Weg fu?r eine baldige ? the way for an early
217
twenty-seven year old abdullah britain blasts in hatcheries planning accused of is .
CCG+LR: twenty-seven year old abdullah is accused of planning hatcheries blasts in britain .
LR: twenty-seven years on charges of planning bombings hatcheries , abdullah in britain .
Reference 1: 27 years old abdullah is accused of planning explosions in britain .
Reference 2: twenty-seven years old abdullah is blamed for planning attacks in britain .
Reference 3: abdullah , 27 , has been blamed for planning the blasts in britain .
Reference 4: abdullah , 27 , has been blamed for planning the blasts in britain .
now musharraf resignation give should .
CCG+LR: now musharraf should give resignation .
LR: now musharraf resignation should be given .
Reference 1: now musharraf should resign .
Reference 2: now , musharraf should resign .
Reference 3: now , musharraf should resign .
Reference 4: musharraf should resign .
Figure 5: Sample devtest (NIST-08) translations of the median-performing tuned CCG syntactic reordering model
(CCG+LR) compared to the median-performing baseline lexicalized reordering model (LR).
? viz., NP/N. The CCG supertag model, how-
ever, will have two distinct label sequences for these
phrases ? viz., NP/N N (NP\NP)/NP NP/N and
NP/N N (NP\NP)/NP NP/N N/N, resp. ? both
of which could be reduced to the single label, NP/N,
using CCG?s syntactic combinators. The supertag
system does not have the means of relating the
reordering patterns of strings of symbols such as
this.11 Such data fragmentation may be leading to
decreased performance, which would indicate the
use of recursive CCG syntax.
6.2 Qualitative Improvements
In addition to improved metric scores, we noted real
qualitative improvements in some examples, as Fig-
ure 5 shows. These examples demonstrate the abil-
ity of the reordering model to navigate the massive,
structure-governed reorderings needed to approxi-
mate the correct answer with the phrase inventory
it is given.
11Its reordering table has more than twice as many entries as
that of the chart-derived model.
6.3 Comparison to the State of the Art
To our knowledge, the state of the art in Urdu-
English translation using the OpenMT data is
listed in the NIST OpenMT 2009 evaluation re-
sults (http://www.itl.nist.gov/iad/
mig/tests/mt/2009/ResultsRelease/
currentUrdu.html). This evaluation accepted
only single system outputs, and used cased refer-
ences. Therefore we had to choose a single system
output and recase its text.
For system selection, we picked the tuned sys-
tem that performed best on the development test
set. For recasing, we trained a lowercased-to-cased
monolingual phrase-based ?translation model? with
no reordering and a cased language model, similar to
what is described in (Baker et al, 2010). The train-
ing text is simply the non-dictionary portion of the
Urdu-English parallel corpus, with its lowercased
version as the source and the original cased text as
the target, both halves tokenized as above. We tuned
on a similar version of the English half of our tuning
218
references. The lowercased output of our system is
fed to this model and the first token of each casing
?translation? is capitalized (if not already).
The official metric of the NIST 2009 evaluation
is BLEU (as implemented in the NIST-distributed
mteval-v13a.pl script).12 The best-performing
system in the constrained data evaluation scored
0.312 w.r.t. the cased references, with the second
and third place systems scoring 0.2395 and 0.2322,
respectively.13 Our best performing MERT-tuned
system (as determined on the devtest data) scores
0.2734 on the test set, putting it between the top two
systems. For comparison, our devtest-best baseline
LR system scores 0.2683 on the test set.
While is generally not useful to test experimental
manipulations based on a single tuning run (Clark et
al., 2011) and with different monolingual language
modelling data, we note these figures simply to situ-
ate our results within the state of the art.
7 Conclusion
We have argued for the use of CCG in phrase-
based translation, due to its flexibility in providing
a wealth of different bracketings that better accom-
modate lexical translation strings. We have also pre-
sented a novel method for using CCG constituent la-
bels in a syntactic reordering model where the syn-
tactic labels span multiple words, do not cross trans-
lation constituent boundaries and are tailored specif-
ically to each translation constituent. The result is a
significant improvement in Urdu-English (SOV ?
SVO) translation scores over two baselines: a tra-
ditional phrase-based baseline with a lexicalized re-
ordering model and a phrase-based baseline with an
additional supertag reordering model. Moreover, we
have provided qualitative examples that confirm the
improvements in automatic metrics.
In future work we would like explore whether
further improvements can be gained by using more
sophisticated reordering models, such as reordering
graphs (Su et al, 2010) and hierarchical reordering
models (Galley and Manning, 2008) both for our
word-based and syntactic reordering models. Fur-
ther, as in prior work (Zollmann et al, 2006; Shen
12ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v13a-20091001.tar.gz.
13We exclude combination entries that are combinations of
multiple systems with different algorithmic approaches.
et al, 2010; Almaghout et al, 2010), our categorial
labels could also be used to derive CCG-augmented
SCFG rules, both lexicalized and unlexicalized, cf.
(Zhao and Al-onaizan, 2008) ? the latter being the
SCFG analogue of our current model.
Acknowledgments
The authors would like to thank Chong Min Lee,
Aoife Cahill and Nitin Madnani at ETS for taking
the time to read earlier drafts of this (and closely re-
lated) work. Their comments and suggestions made
this a better paper. We would also like to thank
the anonymous reviewers for their very helpful feed-
back. The views expressed in this paper do not nec-
essarily reflect those of The Ohio State University or
of Educational Testing Service.
References
Hala Almaghout, Jie Jiang, and Andy Way. 2010.
CCG Augmented Hierarchical Phrase-based Machine
Translation. In Proceedings of the 7th International
Workshop on Spoken Language Translation, Paris,
France.
Amittai Axelrod, Ra Birch Mayne, Chris Callison-Burch,
Miles Osborne, and David Talbot. 2005. Edin-
burgh System Description for the 2005 IWSLT Speech
Translation Evaluation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT-05), Pittsburgh, PA, USA.
Emmon Bach. l976. An Extension of Classical Transfor-
mational Grammar. In Proceedings of the 1976 Con-
ference on Problems of Linguistic Metatheory, pages
183?224, East Lansing, MI, USA.
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim
Mayeld, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2010. Semantically informed machine translation.
Technical Report 002, Johns Hopkins University, Bal-
timore, MD, Human Language Technology Center of
Excellence.
Mauro Cettolo, Marcello Federico, Daniele Pighin, and
Nicola Bertoldi. 2008. Shallow-syntax Phrase-based
Translation: Joint versus Factored String-to-chunk
Models. In Proceedings of AMTA 2008, Honolulu, HI,
USA.
Stephen Clark and James R. Curran. 2004. The Impor-
tance of Supertagging for Wide-Coverage CCG Pars-
ing. In Proceedings of the 20th International Con-
219
ference on Computational Linguistics (COLING-04),
Geneva, Switzerland.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better Hypothesis Testing for Ma-
chine Translation: Controlling for Optimizer Insta-
bility. In Proceedings of the Meeting of the Associ-
ation for Computational Linguistics (ACL-11), Port-
land, OR, USA.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of the Association for
Computational Linguistics (ACL-05), Ann Arbor, MI,
USA.
Marcello Federico and Mauro Cettolo. 2007. Efficient
Handling of n-gram Language Models for Statistical
Machine Translation. In Proceedings of Association
for Computational Linguistics, Prague, The Czech Re-
public.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model. In Proceedings of EMNLP-08.
Hany Hassan. 2009. Lexical Syntax for Statistical Ma-
chine Translation. Ph.D. thesis, Dublin City Univer-
sity, Dublin, Ireland.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of NAACL-HLT, pages 48?54, Edmonton, Al-
berta, CA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the Association for Computational Lin-
guistics, Companion Volume Proceedings of the Demo
and Poster Sessions, Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-Risk Decoding for Statistical Machine Transla-
tion. In Proceedings of HLT-NAACL.
Wang Ling, Jo ao Grac?a, David Martins de Matos, Is-
abel Trancoso, and Alan Black. 2011. Discrimi-
native Phrase-based Lexicalized Reordering Models
using Weighted Reordering Graphs. In Proceedings
of the 5th International Joint Conference on Natural
Language Processing.
Franz Joseph Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160?167, Sapporo,
Japan.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
New String-to-dependency Machine Translation Algo-
rithm with a Target Dependency Language Model. In
Proceedings of the Joint Meeting of the Association
for Computational Linguistics and Human Language
Technologies (ACL-08:HLT), Columbus, OH, USA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-Dependency Statistical Machine Translation.
Computational Linguistics, 36(4):649?671.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
Jinsong Su, Yang Liu, Yajuan Lu?, Haitao Mi, and Qun
Liu. 2010. Learning Lexicalized Reordering Models
from Reordering Graphs. In Proceedings of the ACL
2010; Short Papers.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ?04.
Christoph Tillmann. 2008. A Rule-Driven Dynamic Pro-
gramming Decoder for Statistical MT. In Proceedings
of the Second Workshop on Syntax and Structure in
Statistical Translation (SSST-08).
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An Efficient Two-pass Approach to
Synchronous-CFG Driven Statistical MT. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguistics
Conference (HLT/NAACL-07), Rochester, NY.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese Syntactic Reordering for Statistical Machine
Translation. In Proceedings of EMNLP/CoNLL-07,
Prague, The Czech Republic.
Fei Xia and Michael McCord. 2004. Improving a Statis-
tical MT System with Automatically Learned Rewrite
Patterns. In Proceedings of International Conference
on Computational Linguistics (COLING-04), Geneva,
Switzerland.
Yushi Xu and Stephanie Seneff. 2008. Two-stage Trans-
lation: A Combined Linguistic and Statistical Machine
Translation Framework. In Proceedings of the 8th
Conference of the Association for Machine Transla-
tion in the Americas (AMTA-08), Waikiki, Honolulu,
HI, USA.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-Based Statistical Machine Translation.
In M. Jarke, J. Koehler, and G. Lakemeyer, editors, KI-
2002: Advances in Artificial Intelligence, Proceedings
of the 25th Annual German Conference on AI, (KI-
2002), pages 18?32. Springer Verlag, Aachen, Ger-
many.
220
Bing Zhao and Yaser Al-onaizan. 2008. Generalizing
Local and Non-Local Word-Reordering Patterns for
Syntax-Based Machine Translation. In Proceedings
of The Conference on Empirical Methods in Natural
Language Processig (EMNLP-08).
Andreas Zollmann, Ashish Venugopal, Stephan Vogel,
and Alex Waibel. 2006. The CMU-UKA Syntax Aug-
mented Machine Translation System for IWSLT-06.
In Proceedings of International Workshop on Spoken
Language Translation (IWSLT-06), Kyoto, Japan.
221
