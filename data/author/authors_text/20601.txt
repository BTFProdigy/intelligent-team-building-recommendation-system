Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 25?28,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
CASMACAT: A Computer-assisted Translation Workbench
V. Alabau
?
, C. Buck
?
, M. Carl
?
, F. Casacuberta
?
, M. Garc??a-Mart??nez
?
U. Germann
?
, J. Gonz
?
alez-Rubio
?
, R. Hill
?
, P. Koehn
?
, L. A. Leiva
?
B. Mesa-Lao
?
, D. Ortiz
?
, H. Saint-Amand
?
, G. Sanchis
?
, C. Tsoukala
?
?
PRHLT Research Center, Universitat Polit`ecnica de Val`encia
{valabau,fcn,jegonzalez,luileito,dortiz,gsanchis}@dsic.upv.es
?
Copenhagen Business School, Department of International Business Communication
{ragnar.bonk,mc.isv,mgarcia,bm.ibc}@cbs.dk
?
School of Informatics, University of Edinburgh
{cbuck,ugermann,rhill2,pkoehn,hsamand,ctsoukal}@inf.ed.ac.uk
Abstract
CASMACAT is a modular, web-based
translation workbench that offers ad-
vanced functionalities for computer-aided
translation and the scientific study of hu-
man translation: automatic interaction
with machine translation (MT) engines
and translation memories (TM) to ob-
tain raw translations or close TM matches
for conventional post-editing; interactive
translation prediction based on an MT en-
gine?s search graph, detailed recording and
replay of edit actions and translator?s gaze
(the latter via eye-tracking), and the sup-
port of e-pen as an alternative input device.
The system is open source sofware and in-
terfaces with multiple MT systems.
1 Introduction
CASMACAT
1
(Cognitive Analysis and Statistical
Methods for Advanced Computer Aided Trans-
lation) is a three-year project to develop an
advanced, interactive workbench for computer-
assisted translation (CAT). Currently, at the end of
the second year, the tool includes an array of inno-
vative features that combine to offer a rich, user-
focused working environment not available in any
other CAT tool.
CASMACAT works in close collaboration with
the MATECAT project
2
, another open-source web-
based CAT tool. However, while MATECAT is
concerned with conventional CAT, CASMACAT is
focused on enhancing user interaction and facili-
tating the real-time involvement of human trans-
lators. In particular, CASMACAT provides highly
interactive editing and logging features.
1
http://www.casmacat.eu
2
http://www.matecat.com
Through this combined effort, we hope to foster
further research in the area of CAT tools that im-
prove the translation workflow while appealing to
both professional and amateur translators without
advanced technical skills.
GUI
web
server
CAT
server
MT
server
Javascript      PHP
    Python
  Python
web socket
HTTP
HTTP
Figure 1: Modular design of the workbench: Web-
based components (GUI and web server), CAT
server and MT server can be swapped out.
2 Design and components
The overall design of the CASMACAT workbench
is modular. The system consists of four com-
ponents. (1) a front-end GUI implemented in
HTML5 and JavaScript; (2) a back-end imple-
mented in PHP; (3) a CAT server that manages the
editing process and communicates with the GUI
through web sockets; (4) a machine translation
(MT) server that provides raw translation of source
text as well as additional information, such as a
search graph that efficiently encodes alternative
translation options. Figure 1 illustrates how these
components interact with each other. The CAT
and MT servers are written in Python and inter-
act with a number of software components imple-
mented in C++. All recorded information (source,
translations, edit logs) is permanently stored in a
MySQL database.
These components communicate through a
well-defined API, so that alternative implementa-
tions can be used. This modular architecture al-
25
Figure 2: Translation view for an interactive post-editing task.
lows the system to be used partially. For instance,
the CAT and MT servers can be used separately as
part of a larger translation workflow, or only as a
front-end when an existing MT solution is already
in place.
2.1 CAT server
Some of the interactive features of CASMACAT
require real-time interaction, such as interactive
text-prediction (ITP), so establishing an HTTP
connection every time would cause a significant
network overhead. Instead, the CAT server relies
on web sockets, by means of Python?s Tornadio.
When interactive translation prediction is en-
abled, the CAT server first requests a translation
together with the search graph of the current seg-
ment from the MT server. It keeps a copy of the
search graph and constantly updates and visualizes
the translation prediction based on the edit actions
of the human translator.
2.2 MT server
Many of the functions of the CAT server require
information from an MT server. This information
includes not only the translation of the input sen-
tence, but also n-best lists, search graphs, word
alignments, and so on. Currently, the CASMACAT
workbench supports two different MT servers:
Moses (Koehn et al., 2007) and Thot (Ortiz-
Mart??nez et al., 2005).
The main call to the MT server is a request for
a translation. The request includes the source sen-
tence, source and target language, and optionally
a user ID. The MT server returns an JSON object,
following an API based on Google Translate.
3 Graphical User Interface
Different views, based on the MATECAT GUI,
perform different tasks. The translation view is
the primary one, used when translating or post-
editing, including logging functions about the
translation/post-editing process. Other views im-
plement interfaces to upload new documents or to
manage the documents that are already in the sys-
tem. Additionally, a replay view can visualize all
edit actions for a particular user session, including
eye tracking information, if available.
3.1 Post-Editing
In the translation view (Figure 2), the document
is presented in segments and the assistance fea-
tures provided by CASMACAT work at the segment
level. If working in a post-editing task without
ITP, up to three MT or TM suggestions are pro-
vided for the user to choose. Keyboard shortcuts
are available for performing routine tasks, for in-
stance, loading the next segment or copying source
text into the edit box. The user can assign different
status to each segment, for instance, ?translated?
for finished ones or ?draft? for segments that still
need to be reviewed. Once finished, the translated
document can be downloaded in XLIFF format.
3
In the translation view, all user actions re-
lated to the translation task (e.g. typing activity,
mouse moves, selection of TM proposals, etc.) are
recorded by the logging module, collecting valu-
able information for off-line analyses.
3.2 Interactive Translation Prediction
Here we briefly describe the main advanced CAT
features implemented in the workbench so far.
Intelligent Autocompletion: ITP takes place
every time a keystroke is detected by the sys-
tem (Barrachina et al., 2009). In such event, the
system produces a prediction for the rest of the
sentence according to the text that the user has al-
ready entered. This prediction is placed at the right
of the text cursor.
Confidence Measures: Confidence mea-
sures (CMs) have two main applications in
3
XLIFF is a popular format in the translation industry.
26
MT (Gonz?alez-Rubio et al., 2010). Firstly, CMs
allow the user to clearly spot wrong translations
(e.g., by rendering in red those translations
with very low confidence according to the MT
module). Secondly, CMs can also inform the user
about the translated words that are dubious, but
still have a chance of being correct (e.g., rendered
in orange). Figure 3 illustrates this.
Figure 3: Visualisation of Confidence Measures
Prediction Length Control: Providing the user
with a new prediction whenever a key is pressed
has been proved to be cognitively demanding (Al-
abau et al., 2012). Therefore, the GUI just displays
the prediction up to the first wrong word according
to the CMs provided by the system (Figure 4).
Figure 4: Prediction Length Control
Search and Replace: Most of CAT tools pro-
vide the user with intelligent search and replace
functions for fast text revision. CASMACAT fea-
tures a straightforward function to run search and
replacement rules on the fly.
Word Alignment Information: Alignment of
source and target words is an important part of
the translation process (Brown et al., 1993). To
display their correspondence, they are hihglighted
every time the user places the mouse or the text
cursor on a word; see Figure 5.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figure 5: Visualisation of Word Alignment
Prediction Rejection: With the purpose of eas-
ing user interaction, CASMACAT also supports a
one-click rejection feature (Sanchis-Trilles et al.,
2008). This feature invalidates the current predic-
tion made for the sentence that is being translated,
and provides the user with an alternate one.
3.3 Replay mode and logging functions
The CASMACAT workbench implements detailed
logging of user activity data, which enables both
automatic analysis of translator behaviour and
retrospective replay of a user session. Replay
takes place in the translation view of the GUI
and it displays the screen status of the recorded
translation/post-editing process. The workbench
also features a plugin to enrich the replay mode
with gaze data coming from an eye-tracker. This
eye-tracking integration is possible through a
project-developed web browser extension which,
at the moment, has only been fully tested with SR-
Research EyeLinks
4
.
4 E-pen Interaction
E-pen interaction is intended to be a complemen-
tary input rather than a substitution of the key-
board. The GUI features the minimum compo-
nents necessary for e-pen interaction; see Figure 6.
When the e-pen is enabled, the display of the cur-
rent segment is changed so that the source seg-
ment is shown above the target segment. Then the
drawing area is maximised horizontally, facilitat-
ing handwriting, particularly in tablet devices. An
HTML canvas is also added over the target seg-
ment, where the user?s drawings are handled. This
is achieved by means of MINGESTURES (Leiva
et al., 2013), a highly accurate, high-performance
gesture set for interactive text editing that can dis-
tinguish between gestures and handwriting. Ges-
tures are recognised on the client side so the re-
sponse is almost immediate. Conversely, when
handwritten text is detected, the pen strokes are
sent to the server. The hand-written text recog-
nition (HTR) server is based on iAtros, an open
source HMM decoder.
if any feature not
is available on your network
substitution
Figure 6: Word substitution with e-pen interaction
5 Evaluation
The CASMACAT workbench was recently evalu-
ated in a field trial at Celer Soluciones SL, a
language service provider based in Spain. The
trial involved nine professional translators work-
ing with the workbench to complete different post-
editing tasks from English into Spanish. The pur-
4
http://www.sr-research.com
27
pose of this evaluation was to establish which of
the workbench features are most useful to profes-
sional translators. Three different configurations
were tested:
? PE: The CASMACAT workbench was used
only for conventional post-editing, without
any additional features.
? IA: Only the Intelligent Autocompletion fea-
ture was enabled. This feature was tested sep-
arately because it was observed that human
translators substantially change the way they
interact with the system.
? ITP: All features described in Section 3.2
were included in this configuration, except-
ing CMs, which were deemed to be not accu-
rate enough for use in a human evaluation.
For each configuration, we measured the aver-
age time taken by the translator to produce the fi-
nal translation (on a segment basis), and the aver-
age number of edits required to produce the final
translation. The results are shown in Table 1.
Setup Avg. time (s) Avg. # edits
PE 92.2 ? 4.82 141.39 ? 7.66
IA 86.07 ? 4.92 124.29 ? 7.28
ITP 123.3 ? 29.72 137.22 ? 13.67
Table 1: Evaluation of the different configurations
of the CASMACAT workbench. Edits are measured
in keystrokes, i.e., insertions and deletions.
While differences between these numbers are
not statistically significant, the apparent slowdown
in translation with ITP is due to the fact that all
translators had experience in post-editing but none
of them had ever used a workbench featuring in-
telligent autocompletion before. Therefore, these
were somewhat unsurprising results.
In a post-trial survey, translators indicated that,
on average, they liked the ITP system the best.
They were not fully satisfied with the freedom of
interactivity provided by the IA system. The lack
of any visual aid to control the intelligent auto-
completions provided by the system made transla-
tors think that they had to double-check any of the
proposals made by the system when making only
a few edits.
6 Conclusions
We have introduced the current CASMACAT work-
bench, a next-generation tool for computer as-
sisted translation. Each of the features available
in the most recent prototype of the workbench has
been explained. Additionally, we have presented
an executive report of a field trial that evaluated
genuine users? performance while using the work-
bench. Although E-pen interaction has not yet
been evaluated outside of the laboratory, it will the
subject of future field trials, and a working demon-
stration is available.
Acknowledgements
Work supported by the European Union 7
th
Framework Program (FP7/2007-2013) under the
CASMACAT project (grant agreement n
o
287576).
References
Vicent Alabau, Luis A. Leiva, Daniel Ortiz-Mart??nez,
and Francisco Casacuberta. 2012. User evaluation
of interactive machine translation systems. In Proc.
EAMT, pages 20?23.
Sergio Barrachina et al. 2009. Statistical approaches to
computer-assisted translation. Computational Lin-
guistics, 35(1):3?28.
Peter Brown et al. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational linguistics, 19(2):263?311.
Jes?us Gonz?alez-Rubio, Daniel Ortiz-Mart??nez, and
Francisco Casacuberta. 2010. On the use of confi-
dence measures within an interactive-predictive ma-
chine translation system. In Proc. of EAMT.
Philipp Koehn et al. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL,
pages 177?180.
Luis A. Leiva, Vicent Alabau, and Enrique Vidal.
2013. Error-proof, high-performance, and context-
aware gestures for interactive text edition. In Proc.
of CHI, pages 1227?1232.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2005. Thot: a toolkit to train
phrase-based statistical translation models. In Proc.
of MT Summit X, pages 141?148.
G. Sanchis-Trilles et al. 2008. Improving interactive
machine translation via mouse actions. In Proc. of
EMNLP, pages 485?494.
28
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 90?94,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Inference of Phrase-Based Translation Models
via Minimum Description Length
Jes?us Gonz
?
alez-Rubio and Francisco Casacuberta
Departamento de Sistemas Inform?aticos y Computaci?on
Universitat Polit`ecnica de Val`encia, Camino de Vera s/n, 46021 Valencia (Spain)
{jegonzalez, fcn}@dsic.upv.es
Abstract
We present an unsupervised inference pro-
cedure for phrase-based translation mod-
els based on the minimum description
length principle. In comparison to cur-
rent inference techniques that rely on
long pipelines of training heuristics, this
procedure represents a theoretically well-
founded approach to directly infer phrase
lexicons. Empirical results show that the
proposed inference procedure has the po-
tential to overcome many of the prob-
lems inherent to the current inference ap-
proaches for phrase-based models.
1 Introduction
Since their introduction at the beginning of the
twenty-first century, phrase-based (PB) transla-
tion models (Koehn et al., 2003) have become the
state-of-the-art for statistical machine translation
(SMT). PB model provide a big leap in translation
quality with respect to the previous word-based
translation models (Brown et al., 1990; Vogel et
al., 1996). However, despite their empirical suc-
cess, inference procedures for PB models rely on
a long pipeline of heuristics (Och and Ney, 2003)
and mismatched learning models, such as the long
outperformed word-based models. Latter stages
of the pipeline cannot recover mistakes or omis-
sions made in earlier stages which forces the indi-
vidual stages to massively overgenerate hypothe-
ses. This manifests as a huge redundancy in the
inferred phrase lexicons, which in turn largely pe-
nalizes the efficiency of PB systems at run-time.
The fact that PB models usually cannot generate
the sentence pairs in which they have been trained
in, or that it is even possible to improve the perfor-
mance of a PB system by discarding most of the
learned phrases are clear indicators of these defi-
ciencies (Sanchis-Trilles et al., 2011).
We introduce an unsupervised procedure to in-
fer PB models based on the minimum descrip-
tion length (MDL) principle (Solomonoff, 1964;
Rissanen, 1978). MDL, formally described in
Section 2, is a general inference procedure that
?learns? by ?finding data regularities?. MDL takes
its name from the fact that regularities allow to
compress the data, i.e. to describe it using fewer
symbols than those required to describe the data
literally. As such, MDL embodies a form of Oc-
cam?s Razor in which the best model for a given
data is the one that provides a better trade-off be-
tween goodness-of-fit on the data and ?complex-
ity? or ?richness? of the model.
MDL has been previously used to infer mono-
lingual grammars (Gr?unwald, 1996) and inversion
transduction grammars (Saers et al., 2013). Here,
we adapt the basic principles described in the lat-
ter article to the inference of PB models. The
MDL inference procedure, described in Section 3,
learns PB models by iteratively generalizing an
initial model that perfectly overfits training data.
An MDL objective is used to guide this process.
MDL inference has the following desirable prop-
erties:
? Training and testing are optimized upon the
same model; a basic principle of machine learn-
ing largely ignored in PB models.
? It provides a joint estimation of the structure
(set of bilingual phrases) and the parameters
(phrase probabilities) of PB models.
? It automatically protects against overfitting by
implementing a trade-off between the expres-
siveness of the model and training data fitting.
The empirical evaluation described in Section 4
focuses on understanding the behavior of MDL-
based PB models and their specific traits. That
is, in contrast to a typical PB system building pa-
per, we are not exclusively focused on a short
term boost in translation quality. Instead, we aim
at studying the adequacy and future potential of
MDL as inference procedure for PB models.
90
2 The MDL Principle
Given a set of data D, the MDL principle aims at
obtaining the simplest possible model ? that de-
scribes D as well as possible (Solomonoff, 1964;
Rissanen, 1978). Central to MDL is the one-
to-one correspondence between description length
functions and probability distributions that follows
from the Kraft-McMillan inequality (McMillan,
1956). For any probability distribution Pr(?), it
is possible to construct a coding scheme such that
the length (in bits) of the encoded data is mini-
mum and equal to? log
2
(Pr(D)). In other words,
searching for a minimum description length re-
duces to searching for a good probability distribu-
tion, and vice versa. Taking these considerations
into account, MDL inference is formalized as:
?
? = argmin
?
DL(?,D) (1)
= argmin
?
DL(?) + DL(D | ?) (2)
where DL(?) denotes the description length of
the model, and DL(D | ?) denotes the descrip-
tion length of the data given the model. A com-
plete introductory tutorial of the MDL principle
and methods can be found in (Gr?unwald, 2004).
3 MDL Phrase-Based Models
3.1 Description Length Functions
We start by defining how to compute DL(?) and
DL(D | ?) for any PB model and data set.
Let Pr
?
(D) be the probability of data set
D according to PB model ?. We follow the
Kraft-McMillan inequality and define the de-
scription length of the data given the model as
DL(D | ?) = ? log
2
(Pr
?
(D)), which it is the
lower bound for the description length of the data.
Regarding the description length of the PB
model, DL(?), we compute it by serializing ?
into a sequence of symbols and then computing
the length of the optimal encoding of such se-
quence. To do that, we need one symbol for each
word in the source and target languages, another
symbol to separate the source and target sides in
a phrase pair, and one additional symbol to dis-
tinguish between the different pairs in the phrase
lexicon. For example, the following toy PB model
La|||The casa|||house azul|||blue
is serialized as La|The?casa|house?azul|blue,
where symbol ? separates the phrase pairs, and |
separates the two sides of each pair. Assuming a
uniform distribution over the K different symbols,
each symbol would require ? log
2
(
1
K
) bits to en-
code. We will thus require 3 bits to encode each
of the 8 symbols in the example, and 33 bits to en-
code the whole serialized PB model (11 symbols).
3.2 Inference Procedure
We now describe how to perform the maximiza-
tion in Equation (2). In the case of PB models,
this reduces to a search for the optimal phrase lex-
icon. Obviously, an exhaustive search over all pos-
sible sets of phrase pairs in the data is unfeasible
in practice. Following the ideas in (Vilar and Vi-
dal, 2005), we implement a search procedure that
iteratively generalizes an initial PB model that per-
fectly fits the data. Let D = {f
n
, e
n
}
N
n=1
be a
data set with N sentence pairs, where f
n
are sen-
tences in the source language and e
n
are their cor-
responding translation in the target language. Our
initial PB model will be as follows:
f
1
||| e
1
? ? ? f
n
||| e
n
? ? ? f
N
||| e
N
where the probability of each pair is given by the
number of occurrences of the pair in the data di-
vided by the number of occurrences of the source
(or target) language sentence.
To generalize this initial PB model, we need
to identify parts of the existing phrase pairs that
could be validly used in isolation. As a result, the
PB model will be able to generate new transla-
tions different from the ones in the training data.
From a probabilistic point of view, this process
moves some of the probability mass which is con-
centrated in the training data out to other data still
unseen; the very definition of generalization. Con-
sider a PB model such as:
La casa azul|||The blue house
Esta casa azul|||This blue house
Esta casa verde|||This green house
It can be segmented to obtain a new PB model:
La|||The casa azul|||blue house
Esta|||This casa verde|||green house
which is able to generate one new sentence pair
(La casa verde?The green house) and has a
shorter description length (19 symbols) in compar-
ison to the original model (23 symbols). We only
consider segmentations that bisect the source and
target phrases. More sophisticated segmentation
approaches are beyond the scope of this article.
Algorithm 1 describes the proposed PB infer-
ence by iterative generalization. First, we col-
lect the potential segmentations of the current PB
91
Algorithm 1: Iterative inference procedure.
input : ? (initial PB model)
output : ?? (generalized PB model)
auxiliary : collect(?) (Returns the set of possible
segmentations of model ?)
?DL(s,?) (Returns variation in DL when
segmenting ? according to s)
sort(S) (Sorts segmentation set S by
variation in DL)
commit(S,?) (Apply segmentations in S
to ?, returns variation in DL)
begin1
repeat2
S ? collect(?);3
candidates? [];4
for s ? S do5
?
?
? ?DL(s,?);6
if ?
?
? 0 then7
candidates .append({?
?
, s});8
sort(candidates);9
?? commit(candidates,?);10
until ? > 0 ;11
return?;12
end13
model (line 3). Then, we estimate the variation in
description length due to the application of each
segmentation (lines 4 to 8). Finally, we sort the
segmentations by variation in description length
(line 9) and commit to the best of them (line 10).
Specifically, given that different segmentations
may modify the same phrase pair, we apply each
segmentation only if it only affect phrase pairs
unaffected by previous segmentations in S . The
algorithm stops when none of the segmentations
lead to a reduction in description length. Saers
et al., (2013) follow a similar greedy algorithm to
generalize inversion transduction grammars.
The key component of Algorithm 1 is function
?DL(s,?) that evaluates the impact of a candi-
date segmentation s on the description length of
PB model ?. That is, ?DL(s,?) computes the
difference in description length between the cur-
rent model ? and the model ?
?
that would result
from committing to s:
?DL(s,?) = DL(?
?
)?DL(?)
+ DL(D | ?
?
)?DL(D | ?) (3)
The length difference between the phrase lexi-
cons (DL(?
?
)?DL(?)) is trivial. We merely have
to compute the difference between the lengths of
the phrase pairs added and removed. The differ-
ence for the data is given by ? log
2
(
Pr
?
?
(D)
Pr
?
(D)
)
,
where Pr
?
?
(D) and Pr
?
(D) are the probability
of D according to ?
?
and ? respectively. These
EuTransI (Sp / En)
train tune test
#Sentences 10k 2k 1k
#Words 97k / 99k 23k / 24k 12k / 12k
Vocabulary 687 / 513 510 / 382 571 / 435
OOV ? / ? 0 / 0 0 / 0
Perplexity ? / ? 8.4 / 3.4 8.1 / 3.3
News Commentary (Sp / En)
train tune test
#Sentences 51k 2k 1k
#Words 1.4M / 1.2M 56k / 50k 30k / 26k
Vocabulary 47k / 35k 5k / 5k 8k / 7k
OOV ? / ? 390 / 325 832 / 538
Perplexity ? / ? 136.2 / 197.9 144.2 / 206.0
Table 1: Main figures of the experimental corpora.
M and k stand for millions and thousands of ele-
ments respectively. Perplexity was calculated us-
ing 5-gram language models.
probabilities can be computed by translating the
training data. However, this is a very expensive
process that we cannot afford to perform for each
candidate segmentation. Instead, we estimate the
description length of the data in closed form based
on the probabilities of the phrase pairs involved.
The probability of a phrase pair {
?
f, e?} is computed
as the the number of occurrences of the pair di-
vided by the number of occurrences of the source
(or target) phrase. We thus estimate the probabil-
ities in the segmented model ?
?
by counting the
occurrences of the replaced phrase pairs as occur-
rences of the segmented pairs. Let {
?
f
0
, e?
0
} be
the phrase pair we are splitting into {
?
f
1
, e?
1
} and
{
?
f
2
, e?
2
}. The direct phrase probabilities in ?
?
will
be identical to those in ? except that:
P
?
?
(e?
0
|
?
f
0
) = 0
P
?
?
(e?
1
|
?
f
1
) =
N
?
({
?
f
1
, e?
1
}) + N
?
({
?
f
0
, e?
0
})
N
?
(
?
f
1
) + N
?
({
?
f
0
, e?
0
})
P
?
?
(e?
2
|
?
f
2
) =
N
?
({
?
f
2
, e?
2
}) + N
?
({
?
f
0
, e?
0
})
N
?
(
?
f
2
) + N
?
({
?
f
0
, e?
0
})
where N
?
(?) are counts in ?. Inverse probabilities
are computed accordingly. Finally, we compute
the variation in data description length using:
Pr
?
?
(D)
Pr
?
(D)
?
P
?
?
(e?
1
|
?
f
1
) ? P
?
?
(e?
2
|
?
f
2
)
P
?
(e?
0
|
?
f
0
)
?
P
?
?
(
?
f
1
| e?
1
) ? P
?
?
(
?
f
2
| e?
2
)
P
?
(
?
f
0
| e?
0
)
(4)
92
EUtransI News Commentary
BLEU [%]
Size
BLEU [%]
Size
(tune/test) (tune/test)
SotA 91.6 / 90.9 39.1k 31.4 / 30.7 2.2M
MDL 88.7 / 88.0 2.7k 24.8 / 24.6 79.1k
Table 2: Size (number of phrase pairs) of the
MDL-based PB models, and quality of the gener-
ated translations. We compare against a state-of-
the-art PB inference pipeline (SotA).
For a segmentation set, we first estimate the new
model ?
?
to reflect all the applied segmentations,
and then sum the differences in description length.
4 Empirical Results
We evaluated the proposed inference procedure
on the EuTransI (Amengual et al., 2000) and the
News Commentary (Callison-Burch et al., 2007)
corpora. Table 1 shows their main figures.
We inferred PB models (set of phrase pairs and
their corresponding probabilities) with the training
partitions as described in Section 3.2. Then, we
included these MDL-based PB models in a con-
ventional log-linear model optimized with the tun-
ing partitions (Och, 2003). Finally, we generated
translations for the test partitions using a conven-
tional PB decoder (Koehn et al., 2007).
Table 2 shows size (number of phrase pairs) of
the inferred MDL-based PB models, and BLEU
score (Papineni et al., 2002) of their translations of
the tune and test partitions. As a comparison, we
display results for a state-of-the-art (SotA) PB sys-
tem (Koehn et al., 2007). These results show that
MDL inference obtained much more concise mod-
els (less than one tenth the number of phrases) than
the standard inference pipeline. Additionally, the
translations of the simple EuTransI corpus were of
a similar quality as the ones obtained by the SotA
system. In contrast, the quality of the translations
for News Commentary was significantly lower.
To better understand these results, Figure 1 dis-
plays the histogram of phrase lengths (number of
source words plus target words) of the SotA model
and the MDL-based model for the News Commen-
taries corpus. We first observed that the length of
the phrase pairs followed a completely different
distribution depending on the inference procedure.
Most of the phrase pairs of the MDL-based model
translated one source word by one target word
with an exponential decay in frequency for longer
phrase pairs; a typical distribution of events in nat-
 0
 10
 20
 30
 0  10  20  30  40  50  60  70  80
 
Length of the phrase pair (words)
R
e
l
a
t
i
v
e
 
f
r
e
q
u
e
n
c
y
 
[
%
] SotAMDL
Figure 1: Histogram of lengths (source plus target
words) for the phrase pairs in the inferred models.
ural language (Zipf, 1935). Longer phrase pairs,
about 45% of the total, contain sequences of words
that only appear once in the corpus, and thus, they
cannot be segmented in any way that leads to a re-
duction in description length. Although formally
correct, long phrase pairs generalize poorly which
explains the comparatively poor performance of
MDL inference for the News Commentaries cor-
pus. This problem was largely attenuated for Eu-
TransI due to its simplicity.
5 Conclusions and Future Developments
We have described a simple, unsupervised infer-
ence procedure for PB models that learns phrase
lexicons by iteratively splitting existing phrases
into smaller phrase pairs using a theoretically
well-founded minimum description length objec-
tive. Empirical results have shown that the in-
ferred PB models, far from the artificial redun-
dancy of the conventional PB inference pipeline,
are very parsimonious and provide competitive
translations for simple translation tasks.
The proposed methodology provides a solid
foundation from where to develop new PB infer-
ence approaches that overcome the problems in-
herent to the long pipeline of heuristics that nowa-
days constitute the state-of-the-art. Future devel-
opments in this direction will include:
? A more sophisticated segmentation procedure
that allow to divide the phrases into more that
two segments.
? A hybrid approach where the long phrase pairs
remaining after the MDL inference are further
segmented, e.g., according to a word lexicon.
? The inclusion of lexical models in the definition
of the PB model.
93
Acknowledgments
Work supported by the European Union 7
th
Framework Program (FP7/2007-2013) under the
CasMaCat project (grans agreement n
o
287576),
by Spanish MICINN under grant TIN2012-31723,
and by the Generalitat Valenciana under grant
ALMPR (Prometeo/2009/014).
References
Juan-Carlos Amengual, M. Asunci?on Casta?no, Anto-
nio Castellanos, V??ctor M. Jim?enez, David Llorens,
Andr?es Marzal, Federico Prat, Juan Miguel Vilar,
Jos?e-Miguel Bened??, Francisco Casacuberta, Mois?es
Pastor, and Enrique Vidal. 2000. The eutrans spo-
ken language translation system. Machine Transla-
tion, 15(1-2):75?103.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16:79?85.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Workshop on Statistical Machine
Translation, pages 136?158.
Peter Gr?unwald. 1996. A minimum description length
approach to grammar inference. Connectionist, Sta-
tistical, and Symbolic Approaches to Learning for
Natural Language Processing, pages 203?216.
Peter Gr?unwald. 2004. A tutorial introduc-
tion to the minimum description length principle.
http://arxiv.org/abs/math/0406077.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the Association for Computational
Linguistics, demonstration session, June.
Brockway McMillan. 1956. Two inequalities implied
by unique decipherability. IRE Transactions on In-
formation Theory, 2(4):115?116.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Meeting on Association for Computational Linguis-
tics, pages 160?167. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
Meeting on Association for Computational Linguis-
tics, pages 311?318. Association for Computational
Linguistics.
Jorma Rissanen. 1978. Modeling by shortest data de-
scription. Automatica, 14(5):465 ? 471.
Markus Saers, Karteek Addanki, and Dekai Wu. 2013.
Iterative rule segmentation under minimum descrip-
tion length for unsupervised transduction grammar
induction. In Statistical Language and Speech Pro-
cessing, volume 7978 of Lecture Notes in Computer
Science, pages 224?235. Springer.
Germ?an Sanchis-Trilles, Daniel Ortiz-Mart??nez, Jes?us
Gonz?alez-Rubio, Jorge Gonz?alez, and Francisco
Casacuberta. 2011. Bilingual segmentation for
phrasetable pruning in statistical machine transla-
tion. In Proceedings of the 15th Conference of the
European Association for Machine Translation.
Ray Solomonoff. 1964. A formal theory of inductive
inference, parts 1 and 2. Information and Control,
7:1?22, 224?254.
Juan Miguel Vilar and Enrique Vidal. 2005. A recur-
sive statistical translation model. In Proceedings of
the ACL Workshop on Building and Using Parallel
Texts, pages 199?207.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics, pages 836?841.
George Kingsley Zipf. 1935. The Psychobiology of
Language. Houghton-Mifflin.
94
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 322?328,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
FBK-UPV-UEdin participation in the WMT14 Quality Estimation
shared-task
Jos
?
e G. C. de Souza
?
University of Trento
Fondazione Bruno Kessler
Trento, Italy
desouza@fbk.eu
Jes?us Gonz
?
alez-Rubio
?
PRHLT Group
U. Polit`ecnica de Val`encia
Valencia, Spain
jegonzalez@prhlt.upv.es
Christian Buck
?
University of Edinburgh
School of Informatics
Edinburgh, Scotland, UK
cbuck@lantis.de
Marco Turchi, Matteo Negri
Fondazione Bruno Kessler
turchi,negri@fbk.eu
Abstract
This paper describes the joint submission
of Fondazione Bruno Kessler, Universitat
Polit`ecnica de Val`encia and University of
Edinburgh to the Quality Estimation tasks
of the Workshop on Statistical Machine
Translation 2014. We present our submis-
sions for Task 1.2, 1.3 and 2. Our systems
ranked first for Task 1.2 and for the Binary
and Level1 settings in Task 2.
1 Introduction
Quality Estimation (QE) for Machine Translation
(MT) is the task of evaluating the quality of the
output of an MT system without reference transla-
tions. Within the WMT 2014 QE Shared Task four
evaluation tasks were proposed, covering both
word and sentence level QE. In this work we de-
scribe the Fondazione Bruno Kessler (FBK), Uni-
versitat Polit`ecnica de Val`encia (UPV) and Uni-
versity of Edinburgh (UEdin) approach and sys-
tem setup for the shared task.
We developed models for two sentence-level
tasks: Task 1.2, scoring for post-editing effort,
and Task 1.3, predicting post-editing time, and
for all word-level variants of Task 2, binary and
multiclass classification. As opposed to previous
editions of the shared task, this year the partici-
pants were not supplied with the MT system that
was used to produce the translation. Furthermore
no system-internal features were provided. Thus,
while the trained models are tuned to detect the
errors of a specific system the features have to be
generated independently (black-box).
2 Sentence Level QE
We submitted runs to two sentence-level tasks:
Task 1.2 and Task 1.3. The first task aims at
?
Contributed equally to this work.
predicting the Human mediated Translation Edit
Rate (HTER) (Snover et al., 2006) between a sug-
gestion generated by a machine translation sys-
tem and its manually post-edited version. The
data set contains 1,104 English-Spanish sentence
pairs post-edited by one translator (896 for train-
ing and 208 for test). The second task requires
to predict the time, in milliseconds, that was re-
quired to post edit a translation given by a ma-
chine translation system. Participants are provided
with 858 English-Spanish sentence pairs, source
and suggestion, along with their respective post-
edited sentence and post-editing time in seconds
(650 data points for training and 208 for test). We
participated in the scoring mode of both tasks.
2.1 Features
For our sentence-level submissions we compute
features using different resources that do not use
the MT system internals. We use the same set of
features for both Task 1.2 and 1.3.
QuEst Black-box features (quest79). We ex-
tract 79 black-box features that capture the com-
plexity, fluency and adequacy aspects of the QE
problem. These features are extracted using the
implementation provided by the QuEst framework
(Specia et al., 2013). Among them are the 17 base-
line features provided by the task organizers.
The complexity features are computed on the
source sentence and indicate the complexity of
translating the segment. Examples of these fea-
tures are the language model (LM) probabilities
of the source sentence computed in a corpus of the
source language, different surface counts like the
number of punctuation marks and the number of
tokens in the source sentence, among others.
The fluency features are computed over the
translation generated by the MT system and in-
dicate how fluent the translation is in the target
322
language. One example would again be the LM
probability of the translation given by a LM model
trained on a corpus of the target language. Another
example is the average number of occurrences of
the target word within the target segment.
The third aspect covered by the QuEst features
is the adequacy of the translation with respect to
the source sentence, i.e., how the meaning of the
source is preserved in the translation. Examples of
features are the ratio of nouns, verbs and adjectives
in the source and in the translation. For a more
detailed description of the features in this group
please refer to (Specia et al., 2013).
Word alignment (wla). Following our last
year?s submission (de Souza et al., 2013a) we ex-
plore information about word alignments to ex-
tract quantitative (amount and distribution of the
alignments) and qualitative features (importance
of the aligned terms). Our assumption is that
features that explore what is aligned can bring
improvements to tasks where sentence-level se-
mantic relations need to be identified. We train
the word alignment models with the MGIZA++
toolkit (Gao and Vogel, 2008) implementation of
the IBM models (Brown et al., 1993). The models
are built on the concatenation of Europarl, News
Commentary, and MultiUN parallel corpora made
available in the QE shared task of 2013, compris-
ing about 12.8 million sentence pairs. A more de-
tailed description of the 89 features extracted can
be found in (de Souza et al., 2013a; de Souza et
al., 2013b).
Word Posterior Probabilities (wpp). Using an
external SMT system we produce 100k-best lists
from which we derive Word Posterior Probabili-
ties as detailed in Subsection 3.1.
We use the geometric mean of these probabili-
ties to derive a sentence-level score.
Because the system that we use to produce the
N-best list is not the same that generated the sug-
gestions some suggested words never appear in the
N-best list and thus receive zero probability. To
overcome this issue we first clip the WPPs to a
minimum probability. Using a small sample of the
data to estimate this number we arrive at:
log(p)
min
= ?2.
N-best diversity (div). Using the same 100k-
best list as above we extract a number of measures
that grasp the spatial distribution of hypotheses in
the search space as described in (de Souza et al.,
2013a).
Word Prediction (wpred). We introduce the
use of the predictions provided by the word-level
QE system described in Section 3 to leverage in-
formation for the sentence-level tasks. We com-
bine the binary word-level predictions in different
ways, with the objective of measuring the fluency
of the translation in a more fine-grained way. We
target a quantitative aspect of the words by com-
puting ratios of OK or BAD predictions. Further-
more, we also explore a qualitative aspect by cal-
culating ratios of different classes of words given
by their part-of-speech tags, indicating the qual-
ity of distinct meaningful regions that compose the
translation sentence. In total, we compute 18 fea-
tures:
? number of OK predictions divided by the no.
of words in the translation sentence (1 fea-
ture);
? number of OK function/content words predic-
tions divided by the no. of function/content
words in the translation (2 features);
? number of OK nouns, verbs, proper-nouns,
adjective, pronouns predictions divided by
the total nouns, verbs, proper-nouns, adjec-
tive, pronouns (5 features);
? size of the longest sequence of OK/BAD word
predictions divided by the total number of
OK/BAD predictions in the translation (2 fea-
tures);
? number of OK predicted n-grams divided by
the total number of n-grams in the transla-
tion. We vary n from 2 to 5 (4 features);
? number of words predicted as OK in the
first/second half of the translation divided by
the total number of words in the first/second
half of the translation (2 features).
? number of words predicted as OK in the
first/second quarter of the translation di-
vided by the total number of words in the
first/second quarter of the translation (2 fea-
tures).
For some instances of the sentence-level tasks
we were not able to produce word-level predic-
tions due to an incomplete overlap between the
word-level and sentence-level tasks datasets. For
such data points we use the median of the feature
column for Task 1.2 and the mean for Task 1.3.
323
Method Features Train T1.2 Train T1.3 Test T1.2 Test T1.3
SVR baseline 16.90 16864 15.23 21490
ET baseline 16.25 17888 17.73 19400
ET quest79 + wla + wpp 15.62 17474 14.44 18658
ET quest79 + wla + wpp + div
2
15.57 17471 14.38 18693
ET quest79 + wla + wpp + div + wpred
1
15.05 16392 12.89 17477
Table 1: Training and test results for Task 1.2 and 1.3. Scores are the MAE on a development set
randomly sampled from the training data (20%). Baseline features were provided by the shared task
organizers. We used Support Vector Machines (SVM) regression to train the baseline models (first row).
Submissions are marked with
1
and
2
for primary and secondary, respectively.
2.2 Experimental Setup
We build the sentence-level models for both tasks
(T1.2 and T1.3) with the features described in Sec-
tion 2.1 using one learning algorithm: extremely
randomized trees (ET) (Geurts et al., 2006). ET is
an ensemble of randomized trees in which each
decision tree can be parameterized differently.
When a tree is built, the node splitting step is done
at random by picking the best split among a ran-
dom subset of the input features. All the trees
are grown on the whole training set and the re-
sults of the individual trees are combined by aver-
aging their predictions. The models produced by
this method demonstrated to be robust to a large
number of input features. For our experiments and
submissions we used the ET implementation in-
cluded in the Scikit-learn library (Pedregosa et al.,
2011).
During training we evaluate the models on a
development set. The development set was ob-
tained by randomly sampling 20% of the training
data. The remaining 80% were used for training.
The training process was carried out by optimiz-
ing the ET hyper-parameters with 100 iterations
of random search optimization (Bergstra and Ben-
gio, 2012) set to minimize the mean absolute er-
ror (MAE)
1
on 10-fold cross-validation over the
training data. The ET hyper-parameters optimized
are: the number of decision trees in the ensemble,
the maximum number of features to consider when
looking for the best split, the maximum depth of
the trees used in the ensembles, the minimal num-
ber of samples required to split a node of the tree,
and the minimum number of samples in newly cre-
ated leaves. For the final submissions we run the
random search with 1000 iterations over the whole
training dataset.
1
Given by MAE =
?
N
i=1
|H(s
i
)?V (s
i
)|
N
, where H(s
i
) is
the hypothesis score for the entry s
i
and V (s
i
) is the gold
standard value for s
i
in a dataset with N entries.
2.3 Results
We train models on different combinations of fea-
ture groups (described in Section 2.1). Experi-
ments results are summarized in Table 1. We have
results with baseline features for both SVR and the
ET models. For Task 1.2, adding features from dif-
ferent groups leads to increasing improvements.
The combination of the quest79, wla and wpp
groups outperforms the SVR baseline for Task 1.2
but not for Task 1.3. However, when compared
to the ET model trained with the baseline fea-
tures, it is possible to observe improvements with
this group of features. In addition, adding the
div group on top of the previous three leads to
marginal improvements for both tasks. The best
feature combination is given when adding the fea-
tures based on the word-level predictions, config-
uring the combination of all the feature groups to-
gether (a total of 221 features). For both tasks
this is our primary submission. The contrastive
run for both tasks is the best feature group com-
bination without the word-prediction-based fea-
tures, quest79, wla, wpp and div for Task 1.2 and
quest79, wla, wpp for Task 1.3.
Results on the test set can be found in the two
last columns of Table 1 and are in line with what
we found in the training phase. The rows that do
not correspond to the official submissions and that
are reported on the test set are experiments done
after the evaluation phase. For both tasks the im-
provements increase as we add features on top of
the baseline feature set and the best performance
is reached when using the word prediction fea-
tures with all the other features. The SVR base-
lines performance are the official numbers pro-
vided by the organizers. For Task 1.2 our primary
submission achieves a MAE score lower than the
score achieved during the training phase, show-
ing that the model is robust. For Task 1.3, how-
ever, we do not observe such trend. Even though
324
the primary submission for this task consistently
improves over the other feature combinations, it
does not outperform the score obtained during the
training phase. This might be explained due to
the difference in the distribution between train-
ing and test labels. In Task 1.2 the two distri-
butions are more similar than in Task 1.3, which
presents slightly different distributions between
training and test data.
3 Word-Level QE
Task 2 is the word-level quality estimation of auto-
matically translated news sentences without given
reference translations. Participants are required to
produce a label for each word in one or more of
the following settings:
Binary classification: a OK/BAD label, where
BAD indicates the need for editing the word.
Level1 classification: OK, Accuracy, or
Fluency label specifying a coarser level of
errors for each word, or OK for words with
no error.
Multi-Class classification: one of the 20 error la-
bels described in the shared-task description
or OK for words with no error.
We submit word-level quality estimations for
the English-Spanish translation direction. The cor-
pus contains 1957 training sentences for a total of
47411 Spanish words, and 382 test sentences for a
total of 9613 words.
3.1 Features
Word Posterior Probabilities (WPP) In order
to generate an approximation of the decoder?s
search space as well as an N-best list of possi-
ble translations we re-translate the source using
the system that is available for the 2013 WMT QE
Shared Task (Bojar et al., 2013).
Certainly, there is a mismatch between the orig-
inal system and the one that we used but, since our
system was trained using the same news domain
as the QE data, we assume that both face similar
ambiguous words or possible reorderings. Using
this system we generate a 100k-best list which is
the foundation of several features.
We extract a set of word-level features based on
posterior probabilities computed over N-best lists
as proposed by previous works (Blatz et al., 2004;
Ueffing and Ney, 2007; Sanchis et al., 2007).
Consider a target word e
i
belonging to a transla-
tion e = e
1
. . . e
i
. . . e
|e|
generated from a source
sentence f . Let N (f) be the list of N-best trans-
lations for f . We compute features as the nor-
malized sum of probabilities of those translations
S(e
i
) ? N (f) that ?contain? word e
i
:
1
?
e
??
?N (f)
P(e
??
| f)
?
e
?
?S(e
i
)
P(e
?
| f) (1)
where P(e | f) is the probability translation e given
source sentence f according to the SMT model.
We follow (Zens and Ney, 2006) and extract
three different WPP features depending on the
criteria chosen to compute S(e
i
):
S(e
i
) = {e
?
? N (f) | a=Le(e
?
, e)?e
?
a
i
= e
i
}
S(e
i
) contain those translations e
?
for which the
word Levenshtein-aligned (Levenshtein, 1966) to
position i in e is equal to e
i
.
S(e
i
) = {e
?
? N (f) | e
?
i
= e
i
}
A second option is to select those translations
e
?
that contain the word e
i
at position i.
S(e
i
) = {e
?
? N (f) | ?i
?
: e
?
i
?
= e
i
}
As third option, we select those translations e
?
that contain the word e
i
, disregarding its position.
Confusion Networks (CN) We use the same N-
best list used to compute the WPP features in the
previous section to compute features based on the
graph topology of confusion networks (Luong et
al., 2014). First, we Levenshtein-align all trans-
lations in the N-best list using e as skeleton, and
merge all of them into a confusion network. In this
network, each word-edge is labelled with the pos-
terior probability of the word. The output edges of
each node define different confusion sets of words,
each word belonging to one single confusion set.
Each complete path passing through all nodes in
the network represents one sentence in the N-best
list, and must contain exactly one link from each
confusion set. Looking to the confusion set which
the hypothesis word belongs to, we extract four
different features: maximum and minimum proba-
bility in the set (2 features), number of alternatives
in the set (1 feature) and entropy of the alternatives
in the set (1 feature).
Language Models (LM) As language model
features we produced n-gram length/backoff be-
haviour and conditional probabilities for every
word in the sentence. We employed both an inter-
polated LM taken from the MT system discussed
325
in Section 3 as well as a very large LM which we
built on 62 billion tokens of monolingual data ex-
tracted from Common Crawl, a public web crawl.
While generally following the procedure of Buck
et al. (2014) we apply an additional lowercasing
step before training the model.
Word Lexicons (WL) We compute two dif-
ferent features based on statistical word lexi-
cons (Blatz et al., 2004):
Avg. probability:
1
|f |+1
?
|f |
j=0
P(e
i
| f
j
)
Max. probability: max
0?j?|f |
P(e
i
| f
j
)
where P(e | f) is a probabilistic lexicon, and f
0
is
the source ?NULL? word (Brown et al., 1993).
POS tags (POS) We extract the part-of-speech
(POS) tags for both source and translation sen-
tences using TreeTagger (Schmid, 1994). We use
the actual POS tag of the target word as a feature.
Specifically, we represent it as a one-hot indicator
vector where all values are equal to zero except
the one representing the current tag of the word,
which is set to one. Regarding the source POS
tags, we first compute the lexical probability of
each target word given each source word. Then,
we compute two different feature vectors for each
target word. On the one hand, we use an indica-
tor vector to represent the POS tag of the maxi-
mum probability source word. On the other hand,
we sum up the indicator vectors for all the source
words each one weighted by the lexical probability
of the corresponding word. As a result, we obtain
a vector that represents the probability distribution
of source POS tags for each target word. Addi-
tionally, we extract a binary feature that indicates
whether the word is a stop word or not.
2
Stacking (S) Finally, we also exploit the diverse
granularity of the word labels. The word classes
for the Level1 and Multi-class conditions are fine
grained versions of the Binary annotation, i.e. the
OK examples are the same for all cases.
We re-use our binary predictions as an addi-
tional feature for the finer-grained classes. How-
ever, due to time constrains, we were not able to
run the proper nested cross-validation but used a
model trained on all available data, which there-
fore over-fits on the training data. Cross-validation
results using the stacking approach are thus very
optimistic.
2
https://code.google.com/p/stop-words/
3.2 Classifiers
We use bidirectional long short-term memory
recurrent neural networks (BLSTM-RNNs) as
implemented in the RNNLib package (Graves,
2008). Recurrent neural networks are a connec-
tionist model containing a self-connected hidden
layer. The recurrent connection provides informa-
tion of previous inputs, hence, the network can
benefit from past contextual information. Long
short-term memory is an advanced RNN archi-
tecture that allows context information over long
periods of time. Finally, BLSTM-RNNs com-
bine bidirectional recurrent neural networks and
the long short-term memory architecture allowing
forward and backward context information. Us-
ing such context modelling classifier we can avoid
the use of context-based features that have been
shown to lead to only slight improvements in QE
accuracy (Gonz?alez-Rubio et al., 2013).
As a secondary binary model we train a CRF.
Our choice of implementation is Pocket CRF
3
which, while currently unmaintained, implements
continuous valued features. We use a history of
size 2 for all features and perform 10-fold cross-
validation, training on 9 folds each time.
3.3 Experimental Setup
The free parameters of the BLSTM-RNNs are op-
timized by 10-fold cross-validation on the train-
ing set. Each cross-validation experiment con-
sider eight folds for training, one held-out fold
for development, and a final held-out fold for test-
ing. We estimate the neural network with the eight
training folds using the prediction performance in
the validation fold as stopping criterion. The re-
sult of each complete cross-validation experiment
is the average of the results for the predictions of
the ten held-out test folds. Additionally, to avoid
noise due to the random initialization of the net-
work, we repeat each cross-validation experiment
ten times and average the results. Once the opti-
mal values of the free parameters are established,
we estimate a new BLSTM-RNN using the full
training corpus and we use it as the final model
to predict the class labels of the test words.
Since our objective is to detect words that need
to be edited, we use the weighted averaged F
1
score over the different class labels that denote an
error as our main performance metric (wF1
err
).
We also report the weighted averaged F
1
scores
3
http://pocket-crf-1.sourceforge.net/
326
Binary Level1 MultiClass
Method Features wF1
err
wF1
all
wF1
err
wF1
all
wF1
err
wF1
all
BLSTM-RNNs LM+WPP+CN+WL 35.9 63.0 23.7 59.4 10.7 55.5
+POS 38.5
1
62.7 26.7
1
59.5 12.7
1
55.5
+Stacking ? ? 82.9
2
93.9 64.7
2
88.0
CRF LM+WPP+CN+WL+POS 39.5
2
62.4 0 ? ? ? ?
Table 2: Cross-validation results for the different setups tested for Task 2. Our two submissions are
marked as (
1
) and (
2
) respectively.
over all the classes (wF1
all
).
3.4 Results
Table 2 presents the wF1
err
and wF1
all
scores
for different sets of features. Our initial experi-
ment includes language model (LM), word poste-
rior probability (WPP), confusion network (CN),
and word lexicon (WL) features for a total of 11
features. We extend this basic feature set with the
indicator features based on POS tags for a total of
163 features. We further extend the feature vectors
by adding the stacking feature in a total of 164 fea-
tures.
Analyzing the results we observe that prediction
accuracy is quite low. Our hypothesis is that this is
due to the skewed class distribution. Even for the
binary classification scenario (the most balanced
of the three conditions), OK labels account for two
thirds of the samples. This effect worsens with in-
creasing number of error classes and the resulting
sparsity of observations. As a result, the system
tends to classify all samples as OK which leads to
the low F
1
scores presented in Table 2.
We can observe that the use of POS tags indica-
tor features clearly improved the prediction accu-
racy of the systems in the three conditions. This
setup is our primary submission for the three con-
ditions of task 2.
In addition, we observe that the use of the stack-
ing feature provides a considerable improvement
in prediction accuracy for Level1 and MultiClass.
As discussed above the cross-validation results for
the stacking features are very optimistic. Test pre-
dictions using this setup are our contrastive sub-
mission for Level1 and MultiClass conditions.
Results achieved on the official test set can be
found in Table 3. Much in line with our cross-
validation results the stacking-features prove help-
ful, albeit by a much lower margin. For the bi-
nary task the RNN model strongly outperforms the
CRF.
Setup Binary Level1 MultiClass
BLSTM-RNN 48.7 37.2 17.1
+ Stacking ? 38.5 23.1
CRF 42.6 ? ?
Table 3: Test results for Task 2. Numbers are
weighted averaged F
1
scores (%) for all but the
OK class.
4 Conclusion
This paper describes the approaches and system
setups of FBK, UPV and UEdin in the WMT14
Quality Estimation shared-task. In the sentence-
level QE tasks 1.2 (predicting post-edition effort)
and 1.3 (predicting post-editing time, in ms) we
explored different features and predicted with a
supervised tree-based ensemble learning method.
We were able to improve our results by explor-
ing features based on the word-level predictions
made by the system developed for Task 2. Our best
system for Task 1.2 ranked first among all partici-
pants.
In the word-level QE task (Task 2), we explored
different sets of features using a BLSTM-RNN as
our classification model. Cross-validation results
show that POS indicator features, despite sparse,
were able to improve the results of the baseline
features. Also, the use of the stacking feature pro-
vided a big leap in prediction accuracy. With this
model, we ranked first in the Binary and Level1
settings of Task 2 in the evaluation campaign.
Acknowledgments
This work was supported by the MateCat and Cas-
macat projects, which are funded by the EC un-
der the 7
th
Framework Programme. The authors
would like to thank Francisco
?
Alvaro Mu?noz for
providing the RNN classification software.
327
References
James Bergstra and Yoshua Bengio. 2012. Random
Search for Hyper-Parameter Optimization. Journal
of Machine Learning Research, 13:281?305.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proceedings of
the international conference on Computational Lin-
guistics, pages 315?321.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19:263?311.
Christian Buck, Kenneth Heafield, and Bas van Ooyen.
2014. N-gram Counts and Language Models from
the Common Crawl. In Proceedings of the Lan-
guage Resources and Evaluation Conference.
Jos?e G. C. de Souza, Christian Buck, Marco Turchi,
and Matteo Negri. 2013a. FBK-UEdin participation
to the WMT13 Quality Estimation shared-task. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 352?358.
Jos?e G. C. de Souza, Miquel Espl?a-Gomis, Marco
Turchi, and Matteo Negri. 2013b. Exploiting qual-
itative information from automatic word alignment
for cross-lingual nlp tasks. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 771?776.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3?42.
Jes?us Gonz?alez-Rubio, Jos?e R. Navarro-Cerdan, and
Francisco Casacuberta. 2013. Partial least squares
for word confidence estimation in machine transla-
tion. In 6th Iberian Conference on Pattern Recog-
nition and Image Analysis, (IbPRIA) LNCS 7887,
pages 500?508. Springer.
Alex Graves. 2008. Rnnlib: A recurrent neural
network library for sequence learning problems.
http://sourceforge.net/projects/
rnnl/.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707?710.
Ngoc-Quang Luong, Laurent Besacier, and Benjamin
Lecouteux. 2014. Word confidence estimation and
its integration in sentence quality estimation for ma-
chine translation. In Knowledge and Systems Engi-
neering, volume 244, pages 85?98. Springer.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn : Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Alberto Sanchis, Alfons Juan, and Enrique Vidal.
2007. Estimation of confidence measures for ma-
chine translation. In Proceedings of the Machine
Translation Summit XI, pages 407?412.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12, pages 44?49.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Association for Machine Translation in
the Americas.
Lucia Specia, Kashif Shah, Jos?e G. C. de Souza, and
Trevor Cohn. 2013. QuEst?a translation quality es-
timation framework. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics, pages 79?84.
Nicola Ueffing and Hermann Ney. 2007. Word-
level confidence estimation for machine translation.
Computational Linguistics, 33:9?40.
Richard Zens and Hermann Ney. 2006. N-gram poste-
rior probabilities for statistical machine translation.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 72?77.
328
