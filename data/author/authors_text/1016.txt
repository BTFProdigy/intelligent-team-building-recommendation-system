Real iz ing Express ions of Doubt  in Col laborat ive Dia logue * 
Leah Schroeder  and Sandra  Carber ry  
Computer  and In format ion  Sciences 
Un ivers i ty  of De laware  
Newark ,  DE  19716 
{schroede,  carber ry}@cis .ude l .edu  
Abst ract  
One way to 1)egin a negotiation subdialogue is to ex- 
press doubt at a proposition. However, expressions 
of doubt occur in a variety of forms, each of which 
conveys information about the nature of the doubt 
that is important for the subsequent resolution of 
the conflict. This paper presents our work on real- 
izing expressions of doubt appropriately in natural 
language dialogues. 
1 Introduction 
Participants in a collaborative natural anguage di- 
alogue must develop mutual beliefs about what is 
said, what is meant, and the implications for the task 
at hand. We may think of each utterance as a pro- 
posed change to the agents' common ground (Clark, 
1996). Since autonomous agents enter the dialogue 
with differing domain, world, and personal knowl- 
edge, it is inevitable that some beliefs conveyed by 
an utterance will not be accepted because they con- 
flict with existing beliefs of the agent. However, it 
is also the case that these conflicting belief~ will not 
necessarily result in rejection of the proposed beliefS, 
but in subdialogues to negotiate a modification that 
is acceptable to both agents(Chu-Carroll and Car- 
berry, 1995). One w~y to begin such a subdialoguc 
is to express doubt ~t the beliefs proposed by an 
utterance. In the following example, the boldface 
utterance is expressing doubt at the previous utter- 
ance 1 (Transcripts, 1982)2: 
H: and - there's no reason  why you shouldn't havc 
an ira for last yr 
J: well i thought they just s tar ted  this yr 
* This work was supported by NSF grant ~GER-9354869 
and #CDA-9703088 
1Throughout his paper I use the phrase "doubt at an 
utterance" in place of "doubt at a proposition conveyed or 
implied by an utterance." I do not mean the utterance itself 
is somehow doubted, but that the utterance introduced the 
object of doubt into the dialogue. It may be the case that the 
agent is doubting a proposition expressed in the uttermme, or 
doubting the optimality of, or ability to execute, an action 
suggested in the utterance. 
2All of the examples in this paper, except where otherwise 
noted, are from this source. 
H: oh no .  ira's were available as long as you are 
not a participant in an czisting pension 
An e?pression of doubt is an utterance that con- 
veys uncertain disbelief in a proposition that was 
introduced in an earlier utterance. An expression of 
doubt signals that the speaker does not accept the 
utterance at which she is expressing doubt, but she 
is neither expressing a "neutral" attitude toward it 
nor rejecting it with certainty 3. In the above e?am- 
ple, J cammt be said to be rejecting the proposal 
outright, because her response indicates that she is 
uncertain in her disbelief. 
A natural language system must be able to ex- 
press doubt, particularly in cases where it; has in- 
complete or uncertain knowledge. Exmnination of 
natural anguage corpora shows that expressions of 
doubt may be realized in a variety of forms. Further- 
more, the fbrm of the utterance conveys information 
about the nature of the doubt that is important br 
the subsequent resolution of the conflict. Thus a 
collaborative natural anguage system must be able 
to generate utterances that convey doubt naturally 
and effectively. This paper presents our work on re- 
alizing expressions of doubt appropriately in natural 
language dialogues. 
2 Prev ious  Work  
In Chu-Carroll and Carberry (1998) the collabo- 
rative planning process is modeled as a Propose- 
Evaluate-Modify cycle, in which an agent is able to 
detect conflicts in belief and initiate collaborative 
negotiation subdialogues to attempt o resolve the 
conflicts. They use a modified version of Galliers 
belief revision inechanisln(Galliers, 1992; Logan et 
al., 1994) to determine whether to accept a proposi- 
tion and in determining which conflicting beliefs to 
use to refute an utterance that is not accepted. How- 
ever, their work does not address how an exi)ression 
of doubt should be realized in a natural language 
utterance. 
Vander Linden and Di Eugenio (Vander Linden 
and Di Eugenio, 1996) studied negative imperatives 
3Absolute rejection may be expressed as doubt for the sake 
of politeness. We do not address that issue here. 
740 
in instructional texts. They used machine learn- 
lug to correlate features of an action X's relation- 
ship to the reader in terms of attention, awareness, 
and safety, with whether it was realize(t as Don't X, 
Never X, or Take care not to X. In our research, we 
draw on their notion of identifying how features of 
the generation context correlate with how an utter- 
ance should be expressed. However, our work differs 
Dora theirs in that we must deal with an agent's be- 
lie?s motiw~ting his doubt and we consider a wider 
range of variations in realization. 
3 Communicat ing  an  Express ion  o f  
Doubt  
We assume appropriate mechmfisms for detecting 
conflict and determining when to engage in a sub- 
dialogue by expressing doul)t(Chu-Carroll and Car- 
berry, 1998), as well as an approi)riate belief revision 
mechmfism, and in this paper concentrate on how an 
expression of doubt should be realized as an utter- 
anee. A cooperative agent should be as informative 
as ne.eded, without expressing too much irreleva.nt 
intbnnation(Grice, 1975). Thus, in formulating an 
expression of doubt, we must consider how much the 
doubted ageut needs to know in order to collaborate 
in resolving the doubt and how much we can ex- 
pect him to infer without being exl)lieitly told. In 
addition, Clark (1.996) argues that particiI)ants in 
discourse select; utterances that express their eom- 
muni(:ative intent efficiently, oft, ca in sentence flag- 
meats. Since such efficiency of ezpression is the ex- 
pected natural form of discourse, a hearer is likely 
to (terive unintended imt)lications from significantly 
less economical realizations. 
Expressions of doubt, by our detinition, signal 
nonacceptance because of 'unccrl, ain disbelief. In 
order for the doubted agent to attemt)t o collab- 
orate in resolving the doubt, he needs to know sev- 
eral things. Most basically, he nee(is to recognize 
that there is doubt in a particular utterance. In 
the absence of an objection to an utterance, the 
speaker will assmne an implicit acceptance(Lmnbert 
and Carberry, 1999). To efficiently negotiate an 
acceptable resolution to the belief conflict, ideally 
the doubted agent must know something about the 
1)ellen of the doubtiug agent; in particular, which 
belief(s) are causing her nonacceptance, and the 
strength of these beliefS. If the doubted agent de- 
cides to retain his original beliet:s, this information 
helps him to construct an argument hat will be 
maximally effective and efficient in his attempt to 
convince the doubting agent(Chu-Carroll and Car- 
berry, 1998). 
To i(lentifs~ how expressions of (loubt are realized 
in naturally occurring dialogue and how tiles(; re- 
alizations convey the requisite beliet's, we analyzed 
features of individual ext)ressions of doubt extracted 
from natural corpora, and correlated the various 
forms of the utterances with the features of the un- 
derlying beliefs, t{owever, as explained ill Section 
3.3, the use of machine learning techniques was not 
apt)ropriate due to the nature of our corpus. Sec- 
tion 3.1 discusses features of underlying beliefs and 
Section 3.2 discusses the various forms that an ex- 
pression of doubt can take. Section 3.3 then presents 
it set of rules that relate the two. 
3.1 Bel ie f  features  
As noted above, beliefs play a prominent role in 
expressions of doubt, since a speaker will ideally 
convey enough intbrmation tbr the hearer to dis- 
eern 1) that she is expressing doubt, 2) what she is 
doubting, 3) any support she has tbr the doubt, and 
4) the strength of this supi)ort. In addition, speak- 
ers tend to differentiate new SUl)porting int'ornla- 
lion from that which is already part of the COlIlliIOU 
ground aud which should already have been consid- 
ered. These beliefs are often IIot explicitly stated, 
but are assumed to be inferable by the doubted agent 
based on his knowledge of the previous dialogue, 
knowledge of the other agent's belieN, a model of 
stereotypical beliefs, linguistic knowledge, and the 
particular ealization of the doubting agent's utter- 
alice. 
For example, consider the following assertion and 
two possible responses, each expressing doubt at; 
the prot)osition P, to~,a that John Smith gets $800 
a month fl'om Social Seeurityd: 
S: ,loh, n Smith, (lets $800 a month in Social Security. 
1) U: Isn't h,e less th, an 62 yem's old? 
2) U: $800'? 
In 1) U relies on illutua,1 donmin knowh:dge to ex- 
press (hml)t at Pdo,,bt by contending some ()tiler 
propos i t ion  Pi that implies -,P, to~,bt(Lamllert and 
Carberry, 1999), namely that Slnith is younger than 
62. In the rest of this paper, P, to,,a refers to the 
doubted proposition and Pi to a proposition other 
than Pao,,bt, if any, that is the reason for this doubt. 
In addition, expectations also play a role in ex- 
pressions of doubt. In the simplest case, the violated 
expectation is just that I~to~,bt is fiflse. In other sit- 
uations, an agent may have an expectation that a 
proposition will be false if instmltiated with some 
particular subset of its possible instantiations. Re- 
sponses that conflict with these expectations may 
provoke an expression of doubt, even though the 
doubting agent may have little or no support for 
the ext)ectation. Such violated expectations are of- 
tel1 signaled by elliptical fragments, such as response 
2) above where U conveys not oIfly that she doesn't 
4This is not a naturally occurring example, but was made 
up for exl)lanatory lmrposes. 
741 
accept Pdo~tbt, but also that her doubt steins from 
tile instantiation of tile amount term as 8800. We 
hypothesize that U might accept a t)roposition with 
a different instantiation of tile amount term, lint U 
doesn't explicitly state this, and other instantiations 
may be irrelevant. A violated expectation will be 
referred to as PC and is described further in Sec- 
tion 3.1.2. When and how these expectations arise 
is a topic for fllture research. 
We assume the t)ropositions Pao~,l,t, Pc, and Pi, 
as well as the fact that we want to express doubt, 
as inputs to our generation process. Note that ev- 
ery expression of doubt will be associated with some 
Pdoubt and solne Pc, since for every expression of 
doubt, there must be some doubted proposition and 
some inconsistency between the doubting agent's ex- 
pectations and belief in Pdoubt. There may or may 
not be an associated Pi, depending on the doubt- 
ing agent's beliet~ supporting ~Pdo,,bt. Lack of a Pi 
indicates that tile agent's belief in ~P(to,tbt is unsup- 
ported ~. 
Based on the information that a speaker will ide- 
ally convey when expressing doubt (as discussed at 
the beginning of this section), we hypothesize that 
the following aspects of a speaker's beliefs are sig- 
nificant factors in how an expression of doubt is re- 
alized. 
3.1.1 Features Assoc ia ted  w i th  P,,o,a,t 
Endorsement  of  Pdo,a,t: Refers to the authority 
behind the asserted proi)osition, which imi)acts the 
strength of tlm hearer's doubt(Chu-Carroll and Car- 
berry, 1998) 
? Expert - The information is coming from a do- 
main expert, or coming from someone with first- 
hand knowledge (including personal preferences). 
? Reliable - The agent suggesting the proposition 
is not an expert, trot is considered a generally knowl- 
edgeable source. 
? Questionable - hffbrmation thai; doesn't come 
fl'om an expert or reliable agent, or that is stated 
uncertainly by such an agent. 
3.1.2 Features Assoc ia ted  w i th  P,, 
Pc. feature:  /2~ refers to a violated expectation. In 
the following, we identify three kinds of expectations 
that may be violated by an assertion. For illustra- 
tive purposes, assume that S has made the following 
assertion: 
S: The most you will 9et back on your taxes is $~00. 
? Term-value: 
Pe = False(-I~to,,bt, _term, _value) 
5AIthough human agents may generally be able to offer 
soine weak supt)orl, for their l)ellefs, it is possible, depend- 
ing on the belief revision system used, to have no support ing 
evidence for a belief (Logan et al, 1994). 
Tile doubting agent may fail to accept -P, lo,bt with 
_term instantiated to _value, due to an expectation 
that _value is not one of tile instantiations of _term 
that would make -Pdo,,~t true. For example, the 
hearer of the above assertion by S may have ex- 
pected a much larger vahm than $400, with little 
or no support for this expectation. 
? Constraint: 
P~ = F alse(-P~toubt , _term, _value, _constraint) 
The doubting agent may fail to accept -\]~lo~,bt due 
to an expectation that -Pdo,,~t will be false when 
_term is instantiated with _value, in situations in 
which _constraint holds. This constraint is not a 
term in -Pdo,,~t, lint tile doubting agent believes that 
the speaker of -l~zo,,~t intends that the constraint 
hold. For example, the hearer of the above assertion 
by S may believe that S means $400 for the whole 
3,ear, but may have expected a larger amount unless 
S was referring to, say, quarterly taxes. 
? General: 
P~ = False(-P~lo,bt) 
The doubting agent may fail to accept l%~o,,~t in its 
entirety without having a specific objection to any 
particular term in tile prol)osition. 
3.1.3 Features Assoc ia ted  with Pi ~ 
Commona l i ty  of  Pi refers to tile source of the 
doubting agent's conflicting belief, if any. 
? Old - A prior conflicting belief is already i)art of 
tile explicit common ground of tile dialogue. 
? New - The doubting agent doesn't believe, that 
her conflicting belief is already part of the common 
ground estat)lished t)y the preceding dialogue. 
Endorsement  of  Pi refers to the strength of evi- 
dence supporting the belief 1~ that is in eonfiiet with 
the doubted belief. The endorsements are listed here 
from strongest to weakest. 
? First-hand - Belief is a personal t)reference or 
sometlfing directly experienced. 
? Expert - Belief supported 1)y expert testimony, 
or thought o be common knowledge among experts 
in tlfis domain. 
? Reliable - Belief conmmnicated from someone 
who, while not an expert, is generally considered 
a knowledgeable source of information. 
? Default - Belief believed to be common knowl- 
edge, in tim sense that the speaker strongly believes 
it and strongly believes that others who belong to 
a certain community (namely one which she has in 
common with the other dialogue agent) believe it as 
well. 
awe make the simpl i fying assumpt ion that  only one such 
proposition has been identified for use in an expression of 
doubt, as this is 1;he case in all of the expressions of doubt 
we encountered in our corpus. We leave consideration of ex- 
pressing mult iple l~ 's  in one utterance for fllture work. 
742 
? Derived - Belief is (leriv('.d froln other 1)e.liefs in 
such a way that  it is considered strong. 
? I ly l )othesis - The 1)elief is derived fl'om other be- 
liefs in such a way that  it; is considered weak. This 
category includes beliefs derived from analogy with 
another belief in a similar 1)roposition. 
? None - The belief is unsupported.  
Endorsement  o f  hnp l i ca t ion  ret>rs to the 
strength of evidence SUl)l)Orl;ing the belief that  Pi 
being true ilnl)lies thai; Pdo~,bt is not l;rue. The en- 
dorsements are l isted from strongest o weakest. Wc 
assume the salne definitions as the category above 
and that  the two lists lie on the same strength 
scale. That  is, a.n iml)l ication endorsed as rel iable 
is the same streng(;h as a P+ endorsed as rel iable and 
stronger t lmn a I} endorsed as hypothesis.  The only 
addit ion to this list; is the Logical (,Jldorsement to ac- 
(:omd; for instance.s in which P,~,,,,a can I)e logically 
deduced from Pi. 
? l ,ogieal - ~P,l<,,,bl dircc(,ly inferred from I}. 
? F i rst-han( l  r
? ExI)ert 
? Rel iable 
o l )efault  
? l )erived 
? l Iypothesis  
? NOlle 
3.2 Form features  
Expressions of (lollbl; ()c(:ur in a variol;y of forlllS. We 
dis(Anguish l;holn actor( l ing to l;h0, surfa(:(~ form o17 l;h(~ 
lll;1Ler~tllce~ tim t)l'eSc, n(:(~ of two clue wor(ls, and ~he 
sl)ecifi('ii;y of tim informal;ion conve, ye(1. 
Sur face  Form 
? Surface Negative Question - " Isn't  that, only 
worth what  someone will I)ay ti)r it?" This (;ai;(~gory 
also includes negative tag (tuestions. 
? Simple in ter rogat ive-  "Can I join the \[RA when 
i am 657" 
? S tatement  as Question - "I must tile a return?" 
t n s category also in(:ludes ellit)ti(:al fragments u('h 
as "$4007" 
? Siml)le declarat ion - "I calculated 10." 
? Prot)osit ion within a belief ('lause - "I thought 
they only started this year." 
C lue  word  
? Bill; 
? Even ~hough 
7Tho, (lUO, stion of how l\[lllCh experience is lmedo, d to lO~tl'll 
a Imlief in ;m iml)lical,ion is I)eyond the scope of this 1)a,l)er. 
Spec i f i c i ty  - General forms of the expression (;all 
1)e more or less specific in ti le amount of intbrnmtion 
COil\Toyed. 
? Generic: Sentence that  is a general question of 
the previous utterance.  
h. you .still -you have to file a state income tax 
return as well 
j. i do? 
? Repetit ion:  Rel)etit ion of a phrase from i)revious 
utterance.  
h. OK, what I 'd likc you to do is lend h, im the 20 
thousand. 
1. 20 thousand?  
? l /e i )et i t ion+ : l~,et)etitioil of t)hrase froln 1)revious 
utterance t)lus new intbrmat ion 
h. rigM,, the maz imum amount of credit that you 
will b(', able to get will be $/~00 on their taz rot'am 
e. $400 for the  who le  year?  
? Contradict;: Presentat ion of a 1)citer t, hat  iinl)li(!s 
the negat ion of \])do,,bt 
h. and th, crc's no reason why you shouldn't have 
an ira for last yr 
j. we l l  i thought  they  jus t  s ta r ted  th i s  yr  
? Contra(l icl ;+Source: Pre, senta.tion of a (:ontra(lic- 
l,()ry lmlieJ' and th(' sour(',e of that  1)elief. 
h had told j he nlust t)ay tax on his $6256 
j. ram. h, arry another th, in.q. i have  the  in terna l  
uh  revenue uh  ask  you about  that  6256 
$ uh  s ince  i have  the  fund  he sa id  no!  ,so 
wh, at do i do now? 
? Exp l ic i t+Contrad ic t :  ExI)licit s tatement  of dis- 
belief, followed by a contradictory  belief. 
b. well ah, h,c uh, ... h,c belongs to a money mar- 
kct )t'nd now and uh, th, ey will do that \[invest 
it in govt securities as part of thcir individual 
retirement accou'nt\] for him 
h. i 'm not  so sure  o f  i t . .  they  may move it  
ah  in(;() a in to  a govt  secur i t ies  fund ,  1)ut 
i 'm not  so sure  that  they  can  move it in to  
ind iv idua l  secur i t ies  - check  that  out  
3.3 Rea l i z ing  an  Express ion  of  Doubt  
Many of the exi)ressions of doubt in our COlI)tlS are 
non-ideal,  t)ecause they were not recognized as doul)t 
or because information that  was not included in 
t, he utterance,  l int could have been, was u l t imately  
needed to resolve the doul)t. Thus it was not al)l)ro- 
pr iate to use the corlms as training data  tbr machine 
learning. Consequently, ti le following rules are l)ased 
743 
and implication beliefs that would have caused tile 
form of expression of doubt to be generated. We also 
encouraged subjects to write in beliet's which were 
not inchlded, but none (lid. Out of the 60 instances 
(ten expressions of doubt times six subjects), tim 
subjects five times chose beliefs that we did not rep- 
resent as contributing to the doubt and three times 
failed to recognize a belief that (lid contribute. 
The subjects also rated the beliefs according to 
strength. We evaluated these ratings to see if the 
communicated strengths were correlated with the 
endorsements of beliefs that would have generated 
this form. Since subjects varied in the ranges that 
I;hey used in rating the strengths of the beliefs, we 
looked at tile scores relative to each subjects ratings 
of the other beliefs. 
Most of the strength ratings were consistent with 
the rules. The most fi'equent inconsistency was the 
case in which we would have generated a form based 
on slightly different endorsements for Pi and the in> 
plication, but our subjects rated them equivalent 
strengths. While it may be the case that tmotfle 
don't actually perceive a difli;renee, it may also be 
the case that numerical ratings don't fully capture 
the same information that t, he notion of endorse- 
ments do. 
The only significant inconsistency with our rules 
was one utterance ill which doubt was expressed by 
"1 thought that, but my husl)and, lie wasn't sure if 
that just uleant ss pension." We had represented the, 
husband as a relial/le source an(l t;hus generated ill- 
formatioll about the, source of the conflicting propo 
sit;ion. In this instance, the doul)t was not judged 
very strong l)y our subje(:ts, and tlm agent's t)eli(~f 
in her husband's exl)ertise as relatively weak. In 
future work, we will further explore exi)ression:s of 
doubt for which it is imi)ortant o (:omnmnicate the 
som'(:e of inforlnation. 
\Ve consider this a l)reliminary (;valuation to show 
that the rules we have l'ormulated thus fitr are re;> 
sortable, l?urther evahlation will tie neexle, d to pro- 
vide cvitlem:e that subjects really do draw &J.\[('rcnt 
inferences based on the different forms of exl)ression 
and that our rules accurately captnr(. ~ these ditti?r- 
eUCeS. 
5 Conc lus ion  and  Future  work  
This pape.r has 1)resented rules that could be used 
by a natural language system to realize exl)ressions 
of doubt. We have identified sew;ral forms that are 
used to express doubt ill naturally occurring dia- 
logues. Our rules correlate these forms to 1)eliet~ 
of the doul)ting agent, takiug into consideration the 
l)eliefs that must be conveyed tbr the utterance to 
be a successflll exl)ression of doubt. 
Preliininary evaluation shows that the belief fea- 
ture values in our rules correspond to hmnan sul> 
jects' intuitions about tile strength of tile doubt. In 
addition, the beliefs that would generate ach t:onn 
are consistent with the belief~ that the subjects at- 
trilmte to tile doubting agent when that form was 
used.  
Future work will e(meentrate on refining the fea- 
tures and exploring more explicit reasoning about 
tile beliefs of the other agent. We also plan to ex- 
plore the role of intonation in realizing expressions 
of doubt. 
Re ferences  
Jennifer Chu-Carroll and Sandra Carberry. 1995. 
Comnmnieation for conflict resolution iu multi- 
agent collaborative planning. In Proceedings of 
the .Inter'national CoT@rcncc on Multi-Agent Sys- 
tems, pages 49 56. 
Jennifer Chu-.Carroll and Saudra Carberry. 1998. 
Collaborative response generation in planning di- 
alogues. Computational Linguistics, 24(3):355 
400. 
Iterbert Clark. 1996. Using Langua9('. Cambridge 
University Press. 
Julia. Rose Galliers. 1992. Autonomous belief revi- 
sion and communic.ation, in P. Gardenf'or.% esli- 
tor, Belief Revision, Cambridge tracts ill theoreti- 
cal colnlmter science. Cambridg(; University Press, 
Cambridge, England. 
H. Paul Grice. 1975. Logic and Conversation. In 
P. Cole and J. L. Morgan, editors, Synta:c and ,%- 
ma'u, tic.s IIl: Speech Acts, pages 4 t  58, N.Y. Aca- 
demic Press. 
Lynn Lambert and Sandra Carberry. 1999. A pro- 
tess model for recognizing communicative acts 
and modeling negol, iation subdialogues. Co'mpu- 
tatio'nal \]Ang'aistics , 25:1 54. 
Brian Logan, St, even II.c, ece, Allison Cawsey, Julia 
Galliers, and Karen Sparck .lones. :1994. Belief 
revision and dialogue management in int'ormat;iou 
retrieval. T(.'chnicaI report, University of (Jan> 
bridge Computer lmboratory. 
ra  , ra  ~q * Harry Gross lranscrlpts. 1982. \[ran, crq)ts derived 
Kern tapes of the radio talk show harry gross: 
Streaking of your money. Provided by the l)epl;. 
of Comlmter Science at the Universii,y of Pemlsyi-- 
vania. 
Keith Vmlder Linden and Barbara l)i Eugenio. 1996. 
A corpus study of negative imlleratives in natm'ai 
language instruci;ionso In Proceedings of the 15th. 
International Confl'.rcnce on Uomputat, io'n, al Lin- 
gusitics (COLINU-96)~ Copenhagen. 
746 
Proceedings of the 43rd Annual Meeting of the ACL, pages 223?230,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Exploring and Exploiting the Limited Utility of Captions in Recognizing
Intention in Information Graphics?
Stephanie Elzer1 and Sandra Carberry2 and Daniel Chester2 and Seniz Demir2 and
Nancy Green3 and Ingrid Zukerman4 and Keith Trnka2
1Dept. of Computer Science, Millersville University, Millersville, PA 17551
2Dept. of Computer Science, University of Delaware, Newark, DE 19716
3Dept. of Mathematical Sciences, Univ. of NC at Greensboro, Greensboro, NC 27402
4School of CS & Software Engrg, Monash Univ., Clayton, Victoria 3800 Australia
Abstract
This paper presents a corpus study that ex-
plores the extent to which captions con-
tribute to recognizing the intended mes-
sage of an information graphic. It then
presents an implemented graphic interpre-
tation system that takes into account a va-
riety of communicative signals, and an
evaluation study showing that evidence
obtained from shallow processing of the
graphic?s caption has a significant impact
on the system?s success. This work is part
of a larger project whose goal is to provide
sight-impaired users with effective access
to information graphics.
1 Introduction
Language research has posited that a speaker or
writer executes a speech act whose intended mean-
ing he expects the listener to be able to deduce, and
that the listener identifies the intended meaning by
reasoning about the observed signals and the mutual
beliefs of author and interpreter (Grice, 1969; Clark,
1996). But as noted by Clark (Clark, 1996), lan-
guage is more than just words. It is any ?signal? (or
lack of signal when one is expected), where a sig-
nal is a deliberate action that is intended to convey a
message.
Although some information graphics are only in-
tended to display data values, the overwhelming ma-
jority of the graphics that we have examined (taken
?Authors can be reached via email as fol-
lows: elzer@cs.millersville.edu, nlgreen@uncg.edu,
{carberry, chester, demir, trnka}@cis.udel.edu, In-
grid.Zukerman@infotech.monash.edu.au.
1998 1999 2000 20011000
1500
2000
2500
3000
personal filingsLocal bankruptcy
Figure 1: Graphic from a 2001 Local Newspaper
from newspaper, magazine, and web articles) ap-
pear to have some underlying goal or intended mes-
sage, such as the graphic in Figure 1 whose com-
municative goal is ostensibly to convey the sharp in-
crease in local bankruptcies in the current year com-
pared with the previous decreasing trend. Applying
Clark?s view of language, it is reasonable to presume
that the author of an information graphic expects the
viewer to deduce from the graphic the message that
the graphic was intended to convey, by reasoning
about the graphic itself, the salience of entities in
the graphic, and the graphic?s caption.
This paper adopts Clark?s view of language as any
deliberate signal that is intended to convey a mes-
sage. Section 3 investigates the kinds of signals used
in information graphics. Section 4 presents a cor-
pus study that investigates the extent to which cap-
tions capture the message of the graphic, illustrates
the issues that would arise in trying to fully under-
stand such captions, and proposes shallow process-
ing of the caption to extract evidence from it. Sec-
tion 5 then describes how evidence obtained from
a variety of communicative signals, including shal-
low processing of the graphic?s caption, is used in a
probabilistic system for hypothesizing the intended
message of the graphic. Section 6 presents an eval-
223
10
 5
15
0?680+ 65?79 7?19 35?4980+65?7950?6435?49
10
 5
15
20?347?190?6 20?3450?64
(a) (b)
Figure 2: Two Alternative Graphs from the Same Data
uation showing the system?s success, with particu-
lar attention given to the impact of evidence from
shallow processing of the caption, and Section 7 dis-
cusses future work.
Although we believe that our findings are ex-
tendible to other kinds of information graphics, our
current work focuses on bar charts. This research is
part of a larger project whose goal is a natural lan-
guage system that will provide effective access to
information graphics for individuals with sight im-
pairments, by inferring the intended message under-
lying the graphic, providing an initial summary of
the graphic that includes the intended message along
with notable features of the graphic, and then re-
sponding to follow-up questions from the user.
2 Related Work
Our work is related to efforts on graph summariza-
tion. (Yu et al, 2002) used pattern recognition tech-
niques to summarize interesting features of automat-
ically generated graphs of time-series data from a
gas turbine engine. (Futrelle and Nikolakis, 1995)
developed a constraint grammar for parsing vector-
based visual displays and producing representations
of the elements comprising the display. The goal
of Futrelle?s project is to produce a graphic that
summarizes one or more graphics from a document
(Futrelle, 1999). The summary graphic might be a
simplification of a graphic or a merger of several
graphics from the document, along with an appropri-
ate summary caption. Thus the end result of summa-
rization will itself be a graphic. The long range goal
of our project, on the other hand, is to provide alter-
native access to information graphics via an initial
textual summary followed by an interactive follow-
up component for additional information. The in-
tended message of the graphic will be an important
component of the initial summary, and hypothesiz-
ing it is the goal of our current work.
3 Evidence about the Intended Message
The graphic designer has many alternative ways of
designing a graphic; different designs contain differ-
ent communicative signals and thus convey differ-
ent communicative intents. For example, consider
the two graphics in Figure 2. The graphic in Fig-
ure 2a conveys that average doctor visits per year
is U-shaped by age; it starts out high when one is
very young, decreases into middle age, and then
rises again as one ages. The graphic in Figure 2b
presents the same data; but instead of conveying a
trend, this graphic seems to convey that the elderly
and the young have the highest number of doctor vis-
its per year. These graphics illustrate how choice of
design affects the message that the graphic conveys.
Following the AutoBrief work (Kerpedjiev and
Roth, 2000) (Green et al, 2004) on generating
graphics that fulfill communicative goals, we hy-
pothesize that the designer chooses a design that best
facilitates the perceptual and cognitive tasks that
are most important to conveying his intended mes-
sage, subject to the constraints imposed by compet-
ing tasks. By perceptual tasks we mean tasks that
can be performed by simply viewing the graphic,
such as finding the top of a bar in a bar chart; by
cognitive tasks we mean tasks that are done via men-
tal computations, such as computing the difference
between two numbers.
Thus one source of evidence about the intended
message is the relative difficulty of the perceptual
tasks that the viewer would need to perform in order
to recognize the message. For example, determining
224
the entity with maximum value in a bar chart will be
easiest if the bars are arranged in ascending or de-
scending order of height. We have constructed a set
of rules, based on research by cognitive psycholo-
gists, that estimate the relative difficulty of perform-
ing different perceptual tasks; these rules have been
validated by eye-tracking experiments and are pre-
sented in (Elzer et al, 2004).
Another source of evidence is entities that have
been made salient in the graphic by some kind of fo-
cusing device, such as coloring some elements of the
graphic, annotations such as an asterisk, or an arrow
pointing to a particular location in a graphic. Enti-
ties that have been made salient suggest particular
instantiations of perceptual tasks that the viewer is
expected to perform, such as comparing the heights
of two highlighted bars in a bar chart.
And lastly, one would expect captions to help con-
vey the intended message of an information graphic.
The next section describes a corpus study that we
performed in order to explore the usefulness of cap-
tions and how we might exploit evidence from them.
4 A Corpus Study of Captions
Although one might suggest relying almost ex-
clusively on captions to interpret an information
graphic, (Corio and Lapalme, 1999) found in a cor-
pus study that captions are often very general. The
objective of their corpus study was to categorize the
kinds of information in captions so that their find-
ings could be used in forming rules for generating
graphics with captions.
Our project is instead concerned with recogniz-
ing the intended message of an information graphic.
To investigate how captions might be used in a sys-
tem for understanding information graphics, we per-
formed a corpus study in which we analyzed the
first 100 bar charts from our corpus of information
graphics; this corpus contains a variety of bar charts
from different publication venues. The following
subsections present the results of this corpus study.
4.1 Do Captions Convey the Intended
Message?
Our first investigation explored the extent to which
captions capture the intended message of an infor-
mation graphic. We extracted the first 100 graphics
Category #
Category-1: Captures intention (mostly) 34
Category-2: Captures intention (somewhat) 15
Category-3: Hints at intention 7
Category-4: No contribution to intention 44
Figure 3: Analysis of 100 Captions on Bar Charts
from our corpus of bar charts. The intended mes-
sage of each bar chart had been previously annotated
by two coders. The coders were asked to identify
1) the intended message of the graphic using a list
of 12 high-level intentions (see Section 5 for exam-
ples) and 2) the instantiation of the parameters. For
example, if the coder classified the intended mes-
sage of a graphic as Change-trend, the coder was
also asked to identify where the first trend began,
its general slope (increasing, decreasing, or stable),
where the change in trend occurred, the end of the
second trend, and the slope of the second trend. If
there was disagreement between the coders on either
the intention or the instantiation of the parameters,
we utilized consensus-based annotation (Ang et al,
2002), in which the coders discussed the graphic to
try to come to an agreement. As observed by (Ang
et al, 2002), this allowed us to include the ?harder?
or less obvious graphics in our study, thus lowering
our expected system performance. We then exam-
ined the caption of each graphic, and determined to
what extent the caption captured the graphic?s in-
tended message. Figure 3 shows the results. 44%
of the captions in our corpus did not convey to any
extent the message of the information graphic. The
following categorizes the purposes that these cap-
tions served, along with an example of each:
? general heading (8 captions): ?UGI Monthly
Gas Rates? on a graphic conveying a recent
spike in home heating bills.
? reference to dependent axis (15 captions):
?Lancaster rainfall totals for July? on a
graphic conveying that July-02 was the driest
of the previous decade.
? commentary relevant to graphic (4 captions):
?Basic performers: One look at the best per-
forming stocks in the Standard&Poor?s 500 in-
dex this year shows that companies with ba-
sic businesses are rewarding investors? on a
225
graphic conveying the relative rank of different
stocks, some of which were basic businesses
and some of which were not. This type of in-
formation was classified as deductive by (Corio
and Lapalme, 1999) since it draws a conclusion
from the data depicted in the graphic.
? commentary extending message of graphic (8
captions): ?Profits are getting squeezed? on
a graphic conveying that Southwest Airlines
net income is estimated to increase in 2003 af-
ter falling the preceding three years. Here the
commentary does not draw a conclusion from
the data in the graphic but instead supplements
the graphic?s message. However this type of
caption would probably fall into the deductive
class in (Corio and Lapalme, 1999).
? humor (7 captions): ?The Sound of Sales? on
a graphic conveying the changing trend (down-
ward after years of increase) in record album
sales. This caption has nothing to do with the
change-trend message of the graphic, but ap-
pears to be an attempt at humor.
? conclusion unwarranted by graphic (2 cap-
tions): ?Defense spending declines? on a
graphic that in fact conveys that recent defense
spending is increasing.
Slightly over half the captions (56%) contributed
to understanding the graphic?s intended message.
34% were judged to convey most of the intended
message. For example, the caption ?Tennis play-
ers top nominees? appeared on a graphic whose in-
tended message is to convey that more tennis players
were nominated for the 2003 Laureus World Sports
Award than athletes from any other sport. Since we
argue that captions alone are insufficient for inter-
preting information graphics, in the few cases where
it was unclear whether a caption should be placed
in Category-1 or Category-2, we erred on the side
of over-rating the contribution of a caption to the
graphic?s intended message. For example, consider
the caption ?Chirac is riding high in the polls?
which appeared on a graphic conveying that there
has been a steady increase in Chirac?s approval rat-
ings from 55% to about 75%. Although this caption
does not fully capture the communicative intention
of the graphic (since it does not capture the steady
increase conveyed by the graphic), we placed it in
the first category since one might argue that riding
high in the polls would suggest both high and im-
proving ratings.
15% of the captions were judged to convey only
part of the graphic?s intended message; an example
is ?Drug spending for young outpace seniors? that
appears on a graphic whose intended message ap-
pears to be that there is a downward trend by age for
increased drug spending; we classified the caption
in Category-2 since the caption fails to capture that
the graphic is talking about percent increases in drug
spending, not absolute drug spending, and that the
graphic conveys the downward trend for increases in
drug spending by age group, not just that increases
for the young were greater than for the elderly.
7% of the captions were judged to only hint at the
graphic?s message. An example is ?GM?s Money
Machine? which appeared on a graphic whose in-
tended message was a contrast of recent perfor-
mance against the previous trend ? ie., that al-
though there had been a steady decrease in the per-
centage of GM?s overall income produced by its fi-
nance unit, there was now a substantial increase in
the percentage provided by the finance unit. Since
the term money machine is a colloquialism that sug-
gests making a lot of money, the caption was judged
to hint at the graphic?s intended message.
4.2 Understanding Captions
For the 49 captions in Category 1 or 2 (where the
caption conveyed at least some of the message of
the graphic), we examined how well the caption
could be parsed and understood by a natural lan-
guage system. We found that 47% were fragments
(for example, ?A Growing Biotech Market?), or in-
volved some other kind of ill-formedness (for ex-
ample, ?Running tops in sneaker wear in 2002? or
?More seek financial aid?1). 16% would require ex-
tensive domain knowledge or analogical reasoning
to understand. One example is ?Chirac is riding
high in the polls? which would require understand-
ing the meaning of riding high in the polls. Another
example is ?Bad Moon Rising?; here the verb ris-
ing suggests that something is increasing, but the
1Here we judge the caption to be ill-formed due to the ellip-
sis since More should be More students.
226
system would need to understand that a bad moon
refers to something undesirable (in this case, delin-
quent loans).
4.3 Simple Evidence from Captions
Although our corpus analysis showed that captions
can be helpful in understanding the message con-
veyed by an information graphic, it also showed that
full understanding of a caption would be problem-
atic; moreover, once the caption was understood, we
would still need to relate it to the information ex-
tracted from the graphic itself, which appears to be
a difficult problem.
Thus we began investigating whether shallow pro-
cessing of the caption might provide evidence that
could be effectively combined with other evidence
obtained from the graphic itself. Our analysis pro-
vided the following observations:
? Verbs in a caption often suggest the kind of
message being conveyed by the graphic. An
example from our corpus is ?Boating deaths
decline?; the verb decline suggests that the
graphic conveys a decreasing trend. Another
example from our corpus is ?American Express
total billings still lag?; the verb lag suggests
that the graphic conveys that some entity (in
this case American Express) is ranked behind
some others.
? Adjectives in a caption also often suggest the
kind of message being conveyed by the graphic.
An example from our corpus is ?Air Force has
largest percentage of women?; the adjective
largest suggests that the graphic is conveying
an entity whose value is largest. Adjectives de-
rived from verbs function similarly to verbs.
An example from our corpus is ?Soaring De-
mand for Servers? which is the caption on a
graphic that conveys the rapid increase in de-
mand for servers. Here the adjective soaring is
derived from the verb soar, and suggests that
the graphic is conveying a strong increase.
? Nouns in a caption often refer to an entity that
is a label on the independent axis. When this
occurs, the caption brings the entity into focus
and suggests that it is part of the intended mes-
sage of the graphic. An example from our cor-
pus is ?Germans miss their marks? where the
graphic displays a bar chart that is intended to
convey that Germans are the least happy with
the Euro. Words that usually appear as verbs,
but are used in the caption as a noun, may func-
tion similarly to verbs. An example is ?Cable
On The Rise?; in this caption, rise is used as a
noun, but suggests that the graphic is conveying
an increase.
5 Utilizing Evidence
We developed and implemented a probabilistic
framework for utilizing evidence from a graphic and
its caption to hypothesize the graphic?s intended
message. To identify the intended message of a
new information graphic, the graphic is first given
to a Visual Extraction Module (Chester and Elzer,
2005) that is responsible for recognizing the indi-
vidual components of a graphic, identifying the re-
lationship of the components to one another and to
the graphic as a whole, and classifying the graphic
as to type (bar chart, line graph, etc.); the result is
an XML file that describes the graphic and all of its
components.
Next a Caption Processing Module analyzes the
caption. To utilize verb-related evidence from cap-
tions, we identified a set of verbs that would indicate
each category of high-level goal2, such as recover
for Change-trend and beats for Relative-difference;
we then extended the set of verbs by examining
WordNet for verbs that were closely related in mean-
ing, and constructed a verb class for each set of
closely related verbs. Adjectives such as more and
most were handled in a similar manner. The Caption
Processing Module applies a part-of-speech tagger
and a stemmer to the caption in order to identify
nouns, adjectives, and the root form of verbs and
adjectives derived from verbs. The XML represen-
tation of the graphic is augmented to indicate any
independent axis labels that match nouns in the cap-
tion, and the presence of a verb or adjective class in
the caption.
The Intention Recognition Module then analyzes
the XML file to build the appropriate Bayesian net-
work; the current system is limited to bar charts, but
2As described in the next paragraph, there are 12 categories
of high-level goals.
227
the principles underlying the system should be ex-
tendible to other kinds of information graphics. The
network is described in (Elzer et al, 2005). Very
briefly, our analysis of simple bar charts has shown
that the intended message can be classified into one
of 12 high-level goals; examples of such goals in-
clude:
? Change-trend: Viewer to believe that there
is a <slope-1> trend from <param1>
to <param2> and a significantly differ-
ent <slope-2> trend from <param3> to
<param4>
? Relative-difference: Viewer to believe that the
value of element <param1> is <comparison>
the value of element <param2> where
<comparison> is greater-than, less-than, or
equal-to.
Each category of high-level goal is represented by a
node in the network (whose parent is the top-level
goal node), and instances of these goals (ie., goals
with their parameters instantiated) appear as chil-
dren with inhibitory links (Huber et al, 1994) cap-
turing their mutual exclusivity. Each goal is broken
down further into subtasks (perceptual or cognitive)
that the viewer would need to perform in order to
accomplish the goal of the parent node. The net-
work is built dynamically when the system is pre-
sented with a new information graphic, so that nodes
are added to the network only as suggested by the
graphic. For example, low-level nodes are added for
the easiest primitive perceptual tasks and for per-
ceptual tasks in which a parameter is instantiated
with a salient entity (such as an entity colored dif-
ferently from others in the graphic or an entity that
appears as a noun in the caption), since the graphic
designer might have intended the viewer to perform
these tasks; then higher-level goals that involve these
tasks are added, until eventually a link is established
to the top-level goal node.
Next evidence nodes are added to the network to
capture the kinds of evidence noted in Sections 3
and 4.3. For example, evidence nodes are added to
the network as children of each low-level perceptual
task; these evidence nodes capture the relative dif-
ficulty (categorized as easy, medium, hard, or im-
possible) of performing the perceptual task as esti-
mated by our effort estimation rules mentioned in
Section 3, whether a parameter in the task refers to
an entity that is salient in the graphic, and whether
a parameter in the task refers to an entity that is a
noun in the caption. An evidence node, indicating
for each verb class whether that verb class appears
in the caption (either as a verb, or as an adjective de-
rived from a verb, or as a noun that can also serve as
a verb) is added as a child of the top level goal node.
Adjectives such as more and most that provide evi-
dence are handled in a similar manner.
In a Bayesian network, conditional probability ta-
bles capture the conditional probability of a child
node given the value of its parent(s). For example,
the network requires the conditional probability of
an entity appearing as a noun in the caption given
that recognizing the intended message entails per-
forming a particular perceptual task involving that
entity. Similarly, the network requires the condi-
tional probability, for each class of verb, that the
verb class appears in the caption given that the in-
tended message falls into a particular intention cat-
egory. These probabilities are learned from our cor-
pus of graphics, as described in (Elzer et al, 2005).
6 Evaluation
In this paper, we are particularly interested in
whether shallow processing of captions can con-
tribute to recognizing the intended message of an
information graphic. As mentioned earlier, the in-
tended message of each information graphic in our
corpus of bar charts had been previously annotated
by two coders. To evaluate our approach, we used
leave-one-out cross validation. We performed a se-
ries of experiments in which each graphic in the cor-
pus is selected once as the test graphic, the probabil-
ity tables in the Bayesian network are learned from
the remaining graphics, and the test graphic is pre-
sented to the system as a test case. The system was
judged to fail if either its top-rated hypothesis did
not match the intended message that was assigned
to the graphic by the coders or the probability rat-
ing of the system?s top-rated hypothesis did not ex-
ceed 50%. Overall success was then computed by
averaging together the results of the whole series of
experiments.
Each experiment consisted of two parts, one in
228
Diner?s Club
Discover
American Express
Mastercard
Visa
400 600200
Total credit card purchases per year in billions
Figure 4: A Graphic from Business Week3
which captions were not taken into account in the
Bayesian network and one in which the Bayesian
network included evidence from captions. Our
overall accuracy without the caption evidence was
64.5%, while the inclusion of caption evidence in-
creased accuracy to 79.1% for an absolute increase
in accuracy of 14.6% and a relative improvement of
22.6% over the system?s accuracy without caption
evidence. Thus we conclude that shallow process-
ing of a caption provides evidence that can be effec-
tively utilized in a Bayesian network to recognize
the intended message of an information graphic.
Our analysis of the results provides some interest-
ing insights on the role of elements of the caption.
There appear to be two primary functions of verbs.
The first is to reflect what is in the data, thereby
strengthening the message that would be recognized
without the caption. One example from our corpus
is a graphic with the caption ?Legal immigration to
the U.S. has been rising for decades?. Although
the early part of the graphic displays a change from
decreasing immigration to a steadily increasing im-
migration trend, most of the graphic focuses on the
decades of increasing immigration and the caption
strengthens increasing trend in immigration as the
intended message of the graphic. If we do not in-
clude the caption, our system hypothesizes an in-
creasing trend message with a probability of 66.4%;
other hypotheses include an intended message that
emphasizes the change in trend with a probability
of 15.3%. However, when the verb increasing from
the caption is taken into account, the probability of
increasing trend in immigration being the intended
message rises to 97.9%.
3This is a slight variation of the graphic from Business
Week. In the Business Week graphic, the labels sometimes ap-
The second function of a verb is to focus atten-
tion on some aspect of the data. For example, con-
sider the graphic in Figure 4. Without a caption, our
system hypothesizes that the graphic is intended to
convey the relative rank in billings of different credit
card issuers and assigns it a probability of 72.7%.
Other possibilities have some probability assigned
to them. For example, the intention of conveying
that Visa has the highest billings is assigned a prob-
ability of 26%. Suppose that the graphic had a cap-
tion of ?Billings still lag?; if the verb lag is taken
into account, our system hypothesizes an intended
message of conveying the credit card issuer whose
billings are lowest, namely Diner?s Club; the prob-
ability assigned to this intention is now 88.4%, and
the probability assigned to the intention of convey-
ing the relative rank of different credit card issuers
drops to 7.8%. This is because the verb class con-
taining lag appeared in our corpus as part of the cap-
tion for graphics whose message conveyed an en-
tity with a minimum value, and not with graphics
whose message conveyed the relative rank of all the
depicted entities. On the other hand, if the caption
is ?American Express total billings still lag? (which
is the caption associated with the graphic in our cor-
pus), then we have two pieces of evidence from the
caption ? the verb lag, and the noun American Ex-
press which matches a label. In this case, the proba-
bilities change dramatically; the hypothesis that the
graphic is intended to convey the rank of American
Express (namely third behind Visa and Mastercard)
is assigned a probability of 76% and the probability
drops to 24% that the graphic is intended to con-
vey that Diner?s Club has the lowest billings. This is
not surprising. The presence of the noun American
Express in the caption makes that entity salient and
is very strong evidence that the intended message
places an emphasis on American Express, thus sig-
nificantly affecting the probabilities of the different
hypotheses. On the other hand, the verb class con-
taining lag occurred both in the caption of graphics
whose message was judged to convey the entity with
the minimum value and in the caption of graphics
pear on the bars and sometimes next to them, and the heading
for the dependent axis appears in the empty white space of the
graphic instead of below the values on the horizontal axis as we
show it. Our vision system does not yet have heuristics for rec-
ognizing non-standard placement of labels and axis headings.
229
that conveyed an entity ranked behind some others.
Therefore, conveying the entity with minimum value
is still assigned a non-negligible probability.
7 Future Work
It is rare that a caption contains more than one verb
class; when it does happen, our current system by
default uses the first one that appears. We need to
examine how to handle the occurrence of multiple
verb classes in a caption. Occasionally, labels in the
graphic appear differently in the caption. An exam-
ple is DJIA (for Dow Jones Industrial Average) that
occurs in one graphic as a label but appears as Dow
in the caption. We need to investigate resolving such
coreferences.
We currently limit ourselves to recognizing what
appears to be the primary communicative intention
of an information graphic; in the future we will also
consider secondary intentions. We will also extend
our work to other kinds of information graphics such
as line graphs and pie charts, and to complex graph-
ics, such as grouped and composite bar charts.
8 Summary
To our knowledge, our project is the first to inves-
tigate the problem of understanding the intended
message of an information graphic. This paper
has focused on the communicative evidence present
in an information graphic and how it can be used
in a probabilistic framework to reason about the
graphic?s intended message. The paper has given
particular attention to evidence provided by the
graphic?s caption. Our corpus study showed that
about half of all captions contain some evidence that
contributes to understanding the graphic?s message,
but that fully understanding captions is a difficult
problem. We presented a strategy for extracting ev-
idence from a shallow analysis of the caption and
utilizing it, along with communicative signals from
the graphic itself, in a Bayesian network that hy-
pothesizes the intended message of an information
graphic, and our results demonstrate the effective-
ness of our methodology. Our research is part of a
larger project aimed at providing alternative access
to information graphics for individuals with sight
impairments.
References
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. of the Int?l Conf. on Spoken Language Process-
ing (ICSLP).
D. Chester and S. Elzer. 2005. Getting computers to see
information graphics so users do not have to. To ap-
pear in Proc. of the 15th Int?l Symposium on Method-
ologies for Intelligent Systems.
H. Clark. 1996. Using Language. Cambridge University
Press.
M. Corio and G. Lapalme. 1999. Generation of texts
for information graphics. In Proc. of the 7th European
Workshop on Natural Language Generation, 49?58.
S. Elzer, S. Carberry, N. Green, and J. Hoffman. 2004.
Incorporating perceptual task effort into the recogni-
tion of intention in information graphics. In Proceed-
ings of the 3rd Int?l Conference on Diagrams, LNAI
2980, 255?270.
S. Elzer, S. Carberry, I. Zukerman, D. Chester, N. Green,
S. Demir. 2005. A probabilistic framework for recog-
nizing intention in information graphics. To appear in
Proceedings of the Int?l Joint Conf. on AI (IJCAI).
R. Futrelle and N. Nikolakis. 1995. Efficient analysis of
complex diagrams using constraint-based parsing. In
Proc. of the Third International Conference on Docu-
ment Analysis and Recognition.
R. Futrelle. 1999. Summarization of diagrams in docu-
ments. In I. Mani and M. Maybury, editors, Advances
in Automated Text Summarization. MIT Press.
Nancy Green, Giuseppe Carenini, Stephan Kerpedjiev,
Joe Mattis, Johanna Moore, and Steven Roth. Auto-
brief: an experimental system for the automatic gen-
eration of briefings in integrated text and information
graphics. International Journal of Human-Computer
Studies, 61(1):32?70, 2004.
H. P. Grice. 1969. Utterer?s Meaning and Intentions.
Philosophical Review, 68:147?177.
M. Huber, E. Durfee, and M. Wellman. 1994. The auto-
mated mapping of plans for plan recognition. In Proc.
of Uncertainty in AI, 344?351.
S. Kerpedjiev and S. Roth. 2000. Mapping communica-
tive goals into conceptual tasks to generate graphics in
discourse. In Proc. of Int. Conf. on Intelligent User
Interfaces, 60?67.
J. Yu, J. Hunter, E. Reiter, and S. Sripada. 2002.
Recognising visual patterns to communicate gas tur-
bine time-series data. In ES2002, 105?118.
230
Understanding Information Graphics: A Discourse-Level Problem ?
?Sandra Carberry, ?Stephanie Elzer, ??Nancy Green, ?Kathleen McCoy, and ?Daniel Chester
?Dept. of Computer Science, University of Delaware, Newark, DE 19716
(carberry, elzer, mccoy, chester@cis.udel.edu)
??Dept. of Math. Sciences, Univ. of North Carolina at Greensboro, Greensboro, NC 27402
(nlgreen@uncg.edu)
Abstract
Keywords: graphics, understanding, dis-
course, plan-based models
Information graphics that appear in newspa-
pers and magazines generally have a message that
the viewer is intended to recognize. This paper ar-
gues that understanding such information graph-
ics is a discourse-level problem. In particular,
it requires assimilating information from multi-
ple knowledge sources to recognize the intended
message of the graphic, just as recognizing in-
tention in text does. Moreover, when an article
is composed of text and graphics, the intended
message of the information graphic (its discourse
intention) must be integrated into the discourse
structure of the surrounding text and contributes
to the overall discourse intention of the article.
This paper describes how we extend plan-based
techniques that have been used for understanding
traditional discourse to the understanding of in-
formation graphics. This work is part of a project
to develop an interactive natural language system
that provides sight-impaired users with access to
information graphics.
1 Introduction
Information graphics (non-pictorial graphics
such as bar charts and line graphs) are a variant of
language with many similarities to other forms of
communication. Information graphics are preva-
lent in information resources since they enable
complex information to be assimilated perceptu-
ally with ease. Unfortunately, knowledge sources
such as information graphics are not accessible to
some users. For example, individuals with im-
0The work of the third author was supported by the Na-
tional Science Foundation under Grant No. 0132821.
paired eyesight have limited access to information
graphics, thus preventing them from fully utiliz-
ing information resources.
Some information graphics are only intended
to display data values; (Yu et al, 2002) devel-
oped a pattern recognition algorithm for summa-
rizing interesting features of automatically gener-
ated graphics of time-series data from a gas tur-
bine engine. However, the overwhelming major-
ity of the graphics that we have examined (taken
from newspaper, magazine, and web articles) ap-
pear to have some underlying goal, such as get-
ting the viewer to believe that interest rates have
fallen substantially and that this would therefore
be a good time to refinance a mortgage. We
have found that understanding information graph-
ics is a discourse-level problem. In particular,
it requires assimilating information from multi-
ple knowledge sources to recognize the intended
message of the graphic, just as recognizing inten-
tion in text does. Moreover, the communicative
intention of the information graphic must be in-
tegrated into the discourse intentions of the sur-
rounding text.
We are developing an interactive natural lan-
guage system that infers the intended message
underlying an information graphic, augments it
with related interesting features of the graphic,
provides an initial summary of the graphic, and
then responds to followup questions from the
user. This paper presents the system architecture,
shows why interpreting information graphics is
a discourse-level problem, and outlines how we
extend techniques that have been used for under-
standing traditional discourse to the understand-
ing of information graphics.
2 A Natural Language Modality
Information is the key to knowledge and ef-
fective decision-making. But information is use-
ful only if it is accessible in a form that can be
easily assimilated. For sighted users, information
graphics capture complex information and enable
it to be assimilated perceptually with ease. For
individuals who have serious sight-impairments,
documents that contain information graphics pose
challenging problems. Although devices have
been developed for conveying information graph-
ics in alternative mediums such as musical tones
or tactile images, these approaches have serious
limitations. For example, systems that attempt to
convey graphics via a soundscape(Meijer, 1992)
do not facilitate easy comparison of two line
graphs linked in a single graphical display. More-
over, these approaches require the user to con-
struct a ?mental map? of the graphic, which is
difficult for congenitally blind users who do not
have the personal knowledge to assist them in the
interpretation of the image(Kennel, 1996). The
underlying hypothesis of our work is that alterna-
tive access to what the graphic looks like is not
enough ? the user should be provided with the
message and knowledge that one would gain from
viewing the graphic in order to enable effective
and efficient use of this information resource. To
accomplish this objective, we are developing an
interactive natural language system for communi-
cating the content of an information graphic. Our
methodology offers promise as a means of provid-
ing access to information graphics without expen-
sive equipment, with few limitations on the com-
plexity of the graphic that can be handled, and
with relatively little cognitive load on the user.
3 Architecture and Overview
Our current work is concerned with bar charts,
line graphs, and pie charts, although eventually
we will handle other kinds of graphics. Figure 1
shows the architecture of our system for convey-
ing information graphics. The visual extraction
component (VEC) analyzes the graphic and pro-
vides an XML representation of the graphic to
the intention recognition component (IRC). The
IRC is responsible for recognizing the intended
message of the information graphic and sending it
to the content planning component (CPC), which
will augment the intended message of the graphic
with related interesting features. The message or-
ganization component (MOC) then organizes the
most salient propositions into a coherent sum-
mary, which will be rendered in natural language
and conveyed to the user via speech synthesis.
The followup question component (FQC) will al-
low the user to interactively seek additional infor-
mation about the graphic.
Our work thus far (Section 4) has focused on
understanding an information graphic so that its
intended message can be conveyed to the user.
Section 4.1 discusses the extension of speech act
theory to the generation and understanding of in-
formation graphics. Section 4.2 argues that un-
derstanding information graphics is a discourse-
level problem in which the system must recog-
nize the intended message of the graphic and in-
tegrate it into the intentions of any surrounding
text; it further argues that understanding informa-
tion graphics requires similar kinds of knowledge
and processing as does the understanding of tra-
ditional textual discourse. Section 4.3 provides
a brief overview of the visual extraction compo-
nent that analyzes the graphical image and con-
structs an XML representation of the graphic for
use by the graphic understanding system. Sec-
tion 4.4 then describes how we have extended
techniques used for understanding traditional dis-
course and dialogue to the understanding of infor-
mation graphics. Section 5 gives a brief overview
of future work on the rest of the system. The Ap-
pendix contains information graphics that are part
of the corpus on which our work is based.
4 Understanding Information Graphics
4.1 Intention in Information Graphics
Information graphics are a variant of language.
As noted by Clark(Clark, 1996), language is more
than just words. It is any ?signal? (or lack of sig-
nal when one is expected), where a signal is a de-
liberate action that is intended to convey a mes-
sage. According to speech act theory, a speaker
or writer executes a speech act whose intended
meaning he expects the listener or reader to be
able to deduce(Searle, 1970; Grice, 1969; Clark,
1996). In their work on multimedia generation,
Figure 1: System Architecture
the AutoBrief group proposed that speech act the-
ory could be extended to cover the generation
of graphical representations(Kerpedjiev and Roth,
2000). They developed a multimedia presenta-
tion system that generated text and information
graphics. It included 1) an algorithm that could
map communicative goals to a set of perceptual
and cognitive tasks that must be enabled for a
viewer to recognize the goals and 2) an automatic
graph designer that used constraint satisfaction to
construct an information graphic that best facili-
tated those tasks, subject to competing constraints
among the tasks.
The overwhelming majority of information
graphics accompanying newspaper and magazine
articles appear to carry a message that the de-
signer intends to convey to the viewer by virtue
of the graphic?s design and the data presented in
the graphic. Consider the graphic in Figure 9. It
conveys the message that the salary of women
in science, mathematics, and engineering fields
is consistently less than that of men in the same
fields. Other messages could have been con-
veyed by a different graphic design. For ex-
ample, by grouping the bars for men together,
grouping the bars for women together, and or-
dering the bars for each group by height, the
graphic would have conveyed the message that
both men and women earn the least in the so-
cial sciences and the most in engineering. Or
if the bars for Computer/Mathematical Sciences
were highlighted in Figure 9 by coloring them
significantly differently from the other bars in the
graphic, the graphic would have invoked a com-
parison of the discrepancies between male and
female salaries in Computer/Mathematical Sci-
ences and the salary discrepancies between men
and women in other fields. Although a graphic?s
caption can be helpful in identifying its intended
message (as in Figure 8), Corio performed a large
corpus study(Corio and Lapalme, 1999) in which
he found that captions are often missing or fail
to provide any indication of what the information
graphic conveys (as in Figures 6 and 10). Thus
we cannot rely entirely on the presence of useful
captions to identify the intended message of an
information graphic.
Language research has posited that the listener
or reader who is interpreting a speech act identi-
fies its intended meaning by reasoning about the
observed signals and the mutual beliefs of author
and interpreter(Grice, 1969; Clark, 1996). Ap-
plying this to graphical displays, it is reasonable
to presume that the author of a graphic similarly
expects the viewer to use perceptual skills along
with other knowledge sources to deduce from the
graphic the message that he intended to convey.
Thus we are applying speech act theory in the re-
verse direction of the AutoBrief project, namely
to the recognition of the intended message under-
lying an information graphic.
4.2 A Discourse Level Problem
This section argues that interpreting informa-
tion graphics is a discourse-level problem ? not
only is it necessary to recognize the intention of
the graphic as noted in Section 4.1, but under-
standing an information graphic requires similar
kinds of knowledge and processing as does un-
derstanding traditional discourse.
Grosz and Sidner contended that discourse
has a structure comprised of discourse segments.
Each discourse segment has a discourse seg-
ment purpose that contributes to the discourse
purpose or intention underlying the overall dis-
course(Grosz and Sidner, 1986). When an arti-
cle is comprised of text and graphics, the graphic
generally expands on the text and contributes to
the discourse purpose of the article. Consider the
graphic and partial surrounding text reproduced
in Figure 6. Nowhere in the text is it stated that
the income of black women has risen dramati-
cally over the last decade and has reached the
level of white women. Yet this message is clearly
conveyed by the graphic and contributes to the
overall communicative intention of this portion
of the article ? namely, that there has been a
?monumental shifting of the sands? with regard
to the achievements of black women. Not only
does the intended message of the graphic (its dis-
course segment purpose) contribute to this overall
intention, but in fact the discourse intention of the
graphic helps to recognize the overall intention.
Even when the graphic stands in isolation as
in Figures 7 and 8, understanding the graphic
is a discourse-level problem. Grosz and Sid-
ner(Grosz and Sidner, 1986) claim that a robust
model of discourse understanding must use mul-
tiple knowledge sources in order to recognize the
complex relationships that utterances have to one
another. Information graphics have similar com-
plex relationships among their component ele-
ments. Not only might the graphic include mul-
tiple elements that must be related to one another
(such as multiple lines in a line graph, or individ-
ual bars in a bar chart), but information graphics
often include highlighting of certain elements to
make them particularly salient (as in Figure 10)
or include captions that might contribute to rec-
ognizing the graphic?s intention. The graphic in
Figure 8 includes such a helpful caption, although
many graphics, such as the ones in Figures 7
and 9, do not.
Furthermore, identifying the intended message
of a composite graphic (one comprised of multi-
ple individual graphics) requires relating the in-
dividual graphics to one another to identify the
intended message of the composite. Figure 11 il-
lustrates a composite information graphic. The
discourse purpose of the composite graphic is that
audits of affluent taxpayers are declining with re-
spect to audits of all taxpayers. This message can
only be deduced by relating the two individual
graphics and their underlying messages.
Moreover, understanding information graphics
requires the use of multiple knowledge sources.
In earlier work on recognizing expressions of
doubt, we developed an algorithm that combined
linguistic, contextual, and world knowledge and
applied it to the recognition of complex discourse
acts(Carberry and Lambert, 1999). In the case
of information graphics, the corollary to linguis-
tic knowledge is perceptual knowledge, by which
one recognizes the individual elements of the
graphic (for example, the bars in a bar chart), the
relation of the individual elements in the graphic
to one another, the type of graphic (line graph,
bar chart, pie chart, etc.), and what the different
graphic types can be used to convey. For exam-
ple, both a scatter plot and a pie chart can be
used to portray how an entity (such as govern-
ment income) is divided up among several cate-
gories (such as social welfare, military spending,
etc.); however, a graphic designer will choose a
pie chart if the intent is to convey the relative dis-
tributions as opposed to their absolute amounts.
Furthermore, a particular type of graphic (such as
a line graph) might be appropriate for conveying
several different intentions (maximum data point,
data trend, data variation, etc.).
Contextual and world knowledge are also es-
sential for understanding information graphics.
Contextual knowledge includes the caption as-
sociated with the graphic, any highlighting of
graphic elements that affects the focus of atten-
tion in the graphic, and the discourse structure and
focus of attention in any surrounding text. World
knowledge consists of mutual beliefs between de-
signer and viewer about entities of interest to the
intended viewing audience. For example, if an in-
formation graphic appears in a document targeted
at residents of New York City, then both the de-
signer and the viewer will mutually believe that
entities such as New York City, its football and
baseball teams, etc. will be particularly salient
to the viewer. Our methodology for understand-
ing information graphics takes these knowledge
sources into account.
4.3 The Visual Extraction Component
The visual extraction component (VEC) cap-
tures much of the perceptual knowledge discussed
in Section 4.2. It is responsible for recogniz-
ing the individual components comprising the
graphic, identifying the relationship of the differ-
ent components to one another and to the graphic
as a whole, and classifying the graphic as to type.
Extracted components include not only the bars,
lines, or wedges of a graphic but also the titles of
the axes, the legend, and the graphic?s title or cap-
tion. The present implementation deals only with
gray scale images (in pgm format) of bar charts,
pie charts, and line graphs, though eventually it
will be extended to handle color and other kinds
of information graphics. Words and numbers that
appear in the chart are associated with particular
bars, wedges and lines by their proximity to the
chart component in question. The output of the
visual extraction component is an XML file that
describes the chart and all of its components.
4.4 Applying Discourse Understanding
Strategies
Many researchers have cast the understanding
of discourse and dialogue as a plan recognition
problem ? that is, the writer or speaker (or char-
acters in the case of a story) has an underlying
goal and a plan for accomplishing that goal, and
understanding requires that the reader or listener
infer the plan and in turn the goal that the plan is
intended to achieve. (Perrault and Allen, 1980;
Wilensky, 1983; Litman and Allen, 1987; Car-
berry, 1990; Charniak and Goldman, 1993; Ardis-
onno and Sestero, 1996) are just a few examples
of such systems.
Since understanding information graphics is a
discourse-level problem, we are extending plan
inference techniques to recognizing the intended
message of an information graphic(Elzer et al,
2003) and to identifying its contribution to an
extended discourse that includes both text and
graphics. Planning and plan inference systems re-
quire knowledge about goals and how they can
be achieved. Typically, this is provided by a li-
brary of operators. Each operator encodes a goal
in its header; the body of the operator encodes
the subgoals that must be accomplished in order
to achieve the operator?s goal. A planning sys-
tem starts with a high-level goal, and uses oper-
ators to decompose the goal into a set of simpler
subgoals, which eventually decompose into prim-
itive subgoals that can be accomplished by prim-
itive actions in the domain. On the other hand,
a plan inference system starts with the primitive
goals associated with observed actions, and uses
the operators to chain backwards to higher-level
goals which the lower-level subgoals contribute to
achieving. In the case of traditional discourse and
dialogue, the subgoals in the plan operators are ei-
ther communicative or domain goals, and the ob-
served actions that start the plan inference process
are the speech acts represented by the utterances
in a story or a dialogue.
To extend plan inference to information graph-
ics, the plan operators must include goals that
can be accomplished by viewing an information
graphic, as opposed to being the recipient of an
utterance. As discussed in Section 4.1, the Auto-
Brief project(Kerpedjiev and Roth, 2000) devel-
oped an algorithm to map communicative goals to
a sequence of perceptual and cognitive tasks that
the graphic should support. Perceptual tasks are
tasks that can be performed by simply viewing the
graphic, such as finding the top of a bar in a bar
chart; cognitive tasks are tasks that are performed
via mental computations, such as computing the
difference between two numbers. We draw on
the AutoBrief notion of perceptual and cognitive
tasks enabled by an information graphic. Our plan
operators not only encode knowledge about how
to achieve domain and communicative goals (the
latter of which may require that the viewer per-
form perceptual and cognitive tasks) but they also
encode knowledge about how information-access
tasks, such as finding the value of an entity in
a graphic, can be decomposed into simpler sub-
goals. Figures 2 and 3 present two plan operators
for achieving the goal of finding the value <v> of
an attribute <att> for a graphical element <e>
(for example, the value associated with the top of
a bar in a bar chart). The body of the operator in
Goal: Find-value(<viewer>, <g>, <e>, <ds>, <att>, <v>)
Gloss: Given graphical element <e> in graphic <g>, <viewer> can find the value <v>
in dataset <ds> of attribute <att> for <e>
Data-req: Dependent-variable(<att>, <ds>)
Body: 1. Perceive-dependent-value(<viewer>, <g>, <att>, <e>, <v>)
Figure 2: Operator for achieving a goal perceptually
Goal: Find-value(<viewer>, <g>, <e>, <ds>, <att>, <v>)
Gloss: Given graphical element <e> in graphic <g>, <viewer> can find the value <v>
in dataset <ds> of attribute <att> for <e>
Data-req: Natural-quantitative-ordering(<att>)
Display-const: Ordered-values-on-axis(<g>, <axis>, <att>)
Body: 1. Perceive-info-to-interpolate(<viewer>,<g>,<axis>,<e>,<l1>,<l2>,<f>)
2. Interpolate(<viewer>, <l1>, <l2>, <f>, <v>)
Figure 3: Operator that employs both perceptual and cognitive subgoals
Figure 2 specifies that the goal can be achieved
by a primitive perceptual task in which the viewer
just perceives the value; this could be done, for
example, if the element in the graphic is annotated
with its value, as are the bars in the bar chart in
Figure 8 of the Appendix. On the other hand, the
body of the operator in Figure 3 captures a differ-
ent way of finding the value, one that presumably
requires more effort. It specifies the perceptual
task of finding the values <l1> and <l2> sur-
rounding the desired value on the axis along with
the fraction <f> of the distance that the desired
value lies between <l1> and <l2>, followed by
the cognitive task of interpolating between the re-
trieved values <l1> and <l2>.
Our operators contain data requirements (la-
belled Data-req) which the data must satisfy in
order for the operator to be applicable in a graphic
planning paradigm; they may also contain display
constraints (labelled Display-const) which con-
strain how the information graphic is constructed
if this operator is part of a final plan. In the case
of plan recognition, these constraints are used
in reverse. The display constraints are used to
eliminate operators from consideration, since if
a graphic does not satisfy the operator?s display
constraints, then the operator could not be part of
a plan that led to the graphic. If a graphic meets
the display constraints of an operator, then the
data requirements are used to limit how the op-
erator?s parameters might be instantiated.
4.4.1 Beginning the Plan Inference Process
Traditional plan inference systems used for
language understanding start with the primitive
goal achieved by the speech act in the dialogue
or discourse. In the case of information graphics,
the role of the speech act is played by the primi-
tive perceptual tasks that the viewer performs on
the graphic. To limit the set of perceptual tasks
that are considered, we make two observations:
? The graphic designer has many alternative
ways of designing a graphic, and the de-
sign choices facilitate some perceptual tasks
more than others. Following the Auto-
Brief work(Kerpedjiev and Roth, 2000) on
generating graphics that fulfill communica-
tive goals, we hypothesize that the designer
chooses a design that best facilitates the
tasks that are most important to conveying
his intended message, subject to the con-
straints imposed by competing tasks.
? Entities may become particularly salient by
virtue of highlighting in the graphic (for ex-
ample, coloring certain elements different
from the others, annotating an element with
an asterisk, or exploding one piece of a pie
chart1), by their mention in the caption or
surrounding text, or via world knowledge
1(Mittal, 1997) discusses a variety of such design tech-
niques in the context of distorting the message inferred from
a graphic.
capturing mutual beliefs about entities of in-
terest to the intended audience. We hypoth-
esize that the designer relies on the viewer
recognizing particularly salient entities, in
order to make certain perceptual tasks more
salient to the viewer.
As noted in Section 4.1, one cannot rely on a
graphic?s caption to provide the intended mes-
sage of the graphic. Consequently, the plan in-
ference process starts with both the set of tasks
that are best enabled by the information graphic
and the set of tasks (if any) that are particularly
salient. These will be referred to as candidate
tasks. The next two subsections describe how
candidate tasks are identified.
Identifying the Best Enabled Tasks The
APTE (Analysis of Perceptual Task Effort) sub-
module, shown in Figure 1 as part of the Inten-
tion Recognition Component, captures perceptual
knowledge about performing primitive perceptual
tasks2, and it encapsulates the results of cognitive
psychology research to estimate the relative effort
required for different tasks. The output of APTE
is the set of perceptual tasks that are best enabled
by the graphic. These become candidate tasks.
Each APTE rule captures a primitive percep-
tual task that can be performed on a particu-
lar type of information graphic, the conditions
(graphic design choices) that affect the difficulty
of performing that task, and the estimated effort
expended by a viewer if those conditions are sat-
isfied in the graphic. The condition-computation
pairs are ordered so that the ones producing the
lowest effort estimates appear first in a rule.
To derive the effort estimates in the rules, we
have followed the GOMS approach(Card et al,
1983) by breaking down the tasks that are re-
garded as primitive in our plan operators into
even more basic component tasks, and then sum-
ming the effort estimates for these very basic
tasks. Lohse?s work(Lohse, 1993) is an exam-
ple of the GOMS architecture applied to predict-
ing performance on graph comprehension tasks,
and many of our effort estimates are based on
Lohse?s research. For example, Figure 4 dis-
2Primitive perceptual tasks are those that we do not de-
compose into a set of simpler subtasks; this is not to be con-
fused with the notion of a psychological primitive.
plays the APTE rule for the task of finding the
value associated with the top of a bar in a bar
chart. If the bar is annotated with its value,
then condition-computation pair B1-1 estimates
its effort as 150 units for discriminating the label
(based on work by Lohse(Lohse, 1993)) and 300
units for recognizing a 6-letter word (John and
Newell, 1990). If the bar is not annotated with its
value but is aligned with a tick mark on the axis,
then condition-computation pair B1-2 estimates
the perceptual effort in terms of the distance to the
dependent axis (in order to capture the degrees of
visual arc scanned(Kosslyn, 1989)) plus the effort
of discriminating and recognizing the label. Fig-
ure 5 displays the APTE rule associated with the
first subgoal in Figure 3. It estimates the effort for
the primitive task Perceive-info-to-interpolate as
the effort of the scan to the dependent axis (based
on (Kosslyn, 1989)), the effort of discriminating
the intersection location on the axis (150 units
based on (Lohse, 1993)), plus the effort of the sac-
cade to each label (230 units each (Russo, 1978))
along with the effort involved in discriminating
and recognizing the labels. Similarly, there is a
cognitive rule (not discussed here) for estimating
the effort associated with the cognitive task Inter-
polate (the second subgoal in the operator in Fig-
ure 3). (Elzer et al, 2003a) presents a more ex-
tensive discussion of the cognitive principles un-
derlying the APTE rules.
Given the XML representation of an informa-
tion graphic, each APTE rule that is applicable
to the graphic produces an effort estimate for the
task captured by the rule. When a task might be
instantiated in multiple ways and still satisfy the
conditions of a condition-computation pair (for
example, the task of finding the value of the top
of a bar could be instantiated for each bar in a
bar chart), only the instantiation that produces the
lowest effort estimate becomes a candidate task.
(If the bars are not annotated with values, then the
instantiation that will produce the lowest effort es-
timate for the task of finding the value of the top
of a bar in a bar chart would be the bar with the
shortest scan to the dependent axis.) This is con-
sistent with the idea that the graphic designer will
make the important tasks easy to perform. The
set of perceptual tasks that require the least effort
become candidate tasks.
Rule-1:Estimate effort for task Perceive-dependent-value(<viewer>, <g>, <att>, <e>, <v>)
Graphic-type: bar-chart
Gloss: Compute effort for finding the exact value <v> for attribute <att> represented by top <e>
of a bar <b> in graph <g>
B1-1: IF the top of bar <b> is annotated with a value,
THEN effort=150 + 300
B1-2: IF the top <e> of bar <b> aligns with a labelled tick mark on the dependent axis,
THEN effort=scan + 150 + 300
Figure 4: A rule for estimating effort for the primitive perceptual task Perceive-value
Rule-2:Estimate effort for task
Perceive-info-to-interpolate(<viewer>,<g>,<axis>,<e>,<l1>,<l2>,<f>)
Graphic-type: bar-chart
Gloss: Compute effort for finding the information needed for interpolation, including the labels
<l1> and <l2> on either side of entity <e> on axis <axis> in graph <g>,
and the fraction <f> that is the distance between <l1> and entity <e> on <axis>
relative to the distance between <l1> and <l2>
B2-1: IF <axis> is labelled with values THEN effort=scan + 150 + ((230 + 150 + 300) x 2)
Figure 5: A rule for estimating effort for the primitive perceptual task Perceive-info-to-interpolate
Identifying Particularly Salient Tasks
Salient tasks are those that the viewer might
perform because they relate to entities that are
in the viewer?s current focus of attention, as
determined by contextual knowledge provided
by the caption, highlighting, and the surrounding
text and by world knowledge in the form of
mutual beliefs about items of particular interest
to the viewing audience.
Ideally, a caption will provide clues about the
message that an information graphic is intended
to convey, and thus noun phrases in captions rep-
resent salient entities.3 The graphic designer can
also call into focus certain aspects of the graphic
by using attention-getting devices such as col-
oring it differently from the rest of the graphic,
annotating it with an arrow, etc. Our working
hypothesis is that if the graphic designer goes
to the effort of employing such attention-getting
devices, then the highlighted items almost cer-
tainly contribute to the intended message. Thus
the attributes of these highlighted items (for ex-
ample, the attributes of a highlighted bar in a bar
chart), which are captured in the XML represen-
3Verb phrases in captions also provide evidence, but they
suggest particular operators of interest rather than instanti-
ated perceptual tasks, and thus we associate verbs with oper-
ators in the plan library.
tation of the graphic, are also regarded as salient
entities. Salient entities also include those that
world knowledge suggests are mutually believed
to be of interest to the viewing audience. We en-
vision in the future using the notion of lexical
chains(Silber and McCoy, 2000) to identify enti-
ties that the accompanying text makes particularly
salient. Perceptual tasks that are instantiated with
a salient entity and that can be performed on the
graphic are designated salient tasks.
4.4.2 The Search Process
Candidate tasks consist of the set of percep-
tual tasks that require the least effort and the set of
salient tasks. Once the set of candidate tasks has
been identified, plan inference begins. Initial can-
didate plans are constructed from each operator
in which a candidate task appears as a subgoal;
the root of the candidate plan is the goal of the
operator, and its children are the subgoals in the
body of the operator. Chaining from the root goal
to other operators whose body contains the root
goal as a subgoal produces larger candidate plans
with higher-level goals as the new root goal.
Plan inference systems have used a variety of
heuristics to evaluate candidate plans and to se-
lect the candidate plan to expand further. These
heuristics help to guide the search through the
space of candidate plans in order to hypothe-
size the plan that best represents the user?s in-
tentions. These heuristics have included increas-
ing the rating of partial plans as their arguments
become instantiated(Perrault and Allen, 1980),
preferring coherent discourse moves(Litman and
Allen, 1987; Carberry, 1990), and biasing the
plan inference process based on knowledge about
the user group(Gertner and Webber, 1996). We
have identified several kinds of evidence for guid-
ing plan inference from information graphics, in-
cluding the estimated effort required by a candi-
date plan, the basis for instantiating parameters in
the plan, adherence to the proximity compatibil-
ity principle from cognitive science research, and
the relation between a candidate plan and the es-
tablished discourse context.
Since our working hypothesis is that the
graphic designer tried to enable those tasks neces-
sary to recognize his intended message, candidate
plans that require substantially more effort than
other candidate plans are less likely to represent
the intentions of the designer. The effort associ-
ated with a candidate plan is measured as the sum
of the effort of the tasks comprising it.
There are many ways that a parameter in a task
or subgoal might become instantiated, and the ba-
sis for the instantiation provides evidence about
the likelihood that a hypothesized candidate plan
represents the graphic designer?s intentions. If
an instantiation is suggested by highlighting or
a caption or entities that are particularly salient
to the targeted audience, that partial plan should
be evaluated more favorably since the designer of
the graphic has provided reasons for the viewer to
use these instantiations in recognizing his inten-
tions. Similarly, if the instantiation is one of sev-
eral possible alternatives with no reason for pre-
ferring one over the other, then the partial plan
should be evaluated less favorably since the de-
signer did not give the viewer any reason to prefer
one over the other. This relates to Allen?s forking
heuristic(Perrault and Allen, 1980). The proxim-
ity compatibility principle(Wickens and Carswell,
1995) also suggests that candidate plans which
use similarly encoded elements (for example, all
red bars) in an integrated fashion should be eval-
uated more favorably than those that do not.
If there is a context established by the text pre-
ceding or surrounding the graphic, then candidate
plans whose root goal contributes to the exist-
ing discourse context should be preferred. If the
surrounding text has a reference to the graphic,
then focusing heuristics(Carberry, 1990) will pre-
fer candidate plans that relate most closely to the
current focus of attention at that point in the sur-
rounding text. However, the surrounding text of-
ten does not refer to accompanying graphics, as is
the case in the Newsweek article whose excerpt is
shown in Figure 6. Future work will investigate
how we should handle instances such as this.
5 Response Generation and Followup
The intended message of the graphic must be
augmented with additional propositions that con-
vey interesting features that a viewer would glean
from the graphic. For example, the intended mes-
sage of the graphic in Figure 6 appears to be that
the income of black women has risen dramati-
cally over the last decade and reached the level
of white women. But other interesting features of
the graphic might include the trends over the past
several decades, periods where they were closest,
etc. In future work, we anticipate developing a
methodology for identifying propositions that ex-
pand on the message of the graphic designer and
for including the most salient of these in the sum-
marization of the graphic. We also envision re-
sponding to followup requests for further infor-
mation about the graphic by selecting the highest
ranking propositions that were not included in the
initial message, organizing them into a coherent
response, and conveying it to the user.
6 Summary
This paper has argued that understanding
information graphics is a discourse-level prob-
lem. Not only must the system recognize the in-
tended message of the information graphic, but
the recognition process requires similar kinds of
knowledge sources and similar kinds of process-
ing as does the understanding of traditional dis-
course and dialogue. Moreover, when an article
is composed of text and graphics, the intended
message of the information graphic must be in-
tegrated into the discourse structure of the sur-
rounding text, and it contributes to the overall dis-
course intention of the article.
References
L. Ardisonno and D. Sestero. 1996. Using dynamic
user models in the recognition of the plans of the
user. User Modeling and User-Adapted Interac-
tion, 5(2):157?190.
S. Carberry and L. Lambert. 1999. A process model
for recognizing communicative acts and modeling
negotiation subdialogues. Computational Linguis-
tics, 25(1):1?53.
S. Carberry. 1990. Plan Recognition in Natural Lan-
guage Dialogue. ACL-MIT Press Series on Natural
Language Processing. MIT Press, Cambridge, MA.
S. Card, T. Moran, and A. Newell. 1983. The Psychol-
ogy of Human-Computer Interaction. Lawrence
Erlbaum Associates, Inc., Hillsdale, NJ.
E. Charniak and R. Goldman. 1993. A bayesian
model of plan recognition. Artificial Intelligence,
64:53?79.
H. Clark. 1996. Using Language. Cambridge Univer-
sity Press.
M. Corio and G. Lapalme. 1999. Generation of texts
for information graphics. In Proceedings of the 7th
European Workshop on Natural Language Genera-
tion EWNLG?99, pages 49?58.
S. Elzer, N. Green, and S. Carberry. 2003a. Exploit-
ing cognitive psychology research for recognizing
intention in information graphics. In Proceedings
of the 25th Annual Meeting of the Cognitive Science
Society. To appear.
S. Elzer, N. Green, S. Carberry, and K. McCoy. 2003.
Extending plan inference techniques to recognize
intentions in information graphics. In Proceedings
of the Ninth International Conference on User Mod-
eling. To appear.
A. Gertner and B. Webber. 1996. A Bias Towards
Relevance: Recognizing Plans Where Goal Mini-
mization Fails. In Proc. of the Thirteenth National
Conference on Artificial Intelligence, pages 1133?
1138.
H. P. Grice. 1969. Utterer?s Meaning and Intentions.
Philosophical Review, 68:147?177.
B. Grosz and C. Sidner. 1986. Attention, Intentions,
and the Structure of Discourse. Computational Lin-
guistics, 12(3):175?204.
B. John and A. Newell. 1990. Toward an engi-
neering model of stimulus response compatibility.
In R. Gilmore and T. Reeve, editors, Stimulus-
response compatibility: An integrated approach,
pages 107?115. North-Holland, New York.
A. Kennel. 1996. Audiograf: A diagram-reader for
the blind. In Second Annual ACM Conference on
Assistive Technologies, pages 51?56.
S. Kerpedjiev and S. Roth. 2000. Mapping com-
municative goals into conceptual tasks to generate
graphics in discourse. In Proc. of the International
Conference on Intelligent User Interfaces, pages
60?67.
S. Kosslyn. 1989. Understanding charts and graphs.
Applied Cognitive Psychology, 3:185?226.
D. Litman and J. Allen. 1987. A Plan Recognition
Model for Subdialogues in Conversation. Cognitive
Science, 11:163?200.
G. Lohse. 1993. A cognitive model for understand-
ing graphical perception. Human-Computer Inter-
action, 8:353?388.
Peter B. Meijer. 1992. An experimental system for
auditory image representations. IEEE Transactions
on Biomedical Engineering, 39(2):291?300, Febru-
ary.
V. Mittal. 1997. Visual prompts and graphical design:
A framework for exploring the design space of 2-D
charts and graphs. In Proc. of the Fourteenth Na-
tional Conference on Artificial Intelligence, pages
57?63.
R. Perrault and J. Allen. 1980. A Plan-Based Anal-
ysis of Indirect Speech Acts. American Journal of
Computational Linguistics, 6(3-4):167?182.
J. Russo. 1978. Adaptation of cognitive processes to
eye movement systems. In J. Senders, D. Fisher,
and R. Monty, editors, Eye movements and higher
psychological functions. Lawrence Erlbaum, Hills-
dale, NJ.
J. Searle. 1970. Speech Acts: An Essay in the Phi-
losophy of Language. Cambridge University Press,
London.
G. Silber and K. McCoy. 2000. Efficient text summa-
rization using lexical chains. In Proc. of the Inter-
national Conference on Intelligent User Interfaces,
pages 252?255.
C. Wickens and M. Carswell. 1995. The proximity
compatibility principle: Its psychological founda-
tion and relevance to display design. Human Fac-
tors, 37(3):473?494.
R. Wilensky. 1983. Planning and Understanding.
Addison-Wesley.
J. Yu, J. Hunter, E. Reiter, and S. Sripada. 2002.
Recognising visual patterns to communicate gas
turbine time-series data. In ES2002, pages 105?
118.
Appendix of Graphics from our Corpus
Graphic from Newsweek Article
60 70 80 90 01
$15
10
5
Black women
White women
Median Income
In thousands of 2001 dollars
1948
Relevant Text from Newsweek Article
This is not to say that black women have
climbed the storied crystal stair. They remain
?in the proving stage?, observes Alabama ex-
ecutive Alice Gordon. Nearly 14 percent of
working black women remain below the poverty
level. And women don?t yet out-earn black men.
But the growing educational-achievement gap
portends a monumental shifting of the sands.
College-educated black women already earn
more than the median for all black working men
? or, for that matter, for all women. And as
women in general move up the corporate pyra-
mid, black women, increasingly, are part of the
parade. In 1995 women held less than 9 per-
cent of corporate-officer positions in Fortune
500 companies, according to Catalyst, a New
York-based organization that promotes the inter-
ests of women in business. Last year they held
close to 16 percent, a significant step up. Of
those 2,140 women, 163 were black ? a minus-
cule proportion, but one that is certain to grow.
Figure 6: Excerpt from Newsweek Magazine
How reliable adults think DNA tests are for identifying an individual:
Trusting DNA
Don?t know
Very unreliable
unreliable
Somewhat 
reliable
Somewhat
Very reliable
2%
3%
4%
68%
23%
Figure 7: Standalone Graphic from USA Today
Europe
Canada
African
Other
countries
South
United
States
Africa
21
metric tons
Leading producers in
in gold production
South Africa tops
428
355
187 155
Gold Production
M
et
ric
 T
on
s
Figure 8: Standalone Graphic from USA Today
010,000
Median Salaries (in dollars), Full?Time Employed SMET Doctorates, by Field and Gender, 1997
Computer/All SMET Physical Sciences
Sal
ari
es 
(in 
dol
lars
)
SciencesMathematical
Engineering Life Sciences Social Sciences
80,000
70,000
60,000
50,000
40,000
30,000
20,000
Male
Female
Figure 9: Graphic from Report of the NSF Committee on Equal Opportunities in Science & Engineering
personal filings
Delaware bankruptcy
3000
2500
1000
1500
2000
1998 1999 2000 2001
Figure 10: Graphic from Wilmington News
Journal
0.6%
?96 ?97 ?98 ?99 ?00 ?01
1.0%
2.0%
3.0%
0.8%
0
?96 ?97 ?98 ?99 ?00 ?01
All taxpayers
Affluent taxpayers
1.2%
continue to slide
Audits of affluent
were audited by the IRS:
Percentage of taxpayers who
0
0.6%
1.8%
Figure 11: Graphic from USA Today
Extending Document Summarization to Information Graphics
?Sandra Carberry, ??Stephanie Elzer, ? ? ?Nancy Green, ?Kathleen McCoy and ?Daniel Chester
?Dept. of Computer Science, University of Delaware, Newark, DE 19716
(carberry, mccoy, chester@cis.udel.edu)
??Dept. of Computer Science, Millersville Univ., Millersville, PA 17551
(elzer@cs.millersville.edu)
? ? ?Dept. of Math. Sciences, Univ. of North Carolina at Greensboro, Greensboro, NC 27402
(nlgreen@uncg.edu)
Abstract
Information graphics (non-pictorial graphics such
as bar charts or line graphs) are an important
component of multimedia documents. Often such
graphics convey information that is not contained
elsewhere in the document. Thus document summa-
rization must be extended to include summarization
of information graphics. This paper addresses our
work on graphic summarization. It argues that the
message that the graphic designer intended to con-
vey must play a major role in determining the con-
tent of the summary, and it outlines our approach
to identifying this intended message and using it to
construct the summary.
1 Introduction
Summarization work has focused primarily on the
written words in a document. However, graphics
are an important part of many documents, and they
often convey information that is not included else-
where in the document. Thus as text summarization
branches out, it is essential that it consider the sum-
marization of graphical information in documents.
Graph summarization has received some atten-
tion. (Yu et al, 2002) has used pattern recogni-
tion techniques to summarize interesting features of
automatically generated graphs of time-series data
from a gas turbine engine. (Futrelle and Nikolakis,
1995) developed a constraint grammar formalism
for parsing vector-based visual displays and produc-
ing structured representations of the elements com-
prising the display. The goal of Futrelle?s project
is to produce a graphic that summarizes one or
more graphics from a document (Futrelle, 1999).
The summary graphic might be a simplification of
a graphic or a merger of several graphics from the
document, along with an appropriate summary cap-
tion. Thus the end result of summarization will it-
self be a graphic.
Our project is concerned with information graph-
ics (non-pictorial graphics such as bar charts or line
graphs). Our current focus is on providing an ini-
tial summary of an information graphic, within a
larger interactive natural language system that can
respond to followup questions about the graphic.
There are several useful applications for a system
that can summarize information graphics. For dig-
ital libraries, the initial summary of the graphic
will be used in conjunction with the document
text/summary to provide a more complete represen-
tation of the content of the document to be used
for searching and indexing. In the case of environ-
ments with low-bandwidth transmission and minia-
ture viewing facilities, such as cellular telephones
for accessing the web, the initial summary and fol-
lowup capability will provide an alternative modal-
ity for access to the document.
However, the most compelling application of the
overall system is to provide effective access to in-
formation graphics for individuals with sight im-
pairments. The rapidly growing Information Infras-
tructure has had a major impact on society and the
development of technology. However, the growing
reliance on visual information display paradigms
obliges society to ensure that individuals with visual
impairments can access and assimilate information
resources as effectively as their sighted counter-
parts. The underlying hypothesis of our work is that
alternative access to what the graphic looks like is
not enough ? the user should be provided with the
message and knowledge that one would gain from
viewing the graphic in order to enable effective and
efficient use of this information resource. Thus our
system will present the user with an initial summary
that includes the primary message that the graphic
designer intended to convey, augmented with rel-
evant interesting features of the graphic, and then
interactively allow the user to access more detailed
summaries of information contained in the graphic.
As an example of the kinds of summaries that we
envision, consider the information graphic in Fig-
ure 1. The graphic designer?s communicative goal is
ostensibly to convey the sharp increase in bankrupt-
cies in 2001 compared with the previous decreasing
trend. More detailed features that might be of inter-
est include 1) that bankruptcies had been decreasing
at a steady rate since 1998, 2) that bankruptcies had
been decreasing slowly since 1998, 3) the percent-
age decrease each year, 4) the percentage increase
in bankruptcies in 2001, 5) the absolute increase in
bankruptcies in 2001, and 6) the total number of
bankruptcies in 2001. Thus the initial summary of
this graphic might be
This graphic shows that although
Delaware bankruptcy personal filings
decreased slowly and steadily from 1998
to 2000, they rose sharply in 2001.
Note that the proposed summary includes the hy-
pothesized intended message of the graphic, along
with the first two of the additional interesting fea-
tures of the graphic. The selection of additional fea-
tures to augment the summary is discussed further
in Section 3.3. The system would then respond to
user requests for additional information by present-
ing some or all of the other interesting features that
had been identified, as discussed in Section 3.4.
This paper provides an overview of our project.
Section 2 discusses the essential role of intention
recognition in graphics summarization. It argues
not only that the intended message of the graphic
designer must be inferred and included in a sum-
mary of a graphic, but also that the intended mes-
sage significantly influences the additional propo-
sitions that should be included in the summary.
Section 3 presents our approach to graph summa-
rization. It discusses how we use a computer vi-
sion module to construct an XML representation
that captures the components of the graphic and
their relationship to one another, and how we use
a Bayesian belief network to hypothesize the inten-
tions of the graph designer. The paper then dis-
cusses our plans for constructing a summary that
includes the graphic designer?s intended message
along with highly ranked additional propositions,
and how the lesser ranked propositions will be used
in an interactive natural language system that re-
sponds to the user?s requests for further summaries
of additional features of the graphic.
2 The Role of Intention in Graphics
Summarization
Text summarization has generally relied on statis-
tical techniques and identification and extraction
of key sentences from documents. However, it is
widely acknowledged that to truly understand a text
and produce the best summary, one must under-
stand the document and recognize the intentions of
the author. Recent work in text summarization has
personal filings
Delaware bankruptcy
3000
2500
1000
1500
2000
1998 1999 2000 2001
Figure 1: Graphic from a City Newspaper
60 70 80 90 01
$15
10
5
Black women
White women
Median Income
In thousands of 2001 dollars
1948
Figure 2: Graphic from Newsweek Magazine
begun to address this issue. For example, (Marcu,
2000) presents algorithms for automatically identi-
fying the rhetorical structure of a text and argues
that the hypothesized rhetorical structure can be
successfully used in text summarization.
Information graphics are an important component
of many documents. In some cases, information
graphics are stand-alone and constitute the entire
document. This is the case for many graphics ap-
pearing in newspapers, such as the graphic shown
in Figure 1. On the other hand, when an article is
comprised of text and graphics, the graphic gener-
ally expands on the text and contributes to the dis-
course purpose (Grosz and Sidner, 1986) of the arti-
cle. For example, Figure 2 illustrates a graphic from
Newsweek showing that the income of black women
has risen dramatically over the last decade and has
reached the level of white women. Although this in-
formation is not conveyed elsewhere in the article, it
contributes to the overall communicative intention
of this portion of the article ? namely, that there
has been a ?monumental shifting of the sands? with
regard to the achievements of black women.
Our project is concerned with the understand-
ing and summarization of information graphics: bar
charts, line graphs, pie charts, etc. We contend that
analyzing the data points underlying an informa-
tion graphic is insufficient. One must instead iden-
tify the message that the graphic designer intended
to convey via the design choices that were made
in constructing the graphic. (Although one might
suggest relying on captions to provide the intended
message of a graphic, Corio and Lapalme found
in a large corpus study (Corio and Lapalme, 1999)
that captions are often missing or are very general
and uninformative; our collected corpus of informa-
tion graphics supports their observations.) Design
choices include selection of chart type (bar chart,
pie chart, line graph, etc.), organization of informa-
tion in the chart (for example, aggregation of bars in
a bar chart), and attention-getting devices that high-
light certain aspects of a chart (such as coloring one
bar of a bar chart different from the others). Not
only should the graphic designer?s intended mes-
sage comprise the primary component of any sum-
mary, but this intended message has a strong influ-
ence on the salience of additional propositions that
might be included in the summary.
To see the importance of recognizing the graphic
designer?s intended message, consider the two
graphics in Figure 3. The one on the left, Fig-
ure 3a, appeared in an NSF publication. Both graph-
ics were constructed from the same data set. The
intended message of the graphic in Figure 3a is that
the salary of females is consistently less than that of
males for each of the science and engineering dis-
ciplines.1 Notice that the graphic designer selected
an organization for the graphic in Figure 3a that fa-
cilitated the comparison between male and female
salaries in each field. A different display of the
same data would facilitate different analyses. For
example, the graph in Figure 3b depicts the same
data as the graph in Figure 3a, yet the organiza-
tion tends to draw attention to comparisons within
male and female groups rather than between them,
1This graphic was constructed by a colleague who served
on the NSF panel that prepared the report. Thus we know the
intentions underlying the graphic.
and perhaps an integration/comparison of the mes-
sages conveyed by the two subgraphs. Thus the in-
tended message of the graphic in Figure 3b appears
to be that the ranking of the disciplines by salary are
about the same for both men and women. The dis-
tinctions between presentation formats illustrate the
extent to which the format can itself convey infor-
mation relevant to the graphic designer?s intended
message.
Now let us consider how the intended message
influences additional information that might be in-
cluded in a summary. Suppose that 1) the salary
differential between females and males was signif-
icantly larger in the life sciences than in other dis-
ciplines and 2) the average salary for both females
and males was much larger in engineering than in
any of the other disciplines. Feature 1) would be
particularly interesting and relevant to the intended
message of Figure 3a, and thus should be included
as part of the graphic?s summary. On the other hand,
this aspect would be less relevant to the intended
message of Figure 3b and thus not as important to
include. Similarly, Feature 2) would be particularly
relevant to the intended message of Figure 3b and
thus should be given high priority for inclusion in
its summary. Although an interactive system that
could analyze a graphic to any desired level of de-
tail might extract from the graphic the information
in both 1) and 2) above, we contend that a summary
of the graphic should prioritize content according to
its relevance to the designer?s intended message.
3 Graphic Summarization
Our architecture for graphic summarization consists
of modules for identifying the components of the
graphic, hypothesizing the graphic designer?s in-
tended message, planning the content of the sum-
mary, organizing a coherent summary, and interac-
tive followup. The following sections discuss four
of these modules.
3.1 Analyzing and Classifying a Graphic
The visual extraction module takes a screen image
of an information graphic. It is responsible for rec-
ognizing the individual components comprising the
graphic, identifying the relationship of the different
components to one another and to the graphic as a
whole, and classifying the graphic as to type. This
includes using heuristics (such as relative position
of a string of characters) to identify the axis labels
? for example, that the y-axis label is Delaware
2The source of the leftmost graph is the National Science
Foundation, Survey of Doctorate Recipients, 1997.
  
 
 
 
 
 
 

















 
 
 
 
 
 
 
 
 




































		
		
		
		
		
		
		
		
		





































80,000
70,000
60,000
50,000 50,000
60,000
70,000
80,000
40,000
30,000
20,000 20,000
30,000
40,000
FEMALE SALARIES MALE SALARIES
Computer/All
Math Sci
Engin. Phys.
Sci. Sci.
Social
Sci.
Life Sci.
Social Sci.
A
ll
Com
puter/M
ath Sci.
Phys Sci.
Engineering
Social Sci.
Life Sci.
Com
puter/M
ath Sci.
A
ll
Phys Sci.
Engineering
Life
Female
Male
(a) (b)
Figure 3: Two alternative graphs from the same data2
bankruptcy personal filings in Figure 1. Our cur-
rent implementation deals only with gray scale im-
ages (in pgm format) of bar charts, pie charts, and
line graphs, though eventually it will be extended to
handle color and other kinds of information graph-
ics. The output of the visual extraction component
is an XML file that describes the chart and all of its
components.
3.2 Identifying the Intended Message
The second module of our architecture is respon-
sible for inferring the graphic designer?s intended
message. In their work on multimedia generation,
the AutoBrief group proposed that speech act the-
ory can be extended to the generation of graphical
presentations (Kerpedjiev and Roth, 2000; Green et
al., 2004). They contended that the graphic design
was intended to convey its message by facilitating
requisite perceptual and cognitive tasks. By percep-
tual tasks we mean tasks that can be performed by
simply viewing the graphic, such as finding the top
of a bar in a bar chart; by cognitive tasks we mean
tasks that are done via mental computations, such as
computing the difference between two numbers.
The goal of our intention recognizer is the inverse
of the design process: namely, to use the displayed
graphic as evidence to hypothesize the communica-
tive intentions of its author. This is done by an-
alyzing the graphic to identify evidence about the
designer?s intended message and then using plan
recognition (Carberry, 1990) to hypothesize the au-
thor?s communicative intent.
3.2.1 Evidence about Intention
Following AutoBrief (Kerpedjiev and Roth, 2000),
we hypothesize that the graphic designer chooses
a design that makes important tasks (the ones that
the viewer is intended to perform in recognizing the
graphic?s message) as salient or as easy as possi-
ble. Thus salience and ease of performance should
be taken into account in reasoning about the graphic
designer?s intentions.
There are several ways that a task can be made
salient. The graphic designer can draw attention
to a component of a graphic (make it salient) by
an attention-getting or highlighting device, such as
by coloring a bar in a bar chart differently from
the other bars as in Figure 1 or by exploding a
wedge in a pie chart (Mittal, 1997). Attributes of
the highlighted graphic component are treated as
focused entities. Nouns in captions also serve to
establish focused entities. For example, a caption
such as ?Studying not top priority? would estab-
lish the noun studying as a focused entity. Focused
entities that appear as instantiations of parameters
in perceptual or cognitive tasks serve as evidence
that those tasks might be particularly salient. Sim-
ilarly, verbs that appear in captions serve as evi-
dence for the salience of particular tasks. For ex-
ample, the verb beats in a caption such as ?Canada
Beats Europe? serves as evidence for the salience
of a Recognize relative difference task. In the fu-
ture, we plan to capture the influence of surrounding
text by identifying the important concepts from the
text using lexical chains. Lexical chains have been
used in text summarization (Barzilay et al, 1999),
and our linear time algorithm (Silber and McCoy,
2002) makes their computation feasible even for
large texts. Whether a task is salient and the method
by which it was made salient are used as evidence
in our plan inference system.
The graphic design makes some tasks easier than
others. We use a set of rules, based on research by
cognitive psychologists, to estimate the relative ef-
fort of performing different perceptual and cogni-
tive tasks. These rules, described in (Elzer et al,
2004), have been validated by eye-tracking experi-
ments. Since the viewer is intended to recognize the
message that the graphic designer wants to convey,
we contend that the designer will choose a graphic
design that makes the requisite tasks easy to per-
form. This was illustrated in the two graphics in
Figure 3. The relative effort of performing a task is
thus used as another source of evidence in our plan
inference framework.
3.2.2 The Plan Inference Process
Our plan inference framework takes the form of
a Bayesian belief network. Bayesian belief net-
works have been applied to a variety of problems,
including reasoning about utterances (Charniak and
Goldman, 1993) and observed actions (Albrecht et
al., 1997). The belief network uses plan operators,
along with evidence that is gleaned from the infor-
mation graphic itself (as discussed in the preceding
section), to reason about the likelihood that vari-
ous hypothesized candidate plans represent the in-
tentions of the graphic designer.
Plan Operators for Information Graphics Our
system uses plan operators that capture knowledge
about how the graphic designer?s goal of conveying
a message can be achieved via the viewer perform-
ing certain perceptual and cognitive tasks, as well
as knowledge about how information-access tasks,
such as finding the value of an entity in a graphic,
can be decomposed into simpler subgoals. Our plan
operators consist of:
? Goal: the goal that the operator achieves
? Data-requirements: requirements that the data
must satisfy in order for the operator to be ap-
plicable in a graphic planning paradigm
? Display-constraints: features that constrain
how the graphic is eventually constructed if
this operator is part of the final plan
? Body: lower-level subgoals that must be ac-
complished in order to achieve the overall goal
of the operator.
Figures 4 and 5 present two plan operators for the
goal of finding the value <v> of an attribute <att>
for a graphical element <e> (for example, the value
associated with the top of a bar in a bar chart). The
body of the operator in Figure 4 specifies that the
goal can be achieved by a primitive perceptual task
in which the viewer just perceives the value; this
could be done, for example, if the element in the
graphic is annotated with its value. On the other
hand, the body of the operator in Figure 5 captures a
different way of finding the value, one that presum-
ably requires more effort. It specifies the perceptual
task of finding the values <l1> and <l2> surround-
ing the desired value on the axis along with the frac-
tion <f> of the distance that the desired value lies
between <l1> and <l2>, followed by the cogni-
tive task of interpolating between the retrieved val-
ues <l1> and <l2>.
Plan inference uses the plan operators to reasons
backwards from the XML representation of the ob-
served graphic (constructed by the visual extraction
module briefly described in Section 3.1). The dis-
play constraints are used to eliminate operators from
consideration ? if the graphic does not capture the
operator?s constraints on the display, then the opera-
tor could not have been part of a plan that produced
the graphic. The data requirements are used to in-
stantiate parameters in the operator ? the data must
have had certain characteristics for the operator to
have been included in the graphic designer?s plan,
and these often limit how the operator?s arguments
can be instantiated.
The Bayesian Belief Network The plan operators
are used to dynamically construct a Bayesian net-
work for each new information graphic. The net-
work includes the possible top level communicative
intentions (with uninstantiated parameters), such as
the intention to convey a trend, and the alternative
ways of achieving them via different plan opera-
tors. The perceptual tasks of lowest effort and the
tasks that are hypothesized as potentially salient are
added to the network. Other tasks are entered into
the network as they are inferred during chaining on
the plan operators; unification serves to instantiate
parameters in higher-level nodes. Evidence nodes
are added for each of the tasks entered into the net-
work, and they provide evidence (such as the degree
of perceptual effort required for a task or whether
a parameter of the task is a focused entity in the
graphic as discussed in Section 3.2.1) for or against
the instantiated tasks to which they are linked. Af-
ter propagation of evidence, the top-level intention
with the highest probability is hypothesized as the
graphic designer?s primary intention for the graphic.
Of course, a Bayesian network requires a set of
conditional probabilities, such as 1) the probability
that perceptual Task-A will be of low, medium, or
high effort given that the graphic designer?s plan in-
cludes the viewer performing Task-A, 2) the prob-
ability that parameter <x> of Task-A will be a fo-
Goal: Find-value(<viewer>, <g>, <e>, <ds>, <att>, <v>)
Gloss: Given graphical element <e> in graphic <g>, <viewer> can find the value <v>
in dataset <ds> of attribute <att> for <e>
Data-req: Dependent-variable(<att>, <ds>)
Body: 1. Perceive-dependent-value(<viewer>, <g>, <att>, <e>, <v>)
Figure 4: Operator for achieving a goal perceptually
Goal: Find-value(<viewer>, <g>, <e>, <ds>, <att>, <v>)
Gloss: Given graphical element <e> in graphic <g>, <viewer> can find the value <v>
in dataset <ds> of attribute <att> for <e>
Data-req: Natural-quantitative-ordering(<att>)
Display-const: Ordered-values-on-axis(<g>, <axis>, <att>)
Body: 1. Perceive-info-to-interpolate(<viewer>,<g>,<axis>,<e>,<l1>,<l2>,<f>)
2. Interpolate(<viewer>, <l1>, <l2>, <f>, <v>)
Figure 5: Operator that employs both perceptual and cognitive subgoals
cused entity in the caption given that the graphic de-
signer?s plan includes the viewer performing Task-
A, or 3) the probability that the viewer perform-
ing Task-B will be part of the designer?s intended
plan given that Task-A is part of his plan. (Note that
there may be several alternative ways of perform-
ing a particular task, as illustrated by the two plan
operators displayed in Figures 4 and 5.) We have
collected a rapidly expanding corpus of information
graphics, and have analyzed a small part of this cor-
pus to construct an initial set of probabilities. The
results suggest that our approach is very promising.
We will increase the number of analyzed graphics
to improve the probability estimates.
3.3 Planning the Content of the Summary
The recognized intention of the graphic designer,
such as to convey an overall increasing trend or to
compare salaries of females and males in different
disciplines as in Figure 3a, will provide one set of
highly salient propositions that should be included
in the graphic?s summary. Once the intentions have
been recognized, other visual features of the graphic
will influence the identification of additional salient
propositions.
We conducted a set of experiments in which sub-
jects were asked to write a brief summary of a set of
line graphs, each of which arguably could be said
to have the same high-level intention. Although
each summary included the high-level intention, the
summaries often differed significantly for different
graphs. By comparing these with summaries of the
same graph by different subjects, we have hypoth-
esized that certain features, such as the variance of
the data, can influence the generated summary, and
that the importance of including a specific feature in
a summary is related to the high-level intention of
the graphic. For example, variation in the data will
be relevant for an intention of conveying a trend,
but it will be less important than the overall slope
of the data points. This impact of the intended mes-
sage on the priority of including a specific feature
in a graphic was illustrated in Section 2, where we
showed how a significantly larger differential be-
tween female and male salaries for one particular
discipline would be more relevant to the summary of
the graphic in Figure 3a than for the graphic in Fig-
ure 3b. In addition, our experiments indicate that the
strength of a feature in the graphic also influences
its inclusion in a summary. For example, the more
ragged a sequence of line segments, the more salient
variance becomes for inclusion in a summary.
Once the content planning module has identified
and ranked interesting features that might augment
the intended message of the graphic, the most im-
portant propositions will be organized into a coher-
ent summary that can be stored for access in a digital
library or presented to a user. In the future, we will
also investigate integrating the summary of an infor-
mation graphic with the summary of its surrounding
text.
3.4 Interactive Followup
One of the primary goals of our work is an inter-
active natural language system that can convey the
content of an information graphic to a user with
sight impairments. For this application, the sum-
mary will be rendered in natural language and con-
veyed as an initial summary to the user via speech
synthesis. The system will then provide the user
with the opportunity to seek additional information.
We will utilize the propositions that were not in-
cluded in the initial message as indicative of ad-
ditional information about the graphic that might
be useful. Several kinds of followup will be pro-
vided. For example, if the user requests focused
followup, the system will categorize the remaining
propositions (for example, extreme values, trend de-
tail, etc.) and ask the user to select one of the cate-
gories of further information. The system will then
construct a followup message summarizing the most
important (often all) of the remaining propositions
in the selected category. This interactive followup
will continue until either all the propositions have
been conveyed or the user terminates the followup
cycle.
4 Summary
This paper extends document summarization to the
summarization of information graphics. It argues
that an effective summary must be based on the
message that the graphic designer intended to con-
vey in constructing the graphic, and that this in-
tended message strongly influences the relevance
of other propositions that might be included in the
summary. The paper describes our approach to
graphic summarization, including our plan infer-
ence system for inferring the intended message un-
derlying a graphic. This work has many applica-
tions. These include enabling information graphics
to be accessed via content in a digital library, allow-
ing access to information graphics via devices with
small bandwidth (such as cellular phones), and most
importantly making information graphics accessible
to individuals with sight impairments via an interac-
tive natural language system that can provide sum-
maries at various levels of detail.
References
David Albrecht, Ingrid Zukerman, Ann Nicholson,
and A. Bud. 1997. Towards a bayesian model
for keyhole plan recognition in large domains.
In Proceedings of the Sixth International Confer-
ence on User Modeling, pages 365?376.
R. Barzilay, K. McKeown, and M. Elhadad. 1999.
Information fusion in the context of multi-
document summarization. In Proc. of the 37th
Annual Meeting of the ACL, pages 550?557.
Sandra Carberry. 1990. Plan Recognition in Natu-
ral Language Dialogue. ACL-MIT Press Series
on Natural Language Processing. MIT Press.
Eugene Charniak and Robert Goldman. 1993. A
bayesian model of plan recognition. Artificial In-
telligence Journal, 64:53?79.
Marc Corio and Guy Lapalme. 1999. Generation of
texts for information graphics. In Proceedings of
the 7th European Workshop on Natural Language
Generation EWNLG?99, pages 49?58.
Stephanie Elzer, Nancy Green, Sandra Carberry,
and James Hoffman. 2004. Incorporating per-
ceptual task effort into the recognition of inten-
tion in information graphics. In Diagrammatic
Representation and Inference: Proceedings of
the Third International Conference on the Theory
and Application of Diagrams, LNAI 2980, pages
255?270.
Robert Futrelle and Nikos Nikolakis. 1995. Ef-
ficient analysis of complex diagrams using
constraint-based parsing. In Proceedings of the
Third International Conference on Document
Analysis and Recognition.
Robert Futrelle. 1999. Summarization of diagrams
in documents. In I. Mani and M. Maybury, edi-
tors, Advances in Automated Text Summarization.
MIT Press.
Nancy Green, Giuseppe Carenini, Stephan Kerped-
jiev, Joe Mattis, Johanna Moore, and Steven
Roth. 2004. Autobrief: An experimental system
for the automatic generation of briefings in inte-
grated text and graphics. International Journal of
Human-Computer Studies. to appear.
Barbara Grosz and Candace Sidner. 1986. Atten-
tion, Intentions, and the Structure of Discourse.
Computational Linguistics, 12(3):175?204.
Stephan Kerpedjiev and Steven Roth. 2000. Map-
ping communicative goals into conceptual tasks
to generate graphics in discourse. In Proceed-
ings of the International Conference on Intelli-
gent User Interfaces, pages 60?67.
Daniel Marcu. 2000. The rhetorical parsing of un-
restricted texts: A surface-based approach. Com-
putational Linguistics, 26(3):395?448.
Vibhu Mittal. 1997. Visual prompts and graphical
design: A framework for exploring the design
space of 2-d charts and graphs. In Proceedings
of the Fourteenth National Conference on Artifi-
cial Intelligence, pages 57?63.
Gregory Silber and Kathleen McCoy. 2002. Effi-
ciently computed lexical chains as an intermedi-
ate representation for automatic text summariza-
tion. Computational Linguistics, 28(4):487?496.
Jin Yu, Jim Hunter, Ehud Reiter, and Somaya-
julu Sripada. 2002. Recognising visual patterns
to communicate gas turbine time-series data. In
ES2002, pages 105?118.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 600?609, Dublin, Ireland, August 23-29 2014.
Identifying Important Features for Graph Retrieval
Zhuo Li and Sandra Carberry and Hui Fang* and Kathleen F. McCoy
ivanka@udel.edu carberry@udel.edu hui@udel.edu mccoy@udel.edu
Department Computer and Information Science,
*Department of Electrical and Computer Engineering
University of Delaware
Abstract
Infographics, such as bar charts and line graphs, occur often in popular media and are a rich
knowledge source that should be accessible to users. Unfortunately, information retrieval re-
search has focused on the retrieval of text documents and images, with almost no attention specif-
ically directed toward the retrieval of information graphics. Our work is the first to directly tackle
the retrieval of infographics and to design a system that takes into account their unique charac-
teristics. Learning-to-rank algorithms are applied on a large set of features to develop several
models for infographics retrieval. Evaluation of the models shows that features pertaining to the
structure and the content of graphics should be taken into account when retrieving graphics and
that doing so results in a model with better performance than a baseline model that relies on
matching query words with words in the graphic.
1 Introduction
Infographics are non-pictorial graphics such as bar charts and line graphs. When such graphics appear in
popular media, they generally have a high-level message that they are intended to convey. For example,
the graphic in Figure 1 ostensibly conveys the message that Toyota has the highest profit among the
automobile companies listed. Thus infographics are a form of language since, according to Clark (Clark
and Curran, 2007), language is any deliberate signal that is intended to convey a message.
Although much research has addressed the retrieval of documents, very little attention has been given
to the retrieval of infographics. Yet research has shown that the content of an infographic is often not
included in the article?s text (Carberry et al., 2006). Thus infographics are an important knowledge
source that should be accessible to users of a digital library.
Techniques that have been effective for document or image retrieval are inadequate for the retrieval
of infographics. Current search engines employ strategies similar to those used in document retrieval,
relying primarily on the text surrounding a graphic and web link structures. But the text in the surround-
ing document generally does not refer explicitly to the infographic or even describe its content (Carberry
et al., 2006). An obvious extension to using the article text would be to collect all the words in an
infographic and use it as a bag of words. However, infographics have structure and often a high-level
message, and bag of words approaches ignore this structure and message content.
This paper explores the features that should be taken into account when ranking graphics for retrieval
in response to a user query. Using a learning-to-rank algorithm on a wide range of features (including
structural and content features), we produce a model that performs significantly better than a model that
ignores graph structure and content. Analysis of the model shows that features based on the structure
and content of graphs are very important and should not be ignored. To our knowledge, our research is
the first to take graph structure and content into account when retrieving infographics.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
600
Figure 1: An Example Infographic
2 Related Work
Information retrieval research has focused on the retrieval of text documents and images. Two popular
approaches to text retrieval are the vector space method and probabilistic methods. The vector space
method (Dubin, 2004) represents the document and the query each as a vector of weighted words and
then uses a similarity function to measure the similarity of each document to the query. Most weighting
mechanisms reward words that occur frequently in both the document and query but infrequently in
the overall collection of documents. Probabilistic retrieval models instead estimate the probability that
a document is relevant to a user query. In recent years, the language modeling approach has shown
promise as a retrieval strategy with sound statistical underpinnings (Lv and Zhai, 2009; Manning et al.,
2008). In all of the above approaches, query expansion techniques have been used to expand the query
with synonyms and related words before ranking documents for retrieval. Work on short document and
query expansion have shown improvements in retrieval performance (Arguello et al., 2008; Escalante et
al., 2008; Metzler and Cai, 2011).
Work in Content Based Image Retrieval (CBIR) (Datta et al., 2008) has progressed from systems that
retrieved images based solely on visual similarity, relying on low-level features such as color, texture and
shape ( (Flickner et al., 1995; Swain and Ballard, 1991; Smith and Chang, 1997; Gupta and Jain, 1997),
among others), to systems which attempt to classify and reason about the semantics of the images being
processed (Bradshaw, 2000; Smeulders et al., 2000; Datta et al., 2008). However, images are free-form
with relatively little inherent structure; thus it is extremely difficult to determine what is conveyed by an
image, other than to list the image?s constituent pieces. Most systems that retrieve infographics, such
as SpringerImages (http://www.springerimages.com) and Zanran (http://www.zanran.com), are based on
textual annotations of the graphics as in image retrieval (Gao et al., 2011) or on matching the user?s query
against the text surrounding the graphic. However, the structure and content of the graph are not taken
into consideration.
In this paper, we focus on natural language queries given that such queries allow users to express their
specific information need more clearly than keywords (Phan et al., 2007; Bendersky and Croft, 2009).
Previous work on verbose and natural language queries (Bendersky and Croft, 2008; Liu et al., 2013)
used probabilistic models and natural language processing techniques to identify the key contents in
such queries. Our query processing method not only extracts key entities but also further classifies the
extracted key entities into different components using a learned decision tree model.
3 Problem Formulation
Our research is currently limited to two kinds of infographics: simple bar charts and single line graphs.
We assume that our digital library contains an XML representation of each graphic that includes 1) the
graphic?s image, 2) its structural components: the set of independent axis (x-axis) labels
1
, the entity being
measured on the dependent axis (y-axis), and the text that appears in the graphic?s caption, referred to
as G
x
, G
y
, and G
c
respectively, and 3) the graphic?s intended message G
m
and any entities G
f
that the
1
We will refer to the independent axis as the x-axis and the dependent axis as the y-axis throughout this paper.
601
message focuses on. This paper is not concerned with the computer vision problem of recognizing the
bars, labels, colors, etc. in a graphic; other research efforts, such as the work in (Chester and Elzer, 2005;
Futrelle and Nikolakis, 1995) are addressing the parsing of electronic images such as bar charts and line
graphs.
Prior research on our project has addressed issues that arise in recognizing G
y
, G
m
, and G
f
. The
dependent axis of an infographic often does not explicitly label what is being measured, such as net
profit in Figure 1, and these must be inferred from other text in the graphic. Our prior work (Demir et
al., 2007) identified a hierarchy of graphic components in which pieces of the entity being measured
might appear; a set of heuristics were constructed that extracted these pieces and melded them together
to form what we refer to as a measurement axis descriptor and which is G
y
. The project?s prior work
also identified a set of 17 categories of intended message, such as Rank, Relative-difference, Maximum,
and Rising-trend, that might be conveyed by simple bar charts and line graphs; a Bayesian system (Elzer
et al., 2011; Wu et al., 2010) was developed that utilizes communicative signals in a graphic (such as the
coloring of one bar differently from the other bars) in order to recognize a graphic?s intended message,
including both the message category and the parameters of the message such as any focused entity. For
example, the intended message of the bar chart in Figure 1 is ostensibly that Toyota has the highest net
profit of any of the automobile manufacturers listed; thus its message falls into the Maximum message
category and its focused entity is Toyota.
Our vision is that since graphics have structure and content, the users whose particular information
needs could be satisfied by an infographic will formulate their queries to indicate the requisite structure
of the desired graphics. Thus we assume the use of full-sentence queries so that the semantics of the query
can be analyzed to identify characteristics of relevant graphics. For example, consider the following two
queries that contain similar keywords but represent different information needs:
Q
1
: Which countries have the highest occurrence of rare diseases?
Q
2
: Which rare diseases occur in the most countries?
These two queries contain almost identical words but are asking for completely different graphics. Query
Q
1
is asking for a comparison of countries (independent axis) according to their occurrence of rare dis-
eases (dependent axis) while query Q
2
is asking for a comparison of different rare diseases (independent
axis) according to the number of countries in which they occur (dependent axis). In addition, both queries
are asking for a graphic with a Rank message that ranks countries (query Q
1
) or rare diseases (query Q
2
)
as opposed to a graphic that shows the trend in rare diseases throughout the world.
4 Methodology
To retrieve relevant graphics in response to a user query, the query will first be analyzed to identify requi-
site characteristics of relevant infographics. We have developed learned decision trees (Li et al., 2013a;
Li et al., 2013b) for analyzing a query and identifying the requisite structure of relevant infographics (the
content of the independent axis or x-axis and dependent axis or y-axis, referred to as Q
x
and Q
y
), and
the category of intended message and focused entity, if any, (referred to as Q
m
and Q
f
) that will best
satisfy the user?s information need.
Given a new user query, it is parsed and noun phrases are extracted. Each query-phrase pair, consisting
of a query and an extracted noun phrase, is processed by a decision tree that determines whether the noun
phrase represents x-axis content, y-axis content, or neither. Attributes used by this decision tree include
whether the main verb of the query is a comparison verb (such as ?differ? and ?compare?) or a trend
verb (such as ?change? and ?decrease?), whether the noun phrase is preceded by a quantity phrase such
as ?the number of? suggesting that the noun phrase specifies y-axis content of relevant infographics, and
whether the noun phrase describes a period of time.
Similarly, another decision tree is constructed to identify the category of graph intended message
(such as Trend or Rank) that the query desires, using a subset of the attributes from the axes decision tree
combined with the classification results of the axes decision tree. An example of the reused attributes is
the class of the main verb in the user query; for example, a comparison main verb suggests that relevant
infographics will convey a comparison-based intended message, such as a Relative-difference or Rank
602
intended message. Other attributes include the presence of a superlative or comparative in the query
and attributes depending on the identified content of the x and y axes by the axes decision tree, such
as the number of x-axis entities, their plurality, and whether an x-axis entity describes a time interval.
A third decision tree is constructed for identifying whether a noun phrase describes a specific focused
x-axis entity. Then the infographics in the digital library must be rank-ordered according to how well
they satisfy the requirements of the user query.
This paper is concerned with identifying the most important features in a metric for rank-ordering
the graphics in response to a user query. We experiment with two learning-to-rank algorithms and 56
features that include both general features such as bag of words comparisons and structural and content
features. Our hypothesis is that structural and content-based features play an important role in graph
retrieval and cannot be ignored. Section 5 discusses the features used in our experiments, Section 6
discusses the learning algorithms, Section 6.1 compares the resultant models with a baseline that uses
just general features treating query and graphic each as one bag of words, and Section 6.2 discusses the
features that appear most influential in the models.
5 Features
We consider three kinds of features: 1) general features that compare words in the query with words in
the graphic, 2) structural features that compare the requisite structure hypothesized from the query with
the structure of candidate infographics, and 3) content-based features that compare the requisite message
hypothesized from the user query with the intended message of candidate graphics.
Query expansion is a commonly used strategy in information retrieval to bridge the vocabulary gap
between terms in a query and those in documents. The basic idea is to expand the original query with
terms that are semantically similar to the ones in the query. This addresses the problem encountered when
the query uses the word car but the document uses the term automobile. But retrieval of information
graphics presents an additional problem. Consider a query such as ?Which car manufacturer has the
highest net profit?? A graphic such as the one in Figure 1 displays a set of car manufacturers on the x-
axis (Toyota, Nissan, etc.) but nowhere in the graphic does the word car or a synonym appear. Identifying
the ontological category, such as car or automobile, of these labels is crucial since the user?s query often
generalizes the entities on the independent axis of relevant graphs rather than listing them.
To expand a given text string s, we use Wikimantic (Boston et al., 2013), a term expansion method
that uses Wikipedia articles as topic concepts. A topic concept is a unigram distribution built from words
in the Wikipedia article for that topic. A string s is interpreted by Wikimantic into a mixture concept that
is a weighted vector of topic concepts that capture the semantic meaning of the words in s. Each topic
concept is weighted by the likelihood that the concept (Wikipedia article) generates the text string s. The
weighted concepts are then used to produce a unigram distribution of words that serve as the expansion of
the terms in the string s. One issue in graph retrieval is correlating the requisite x-axis content specified
in the user query with the x-axis labels in graphs. A query such as ?Which car manufacturer has ... ??
is requesting a graph where ?car manufacturers? are listed on the x-axis. Thus we need to recognize
individual x-axis words which are often proper nouns (e.g., ?Ford?, ?Nissan?, ?Honda?) as instances of
car manufacturers. In the case of labels on the independent axis (such as Toyota, Nissan, Honda, etc.),
words such as car or automobile are part of the produced unigram distribution ? that is, as a side effect,
the ontological category of the individual entities becomes part of the term expansion.
We use Wikimantic to interpret and expand each of the graph components G
x
, G
y
, G
f
, and G
c
. The
expansion of the graph components (as opposed to the typical expansion of the query) accomplishes two
objectives: 1) it addresses the problem of sparse graphic text by adding semantically similar words and
2) it addresses the problem of terms in the query capturing general classes (such as car or automobile)
when the graphic instead contains an enumeration of members of the general class. Expansion of the
words in the graphics, unlike query expansion, has the added advantage that it is completed in advance
and off-line.
603
5.1 General Features
Our general feature set includes 17 general features capturing a variety of different kinds of relevance
scorings between two bags of words consisting respectively of words from the user query and words
from the candidate infographic:
? GF
1
: A modified version of Okapi-BM25 (Fang et al., 2004) calculated as:
Okapi-BM25 Score =
?
w?Q
log
|D|+1
df
w
+1
?
tf
w
?(1+k
1
)
tf
w
+k
1
where Q is a query, |D| is the number of graphs in the digital library, w is a query word in Q,
df
w
is the frequency of graphs containing word w in the digital library, tf
w
is the frequency of
word w in the text expansion of the given graphic, and k
1
is a parameter that is typically set to 1.2.
Okapi-BM25 is a bag-of-words ranking function used in many information retrieval systems. Our
modified version of Okapi-BM25 addresses the problem of negative values that can occur with the
original Okapi formula. In addition, our formula does not take text length or query term frequency
into account since graphics have relatively similar amounts of text and most terms in a query occur
only once.
? GF
2
: The term frequency-inverse document frequency (tf-idf) value of query words that appear in
the expanded graphic.
? GF
3
: The maximum, minimum, and arithmetic mean of the term frequency (tf) of query words that
appear in the expanded graphic.
? GF
4
: The maximum, minimum, and arithmetic mean of inverse document (graphic) frequency (idf)
of query words that appear in the expanded graphic.
5.2 Structural Features
Our structural feature set includes 35 features: 17 that address how well a graphic?s x-axis (independent
axis) relates to the requisite x-axis content hypothesized from the user?s query and 18 that address how
well a graphic?s y-axis (dependent axis) content captures the requisite dependent axis content hypothe-
sized from the query. The following are a few of the x-axis features:
? SFX
1
: The Okapi-BM25 value using the same modified formula as for general features, given the
query x-axis words and the text expansion of the x-axis labels in the graphic.
? SFX
2
: The tf-idf of x-axis words hypothesized from the query that appear in the expansion of the
x-axis labels in the graphic.
? SFX
3
: The maximum, minimum, and arithmetic mean of tf of x-axis words hypothesized from the
query that appear in the expansion of the x-axis labels in the graphic.
? SFX
4
: The maximum, minimum, and arithmetic mean of idf of x-axis words hypothesized from
the query that appear in the expansion of the x-axis labels in the graphic.
The y-axis features (SFY
1
, SFY
2
, SFY
3
, and SFY
4
) include the same relevance measurements as
used for the x-axis features; for example, feature SFY
1
captures the Okapi-BM25 score for the y-axis
content hypothesized from the query and the text expansion of the graphic y-axis words, and feature
SFY
2
is the tf-idf score for the y-axis content hypothesized from the query and the expansion of the
graphic y-axis words. One additional feature that is specific to the y-axis is:
? SFY
5
: The posterior probability of the Wikimantic (Boston et al., 2013) mixture concept
2
for the
y-axis words hypothesized from the query, given the Wikimantic mixture concept representing the
y-axis words in the graph, referred to as p(Q
y
|G
y
). Both query y-axis words and the graphic y-axis
2
A Wikimantic mixture concept is a set of weighted concepts (Boston et al., 2013).
604
descriptor are each interpreted by Wikimantic into a mixture concept, M
qy
and M
gy
respectively.
Recall from the introduction to Section 5 that a mixture concept is a weighted vector of topic con-
cepts that defines the semantic meaning of a term or set of terms. For example, the mixture concept
for the country China is represented by a vector of topic concepts such as ?China?, ?People?s Re-
public of China?, ?Mainland China?, and so on. Wikimantic estimates the probability of a concept
given another concept by the amount of overlapping words between the two concepts. For example,
the topic concept for the country ?United States? is likely to contain similar words to the concept
for ?China?, such as the words ?country?, ?nation?, ?region?, ?capital?, ?GDP?, etc. Therefore
the probability of United States given China is likely to be higher than that of United States given
the topic ?rugby?.
5.3 Content Features
Our content feature set contains four features that address how well the intended message of a graphic
captures the requisite message content hypothesized from the user?s query. Ideally, a relevant graphic?s
intended message G
m
will match the message category Q
m
hypothesized from the user?s query. When
the two do not match exactly, we use a hierarchy of message categories and the concept of relaxation
as the paradigm for estimating how much perceptual effort would be required to extract the message
specified by the query from the graphic. For example, suppose that the query requests a Rank message;
graphics with Rank messages will convey the rank of a specific entity by arranging the entities in order of
value and highlighting in some way the entity whose rank is being conveyed. Graphics with a Rank-all
intended message will convey the rank of a set of entities without highlighting any specific entity; the
Rank-all message category appears as a parent of Rank in the message hierarchy since it is less specific
than Rank. Although one can identify the rank of a specific entity from a graphic whose intended message
is a Rank-all message, it is perceptually more difficult since one must search through the graph for the
entity whose rank is desired. By moving up or down the message hierarchy from Q
m
to G
m
, Q
m
is
relaxed to match different G
m
. The greater the degree of relaxation involved, the less message-relevant
the infographic is to the user query. The four content-based features are:
? CF
1
: Whether the message category Q
m
hypothesized from the user?s query matches exactly the
intended message category G
m
of the graphic.
? CF
2
: The amount of relaxation needed to relax the message category Q
m
hypothesized from the
user?s query so that it matches the intended message category G
m
of the graphic.
? CF
3
: The Okapi-BM25 value given the intended message focused entity Q
f
(if any) hypothesized
from the user?s query and the focused entity G
f
in the graphic, if any.
? CF
4
: The Okapi-BM25 value given the intended message focused entity Q
f
(if any) hypothesized
from the user?s query and the non-focused x-axis entities G
nf
in the graphic.
6 Constructing a Ranking Model for Graph Retrieval
Learning-to-rank algorithms (Liu, 2009) construct a learned model that ranks objects based on partially
ordered training data. Tree-based ensemble methods have been shown to be very effective (Chapelle
and Chang, 2011). We experimented with two state-of-the-art tree-based learning-to-rank algorithms as
implemented in the RankLib library (http://people.cs.umass.edu/vdang/ranklib.html): Multiple Additive
Regression Trees abbreviated as MART (Friedman, 2001) and Random Forest (Breiman, 2001).
A human subject experiment was performed to collect a set of 152 full sentence user queries from
five topics. The queries were collected from 5 different tasks and covered a variety of topics involving
companies. Two sample queries are ?What credit card company made the most money in 2008?? and
?How does Avis rank compared to other car rental companies in revenue??. We used the collected
queries to search on popular commercial image search engines to get more infographics from the same
topics. These commercial search engines include Google Image, Microsoft Bing Image Search, and
Picsearch. This produced a set of 257 infographics that are in the topics of the collected queries. Each
605
query-infographic pair was assigned a relevance score on a scale of 0-3 by an undergraduate researcher.
A query-infographic pair was assigned 3 points if the infographic was considered highly relevant to the
query and 0 points if it was irrelevant. Query-infographic pairs where the graphic was somewhat relevant
to the query were assigned 1 or 2 points, depending on the judged degree of relevance of the graphic to
the query. This produced a corpus for training and testing.
Using MART and Random Forest, we developed four models from all 56 features, including the
structural and content features. Two of the models were built using our learned decision trees (Li et
al., 2013b; Li et al., 2013a) to analyze the queries and hypothesize the requisite x-axis content, y-axis
content, message category, and focused entity (if any); see the second row of Table 1. Since the learned
decision trees are not perfect, the other two models were built from hand-labelled data; see the last row of
Table 1. In addition, two baseline models were constructed using only the general features and omitting
the structural and content-based features.
6.1 Evaluating the Models
Normalized Discounted Cumulative Gain (NDCG) (Ja?rvelin and Keka?la?inen, 2002) is used to evaluate
the retrieval result. Table 1 displays the NDCG@10 results. In each case, we averaged together the
NDCG results of 10 runs using the Bootstrapping Method (Tan et al., 2006) in which the query data set
is sampled with replacement to select 152 queries; these 152 queries, and for each query the relevance
judgements assigned to each of the graphics, comprised the training set, with the unselected queries
and their relevance judgements comprising the testing set. The Bootstrapping method is a widely used
evaluation method for small datasets. Typically, approximately 63% of the dataset is selected for the
training set (with some items appearing more than once in the training set) and 37% for the testing set.
The second row of Table 1 provides results when each query is processed by our learned decision trees to
extract the structural content and message category that the query specifies. However, the decision trees
are imperfect. To determine whether our system could do even better if the decision trees were improved,
the third row of Table 1 reports results when each query was hand-labelled with the correctly extracted
structural and message content.
The models using all 56 features produced significantly better results than the baseline model that
used just the general features, indicating that structural and content-based features are very important
and must be taken into account in graph retrieval. In addition, the models built from the hand-labelled
data produced better results than the models where the structural and content features were automatically
extracted from the queries using the learned decision trees; this suggests that improving the decision trees
that process the queries would improve the accuracy of the learned graph retrieval models. In some cases,
the Random Forest learned model performed better than the MART model, but the improvement was not
significant. The experimental results show that both MART and Random Forest using all 56 features,
either using the hand-labelled query data or decision tree query data, provide significantly better results
than the baseline approach (p<0.0005).
Algorithm MART Random Forest
Baseline 0.4943 0.4935
Decision Tree Query Data 0.6239 0.6258
Hand-labelled Query Data 0.6723 0.6758
Table 1: NDCG@10 Results
Figure 2 displays the NDCG@k results for different values of k. The bottom solid line and the line
composed of triangles depict the baseline results, the middle dashed line and the line composed of circles
depict the results using the decision tree query data, and the top solid line and the line composed of
triangles depict the results using the hand-labelled data. All of the models improve as k increases. Most
important, both our MART and Random Forest models constructed from all 56 features perform much
better than the baseline models for all values of k. Thus we conclude that the use of structural and content
features helps in selecting the most relevant graphic as well as the most relevant sets of graphics.
606
Figure 2: NDCG@k for Various Values of n
6.2 Analysis of Influential Features
In both MART and Random Forest, features that are used at the top levels of each tree are more important
in ranking a graphic than features that appear lower in the tree. We analyzed the importance of each of
the 56 features based on the level in each tree where the feature is first used. 70% of the top ten most
important features in the trees produced by both MART and Random Forest were structural or content
features. The most influential two features in trees produced by MART were SFY
5
which captures
p(Q
y
| G
y
) and SFX
2
which captures the tf-idf of x-axis words hypothesized from the query that appear
in the expansion of the x-axis labels in the graphic. Although these two features were not the two
most influential features in the trees produced by Random Forest, they did appear among the top 5
features. Two content-based features appeared among the top ten most important features: CF
3
which
captures the relevance of the focused entity Q
f
(if any) hypothesized from the query to the focused
entity G
f
(if any) in the graphic and CF
4
which captures the relevance of the focused entity Q
f
(if any)
hypothesized from the query to the non-focused entities G
fx
in the graphic. The content features CF
1
and CF
2
that measure relevance of the message category hypothesized from the query to the intended
message category in a candidate graphic appeared among the top 20 features but not among the top 10
features. Further inspection of the trees and analysis of the queries and graphics leads us to believe
that message category relevance is influential in refining the ranking of graphics once graphics with
appropriate structural content have been identified. Our future work will examine these two features
more closely and determine whether modifications of them, or changes in how they are used, will improve
results.
Based on these results, we conclude that structural and content-based features are important when
ranking infographics for retrieval and must be taken into account in an effective graph retrieval system.
7 Conclusion and Future Work
To our knowledge, no other research effort has considered the use of structural and content-based fea-
tures when ranking graphics for retrieval from a digital library. We developed learned models that take
into account how well the structure and content of an infographic matches the requisite structure and con-
tent hypothesized from the user query, and showed that these models perform significantly better than
baseline models that ignore graph structure and message content. In addition, an analysis of the learned
models showed which structural and content features were most influential. In our future work, we will
improve our methods for hypothesizing requisite features of relevant graphics and will analyze our re-
laxation metric to determine whether an improved metric will play a more influential role in ranking
graphics for retrieval.
Acknowledgements
This work was supported by the National Science Foundation under grant III-1016916 and IIS-1017026.
607
References
Jaime Arguello, Jonathan L Elsas, Jamie Callan, and Jaime G Carbonell. 2008. Document representation and
query expansion models for blog recommendation. ICWSM, 2008(0):1.
Michael Bendersky and W Bruce Croft. 2008. Discovering key concepts in verbose queries. In Proceedings of the
31st annual international ACM SIGIR conference on Research and development in information retrieval, pages
491?498. ACM.
Michael Bendersky and W Bruce Croft. 2009. Analysis of long queries in a large scale search log. In Proceedings
of the 2009 workshop on Web Search Click Data, pages 8?14. ACM.
Christopher Boston, Hui Fang, Sandra Carberry, Hao Wu, and Xitong Liu. 2013. Wikimantic: Toward effective
disambiguation and expansion of queries. Data & Knowledge Engineering.
Ben Bradshaw. 2000. Semantic based image retrieval: a probabilistic approach. In Proceedings of the eighth ACM
international conference on Multimedia, pages 167?176. ACM.
Leo Breiman. 2001. Random forests. Machine Learning, 45(1):5?32.
Sandra Carberry, Stephanie Elzer, and Seniz Demir. 2006. Information graphics: an untapped resource for digital
libraries. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development
in information retrieval, pages 581?588. ACM.
Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge overview. In Yahoo! Learning to Rank
Challenge, pages 1?24.
Daniel Chester and Stephanie Elzer. 2005. Getting computers to see information graphics so users do not have to.
In Foundations of Intelligent Systems, pages 660?668. Springer.
S. Clark and J.R. Curran. 2007. Wide-coverage efficient statistical parsing with ccg and log-linear models. Com-
putational Linguistics, 33(4):493?552.
Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z Wang. 2008. Image retrieval: Ideas, influences, and trends of the
new age. ACM Computing Surveys (CSUR), 40(2):5.
Seniz Demir, Sandra Carberry, and Stephanie Elzer. 2007. Effectively realizing the inferred message of an in-
formation graphic. In Proceedings of the International Conference on Recent Advances in Natural Language
Processing (RANLP), pages 150?156.
David Dubin. 2004. The most influential paper gerard salton never wrote.
Stephanie Elzer, Sandra Carberry, and Ingrid Zukerman. 2011. The automated understanding of simple bar charts.
Artificial Intelligence, 175(2):526?555.
Hugo Jair Escalante, Carlos Herna?ndez, Aurelio Lo?pez, Heidy Mar??n, Manuel Montes, Eduardo Morales, Enrique
Sucar, and Luis Villasen?or. 2008. Towards annotation-based query and document expansion for image retrieval.
In Advances in Multilingual and Multimodal Information Retrieval, pages 546?553. Springer.
Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A formal study of information retrieval heuristics. In Proceed-
ings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information
Retrieval, SIGIR ?04, pages 49?56, New York, NY, USA. ACM.
Myron Flickner, Harpreet Sawhney, Wayne Niblack, Jonathan Ashley, Qian Huang, Byron Dom, Monika Gorkani,
Jim Hafner, Denis Lee, Dragutin Petkovic, et al. 1995. Query by image and video content: The qbic system.
Computer, 28(9):23?32.
Jerome H. Friedman. 2001. Greedy function approximation: A gradient boosting machine. The Annals of Statis-
tics, 29(5):1189?1232, 10.
Robert P Futrelle and Nikos Nikolakis. 1995. Efficient analysis of complex diagrams using constraint-based
parsing. In Document Analysis and Recognition, 1995., Proceedings of the Third International Conference on,
volume 2, pages 782?790. IEEE.
Y. Gao, M. Wang, H. Luan, J. Shen, S. Yan, and D. Tao. 2011. Tag-based social image search with visual-text
joint hypergraph learning. In Proceedings of the 19th ACM international conference on Multimedia, pages
1517?1520. ACM.
608
Amarnath Gupta and Ramesh Jain. 1997. Visual information retrieval. Communications of the ACM, 40(5):70?79.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2002. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf.
Syst., 20(4):422?446, October.
Zhuo Li, Matthew Stagitis, Sandra Carberry, and Kathleen F. McCoy. 2013a. Towards retrieving relevant informa-
tion graphics. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development
in Information Retrieval, SIGIR ?13, pages 789?792, New York, NY, USA. ACM.
Zhuo Li, Matthew Stagitis, Kathleen McCoy, and Sandra Carberry. 2013b. Towards finding relevant information
graphics: Identifying the independent and dependent axis from user-written queries.
Jingjing Liu, Panupong Pasupat, Yining Wang, Scott Cyphers, and Jim Glass. 2013. Query understanding en-
hanced by hierarchical parsing structures. In Automatic Speech Recognition and Understanding (ASRU), 2013
IEEE Workshop on, pages 72?77. IEEE.
Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval,
3(3):225?331.
Yuanhua Lv and ChengXiang Zhai. 2009. Positional language models for information retrieval. In Proceedings
of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages
299?306. ACM.
Christopher D Manning, Prabhakar Raghavan, and Hinrich Schu?tze. 2008. Introduction to information retrieval,
volume 1. Cambridge university press Cambridge.
Donald Metzler and Congxing Cai. 2011. Usc/isi at trec 2011: Microblog track. In TREC.
Nina Phan, Peter Bailey, and RossWilkinson. 2007. Understanding the relationship of information need specificity
to search query length. In Proceedings of the 30th annual international ACM SIGIR conference on Research
and development in information retrieval, pages 709?710. ACM.
Arnold WM Smeulders, Marcel Worring, Simone Santini, Amarnath Gupta, and Ramesh Jain. 2000. Content-
based image retrieval at the end of the early years. Pattern Analysis and Machine Intelligence, IEEE Transac-
tions on, 22(12):1349?1380.
John R Smith and Shih-fu Chang. 1997. Querying by color regions using the visualseek content-based visual
query system. Intelligent multimedia information retrieval, 7(3):23?41.
Michael J Swain and Dana H Ballard. 1991. Color indexing. International journal of computer vision, 7(1):11?32.
Pang-Ning Tan, Michael Steinbach, Vipin Kumar, et al. 2006. Introduction to data mining. WP Co.
Peng Wu, Sandra Carberry, Stephanie Elzer, and Daniel Chester. 2010. Recognizing the intended message of line
graphs. In Diagrammatic Representation and Inference, pages 220?234. Springer.
609
Summarizing Information Graphics Textually
Seniz Demir?
TUBITAK-BILGEM
Sandra Carberry??
University of Delaware
Kathleen F. McCoy?
University of Delaware
Information graphics (such as bar charts and line graphs) play a vital role in many
multimodal documents. The majority of information graphics that appear in popular media
are intended to convey a message and the graphic designer uses deliberate communicative
signals, such as highlighting certain aspects of the graphic, in order to bring that message
out. The graphic, whose communicative goal (intended message) is often not captured by the
document?s accompanying text, contributes to the overall purpose of the document and cannot be
ignored. This article presents our approach to providing the high-level content of a non-scientific
information graphic via a brief textual summary which includes the intended message and the
salient features of the graphic. This work brings together insights obtained from empirical studies
in order to determine what should be contained in the summaries of this form of non-linguistic
input data, and how the information required for realizing the selected content can be extracted
from the visual image and the textual components of the graphic. This work also presents a
novel bottom?up generation approach to simultaneously construct the discourse and sentence
structures of textual summaries by leveraging different discourse related considerations such as
the syntactic complexity of realized sentences and clause embeddings. The effectiveness of our
work was validated by different evaluation studies.
1. Introduction
Graphical representations are widely used to depict quantitative data and the relations
among them (Friendly 2008). Although some graphics are constructed from raw data
only for visualization purposes, the majority of information graphics (such as bar charts
and line graphs) found in popular media (such as magazines and newspapers) are
? The Scientific and Technological Research Council of Turkey, Center of Research for Advanced
Technologies of Informatics and Information Security, Gebze, Kocaeli, TURKEY, 41470.
E-mail: senizd@uekae.tubitak.gov.tr. (This work was done while the author was a graduate student at
the Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716.)
?? Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716.
E-mail: carberry@cis.udel.edu.
? Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716.
E-mail: mccoy@cis.udel.edu.
Submission received: 20 April 2010; revised submission received: 8 July 2011; accepted for publication:
6 September 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 3
Figure 1
Graphic conveying a maximum bar.
constructed to convey a message. For example, the graphic in Figure 1 ostensibly is
intended to convey that ?The United States has the highest number of hacker attacks
among the countries listed.? The graphic designer made deliberate choices in order to
bring that message out. For example, the bar representing the United States is high-
lighted with a different color from the other bars and the bars are sorted with respect to
their values instead of their labels so that the bar with the highest value can be easily
recognized. Such choices, we argue, are examples of communicative signals that graphic
designers use. Under Clark?s definition (1996), language is not just text and utterances,
but instead includes any deliberate signal (such as gestures and facial expressions) that
is intended to convey a message; thus an information graphic is a form of language.
In popular media, information graphics often appear as part of a multimodal
document. Carberry, Elzer, and Demir (2006) conducted a corpus study of information
graphics from popular media, where the extent to which the message of a graphic is
also captured by the text of the accompanying document was analyzed. One hundred
randomly selected graphics of different kinds (e.g., bar charts and line graphs) were
collected from newspapers and magazines along with their articles. It was observed
that in 26% of the instances, the text conveyed only a small portion of the graphic?s
message and in 35% of the instances, the text didn?t capture the graphic?s message
at all. Thus graphics, together with the textual segments, contribute to the overall
purpose of a document (Grosz and Sidner 1986) and cannot be ignored. We argue that
information graphics are an important knowledge resource that should be exploited,
and understanding the intention of a graphic is the first step towards exploiting it.
This article presents our novel approach to identifying and textually conveying
the high-level content of an information graphic (the message and knowledge that one
would gain from viewing a graphic) from popular media. Our system summarizes this
form of non-linguistic input data by utilizing the inferred intention of the graphic de-
signer and the communicative signals present in the visual representation. Our overall
goal is to generate a succinct coherent summary of a graphic that captures the intended
message of the graphic and its visually salient features, which we hypothesize as being
related to the intended message. Input to our system is the intention of the graphic
inferred by the Bayesian Inference System (Elzer, Carberry, and Zukerman 2011), and
an XML representation of the visual graphic (Chester and Elzer 2005) that specifies the
components of the graphic such as the number of bars and the heights of each bar. Our
work focuses on the generation issues inherent in generating a textual summary of a
graphic given this information. The current implementation of the system is applicable
to only one kind of information graphic, simple bar charts, but we hypothesize that the
overall summarization approach could be extended to other kinds of graphics.
528
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
In this article, we investigate answers to the following questions: (1) Among all
possible information that could be conveyed about a bar chart, what should be included
in its summary? (2) How should the content of a summary be organized into a coherent
text? (3) How should the text structure be best realized in natural language? Given the
intended message and the XML representation of a graphic, our system first determines
the content of the graphic?s summary (a list of propositions) by applying the content
identification rules constructed for that intended message category. Our system then
produces a coherent organization of the selected content by applying a bottom?up
approach which leverages a variety of considerations (such as the syntactic complexity
of the realized sentences and clause embeddings) in choosing how to aggregate informa-
tion into sentence-sized units. The system finally orders and realizes the sentence-sized
units in natural language and generates referring expressions for graphical elements
that are required in realization.
The rest of this article is structured as follows. Section 2 discusses related work
on summarization of non-linguistic input data and describes some natural language
applications which could benefit from summaries generated by our work. Section 3
outlines our summarization framework. Section 4 is concerned with identifying the
propositional content of a summary and presents our content-identification rules that
specify what should be included in the summary of a graphic. Section 5 describes
our bottom?up approach, which applies operators to relate propositions selected for
inclusion, explores aggregating them into sentence-sized units, and selects the best orga-
nization via an evaluation metric. Section 6 presents our sentence-ordering mechanism,
which incorporates centering theory to specify the order in which the sentence-sized
units should be presented. Section 7 describes how our system realizes the selected
content in natural language. Particular attention is devoted to our methodology for
generating referring expressions for certain graphical elements such as a descriptor
of what is being measured in the graphic. Section 8 presents a user study that was
conducted to evaluate the effectiveness of the generated summaries for the purposes
of this research by measuring readers? comprehension. Section 9 concludes the article
and outlines our future work.
2. Background
2.1 Related Work
There has been a growing interest in language systems that generate textual summaries
of non-linguistic input data (Reiter 2007). The overall goal of these systems, generally
referred to as data-to-text systems, is to enable efficient processing of large volumes
of numeric data by supporting traditional visualisation modalities and to reduce
the effort spent by human experts on analyzing the data. Various examples of data-
to-text systems in the literature include systems that summarize weather forecast
data (Goldberg, Driedger, and Kittredge 1994; Coch 1998), stock market data (Kukich
1983), and georeferenced data (Turner, Sripada, and Reiter 2009).
One of the most successful data-to-text generation research efforts is the SumTime
project, which uses pattern recognition techniques to generate textual summaries of
automatically generated time-series data in order to convey the significant and inter-
esting events (such as spikes and oscillations) that a domain expert would recognize
by analyzing the data. The SumTime-Mousam (Somayajulu, Reiter, and Davy 2003)
and SumTime-Turbine (Yu et al 2007) systems were designed to summarize weather
forecast data and the data from gas turbine engines, respectively. More recently, the
529
Computational Linguistics Volume 38, Number 3
project was extended to the medical domain. The BabyTalk (Gatt et al 2009) project
produces textual summaries of clinical data collected for babies in a neonatal intensive
care unit, where the summaries are intended to present key information to medical staff
for decision support. The implemented prototype (BT-45) (Portet et al 2009) generates
multi-paragraph summaries from large quantities of heterogeneous data (e.g., time
series sensor data and the records of actions taken by the medical staff). The overall
goal of these systems (identifying and presenting significant events) is similar to our
goal of generating a summary that conveys what a person would get by viewing an
information graphic, and these systems contend with each of the generation issues we
must face with our system. Our generation methodology, however, is different from the
approaches deployed in these systems in various respects. For example, BT-45 produces
multi-paragraph summaries where each paragraph presents first a key event (of highest
importance), then events related to the key event (e.g., an event that causes the key
event), and finally other co-temporal events. Our system, on the other hand, produces
single-paragraph summaries where the selected propositions are grouped and ordered
with respect to the kind of information they convey. In addition, BT-45 performs a
limited amount of aggregation at the conceptual level, where the aggregation is used
to express the relations between events with the use of temporal adverbials and cue
phrases (such as as a result). Contrarily, our system syntactically aggregates the selected
propositions with respect to the entities they share.
There is also a growing literature on summarizing numeric data visualized via
graphical representations. One of the recent studies, the iGRAPH-Lite (Ferres et al
2007) system, provides visually impaired users access to the information in a graphic via
keyboard commands. The system is specifically designed for the graphics that appear
in ?The Daily? (Statistics Canada?s main dissemination venue) and presents the user
with a template-based textual summary of the graphic. Although this system is very
useful for in depth analysis of statistical graphs and interpreting numeric data, it is
not appropriate for graphics from popular media where the intended message of the
graphic is important. In the iGRAPH-Lite system, the summary generated for a graphic
conveys the same information (such as the title of the graphic, and the maximum and
minimum values) no matter what the visual features of the graphic are. The content of
the summaries that our system generates, however, is dependent on the intention and
the visual features of the graphic. Moreover, that system does not consider many of the
generation issues that we address in our work.
Choosing an appropriate presentation for a large amount of quantitative data is
a difficult and time-consuming task (Foster 1999). A variety of systems were built to
automatically generate presentations of statistical data?such as the PostGraphe sys-
tem (Corio and Lapalme 1999; Fasciano and Lapalme 2000), which generates graphics
and complementary text based on the information explicitly given by the user such
as the intention to be conveyed in the graphic and the data of special interest to the
user. The content of the accompanying text is determined according to the intention
of the graphic and the features of the data. Moreover, the generated texts are intended
to reinforce some important facts that are visually present in the graphic. In this re-
spect, the generation in PostGraphe is similar to our work, although the output texts
have a limited range and are heavily dependent on the information explicitly given
by the user.
2.2 Role of Graphical Summaries in Natural Language Applications
2.2.1 Accessibility. Electronic documents that contain information graphics pose chal-
lenging problems for visually impaired individuals. The information residing in the
530
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
text can be delivered via screen reader programs but visually impaired individuals are
generally stymied when they come across graphics. These individuals can only receive
the ALT text (human-generated text that conveys the content of a graphic) associated
with the graphic. Many electronic documents do not provide ALT texts and even in the
cases where ALT text is present, it is often very general or inadequate for conveying the
intended message of the graphic (Lazar, Kleinman, and Malarkey 2007).
Researchers have explored different techniques for providing access to the in-
formational content of graphics for visually impaired users, such as sound (Meijer
1992; Alty and Rigas 1998), touch (Ina 1996; Jayant et al 2007), or a combination
of the two (Kennel 1996; Ramloll et al 2000). Unfortunately, these approaches have
serious limitations such as requiring the use of special equipment (e.g., printers and
touch panels) or preparation work done by sighted individuals. Research has also
investigated language-based accessibility systems to provide access to graphics (Kurze
1995; Ferres et al 2007). As mentioned in Section 2.1, these language-based systems
are not appropriate for graphics in articles from popular media where the intended
message of the graphic is important. We hypothesize that providing alternative access
to what the graphic looks like is not enough and that the user should be provided
with the message and knowledge that one would gain from viewing the graphic. We
argue that the textual summaries generated by our approach could be associated with
graphics as ALT texts so that individuals with sight impairments would be provided
with the high-level content of graphics while reading electronic documents via screen
readers.
2.2.2 Document Summarization. Research has extensively investigated various techniques
for single (Hovy and Lin 1996; Baldwin andMorton 1998) and multi-document summa-
rization (Goldstein et al 2000; Schiffman, Nenkova, andMcKeown 2002). The summary
should provide the topic and an overview of the summarized documents by identifying
the important and interesting aspects of these documents. Document summarizers
generally evaluate and extract items of information from documents according to their
relevance to a particular request (such as a request for a person or an event) and address
discourse related issues such as removing redundancies (Radev et al 2004) and ordering
sentences (Barzilay, Elhadad, and McKeown 2002) in order to make the summary more
coherent.
It is widely accepted that to produce a good summary of a document, one must
understand the document and recognize the communicative intentions of the author.
Summarization work primarily focuses on the text of a document but, as mentioned
earlier, information graphics are an important part of many multimodal documents
that appear in popular media and these graphics contribute to the overall commu-
nicative intention of the document. We argue that document summarization should
capture the high-level content of graphics that are included in the document, because
information graphics often convey information that is not repeated elsewhere in the
document. We believe that the summary of a graphic generated by our system, which
provides the intended message of the graphic and the information that would be
perceived with a casual look at the graphic, might help in summarizing multi-modal
documents.1
1 Our colleagues are currently investigating how the findings from this work can be used in
communicating the content of multimodal documents.
531
Computational Linguistics Volume 38, Number 3
3. System Overview
Figure 2 provides an overview of the overall system architecture. The inputs to our
system are an XML representation of a bar chart and the intended message of the chart;
the former is the responsibility of a Visual Extraction System (Chester and Elzer 2005)
and the latter is the responsibility of a Bayesian Inference System (Elzer, Carberry, and
Zukerman 2011). Given these inputs, the Content Identification Module (CIM) first
identifies the salient and important features of a graphic that are used to augment its
inferred message in the summary. The propositions conveying the selected features and
the inferred message of the graphic are then passed to the Text Structuring and Aggre-
gation Module (TSAM). This module produces a partial ordering of the propositions
according to the kind of information they convey, and aggregates them into sentence-
sized units. The Sentence OrderingModule (SOM) then determines the final ordering of
the sentence-sized units. Finally, the Sentence Generation Module (SGM) realizes these
units in natural language, giving particular attention to generating referring expressions
for graphical elements when appropriate. In the rest of this section, we briefly present
the systems that provide input to our work and describe the corpus of bar charts
used for developing and testing our system. The following sections then describe the
modules implemented within our system in greater detail, starting from the Content
Identification Module.
Figure 2
System architecture.
532
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
3.1 Visual Extraction System
The Visual Extraction System (Chester and Elzer 2005) analyzes a graphic image (visual
image of a bar chart) and creates an XML representation specifying the components
of the graphic, such as the height and color of each bar, any annotations on a bar, the
caption of the graphic, and so forth. The current implementation handles vertical and
horizontal bar charts that are clearly drawn with specific fonts and no overlapping
characters. The charts can have a variety of textual components such as axis labels,
caption, further descriptive text, text inside the graphic, and text below the graphic.
The current system cannot handle 3D charts, charts where the bars are represented by
icons, or charts containing texts at multiple angles, however.
3.2 Bayesian Inference System for Intention Recognition
The Bayesian Inference System (Elzer, Carberry, and Zukerman 2011) treats an informa-
tion graphic as a form of language with a communicative intention, and reasons about
the communicative signals present in the graphic to recognize its intendedmessage. The
system is currently limited to simple bar charts and takes as input the XML representa-
tion of the chart produced by the Visual Extraction System described previously.
Three kinds of communicative signals that appear in bar charts are extracted from a
graphic and utilized by the system. The first kind of signal is the relative effort required
for various perceptual and cognitive tasks. The system adopts the AutoBrief (Kerpedjiev
and Roth 2000) hypothesis that the graphic designer chooses the best design to facilitate
the perceptual and cognitive tasks that a viewer will need to perform on the graphic.
Thus, the relative effort for different perceptual tasks serves as a communicative signal
about what message the graphic designer intended to convey (Elzer et al 2006). The
second and third types of communicative signals used in the system are salience and
the presence of certain verbs and adjectives in the caption that suggest a particular
message category. The presence of any of these three kinds of communicative signals
are entered into a Bayesian network as evidence. The top level of the network captures
one of the 12 message categories that have been identified as the kinds of messages that
can be conveyed by a bar chart, such as conveying a change in trend (Changing Trend)
or conveying the bar with the highest value (Maximum Bar). The system produces as
output the hypothesized intended message of a bar chart as one of these 12 message
categories, along with the instantiated parameters of the message category, in the form
of a logical representation such as Maximum Bar(first bar) for the graphic in Figure 1
and Increasing Trend(first bar, last bar) for the graphic in Figure 3a.
3.3 Corpus of Graphics
We collected 82 groups of graphics along with their articles from 11 different magazines
(such as Newsweek and Business Week) and newspapers. These groups of graphics
varied in their structural organization: 60% consisted solely of a simple bar chart (e.g.,
the graphic in Figure 1 on Page 2) and 40% were composite graphics (e.g., the graphic
in Figure 8a in Section 7.1.1) consisting of at least one simple bar chart along with
other bar charts or other kinds of graphics (e.g., stacked bar charts or line graphs). We
selected at least one simple bar chart from each group and our corpus contained a total
of 107 bar charts. The Bayesian Inference System had an overall success rate of 79.1% in
recognizing the correct intended message for the bar charts in our corpus using leave-
one-out cross-validation (Elzer, Carberry, and Zukerman 2011).
533
Computational Linguistics Volume 38, Number 3
Figure 3
(a) Graphic conveying an increasing trend. (b) Graphic conveying the ranking of all bars.
In the work described in this article, we only used the bar charts whose intended
message was correctly recognized by the Bayesian Inference System and associated each
chart with the inferred message category. Here, our intent is to describe a generation
approach that works through a novel problem from beginning to end by handling a
multitude of generation issues. Thus, using bar charts with the perfect intention is
reasonably appropriate within the scope of the present work. For each bar chart, we
also used the XML representation that was utilized by the Bayesian Inference System.
Slightly less than half of the selected bar charts were kept for testing the system per-
formance (which we refer to as the test corpus), and the remaining graphs were used
for developing the system (which we refer to as the development corpus). Because the
number of graphics in the development corpus was quite limited, we constructed a
number of bar charts2 in order to examine the effects of individual salient features
observed in the graphics from the development corpus. These graphs, most of which
were obtained by modifying original graphics, enabled us to increase the number of
graphics in the development corpus and to explore the system behavior in various new
cases.
4. Content Identification Module (CIM)
Our ultimate goal is to generate a brief and coherent summary of a graphic. Identifying
and realizing the high-level informational content of a graphic is not an easy task,
however. First, a graphic depicts a large amount of information and therefore it would
be impractical to attempt to provide all of this information textually to a user. Second, a
graphic is chosen as the communication medium because a reader can get information
from it at many different levels. A casual look at the graphic is likely to convey the
intended message of the graphic and its salient features. At the same time, a reader
could spend much more time examining the graphic to further investigate something
of interest or something they noticed during their casual glance.
In order to address the task of identifying the content of a summary, we extend
to simple bar charts the insights obtained from an informal experiment where human
participants were asked to write a brief summary of a series of line graphs with the
same high-level intention (McCoy et al 2001). The most important insight gained from
2 The graphics that we constructed were not used in any of the evaluation experiments with human
participants described throughout this article.
534
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
this study is that the intended message of a graphic was conveyed in all summaries no
matter what the visual features of the graphic were. It was observed that the participants
augmented the intended message with salient features of the graphic (e.g., if a line
graph is displaying an increasing trend and the variance in that trend is large, then
the variance is salient) and that what was found salient depended on the graphic?s
intended message. Because the participants generated similar summaries for a par-
ticular graphic, we hypothesize that they perceived the same salient features for that
graphic. Although the set of features that might be salient is the same for different
graphics sharing the same underlying intention, the differences observed between the
summaries generated for different graphics with the same intention can be explained
by whether or not the features are salient in those graphics. The fact that the summaries
did not include all information that could be extracted from the graphic (such as
the value of every point in a line graph) but only visually salient features, correlates
with Grice?s Maxim of Quantity (1975) which states that one?s discourse contribution
should be as informative as necessary for the purposes of the current exchange but not
more so.
To extend these observations to constructing brief summaries of bar charts, we
hypothesize that (1) the intended message of the bar chart should form the core of
its textual summary and (2) the most significant and salient features of the bar chart,
which are related to its intended message, should be identified and included in that
summary. The inferred intended message of a bar chart serves as a starting point for
our content identification approach. In the rest of this section, we first describe a series
of experiments that we conducted to identify what constitutes the salient features of
a given bar chart and in which circumstances these features should be included in its
textual summary. We then present the content identification rules that were constructed
to automatically select appropriate content for the summary of a bar chart.
4.1 Experiments
We conducted a set of formal experiments to find patterns between the intended mes-
sage of a graphic, salient visual features of the displayed data, and the propositions
selected for inclusion in a brief summary. We identified the set of all propositions
(PROPALL) that capture information that we envisioned someone might determine
by looking at a bar chart. This set included a wide variety of pieces of information
present in a bar chart and contained propositions common to all bar charts as well
as propositions which were applicable only to some of the message categories. The
following is a subset of the identified propositions. In this example, Propositions 1?4
are common to all bar charts; in contrast, Propositions 5?8 are only present when the
bar chart is intended to convey a trend:
 The labels of all bars (Proposition 1)
 The value of a bar (Proposition 2)
 The percentage difference between the values of two bars (Proposition 3)
 The average of all bar values (Proposition 4)
 The range of the bar values in the trend (Proposition 5)
 The overall percentage change in the trend (Proposition 6)
535
Computational Linguistics Volume 38, Number 3
 The change observed at a time period (Proposition 7)
 The difference between the largest and the smallest changes observed in
the trend (Proposition 8)
Some propositions, which we refer to as open propositions, require instantiation
(such as Propositions 2, 3, and 7 given here) and the information that they convey varies
according to their instantiations.3 In addition, the instantiation of an open proposition
may duplicate another proposition. For example, if the Proposition 3 is instantiatedwith
the first and the last bars of the trend, then the information conveyed by that proposition
is exactly the same as Proposition 6.
To keep the size of the experiment reasonable, we selected 8 message categories
from among the 12 categories that could be recognized by the Bayesian Inference Sys-
tem; these categories were the onesmost frequently observed in our corpus and could be
used as a model for the remaining message categories. These categories were Increasing
Trend, Decreasing Trend, Changing Trend, Contrast Point with Trend, Maximum Bar,
Rank Bar, Rank All, and Relative Difference. In the experiments, we did not use the
categoriesMinimumBar (which can bemodeled viaMaximumBar), Relative Difference
with Degree (which can be modeled via Relative Difference), Stable Trend (which was
not observed in the corpus), and Present Data (which is the default category selected
when the system cannot infer an intended message for the graphic).
For each message category, we selected two to three original graphics from the
development corpus, where the graphics with the same intended message presented
different visual features. For example, we selected two graphics conveying that a par-
ticular bar has the highest value among the bars listed, but only in one of these graphics
was the value of the maximum bar significantly larger than the values of the other bars
(such as the graphic in Figure 1). In total, 21 graphics were used in the experiments and
these graphics covered all selected intended message categories. Because the number of
propositions applicable to each message category was quite large, 10?12 propositions
were presented for each graphic. Each graphic was presented to at least four partici-
pants. Overall, the experiments covered all selected intended message categories and
all identified propositions.
Twenty participants, who were unaware of our system, participated in the experi-
ments. The participants were graduate students or recent Ph.D. graduates from a variety
of departments at the University of Delaware. Each experiment started with a brief
description of the task, where the participants were told to assume that in each case
the graphic was part of an article that the user is reading and that the most important
information depicted in the graphic should be conveyed in its summary. They were also
told that they would be given an information graphic along with a sentence conveying
the intended message of the graphic and a set of propositions, and would be asked to
classify these additional propositions into one of three classes according to how impor-
tant they felt it was to include that proposition in the textual summary:4 (1) Essential:
This proposition should be included in the brief textual summary, (2) Possible: This
proposition could be included in the brief textual summary but it?s not essential, and (3)
Not Important: This proposition should not be included in the brief textual summary.
3 We used open propositions in order to keep PROPALL within a manageable size.
4 The participants were also asked to instantiate the open propositions that they classified as Essential or
Possible.
536
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
4.2 Analysis
To analyze the experiment?s results, we first assigned a numeric score to each
class indicating the level of importance assigned by the participants: Essential = 3,
Possible = 1, Not-important = 0. We then calculated an ?importance level? (IL) for each
proposition with respect to a particular graphic, where the importance level estimates
how important it is for that proposition to be included in the graphic?s summary.
The importance level of a proposition was computed by summing the numeric
scores associated with the classes assigned by the participants. For example, if three
participants classified a proposition as Essential and two participants as Possible, the
importance level of that proposition in the graphic was (3? 3) + (2? 1) = 11. In cases
where a proposition (Prop A) and an instantiated open proposition which conveyed
the same information were classified by a participant into different classes for the same
graphic, the classification of the proposition that came earlier in the presentation was
used in computing the importance level of Prop A.
Given these computed scores, we needed to identify which propositions to con-
sider further for inclusion in a summary. Because there was a divergence between
the sets of propositions that were classified as essential by different participants, we
decided to capture the general tendency of the participants. For this purpose, we
defined majority importance level as a ranking criteria, which is the importance level
that would be obtained if half of the participants classify a proposition as essential.
For example, the majority importance level would be (6? 3)/2 = 9 if there were six
participants. We classified a proposition as a highly rated proposition if its importance
level was equal to or above the majority importance level.5 The propositions that were
classified as highly rated for the graphics with a message category formed the set of
highly rated propositions that should be considered for inclusion for that message
category.
We had to ensure that the propositions presented to the participants (PROPALL)
actually covered all information that is important enough to include in the summary of
a bar chart. Thus, for each graphic, we also asked participants if there was anything else
they felt should be included in the brief summary of the graphic. We received only a
few isolated suggestions such as a proposition conveying what type of a curve could fit
the trend. Moreover, these suggestions were not common among the participants, and
nothing was mentioned by more than one participant (indeed most did not make any
suggestions). Thus, we concluded that these suggestions were not appropriate for the
textual summary of a bar chart.
4.3 Content Identification Rules for Message Categories
Using the importance level scores, we needed to identify the subset of the highly rated
propositions that should be included in the textual summary in addition to the graphic?s
intended message. For each message category, we examined the similarities and the
differences between the sets of highly rated propositions identified for the graphics
5 The reason behind assigning particular scores (3,1,0) to the classes is to guarantee that a proposition will
not be selected as a highly rated proposition if none of the participants thought that it was essential.
Assume k participants classified a proposition (Prop A). The majority importance level of this proposition
(MIL(Prop A)) is (3? k)/2. A proposition is classified as highly rated if its importance level (IL(Prop A))
is equal to or greater than the majority importance level (IL(Prop A) ?MIL(Prop A)). If all of the
participants classified the proposition as Possible, the IL(Prop A) is 1? k, which is less than MIL(Prop A).
537
Computational Linguistics Volume 38, Number 3
associated with that message category, related these differences to the visual features
present in these graphics, and constructed a set of content identification rules for
identifying propositions to be included in the summary of a graphic from that message
category. If a proposition was marked as highly rated for all graphics in a particular
message category, then its selection was not dependent on particular visual features
present in these graphics. In such cases, our content identification rule simply states that
the proposition should be included in the textual summary for every graphic whose
inferred message falls into that message category. For the other propositions that are
highly rated for only a subset of the graphics in a message category, we identified a fea-
ture that was present in the graphics where the proposition was marked as highly rated
and was absent when it was not marked as highly rated, and our content identification
rules use the presence of this feature in the graphic as a condition for the proposition
to be included in the textual summary. In addition, we observed that a highly rated
proposition for a message category might require inclusion of another proposition for
realization purposes. For example, in the Rank All message category, the proposition
indicating the rank of each bar was identified as highly rated and thus could be included
in the textual summary. Because the rank of a bar cannot be conveyed without its label,
we added the proposition indicating the label of each bar to the content identification
rule containing the rank proposition, although this extra proposition was not explicitly
selected by the participants for inclusion. Notice that these steps?identifying features
that distinguish one subset of graphs from the other and identifying propositions that
need to be included for realizing other propositions?make it difficult to use machine
learning for this task. In our case the number of possible features that can be extracted
from a graphic is very large and it is difficult to know which features from among
those may be important/defining in advance. In addition, the number of graphics in
our development corpus is too small to expect machine learning to be effective.
The following are glosses of two partial sets of representative content identification
rules. The first set is applicable to a graphic conveying an increasing trend and the
second set is applicable to a graphic conveying the rankings of all bars present in the
graph:
 Increasing Trend message category:6
1. If (message category equals ?increasing trend?) then
include(proposition conveying the rate of increase of the trend):
Include the proposition conveying the rate of increase of the trend
2. If (message category equals ?increasing trend?) and
notsteady7(trend) then include(proposition conveying the
period(s) with a decrease):8
If the trend is not steady and has variability, then include the proposition
indicating where the trend varies
6 The ?notsteady? function returns true if its argument is not a steady trend; the ?value? function returns
the values of all members of its argument; the ?greaterthan? function returns true if the left argument is
greater than the right argument; the ?withinrange? function returns true if all members of its left
argument are within the range given by its right argument; the ?average? function returns the average of
the values of all members of its argument.
7 A trend is unsteady if there is at least one period with a decrease in contrast with the increasing trend.
8 The inclusion of propositions whose absence might lead the user to draw false conclusions is consistent
with Joshi, Webber, and Weischedel?s (1984) maxim, which states that a system should not only produce
correct information but should also prevent the user from drawing false inferences.
538
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
3. If (message category equals ?increasing trend?) and (value(last bar)
greaterthan (3*value(first bar))) then include(proposition
conveying the overall percentage increase in the trend):
If the overall percentage increase in the trend is significantly large, then
include the proposition conveying the percentage increase in the trend
 Rank All message category:
1. If (message category equals ?rank all?) then include(propositions
conveying the label and the value of the highest bar):
Include the propositions conveying the label and the value of the
highest bar
2. If (message category equals ?rank all?) and (value(all bars)
withinrange ((0.7*average(all bars)),(1.3*average(all bars)))) then
include(proposition indicating that the bar values vary slightly):
If the values of bars are close to each other, then include the proposition
indicating that the bar values vary slightly
3. If (message category equals ?rank all?) and (not(value(all bars)
withinrange ((0.7*average(all bars)),(1.3*average(all bars))))) then
include(propositions conveying the label and the value of the
lowest bar):
If the values of bars are not close to each other, then include the
propositions conveying the label and the value of the lowest bar
We defined the conditions of all content identification rules as a conjunction of one
or more expressions where some expressions required us to determine threshold values
to be used for comparison purposes. For example, we observed that the proposition
conveying the overall percentage change in the trend was marked as highly rated
only for graphics which depicted a significant change in the trend. We handled this
situation for graphics with an increasing trend by defining the third content identi-
fication rule (shown earlier) where we needed to set the lowest threshold at which
an overall increase observed in a trend can be accepted as significantly large. For
setting such threshold values, we examined all graphs in the development corpus
to which the corresponding content identification rule is applicable (i.e., the graphs
associated with the message category for which the rule is defined) and used our
intuitions about whether the proposition captured by the rule should be selected for
inclusion in the summaries of these graphs. We set the threshold values using the
results obtained from group discussions such that the final setting classified all of
the original graphics the way the participants did in the experiments described in
Section 4.1.
When the content identification rules constructed for the Increasing Trend message
category are applied to the bar chart in Figure 3a, the following pieces of information
are selected for inclusion in addition to the intended message of the graphic:
 The rate of increase of the trend, which is slight
 The small drop observed in the year 1999
 The overall percentage increase in the trend, which is 225%
539
Computational Linguistics Volume 38, Number 3
When the content identification rules constructed for the RankAll message category
are applied to the bar chart in Figure 3b, the following pieces of information are selected
for inclusion in addition to the intended message of the graphic:
 The label and the value of the highest bar, which is Army with 233,030
 The label and the value of the lowest bar, which is Other defense agencies
with 100,678
 The label and the ranking of each bar:9 Army is the highest, Navy is the
second highest, Air Force is the third highest, and Other defense agencies
is the lowest
4.4 Evaluation of the Content Identification Module
We conducted a user study to assess the effectiveness of our content identification
module in identifying the most important information that should be conveyed about
a bar chart. More specifically, the study had three goals: (1) to determine whether the
set of highly rated propositions that we identified for each message category contains
all propositions that should be considered for inclusion in the summaries of graphics
with that message category; (2) to determine how successful our content identification
rules are in selecting highly rated propositions for inclusion in the summary; and (3)
to determine whether the information conveyed by the highly rated propositions is
misleading or not.
Nineteen students majoring in different disciplines (such as Computer Science and
Materials Science and Engineering) at the University of Delaware were participants
in the study. These students neither participated in the earlier study described in Sec-
tion 4.1 nor were aware of our system. Twelve graphics from the test corpus (described
in Section 3.3) whose intended message was correctly identified by the Bayesian Infer-
ence System were used in the experiments. Once the intended message was recognized,
the corresponding content identification rules were executed in order to determine the
content of the graphic?s summary. Prior to the experiment, all participants were told
that they would be given a summary and that it should include the most important
information that they thought should be conveyed about the graphic. Each participant
was presented with three graphics from among the selected graphics such that each
graphic was viewed by at least four participants. For each graphic, the participants
were first given the summary of the graphic generated by our approach and then shown
the graphic. The participants were then asked to specify if there was anything omitted
that they thought was important and therefore should be included in the summary. In
addition, the participants were asked to specify whether or not they were surprised
or felt that the summary was misleading (i.e., whether the bar chart was similar to
what they expected to see after reading its summary). Note that our summaries with
relatively few propositions are quite short. Thus our evaluation focused on determin-
ing whether anything of importance was missing from the summary or whether the
summary was misleading. In the experiments, we did not ask the participants to rate
9 This piece of information is selected by a rule defined for the Rank All message category not shown in the
bulleted list on the previous page.
540
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
the content of summaries on a numeric scale in order to restrict them to evaluating only
the selected content as opposed to its presentation (i.e., the organization and realization
of the summary).
Feedback that we received from the participants was very promising. In most of
the cases (43 out of 57 cases), the participants were satisfied with the content that our
approach selected for the presented bar charts. There were a number of suggestions for
what should be added to the summaries in addition towhat had already been conveyed,
and in a couple of these cases, we observed that a highly rated proposition which was
not selected by the corresponding content identification rule was contrarily suggested
by the participants. There was no consensus in these suggestions, however, as none
was made by more than two participants. Some of the participants (3 out of 19) even
commented that we provided more information than they could easily get from just
looking at the graphic. In addition, a few participants (2 out of 19) commented that,
in some graphics, they didn?t agree with the degree (e.g., moderate or steep) assessed
by our approach for differences between bar values (e.g., the rate of change of the
trend), and therefore they thought the summary was misleading. Because there wasn?t
any common consensus among the participants, we didn?t address this very subjective
issue. Overall, we conclude that the sets of highly rated propositions that we identified
contain the most important information that should be considered for inclusion in the
summaries of bar charts and that our system effectively selects highly rated propositions
for inclusion when appropriate.
5. Text Structuring and Aggregation Module (TSAM)
A coherent text has an underlying structure where the informational content is pre-
sented in some particular order. Good text structure and information ordering have
proven to enhance the text?s quality by improving user comprehension. For example,
Barzilay, Elhadad, and McKeown (2002) showed that the ordering has a significant
impact on the overall quality of the summaries generated in theMULTIGEN system. Al-
though previous research highlights a variety of structuring techniques, there are three
prominent approaches that we looked to for guidance: top?down planning, application
of schemata, and bottom?up planning.
In top?down planning (Hovy 1988, 1993; Moore and Paris 1993), the assumption is
that a discourse is coherent if the hearer can recognize the communicative role of each
of its segments and the relation between these segments (generally mapped from the
set of relations defined in rhetorical structure theory (RST; Mann and Thompson 1987).
The discourse is usually represented as a tree-like structure and the planner constructs
a text plan by applying plan operators starting from the initial goal.
In the TEXT system (McKeown 1985), a collection of naturally occurring texts
were analyzed to identify certain discourse patterns for different discourse goals, and
these patterns were represented as schemas which are defined in terms of rhetorical
predicates. The schemas both specify what should be included in the generated texts
and how they should be ordered given a discourse goal. Lester and Porter (1997)
used explanation design packages, schema-like structures with procedural constructs
(for example, the inclusion of a proposition can be constrained by a condition), in
the KNIGHT system, which is designed to generate explanations from a large-scale
biology knowledge base. Paris (1988) applied the idea of schemata in the TAILOR
system to tailor object descriptions according to the user?s level of knowledge about
the domain.
541
Computational Linguistics Volume 38, Number 3
Marcu (1998) argued that text coherence can be achieved by satisfying local con-
straints on ordering and clustering of semantic units to be realized. He developed a
constraint satisfaction based approach to select the best plan that can be constructed
from a given set of textual units and RST relations between them, and showed that
such bottom?up planning overcomes the major weakness of top?down approaches
by guaranteeing that all semantic units are subsumed by the resulting text plan. The
ILEX system (O?Donnell et al 2001), which generates descriptions for exhibits in a
museum gallery, utilizes a similar bottom?up planning approach (Mellish et al 1998)
where the best rhetorical structure tree over the semantic units is used as the text
structure.
Because our content identification rules identify a set of propositions to be con-
veyed, it appears that a bottom?up approach that ensures that all propositions will be
included is in order. At the same time, it is important that our generated text adheres to
an overall discourse organization such as is provided by the top?down approaches.
Because of the nature of the propositions (the kinds of rhetorical relations that can
exist between propositions in a descriptive domain are arguably limited [O?Donnell
et al 2001]), however, a structure such as RST is not helpful here. Thus, the top?down
planning approach does not appear to fit. Although something akin to a schema might
work, it is not clear that our individual propositions fit into the kind of patterns used
in the schema-based approach. Instead we use what can be considered a combination
of a schema and a bottom?up approach to structure the discourse. In particular, we
use the notion of global focus (Grosz and Sidner 1986) and group together proposi-
tions according to the kind of information they convey about the graphic. We define
three proposition classes (message-related, specific, and computational) to classify the
propositions selected for inclusion in a textual summary. The message-related class
contains propositions that convey the intended message of the graphic. The specific
class contains the propositions that focus on specific pieces of information in the
graphic, such as the proposition conveying the period with an exceptional drop in a
graphic with an increasing trend or the proposition conveying the period with a change
which is significantly larger than the changes observed in other periods in a graphic
with a trend. Lastly, propositions in the computational class capture computations or
abstractions over the whole graphic, such as the proposition conveying the rate of
increase in a graphic with an increasing trend or the proposition conveying the overall
percentage change in the trend. In our system, all propositions within a class will
be delivered as a block. But we must decide how to order these blocks with respect
to each other. In order to emphasize the intended message of the graphic (the most
important piece of the summary), we hypothesize that the message-related propositions
should be presented first. We also hypothesize that it is appropriate to close the textual
summary by bringing the whole graphic back into the user?s focus of attention (Grosz
and Sidner 1986) (via the propositions in the computational class). Thus, we define an
ordering of the proposition classes (creating a partial ordering over the propositions)
and present first the message-related propositions, then the specific propositions, and
finally the computational propositions. Section 6 will address the issue of ordering the
propositions within these three classes.
5.1 Representing Summary Content
First we needed to have a representation of content that would provide us with the
most flexibility in structuring and realizing content. For this we used a set of basic
propositions. These were minimal information units that could be combined to form
542
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
the intended message and all of the propositions identified in our content identification
rules. This representation scheme increases the number of aggregation and realization
possibilities that could be explored by the system, which is described in the next
subsection. We defined two kinds of knowledge-base predicates to represent the basic
propositions:
(1) Relative Knowledge Base: These predicates are used to represent the basic
propositions which introduce graphical elements or express relations
between the graphical elements.
(2) Attributive Knowledge Base: These predicates are used to represent the
basic propositions which present an attribute or a characteristic of a
graphical element.
Each predicate contains at least two arguments and we refer to the first argument
as the main entity and the others as the secondary entities. The main entity of each
predicate is a graphical element and the secondary entities are either a string constant
or a graphical element. Some of the graphical elements that we used in this work are as
follows:
 graphic: ?the graphic itself?
 trend: ?the trend observed in the graphic?
 descriptor: ?a referring expression that represents what is being measured
in the graphic?10
 bar(x): ?a particular bar in the graphic?
1 <= x <= n where n = number of bars in the graph
 all bars: ?all bars depicted in the graphic? bset = {bar(x) | 1 <= x <= n}
 period(x,y): ?a period depicted in the graphic? 1 ? x < n and 1 < y ? n
 change(x,y): ? the change between the values of any two bars?
1 ? x < n and 1 < y ? n
 all changes: ?changes between all pairs of bars of the graphic?
cset = {change(x, y) | 1 ? x < n, 1 < y ? n}
 trend period: ?the period over which the trend is observed?
 graph period: ?the period which is depicted by the graphic?
 trend change: ?the overall change observed in the trend?
Table 1 presents sample instantiations of a subset of the predicates that we defined
for this work along with a possible realization for each instantiation. Although the
number of arguments in Relative Knowledge Base predicates (predicates 1 to 15) varies,
10 How that referring expression is extracted from the text associated with the graphic is described in
detail in Section 7.1. For example, the descriptor identified by our system for the graphic in Figure 4
is the dollar value of net profit.
543
Computational Linguistics Volume 38, Number 3
Table 1
Sample instantiations and possible realizations of a subset of our predicates.
1 shows(graphic,trend)
The graphic shows a trend
2 focuses(graphic,bar(3))
The graphic is about the third bar
3 covers(graphic, graph period)
The graphic covers the graph period
4 exists(trend,descriptor)
The trend is in the descriptor
5 has(trend,trend period)11
The trend is over the trend period
6 starts(trend period,?2001?)
The trend period starts at the year 2001
7 ends(trend period,?2010?)
The trend period ends at the year 2010
8 ranges(descriptor,?from?,?20 percent?,trend period)
The descriptor ranges from 20 percent over the trend period
9 hasextreme(descriptor,?largest?,change(3,4),period(3,4))
The descriptor shows the largest change between the third and the fourth bars
10 averages(descriptor,all bars,?55.4 billion dollars?)
The descriptor for all bars averages to 55.4 billion dollars
11 comprises(descriptor,trend change,trend period)
The descriptor comprises a trend change over the trend period
12 occurs(change(2,3),period(2,3))
A change occurs between the second and the third bars
13 hasdifference(change(1,5),bar(1),bar(5),descriptor)
A difference is observed between the descriptor of the first bar and that of the fifth bar
14 observed(all changes,?every?,interval,trend period)
Changes are observed every interval over the trend period
15 presents(descriptor,bar(3),?12 percent?)
The descriptor for the third bar is 12 percent
16 hasattr(trend change,?type?,?increase?)
The trend change is an increase
17 hasattr(change(2,3),?degree?,?moderate?)
The change is of degree moderate
18 hasattr(change(2,3),?amount?,?70 dollars?)
The change amount is 70 dollars
19 hasattr(trend change,?percentage amount?,?22 percent?)
The trend change percentage amount is 22 percent
20 hasattr(all changes,?rate?,?slight?)
Changes are slight changes
all Attributive Knowledge Base predicates (encoded as hasattr) consist of three argu-
ments, where the first argument is the graphical element being described, the second
is an attribute of the graphical element, and the third is the value of that attribute
(predicates 16 to 20).12
11 Notice that the graphical element trend period in 5 is the main entity in 6 and 7. These all might be
combined using the And operator to produce the realization The trend starts at the year 2001 and ends
at the year 2010.
12 Because all Attributive Knowledge Base predicates have the same form, the amount and unit of a change
are represented as a single string which is derived from the textual components of the graphic (such as
70 dollars in Predicate 18).
544
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Figure 4
Graphic conveying a decreasing trend.
For example, consider how the propositions given in Section 4.1 can be represented
with the predicates shown in Table 1. Some of those propositions require a single
predicate. For example, the proposition conveying the value of a bar (Proposition 2)
can be represented via the predicate ?presents? (Predicate 15) and the proposition
conveying the average of all bar values (Proposition 4) via the predicate ?aver-
ages? (Predicate 10). On the other hand, some propositions require more than one
predicate. For example, the proposition conveying the overall percentage change
in the trend shown in Figure 4 (Proposition 6) can be represented via the predi-
cates ?comprises(descriptor,trend change,trend period)? (Predicate 11), ?starts(trend
period,?1998?)? (Predicate 6), ?ends(trend period,?2006?)? (Predicate 7), ?hasattr(trend
change,?type?,?decrease?)? (Predicate 16), and ?hasattr(trend change,?percentage
amount?,?65 percent?)?(Predicate 19). The same set of predicates can be used to rep-
resent the overall amount of change in the trend by replacing the constant ?percentage
amount? with the string ?amount? in Predicate 19.
As is shown by the possible realizations included in Table 1, each basic proposition
can be realized as a single sentence. Although we determined a couple of different
ways (i.e., simple sentences) of realizing each basic proposition, our current imple-
mentation always chooses a single realization (which we refer to as ?the realization
associated with the proposition?) and the main entity is always realized in subject
position.13
5.2 Aggregating Summary Content
The straightforward way of presenting the informational content of a summary is to
convey each proposition as a single sentence while preserving the partial ordering of
the proposition classes. The resultant text would not be very natural and coherent,
however. Aggregation is the process of removing redundancies during the generation
of a more concise and fluent text (Shaw 1998; Dalianis 1999). Aggregation (typically
syntactic aggregation [Reiter and Dale 2000]) has received considerable attention
from the NLG community (McKeown et al 1997; O?Donnell et al 2001; Barzilay and
Lapata 2006), and has been applied in various existing generation systems such as the
intelligent tutoring application developed by Di Eugenio et al (2005). Our aggregation
mechanism works to combine propositions into more complex structures. It takes
13 We leave it as a future work to explore how different realizations for a proposition, including ones
where the main entity is not in subject position, can be utilized by our approach.
545
Computational Linguistics Volume 38, Number 3
advantage of the two types of predicates (Relative Knowledge Base and Attributive
Knowledge Base predicates) and the shared entities between predicates. In order to
relate propositions and explore syntactically aggregating them, our mechanism treats
each proposition as a single node tree which can be realized as a sentence and attempts
to form more complex trees by combining individual trees via four kinds of operators
in such a way so that the more complex tree (containing multiple propositions) can still
be realized as a single sentence. The first operator (Attribute Operator) works only on
propositions with an Attributive Knowledge Base predicate and essentially identifies
opportunities to realize such a proposition as an adjective attached to a noun object
in the realization of another proposition. The remaining three operators, which do
not work on propositions with an Attributive Knowledge Base predicate, introduce
new nodes corresponding to operational predicates (And, Same, and Which) with a
single entity into the tree structures. Two of these operators (And Operator and Which
Operator) work on trees rooted by a proposition with a Relative Knowledge Base or an
And predicate. These operators look for opportunities for VP conjunction and relative
clauses, respectively. The third operator (Same Operator) works on trees rooted by a
proposition with a Relative Knowledge Base predicate and identifies opportunities for
NP conjunction. Although each predicate is associated with a unique realization in the
current implementation, none of these operators depend on how the corresponding
predicates or the entities in those predicates are realized.
Having defined the operators we next had to turn to the problem of determining
how these operators should be applied (e.g., which combinations are preferred). The
operators we defined are similar to the clause-combining operations used by the SPoT
sentence planner (Walker, Rambow, and Rogati 2002; Stent, Prasad, and Walker 2004;
Walker et al 2007) in the travel planner system AMELIA. In AMELIA, for each of
the 100 different input text plans, a set of possible sentence plans (up to 20 plans)
were generated by randomly selecting which operations to apply according to assumed
preferences for operations. These possible sentence plans were then rated by two judges
and the collected ratings were used to train the SPoT planner. Although we greatly
drew from the work on SPoT as we developed our aggregation method, we chose
not to follow their learning methodology. In the SPoT system, some of the features
were domain- and task-dependent and thus porting to a new domain would require
retraining. In addition, the judgments of the two raters were collected in isolation and
it is unclear how these would translate to the task situation the texts were intended
for. Although this methodology was innovative and necessary for SPoT because of
the large number of possible text plans, we chose to select the best text plan on the
basis of theoretically informed complexity features balancing sentence complexity and
number of sentences. Because our text plans are significantly more constrained, it is
possible to enumerate each of them and choose the one that best fits our rating cri-
teria.14 This has the added benefit of better understanding the complexity features by
evaluating the resulting text. In addition, our method would be open to both upgrad-
ing the selection criteria and adding further aggregation operators without requiring
retraining.15
14 Although in our implementation we do enumerate all plans before the rating criteria are applied to select
the best one, it is in principle possible to generate the text plans in an order that would allow maximizing
the scoring functions without first enumerating all possibilities. This is left for future work.
15 Such modifications and additions need to be empirically evaluated with empirical data, however.
546
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Operator: Attribute Operator
Gloss: This operator attaches a single node tree that consists solely of a proposition with
an Attributive Knowledge Base predicate, as a direct subchild of a node N with a Relative
Knowledge Base predicate in another tree, if the main entity of the Attributive Knowledge Base
predicate is an entity (main or secondary) for the proposition at node N.
Input: T1 and T2
Constraints:
1. (pred(T1-root)==?hasattr?)
2. ((pred(T2-node)!=?hasattr?) ? (pred(T2-node)!=?And?) ?
(pred(T2-node)!=?Which?))
3. ((main ent(T1-root)==main ent(T2-node)) ?
(main ent(T1-root)==secondary ent(T2-node)))
Output: A modified T2 such that
1. left child(T2-node)?T1
Glossary:
1. Tx-root: the root node of tree Tx
2. Tx-node: any node in tree Tx (including Tx-root)
3. pred(Tx-node): the predicate at Tx-node
4. left/right child(Tx-node): the leftmost/rightmost child of Tx-node
5. main/secondary ent(Tx-node): the main/secondary entity of the proposition at Tx-node
6. !=: not equal, ==: equal, !: not,?: assign
Operator: And Operator
Gloss: This operator combines two trees if the propositions at their root share the same main
entity. A proposition containing an And predicate with the same main entity forms the root of
the new tree and the trees that are combined form the immediate descendents of this root.
Input: T1 and T2
Constraints:
1. ((pred(T1-root)!=?hasattr?) ? (pred(T2-root)!=?hasattr?))
2. !((pred(T1-root)==?And?) ? (pred(T2-root)==?And?))
3. (main ent(T1-root)==main ent(T2-root))
Output: a new tree T3 where the root node has two immediate children such that
1. pred(T3-root)??And? ?main ent(T3-root)?main ent(T1-root)
2. left child(T3-root)?T1 ? right child(T3-root)?T2
Operator:Which Operator
Gloss: This operator attaches a tree (Tree A) as a descendent of a node N in another tree (Tree B)
via a Which predicate, if the main entity of the proposition at the root of Tree A is a secondary
entity for the proposition at node N of the other tree (Tree B). That particular entity forms the
main entity of the Which predicate. Thus, Tree A will be an immediate child of the node with
the Which predicate and the node with the Which predicate will be an immediate child of node
N in Tree B.
Input: T1 and T2
Constraints:
1. ((pred(T1-root)!=?hasattr?)?(pred(T2-node)!=?hasattr?))
2. (main ent(T1-root)==secondary ent(T2-node))
Output: A modified T2 via the addition of a new node (Node x) with a single immediate child
such that
1. pred(Node x)? ?Which? ?main ent(Node x)?main ent(T1-root)
2. right child(T2-node)?Node x ? left child(Node x)?T1
547
Computational Linguistics Volume 38, Number 3
Operator: Same Operator
Gloss: This operator combines two trees if the propositions at their root contain the same
predicate but the main entities of these predicates are different. A proposition with a Same
predicate forms the root of the new tree, and the trees that are combined form the descendents
of this root. Since the descendents of the new tree have different main entities, the main entity of
the Same predicate is some unique element not occurring elsewhere in the tree. For instance, in
our implementation this element is obtained by appending a unique number, which isn?t used
in another Same predicate in the current forest, to the term random (such as random0).
Input: T1 and T2
Constraints:
1. ((pred(T1-root)!=?hasattr?)?(pred(T2-root)!=?hasattr?) ?
(pred(T1-root)!=?And?) ? (pred(T2-root)!=?And?) ?
(pred(T1-root)!=?Same?) ? (pred(T2-root)!=?Same?))
2. (pred(T1-root)==pred(T2-root))
3. (main ent(T1-root)!=main ent(T2-root))
Output: a new tree T3 where the root node has two immediate children such that
1. pred(T3-root)??Same? ?main ent(T3-root)? a unique element
2. left child(T3-root)?T1 ? right child(T3-root)?T2
In our work, we thus developed a method that would choose a text plan on prin-
cipled reasoning concerning the resulting text. In particular, we looked to balance sen-
tence complexity and the number of sentences in the generated text. Moreover, whereas
such a method was not applicable in the case of SPoT (because of the significantly larger
set of operators with few constraints resulting in potential text plans too numerous to
evaluate), our work differs in several aspects that make it reasonable to generate all
text plans and apply an evaluation metric. First, our system has a small number of
aggregation operators and all operators cannot be applied to all kinds of predicates (e.g.,
the Attribute Operator cannot be applied to the Relative Knowledge Base predicates).
Second, the number of possible sets of basic propositions that our system needs to
organize is significantly lower than the number of possible text plans that the SPoT
planner needs to consider. Finally, although it is not practical in SPoT to list all possible
sentence plans that might be generated for a particular text plan (since the possibilities
are too great), generating all possible combinations of propositions in a proposition class
(such as message related class) is practical in our work. This is due to the fact that the
number of basic propositions in each class is fairly small (e.g., usually between 5 to
15 propositions) and that the nature of the operators and constraints that we put on
their application enable us to prune the space of possible combinations. Some of these
constraints are: (1) The And Operator produces only one complex tree from a pair of
trees and it cannot combine two trees if both trees have a proposition with an And
predicate at their roots (thus we limit the number of conjuncts in a conjoined sentence
to three at most16), and (2) the Attribute Operator produces only one complex tree in
cases where a single node tree (Tree A) can be attached as a direct subchild of more than
one node in another tree (Tree B); the parent of Tree A is the first such node found by
preorder traversal of Tree B.
Our implementation first generates all possible text plans for the propositions
within each class (message-related, specific, and computational). Each text plan is
represented as a forest where each tree in the forest represents a sentence. Initially,
each proposition class is treated as a forest consisting of all single node trees in that
class (initial candidate forest), and the operators are applied to that forest in order
to produce new candidate forests for the proposition class. Anytime two trees in a
16 We set this limit in order to avoid sentences that are too complex to comprehend.
548
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Figure 5
A candidate forest for each proposition class.
candidate forest are combined via an operator, a new candidate forest is produced;
the new candidate forest is added to the existing set of candidate forests, thereby
increasing the number of candidate forests. Within each class, our approach first applies
the And Operator to all possible pairs of trees in the initial candidate forest, which
produces new candidate forests. The Same Operator is then applied to all possible
pairs of trees in each candidate forest. Similarly, the Which Operator and the At-
tribute Operator are applied to trees in the candidate forests produced earlier. The
result of this aggregation is a number of candidate forests with one or more trees
(each using different aggregation) for each of the proposition classes. For example,
Figure 5 shows one candidate forest that can be constructed for each proposition class
by applying these operators to the propositions selected for the graphic in Figure 4,
where each forest resulting from the aggregation consists of a single tree.17 In this
example, the Attributive Knowledge Base predicates (*) are attached to their parents
by the Attribute Operator, the nodes containing And predicates (**) are produced
by the And Operator, and the Which predicates (***) are produced by the Which
Operator.
17 The nodes represented with black circles correspond to the individual predicates. These individual
predicates within each class form the single node trees upon which the operators work.
549
Computational Linguistics Volume 38, Number 3
5.3 Evaluating a Text Structure
Different combinations of operators produce different candidate forests in each propo-
sition class and consequently lead to different realized text with a different complexity
of sentences. The set of candidate forests for each proposition class must be evaluated
to determine which one is best. Our objective is to find a forest that would produce
text which stands at a midpoint between two extremes: a text where each proposition
is realized as a single sentence and a text where groups of propositions are realized
with sentences that are too complex. Our evaluation metric to identify the best forest
leverages different considerations to balance these extremes. The first two criteria are
concerned with the number and syntactic complexity of sentences that will be used to
realize a forest. The third criteria takes into consideration how hard it is to comprehend
the relative clauses embedded in these sentences. The insights that we use in selecting
the best forest (e.g., balancing semantic importance, overall text structure, aggregation,
and readibility due to sentence complexity) represent our novel contributions to the
text structuring and aggregation literature. The theory that underlies our evaluation
metric (i.e., what it is we are balancing in the generation) is widely applicable to other
data-to-text generation domains because it uses general principles from the literature
and has the potential to be improved.
5.3.1 Sentence Complexity. Each tree (single node or complex) in a forest represents a
set of propositions that can be realized as a single sentence. Our aggregation rules
enable us to combine these simple sentences into more complex syntactic structures.
In the literature, different measures to assess syntactic complexity of written text and
spoken language samples have been proposed, with different considerations such as
the right branching nature of English (Yngve 1960) and dependency distance between
lexemes (Lin 1996). We apply the revised D-level sentence complexity scale (Covington
et al 2006) as the basis of our syntactic complexity measure. The D-level scale measures
the complexity of a sentence according to its syntactic structure and the sequence in
which children acquire the ability to use different types of syntactic structures. The
sentence types with the lowest score are those that children acquire first and there-
fore are the simplest types. Eight levels are defined in the study, some of which are
simple sentences, coordinated structures (conjoined phrases or sentences), non-finite
clause in adjunct positions, and sentences with more than one level of relative clause
embedding.
Among the eight levels defined in that study, the levels of interest in our work
are simple sentences, conjoined sentences, sentences with a relative clause modifying
the object of the main verb, non-finite clauses in adjunct positions, and sentences
with more than one level of embedding. However, the definition of sentence types at
each level is too general. For example, the sentences There is a trend and There is a
trend in the dollar value of net profit over the period from the year 1998 to the year 2006
are both classified as simple sentences with the lowest complexity score under the
D-level classification. We argue that although these sentences have a lower complexity
than the sentences with higher D-level scores, their complexities are not the same. We
make a finer distinction between sentence types defined in the D-level classification
and use the complexity levels shown in Table 2. For example, according to our clas-
sification, a simple sentence with more than one adjunct or preposition has a higher
complexity than a simple sentence without an adjunct. We preserve the ordering of
the complexity levels in the D-level classification. For example, in our classification,
550
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Table 2
Our syntactic complexity levels.
Complexity Syntactic Form
Level 0 Simple sentence with up to one prepositional phrase or adjunct
Level 1 Simple sentence with more than one prepositional phrase or adjunct
Level 2 Conjoined sentence (two simple sentences?Level 0 or 1)
Level 3 Conjoined sentence (more than two simple sentences?Level 0 or 1)
Level 4 Sentence with one level of embedding
(relative clause that is modifying object of main verb)
Level 5 Non-finite clause in adjunct positions
Level 6 Sentence with more than one level of embedding
Levels 0 and 1 correspond to the class of simple sentences in the D-level classification
and have a lower complexity than Levels 2 and 3, which correspond to the class of
coordinated structures with a higher complexity than simple sentences in the D-level
classification.
Each basic proposition in our system can be realized as a simple sentence containing
at most one prepositional phrase or adjunct.18 Each single node tree with a Relative or
Attributive Knowledge Base predicate at its root has the lowest syntactic complexity
(Level 0) in this classification.
Themost straightforwardway of realizing amore complex tree would be conjoining
the realizations of subtrees rooted by a proposition with an And or a Same predicate,
embedding the realization of a subtree rooted by a proposition with a Which predicate
as a relative clause, and realizing a subtree that consists solely of a proposition with
an Attributive Knowledge Base predicate as an adjective or a prepositional phrase.
For example, the tree rooted by shows(graphic,trend) in Figure 5 can be realized as
The graphic shows a decreasing trend, which is in the dollar value of net profit and is over
the period, which starts at the year 1998 and ends at the year 2006. The resultant text
is fairly complicated, however, and a more sophisticated realization would likely
lead to a lower syntactic complexity score. We defined a number of And predicate
and Which predicate complexity estimators to look for realization opportunities in
a complex tree structure so that a syntactic complexity score which is lower than
what the most straightforward realization would produce can be assigned to that tree.
These estimators compute the syntactic complexity of a complex tree by examining
the associated realizations of all aggregated propositions in that tree in a bottom?up
fashion. Because the complex trees that are rooted by a proposition with a Same
predicate would always be realized as a conjoined sentence (Level 2), we did not define
complexity estimators for this kind of predicate.
The And predicate complexity estimators check whether or not the realizations
of two subtrees rooted by a proposition with an And predicate can be combined into
a simple sentence (Level 1), or a conjoined sentence which consists of two independent
sentences (Level 2) if one of the subtrees is rooted by a proposition with an And
predicate. For example, the And predicate estimators can successfully identify the
18 In the current implementation, there is a single realization associated with each basic proposition with
the main entity in subject position.
551
Computational Linguistics Volume 38, Number 3
following realization opportunities (based on the representations of the sentences as
propositions):
 The period starts at the year 1998. AND The period ends at the year 2006.
can be combined into:
The period is from the year 1998 to the year 2006. (Level 1)
 The trend is in the dollar value of net profit. AND The trend is over the period
from the year 1998 to the year 2006. can be combined into:
The trend is in the dollar value of net profit over the period from the year 1998
to the year 2006. (Level 1)
 The dollar value of net profit ranges from 2.77 billion dollars over the period.
AND The dollar value of net profit ranges to 0.96 billion dollars over the
period. AND The dollar value of net profit shows the largest drop of about
0.56 billion dollars between the year 2000 and the year 2001. can be
combined into:19
The dollar value of net profit ranges from 2.77 to 0.96 billion dollars over the
period and shows the largest drop of about 0.56 billion dollars between the
year 2000 and the year 2001. (Level 2)
The Which predicate complexity estimators check whether a tree rooted by a
proposition with a Which predicate can be realized as a simple adjunct or a prepo-
sitional phrase attached to the modified entity rather than a more complex relative
clause (which could increase the complexity level). For example, the Which predicate
estimators can successfully identify the following realization opportunities (based on
the representations of the sentences as propositions):
 The trend is over the period.WHICH The period is from the year 1998 to the
year 2006. can be realized as:
The trend is over the period from the year 1998 to the year 2006. (Level 1)
 The graphic shows a trend.WHICH The trend is in the dollar value of net profit
over the period from the year 1998 to the year 2006. can be realized as:
The graphic shows a trend in the dollar value of net profit over the period from
the year 1998 to the year 2006. (Level 1)
In our generation approach, multiple realizations for each proposition can be incor-
porated by defining new complexity estimators in addition to the estimators that are
used in the current implementation. Defining such estimators, which will not change
the task complexity or the underlying methodology, would add to the generalizability
of our approach.
19 In this case, the single node trees that correspond to the propositions conveying the range of the trend
form the immediate descendents of a tree rooted by a proposition with an And predicate, and that tree
with the And predicate at its root and the tree corresponding to the proposition conveying the largest
drop constitute the immediate descendents of a tree rooted by another proposition with an And
predicate.
552
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
5.3.2 Relative Clause Embedding. In cases where a tree rooted by a proposition with a
Which predicate cannot be realized as a simple adjunct or a prepositional phrase, it will
be realized by a relative clause. In the D-level classification, the complexity of a sentence
with an embedded clause is determined according to the grammatical role (subject
or object) of the entity that is modified by that clause, not the syntactic complexity
or position (center-embedded or right-branching) of the clause in the sentence. For
instance, a sentence with a complex center-embedded relative clause modifying an
object receives the same syntactic complexity score as a sentence with a simple right-
branching relative clause modifying an object. As argued in the literature, however,
center-embedded relative clauses are more difficult to comprehend than corresponding
right-branching clauses (Johnson 1998; Kidd and Bavin 2002). To capture this, our
evaluation metric for identifying the best structure penalizes Which predicates that
will be realized as a relative clause based on the clause?s syntactic complexity and
position in the sentence (which we refer to as ?comprehension complexity of a relative
clause?). For example, the following sentences receive different scores by our evaluation
metric with respect to clause embedding; the first one with a right-branching clause
(simpler) has a lower score than the second sentence with a center-embedded clause
(more complex):
 The graphic shows a decreasing trend over the period from the year 1998
to the year 2006 in the dollar value of net profit, which is 2.7 billion dollars
in the year 1999.
 The graphic shows a decreasing trend in the dollar value of net profit, which is
2.7 billion dollars in the year 1999, over the period from the year 1998 to the
year 2006.
The embedded clause (Level 0) in the first of the following sentences has a lower
syntactic score than the clause (Level 2) embedded in the second sentence. Because
our evaluation metric takes into consideration both the syntactic complexity of an
embedded clause and its position in the sentence, the first sentence receives a lower
score than the second sentence.
 The graphic shows a decreasing trend in the dollar value of net profit, which is
2.7 billion dollars in the year 1999, over the period from the year 1998 to the
year 2006.
 The graphic shows a decreasing trend in the dollar value of net profit, which is
2.7 billion dollars in the year 1999 and is 2.58 billion dollars in the year 2000,
over the period from the year 1998 to the year 2006.
5.3.3 Evaluation Metric. Our evaluation metric takes three criteria into account: the
number of sentences that will be used to realize a forest, the syntactic complexities of
these sentences, and the comprehension complexities of the embedded relative clauses.
Our metric evaluates the overall score of a candidate forest by summing the normalized
scores that the forest receives with respect to each criteria. The score of a forest (e.g.,
Forest A) is calculated as follows:
score(A) = nm1(sentence(A))+ nm2(complexity(A))+ nm3(clause(A)) (1)
553
Computational Linguistics Volume 38, Number 3
where:20
sentence(A): stands for the number of sentences that will be used to realize forest A
and equals the number of trees in that forest.
complexity(A): stands for the overall syntactic complexity of forest A and equals the
sum of the complexities of sentences that will be used to realize that forest.
clause(A): stands for the overall comprehension complexity of all relative clauses
in sentences used for realizing forest A and equals the sum of the comprehension
complexities of all clauses. The comprehension complexity of a relative clause equals
the product of its syntactic complexity and its position in the sentence, which is equal
to 2 if it is a center-embedded clause and is equal to 1 if it is a right-branching clause.
Consider, for example, the forest shown in Figure 6. Because the forest contains a
single tree, it receives a score of 1 for the sentence criteria. The syntactic complexity score
of the sentence that will be used to realize that tree is computed in a bottom?up fashion
as follows. The lowest syntactic complexity score (Level 0) is assigned to all leaf nodes,
and all inner nodes that only have single node trees with anAttributive Knowledge Base
predicate as descendents (as shown in Figure 6). Each of the remaining inner nodes is
then assessed with a syntactic complexity score once the complexity scores for all of
its descendents are computed (i.e., once the best realization possibility with the lowest
syntactic complexity for each descendent tree is determined). If an inner node contains
a proposition with an And predicate, its syntactic complexity score is computed via
the And predicate complexity estimators. Similarly, the Which predicate complexity
estimators are used to compute the syntactic complexity scores for all inner nodes with
aWhich predicate. The syntactic complexity score for the parent node of a tree rooted by
a proposition with aWhich predicate is computed based on whether or not that tree will
be realized as a relative clause (as indicated by the complexity score of the root node of
that tree). The forest shown in Figure 6 receives a score of 4 for the complexity criteria,
which is equal to the syntactic complexity score assigned to the parent node of the tree.
In Figure 6, only the tree rooted by Node 4 will be realized as a relative clause. Because
that relative clause, which receives a syntactic complexity score of 2, will be realized
as a center-embedded clause, the forest shown in Figure 6 receives a score of 4 for the
clause criteria.
In the current implementation, once the scores with respect to a criteria are com-
puted for each candidate forest, these scores (e.g., sentence(A)) are normalized with
respect to the maximum score (e.g., max(sentence(all forests))) by dividing each score
by the maximum of the computed scores. For instance, nm1(sentence(A)) is the nor-
malized score that the forest A receives with respect to the sentence criteria and is
equal to sentence(A)/max(sentence(all forests)). Thus, the normalized score that a forest
receives for each criteria is always between 0 and 1 and therefore each criteria has
an equal impact on the overall score of a forest.21 The normalized scores obtained
for a candidate forest are then summed to obtain the overall score for that forest. For
example, assume that three candidate forests, the first of which is shown in Figure 6,
20 The terms nm1, nm2, and nm3 stand for the normalized score of a given criteria.
21 The simplifying assumption of assigning equal weights to each criteria would be better optimized with
machine learning, as discussed in detail in Section 9.
554
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Figure 6
A forest containing a single tree.
555
Computational Linguistics Volume 38, Number 3
Table 3
Overall evaluation scores.
Sentence Complexity Clause Overall Score
First Forest 1(0.5) 4(1) 4(1) 2.5
Second Forest 1(0.5) 4(1) 2(0.5) 2
Third Forest 2(1) 3(0.75) 0(0) 1.75
are constructed from a set of propositions. One possible way of realizing the forest in
Figure 6 would be The graphic shows a decreasing trend in the dollar value of net profit,
which shows the largest drop of about 0.56 billion dollars between the year 2000 and the
year 2001, and shows the smallest drop of nearly 0.07 billion dollars between the year 1998
and the year 1999, over the period from the year 1998 to the year 2006. Suppose that the
second forest is similar to Figure 6 except that the children (Node 2 and Node 3) of
the And(trend) node are swapped.22 Suppose also that the third forest is similar to
Figure 6 except that the tree is decomposed into two trees, which are rooted by Node 1
and Node 5, respectively. The first tree rooted by Node 1 consists of the nodes marked
with (*) and the second tree rooted by Node 5 consists of the nodes marked with (**).
Table 3 shows the actual and the normalized scores (shown in parentheses) for each
forest with respect to each criteria, and the overall score assigned by our evaluation
metric.
The number of sentences (1) and the overall sentence complexity (Level 4) are the
same for the first and second forests. The third forest has more sentences (2) but lower
overall sentence complexity (Level 3) than the other two forests. The first forest has a
center-embedded relative clause and receives a score of 4 for the clause criteria: the
product of the complexity of the relative clause (2) and its position (2). On the other
hand, the second forest has a right-branching relative clause and receives a score of 2
for the same criteria: the product of the complexity of the relative clause (2) and its
position (1). The third forest doesn?t have an embedded clause and receives a score of 0
for the clause criteria.
5.4 Identifying the Best Text Structure
Our approach selects the forest which receives the lowest evaluation score as the best
forest that can be obtained from a set of input propositions. For example, according
to the scores shown in Table 3, the third forest, which could be realized as The graphic
shows a decreasing trend in the dollar value of net profit over the period from the year 1998
to the year 2006. The dollar value of net profit shows the largest drop of about 0.56 billion
dollars between the year 2000 and the year 2001 and shows the smallest drop of nearly 0.07
billion dollars between the year 1998 and the year 1999., would be selected as the best
among the three forests. The initial overall text structure of a brief summary con-
sists of the best forests identified for the message related, specific, and computational
classes.
As a final step, we check whether we can improve the evaluation of the overall
structure of the summary by moving trees (i.e., trees rooted by a Level-0 node such as
22 This swapping would cause the relative clause rooted by Node 4 to be a right-branching clause.
556
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
And(descriptor) in Figure 5, Specific) or subtrees (i.e., trees rooted by a Level-1 node
with an And or a Relative Knowledge Base predicate such as hasextreme (descrip-
tor,?largest?,change(3,4),period(3,4)) in Figure 5, Specific) between the best forests for
the three proposition classes. For example, the best forest for the specific class might
contain a tree that conveys information about an entity introduced by a proposition
in the message related class. Moving this tree to the message related class and using an
operator to combine it with the tree introducing the entity might improve the evaluation
of the overall structure of the summary. For example, for the graphic in Figure 4, this
movement would allow our system to evaluate a structure where the tree rooted by the
specific proposition And(descriptor) (shown in Figure 5, Specific) is attached as a de-
scendent of the tree rooted by the message related proposition exists(trend,descriptor)
(shown in Figure 5,Message Related) via aWhich Operator.We explore all such possible
movements between best forests for the proposition classes (if any) and determine
the best overall text structure of the summary. To be consistent with the motivation
behind the initial groupings of the propositions, we do not allow movements out
of the message related class or any movement that will empty the computational
class.
5.5 Evaluation of the Text Structuring and Aggregation Module
Our text structuring and aggregation approach consists of several different components,
all of which contribute to the quality of the generated text. Our study focused on
whether or not our decisions with respect to these components contributed to the
perceived quality of the resultant summary: the organization and ordering (O) of
the content (partial ordering of the propositions within classes and classification of
the propositions), the aggregation (A) of the information into more complex tree struc-
tures (candidate forests constructed via operators), and the metric (E) used to evaluate
candidate forests that represent different possible aggregations of the informational
content.
We conducted an experiment with 15 participants (university students and gradu-
ate students) who were presented with six different summaries of twelve graphics from
the test corpus (described in Section 3.3). The participants neither participated in earlier
studies (described in Sections 4.1 and 4.4) nor were involved in this work. All presented
summaries were automatically produced by our generation approach. The participants
were not told how the presented summaries were produced (i.e., human-generated
or computer-generated), however. We focused on graphics with an increasing or a
decreasing trend, since these message categories exhibit the greatest variety of possible
summaries. For each of the graphics, the participants were given a set of summaries in
random order and asked to rank them in terms of their quality in conveying the content.
The summaries varied according to the test parameters as follows:23
 S O+A+E+: A summary that uses the ordering rules, the aggregation
rules, and receives the lowest (best) overall score by the evaluation
metric. This is the summary selected as best by the TSAMModule.
23 Although eight different summaries are logically possible with three different variables, we limited the
number to four (the second and the third in which exactly one of the components was turned off and the
fourth where all components were turned off) in order to keep the experiment within a manageable size.
557
Computational Linguistics Volume 38, Number 3
Table 4
Ranking of summary types.
Summary Type Best 2nd 3rd 4th
S O+A+E+ 65.6% 26.6% 6.7% 1.1%
S O+A+E- 16.7% 32.2% 33.3% 17.8%
S O-A+E+ 16.7% 30% 40% 13.3%
S O-A-E- 1% 11.2% 20% 67.8%
 S O+A+E-: A summary that uses the ordering and aggregation rules,
but does not receive the lowest overall score by the evaluation metric.
This is the summary that received the second lowest score.
 S O-A+E+: A summary where the propositions are randomly ordered,
but aggregation takes place, and it receives the lowest (best) overall
score by the evaluation metric.
 S O-A-E-: A summary consisting of single sentences that are randomly
ordered.
Table 4 presents the results of the experiment. It is particularly noteworthy that the
summary selected as the best by the Text Structuring andAggregationModulewasmost
often (65.6% of the time) rated as the best summary and overwhelmingly (92.2% of the
time) rated as one of the top two summaries. The table shows that omitting the eval-
uation metric (S O+A+E-) or omitting ordering of propositions (S O-A+E+) results in
summaries that are substantially less preferred by the participants. Overall, the results
shown in Table 4 validate our ordering, aggregation, and evaluation methodology.
6. Sentence Ordering Module (SOM)
With the use of different kinds of operators and an evaluation metric, our system
determines the partial ordering and the structure of sentences that will be used to realize
the selected content but doesn?t impose ordering constraints (final ordering) on the sen-
tences within each proposition class. To decide in which sequence the sentences should
be conveyed, we take advantage of the fact that each proposition has a defined main
entity, which will be realized in the subject position of the sentence that will be used to
realize the proposition. Identifying the subject of the realized sentences in advance al-
lows us to use centering theory (Grosz, Weinstein, and Joshi 1995) to generate a text that
is most coherent according to this theory.24 The theory outlines the principles of local
text coherence in terms of the way the discourse entities are introduced and discussed,
and the transitions between successive utterances in terms of the entities in the hearer?s
center of attention. Although some fundamental concepts of the theory, such as the
ranking of entities in an utterance, aren?t explicitly specified, various researchers have
applied centering theory to language generation (Kibble and Power 2004; Karamanis
et al 2009) with different interpretations. In our work, each sentence is regarded as an
24 If this assumption is relaxed, then centering theory would not be appropriate for an ordering component.
In that case, a focusing theory such as McCoy and Cheng (1991), or Suri and McCoy (1994) could be used
to order the sentences to be realized.
558
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
utterance. Following Brennan, Friedman, and Pollard (1987) and Grosz, Weinstein, and
Joshi (1995), we rank the entities with respect to their grammatical functions where the
entity in subject position is the most salient entity. When ordering sentences, we take
into account the preference order for centering transitions: continue is preferred over
retain, which is preferred over smooth shift, which is in turn preferred over rough shift.
For all message categories, the number of sentences in a proposition class would
be limited (less than five) even if all of the highly rated propositions identified for
that message category are selected for inclusion. Thus, a straightforward ?generate and
test? strategy is appropriate for ordering sentences in our case. For each proposition
class, all possible orderings of the sentences within that class are generated. We assign
different numeric scores to each centering transition, where continue receives a score
of 3, and retain and smooth shift receive scores of 2 and 1, respectively. The rough shift
transitions are assessed a score of 0. For each candidate ordering, we sum the scores
for the kinds of transitions observed between consecutive sentences. The ordering that
receives the highest score is selected as the best ordering for that proposition class. First,
the best ordering of the sentences in the message related proposition class is selected.
The subject of the last sentence in that ordering is used as the backward-looking center
of the previous utterance when determining the best ordering of the sentences in the
specific proposition class. Similarly, the subject of the last sentence in the best ordering
for the specific class is used as the backward-looking center when identifying the best
ordering for the computational proposition class.
For graphics that depict a time period, we also utilize the time periods mentioned in
each conjunct of a conjoined sentence in order to specify in which order these conjuncts
will be conveyed in the realized text. If the time periods mentioned in each conjunct of
a conjoined sentence are different, these conjuncts are ordered such that the time period
in focus in the first conjunct subsumes or precedes the time period in focus in the second
conjunct. Consider how individual sentences in the following compound sentences are
ordered by our approach:
 The dollar value of net profit ranges from 2.77 to 0.96 billion dollars over the
period from the year 1998 to the year 2006 and shows the largest drop of about
0.56 billion dollars between the year 2000 and the year 2001.
 The dollar value of net profit shows the smallest drop of nearly 0.07 billion dollars
between the year 1998 and the year 1999 and shows the largest drop of about
0.56 billion dollars between the year 2000 and the year 2001.
Note that in the first conjoined sentence, the time period mentioned in the first con-
junct (from 1998 to 2006) subsumes the time period mentioned in the second conjunct
(between 2000 and 2001). On the other hand, in the second conjoined sentence, the time
period mentioned in the first conjunct (between 1998 to 1999) precedes the time period
mentioned in the second conjunct (between 2000 and 2001).
7. Sentence Generation Module (SGM)
To realize the summaries in natural language, we use the FUF/SURGE surface real-
izer (Elhadad and Robin 1999), which offers the richest knowledge of English syntax and
widest coverage among the publicly available realizers such as REALPRO (Lavoie and
Rambow 1997). The realization of the sentence-sized units requires referring expressions
559
Computational Linguistics Volume 38, Number 3
for certain graphical elements, however. Our system handles three different issues with
respect to referring expression generation:
 Generating a referring expression for the dependent axis. Information
graphics often do not label the dependent axis with a full descriptor of
what is being measured (which we call themeasurement axis descriptor).
In such a situation, a referring expression for this element must be
extracted from the text of the graphic. For example, to realize its summary,
a measurement axis descriptor (e.g., the dollar value of Chicago Federal Home
Loan Bank?s mortgage program assets) must be generated for the graphic in
Figure 7a, whose dependent axis is not explicitly labeled with the
descriptor.
 Generating a referring expression in order to refer to the bars on the
independent axis (e.g., the countries for the graphic in Figure 1). Such an
expression must be inferred from the bar labels or extracted from the text
of the graphic. This referring expression is often used in the summaries of
graphics in some message categories (e.g., Maximum Bar) that require
comparing a bar with others (e.g., distinguishing the bar with the
maximum value from all other bars).
 It was shown that people prefer less informative descriptions for
subsequent mentions of an entity (Krahmer and Theune 2002). In order to
generate more natural summaries, the syntactic forms of the subsequent
mentions of discourse entities should be constructed in a way that helps
text coherence.
7.1 Measurement Axis Descriptor
Generation of referring expressions (noun phrases) is one of the key problems explored
within the natural language generation literature. There is a growing body of research
in this area that, given a knowledge base of entities and their properties, deals with
Figure 7
(a) Graphic from Business Week. (b) Graphic from Business Week.
560
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
determining the set of properties that would single out the target entity (Dale and Reiter
1995; Krahmer, Van Erk, and Verleg 2003). More recently, the generation of referring
expressions has been proposed as a postprocessing technique to deal with the lack of
text coherence in extractive multidocument summarization (Belz et al 2008). Nenkova
and McKeown (2003) developed a method to improve the coherence of a multidoc-
ument summary of newswire texts by regenerating referring expressions for the first
and subsequent mentions of people?s names where the expressions are extracted from
the text of the input documents according to a set of rewrite rules. The task that we
face is similar to this recent body of research in that contextually appropriate referring
expressions for certain graphical elements should be extracted from the text of the
graphic. At the same time, our task is more complex in some respects. First, it is often the
case that the required referring expression isn?t explicitly given as a single unit and thus
must be constructed by extracting and combining pieces of information from the text of
the graphic. Second, in some cases where the dependent axis is explicitly labelled with a
descriptor, it still needs to be augmented. We undertook a corpus study in order to iden-
tify how a measurement axis descriptor could be generated from the text of a graphic;
the results of the analysis form the basis for the heuristics and augmentation rules
we developed for generating the measurement axis descriptor for a graphic. In Demir,
Carberry, and Elzer (2009), we outlined this problem as generating a graphical element
required for realizing the intended message of a graphic and thoroughly described the
technical details of our approach. Here, however, we treat this particular aspect as a
novel text-to-text generation methodology which is combined with other data-to-text
approaches in a complete NLG system. Thus, our focus in this section is to highlight
a new way of using the text associated with images which has been earlier exploited
by various NLP tasks such as indexing and retrieval of images (Pastra, Saggion, and
Wilks 2003).
7.1.1 Corpus Analysis. Graphic designers generally use text within and around a graphic
to present information related to the graphic. We started our analysis by examining how
texts are distributed around each group of graphics. We observed that graphics (indi-
vidual or composite) contain a set of component texts that are visually distinguished
from one another by blank lines, by different fonts/styles, or by different directions
and positions in the graphic. Although the number of component texts present in a
graphic may vary, our analysis recognized an alignment or leveling of text contained in
a graphic, which we refer to as ?text levels.?
We observed seven text levels which we refer to as Overall Caption, Overall
Description, Caption, Description, Dependent Axis Label, Text In Graphic, and Text
Under Graphic. Not every level appears in every graphic. Overall Caption and Over-
all Description apply to composite graphics that contain more than one individual
graphic (the graphics might be of different kinds) and refer to the entire collection
of graphics in the composite. In composite graphics, Overall Caption is the text that
appears at the top of the overall group and serves as a caption for the whole set (such
as Tallying Up the Hits in Figure 8a). In composite graphics, there is often another
text component placed under the Overall Caption but distinguished from it by a line
break or a change in font. This text component, if present, is also pertinent to all
individual graphics in the composite graphic and elaborates on them. We refer to such
text as the Overall Description (such as Yahoo once relied entirely on banner ads. Now it?s
broadened its business mix in Figure 8a). Caption and Description serve the same roles
for an individual graphic. For example, the Caption for the bar chart in Figure 8a is
Active Users and the Description is Registered users in millions. The Caption of Figure 8b
561
Computational Linguistics Volume 38, Number 3
Figure 8
(a) A composite graph from Newsweek.25 (b) Graphic from Business Week.
Table 5
Text levels in bar charts.
Text level Frequency of occurrence
Overall Caption 31.8% (?34/107)
Overall Description 17.8% (?19/107)
Caption 99.0% (?106/107)
Description 54.2% (?58/107)
Text In Graphic 39.3% (?42/107)
Dependent Axis Label 18.7% (?20/107)
Text Under Graphic 7.5% (?8/107)
is A Growing Biotech Market but this graphic does not have a Description. There may be
a label on the dependent axis itself and we refer to it as Dependent Axis Label (such
as Revenues (in billions) in Figure 8b). In addition to the text levels described so far
which appear outside the borders of a graphic, we have observed that there is often
a text component residing within the borders of a graphic which we refer to as Text
In Graphic (such as U.S. Biotech Revenues, 1992?2001 in Figure 8b). Finally, Text Under
Graphic is the text under a graphic which usually starts with a marker symbol (such
as *) and is essentially a footnote (such as U.S. only, one available seat flown one mile, year
ending June 2002 in Figure 7b). Each Text Under Graphic has a referrer elsewhere that
ends with the same marker and that referrer could be at any text level of the graphic. A
graphic might have more than one Text Under Graphic but each is differentiated with
a different marker. For each of the 107 graphics in our corpus (described in Section 3.3),
the Visual Extraction System extracts these text levels from the graphical image of the
bar chart and inserts them into the graph?s XML representation. Table 5 lists the various
text levels, along with how often they appeared in the graphics in our corpus.
25 This figure displays two of the five individual graphs constituting the composite graphic that appeared in
Newsweek.
562
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Two annotators first analyzed each graphic in our corpus and determined a mea-
surement axis descriptor for the graphic; the annotators used both the information
residing within the text components of the graphic and the article, and commonsense
knowledge. All ideal descriptors were noun phrases or wh-phrases26 (such asWhat?s the
most important issue affecting voters? vote? on a graphic depicting survey results). After the
descriptors were identified, we analyzed the graphics to see how the descriptors could
be generated from the text components of the graphic. We observed that an acceptable
measurement axis descriptor often cannot be extracted as a whole from a single text
level and instead must be put together by extracting pieces from one or several different
text levels; the pieces, though coming from a single level, might not be contiguous in that
level and still need to be melded into a coherent whole. In some cases, the information
is also retrieved from other graphics in the same composite or from the article?s text.
Our analysis has also led us to hypothesize that the ideal measurement axis descriptor
can be viewed as consisting of a core?a basic noun or wh-phrase from one text level
that is often augmented with text from another level (or in some cases, from text in the
accompanying article or other graphs in the same composite) to be more descriptive
and complete. For example, for the bar chart in Figure 8a, registered users is the core
of the ideal measurement axis descriptor which is Yahoo?s registered users. The core is
found in the Description and the augmentation to the core is found in the Overall
Description. When more than one text level is used, the text levels that contain pieces of
themeasurement axis descriptor also vary among the graphics.We observed thatmostly
text levels particular to a graphic (such as Text In Graphic and Description) contain the
pieces of the descriptor as opposed to the levels containing shared information (such
as Overall Description), and with the exception of Text Under Graphic, the ordering of
text levels in Table 5 forms a hierarchy of textual components, with Overall Caption
and Dependent Axis Label respectively at the top and bottom of the hierarchy, such that
the core generally appears in the lowest text level present in the graphic. During the
corpus analysis, we observed three ways in which a core extracted from one text level
was augmented with text from another text level:
 Expansion of the noun phrase: The nouns in the core of the descriptor
were replaced with a noun phrase at another text level which has the same
noun as its head. The replaced noun phrase appeared in a text level higher
in the precedence order than the text level at which the core appears.
Consider, for example, Figure 8b. The core of the descriptor is Revenues
(appearing in the Dependent Axis Label), which is reasonable enough
to be the core, but the noun Revenues should be replaced with U.S. Biotech
Revenues in order to be complete.
 Specialization of the noun phrase: The core was augmented with a
proper noun which specialized the descriptor to a specific entity.
Consider, for example, Figure 8a, which shows a composite graph where
individual graphics present different attributes of the same particular
entity (Yahoo). The ideal measurement axis descriptor of the bar chart
(Yahoo?s registered users) consists of the core registered users (appearing in
the Description) augmented with the proper noun Yahoo that appears in
the Overall Description.
26 Generally seen in graphics presenting the results of a survey.
563
Computational Linguistics Volume 38, Number 3
 Addition of detail: Text Under Graphic typically serves as a footnote to
give specialized detail about the graphic which is not as important as the
information given in other text levels. If the Text Under Graphic began
with a footnote marker, such as an asterisk, and the core was followed by
the same marker, then Text Under Graphic added detail to the core.
Consider, for example, Figure 7b, where unit costs is the core but the ideal
measurement axis descriptor (Unit costs, U.S. only, one available seat flown
one mile, year ending June 2002) also contains the information from the
Text Under Graphic.
7.1.2Methodology. First, preprocessing deletes the scale and unit indicators (phrases used
to give the unit and/or a scale of the values presented in the graphic), and the ontolog-
ical category of the bar labels (if explicitly marked by the preposition by) from the text
levels. Next, heuristics are used to identify the core of the measurement axis descriptor
by extracting a noun phrase or a wh-phrase from a text level of the graphic. Three kinds
of augmentation rules, corresponding to the three kinds of augmentation observed in
our corpus, are then applied to the core to produce the measurement axis descriptor.
If none of the augmentation rules are applicable, then the core of the descriptor forms
the measurement axis descriptor. Finally, if the measurement axis descriptor does not
already contain the unit of measurement (such as percent), the phrase indicating the unit
of measurement is appended to the front of the measurement axis descriptor.
For identifying the core of the measurement axis descriptor, we developed nine
heuristics that are dependent on the parses of the text levels. Two of these heuristics are
restricted to Dependent Axis Label and Text In Graphic, and the remaining heuristics
are applicable to all other text levels. The application of the heuristics gives preference
to text levels that are lower in the hierarchy and if a core is not identified at one text
level, the applicable heuristics are applied, in order, to the next higher text level in
the hierarchy. For example, suppose that the graphic contains only a Description and
a Caption and thus the first two heuristics are not applicable. The next seven heuristics
are first applied to the Description and then to the Caption. The following presents three
representative heuristics where the first two heuristics are applicable only to Dependent
Axis Label and Text In Graphic:27
 Heuristic-1: If the Dependent Axis Label consists of a single noun phrase
that is not a scale or unit indicator, that noun phrase is the core of the
measurement axis descriptor.
 Heuristic-2: If Text In Graphic consists solely of a noun phrase, then that
noun phrase is the core; otherwise, if Text In Graphic is a sentence, the
noun phrase that is the subject of the sentence is the core.
 Heuristic-6: If a fragment at the text level consists solely of a noun phrase,
and the noun phrase is not a proper noun, that noun phrase is the core.
Once the core is identified, augmentation rules are applied to fill out the descriptor.
For example, consider the graphic in Figure 8b where Heuristic-1 identifies Revenues
in Dependent Axis Label as the core. Because the core and the Text In Graphic, U.S.
Biotech Revenues, have the same head noun, the augmentation rule for expansion
27 All of the heuristics and augmentation rules can be found in Demir, Carberry, and Elzer (2009).
564
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
produces U.S. Biotech Revenues as the augmented core. After adding a phrase for
the unit of measurement, the referring expression for the dependent axis becomes
The dollar value of U.S. Biotech Revenues. As another example, consider the graphic in
Figure 7b. Our work uses Heuristic-2 and the augmentation rule for adding detail.
After adding a phrase representing the unit of measurement, the referring expression
for the dependent axis becomes The cent value of unit costs (U.S. only, one way available seat
flown one mile, year ending June 2002). Finally, consider the graphic in Figure 8a, where
Heuristic-6 identifies the noun phrase registered users as the core.28 The augmentation
rule for specialization finds that Yahoo is the only proper noun in the text levels and
does not match a bar label, and forms Yahoo?s registered users. After adding a phrase
representing the unit of measurement, the referring expression for the dependent axis
becomes The number of Yahoo?s registered users.
In order to evaluate our approach, we constructed a distinct test corpus consisting
of 205 randomly selected bar charts from 21 different newspapers and magazines;
only six of these sources were also used to gather the corpus described in Section 3.3.
For each graphic, we used our approach to generate the referring expression for the
dependent axis. Finally, the resultant output and three baselines were evaluated by
two evaluators (Demir, Carberry, and Elzer 2009). The evaluation results showed that
our approach performs much better than any of the baselines for the 205 graphics in
the corpus. The detailed analysis of the results also showed that our methodology is
applicable to a wider range of sources in popular media.
7.2 Generating an Expression for Referring to All Bars
For some message categories (for example, Maximum Bar), the identification of the
ontological category for the bar labels results in better natural language than merely
using a generic expression; for example, compare the phrase among the countries listed
with the phrase among the entities listed in producing natural language text for the
message conveyed by the graphic in Figure 1. There are a number of different pub-
licly available ontologies such as WordNet Fellbaum (1998) and OpenCyc (2011). In
our work, we need a knowledge base that offers both the semantic relations between
words and general commonsense knowledge. For example, WordNet could not iden-
tify Jacques Chirac, a former president of France, whereas OpenCyc ontology contains
this information. Therefore, we use OpenCyc ontology version v0.7.8b to identify the
ontological categories of bar labels in our work. Our implemented system currently
finds the most specific category that is a common category for at least two of the bar
labels and identifies it as the ontological category.
Grice?s Maxim of Quantity (1975) states that one?s discourse contribution should be
as informative as necessary for the purposes of the exchange but not more so. If our
system were to enumerate all entities involved in a comparison message, the realization
of the inferred message might be lengthy and the enumeration of little utility to the
user. Thus we set a cut-off C, such that if the number of entities involved in a Maximum
Bar or Rank Bar message exceeds C, they are not enumerated but rather we use the
generated referring expression for all bars. The cut-off value is currently set to 5 in our
implemented system.
28 The preprocessing of this text level would remove In millions because it is a scale indicator.
565
Computational Linguistics Volume 38, Number 3
7.3 Subsequent Mentions of Discourse Entities
In the current implementation of the system, the syntactic form of a subsequent mention
of an entity is determined based on its salience status in the context. In particular, the
backward-looking center of an utterance29 is replaced with a less informative definite
noun phrase after a continue or a retain transition because the backward-looking cen-
ter remains the same in the latter utterance. In such cases, the definite noun phrase
is constructed by adding the demonstrative this or these to the front of the head
noun of the backward-looking center, such as these revenues for the phrase U.S. biotech
revenues.
7.4 Example Summaries
For the graphic in Figure 1, our system generates the following textual summary: The
graphic shows that United States at 32,434 has the highest number of hacker attacks among the
countries Brazil, Britain, Germany, Italy, and United States. United States has 5.93 times more
attacks than the average of the other countries.
For the graphic in Figure 3a, the following textual summary is generated: The graphic
shows an increasing trend in the dollar value of Lands? End annual revenue over the period from
the year 1992 to the year 2001. The dollar value of Lands? End annual revenue shows an increase
of nearly 225 percent. Except for a small drop in the year 1999, slight increases are observed
almost every year.
For the graphic in Figure 3b, our system generates the following summary: The
graphic compares the defense agencies army, navy, air force, and other defense agencies, which
are sorted in descending order with respect to the number of civilian employees. The number
of civilian employees is highest for army at 233,030 and is lowest for other defense agencies at
100,678.30
For the graphic in Figure 4, the following textual summary is generated: The graphic
shows a decreasing trend in the dollar value of net profit over the period from the year 1998 to the
year 2006. The dollar value of net profit ranges from 2.77 to 0.96 billion dollars over the period
from the year 1998 to the year 2006 and shows the largest drop of about 0.56 billion dollars
between the year 2000 and the year 2001. Slight decreases are observed almost every year.
For the graphic in Figure 8b, our system generates the following summary: The
graphic shows an increasing trend in the dollar value of U.S. Biotech Revenues over the period
from the year 1992 to the year 2001. Increasing slightly every year, the dollar value of U.S.
Biotech Revenues shows an increase of nearly 265 percent and ranges from 7.87 to 28.52 billion
dollars.
For the graphic in Figure 7a, the following textual summary is generated: In the year
2003, the graphic shows a much higher rise in the dollar value of Chicago Federal Home Loan
Bank?s mortgage program assets in contrast with the moderate increases over the period from the
year 1998 to the year 2002. The dollar value of Chicago Federal Home Loan Bank?s mortgage
program assets reaches to 94.23 billion dollars in the year 2003. The dollar value of these assets
in the year 2003 is nearly 49.1 times higher than that in the year 1998.
For the graphic in Figure 9, the following textual summary is generated: This graphic
is about American Express. The graphic shows that American Express at 255 billion dollars is
29 The backward-looking center of a current utterance is the most highly ranked entity of the previous
utterance that is realized in the current utterance.
30 The reason for saying that the defense agencies are sorted in decreasing order is not to enable the reader
to visualize the graphic but rather that it subsumes giving the ranking of each bar.
566
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Figure 9
Graphic conveying the rank of a bar.
the third highest with respect to the dollar value of total credit-card purchases per year among
the entities Visa, Mastercard, Discover, Diner?s Club, and American Express.
8. Evaluation of the Effectiveness of the Textual Summaries
The earlier user studies (Sections 4.4 and 5.5) demonstrated the effectiveness of our gen-
eration methodology in identifying and presenting the high-level content of bar charts.
The success of a generation system depends not only on the quality of the produced text,
however, but also on whether the text achieves the impact that it is intended to make on
readers (such as enabling readers to perform a task or changing their opinions in some
context). We conducted an evaluation study to measure how adequate and effective
the summaries generated by our system are for our purpose of providing the message
and high-level knowledge that one would gain from viewing a graphic. Specifically, we
were interested in (1) what amount of information is retained by a reader from reading
the summary generated by our system, (2) whether someone reading the summary
garners the most important knowledge conveyed by the graphic, and (3) whether the
knowledge gleaned from the summary is consistent with the actual graphic.
In this study, we used four graphics from the test corpus (described in Section 3.3)
with different intended messages. These graphics conveyed an increasing trend (i.e.,
Figure 3a), a decreasing trend, the rank of the maximum bar (i.e., Figure 1), and the
rank of a bar (i.e., Figure 9) among all bars. In the first part of the study, each of the
18 participants (graduate students) was first presented with the summaries generated
by our system for these graphics; the participants neither saw the original graphics
(the graphical images of the bar charts) nor were aware of our system and how the
summaries were generated. For each summary, the participants were asked to draw
the bar chart being described in that summary. Although enabling a reader to redraw
the graphic is not a goal of our work, comparing a reader?s mental representation
of the graphic with the actual graphic allows us to identify whether there are any
inconsistencies between knowledge gleaned from the summary and the content of the
actual graphic.
In the second part of the study, we asked three evaluators not involved in this re-
search to evaluate the drawings that we collected from the participants. The evaluators
were Ph.D. students from the University of Delaware (none of them were the authors
of this work) and had an overall knowledge about our summarization approach (i.e.,
what is intended to be conveyed in the summaries of graphics). The evaluators were
567
Computational Linguistics Volume 38, Number 3
first told that a set of participants were presented with brief summaries of bar charts
and asked to draw the corresponding bar charts based on the information presented
in those summaries. They were also told that each summary only conveyed what is
identified by our system as the most important information that should be conveyed
about the bar chart being described. The evaluators were presented with the graphical
images of the four bar charts used in the study (none saw the summaries presented in
the first part of the study) and the drawings collected from the participants, and then
asked to rate each drawing using the following evaluation scores:
 5: The drawing is essentially the same as the original graphic
 4: The drawing captures all important information from the original
graphic but requires some minor modifications
 3: The drawing captures most of the important information from the
original graphic but is missing one significant piece of information
 2: The drawing reflects some information from the original graphic but
requires major modifications
 1: The drawing fails to reflect the original graphic
The average score that a drawing received from the evaluators ranged from 3 to 5.
The evaluators viewed the drawings drawn for the graphics with a trend more favor-
ably and assigned a score of 4 or more in most cases. For each graphic, we computed the
average score given by the evaluators to all drawings constructed from the summary
of that graphic (i.e., the average of the three scores given to each of the 18 drawings
drawn from the same summary). The graphics conveying an increasing (Figure 3a) and
a decreasing trend received a score of 4.22 and 4.63, respectively. The evaluators gave an
average score of 3.53 and 4.07 to the graphics which conveyed the rank of the maximum
bar (Figure 1) and the rank of a bar among all bars (Figure 9). Because we do not present
all features of a bar chart in its summary (such as all bar values), obtaining an average
score of less than 5 for all bar charts is not surprising.
We also asked the evaluators to specify a reason (i.e., what is missing or should be
changed) for the cases where they assigned a score of less than 4. Once we analyzed
their feedback for the drawings with a trend, we observed that missing values on
the dependent axis (i.e., tick mark labels) and missing measurement axis descriptors
(although given in the summaries) were the main reasons. We argue that presenting
tick mark labels is more appropriate for summaries that describe scientific graphics
(such as the summaries generated by the iGRAPH-Lite system [Ferres et al 2007]) in
contrast to the summaries that we generate for conveying the high-level content of
a graphic. The evaluators indicated incorrect rankings of some bars as the reason for
giving lower scores to the drawings that present the rank of the maximum bar or the
rank of a bar among all bars; this is due to the fact that our summaries did not convey
the rankings and the values of all bars in those cases. Because the intention of the
corresponding graphics is to convey the rank of a single bar (not all bars), we argue that
our summaries facilitate the readers to get the main purpose of these graphics. Overall,
this study demonstrated that our summarization approach is effective in conveying
the high-level content of bar charts so as to enable readers to correctly understand the
main point of the graphic. We are planning to conduct future studies to explore the
effectiveness of our approach further, however. One possible evaluation could be asking
568
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
a different set of participants to draw the bar charts that we used in this study by reading
their summaries produced by an appropriate baseline approach, and comparing the
scores that those new set of drawings received from the same three evaluators with our
current results. This evaluation will also allow us to determine whether the evaluators
judged our summarization approach favorably because they were aware of the overall
approach (i.e., brief summaries are generated by our system and these summaries do
not contain everything that can be conveyed by the corresponding bar charts).
Favorable results were also achieved when people with visual impairments were
presented with the brief summaries generated by our work in an interactive system
which also enabled the user to ask follow-up questions to learn more about about the
graphic. More on that system and study can be found in Demir et al (2010).
9. Conclusion and Future Work
The majority of information graphics from popular media are intended to convey a
message that is often not captured by the text of the document. Thus graphics, along
with the textual segments, contribute to the overall purpose of a multimodal document
and cannot be ignored. The work presented in this article is the first to apply natural lan-
guage generation technology to provide themessage and high-level knowledge that one
would gain by viewing graphics from popular media via brief textual summaries. Our
summarization approach treats a graphic as a form of language and utilizes the inferred
intention of the graphic designer, the communicative signals present in the graphic, and
the significant visual features of the underlying data in determining what to convey
about that graph. Our approach uses a set of content identification rules constructed for
each intended message category of a bar chart in determining the content of the sum-
maries. The propositions selected for inclusion by these rules are organized into a text
structure by applying a novel bottom?up approach which leverages different discourse
related considerations such as the number and syntactic complexity of sentences and
clause embeddings that will be used for realization. Following the generation of refer-
ring expressions for certain graphical elements, the structure of a summary is realized
in natural language via a publicly available realizer. Three different evaluation studies
validated the effectiveness of our approach in selecting and coherently organizing the
most important information that should be conveyed about a bar chart, and enabling
readers to correctly understand the high-level knowledge of the graphic.
In addition to the application area, this article makes contributions to two broad
areas of research: data-to-text generation systems and text-to-text generation of referring
expressions. Here we have viewed the generation of a summary of an information
graphic as a data-to-text generation problem. Any data-to-text generation system must
solve several important problems: (1) out of all of the information in the data, extract
out that information that is important enough to be included in the text, (2) structure
the information so it can be realized as a coherent text, (3) aggregate propositions to
be conveyed in the text into more complex yet understandable sentence structures, (4)
order the resulting sentence structures so as to maintain text fluency, and (5) realize the
information as English sentences and generate appropriate referring expressions. Our
work has addressed each of these issues in a systematic fashion maintaining modularity
of system components and following a development methodology that includes human
input for making system decisions, and a thorough evaluation of each module as well
as final system evaluation.
Although the specific implementations that we developed are geared toward gen-
erating summaries of bar charts, the groundwork described in this article is currently
569
Computational Linguistics Volume 38, Number 3
being used by our own group to investigate summarizing other types of graphics (such
as line graphs and grouped bar charts) from popular media. A Bayesian system for
recognizing the intended message of line graphs has already been developed (Wu et al
2010) and the work on constructing the content identification rules for line graphs is
under way (Greenbacker, Carberry, and McCoy 2011).
The module that generates referring expressions represents a sophisticated study of
text-to-text referring expression generation. Referring expression generation is a vibrant
field. Although the particular rules used for extracting an appropriate referring expres-
sion are unique to referring expressions for graphical elements inside an information
graphic (note: not just bar charts), the work has uncovered some rather interesting
properties in terms of extracting expressions from text which may generalize to other
domains as well. Our work is unique in that a full referring expression is pieced together
from text not directly referring to the item in question. In order to do this, we identified
text levels and rules for extracting the core expression along with potential important
modifiers. One can imagine this message carrying over to other types of data-to-text
activities as well as to more standard text-to-text generation problems.
In future work, we intend to build on the work reported here in several ways.
Corpus studies where human subjects tasked with writing brief summaries of graph-
ics (the kind of summaries that our system intends to generate) would be of great
potential in informing generation decisions that our system makes at different levels.
Moreover, experts with extensive knowledge of the domain or the targeted end users
were shown to be of greater supply to the development of manyNLG systems. Learning
from summaries written by subjects (especially expert writers) would be an exciting
area of future research. We also believe that our approach would benefit from these
corpus studies towards exploring how the fluency of the summaries can be further
improved particularly by reducing the occasional verbosity in order to achieve tex-
tual economy (Stone and Webber 1998). For example, these efforts might help us to
determine how measurement axis descriptors can be more appropriately phrased in
different situations. Improving the current evaluation metric for choosing a text plan is
also in our research agenda. We utilize three criteria in our evaluation metric for deter-
mining the best structure that can be obtained by applying the operators to the selected
propositions. In addition, each criteria (the number of sentences, the overall syntactic
complexity of sentences, and the overall comprehension complexity of all embedded
clauses) has the same impact on the selection process. We are considering conducting
more user studies in order to identify what other criteria should be taken into account
and how important each criteria should be in relation to all the others. Moreover,
exploring the broader applicability of the novel aspects of our work in other settings
is an interesting topic for future work. Finally, evaluating the utility of our theoretically
informed aggregation mechanism in comparison to or in conjunction with the more
surface-oriented mechanism of the SPoT system would be a promising area for further
research.
To our best knowledge, what kinds of summaries best serve the needs of visually
impaired individuals has not been throughly studied before. As mentioned in Sec-
tion 2.2.1, we believe that our summaries, once associated with graphics as ALT texts,
might be of help to these individuals while reading electronic documents via screen
readers. One fruitful research direction would be to present such individuals with our
summaries in real-time scenarios and to mine their informational and presentational
needs. Such a study would probably provide insights with regard to this question
and potentially lead to guidelines that human summarizers could follow in generating
summaries for people with visual impairments.
570
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Acknowledgments
The authors would like to thank the study
volunteers for their time and willingness
to participate. This material is based upon
work supported by the National Institute on
Disability and Rehabilitation Research under
grant no. H133G080047. For all graphics that
were used in our evaluations and are given
as examples in this article, inputs (i.e., the
inferred intended message and the XML
representation of the graphic) and outputs
(i.e., the textual summary of the graphic) of
our system can be found at the following
Web page: www.cis.udel.edu/?carberry/
barcharts-corpus/.
References
Alty, James L. and Dimitrios I. Rigas. 1998.
Communicating graphical information
to blind users using music: the role of
context. In Proceedings of the SIGCHI
Conference on Human Factors in Computing
Systems, pages 574?581, Los Angeles, CA.
Baldwin, Breck and Thomas Morton.
1998. Dynamic coreference-based
summarization. In Proceedings of the
3rd Conference on Empirical Methods in
Natural Language Processing, pages 1?6,
Granada.
Barzilay, Regina, Noemie Elhadad, and
Kathleen McKeown. 2002. Inferring
strategies for sentence ordering in
multidocument news summarization.
Journal of Artificial Intelligence Research,
17:35?55.
Barzilay, Regina and Mirella Lapata.
2006. Aggregation via set partitioning
for natural language generation.
In Proceedings of the Human Language
Technology Conference of the North
American Chapter of the Association of
Computational Linguistics, pages 359?366,
New York, NY.
Belz, Anja, Eric Kow, Jette Viethen, and
Albert Gatt. 2008. The Grec challenge
2008: Overview and evaluation results.
In Proceedings of the 5th International
Natural Language Generation Conference,
pages 183?191, Salt Fork, OH.
Brennan, Susan E., Marilyn W. Friedman,
and Carl J. Pollard. 1987. A centering
approach to pronouns. In Proceedings
of the Annual Meeting of the Association of
Computational Linguistics, pages 155?162,
Stanford, CA.
Carberry, Sandra, Stephanie Elzer, and Seniz
Demir. 2006. Information graphics: An
untapped resource for digital libraries.
In Proceedings of the ACM Special Interest
Group on Information Retrieval Conference,
pages 581?588, Seattle, WA.
Chester, Daniel and Stephanie Elzer. 2005.
Getting computers to see information
graphics so users do not have to. In
Proceedings of the 15th International
Symposium on Methodologies for Intelligent
Systems, pages 660?668, Saratoga
Springs, NY.
Clark, Herbert. 1996. Using Language.
Cambridge University Press, Cambridge.
Coch, Jose. 1998. Interactive generation and
knowledge administration in multimeteo.
In Proceedings of 9th International Workshop
on Natural Language Generation,
pages 300?303, Niagara-on-the-Lake.
Corio, Marc and Guy Lapalme. 1999.
Generation of texts for information
graphics. In Proceedings of the 7th European
Workshop on Natural Language Generation,
pages 49?58, Toulouse.
Covington, M., C. He, C. Brown, L. Naci,
and J. Brown. 2006. How complex is that
sentence? A proposed revision of the
rosenberg and abbeduto D-level scale.
Research Report, Artificial Intelligence
Center, University of Georgia.
Cycorp. Open Cyc. 2011.
http://www.cyc.com.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
gricean maxims in the generation of
referring expressions. Cognitive Science,
19(2):233?263.
Dalianis, Hercules. 1999. Aggregation
in natural language generation.
Computational Intelligence, 15(4):384?414.
Demir, Seniz, Sandra Carberry, and
Stephanie Elzer. 2009. Issues in Realizing
the Overall Message of a Bar Chart,
John Benjamins, 5th edition.
Amsterdam, pages 311?320.
Demir, Seniz, David Oliver, Edward
Schwartz, Stephanie Elzer, Sandra
Carberry, Kathleen F. McCoy, and Daniel
Chester. 2010. Interactive sight: Textual
access to simple bar charts. The New
Review of Hypermedia and Multimedia,
16(3):245?279.
Di Eugenio, Barbara, Davide Fossati,
Dan Yu, Susan Haller, and Michael Glass.
2005. Aggregation improves learning:
Experiments in natural language
generation for intelligent tutoring
systems. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics, pages 50?57,
Ann Arbor, MI.
571
Computational Linguistics Volume 38, Number 3
Elhadad, M. and J. Robin. 1999. SURGE:
A comprehensive plug-in syntactic
realization component for text generation.
Technical Report, Department of
Computer Science, Ben Gurion
University. Beersheba, Israel.
Elzer, Stephanie, Sandra Carberry, and
Ingrid Zukerman. 2011. The automated
understanding of simple bar charts.
Artificial Intelligence, 175(2):526?555.
Elzer, Stephanie, Nancy Green, Sandra
Carberry, and James Hoffman. 2006. A
model of perceptual task effort for bar
charts and its role in recognizing intention.
International Journal on User Modeling and
User-Adapted Interaction, 16(1):1?30.
Fasciano, Massimo and Guy Lapalme. 2000.
Intentions in the coordinated generation
of graphics and text from tabular data.
Knowledge and Information Systems,
2(3):310?339.
Fellbaum, Christiane. 1998.WordNet:
An Electronic Lexical Database.
The MIT Press, Cambridge, MA.
Ferres, Leo, Petro Verkhogliad, Gitte
Lindgaard, Louis Boucher, Antoine
Chretien, and Martin Lachance. 2007.
Improving accessibility to statistical
graphs: the igraph-lite system.
In Proceedings of the 9th International
ACM SIGACCESS Conference on
Computers and Accessibility, pages 67?74,
Tempe, AZ.
Foster, Mary Ellen. 1999. Automatically
generating text to accompany
information graphics. Master?s Thesis,
University of Toronto.
Friendly, Michael. 2008. A brief history of
data visualization. In C. Chen, W. Ha?rdle,
and A. Unwin, editors, Handbook of
Computational Statistics: Data Visualization,
volume III. Springer-Verlag, Heidelberg,
pages 1?34.
Gatt, Albert, Francois Portet, Ehud Reiter,
Jim Hunter, Saad Mahamood, Wendy
Moncur, and Somayajulu Sripada. 2009.
From data to text in the neonatal intensive
care unit: Using NLG technology for
decision support and information
management. AI Communications,
22(3):153?186.
Goldberg, Eli, Norbert Driedger, and
Richard I. Kittredge. 1994. Using
natural-language processing to produce
weather forecasts. IEEE Expert: Intelligent
Systems and Their Applications, 9(2):45?53.
Goldstein, Jade, Vibhu Mittal, Jaime
Carbonell, and Mark Kantrowitz. 2000.
Multi-document summarization by
sentence extraction. In Proceedings
of the NAACL-ANLP Workshop on
Automatic Summarization, pages 40?48,
Seattle, WA.
Greenbacker, Charlie, Sandra Carberry, and
Kathleen F. McCoy. 2011. A corpus of
human-written summaries of line graphs.
In Proceedings of the EMNLP 2011 Workshop
on Language Generation and Evaluation
(UCNLG+Eval), pages 23?27, Edinburgh.
Grice, H. Paul. 1975. Logic and conversation.
Speech Acts, 3:41?58.
Grosz, Barbara and Candace Sidner. 1986.
Attention, intentions, and the structure
of discourse. Computational Linguistics,
12(3):175?204.
Grosz, Barbara J., Scott Weinstein, and
Aravind K. Joshi. 1995. Centering:
A framework for modeling the local
coherence of discourse. Computational
Linguistics, 21(2):203?225.
Hovy, Eduard H. 1988. Planning coherent
multisentential text. In Proceedings of
the 26th Annual Meeting of the Association
for Computational Linguistics,
pages 163?169, Buffalo, NY.
Hovy, Eduard H. 1993. Automated discourse
generation using discourse structure
relations. Artificial Intelligence,
63(1-2):341?385.
Hovy, Eduard and Chin-Yew Lin. 1996.
Automated text summarization and the
summarist system. In Proceedings of the
Workshop on TIPSTER Text Program,
pages 197?214, Vienna, VA.
Ina, Satoshi. 1996. Computer graphics for
the blind. SIGCAPH Computers and the
Physically Handicapped, 55:16?23.
Jayant, Chandrika, Matt Renzelmann,
Dana Wen, Satria Krisnandi, Richard
Ladner, and Dan Comden. 2007.
Automated tactile graphics translation: in
the field. In Proceedings of the 9th
International ACM SIGACCESS Conference
on Computers and Accessibility, pages 75?82,
Tempe, AZ.
Johnson, Mark. 1998. Proof nets and the
complexity of processing center embedded
constructions. Journal of Logic, Language
and Information, 7(4):433?447.
Joshi, Aravind, Bonnie Webber, and
Ralph Weischedel. 1984. Living up to
expectations: Computing expert responses.
In Proceedings of the National Conference on
Artificial Intelligence, pages 169?175,
Austin, TX.
Karamanis, Nikiforos, Chris Mellish,
Massimo Poesio, and Jon Oberlander.
2009. Evaluating centering for information
572
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
ordering using corpora. Computational
Linguistics, 35(1):29?46.
Kennel, A. 1996. Audiograf: A
diagram-reader for the blind. In
Proceedings of the 2nd Annual ACM
Conference on Assistive Technologies,
pages 51?56, Vancover, BC, Canada.
Kerpedjiev, Stephan and Steven Roth.
2000. Mapping communicative goals
into conceptual tasks to generate
graphics in discourse. In Proceedings
of the International Conference on
Intelligent User Interfaces, pages 60?67,
New Orleans, LA.
Kibble, Rodger and Richard Power. 2004.
Optimizing referential coherence in text
generation. Computational Linguistics,
30(4):401?416.
Kidd, Evan and Edith Bavin. 2002.
English-speaking children?s
comprehension of relative clauses:
Evidence for general-cognitive and
language-specific constraints on
development. Journal of Psycholinguistic
Research, 31(6):599?617.
Krahmer, E. and M. Theune. 2002. Efficient
context-sensitive generation of referring
expressions. In K. van Deemter and
R. Kibble, editors, Information Sharing:
Reference and Presupposition in Language
Generation and Interpretation, Center for the
Study of Language and Information-Lecture
Notes, volume 143 of CSLI Lecture Notes.
CSLI Publications, Stanford, CA,
pages 233?264.
Krahmer, Emiel, Sebastiaan Van Erk,
and Andre? Verleg. 2003. Graph-based
generation of referring expressions.
Computational Linguistics, 29(1):53?72.
Kukich, Karen. 1983. Design of a
knowledge-based report generator.
In Proceedings of the 21st Annual Meeting of
the Association for Computational
Linguistics, pages 145?150,
Cambridge, MA.
Kurze, Martin. 1995. Giving blind people
access to graphics (example: business
graphics). In Proceedings of the
Software-Ergonomie Workshop, Dannstadt,
Bremen.
Lavoie, Benoit and Owen Rambow. 1997.
A fast and portable realizer for text
generation systems. In Proceedings of
the 5th Conference on Applied Natural
Language Processing, pages 265?268,
Washington, DC.
Lazar, J., A. Allen, J. Kleinman, and
C. Malarkey. 2007. What frustrates screen
reader users on the web: A study of
100 blind users. International Journal of
Human-Computer Interaction, 22(3):247?269.
Lester, James C. and Bruce W. Porter. 1997.
Developing and empirically evaluating
robust explanation generators: The
KNIGHT experiments. Computational
Linguistics, 23(1):65?101.
Lin, Dekang. 1996. On the structural
complexity of natural language
sentences. In Proceedings of the International
Conference on Computational Linguistics,
pages 729?733, Copenhagen, Denmark.
Mann, William C. and Sandra A. Thompson.
1987. Rhetorical structure theory: A theory
of text organization. In Livia Polanyi,
editor, The Structure of Discourse. Ablex
Publishing Corporation, Norwood, NJ.
Marcu, Daniel. 1998. The Rhetorical Parsing,
Summarization, and Generation of Natural
Language Texts. Ph.D. thesis, Department of
Computer Science, University of Toronto.
McCoy, Kathleen F., Sandra Carberry, Tom
Roper, and Nancy Green. 2001. Towards
generating textual summaries of graphs.
In Proceedings of the 1st International
Conference on Universal Access in Human-
Computer Interaction, pages 695?699,
New Orleans, LA.
McCoy, Kathleen F. and Jeannette Cheng.
1991. Focus of attention: Constraining
what can be said next. In Cecile Paris,
William Swartout, and William Mann,
editors, Natural Language Generation in
Artificial Intelligence and Computational
Linguistics. Kluwer Academic Publishers,
Berlin, pages 103?124.
McKeown, Kathleen R. 1985. Discourse
strategies for generating natural-language
text. Artificial Intelligence, 27(1):1?41.
McKeown, Kathleen R., Shimei Pan, James
Shaw, Desmond A. Jordan, and Barry A.
Allen. 1997. Language generation for
multimedia healthcare briefings. In
Proceedings of the 5th Conference on Applied
Natural Language Processing, pages
277?282, Washington, DC.
Meijer, Peter B. 1992. An experimental
system for auditory image representations.
IEEE Transactions on Biomedical Engineering,
39(2):112?121.
Mellish, Chris, Alisdair Knott, Jon
Oberlander, and Mick O?Donnell. 1998.
Experiments using stochastic search
for text planning. In Proceedings of the
9th International Workshop on Natural
Language Generation, pages 98?107,
Niagara-on-the-Lake.
Moore, Johanna D. and Cecile Paris.
1993. Planning text for advisory
573
Computational Linguistics Volume 38, Number 3
dialogues: Capturing intentional and
rhetorical information. Computational
Linguistics, 19(4):651?694.
Nenkova, Ani and Kathleen McKeown. 2003.
References to named entities: a corpus
study. In Proceedings of the Conference of the
North American Chapter of the Association for
Computational Linguistics on Human
Language Technology, pages 70?72,
Edmonton.
O?Donnell, M., C. Mellish, J. Oberlander, and
A. Knott. 2001. Ilex: an architecture for a
dynamic hypertext generation system.
Natural Language Engineering, 7(3):225?250.
Paris, Cecile. 1988. Tailoring object
descriptions to a user?s level of expertise.
Computational Linguistics, 14(3):64?78.
Pastra, Katerina, Horacio Saggion, and
Yorick Wilks. 2003. Extracting relational
facts for indexing and retrieval of
crime-scene photographs. Knowledge-Based
Systems, 16(5-6):313?320.
Portet, Francois, Ehud Reiter, Albert Gatt,
Jim Hunter, Somayajulu Sripada, Yvonne
Freer, and Cindy Sykes. 2009. Automatic
generation of textual summaries from
neonatal intensive care data. Artificial
Intelligence, 173(7-8):789?816.
Radev, Dragomir R., Hongyan Jing,
Malgorzata Stys, and Daniel Tam. 2004.
Centroid-based summarization of multiple
documents. Information Processing and
Management: An International Journal,
40(6):919?938.
Ramloll, Rameshsharma, Wai Yu, Stephen
Brewster, Beate Riedel, Mike Burton, and
Gisela Dimigen. 2000. Constructing
sonified haptic line graphs for the blind
student: First steps. In Proceedings of the
4th International ACM Conference on
Assistive Technologies, pages 17?25,
Arlington, VA.
Reiter, Ehud. 2007. An architecture for
data-to-text systems. In Proceedings of
the 11th European Workshop on Natural
Language Generation, pages 97?104,
Schloss Dagstuhl.
Reiter, Ehud and Robert Dale. 2000. Building
Natural-language Generation Systems.
Cambridge University Press, Cambridge.
Schiffman, Barry, Ani Nenkova, and
Kathleen McKeown. 2002. Experiments
in multidocument summarization.
In Proceedings of the 2nd International
Conference on Human Language Technology
Research, pages 52?58, San Diego, CA.
Shaw, James. 1998. Clause aggregation using
linguistics knowledge. In Proceedings of
the 9th International Workshop on Natural
Language Generation, pages 138?147,
Niagara-on-the-Lake.
Somayajulu, Sripada, Ehud Reiter, and Ian
Davy. 2003. Sumtime-mousam:
Configurable marine weather forecast
generator. Expert Update, 6(3):4?10.
Stent, A., Rashmi Prasad, and Marilyn
Walker. 2004. Trainable sentence
planning for complex information
presentation in spoken dialog systems.
In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics,
pages 79?86, Barcelona.
Stone, Matthew and Bonnie Webber. 1998.
Textual economy through closely coupled
syntax and semantics. In Proceedings
of the International Natural Language
Generation Conference, pages 178?187,
Niagara-on-the-Lake.
Suri, Linda Z. and Kathleen F. McCoy. 1994.
Raft/rapr and centering: A comparison
and discussion of problems related to
processing complex sentences.
Computational Linguistics, 20(2):301?317.
Turner, Ross, Yaji Sripada, and Ehud Reiter.
2009. Generating approximate geographic
descriptions. In Proceedings of the 12th
European Workshop on Natural Language
Generation, pages 42?49, Athens.
Walker, M., O. Rambow, and M. Rogati. 2002.
Training a sentence planner for spoken
dialogue using boosting. Computer Speech
and Language: Special Issue on Spoken
Language Generation, 16(3):409?434.
Walker, M., A. Stent, F. Mairesse, and
R. Prasad. 2007. Individual and domain
adaptation in sentence planning for
dialogue. Journal of Artificial Intelligence
Research, 30(1):413?456.
Wu, Peng, Sandra Carberry, Stephanie Elzer,
and Daniel Chester. 2010. Recognizing
the intended message of line graphs.
In Proceedings of the International Conference
on the Theory and Application of Diagrams,
pages 220?234, Portland, OR.
Yngve, Victor H. 1960. A model and an
hypothesis for language structure.
American Philosophical Society, 104:444?466.
Yu, Jin, Ehud Reiter, Jim Hunter, and
Chris Mellish. 2007. Choosing the
content of textual summaries of large
time-series data sets. Natural Engineering,
13(1):25?49.
574
Generating Textual Summaries of Bar Charts
Seniz Demir Sandra Carberry Kathleen F. McCoy
Department of Computer Science
University of Delaware
Newark, DE 19716
{demir, carberry, mccoy}@cis.udel.edu
Abstract
Information graphics, such as bar charts and
line graphs, play an important role in mul-
timodal documents. This paper presents a
novel approach to producing a brief textual
summary of a simple bar chart. It outlines
our approach to augmenting the core message
of the graphic to produce a brief summary.
Our method simultaneously constructs both
the discourse and sentence structures of the
textual summary using a bottom-up approach.
The result is then realized in natural language.
An evaluation study validates our generation
methodology.
1 Introduction
Information graphics, such as bar charts and line
graphs, are an important component of a multi-
modal document. However, summarization has fo-
cused primarily on the text of a document. But as
shown in (Carberry et al, 2006), information graph-
ics in magazine and newspaper articles often convey
a message that is not repeated in the article?s text.
Thus information graphics cannot be ignored.
Individuals with sight impairments can access the
text of an electronic document via text to speech
systems. But such individuals are stymied when
they encounter information graphics. The SIGHT
system (Elzer et al, 2007) has the goal of provid-
ing the user with the message and knowledge that
one would gain from viewing the graphic, rather
than providing alternative access to what the graphic
looks like. In the current Bayesian network imple-
mentation, the system uses the communicative sig-
nals present in the graphic to recognize one of the
twelve message categories that can be conveyed by
a bar chart and produces a logical representation
of what we will refer to as the core message con-
veyed by the graphic; this representation is trans-
lated into natural language via templates. For exam-
ple, the system determines that the core message of
the graphic in Figure 1 is that the bar for the United
States has the maximum value of the entities listed,
and the system produces Maximum(First Bar) as the
logical representation of that message. However,
this is insufficient as a summary of the graphic since
it doesn?t convey the particularly significant features
of the graphic such as the fact that the number of
hacker attacks in the United States is far greater than
in the other countries listed.
In this paper, we explore the generation of an
effective initial summary of a bar chart within the
SIGHT system. Input to our system is a logical rep-
resentation of the graphic?s core message (as pro-
duced by SIGHT) and the XML representation of
the graphic which specifies the components of the
graphic such as the number of bars and the heights
of each bar. Our goal is to generate a succinct coher-
ent summary of a graphic that captures its core mes-
sage and the most important and significant features.
At the same time, the summary need not include all
information that could be extracted from the graphic
since future work on SIGHT includes a mechanism
for responding to follow-up questions from the user.
Our work is unique in that it generates a sum-
mary of the content of a bar chart with no domain
restriction and constructs a high-quality but brief
summary by incorporating the graphic?s core mes-
7
C o u n t r i e s  W i t h  t h e  M o s t  H a c k e r  
A t t a c k s ,  2 0 0 2
W o r l d w i d e ,  t h e  a t t a c k s  j u m p e d
U n i t e d  S t a t e s 2 4 , 4 3 4
B r a z i l
B r i t a i n
G e r m a n y
I ta ly
0 5 , 0 0 0 1 0 , 0 0 0 1 5 , 0 0 0 2 0 , 0 0 0
Figure 1: Graphic conveying a maximum bar.
sage and the most important and significant features
of the graphic. Our system produces a coherent or-
ganization of the content of the summary that we
hypothesize lends itself to follow-up questions, ap-
plies the notion of syntactic complexity in choosing
how to aggregate information into sentence-sized
pieces, utilizes a sentence realizer to convey the out-
put in natural language, and applies heuristics for
constructing referents for graphical elements. Thus,
our work addresses several generation problems.
Section 2 discusses related work in the area
of graph summarization. Section 3 presents our
methodology for identifying the content of initial
summaries. Section 4 describes our approach for or-
ganizing the summaries. Section 5 presents some
issues that we addressed in realizing the summaries
and a few example summaries generated by our sys-
tem. Section 6 discusses the results of an evaluation
study that validates our methodology.
2 Related Work
Graph summarization has received some attention.
The SumTime project uses pattern recognition tech-
niques to generate textual summaries of automati-
cally generated time-series data; different systems
have been designed in three domains, such as
SumTime-Turbine (Yu et al, 2007) for data from
gas turbine engines. However, each of these sys-
tems is domain-dependent. We view this as a very
different problem from the one that we address in
this paper, since we are working on domain indepen-
dent graphical representations. The iGRAPH-Lite
system (Ferres et al, 2007), whose main objective
is to make the information in a graphic accessible
to blind users via keyboard commands, uses tem-
plates to provide a short textual summary of what
the graphic looks like, but their summary is not con-
cerned with the higher level knowledge conveyed by
the graphic. The goal of Futrelle?s project (Futrelle,
1999) is to produce a summary graphic that captures
the content of one or more graphics. However, the
end result is itself a graphic, not text.
3 Informational Content of the Summaries
In (McCoy et al, 2001), we reported an informal
experiment in which human subjects were asked to
write a brief summary of a series of line graphs with
the same high-level intention. This experiment led
to three observations:
? the intended message of the graphic was con-
tained in all summaries.
? summaries of the same graphic by different
subjects were similar.
? summaries of different graphics in the same
message category (such as Rising Trend) var-
ied in the information provided.
Subjects included more than the core message of
the graphic in their summaries. The extra infor-
mation could be explained as capturing features of
the graphic that were visually salient. It was hy-
pothesized that what is taken as visually salient in a
graphic relates to the overall message of the graphic.
For example, in the line graphs analyzed, if the over-
all message of the graphic is an increasing trend and
the variance in that trend is large, then the variance
is salient. The fact that the summaries only included
the core message and the visually salient features,
correlates with Grice?s Maxim of Quantity (Grice,
1975) which states that one?s discourse contribution
should be as informative as necessary for the pur-
poses of the current exchange but not more so.
To extend these observations to other kinds of in-
formation graphics, particulary to simple bar charts,
we needed to identify the kinds of features that were
salient with respect to the graphic?s overall message.
For this purpose, we conducted a set of formal ex-
periments with human subjects. Our goal was a set
of rules that took into account the message category
(such as Rising Trend or Rank of Entity) and the vi-
sual features of the graphic to specify what proposi-
tions should be included in the initial summary.
8
3.1 Experimental Setup
Based on our experience with summarizing graph-
ics, we first identified the set of all propositions
(PROP ALL) that reflect information that we envi-
sioned someone might ask about a simple bar chart.
Twenty graduate students from a variety of depart-
ments participated in the experiments. Each subject
was given an information graphic along with a sen-
tence conveying the intended message of the graphic
(as identified by the SIGHT system) and a subset of
PROP ALL, and was asked to classify these addi-
tional propositions into one of three classes accord-
ing to how important they felt it was to include that
proposition in the initial summary: essential, pos-
sible, and not important. Subjects were told to
assume that the graphic was part of an article that
the user is reading and that he would be able to ask
follow-up questions after receiving the summary.
Twenty-one graphics were used in the experi-
ments and each participant was given six graphics
to view. The graphics were either selected from arti-
cles in popular media or constructed from scratch to
present a number of different salient features within
the same graphic. Eight different message categories
were tested in the experiments1 and the graphics
from the same message category reflected different
salient features that had been observed in a corpus
of collected graphics.
3.2 Computationalizing Proposition Selection
To analyze the experiment?s results, we assigned a
numeric score to each category (essential=3, possi-
ble=1, not important=0) indicating the level of im-
portance assigned by the subjects. We computed
an importance level (IL) for each proposition that
might be included in a summary of a graphic, esti-
mating how important it is to be included in the ini-
tial summary. The IL of a proposition is computed
based on the average of the importance assigned by
each subject. We classify a proposition as a highly-
rated proposition in a graphic (and therefore worthy
of being included in the initial summary) if its im-
portance level is equal to or above the importance
level that would be obtained if at least half of the
1We did not test message categories that are the opposite of
categories used in the experiments, such as Minimum which is
the opposite of Maximum and thus can be modeled similarly.
subjects had classified the proposition as essential.
The particular assignments of values to categories
shown above was used in order to emphasize in-
stances in which subjects placed a proposition in the
essential category.
For each message category, we analyzed the sets
of highly-rated propositions identified for the graph-
ics associated with that message category and hand
constructed a set of content identification rules spec-
ifying whether a proposition should be included in
the initial summary. If a proposition was classified
as highly-rated for all graphics in a particular mes-
sage category, the content identification rule stated
that the proposition should be included in the ini-
tial summary for every graphic whose core message
fell into that message category. For the other highly-
rated propositions for a message category, we iden-
tified a feature that was visually salient only in the
graphics where the proposition was marked as es-
sential, and our content identification rule used the
presence of this feature in the graphic as a condi-
tion for the proposition to be included in the initial
summary. Thirty content identification rules were
defined in the system.
Besides the propositions capturing salient features
of the graphic, we observed that the subjects se-
lected propositions whose absence might lead the
user to draw false conclusions by default (for exam-
ple, the propositions indicating that the trend start-
ing from 1997 does not cover the full range of bar
labels in the graphic in Figure 2). We constructed
rules to add such content. This correlates with the
maxim in (Joshi et al, 1984) which states that a sys-
tem should not only produce correct information but
should also prevent the user from drawing false in-
ferences.
The following are glosses of two representative
content identification rules applicable to a graphic
whose core message is an increasing trend:
? If (message category equals ?increas-
ing trend?) then include(propositions con-
veying the rate of increase of the trend)
? If (message category equals ?increas-
ing trend?) and (coverage(graphic) not equal
coverage(trend)) then include(propositions
indicating that the trend does not cover the full
range of bar labels)
To see how our rules might affect the generated
9
summary, consider the graphic in Figure 2. The
SIGHT system recognizes the core message as an
increasing trend from 1997 to 2002. The content
identification rules defined for increasing trend se-
lect the following pieces of additional information
to include in the initial summary of the graphic:
? The overall rate of increase of the trend, which
is moderate
? The range of the bar values in the trend:
$480,000 to $1,230,000
? The fact that there is an unusually steep rise be-
tween 2000 and 2001
? The period that the graphic covers, which is
from 1996 to 2002
4 Organizing Coherent Summaries
At this point in the processing, the system has iden-
tified the informational content to be conveyed in the
initial summary, which consists of the core message
and the set of propositions identified by the content
identification rules. We need to represent and orga-
nize the information to be communicated, determine
how the content should be aggregated into sentence-
sized pieces, and make decisions about referring ex-
pressions (Reiter and Dale, 1997).
To represent and help organize the propo-
sitions that should be included in the ini-
tial summary, we use two kinds of predicates.
Relative predicates, such as differs, are used to
express relations between the graphical elements.
For example, differs(bar(A),maximum bar,50%)
shows that the percentage difference between the
values of bar(A) and the maximum bar is 50%.
Attributive predicates, such as has attribute, are
used to elaborate the graphical elements. For ex-
ample, has attribute(trend,?type?,?increasing?)
shows that the trend observed in the graphic is an
increasing trend. We refer to the first argument of
a predicate as its main entity and the others as sec-
ondary entities.
Since a top-down planning approach does not
guarantee that all propositions will be covered by
the final text plan (Marcu, 1997), we use a bottom-
up planner for structuring the initial summaries. The
propositions selected for inclusion in the initial sum-
maries can be classified as message related, spe-
cific, or computational based on the type of in-
1 9 9 6 2 0 0 12 0 0 01 9 9 81 9 9 7 1 9 9 9 2 0 0 2
4 0 0
0
8 0 0
$ 1 , 2 0 0 , 0 0 0
R i s i n g  J u r y  A w a r d s
M e a n
4 8 0 , 0 0 0
1 , 2 3 0 , 0 0 0
Figure 2: Graphic with an increasing trend.
formation they convey. The core message of the
graphic is captured by the message related proposi-
tions. Specific propositions focus on specific pieces
of information in the graphic such as the propo-
sitions conveying the unusually large rise between
2000 and 2001 in the graphic in Figure 2. On the
other hand, computational propositions require com-
putations or abstractions over the whole graphic,
such as the propositions conveying the rate of in-
crease in the graphic in Figure 2. Once the propo-
sitions to be included in the summary are identified,
we assign them to one of these three classes.
We hypothesize that the message-related class
of propositions should be presented first since this
places emphasis on the core message of the graphic.
We anticipate that the user will ask follow-up ques-
tions after receiving the initial summary. Therefore,
it is appropriate to close the initial summary with
propositions from the computational class so that
the whole graphic is in the user?s focus of atten-
tion (Grosz and Sidner, 1986). Thus we hypothe-
size that a good ordering of propositions in the ini-
tial summary is the message-related class, the spe-
cific class, and finally the computational class. This
produces a partial ordering of the propositions to be
included in the summary.
Each proposition can be realized as a single sen-
tence. For example, shows(graphic,trend) can be
realized as ?The graphic shows a trend? or ?There
is a trend in the graphic?. Consequently, a set of
propositions can be viewed as a set of single sen-
tences. Figure 3 shows the propositions in the mes-
sage related class for the graphic in Figure 2, along
with a possible realization for each.
Although each proposition could be conveyed as
a single sentence, the result is unnatural and not very
10
shows(graphic,trend)
exists(trend,descriptor for dependent axis)
has_attr(trend,type,increasing)
has(trend,period)
starts(period,at,1997)
ends(period,at,2002)
The graphic shows a trend
There is a trend in the mean dollar
value of jury awards
The trend is an increasing trend
There is a trend over the period
The period starts at 1997
The period ends at 2002
Figure 3: Propositions in Message related Class.
coherent. Thus, we define operators to relate propo-
sitions and explore aggregating them. The opera-
tors work on trees; initially, each proposition is the
root of a single node tree. Each tree represents one
or more propositions that can be realized as a sin-
gle sentence, and operators combine individual trees
in a class into more complex trees. The following
are two such operators which work on relative pred-
icates:
? And Operator: This operator combines two
trees if their root propositions share the same
main entity. An And predicate with the same
main entity forms the root of the new tree, and
the trees that are combined form the descen-
dents of this root.
? Which Operator: This operator attaches one
tree as a descendent of another tree, connected
by a Which predicate, if the main entity of the
proposition at the root of the first tree is a sec-
ondary entity in the proposition at the root of
the second tree. That particular entity forms the
main entity of the Which predicate.
One possible result of applying our operators to
the set of propositions (single node trees) in Figure 3
produces the single but more complex tree shown
in Figure 4. The And predicate (***) in Figure 4
is produced by the And Operator and the resultant
subtree is attached to the predicate shows by the
Which Operator.
4.1 Evaluating Structures
Different combinations of operators produce differ-
ent sets of trees, each of which represents a differ-
ent text structure and consequently leads to differ-
shows(graphic,trend)
exists(trend,descriptor for dependent axis)
has_attr(trend,type,increasing)
has(trend,period)
starts(period,at,1997) ends(period,at,2002)
Which(trend)
And(trend)
Which(period)
And(period)
*
**
***
Figure 4: Best Structure of Message related Class.
ent realized text. These structures must be evalu-
ated to determine which one is the best. We don?t
want a structure where each proposition is realized
as a single sentence nor a structure where groups of
propositions are realized with sentences that are too
complex. Our objective is to find a structure which
stands at a mediatory point between these extremes.
Each tree in a structure represents a set of propo-
sitions that can be realized as a single sentence. The
most straightforward way of realizing a tree would
be conjoining the realizations of subtrees rooted by
an And predicate, embedding the realization of a
subtree rooted by a Which predicate as a relative
clause, and realizing a subtree that consists solely
of an attributive predicate as an adjective or a prepo-
sitional phrase. However, care must be taken that the
sentence realization of a tree is not too complex.
Research has used a number of different measures
to assess syntactic complexity of written text and
spoken language samples (Roark et al, 2007). We
apply the notion of syntactic complexity to evalu-
ate the semantic units (predicates) that will be re-
alized. The revised D-level sentence complexity
scale (Covington et al, 2006) forms the core of our
syntactic complexity measure. The D-Level scale
measures the complexity of a sentence according to
the sequence in which children acquire the ability
to use different types of sentences. The sentence
types with the lowest score are those that children
acquire first and therefore are the simplest types.
Among the seven levels defined in the revised D-
Level scale, the levels of interest in our work are
(in order of increasing complexity): simple sen-
tences, conjoined sentences, sentences with a rela-
11
Message Related:
The graphic shows an increasing trend in the mean dol-
lar value of jury awards over the period from 1997 to
2002.
Specific:
The value of these awards ranges from 480,000 dol-
lars to 1,230,000 dollars. The graphic covers a period
from 1996 to 2002. Between 2000 and 2001, an unusu-
ally steep rise is observed in the value of these awards.
Computational:
Moderate increases are observed every year during the
period from 1997 to 2002 in the value of these awards.
Table 1: Realization of the Initial Overall Structure.
Message Related:
Although the graphic covers a period from 1996 to 2002,
the graphic shows an increasing trend in the mean dollar
value of jury awards over the period from 1997 to 2002.
Specific:
The value of these awards ranges from 480,000 dollars to
1,230,000 dollars.
Computational:
Except for a steep rise between 2000 and 2001, moderate
increases are observed every year in the value of these
awards.
Table 2: Realization of the Final Overall Structure.
tive clause, and sentences with more than one level
of embedding. However, the definition of sentence
types at each level is too general. Therefore, we
use a finer distinction between sentence types within
each D-level, such as a simple sentence with more
than one preposition has a higher complexity than a
simple sentence with a single preposition.
To measure the complexity of sentences that will
realize a structure, we define a number of complex-
ity estimators. A tree consisting of a single node is
identified as having the lowest syntactic complex-
ity. We use And predicate and Which predicate es-
timators to estimate the complexity of the sentences
used to realize more complex trees. To do this, esti-
mators look for realization opportunities that would
produce lower complexity values than what the most
straightforward realization would produce. For in-
stance, the And predicate estimators check whether
or not the realizations of two subtrees rooted by
an And predicate can be combined into a single
sentence which is not a compound sentence con-
sisting of two independent sentences. These es-
timators have similar considerations to the clause-
combining operations used by Walker et.al (2002)
in the SPoT sentence planner. For example, one of
the And predicate estimators can successfully deter-
mine that the realizations ?There is a trend in the
mean dollar value of jury awards? and ?There is a
trend over the period from 1997 to 2002?2 can be
combined into ?There is a trend in the mean dollar
value of jury awards over the period from 1997 to
2Assume that the subtree in Figure 4 rooted by
Which(period) can be realized as ?from 1997 to 2002?.
2002? for the tree in Figure 4. This results in a com-
plexity score which is lower than the score of a com-
pound sentence consisting of two conjoined inde-
pendent sentences. The Which predicate estimators
check whether a tree rooted by a Which predicate
can be realized as an adjunct to the modified entity
rather than as a more complex relative clause. For
example, one of the Which predicate estimators can
identify that * and the subtree rooted at ** in Fig-
ure 4 can be realized as ?The graphic shows a trend
in the mean dollar value of jury awards over the pe-
riod from 1997 to 2002?.
Center-embedded relative clauses are more dif-
ficult to comprehend than corresponding right-
branching clauses (Kidd and Bavin, 2002). Our
complexity metric for evaluating the complexity of a
tree penalizes Which predicates that will be realized
as a relative clause in the middle of a sentence more
than one that appears at the end. Once the complex-
ity metric has evaluated the complexity of each tree
in a candidate structure, the complexities of the trees
are summed to get the complexity of the structure.
Our metric for selecting the best structure balances
the number of sentences and their complexity.
The initial overall structure of the summary con-
sists of the best structures for the message related,
specific, and computational classes. As a final step,
we check whether we can improve the evaluation of
the overall structure of the summary by moving trees
or subtrees between the best structures for the three
classes. For example, the best structure for the spe-
cific class might contain a tree that conveys infor-
mation about an entity introduced by a proposition
12
in the message related class. Moving this tree to the
message-related class and using the Which opera-
tor to combine it with the tree introducing the entity
(thereby realizing it as a relative clause) might im-
prove the evaluation of the overall structure of the
summary. To be consistent with the motivation be-
hind the initial groupings of the propositions, we do
not allow movements from the message related class
or any movement that will empty the computational
class. Table 1 presents the summary that our sys-
tem generates for the graphic in Figure 2 before the
movements between classes, and Table 2 presents
the summary after the movements.
5 Realizing Summaries
To realize the summaries in natural language, we
use the FUF/SURGE surface realizer (Elhadad and
Robin, 1996) with some changes made to address a
few problems encountered with respect to the use of
conjunctions and subject-ellipsises. Different strate-
gies are defined in the system for aggregating the
realizations of trees that are linked with operators.
The strategy selected by the system is based on the
relation (such as concession) that holds between the
propositions at the root of the trees and the syntac-
tic forms of their realization opportunities. For ex-
ample, the system uses different strategies for ag-
gregating the trees rooted by the And predicates in
Figure 4, where the tree rooted by And(period) is re-
alized as a combination of prepositional phrases and
the tree rooted by And(trend) is realized as a full
sentence containing a set of prepositional phrases.
5.1 A Descriptor for the Dependent Axis
The dependent axis of an information graphic is of-
ten not labelled with a full descriptor of what is be-
ing measured and therefore a mechanism for extract-
ing an appropriate descriptor had to be developed.
We undertook a corpus analysis and implemented a
system to realize the descriptors (Demir et al, 2007).
Our corpus analysis found that the full descriptor of-
ten must be built from pieces of text extracted from
different places in the graphic. We identified seven
text levels (text components) which form a hierar-
chy (the top being the Overall Caption and the bot-
tom being the Dependent Axis Label), and we ob-
served that the texts at the lower levels are more
? 95 ?00?99?97?96 ?98 ?01 ?02
6
4
2
0
1 0  p e r c e n t
8
A n n u a l  p e r c e n t  c h a n g e  i n  
g l o b a l  o u t p u t
Figure 5: Graphic conveying a contrast change.
likely to contribute to the descriptor than the texts
at the higher levels. We developed a set of heuristics
and augmentation rules for constructing the descrip-
tor for the dependent axis and validated them on a
previously unseen corpus of graphics.
5.2 Referent Generation
The descriptor that our system constructs is always a
noun phrase, but it may be quite long. Our referring
expression generator uses the full descriptor when
the dependent axis is first referenced in the text, but
only the head noun for subsequent references. To
relate the head noun to the descriptor in the text, the
demonstratives ?this? or ?these? is added to the front
of the head noun unless it follows a comparison such
as ?more?. This simple mechanism appears to work
well for our initial summaries.
5.3 Example Summaries
For the graphic in Figure 1, the SIGHT system posits
that the graphic?s core message is that the bar for
the United States has the maximum value among the
bars listed. Our system adds additional propositions
and produces the following summary:
?The graphic shows that United States with 24,434
has the highest number of hacker attacks among
the countries3 Brazil, Britain, Germany, Italy, and
United States. United States has 4.9 times more at-
tacks than the average of other countries.?
For the graphic in Figure 5, the SIGHT system
posits that the core message is that the change at the
last bar is in contrast with the previous increasing
trend. Our system generates the following summary:
3Our system has a module which identifies the ontological
category of the bar labels (Demir et al, 2007).
13
?The graphic shows a small drop in 2002 in contrast
with the increasing trend in annual percent change
in global output over the period from 1995 to 2001.
Except for a small drop in 1996, varying increases
are observed every year during the period from 1995
to 2001 in this change, which shows an overall in-
crease of 485.4 percent and shows a decrease of 8.7
percent between 2001 and 2002.?
6 Evaluation
The work presented in this paper consists of sev-
eral different components, all of which we claim
contribute to the quality of the graphical summary.
Our evaluation focused on three of these and eval-
uated whether or not our decisions with respect to
these components contributed to the perceived qual-
ity of the summary: the organization and ordering of
the informational content (O), the aggregation of the
information into more complex structures (A), and
the metric used to evaluate the structures that repre-
sent different possible aggregations of the informa-
tional content (E). We conducted an experiment with
fifteen participants where they were presented with
four different summaries of twelve graphics from a
variety of domains. We focused on increasing and
decreasing trend graphics since they have the great-
est variety of possible summaries. For each of the
graphics, the participants were asked to rank ran-
domly ordered summaries in terms of their quality
in conveying the informational content. The sum-
maries varied according to the test parameters:
? S O+A+E+: A summary that uses the ordering
rules, the aggregation rules, and is rated highest
by the evaluation metric. This is the summary
produced by our system.
? S O+A+E-: A summary that uses the ordering
and aggregation rules, but was not rated highest
by the evaluation metric.
? S O-A+E+: A summary where the proposi-
tions are randomly ordered, but aggregation
takes place, and the aggregation is rated best
by the evaluation metric.
? S O-A-E-: A summary consisting of single
sentences that are randomly ordered.
The results of the experiment are presented in Ta-
ble 3. It is particularly noteworthy that the summary
generated by our system was most often (65.6% of
Summary type Best 2nd 3rd 4th
S O+A+E+ 65.6% 26.6% 6.7% 1.1%
S O+A+E- 16.7% 32.2% 33.3% 17.8%
S O-A+E+ 16.7% 30% 40% 13.3%
S O-A-E- 1% 11.2% 20% 67.8%
Table 3: Ranking of Summary Types.
the time) rated as the best summary and overwhelm-
ingly (92.2% of the time) rated as one of the top two
summaries. The table shows that omitting the eval-
uation metric (S O+A+E-) or omitting ordering of
propositions (S O-A+E+) results in summaries that
are substantially less preferred by the subjects. We
gained insights into improving the system?s perfor-
mance by looking at the comments made by the sub-
jects when our summary was not selected as the best.
For example, it appears that the subjects did not like
summaries where exceptions were fronted on the
core message with an ?although? clause rather than
following the core message (S O+A+E-). We hy-
pothesize that fronting the exception detracted from
the core message. This is easily remedied in our
evaluation metric. Overall, the results shown in Ta-
ble 3 support our methodology for generating sum-
maries. In the future, we will test the summaries
with blind individuals to determine their effective-
ness in providing alternative access to graphics.
7 Conclusion
This paper has presented our methodology for gen-
erating brief and coherent summaries of simple bar
charts. Our work is the first to address the prob-
lem of summarizing the content of bar charts. We
have presented our approach for identifying the ap-
propriate content of an initial summary, ordering and
aggregating the included propositions, and evaluat-
ing the resultant summary structures to select the
best one. Overall, the results of an evaluation study
validate our ordering, aggregation, and evaluation
methodology.
Acknowledgements
We would like to thank Dr. Stephanie Elzer for her
advice, help, and implementation of the SIGHT sys-
tem upon which this work is built and Dr. Charles
Callaway for his valuable help in addressing the
problems encountered with the realizer.
14
References
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proc. of SIGIR?2006.
Michael A. Covington, Congzhou He, Cati Brown, Lo-
rina Naci, and John Brown 2006. How complex is that
sentence? A proposed revision of the Rosenberg and
Abbeduto D-Level scale. Research Report, AI Center,
University of Georgia.
Seniz Demir, Sandra Carberry, and Stephanie Elzer.
2007. Effectively realizing the inferred message of an
information graphic. In Proc. of RANLP?2007.
Michael Elhadad and Jacques Robin. 1996. An overview
of SURGE: a re-usable comprehensive syntactic real-
ization component. In Proc. of INLG?1996.
Stephanie Elzer, Edward Schwartz, Sandra Carberry,
Daniel Chester, Seniz Demir, and Peng Wu. 2007.
A browser extension for providing visually impaired
users access to the content of bar charts on the web. In
Proc. of WEBIST?2007.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proc. of ASSETS?2007.
Robert Futrelle. 1999. Summarization of diagrams in
documents. In Advances in Automated Text Summa-
rization. MIT Press, pp. 403-421.
Paul Grice. 1975. Logic and conversation. In Syntax and
Semantics, Speech Acts, 3:41-58.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. In Compu-
tational Linguistics, 12(3):175?204.
Aravind Joshi, Bonnie Webber, and Ralph Weischedel.
1984. Living up to expectations: computing expert
responses. In Proc. of NCAI?1984.
Evan Kidd and Edith Bavin. 2002. English-speaking
childrens comprehension of relative clauses: evidence
for general-cognitive and language-specific constraints
on development. In Journal of Psycholinguistic Re-
search, 31(6):599-617.
Daniel Marcu. 1997. The rhetorical parsing, summariza-
tion, and generation of natural language texts. PhD
thesis, Department of Computer Science, University of
Toronto.
Kathleen F. McCoy, Sandra Carberry, Tom Roper, and
Nancy Green. 2001. Towards generating textual sum-
maries of graphs. In Proc. of HCI?2001.
Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. In Natural Lan-
guage Engineering, 3(1):57?87.
Brian Roark, Margaret Mitchell, and Kristy Holling-
shead. 2007. Syntactic complexity measures for
detecting mild cognitive impairment. In Proc. of
BioNLP?2007.
Marilyn Walker, Owen Rambow, and Monica Rogati.
2002. Training a sentence planner for spoken dialogue
using boosting. In Computer Speech and Language:
Special Issue on Spoken Language Generation, 16(3-
4):409-433.
Jin Yu, Ehud Reiter, Jim Hunter, and Chris Mellish.
2007. Choosing the content of textual summaries
of large time-series data sets. In Natural Language
Engineering,13(1):25?49.
15
A Discourse-Aware Graph-Based Content-Selection Framework
Seniz Demir Sandra Carberry Kathleen F. McCoy
Department of Computer Science
University of Delaware
Newark, DE 19716
{demir,carberry,mccoy}@cis.udel.edu
Abstract
This paper presents an easy-to-adapt,
discourse-aware framework that can be
utilized as the content selection compo-
nent of a generation system whose goal is
to deliver descriptive texts in several turns.
Our framework involves a novel use of a
graph-based ranking algorithm, to itera-
tively determine what content to convey to
a given request while taking into account
various considerations such as capturing a
priori importance of information, convey-
ing related information, avoiding redun-
dancy, and incorporating the effects of dis-
course history. We illustrate and evaluate
this framework in an accessibility system
for sight-impaired individuals.
1 Introduction
Content selection is the task responsible for deter-
mining what to convey in the output of a gener-
ation system at the current exchange (Reiter and
Dale, 1997). This very domain dependent task
is extremely important from the perspective of
users (Sripada et al, 2001) who have been ob-
served to be tolerant of realization problems as
long as the appropriate content is expressed. The
NLG community has proposed various content
selection approaches since early systems (Moore
and Paris, 1993; McKeown, 1985) which placed
emphasis on text structure and adapted planning
techniques or schemas to meet discourse goals.
This paper proposes a domain-independent
framework which can be incorporated as a content
selection component in a system whose goal is to
deliver descriptive or explanatory texts, such as the
ILEX (O?Donnell et al, 2001), KNIGHT (Lester
and Porter, 1997), and POLIBOX (Chiarcos and
Stede, 2004) systems. At the core of our frame-
work lies a novel use of a graph-based ranking al-
gorithm, which exploits discourse related consid-
erations in determining what content to convey in
response to a request for information. This frame-
work provides the ability to generate successive
history-aware texts and the flexibility to generate
different texts with different parameter settings.
One discourse consideration is the tenet that the
propositions selected for inclusion in a text should
be in some way related to one another. Thus,
the selection process should be influenced by the
relevance of information to what has already been
selected for inclusion. Moreover, we argue that
if the information given in a proposition can be
deduced from the information provided by any
other proposition in the text, this would introduce
redundancy and should be avoided.
Many systems (such as MATCH (Walker et al,
2004) and GEA (Carenini and Moore, 2006)) con-
tain a user model which is employed to adapt con-
tent selection to the user?s preferences (Reiter and
Dale, 1997). Our framework provides a facility
to model a stereotypical user by incorporating the
a priori importance of propositions. This facility
can also be used to capture the preferences of a
particular user.
In a dialogue system, utterances that are gen-
erated without exploiting the previous discourse
seem awkward and unnatural (Moore, 1993). Our
framework takes the previous discourse into ac-
count so as to omit recently communicated propo-
sitions and to determine when repetition of a pre-
viously communicated proposition is appropriate.
To our knowledge, our work is the first effort
utilizing a graph-based ranking algorithm for con-
tent selection, while taking into account what in-
formation preferably should and shouldn?t be con-
veyed together, the a priori importance of infor-
mation, and the discourse history. Our framework
is a domain-independent methodology containing
domain-dependent features that must be instanti-
ated when applying the methodology to a domain.
Section 2 describes our domain-independent
methodology for determining the content of a re-
sponse. Section 3 illustrates its application in an
accessibility system for sight-impaired individuals
and shows the generation flexibility provided by
this framework. Finally, Section 4 discusses the
results of user studies conducted to evaluate the
effectiveness of our methodology.
2 A Graph-based Content Selection
Framework
Our domain-independent framework can be ap-
plied to any domain where there is a set of proposi-
tions that might be conveyed and where a bottom-
up strategy for content selection is appropriate. It
is particularly useful when the set of propositions
should be delivered a little at a time. For exam-
ple, the ILEX system (O?Donnell et al, 2001) uses
multiple descriptions to convey the available infor-
mation about a museum artifact, since the length
of the text that can be displayed on a page is lim-
ited. In order to use our framework, an application
developer should identify the set of propositions
that might be conveyed in the domain, specify the
relations between these propositions, and option-
ally assess a priori importance of the propositions.
Our framework uses a weighted undirected
graph (relation graph), where the propositions
are captured as vertices of the graph and the
edges represent relations between these proposi-
tions. While the number and kinds of relations
represented is up to the developer, the frame-
work does require the use of one specific rela-
tion (Redundancy Relation) that is generalizable
to any descriptive domain. Redundancy Relation
must be specified between two propositions if they
provide similar kinds of information or the infor-
mation provided by one of the propositions can
be deduced from the information provided by the
other. For example, consider applying the frame-
work to the ILEX domain. Since the proposition
that ?this jewelry is produced by a single crafts-
man? can be deduced from the proposition that
?this jewelry is made by a British designer?, these
propositions should be connected with a Redun-
dancy Relation in the relation graph.
There is at most one edge between any two ver-
tices and the weight of that edge represents how
important it is to convey the corresponding propo-
sitions in the same text (which we refer to as
the strength of the relation between these proposi-
tions). For example, suppose that once a museum
artifact is introduced in ILEX, it is more impor-
tant to convey its design style in the same descrip-
tion as opposed to where it is produced. In this
case, the weight of the edge between the proposi-
tions introducing the artifact and its style should
be higher than the weight of the edge between the
propositions introducing the artifact and its pro-
duction place.
The framework incorporates a stereotyp-
ical user model via an additional vertex
(priority vertex) in the relation graph. The
priority vertex is connected to all other vertices
in the graph. The weight of the edge between
a vertex and the priority vertex represents the a
priori importance of that vertex, which in turn
specifies the importance of the corresponding
proposition. For example, suppose that in the
ILEX domain an artifact has two features that
are connected to the proposition introducing the
artifact by the ?feature-of? relation. The a priori
importance of one of these features over the
other can be specified by giving a higher weight
to the edge connecting this proposition to the
priority vertex than is given to the edge between
the other feature and the priority vertex. This
captures a priori importance and makes it more
likely that the important feature will be included
in the artifact?s description.
2.1 Our Ranking Algorithm
With this graph-based setting, the most important
thing to say is the proposition which is most cen-
tral. Several centrality algorithms have been pro-
posed in the literature (Freeman, 1979; Navigli
and Lapata, 2007) for calculating the importance
scores of vertices in a graph. The well-known
PageRank centrality (Brin and Page, 1998) calcu-
lates the importance of a vertex by taking into ac-
count the importance of all other vertices and the
relation of vertices to one another. This metric has
been applied to various tasks such as word sense
disambiguation (Sinha and Mihalcea, 2007) and
text summarization (Erkan and Radev, 2004). We
adopted the weighted PageRank metric (Sinha and
Mihalcea, 2007) for our framework and therefore
compute the importance score of a vertex (Vx) as:
PR(V x) = (1? d) + d ?
?
(V x,V y)?E
wyx
?
wyz
(V z ,V y)?E
PR(V y)
where wxy is the weight associated with the edge
between vertices (Vx) and (Vy), E is the set of all
edges, and d is the damping factor, set to 0.85,
which is its usual setting.
Once the propositions in a domain are captured
in a relation graph with weights assigned to the
edges between them, the straightforward way of
identifying the propositions to be conveyed in the
generated text would be to calculate the impor-
tance of each vertex via the formula above and
then select the k vertices with the highest scores.
However, this straightforward application would
fail to address the discourse issues cited earlier.
Thus we select propositions incrementally, where
with each proposition selected, weights in the
graph are adjusted causing related propositions to
be highlighted and redundant information to be re-
pelled. Because our responses are delivered over
several turns, we also adjust weights between re-
sponses to reflect that discourse situation.
Our algorithm, shown in Figure 1, is run each
time a response text is to be generated. For each
new response, the algorithm begins by adjusting
the importance of the priority vertex (making it
high) and clearing the list of selected propositions.
Step 2 is the heart of the algorithm for generating a
single response. It incrementally selects proposi-
tions to include in the current response, and ad-
justs weights to reflect what has been selected.
In particular, in order to select a proposition, im-
portance scores are computed using the weighted
PageRank metric for all vertices corresponding to
propositions that have not yet been selected for in-
clusion in this response (Step 2-a), and only the
proposition that receives the highest score is se-
lected (Step 2-b). Then, adjustments are made to
achieve four goals toward taking discourse infor-
mation into account (Steps 2-c thru 2-g) before the
PageRank algorithm is run again to select the next
proposition. Steps 3 and 4 adjust weights to reflect
the completed response and to prepare for gener-
ating the next response.
Our first goal is to reflect the a priori impor-
tance of propositions in the selection process. For
this purpose, we always assign the highest (or
one of the highest) importance scores to the pri-
ority vertex among the other vertices (Steps 1 and
2-g). This will make the priority vertex as influen-
tial as any other neighbor of a vertex when calcu-
lating its importance.
Our second goal is to select propositions that are
relevant to previously selected propositions, or in
terms of the graph-based notation, to attract the
selection of vertices that are connected to the se-
lected vertices. To achieve this, we increase the
importance of the vertices corresponding to se-
lected propositions so that the propositions related
to them have a higher probability of being chosen
as the next proposition to include (Step 2-g).
Our third goal is to avoid selecting propositions
that preferably shouldn?t be communicated with
previously selected propositions if other related
propositions are available. To accomplish this, we
introduce the term repellers to refer to the kinds
of relations between propositions that are dispre-
ferred over other relations once one of the propo-
sitions is selected for inclusion. Once a proposi-
tion is selected, we penalize the weights on the
edges between the corresponding vertex and other
vertices that are connected by a repeller (Step 2-
d). We don?t provide any general repellers in the
framework, but rather this is left for the developer
familiar with the domain; any number (zero or
more) and kinds of relations could be identified as
repellers for a particular application domain. For
example, suppose that in the ILEX domain, some
artifacts (such as necklaces) have as features both
a set of design characteristics and the person who
found the artifact. Once the artifact is introduced,
it becomes more important to present the design
characteristics rather than the person who found
that artifact. This preference might be captured by
classifying the relation connecting the proposition
conveying the person who found it to the proposi-
tion introducing the artifact as arepeller.
Our fourth goal is to avoid redundancy by dis-
couraging the selection of propositions connected
by a Redundancy Relation to previously selected
propositions. Once a proposition is selected, we
identify the vertices (redundant to selected ver-
tices) which are connected to the selected ver-
tex by the Redundancy Relation (Step 2-e). For
each redundant to selected vertex, we penalize the
weights on the edges of the vertex except the edge
connected to the priority vertex (Step 2-f) and
hence decrease the probability of that vertex being
chosen for inclusion in the same response.
We have so far described how the content of a
single response is constructed in our framework.
To capture a situation where the system is engaged
in a dialogue with the user and must generate addi-
tional responses for each subsequent user request,
we need to ensure that discourse flows naturally.
Thus, the ranking algorithm must take the previ-
Figure 1: Our Ranking Algorithm for Content Selection.
ous discourse into account in order to identify and
preferably select propositions that have not been
conveyed before and to determine when repetition
of a previously communicated proposition is ap-
propriate. So once a proposition is included in a
response, we have to reduce its ability to compete
for inclusion in subsequent responses. Thus once a
proposition is conveyed in a response, the weight
of the edge connecting the corresponding vertex
to the priority vertex is reduced (Step 2-c in Fig-
ure 1). Once a response is completed, we penal-
ize the weights of the edges of each vertex that
has been selected for inclusion in the current re-
sponse via a penalty factor (if they aren?t already
adjusted) (Step 3 in Figure 1). We use the same
penalty factor (which is used in Step 2-d in Fig-
ure 1) on each edge so that all edges connected to
a selected vertex are penalized equally. However,
it isn?t enough just to penalize the edges of the ver-
tices corresponding to the communicated proposi-
tions. Even after the penalties are applied, a propo-
sition that has just been communicated might re-
ceive a higher importance score than an uncommu-
nicated proposition1. In order to allow all propo-
sitions to become important enough to be said at
some point, the algorithm increases the weights
of the edges of all other vertices in the graph if
they haven?t already been decreased (Step 4 in Fig-
ure 1), thereby increasing their ability to compete
in subsequent responses. In the current implemen-
tation, the weight of an edge is increased via a
boost factor after a response if it is not connected
to a proposition included in that response. The
1We observed that it might happen if a vertex is connected
only to the priority vertex.
boost factor ensures that all propositions will even-
tually become important enough for inclusion.
3 Application in a Particular Domain
This section illustrates the application of our
framework to a particular domain and how our
framework facilitates flexible content selection.
Our example is content selection in the SIGHT
system (Elzer et al, 2007), whose goal is to pro-
vide visually impaired users with the knowledge
that one would gain from viewing information
graphics (such as bar charts) that appear in popu-
lar media. In the current implementation, SIGHT
constructs a brief initial summary (Demir et al,
2008) that conveys the primary message of a bar
chart along with its salient features. We enhanced
the current SIGHT system to respond to user?s
follow-up requests for more information about the
graphic, where the request does not specify the
kind of information that is desired.
The first step in using our framework is deter-
mining the set of propositions that might be con-
veyed in this domain. In our earlier work (Demir
et al, 2008), we identified a set of propositions
that capture information that could be determined
by looking at a bar chart, and for each message
type defined in SIGHT, specified a subset of these
propositions that are related to this message type.
In our example, we use these propositions as can-
didates for inclusion in follow-up responses. Fig-
ure 2 presents a portion of the relation graph,
where some of the identified propositions are rep-
resented as vertices.
The second step is optionally assessing the a
priori importance of each proposition. In user
Figure 2: Subgraph of the Relation graph for Increasing and Decreasing Trend Message Types.
studies (Demir et al, 2008), we asked subjects to
classify the propositions given for a message type
into one of three classes according to their impor-
tance for inclusion in the initial summary: essen-
tial, possible, and not important. We leverage
this information as the a priori importance of ver-
tices in our graph representation. We define three
priority classes. For the propositions that were not
selected as essential by any participant, we clas-
sify the edges connecting these propositions to the
priority vertex into Possible class. For the propo-
sitions which were selected as essential by a single
participant, we classify the edges connecting them
to the priority vertex into Important class. The
edges of the remaining propositions are classified
into Highly Important class. In this example in-
stantiation, we assigned different numeric scores
to these classes where Highly Important and Pos-
sible received the highest and lowest scores re-
spectively.
The third step requires specifying the relations
between every pair of related propositions and de-
termining the weights associated with these re-
lations in the relation graph. First, we identi-
fied propositions which we decided should be
connected by the Redundancy Relation (such as
the propositions conveying ?the overall amount of
change in the trend? and ?the range of the trend?).
Next, we had to determine other relations and as-
sign relative weights. Instead of defining a unique
relation for each related pair, we defined three re-
lation classes, and assigned the relations between
related propositions to one of these classes:
? Period Relation: expresses a relation be-
tween two propositions that span the same
time period
? Entity Relation: expresses a relation be-
tween two propositions if the entities in-
volved in the propositions overlap
? Contrast Relation: expresses a relation be-
tween two propositions if the information
provided by one of the propositions contrasts
with the information provided by the other
We determined that it was very common in
this domain to deliver contrasting propositions to-
gether (similar to other domains (Marcu, 1998))
and therefore we assigned the highest score to the
Contrast Relation class. For local focusing pur-
poses, it is desirable that propositions involving
common entities be delivered in the same response
and thus the Entity Relation class was given the
second highest score. On the other hand, two
propositions which only share the same period are
not very related and conveying such propositions
in the same response could cause the text to appear
?choppy?. We thus identified the Period Relation
class as a repeller and assigned the second low-
est score to relations in that class. Since we don?t
want redundancy in the generated text, the lowest
score was assigned to the Redundancy Relation
class. The next section shows how associating
particular weights with the priority and relation
classes changes the behavior of the framework.
In the domain of graphics, a collection of de-
scriptions of the targeted kind which would facil-
itate a learning based model isn?t available. How-
ever, the accessibility of a corpus in a new domain
would allow the identification of the propositions
along with their relations to each other and the de-
termination of what weighting scheme and adjust-
ment policy will produce the corpus within reason-
able bounds.
3.1 Generating Flexible Responses
The behavior of our framework is dependent on a
number of design parameters such as the weights
associated with various relations, the identification
of repellers, the a priori importance of informa-
tion (if applicable), and the extent to which con-
veying redundant information should be avoided.
The framework allows the application developer
to adjust these factors resulting in the selection of
different content and the generation of different re-
sponses. For instance, in a very straightforward
setting where the same numeric score is assigned
to all relations, the a priori importance of infor-
mation would be the major determining factor in
the selection process. In this section, we will il-
lustrate our framework?s behavior in SIGHT with
three different scenarios. In each case, the user is
assumed to post two consecutive requests for ad-
ditional information about the graphic in Figure 3
after receiving its initial summary.
In our first scenario (which we refer to as ?base-
setting?), the following values have been given to
various design parameters that must be specified in
order to run the ranking algorithm. 1) The weights
of the relations are set to the numeric scores shown
in the text labelled Edges at the bottom (right side)
of Figure 2. 2) The stopping criteria which speci-
fies the number of propositions selected for inclu-
sion in a follow-up response (Step 2 in Figure 1)
is set to four. 3) The amount of decrease in the
weight of the edge between the priority vertex and
the vertex selected for inclusion (Step 2-c in Fig-
ure 1) is set to that edge?s original weight. Thus,
in our example, the weight of that edge is set to 0
once a proposition has been selected for inclusion.
4) The penalty and the redundancy penalty factors
which are used to penalize the edges of a selected
vertex and the vertices redundant to the selected
vertex (Steps 2-d and 3, and 2-f in Figure 1) are
set to the quotient of the highest numeric score
initially assigned to a relation class divided by the
lowest numeric score initially assigned to a rela-
tion class. A penalized score for a relation class
is computed by dividing its initial score by the
penalty factor. The edges of a vertex are penalized
by assigning the penalized scores to these edges
based on the relations that they represent. This set-
ting guarantees that the weight of an edge which
represents the strongest relation cannot be penal-
ized to be lower than the score initially assigned
to the weakest relation. 5) The boost factor which
is used to favor the selection of previously uncon-
veyed propositions for inclusion in subsequent re-
sponses (Step 4 in Figure 1) is set to the square
root of the penalty factor. Thus, the weights of
the edges connected to vertices of previously com-
municated propositions are restored to their initial
scores slowly.
Since in our example, the initial summary has
already been presented, we treat the propositions
conveyed in that summary (P1 and P5 in Figure 2)
as if they had been conveyed in a follow-up re-
sponse and penalize the edges of their correspond-
ing vertices (Steps 2-c and 3 in Figure 1). Thus,
before we invoke the algorithm to construct the
first follow-up response, the weights of edges of
the graph are as shown in Figure 2-A. Within this
base-setting, SIGHT generates the set of follow-up
responses shown in Figure 3A.
In our first scenario (base-setting), we assumed
that the user is capable of making mathematical
deductions such as inferring ?the overall amount
of change in the trend? from ?the range of the
trend?; thus we identified such propositions as
sharing a Redundancy Relation. Young read-
ers (such as fourth graders) might not find these
propositions as redundant because they are lack-
ing in mathematical skills. In our second sce-
nario, we address this issue by setting the re-
dundancy penalty factor to 1 (Step 2-f in Fig-
ure 1) and thus eliminate the penalty on the Re-
dundancy Relation. Now, for the same graphic,
SIGHT generates, in turn, the second alternative
set of responses shown in Figure 3B. The re-
sponses for the two scenarios differ in the second
follow-up response. In the first scenario, a descrip-
tion of the smallest drop was included. However,
in the second scenario, this proposition is replaced
with the overall amount of change in the trend.
This proposition was excluded in the first sce-
nario because the redundancy penalty factor made
it drop in importance.
Our third scenario shows how altering the
weights assigned to relations may change the re-
sponses. Consider a situation where the Con-
trast Relation is given even higher importance by
doubling its score; this might occur in a univer-
sity course domain where courses on the same
general topic are contrasted. SIGHT would then
generate the third alternative set of follow-up re-
sponses shown in Figure 3C. The algorithm is
more strongly forced to group propositions that
Figure 3: Initial Summary and Follow-up Responses.
are in a contrast relation (shown in bold), which
changes the ranking of these propositions.
4 Evaluation
To determine whether our framework selects ap-
propriate content within the context of an applica-
tion, and to assess the contribution of the discourse
related considerations to the selected content and
their impact on readers? satisfaction, we conducted
two user studies. In both studies, the partici-
pants were told that the initial summary should
include the most important information about the
graphic and that the remaining pieces of informa-
tion should be conveyed via follow-up responses.
The participants were also told that the informa-
tion in the first response should be more important
than the information in subsequent responses.
Our goal in the first study was to evaluate the
effectiveness of our framework (base-setting) in
determining the content of follow-up responses in
SIGHT. To our knowledge, no one else has gener-
ated high-level descriptions of information graph-
ics, and therefore evaluation using implementa-
tions of existing content selection modules in the
domain of graphics as a baseline is not feasible.
Thus, we evaluated our framework by comparing
the content that it selects for inclusion in a follow-
up response for a particular graphic with the con-
tent chosen by human subjects for the same re-
sponse. Twenty one university students partici-
pated in the first study and each participant was
presented with the same four graphics. For each
graphic, the participants were first presented with
its initial summary and the set of propositions (18
different propositions) that were used to construct
the relation graph in our framework. The partic-
ipants were then asked to select the four propo-
sitions that they thought were most important to
convey in the first follow-up response.
For each graphic, we ranked the propositions
with respect to the number of times that they were
selected by the participants and determined the po-
sition of each proposition selected by our frame-
work for inclusion in the first follow-up response
with respect to this ranking. The propositions se-
lected by our framework were ranked by the par-
ticipants as the 1st, 2nd, 3rd, and 5th in the first
graphic, as the 1st, 3rd, 4th, and 5th in the sec-
ond graphic, as the 1st, 2nd, 3rd, and 6th in the
third graphic, and as the 2nd, 3rd, 4th, and 6th
in the fourth graphic. Thus for every graph, three
of the four propositions selected by our frame-
work were also in the top four highly-rated propo-
sitions selected by the participants. Therefore,
this study demonstrated that our content selection
framework selects the most important information
for inclusion in a response at the current exchange.
We argued that simply running PageRank to se-
lect the highly-rated propositions is likely to lead
to text that does not cohere because it may con-
tain unrelated or redundant propositions, or fail
to communicate related propositions. Thus, our
approach iteratively runs PageRank and includes
discourse related factors in order to allow what
has been selected to influence the future selections
and consequently improve text coherence. To ver-
ify this argument, we conducted a second study
with four graphics and two different sets of follow-
up responses (each consisting of two consecutive
responses) generated for each graphic. We con-
structed the first set of responses (baseline) by
running PageRank to completion and selecting the
top eight highly-rated propositions, where the top
four propositions form the first response. The con-
tent of the second set of responses was identified
by our approach. Twelve university students (who
did not participate in the first study) were pre-
sented with these four graphics along with their
initial summaries. Each participant was also pre-
sented with the set of responses generated by our
approach in two graphics and the set of responses
generated by the baseline in other cases; the par-
ticipants were unaware of how the follow-up re-
sponses were generated. Overall, each set of re-
sponses was presented to six participants.
We asked the participants to evaluate the set
of responses in terms of their quality in convey-
ing additional information (from 1 to 5 with 5 be-
ing the best). We also asked each participant to
choose which set of responses (from among the
four sets of responses presented to them) best pro-
vides further information about the correspond-
ing graphic. The participants gave the set of re-
sponses generated by our approach an average rat-
ing of 4.33. The average participant rating for
the set of responses generated by the baseline was
3.96. In addition, the lowest score given to the
set of responses generated by our approach was
3, whereas the lowest score that the baseline re-
ceived was 2. We also observed that the set of re-
sponses generated by our approach was selected
as the best set by eight of the twelve participants.
Three of the remaining four participants selected
the set of responses generated by the baseline as
best (although they gave the same score to a set
of responses generated by our approach). In these
cases, the participants emphasized the wording
of the responses as the reason for their selection.
Thus this study demonstrated that the inclusion of
discourse related factors in our approach, in addi-
tion to the use of PageRank (which utilizes the a
priori importance of the propositions and their re-
lations to each other), contributes to text coherence
and improves readers? satisfaction.
5 Conclusion
This paper has presented our implemented
domain-independent content selection framework,
which contains domain-dependent features that
must be instantiated when applying it to a particu-
lar domain. To our knowledge, our work is the first
to select appropriate content by using an incre-
mental graph-based ranking algorithm that takes
into account the tendency for some information to
seem related or redundant to other information, the
a priori importance of information, and what has
already been said in the previous discourse. Al-
though our framework requires a knowledge engi-
neering phase to port it to a new domain, it handles
discourse issues without requiring that the devel-
oper write code to address them. We have demon-
strated how our framework was incorporated in
an accessibility system whose goal is the genera-
tion of texts to describe information graphics. The
evaluation studies of our framework within that
accessibility system show its effectiveness in de-
termining the content of follow-up responses.
6 Acknowledgements
The authors would like to thank Debra Yarrington
and the members of the NLP-AI Lab at UD for
their help throughout the evaluation of this work.
This material is based upon work supported by the
National Institute on Disability and Rehabilitation
Research under Grant No. H133G080047.
References
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual Web search engine. Computer
Networks and ISDN Systems, 30(1-7):107?117.
G. Carenini and J. Moore. 2006. Generating and eval-
uating evaluative arguments. Artificial Intelligence,
170(11):925?452.
C. Chiarcos and M. Stede. 2004. Salience-Driven Text
Planning. In Proc. of INLG?04.
S. Demir, S. Carberry, and K. F. McCoy. 2008. Gener-
ating Textual Summaries of Bar Charts. In Proc. of
INLG?08.
S. Elzer, E. Schwartz, S. Carberry, D. Chester,
S. Demir, and P. Wu. 2007. A browser extension
for providing visually impaired users access to the
content of bar charts on the web. In Proc. of WE-
BIST?2007.
G. Erkan and D. Radev. 2004. LexRank: Graph-
based Lexical Centrality as Salience in Text Summa-
rization. Journal of Artificial Intelligence Research,
22:457?479.
L. C. Freeman. 1979. Centrality in Social Networks: I.
Conceptual Clarification. Social Networks, 1:215?
239.
J. Lester and B. Porter. 1997. Developing and empir-
ically evaluating robust explanation generators: the
KNIGHT experiments. Computational Linguistics,
23(1):65?101.
D. Marcu. 1998. The rhetorical parsing, summariza-
tion, and generation of natural language texts. PhD.
Thesis, Department of Computer Science, University
of Toronto.
K. McKeown. 1985. Discourse strategies for gener-
ating natural-language text. Artificial Intelligence,
27(1):1?41.
J. Moore and C. Paris. 1993. Planning text for advisory
dialogues: capturing intentional and rhetorical infor-
mation. Computational Linguistics, 19(4):651?694.
J. Moore. 1993. Indexing and exploiting a discourse
history to generate context-sensitive explanations.
In Proc. of HLT?93, 165?170.
R. Navigli and M. Lapata. 2007. Graph Connectiv-
ity Measures for Unsupervised Word Sense Disam-
biguation. In Proc. of IJCAI?07, 1683?1688.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: an architecture for a dynamic hypertext
generation system. In Natural Language Engineer-
ing, 7(3):225?250.
E. Reiter and R. Dale. 1997. Building applied natural
language generation systems. In Natural Language
Engineering, 3(1):57?87.
R. Sinha and R. Mihalcea. 2007. Unsupervised Graph-
based Word Sense Disambiguation Using Measures
of Word Semantic Similarity. In Proc. of ICSC?07.
S. Sripada, E. Reiter, J. Hunter, and J. Yu. 2001. A
Two-Stage Model for Content Determination. In
Proc. of ENLGW?01.
M. Walker, S.J. Whittaker, A. Stent, P. Maloor,
J. Moore, M. Johnston, and G. Vasireddy. 2004.
Generation and evaluation of user tailored responses
in multimodal dialogue. In Cognitive Science,
28(5):811?840.
Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 41?48,
Portland, Oregon, June 23, 2011. c?2011 Association for Computational Linguistics
Abstractive Summarization of Line Graphs from Popular Media
Charles F. Greenbacker Peng Wu
Sandra Carberry Kathleen F. McCoy Stephanie Elzer*
Department of Computer and Information Sciences
University of Delaware, Newark, Delaware, USA
[charlieg|pwu|carberry|mccoy]@cis.udel.edu
*Department of Computer Science
Millersville University, Millersville, Pennsylvania, USA
elzer@cs.millersville.edu
Abstract
Information graphics (bar charts, line graphs,
etc.) in popular media generally have a dis-
course goal that contributes to achieving the
communicative intent of a multimodal docu-
ment. This paper presents our work on ab-
stractive summarization of line graphs. Our
methodology involves hypothesizing the in-
tended message of a line graph and using it
as the core of a summary of the graphic. This
core is then augmented with salient proposi-
tions that elaborate on the intended message.
1 Introduction
Summarization research has focused primarily on
summarizing textual documents, and until recently,
other kinds of communicative vehicles have been
largely ignored. As noted by Clark (1996), language
is more than just words ? it is any signal that is
intended to convey a message. Information graph-
ics (non-pictorial graphics such as bar charts, line
graphs, etc.) in popular media such as Newsweek,
Businessweek, or newspapers, generally have a com-
municative goal or intended message. For exam-
ple, the graphic in Figure 1 is intended to convey
a changing trend in sea levels ? relatively flat from
1900 to 1930 and then rising from 1930 to 2003.
Thus, using Clark?s view of language, information
graphics are a means of communication.
Research has shown that the content of informa-
tion graphics in popular media is usually not re-
peated in the text of the accompanying article (Car-
berry et al, 2006). The captions of such graphics
are also often uninformative or convey little of the
graphic?s high-level message (Elzer et al, 2005).
This contrasts with scientific documents in which
graphics are often used to visualize data, with ex-
plicit references to the graphic being used to explain
their content (e.g., ?As shown in Fig. A...?). Infor-
mation graphics in popular media contribute to the
overall communicative goal of a multimodal docu-
ment and should not be ignored.
Our work is concerned with the summarization
of information graphics from popular media. Such
summaries have several major applications: 1) they
can be integrated with the summary of a multimodal
document?s text, thereby producing a richer sum-
mary of the overall document?s content; 2) they can
be stored in a digital library along with the graphic
itself and used to retrieve appropriate graphics in re-
sponse to user queries; and 3) for individuals with
sight impairments, they can be used along with a
screen reader to convey not only the text of a docu-
ment, but also the content of the document?s graph-
ics. In this paper we present our work on summariz-
ing line graphs. This builds on our previous efforts
into summarizing bar charts (Demir et al, 2008;
Elzer et al, 2011); however, line graphs have dif-
ferent messages and communicative signals than bar
charts and their continuous nature requires different
processing. In addition, a very different set of visual
features must be taken into account in deciding the
importance of including a proposition in a summary.
2 Methodology
Most summarization research has focused on ex-
tractive techniques by which segments of text are
extracted and put together to form the summary.
41
?
102468 1900?1
0?20
?50?60
?70?80
?90
?03
?30?40
2000
10
8.9
1.979 inches over
 the past ce
ntury. Ann
ual differen
ce from Se
attle?s
In the seatt
le area, for
 example, t
he Pacific 
Ocean has 
risen nearly
 
they are ris
ing about 0
.04?0.09 o
f an inch ea
ch year.
Sea levels 
fluctuate ar
ound the gl
obe, but oc
eanographe
rs believe
Ocean leve
ls rising
1899 sea le
vel, in inch
es:
Figure 1: From ?Worry flows from Arctic ice to tropical
waters? in USA Today, May 31, 2006.
However, the Holy Grail of summarization work is
abstractive summarization in which the document?s
content is understood and the important concepts are
integrated into a coherent summary. For informa-
tion graphics, extractive summarization might mean
treating the text in the graphic (e.g., the caption) as if
it were document text. One could imagine perhaps
expanding this view to include selecting particular
data points or segments and constructing sentences
that convey them. Abstractive summarization, on
the other hand, requires that the high-level content
of the graphic be identified and conveyed in the sum-
mary. The goal of our work is abstractive summa-
rization. The main issues are identifying the knowl-
edge conveyed by a graphic, selecting the concepts
that should be conveyed in a summary, and integrat-
ing them into coherent natural language sentences.
As noted in the Introduction, information graphics
in popular media generally have a high-level mes-
sage that they are intended to convey. This mes-
sage constitutes the primary communicative or dis-
course goal (Grosz and Sidner, 1986) of the graphic
and captures its main contribution to the overall dis-
course goal of the entire document. However, the
graphic also includes salient features that are impor-
tant components of the graphic?s content. For exam-
ple, the graphic in Figure 1 is very jagged with sharp
fluctuations, indicating that short-term changes have
been inconsistent. Since the graphic?s intended mes-
sage represents its primary discourse goal, we con-
tend that this message should form the core or fo-
cus of the graphic?s summary. The salient features
should be used to augment the summary of the graph
and elaborate on its intended message. Thus, our
methodology consists of the following steps: 1) hy-
pothesize the graphic?s primary discourse or com-
municative goal (i.e., its intended message), 2) iden-
tify additional propositions that are salient in the
graphic, and 3) construct a natural language sum-
mary that integrates the intended message and the
additional salient propositions into a coherent text.
Section 3 presents our methodology for hypothe-
sizing a line graph?s intended message or discourse
goal. It starts with an XML representation of the
graphic that specifies the x-y coordinates of the sam-
pled pixels along the data series in the line graph, the
axes with tick marks and labels, the caption, etc.;
constructing the XML representation is the respon-
sibility of a Visual Extraction Module similar to the
one for bar charts described by Chester and Elzer
(2005). Section 4 presents our work on identifying
the additional propositions that elaborate on the in-
tended message and should be included in the sum-
mary. Section 5 discusses future work on realizing
the propositions in a natural language summary, and
Section 6 reviews related work in multimodal and
abstractive summarization.
3 Identifying a Line Graph?s Message
Research has shown that human subjects have a
strong tendency to use line graphs to portray trend
relationships, as well as a strong tendency to de-
scribe line graphs in terms of trends (Zacks and
Tversky, 1999). We analyzed a corpus of sim-
ple line graphs collected from various popular me-
dia including USA Today, Businessweek, and The
(Wilmington) News Journal, and identified a set of
10 high-level message categories that capture the
kinds of messages that are conveyed by a simple
line graph. Table 1 defines four of them. The com-
plete list can be found in (Wu et al, 2010b). Each
of these messages requires recognizing the visual
trend(s) in the depicted data. We use a support vec-
tor machine (SVM) to first segment the line graph
into a sequence of visually-distinguishable trends;
this sequence is then input into a Bayesian net-
work that reasons with evidence from the graphic
42
Intention Category Description
RT: Rising-trend There is a rising trend from <param1> to <param2>.
CT: Change-trend There is a <direction2> trend from <param2> to <param3> that is signifi-
cantly different from the <direction1> trend from <param1> to <param2>.
CTR:
Change-trend-return
There is a <direction1> trend from <param3> to <param4> that is different
from the <direction2> trend between <param2> and <param3> and reflects
a return to the kind of <direction1> trend from <param1> to <param2>.
BJ: Big-jump There was a very significant sudden jump in value between <param1> and
<param2> which may or may not be sustained.
Table 1: Four categories of High Level Messages for Line Graphs
in order to recognize the graphic?s intended mes-
sage. The next two subsections outline these
steps. (Our corpus of line graphs can be found at
www.cis.udel.edu/?carberry/Graphs/viewallgraphs.php)
3.1 Segmenting a Line Graph
A line graph can consist of many short, jagged
line segments, although a viewer of the graphic ab-
stracts from it a sequence of visually-distinguishable
trends. For example, the line graph in Figure 1 con-
sists of two trends: a relatively stable trend from
1900 to 1930 and a longer, increasing trend from
1930 to 2003. Our Graph Segmentation Module
(GSM) takes a top-down approach (Keogh et al,
2001) to generalize the line graph into sequences of
rising, falling, and stable segments, where a segment
is a series of connected data points. The GSM starts
with the entire line graph as a single segment and
uses a learned model to recursively decide whether
each segment should be split into two subsegments;
if the decision is to split, the division is made at the
point being the greatest distance from a straight line
between the two end points of the original segment.
This process is repeated on each subsegment until
no further splits are identified. The GSM returns a
sequence of straight lines representing a linear re-
gression of the points in each subsegment, where
each straight line is presumed to capture a visually-
distinguishable trend in the original graphic.
We used Sequential Minimal Optimization (Platt,
1999) in training an SVM to make segment split-
ting decisions. We chose to use an SVM because it
works well with high-dimensional data and a rela-
tively small training set, and lessens the chance of
overfitting by using the maximum margin separat-
ing hyperplane which minimizes the worst-case gen-
eralization errors (Tan et al, 2005). 18 attributes,
falling into two categories, were used in building
the data model (Wu et al, 2010a). The first cat-
egory captures statistical tests computed from the
sampled data points in the XML representation of
the graphic; these tests estimate how different the
segment is from a linear regression (i.e., a straight
line). The second category of attributes captures
global features of the graphic. For example, one
such attribute relates the segment size to the size of
the entire graphic, based on the hypothesis that seg-
ments comprising more of the total graph may be
stronger candidates for splitting than segments that
comprise only a small portion of the graph.
Our Graph Segmentation Module was trained
on a set of 649 instances that required a split/no-
split decision. Using leave-one-out cross validation,
in which one instance is used for testing and the
other 648 instances are used for training, our model
achieved an overall accuracy rate of 88.29%.
3.2 A Bayesian Recognition System
Once the line graph has been converted into
a sequence of visually-distinguishable trends, a
Bayesian network is built that captures the possible
intended messages for the graphic and the evidence
for or against each message. We adopted a Bayesian
network because it weighs different pieces of evi-
dence and assigns a probability to each candidate
intended message. The next subsections briefly out-
line the Bayesian network and its evaluation; details
can be found in (Wu et al, 2010b).
Structure of the Bayesian Network Figure 2
shows a portion of the Bayesian network constructed
for Figure 1. The top-level node in our Bayesian net-
work represents all of the high-level message cat-
43
Intended Message
... ...
... ...
...
CT?Suggestion?1
CT IntentionRT Intention
EvidenceOtherPointsAnnotated
Have SuggestionEvidence
Portion of GraphicEvidence EndpointsAnnotatedEvidence EvidenceSplittingPointsAnnotated
Adjective in CaptionEvidence
Verb in CaptionEvidence
Figure 2: A portion of the Bayesian network
egories. Each of these possible non-parameterized
message categories is repeated as a child of the
top-level node; this is purely for ease of repre-
sentation. Up to this point, the Bayesian net-
work is a static structure with conditional proba-
bility tables capturing the a priori probability of
each category of intended message. When given
a line graph to analyze, an extension of this net-
work is built dynamically according to the partic-
ulars of the graph itself. Candidate (concrete) in-
tended messages, having actual instantiated param-
eters, appear beneath the high-level message cat-
egory nodes. These candidates are introduced by
a Suggestion Generation Module; it dynamically
constructs all possible intended messages with con-
crete parameters using the visually-distinguishable
trends (rising, falling, or stable) identified by the
Graph Segmentation Module. For example, for each
visually-distinguishable trend, a Rising, Falling, or
Stable trend message is suggested; similary, for each
sequence of two visually-distinguishable trends, a
Change-trend message is suggested. For the graphic
in Figure 1, six candidate messages will be gener-
ated, including RT(1930, 2003), CT(1900, stable,
1930, rise, 2003) and BJ(1930, 2003) (see Table 1).
Entering Evidence into the Bayesian Network
Just as listeners use evidence to identify the intended
meaning of a speaker?s utterance, so also must a
viewer use evidence to recognize a graphic?s in-
tended message. The evidence for or against each
of the candidate intended messages must be entered
into the Bayesian network. We identified three kinds
of evidence that are used in line graphs: attention-
getting devices explicitly added by the graphic de-
signer (e.g., the annotation of a point with its value),
aspects of a graphic that are perceptually-salient
(e.g., the slope of a segment), and clues that sug-
gest the general message category (e.g., a verb [or
noun derived from a verb such as rebound] in the
caption which might indicate a Change-trend mes-
sage). The first two kinds of evidence are attached
to the Bayesian network as children of each candi-
date message node, such as the child nodes of ?CT-
Suggestion-1? in Figure 2. The third kind of evi-
dence is attached to the top level node as child nodes
named ?Verb in Caption Evidence? and ?Adjective
in Caption Evidence? in Figure 2.
Bayesian Network Inference We evaluated the
performance of our system for recognizing a line
graph?s intended message on a corpus of 215 line
graphs using leave-one-out cross validation in which
one graph is held out as a test graph and the con-
ditional probability tables for the Bayesian network
are computed from the other 214 graphs. Our sys-
tem recognized the correct intended message with
the correct parameters for 157 line graphs, resulting
in a 73.36% overall accuracy rate.
4 Identifying Elaborative Propositions
Once the intended message has been determined,
the next step is to identify additional important
informational propositions1 conveyed by the line
graph which should be included in the summary.
To accomplish this, we collected data to determine
what kinds of propositions in what situations were
deemed most important by human subjects, and de-
veloped rules designed to make similar assessments
based on the graphic?s intended message and visual
features present in the graphic.
4.1 Collecting Data from Human Subjects
Participants in our study were given 23 different line
graphs. With each graph, the subjects were provided
1We define a ?proposition? as a logical representation de-
scribing a relationship between one or more concepts, while a
?sentence? is a surface form realizing one or more propositions.
44
Figure 3: From ?This Cable Outfit Is Getting Tuned In?
in Businessweek magazine, Oct 4, 1999.
with an initial sentence describing the overall in-
tended message of the graphic. The subjects were
asked to add additional sentences so that the com-
pleted summary captured the most important infor-
mation conveyed by the graphic. The graphs were
presented to the subjects in different orders, and the
subjects completed as many graphs as they wanted
during the one hour study session. The set covered
the eight most prevalent of our intended message
categories and a variety of visual features. Roughly
half of the graphs were real-world examples from
the corpus used to train the Bayesian network in
Section 3.2, (e.g., Figure 3), with the others created
specifically to fill a gap in the coverage of intended
messages and visual features.
We collected a total of 998 summaries written by
69 human subjects for the 23 different line graphs.
The number of summaries we received per graph
ranged from 37 to 50. Most of the summaries were
between one and four sentences long, in addition to
the initial sentence (capturing the graphic?s intended
message) that was provided for each graph. A rep-
resentative sample summary collected for the line
graph shown in Figure 3 is as follows, with the initial
sentence provided to the study participants in italics:
This line graph shows a big jump in Blon-
der Tongue Laboratories stock price in
August ?99. The graph has many peaks
and valleys between March 26th 1999 to
August ?99 but maintains an average stock
price of around 6 dollars. However, in Au-
gust ?99 the stock price jumps sharply to
around 10 dollars before dropping quickly
to around 9 dollars by September 21st.
4.2 Extracting & Weighting Propositions
The data collected during the study was analyzed by
a human annotator who manually coded the propo-
sitions that appeared in each individual summary in
order to determine, for each graphic, which proposi-
tions were used and how often. For example, the set
of propositions coded in the sample summary from
Section 4.1 were:
? volatile(26Mar99, Aug99)
? average val(26Mar99, Aug99, $6)
? jump 1(Aug99, $10)
? steep(jump 1)
? decrease 1(Aug99, $10, 21Sep99, $9)
? steep(decrease 1)
From this information, we formulated a set of
rules governing the use of each proposition accord-
ing to the intended message category and various
visual features. Our intuition was that by finding
and exploiting a correlation between the intended
message category and/or certain visual features and
the propositions appearing most often in the human-
written summaries, our system could use these in-
dicators to determine which propositions are most
salient in new graphs. Our rules assign a weight
to each proposition in the situation captured by the
rule; these weights are based on the relative fre-
quency of the proposition being used in summaries
reflecting similar situations in our corpus study. The
rules are organized into three types:
1. Message Category-only (M):
IF M = m THEN select P with weight w1
2. Visual Feature-only (V):
IF V = v THEN select P with weight w2
3. Message Category + Visual Feature:
IF M = m and V = v
THEN select P with weight w2
We constructed type 1 (Message Category-only)
rules when a plurality of human-written summaries
45
in our corpus for all line graphs belonging to a
given message category contain the proposition. A
weight was assigned according to the frequency with
which the proposition was included. This weighting,
shown in Equation 1, is based on the proportion of
summaries for each line graph in the corpus having
intended message m and containing proposition P.
w1 =
n?
i=1
Pi
Si
(1)
In this equation, n is the number of line graphs in
this intended message category, Si is the total num-
ber of summaries for a particular line graph with this
intended message category, and Pi is the number of
these summaries that contain the proposition.
Intuitively, a proposition appearing in all sum-
maries for all graphs in a given message category
will have a weight of 1.0, while a proposition which
never appears will have a weight of zero. How-
ever, a proposition appearing in all summaries for
half of the graphs in a category, and rarely for the
other half of the graphs in that category, will have a
much lower weight than one which appears in half
of the summaries for all the graphs in that category,
even though the overall frequencies could be equal
for both. In this case, the message category is an
insufficient signal, and it is likely that the former
proposition is more highly correlated to some par-
ticular visual feature than to the message category.
Weights for type 2 and type 3 rules (Visual
Feature-only and Message Category + Visual Fea-
ture) are slightly more complicated in that they in-
volve a measure of degree for the associated visual
feature rather than simply its presence. The defini-
tion of this measure varies depending on the nature
of the visual feature (e.g., steepness of a trend line,
volatility), but all such measures range from zero to
one. Additionally, since the impact of a visual fea-
ture is a matter of degree, the weighting cannot rely
on a simple proportion of summaries containing the
proposition as in type 1 rules. Instead, it is neces-
sary to find the covariance between the magnitude of
the visual feature (|v|) and how frequently the corre-
sponding proposition is used (PS ) in the corpus sum-
maries for the n graphs having this visual feature, as
shown in Equation 2.
Cov(|v|,
P
S
) =
[(?n
i=1 |vi|
n
?n
i=1
Pi
Si
n
)
?
?n
i=1 |vi|
Pi
Si
n
] (2)
Then for a particular graphic whose magnitude for
this feature is |v|, we compute the weight w2 for the
proposition P as shown in Equation 3.
w2 = |v| ? Cov(|v|,
P
S
) (3)
This way, the stronger a certain visual feature is in a
given line graph, the higher the weight for the asso-
ciated proposition.
Type 3 rules (Message Category + Visual Fea-
ture) differ only from type 2 rules in that they are
restricted to a particular intended message category,
rather than any line graph having the visual feature
in question. For example, a proposition compar-
ing the slope of two trends may be appropriate for
a graph in the Change-trend message category, but
does not make sense for a line graph with only a sin-
gle trend (e.g., Rising-trend).
Once all propositions have been extracted and
ranked, these weights are passed along to a graph-
based content selection framework (Demir et al,
2010) that iteratively selects for inclusion in the ini-
tial summary those propositions which provide the
best coverage of the highest-ranked information.
4.3 Sample Rule Application
Figures 1 and 4 consist of two different line graphs
with the same intended message category: Change-
trend. Figure 1 shows a stable trend in annual sea
level difference from 1900 to 1930, followed by a
rising trend through 2003, while Figure 4 shows a
rising trend in Durango sales from 1997 to 1999,
followed by a falling trend through 2006. Proposi-
tions associated with type 1 rules will have the same
weights for both graphs, but propositions related to
visual features may have different weights. For ex-
ample, the graph in Figure 1 is far more volatile than
the graph in Figure 4. Thus, the type 2 rule associ-
ated with volatility will have a very high weight for
the graph in Figure 1 and will almost certainly be in-
cluded in the initial summary of that line graph (e.g.,
46
20062005200420032002
19971998
1999
20012000
200,000 150,000199
9: 189,840
70,6062006:
50,000100,000Declining Du
rango sales
0
Figure 4: From ?Chrysler: Plant had $800 million im-
pact? in The (Wilmington) News Journal, Feb 15, 2007.
?The values vary a lot...?, ?The trend is unstable...?),
possibly displacing a type 1 proposition that would
still appear in the summary for the graph in Figure 4.
5 Future Work
Once the propositions that should be included in the
summary have been selected, they must be coher-
ently organized and realized as natural language sen-
tences. We anticipate using the FUF/SURGE sur-
face realizer (Elhadad and Robin, 1996); our col-
lected corpus of line graph summaries provides a
large set of real-world expressions to draw from
when crafting the surface realization forms our sys-
tem will produce for the final-output summaries.
Our summarization methodology must also be eval-
uated. In particular, we must evaluate the rules for
identifying the additional informational propositions
that are used to elaborate the overall intended mes-
sage, and the quality of the summaries both in terms
of content and coherence.
6 Related Work
Image summarization has focused on constructing a
smaller image that contains the important content of
a larger image (Shi et al, 2009), selecting a set of
representative images that summarize a collection
of images (Baratis et al, 2008), or constructing a
new diagram that summarizes one or more diagrams
(Futrelle, 1999). However, all of these efforts pro-
duce an image as the end product, not a textual sum-
mary of the content of the image(s).
Ferres et al (2007) developed a system for con-
veying graphs to blind users, but it generates the
same basic information for each instance of a graph
type (e.g., line graphs) regardless of the individual
graph?s specific characteristics. Efforts toward sum-
marizing multimodal documents containing graph-
ics have included na??ve approaches relying on cap-
tions and direct references to the image in the text
(Bhatia et al, 2009), while content-based image
analysis and NLP techniques are being combined for
multimodal document indexing and retrieval in the
medical domain (Ne?ve?ol et al, 2009).
Jing and McKeown (1999) approached abstrac-
tive summarization as a text-to-text generation task,
modifying sentences from the original document via
editing and rewriting. There have been some at-
tempts to do abstractive summarization from seman-
tic models, but most of it has focused on text docu-
ments (Rau et al, 1989; Reimer and Hahn, 1988),
though Alexandersson (2003) used abstraction and
semantic modeling for speech-to-speech translation
and multilingual summary generation.
7 Discussion
Information graphics play an important communica-
tive role in popular media and cannot be ignored.
We have presented our methodology for construct-
ing a summary of a line graph. Our method is ab-
stractive, in that we identify the important high-level
knowledge conveyed by a graphic and capture it in
propositions to be realized in novel, coherent natu-
ral language sentences. The resulting summary can
be integrated with a summary of the document?s text
to produce a rich summary of the entire multimodal
document. In addition, the graphic?s summary can
be used along with a screen reader to provide sight-
impaired users with full access to the knowledge
conveyed by multimodal documents.
Acknowledgments
This work was supported in part by the National In-
stitute on Disability and Rehabilitation Research un-
der Grant No. H133G080047.
References
Jan Alexandersson. 2003. Hybrid Discourse Modeling
and Summarization for a Speech-to-Speech Transla-
tion System. Ph.D. thesis, Saarland University.
Evdoxios Baratis, Euripides Petrakis, and Evangelos Mil-
ios. 2008. Automatic web site summarization by im-
age content: A case study with logo and trademark
47
images. IEEE Transactions on Knowledge and Data
Engineering, 20(9):1195?1204.
Sumit Bhatia, Shibamouli Lahiri, and Prasenjit Mitra.
2009. Generating synopses for document-element
search. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management, CIKM
?09, pages 2003?2006, Hong Kong, November. ACM.
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proc. of the 29th Annual Int?l ACM
SIGIR Conf. on Research & Development in Informa-
tion Retrieval, SIGIR ?06, pages 581?588, Seattle, Au-
gust. ACM.
Daniel Chester and Stephanie Elzer. 2005. Getting com-
puters to see information graphics so users do not have
to. In Proceedings of the 15th International Sympo-
sium on Methodologies for Intelligent Systems (LNAI
3488), ISMIS 2005, pages 660?668, Saratoga Springs,
NY, June. Springer-Verlag.
Herbert Clark. 1996. Using Language. Cambridge Uni-
versity Press.
Seniz Demir, Sandra Carberry, and Kathleen F. McCoy.
2008. Generating textual summaries of bar charts.
In Proceedings of the 5th International Natural Lan-
guage Generation Conference, INLG 2008, pages 7?
15, Salt Fork, Ohio, June. ACL.
Seniz Demir, Sandra Carberry, and Kathleen F. Mc-
Coy. 2010. A discourse-aware graph-based content-
selection framework. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 17?26, Trim, Ireland, July. ACL.
Michael Elhadad and Jacques Robin. 1996. An overview
of SURGE: a re-usable comprehensive syntactic re-
alization component. In Proceedings of the 8th In-
ternational Natural Language Generation Workshop
(Posters & Demos), Sussex, UK, June. ACL.
Stephanie Elzer, Sandra Carberry, Daniel Chester, Seniz
Demir, Nancy Green, Ingrid Zukerman, and Keith
Trnka. 2005. Exploring and exploiting the limited
utility of captions in recognizing intention in infor-
mation graphics. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 223?230, Ann Arbor, June. ACL.
Stephanie Elzer, Sandra Carberry, and Ingrid Zukerman.
2011. The automated understanding of simple bar
charts. Artificial Intelligence, 175:526?555, February.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proc. of the 9th Int?l ACM
SIGACCESS Conf. on Computers & Accessibility, AS-
SETS ?07, pages 67?74, Tempe, October. ACM.
Robert P. Futrelle. 1999. Summarization of diagrams in
documents. In I. Mani and M. Maybury, editors, Ad-
vances in Automatic Text Summarization. MIT Press.
Barbara Grosz and Candace Sidner. 1986. Attention,
Intentions, and the Structure of Discourse. Computa-
tional Linguistics, 12(3):175?204.
Hongyan Jing and Kathleen R. McKeown. 1999. The
decomposition of human-written summary sentences.
In Proc. of the 22nd Annual Int?l ACM SIGIR Conf.
on Research & Development in Information Retrieval,
SIGIR ?99, pages 129?136, Berkeley, August. ACM.
Eamonn J. Keogh, Selina Chu, David Hart, and
Michael J. Pazzani. 2001. An online algorithm
for segmenting time series. In Proceedings of the
2001 IEEE International Conference on Data Mining,
ICDM ?01, pages 289?296, Washington, DC. IEEE.
Aure?lie Ne?ve?ol, Thomas M. Deserno, Ste?fan J. Darmoni,
Mark Oliver Gu?ld, and Alan R. Aronson. 2009. Nat-
ural language processing versus content-based image
analysis for medical document retrieval. Journal of the
American Society for Information Science and Tech-
nology, 60(1):123?134.
John C. Platt. 1999. Fast training of support vector
machines using sequential minimal optimization. In
B. Scho?lkopf, C. J. C. Burges, and A. J. Smola, editors,
Advances in kernel methods: support vector learning,
pages 185?208. MIT Press, Cambridge, MA, USA.
Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. 1989. In-
formation extraction and text summarization using lin-
guistic knowledge acquisition. Information Process-
ing & Management, 25(4):419 ? 428.
Ulrich Reimer and Udo Hahn. 1988. Text condensation
as knowledge base abstraction. In Proceedings of the
4th Conference on Artificial Intelligence Applications,
CAIA ?88, pages 338?344, San Diego, March. IEEE.
Liang Shi, Jinqiao Wang, Lei Xu, Hanqing Lu, and
Changsheng Xu. 2009. Context saliency based im-
age summarization. In Proceedings of the 2009 IEEE
international conference on Multimedia and Expo,
ICME ?09, pages 270?273, New York. IEEE.
Pang-Ning Tan, Michael Steinbach, and Vipin Kumar.
2005. Introduction to Data Mining. Addison Wesley.
Peng Wu, Sandra Carberry, and Stephanie Elzer. 2010a.
Segmenting line graphs into trends. In Proceedings of
the 2010 International Conference on Artificial Intel-
ligence, ICAI ?10, pages 697?703, Las Vegas, July.
Peng Wu, Sandra Carberry, Stephanie Elzer, and Daniel
Chester. 2010b. Recognizing the intended message
of line graphs. In Proc. of the 6th Int?l Conf. on Dia-
grammatic Representation & Inference, Diagrams ?10,
pages 220?234, Portland. Springer-Verlag.
Jeff Zacks and Barbara Tversky. 1999. Bars and lines:
A study of graphic communication. Memory & Cog-
nition, 27:1073?1079.
48
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 52?62,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Improving the Accessibility of Line Graphs in Multimodal Documents
Charles F. Greenbacker Peng Wu Sandra Carberry Kathleen F. McCoy
Stephanie Elzer* David D. McDonald? Daniel Chester Seniz Demir?
Dept. of Computer & Information Sciences, University of Delaware, USA
[charlieg|pwu|carberry|mccoy|chester]@cis.udel.edu
*Dept. of Computer Science, Millersville University, USA elzer@cs.millersville.edu
?SIFT LLC., Boston, Massachusetts, USA dmcdonald@sift.info
?TU?BI?TAK BI?LGEM, Gebze, Kocaeli, Turkey senizd@uekae.tubitak.gov.tr
Abstract
This paper describes our work on improv-
ing access to the content of multimodal docu-
ments containing line graphs in popular media
for people with visual impairments. We pro-
vide an overview of our implemented system,
including our method for recognizing and con-
veying the intended message of a line graph.
The textual description of the graphic gener-
ated by our system is presented at the most rel-
evant point in the document. We also describe
ongoing work into obtaining additional propo-
sitions that elaborate on the intended message,
and examine the potential benefits of analyz-
ing the text and graphical content together in
order to extend our system to produce sum-
maries of entire multimodal documents.
1 Introduction
Individuals with visual impairments have difficulty
accessing the information contained in multimodal
documents. Although screen-reading software can
render the text of the document as speech, the graph-
ical content is largely inaccessible. Here we con-
sider information graphics (e.g., bar charts, line
graphs) often found in popular media sources such
as Time magazine, Businessweek, and USA Today.
These graphics are typically intended to convey a
message that is an important part of the overall story,
yet this message is generally not repeated in the ar-
ticle text (Carberry et al, 2006). People who are
unable to see and assimilate the graphical material
will be left with only partial information.
While some work has addressed the accessibility
of scientific graphics through alternative means like
touch or sound (see Section 7), such graphs are de-
signed for an audience of experts trained to use them
for data visualization. In contrast, graphs in popular
media are constructed to make a point which should
be obvious without complicated scientific reasoning.
We are thus interested in generating a textual pre-
sentation of the content of graphs in popular media.
Other research has focused on textual descriptions
(e.g., Ferres et al (2007)); however in that work the
same information is included in the textual summary
for each instance of a graph type (i.e., all summaries
of line graphs contain the same sorts of informa-
tion), and the summary does not attempt to present
the overall intended message of the graph.
SIGHT (Demir et al, 2008; Elzer et al, 2011) is
a natural language system whose overall goal is pro-
viding blind users with interactive access to multi-
modal documents from electronically-available pop-
ular media sources. To date, the SIGHT project
has concentrated on simple bar charts. Its user in-
terface is implemented as a browser helper object
within Internet Explorer that works with the JAWS
screen reader. When the system detects a bar chart
in a document being read by the user, it prompts the
user to use keystrokes to request a brief summary of
the graphic capturing its primary contribution to the
overall communicative goal of the document. The
summary text can either be read to the user with
JAWS or read by the user with a screen magnifier
tool. The interface also enables the user to request
further information about the graphic, if desired.
However, SIGHT is limited to bar charts only.
In this work, we follow the methodology put forth
by SIGHT, but investigate producing a summary of
52
?
102468 1900?10
?20
?50?60
?70?80
?90?
03
?30?40
2000
10
8.9
1.979 inches over t
he past centu
ry. Annual d
ifference fro
m Seattle?s
In the seattl
e area, for e
xample, the
 Pacific Oce
an has risen
 nearly 
they are risi
ng about 0.0
4?0.09 of an
 inch each y
ear.
Sea levels fl
uctuate arou
nd the globe
, but oceano
graphers bel
ieve
Ocean level
s rising
1899 sea lev
el, in inches
:
Figure 1: From ?Worry flows from Arctic ice to tropical
waters? in USA Today, May 31, 2006.
line graphs. Line graphs have different discourse
goals and communicative signals than bar charts,1
and thus require significantly different processing.
In addition, our work addresses the issue of coher-
ent placement of a graphic?s summary when reading
the text to the user and considers the summarization
of entire documents ? not just their graphics.
2 Message Recognition for Line Graphs
This section provides an overview of our imple-
mented method for identifying the intended message
of a line graph. In processing a line graph, a vi-
sual extraction module first analyzes the image file
and produces an XML representation which fully
specifies the graphic (including the beginning and
ending points of each segment, any annotations on
points, axis labels, the caption, etc.). To identify
the intended message of a line graph consisting of
many short, jagged segments, we must generalize
it into a sequence of visually-distinguishable trends.
This is performed by a graph segmentation module
which uses a support vector machine and a variety
of attributes (including statistical tests) to produce a
model that transforms the graphic into a sequence of
straight lines representing visually-distinguishable
trends. For example, the line graph in Figure 1 is
divided into a stable trend from 1900 to 1930 and a
rising trend from 1930 to 2003. Similarly, the line
graph in Figure 2 is divided into a rising trend from
1Bar charts present data as discrete bars and are often used
to compare entities, while line graphs contain continuous data
series and are designed to portray longer trend relationships.
20062005200420032002
19971998
1999
20012000
200,000 150,0001999
: 189,840
70,6062006:
50,000100,000Declining Dur
ango sales
0
Figure 2: From ?Chrysler: Plant had $800 million im-
pact? in The (Wilmington) News Journal, Feb 15, 2007.
1997 to 1999 and a falling trend from 1999 to 2006.
In analyzing a corpus of around 100 line graphs
collected from several popular media sources, we
identified 10 intended message categories (includ-
ing rising-trend, change-trend, change-trend-return,
and big-jump, etc.), that seem to capture the kinds
of high-level messages conveyed by line graphs. A
suggestion generation module uses the sequence of
trends identified in the line graph to construct all
of its possible candidate messages in these message
categories. For example, if a graph contains three
trends, several candidate messages are constructed,
including two change-trend messages (one for each
adjacent pair of trends), a change-trend-return mes-
sage if the first and third trends are of the same type
(rising, falling, or stable), as well as a rising, falling,
or stable trend message for each individual trend.
Next, various communicative signals are ex-
tracted from the graphic, including visual features
(such as a point annotated with its value) that draw
attention to a particular part of the line graph, and
linguistic clues (such as the presence of certain
words in the caption) that suggest a particular in-
tended message category. Figure 2 contains several
such signals, including two annotated points and the
word declining in its caption. Next, a Bayesian net-
work is built to estimate the probability of the can-
didate messages; the extracted communicative sig-
nals serve as evidence for or against each candidate
message. For Figure 2, our system produces change-
trend(1997, rise, 1999, fall, 2006) as the logical rep-
resentation of the most probable intended message.
Since the dependent axis is often not explicitly la-
beled, a series of heuristics are used to identify an
appropriate referent, which we term the measure-
ment axis descriptor. In Figure 2, the measurement
axis descriptor is identified as durango sales. The
53
intended message and measurement axis descriptor
are then passed to a realization component which
uses FUF/SURGE (Elhadad and Robin, 1996) to
generate the following initial description:
This graphic conveys a changing trend in
durango sales, rising from 1997 to 1999
and then falling to 2006.
3 Identifying a Relevant Paragraph
In presenting a multimodal document to a user via a
screen reader, if the author does not specify a read-
ing order in the accessibility preferences, it is not
entirely clear where the description of the graph-
ical content should be given. The text of scien-
tific articles normally makes explicit references to
any graphs contained in the document; in this case,
it makes sense to insert the graphical description
alongside the first such reference. However, popular
media articles rarely contain explicit references to
graphics. We hypothesize that describing the graphi-
cal content together with the most relevant portion of
the article text will result in a more coherent presen-
tation. Results of an experiment described in Sec-
tion 3.3 suggest the paragraph which is geograph-
ically closest to the graphic is very often not rele-
vant. Thus, our task becomes identifying the portion
of the text that is most relevant to the graph.
We have developed a method for identifying the
most relevant paragraph by measuring the similarity
between the graphic?s textual components and the
content of each individual paragraph in the docu-
ment. An information graphic?s textual components
may consist of a title, caption, and any additional
descriptions it contains (e.g., the five lines of text in
Figure 1 beneath the caption Ocean levels rising).
An initial method (P-KL) based on KL divergence
measures the similarity between a paragraph and the
graphic?s textual component; a second method (P-
KLA) is an extension of the first that incorporates
an augmented version of the textual component.
3.1 Method P-KL: KL Divergence
Kullback-Leibler (KL) divergence (Kullback, 1968)
is widely used to measure the similarity between two
language models. It can be expressed as:
DKL(p||q) =
?
i?V
p(i)log
p(i)
q(i)
where i is the index of a word in vocabulary V , and
p and q are two distributions of words. Liu et al
(Liu and Croft, 2002) applied KL divergence to text
passages in order to improve the accuracy of docu-
ment retrieval. For our task, p is a smoothed word
distribution built from the line graph?s textual com-
ponent, and q is another smoothed word distribution
built from a paragraph in the article text. Smoothing
addresses the problem of zero occurrences of a word
in the distributions. We rank the paragraphs by their
KL divergence scores from lowest to highest, since
lower scores indicate a higher similarity.
3.2 Method P-KLA: Using Augmented Text
In analyzing paragraphs relevant to the graphics, we
realized that they included words that were germane
to describing information graphics in general, but
not related to the domains of individual graphs. This
led us to build a set of ?expansion words? that tend to
appear in paragraphs relevant to information graph-
ics. If we could identify domain-independent terms
that were correlated with information graphics in
general, these expansion words could then be added
to the textual component of a graphic when measur-
ing its similarity to a paragraph in the article text.
We constructed the expansion word set using an
iterative process. The first step is to use P-KL to
identify m pseudo-relevant paragraphs in the cor-
responding document for each graphic in the train-
ing set (the current implementation uses m = 3).
This is similar to pseudo-relevance feedback used in
IR (Zhai, 2008), except only a single query is used
in the IR application, whereas we consider many
pairs of graphics and documents to obtain an ex-
pansion set applicable to any subsequent informa-
tion graphic. Given n graphics in the training set,
we identify (up to) m ? n relevant paragraphs.
The second step is to extract a set of words re-
lated to information graphics from these m ?n para-
graphs. We assume the collection of pseudo-relevant
paragraphs was generated by two models, one pro-
ducing words relevant to the information graphics
and another producing words relevant to the topics
of the individual documents. Let Wg represent the
word frequency vector yielding words relevant to
the graphics, Wa represent the word frequency vec-
tor yielding words relevant to the document topics,
and Wp represent the word frequency vector of the
54
pseudo-relevant paragraphs. We compute Wp from
the pseudo-relevant paragraphs themselves, and we
estimate Wa using the word frequencies from the
article text in the documents. Finally, we compute
Wg by filtering-out the components ofWa fromWp.
This process is related to the work by Widdows
(2003) on orthogonal negation of vector spaces.
The task can be formulated as follows:
1. Wp = ?Wa + ?Wg where ? > 0 and ? > 0,
which means the word frequency vector for
the pseudo-relevant paragraphs is a linear com-
bination of the background (topic) word fre-
quency vector and the graphic word vector.
2. < Wa,Wg >= 0 which means the background
word vector is orthogonal to the graph descrip-
tion word vector, under the assumption that the
graph description word vector is independent of
the background word vector and that these two
share minimal information.
3. Wg is assumed to be a unit vector, since we are
only interested in the relative rank of the word
frequencies, not their actual values.
Solving the above equations, we obtain:
? =
< Wp,Wa >
< Wa,Wa >
Wg = normalized
(
Wp ?
< Wp,Wa >
< Wa,Wa >
?Wa
)
After computing Wg, we use WordNet to filter-
out words having a predominant sense other than
verb or adjective, under the assumption that nouns
will be mainly relevant to the domains or topics
of the graphs (and are thus ?noise?) whereas we
want a general set of words (e.g., ?increasing?)
that are typically used when describing the data in
any graph. As a rough estimate of whether a word
is predominantly a verb or adjective, we determine
whether there are more verb and adjective senses of
the word in WordNet than there are noun senses.
Next, we rank the words in the filteredWg accord-
ing to frequency and select the k most frequent as
our expansion word list (we used k = 25 in our ex-
periments). The two steps (identifyingm?n pseudo-
relevant paragraphs and then extracting a word list of
size k to expand the graphics? textual components)
are applied iteratively until convergence occurs or
minimal changes are observed between iterations.
In addition, parameters of the intended message
that represent points on the x-axis capture domain-
specific content of the graphic?s communicative
goal. For example, the intended message of the line
graph in Figure 1 conveys a changing trend from
1900 to 2003 with the change occurring in 1930. To
help identify relevant paragraphs mentioning these
years, we also add these parameters of the intended
message to the augmented word list.
The result of this process is the final expansion
word list used in method P-KLA. Because the tex-
tual component may be even shorter than the expan-
sion word list, we do not add a word from the expan-
sion word list to the textual component unless the
paragraph being compared also contains this word.
3.3 Results of P-KL and P-KLA
334 training graphs with their accompanying articles
were used to build the expansion word set. A sepa-
rate set of 66 test graphs and articles was analyzed
by two human annotators who identified the para-
graphs in each document that were most relevant to
its associated information graphic, ranking them in
terms of relevance. On average, annotator 1 selected
2.00 paragraphs and annotator 2 selected 1.71 para-
graphs. The annotators agreed on the top ranked
paragraph for only 63.6% of the graphs. Consid-
ering the agreement by chance, we can calculate the
kappa statistic as 0.594. This fact shows that the
most relevant paragraph is not necessarily obvious
and multiple plausible options may exist.
We applied both P-KL and P-KLA to the test set,
with each method producing a list of the paragraphs
ranked by relevance. Since our goal is to provide
the summary of the graphic at a suitable point in the
article text, two evaluation criteria are appropriate:
1. TOP: the method?s success rate in selecting
the most relevant paragraph, measured as how
often it chooses the paragraph ranked highest
by either of the annotators
2. COVERED: the method?s success rate in se-
lecting a relevant paragraph, measured as how
often it chooses one of the relevant paragraphs
identified by the annotators
Table 1 provides the success rates of both of our
methods for the TOP and COVERED criteria, along
with a simple baseline that selected the paragraph
55
geographically-closest to the graphic. These results
show that both methods outperform the baseline,
and that P-KLA further improves on P-KL. P-KLA
selects the best paragraph in 60.6% of test cases,
and selects a relevant paragraph in 71.2% of the
cases. For both TOP and COVERED, P-KLA nearly
doubles the baseline success rate. The improve-
ment of P-KLA over P-KL suggests that our expan-
sion set successfully adds salient words to the tex-
tual component. A one-sided Z-test for proportion
based on binomial distribution is shown in Table 1
and indicates that the improvements of P-KL over
the baseline and P-KLA over P-KL are statistically-
significant at the 0.05 level across both criteria. The
Z-test is calculated as:
p? p0
?
p0(1?p0)
n
where p0 is the lower result and p is the improved
result. The null hypothesis is H0 : p = p0 and the
alternative hypothesis is H1 : p > p0.
3.4 Using relevant paragraph identification to
improve the accessibility of line graphs
Our system improves on SIGHT by using method
P-KLA to identify the paragraph that is most rele-
vant to an information graphic. When this paragraph
is encountered, the user is asked whether he or she
would like to access the content of the graphic. For
example, our system identifies the following para-
graph as most relevant to Figure 2:
Doing so likely would require the com-
pany to bring in a new model. Sales of
the Durango and other gas-guzzling SUVs
have slumped in recent years as prices at
the pump spiked.
In contrast, the geographically-closest paragraph has
little relevance to the graphic:
?We have three years to prove to them
we need to stay open,? said Sam Latham,
president of the AFL-CIO in Delaware,
who retired from Chrysler after 39 years.
4 Identifying Additional Propositions
After the intended message has been identified, the
system next looks to identify elaborative informa-
tional propositions that are salient in the graphic.
These additional propositions expand on the initial
description of the graph by filling-in details about
the knowledge being conveyed (e.g., noteworthy
points, properties of trends, visual features) in order
to round-out a summary of the graphic.
We collected a corpus of 965 human-written sum-
maries for 23 different line graphs to discover which
propositions were deemed most salient under varied
conditions.2 Subjects received an initial description
of the graph?s intended message, and were asked to
write additional sentences capturing the most impor-
tant information conveyed by the graph. The propo-
sitions appearing in each summary were manually
coded by an annotator to determine which were most
prevalent. From this data, we developed rules to
identify important propositions in new graphs. The
rules assign weights to propositions indicating their
importance, and the weights can be compared to de-
cide which propositions to include in a summary.
Three types of rules were built. Type-1 (message
category-only) rules were created when a plurality
of summaries for all graphs having a given intended
message contained the same proposition (e.g., pro-
vide the final value for all rising-trend and falling-
trend graphs). Weights for type-1 rules were based
on the frequency with which the proposition ap-
peared in summaries for graphs in this category.
Type-2 (visual feature-only) rules were built when
there was a correlation between a visual feature and
the use of a proposition describing that feature, re-
gardless of the graph?s message category (e.g., men-
tion whether the graph is highly volatile). Type-2
rule weights are a function of the covariance be-
tween the magnitude of the visual feature (e.g., de-
gree of volatility) and the proportion of summaries
mentioning this proposition for each graph.
For propositions associated with visual features
linked to a particular message category (e.g., de-
scribe the trend immediately following a big-jump
or big-fall when it terminates prior to the end of the
graph), we constructed Type-3 (message category
+ visual feature) rules. Type-3 weights were cal-
culated just like Type-2 weights, except the graphs
were limited to the given category.
As an example of identifying additional proposi-
2This corpus is described in greater detail by Greenbacker et
al. (2011) and is available at www.cis.udel.edu/~mccoy/corpora
56
closest P-KL significance level over closest P-KLA significance level over P-KL
TOP 0.272 0.469 (z = 3.5966, p < 0.01) 0.606 (z = 2.2303, p < 0.025)
COVERED 0.378 0.606 (z = 3.8200, p < 0.01) 0.712 (z = 1.7624, p < 0.05)
Table 1: Success rates for baseline method (?closest?), P-KL, and P-KLA using the TOP and COVERED criteria.
tions, consider Figures 1 and 2. Both line graphs
belong to the same intended message category:
change-trend. However, the graph in Figure 1 is far
more volatile than Figure 2, and thus it is likely that
we would want to mention this proposition (i.e., ?the
graph shows a high degree of volatility...?) in a sum-
mary of Figure 1. By finding the covariance between
the visual feature (i.e., volatility) and the frequency
with which a corresponding proposition was anno-
tated in the corpus summaries, a Type-2 rule assigns
a weight to this proposition based on the magnitude
of the visual feature. Thus, the volatility proposi-
tion will be weighted strongly for Figure 1, and will
likely be selected to appear in the initial summary,
while the weight for Figure 2 will be very low.
5 Integrating Text and Graphics
Until now, our system has only produced summaries
for the graphical content of multimodal documents.
However, a user might prefer a summary of the en-
tire document. Possible use cases include examining
this summary to decide whether to invest the time re-
quired to read a lengthy article with a screen reader,
or simply addressing the common problem of having
too much material to review in too little time (i.e.,
information overload). We are developing a system
extension that will allow users to request summaries
of arbitrary length that cover both the text and graph-
ical content of a multimodal document.
Graphics in popular media convey a message that
is generally not repeated in the article text. For ex-
ample, the March 3, 2003 issue of Newsweek con-
tained an article entitled, ?The Black Gender Gap,?
which described the professional achievements of
black women. It included a line graph (Figure 3)
showing that the historical gap in income equality
between white women and black women had been
closed, yet this important message appears nowhere
in the article text. Other work in multimodal doc-
ument summarization has relied on image captions
and direct references to the graphic in the text (Bha-
tia et al, 2009); however, these textual elements do
Figure 3: From ?The Black Gender Gap? in Newsweek,
Mar 3, 2003.
not necessarily capture the message conveyed by in-
formation graphics in popular media. Thus, the user
may miss out on an essential component of the over-
all communicative goal of the document if the sum-
mary covers only material presented in the text.
One approach to producing a summary of the en-
tire multimodal document might be to ?concatenate?
a traditional extraction-based summary of the text
(Kupiec et al, 1995; Witbrock and Mittal, 1999)
with the description generated for the graphics by
our existing system. The summary of the graphi-
cal content could be simply inserted wherever it is
deemed most relevant in the text summary. How-
ever, such an approach would overlook the relation-
ships and interactions between the text and graphical
content. The information graphics may make certain
concepts mentioned in the text more salient, and vice
versa. Unless we consider the contributions of both
the text and graphics together during the content se-
lection phase, the most important information might
not appear in the summary of the document.
Instead, we must produce a summary that inte-
grates the content conveyed by the text and graphics.
We contend that this integration must occur at the se-
mantic level if it is to take into account the influence
of the graphic?s content on the salience of concepts
in the text and vice versa. Our tack is to first build
a single semantic model of the concepts expressed
in both the article text and information graphics, and
then use this model as the basis for generating an
abstractive summary of the multimodal document.
57
Drawing from a model of the semantic content of the
document, we select as many or as few concepts as
we wish, at any level of detail, to produce summaries
of arbitrary length. This will permit the user to re-
quest a quick overview in order to decide whether to
read the original document, or a more comprehen-
sive synopsis to obtain the most important content
without having to read the entire article.
5.1 Semantic Modeling of Multimodal
Documents
Content gathered from the article text by a seman-
tic parser and from the information graphics by
our graph understanding system is combined into
a single semantic model based on typed, struc-
tured objects organized under a foundational ontol-
ogy (McDonald, 2000a). For the semantic pars-
ing of text, we use Sparser (McDonald, 1992), a
bottom-up, phrase-structure-based chart parser, op-
timized for semantic grammars and partial parsing.3
Using a built-in model of core English grammar
plus domain-specific grammars, Sparser extracts in-
formation from the text and produces categorized
objects as a semantic representation (McDonald,
2000b). The intended message and salient additional
propositions identified by our system for the infor-
mation graphics are decomposed and added to the
model constructed by Sparser.4
Model entries contain slots for attributes in the
concept category?s ontology definition (fillable by
other concepts or symbols), the original phrasings
mentioning this concept in the text (represented as
parameterized synchronous TAG derivation trees),
and markers recording document structure (i.e.,
where in the text [including title, headings, etc.] or
graphic the concept appeared). Figure 4 shows some
of the information contained in a small portion of
the semantic model built for an article entitled ?Will
Medtronic?s Pulse Quicken?? from the May 29,
2006 edition of Businessweek magazine5, which in-
cluded a line graph. Nodes correspond to concepts
3https://github.com/charlieg/Sparser
4Although the framework is general enough to accommo-
date any modality (e.g., images, video) given suitable seman-
tic analysis tools, our prototype implementation focuses on bar
charts and line graphs analyzed by SIGHT.
5http://www.businessweek.com/magazine/
content/06_22/b3986120.htm
and edges denote relationships between concepts;
dashed lines indicate links to concepts not shown in
this figure. Nodes are labelled with the name of the
conceptual category they instantiate, and a number
to distinguish between individuals. The middle of
each box displays the attributes of the concept, while
the bottom portion shows some of the original text
phrasings. Angle brackets (<>) note references to
other concepts, and hash marks (#) indicate a sym-
bol that has not been instantiated as a concept.
P1S1: "medical device
    giant Medtronic"
P1S5: "Medtronic"
Name: "Medtronic"
Stock: "MDT"
Industry: (#pacemakers,
    #defibrillators,
    #medical devices)
Company1
P1S4: "Joanne
    Wuensch"
P1S7: "Wuensch"
FirstName: "Joanne"
LastName: "Wuensch"
Person1
P1S4: "a 12-month
    target of 62"
Person: <Person 1>
Company: <Company 1>
Price: $62.00
Horizon: #12_months
TargetStockPrice1
Figure 4: Detail of model for Businessweek article.
5.2 Rating Content in Semantic Models
The model is then rated to determine which items are
most salient. The concepts conveying the most in-
formation and having the most connections to other
important concepts in the model are the ones that
should be chosen for the summary. The importance
of each concept is rated according to a measure of
information density (ID) involving several factors:6
Saturation Level Completeness of attributes in
model entry: a concept?s filled-in slots (f ) vs. its
total slots (s), and the importance of the concepts
(ci) filling those slots:
f
s ? log(s) ?
?f
i=1 ID(ci)
Connectedness Number of connections (n) with
other concepts (cj), and the importance of these con-
nected concepts:
?n
j=1 ID(cj)
Frequency Number of observed phrasings (e) re-
alizing the concept in text of the current document
Prominence in Text Prominence based on docu-
ment structure (WD) and rhetorical devices (WR)
Graph Salience Salience assessed by the graph
understanding system (WG) ? only applies to con-
cepts appearing in the graphics
6The first three factors are similar to the dominant slot
fillers, connectivity patterns, and frequency criteria described
by Reimer and Hahn (1988).
58
Saturation corresponds to the completeness of the
concept in the model. The more attribute slots that
are filled, the more we know about a particular con-
cept instance. However, this measure is highly sen-
sitive to the degree of detail provided in the seman-
tic grammar and ontology class definition (whether
created by hand or automatically). A concept having
two slots, both of which are filled-out, is not neces-
sarily more important than a concept with only 12
of its 15 slots filled. The more important a concept
category is in a given domain, the more detailed its
ontology class definition will likely be. Thus, we
can assume that a concept definition having a dozen
or more slots is, broadly speaking, more important
in the domain than a less well-defined concept hav-
ing only one or two slots. This insight is the basis of
a normalization factor (log(s)) used in ID.
Saturation differs somewhat from repetition in
that it attempts to measure the amount of informa-
tion associated with a concept, rather than simply
the number of times a concept is mentioned in the
text. For example, a news article about a proposed
law might mention ?Washington? several times, but
the fact that the debate took place in Washington,
D.C. is unlikely to be an important part of the article.
However, the key provisions of the bill, which may
individually be mentioned only once, are likely more
important as a greater amount of detail is provided
concerning them. Simple repetition is not necessar-
ily indicative of the importance of a concept, but if a
large amount of information is provided for a given
concept, it is safe to assume the concept is important
in the context of that document.
Document structure (WD) is another important
clue in determining which elements of a text are
important enough to include in a summary (Marcu,
1997). If a concept is featured prominently in the
title, or appears in the first or final paragraphs, it is
likely more important than a concept buried in the
middle of the document. Importance is also affected
by certain rhetorical devices (WR) which serve to
highlight particular concepts. Being used in an id-
iom, or compared to another concept by means of
juxtaposition suggests that a given concept may hold
special significance. Finally, the weights assigned
by our graph understanding system for the additional
propositions identified in the graphics are incorpo-
rated into the ID of the concepts involved as WG.
5.3 Selecting Content for a Summary
To select concepts for inclusion in the summary,
the model will then be passed to a discourse-aware
graph-based content selection framework (Demir et
al., 2010), which selects concepts one at a time
and iteratively re-weights the remaining items so
as to include related concepts and avoid redun-
dancy. This algorithm incorporates PageRank (Page
et al, 1999), but with several modifications. In ad-
dition to centrality assessment based on relation-
ships between concepts, it includes apriori impor-
tance nodes enabling us to incorporate concept com-
pleteness, number of expressions, document struc-
ture, and rhetorical devices. More importantly from
a summary generation perspective, the algorithm it-
eratively picks concepts one at a time, and re-ranks
the remaining entries by increasing the weight of re-
lated items and discounting redundant ones. This
allows us to select concepts that complement each
other while simultaneously avoiding redundancy.
6 Generating an Abstractive Summary of
a Multimodal Document
Figure 4 shows the two most important concepts
(Company1 & Person1) selected from the Medtronic
article in Section 5.1. Following McDonald and
Greenbacker (2010), we use the phrasings observed
by the parser as the ?raw material? for expressing
these selected concepts. Reusing the original phras-
ings reduces the reliance on built-in or ?canned?
constructions, and allows the summary to reflect the
style of the original text. The derivation trees stored
in the model to realize a particular concept may use
different syntactic constituents (e.g., noun phrases,
verb phrases). Multiple trees are often available for
each concept, and we must select particular trees that
fit together to form a complete sentence.
The semantic model also contains concepts rep-
resenting propositions extracted from the graphics,
as well as relationships connecting these graphical
concepts with those derived from the text, and there
are no existing phrasings in the original document
that can be reused to convey this graphical content.
However, the set of proposition types that can be ex-
tracted from the graphics is finite. To ensure that we
have realizations for every concept in our model, we
create TAG derivation trees for each type of graphi-
59
cal proposition. As long as realizations are supplied
for every proposition that can be decomposed in the
model, our system will never be stuck with a concept
without the means to express it.
The set of expressions is augmented by many
built-in realizations for common semantic relation-
ships (e.g., ?is-a,? ?has-a?), as well as expressions
inherited from other conceptual categories in the hi-
erarchy. If the observed expressions are retained as
the system analyzes multiple documents over time,
making these realizations available for later use by
concepts in the same category, the variety of utter-
ances we can generate is increased greatly.
By using synchronous TAG trees, we know that
the syntactic realizations of two semantically-related
concepts will fit together syntactically (via substitu-
tion or adjunction). However, the concepts selected
for the summary of the Medtronic article (Com-
pany1 & Person1), are not directly connected in the
model. To produce a single summary sentence for
these two concepts, we must find a way of express-
ing them together with the available phrasings. This
can be accomplished by using an intermediary con-
cept that connects both of the selected items in the
semantic model, in order to ?bridge the gap? be-
tween them. In this example, a reasonable option
would be TargetStockPrice1, one of the many con-
cepts linking Company1 and Person1. Combining
original phrasings from all three concepts (via sub-
stitution and adjunction operations on the underly-
ing TAG trees), along with a ?built-in? realization
inherited by the TargetStockPrice category (a sub-
type of Expectation), yields this surface form:
Wuensch expects a 12-month target of 62
for medical device giant Medtronic.
7 Related Work
Research into providing alternative access to graph-
ics has taken both verbal and non-verbal approaches.
Kurze (1995) presented a verbal description of the
properties (e.g., diagram style, number of data sets,
range and labels of axes) of business graphics. Fer-
res et al (2007) produced short descriptions of the
information in graphs using template-driven genera-
tion based on the graph type. The SIGHT project
(Demir et al, 2008; Elzer et al, 2011) generated
summaries of the high-level message content con-
veyed by simple bar charts. Other modalities, like
sound (Meijer, 1992; Alty and Rigas, 1998; Choi
and Walker, 2010) and touch (Ina, 1996; Krufka et
al., 2007), have been used to impart graphics via a
substitute medium. Yu et al (2002) and Abu Doush
et al (2010) combined haptic and aural feedback,
enabling users to navigate and explore a chart.
8 Discussion
This paper presented our system for providing ac-
cess to the full content of multimodal documents
with line graphs in popular media. Such graph-
ics generally have a high-level communicative goal
which should constitute the core of a graphic?s sum-
mary. Rather than providing this summary at the
point where the graphic is first encountered, our sys-
tem identifies the most relevant paragraph in the
article and relays the graphic?s summary at this
point, thus increasing the presentation?s coherence.
System extensions currently in development will
provide a more integrative and accessible way for
visually-impaired readers to experience multimodal
documents. By producing abstractive summaries of
the entire document, we reduce the amount of time
and effort required to assimiliate the information
conveyed by such documents in popular media.
Several tasks remain as future work. The intended
message descriptions generated by our system need
to be evaluated by both sighted and non-sighted hu-
man subjects for clarity and accuracy. We intend
to test our hypothesis that graphics ought to be de-
scribed alongside the most relevant part of the text
by performing an experiment designed to determine
the presentation order preferred by people who are
blind. The rules developed to identify elaborative
propositions also must be validated by a corpus or
user study. Finally, once the system is fully imple-
mented, the abstractive summaries generated for en-
tire multimodal documents will need to be evaluated
by both sighted and sight-impaired judges.
Acknowledgments
This work was supported in part by the by the Na-
tional Institute on Disability and Rehabilitation Re-
search under grant H133G080047 and by the Na-
tional Science Foundation under grant IIS-0534948.
60
References
Iyad Abu Doush, Enrico Pontelli, Tran Cao Son, Dominic
Simon, and Ou Ma. 2010. Multimodal presenta-
tion of two-dimensional charts: An investigation using
Open Office XML and Microsoft Excel. ACM Trans-
actions on Accessible Computing (TACCESS), 3:8:1?
8:50, November.
James L. Alty and Dimitrios I. Rigas. 1998. Communi-
cating graphical information to blind users using mu-
sic: the role of context. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems,
CHI ?98, pages 574?581, Los Angeles, April. ACM.
Sumit Bhatia, Shibamouli Lahiri, and Prasenjit Mitra.
2009. Generating synopses for document-element
search. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management, CIKM
?09, pages 2003?2006, Hong Kong, November. ACM.
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ?06,
pages 581?588, Seattle, August. ACM.
Stephen H. Choi and Bruce N. Walker. 2010. Digitizer
auditory graph: making graphs accessible to the visu-
ally impaired. In Proceedings of the 28th International
Conference on Human Factors in Computing Systems,
CHI ?10, pages 3445?3450, Atlanta, April. ACM.
Seniz Demir, Sandra Carberry, and Kathleen F. McCoy.
2008. Generating textual summaries of bar charts.
In Proceedings of the 5th International Natural Lan-
guage Generation Conference, INLG 2008, pages 7?
15, Salt Fork, Ohio, June. ACL.
Seniz Demir, Sandra Carberry, and Kathleen F. Mc-
Coy. 2010. A discourse-aware graph-based content-
selection framework. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 17?26, Trim, Ireland, July. ACL.
Michael Elhadad and Jacques Robin. 1996. An overview
of SURGE: a re-usable comprehensive syntactic re-
alization component. In Proceedings of the 8th In-
ternational Natural Language Generation Workshop
(Posters and Demonstrations), Sussex, UK, June.
ACL.
Stephanie Elzer, Sandra Carberry, and Ingrid Zukerman.
2011. The automated understanding of simple bar
charts. Artificial Intelligence, 175:526?555, February.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proceedings of the 9th Inter-
national ACM SIGACCESS Conference on Computers
and Accessibility, ASSETS ?07, pages 67?74, Tempe,
October. ACM.
Charles F. Greenbacker, Sandra Carberry, and Kathleen F.
McCoy. 2011. A corpus of human-written summaries
of line graphs. In Proceedings of the EMNLP 2011
Workshop on Language Generation and Evaluation,
UCNLG+Eval, Edinburgh, July. ACL. (to appear).
Satoshi Ina. 1996. Computer graphics for the blind. SIG-
CAPH Newsletter on Computers and the Physically
Handicapped, pages 16?23, June. Issue 55.
Stephen E. Krufka, Kenneth E. Barner, and Tuncer Can
Aysal. 2007. Visual to tactile conversion of vector
graphics. IEEE Transactions on Neural Systems and
Rehabilitation Engineering, 15(2):310?321, June.
Solomon Kullback. 1968. Information Theory and
Statistics. Dover, revised 2nd edition.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of the 18th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, SIGIR ?95, pages 68?73, Seattle, July. ACM.
Martin Kurze. 1995. Giving blind people access
to graphics (example: Business graphics). In Pro-
ceedings of the Software-Ergonomie ?95 Workshop
on Nicht-visuelle graphische Benutzungsoberfla?chen
(Non-visual Graphical User Interfaces), Darmstadt,
Germany, February.
Xiaoyong Liu and W. Bruce Croft. 2002. Passage re-
trieval based on language models. In Proceedings of
the eleventh international conference on Information
and knowledge management, CIKM ?02, pages 375?
382.
Daniel C. Marcu. 1997. The Rhetorical Parsing, Summa-
rization, and Generation of Natural Language Texts.
Ph.D. thesis, University of Toronto, December.
David D. McDonald and Charles F. Greenbacker. 2010.
?If you?ve heard it, you can say it? - towards an ac-
count of expressibility. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 185?190, Trim, Ireland, July. ACL.
David D. McDonald. 1992. An efficient chart-based
algorithm for partial-parsing of unrestricted texts. In
Proceedings of the 3rd Conference on Applied Natural
Language Processing, pages 193?200, Trento, March.
ACL.
David D. McDonald. 2000a. Issues in the repre-
sentation of real texts: the design of KRISP. In
Lucja M. Iwan?ska and Stuart C. Shapiro, editors, Nat-
ural Language Processing and Knowledge Represen-
tation, pages 77?110. MIT Press, Cambridge, MA.
David D. McDonald. 2000b. Partially saturated refer-
ents as a source of complexity in semantic interpreta-
tion. In Proceedings of the NAACL-ANLP 2000 Work-
shop on Syntactic and Semantic Complexity in Natural
61
Language Processing Systems, pages 51?58, Seattle,
April. ACL.
Peter B.L. Meijer. 1992. An experimental system for
auditory image representations. IEEE Transactions on
Biomedical Engineering, 39(2):112?121, February.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-
66, Stanford InfoLab, November. Previous number:
SIDL-WP-1999-0120.
Ulrich Reimer and Udo Hahn. 1988. Text condensation
as knowledge base abstraction. In Proceedings of the
4th Conference on Artificial Intelligence Applications,
CAIA ?88, pages 338?344, San Diego, March. IEEE.
Dominic Widdows. 2003. Orthogonal negation in vector
spaces for modelling word-meanings and document
retrieval. In Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics - Volume
1, ACL ?03, pages 136?143, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
summarization: a statistical approach to generating
highly condensed non-extractive summaries. In Pro-
ceedings of the 22nd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR ?99, pages 315?316, Berkeley,
August. ACM.
Wai Yu, Douglas Reid, and Stephen Brewster. 2002.
Web-based multimodal graphs for visually impaired
people. In Proceedings of the 1st Cambridge Work-
shop on Universal Access and Assistive Technology,
CWUAAT ?02, pages 97?108, Cambridge, March.
Chengxiang Zhai. 2008. Statistical Language Models
for Information Retrieval. Morgan and Claypool Pub-
lishers, December.
62
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 23?27,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
A Corpus of Human-written Summaries of Line Graphs
Charles F. Greenbacker, Sandra Carberry, and Kathleen F. McCoy
Department of Computer and Information Sciences
University of Delaware, Newark, Delaware, USA
[charlieg|carberry|mccoy]@cis.udel.edu
Abstract
We describe a corpus of human-written En-
glish language summaries of line graphs. This
corpus is intended to help develop a system
to automatically generate summaries captur-
ing the most salient information conveyed by
line graphs in popular media, as well as to
evaluate the output of such a system.
1 Motivation
We are developing a system designed to automati-
cally generate summaries of the high-level knowl-
edge conveyed by line graphs found in multimodal
documents from popular media sources (e.g., mag-
azines, newspapers). Intended applications include
making these graphics more accessible for people
with visual impairments and indexing their infor-
mational content for digital libraries. Information
graphics like line graphs are generally included in
a multimodal document in order to make a point
supporting the overall communicative intent of the
document. Our goal is to produce summaries that
convey the knowledge gleaned by humans when in-
formally viewing the graphic, focusing on the ?take-
away? message rather than the raw data points.1
Studies have shown (Carberry et al, 2006) that
the captions of information graphics in popular me-
dia often do not repeat the message conveyed by the
graphic itself; such captions are thus not appropriate
for use as a summary. Furthermore, while scientific
graphs are designed for experts trained in their use
1Users generally prefer conceptual image descriptions over
perceptual descriptions (Jo?rgensen, 1998; Hollink et al, 2004).
for data visualization, information graphics in pop-
ular media are meant to be understood by all read-
ers, including those with only a primary school ed-
ucation. Accordingly, summaries for these graphics
should be tailored for the same general audience.
Research into information graphics by Wu et al
(2010) has identified a limited number of intended
message categories conveyed by line graphs in pop-
ular media. Their efforts included the creation of a
corpus2 of line graphs marked with the overall in-
tended message identified by human annotators.
However, we hypothesize that an effective sum-
mary should present the graph?s intended message
plus additional informational propositions that elab-
orate on this message. McCoy et al (2001) observed
that the intended message was consistently included
in line graph summaries written by human subjects.
Furthermore, participants in that study augmented
the intended message with descriptions of salient vi-
sual features of the graphic (e.g., steepness of a trend
line, volatility of data values). As part of the pro-
cess of building a system to identify which visual
features are salient and to describe them using nat-
ural language expressions, we collected a corpus of
human-written summaries of line graphs.
2 Building the Corpus
We selected 23 different line graphs for use in build-
ing our corpus. This set covered the eight most-
common intended message categories from the Wu
corpus; only Point Correlation and Stable Trend
were omitted. Table 1 shows the distribution of
2www.cis.udel.edu/~carberry/Graphs/viewallgraphs.php
23
Message Category No. (graphs)
Big Fall (BF) 4 (20?23)
Big Jump (BJ) 2 (18, 19)
Changing Trend (CT) 4 (8?11)
Change Trend Return (CTR) 2 (12, 13)
Contrast Trend with
Last Segment (CTLS)
2 (14, 15)
Contrast Segment with
Changing Trend (CSCT)
2 (16, 17)
Rising Trend (RT) 4 (1?4)
Falling Trend (FT) 3 (5?7)
Total 23 (1?23)
Table 1: Distribution of overall intended message cate-
gories in the set of line graphs used to build the corpus.
graphs across message categories.3 Ten of the line
graphs were real world examples in popular media
taken from the Wu corpus (e.g., Figure 1). Another
ten graphs were adapted from items in the Wu cor-
pus ? modified in order to isolate visual features so
that their individual effects could be analyzed (e.g.,
Figure 2). The remaining three line graphs were cre-
ated specifically to fill a gap in the coverage of in-
tended messages and visual features for which no
good example was available (e.g., Figure 3). Our
goal was to include as many different combinations
of message category and visual features as possible
(e.g., for graphs containing a dramatic change in val-
ues because of a big jump or fall, we included ex-
amples which sustained the change as well as others
that did not sustain the change).
69 subjects participated in our study. All were
native English speakers, 18 years of age or older,
without major sight impairments, and enrolled in an
introductory computer science course at a university
in the US. They received a small amount of extra
credit in their course for participating in this study.
Each participant was given the full set of 23 line
graphs in differing orders. With each graph, the sub-
jects were presented with an initial summary sen-
tence describing the overall intended message of the
graphic, as identified by a human annotator. The
captions for Figures 1, 2, and 3 each contain the cor-
responding initial summary sentence that was pro-
vided to the participants. Participants were tasked
with writing additional sentences so that the com-
3Category descriptions can be found in (Wu et al, 2010).
Figure 1: From ?This Cable Outfit Is Getting Tuned In?
in Businessweek magazine, Oct 4, 1999. (Initial sentence:
?This line graph shows a big jump in Blonder Tongue
Laboratories stock price in August ?99.?)
pleted summary of each line graph captured the most
important information conveyed by the graphic, fin-
ishing as many or as few of the 23 graphs as they
wished during a single one-hour session.
Participants were told that we were developing a
system to convey an initial summary of an informa-
tion graphic from popular media (as opposed to text-
books or scientific articles) to blind users via speech.
We indicated that the summaries they write should
be brief (though we did not specify any length re-
quirements), but ought to include all essential infor-
mation provided by the graphic. Subjects were only
given the graphics and did not receive the original ar-
ticle text (if any existed) that accompanied the real-
world graphs. Finally, the participants were told that
a person able to see the graphics should not think
that the summaries they wrote were misleading.
3 Corpus Characteristics
A total of 965 summaries were collected, ranging
from 37 to 49 summaries for each individual line
graph. Table 2 offers some descriptive statistics for
the corpus as a whole, while Table 3 lists the ten
most commonly-occurring content words.
Sample summary 1 (18-4.txt) was written for Fig-
ure 1, summary 2 (7-40.txt) for Figure 2, and sum-
maries 3 (9-2.txt) and 4 (9-5.txt) both for Figure 3:
24
!Figure 2: Adapted from original in ?Dell goes with a few
AMD chips,? USA Today, Oct 19, 2006. (Initial sentence:
?This line graph shows a falling trend in Dell stock from
May ?05 to May ?06.?)
From March 26, 1999 the graph rises and de-
clines up until August 1999 where it rises at
about a 90-degree angle then declines again.
(1)
The graph peaked in July ?05 but then sharply
decreased after that. It had several sharp in-
clines and declines and ended with a shaper
decline from March ?06 to May ?06.
(2)
February has a much larger amount of jackets
sold than the other months shown. From dec-
ember to january, there was a slight drop in
the amount of jackets sold and then a large
spike from january to february.
(3)
The values in November and May are pretty
close, with both being around 37 or 38
jackets. At its peak (February), around 47
jackets were sold.
(4)
4 Potential Usage
To our knowledge, this is the first and only publicly-
available corpus of line graph summaries. It has sev-
eral possible applications in both natural language
generation and evaluation tasks. By finding and ex-
amining patterns in the summaries, we can discover
which propositions are found to be most salient for
certain kinds of graphs. We are currently analyzing
the collected corpus for this very purpose ? to iden-
tify relationships between visual features, intended
messages, and the relative importance of includ-
ing corresponding propositions in a summary (e.g.,
volatility is more salient in Figure 2 than Figure 3).
!Figure 3: Sample line graph created for this study. (Ini-
tial sentence: ?This line graph shows a rising trend in
Boscov?s jacket sales from November to February fol-
lowed by a falling trend through May.?)
Metric Value
total characters 213,261
total words (w) 45,217
total sentences 2,184
characters per word 4.72
words per sentence 20.70
sentences per summary 2.26
unique words (u) 1,831
lexical diversity (w/u) 24.70
hapax legomena 699
pct. of unique words 38.18%
pct. of total words 1.55%
Table 2: Various descriptive statistics for the corpus.
Not only does this corpus offer insight into what
humans perceive to be the most important informa-
tion conveyed by line graphs, it provides a large set
of real-world expressions from which to draw when
crafting the surface realization forms for summaries
of line graphs. From a generation perspective, this
collection of summaries offers copious examples of
the expressions human use to describe characteris-
tics of information graphics. The corpus could also
be used to determine the proper structural character-
istics of a line graph summary (e.g., when multiple
information is included, how propositions are aggre-
gated into sentences, which details come first).
The evaluation of graph understanding systems
will also benefit from the use of this corpus. It will
enable comparisons between system and human-
25
Word Count Word Count
graph 715 stock 287
price 349 increase 280
august 305 may 279
dollars 300 decrease 192
around 299 trend 183
Table 3: The ten most frequently occurring words in the
corpus (omitting stopwords and punctuation).
generated descriptions at the propositional (content)
level, as well as judgments involving clarity and co-
herence. The set of summaries for each graph may
be used as a ?gold standard? against which to com-
pare automatically-generated summaries in prefer-
ence judgment experiments involving human judges.
We are currently developing rules for identifying
the most salient information conveyed by a given
line graph based on an analysis of this corpus, and
will also use the expressions in the collected sum-
maries as examples for surface realization during the
summary generation process. Additionally, we are
planning to use the corpus during part of the evalu-
ation phase of our project, by asking human judges
to compare these human-written summaries against
our system?s output across multiple dimensions of
preference. It may also be useful to perform some
additional human subjects experiments to determine
which summaries in the corpus are found to be most
helpful and understandable.
5 Related Work
Prior to this study, we performed an initial investi-
gation based on a questionnaire similar to the one
used by Demir (2010) for bar charts. A group of
human subjects was asked to review several line
graphs and indicate how important it would be to
include various propositions in an initial summary
of each graphic. Although this method was effec-
tive with bar charts, it proved to be far too cumber-
some to work with line graphs. Bar charts are some-
what simpler, propositionally-speaking, as there are
fewer informational propositions that can be ex-
tracted from data represented as discrete bars rather
than as a continuous data series in a line graph.
It required far more effort for subjects to evaluate
the relative importance of each individual proposi-
tion than to simply provide (in the form of a writ-
ten summary) the set of propositions they consid-
ered to be most important. In the end, the summary-
based approach allowed for a more direct exami-
nation of salience judgments without subjects be-
ing constrained or influenced by the questions and
structure of the questionnaire-based approach, with
the added bonus of producing a reusable corpus of
human-written summaries of line graphs.
McCoy et al (2001) performed a study in which
participants were asked to write brief summaries for
a series of line graphs. While they did not release
a corpus for distribution, their analysis did suggest
that a graph?s visual features could be used to help
select salient propositions to include in a summary.
Although several corpora exist for general im-
age descriptions, we are unaware of any other cor-
pora of human-written summaries for information
graphics. Jo?rgensen (1998) collected unconstrained
descriptions of pictorial images, while Hollink et
al. (2004) analyzed descriptions of mental images
formed by subjects to illustrate a given text pas-
sage. Aker and Gaizauskas (2010) built a corpus of
human-generated captions for location-related im-
ages. Large collections of general image captions
have been assembled for information retrieval tasks
(Smeaton and Quigley, 1996; Tribble, 2010). Roy
(2002) evaluated automatically-generated descrip-
tions of visual scenes against human-generated de-
scriptions. The developers of the iGraph-Lite system
(Ferres et al, 2007) released a corpus of descrip-
tions for over 500 graphs collected from Statistics
Canada, but these descriptions were generated auto-
matically by their system and not written by human
authors. Additionally, the descriptions contained in
their corpus focus on the quantitative data presented
in the graphics rather than the high-level message,
and tend to vary only slightly between graphs.4
Since using corpus texts as a ?gold standard? in
generation and evaluation can be tricky (Reiter and
Sripada, 2002), we tried to mitigate some of the
common problems, including giving participants as
much time as they wanted for each summary to
avoid ?hurried writing.? However, as we intend to
use this corpus to understand which propositions hu-
mans find salient for line graphs, as well as generat-
4The iGraph-Lite system provides the same information for
each instance of a graph type (i.e., all summaries of line graphs
contain the same sorts of information).
26
ing and evaluating new summaries, a larger collec-
tion of examples written by many authors for several
different graphics was more desirable than a smaller
corpus of higher-quality texts from fewer authors.
6 Availability
The corpus is freely available for download5 without
restrictions under an open source license.
The structure of the corpus is as follows. The
?summaries? directory consists of a series of subdi-
rectories numbered 1-23 containing the summaries
for all 23 line graphs, with each summary stored in
a separate file (encoded as ASCII text). The files
are named according to the graph they are associ-
ated with and their position in that graph?s collec-
tion (e.g., 8-10.txt is the 10th summary for the 8th
line graph, and is located in the directory named 8).
The root of the distribution package contains a
directory of original image files for the line graphs
(named ?line graphs?), the initial sentences describ-
ing each graph?s intended message (which was pro-
vided to the participants) in sentences.txt, and a
README file describing the corpus layout.
The corpus is easily loaded with NLTK (Loper
and Bird, 2002) using these Python commands:
from nltk.corpus import PlaintextCorpusReader
LGSroot = './LGSummaryCorpus/summaries'
corpus = PlaintextCorpusReader(LGSroot, '.*')
Acknowledgments
This work was supported in part by the National In-
stitute on Disability and Rehabilitation Research un-
der Grant No. H133G080047.
References
Ahmet Aker and Robert Gaizauskas. 2010. Model sum-
maries for location-related images. In Proceedings
of the Seventh Conference on International Language
Resources and Evaluation, LREC ?10, pages 3119?
3124, Malta, May. ELRA.
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ?06,
pages 581?588, Seattle, August. ACM.
5www.cis.udel.edu/~mccoy/corpora
Seniz Demir. 2010. SIGHT for Visually Impaired Users:
Summarizing Information Graphics Textually. Ph.D.
thesis, University of Delaware, February.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proceedings of the 9th Inter-
national ACM SIGACCESS Conference on Computers
and Accessibility, ASSETS ?07, pages 67?74, Tempe,
October. ACM.
L. Hollink, A. Th. Schreiber, B. J. Wielinga, and M. Wor-
ring. 2004. Classification of user image descriptions.
International Journal of Human-Computer Studies,
61(5):601?626, November.
Corinne Jo?rgensen. 1998. Attributes of images in de-
scribing tasks. Information Processing and Manage-
ment, 34:161?174, March?May.
Edward Loper and Steven Bird. 2002. NLTK: The nat-
ural language toolkit. In Proceedings of the ACL-02
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Compu-
tational Linguistics, pages 63?70, Philadelphia, July.
ACL.
Kathleen F. McCoy, M. Sandra Carberry, Tom Roper,
and Nancy Green. 2001. Towards generating textual
summaries of graphs. In Proceedings of the 1st Inter-
national Conference on Universal Access in Human-
Computer Interaction, UAHCI 2001, pages 695?699,
New Orleans, August. Lawrence Erlbaum.
Ehud Reiter and Somayajulu Sripada. 2002. Should cor-
pora texts be gold standards for NLG? In Proceed-
ings of the Second International Conference on Natu-
ral Language Generation, INLG 2002, pages 97?104,
Harriman, New York, July. ACL.
Deb K. Roy. 2002. Learning visually grounded words
and syntax for a scene description task. Computer
Speech & Language, 16(3?4):353?385, July?October.
Alan F. Smeaton and Ian Quigley. 1996. Experiments on
using semantic distances between words in image cap-
tion retrieval. In Proceedings of the 19th Annual Inter-
national ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, SIGIR ?96, pages
174?180, Zurich, August. ACM.
Alicia Tribble. 2010. Textual Inference for Retrieving
Labeled Object Descriptions. Ph.D. thesis, Carnegie
Mellon University, April.
Peng Wu, Sandra Carberry, Stephanie Elzer, and Daniel
Chester. 2010. Recognizing the intended message
of line graphs. In Proceedings of the Sixth Interna-
tional Conference on the Theory and Application of
Diagrams, Diagrams 2010, pages 220?234, Portland,
Oregon, August. Springer-Verlag.
27
Proceedings of the 8th International Natural Language Generation Conference, pages 64?73,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
 
 
Adapting Graph Summaries to the Users? Reading Levels 
 
 
Priscilla Moraes, Kathleen McCoy and Sandra Carberry 
Department of Computer and Information Sciences 
University of Delaware, Newark, Delaware, USA 
[pmoraes | mccoy | carberry]@udel.edu 
 
  
 
Abstract 
Deciding on the complexity of a generated text 
in NLG systems is a contentious task. Some 
systems propose the generation of simple text 
for low-skilled readers; some choose what 
they anticipate to be a ?good measure? of 
complexity by balancing sentence length and 
number of sentences (using scales such as the 
D-level sentence complexity) for the text; 
while others target high-skilled readers. In this 
work, we discuss an approach that aims to lev-
erage the experience of the reader when read-
ing generated text by matching the syntactic 
complexity of the generated text to the reading 
level of the surrounding text. We propose an 
approach for sentence aggregation and lexical 
choice that allows generated summaries of line 
graphs in multimodal articles available online 
to match the reading level of the text of the ar-
ticle in which the graphs appear. The tech-
nique is developed in the context of the 
SIGHT (Summarizing Information Graphics 
Textually) system. This paper tackles the mi-
cro planning phase of sentence generation dis-
cussing additionally the steps of lexical 
choice, and pronominalization.  
1 Introduction 
Multimodal documents from online popular me-
dia often contain information graphics that aug-
ment the information found in the text. These 
graphics, however, are inaccessible to blind us-
ers.  The SIGHT system is an ongoing project 
that proposes methods of making this infor-
mation accessible to visually impaired users by 
generating a textual summary capturing the high-
level message of the graphic along with visually 
distinctive features. Figure 1 shows an example 
of an information graphic found in popular me-
dia. This graphic ostensibly conveys that there 
was a change in the trend of ocean levels, which 
is first stable until about 1940 and then rising 
through 2003. Earlier work on the system (Wu, 
Carberry, Elzer, & Chester, 2010) is able to infer 
this high-level message given a representation of 
the graphic. 
Nevertheless, a generated summary should 
convey more than just the intended message. It 
should provide important visual features that 
jump out at a person who views the graphic 
(such as the fluctuation in the data values as seen 
in the graph in Figure 1). The set of remarkable 
features is different for different graphics. Previ-
ous work of ours (Moraes, Carberry, & McCoy, 
2013) presents methods that capture these most 
important features and allow the composition of 
customized summaries for each graph. Thus, 
given a graphic, our previous work has resulted 
in a system that can produce a set of propositions 
to include in its summary. In this paper, we turn 
to the subsequent phases of generation: given a 
set of propositions, how these propositions 
should be realized such that the resultant text is 
adapted to the user?s reading level and thus is 
coherent and understandable. 
Therefore, this work presents novel strategies 
that have been deployed in the text generation 
phase of the SIGHT system applied to line 
graphs. It describes the micro planning phase, 
emphasizing sentence aggregation, lexical choice 
and pronominalization. The contribution of this 
work is the provision of coherent and concise 
textual summaries that narrate line graphs? high-
level content to visually impaired users through 
approaches that rely on 1) making the right 
wording choices and 2) making appropriate syn-
tactical decisions in order to achieve a desired 
reading level for the generated text. 
Previous work in generation assumes a partic-
ular level of complexity for all texts created. Our 
hypothesis is that the graph?s summary should 
vary depending on the user?s reading level.  Alt-
hough one could explicitly inquire about the us-
er?s reading level, this would be intrusive and 
64
  
would detract from the overall experience.  Thus 
we hypothesize that the level of complexity of 
the article in which the graph appears roughly 
equates with the user?s reading level --- that is, 
users generally choose articles that are at their 
own reading comfort level.  Therefore, our ap-
proach is to generate summaries that reflect the 
reading level of the accompanying article.  Not 
only will such summaries be coherent and under-
standable to the user, but also the summary 
should fit seamlessly into the user?s reading of 
the article. 
The decision to match the text complexity of 
the generated text to that of the article?s text was 
inspired by results of an experiment performed 
with college students aiming to evaluate the con-
tent determination output. In the experiment, sen-
tences were generated for each proposition se-
lected by the system. Comments made by the 
subjects revealed that the simplest possible text 
was not easier to understand. Rather, it caused 
them confusion and discomfort when reading it. 
Based on these results, we decided to tackle the 
problem of deciding on the text complexity of 
automatically generated text by following the 
same syntactical complexity of the surrounding 
text, by reading level. In addition, we use word 
frequencies to select more common lexical items 
to compose summaries of lower reading levels.  
The next section presents the background and 
motivation for our work. Section 3 discusses 
some related work concerned with text genera-
tion and simplification. Section 4 presents our 
proposed approach to text generation that adapts 
the output to the reading level of the surrounding 
text. Section 5 shows some examples of text 
generated in different grade level groups. Section 
6 shows our preliminary evaluation and it is fol-
lowed by some conclusions and ideas for future 
work in Section 7 and 8, respectively. 
2 Background 
The approaches presented in this work are de-
ployed in the context of the SIGHT system. The 
system is concerned with providing access to 
information graphics present in multimodal doc-
uments from popular media such as the graphic 
in Figure 1. For this graphic, the content selec-
tion module1 (Moraes et al., 2013) chooses the 
following propositions for inclusion in the initial 
summary: 
? graph type (line graph); 
                                                 
1 The content selection module has been presented in a pre-
vious paper and is outside the scope of this paper. 
? entity being measured (annual difference 
from Seattle's 1899 sea level, in inches); 
? the intended message of the graphic 
(changing trend: stable then rising); 
? the high fluctuation of the data values; 
? the description of the individual seg-
ments of the graphic; 
? the initial value (annotated end point); 
? the ending value (annotated end point). 
 Figure 1: Example of a graphic that has a Chang-
ing Trend as its intended message and presents 
out-standing visual features (volatility and anno-
tations on end points). 
These propositions are not necessarily selected 
in this listed order, nor in the order they will be 
mentioned in the summary. They are selected 
based on their overall importance in the context 
of the graphic since the content selection frame-
work is based on an adapted version of a cen-
trality-based algorithm. Once these propositions 
are selected, an overarching organizational strat-
egy must be chosen to decide on the most appro-
priate ordering. Our system gives most im-
portance to the overall intended message of the 
graphic and thus this will be mentioned first. 
Next, a description of the features of the individ-
ual trend(s) will be provided. Finally, summary 
information about the whole graph will be given. 
The system must make further decisions when 
the graph conveys more than one trend (such as 
the graph in Figure 1). For such cases, the system 
must further decide whether to organize the de-
scription of the trends (1) by the trends them-
selves ? e.g. either in left to right order - when no 
trend is considered more important than the oth-
ers; or (2) by importance ? when a trend has a 
65
  
greater set of features selected for the discourse 
or it composes a candidate intended message, 
which augments the intended message (Moraes 
et al., 2013). In the latter case, if a piece of the 
graphic (trend) has significantly more features 
selected, meaning that it possesses a higher num-
ber of visually outstanding features, it will be 
described first, then followed by the other trends. 
The organization of the sentences is a separate 
step that happens prior to the realization phase, 
which is the focus here, and will not be discussed 
further in this paper. 
Having the set of ordered propositions select-
ed, the question that arises is how to realize this 
information to the user. The most straightforward 
way of realizing the summary would be to realize 
each proposition as a single sentence. This strat-
egy was applied in an evaluation experiment 
(briefly described next) that aimed to test the 
preciseness of the content selection framework. 
The experiment presented the subjects with line 
graphs and their correspondent generated initial 
summaries (the propositions were properly or-
dered for this experiment). Subjects were asked 
whether or not the most important information 
about the graphic was part of the summary and 
whether the summary presented unnecessary or 
redundant information. They were also offered 
the opportunity to provide additional comments. 
For the experiment, the initial summary for the 
graphic in Figure 1 was the following: 
The image shows a line 
graph. The line graph is 
highly volatile. The line 
graph presents the number of 
annual difference from Seat-
tle's 1899 sea level, in 
inches. The line graph shows 
a trend that changes. The 
changing trend consists of a 
stable trend from 1900 to 
1928 followed by a rising 
trend through 2003. The 
first segment is the stable 
trend. The stable trend has 
a starting value of 1.97 
inches. The second segment 
is the rising trend. The 
rising trend has an ending 
value of 8.9 inches. 
Although the experiment was intended to evalu-
ate the content present in the summaries, various 
comments addressed the syntactical construction 
of the text. These comments highlighted the lack 
of aggregation and pronominalization. For in-
stance, a common theme of the comments was 
that some of the information could be ?com-
bined? and presented more succinctly. 
All the participants of the experiment were 
graduate students. These results showed that 
more sophisticated readers prefer text that is 
more sophisticated. This finding pointed to the 
necessity of an aggregation step before the deliv-
ery of the summaries. However, questions arose 
concerning how much aggregation to do, how to 
measure aggregation to choose one strategy over 
another, or to decide on a desired level of aggre-
gation. 
To answer the above questions, we decided to 
examine the text complexity of the text surround-
ing the graphic --- that is, the text from the article 
in which the graph appears. We presume that this 
text complexity equates with the user?s reading 
level and thus summaries at this level of com-
plexity will be understandable and coherent to 
the users.  This approach seemed to be the best 
way of customizing the text complexity of the 
summaries in order to tailor summaries to indi-
vidual users. 
3 Related Work 
Research on generating text concerned with low-
skilled users has been conducted by (Williams & 
Reiter, 2004, 2005a, 2005b, 2008; Williams, 
Reiter, & Osman, 2003). As stated by (Williams 
& Reiter, 2005b), most NLG systems generate 
text for readers with good reading ability. Thus, 
they developed a system called SkillSum which 
adapts its output for readers with poor literacy 
after assessing their reading and numeracy skills. 
Their results show that, for these target readers, 
the micro planning choices made by SkillSum 
enhanced readability. (Siddharthan, 2003) pro-
poses a regeneration phase for syntactical text 
simplification in order to preserve discourse 
structure ?aiming to make the text easier to read 
for some target group (like aphasics and people 
with low reading ages) or easier to process by 
some program (like a parser or machine transla-
tion system). (J. Carroll et al., 1999) presents a 
text simplification methodology to help lan-
guage-impaired users. (Rello & Baeza-Yates, 
2012) investigates dyslexic errors on the Web 
and (Rello, Baeza-Yates, Bott, & Saggion, 2013) 
propose a system that uses lexical simplification 
to enhance readability and understandability of 
text for people with dyslexia. They help users to 
understand the text by offering as options the 
replacement of more complicated lexical items 
66
  
by simpler vocabulary. They performed experi-
ments with people with no visual impairments 
and with people with dyslexia and concluded that 
the system improved readability for the users 
with dyslexia and improved comprehensibility 
for users with no visual impairments. Experi-
ments performed with blind users and the usabil-
ity of a system that provides access to charts and 
graphs is presented by (Ferres, Lindgaard, 
Sumegi, & Tsuji, 2013). 
Other NLG systems make decisions on text 
complexity based on available scales such as the 
D-level sentence complexity (Covington, He, 
Brown, Naci, & Brown, 2006). One example is 
presented in (Demir et al., 2010) where tree 
structures are built representing all the possible 
ways sentences can be aggregated and the choice 
of the tree tries to balance the number of sen-
tences, their D-level complexity, and the types of 
relative clauses. 
Although text simplification is crucial to target 
low-skilled readers and users with language dis-
abilities, our experiment with college students 
showed that the simplest text was rather unpleas-
ant to read for them. We therefore propose a 
technique that focuses on adjusting the generated 
text to the reading level of the surrounding text. 
Thus, our system should satisfy both high-level 
and low-level readers. 
4 Aggregation and Text Complexity 
The initial summaries generated by the system 
are composed of individual sentences that were 
realized from atomic concept units. Since we use 
a bottom-up approach when selecting content, in 
order to achieve different text complexity levels, 
a sentence aggregation step is needed. The ag-
gregation module is in charge of merging propo-
sitions that describe an entity, creating a more 
complex sentence that will encompass the infor-
mation selected that describes the referring ex-
pression. 
The approach proposed by (Wilkinson, 1995) 
presents the aggregation process divided in two 
major steps: semantic grouping and sentence 
structuring. Although they are interdependent, 
both are needed in order to achieve aggregation 
in a text. Initiatives on automatic aggregation (or 
only semantic grouping) of text using learning 
techniques also exist. (Barzilay, 2006), 
(Bayyarapu, 2011), (Walker, Rambow, & Rogati, 
2001) are some examples of learning aggregation 
rules and grouping constrains in order to aggre-
gate text. (Demir, 2010) presents a mechanism in 
which each proposition is a single node tree 
which can be realized as a sentence and attempts 
to form more complex trees by combining trees 
in such a way so that the more complex tree 
(containing multiple propositions) can still be 
realized as a single sentence. In order to decide 
which tree is the best one to be realized, Demir?s 
work applies the revised D-level sentence com-
plexity scale, which measures the syntactic com-
plexity of a sentence according to its syntactic 
structure. 
Although learning methodologies are innova-
tive, they strive to train the algorithms in order to 
choose the best text plan based in a specific task 
or environment (defined by the training data and 
the decision of which plan is the ?best? given the 
human subjects? judgments). Our contention is 
that a given sentence plan can be perfectly suita-
ble in one context and, at the same time, be inef-
fective in another one, making the choice of the 
best text plan a variable. For this reason, we de-
cided to take into consideration the article read-
ing level when choosing the text plan that will be 
used to design the aggregation of summaries 
generated by our system. This approach allows 
the summary of the line graph to fit coherently 
within the article?s text. Text plans, in the con-
text of this work, refer to the different set of rules 
that are followed in order to aggregate proposi-
tions before the realization phase. Each text plan 
decides how propositions related to a given enti-
ty should be combined in order to produce sen-
tences. 
4.1 Reading Level Assessment 
Much effort has been devoted to developing au-
tomated approaches for assessing text complexi-
ty. Some examples are the use of support vector 
machines (Schwarm & Ostendorf, 2005) in order 
to find topical texts at a given reading level. An-
other approach is the use of statistical language 
models (Collins-Thompson & Callan, 2005; 
Collins-Thompson & Callan, 2004) for predict-
ing reading difficulty. The combination of vo-
cabulary and grammatical features in order to 
predict reading difficulty for first and second 
language texts is the object of study in (Heilman, 
Collins-Thompson, Callan, & Eskenazi, 2007).  
(Sheehan, Kostin, Futagi, & Flor, 2010) de-
veloped a system called SourceRater (now 
named TextEvaluator), which considers features 
of text that go beyond syntactical features. The 
authors list a set of dimensions of text that influ-
ences in a text reading complexity. These dimen-
sions are: Spoken vs. Written Language, Aca-
67
  
demic Orientation, Syntactic Complexity, Narra-
tive Style, Overt Expression of Persuasion, Vo-
cabulary Difficulty, and Negation. They divide 
texts into literary and informational in order to 
assess these features and their impact in reading 
difficulty after finding that these styles have sub-
stantial differences. They evaluate their tech-
nique by comparing their results with assess-
ments done using Flesh-Kincaid reading level 
assessment (Kincaid, Fishburne, Rogers, & 
Chissom, 1975) applied to text categorized into 
grade levels by the Common Core Standards 
("Common Core State Standards Initiative," 
2014). 
Another tool, Coh-Metrix (Graesser et al., 
2004), was designed to analyze text on measures 
of cohesion, language and readability. This eval-
uator also categorizes the input text into one of 
Scientific, Narrative or Informational and it con-
siders features such as cohesion relations, user 
world knowledge, language, and discourse char-
acteristics besides syntactical features such as 
word and sentence length when assessing the text 
complexity. 
To generate text that complies with a given 
reading level, we consider that a common, well-
know, widely-used metric such as Flesch-
Kincaid or SMOG (Laughlin, 1969) will suffice 
for providing input to the text planning phase of 
our system. To assure the usefulness of this met-
ric in our context, we evaluated the similarity 
between assessments done by Flesch-Kincaid 
and SMOG and assessments made by TextEvalu-
ator. For this comparison, we used 55 articles 
from our corpus2. The results showed that for 
only 20 percent of the articles was the reading 
level assessment provided by Flesch-Kincaid and 
SMOG different from the text complexity classi-
fication done by TextEvaluator. From these re-
sults, we concluded that simple reading assess-
ments such as Flesch-Kincaid and SMOG would 
suffice for guiding the choice of syntactical text 
complexity in our generated summaries. 
4.2 Generating Summaries for Different 
Reading Levels 
When generating the initial summaries of line 
graphs, our system creates different text plans for 
each group of grade levels (each group compris-
es two or more grade levels starting at the 5th 
grade) and applies the appropriate one depending 
                                                 
2 Our Digital Library contains multimodal articles collected 
from popular media. It is available at 
http://ir.cis.udel.edu/~moraes/udgraphs 
upon the assessed reading level of the text in the 
article containing the graphic. 
Because the summary is not long enough to be 
exact when determining its reading level (since 
longer texts result in more accurate assessment 
of their reading level), we decided not to create 
one text plan for each grade level. Instead, we 
have created five grade level groups and each 
one comprises two or more grades. For each 
group of grade levels, we define a text plan that 
increases a sentence syntactic structure complex-
ity as the grade gets higher. We define a text plan 
for summaries that can range between grades 5 
(inclusive) and 7 (exclusive), another text plan 
for grades between 7 (inclusive) and 9 (excusive). 
A third text plan is defined for grades 9 inclusive 
and 11 (exclusive), one for 11 (inclusive) and 13 
(exclusive) and, finally, another one for grades 
greater than or equal to 13 (college level). 
The content selection framework, as men-
tioned earlier, defines the content of a given 
summary dynamically. Due to this fact, the 
amount of information (or the number of propo-
sitions) selected for inclusion in a summary var-
ies per graphic. Our intention is to make sure that 
the reading level of the summaries generated by 
our system do not exceed the reading level of 
their respective article?s text. It is admissible, 
however, for the summary to have a slightly 
lower reading level than the one from the text. 
The organization phase, which is a previous 
step, divides the set of propositions produced by 
the content selection module into three groups: 1) 
propositions that comprise an introduction con-
taining the high-level message of the graphic, 2) 
propositions that detail the individual trends of 
the graph, and 3) propositions that convey com-
putational information about the overall graph. 
Thus, from the set of selected propositions, the 
text plan of a given group defines rules on Noun 
Phrase (NP) density and lexical choice. When 
describing an entity, attributes of this entity can 
be added to the NP as modifiers using either ad-
jectives e.g. ?a steep rising trend?, conjunctions 
e.g., ?the rising trend is steep and volatile? or 
relative clauses e.g. ?a rising trend, which is 
steep?. When the modifier of an NP is a Verb 
Phrase (VP), it is combined using a relative 
clause e.g., ?the line graph, which presents the 
number of jackets sold in 2013...? VPs can be 
modified by adverbs e.g., ?the falling trend is 
very steep?. The text plans applies rules within 
sets of propositions that are grouped hierarchical-
ly. Within these major groups, propositions can 
only be aggregated if they belong to the same 
68
  
entity. The decision of using one syntactic struc-
ture over the other is currently based on dis-
course strategies. The complexity added by a 
relative clause over the one added by an adjec-
tive, for example, is the focus of current investi-
gation (more details in Section 8) and will be 
considered when choosing one construction over 
another.  
4.3 Lexical Choice 
Most of the work on text simplification and read-
ability assessment considers lexicalization a cru-
cial aspect for readability and comprehensibility. 
(Rello, Baeza-Yates, Bott, & Saggion, 2013) pre-
sents a system that increases the understandabil-
ity and readability of text by helping users under-
stand the text by replacing complex words with 
more common ones in the lexicon.  (Laughlin, 
1969) states that longer and more precise words 
are usually harder to understand.  
This led us to use more common words at 
lower grade levels to increase the chance of the 
text being easily understood by the reader. For 
this, we use the Word Frequency Data from the 
Corpus of Contemporary American English 
(Davies, 2008). Precise and specific words 
(which are less frequently used) that describe 
visual features of line graphs such as volatility 
and steepness are replaced by other words or ex-
pressions that are more commonly used but still 
carry the same meaning, such as ?peaks and val-
leys? or ?ups and downs?. The experiment pre-
sented in Section 6 corroborates this claim, 
showing that college level students were com-
fortable with the use of such lexical items where-
as fifth graders complained about them and as-
serted they did not know their meanings. Future 
work concerns the use of lexical items catego-
rized by reading levels (details in Section 8).  
4.4 Pronominalization 
Another important feature is the pronominaliza-
tion of referring expressions. This technique 
avoids reintroduction of entities every time they 
are mentioned. The experiment mentioned in 
Section 2 showed that the reintroduction of enti-
ties or the repetition of referring expressions 
(when a pronoun could be used) in fact jeopard-
ized the understanding of some passages in the 
summaries. The participants would usually com-
plain that a given summary was confusing be-
cause it could be ?better presented? and they 
would additionally provide us with comments 
regarding the reintroduction of the referring ex-
pressions. From these results, we concluded that 
it would be valuable to include a pronominaliza-
tion step in the aggregation phase so that even 
the summaries that are at a lower grade level 
would not repeat the referring expression when 
using multiple non aggregated sentences. 
The propositions chosen by the content selec-
tion framework contain the information about 
their memberships (features such as volatility 
and steepness point to the segment of the graphic 
they belong to). This membership information is 
the clue used to define discourse focus. Our work 
follows the approach applied in the TEXT sys-
tem (McKeown, 1992), in which pronouns are 
used in order to refer to the entity being focused 
in subsequent sentences. Also inspired by the 
work presented by (McCoy & Strube, 1999) our 
system makes use of other anaphoric expressions 
besides pronouns, such as ?the trend? or ?the 
graph?. These alternative anaphoric expressions 
are used to reintroduce entities when the dis-
course focus changes. The following example 
shows the use of pronouns and the reintroduction 
of the entity in the last set of propositions. The 
entities that are in focus in each sentence are un-
derlined and the referring expressions are bolded. 
The image shows a line 
graph. The line graph pre-
sents the number of cumula-
tive, global unredeemed fre-
quent-flier miles. It con-
veys a rising trend from 
1999 to 2005. It has a 
starting value of 5.5. It 
has an ending value of 14.2. 
The graph shows an overall 
increase of 8.7. 
The last sentence changes the focus back to 
the overall graph. Even though the entity line 
graph was already mentioned, the focus had 
changed to the entity rising trend, so when the 
focus returns to the entity line graph, the system 
makes use of a definite reference to reintroduce 
it. 
5 Examples of Summaries Generated 
for Different Reading Levels 
Below are examples of some of the summaries 
that our system generates for the graph in Figure 
1 at different reading levels. Their assessed read-
ing levels provided by SMOG are also shown3. 
The summaries in these examples are also pro-
                                                 
3 These results were obtained from using a tool available in 
the GNU project Style and Diction (FSF, 2005). 
69
  
nominalized. The pronominalization phase is 
described in Section 4.4. 
Summary for Grades > 5 and <= 7 
The image shows a line 
graph. The line graph has 
ups and downs. It presents 
the number of annual differ-
ence from Seattle's 1899 sea 
level, in inches. It conveys 
a changing trend. It con-
sists of a stable trend from 
1900 to 1928 followed by a 
rising trend through 2003. 
The first segment is the 
stable trend. It has a 
starting value of 1.97 inch-
es. The second segment is 
the rising trend. It has an 
ending value of 8.9 inches. 
(SMOG 4.8) 
 
Summary for Grades > 11 and <= 13 
The image shows a highly 
volatile line graph, which 
presents the number of annu-
al difference from Seattle's 
1899 sea level, in inches, 
in addition to conveying a 
changing trend that consists 
of a stable trend from 1900 
to 1928 followed by a rising 
trend through 2003. The 
first segment is the stable 
trend that has starting val-
ue of 1.97 inches. The sec-
ond segment is the rising 
trend that has ending value 
of 8.9 inches.  
(SMOG 10.0) 
The assessed reading level of these passages 
are below the maximum threshold due to the lim-
ited number of propositions selected by the con-
tent determination algorithm. 
6 Evaluation 
This work on aggregation was motivated by the 
evaluation described in Section 2, which was 
intended to evaluate the content selection phase 
of the system. Much to our surprise, many of the 
comments indicated that the summaries were 
difficult to read because they lacked aggregation! 
This result caused us to implement the work pre-
sented here. Our first evaluation therefore repli-
cated our first experiment where, instead of using 
a simple sentence for each proposition, sentences 
were aggregated to reflect a 7th ? 9th grade read-
ing level (the level slightly lower than the medi-
an of the articles collected for our corpus).  
Table 1 compares the results of these two ini-
tial experiments. The results4  show a dramatic 
drop in the comments related to issues with ag-
gregation. From this preliminary experiment re-
sults, we felt encouraged to pursue the generation 
of summaries suited to grade levels. 
 Number 
of  
Subjects 
Number 
of 
Responses 
Number 
of 
complaints 
Experiment 
1 16 201 22 
Experiment 
2 29 331 4 
Table 1. Comparison of results from preliminary 
experiment. 
Our second experiment targeted our genera-
tion of grade-level appropriate text. In this exper-
iment, we wished to judge whether readers at 
different reading levels would prefer texts gener-
ated by our system aimed at their reading level. 
We therefore recruited two groups of partici-
pants: (1) students from a fifth grade elementary 
school in the area and (2) undergraduate students 
in an introductory CS course at a university. 
Participants were presented with 2 summaries 
from each of 5 different graphs. One of the 
summaries was generated to be at a 5th ? 7th 
grade reading level and the other at a 11th ? 13th 
grade reading level. The participants were asked 
to select the summary they liked the best and to 
provide comments on what they did not like in 
either summary. 
Table 2 shows the results of this experiment. 
Five students from 5th grade and thirty-four 
freshmen college students were recruited to par-
ticipate. From these results we can see that, in 
fact, the majority in both groups preferred the 
grade-level appropriate summary. For the fresh-
men college students, the fact that the subjects 
were almost evenly split on their choices, even 
though they are at the same grade level, was ex-
pected. This shows that reading preferences may 
vary even among people from same age/grade 
level. Since there were subjects who preferred 
simple to complex text, we can assume that read-
ing skills can vary even within a grade level 
group. Our contention is that readers who prefer 
simple text would read venues that use simple 
text structure and syntax. That is where our ap-
                                                 
4 The number of complaints presented in Table 1 are con-
cerned only with syntactical issues. 
70
  
proach plays an even better role when looking 
into the surrounding text the user is reading. Fol-
lowing this approach, instead of assessing or ask-
ing the user which level they are in, gives us 
more chances of being successful at producing 
text that will be more appropriate to each user. 
Analyzing the results on the choices of the op-
posite summary to their target group, we noticed 
that there was an agreement amongst subjects 
regarding the type of the graph. Kids who 
showed a preference for the complex text, for 
example, did so only for graphics describing a 
simple trend, therefore having a small amount of 
information an making it easy for them to follow. 
Some college students who chose the simpler 
summary provided comments that showed to be 
independent of the reading level decisions of the 
system. Some subjects pointed that a default 
connective applied by the realizer (?in addition 
to?) was making the summary complicated to 
read. That can actually be the cause of the choice 
for the simple summary, and not necessarily the 
amount of aggregation. To address this, we con-
sider that changing the connective to a more 
common one (e.g. ?and?) would make the text 
more fluid.  
From these results, we conclude that, indeed, 
adapting the generated text to the complexity of 
text commonly read by a user is a promising path 
to follow. An experiment where we provide the 
subjects with the article accompanying the graph 
and ask them to choose the summary that they 
believe fits the text complexity of the summary is 
intended and planned as future work. We have 
initiated investigation in some automated ways 
of generating text within these different grade 
level groups and we discuss it further in Section 
8. 
 
 
Chose Sum-
maries for 5th ? 
7th Grades (%) 
Chose Summar-
ies for 11th - 13th 
Grades (%) 
5th grade 80 20 
Freshmen 
students 47 53 
Table 2. Results from experiment measuring 
choices of summaries in different reading levels. 
7 Conclusion 
Most NLG systems available today generate 
text that focus on specific target readers. Some of 
them focus on text generation for low-skilled 
readers, while others generate text for high-
skilled readers. In this work, we presented an 
approach that offers a solution that attends to the 
needs of readers at different grade levels. 
Our system generates initial summaries of line 
graphs available in popular media, so visually 
impaired users can have access to the high-level 
message these resources carry. Our contention is 
that users read articles from venues that they feel 
comfortable with reading. Therefore, we assert 
that generating summaries that fit the text com-
plexity of the overall article leverages the quality 
of the generated text. We showed an approach 
that uses Flesch-Kincaid and SMOG reading as-
sessments in order to determine the syntactical 
complexity of the generated text. From the ex-
periments performed, we conclude that pursuing 
the generation of natural language text that fits 
the reading level of the surrounding text is prom-
ising. 
8 Path Forward 
Investigation on more automated ways of decid-
ing on how to aggregate propositions is the next 
step to take. Our current aggregation method re-
lies on templates for each group. We anticipate 
some techniques to learn how different text con-
structions can affect reading measures and then 
using them when choosing an adjective over a 
relative clause for increasing the NP density and 
use of passive voice, for example. This would 
allow the aggregation phase to be easily applied 
to NLG systems in different contexts.  
Another important point is the choice of lexi-
cal items by reading level or age. We plan on 
investigating how the usage of word frequency 
by age/grade level (Carroll, 1972) might help 
achieving a more appropriate summary for a giv-
en grade level. Then, the lexical items that are 
listed as common to the target grade reading lev-
el would be applied in their respective context. 
Some comments provided on the second ex-
periment described in Section 6 were that it was 
not so easy to understand long sentences on 
which values and dates were also present. This 
aspect deserves investigation on acquiring nu-
meracy skills along with reading skills as clues to 
assess the best text complexity to present. Re-
search that assess numeracy and literacy skills of 
users is presented by (Williams & Reiter, 2008). 
From the accessibility prospective, an experi-
ment with blind users is anticipated. We intend 
to evaluate the effect of generating text in differ-
ent reading levels for people with visual and/or 
reading impairments. 
71
  
References  
Barzilay, R. (2006). Aggregation via set partitioning 
for natural language generation. Paper 
presented at the In HLT-NAACL. 
Bayyarapu, H. S. (2011). Efficient algorithm for 
Context Sensitive Aggregation in Natural 
Language generation. Paper presented at the 
RANLP. 
Carroll, J. B. (1972). A New Word Frequency Book. 
Elementary English, 49(7), pp. 1070-1074.  
Collins-Thompson, K., & Callan, J. (2005). Predicting 
Reading Difficulty with Statistical Language 
Models. J. Am. Soc. Inf. Sci. Technol., 
56(13), 1448-1462.  
Collins-Thompson, K., & Callan, J. P. (2004). A 
Language Modeling Approach to Predicting 
Reading Difficulty. Paper presented at the 
HLT-NAACL. 
Common Core State Standards Initiative. (2014).   
Retrieved 2014-01-09, from 
http://www.corestandards.org/ 
Covington, M., He, C., Brown, C., Naci, L., & 
Brown, J. (2006). How Complex is that 
Sentence? A Proposed Revision of the 
Rosenberg and Abbeduto D-Level Scale. 
Paper presented at the Research Report, 
Artificial Intelligence Center, University of 
Georgia. 
Davies, M. (2008). Word frequency data: Corpus of 
Contemporary American English. 
Demir, S. (2010). Sight for visually impaired users: 
Summarizing information graphics textually. 
University of Delaware.    
Demir, S., Oliver, D., Schwartz, E., Elzer, S., 
Carberry, S., McCoy, K. F., & Chester, D. 
(2010). Interactive SIGHT: textual access to 
simple bar charts. New Rev. Hypermedia 
Multimedia, 16, 245-279.  
Ferres, L., Lindgaard, G., Sumegi, L., & Tsuji, B. 
(2013). Evaluating a Tool for Improving 
Accessibility to Charts and Graphs. ACM 
Trans. Comput.-Hum. Interact., 20(5), 28:21-
28:32.  
FSF. (2005). Style and Diction GNU project. from 
www.gnu.org/software/diction 
Graesser, A. C., McNamara, D. S., Louwerse, M. M., 
Cai, Z., Dempsey, K., Floyd, Y., . . . 
Correspondence, F. Y. (2004). Coh-Metrix: 
Analysis of text on cohesion and language. 
Paper presented at the M. Louwerse Topics 
in Cognitive Science. 
Heilman, M., Collins-Thompson, K., Callan, J., & 
Eskenazi, M. (2007). Combining Lexical and 
Grammatical Features to Improve 
Readability Measures for First and Second 
Language Texts. Paper presented at the HLT-
NAACL. 
Kincaid, J. P., Fishburne, R. P., Rogers, R. L., & 
Chissom, B. S. (1975). Derivation of New 
Readability Formulas (Automated 
Readability Index, Fog Count and Flesch 
Reading Ease Formula) for Navy Enlisted 
Personnel. 
Laughlin, G. H. M. (1969). SMOG Grading-a New 
Readability Formula. Journal of Reading, 
12(8), pp. 639-646.  
McCoy, K., & Strube, M. (1999). Generating 
Anaphoric Expressions: Pronoun or Definite 
Description? Paper presented at the ACL 
WORKSHOP ON DISCOURSE AND 
REFERENCE STRUCTURE. 
McKeown, K. (1992). Text Generation: Cambridge 
University Press. 
Moraes, P. S., Carberry, S., & McCoy, K. (2013). 
Providing access to the high-level content of 
line graphs from online popular media. 
Paper presented at the Proceedings of the 
10th International Cross-Disciplinary 
Conference on Web Accessibility, Rio de 
Janeiro, Brazil. 
Rello, L., Baeza-Yates, R., Bott, S., & Saggion, H. 
(2013). Simplify or Help?: Text 
Simplification Strategies for People with 
Dyslexia. Paper presented at the Proceedings 
of the 10th International Cross-Disciplinary 
Conference on Web Accessibility, New 
York, NY, USA. 
Rello, L., & Baeza-Yates, R. A. (2012). The presence 
of English and Spanish dyslexia in the Web. 
The New Review of Hypermedia and 
Multimedia, 18(3), 131-158.  
Schwarm, S. E., & Ostendorf, M. (2005). Reading 
Level Assessment Using Support Vector 
Machines and Statistical Language Models. 
Paper presented at the Proceedings of the 
43rd Annual Meeting on Association for 
Computational Linguistics, Stroudsburg, PA, 
USA. 
Sheehan, K. M., Kostin, I., Futagi, Y., & Flor, M. 
(2010). Generating automated text 
complexity classifications that are aligned 
with targeted text complexity standards. 
Walker, M. A., Rambow, O., & Rogati, M. (2001). 
SPoT: a trainable sentence planner. Paper 
presented at the Proceedings of the second 
meeting of the North American Chapter of 
the Association for Computational 
Linguistics on Language technologies, 
Stroudsburg, PA, USA. 
Wilkinson, J. (1995). Aggregation in Natural 
Language Generation: Another Look. 
Williams, S., & Reiter, E. (2008). Generating basic 
skills reports for low-skilled readers*. 
Natural Language Engineering, 14(4), 495-
525.  
Wu, P., Carberry, S., Elzer, S., & Chester, D. (2010). 
Recognizing the intended message of line 
graphs. Paper presented at the Proceedings 
of the 6th international conference on 
72
  
Diagrammatic representation and inference, 
Berlin, Heidelberg. 
 
73
Proceedings of the 8th International Natural Language Generation Conference, pages 95?98,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Generating Summaries of Line Graphs 
 
Priscilla Moraes, Gabriel Sina, Kathleen McCoy and Sandra Carberry 
Department of Computer and Information Sciences 
University of Delaware, Newark, Delaware, USA 
 [pmoraes | gsina | mccoy | carberry]@udel.edu 
 
Abstract 
This demo presents a Natural Language Gener-
ation (NLG) system that generates summaries 
of informational graphics, specifically simple 
line graphs, present in popular media. The sys-
tem is intended to capture the high-level 
knowledge conveyed by the graphic and its out-
standing visual features. It comprises a content 
selection phase that extracts the most important 
content of the graphic, an organization phase, 
which orders the propositions in a coherent 
manner, and a realization phase that uses the 
text surrounding the article to make decisions 
on the choice of lexical items and amount of ag-
gregation applied to the propositions to gener-
ate the summary of the graphic. 
1 Introduction 
Multimodal documents from online popular me-
dia often contain information graphics that aug-
ment the information found in the text. These 
graphics, however, are inaccessible for visually 
impaired users or in environments where the im-
age cannot be processed/displayed. Our system 
captures the high-level content of the graphic and 
produces a textual summary that conveys it. Fig-
ure 1 shows the system architecture.  
The first step is the identification of the pres-
ence of a graphical image in the web page by a 
Browser Helper Object (BHO) (Elzer et al., 2007). 
If a graphic is present on the web page, the Graph-
ical Information Extraction Module (VEM) 
(Chester & Elzer, 2005) is triggered by the BHO  
in order to extract the data from the image. The 
VEM then produces an XML representation of the 
graphic that is used by the Intention Recognition 
Module (IRM) for simple bar charts (Elzer, 
Green, Carberry, & Hoffman, 2006), simple line 
graphs (Wu, Carberry, Elzer, & Chester, 2010) 
and grouped bar charts (R. Burns, Carberry, & 
Elzer, 2010; R. Burns, Carberry, & Schwartz, 
2013; R. J. Burns, 2013). The XML representation 
1 http://ir.cis.udel.edu/~moraes/udgraphs 
of the graphic, along with the intended message 
identified by the IRM, is sent to the Generation 
Module (GM), which produces a textual summary 
of the most important content presented in the 
graphic. The system produces an initial summary 
and follow-up responses for simple bar charts 
(Demir, Carberry, & Elzer, 2009; Demir, 
Carberry, & McCoy, 2008) and this demo pre-
sents the GM for simple line graphs. 
This demo focuses on presenting the generation 
phase of the system. For that, we will demonstrate 
the generation of summaries in the context of a 
digital library that is available online 1 and that 
contains information graphics collected from 
online popular media, along with the articles con-
taining the graphics. In addition, we have included 
hand-generated XML representations for the 
graphics (the current VEM is not fully robust). For 
each article that contains a graph, the user can 
choose to have access to the generated summary 
by clicking on the ?Generate summary? button 
(highlighted in Figure 2). Figure 2 shows a screen-
shot on which the graph shown in Figure 3 has its 
article featured. 
For accessibility projects that may use our sys-
tem (applications developed for visually impaired 
users, for example), the application might use a 
combination of key strokes to allow user interac-
tion. The module of the system that is the focus of 
this demo is the Generation Module. 
 
 
Figure 1: System Architecture
                                                
95
 
Figure 2: Digital library screenshot where we have added summary generation functionality. 
2 Generation Module 
For generating summaries of line graphs, the first 
step is the selection of content. In order to select 
the most important features of the line graph that 
should be conveyed in the summary, the system 
represents the intended message and the visual 
features identified by a human subject experiment 
(Greenbacker, Carberry, & McCoy, 2011) using a 
graph. A centrality-based algorithm, which is an 
adapted version of PageRank (Page, Brin, 
Motwani, & Winograd, 1999), is then imple-
mented to select the most important information 
(represented as nodes in the graph). This imple-
mentation allows semantic relationships between 
propositions to be represented on the edges of the 
graph. The core of the content selection frame-
work is to detect present outstanding visual fea-
tures in the graphic, along with its intended mes-
sage, in order to select nodes. Details in the con-
tent selection phase are available in the work pre-
sented at (P. S. Moraes, Carberry, & McCoy, 
2013). 
The next phase is the organization of the se-
lected content. The organization phase works by 
ordering the selected propositions such that the 
delivered summary is fluent and coherent. The 
summaries are organized having an introduction 
section, a detailed section and a conclusion. The 
introduction consists of overall information about 
the line graph (the type of the graph, the entity be-
ing measured, the volatility of the graph and its 
intended message). The identified trends are de-
scribed in the detail section. For this part of the 
summary, pieces of the graphic that outstand due 
to its visual features may be described first, being 
followed by other trends. Finally, the conclusion 
section of the summary presents computational 
information about the graphic (overall value and 
rate change, time span of the graphic, maximum 
and minimum points and dates when they occur). 
The strategies on organizing the summaries are 
described in (P. Moraes, McCoy, & Carberry, 
2014). 
The last step of the Generation Module is the 
aggregation of propositions into more complex 
sentences. This decision is usually left to the de-
signer?s choice on how much aggregation to per-
form when generating text. Some systems are de-
signed to generate simple text for people with low 
reading abilities (Williams & Reiter, 2005a). As 
stated by (Williams & Reiter, 2005b), most NLG 
systems available generate text for high-skilled 
users. Our system generates line graph summaries 
that fit the reading level of the article in which the 
line graph appears. We contend that users gener-
ally read articles from venues they feel comforta-
ble with reading. In this manner, we intrinsically 
assess the user?s reading level without needing to 
actively survey it. 
 
Figure 3: A line graph present in popular media. 
96
The first step of the aggregation phase is to as-
sess the reading level of the article?s text. There is 
a myriad of techniques to measure the reading 
level of text. Much of them use machine learning 
techniques in order to learn text constructions and 
lexicalization used in different grade levels. As 
presented in (P. Moraes et al., 2014), simpler and 
well established reading level measurement tech-
niques suffice for our scenario. The work shows 
that Flesh-Kincaid (Kincaid, Fishburne, Rogers, 
& Chissom, 1975) and SMOG (Laughlin, 1969) 
provide the set of information needed by the sys-
tem in order to make decisions of syntactical text 
complexity. 
After assessing the reading level of the article, 
the system then uses the text plan that applies to 
the identified reading level. Text plans define 
rules on Noun Phrase (NP) density and lexical 
choice. When describing an entity, attributes of 
this entity can be added to the NP as modifiers us-
ing either adjectives e.g. ?a highly volatile rising 
trend?, conjunctions e.g., ?the rising trend is vol-
atile and steep? or relative clauses e.g. ?a rising 
trend, which is highly volatile?. When the modi-
fier of an NP is a Verb Phrase (VP), it is combined 
using a relative clause e.g., ?the line graph, which 
presents the number of jackets sold in 2013...? 
VPs can be modified by adverbs e.g., ?the falling 
trend is very steep?. The text plans apply rules 
within sets of propositions that are grouped hier-
archically. The system then uses the appropriate 
lexical items (highly volatile vs ups and downs; 
conveys vs shows) and applies the appropriate 
amount of aggregation in order to realize sen-
tences. 
 
Figure 4: Pop up window with the resulting sum-
mary generated by the system. 
Figure 4 and Figure 5 display the summaries 
generated for a user whose reading level is 11th-
13th grade and 5th-7th grade respectively. From 
these one can see the different aggregation and 
lexical choice decisions made for the different 
reading levels. The system also includes appropri-
ate pronominalization in order to avoid repetition 
of the referring expressions (P. Moraes et al., 
2014). 
 
Figure 5: Example of a summary adapted to the 
reading level of grades 5 to 7. 
For the surface realization phase we use 
FUF/SURGE (Elhadad & Robin, 1999) to create 
the templates for realization. The template are cre-
ated based on the text plans defined for a given 
reading level, as described above. 
3 Conclusion 
This paper presents the demonstration of the gen-
eration module of SIGHT. For the demo, the gen-
eration module works on a digital library that ar-
chives informational graphics collected from pop-
ular media available online. The aggregation 
phase of the generation module tailors the syntac-
tical complexity of the generated text to that of the 
article?s text in which the graphic appears.  
An evaluation of the text summaries generated 
at different reading level is presented at (P. 
Moraes et al., 2014). It shows that, indeed, differ-
ent users have different preferences regarding dif-
ferent text designs. 
4 Future Work 
A more automated way of defining a text plan for 
a given reading level is under investigation. We 
will explore techniques for learning how different 
text constructions can affect reading measures and 
then using these learned models when choosing an 
97
adjective over a relative clause for increasing the 
NP density and use of passive voice, for example.  
Choosing lexical items that are classified by 
age is another possibility. We plan on investigat-
ing how the usage of word frequency by age/grade 
level (Carroll, 1972) might influence the overall 
generated summaries. 
5 Acknowledgement 
Gabriel Sina was supported by the Coor-
dena??o de Aperfei?oamento de Pessoal de N?vel 
Superior from Brazil CAPES ? in Portuguese. 
References  
Burns, R., Carberry, S., & Elzer, S. (2010). Visual and 
spatial factors in a bayesian reasoning 
framework for the recognition of intended 
messages in grouped bar charts. Paper 
presented at the Proceedings of the AAAI 
Workshop on Visual Representations and 
Reasoning. 
Burns, R., Carberry, S., & Schwartz, S. E. (2013). 
Modeling a Graph Viewer's Effort in 
Recognizing Messages Conveyed by Grouped 
Bar Charts. Paper presented at the UMAP. 
Burns, R. J. (2013). Automated intention recognition of 
grouped bar charts in multimodal documents. 
University of Delaware, Ann Arbor. 
Retrieved from 
http://search.proquest.com/docview/1318643
227?accountid=10457   
Carroll, J. B. (1972). A New Word Frequency Book. 
Elementary English, 49(7), pp. 1070-1074.  
Chester, D., & Elzer, S. (2005). Getting computers to 
see information graphics so users do not have 
to. Paper presented at the the Proceedings of 
the 15th International Symposium on 
Methodologies for Intelligent Systems. 
Demir, S., Carberry, S., & Elzer, S. (2009). Issues in 
Realizing the Overall Message of a Bar Chart. 
In N. Nicolov, G. Angelova & R. Mitkov 
(Eds.), Recent Advances in Natural Language 
Processing V (pp. 311-320): John Benjamins. 
Demir, S., Carberry, S., & McCoy, K. F. (2008). 
Generating textual summaries of bar charts. 
Paper presented at the Proceedings of the 
Fifth International Natural Language 
Generation Conference, Stroudsburg, PA, 
USA. 
Elhadad, M., & Robin, J. (1999). SURGE: a 
comprehensive plug-in syntactic realization 
component for text generation. 
Computational Linguistics.  
Elzer, S., Green, N., Carberry, S., & Hoffman, J. 
(2006). A Model of Perceptual Task Effort for 
Bar Charts and its Role in Recognizing 
Intention. International Journal on User 
Modeling and User-Adapted Interaction, 
16(1), 1-30.  
Elzer, S., Schwartz, E., Carberry, S., Chester, D., 
Demir, S., & Wu, P. (2007). A Browser 
Extension For Providing Visually Impaired 
Users Access To The Content Of Bar Charts 
On The Web. Paper presented at the the 
Proceedings of the International Conference 
on Web Information Systems and 
Technologies. 
Greenbacker, C., Carberry, S., & McCoy, K. (2011, 
July). A Corpus of Human-written Summaries 
of Line Graphs. Paper presented at the 
Proceedings of the UCNLG+Eval: Language 
Generation and Evaluation Workshop, 
Edinburgh, Scotland. 
Kincaid, J. P., Fishburne, R. P., Rogers, R. L., & 
Chissom, B. S. (1975). Derivation of New 
Readability Formulas (Automated 
Readability Index, Fog Count and Flesch 
Reading Ease Formula) for Navy Enlisted 
Personnel. 
Laughlin, G. H. M. (1969). SMOG Grading-a New 
Readability Formula. Journal of Reading, 
12(8), pp. 639-646.  
Moraes, P., McCoy, K., & Carberry, S. (2014). 
Adapting Graph Summaries to the Users? 
Reading Levels. Paper presented at the 
Proceedings of the 8th International Natural 
Language Generation Conference. 
Moraes, P. S., Carberry, S., & McCoy, K. (2013). 
Providing access to the high-level content of 
line graphs from online popular media. Paper 
presented at the Proceedings of the 10th 
International Cross-Disciplinary Conference 
on Web Accessibility, Rio de Janeiro, Brazil. 
Page, L., Brin, S., Motwani, R., & Winograd, T. 
(1999). The PageRank Citation Ranking: 
Bringing Order to the Web: Stanford InfoLab. 
Williams, S., & Reiter, E. (2005a). Appropriate 
Microplanning Choices for Low-Skilled 
Readers. Paper presented at the IJCAI. 
Williams, S., & Reiter, E. (2005b). Generating 
readable texts for readers with low basic 
skills. Paper presented at the Proceedings of 
the 10th European Workshop on Natural 
Language Generation (EWNLG 2005). 
Wu, P., Carberry, S., Elzer, S., & Chester, D. (2010). 
Recognizing the intended message of line 
graphs. Paper presented at the Proceedings of 
the 6th international conference on 
Diagrammatic representation and inference, 
Berlin, Heidelberg. 
 
98
