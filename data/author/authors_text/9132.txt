Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 165?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Alignments and Leveraging Natural Logic
Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon
Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage
Eric Yeh, Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{natec,dcer,grenager,dlwh,loeki,wcmac,mcdm,dramage,yeh1,manning}@stanford.edu
Abstract
We describe an approach to textual infer-
ence that improves alignments at both the
typed dependency level and at a deeper se-
mantic level. We present a machine learning
approach to alignment scoring, a stochas-
tic search procedure, and a new tool that
finds deeper semantic alignments, allowing
rapid development of semantic features over
the aligned graphs. Further, we describe a
complementary semantic component based
on natural logic, which shows an added gain
of 3.13% accuracy on the RTE3 test set.
1 Introduction
Among the many approaches to textual inference,
alignment of dependency graphs has shown utility
in determining entailment without the use of deep
understanding. However, discovering alignments
requires a scoring function that accurately scores
alignment and a search procedure capable of approx-
imating the optimal mapping within a large search
space. We address the former requirement through
a machine learning approach for acquiring lexical
feature weights, and we address the latter with an
approximate stochastic approach to search.
Unfortunately, the most accurate aligner can-
not capture deeper semantic relations between two
pieces of text. For this, we have developed a tool,
Semgrex, that allows the rapid development of de-
pendency rules to find specific entailments, such as
familial or locative relations, a common occurence
in textual entailment data. Instead of writing code by
hand to capture patterns in the dependency graphs,
we develop a separate rule-base that operates over
aligned dependency graphs. Further, we describe a
separate natural logic component that complements
our textual inference system, making local entail-
ment decisions based on monotonic assumptions.
The next section gives a brief overview of the sys-
tem architecture, followed by our proposal for im-
proving alignment scoring and search. New coref-
erence features and the Semgrex tool are then de-
scribed, followed by a description of natural logic.
2 System Overview
Our system is a three stage architecture that con-
ducts linguistic analysis, builds an alignment be-
tween dependency graphs of the text and hypothesis,
and performs inference to determine entailment.
Linguistic analysis identifies semantic entities, re-
lationships, and structure within the given text and
hypothesis. Typed dependency graphs are passed
to the aligner, as well as lexical features such as
named entities, synonymity, part of speech, etc. The
alignment stage then performs dependency graph
alignment between the hypothesis and text graphs,
searching the space of possible alignments for the
highest scoring alignment. Improvements to the
scorer, search algorithm, and automatically learned
weights are described in the next section.
The final inference stage determines if the hy-
pothesis is entailed by the text. We construct a set
of features from the previous stages ranging from
antonyms and polarity to graph structure and seman-
tic relations. Each feature is weighted according to a
set of hand-crafted or machine-learned weights over
165
the development dataset. We do not describe the fea-
tures here; the reader is referred to de Marneffe et al
(2006a) for more details. A novel component that
leverages natural logic is also used to make the final
entailment decisions, described in section 6.
3 Alignment Model
We examine three tasks undertaken to improve the
alignment phase: (1) the construction of manu-
ally aligned data which enables automatic learning
of alignment models, and effectively decouples the
alignment and inference development efforts, (2) the
development of new search procedures for finding
high-quality alignments, and (3) the use of machine
learning techniques to automatically learn the pa-
rameters of alignment scoring models.
3.1 Manual Alignment Annotation
While work such as Raina et al (2005) has tried
to learn feature alignment weights by credit assign-
ment backward from whether an item is answered
correctly, this can be very difficult, and here we fol-
low Hickl et al (2006) in using supervised gold-
standard alignments, which help us to evaluate and
improve alignment and inference independently.
We built a web-based tool that allows annotators
to mark semantic relationships between text and hy-
pothesis words. A table with the hypothesis words
on one axis and the text on the other allows re-
lationships to be marked in the corresponding ta-
ble cell with one of four options. These relation-
ships include text to hypothesis entailment, hypothe-
sis to text entailment, synonymy, and antonymy. Ex-
amples of entailment (from the RTE 2005 dataset)
include pairs such as drinking/consumption, coro-
navirus/virus, and Royal Navy/British. By distin-
guishing between these different types of align-
ments, we can capture some limited semantics in the
alignment process, but full exploitation of this infor-
mation is left to future work.
We annotated the complete RTE2 dev and
RTE3 dev datasets, for a total of 1600 aligned
text/hypothesis pairs (the data is available at
http://nlp.stanford.edu/projects/rte/).
3.2 Improving Alignment Search
In order to find ?good? alignments, we define both a
formal model for scoring the quality of a proposed
alignment and a search procedure over the alignment
space. Our goal is to build a model that maximizes
the total alignment score of the full datasetD, which
we take to be the sum of the alignment scores for all
individual text/hypothesis pairs (t, h).
Each of the text and hypothesis is a semantic de-
pendency graph; n(h) is the set of nodes (words)
and e(h) is the set of edges (grammatical relations)
in a hypothesis h. An alignment a : n(h) 7? n(t) ?
{null} maps each hypothesis word to a text word
or to a null symbol, much like an IBM-style ma-
chine translation model. We assume that the align-
ment score s(t, h, a) is the sum of two terms, the first
scoring aligned word pairs and the second the match
between an edge between two words in the hypoth-
esis graph and the corresponding path between the
words in the text graph. Each of these is a sum, over
the scoring function for individual word pairs sw and
the scoring function for edge path pairs se:
s(t, h, a) =
?
hi?n(h)
sw(hi, a(hi))
+
?
(hi,hj)?e(h)
se((hi, hj), (a(hi), a(hj)))
The space of alignments for a hypothesis with m
words and a text with n words contains (n + 1)m
possible alignments, making exhaustive search in-
tractable. However, since the bulk of the alignment
score depends on local factors, we have explored
several search strategies and found that stochastic
local search produces better quality solutions.
Stochastic search is inspired by Gibbs sampling
and operates on a complete state formulation of the
search problem. We initialize the algorithm with the
complete alignment that maximizes the greedy word
pair scores. Then, in each step of the search, we
seek to randomly replace an alignment for a single
hypothesis word hi. For each possible text word tj
(including null), we compute the alignment score if
we were to align hi with tj . Treating these scores as
log probabilities, we create a normalized distribution
from which we sample one alignment. This Gibbs
sampler is guaranteed to give us samples from the
posterior distribution over alignments defined im-
plicitly by the scoring function. As we wish to find a
maximum of the function, we use simulated anneal-
ing by including a temperature parameter to smooth
166
the sampling distribution as a function of time. This
allows us to initially explore the space widely, but
later to converge to a local maximum which is hope-
fully the global maximum.
3.3 Learning Alignment Models
Last year, we manually defined the alignment scor-
ing function (de Marneffe et al, 2006a). However,
the existence of the gold standard alignments de-
scribed in section 3.1 enables the automatic learning
of a scoring function. For both the word and edge
scorers, we choose a linear model where the score is
the dot product of a feature and a weight vector:
sw(hi, tj) = ?w ? f(hi, tj), and
se((hi, hj), (tk, t`)) = ?e ? f((hi, hj), (tk, t`)).
Recent results in machine learning show the ef-
fectiveness of online learning algorithms for struc-
ture prediction tasks. Online algorithms update their
model at each iteration step over the training set. For
each datum, they use the current weight vector to
make a prediction which is compared to the correct
label. The weight vector is updated as a function
of the difference. We compared two different up-
date rules: the perceptron update and the MIRA up-
date. In the perceptron update, for an incorrect pre-
diction, the weight vector is modified by adding a
multiple of the difference between the feature vector
of the correct label and the feature vector of the pre-
dicted label. We use the adaptation of this algorithm
to structure prediction, first proposed by (Collins,
2002). TheMIRA update is a proposed improvement
that attempts to make the minimal modification to
the weight vector such that the score of the incorrect
prediction for the example is lower than the score of
the correct label (Crammer and Singer, 2001).
We compare the performance of the perceptron
and MIRA algorithms on 10-fold cross-validation
on the RTE2 dev dataset. Both algorithms improve
with each pass over the dataset. Most improve-
ment is within the first five passes. Table 1 shows
runs for both algorithms over 10 passes through the
dataset. MIRA consistently outperforms perceptron
learning. Moreover, scoring alignments based on the
learned weights marginally outperforms our hand-
constructed scoring function by 1.7% absolute.
A puzzling problem is that our overall per-
formance decreased 0.87% with the addition of
Perfectly aligned
Individual words Text/hypothesis pairs
Perceptron 4675 271
MIRA 4775 283
Table 1: Perceptron and MIRA results on 10-fold cross-
validation on RTE2 dev for 10 passes.
RTE3 dev alignment data. We believe this is due
to a larger proportion of ?irrelevant? and ?relation?
pairs. Irrelevant pairs are those where the text and
hypothesis are completely unrelated. Relation pairs
are those where the correct entailment judgment re-
lies on the extraction of relations such as X works
for Y, X is located in Y, or X is the wife of Y. Both
of these categories do not rely on alignments for en-
tailment decisions, and hence introduce noise.
4 Coreference
In RTE3, 135 pairs in RTE3 dev and 117 in
RTE3 test have lengths classified as ?long,? with
642 personal pronouns identified in RTE3 dev and
504 in RTE3 test. These numbers suggest that re-
solving pronomial anaphora plays an important role
in making good entailment decisions. For exam-
ple, identifying the first ?he? as referring to ?Yunus?
in this pair from RTE3 dev can help alignment and
other system features.
P: Yunus, who shared the 1.4 million prize Friday with the
Grameen Bank that he founded 30 years ago, pioneered the con-
cept of ?microcredit.?
H: Yunus founded the Grameen Bank 30 years ago.
Indeed, 52 of the first 200 pairs from RTE3 dev
were deemed by a human evaluator to rely on ref-
erence information. We used the OpenNLP1 pack-
age?s maximum-entropy coreference utility to per-
form resolution on parse trees and named-entity data
from our system. Found relations are stored and
used by the alignment stage for word similarity.
We evaluated our system with and without coref-
erence over RTE3 dev and RTE3 test. Results are
shown in Table 3. The presence of reference infor-
mation helped, approaching significance on the de-
velopment set (p < 0.1, McNemar?s test, 2-tailed),
but not on the test set. Examination of alignments
and features between the two runs shows that the
alignments do not differ significantly, but associated
1http://opennlp.sourceforge.net/
167
weights do, thus affecting entailment threshold tun-
ing. We believe coreference needs to be integrated
into all the featurizers and lexical resources, rather
than only with word matching, in order to make fur-
ther gains.
5 Semgrex Language
A core part of an entailment system is the ability to
find semantically equivalent patterns in text. Pre-
viously, we wrote tedious graph traversal code by
hand for each desired pattern. As a remedy, we
wrote Semgrex, a pattern language for dependency
graphs. We use Semgrex atop the typed dependen-
cies from the Stanford Parser (de Marneffe et al,
2006b), as aligned in the alignment phase, to iden-
tify both semantic patterns in a single text and over
two aligned pieces of text. The syntax of the lan-
guage was modeled after tgrep/Tregex, query lan-
guages used to find syntactic patterns in trees (Levy
and Andrew, 2006). This speeds up the process of
graph search and reduces errors that occur in com-
plicated traversal code.
5.1 Semgrex Features
Rather than providing regular expression match-
ing of atomic tree labels, as in most tree pattern
languages, Semgrex represents nodes as a (non-
recursive) attribute-value matrix. It then uses regular
expressions for subsets of attribute values. For ex-
ample, {word:run;tag:/?NN/} refers to any
node that has a value run for the attribute word and
a tag that starts with NN, while {} refers to any node
in the graph.
However, the most important part of Semgrex is
that it allows you to specify relations between nodes.
For example, {} <nsubj {} finds all the depen-
dents of nsubj relations. Logical connectives can
be used to form more complex patterns and node
naming can help retrieve matched nodes from the
patterns. Four base relations, shown in figure 1, al-
low you to specify the type of relation between two
nodes, in addition to an alignment relation (@) be-
tween two graphs.
5.2 Entailment Patterns
A particularly useful application of Semgrex is to
create relation entailment patterns. In particular, the
IE subtask of RTE has many characteristics that are
Semgrex Relations
Symbol #Description
{A} >reln {B} A is the governor of a reln relation
with B
{A} <reln {B} A is the dependent of a reln relation
with B
{A} >>reln {B} A dominates a node that is the
governor of a reln relation with B
{A} <<reln {B} A is the dependent of a node that is
dominated by B
{A} @ {B} A aligns to B
Figure 1: Semgrex relations between nodes.
not well suited to the core alignment features of our
system. We began integrating Semgrex into our sys-
tem by creating semantic alignment rules for these
IE tasks.
T: Bill Clinton?s wife Hillary was in Wichita today, continuing
her campaign.
H: Bill Clinton is married to Hillary. (TRUE)
Pattern:
({}=1
<nsubjpass ({word:married} >pp to {}=2))
@ ({} >poss ({lemma:/wife/} >appos {}=3))
This is a simplified version of a pattern that looks
for marriage relations. If it matches, additional pro-
grammatic checks ensure that the nodes labeled 2
and 3 are either aligned or coreferent. If they are,
then we add a MATCH feature, otherwise we add a
MISMATCH. Patterns included other familial rela-
tions and employer-employee relations. These pat-
terns serve both as a necessary component of an IE
entailment system and as a test drive of Semgrex.
5.3 Range of Application
Our rules for marriage relations correctly matched
six examples in the RTE3 development set and one
in the test set. Due to our system?s weaker per-
formance on the IE subtask of the data, we ana-
lyzed 200 examples in the development set for Sem-
grex applicability. We identified several relational
classes, including the following:
? Work: works for, holds the position of
? Location: lives in, is located in
? Relative: wife/husband of, are relatives
? Membership: is an employee of, is part of
? Business: is a partner of, owns
? Base: is based in, headquarters in
These relations make up at least 7% of the data, sug-
gesting utility from capturing other relations.
168
6 Natural Logic
We developed a computational model of natural
logic, the NatLog system, as another inference en-
gine for our RTE system. NatLog complements our
core broad-coverage system by trading lower recall
for higher precision, similar to (Bos and Markert,
2006). Natural logic avoids difficulties with translat-
ing natural language into first-order logic (FOL) by
forgoing logical notation and model theory in favor
of natural language. Proofs are expressed as incre-
mental edits to natural language expressions. Edits
represent conceptual contractions and expansions,
with truth preservation specified natural logic. For
further details, we refer the reader to (Sa?nchez Va-
lencia, 1995).
We define an entailment relation v between
nouns (hammer v tool), adjectives (deafening v
loud), verbs (sprint v run), modifiers, connectives
and quantifiers. In ordinary (upward-monotone)
contexts, the entailment relation between compound
expressions mirrors the entailment relations be-
tween their parts. Thus tango in Paris v dance
in France, since tango v dance and in Paris v in
France. However, many linguistic constructions cre-
ate downward-monotone contexts, including nega-
tion (didn?t sing v didn?t yodel), restrictive quanti-
fiers (few beetles v few insects) and many others.
NatLog uses a three-stage architecture, compris-
ing linguistic pre-processing, alignment, and entail-
ment classification. In pre-processing, we define a
list of expressions that affect monotonicity, and de-
fine Tregex patterns that recognize each occurrence
and its scope. This monotonicity marking can cor-
rectly account for multiple monotonicity inversions,
as in no soldier without a uniform, and marks each
token span with its final effective monotonicity.
In the second stage, word alignments from our
RTE system are represented as a sequence of atomic
edits over token spans, as entailment relations
are described across incremental edits in NatLog.
Aligned pairs generate substitution edits, unaligned
premise words yield deletion edits, and unaligned
hypothesis words yield insertion edits. Where pos-
sible, contiguous sequences of word-level edits are
collected into span edits.
In the final stage, we use a decision-tree classi-
fier to predict the elementary entailment relation (ta-
relation symbol in terms of v RTE
equivalent p = h p v h, h v p yes
forward p < h p v h, h 6v p yes
reverse p = h h v p, p 6v h no
independent p # h p 6v h, h 6v p no
exclusive p | h p v ?h, h v ?p no
Table 2: NatLog?s five elementary entailment relations. The last
column indicates correspondences to RTE answers.
ble 2) for each atomic edit. Edit features include
the type, effective monotonicity at affected tokens,
and their lexical features, including syntactic cate-
gory, lemma similarity, and WordNet-derived mea-
sures of synonymy, hyponymy, and antonymy. The
classifier was trained on a set of 69 problems de-
signed to exercise the feature space, learning heuris-
tics such as deletion in an upward-monotone context
yields<, substitution of a hypernym in a downward-
monotone context yields =, and substitution of an
antonym yields |.
To produce a top-level entailment judgment, the
atomic entailment predictions associated with each
edit are composed in a fairly obvious way. If r is any
entailment relation, then (= ? r) ? r, but (# ? r) ?
#. < and= are transitive, but (< ? =) ? #, and so
on.
We do not expect NatLog to be a general-purpose
solution for RTE problems. Many problems depend
on types of inference that it does not address, such
as paraphrase or relation extraction. Most pairs have
large edit distances, and more atomic edits means
a greater chance of errors propagating to the final
output: given the entailment composition rules, the
system can answer yes only if all atomic-level pre-
dictions are either< or =. Instead, we hope to make
reliable predictions on a subset of the RTE problems.
Table 3 shows NatLog performance on RTE3. It
makes positive predictions on few problems (18%
on development set, 24% on test), but achieves good
precision relative to our RTE system (76% and 68%,
respectively). For comparison, the FOL-based sys-
tem reported in (Bos and Markert, 2006) attained a
precision of 76% on RTE2, but made a positive pre-
diction in only 4% of cases. This high precision sug-
gests that superior performance can be achieved by
hybridizing NatLog with our core RTE system.
The reader is referred to (MacCartney and Man-
169
ID Premise(s) Hypothesis Answer
518 The French railway company SNCF is cooperating in
the project.
The French railway company is called SNCF. yes
601 NUCOR has pioneered a giant mini-mill in which steel
is poured into continuous casting machines.
Nucor has pioneered the first mini-mill. no
Table 4: Illustrative examples from the RTE3 test suite
RTE3 Development Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.25 68.66 66.99 67.25
Core -coref 49.88 66.42 64.32 64.88
NatLog 18.00 76.39 26.70 58.00
Hybrid, bal. 50.00 69.75 67.72 68.25
Hybrid, opt. 55.13 69.16 74.03 69.63
RTE3 Test Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.00 61.75 60.24 60.50
Core -coref 50.00 60.25 58.78 59.00
NatLog 23.88 68.06 31.71 57.38
Hybrid, bal. 50.00 64.50 62.93 63.25
Hybrid, opt. 54.13 63.74 67.32 63.62
Table 3: Performance on the RTE3 development and test sets.
% yes indicates the proportion of yes predictions made by the
system. Precision and recall are shown for the yes label.
ning, 2007) for more details on NatLog.
7 System Results
Our core systemmakes yes/no predictions by thresh-
olding a real-valued inference score. To construct
a hybrid system, we adjust the inference score by
+x if NatLog predicts yes, ?x otherwise. x is cho-
sen by optimizing development set accuracy when
adjusting the threshold to generate balanced predic-
tions (equal numbers of yes and no). As another
experiment, we fix x at this value and adjust the
threshold to optimize development set accuracy, re-
sulting in an excess of yes predictions. Results for
these two cases are shown in Table 3. Parameter
values tuned on development data yielded the best
performance. The optimized hybrid system attained
an absolute accuracy gain of 3.12% over our RTE
system, corresponding to an extra 25 problems an-
swered correctly. This result is statistically signifi-
cant (p < 0.01, McNemar?s test, 2-tailed).
The gain cannot be fully attributed to NatLog?s
success in handling the kind of inferences about
monotonicity which are the staple of natural logic.
Indeed, such inferences are quite rare in the RTE
data. Rather, NatLog seems to have gained primarily
by being more precise. In some cases, this precision
works against it: NatLog answers no to problem 518
(table 4) because it cannot account for the insertion
of called. On the other hand, it correctly rejects the
hypothesis in problem 601 because it cannot account
for the insertion of first, whereas the less-precise
core system was happy to allow it.
Acknowledgements
This material is based upon work supported in
part by the Disruptive Technology Office (DTO)?s
AQUAINT Phase III Program.
References
Johan Bos and Katja Markert. 2006. When logical inference
helps determining textual entailment (and when it doesn?t).
In Proceedings of the Second PASCAL RTE Challenge.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of EMNLP-2002.
Koby Crammer and Yoram Singer. 2001. Ultraconservative
online algorithms for multiclass problems. In Proceedings
of COLT-2001.
Marie-Catherine de Marneffe, Bill MacCartney, Trond Grena-
ger, Daniel Cer, Anna Rafferty, and Christopher D. Manning.
2006a. Learning to distinguish valid textual entailments. In
Second Pascal RTE Challenge Workshop.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006b. Generating typed dependency
parses from phrase structure parses. In 5th Int. Conference
on Language Resources and Evaluation (LREC 2006).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006. Recognizing textual entail-
ment with LCC?s GROUNDHOG system. In Proceedings of
the Second PASCAL RTE Challenge.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
Proceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation.
Bill MacCartney and Christopher D. Manning. 2007. Natu-
ral logic for textual inference. In ACL Workshop on Textual
Entailment and Paraphrasing.
Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005.
Robust textual inference via learning and abductive reason-
ing. In AAAI 2005, pages 1099?1105.
Victor Sa?nchez Valencia. 1995. Parsing-driven inference: Nat-
ural logic. Linguistic Analysis, 25:258?285.
170
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 41?49,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
WikiWalk: Random walks on Wikipedia for Semantic Relatedness
Eric Yeh, Daniel Ramage,
Christopher D. Manning
Computer Science Department,
Stanford University
Stanford, CA, USA
{yeh1,dramage,manning}@cs.stanford.edu
Eneko Agirre, Aitor Soroa
Ixa Taldea
University of the Basque Country
Donostia, Basque Country
{e.agirre,a.soroa}@ehu.es
Abstract
Computing semantic relatedness of natural
language texts is a key component of tasks
such as information retrieval and sum-
marization, and often depends on knowl-
edge of a broad range of real-world con-
cepts and relationships. We address this
knowledge integration issue by comput-
ing semantic relatedness using person-
alized PageRank (random walks) on a
graph derived from Wikipedia. This pa-
per evaluates methods for building the
graph, including link selection strategies,
and two methods for representing input
texts as distributions over the graph nodes:
one based on a dictionary lookup, the
other based on Explicit Semantic Analy-
sis. We evaluate our techniques on stan-
dard word relatedness and text similarity
datasets, finding that they capture similar-
ity information complementary to existing
Wikipedia-based relatedness measures, re-
sulting in small improvements on a state-
of-the-art measure.
1 Introduction
Many problems in NLP call for numerical mea-
sures of semantic relatedness, including document
summarization, information retrieval, and textual
entailment. Often, measuring the relatedness of
words or text passages requires world knowledge
about entities and concepts that are beyond the
scope of any single word in the document. Con-
sider, for instance, the following pair:
1. Emancipation Proclamation
2. Gettysburg Address
To correctly assess that these examples are re-
lated requires knowledge of the United States Civil
War found neither in the examples themselves nor
in traditional lexical resources such as WordNet
(Fellbaum, 1998). Fortunately, a massive collabo-
ratively constructed knowledge resource is avail-
able that has specific articles dedicated to both.
Wikipedia is an online encyclopedia containing
around one million articles on a wide variety of
topics maintained by over one hundred thousand
volunteer editors with quality comparable to that
of traditional encyclopedias.
Recent work has shown that Wikipedia can be
used as the basis of successful measures of se-
mantic relatedness between words or text pas-
sages (Strube and Ponzetto, 2006; Gabrilovich and
Markovitch, 2007; Milne and Witten, 2008). The
most successful measure, Explicit Semantic Anal-
ysis (ESA) (Gabrilovich and Markovitch, 2007),
treats each article as its own dimension in a vec-
tor space. Texts are compared by first projecting
them into the space of Wikipedia articles and then
comparing the resulting vectors.
In addition to article text, Wikipedia stores a
great deal of information about the relationships
between the articles in the form of hyperlinks, info
boxes, and category pages. Despite a long his-
tory of research demonstrating the effectiveness
of incorporating link information into relatedness
measures based on the WordNet graph (Budanit-
sky and Hirst, 2006), previous work on Wikipedia
has made limited use of this relationship infor-
mation, using only category links (Bunescu and
Pasca, 2006) or just the actual links in a page
(Gabrilovich and Markovitch, 2007; Milne and
Witten, 2008).
In this work, we combine previous approaches
by converting Wikipedia into a graph, mapping in-
put texts into the graph, and performing random
walks based on Personalized PageRank (Haveli-
wala, 2002) to obtain stationary distributions that
characterize each text. Semantic relatedness be-
tween two texts is computed by comparing their
distributions. In contrast to previous work, we
explore the use of all these link types when con-
41
structing the Wikipedia graph, the intuition being
these links, or some combination of them, con-
tain additional information that would allow a gain
over methods that use only just the article text. We
also discuss two methods for performing the initial
mapping of input texts to the graph, using tech-
niques from previous studies that utilized Word-
Net graphs and Wikipedia article text.
We find that performance is signficantly af-
fected by the strategy used to initialize the graph
walk, as well as the links selected when con-
structing the Wikipedia graph. Our best system
combines an ESA-initialized vector with random
walks, improving on state-of-the-art results over
the (Lee et al, 2005) dataset. An analysis of
the output demonstrates that, while the gains are
small, the random walk adds complementary re-
latedness information not present in the page text.
2 Preliminaries
A wide range of different methods, from corpus-
based distributional similarity methods, such as
Latent Semantic Analysis (Landauer et al, 1998),
to knowledge-based ones that employ structured
sources such as WordNet,
1
have been developed
to score semantic relatedness and similarity. We
now review two leading techniques which we use
as starting points for our approach: those that per-
form random walks over WordNet?s graph struc-
ture, and those that utilize Wikipedia as an under-
lying data source.
2.1 Random Graph Walks for Semantic
Relatedness
Some of the best performing WordNet-based al-
gorithms for computing semantic relatedness are
based on the popular Personalized PageRank al-
gorithm (Hughes and Ramage, 2007; Agirre and
Soroa, 2009). These approaches start by taking
WordNet as a graph of concepts G = (V,E) with
a set of vertices V derived from WordNet synsets
and a set of edges E representing relations be-
tween synsets. Both algorithms can be viewed
as random walk processes that postulate the ex-
istence of a particle that randomly traverses the
graph, but at any time may jump, or teleport, to
a new vertex with a given teleport probability. In
standard PageRank (Brin and Page, 1998), this tar-
get is chosen uniformly, whereas for Personalized
1
See (Budanitsky and Hirst, 2006) for a survey.
PageRank it is chosen from a nonuniform distribu-
tion of nodes, specified by a teleport vector.
The final weight of node i represents the propor-
tion of time the random particle spends visiting it
after a sufficiently long time, and corresponds to
that node?s structural importance in the graph. Be-
cause the resulting vector is the stationary distri-
bution of a Markov chain, it is unique for a par-
ticular walk formulation. As the teleport vector
is nonuniform, the stationary distribution will be
biased towards specific parts of the graph. In the
case of (Hughes and Ramage, 2007) and (Agirre
and Soroa, 2009), the teleport vector is used to re-
flect the input texts to be compared, by biasing the
stationary distribution towards the neighborhood
of each word?s mapping.
The computation of relatedness for a word pair
can be summarized in three steps: First, each input
word is mapped with to its respective synsets in
the graph, creating its teleport vector. In the case
words with multiple synsets (senses), the synsets
are weighted uniformly. Personalized PageRank is
then executed to compute the stationary distribu-
tion for each word, using their respective teleport
vectors. Finally, the stationary distributions for
each word pair are scored with a measure of vector
similarity, such as cosine similarity. The method
to compute relatedness for text pairs is analogous,
with the only difference being in the first step all
words are considered, and thus the stationary dis-
tribution is biased towards all synsets of the words
in the text.
2.2 Wikipedia as a Semantic Resource
Recent Wikipedia-based lexical semantic related-
ness approaches have been found to outperform
measures based on the WordNet graph. Two such
methods stand out: Wikipedia Link-based Mea-
sure (WLM) (Milne and Witten, 2008), and Ex-
plicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007).
WLM uses the anchors found in the body of
Wikipedia articles, treating them as links to other
articles. Each article is represented by a list of
its incoming and outgoing links. For word relat-
edness, the set of articles are first identified by
matching the word to the text in the anchors, and
the score is derived using several weighting strate-
gies applied to the overlap score of the articles?
links. WLM does not make further use of the link
graph, nor does it attempt to differentiate the links.
42
In contrast to WLM, Explicit Semantic Analy-
sis (ESA) is a vector space comparison algorithm
that does not use the link structure, relying solely
on the Wikipedia article text. Unlike Latent Se-
mantic Analysis (LSA), the underlying concept
space is not computationally derived, but is instead
based on Wikipedia articles. For a candidate text,
each dimension in its ESA vector corresponds to
a Wikipedia article, with the score being the sim-
ilarity of the text with the article text, subject to
TF-IDF weighting. The relatedness of two texts
is computed as the cosine similarity of their ESA
vectors.
Although ESA reports the best results to date
on both the WordSim-353 dataset as well as the
Lee sentence similarity dataset, it does not utilize
the link structure, which motivated a combined ap-
proach as follows.
2.3 A Combined Approach
In this work, we base our random walk algorithms
after the ones described in (Hughes and Ramage,
2007) and (Agirre et al, 2009), but use Wikipedia-
based methods to construct the graph. As in previ-
ous studies, we obtain a relatedness score between
a pair of texts by performing random walks over
a graph to compute a stationary distribution for
each text. For our evaluations, the score is simply
the cosine similarity between the distributions. In
the following sections, we describe how we built
graphs from Wikipedia, and how input texts are
initially mapped into these structures.
3 Building a Wikipedia Graph
In order to obtain the graph structure of Wikipedia,
we simply treat the articles as vertices, and
the links between articles as the edges. There
are several sources of pre-processed Wikipedia
dumps which could be used to extract the arti-
cles and links between articles, including DBpe-
dia (Auer et al, 2008), which provides a rela-
tional database representation of Wikipedia, and
Wikipedia-Miner
2
, which produces similar infor-
mation from Wikipedia dumps directly. In this
work we used a combination of Wikipedia-Miner
and custom processing scripts. The dump used in
this work is from mid 2008.
As in (Milne and Witten, 2008), anchors in
Wikipedia articles are used to define links between
2
http://wikipedia-miner.sourceforge.net
articles. Because of different distributional proper-
ties, we explicitly distinguish three types of links,
in order to explore their impact on the graph walk.
Infobox links are anchors found in the infobox
section of Wikipedia articles. Article in-
foboxes, when present, often enumerate
defining attributes and characteristics for that
article?s topic.
Categorical links reference articles whose titles
belong in the Wiki namespace ?Category,?
as well as those with titles beginning with
?List of.? These pages are often just lists of
anchors to other articles, which may be use-
ful for capturing categorical information that
roughly contains a mixture of hyponymy and
meronymy relations between articles.
Content links are those that are not already clas-
sified as infobox nor categorical, and are in-
tended to represent the set of miscellaneous
anchors found solely in the article body.
These may include links already found in the
categorical and infobox categories.
Links can be further factored out according to
generality, a concept introduced in (Gabrilovich
and Markovitch, 2009). We say that one article
is more general than another when the number of
inlinks is larger. Although only a rough heuris-
tic, the intuition is that articles on general top-
ics will receive many links, whereas specific ar-
ticles will receive fewer. We will use +k notation
for links which point to more general articles, i.e.,
where the difference in generality between source
s and target t is #inlink(t)/#inlink(s) ? k.
We will use ?k for links to less general articles,
i.e., #inlink(s)/#inlink(t) ? k. Finally we
use =k when the generality is in the same order
of magnitude, i.e., when the link is neither +k
nor ?k. The original notion of generality from
(Gabrilovich and Markovitch, 2009) restricts con-
sideration to only more general articles by one or-
der of magnitude (+10), without reference to the
link types introduced above.
Given the size of the Wikipedia graph, we ex-
plored further methods inspired by (Gabrilovich
and Markovitch, 2009) to make the graph smaller.
We discarded articles with fewer than 2,000 non-
stop words and articles with fewer than 5 outgoing
and incoming links. We will refer to the complete
43
graph as full and to this reduced graph as reduced.
3
4 Initializing a Wikipedia Graph Walk
In order to apply Personalized PageRank to a
given passage of text or word, we need to con-
struct a custom teleport vector, representing the
initial distribution of mass over the article nodes.
In this section we introduce two such methods,
one based on constructing a direct mapping from
individual words to Wikipedia articles (which we
call dictionary-based initialization), and the other
based directly on the results of ESA. We will see
each technique in turn.
4.1 Dictionary based initialization
Given a target word, we would like to define
its teleport vector using the set of articles in
Wikipedia to which the word refers. This is analo-
gous to a dictionary, where an entry lists the set of
meanings pertaining to the entry.
We explored several methods for building such
a dictionary. The first method constructed the dic-
tionary using the article title directly, while also
including redirection pages and disambiguation
pages for additional ways to refer to the article. In
addition, we can use the anchor text to refer to arti-
cles, and we turned to Wikipedia-Miner to extract
this information. Anchors are indeed a rich source
of information, as they help to relate similar words
to Wikipedia articles. For instance, links to page
Monk are created by using textual anchors such as
lama, brothers, monastery, etc. As a result, the
dictionary entries for those words will have a link
to the Monk page. This information turned out to
be very valuable, so all experiments have been car-
ried out using anchors.
An additional difficulty was that any of these
methods yielded dictionaries where the entries
could refer to tens, even hundreds of articles. In
most of the cases we could see that relevant arti-
cles were followed by a long tail of loosely related
articles. We tried two methods to prune the dic-
tionary. The first, coarse, method was to eliminate
all articles whose title contains a space. The mo-
tivation was that our lexical semantic relatedness
datasets (cf. Section 5) do not contain multiword
entries (e.g., United States). In the second method,
we pruned articles from the dictionary which ac-
3
In order to keep category and infobox links, the 2,000
non-stop word filter was not applied to categories and lists of
pages.
Graphs
Graph # Vertices # Edges
Full 2,483,041 49,602,752
Reduced 1,002,411 30,939,288
Dictionaries
Dictionary # Entries Avg. Articles
all 6,660,315 1.31
1% 6,660,306 1.12
1% noent 1,058,471 1.04
Table 1: Graph and dictionary sizes. Avg. Articles
column details the average number of articles per
entry.
counted for less than 1% or 10% of the occur-
rences of that anchor word, as suggested by (Milne
and Witten, 2008).
In short, for this method of initialization, we ex-
plored the use of the following variants: all, all ar-
ticles are introduced in the dictionary; noent, arti-
cles with space characters are omitted; 1% (10%),
anchors that account for less than 1% (10%) of the
total number of anchors for that entry are omitted.
We did not use stemming. If a target word has no
matching Wikipedia article in the dictionary, then
it is ignored.
Table 1 shows the numbers for some graph and
dictionary versions. Although the average number
of articles per entry in the dictionary might seem
low, it is actually quite high for the words in the
datasets: for MC it?s 5.92, and for wordsim353 it?s
42.14. If we keep the articles accounting for 10%
of all occurrences, the numbers drops drastically
to 1.85 and 1.64 respectively.
As we will see in the results section, smaller
graphs and dictionaries are able to attain higher
results, but at the cost of losing information for
some words. That is, we observed that some fac-
tored, smaller graphs contained less noise, but that
meant that some articles and words are isolated in
the graph, and therefore we are not able to com-
pute relatedness for them. As a solution, we de-
vised an alternative way to initialize the random
walk. Instead of initializing it according to the ar-
ticles in the dictionary, we initialized it with the
vector weights returned by ESA, as explained in
the next section.
44
4.2 Initialization with ESA
In addition to the dictionary based approach, we
also explored the use of ESA to construct the tele-
port vector. In contrast to dictionary initialization,
ESA uses the text of the article body instead of an-
chor text or the article titles. Because ESA maps
query text to a weighted vector of Wikipedia arti-
cles, it can be naturally adapted as a teleport vector
for a random walk with a simple L
1
normaliza-
tion. We used Apache Lucene
4
to implement both
ESA?s repository of Wikipedia articles, and to re-
turn vectors for queries. Each article is indexed as
its own document, with page text preprocessed to
strip out Wiki markup.
Although we followed the steps outlined in
(Gabrilovich and Markovitch, 2007), we had to
add an extension to the algorithm: for a return
vector from ESA, we order the articles by score,
and retain only the scores for the top-n articles,
setting the scores of the remaining articles to 0.
Without this modification, our performance results
were will below the reported numbers, but with a
cutoff at 625 (determined by a basic grid search),
we obtained a correlation of 0.76 on the Lee sen-
tence similarity dataset, over the previously pub-
lished score of 0.72.
4.3 Teleport Probability
For this work, we used a value of 0.15 as the prob-
ability of returning to the teleport distribution at
any given step. The walk terminates when the vec-
tor converges with an L
1
error of 0.0001 (circa 30
iterations). Some preliminary experiments on a re-
lated Word Sense Disambiguation task indicated
that in this context, our algorithm is quite robust to
these values, and we did not optimize them. How-
ever, we will discuss using different return param-
eters in Section 6.1.
5 Experiments
In this section, we compare the two methods of
initialization as well as several types of edges. For
a set of pairs, system performance is evaluated by
how well the generated scores correlate with the
gold scores. Gold scores for each pair are the av-
erage of human judgments for that pair. In order to
compare against previous results obtained on the
datasets, we use the Spearman correlation coeffi-
cient on the Miller Charles (MC) and WordSim-
353 word-pair datasets, and the Pearson correla-
4
http://lucene.apache.org
Dictionary Graph MC
all full 0.369
1% full 0.610
1%, noent full 0.565 (0.824)
1% reduced 0.563
1% reduced +2 0.530
1% reduced +4 0.601
1% reduced +8 0.512
1% reduced +10 0.491 (0.522)
10% full 0.604 (0.750)
10% reduced 0.605 (0.751)
10% reduced +2 0.491 (0.540)
10% reduced +4 0.476 (0.519)
10% reduced +8 0.474 (0.506)
10% reduced +10 0.430 (0.484)
WordNet 0.90 / 0.89
WLM 0.70
ESA 0.72
Table 2: Spearman correlation on the MC dataset
with dictionary-based initialization. Refer to Sec-
tion 3 for explanation of dictionary and graph
building methods. Between parenthesis, results
excluding pairs which had a word with an empty
dictionary entry.
tion coefficient on the (Lee et al, 2005) document-
pair dataset.
5.1 Dictionary-based Initialization
Given the smaller size of the MC dataset, we
explored the effect of the different variants to
build the graph and dictionary on this dataset.
Some selected results are shown in Table 2, along-
side those of related work, where we used Word-
Net for (Hughes and Ramage, 2007) and (Agirre
et al, 2009) (separated by ?/? in the results),
WLM for (Milne and Witten, 2008) and ESA for
(Gabrilovich and Markovitch, 2007).
We can observe that using the full graph and
dictionaries yields very low results. Reducing the
dictionary (removing articles with less than 1% or
10% of the total occurrences) produces higher re-
sults, but reducing the graph does not provide any
improvement. On a closer look, we realized that
pruning the dictionary to 10% or removing multi-
words (noent) caused some words to not get any
link to articles (e.g., magician). If we evaluate
only over pairs where both words get a Personal-
ized PageRank vector, the results raise up to 0.751
and 0.824, respectively, placing our method close
45
Dictionary Graph WordSim-353
1% full 0.449
1%, noent full 0.440 (0.634)
1% reduced 0.485
WordNet 0.55 / 0.66
WLM 0.69
ESA 0.75
WikiRelate 0.50
Table 3: Spearman correlation on the WordSim-
353 dataset with dictionary-based initialization.
Refer to Section 3 for explanation of dictionary
and graph building methods. Between parenthe-
sis, results excluding pairs which had a word with
an empty dictionary entry.
Dictionary Graph (Lee et al, 2005)
1%, noent Full 0.308
1% Reduced +4 0.269
ESA 0.72
Table 4: Pearson correlation on (Lee et al, 2005)
with dictionary-based initialization. Refer to Sec-
tion 3 for explanation of dictionary and graph
building methods.
to the best results on the MC dataset. This came
at the cost of not being able to judge the related-
ness of 3 and 5 pairs, respectively. We think that
removing multiwords (noent) is probably too dras-
tic, but the positive effect is congruent with (Milne
and Witten, 2008), who suggested that the cover-
age of certain words in Wikipedia is not adequate.
The results in Table 3 show the Spearman cor-
relation for some selected runs over the WordSim-
353 dataset. Again we see that a restrictive dic-
tionary allows for better results on the pairs which
do get a dictionary entry, up to 0.63. WikiRelate
refers to the results in (Strube and Ponzetto, 2006).
We only tested a few combinations over (Lee et
al., 2005), with results given in Table 4. These are
well below state-of-the-art, and show that initial-
izing the random walk with all words in the doc-
ument does not characterize the documents well,
resulting in low correlation.
5.2 ESA-based initialization
While the results using a dictionary based ap-
proach were encouraging, they did not come close
to the state-of-the-art results achieved by ESA.
Here, we explore combining ESA and random
Method Text Sim
ESA@625 0.766
ESA@625+Walk All 0.556
ESA@625+Walk Categories 0.410
ESA@625+Walk Content 0.536
ESA@625+Walk Infobox 0.710
Table 5: Pearson correlation on the (Lee et al,
2005) dataset when walking on various types of
links. Note that walking tends to hurt performance
overall, with Infobox links by far the least harm-
ful.
walks, by using ESA to initialize the teleport vec-
tor. Following section 4.2, we used a top-n cutoff
of 625.
Table 5 displays the results of our ESA im-
plementation followed by a walk from that ESA
distribution. Walking on any link type actually
depresses performance below the baseline ESA
value, although the Infobox links seem the least
harmful.
However, as mentioned in Section 3, links be-
tween articles represent many different types of
relationships beyond the few well-defined links
present in lexical resources like WordNet. This
also extends to where the link is found, and the ar-
ticle it is pointing to. As such, not all links are cre-
ated equal, and we expect that some types of links
at different levels of generality will perform bet-
ter or worse than others. Table 6 presents a sam-
ple grid search across the category links choosing
more general, less general, or similar generality at
several factors of k, showing that there is a consis-
tent pattern across multiple link types. Note that
the best value indeed improves upon the score of
the ESA distribution, albeit modestly.
We performed a similar analysis across all link
types and found that the best link types were Cat-
egory links at +6 and Infobox links at =2. Intu-
itively, these link types make sense: for seman-
tic relatedness, it seem reasonable to expect more
general pages within the same category to help.
And for Infobox links, much rarer and much more
common pages can both introduce their own kind
of noise. While the improvement from each type
of edge walk is small, they are additive?the best
results on the sentence similarity dataset was from
walking across both link types. Our final Pearson
correlation coefficient of .772 is to our knowledge
the highest number reported in the literature, al-
46
Generality of Category links
+k -k =k
k = 2 0.760 0.685 0.462
k = 4 0.766 0.699 0.356
k = 6 0.771 0.729 0.334
k = 8 0.768 0.729 0.352
k = 10 0.768 0.720 0.352
Table 6: Pearson correlation on the (Lee et al,
2005) with random walks over only a subset of
the edges in the Category link information (scores
.410 when taking all edges). Note that factoring
the graph by link generality can be very helpful to
the walk.
Method Text Sim
ESA@625 0.766
ESA@625+Walk Cat@+6 0.770
ESA@625+Walk Cat@+6 Inf@=2 0.772
Bag of words (Lee et al, 2005) 0.5
LDA (Lee et al, 2005) 0.60
ESA* 0.72
Table 7: Pearson correlation on the (Lee et al,
2005) dataset for our best sytems compared to pre-
viously reported numbers. ESA* is the score for
raw ESA as reported number in (Gabrilovich and
Markovitch, 2007).
beit only a small improvement over our ESA@625
score.
Despite the results obtained for text similarity,
the best settings found for the Lee dataset did not
translate to consistent improvements over the ESA
baseline for Spearman rank correlation on the lex-
ical similarity datasets. While our scores on the
MC dataset of 30 word pairs did improve with the
walk in roughly the same way as in Lee, no such
improvements were found on the larger WordSim-
353 data. On WordSim-353, our implementa-
tion of ESA scored 0.709 (versus Gabrilovich?s
reported ESA score of 0.75), and our walk on
Cat@+6 showing no gain or loss. In contrast to
the text similarity dataset, Infobox links were no
longer helpful, bringing the correlation down to
.699. We believe this is because Infobox links
helped the most with entities, which are very rare
in the WordSim-353 data, but are more common
in the Lee dataset.
6 Discussion
Our results suggest that even with a simple
dictionary-based approach, the graph of Wikipedia
links can act as an effective resource for comput-
ing semantic relatedness. However, the dictio-
nary approach alone was unable to reach the re-
sults of state-of-the-art models using Wikipedia
(Gabrilovich and Markovitch, 2007; Milne and
Witten, 2008) or using the same technique on
WordNet (Hughes and Ramage, 2007; Agirre
et al, 2009). Thus, it seems that the text of
Wikipedia provides a stronger signal than the link
structure. However, a pruned dictionary can im-
prove the results of the dictionary based initial-
ization, which indicates that some links are in-
formative for semantic relatedness while others
are not. The careful pruning, disambiguation and
weighting functions presented in (Milne and Wit-
ten, 2008) are directions for future work.
The use of WordNet as a graph provided ex-
cellent results (Hughes and Ramage, 2007), close
to those of ESA. In contrast with our dictionary-
based initialization on Wikipedia, no pruning of
dictionary or graph seem necessary to obtain high
results with WordNet. One straightforward expla-
nation is that Wikipedia is a noisy source of link
information. In fact, both ESA and (Milne and
Witten, 2008) use ad-hoc pruning strategies in or-
der to obtain good results.
6.1 ESA and Walk Comparison
By using ESA to generate the teleport distribu-
tion, we were able to introduce small gains us-
ing the random walk. Because these gains were
small, it is plausible that the walk introduces only
modest changes from the initial ESA teleport dis-
tributions. To evaluate this, we examined the dif-
ferences between the vector returned by ESA and
distribution over the equivalent nodes in the graph
after performing a random walk starting with that
ESA vector.
For this analysis, we took all of the text entries
used in this study, and generated two distributions
over the Wikipedia graph, one using ESA@625,
the other the result of performing a random walk
starting at ESA@625. We generated a list of the
concept nodes for both distributions, sorted in de-
creasing order by their associated scores. Start-
ing from the beginning of both lists, we then
counted the number of matched nodes until they
disagreed on ordering, giving a simple view of
47
Walk Type Avg Std Max
MC Cat@+6 12.1 7.73 35
Cat@+6 Inf@=2 5.39 5.81 20
WordSim Cat@+6 12.0 10.6 70
Cat@+6 Inf@=2 5.74 7.78 54
Lee Cat@+6 28.3 89.7 625
Cat@+6 Inf@=2 4.24 14.8 103
Table 8: Statistics for first concept match length,
by run and walk type.
how the walk perturbed the strongest factors in the
graphs. We performed this for both the best per-
forming walk models (ESA@625+Walk Cat@+6
and ESA@625+Walk Cat@+6 Inf@=2) against
ESA@625. Results are given in Table 8.
As expected, adding edges to the random walk
increases the amount of change from the graph,
as initialized by ESA. A cursory examination of
the distributions also revealed a number of outliers
with extremely high match lengths: these were
likely due to the fact that the selected edge types
were already extremely specialized. Thus for a
number of concept nodes, it is likely they did not
have any outbound edges at all.
Having established that the random walk does
indeed have an impact on the ESA vectors, the
next question is if changes via graph walk are
consistently helpful. To answer this, we com-
pared the performance of the walk on the (Lee et
al., 2005) dataset for probabilities at selected val-
ues, using the best link pruned Wikipedia graph
(ESA@625+Walk Cat@+6 Inf@=2), and using all
of the available edges in the graph for compari-
son. Here, a lower probability means the distribu-
tion spreads out further into the graph, compared
to higher values, where the distribution varies only
slightly from the ESA vector. Results are given in
Table 9. Performance for the pruned graph im-
proves as the return probability decreases, with
larger changes introduced by the graph walk re-
sulting in better scores, whereas using all available
links decreases performance. This reinforces the
notion that Wikipedia links are indeed noisy, but
that within a selected edge subset, making use of
all information via the random walk indeed results
in gains.
7 Conclusion
This paper has demonstrated that performing ran-
dom walks with Personalized PageRank over the
Prob Corr (Pruned) Corr (All)
0.01 0.772 0.246
0.10 0.773 0.500
0.15 0.772 0.556
0.30 0.771 0.682
0.45 0.769 0.737
0.60 0.767 0.758
0.90 0.766 0.766
0.99 0.766 0.766
Table 9: Return probability vs. correlation, on tex-
tual similarity data (Lee et al, 2005).
Wikipedia graph is a feasible and potentially fruit-
ful means of computing semantic relatedness for
words and texts. We have explored two methods of
initializing the teleport vector: a dictionary-based
method and a method based on ESA, the cur-
rent state-of-the-art technique. Our results show
the importance of pruning the dictionary, and for
Wikipedia link structure, the importance of both
categorizing by anchor type and comparative gen-
erality. We report small improvements over the
state-of-the-art on (Lee et al, 2005) using ESA as
a teleport vector and a limited set of links from
Wikipedia category pages and infoboxes.
In future work, we plan to explore new ways
to construct nodes, edges, and dictionary entries
when constructing the Wikipedia graph and dic-
tionary. We believe that finer grained methods of
graph construction promise to improve the value
of the Wikipedia link structure. We also plan to
further investigate the differences between Word-
Net and Wikipedia and how these may be com-
bined, from the perspective of graph and random
walk techniques. A public distribution of software
used for these experiments will also be made avail-
able.
5
Acknowledgements
The authors would like to thank Michael D. Lee
and Brandon Pincombe for access to their textual
similarity dataset, and the reviewers for their help-
ful comments. Eneko Agirre performed part of
the work while visiting Stanford, thanks to a grant
from the Science Ministry of Spain.
5
Please see http://nlp.stanford.edu/
software and http://ixa2.si.ehu.es/ukb
48
References
E. Agirre and A. Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proceedings
of 14th Conference of the European Chapter of the
Association for Computational Linguistics, Athens,
Greece.
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pasc?a, and A. Soroa. 2009. A study on similarity
and relatedness using distributional and WordNet-
based approaches. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies,
Boulder, USA.
S Auer, C Bizer, G Kobilarov, J Lehmann, R Cyganiak,
and Z Ives. 2008. Dbpedia: A nucleus for a web
of open data. In Proceedings of 6th International
Semantic Web Conference, 2nd Asian Semantic Web
Conference (ISWC+ASWC 2007), pages 722?735.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1-7).
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic relat-
edness. Computational Linguistics, 32(1):13?47.
R. C. Bunescu and M. Pasca. 2006. Using encyclo-
pedic knowledge for named entity disambiguation.
In Proceedings of 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. MIT Press.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-07).
E. Gabrilovich and S. Markovitch. 2009. Wikipedia-
based semantic interpretation. Journal of Artificial
Intelligence Research, 34:443?498.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02, pages 517?526, New York, NY, USA.
ACM.
T. Hughes and D. Ramage. 2007. Lexical semantic re-
latedness with random graph walks. In Proceedings
of EMNLP-CoNLL, pages 581?589.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. An
introduction to latent semantic analysis. Discourse
Processes, 25(2-3):259?284.
M. D. Lee, B. Pincombe, and M. Welsh. 2005. An em-
pirical evaluation of models of text document sim-
ilarity. In Proceedings of the 27th Annual Confer-
ence of the Cognitive Science Society, pages 1254?
1259, Mahwah, NJ. Erlbaum.
D. Milne and I.H. Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
Wikipedia links. In Proceedings of the first AAAI
Workshop on Wikipedia and Artifical Intellegence
(WIKIAI?08), Chicago, I.L.
M. Strube and S.P. Ponzetto. 2006. Wikirelate! com-
puting semantic relatedness using Wikipedia. In
Proceedings of the 21st National Conference on Ar-
tificial Intelligence, pages 1419?1424.
49
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 617?623,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SRIUBC: Simple Similarity Features for Semantic Textual Similarity
Eric Yeh
SRI International
Menlo Park, CA USA
yeh@ai.sri.com
Eneko Agirre
University of the Basque Country
Donostia, Basque Country
e.agirre@ehu.es
Abstract
We describe the systems submitted by SRI In-
ternational and the University of the Basque
Country for the Semantic Textual Similarity
(STS) SemEval-2012 task. Our systems fo-
cused on using a simple set of features, fea-
turing a mix of semantic similarity resources,
lexical match heuristics, and part of speech
(POS) information. We also incorporate pre-
cision focused scores over lexical and POS in-
formation derived from the BLEU measure,
and lexical and POS features computed over
split-bigrams from the ROUGE-S measure.
These were used to train support vector re-
gressors over the pairs in the training data.
From the three systems we submitted, two per-
formed well in the overall ranking, with split-
bigrams improving performance over pairs
drawn from the MSR Research Video De-
scription Corpus. Our third system maintained
three separate regressors, each trained specif-
ically for the STS dataset they were drawn
from. It used a multinomial classifier to pre-
dict which dataset regressor would be most ap-
propriate to score a given pair, and used it to
score that pair. This system underperformed,
primarily due to errors in the dataset predictor.
1 Introduction
Previous semantic similarity tasks, such as para-
phrase identification or recognizing textual entail-
ment, have focused on performing binary decisions.
These problems are usually framed in terms of iden-
tifying whether a pair of texts exhibit the needed
similarity or entailment relationship or not. In many
cases, such as producing a ranking over similarity
scores, a soft measure of similarity between a pair
of texts would be more desirable.
We contributed three systems for the 2012 Se-
mantic Textual Similarity (STS) task (Agirre et al,
2012). These are:
1. System 1, which used a combination of seman-
tic similarity, lexical similarity, and precision
focused part-of-speech (POS) features.
2. System 2, which used features from System
1, with the addition of skip-bigram features
derived from the ROUGE-S (Lin, 2004) mea-
sure. POS variants of skip-bigrams were incor-
porated as well.
3. System 3, used the features from above to first
classify the dataset the pair was drawn from,
and then applied regressors trained for that
dataset.
Our systems characterize sentence pairs as feature
vectors, populated by a variety of scorers that will be
described below. During training, we used support
vector regression (SVR) to train regressors against
these vectors and their associated similarity scores.
The STS training data is divided into three
datasets, reflecting their origin: Microsoft Research
Paraphrase Corpus (MSRpar), MSR Research Video
Description Corpus (MSRvid), and WMT2008 De-
velopment dataset (SMTeuroparl). We trained indi-
vidual regressors for each of these datasets, and ap-
plied them to their counterparts in the testing set.
Both Systems 1 and 2 used the following types of
features:
617
1. Resource based word to word semantic similar-
ities.
2. Cosine-based lexical similarity measure.
3. Bilingual Evaluation Understudy (BLEU) (Pa-
pineni et al, 2002) lexical overlap.
4. Precision focused Part of Speech (POS) fea-
tures.
System 2 added the following features:
1. Lexically motivated skip-bigram overlap.
2. Precision focused skip-bigram POS features.
One of the primary motivations for our the choice
of features was to use relatively simple and fast fea-
tures, which can be scaled up to large datasets, given
appropriate caching and pre-generated lookups. As
the test phase included surprise datasets, whose ori-
gin was not disclosed, we also trained a fourth model
using all of the training data from all three datasets.
Systems 1 and 2 employed this strategy for the sur-
prise data.
Since the statistics for each of the training datasets
varied, directly pooling them together may not be
the best strategy when scoring the surprise data,
whose origins were unknown. To account for this,
System 3 treated this as a gated regression problem,
where pairs are considered to originate strictly from
one dataset, and to score using a model specifically
tailored for that dataset. We first trained regressors
on each of the datasets separately. Then we trained
a classifier to predict which dataset a given pair is
likeliest to have been drawn from, and then applied
the matching trained regressor to obtain its score.
This team included one of the organizers. We
want to stress that we took all measures to make our
participation on the same conditions as the rest of
participants. In particular, the organizer did not al-
low the other member of the team to access any data
or information which was not already available for
the rest of participants.
For the rest of this system description, we first
outline the scorers used to populate the feature vec-
tors used for Systems 1 and 2. We then describe
the setup for performing the regression. We follow
with an explanation of our strategies for dealing with
the surprise data, including a description of System
3. We then summarize performance over the the
datasets, and discuss future avenues of investigation.
2 Resource Based Similarity
Our system uses several resources for assessing the
word to word similarity between a pair of sentences.
In order to pool together the similarity scores for a
given pair, we employed the Semantic Matrix (Fer-
nando and Stevenson, 2008) framework. To gen-
erate the scores, we used several resources, princi-
pally those derived from the relation graph of Word-
Net (Fellbaum, 1998), and those derived from distri-
butional resources, namely Explicit Semantic Anal-
ysis (Gabrilovich and Markovitch, 2009), and the
Dekang Lin Proximity-based Thesaurus 1. We now
describe the Semantic Matrix method, and follow
with descriptions of each of the resources used.
2.1 Semantic Matrix
The Semantic Matrix is a method for pooling all
of the pairwise similarity scores between the to-
kens found in two input strings. In order to score
the similarity between a pair of strings s1 and s2
we first identify all of the unique vocabulary words
from these strings to derive their corresponding oc-
currence vectors v1 and v2. Each dimension of
these vectors corresponds to a unique vocabulary
word, and binary values were used, corresponding
to whether that word was observed. The similarity
score for pair, sim(s1, s2), is given by Formula 1.
sim(s1, s2) =
vT1 Wv2
?v1? ?v2?
(1)
with W being the symmetric matrix marking the
similarity between pairs of words in the vocabulary.
We note that this is similar to the Mahalanobis dis-
tance, except adjusted to produce a similarity. For
this experiment, we normalized matrix entries so all
values lay in the 0-1 range.
As named entities and other words encountered
may not appear in one or more of the resources used,
we applied the identity to W. This is equivalent to
adding a strict lexical match fallback on top of the
similarity measure.
1http://webdocs.cs.ualberta.ca/ lindek/downloads.htm
618
Per (Fernando and Stevenson, 2008), a filter was
applied over the values of W. Any entries that fell
below a given threshold value were flattened to zero,
in order to prevent low scoring similarities from
overwhelming the score. From previous studies over
MSRpar, we applied a threshold of 0.9.
For our experiments, each of the word to word
similarity scorers described below were used to gen-
erate a corresponding word similarity matrix W,
with scores generated using the Semantic Matrix.
2.2 WordNet Similarity
We used several methods to obtain word to word
similarities from WordNet. WordNet is a lexical-
semantic resource that describes typed relationships
between synsets, semantic categories a word may
belong to. Similarity scoring methods identify the
synsets associated with a pair of words, and then use
this relationship graph to generate a score.
The first set of scorers were generated from the
Leacock-Chodorow, Lin, and Wu-Palmer measures
from the WordNet Similarity package (Pedersen et
al., 2004). For each of these measures, we averaged
across all of the possible synsets between a given
pair of words.
Another scorer we used was Personalized PageR-
ank (PPR) (Agirre et al, 2010), a topic sensitive
variant of the PageRank algorithm (Page et al,
1999) that uses a random walk process to identify
the significant nodes of a graph given its link struc-
ture. We first derived a graph G from WordNet,
treating synsets as the vertices and the relationships
between synsets as the edges. To obtain a signature
for a given word, we apply topic sensitive PageRank
(Haveliwala, 2002) over G, using the synsets asso-
ciated with the word as the initial distribution. At
convergence, we convert the stationary distribution
into a vector. The similarity between two words is
the cosine similarity between their vectors.
2.3 Distributional Resources
In contrast with the structure based WordNet based
methods, distributional methods use statistical prop-
erties of corpora to derive similarity scores. We gen-
erated two scorers, one based on Explicit Seman-
tic Analysis (ESA), and the other on the Dekang
Lin Proximity-based Thesaurus. For a given word,
ESA generates a concept vector, where the con-
cepts are Wikipedia articles, and the score measures
how closely associated that word is with the textual
content of the article. To score the similarity be-
tween two words, we computed the cosine similar-
ity of their concept vectors. This method proved to
give state-of-the-art performance on the WordSim-
353 word pair relatedness dataset (Finkelstein et al,
2002).
The Lin Proximity-based Thesaurus identifies
the neighborhood around words encountered in the
Reuters and Text Retrieval Conference (TREC). For
a given word, the Thesaurus identifies the top 200
words with the most similar neighborhoods, listing
the score based on these matches. For our experi-
ments, we treated these as feature vectors, with the
intuition being similar words should share similar
neighbors. Again, the similarity score between two
words was scored using the cosine similarity of their
vectors.
3 Cosine Similarity
Another scorer we used was the cosine similarity
over the lemmas found in the sentences in a pair.
For generating the vectors used in the cosine simi-
larity computation, we used the term frequency of
the lemmas.
4 BLEU Features
BLEU is a measure developed to automatically as-
sess how closely sentences generated by machine
translation systems match reference human gener-
ated texts. BLEU is a directional measurement, and
works on the assumption that the more lexically sim-
ilar a system generated sentence is to a reference sen-
tence, a human generated translation, the better the
system sentence is. This can also be seen as a stand-
in for the semantic similarity of the pairs, as was
shown when BLEU was applied to the paraphrase
identification identification problem in (Finch et al,
2005).
The BLEU score for a given system sentence and
reference sentence of order N is computed using
Formula 2.
BLEU(sys, ref) = B ? exp
N?
n=1
1
N
log(pn) (2)
619
B is a brevity penalty used to prevent degenerate
translations. Given this has little bearing on our ex-
periments, we set its value to 1 for our experiments.
Following (Papineni et al, 2002), we give each order
n equal weight in the geometric mean. The proba-
bility of an order n-gram from the system sentence
being found in the reference, pn, is given in Formula
3.
pn =
?
ngram?sys countsys?ref (ngram)
?
ngram?sys countsys(ngram)
(3)
countsys(ngram) is frequency of oc-
currence for the given n-gram in the sys-
tem sentence. The numerator term is
computed as countsys?ref (ngram) =
min(countsys(ngram), countref (ngram)) where
countref (ngram) is the frequency of occurrence
of that n-gram in the reference sentence. This
is equivalent to having each n-gram have a 1-1
mapping with a matching n-gram in the reference
(if any), and counting the number of mappings.
As there is a risk of higher order system n-grams
having no matches in the reference, we apply Lapla-
cian smoothing to the n-gram counts.
BLEU is considered to be a precision focused
measure, as it only measures how much of the sys-
tem sentence matches a reference sentence. Follow-
ing (Finch et al, 2005), we obtain a modified BLEU
score for strings s1 and s2 of a pair by averaging the
BLEU scores where each takes a turn as the system
sentence, as given in Formula 4.
Score(s1, s2) =
1
2
BLEU(s1, s2) ? BLEU(s2, s1)
(4)
For our experiments, we used BLEU scores of or-
der N = 1..4, over n-grams formed over the sen-
tence lemmas, and used these as features for charac-
terizing a given pair.
4.1 Precision Focused POS Features
From past experiments with paraphrase identifica-
tion over the MSR Paraphrase Corpus, we have
found including POS information to be beneficial.
To this capture this kind of information, we gen-
erated precision focused POS features, which mea-
sures the following between the sentences in a prob-
lem pair:
1. The overlap in POS tags.
2. The mismatch in POS tags.
We follow the formulation for POS vectors given
in (Finch et al, 2005). For a given sentence pair,
we identify the set of words whose lemmas were
matched in both the system and reference sentences,
Wmatch and those with no matches, Wmiss. Using
the directional notion of system and reference sen-
tences from BLEU, for each word w ?Wmatch,
POSMatch(t, sys, ref) =
?
w?Wmatch
countt(w)
|sys|
(5)
where countt is 1 if wordw has the matching POS
tag, and 0 otherwise. |sys| is the token count of the
system sentence. This is deemed to be precision-
focused, as this computation is done over candidates
found in the system sentence.
To generate the score for missing POS tags, we
perform a similar computation,
POSMiss(t, sys, ref) =
?
w?Wmiss
countt(w)
|sys|
(6)
To score the POS match and misses between a
pair, we follow Formula 4 and average the scores
for each POS tag, where the sentences in a given
pair swap positions as the system and reference sen-
tences.
5 Split-Bigram Features
System 2 added split-bigram features, which were
derived from the ROUGE-S measure. Like bigrams,
split-bigrams consist of an ordered pair of distinct
tokens drawn from a source sentence. Unlike bi-
grams, split-bigrams allow for a number of inter-
vening tokens to appear between the split-bigram to-
kens. For example, ?The cat ate fish.? would gen-
erate the following split-bigrams the?cat, the?ate,
the?fish, cat?ate, cat?fish, and ate?fish. The in-
tent of split-bigrams is to quickly capture long range
620
dependencies, without requiring a parse of the sen-
tence.
Similar to ROUGE-S, we used lexical overlap
of the split-bigrams as an approximation of seman-
tic similarity. As our pairs are bidirectional, we
used the same framework (Formula 2) for obtain-
ing BLEU scores to generate split-bigram overlap
scores for our pairs. Here, counts are obtained over
split-bigrams found in the system and reference sen-
tences, and the order was set to 1.
For generating the skip-bigram overlap score for
a pair, we used a maximum distance of three.
5.1 Skip-Bigram POS Features
In the same vein as the precision focused POS
features, we used the POS tags of matched split-
bigrams as features, where the frequency of the
POS tags in split-bigrams, t ? t?, were used.
Here, Bmatch represents the split-bigrams which
were found in both the system and reference sen-
tences, matched on lexical content.
SBMatch(t? t?, sys, ref) =
?
b?Bmatch
countt?t?(b)
|sys|
(7)
Due to sparsity, we only considered scores from
split-bigram matches between the system and ref-
erence sentences, and do not model split-bigram
misses. As before, we generate scores for each split-
bigram tag sequence by averaging the scores where
both sentences in a pair have swapped positions. For
our experiments, we only considered split-bigram
POS features of up to distance 3. In our initial exper-
iments we found split-bigram POS features helped
only in the case of shorter sentence pairs, so we only
generated features if both the sentences in a given
pair contained ten tokens or less.
6 Experimental Setup
For all three systems, we used the Stanford
CoreNLP (Toutanova et al, 2003) package to per-
form lemmatization and POS tagging of the in-
put sentences. For regressors, we used LibSVM?s
(Chang and Lin, 2011) support vector regression ca-
pability, using radial basis kernels. Based off of tun-
ing on the training set, we set ? = 1 and the default
Dataset Mean Std.Dev
MSRpar 3.322 0.9294
MSRvid 2.135 1.595
SMTeur 4.307 0.7114
Table 1: Means and standard deviations of similarity
scores for each of the training datasets.
slack value.
From previous experience with paraphrase iden-
tification over the MSR Paraphrase Corpus, we re-
tained stop words in all of our experiments.
7 Dealing with Surprise Data
As the STS training data was broken into three sep-
arate datasets, each with their own distinct statistics,
we developed three regressors trained individually
on each of these datasets. This presented a problem
when dealing with surprise datasets, whose statistics
were not known.
The approach taken by Systems 1 and 2 was sim-
ply to pool together all three training datasets into a
single dataset and train a single regressor on that uni-
fied model. We then applied that regressor against
the two surprise datasets, OnWN and SMTnews.
Analysis of the similarity score statistics showed
that they varied greatly between each of the train-
ing sets, as given in Table 1. Thus combining the
datasets blindly, as with Systems 1 and 2, may prove
to be a suboptimal strategy. The approach taken by
System 3 was to consider the feature vectors them-
selves as capturing information about which dataset
they were drawn from, and to use a classifier to pre-
dict that dataset. We then emit the score from the
regressor trained on just that matching dataset. We
used the Stanford Classifier?s (Manning and Klein,
2003) multinomial logistic regression as our dataset
predictor, using the feature vectors from System 2.
Five-fold cross validation over the training data
showed the dataset predictor to have an overall ac-
curacy of 91.75%.
In order to assess performance over the known
datasets at test time, System 3 also applied the same
strategy for the MSRpar, MSRvid, and SMTeuroparl
test sets.
621
Sys All Allnorm Mean MSRpar MSRvid SMTeur OnWN SMTnews
1 0.7513 / 11 0.8017 / 40 0.5997 / 22 0.6084 0.7458 0.4688 0.6315 0.3994
2 0.7562 / 10 0.8111 / 24 0.5858 / 33 0.6050 0.7939 0.4294 0.5871 0.3366
3 0.6876 / 21 0.7812 / 54 0.4668 / 68 0.4791 0.7901 0.2159 0.3843 0.2801
Table 2: Pearson correlation of described systems against test data, by dataset. Overall measures are All indicates the
combined Pearson, Allnorm the normalized variant, and Mean the macro average of Pearson correlations. Rank for
the system in the overall measure is given after the slash.
Guess/Gold MSRpar MSRvid SMTeur
MSRpar 664 7 75
MSRvid 7 737 10
SMTeur 79 6 649
Table 3: Confusion for the dataset predictor, used to pre-
dict which dataset a pair was drawn from. This was
ddrawn using five-fold cross validation over the training
set, with columns representing golds and guesses as rows.
8 Results and Discussion
Results on the test data for each of the systems
against the individual datasets, are given in Table
2, given in Pearson linear correlation with the gold
standard scores. Overall measures for the systems
are given, along with their overall ranking.
The split-bigram features in System 2 contributed
primarily to performance over the MSRvid dataset,
while degrading performance on the other datasets
slightly. This is likely a result of increasing spar-
sity in the feature space, but overall this system per-
formed well. System 3 underperformed on most
datasets, asides from its performance on MSRvid.
The confusion generated over five-fold cross vali-
dation over the training set is given in Table 3, and
precision, recall, and F1 scores by dataset label from
five-fold cross validation over the training set are
given in Table 4. As these show, predictor errors lay
primarily in confusing MSRpar for SMTeuroparl,
and vice versa. This error was significant enough to
reduce performance on both the MSRpar and SM-
Teuroparl test sets. This proved to be enough to re-
duce the scores between these two datasets.
9 Conclusion and Future Work
Our STS systems have shown that relatively sim-
ple syntax free methods can be employed to the
STS task. Future avenues of investigation would
Dataset Prec Rec F1
MSRpar 0.8901 0.8853 0.8877
MSRvid 0.9775 0.9827 0.9801
SMTeur 0.8842 0.8842 0.8842
Table 4: Results on classifying pairs by source dataset,
using five-fold cross validation over training data.
be to include the use of syntactic information, in
order to obtain better predicate-argument informa-
tion. Syntactic information has proven useful for
the paraphrase identification task over MSRpar, as
demonstrated in studies such as (Das and Smith,
2009) and (Socher et al, 2011). Furthermore, a
qualitative assessment of the pairs across different
datasets showed relatively significant differences,
which would strengthen the argument for develop-
ing features and methods specific to each dataset.
Another improvement would be to develop a bet-
ter dataset predictor for System 3. Also recognizing
there may be ways to normalize and rescale scores
across datasets so the regression models used do not
have to account for differing means and standard de-
viations.
Finally, there are other bodies of source data that
may be adapted for use with the STS task, such as
the paraphrasing pairs of the Recognizing Textual
Entailment challenges, human generated reference
translations for machine translation evaluation, and
human generated summaries used for summariza-
tion evaluations. Although these are gold decisions,
at the very least they could provide a source of high
similarity pairs, from which one could manufacture
lower scoring variants.
Acknowledgments
The authors would like to thank the Semantic Tex-
tual Similarity organizers for all of their hard work
622
and effort.
One of the authors was supported by the
Intelligence Advanced Research Projects Activ-
ity (IARPA) via Air Force Research Laboratory
(AFRL) contract number FA8650-10-C-7058. The
U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of IARPA,
AFRL, or the U.S. Government.
Eneko Agirre was partially funded by the
European Communitys Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 270082 (PATHS project) and the Ministry
of Economy under grant TIN2009-14715-C04-01
(KNOW2 project)
References
Eneko Agirre, Montse Cuadros, German Rigau, and Aitor
Soroa. 2010. Exploring knowledge bases for similar-
ity. In Proceedings of the International Conference on
Language Resources and Evaluation 2010.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In In Proceedings of the Joint Confer-
ence of the Annual Meeting of the Association for
Computational Linguistics and the International Joint
Conference on Natural Language Processing(ACL
2009), pages 468?476, Singapore.
Christine Fellbaum. 1998. WordNet - An Electronic Lex-
ical Database. MIT Press.
Samuel Fernando and Mark Stevenson. 2008. A se-
mantic similarity approach to paraphrase detection. In
Computational Linguistics UK (CLUK 2008) 11th An-
nual Research Colloqium.
Andrew Finch, Young-Sook Hwang, and Eiichio Sumita.
2005. Using machine translation evaluation tech-
niques to determine sentence-level semantic equiva-
lence. In Proceedings of the Third International Work-
shop on Paraphrasing (IWP 2005), pages 17?24, Jeju
Island, South Korea.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
Evgeniy Gabrilovich and Shaul Markovitch. 2009.
Wikipedia-based semantic interpretation. Journal of
Artificial Intelligence Research, 34:443?498.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank.
In WWW ?02, pages 517?526, New York, NY, USA.
ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. pages 74?81, Barcelona,
Spain, jul. Association for Computational Linguistics.
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, maxent models, and conditional estimation with-
out magic. In Proceedings of the 2003 Conference
of the North American Chapter of the Association for
Computational Linguistics on Human Language Tech-
nology: Tutorials - Volume 5, NAACL-Tutorials ?03,
pages 8?8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-
66, Stanford InfoLab, November. Previous number =
SIDL-WP-1999-0120.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concepts. In Proceedings of the Nine-
teenth National Conference on Artificial Intelligence
(Intelligent Systems Demonstrations), pages 1024?
1025, San Jose, CA, July.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems 24.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
623
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 155?161, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
SRIUBC-Core: Multiword Soft Similarity Models for Textual Similarity
Eric Yeh
SRI International
Menlo Park, CA USA
yeh@ai.sri.com
Eneko Agirre
University of Basque Country
Donostia, Basque Country
e.agirre@ehu.es
Abstract
In this year?s Semantic Textual Similarity
evaluation, we explore the contribution of
models that provide soft similarity scores
across spans of multiple words, over the pre-
vious year?s system. To this end, we ex-
plored the use of neural probabilistic language
models and a TF-IDF weighted variant of Ex-
plicit Semantic Analysis. The neural language
model systems used vector representations of
individual words, where these vectors were
derived by training them against the context
of words encountered, and thus reflect the dis-
tributional characteristics of their usage. To
generate a similarity score between spans, we
experimented with using tiled vectors and Re-
stricted Boltzmann Machines to identify simi-
lar encodings. We find that these soft similar-
ity methods generally outperformed our previ-
ous year?s systems, albeit they did not perform
as well in the overall rankings. A simple anal-
ysis of the soft similarity resources over two
word phrases is provided, and future areas of
improvement are described.
1 Introduction
For this year?s Semantic Textual Similarity (STS)
evaluation, we built upon the best performing sys-
tem we deployed last year with several methods for
exploring the soft similarity between windows of
words, instead of relying just on single token-to-
token similarities. From the previous year?s eval-
uation, we were impressed by the performance of
features derived from bigrams and skip bigrams. Bi-
grams capture the relationship between two concur-
rent words, while skip bigrams can capture longer
distance relationships. We found that characterizing
the overlap in skip bigrams between the sentences in
a STS problem pair proved to be a major contributor
to last year?s system?s performance.
Skip bigrams were matched on two criteria, lexi-
cal matches, and via part of speech (POS). Lexical
matching is brittle, and even if the match were made
on lemmas, we lose the ability to match against syn-
onyms. We could rely on the token-to-token simi-
larity methods to account for these non-lexical sim-
ilarities, but these do not account for sequence nor
dependencies in the sentencees. Using POS based
matching allows for a level of generalization, but at
a much broader level. What we would like to have
is a model that can capture these long distance re-
lationships at a level that is less broad than POS
matching, but allows for a soft similarity scoring be-
tween words. In addition, the ability to encompass
a larger window without having to manually insert
skips would be desirable as well.
To this end we decided to explore the use of neu-
ral probabilistic language models (NLPM) for cap-
turing this kind of behavior (Bengio et al, 2003).
NLPMs represent individual words as real valued
vectors, often at a much lower dimensionality than
the original vocabulary. By training these rep-
resentations to maximize a criterion such as log-
likelihood of target word given the other words in its
neighborhood, the word vectors themselves can cap-
ture commonalities between words that have been
used in similar contexts. In previous studies, these
vectors themselves can capture distributionally de-
rived similarities, by directly comparing the word
vectors themselves using simple measures such as
155
Euclidean distance (Collobert and Weston, 2008).
In addition, we fielded a variant of Explicit
Semantic Analysis (Gabrilovich and Markovitch,
2009) that used TF-IDF weightings, instead of using
the raw concept vectors themselves. From previous
experiments, we found that using TF-IDF weight-
ings on the words in a pair gave a boost in perfor-
mance over sentence length comparisons and above,
so this simple modification was incorporated into
our system.
In order to identify the contribution of these soft
similarity methods against last year?s system, we
fielded three systems:
1. System 1, the system from the previous year,
incorporating semantic similarity resources,
precision focused and Bilingual Evaluation Un-
derstudy (BLEU) overlaps (Papineni et al,
2002), and several types of skip-bigrams.
2. System 2, features just the new NLPM scores
and TFIDF-ESA.
3. System 3, combines System 1 and System 2.
For the rest of this system description, we briefly
describe the previous year?s system (System 1), the
TFIDF weighted Explicit Semantic Analysis, and
the NLPM systems. We then describe the experi-
ment setup, and follow up with results and analysis.
2 System 1
The system we used in SemEval 2012 consisted of
the following components:
1. Resource based word-to-word similarities,
combined using a Semantic Matrix (Fernando
and Stevenson, 2008).
2. Cosine-based lexical overlap measure.
3. Bilingual Evaluation Understudy (BLEU) (Pa-
pineni et al, 2002) lexical overlap.
4. Precision focused part-of-speech (POS) fea-
tures.
5. Lexical match skip-bigram overlap.
6. Precision focused skip-bigram POS features.
The Semantic Matrix assesses similarity between
a pair s1 and s2 by summing over all of the word
to word similarities between the pair, subject to nor-
malization, as given by Formula 1.
sim(s1, s2) =
vT1 Wv2
?v1? ?v2?
(1)
The matrix W is a symmetric matrix that en-
codes the word to word similarities, derived from
the underlying resources this is drawn from. From
the previous year?s assessment, we used similarities
derived from Personalized PageRank (Agirre et al,
2010) over WordNet (Fellbaum, 1998), the Explicit
Semantic Analysis (Gabrilovich and Markovitch,
2009) concept vector signatures for each lemma, and
the Dekang Lin Proximity-based Thesaurus 1.
The cosine-based lexical overlap measure simply
measures the cosine similarity, using strict lexical
overlap, between the sentence pairs. The BLEU,
precision focused POS, and skip-bigrams are direc-
tional measures, which measure how well a target
sentence matches a source sentence. To score pair of
sentences, we simply averaged the score where one
sentence is the source, the other the target, and then
vice versa. These directional measures were origi-
nally used as a precision focused means to assess the
quality of machine translations output against ref-
erence translations. Following (Finch et al, 2005),
these measures have also been shown to be good for
assessing semantic similarity between pairs of sen-
tences.
For BLEU, we measured how well ngrams of or-
der one through four were matched by the target sen-
tence, matching solely on lexical matches, or POS
matches. Skip bigrams performed similarly, except
the bigrams were not contiguous. The precision fo-
cused POS features assess how well each POS tag
found in the source sentence has been matched in
the target sentence, where the matches are first done
via a lemma match.
To combine the scores from these features, we
used the LIBSVM Support Vector Regression (SVR)
package (Chang and Lin, 2011), trained on the train-
ing pair gold scores. Per the previous year, we used
a radial basis kernel with a degree of three.
1http://webdocs.cs.ualberta.ca/ lindek/downloads.htm
156
For a more in-depth description of System 1,
please refer to (Yeh and Agirre, 2012).
3 TFIDF-ESA
This year instead of using Explicit Semantic Anal-
ysis (ESA) to populate a word-by-word similarity
matrix, we used ESA to derive a similarity score be-
tween the sentences in a STS pair. For a given sen-
tence, we basically treated it as an IR query against
the ESA concept-base: we tokenized the words, ex-
tracted the ESA concept vectors, and performed a
TFIDF weighted average to arrive at the sentence
vector. A cutoff of the top 1000 scoring concepts
was further applied, per previous experience, to im-
prove performance. The similarity score for two
sentence vectors was computed using cosine simi-
larity.
4 Neural Probabilistic Language Models
Neural probabilistic language models represent
words as real valued vectors, where these vectors are
trained to jointly capture the distributional statistics
of their context words and the positions these words
occur at. These representations are usually at a much
lower dimensionality than that of the original vocab-
ulary, forcing some form of compression to occur in
the vocabulary. The intent is to train a model that
can account for words that have not been observed
in a given context before, but that word vector has
enough similarity to another word that has been en-
countered in that context before.
Earlier models simply learnt how to model the
next word in a sequence, where each word in the vo-
cabulary is initially represented by a randomly ini-
tialized vector. For each instance, a larger vector is
assembled from the concatenation of the vectors of
the words observed, and act as inputs into a model.
This model itself is optimized to maximize the like-
lihood of the next word in the observed sequence,
with the errors backpropagated through the vectors,
with the parameters for the vectors being tied (Ben-
gio et al, 2003).
In later studies, these representations are the
product of training a neural network to maxi-
mize the margin between the scores it assigns to
observed ?correct? examples, which should have
higher scores, and ?corrupted examples,? where the
"heart"
dim=50
"attack"
dim=50
"heart attack"
dim=100
Figure 1: Vector Window encoding for the phrase ?heart
attack.?
token of interest is swapped out to produce an in-
correct example and preferably a lower score. As
shown in (Collobert and Weston, 2008) and then
(Huang et al, 2012), simple distance measures us-
ing the representations derived from this process are
both useful for assessing word similarity and relat-
edness. For this study, we used the contextually
trained language vectors provided by (Huang et al,
2012), which were trained to maximize the margin
between training pairs and to account for document
context as well. The dimensionality of these vectors
was 50.
As we are interested in capturing information at
a level greater than individual words, we used two
methods to combine these NLPM word vectors to
represent an order n ngram: a Vector Window
where we simply concatenated the word vectors, and
one that relied on encodings learnt by Restricted
Boltzmann Machines.
For this work, we experimented with generating
encodings for ngrams sized 2,3,5,10, and 21. The
smaller sizes correspond to commonly those com-
monly used to match ngrams, while the larger ones
were used to take advantage of the reduced sparsity.
Similarities between a pair of ngram encodings is
given similarity of their vector encodings.
4.1 Vector Window
The most direct way to encode an order n ngram as
a vector is to concatenate the n NLPM word vectors
together, in order. For example, to encode ?heart
attack?, the vectors for ?heart? and ?attack?, both
with dimensionality 50, are linked together to form
a larger vector with dimensionality 100 (Figure 1).
For size n vector windows where the total number
of tokens is less than n, we pad the left and right
sides of the window with a ?negative? token, which
was selected to be a vector that, on the average, is
anticorrelated with all the vectors in the vocabulary.
157
"heart attack"
compressed encoding
RBM trained encoder
original vector
Figure 2: Using a RBM trained compressor to generate a
compressed encoding of ?heart attack.?
4.2 Restricted Boltzmann Machines
Although the word vectors we used were trained
against a ten word context, the vector windows may
not be able to describe similarities at multiword
level, as the method is still performing comparisons
at a word-to-word level. For example the vector win-
dow score for the related phrases heart attack and
cardiac arrest is 0.35. In order to account for sim-
ilarities at a multiword level, we trained Restricted
Boltzmann Machines (RBM) to further encode these
vector windows (Hinton, 2002). A RBM is a bi-
partite undirected graphical model, where the only
edges are between a layer of input variables and a
layer of latent variables. The latent layer consists of
sigmoid units, allowing for non-linear combinations
of the inputs. The training objective is to learn a set
of weights that maximize the likelihood of training
observations, and given the independences inherent,
in the model it can be trained quickly and effectively
via Contrastive Divergence. The end effect is the
system attempts to force the latent layer to learn an
encoding of the input variables, usually at a lower di-
mensionality. In our case, by compressing their dis-
tributional representations we hope to amplify sig-
nificant similarities between multiword expressions,
albeit for those of the same size.
To derive a RBM based encoding, we first gen-
erate a vector window for the ngram, and then used
the trained RBM to arrive at the compressed vector
(Figure 2). As before, we derive a similarity score
between two RBM based encodings by comparing
their cosine distance.
Following the above example, the vectors from an
RBM trained system for heart attack and cardiac ar-
rest score the pair at a higher similarity, 0.54. For
phrases that are unrelated, comparing door key with
cardiac arrest gives a score of -0.14 with the vector
window, and RBM this is -0.17.
To train a RBM encoder for order n ngrams,
we generated n sized vector windows over ngrams
drawn from the English language articles in
Wikipedia. The language dump was filtered to larger
sized articles, in order to avoid pages likely to be
content-free, such as redirects. The training set
size consisted of 35,581,434 words, which was split
apart into 1,519,256 sentences using the OpenNLP
sentence splitter tool 2. The dimensionality of the
encoding layer was set to 50 for window sizes 2,3,5,
and 200 for the larger windows.
4.3 Combining word and ngram similarity
scores
In order to produce an overall similarity score, we
used a variant of the weighted variant of the simi-
larity combination method given in (Mihalcea et al,
2006). Here, we generated a directional similarity
score from a source to target by the following,
sim(S, T ) =
?
s?S maxSim(s, T )
|S|
(2)
where maxSim(s, T ) represents the maximum
similarity between the token s and the set of tokens
in the target sentence, T . In the case of ngrams with
order 2 or greater, we treat each ngram as a token for
the combination.
avgsim(T1, T2) =
1
2
(sim(T1, T2) + sim(T2, T1))
(3)
Unlike the original method, we treated each term
equally, in order to account for ngrams with order
2 and above. We also did not filter based off of the
part of speech, relying on the scores themselves to
help perform the filtering.
In addition to the given word window sizes,
we also directly assess the word-to-word similarity
scores by comparing the word vectors directly, using
a window size of one.
5 Evaluation Setup
System 2, the TFIDF-ESA score for a pair is a fea-
ture. For each of the given ngram sizes, we treated
2http://opennlp.apache.org/
158
Training (2012) Test (2013)
Surprise1 (ONWN) FNWN
MSRPar Headlines
Surprise1 (ONWN) ONWN
Surprise2 (SMT) SMT
Table 1: Train (2012) and Test (2013) sets used to train
the regressors.
the ngram similarity scores from the Vector Window
and RBM methods as individual features. System
3 combines the features from System 2 with those
from System 1. For Systems 2 and 3, the SVR setup
used by System 1 was used to develop scorers. As no
training immediate training sets were provided for
the evaluation sets, we used the train and test parti-
tions given in Table 1, training on both the 2012 train
and test data, where gold scores were available.
6 Results and Discussion
The results of our three runs are given in the top half
of Table 2. To get a better sense of the contribution
of the new components, we also ran the NLPM vec-
tor window and RBM window models and TFIDF-
ESA components individually against the test sets.
The NLPM system was trained using the same SVR
setup as the main experiment.
In order to provide a lexical match comparison for
the NLPM system, we experimented with a ngram
matching system, where ngrams of size 1,2,3,5,10,
and 21 were used to generate similarity scores via
the same combination method as the NLPM models.
Here, hard matching was performed, where match-
ing ngrams were given a score of 1, else 0. Again,
we used the main experiment SVR setup to combine
the scores from the various ngram sizes.
We found that overall the previous year?s sys-
tem did not perform adequately on the evaluation
datasets, short of the headlines dataset. Oddly
enough, TFIDF-ESA by itself would have arrived at
a good correlation with OnWN: one possible expla-
nation for this would be the fact that TFIDF-ESA
by itself is essentially an order-free ?bag of words?
model that assesses soft token to token similarity. As
the other systems incorporate either some notion of
sequence and/or require strict lexical matching, it is
possible that characterization does not help with the
OnWN sense definitions.
Combining the new features with the previous
year?s system gave poorer performance; a prelimi-
nary assessment over the training sets showed some
degree of overfitting, likely due to high correlation
between the NLPM features and last year?s direc-
tional measures.
When using the same combination method, ngram
matching via lexical content over ngrams gave
poorer results than those from NLPM models, as
given in Table 2. This would also argue for identi-
fying better combination methods than the averaged
maximum similarity method used here.
What is interesting to note is that the NLPM and
TFIDF-ESA systems do not rely on any part of
speech information, nor hand-crafted semantic sim-
ilarity resources. Instead, these methods are de-
rived from large scale corpora, and generally out-
performed the previous year?s system which relied
on that extra information.
To get a better understanding of the NLPM and
TFIDF-ESA models, we compared how the com-
ponents would score the similarity between pairs of
two word phrases, given in Table 3. At least over this
small sampling we genearted, we found that in gen-
eral the RBM method tended to have a much wider
range of scores than the Vector Window, although
both methods were very correlated. Both systems
had very low correlation with TFIDF-ESA.
7 Future Work
One area of improvement would be to develop a bet-
ter method for combining the various ngram simi-
larity scores provided by the NLPMs. When using
lexical matching of ngrams, we found that the com-
bination method used here proved inferior to the di-
rectional measures from the previous year?s systems.
This would argue for a better way to use the NLPMs.
As training STS pairs are available with gold scores,
this would argue for some form of supervised train-
ing. For training similarities between multiword ex-
pressions, proxy measures for similarity, such as the
Normalized Google Distance (Cilibrasi and Vita?nyi,
2004), may be feasible.
Another avenue would be to allow the NLPM
methods to encode arbitrary sized text spans, as the
current restriction on spans being the same size is
159
System headlines OnWN FNWN SMT mean rank
SRIUBC-system1 (Baseline) 0.6083 0.2915 0.2790 0.3065 0.4011 66
SRIUBC-system2 (NLPM, TFIDF-ESA) 0.6359 0.3664 0.2713 0.3476 0.4420 57
SRIUBC-system3 (Combined) 0.5443 0.2843 0.2705 0.3275 0.3842 70
NLPM 0.5791 0.3157 0.3211 0.2698 0.3714
TFIDF-ESA 0.5739 0.7222 0.1781 0.2980 0.4431
Lex-only 0.5455 0.3237 0.2095 0.3146 0.3483
Table 2: Pearson correlation of systems against the test datasets (top). The test set performance for the new Neural
Probabilistic Language Model (NLPM) and TFIDF-ESA components are given, along with a lexical-only variant for
comparison (bottom).
String 1 String 2 Vec. Window RBM Window TFIDF-ESA
heart attack cardiac arrest 0.354 0.544 0.182
door key cardiac arrest -0.14 -0.177 0
baby food cat food 0.762 0.907 0.079
dog food cat food 0.886 0.914 0.158
rotten food baby food 0.482 0.473 0.071
frozen solid thawed out 0.046 -0.331 0.102
severely burnt frozen stiff -0.023 -0.155 0
uphill slog raced downhill 0.03 -0.322 0.043
small cat large dog 0.817 0.905 0.007
ran along sprinted by 0.31 0.238 0.004
ran quickly jogged rapidly 0.349 0.327 0.001
deathly ill very sick 0.002 0.177 0.004
ran to raced to 0.815 0.829 0.013
free drinks drinks free 0.001 0.042 1
door key combination lock 0.098 0.093 0.104
frog blast vent core 0.003 0.268 0.004
Table 3: Cosine similarity of two input strings, as given by the vectors generated from the Vector Window size 2, RBM
Window size 2, and TFIDF-ESA.
unrealistic. One possibility is to use recurrent neural
network techniques to generate this type of encod-
ing.
Finally, the size of the Wikipedia dump used to
train the Restricted Boltzmann Machines could be
at issue, as 35 million words could be considered
small compared to the full range of expressions we
would wish to capture, especially for the larger win-
dow spans. A larger training corpus may be needed
to fully see the benefit from RBMs.
Acknowledgments
Supported by the Artificial Intelligence Center at SRI In-
ternational. The views and conclusions contained herein
are those of the authors and should not be interpreted as
necessarily representing the official policies or endorse-
ments, either expressed or implied, of the Artificial Intel-
ligence Center, or SRI International.
References
Eneko Agirre, Montse Cuadros, German Rigau, and Aitor
Soroa. 2010. Exploring knowledge bases for similar-
ity. In Proceedings of the International Conference on
Language Resources and Evaluation 2010.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
160
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Rudi Cilibrasi and Paul M. B. Vita?nyi. 2004. The google
similarity distance. CoRR, abs/cs/0412098.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In International Conference
on Machine Learning, ICML.
Christine Fellbaum. 1998. WordNet - An Electronic Lex-
ical Database. MIT Press.
Samuel Fernando and Mark Stevenson. 2008. A se-
mantic similarity approach to paraphrase detection. In
Computational Linguistics UK (CLUK 2008) 11th An-
nual Research Colloqium.
Andrew Finch, Young-Sook Hwang, and Eiichio Sumita.
2005. Using machine translation evaluation tech-
niques to determine sentence-level semantic equiva-
lence. In Proceedings of the Third International Work-
shop on Paraphrasing (IWP 2005), pages 17?24, Jeju
Island, South Korea.
Evgeniy Gabrilovich and Shaul Markovitch. 2009.
Wikipedia-based semantic interpretation. Journal of
Artificial Intelligence Research, 34:443?498.
Geoffrey E. Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural Com-
putation, 14(8):1771?1800.
Eric H. Huang, Richard Socher, Christopher D. Manning,
and Andrew Y. Ng. 2012. Improving Word Represen-
tations via Global Context and Multiple Word Proto-
types. In Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of the
American Association for Artificial Intelligence (AAAI
2006), Boston, Massachusetts, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Eric Yeh and Eneko Agirre. 2012. Sri and ubc: Simple
similarity features for semantic textual similarity. In
Proceedings of SemEval 2012.
161
