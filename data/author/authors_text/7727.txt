Creating a Finite-State Parser with Application Semantics
Owen Rambow
University of Pennsylvania
Philadelphia, PA 19104
USA
Srinivas Bangalore
AT&T Labs ? Research
Florham Park, NJ 07932
USA
Tahir Butt
Johns Hopkins University
Baltimore, MD 21218
USA
Alexis Nasr
Universite? Paris 7
75005 Paris
France
Richard Sproat
AT&T Labs ? Research
Florham Park, NJ 07932
USA
rambow@unagi.cis.upenn.edu
Abstract
Parsli is a finite-state (FS) parser which can be
tailored to the lexicon, syntax, and semantics
of a particular application using a hand-editable
declarative lexicon. The lexicon is defined in
terms of a lexicalized Tree Adjoining Grammar,
which is subsequently mapped to a FS represen-
tation. This approach gives the application de-
signer better and easier control over the natural
language understanding component than using
an off-the-shelf parser. We present results using
Parsli on an application that creates 3D-images
from typed input.
1 Parsing and Application-Specific
Semantics
One type of Natural Language Understanding
(NLU) application is exemplified by the database
access problem: the user may type in free source
language text, but the NLU component must
map this text to a fixed set of actions dictated
by the underlying application program. We
will call such NLU applications ?application-
semantic NLU?. Other examples of application-
semantic NLU include interfaces to command-
based applications (such as airline reservation
systems), often in the guise of dialog systems.
Several general-purpose off-the-shelf (OTS)
parsers have become widely available (Lin,
1994; Collins, 1997). For application-semantic
NLU, it is possible to use such an OTS parser in
conjunction with a post-processor which trans-
fers the output of the parser (be it phrase struc-
ture or dependency) to the domain semantics. In
addition to mapping the parser output to appli-
cation semantics, the post-processor often must
also ?correct? the output of the parser: the parser
may be tailored for a particular domain (such as
Wall Street Journal (WSJ) text), but the new do-
main presents linguistic constructions not found
in the original domain (such as questions). It
may also be the case that the OTS parser consis-
tently misanalyzes certain lexemes because they
do not occur in the OTS corpus, or occur there
with different syntactic properties. While many
of the parsers can be retrained, often an anno-
tated corpus is not available in the application
domain (since, for example, the application it-
self is still under development and there is not
yet a user community). The process of retraining
may also be quite complex in practice. A further
disadvantage of this approach is that the post-
processor must typically be written by hand, as
procedural code. In addition, the application-
semantic NLU may not even exploit the strengths
of the OTS parser, because the NLU required
for the application is not only different (ques-
tions), but generally simpler (the WSJ contains
very long and syntactically complex sentences
which are not likely to be found as input in in-
teractive systems, including dialog systems).
This discussion suggests that we (i) need an
easy way to specify application semantics for a
parser and (ii) that we do not usually need the full
power of a full recursive parser. In this paper, we
suggest that application-semantic NLP may be
better served by a lexicalized finite-state (FS)
parser. We present PARSLI, a FS parser which
can be tailored to the application semantics us-
ing a hand-editable declarative lexicon. This ap-
proach gives the application designer better and
easier control over the NLU component. Further-
more, while the finite-state approach may not be
sufficient for WSJ text (given its syntactic com-
plexity), it is sufficient for most interactive sys-
tems, and the advantage in speed offered by FS
approaches in more crucial in interactive appli-
cations. Finally, in speech-based systems, the
lattice that is output from the speech recognition
component can easily used as input to a FS-based
parser.
2 Sample Application: WORDSEYE
WORDSEYE (Coyne and Sproat, 2001) is a
system for converting English text into three-
dimensional graphical scenes that represent that
text. WORDSEYE performs syntactic and se-
mantic analysis on the input text, producing a
description of the arrangement of objects in a
scene. An image is then generated from this
scene description. At the core of WORDSEYE
is the notion of a ?pose?, which can be loosely
defined as a figure (e.g. a human figure) in a con-
figuration suggestive of a particular action.
For WORDSEYE, the NLP task is thus to
map from an input sentence to a representation
that the graphics engine can directly interpret in
terms of poses. The graphical component can
render a fixed set of situations (as determined by
its designer); each situation has several actors in
situation-specific poses, and each situation can
be described linguistically using a given set of
verbs. For example, the graphical component
may have a way of depicting a commercial trans-
action, with two humans in particular poses (the
buyer and the seller), the goods being purchased,
and the payment amount. In English, we have
different verbs that can be used to describe this
situation (buy, sell, cost, and so on). These verbs
have different mappings of their syntactic argu-
ments to the components in the graphical repre-
sentation. We assume a mapping from syntax to
domain semantics, leaving to lexical semantics
the question of how such a mapping is devised
and derived. (For many applications, such map-
pings can be derived by hand, with the seman-
tic representation an ad-hoc notation.) We show
a sample of such mapping in Figure 1. Here,
we assume that the graphics engine of WORD-
SEYE knows how to depict a TRANSACTION
when some of the semantic arguments of a trans-
action (such as CUSTOMER, ITEM, AMOUNT)
are specified.
We show some sample transductions in Fig-
ure 2. In the output, syntactic constituents are
bracketed. Following each argument is informa-
tion about its grammatical function (?GF=0? for
example) and about its semantic role (ITEM for
example). If a lexical item has a semantics of
its own, the semantics replaces the lexical item
(this is the case for verbs), otherwise the lexical
item remains in place. In the case of the transi-
tive cost, the verbal semantics in Figure 1 spec-
ifies an implicit CUSTOMER argument. This is
generated when cost is used transitively, as can
be seen in Figure 2.
3 Mapping Tree Adjoining Grammar
to Finite State Machines
What is crucial for being able to define a map-
ping from words to application semantics is a
very abstract notion of grammatical function: in
devising such a mapping, we are not interested
in how English realizes certain syntactic argu-
ments, i.e., in the phrase structure of the verbal
projection. Instead, we just want to be able to re-
fer to syntactic functions, such as subject or indi-
rect object. Tree Adjoining Grammar (TAG) rep-
resents the entire syntactic projection from a lex-
eme in its elementary structures in an elementary
tree; because of this, each elementary tree can
be associated with a lexical item (lexicalization,
(Joshi and Schabes, 1991)). Each lexical item
can be associated with one or more trees which
represent the lexeme?s valency; these trees are
referred to as its supertags. In a derivation, sub-
stituting or adjoining the tree of one lexeme into
that of another creates a direct dependency be-
tween them. The syntactic functions are labeled
with integers starting with zero (to avoid discus-
sions about names), and are retained across op-
erations such as topicalization, dative shift and
passivization.
A TAG consists of a set of elementary trees of
two types, initial trees and auxiliary trees. These
trees are then combined using two operations,
substitution and adjunction. In substitution, an
initial tree is appended to a specially marked
node with the same label as the initial tree?s root
node. In adjunction, a non-substitution node is
rewritten by an auxiliary tree, which has a spe-
cially marked frontier node called the footnode.
The effect is to insert the auxiliary tree into the
middle of the other tree.
We distinguish two types of auxiliary trees.
Adjunct auxiliary trees are used for adjuncts;
they have the property that the footnode is al-
Verb Supertag Verb semantics Argument semantics
paid A nx0Vnx1 transaction 0=Customer 1=Amount
cost A nx0Vnx1 transaction 0=Item 1=Amount Implicit=Customer
cost A nx0Vnx2nx1 transaction 0=Item 1=Amount 2=Customer
bought, purchased A nx0Vnx1 transaction 0=Customer 1=Item
socks A NXN none none
Figure 1: Sample entries for a commercial transaction situation
In: I bought socks
Out: ( ( I ) GF=0 AS=CUSTOMER TRANSACTION ( socks ) GF=1 AS=ITEM )
In:the pajamas cost my mother-in-law 12 dollars
Out: ( ( ( the ) pajamas ) GF=0 AS=ITEM TRANSACTION ( ( my ) mother-in-law ) GF=2 AS=CUSTOMER ( (
12 ) dollars ) GF=1 AS=AMOUNT )
In: the pajamas cost 12 dollars
Out: ( ( ( the ) pajamas ) GF=0 AS=ITEM TRANSACTION IMP:CUSTOMER ( ( 12 ) dollars ) GF=1
AS=AMOUNT )
Figure 2: Sample transductions generated by Parsli (?GF? for grammatical function, ?AS? for argu-
ment semantics, ?Imp? for implicit argument)
ways a daughter node of the root node, and the
label on these nodes is not, linguistically speak-
ing, part of the projection of the lexical item of
that tree. For example, an adjective will project
to AdjP, but the root- and footnode of its tree will
be labeled NP, since an adjective adjoins to NP.
We will refer to the root- and footnode of an ad-
junct auxiliary tree as its passive valency struc-
ture. Note that the tree for an adjective also spec-
ifies whether it adjoins from the left (footnode
on right) or right (footnode on left). Predicative
auxiliary trees are projected from verbs which
subcategorize for clauses. Since a verb projects
to a clausal category, and has a node labeled with
a clausal category on its frontier (for the argu-
ment), the resulting tree can be interpreted as an
auxiliary tree, which is useful in analyzing long-
distance wh-movement (Frank, 2001).
To derive a finite-state transducer (FST) from
a TAG, we do a depth-first traversal of each ele-
mentary tree (but excluding the passive valency
structure, if present) to obtain a sequence of non-
terminal nodes. For predicative auxiliary trees,
we stop at the footnode. Each node becomes two
states of the FST, one state representing the node
on the downward traversal on the left side, the
other representing the state on the upward traver-
sal, on the right side. For leaf nodes, the two
states are juxtaposed. The states are linearly con-
nected with   -transitions, with the left node state
of the root node the start state, and its right node
state the final state (except for predicative auxil-
iary trees ? see above). To each non-leaf state,
we add one self loop transition for each tree in
the grammar that can adjoin at that state from
the specified direction (i.e., for a state represent-
ing a node on the downward traversal, the auxil-
iary tree must adjoin from the left), labeled with
the tree name. For each pair of adjacent states
representing a substitution node, we add transi-
tions between them labeled with the names of
the trees that can substitute there. We output the
number of the grammatical function, and the ar-
gument semantics, if any is specified. For the
lexical head, we transition on the head, and out-
put the semantics if defined, or simply the lex-
eme otherwise. There are no other types of leaf
nodes since we do not traverse the passive va-
lency structure of adjunct auxiliary tees. At the
beginning of each FST, an   -transition outputs an
open-bracket, and at the end, an   -transition out-
puts a close-bracket. The result of this phase of
the conversion is a set of FSTs, one per elemen-
tary tree of the grammar. We will refer to them
as ?elementary FSTs?.
0 1<epsilon>:( 2A_NXG:GF=0
A_NXN:GF=0
3<epsilon>:FE=Customer 4ordered:transaction 5A_NXG:GF=1
A_NXN:GF=1
6<epsilon>:FE=Item 7<epsilon>:)
Figure 4: FST corresponding to TAG tree in Figure 3
S
NP

Arg0
VP
V
ordered
NP

Arg1
Figure 3: TAG tree for word ordered; the dow-
narrow indicates a substitution node for the nom-
inal argument
4 Constructing the Parser
In our approach, each elementary FST describes
the syntactic potential of a set of (syntactically
similar) words (as explained in Section 3). There
are several ways of associating words with FSTs.
Since FSTs correspond directly to supertags (i.e.,
trees in a TAG grammar), the basic way to
achieve such a mapping is to list words paired
with supertags, along with the desired seman-
tic associated with each argument position (see
Figure 1). The parser can also be divided into
a lexical machine which transduces words to
classes, and a syntactic machine, which trans-
duces classes to semantics. This approach has
the advantage of reducing the size of the over-
all machine since the syntax is factored from the
lexicon.
The lexical machine transduces input words to
classes. To determine the mapping from word to
supertag, we use the lexical probability  	

where 	 is the word and  the class. These
are derived by maximum likelihood estimation
from a corpus. Once we have determined for all
words which classes we want to pair them with,
we create a disjunctive FST for all words associ-
ated with a given supertag machine, which trans-
duces the words to the class name. We replaces
the class?s FST (as determined by its associated
supertag(s)) with the disjunctive head FST. The
weights on the lexical transitions are the nega-
tive logarithm of the emit probability 	 
 (ob-
tained in the same manner as are the lexical prob-
abilities).
For the syntactic machine, we take each ele-
mentary tree machine which corresponds to an
initial tree (i.e., a tree which need not be ad-
joined) and form their union. We then perform
a series of iterative replacements; in each iter-
ation, we replace each arc labeled by the name
of an elementary tree machine by the lexicalized
version of that tree machine. Of course, in each
iteration, there are many more replacements than
in the previous iteration. We use 5 rounds of iter-
ation; obviously, the number of iterations restrict
the syntactic complexity (but not the length) of
recognized input. However, because we output
brackets in the FSTs, we obtain a parse with
full syntactic/lexical semantic (i.e., dependency)
structure, not a ?shallow parse?.
This construction is in many ways similar to
similar constructions proposed for CFGs, in par-
ticular that of (Nederhof, 2000). One difference
is that, since we start from TAG, recursion is al-
ready factored, and we need not find cycles in the
rules of the grammar.
5 Experimental Results
We present results in which our classes are de-
fined entirely with respect to syntactic behav-
ior. This is because we do not have available
an important corpus annotated with semantics.
We train on the Wall Street Journal (WSJ) cor-
pus. We evaluate by taking a list of 205 sen-
tences which are chosen at random from entries
to WORDSEYE made by the developers (who
were testing the graphical component using a dif-
ferent parser). Their average length is 6.3 words.
We annotated the sentences by hand for the de-
sired dependency structure, and then compared
the structural output of PARSLI to the gold stan-
dard (we disregarded the functional and seman-
tic annotations produced by PARSLI). We eval-
uate performance using accuracy, the ration of
n Correctness Accuracy Nb
2 1.00 1.00 12
4 0.83 0.84 30
6 0.70 0.82 121
8 0.62 0.80 178
12 0.59 0.79 202
16 0.58 0.79 204
20 0.58 0.78 205
Figure 5: Results for sentences with  or fewer
words; Nb refers to the number of sentences in
this category
n Correctness Accuracy
1 0.58 0.78
2 0.60 0.79
4 0.62 0.81
8 0.69 0.85
12 0.68 0.86
20 0.70 0.87
30 0.73 0.89
Figure 6: Results for  -best analyses
the number of dependency arcs which are cor-
rectly found (same head and daughter nodes) in
the best parse for each sentence to the number
of arcs in the entire test corpus. We also report
the percentage of sentences for which we find the
correct dependency tree (correctness). For our
test corpus, we obtain an accuracy of 0.78 and
a correctness of 0.58. The average transduction
time per sentence (including initialization of the
parser) is 0.29 s. Figure 5 shows the dependence
of the scores on sentence length. As expected,
the longer the sentence, the worse the score.
We can obtain the n-best paths through the
FST; the scores for n-best paths are summarized
in Figure 6. Since the scores keep increasing, we
believe that we can further improve our 1-best
results by better choosing the correct path. We
intend to adapt the FSTs to use probabilities of
attaching particular supertags to other supertags
(rather than uniform weights for all attachments)
in order to better model the probability of differ-
ent analyses. Another option, of course, is bilex-
ical probabilities.
6 Discussion and Outlook
We have presented PARSLI, a system that takes
a high-level specification of domain lexical se-
mantics and generates a finite-state parser that
transduces input to the specified semantics.
PARSLI uses Tree Adjoining Grammar as an in-
terface between syntax and lexical semantics.
Initial evaluation results are encouraging, and we
expect to greatly improve on current 1-best re-
sults by using probabilities of syntactic combi-
nation. While we have argued that many appli-
cations do not need a fully recursive parser, the
same approach to using TAG as an intermediate
between application semantics and syntax can be
used in a chart parser; for a chart parser using the
FS machines discussed in this paper, see (Nasr et
al., 2002).
References
Michael Collins. 1997. Three generative, lex-
icalised models for statistical parsing. In
Proceedings of the 35th Annual Meeting of
the Association for Computational Linguis-
tics, Madrid, Spain, July.
Bob Coyne and Richard Sproat. 2001. Word-
sEye: An automatic text-to-scene conversion
system. In SIGGRAPH 2001, Los Angeles,
CA.
Robert Frank. 2001. Phrase Structure Composi-
tion and Syntactic Dependencies. MIT Press,
Cambridge, Mass.
Aravind K. Joshi and Yves Schabes. 1991. Tree-
adjoining grammars and lexicalized gram-
mars. In Maurice Nivat and Andreas Podel-
ski, editors, Definability and Recognizability
of Sets of Trees. Elsevier.
Dekang Lin. 1994. PRINCIPAR?an efficient,
broad-coverage, principle-based parser. In
coling94, pages 482?488, Kyoto, Japan.
Alexis Nasr, Owen Rambow, John Chen, and
Srinivas Bangalore. 2002. Context-free pars-
ing of a tree adjoining grammar using finite-
state machines. In Proceedings of the Sixth
International Workshop on tree Adjoining
Grammar and related Formalisms (TAG+6),
Venice, Italy.
Mark-Jan Nederhof. 2000. Practical experi-
ments with regular approximation of context-
free languages. Computational Linguistics,
26(1):17?44.
The First International Chinese Word Segmentation Bakeoff
Richard Sproat
AT&T Labs ? Research
180 Park Avenue, Florham Park, NJ, 07932, USA
rws@research.att.com
Thomas Emerson
Basis Technology
150 CambridgePark Drive
Cambridge, MA 02140, USA
tree@basistech.com
Abstract
This paper presents the results from the
ACL-SIGHAN-sponsored First Interna-
tional Chinese Word Segmentation Bake-
off held in 2003 and reported in con-
junction with the Second SIGHAN Work-
shop on Chinese Language Processing,
Sapporo, Japan. We give the motivation
for having an international segmentation
contest (given that there have been two
within-China contests to date) and we re-
port on the results of this first international
contest, analyze these results, and make
some recommendations for the future.
1 Introduction
Chinese word segmentation is a difficult problem
that has received a lot of attention in the literature;
reviews of some of the various approaches can be
found in (Wang et al, 1990; Wu and Tseng, 1993;
Sproat and Shih, 2001). The problem with this liter-
ature has always been that it is very hard to compare
systems, due to the lack of any common standard test
set. Thus, an approach that seems very promising
based on its published report is nonetheless hard to
compare fairly with other systems, since the systems
are often tested on their own selected test corpora.
Part of the problem is also that there is no single
accepted segmentation standard: There are several,
including the four standards used in this evaluation.
A number of segmentation contests have been
held in recent years within Mainland China, in the
context of more general evaluations for Chinese-
English machine translation. See (Yao, 2001; Yao,
2002) for the first and second of these; the third eval-
uation will be held in August 2003. The test cor-
pora were segmented according to the Chinese na-
tional standard GB 13715 (GB/T 13715?92, 1993),
though some lenience was granted in the case of
plausible alternative segmentations (Yao, 2001); so
while GB 13715 specifies the segmentation   / 
for Mao Zedong,    was also allowed. Accura-
cies in the mid 80?s to mid 90?s were reported for the
four systems that participated in the first evaluation,
with higher scores (many in the high nineties) being
reported for the second evaluation.
The motivations for holding the current contest
are twofold. First of all, by making the contest in-
ternational, we are encouraging participation from
people and institutions who work on Chinese word
segmentation anywhere in the world. The final set of
participants in the bakeoff include two from Main-
land China, three from Hong Kong, one from Japan,
one from Singapore, one from Taiwan and four from
the United States.
Secondly, as we have already noted, there are at
least four distinct standards in active use in the sense
that large corpora are being developed according to
those standards; see Section 2.1. It has also been
observed that different segmentation standards are
appropriate for different purposes; that the segmen-
tation standard that one might prefer for information
retrieval applications is likely to be different from
the one that one would prefer for text-to-speech syn-
thesis; see (Wu, 2003) for useful discussion. Thus,
while we do not subscribe to the view that any of
the extant standards are, in fact, appropriate for any
particular application, nevertheless, it seems desir-
able to have a contest where people are tested against
more than one standard.
A third point is that we decided early on that we
would not be lenient in our scoring, so that alter-
native segmentations as in the case of     Mao
Zedong, cited above, would not be allowed. While
it would be fairly straightforward (in many cases)
to automatically score both alternatives, we felt we
could provide a more objective measure if we went
strictly by the particular segmentation standard be-
ing tested on, and simply did not get into the busi-
ness of deciding upon allowable alternatives.
Comparing segmenters is difficult. This is not
only because of differences in segmentation stan-
dards but also due to differences in the design of
systems: Systems based exclusively (or even pri-
marily) on lexical and grammatical analysis will of-
ten be at a disadvantage during the comparison com-
pared to systems trained exclusively on the training
data. Competitions also may fail to predict the per-
formance of the segmenter on new texts outside the
training and testing sets. The handling of out-of-
vocabulary words becomes a much larger issue in
these situations than is accounted for within the test
environment: A system that performs admirably in
the competition may perform poorly on texts from
different registers.
Another issue that is not accounted for in the
current collection of evaluations is the handling of
short strings with minimal context, such as queries
submitted to a search engine. This has been stud-
ied indirectly through the cross-language informa-
tion retrieval work performed for the TREC 5 and
TREC 6 competitions (Smeaton and Wilkinson,
1997; Wilkinson, 1998).
This report summarizes the results of this First
International Chinese Word Segmentation Bakeoff,
provides some analysis of the results, and makes
specific recommendations for future bakeoffs. One
thing we do not do here is get into the details of spe-
cific systems; each of the participants was required
to provide a four page description of their system
along with detailed discussion of their results, and
these papers are published in this volume.
2 Details of the contest
2.1 Corpora
The corpora are detailed in Table 1. Links
to descriptions of the corpora can be found at
http://www.sighan.org/bakeoff2003/
bakeoff_instr.html; publications on spe-
cific corpora are (Huang et al, 1997) (Academia
Sinica), (Xia, 1999) (Chinese Treebank); the
Beijing University standard is very similar to that
outlined in (GB/T 13715?92, 1993). Table 1 lists
the abbreviations for the four corpora that will be
used throughout this paper. The suffixes ?o? and
?c? will be used to denote open and closed tracks,
respectively: Thus ?ASo,c? denotes the Academia
Sinica corpus, both open and closed tracks; and
?PKc? denotes the Beijing University corpus, closed
track.
During the course of this bakeoff, a number of
inconsistencies in segmentation were noted in the
CTB corpus by one of the participants. This was
done early enough so that it was possible for the
CTB developers to correct some of the more com-
mon cases, both in the training and the test data.
The revised training data was posted for participants,
and the revised test data was used during the testing
phase.
Inconsistencies were also noted by another par-
ticipant for the AS corpus. Unfortunately this came
too late in the process to correct the data. However,
some informal tests on the revised testing data indi-
cated that the differences were minor.
2.2 Rules and Procedures
The contest followed a strict set of guidelines and
a rigid timetable. The detailed instructions for the
bakeoff can be found at http://www.sighan.
org/bakeoff2003/bakeoff_instr.html
(with simplified and traditional Chinese versions
also available). Training material was available
starting March 15, testing material was available
April 22, and the results had to be returned to the
SIGHAN ftp site by April 25 no later than 17:00
EDT.
Upon initial registration sites were required to de-
clare which corpora they would be training and test-
ing on, and whether they would be participating in
the open or closed tracks (or both) on each corpus,
Corpus Abbrev. Encoding # Train. Words # Test. Words
Academia Sinica AS Big Five (MS Codepage 950) 5.8M 12K
U. Penn Chinese Treebank CTB EUC-CN (GB 2312-80) 250K 40K
Hong Kong CityU HK Big Five (HKSCS) 240K 35K
Beijing University PK GBK (MS Codepage 936) 1.1M 17K
Table 1: Corpora used.
where these were defined as follows:
  For the open test sites were allowed to train
on the training set for a particular corpus, and
in addition they could use any other mate-
rial including material from other training cor-
pora, proprietary dictionaries, material from
the WWW and so forth. However, if a site
selected the open track the site was required
to explain what percentage of the results came
from which sources. For example, if the sys-
tem did particularly well on out-of-vocabulary
words then the participants were required to ex-
plain if, for example, those results could mostly
be attributed to having a good dictionary.
  In the closed test, participants could only use
training material from the training data for the
particular corpus being testing on. No other
material was allowed.
Other obvious restrictions applied: Participants
were prohibited from testing on corpora from their
own sites, and by signing up for a particular track,
participants were declaring implicitly that they had
not previously seen the test corpus for that track.
Scoring was completely automatic. Note that the
scoring software does not correct for cases where a
participant converted from one coding scheme into
another, and any such cases were counted as er-
rors. Results were returned to participants within
a couple of days of submission of the segmented
test data. The script used for scoring can be
downloaded from http://www.sighan.org/
bakeoff2003/score; it is a simple Perl script
that depends upon a version of diff (e.g. GNU diffu-
tils 2.7.2), that supports the -y flag for side-by-side
output format.
2.3 Participating sites
Participating sites are shown in Table 2. These are a
subset of the sites who had registered for the bake-
off, as some sites withdrew due to technical difficul-
ties.
3 Further details of the corpora
An unfortunate, and sometimes unforseen, complex-
ity in dealing with Chinese text on the computer is
the plethora of character sets and character encod-
ings used throughout Greater China. This is demon-
strated in the Encoding column of Table 1:
1. Both AS and HK utilize complex-form (or ?tra-
ditional?) characters, using variants of the Big
Five character set. The Academia Sinica cor-
pus is composed almost entirely of characters
in pure Big Five (four characters, 0xFB5B,
0xFA76, 0xFB7A, and 0xFAAF are outside
the encoding range of Big Five), while the
City University corpus utilizes 38 (34 unique)
characters from the Hong Kong Supplementary
Character Set (HKSCS) extension to Big Five.
2. The CTB and PK corpora each use simple-form
(or ?simplified?) characters, using the EUC-
CN encoding of the GB 2312-80 character set.
However, The PKU corpus includes characters
that are not part of GB 2312-80, but are en-
coded in GBK. GBK is an extension of GB
2312-80 that incorporates some 18,000 hanzi
found in Unicode 2.1 within the GB-2312 code
space. Only Microsoft?s CP936 implements
GBK.
This variation of encoding is exacerbated by the
usual lack of specific declaration in the files. Gener-
ally a file is said to be ?Big Five? or ?GB?, when in
actuality the file is encoded in a variation of these.
This is problematic in systems that utilize Unicode
Site ID Site Name Domain Contact Tracks
S01 Inst. of Comp. Tech.,CAS CN Huaping ZHANG ASo CTBo,c HKc PKo,c
S02 ICL, Beijing U CN Baobao CHANG CTBo,c
S03 HK Polytechnic University HK Qin LU ASo CTBo HKo PKo
S04 U of Hong Kong HK Guohong FU PKo,c
S05 HK CityU HK Chunyu KIT ASc CTBc PKc
S06 Nara IST JP Chooi Ling GOH ASc CTBc HKc PKc
S07 Inst. for Infocomm Research SG Guodong ZHOU PKc
S08 CKIP Ac. Sinica Taiwan TW Wei Yun MA HKo,c PKo,c
S09 UC Berkeley US Aitao CHEN ASc PKc
S10 Microsoft Research US Andi WU CTBo,c PKo,c
S11 SYSTRAN Software, Inc. US Jin YANG ASo CTBo HKo PKo
S12 U Penn US Nianwen XUE ASc HKc
Table 2: Participating sites and associated tracks.
internally, since transcoding back to the original en-
coding may lose information.
4 Results
4.1 Baseline and topline experiments
We computed a baseline for each of the corpora by
compiling a dictionary of all and only the words in
the training portion of the corpus. We then used this
dictionary with a simple maximum matching algo-
rithm to segment the test corpus. The results of this
experiment are presented in Table 3. In this and sub-
sequent tables, we list the word count for the test
corpus, test recall (R), test precision (P), F score1,
the out-of-vocabulary (OOV) rate for the test corpus,
the recall on OOV words (R    ), and the recall on
in-vocabulary (R   ) words. Per normal usage, OOV
is defined as the set of words in the test corpus not
occurring in the training corpus.2 We expect sys-
tems to do at least as well as this baseline.
As a nominal topline we ran the same maximum
matching experiments, but this time populating the
dictionary only with words from the test corpus; this
is of course a ?cheating? experiment since one could
1We use a balanced F score, so that 	

 .
2Note that the OOV recall in Table 3 should in theory be
0.0, but is not always zero because the maximum matching al-
gorithm might get lucky. In particular, if the dictionary con-
tains no word starting with some character  , then the maximum
matching algorithm with move on to the next character, leaving
 segmented as a word on its own. If it happens that  is in fact a
single-character word, then the algorithm will have fortuitously
done the right thing.
not reasonably know exactly the set of words that
occur in the test corpus. Since this is better than one
could hope for in practice, we would expect systems
to generally underperform this topline. The results
of this ?cheating? experiment are given in Table 4.3
4.2 Raw scores
4.2.1 Closed Tests
Results for the closed tests are presented in Ta-
bles 5?8. Column headings are as above, except for
?c  ?, and ?c ? for which see Section 4.3.
4.2.2 Open Tests
Results for the open tests are presented in Ta-
bles 9?12; again, see Section 4.3 for the explanation
of ?c  ?, and ?c ?.
4.3 Statistical significance of the results
Let us assume that the recall rates for the various
system represent the probability Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 579?586, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Emotions from text: machine learning for text-based emotion prediction
Cecilia Ovesdotter Alm?
Dept. of Linguistics
UIUC
Illinois, USA
ebbaalm@uiuc.edu
Dan Roth
Dept. of Computer Science
UIUC
Illinois, USA
danr@uiuc.edu
Richard Sproat
Dept. of Linguistics
Dept. of Electrical Eng.
UIUC
Illinois, USA
rws@uiuc.edu
Abstract
In addition to information, text con-
tains attitudinal, and more specifically,
emotional content. This paper explores
the text-based emotion prediction prob-
lem empirically, using supervised machine
learning with the SNoW learning archi-
tecture. The goal is to classify the emo-
tional affinity of sentences in the narra-
tive domain of children?s fairy tales, for
subsequent usage in appropriate expres-
sive rendering of text-to-speech synthe-
sis. Initial experiments on a preliminary
data set of 22 fairy tales show encourag-
ing results over a na??ve baseline and BOW
approach for classification of emotional
versus non-emotional contents, with some
dependency on parameter tuning. We
also discuss results for a tripartite model
which covers emotional valence, as well
as feature set alernations. In addition, we
present plans for a more cognitively sound
sequential model, taking into considera-
tion a larger set of basic emotions.
1 Introduction
Text does not only communicate informative con-
tents, but also attitudinal information, including
emotional states. The following reports on an em-
pirical study of text-based emotion prediction.
Section 2 gives a brief overview of the intended
application area, whereas section 3 summarizes re-
lated work. Next, section 4 explains the empirical
study, including the machine learning model, the
corpus, the feature set, parameter tuning, etc. Sec-
tion 5 presents experimental results from two classi-
fication tasks and feature set modifications. Section
6 describes the agenda for refining the model, before
presenting concluding remarks in 7.
2 Application area: Text-to-speech
Narrative text is often especially prone to having
emotional contents. In the literary genre of fairy
tales, emotions such as HAPPINESS and ANGER and
related cognitive states, e.g. LOVE or HATE, become
integral parts of the story plot, and thus are of par-
ticular importance. Moreover, the story teller read-
ing the story interprets emotions in order to orally
convey the story in a fashion which makes the story
come alive and catches the listeners? attention.
In speech, speakers effectively express emotions
by modifying prosody, including pitch, intensity,
and durational cues in the speech signal. Thus, in
order to make text-to-speech synthesis sound as nat-
ural and engaging as possible, it is important to con-
vey the emotional stance in the text. However, this
implies first having identified the appropriate emo-
tional meaning of the corresponding text passage.
Thus, an application for emotional text-to-speech
synthesis has to solve two basic problems. First,
what emotion or emotions most appropriately de-
scribe a certain text passage, and second, given a text
passage and a specified emotional mark-up, how to
render the prosodic contour in order to convey the
emotional content, (Cahn, 1990). The text-based
emotion prediction task (TEP) addresses the first of
these two problems.
579
3 Previous work
For a complete general overview of the field of af-
fective computing, see (Picard, 1997). (Liu, Lieber-
man and Selker, 2003) is a rare study in text-
based inference of sentence-level emotional affin-
ity. The authors adopt the notion of basic emotions,
cf. (Ekman, 1993), and use six emotion categories:
ANGER, DISGUST, FEAR, HAPPINESS, SADNESS,
SURPRISE. They critique statistical NLP for being
unsuccessful at the small sentence level, and instead
use a database of common-sense knowledge and cre-
ate affect models which are combined to form a rep-
resentation of the emotional affinity of a sentence.
At its core, the approach remains dependent on an
emotion lexicon and hand-crafted rules for concep-
tual polarity. In order to be effective, emotion recog-
nition must go beyond such resources; the authors
note themselves that lexical affinity is fragile. The
method was tested on 20 users? preferences for an
email-client, based on user-composed text emails
describing short but colorful events. While the users
preferred the emotional client, this evaluation does
not reveal emotion classification accuracy, nor how
well the model generalizes on a large data set.
Whereas work on emotion classification from
the point of view of natural speech and human-
computer dialogues is fairly extensive, e.g. (Scherer,
2003), (Litman and Forbes-Riley, 2004), this ap-
pears not to be the case for text-to-speech synthe-
sis (TTS). A short study by (Sugimoto et al, 2004)
addresses sentence-level emotion recognition for
Japanese TTS. Their model uses a composition as-
sumption: the emotion of a sentence is a function of
the emotional affinity of the words in the sentence.
They obtain emotional judgements of 73 adjectives
and a set of sentences from 15 human subjects and
compute words? emotional strength based on the ra-
tio of times a word or a sentence was judged to fall
into a particular emotion bucket, given the number
of human subjects. Additionally, they conducted an
interactive experiment concerning the acoustic ren-
dering of emotion, using manual tuning of prosodic
parameters for Japanese sentences. While the au-
thors actually address the two fundamental problems
of emotional TTS, their approach is impractical and
most likely cannot scale up for a real corpus. Again,
while lexical items with clear emotional meaning,
such as happy or sad, matter, emotion classifica-
tion probably needs to consider additional inference
mechanisms. Moreover, a na??ve compositional ap-
proach to emotion recognition is risky due to simple
linguistic facts, such as context-dependent seman-
tics, domination of words with multiple meanings,
and emotional negation.
Many NLP problems address attitudinal mean-
ing distinctions in text, e.g. detecting subjective
opinion documents or expressions, e.g. (Wiebe et
al, 2004), measuring strength of subjective clauses
(Wilson, Wiebe and Hwa, 2004), determining word
polarity (Hatzivassiloglou and McKeown, 1997) or
texts? attitudinal valence, e.g. (Turney, 2002), (Bai,
Padman and Airoldi, 2004), (Beineke, Hastie and
Vaithyanathan, 2003), (Mullen and Collier, 2003),
(Pang and Lee, 2003). Here, it suffices to say that
the targets, the domain, and the intended application
differ; our goal is to classify emotional text passages
in children?s stories, and eventually use this infor-
mation for rendering expressive child-directed sto-
rytelling in a text-to-speech application. This can be
useful, e.g. in therapeutic education of children with
communication disorders (van Santen et al, 2003).
4 Empirical study
This part covers the experimental study with a for-
mal problem definition, computational implementa-
tion, data, features, and a note on parameter tuning.
4.1 Machine learning model
Determining emotion of a linguistic unit can be
cast as a multi-class classification problem. For
the flat case, let T denote the text, and s an em-
bedded linguistic unit, such as a sentence, where
s ? T . Let k be the number of emotion classes E =
{em1, em2, .., emk}, where em1 denotes the special
case of neutrality, or absence of emotion. The goal
is to determine a mapping function f : s ? emi,
such that we obtain an ordered labeled pair (s, emi).
The mapping is based on F = {f1, f2, .., fn}, where
F contains the features derived from the text.
Furthermore, if multiple emotion classes can
characterize s, then given E? ? E, the target of the
mapping function becomes the ordered pair (s,E?).
Finally, as further discussed in section 6, the hier-
archical case of label assignment requires a sequen-
580
tial model that further defines levels of coarse ver-
sus fine-grained classifiers, as done by (Li and Roth,
2002) for the question classification problem.
4.2 Implementation
Whereas our goal is to predict finer emotional mean-
ing distinctions according to emotional categories in
speech; in this study, we focus on the basic task of
recognizing emotional passages and on determining
their valence (i.e. positive versus negative) because
we currently do not have enough training data to ex-
plore finer-grained distinctions. The goal here is to
get a good understanding of the nature of the TEP
problem and explore features which may be useful.
We explore two cases of flat classification, us-
ing a variation of the Winnow update rule imple-
mented in the SNoW learning architecture (Carl-
son et al, 1999),1 which learns a linear classifier
in feature space, and has been successful in sev-
eral NLP applications, e.g. semantic role labeling
(Koomen, Punyakanok, Roth and Yih, 2005). In
the first case, the set of emotion classes E consists
of EMOTIONAL versus non-emotional or NEUTRAL,
i.e. E = {N,E}. In the second case, E has been
incremented with emotional distinctions according
to the valence, i.e. E = {N,PE,NE}. Experi-
ments used 10-fold cross-validation, with 90% train
and 10% test data.2
4.3 Data
The goal of our current data annotation project is
to annotate a corpus of approximately 185 children
stories, including Grimms?, H.C. Andersen?s and B.
Potter?s stories. So far, the annotation process pro-
ceeds as follows: annotators work in pairs on the
same stories. They have been trained separately and
work independently in order to avoid any annota-
tion bias and get a true understanding of the task
difficulty. Each annotator marks the sentence level
with one of eight primary emotions, see table 1, re-
flecting an extended set of basic emotions (Ekman,
1993). In order to make the annotation process more
focused, emotion is annotated from the point of view
of the text, i.e. the feeler in the sentence. While the
primary emotions are targets, the sentences are also
1Available from http://l2r.cs.uiuc.edu/?cogcomp/
2Experiments were also run for Perceptron, however the re-
sults are not included. Overall, Perceptron performed worse.
marked for other affective contents, i.e. background
mood, secondary emotions via intensity, feeler, and
textual cues. Disagreements in annotations are re-
solved by a second pass of tie-breaking by the first
author, who chooses one of the competing labels.
Eventually, the completed annotations will be made
available.
Table 1: Basic emotions used in annotation
Abbreviation Emotion class
A ANGRY
D DISGUSTED
F FEARFUL
H HAPPY
Sa SAD
Su+ POSITIVELY SURPRISED
Su- NEGATIVELY SURPRISED
Emotion annotation is hard; interannotator agree-
ment currently range at ? = .24 ? .51, with the ra-
tio of observed annotation overlap ranging between
45-64%, depending on annotator pair and stories as-
signed. This is expected, given the subjective nature
of the annotation task. The lack of a clear defini-
tion for emotion vs. non-emotion is acknowledged
across the emotion literature, and contributes to dy-
namic and shifting annotation targets. Indeed, a
common source of confusion is NEUTRAL, i.e. de-
ciding whether or not a sentence is emotional or
non-emotional. Emotion perception also depends on
which character?s point-of-view the annotator takes,
and on extratextual factors such as annotator?s per-
sonality or mood. It is possible that by focusing
more on the training of annotator pairs, particularly
on joint training, agreement might improve. How-
ever, that would also result in a bias, which is prob-
ably not preferable to actual perception. Moreover,
what agreement levels are needed for successful ex-
pressive TTS remains an empirical question.
The current data set consisted of a preliminary an-
notated and tie-broken data set of 1580 sentence, or
22 Grimms? tales. The label distribution is in table
2. NEUTRAL was most frequent with 59.94%.
Table 2: Percent of annotated labels
A D F H
12.34% 0.89% 7.03% 6.77%
N SA SU+ SU.-
59.94% 7.34% 2.59% 3.10%
581
Table 3: % EMOTIONAL vs. NEUTRAL examples
E N
40.06% 59.94%
Table 4: % POSITIVE vs. NEGATIVE vs. NEUTRAL
PE NE N
9.87% 30.19% 59.94%
Next, for the purpose of this study, all emotional
classes, i.e. A, D, F, H, SA, SU+, SU-, were com-
bined into one emotional superclass E for the first
experiment, as shown in table 3. For the second ex-
periment, we used two emotional classes, i.e. pos-
itive versus negative emotions; PE={H, SU+} and
NE={A, D, F, SA, SU-}, as seen in table 4.
4.4 Feature set
The feature extraction was written in python. SNoW
only requires active features as input, which resulted
in a typical feature vector size of around 30 features.
The features are listed below. They were imple-
mented as boolean values, with continuous values
represented by ranges. The ranges generally over-
lapped, in order to get more generalization coverage.
1. First sentence in story
2. Conjunctions of selected features (see below)
3. Direct speech (i.e. whole quote) in sentence
4. Thematic story type (3 top and 15 sub-types)
5. Special punctuation (! and ?)
6. Complete upper-case word
7. Sentence length in words (0-1, 2-3, 4-8, 9-15,
16-25, 26-35, >35)
8. Ranges of story progress (5-100%, 15-100%,
80-100%, 90-100%)
9. Percent of JJ, N, V, RB (0%, 1-100%, 50-
100%, 80-100%)
10. V count in sentence, excluding participles (0-1,
0-3, 0-5, 0-7, 0-9, > 9)
11. Positive and negative word counts ( ? 1, ? 2,
? 3, ? 4, ? 5, ? 6)
12. WordNet emotion words
13. Interjections and affective words
14. Content BOW: N, V, JJ, RB words by POS
Feature conjunctions covered pairings of counts of
positive and negative words with range of story
progress or interjections, respectively.
Feature groups 1, 3, 5, 6, 7, 8, 9, 10 and 14 are ex-
tracted automatically from the sentences in the sto-
ries; with the SNoW POS-tagger used for features
9, 10, and 14. Group 10 reflects how many verbs
are active in a sentence. Together with the quotation
and punctuation, verb domination intends to capture
the assumption that emotion is often accompanied
by increased action and interaction. Feature group
4 is based on Finish scholar Antti Aarne?s classes
of folk-tale types according to their informative the-
matic contents (Aarne, 1964). The current tales
have 3 top story types (ANIMAL TALES, ORDINARY
FOLK-TALES, and JOKES AND ANECDOTES), and
15 subtypes (e.g. supernatural helpers is a subtype
of the ORDINARY FOLK-TALE). This feature intends
to provide an idea about the story?s general affective
personality (Picard, 1997), whereas the feature re-
flecting the story progress is hoped to capture that
some emotions may be more prevalent in certain
sections of the story (e.g. the happy end).
For semantic tasks, words are obviously impor-
tant. In addition to considering ?content words?, we
also explored specific word lists. Group 11 uses
2 lists of 1636 positive and 2008 negative words,
obtained from (Di Cicco et al, online). Group 12
uses lexical lists extracted from WordNet (Fellbaum,
1998), on the basis of the primary emotion words
in their adjectival and nominal forms. For the ad-
jectives, Py-WordNet?s (Steele et al, 2004) SIMI-
LAR feature was used to retrieve similar items of
the primary emotion adjectives, exploring one addi-
tional level in the hierarchy (i.e. similar items of all
senses of all words in the synset). For the nouns and
any identical verbal homonyms, synonyms and hy-
ponyms were extracted manually.3 Feature group 13
used a short list of 22 interjections collected manu-
ally by browsing educational ESL sites, whereas the
affective word list of 771 words consisted of a com-
bination of the non-neutral words from (Johnson-
Laird and Oatley, 1989) and (Siegle, online). Only a
subset of these lexical lists actually occurred.4
3Multi-words were transformed to hyphenated form.
4At this point, neither stems and bigrams nor a list of ono-
matopoeic words contribute to accuracy. Intermediate resource
processing inserted some feature noise.
582
The above feature set is henceforth referred to as
all features, whereas content BOW is just group 14.
The content BOW is a more interesting baseline than
the na??ve one, P(Neutral), i.e. always assigning the
most likely NEUTRAL category. Lastly, emotions
blend and transform (Liu, Lieberman and Selker,
2003). Thus, emotion and background mood of im-
mediately adjacent sentences, i.e. the sequencing,
seems important. At this point, it is not implemented
automatically. Instead, it was extracted from the
manual emotion and mood annotations. If sequenc-
ing seemed important, an automatic method using
sequential target activation could be added next.
4.5 Parameter tuning
The Winnow parameters that were tuned included
promotional ?, demotional ?, activation threshold
?, initial weights ?, and the regularization parame-
ter, S, which implements a margin between positive
and negative examples. Given the currently fairly
limited data, results from 2 alternative tuning meth-
ods, applied to all features, are reported.
? For the condition called sep-tune-eval, 50%
of the sentences were randomly selected and
set aside to be used for the parameter tuning
process only. Of this subset, 10% were subse-
quently randomly chosen as test set with the re-
maining 90% used for training during the auto-
matic tuning process, which covered 4356 dif-
ferent parameter combinations. Resulting pa-
rameters were: ? = 1.1, ? = 0.5, ? = 5,
? = 1.0, S = 0.5. The remaining half of
the data was used for training and testing in the
10-fold cross-validation evaluation. (Also, note
the slight change for P(Neutral) in table 5, due
to randomly splitting the data.)
? Given that the data set is currently small, for the
condition named same-tune-eval, tuning was
performed automatically on all data using a
slightly smaller set of combinations, and then
manually adjusted against the 10-fold cross-
validation process. Resulting parameters were:
? = 1.2, ? = 0.9, ? = 4, ? = 1, S = 0.5. All
data was used for evaluation.
Emotion classification was sensitive to the selected
tuning data. Generally, a smaller tuning set resulted
in pejorative parameter settings. The random selec-
tion could make a difference, but was not explored.
5 Results and discussion
This section first presents the results from exper-
iments with the two different confusion sets de-
scribed above, as well as feature experimentation.
5.1 Classification results
Average accuracy from 10-fold cross validation for
the first experiment, i.e. classifying sentences as ei-
ther NEUTRAL or EMOTIONAL, are included in ta-
ble 5 and figure 1 for the two tuning conditions on
the main feature sets and baselines. As expected,
Table 5: Mean classification accuracy: N vs. E, 2 conditions
same-tune-eval sep-tune-eval
P(Neutral) 59.94 60.05
Content BOW 61.01 58.30
All features except BOW 64.68 63.45
All features 68.99 63.31
All features + sequencing 69.37 62.94
degree of success reflects parameter settings, both
for content BOW and all features. Nevertheless, un-
der these circumstances, performance above a na??ve
baseline and a BOW approach is obtained. More-
over, sequencing shows potential for contributing
in one case. However, observations also point to
three issues: first, the current data set appears to
be too small. Second, the data is not easily separa-
ble. This comes as no surprise, given the subjective
nature of the task, and the rather low interannota-
tor agreement, reported above. Moreover, despite
the schematic narrative plots of children?s stories,
tales still differ in their overall affective orientation,
which increases data complexity. Third and finally,
the EMOTION class is combined by basic emotion
labels, rather than an original annotated label.
More detailed averaged results from 10-fold
cross-validation are included in table 6 using all
features and the separated tuning and evaluation
data condition sep-tune-eval. With these parame-
ters, approximately 3% improvement in accuracy
over the na??ve baseline P(Neutral) was recorded,
and 5% over the content BOW, which obviously did
poorly with these parameters. Moreover, precision is
583
0 10 20 30 40 50 60 70
same-tune-eval
sep-tune-eval
Tuning sets
% Accuracy
P(Neutral) Content BOWAll features except BOW All featuresAll features + sequencing
Figure 1: Accuracy under different conditions (in %)
Table 6: Classifying N vs. E (all features, sep-tune-eval)
Measure N E
Averaged accuracy 0.63 0.63
Averaged error 0.37 0.37
Averaged precision 0.66 0.56
Averaged recall 0.75 0.42
Averaged F-score 0.70 0.47
higher than recall for the combined EMOTION class.
In comparison, with the same-tune-eval procedure,
the accuracy improved by approximately 9% over
P(Neutral) and by 8% over content BOW.
In the second experiment, the emotion category
was split into two classes: emotions with positive
versus negative valence. The results in terms of pre-
cision, recall, and F-score are included in table 7, us-
ing all features and the sep-tune-eval condition. The
decrease in performance for the emotion classes mir-
rors the smaller amounts of data available for each
class. As noted in section 4.3, only 9.87% of the
sentences were annotated with a positive emotion,
and the results for this class are worse. Thus, perfor-
mance seems likely to improve as more annotated
story data becomes available; at this point, we are
experimenting with merely around 12% of the total
texts targeted by the data annotation project.
5.2 Feature experiments
Emotions are poorly understood, and it is espe-
cially unclear which features may be important for
their recognition from text. Thus, we experimented
Table 7: N, PE, and NE (all features, sep-tune-eval)
N NE PE
Averaged precision 0.64 0.45 0.13
Averaged recall 0.75 0.27 0.19
Averaged F-score 0.69 0.32 0.13
Table 8: Feature group members
Word lists interj., WordNet, affective lists, pos/neg
Syntactic length ranges, % POS, V-count ranges
Story-related % story-progress, 1st sent., story type
Orthographic punctuation, upper-case words, quote
Conjunctions Conjunctions with pos/neg
Content BOW Words (N,V,Adj, Adv)
with different feature configurations. Starting with
all features, again using 10-fold cross-validation for
the separated tuning-evaluation condition sep-tune-
eval, one additional feature group was removed un-
til none remained. The feature groups are listed in
table 8. Figure 2 on the next page shows the accu-
racy at each step of the cumulative subtraction pro-
cess. While some feature groups, e.g. syntactic, ap-
peared less important, the removal order mattered;
e.g. if syntactic features were removed first, accu-
racy decreased. This fact also illustrated that fea-
tures work together; removing any group degraded
performance because features interact and there is
no true independence. It was observed that fea-
tures? contributions were sensitive to parameter tun-
ing. Clearly, further work on developing features
which fit the TEP problem is needed.
6 Refining the model
This was a ?first pass? of addressing TEP for TTS.
At this point, the annotation project is still on-going,
and we only had a fairly small data set to draw on.
Nevertheless, results indicate that our learning ap-
proach benefits emotion recognition. For example,
the following instances, also labeled with the same
valence by both annotators, were correctly classified
both in the binary (N vs. E) and the tripartite polar-
ity task (N, NE, PE), given the separated tuning and
evaluation data condition, and using all features:
(1a) E/NE: Then he offered the dwarfs money, and prayed and
besought them to let him take her away; but they said, ?We will
not part with her for all the gold in the world.?
584
Cumulative removal of feature groups
61.81
63.31
62.57
57.95
58.30
58.93
59.56
55
60
65
All features
- Word lists
- Syntactic
- Story-related
- Orthographic
- Conjunctions
- Content words
% A
ccur
acy
All features P(Neutral) BOW
Figure 2: Averaged effect of feature group removal, using sep-tune-eval
(1b) N: And so the little girl really did grow up; her skin was as
white as snow, her cheeks as rosy as the blood, and her hair as
black as ebony; and she was called Snowdrop.
(2a) E/NE: ?Ah,? she answered, ?have I not reason to weep?
(2b) N: Nevertheless, he wished to try him first, and took a stone
in his hand and squeezed it together so that water dropped out
of it.
Cases (1a) and (1b) are from the well-known FOLK
TALE Snowdrop, also called Snow White. (1a)
and (1b) are also correctly classified by the sim-
ple content BOW approach, although our approach
has higher prediction confidence for E/NE (1a); it
also considers, e.g. direct speech, a fairly high verb
count, advanced story progress, connotative words
and conjunctions thereof with story progress fea-
tures, all of which the BOW misses. In addition, the
simple content BOW approach makes incorrect pre-
dictions at both the bipartite and tripartite levels for
examples (2a) and (2b) from the JOKES AND ANEC-
DOTES stories Clever Hans and The Valiant Little
Tailor, while our classifier captures the affective dif-
ferences by considering, e.g. distinctions in verb
count, interjection, POS, sentence length, connota-
tions, story subtype, and conjunctions.
Next, we intend to use a larger data set to conduct
a more complete study to establish mature findings.
We also plan to explore finer emotional meaning dis-
tinctions, by using a hierarchical sequential model
which better corresponds to different levels of cog-
nitive difficulty in emotional categorization by hu-
mans, and to classify the full set of basic level emo-
tional categories discussed in section 4.3. Sequential
modeling of simple classifiers has been successfully
employed to question classification, for example by
(Li and Roth, 2002). In addition, we are working
on refining and improving the feature set, and given
more data, tuning can be improved on a sufficiently
large development set. The three subcorpora in the
annotation project can reveal how authorship affects
emotion perception and classification.
Moreover, arousal appears to be an important
dimension for emotional prosody (Scherer, 2003),
especially in storytelling (Alm and Sproat, 2005).
Thus, we are planning on exploring degrees of emo-
tional intensity in a learning scenario, i.e. a prob-
lem similar to measuring strength of opinion clauses
(Wilson, Wiebe and Hwa, 2004).
Finally, emotions are not discrete objects; rather
they have transitional nature, and blend and overlap
along the temporal dimension. For example, (Liu,
Lieberman and Selker, 2003) include parallel esti-
mations of emotional activity, and include smooth-
585
ing techniques such as interpolation and decay to
capture sequential and interactive emotional activity.
Observations from tales indicate that some emotions
are more likely to be prolonged than others.
7 Conclusion
This paper has discussed an empirical study of the
text-based emotion prediction problem in the do-
main of children?s fairy tales, with child-directed ex-
pressive text-to-speech synthesis as goal. Besides
reporting on encouraging results in a first set of com-
putational experiments using supervised machine
learning, we have set forth a research agenda for
tackling the TEP problem more comprehensively.
8 Acknowledgments
We are grateful to the annotators, in particular A.
Rasmussen and S. Siddiqui. We also thank two
anonymous reviewers for comments. This work was
funded by NSF under award ITR-#0205731, and NS
ITR IIS-0428472. The annotation is supported by
UIUC?s Research Board. The authors take sole re-
sponsibility for the work.
References
Antti Aarne. 1964. The Types of the Folk-Tale: a Classification
and Bibliography. Helsinki: Suomalainen Tiedeakatemia.
Cecilia O. Alm, and Richard Sproat. 2005. Perceptions of emo-
tions in expressive storytelling. INTERSPEECH 2005.
Xue Bai, Rema Padman, and Edoardo Airoldi. 2004. Sen-
timent extraction from unstructured text using tabu search-
enhanced Markov blankets. In MSW2004, Seattle.
Philip Beineke, Trevor Hastie, and Shivakumar Vaithyanathan.
2004. The sentimental factor: improving review classifi-
cation via human-provided information. In Proceedings of
ACL, 263?270.
Janet Cahn. 1990. The generation of affect in synthesized
Speech. Journal of the American Voice I/O Society, 8:1?19.
Andrew Carlson, Chad Cumby, Nicholas Rizzolo, Jeff Rosen,
and Dan Roth. 1999. The SNoW Learning Architecture.
Technical Report UIUCDCS-R-99-2101, UIUC Comp. Sci.
Stacey Di Cicco et al General Inquirer Pos./Neg. lists
http://www.webuse.umd.edu:9090/
Paul Ekman. 1993. Facial expression and emotion. American
Psychologist, 48(4), 384?392.
Christiane Fellbaum, Ed. 1998. WordNet: An Electronic Lexi-
cal Database. MIT Press, Cambridge, Mass.
Vasileios Hatzivassiloglou, and Kathleen McKeown. 1997.
Predicting the semantic orientation of adjectives. In Pro-
ceedings of ACL, 174?181.
Philip Johnson-Laird, and Keith Oatley. 1989. The language
of emotions: an analysis of a semantic field. Cognition and
Emotion, 3:81?123.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2005. Generalized inference with multiple semantic role la-
beling systems. In Proceedings of the Annual Conference on
Computational Language Learning (CoNLL), 181?184.
Diane Litman, and Kate Forbes-Riley. 2004. Predicting stu-
dent emotions in computer-human tutoring dialogues. In
Proceedings of ACL, 351?358.
Xin Li, and Dan Roth. 2002. Learning question classifiers: the
role of semantic information. In Proc. International Confer-
ence on Computational Linguistics (COLING), 556?562.
Hugo Liu, Henry Lieberman, and Ted Selker. 2003. A model of
textual affect sensing using real-world knowledge. In ACM
Conference on Intelligent User Interfaces, 125?132.
Tony Mullen, and Nigel Collier. 2004. Sentiment analy-
sis using support vector machines with diverse information
sources. In Proceedings of EMNLP, 412?418.
Bo Pang, and Lillian Lee. 2004. A sentimental education: sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proceedings of ACL, 271?278.
Rosalind Picard. 1997. Affective computing. MIT Press, Cam-
bridge, Mass.
Dan Roth. 1998. Learning to resolve natural language ambigu-
ities: a unified approach. In AAAI, 806?813.
Klaus Scherer. 2003. Vocal communication of emotion: a
review of research paradigms. Speech Commununication,
40(1-2):227?256.
Greg Siegle. The Balanced Affective Word List
http://www.sci.sdsu.edu/CAL/wordlist/words.prn
Oliver Steele et al Py-WordNet
http://osteele.com/projects/pywordnet/
Futoshi Sugimoto et al 2004. A method to classify emotional
expressions of text and synthesize speech. In IEEE, 611?
614.
Peter Turney. 2002. Thumbs up or thumbs down? Semantic
orientation applied to unsupervised classification of reviews.
In Proceedings of ACL, 417?424.
Jan van Santen et al 2003. Applications of computer gen-
erated expressive speech for communication disorders. In
EUROSPEECH 2003, 1657?1660.
Janyce Wiebe et al 2004. Learning subjective language. Jour-
nal of Computational Linguistics, 30(3):277?308.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004. Just
how mad are you? Finding strong and weak opinion clauses.
In Proceedings of the Nineteenth National Conference on Ar-
tificial Intelligence (AAAI), 761?769.
586
Book Review
Mathematical Linguistics
Andra?s Kornai
(MetaCarta Inc.)
Springer (Advanced information and knowledge processing series, edited by
Lakhmi Jain), 2008, xiii+289 pp; ISBN 978-1-84628-985-9, $99.00
Reviewed by
Richard Sproat and Roxana G??rju
University of Illinois at Urbana-Champaign
For readers of traditional textbooks such as that of Partee, ter Meulen, and Wall (1990),
the term ?mathematical linguistics? denotes a rather narrowly circumscribed set of issues
including automata theory, set theory, and lambda calculus, with maybe a little formal
language theory thrown in. Kornai?s contribution is refreshingly different in that he
treats, in this relatively compact volume, practically all areas of linguistics, phonetics,
and speech and language processing.
Kornai?s motivation for writing this book is to present ?a single entry point to
the central methods and concepts of linguistics that are made largely inaccessible
to the mathematician, computer scientist, or engineer by the surprisingly adversarial
style of argumentation . . . and the proliferation of unmotivated notation and formal-
ism . . . all too often encountered in research papers and monographs in the humanities?
(page viii). There is no question that much of what passes for rigor (mathematical and
scientific) in linguistics is a joke and that there is clearly a need for any work that can
place the field on a more solid footing. It also seems likely that Kornai is the only person
who could have written this book.
The book is divided into ten chapters, including a short introductory chapter, which
lays the groundwork and identifies the potential audience, and a concluding chapter
where Kornai reveals his own views on what is important in the field, which in the
interests of balance he has largely suppressed throughout the book. Chapter 2 is also
introductory in that it presents basic concepts of generation (via a ruleset), axioms, and
string rewriting.
The main chapters (3?9) deal with a variety of topic areas relating to language
and speech, starting with phonology in Chapter 3. This chapter introduces the notion
of phonemes, distinctive features, autosegmental phonology, and computation using
finite automata. Kornai offers many details that are of course lacking in most linguistic
treatments, such as a proof that the number of well-formed association lines between
two tiers of length n is asymptotically (6+ 4
?
2)n.
Chapter 4 deals with morphology, which for Kornai includes not only word forma-
tion, but also prosody (including stress assignment and moraic structure), as well as
Optimality Theory and Zipf?s law.
The fifth chapter treats syntax, including categorial grammar, phrase structure,
dependency frameworks, valency, and weighted models of grammar, ending with a
discussion of weighted finite automata and hidden Markov models. In the context of
weighted models, Kornai implies that Chomsky?s original notion of degree of gram-
maticality fits naturally as an instance of a weighted model with a particular semiring;
of course, exactly what the ? and ? operators of that semiring map to remain to
Computational Linguistics Volume 34, Number 4
be seen insofar as the notion ?degree of grammaticality? has never been rigorously
defined.
Chapter 6, on Semantics, starts with a discussion of various standard paradoxes
such as the Liar, and then moves on to an overview of Montague?s theory, type theory,
and grammatical semantics. Throughout the discussion, Kornai underscores the fun-
damental limitations of theories of semantics that are based purely upon evaluation of
truth conditions for artificial fragments, an important point for anyone who wants to
go beyond theoretical philosophically inspired models and consider semantic interpre-
tation in the real world.
Complexity is the topic of Chapter 7. This is not the Chomsky-hierarchy notion of
complexity, but rather deals with information theory, in particular entropy, Kolmogorov
complexity, and a short section on learning, including identification in the limit and PAC
learning.
Pattern recognition is divided across two chapters, with Chapter 8 laying the es-
sential groundwork of linguistic pattern recognition, and Chapter 9 presenting details
on speech processing and handwriting recognition. This includes feature extraction: In
the case of speech recognition, Kornai reviews the frequency representation of speech
signals, and defines the cepstrum. Discussion of acoustic models leads us to phonemes
as hidden units, with a slight detour into the fine-grained distinctions between different
levels of phonemic analysis in the once popular but now largely discredited theory of
Lexical Phonology.
Each chapter ends with a section entitled ?Further Reading,? and the texts referred
to are generally quite useful as material for readers who wish to explore the issues
further.
According to Wikipedia, Kornai is a ?well-known mathematical linguist? whose
Erdo?s number is 2. Unfortunately, neither of us can claim Kornai?s mathematical so-
phistication or stature, but on the other hand this makes us good judges of the book?s
potential audience; and herein lies a problem. Kornai?s target is ?anyone with suffi-
cient general mathematical maturity? with ?[n]o prior knowledge of linguistics or lan-
guages . . . assumed on the part of the reader? (page viii). This suggests that the book
is not primarily aimed at linguists, and certainly the mathematical maturity assumed
puts this book well beyond the reach of most linguists, so that it could not easily be
used in an introductory course on mathematical linguistics in a linguistics program. It
is probably beyond the reach of many computer science students as well.
What about those who do have the mathematical maturity, but know nothing
about linguistics? The problem here is that in many cases Kornai does not give enough
background (or any background) to appreciate the significance of the particular issues
being discussed. For example, on page 77 Kornai gives weak crossover and heavy NP shift
as examples of phenomena that have ?weak? effects on grammaticality, and resumptive
pronouns as examples of phenomena that are marginal in some languages (such as Eng-
lish). But nowhere does he explain what these terms denote, which means that these are
throw-away comments for anyone who does not already know. Section 3.2 introduces
phonological features and feature geometry and sketches some of the mathematical
properties of systems with features; but very little background is given on what features
are supposed to represent. The short discussion of Optimality Theory (pages 67?69)
hardly gives enough background to give a feel for the main points of that approach.
In other cases, topics are introduced but their importance to surrounding topics is hard
to fathom. For example, in Section 6.1.3 a discussion of the Berry paradox leads into a
digression on how to implement digit-sequence-to-number-name mappings as finite-
state transducers. Apart from giving Kornai an opportunity to emphasize that this is
616
Book Review
trivial to do (something that is true in principle, but less true in practice, depending
upon the language), it is not clear what purpose this digression serves.
There are also a number of places where issues are presented in a non-standard
way, which might make sense from some points of view, but not if you are trying to
introduce someone to the way the field is practiced. It is odd, for instance, that prosody
is introduced not in the chapter on phonology but in the one on morphology. It is
also somewhat odd that Zipf?s law gets introduced in the morphology chapter. (And
why is it that nowhere does Kornai cite Baayen?s excellent book on word-frequency
distributions (Baayen 2001), which would be a very useful source of further information
on this topic to any reader of Kornai?s book?)
Some material presented is puzzling or simply wrong. It is not explained in what
sense German has a ?pure SVO construction? (page 103) in contradistinction to the
normal assumption that German is verb-second. The Cypriot syllabary does not date
from the 15th century BCE (page 54); Latin does not have two locative cases (page 90)?
indeed, it does not even have one locative case, so-called; the basic Hangul letter shapes
(introduced on page 31 to make a point about phonetic features) are, with two excep-
tions, completely incorrect?probably it would have been better to use a real Korean
font rather than trying to imitate the jamowith LATEX math symbols. There are of course a
great many places where the discussion is useful and informative, but there are enough
examples of the kinds we have outlined that the uninitiated reader should be careful.
As far as we can see, the most likely readership of this book consists of (computa-
tional) linguists and others who already know the linguistic issues, have a fairly strong
formal and mathematical background, and could benefit from the more-precise and
more-rigorous mathematical expositions that Kornai provides.
Throughout the book, Kornai pauses occasionally to present exercises to the reader.
These range from relatively simple to major research projects. As with other aspects of
this book, the distribution of topics for the exercises is somewhat erratic. Thus, on page
184, in the chapter on complexity, we are offered exercises 7.6 and 7.7 in close proximity:
Exercise 7.6 Prove that a regular language is prefix-free iff it is accepted by a DFSA
with no transitions out of accepting states. Is a prefix-free language context-free iff it
is accepted by a DPDA with the same restriction on its control?
...
Exercise 7.7 Research the role of the ascii codes 0x02 (STX), 0x03 (ETX), and 0x16
(SYN).
But variety is, after all, what keeps things interesting.
References
Baayen, R. Harald 2001. Word Frequency
Distributions. Kluwer Academic
Publishers, Dordrecht.
Partee, Barbara, Alice ter Meulen, and Robert
Wall. 1990. Mathematical Methods in
Linguistics. Kluwer Academic Publishers,
Dordrecht.
Richard Sproat is Professor of Linguistics and Electrical and Computer Engineering at the Uni-
versity of Illinois at Urbana-Champaign. He works on computational morphology, text normal-
ization, and speech processing. His Erdo?s number is 4. Roxana G??rju is Assistant Professor of
Linguistics at the University of Illinois at Urbana-Champaign. She has a Ph.D. in Computer
Science and works on computational semantics, pragmatics, and inference. Her Erdo?s number
is also 4. Their address is Department of Linguistics, University of Illinois at Urbana-Champaign,
Foreign Languages Building 4016D, 707 South Matthews Avenue, MC-168, Urbana, IL, 61801;
e-mail: rws@uiuc.edu and girju@uiuc.edu.
617

Lattice-Based Search for Spoken Utterance Retrieval
Murat Saraclar
AT&T Labs ? Research
180 Park Ave. Florham Park, NJ 07932
murat@research.att.com
Richard Sproat
University of Illinois at Urbana-Champaign
Urbana, IL 61801
rws@uiuc.edu
Abstract
Recent work on spoken document retrieval has
suggested that it is adequate to take the single-
best output of ASR, and perform text retrieval
on this output. This is reasonable enough for
the task of retrieving broadcast news stories,
where word error rates are relatively low, and
the stories are long enough to contain much
redundancy. But it is patently not reasonable
if one?s task is to retrieve a short snippet of
speech in a domain where WER?s can be as
high as 50%; such would be the situation with
teleconference speech, where one?s task is to
find if and when a participant uttered a certain
phrase.
In this paper we propose an indexing proce-
dure for spoken utterance retrieval that works
on lattices rather than just single-best text. We
demonstrate that this procedure can improve F
scores by over five points compared to single-
best retrieval on tasks with poor WER and low
redundancy. The representation is flexible so
that we can represent both word lattices, as
well as phone lattices, the latter being impor-
tant for improving performance when search-
ing for phrases containing OOV words.
1 Introduction
Automatic systems for indexing, archiving, searching and
browsing of large amounts of spoken communications
have become a reality in the last decade. Most such sys-
tems use an automatic speech recognition (ASR) compo-
nent to convert speech to text which is then used as an
input to a standard text based information retrieval (IR)
component. This strategy works reasonably well when
speech recognition output is mostly correct or the docu-
ments are long enough so that some occurrences of the
query terms are recognized correctly.
Most of the research has concentrated on retrieval of
Broadcast News type of spoken documents where speech
is relatively clean and the documents are relatively long.
In addition it is possible to find large amounts of text with
similar content in order to build better language models
and enhance retrieval through use of similar documents.
We are interested in extending this to telephone con-
versations and teleconferences. Our task is locating oc-
currences of a query in spoken communications to aid
browsing. This is not exactly spoken document retrieval.
In fact, it is more similar to word spotting. Each docu-
ment is a short segment of audio.
Although reasonable retrieval performance can be ob-
tained using the best ASR hypothesis for tasks with
moderate (? 20%) word error rates, tasks with higher
(40? 50%) word error rates require use of multiple ASR
hypotheses. Use of ASR lattices makes the system more
robust to recognition errors.
Almost all ASR systems have a closed vocabulary.
This restriction comes from run-time requirements as
well as the finite amount of data used for training the
language models of the ASR systems. Typically the
recognition vocabulary is taken to be the words appear-
ing in the language model training corpus. Sometimes
the vocabulary is further reduced to only include the
most frequent words in the corpus. The words that are
not in this closed vocabulary ? the out of vocabulary
(OOV) words ? will not be recognized by the ASR sys-
tem, contributing to recognition errors. The effects of
OOV words in spoken document retrieval are discussed
by Woodland et al (2000). Using phonetic search helps
retrieve OOV words.
This paper is organized as follows. In Section 2 we
give an overview of related work, focusing on methods
dealing with speech recognition errors and OOV queries.
We present the methods used in this study in Section 3.
Experimental setup and results are given in Section 4. Fi-
nally, our conclusions are presented in Section 5.
2 Related Work
There are commercial systems including Nexidia/Fast-
Talk (www.nexidia.com), Virage/AudioLogger
(www.virage.com), Convera (www.convera.com)
as well as research systems like AT&T DVL (Cox et
al., 1998), AT&T ScanMail (Hirschberg et al, 2001),
BBN Rough?n?Ready (Makhoul et al, 2000), CMU
Informedia (www.informedia.cs.cmu.edu),
SpeechBot (www.speechbot.com), among others.
Also between 1997 and 2000, the Test REtrieval Con-
ference (TREC) had a spoken document retrieval (SDR)
track with many participants (Garofolo et al, 2000).
NIST TREC-9 SDR Web Site (2000) states that:
The results of the TREC-9 2000 SDR eval-
uation presented at TREC on November 14,
2000 showed that retrieval performance for
sites on their own recognizer transcripts was
virtually the same as their performance on the
human reference transcripts. Therefore, re-
trieval of excerpts from broadcast news using
automatic speech recognition for transcription
was deemed to be a solved problem - even with
word error rates of 30%.
PhD Theses written on this topic include James (1995),
Wechsler (1998), Siegler (1999) and Ng (2000).
Jones et al (1996) describe a system that com-
bines a large vocabulary continuous speech recognition
(LVCSR) system and a phone-lattice word spotter (WS)
for retrieval of voice and video mail messages (Brown
et al, 1996). Witbrock and Hauptmann (1997) present
a system where a phonetic transcript is obtained from
the word transcript and retrieval is performed using
both word and phone indices. Wechsler et al (1998)
present new techniques including a new method to
detect occurrences of query features, a new method
to estimate occurrence probabilities, a collection-wide
probability re-estimation technique and feature length
weighting. Srinivasan and Petkovic (2000) introduce a
method for phonetic retrieval based on the probabilis-
tic formulation of term weighting using phone confu-
sion data. Amir et al (2001) use indexing based on con-
fusable phone groups and a Bayesian phonetic edit dis-
tance for phonetic speech retrieval. Logan et al (2002)
compare three indexing methods based on words,
syllable-like particles, and phonemes to study the
problem of OOV queries in audio indexing systems.
Logan and Van Thong (2002) give an alternate approach
to the OOV query problem by expanding query words
into in-vocabulary phrases while taking acoustic confus-
ability and language model scores into account.
Of the previous work, the most similar approach to the
one proposed here is that of Jones et al (1996), in that
they used phone lattices to aid in word spotting, in ad-
dition to single-best output from LVCSR. Our proposal
might be thought of as a generalization of their approach
in that we use lattices as the sole representation over
which retrieval is performed. We believe that lattices are
a more natural representation for retrieval in cases where
there is a high degree of uncertainty about what was said,
which is typically the case in LVCSR systems for con-
versational speech. We feel that our results, presented
below, bear out this belief. Also novel in our approach is
the use of indexed lattices allowing for efficient retrieval.
As we note below, in the limit where one is using one-best
output, the indexed lattices reduce to the normal inverted
index used in text retrieval.
3 Methods
In this section we describe the overall structure of our
system and give details of the techniques used in our
investigations. The system consists of three main com-
ponents. First, the ASR component is used to convert
speech into a lattice representation, together with timing
information. Second, this representation is indexed for
efficient retrieval. These two steps are performed off-line.
Finally, when the user enters a query the index is searched
and matching audio segments are returned.
3.1 Automatic Speech Recognition
We use a state-of-the-art HMM based large vocabulary
continuous speech recognition (LVCSR) system. The
acoustic models consist of decision tree state clustered
triphones and the output distributions are mixtures of
Gaussians. The language models are pruned backoff tri-
gram models. The pronunciation dictionaries contain few
alternative pronunciations. Pronunciations that are not
in our baseline pronunciation dictionary (including OOV
query words) are generated using a text-to-speech (TTS)
frontend. The TTS frontend can produce multiple pro-
nunciations. The ASR systems used in this study are
single pass systems. The recognition networks are rep-
resented as weighted finite state machines (FSMs).
The output of the ASR system is also represented as an
FSM and may be in the form of a best hypothesis string
or a lattice of alternate hypotheses. The labels on the arcs
of the FSM may be words or phones, and the conversion
between the two can easily be done using FSM composi-
tion. The costs on the arcs are negative log likelihoods.
Additionally, timing information can also be present in
the output.
3.2 Lattice Indexing and Retrieval
In the case of lattices, we store a set of indices, one for
each arc label (word or phone) l, that records the lat-
tice number L[a], input-state k[a] of each arc a labeled
with l in each lattice, along with the probability mass
f(k[a]) leading to that state, the probability of the arc
itself p(a|k[a]) and an index for the next state. To re-
trieve a single label from a set of lattices representing a
speech corpus one simply retrieves all arcs in each lattice
from the label index. The lattices are first normalized by
weight pushing (Mohri et al, 2002) so that the probabil-
ity of the set of all paths leading from the arc to the final
state is 1. After weight pushing, for a given arc a, the
probability of the set of all paths containing that arc is
given by
p(a) =
?
pi?L:a?pi
p(pi) = f(k[a])p(a|k[a])
namely the probability of all paths leading into that arc,
multiplied by the probability of the arc itself. For a lattice
L we construct a ?count? C(l|L) for a given label l using
the information stored in the index I(l) as follows,
C(l|L) =
?
pi?L
p(pi)C(l|pi)
=
?
pi?L
(
p(pi)
?
a?pi
?(a, l)
)
=
?
a?L
(
?(a, l)
?
pi?L:a?pi
p(pi)
)
=
?
a?I(l):L[a]=L
p(a)
=
?
a?I(l):L[a]=L
f(k[a])p(a|k[a])
where C(l|pi) is the number of times l is seen on path pi
and ?(a, l) is 1 if arc a has the label l and 0 otherwise. Re-
trieval can be thresholded so that matches below a certain
count are not returned.
To search a multilabel expression (e.g. a multi-
word phrase) w1w2 . . . wn we seek on each label in
the expression, and then for each (wi, wi+1) join the
output states of wi with the matching input states of
wi+1; in this way we retrieve just those path seg-
ments in each lattice that match the entire multi-label
expression. The probability of each match is de-
fined as f(k[a1])p(a1|k[a1])p(a2|k[a2]) . . . p(an|k[an]),
where p(ai|k[ai]) is the probability of the ith arc in the
expression starting in arc a1. The total ?count? for the
lattice is computed as defined above.
Note that in the limit case where each lattice is an un-
weighted single path ? i.e. a string of labels ? the above
scheme reduces to a standard inverted index.
The count C(l|L) can be interpreted as a lattice-based
confidence measure. Although it may be possible to use
more sophisticated confidence measures, use of (poste-
rior) probabilities allows for a simple factorization which
makes indexing efficient.
3.3 Indexing Using Sub-word Units
In order to deal with queries that contain OOV words we
investigate the use of sub-word units for indexing. In this
study we use phones as the sub-word units. There are two
methods for obtaining phonetic representation of an input
utterance.
1. Phone recognition using an ASR system where
recognition units are phones. This is achieved by
using a phone level language model instead of the
word level language model used in the baseline ASR
system.
2. Converting the word level representation of the ut-
terance into a phone level representation. This is
achieved by using the baseline ASR system and re-
placing each word in the output by its pronuncia-
tion(s) in terms of phones.
Both methods have their shortcomings. Phone recogni-
tion is known to be less accurate than word recognition.
On the other hand, the second method can only generate
phone strings that are substrings of the pronunciations of
in-vocabulary word strings. An alternative is to use hy-
brid language models used for OOV word detection (Yaz-
gan and Saraclar, 2004).
For retrieval, each query word is converted into phone
string(s) by using its pronunciation(s). The phone index
can then be searched for each phone string. Note that this
approach will generate many false alarms, particularly for
short query words, which are likely to be substrings of
longer words. In order to control for this a bound on min-
imum pronunciation length can be utilized. Since most
short words are in vocabulary this bound has little effect
on recall.
3.4 Using Both Word and Sub-word Indices
Given a word index and a sub-word index, it is possible to
improve the retrieval performance of the system by using
both indices. There are many strategies for doing this.
1. combination:
Search both the word index and the sub-word index,
combine the results.
2. vocabulary cascade:
Search the word index for in-vocabulary queries,
search the sub-word index for OOV queries.
3. search cascade:
Search the word index,
if no result is returned search the sub-word index.
In the first case, if the indices are obtained from ASR
best hypotheses, then the result combination is a simple
union of the separate sets of results. However, if indices
are obtained from lattices, then in addition to taking a
union of results, retrieval can be done using a combined
score. Given a query q, let Cw(q) and Cp(q) be the lattice
counts obtained from the word index and the phone index
respectively. We also define the normalized lattice count
for the phone index as
Cnormp (q) = (Cp(q))
1
|pron(q)|
where |pron(q)| is the length of the pronunciation of
query q. We then define the combined score to be
Cwp(q) = Cw(q) + ?C
norm
p (q)
where ? is an empirically determined scaling factor.
In the other cases, instead of using two different thresh-
olds we use a single threshold on Cw(q) and Cnormp (q)
during retrieval.
4 Experiments
4.1 Evaluation Metrics
For evaluating ASR performance we use the standard
word error rate (WER) as our metric. Since we are in-
terested in retrieval we use OOV rate by type to measure
the OOV word characteristics. For evaluating retrieval
performance we use precision and recall with respect to
manual transcriptions. Let Correct(q) be the number of
times the query q is found correctly, Answer(q) be the
number of answers to the query q, and Reference(q) be
the number of times q is found in the reference.
Precision(q) =
Correct(q)
Answer(q)
Recall(q) =
Correct(q)
Reference(q)
We compute precision and recall rates for each query and
report the average over all queries. The set of queries Q
consists of all the words seen in the reference except for
a stoplist of 100 most common words. The measurement
is not weighted by frequency ? i.e. each query q ? Q
is presented to the system only once, independent of the
number of occurences of q in the transcriptions.
Precision =
1
|Q|
?
q?Q
Precision(q)
Recall =
1
|Q|
?
q?Q
Recall(q)
For lattice based retrieval methods, different operating
points can be obtained by changing the threshold. The
precision and recall at these operating points can be plot-
ted as a curve.
In addition to individual precision-recall values we
also compute the F-measure defined as
F =
2? Precision? Recall
Precision + Recall
and report the maximum F-measure (maxF) to summa-
rize the information in a precision-recall curve.
4.2 Corpora
We use three different corpora to assess the effectiveness
of different retrieval techniques.
The first corpus is the DARPA Broadcast News cor-
pus consisting of excerpts from TV or radio programs
including various acoustic conditions. The test set is
the 1998 Hub-4 Broadcast News (hub4e98) evaluation
test set (available from LDC, Catalog no. LDC2000S86)
which is 3 hours long and was manually segmented into
940 segments. It contains 32411 word tokens and 4885
word types. For ASR we use a real-time system (Saraclar
et al, 2002). Since the system was designed for SDR,
the recognition vocabulary of the system has over 200K
words. The pronunciation dictionary has 1.25 pronuncia-
tions per word.
The second corpus is the Switchboard corpus consist-
ing of two party telephone conversations. The test set is
the RT02 evaluation test set which is 5 hours long, has
120 conversation sides and was manually segmented into
6266 segments. It contains 65255 word tokens and 3788
word types. For ASR we use the first pass of the evalua-
tion system (Ljolje et al, 2002). The recognition vocab-
ulary of the system has over 45K words. For these words
the average number of pronunciations per word is 1.07.
The third corpus is named Teleconferences since it con-
sists of multiparty teleconferences on various topics. The
audio from the legs of the conference are summed and
recorded as a single channel. A test set of six telecon-
ferences (about 3.5 hours) was transcribed. It contains
31106 word tokens and 2779 word types. Calls are auto-
matically segmented into a total of 1157 segments prior
to ASR, using an algorithm that detects changes in the
acoustics. We again use the first pass of the Switchboard
evaluation system for ASR.
In Table 1 we present the ASR performance on these
three tasks as well as the OOV Rate by type of the cor-
pora. It is important to note that the recognition vocab-
ulary for the Switchboard and Teleconferences tasks are
the same and no data from the Teleconferences task was
used while building the ASR systems. The mismatch be-
tween the Teleconference data and the models trained on
the Switchboard corpus contributes to the significant in-
crease in WER.
4.3 Using ASR Best Word Hypotheses
As a baseline, we use the best word hypotheses of the
ASR system for indexing and retrieval. The performance
Task WER OOV Rate by Type
Broadcast News ?20% 0.6%
Switchboard ?40% 6%
Teleconferences ?50% 12%
Table 1: Word Error Rate (WER) and OOV Rate (by
type) of various LVCSR tasks
of this baseline system is given in Table 2. As ex-
pected, we obtain very good performance on the Broad-
cast News corpus. It is interesting to note that when mov-
ing from Switchboard to Teleconferences the degradation
in precision-recall is the same as the degradation in WER.
Task WER Precision Recall
Broadcast News ?20% 92% 77%
Switchboard ?40% 74% 47%
Teleconferences ?50% 65% 37%
Table 2: Precision Recall for ASR 1-best
4.4 Using ASR Word Lattices
In the second set of experiments we investigate the use
of ASR word lattices. In order to reduce storage require-
ments, lattices can be pruned to contain only the paths
whose costs (i.e. negative log likelihood) are within a
threshold with respect to the best path. The smaller this
cost threshold is, the smaller the lattices and the index
files are. In Figure 1 we present the precision-recall
curves for different pruning thresholds on the Telecon-
ferences task.
0 20 40 60 80 1000
20
40
60
80
100 Precision vs Recall on Teleconferences
Precision
Reca
ll
1?best word hypothesisword latticesword lattices (prune=6)word lattices (prune=4)word lattices (prune=2)
Figure 1: Precision Recall using word lattices for tele-
conferences
In Table 3 the resulting index sizes and maximum F-
measure values are given. On the teleconferences task we
observed that cost=6 yields good results, and used this
value for the rest of the experiments. Note that this in-
creases the index size with respect to the ASR 1-best case
by 3 times for Broadcast News, by 5 times for Switch-
board and by 9 times for Teleconferences.
Task Pruning Size (MB) maxF
Broadcast News nbest=1 29 84.0
Broadcast News cost=6 91 84.8
Switchboard nbest=1 18 57.1
Switchboard cost=6 90 58.4
Teleconferences nbest=1 16 47.4
Teleconferences cost=2 29 49.5
Teleconferences cost=4 62 50.0
Teleconferences cost=6 142 50.3
Teleconferences cost=12 3100 50.1
Table 3: Comparison of index sizes
4.5 Using ASR Phone Lattices
Next, we compare using the two methods of phonetic
transcription discussed in Section 3.3 ? phone recogni-
tion and word-to-phone conversion ? for retrieval using
only phone lattices. In Table 4 the precision and recall
values that yield the maximum F-measure as well as the
maximum F-measure values are presented. These results
clearly indicate that phone recognition is inferior for our
purposes.
Source for Indexing Precision Recall maxF
Phone Recognition 25.6 37.3 30.4
Conversion from Words 43.1 48.5 45.6
Table 4: Comparison of different sources for the phone
index on the Teleconferences corpus
4.6 Using ASR Word and Phone Lattices
We investigated using the strategies mentioned in Sec-
tion 3.4, and found strategy 3 ? search the word index, if
no result is returned search the phone index ? to be su-
perior to others. We give a comparison of the maximum
F-values for the three strategies in Table 5.
Strategy maxF
1.combination 50.5
2.vocabulary cascade 51.0
3.search cascade 52.8
Table 5: Comparison of different strategies for using
word and phone indices
In Figure 2 we present results for this strategy on the
Teleconferences corpus. The phone indices used in these
experiments were obtained by converting the word lat-
tices into phone lattices. Using the phone indices ob-
tained by phone recognition gave significantly worse re-
sults.
0 20 40 60 80 1000
20
40
60
80
100 Precision vs Recall on Teleconferences
Precision
Reca
ll
1?best word hypothesisword latticesword and phone lattices
Figure 2: Comparison of word lattices and word/phone
hybrid strategies for teleconferences
4.7 Effect of Minimum Pronunciation Length for
Queries
When searching for words with short pronunciations in
the phone index the system will produce many false
alarms. One way of reducing the number of false alarms
is to disallow queries with short pronunciations. In Fig-
ure 3 we show the effect of imposing a minimum pronun-
ciation length for queries. For a query to be answered its
pronunciation has to have more than minphone phones,
otherwise no answers are returned. Best maximum F-
measure result is obtained using minphone=3.
4.8 Effects of Recognition Vocabulary Size
In Figure 4 we present results for different recognition
vocabulary sizes (5k, 20k, 45k) on the Switchboard cor-
pus. The OOV rates by type are 32%, 10% and 6% re-
spectively. The word error rates are 41.5%, 40.1% and
40.1% respectively. The precision recall curves are al-
most the same for 20k and 45k vocabulary sizes.
4.9 Using Word Pair Queries
So far, in all the experiments the query list consisted of
single words. In order to observe the behavior of various
methods when faced with longer queries we used a set of
0 20 40 60 80 1000
20
40
60
80
100 Effect of Minimum Pronunciation Length
Precision
Reca
ll
1?best word hypothesisword latticesminphone=0minphone=3minphone=5
Figure 3: Effect of minimum pronunciation length using
a word/phone hybrid strategy for teleconferences
word pair queries. Instead of using all the word pairs seen
in the reference transcriptions, we chose the ones which
were more likely to occur together than with other words.
For this, we sorted the word pairs (w1, w2) according to
their pointwise mutual information
log
p(w1, w2)
p(w1)p(w2)
and used the top pairs as queries in our experiments. Note
that in these experiments only the query set is changed
and the indices remain the same as before.
As it turns out, the precision of the system is very high
on this type of queries. For this reason, it is more in-
teresting to look at the operating point that achieves the
maximum F-measure for each technique, which in this
case coincides with the point that yields the highest re-
call. In Table 6 we present results on the Switchboard
corpus using 1004 word pair queries. Using word lat-
tices it is possible to increase the recall of the system by
16.4% while degrading the precision by only 2.2%. Us-
ing phone lattices we can get another 3.7% increase in
recall for 1.2% loss in precision. The final system still
has 95% precision.
System Precision Recall maxF
Word 1-best 98.3 29.7 45.6
Word lattices 96.1 46.1 62.3
Word+Phone lattices 94.9 49.8 65.4
Table 6: Results for word pair queries on Switchboard
0 20 40 60 80 1000
20
40
60
80
100 Effect of Recognition Vocabulary Size
Precision
Reca
ll
word (45k)word (20k)word (5k)word+phone (45k)word+phone (20k)word+phone (5k)
Figure 4: Comparison of various recognition vocabulary
sizes for Switchboard
4.10 Summary of Results on Different Corpora
Finally, we make a comparison of various techniques on
different tasks. In Table 7 maximum F-measure (maxF)
is given. Using word lattices yields a relative gain of 3-
5% in maxF over using best word hypotheses. For the
final system that uses both word and phone lattices, the
relative gain over the baseline increases to 8-12%.
Task System
1-best W Lats W+P Lats
Broadcast News 84.0 84.8 86.0
Switchboard 57.1 58.4 60.5
Teleconferences 47.4 50.3 52.8
Table 7: Maximum F-measure for various systems and
tasks
In Figure 5 we present the precision recall curves.
The gain from using better techniques utilizing word
and phone lattices increases as retrieval performance gets
worse.
5 Conclusion
We proposed an indexing procedure for spoken utter-
ance retrieval that works on ASR lattices rather than just
single-best text. We demonstrated that this procedure can
improve maximum F-measure by over five points com-
pared to single-best retrieval on tasks with poor WER
and low redundancy. The representation is flexible so
that we can represent both word lattices, as well as phone
lattices, the latter being important for improving per-
formance when searching for phrases containing OOV
0 20 40 60 80 1000
20
40
60
80
100 Precision vs Recall Comparison
Precision
Reca
ll
Teleconferences
SwitchboardBroadcast News
Figure 5: Precision Recall for various techniques on dif-
ferent tasks. The tasks are Broadcast News (+), Switch-
board (x), and Teleconferences (o). The techniques are
using best word hypotheses (single points), using word
lattices (solid lines), and using word and phone lattices
(dashed lines).
words. It is important to note that spoken utterance re-
trieval for conversational speech has different properties
than spoken document retrieval for broadcast news. Al-
though consistent improvements were observed on a va-
riety of tasks including Broadcast News, the procedure
proposed here is most beneficial for more difficult con-
versational speech tasks like Switchboard and Telecon-
ferences.
References
A. Amir, A. Efrat, and S. Srinivasan. 2001. Advances
in phonetic word spotting. In Proceedings of the Tenth
International Conference on Information and Knowl-
edge Management, pages 580?582, Atlanta, Georgia,
USA.
M. G. Brown, J. T. Foote, G. J. F. Jones, K. Sparck Jones,
and S. J. Young. 1996. Open-vocabulary speech in-
dexing for voice and video mail retrieval. In Proc.
ACM Multimedia 96, pages 307?316, Boston, Novem-
ber.
R. V. Cox, B. Haskell, Y. LeCun, B. Shahraray, and L. Ra-
biner. 1998. On the application of multimedia pro-
cessing to telecommunications. Proceedings of the
IEEE, 86(5):755?824, May.
J. Garofolo, G. Auzanne, and E. Voorhees. 2000. The
TREC spoken document retrieval track: A success
story. In Proceedings of the Recherche d?Informations
Assiste par Ordinateur: Content Based Multimedia In-
formation Access Conference.
J. Hirschberg, M. Bacchiani, D. Hindle, P. Isenhour,
A. Rosenberg, L. Stark, L. Stead, S. Whittaker, and
G. Zamchick. 2001. Scanmail: Browsing and search-
ing speech data by content. In Proceedings of the
European Conference on Speech Communication and
Technology (Eurospeech), Aalborg, Denmark.
David Anthony James. 1995. The Application of Classi-
cal Information Retrieval Techniques to Spoken Docu-
ments. Ph.D. thesis, University of Cambridge, Down-
ing College.
G. J. F. Jones, J. T. Foote, K. Sparck Jones, and S. J.
Young. 1996. Retrieving spoken documents by com-
bining multiple index sources. In Proc. SIGIR 96,
pages 30?38, Zu?rich, August.
A. Ljolje, M. Saraclar, M. Bacchiani, M. Collins, and
B. Roark. 2002. The AT&T RT-02 STT system. In
Proc. RT02 Workshop, Vienna, Virginia.
B. Logan and JM Van Thong. 2002. Confusion-based
query expansion for OOV words in spoken document
retrieval. In Proceedings of the International Confer-
ence on Spoken Language Processing (ICSLP), Den-
ver, Colorado, USA.
B. Logan, P. Moreno, and O. Deshmukh. 2002. Word
and sub-word indexing approaches for reducing the ef-
fects of OOV queries on spoken audio. In Proc. HLT.
J. Makhoul, F. Kubala, T. Leek, D. Liu, L. Nguyen,
R. Schwartz, and A. Srivastava. 2000. Speech and
language technologies for audio indexing and retrieval.
Proceedings of the IEEE, 88(8):1338?1353, August.
M. Mohri, F. Pereira, and M. Riley. 2002. Weighted
finite-state transducers in speech recognition. Com-
puter Speech and Language, 16(1):69?88.
Kenney Ng. 2000. Subword-Based Approaches for Spo-
ken Document Retrieval. Ph.D. thesis, Massachusetts
Institute of Technology.
NIST TREC-9 SDR Web Site. 2000.
www.nist.gov/speech/tests/sdr/sdr2000/sdr2000.htm.
M. Saraclar, M. Riley, E. Bocchieri, and V. Goffin. 2002.
Towards automatic closed captioning: Low latency
real time broadcast news transcription. In Proceedings
of the International Conference on Spoken Language
Processing (ICSLP), Denver, Colorado, USA.
Matthew A. Siegler. 1999. Integration of Continuous
Speech Recognition and Information Retrieval for Mu-
tually Optimal Performance. Ph.D. thesis, Carnegie
Mellon University.
S. Srinivasan and D. Petkovic. 2000. Phonetic confu-
sion matrix based spoken document retrieval. In Pro-
ceedings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 81?87.
M. Wechsler, E. Munteanu, and P. Sca?uble. 1998. New
techniques for open-vocabulary spoken document re-
trieval. In Proceedings of the 21st Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 20?27, Mel-
bourne, Australia.
Martin Wechsler. 1998. Spoken Document Retrieval
Based on Phoneme Recognition. Ph.D. thesis, Swiss
Federal Institute of Technology (ETH), Zurich.
M. Witbrock and A. Hauptmann. 1997. Using words and
phonetic strings for efficient information retrieval from
imperfectly transcribed spoken documents. In 2nd
ACM International Conference on Digital Libraries
(DL?97), pages 30?35, Philadelphia, PA, July.
P.C. Woodland, S.E. Johnson, P. Jourlin, and K.Sparck
Jones. 2000. Effects of out of vocabulary words
in spoken document retrieval. In Proc. SIGIR, pages
372?374, Athens, Greece.
A. Yazgan and M. Saraclar. 2004. Hybrid language mod-
els for out of vocabulary word detection in large vocab-
ulary conversational speech recognition. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), Mon-
treal, Canada.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 73?80,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Named Entity Transliteration with Comparable Corpora
Richard Sproat, Tao Tao, ChengXiang Zhai
University of Illinois at Urbana-Champaign, Urbana, IL, 61801
rws@uiuc.edu, {taotao,czhai}@cs.uiuc.edu
Abstract
In this paper we investigate Chinese-
English name transliteration using compa-
rable corpora, corpora where texts in the
two languages deal in some of the same
topics ? and therefore share references
to named entities ? but are not transla-
tions of each other. We present two dis-
tinct methods for transliteration, one ap-
proach using phonetic transliteration, and
the second using the temporal distribu-
tion of candidate pairs. Each of these ap-
proaches works quite well, but by com-
bining the approaches one can achieve
even better results. We then propose a
novel score propagation method that uti-
lizes the co-occurrence of transliteration
pairs within document pairs. This prop-
agation method achieves further improve-
ment over the best results from the previ-
ous step.
1 Introduction
As part of a more general project on multilin-
gual named entity identification, we are interested
in the problem of name transliteration across lan-
guages that use different scripts. One particular is-
sue is the discovery of named entities in ?compara-
ble? texts in multiple languages, where by compa-
rable we mean texts that are about the same topic,
but are not in general translations of each other.
For example, if one were to go through an English,
Chinese and Arabic newspaper on the same day,
it is likely that the more important international
events in various topics such as politics, business,
science and sports, would each be covered in each
of the newspapers. Names of the same persons,
locations and so forth ? which are often translit-
erated rather than translated ? would be found in
comparable stories across the three papers.1 We
wish to use this expectation to leverage translit-
eration, and thus the identification of named enti-
ties across languages. Our idea is that the occur-
rence of a cluster of names in, say, an English text,
should be useful if we find a cluster of what looks
like the same names in a Chinese or Arabic text.
An example of what we are referring to can be
found in Figure 1. These are fragments of two
stories from the June 8, 2001 Xinhua English and
Chinese newswires, each covering an international
women?s badminton championship. Though these
two stories are from the same newswire source,
and cover the same event, they are not translations
of each other. Still, not surprisingly, a lot of the
names that occur in one, also occur in the other.
Thus (Camilla) Martin shows up in the Chinese
version as ??? ma-er-ting; Judith Meulendijks
is ??????? yu mo-lun-di-ke-si; and Mette
Sorensen is ?????mai su-lun-sen. Several
other correspondences also occur. While some of
the transliterations are ?standard? ? thus Martin
is conventionally transliterated as ??? ma-er-
ting ? many of them were clearly more novel,
though all of them follow the standard Chinese
conventions for transliterating foreign names.
These sample documents illustrate an important
point: if a document in language L1 has a set of
names, and one finds a document in L2 containing
a set of names that look as if they could be translit-
erations of the names in the L1 document, then
this should boost one?s confidence that the two sets
of names are indeed transliterations of each other.
We will demonstrate that this intuition is correct.
1Many names, particularly of organizations, may be trans-
lated rather than transliterated; the transliteration method we
discuss here obviously will not account for such cases, though
the time correlation and propagation methods we discuss will
still be useful.
73
Dai Yun Nips World No. 1 Martin to Shake off Olympic
Shadow . . . In the day?s other matches, second seed Zhou Mi
overwhelmed Ling Wan Ting of Hong Kong, China 11-4, 11-
4, Zhang Ning defeat Judith Meulendijks of Netherlands 11-
2, 11-9 and third seed Gong Ruina took 21 minutes to elimi-
nate Tine Rasmussen of Denmark 11-1, 11-1, enabling China
to claim five quarterfinal places in the women?s singles.
? ? ? ? ? ?  ? ? ? ? ? ? ? ? ? ? ?
. . . ??????,????????4???,?? . . .
? ? ? ? ? ? ? ? ? ? ? ? 11:1? ? ? ? ? ?
?? ????,??????11:2?11:9????? ?
???????,??????11:4?11:1? ???
??????
Figure 1: Sample from two stories about an inter-
national women?s badminton championship.
2 Previous Work
In previous work on Chinese named-entity
transliteration ? e.g. (Meng et al, 2001; Gao
et al, 2004), the problem has been cast as the
problem of producing, for a given Chinese name,
an English equivalent such as one might need in
a machine translation system. For example, for
the name ??????wei wei-lian-mu-si, one
would like to arrive at the English name V(enus)
Williams. Common approaches include source-
channel methods, following (Knight and Graehl,
1998) or maximum-entropy models.
Comparable corpora have been studied exten-
sively in the literature (e.g.,(Fung, 1995; Rapp,
1995; Tanaka and Iwasaki, 1996; Franz et al,
1998; Ballesteros and Croft, 1998; Masuichi et al,
2000; Sadat et al, 2003)), but transliteration in the
context of comparable corpora has not been well
addressed.
The general idea of exploiting frequency corre-
lations to acquire word translations from compara-
ble corpora has been explored in several previous
studies (e.g., (Fung, 1995; Rapp, 1995; Tanaka
and Iwasaki, 1996)).Recently, a method based on
Pearson correlation was proposed to mine word
pairs from comparable corpora (Tao and Zhai,
2005), an idea similar to the method used in (Kay
and Roscheisen, 1993) for sentence alignment. In
our work, we adopt the method proposed in (Tao
and Zhai, 2005) and apply it to the problem of
transliteration. We also study several variations of
the similarity measures.
Mining transliterations from multilingual web
pages was studied in (Zhang and Vines, 2004);
Our work differs from this work in that we use
comparable corpora (in particular, news data) and
leverage the time correlation information naturally
available in comparable corpora.
3 Chinese Transliteration with
Comparable Corpora
We assume that we have comparable corpora, con-
sisting of newspaper articles in English and Chi-
nese from the same day, or almost the same day. In
our experiments we use data from the English and
Chinese stories from the Xinhua News agency for
about 6 months of 2001.2 We assume that we have
identified names for persons and locations?two
types that have a strong tendency to be translit-
erated wholly or mostly phonetically?in the En-
glish text; in this work we use the named-entity
recognizer described in (Li et al, 2004), which
is based on the SNoW machine learning toolkit
(Carlson et al, 1999).
To perform the transliteration task, we propose
the following general three-step approach:
1. Given an English name, identify candi-
date Chinese character n-grams as possible
transliterations.
2. Score each candidate based on how likely the
candidate is to be a transliteration of the En-
glish name. We propose two different scoring
methods. The first involves phonetic scoring,
and the second uses the frequency profile of
the candidate pair over time. We will show
that each of these approaches works quite
well, but by combining the approaches one
can achieve even better results.
3. Propagate scores of all the candidate translit-
eration pairs globally based on their co-
occurrences in document pairs in the compa-
rable corpora.
The intuition behind the third step is the following.
Suppose several high-confidence name transliter-
ation pairs occur in a pair of English and Chi-
nese documents. Intuitively, this would increase
our confidence in the other plausible translitera-
tion pairs in the same document pair. We thus pro-
pose a score propagation method to allow these
high-confidence pairs to propagate some of their
2Available from the LDC via the English Gigaword
(LDC2003T05) and Chinese Gigaword (LDC2003T09) cor-
pora.
74
scores to other co-occurring transliteration pairs.
As we will show later, such a propagation strat-
egy can generally further improve the translitera-
tion accuracy; in particular, it can further improve
the already high performance from combining the
two scoring methods.
3.1 Candidate Selection
The English named entity candidate selection pro-
cess was already described above. Candidate Chi-
nese transliterations are generated by consulting
a list of characters that are frequently used for
transliterating foreign names. As discussed else-
where (Sproat et al, 1996), a subset of a few hun-
dred characters (out of several thousand) tends to
be used overwhelmingly for transliterating foreign
names into Chinese. We use a list of 495 such
characters, derived from various online dictionar-
ies. A sequence of three or more characters from
the list is taken as a possible name. If the character
??? occurs, which is frequently used to represent
the space between parts of an English name, then
at least one character to the left and right of this
character will be collected, even if the character in
question is not in the list of ?foreign? characters.
Armed with the English and Chinese candidate
lists, we then consider the pairing of every En-
glish candidate with every Chinese candidate. Ob-
viously it would be impractical to do this for all of
the candidates generated for, say, an entire year:
we consider as plausible pairings those candidates
that occur within a day of each other in the two
corpora.
3.2 Candidate scoring based on
pronunciation
We adopt a source-channel model for scoring
English-Chinese transliteration pairs. In general,
we seek to estimate P (e|c), where e is a word in
Roman script, and c is a word in Chinese script.
Since Chinese transliteration is mostly based on
pronunciation, we estimate P (e?|c?), where e? is
the pronunciation of e and c? is the pronunciation
of c. Again following standard practice, we de-
compose the estimate of P (e?|c?) as P (e?|c?) =
?
i P (e?i|c?i). Here, e?i is the ith subsequence of
the English phone string, and c?i is the ith subse-
quence of the Chinese phone string. Since Chi-
nese transliteration attempts to match the syllable-
sized characters to equivalent sounding spans of
the English language, we fix the c?i to be syllables,
and let the e?i range over all possible subsequences
of the English phone string. For training data we
have a small list of 721 names in Roman script and
their Chinese equivalent.3 Pronunciations for En-
glish words are obtained using the Festival text-to-
speech system (Taylor et al, 1998); for Chinese,
we use the standard pinyin transliteration of the
characters. English-Chinese pairs in our training
dictionary were aligned using the alignment algo-
rithm from (Kruskal, 1999), and a hand-derived
set of 21 rules-of-thumb: for example, we have
rules that encode the fact that Chinese /l/ can cor-
respond to English /r/, /n/ or /er/; and that Chinese
/w/ may be used to represent /v/. Given that there
are over 400 syllables in Mandarin (not count-
ing tone) and each of these syllables can match
a large number of potential English phone spans,
this is clearly not enough training data to cover all
the parameters, and so we use Good-Turing esti-
mation to estimate probabilities for unseen corre-
spondences. Since we would like to filter implau-
sible transliteration pairs we are less lenient than
standard estimation techniques in that we are will-
ing to assign zero probability to some correspon-
dences. Thus we set a hard rule that for an En-
glish phone span to correspond to a Chinese sylla-
ble, the initial phone of the English span must have
been seen in the training data as corresponding to
the initial of the Chinese syllable some minimum
number of times. For consonant-initial syllables
we set the minimum to 4. We omit further details
of our estimation technique for lack of space. This
phonetic correspondence model can then be used
to score putative transliteration pairs.
3.3 Candidate Scoring based on Frequency
Correlation
Names of the same entity that occur in different
languages often have correlated frequency patterns
due to common triggers such as a major event.
Thus if we have comparable news articles over a
sufficiently long time period, it is possible to ex-
ploit such correlations to learn the associations of
names in different languages. The idea of exploit-
ing frequency correlation has been well studied.
(See the previous work section.) We adopt the
method proposed in (Tao and Zhai, 2005), which
3The LDC provides a much larger list of transliterated
Chinese-English names, but we did not use this here for two
reasons. First, we have found it it be quite noisy. Secondly,
we were interested in seeing how well one could do with a
limited resource of just a few hundred names, which is a more
realistic scenario for languages that have fewer resources than
English and Chinese.
75
works as follows: We pool all documents in a sin-
gle day to form a large pseudo-document. Then,
for each transliteration candidate (both Chinese
and English), we compute its frequency in each
of those pseudo-documents and obtain a raw fre-
quency vector. We further normalize the raw fre-
quency vector so that it becomes a frequency dis-
tribution over all the time points (days). In order
to compute the similarity between two distribution
vectors, The Pearson correlation coefficient was
used in (Tao and Zhai, 2005); here we also consid-
ered two other commonly used measures ? cosine
(Salton and McGill, 1983), and Jensen-Shannon
divergence (Lin, 1991), though our results show
that Pearson correlation coefficient performs bet-
ter than these two other methods.
3.4 Score Propagation
In both scoring methods described above, scoring
of each candidate transliteration pair is indepen-
dent of the other. As we have noted, document
pairs that contain lots of plausible transliteration
pairs should be viewed as more plausible docu-
ment pairs; at the same time, in such a situation we
should also trust the putative transliteration pairs
more. Thus these document pairs and translitera-
tion pairs mutually ?reinforce? each other, and this
can be exploited to further optimize our translit-
eration scores by allowing transliteration pairs to
propagate their scores to each other according to
their co-occurrence strengths.
Formally, suppose the current generation of
transliteration scores are (ei, ci, wi) i = 1, ..., n,
where (ei, ci) is a distinct pair of English and Chi-
nese names. Note that although for any i 6= j, we
have (ei, ci) 6= (ej , cj), it is possible that ei = ej
or ci = cj for some i 6= j. wi is the transliteration
score of (ei, ci).
These pairs along with their co-occurrence re-
lation computed based on our comparable cor-
pora can be formally represented by a graph as
shown in Figure 2. In such a graph, a node repre-
sents (ei, ci, wi). An edge between (ei, ci, wi) and
(ej , cj , wj) is constructed iff (ei, ci) and (ej , cj)
co-occur in a certain document pair (Et, Ct), i.e.
there exists a document pair (Et, Ct), such that
ei, ej ? Et and ci, cj ? Ct. Given a node
(ei, ci, wi), we refer to all its directly-connected
nodes as its ?neighbors?. The documents do not
appear explicitly in the graph, but they implicitly
affect the graph?s topology and the weight of each
edge. Our idea of score propagation can now be
formulated as the following recursive equation for
w1
w4
w2
w3
w5
w6
w7
(e4, c4)
(e3, c3)
(e5, c5)
(e5, c5)
(e2, c2)
(e7, c7)
(e6, c6)
Figure 2: Graph representing transliteration pairs
and cooccurence relations.
updating the scores of all the transliteration pairs.
w(k)i = ?? w
(k?1)
i + (1 ? ?) ?
n
?
j 6=i,j=1
(w(k?1)j ? P (j|i)),
where w(k)i is the new score of the pair (ei, ci)
after an iteration, while w(k?1)i is its old score
before updating; ? ? [0, 1] is a parameter to
control the overall amount of propagation (when
? = 1, no propagation occurs); P (j|i) is the con-
ditional probability of propagating a score from
node (ej , cj , wj) to node (ei, ci, wi).
We estimate P (j|i) in two different ways: 1)
The number of cooccurrences in the whole collec-
tion (Denote as CO). P (j|i) = C(i,j)?
j? C(i,j?)
, where
C(i, j) is the cooccurrence count of (ei, ci) and
(ej , cj); 2) A mutual information-based method
(Denote as MI). P (j|i) = MI(i,j)?
j? MI(i,j?)
, where
MI(i, j) is the mutual information of (ei, ci) and
(ej , cj). As we will show, the CO method works
better. Note that the transition probabilities be-
tween indirect neighbors are always 0. Thus prop-
agation only happens between direct neighbors.
This formulation is very similar to PageRank,
a link-based ranking algorithm for Web retrieval
(Brin and Page, 1998). However, our motivation
is propagating scores to exploit cooccurrences, so
we do not necessarily want the equation to con-
verge. Indeed, our results show that although the
initial iterations always help improve accuracy, too
many iterations actually would decrease the per-
formance.
4 Evaluation
We use a comparable English-Chinese corpus to
evaluate our methods for Chinese transliteration.
We take one day?s worth of comparable news arti-
cles (234 Chinese stories and 322 English stories),
generate about 600 English names with the entity
recognizer (Li et al, 2004) as described above, and
76
find potential Chinese transliterations also as pre-
viously described. We generated 627 Chinese can-
didates. In principle, all these 600 ? 627 pairs are
potential transliterations. We then apply the pho-
netic and time correlation methods to score and
rank all the candidate Chinese-English correspon-
dences.
To evaluate the proposed transliteration meth-
ods quantitatively, we measure the accuracy of the
ranked list by Mean Reciprocal Rank (MRR), a
measure commonly used in information retrieval
when there is precisely one correct answer (Kan-
tor and Voorhees, 2000). The reciprocal rank is
the reciprocal of the rank of the correct answer.
For example, if the correct answer is ranked as the
first, the reciprocal rank would be 1.0, whereas if
it is ranked the second, it would be 0.5, and so
forth. To evaluate the results for a set of English
names, we take the mean of the reciprocal rank of
each English name.
We attempted to create a complete set of an-
swers for all the English names in our test set,
but a small number of English names do not seem
to have any standard transliteration according to
the resources that we consulted. We ended up
with a list of about 490 out of the 600 English
names judged. We further notice that some an-
swers (about 20%) are not in our Chinese candi-
date set. This could be due to two reasons: (1) The
answer does not occur in the Chinese news articles
we look at. (2) The answer is there, but our candi-
date generation method has missed it. In order to
see more clearly how accurate each method is for
ranking the candidates, we also compute the MRR
for the subset of English names whose transliter-
ation answers are in our candidate list. We dis-
tinguish the MRRs computed on these two sets of
English names as ?AllMRR? and ?CoreMRR?.
Below we first discuss the results of each of the
two methods. We then compare the two methods
and discuss results from combining the two meth-
ods.
4.1 Phonetic Correspondence
We show sample results for the phonetic scoring
method in Table 1. This table shows the 10 high-
est scoring transliterations for each Chinese char-
acter sequence based on all texts in the Chinese
and English Xinhua newswire for the 13th of Au-
gust, 2001. 8 out of these 10 are correct. For all
the English names the MRR is 0.3, and for the
?paris ??? pei-lei-si 3.51
iraq ??? yi-la-ke 3.74
staub ??? si-ta-bo 4.45
canada ?? jia-na-da 4.85
belfast ????? bei-er-fa-si-te 4.90
fischer ??? fei-she-er 4.91
philippine ??? fei-lu?-bin 4.97
lesotho ?? lai-suo-two 5.12
?tirana ??? tye-lu-na 5.15
freeman ??? fu-li-man 5.26
Table 1: Ten highest-scoring matches for the Xin-
hua corpus for 8/13/01. The final column is the
?log P estimate for the transliteration. Starred
entries are incorrect.
core names it is 0.89. Thus on average, the cor-
rect answer, if it is included in our candidate list,
is ranked mostly as the first one.
4.2 Frequency correlation
Similarity AllMRR CoreMRR
Pearson 0.1360 0.3643
Cosine 0.1141 0.3015
JS-div 0.0785 0.2016
Table 2: MRRs of the frequency correlation meth-
ods.
We proposed three similarity measures for the
frequency correlation method, i.e., the Cosine,
Pearson coefficient, and Jensen-Shannon diver-
gence. In Table 2, we show their MRRs. Given
that the only resource the method needs is compa-
rable text documents over a sufficiently long pe-
riod, these results are quite encouraging. For ex-
ample, with Pearson correlation, when the Chinese
transliteration of an English name is included in
our candidate list, the correct answer is, on aver-
age, ranked at the 3rd place or better. The results
thus show that the idea of exploiting frequency
correlation does work. We also see that among
the three similarity measures, Pearson correlation
performs the best; it performs better than Cosine,
which is better than JS-divergence.
Compared with the phonetic correspondence
method, the performance of the frequency correla-
tion method is in general much worse, which is not
surprising, given the fact that terms may be corre-
lated merely because they are topically related.
77
4.3 Combination of phonetic correspondence
and frequency correlation
Method AllMRR CoreMRR
Phonetic 0.2999 0.8895
Freq 0.1360 0.3643
Freq+PhoneticFilter 0.3062 0.9083
Freq+PhoneticScore 0.3194 0.9474
Table 3: Effectiveness of combining the two scor-
ing methods.
Since the two methods exploit complementary
resources, it is natural to see if we can improve
performance by combining the two methods. In-
deed, intuitively the best candidate is the one that
has a good pronunciation alignment as well as a
correlated frequency distribution with the English
name. We evaluated two strategies for combining
the two methods. The first strategy is to use the
phonetic model to filter out (clearly impossible)
candidates and then use the frequency correlation
method to rank the candidates. The second is to
combine the scores of these two methods. Since
the correlation coefficient has a maximum value
of 1, we normalize the phonetic correspondence
score by dividing all scores by the maximum score
so that the maximum normalized value is also 1.
We then take the average of the two scores and
rank the candidates based on their average scores.
Note that the second strategy implies the applica-
tion of the first strategy.
The results of these two combination strategies
are shown in Table 3 along with the results of the
two individual methods. We see that both com-
bination strategies are effective and the MRRs of
the combined results are all better than those of the
two individual methods. It is interesting to see that
the benefit of applying the phonetic correspon-
dence model as a filter is quite significant. Indeed,
although the performance of the frequency corre-
lation method alone is much worse than that of the
phonetic correspondence method, when working
on the subset of candidates passing the phonetic
filter (i.e., those candidates that have a reasonable
phonetic alignment with the English name), it can
outperform the phonetic correspondence method.
This once again indicates that exploiting the fre-
quency correlation can be effective. When com-
bining the scores of these two methods, we not
only (implicitly) apply the phonetic filter, but also
exploit the discriminative power provided by the
phonetic correspondence scores and this is shown
to bring in additional benefit, giving the best per-
formance among all the methods.
4.4 Error Analysis
From the results above, we see that the MRRs for
the core English names are substantially higher
than those for all the English names. This means
that our methods perform very well whenever we
have the answer in our candidate list, but we have
also missed the answers for many English names.
The missing of an answer in the candidate list is
thus a major source of errors. To further under-
stand the upper bound of our method, we manu-
ally add the missing correct answers to our can-
didate set and apply all the methods to rank this
augmented set of candidates. The performance is
reported in Table 4 with the corresponding perfor-
mance on the original candidate set. We see that,
Method ALLMRR
Original Augmented
Phonetic 0.2999 0.7157
Freq 0.1360 0.3455
Freq+PhoneticFilter 0.3062 0.6232
Freq+PhoneticScore 0.3194 0.7338
Table 4: MRRs on the augmented candidate list.
as expected, the performance on the augmented
candidate list, which can be interpreted as an up-
per bound of our method, is indeed much better,
suggesting that if we can somehow improve the
candidate generation method to include the an-
swers in the list, we can expect to significantly im-
prove the performance for all the methods. This
is clearly an interesting topic for further research.
The relative performance of different methods on
this augmented candidate list is roughly the same
as on the original candidate list, except that the
?Freq+PhoneticFilter? is slightly worse than that
of the phonetic method alone, though it is still
much better than the performance of the frequency
correlation alone. One possible explanation may
be that since these names do not necessarily oc-
cur in our comparable corpora, we may not have
sufficient frequency observations for some of the
names.
78
Method AllMRR CoreMRR
init. CO MI init. CO MI
Freq+PhoneticFilter 0.3171 0.3255 0.3255 0.9058 0.9372 0.9372
Freq+PhoneticScore 0.3290 0.3373 0.3392 0.9422 0.9659 0.9573
Table 5: Effectiveness of score propagation.
4.5 Experiments on score propagation
To demonstrate that score propagation can further
help transliteration, we use the combination scores
in Table 3 as the initial scores, and apply our prop-
agation algorithm to iteratively update them. We
remove the entries when they do not co-occur with
others. There are 25 such English name candi-
dates. Thus, the initial scores are actually slightly
different from the values in Table 3. We show
the new scores and the best propagation scores in
Table 5. In the table, ?init.? refers to the initial
scores. and ?CO? and ?MI? stand for best scores
obtained using either the co-occurrence or mutual
information method. While both methods result
in gains, CO very slightly outperforms the MI ap-
proach. In the score propagation process, we in-
troduce two additional parameters: the interpola-
tion parameter ? and the number of iterations k.
Figure 3 and Figure 4 show the effects of these
parameters. Intuitively, we want to preserve the
initial score of a pair, but add a slight boost from
its neighbors. Thus, we set ? very close to 1 (0.9
and 0.95), and allow the system to perform 20 it-
erations. In both figures, the first few iterations
certainly leverage the transliteration, demonstrat-
ing that the propagation method works. However,
we observe that the performance drops when more
iterations are used, presumably due to noise intro-
duced from more distantly connected nodes. Thus,
a relatively conservative approach is to choose a
high ? value, and run only a few iterations. Note,
finally, that the CO method seems to be more sta-
ble than the MI method.
5 Conclusions and Future Work
In this paper we have discussed the problem of
Chinese-English name transliteration as one com-
ponent of a system to find matching names in com-
parable corpora. We have proposed two methods
for transliteration, one that is more traditional and
based on phonetic correspondences, and one that
is based on word distributions and adopts meth-
ods from information retrieval. We have shown
 0.76
 0.78
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0.98
 0  2  4  6  8  10  12  14  16  18  20
M
RR
 v
al
ue
s
number of iterations
alpha=0.9, MI
alpha=0.9, CO
alpha=0.95, MI
alpha=0.95, CO
Figure 3: Propagation: Core items
that both methods yield good results, and that even
better results can be achieved by combining the
methods. We have further showed that one can
improve upon the combined model by using rein-
forcement via score propagation when translitera-
tion pairs cluster together in document pairs.
The work we report is ongoing. We are inves-
tigating transliterations among several language
pairs, and are extending these methods to Ko-
rean, Arabic, Russian and Hindi ? see (Tao et al,
2006).
6 Acknowledgments
This work was funded by Dept. of the Interior con-
tract NBCHC040176 (REFLEX). We also thank
three anonymous reviewers for ACL06.
References
Lisa Ballesteros and W. Bruce Croft. 1998. Resolv-
ing ambiguity for cross-language retrieval. In Re-
search and Development in Information Retrieval,
pages 64?71.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks and ISDN Systems, 30:107?
117.
79
 0.28
 0.29
 0.3
 0.31
 0.32
 0.33
 0.34
 0  2  4  6  8  10  12  14  16  18  20
M
RR
 v
al
ue
s
number of iterations
alpha=0.9, MI
alpha=0.9, CO
alpha=0.95, MI
alpha=0.95, CO
Figure 4: Propagation: All items
A. Carlson, C. Cumby, J. Rosen, and D. Roth. 1999.
The SNoW learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC CS Dept.
Martin Franz, J. Scott McCarley, and Salim Roukos.
1998. Ad hoc and multilingual information retrieval
at IBM. In Text REtrieval Conference, pages 104?
115.
Pascale Fung. 1995. A pattern matching method
for finding noun and proper noun translations from
noisy parallel corpora. In Proceedings of ACL 1995,
pages 236?243.
W. Gao, K.-F. Wong, and W. Lam. 2004. Phoneme-
based transliteration of foreign names for OOV
problem. In IJCNLP, pages 374?381, Sanya,
Hainan.
P. Kantor and E. Voorhees. 2000. The TREC-5 confu-
sion track: Comparing retrieval methods for scanned
text. Information Retrieval, 2:165?176.
M. Kay and M. Roscheisen. 1993. Text translation
alignment. Computational Linguistics, 19(1):75?
102.
K. Knight and J. Graehl. 1998. Machine translitera-
tion. CL, 24(4).
J. Kruskal. 1999. An overview of sequence compar-
ison. In D. Sankoff and J. Kruskal, editors, Time
Warps, String Edits, and Macromolecules, chapter 1,
pages 1?44. CSLI, 2nd edition.
X. Li, P. Morie, and D. Roth. 2004. Robust reading:
Identification and tracing of ambiguous names. In
NAACL-2004.
J. Lin. 1991. Divergence measures based on the shan-
non entropy. IEEE Transactions on Information
Theory, 37(1):145?151.
H. Masuichi, R. Flournoy, S. Kaufmann, and S. Peters.
2000. A bootstrapping method for extracting bilin-
gual text pairs.
H.M. Meng, W.K Lo, B. Chen, and K. Tang. 2001.
Generating phonetic cognates to handle named enti-
ties in English-Chinese cross-languge spoken doc-
ument retrieval. In Proceedings of the Automatic
Speech Recognition and Understanding Workshop.
R. Rapp. 1995. Identifying word translations in non-
parallel texts. In Proceedings of ACL 1995, pages
320?322.
Fatiha Sadat, Masatoshi Yoshikawa, and Shunsuke Ue-
mura. 2003. Bilingual terminology acquisition from
comparable corpora and phrasal translation to cross-
language information retrieval. In ACL ?03, pages
141?144.
G. Salton and M. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill.
R. Sproat, C. Shih, W. Gale, and N. Chang. 1996. A
stochastic finite-state word-segmentation algorithm
for Chinese. CL, 22(3).
K. Tanaka and H. Iwasaki. 1996. Extraction of lexical
translation from non-aligned corpora. In Proceed-
ings of COLING 1996.
Tao Tao and ChengXiang Zhai. 2005. Mining compa-
rable bilingual text corpora for cross-language infor-
mation integration. In KDD?05, pages 691?696.
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard
Sproat, and ChengXiang Zhai. 2006. Unsupervised
named entity transliteration using temporal and pho-
netic correlation. In EMNLP 2006, Sydney, July.
P. Taylor, A. Black, and R. Caley. 1998. The archi-
tecture of the Festival speech synthesis system. In
Proceedings of the Third ESCA Workshop on Speech
Synthesis, pages 147?151, Jenolan Caves, Australia.
Ying Zhang and Phil Vines. 2004. Using the web for
automated translation extraction in cross-language
information retrieval. In SIGIR ?04, pages 162?169.
80
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 112?119,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Multilingual Transliteration Using Feature based Phonetic Method  
Su-Youn Yoon, Kyoung-Young Kim and Richard Sproat  
University of Illinois at Urbana-Champaign 
{syoon9,kkim36,rws}@uiuc.edu 
 
 
Abstract 
In this paper we investigate named entity 
transliteration based on a phonetic scoring 
method. The phonetic method is computed 
using phonetic features and carefully 
designed pseudo features. The proposed 
method is tested with four languages ? 
Arabic, Chinese, Hindi and Korean ? and 
one source language ? English, using 
comparable corpora. The proposed method 
is developed from the phonetic method 
originally proposed in Tao et al (2006). In 
contrast to the phonetic method in Tao et al 
(2006) constructed on the basis of pure 
linguistic knowledge, the method in this 
study is trained using the Winnow machine 
learning algorithm. There is salient 
improvement in Hindi and Arabic 
compared to the previous study. Moreover, 
we demonstrate that the method can also 
achieve comparable results, when it is 
trained on language data different from the 
target language. The method can be applied 
both with minimal data, and without target 
language data for various languages.  
1 Introduction. 
In this paper, we develop a multi-lingual 
transliteration system for named entities. Named 
entity transliteration is the process of producing, 
for a name in a source language, a set of one or 
more transliteration candidates in a target language. 
The correct transliteration of named entities is 
crucial, since they are frequent and important key 
words in information retrieval. In addition, 
requests in retrieving relevant documents in 
multiple languages require the development of the 
multi-lingual system.  
The system is constructed using paired 
comparable texts. The comparable texts are about 
the same or related topics, but are not, in general, 
translations of each other. Using this data, the 
transliteration method aims to find transliteration 
correspondences in the paired languages. For 
example, if there were an English and Arabic 
newspaper on the same day, each of the 
newspapers would contain articles about the same 
important international events. From these 
comparable articles across the paired languages, 
the same named entities are expected to be found. 
Thus, from the named entities in an English 
newspaper, the method would find transliteration 
correspondences in comparable texts in other 
languages. 
The multi-lingual transliteration system entails 
solving several problems which are very 
challenging. First, it should show stable 
performance for many unrelated languages. The 
transliteration will be influenced by the difference 
in the phonological systems of the language pairs, 
and the process of transliteration differs according 
to the languages involved. For example, in Arabic 
texts, short vowels are rarely written while long 
vowels are written. When transliterating English 
names, the vowels are disappeared or written as 
long vowels. For example London is transliterated 
as lndn ?????????, and both vowels are not represented 
in the transliteration. However, Washington is 
often transliterated as  wSnjTwn ?????? ????????????? , and 
the final vowel is realized with long vowel. 
Transliterations in Chinese are very different from 
the original English pronunciation due to the 
112
limited syllable structure and phoneme inventory 
of Chinese. For example, Chinese does not allow 
consonant clusters or coda consonants except [n,N], 
and this results in deletion, substitution of 
consonants or insertion of vowels. Thus while a 
syllable initial /d/ may surface as in Baghdad 
??? ba-ge-da, note that the syllable final /d/ is 
not represented. Multi-lingual transliteration 
system should solve these language dependent 
characteristics.  
One of the most important concerns in a 
multilingual transliteration system is its 
applicability given a small amount of training data, 
or even no training data: for arbitrary language 
pairs, one cannot in general assume resources such 
as name dictionaries. Indeed, for some rarely 
spoken languages, it is practically impossible to 
find enough training data. Therefore, the proposed 
method aims to obtain comparable performance 
with little training data.  
2 Previous Work 
Previous work ? e.g. (Knight and Graehl, 1998; 
Meng et al, 2001; Al-Onaizan and Knight, 2002; 
Gao et al, 2004) ? has mostly assumed that one 
has a training lexicon of transliteration pairs, from 
which one can learn a model, often a source-
channel or MaxEnt-based model. 
Comparable corpora have been studied 
extensively in the literature, but transliteration in 
the context of comparable corpora has not been 
well addressed. In our work, we adopt the method 
proposed in (Tao et al, 2006) and apply it to the 
problem of transliteration. 
Measuring phonetic similarity between words 
has been studied for a long time. In many studies, 
two strings are aligned using a string alignment 
algorithm, and an edit distance (the sum of the cost 
for each edit operation), is used as the phonetic 
distance between them. The resulting distance 
depends on the costs of the edit operation. There 
are several approaches that use distinctive features 
to determine the costs of the edit operation. Gildea 
and Jurafsky (1996) counted the number of 
features whose values are different, and used them 
as a substitution cost. However, this approach has a 
crucial limitation: the cost does not consider the 
importance of the features. Nerbonne and Heeringa 
(1997) assigned a weight for each feature based on 
entropy and information gain, but the results were 
even less accurate than the method without weight. 
3 Phonetic transliteration method 
In this paper, the phonetic transliteration is 
performed using the following steps:  
1) Generation of the pronunciation for 
English words and target words: 
a. Pronunciations for English words are obtained 
using the Festival text-to-speech system (Taylor et 
al., 1998).  
b. Target words are automatically converted into 
their phonemic level transcriptions by various 
language-dependent means. In the case of 
Mandarin Chinese, this is based on the standard 
Pinyin transliteration system. Arabic words are 
converted based on orthography, and the resulting 
transcriptions are reasonably correct except for the 
fact that short vowels were not represented. 
Similarly, the pronunciation of Hindi and Korean 
can be well-approximated based on the standard 
orthographic representation. All pronunciations are 
based on the WorldBet transliteration system 
(Hieronymus, 1995), an ascii-only version of the 
IPA. 
2) Training a linear classifier using the 
Winnow algorithm: 
A linear classifier is trained using the training 
data which is composed of transliteration pairs and 
non-transliteration pairs. Transliteration pairs are 
extracted from the transliteration dictionary, while 
non-transliteration pairs are composed of an 
English named entity and a random word from the 
target language newspaper.  
a. For all the training data, the pairs of 
pronunciations are aligned using standard string 
alignment algorithm based on Kruskal (1999). The 
substitution/insertion/deletion cost for the string 
alignment algorithm is based on the baseline cost 
from (Tao et al 2006). 
b. All phonemes in the pronunciations are 
decomposed into their features. The features used 
in this study will be explained in detail in part 3.1.  
c. For every phoneme pair (p1, p2) in the aligned 
pronunciations, a feature xi has a ?+1? value or a ??
1? value: 
 
xi =   +1   when p1 and p2  have the same 
values for feature xi 
?1   otherwise 
113
d. A linear classifier is trained using the 
Winnow algorithm from the SNoW toolkit 
(Carlson et al, 1999).  
 
3) Scoring English-target word pair: 
a. For a given English word, the score between it 
and a target word is computed using the linear 
classifier. 
b. The score ranges from 0 to any positive 
number, and the candidate with the highest score is 
selected as the transliteration of the given English 
name.  
 
3.1  Feature set 
Halle and Clements (1983)?s distinctive features 
are used in order to model the substitution/ 
insertion/deletion costs for the string-alignment 
algorithm and linear classifier. A distinctive 
feature is a feature that describes the phonetic 
characteristics of phonetic segments. 
However, distinctive features alone are not 
enough to model the frequent sound change 
patterns that occur when words are adapted across 
languages. For example, stop and fricative 
consonants such as /p, t, k, b, d, g, s, z/ are 
frequently deleted when they appear in the coda 
position. This tendency is extremely salient when 
the target languages do not allow coda consonants 
or consonant clusters. For example, since Chinese 
only allows /n, N/ in coda position, stop consonants 
in the coda position are frequently lost; Stanford is 
transliterated as sitanfu, with the final /d/ lost. 
Since traditional distinctive features do not 
consider the position in the syllable, this pattern 
cannot be captured by distinctive features alone. 
To capture these sound change patterns, additional 
features such as ?deletion of stop/fricative 
consonant in the coda position? must be considered.  
Based on the pronunciation error data of learners 
of English as a second language as reported in 
(Swan and Smith, 2002), we propose the use of 
what we will term pseudofeatures. The pseudo 
features in this study are same as in Tao et al 
(2006). Swan & Smith (2002)?s study covers 25 
languages including Asian languages such as Thai, 
Korean, Chinese and Japanese, European 
languages such as German, Italian, French and 
Polish, and Middle East languages such as Arabic 
and Farsi. The substitution/insertion/deletion errors 
of phonemes were collected from this data. The 
following types of errors frequently occur in 
second language learners? speech production.  
(1) Substitution: If the learner?s first language 
does not have a particular phoneme found in 
English, it is substituted by the most similar 
phoneme in their first language. 
(2) Insertion: If the learner?s first language does 
not have a particular consonant cluster in English, 
a vowel is inserted. 
(3) Deletion: If the learner?s first language does 
not have a particular consonant cluster in English, 
one consonant in the consonant cluster is deleted. 
The same substitution/deletion/insertion patterns 
in a second language learner?s errors also appear in 
the transliteration of foreign names. The deletion 
of the stop consonant which appears in English-
Chinese transliterations occurs frequently in the 
English pronunciation spoken by Chinese speakers. 
Therefore, the error patterns in second language 
learners? can be used in transliteration. 
Based on (1) ~ (3), 21 pseudo features were 
designed. All features have binary values. Using 
these 21 pseudo features and 20 distinctive features, 
a linear classifier is trained. Some examples of 
pseudo features are presented in Table 1.  
 
Pseudo-  
Feature Description Example 
Consonant-
coda 
Substitution 
of consonant 
feature in 
coda position 
 
Sonorant-
coda 
Substitution 
of sonorant 
feature in 
coda position 
Substitution 
between [N] and 
[g] in coda 
position in Arabic
Labial-coda
Substitution 
of labial 
feature in 
coda position 
Substitution 
between [m] and 
[n] in coda 
position in Chinese
j-exception
Substitution 
of [j] and [dZ] 
Spanish/Catalan 
and Festival error
w-exception Substitution of [v] and [w] 
Chinese/Farsi and 
Festival error 
Table 1. Examples of pseudo features  
 
114
3.2 Scoring the English-target word pair  
A linear classifier is trained using the Winnow 
algorithm from the SNoW toolkit.  
The Winnow algorithm is one of the update 
rules for linear classifier. A linear classifier is an 
algorithm to find a linear function that best 
separates the data. For the set of features X and set 
of weights W, the linear classifier is defined as [1] 
(Mitchell, T., 1997) 
1 2
1 2
0 1 1 2 2
  { , ,  ... }
  { , , ... } 
( )   1        ...    0   
             -1  
n
n
n n
X x x x
W w w w
f x if w wx w x w x
otherwise
=
=
= + + + + >
[1] 
 
The linear function assigns label +1 when the 
paired target language word is the transliteration of 
given English word, while it assigns label ?1 when 
it is not a transliteration of given English word.  
The score of an English word and target word 
pair is computed using equation [2] which is part 
of the definition of f(x) in equation [1]. 
0
1
n
i i
i
w w x
=
+?   [2] 
The output of equation [2] is termed the target 
node activation. If this value is high, class 1 is 
more activated, and the pair is more likely to be a 
transliteration pair. To illustrate, let us assume 
there are two candidates in target language (t1 and 
t2) for an English word e. If the score of (e, t1) is 
higher than the score of (e, t2), the pair (e, t1) has 
stronger activation than (e, t2). It means that t1  
scores higher as the transliteration of e than t2. 
Therefore, the candidate with the highest score (in 
this case t1) is selected as the transliteration of the 
given English name. 
4 Experiment and Results 
The linear function was trained for each 
language, separately. 500 transliteration pairs were 
randomly selected from each transliteration 
dictionary, and used as positive examples in the 
training procedure. This is quite small compared to 
previous approaches such as Knight and Graehl 
(1998) or Gao et al (2004). In addition, 1500 
words were randomly selected from the newspaper 
in the target languages, and paired with English 
words in the positive examples. A total of 750,000 
pairs (500 English words? 1500 target words) were 
generated, and used as negative examples in the 
training procedure. 
Table 2 presents the source of training data for 
each language.  
 
 Transliteration pair Target word 
Arabic New Mexico State University 
Xinhua Arabic 
newswire 
Chinese Behavior Design Corporation 
Xinhua  
Chinese  
newswire 
Hindi Naidunia Hindi newswire  
Naidunia Hindi 
newswire 
Korean
the National  
Institute of the 
Korean language 
Chosun  
Korean  
newspaper 
Table 2. Sources of the training data 
The phonetic transliteration method was 
evaluated using comparable corpora, consisting of 
newspaper articles in English and the target 
languages?Arabic, Chinese, Hindi, and Korean?
from the same day, or almost the same day. Using 
comparable corpora, the named-entities for persons 
and locations were extracted from the English text; 
in this paper, the English named-entities were 
extracted using the named-entity recognizer 
described in Li et al (2004), based on the SNoW 
machine learning toolkit (Carlson et al, 1999).  
The transliteration task was performed using the 
following steps:  
1) English text was tagged using the named-
entity recognizer. The 200 most frequent named 
entities were extracted from seven days? worth of 
the English newswire text. Among pronunciations 
of words generated by the Festival text-to speech 
system, 3% contained errors representing 
monophthongs instead of diphthongs or vice versa. 
1.5% of all cases misrepresented single consonant, 
and 6% showed errors in the vowels. Overall, 
10.5% of the tokens contained pronunciation errors 
which could trigger errors in transliteration. 
2) To generate the Arabic and Hindi candidates, 
all words from the same seven days were extracted. 
In the case of Korean corpus, the collection of 
newspapers was from every five days, unlike the 
other three language corpora which were collected 
every day; therefore, candidates of Korean were 
115
generated from one month of newspapers, since 
seven days of newspaper articles did not show a 
sufficient number of transliteration candidates. 
This caused the total number of candidates to be 
much bigger than for the other languages.  
The words were stemmed all possible ways 
using simple hand-developed affix lists: for 
example, given a Hindi word c1c2c3, if both c3 
and c2c3 are in the suffix and ending list, then this 
single word generated three possible candidates: c1, 
c1c2, and c1c2c3.  
3) Segmenting Chinese sentences requires a 
dictionary or supervised segmenter. Since the goal 
is to use minimal knowledge or data from the 
target language, using supervised methods is 
inappropriate for our approach. Therefore, Chinese 
sentences were not segmented. Using the 495 
characters that are frequently used for 
transliterating foreign names (Sproat et al, 1996), 
a sequence of three of more characters from the list 
was taken as a possible candidate for Chinese. 
4) For the given 200 English named entities and 
target language candidate lists, all the possible 
pairings of English and target-language name were 
considered as possible transliteration pairs.  
The number of candidates for each target 
language is presented in Table 3. 
 
Language The number of candidates 
Arabic 12,466 
Chinese 6,291 
Hindi 10,169 
Korean 42,757 
Table 3. Number of candidates for each target 
language. 
5) Node activation scores were calculated for 
each pair in the test data, and the candidates were 
ranked by their score. The candidate with the 
highest node activation score was selected as the 
transliteration of the given English name.  
Some examples of English words and the top 
three ranking candidates among all of the potential 
target-language candidates were given in Tables 4, 
5. Starred entries are correct. 
 
Candidate English 
Word Rank Script Romanization 
Arafat 
*1 
2 
3 
????
????
??? 
a-la-fa-te 
la-fa-di-ao
la-wei-qi 
Table 4. Examples of the top-3 candidates in the 
transliteration of English ? Chinese 
Candidate English 
Word Rank
Script Romanization 
*1 ??? be-thu-nam
2 ???? be-thu-nam-chug Vietnam 
3 ???? pyo-jun-e-wa 
*1 ??????? 
o-su-thu-
ley-il-li-a 
2 ??? us-tol-la Australia
3 ????????? 
o-su-thu-
ley-il-li-a-
ey-se 
Table 5. Examples of the top-3 candidates in the 
transliteration of English-Korean 
To evaluate the proposed transliteration methods 
quantitatively, the Mean Reciprocal Rank (MRR), 
a measure commonly used in information retrieval 
when there is precisely one correct answer (Kandor 
and Vorhees, 2000) was measured, following Tao 
and Zhai (2005).  
 
Since the evaluation data obtained from the 
comparable corpus was small, the systems were 
evaluated using both held-out data from the 
transliteration dictionary and comparable corpus.  
 
First, the results of the held-out data will be 
presented. For a given English name and target 
language candidates, all possible combinations 
were generated. Table 6 presents the size of held-
out data, and Table 7 presents MRR of the held-out 
data.  
 
116
 Number 
of English 
named 
entities 
Number of 
Candidates 
in target 
language 
Number of 
total pairs 
used in the 
evaluation
Arabic 500 1,500 750,000 
Chinese 500 1,500 750,000 
Hindi 100 1,500 150,000 
Korean 100 1,500 150,000 
Table 6. Size of the test data 
Winnow 
 Baseline  Total 
feature 
distinctive 
feature 
only 
Arabic 0.66 0.74 0.70 
Chinese 0.74 0.74 0.72 
Hindi 0.87 0.91 0.91 
Korean 0.82 0.85 0.82 
Table 7. MRRs of the phonetic transliteration 
The baseline was computed using the phonetic 
transliteration method proposed in Tao et al 
(2006). In contrast to the method in this study, the 
baseline system is purely based on linguistic 
knowledge. In the baseline system, the edit 
distance, which was the result of the string 
alignment algorithm, was used as the score of an 
English-target word pair. The performance of the 
edit distance was dependent on insertion/deletion/ 
substitution costs. These costs were determined 
based on the distinctive features and pseudo 
features, based on the pure linguistic knowledge 
without training data. As illustrated in Table 7, the 
phonetic transliteration method using features 
worked adequately for multilingual data, as 
phonetic features are universal, unlike the 
phonemes which are composed of them. Adopting 
phonetic features as the units for transliteration 
yielded the baseline performance.  
In order to evaluate the effectiveness of pseudo 
features, the method was trained using two 
different feature sets: a total feature set and a 
distinctive feature-only set. For Arabic, Chinese 
and Korean, the MRR of the total feature set was 
higher than the MRR of the distinctive feature-only 
set. The improvement of the total set was 4% for 
Arabic, 2.6% for Chinese, 2.4% for Korean. There 
was no improvement of the total set in Hindi. In 
general, the pseudo features improved the accuracy 
of the transliteration. 
For all languages, the MRR of the Winnow 
algorithm with the total feature set was higher than 
the baseline. There was 7% improvement for 
Arabic, 0.7% improvement for Chinese, 4% 
improvement for Hindi and 3% improvement for 
Korean.  
 
We turn now to the results on comparable 
corpora. We attempted to create a complete set of 
answers for the 200 English names in our test set, 
but part of the English names did not seem to have 
any standard transliteration in the target language 
according to the native speaker?s judgment. 
Accordingly, we removed these names from the 
evaluation set. Thus, the resulting list was less than 
200 English names, as shown in the second column 
of Table 8; (Table 8 All). Furthermore, some 
correct transliterations were not found in our 
candidate list for the target languages, since the 
answer never occurred in the target news articles; 
(Table 8 Missing). Thus this results in a smaller 
number of candidates to evaluate. This smaller 
number is given in the fourth column of Table 8; 
(Table 8 Core).  
 
Language # All # Missing #Core 
Arabic 192 121 71 
Chinese 186 92 94 
Hindi 144 83 61 
Korean 195 114 81 
Table 8. Number of evaluated English Name 
 
MRRs were computed on the two sets 
represented by the count in column 2, and the 
smaller set represented by the count in column 4. 
We termed the former MRR ?AllMRR? and the 
latter ?CoreMRR?. In Table 9, ?CoreMRR? and 
?AllMRR? of the method were presented.  
 
117
Baseline  Winnow  
 All-
MRR 
Core
MRR 
All-
MRR 
Core
MRR
Arabic 0.20 0.53 0.22 0.61
Chinese 0.25 0.49 0.25 0.50
Hindi 0.30 0.69 0.36 0.86
Korean 0.30 0.71 0.29 0.69
Table 9. MRRs of the phonetic transliteration 
In both methods, CoreMRRs were higher than 
0.49 for all languages. That is, if the answer is in 
the target language texts, then the method finds the 
correct answer within the top 2 words.  
As with the previously discussed results, there 
were salient improvements in Arabic and Hindi 
when using the Winnow algorithm. The MRRs of 
the Winnow algorithm except Korean were higher 
than the baseline. There was 7% improvement for 
Arabic and 17% improvement for Hindi in 
CoreMRR. In contrast to the 3% improvement in 
held-out data, there was a 2% decrease in Korean: 
the MRRs of Korean from the Winnow algorithm 
were lower than baseline, possibly because of the 
limited size of the evaluation data. Similar to the 
results of held-out data, the improvement in 
Chinese was small (1%).  
The MRRs of Hindi and the MRRs of Korean 
were higher than the MRRs of Arabic and Chinese. 
The lower MRRs of Arabic and Chinese may result 
from the phonological structures of the languages. 
In general, transliteration of English word into 
Arabic and Chinese is much more irregular than 
the transliteration into Hindi and Korean in terms 
of phonetics.  
 
To test the applicability to languages for which 
training data is not available, we also investigated 
the use of models trained on language pairs 
different from the target language pair. Thus, for 
each test language pair, we evaluated the 
performance of models trained on each of the other 
language pairs. For example, three models were 
trained using Chinese, Hindi, and Korean, and they 
were tested with Arabic data. The CoreMRRs of 
this experiment were presented in Table 10. Note 
that the diagonal in this Table represents the 
within-language-pair training and testing scenario 
that we reported on above. 
test data 
 
Arabic Chinese Hindi
Kore
an 
Arabic 0.61 0.50 0.86 0.63
Chinese 0.59 0.50 0.80 0.66
Hindi 0.59 0.54 0.86 0.67
train
-ing 
data
Korean 0.56 0.51 0.76 0.69
Table 10. MRRs for the phonetic transliteration 2  
For Arabic, Hindi, and Korean, MRRs were 
indeed the highest when the methods were trained 
using data from the same language, as indicated by 
the boldface MRR scores on the diagonal. In 
general, however, the MRRs were not saliently 
lower across the board when using different 
language data than using same-language data in 
training and testing. For all languages, MRRs for 
the cross-language case were best when the 
methods were trained using Hindi. The differences 
between MRRs of the method trained from Hindi 
and MRRs of the method by homogeneous 
language data were 2% for Arabic and Korean. In 
the case of Chinese, MRRs of the method trained 
by Hindi was actually better than MRRs obtained 
by Chinese training data. Hindi has a large 
phoneme inventory compared to Korean, Arabic, 
and Chinese, so the relationship between English 
phonemes and Hindi phonemes is relatively regular, 
and only small number of language specific 
transliteration rules exist. That is, the language 
specific influences from Hindi are smaller than 
those from other languages. This characteristic of 
Hindi may result in the high MRRs for other 
languages. What these results imply is that named 
entity transliteration could be performed without 
training data for the target language with phonetic 
feature as a unit. This approach is especially 
valuable for languages for which training data is 
minimal or lacking. 
 
5 Conclusion 
In this paper, a phonetic method for multilingual 
transliteration was proposed. The method was 
based on string alignment, and linear classifiers 
trained using the Winnow algorithm. In order to 
learn both language-universal and language-
specific transliteration characteristics, distinctive 
118
features and pseudo features were used in training. 
The method can be trained using a small amount of 
training data, and the performance decreases only 
by a small degree when it is trained with a 
language different from the test data. Therefore, 
this method is extremely useful for 
underrepresented languages for which training data 
is difficult to find. 
Acknowledgments 
This work was funded the National Security 
Agency contract NBCHC040176 (REFLEX) and a 
Google Research grant.  
 
References 
Y. Al-Onaizan and K. Knight. 2002. Machine 
transliteration of names in Arabic text. In 
Proceedings of the ACL Workshop on Computational 
Approaches to Semitic Languages, Philadelphia, PA. 
Andrew J. Carlson, Chad M. Cumby, Jeff L. Rosen, and 
Dan Roth. 1999. The SNoW learning architecture. 
Technical Report UIUCDCS-R-99-2101, UIUC CS 
Dept. 
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004. 
Phoneme based transliteration of foreign names for 
OOV problem. Proceeding of IJCNLP, 374?381. 
Daniel Gildea and Daniel Jurafsky. 1996. Learning Bias 
and Phonological-Rule Induction. Computational 
Linguistics 22(4):497?530. 
Morris Halle and G.N. Clements. 1983. Problem book 
in phonology. MIT press, Cambridge. 
James Hieronymus. 1995. Ascii phonetic symbols for 
the world?s languages: Worldbet. 
http://www.ling.ohio-tate.edu/ edwards/worldbet.pdf. 
Paul B. Kantor and Ellen B. Voorhees. 2000. The 
TREC-5 confusion track: Comparing retrieval 
methods for scanned text. Information Retrieval, 2: 
165?176. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
transliteration. Computational Linguistics, 24(4). 
Joseph B. Kruskal. 1999. An overview of sequence 
comparison. Time Warps, String Edits, and 
Macromolecules, CSLI, 2nd edition, 1?44. 
Xin Li, Paul Morie, and Dan Roth. 2004. Robust 
reading: Identification and tracing of ambiguous 
names. Proceeding of NAACL-2004. 
H.M. Meng, W.K Lo, B. Chen, and K. Tang. 2001. 
Generating phonetic cognates to handle named 
entities in English-Chinese cross-language spoken 
document retrieval. In Proceedings of the Automatic 
Speech Recognition and Understanding Workshop. 
Tom M. Mitchell. 1997. Machine Learning, McCraw-
Hill, Boston. 
John Nerbonne and Wilbert Heeringa. 1997. Measuring 
Dialect Distance Phonetically. Proceedings of the 3rd 
Meeting of the ACL Special Interest Group in 
Computational Phonology. 
Richard Sproat, Chilin. Shih, William A. Gale, and 
Nancy Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational 
Linguistics, 22(3).  
Michael Swan and Bernard Smith. 2002. Learner 
English, Cambridge University Press, Cambridge . 
Tao Tao and ChengXiang Zhai. 2005. Mining 
comparable bilingual text corpora for cross-language 
information integration. Proceeding of the eleventh 
ACM SIGKDD international conference on 
Knowledge discovery in data mining, 691?696. 
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard 
Sproat and ChengXiang Zhai. "Unsupervised Named 
Entity Transliteration Using Temporal and Phonetic 
Correlation." EMNLP, July 22-23, 2006, Sydney, 
Australia. 
Paul A. Taylor, Alan Black, and Richard Caley. 1998. 
The architecture of the Festival speech synthesis 
system. Proceedings of the Third ESCAWorkshop on 
SpeechSynthesis, 147?151. 
119
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 250?257,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Named Entity Transliteration Using Temporal and Phonetic
Correlation
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard Sproat and ChengXiang Zhai
University of Illinois at Urbana-Champaign
{syoon9,afister2,rws}@uiuc.edu, {taotao,czhai}@cs.uiuc.edu
Abstract
In this paper we investigate unsuper-
vised name transliteration using compara-
ble corpora, corpora where texts in the two
languages deal in some of the same top-
ics ? and therefore share references to
named entities ? but are not translations
of each other. We present two distinct
methods for transliteration, one approach
using an unsupervised phonetic translit-
eration method, and the other using the
temporal distribution of candidate pairs.
Each of these approaches works quite
well, but by combining the approaches
one can achieve even better results. We
believe that the novelty of our approach
lies in the phonetic-based scoring method,
which is based on a combination of care-
fully crafted phonetic features, and empiri-
cal results from the pronunciation errors of
second-language learners of English. Un-
like previous approaches to transliteration,
this method can in principle work with any
pair of languages in the absence of a train-
ing dictionary, provided one has an esti-
mate of the pronunciation of words in text.
1 Introduction
As a part of a on-going project on multilingual
named entity identification, we investigate unsu-
pervised methods for transliteration across lan-
guages that use different scripts. Starting from
paired comparable texts that are about the same
topic, but are not in general translations of each
other, we aim to find the transliteration correspon-
dences of the paired languages. For example, if
there were an English and Arabic newspaper on
the same day, each of the newspapers would likely
contain articles about the same important inter-
national events. From these comparable articles
across the two languages, the same named enti-
ties such as persons and locations would likely be
found. For at least some of the English named
entities, we would therefore expect to find Ara-
bic equivalents, many of which would in fact be
transliterations.
The characteristics of transliteration differ ac-
cording to the languages involved. In particular,
the exact transliteration of say, an English name
is highly dependent on the language since this will
be influenced by the difference in the phonological
systems of the language pairs. In order to show the
reliability of a multi-lingual transliteration model,
it should be tested with a variety of different lan-
guages. We have tested our transliteration meth-
ods with three unrelated target languages ? Ara-
bic, Chinese and Hindi, and a common source lan-
guage ? English. Transliteration from English to
Arabic and Chinese is complicated (Al-Onaizan
and Knight, 2002). For example, while Arabic or-
thography has a conventional way of writing long
vowels using selected consonant symbols ? ba-
sically <w>, <y> and <?>, in ordinary text
short vowels are rarely written. When transliter-
ating English names there is the option of repre-
senting the vowels as either short (i.e. unwrit-
ten) or long (i.e. written with one of the above
three mentioned consonant symbols). For exam-
ple London is transliterated as     lndn, with no
vowels; Washington often as  
	   wSnjTwn,
with <w> representing the final <o>. Transliter-
ations in Chinese are very different from the orig-
inal English pronunciation due to the limited syl-
lable structure and phoneme inventory of Chinese.
For example, Chinese does not allow consonant
clusters or coda consonants except [n, N], and this
results in deletion, substitution of consonants or
insertion of vowels. Thus while a syllable initial
/d/ may surface as in Baghdad  ba-ge-da,
note that the syllable final /d/ is not represented.
250
Hindi transliteration is not well-studied, but it is
in principle easier than Arabic and Chinese since
Hindi phonotactics is much more similar to that of
English.
2 Previous Work
Named entity transliteration is the problem of pro-
ducing, for a name in a source language, a set
of one or more transliteration candidates in a tar-
get language. Previous work ? e.g. (Knight and
Graehl, 1998; Meng et al, 2001; Al-Onaizan and
Knight, 2002; Gao et al, 2004) ? has mostly as-
sumed that one has a training lexicon of translit-
eration pairs, from which one can learn a model,
often a source-channel or MaxEnt-based model.
Comparable corpora have been studied exten-
sively in the literature ? e.g.,(Fung, 1995; Rapp,
1995; Tanaka and Iwasaki, 1996; Franz et al,
1998; Ballesteros and Croft, 1998; Masuichi et
al., 2000; Sadat et al, 2004), but transliteration
in the context of comparable corpora has not been
well addressed. The general idea of exploiting
time correlations to acquire word translations from
comparable corpora has been explored in several
previous studies ? e.g., (Fung, 1995; Rapp, 1995;
Tanaka and Iwasaki, 1996). Recently, a Pearson
correlation method was proposed to mine word
pairs from comparable corpora (Tao and Zhai,
2005); this idea is similar to the method used in
(Kay and Roscheisen, 1993) for sentence align-
ment. In our work, we adopt the method proposed
in (Tao and Zhai, 2005) and apply it to the problem
of transliteration; note that (Tao and Zhai, 2005)
compares several different metrics for time corre-
lation, as we also note below ? and see (Sproat et
al., 2006).
3 Transliteration with Comparable
Corpora
We start from comparable corpora, consisting of
newspaper articles in English and the target lan-
guages for the same time period. In this paper, the
target languages are Arabic, Chinese and Hindi.
We then extract named-entities in the English text
using the named-entity recognizer described in (Li
et al, 2004), which is based on the SNoW machine
learning toolkit (Carlson et al, 1999). To perform
transliteration, we use the following general ap-
proach: 1 Extract named entities from the English
corpus for each day; 2 Extract candidates from the
same day?s newspapers in the target language; 3
For each English named entity, score and rank the
target-language candidates as potential transliter-
ations. We apply two unsupervised methods ?
time correlation and pronunciation-based methods
? independently, and in combination.
3.1 Candidate scoring based on
pronunciation
Our phonetic transliteration score uses a standard
string-alignment and alignment-scoring technique
based on (Kruskal, 1999) in that the distance is de-
termined by a combination of substitution, inser-
tion and deletion costs. These costs are computed
from a language-universal cost matrix based on
phonological features and the degree of phonetic
similarity. (Our technique is thus similar to other
work on phonetic similarity such as (Frisch, 1996)
though details differ.) We construct a single cost
matrix, and apply it to English and all target lan-
guages. This technique requires the knowledge of
the phonetics and the sound change patterns of the
language, but it does not require a transliteration-
pair training dictionary. In this paper we assume
the WorldBet transliteration system (Hieronymus,
1995), an ASCII-only version of the IPA.
The cost matrix is constructed in the following
way. All phonemes are decomposed into stan-
dard phonological features. However, phonolog-
ical features alone are not enough to model the
possible substution/insertion/deletion patterns of
languages. For example, /h/ is more frequently
deleted than other consonants, whereas no single
phonological feature allows us to distinguish /h/
from other consonants. Similarly, stop and frica-
tive consonants such as /p, t, k, b, d, g, s, z/ are
frequently deleted when they appear in the coda
position. This tendency is very salient when the
target languages do not allow coda consonants or
consonant clusters. So, Chinese only allows [n,
N] in coda position, and stop consonants in coda
position are frequently lost; Stanford is translit-
erated as sitanfu, with the final /d/ lost. Since
phonological features do not consider the posi-
tion in the syllable, this pattern cannot be cap-
tured by conventional phonological features alone.
To capture this, an additional feature ?deletion
of stop/fricative consonant in the coda position?
is added. We base these observations, and the
concomitant pseudofeatures on pronunciation er-
ror data of learners of English as a second lan-
guage, as reported in (Swan and Smith, 2002). Er-
251
rors in second language pronunciation are deter-
mined by the difference in the phonological sys-
tem of learner?s first and second language. The
same substitution/deletion/insertion patterns in the
second language learner?s errors appear also in
the transliteration of foreign names. For exam-
ple, if the learner?s first language does not have
a particular phoneme found in English, it is sub-
stituted by the most similar phoneme in their first
language. Since Chinese does not have /v/, it is
frequently substituted by /w/ or /f/. This sub-
stitution occurs frequently in the transliteration
of foreign names in Chinese. Swan & Smith?s
study covers 25 languages, and includes Asian
languages such as Thai, Korean, Chinese and
Japanese, European languages such as German,
Italian, French, and Polish and Middle Eastern
languages such as Arabic and Farsi. Frequent sub-
stitution/insertion/deletion patterns of phonemes
are collected from these data. Some examples are
presented in Table 1.
Twenty phonological features and 14 pseud-
ofeatures are used for the construction of the cost
matrix. All features are classified into 5 classes.
There are 4 classes of consonantal features ?
place, manner, laryngeality and major (conso-
nant, sonorant, syllabicity), and a separate class
of vocalic features. The purpose of these classes
is to define groups of features which share the
same substitution/insertion/deletion costs. For-
mally, given a class C, and a cost CC , for each
feature f ? C, CC defines the cost of substitut-
ing a different value for f than the one present in
the source phoneme. Among manner features, the
feature continuous is classified separately, since
the substitution between stop and fricative con-
sonants is very frequent; but between, say, nasals
and fricatives such substitution is much less com-
mon. The cost for frequent sound change pat-
terns should be low. Based on our intuitions, our
pseudofeatures are classified into one or another
of the above-mentioned five classes. The substitu-
tion/deletion/insertion cost for a pair of phonemes
is the sum of the individual costs of the features
which are different between the two phonemes.
For example, /n/ and /p/ are different in sonorant,
labial and coronal features. Therefore, the substi-
tution cost of /n/ for /p/ is the sum of the sonorant,
labial and coronal cost (20+10+10 = 40). Features
and associated costs are shown in Table 2. Sam-
ple substitution, insertion, and deletion costs for
/g/ are presented in Table 3.
The resulting cost matrix based on these prin-
ciples is then used to calculate the edit distance
between two phonetic strings. Pronunciations for
English words are obtained using the Festival text-
to-speech system (Taylor et al, 1998), and the tar-
get language words are automatically converted
into their phonemic level transcriptions by various
language-dependent means. In the case of Man-
darin Chinese this is based on the standard pinyin
transliteration system. For Arabic this is based
on the orthography, which works reasonably well
given that (apart from the fact that short vowels
are no represented) the script is fairly phonemic.
Similarly, the pronunciation of Hindi can be rea-
sonably well-approximated based on the standard
Devanagari orthographic representation. The edit
cost for the pair of strings is normalized by the
number of phonemes. The resulting score ranges
from zero upwards; the score is used to rank can-
didate transliterations, with the candidate having
the lowest cost being considered the most likely
transliteration. Some examples of English words
and the top three ranking candidates among all of
the potential target-language candidates are given
in Table 4.1 Starred entries are correct.
3.2 Candidate scoring based on time
correlation
Names of the same entity that occur in different
languages often have correlated frequency patterns
due to common triggers such as a major event. For
example, the 2004 tsunami disaster was covered
in news articles in many different languages. We
would thus expect to see a peak of frequency of
names such as Sri Lanka, India, and Indonesia in
news articles published in multiple languages in
the same time period. In general, we may expect
topically related names in different languages to
tend to co-occur together over time. Thus if we
have comparable news articles over a sufficiently
long time period, it is possible to exploit such cor-
relations to learn the associations of names in dif-
ferent languages.
The idea of exploiting time correlation has been
well studied. We adopt the method proposed in
(Tao and Zhai, 2005) to represent the source name
and each name candidate with a frequency vector
and score each candidate by the similarity of the
1We describe candidate selection for each of the target
languages later.
252
Input Output Position
D D, d, z everywhere
T T, t, s everywhere
N N, n, g everywhere
p/t/k deletion coda
Table 1: Substitution/insertion/deletion patterns for phonemes based on English second-language
learner?s data reported in (Swan and Smith, 2002). Each row shows an input phoneme class, possi-
ble output phonemes (including null), and the positions where the substitution (or deletion) is likely to
occur.
Class Feature Cost
Major features and Consonant Del consonant 20
sonorant
consonant deletion
Place features and Vowel Del coronal 10
vowel del/ins
stop/fricative consonant del at coda position
h del/ins
Manner features nasal 5
dorsal feature for palatal consonants
Vowel features and Exceptions vowel height 3
vowel place
exceptional
Manner/ Laryngeal features continuous 1.5
voicing
Table 2: Examples of features and associated costs. Pseudofeatures are shown in boldface. Exceptional
denotes a situation such as the semivowel [j] substituting for the affricate [dZ]. Substitutions between
these two sounds actually occur frequently in second-language error data.
two frequency vectors. This is very similar to the
case in information retrieval where a query and a
document are often represented by a term vector
and documents are ranked by the similarity be-
tween their vectors and the query vector (Salton
and McGill, 1983). But the vectors are very dif-
ferent and should be constructed in quite differ-
ent ways. Following (Tao and Zhai, 2005), we
also normalize the raw frequency vector so that
it becomes a frequency distribution over all the
time points. In order to compute the similarity be-
tween two distribution vectors ~x = (x1, ..., xT )
and ~y = (y1, ..., yT ), the Pearson correlation co-
efficient was used in (Tao and Zhai, 2005). We
also consider two other commonly used measures
? cosine (Salton and McGill, 1983), and Jensen-
Shannon divergence (Lin, 1991), though our re-
sults show that Pearson correlation coefficient per-
forms better than these two other methods. Since
the time correlation method and the phonetic cor-
respondence method exploit distinct resources, it
makes sense to combine them. We explore two ap-
proaches to combining these two methods, namely
score combination and rank combination. These
will be defined below in Section 4.2.
4 Experiments
We evaluate our algorithms on three compara-
ble corpora: English/Arabic, English/Chinese, and
English/Hindi. Data statistics are shown in Ta-
ble 5.
From each data set in Table 5, we picked out all
news articles from seven randomly selected days.
We identified about 6800 English names using the
entity recognizer from (Carlson et al, 1999), and
chose the most frequent 200 names as our English
named entity candidates. Note that we chose the
most frequent names because the reliability of the
statistical correlation depends on the size of sam-
ple data. When a name is rare in a collection,
253
Source Target Cost Target Cost
g g 0 r 40.5
kh 2.5 e 44.5
cCh 5.5 del 24
tsh 17.5 ins 20
N 26.5
Table 3: Substitution/deletion/insertion costs for /g/.
English Candidate
Script Worldbet
Philippines 1       
 
 f l b y n
*2     
	    
 
 f l b y n y t
3            f l b y n a
Megawati *1 
 
 
  m h a f th
2          m i j a w a t a
3        m a k w z a
English Candidate
Script Romanization Worldbet
Belgium *1 Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 82?87,
Prague, June 2007. c?2007 Association for Computational Linguistics
Multilingual Word Sense Discrimination: A Comparative Cross-Linguistic
Study
Alla Rozovskaya
Department of Linguistics
Univ. of Illinois at Urbana-Champaign
Urbana, IL 61801
rozovska@uiuc.edu
Richard Sproat
Department of Linguistics
Univ. of Illinois at Urbana-Champaign
Urbana, IL 61801
rws@uiuc.edu
Abstract
We describe a study that evaluates an ap-
proach to Word Sense Discrimination on
three languages with different linguistic
structures, English, Hebrew, and Russian.
The goal of the study is to determine
whether there are significant performance
differences for the languages and to iden-
tify language-specific problems. The algo-
rithm is tested on semantically ambiguous
words using data from Wikipedia, an online
encyclopedia. We evaluate the induced clus-
ters against sense clusters created manually.
The results suggest a correlation between the
algorithm?s performance and morphological
complexity of the language. In particular,
we obtain FScores of 0.68 , 0.66 and 0.61 for
English, Hebrew, and Russian, respectively.
Moreover, we perform an experiment on
Russian, in which the context terms are lem-
matized. The lemma-based approach signif-
icantly improves the results over the word-
based approach, by increasing the FScore by
16%. This result demonstrates the impor-
tance of morphological analysis for the task
for morphologically rich languages like Rus-
sian.
1 Introduction
Ambiguity is pervasive in natural languages and cre-
ates an additional challenge for Natural Language
applications. Determining the sense of an ambigu-
ous word in a given context may benefit many NLP
tasks, such as Machine Translation, Question An-
swering, or Text-to-Speech synthesis.
The Word Sense Discrimination (WSD) or Word
Sense Induction task consists of grouping together
the occurrences of a semantically ambiguous term
according to its senses. Word Sense Discrimination
is similar to Word Sense Disambiguation, but allows
for a more unsupervised approach to the problem,
since it does not require a pre-defined set of senses.
This is important, given the number of potentially
ambiguous words in a language. Moreover, labeling
an occurrence with its sense is not always necessary.
For example, in Information Retrieval WSD would
be useful for the identification of documents relevant
to a query containing an ambiguous term.
Different approaches to WSD have been pro-
posed, but the evaluation is often conducted using
a single language, so it is difficult to predict per-
formance on another language. To the best of our
knowledge, there has not been a systematic com-
parative analysis of WSD systems on different lan-
guages. Yet, it is interesting to see whether there
are significant differences in performance when a
method is applied to several languages that have dif-
ferent linguistic structures. Identifying the reasons
for performance differences might suggest what fea-
tures are useful for the task.
The present project adopts an approach to WSD
that is based on similarity measure between context
terms of an ambiguous word. We compare the per-
formance of an algorithm for WSD on English, He-
brew, and Russian, using lexically ambiguous words
and corpora of similar sizes.
We believe that testing on the above languages
82
might give an idea about how accuracy of an algo-
rithm for WSD is affected by language choice. Rus-
sian is a member of the Slavic language group and is
morphologically rich. Verbs, nouns, and adjectives
are characterized by a developed inflectional system,
which results in a large number of wordforms. He-
brew is a Semitic language, and is complex in a dif-
ferent way. In addition to the root-pattern morphol-
ogy that affects the word stem, it also has a com-
plex verb declination system. Moreover, function
words, such as prepositions and determiners, cliti-
cize, thereby increasing the number of wordforms.
Lastly, cliticization, coupled with the absence of
short vowels in text, introduces an additional level
of ambiguity for Hebrew.
There are two main findings to this study. First,
we show that the morphological complexity of the
language affects the performance of the algorithm
for WSD. Second, the lemma-based approach to
RussianWSD significantly improves the results over
the word-based approach.
The rest of the paper is structured as follows:
first, we describe previous work that is related to the
project. Section 3 provides details about the algo-
rithm for WSD that we use. We then describe the
experiments and the evaluation methodology in Sec-
tions 4 and 5, respectively. We conclude with a dis-
cussion of the results and directions for future work.
2 Related Work
First, we describe several approaches to WSD that
are most relevant to the present project: Since we
are dealing with languages that do not have many
linguistic resources available, we chose a most unsu-
pervised, knowledge-poor approach to the task that
relies on words occurring in the context of an am-
biguous word. Next, we consider two papers on
WSD that provide evaluation for two languages. Fi-
nally, we describe work that is concerned with the
role of morphology for the task.
2.1 Approaches to Word Sense Discrimination
Pantel and Lin (2002) learn word sense induction
from an untagged corpus by finding the set of the
most similar words to the target and by clustering
the words. Each word cluster corresponds to a sense.
Thus, senses are viewed as clusters of words.
Another approach is based on clustering the oc-
currences of an ambiguous word in a corpus into
clusters that correspond to distinct senses of the
word. Based on this approach, a sense is defined
as a cluster of contexts of an ambiguous word. Each
occurrence of an ambiguous word is represented as a
vector of features, where features are based on terms
occurring in the context of the target word. For ex-
ample, Pedersen and Bruce (1997) cluster the oc-
currences of an ambiguous word by constructing a
vector of terms occurring in the context of the tar-
get. Schu?tze (1992) presents a method that explores
the similarity between the context terms occurring
around the target. This is accomplished by consider-
ing feature vectors of context terms of the ambigu-
ous word. The algorithm is evaluated on natural and
artificially-constructed ambiguous English words.
Sproat and van Santen (1998) introduce a tech-
nique for automatic detection of ambiguous words
in a corpus and measuring their degree of polysemy.
This technique employs a similarity measure be-
tween the context terms similar in spirit to the one
in (Schu?tze, 1992) and singular value decomposi-
tion in order to detect context terms that are impor-
tant for disambiguating the target. They show that
the method is capable of identifying polysemous En-
glish words.
2.2 Cross-Linguistic Study of WSD
Levinson (1999) presents an approach to WSD that
is evaluated on English and Hebrew. He finds 50
most similar words to the target and clusters them
into groups, the number of groups being the num-
ber of senses. He reports comparable results for
the two languages, but he uses both morphologi-
cally and lexically ambiguous words. Moreover, the
evaluation methodology focuses on the success of
disambiguation for an ambiguous word, and reports
the number of ambiguous words that were disam-
biguated successfully.
Davidov and Rappoport (2006) describe an al-
gorithm for unsupervised discovery of word cate-
gories and evaluate it on Russian and English cor-
pora. However, the focus of their work is on the dis-
covery of semantic categories and from the results
they report for the two languages it is difficult to in-
fer how the languages compare against each other.
We conduct a more thorough evaluation. We also
83
control cross-linguistically for number of training
examples and level of ambiguity of selected words,
as described in Section 4.
2.3 Morphology and WSD
McRoy (1992) describes a study of different sources
useful for word sense disambiguation, including
morphological information. She reports that mor-
phology is useful, but the focus is on derivational
morphology of the English language. In the present
context, we are interested in the effect of inflectional
morphology onWSD, especially for languages, such
as Russian and Hebrew.
Gaustad (2004) proposes a lemma-based ap-
proach to a Maximum Entropy Word Sense Disam-
biguation System for Dutch. She shows that collaps-
ing wordforms of an ambiguous word yields a more
robust classifier due to the availability of more train-
ing data. The results indicate an improvement of this
approach over classification based on wordforms.
3 Approach
Our algorithm relies on the method for selection of
relevant contextual terms and on distance measure
between them introduced in (Sproat and van Santen,
1998) and on the approach described in (Schu?tze,
1998), though the details of clustering differ slightly.
The intuition behind the algorithm can be summa-
rized as follows: (1) words that occur in the context
of the ambiguous word are useful for determining
its sense; and (2) contextual terms of an ambiguous
word belong to topics corresponding to the senses
of the ambiguous word. Before describing the algo-
rithm in detail, we give an overview of the system.
The algorithm starts by collecting all the occur-
rences of an ambiguous word in the corpus together
with the surrounding context. Next, we build a sym-
metric distance matrix D, where rows and columns
correspond to context terms, and D[i][j] is the dis-
tance value of term i and term j. The distance mea-
sure is supposed to reflect how the two terms are
close semantically (whether they are related to the
same topic). For example, we would expect the dis-
tance between the words financial and money to be
smaller than the distance between the words finan-
cial and river: The first pair is more likely to occur
in the same context, than the second one. Using the
distance measure, the context terms are partitioned
into sense clusters. Finally, we group the sentences
containing the ambiguous word into sentence clus-
ters using the context term clusters.
We now describe each step in detail:
1. We collect contextual terms of an ambigu-
ous word w in a context window of 50 words
around the target. Each context term t is as-
signed a weight (Sproat and J. van Santen,
1998):
wt =
CO(t|w)
FREQ(t)
(1)
CO(t|w) is the frequency of the term in the
context of w, and FREQ(t) is the frequency
of the term in the corpus. Term weights are
used to select context terms that will be help-
ful in determining the sense of the ambiguous
word in a particular context. Furthermore, term
weights are employed in (4) in sentence clus-
tering.
2. For each pair ti and tj of context terms, we
compute the distance between them (Sproat
and J. van Santen,1998):
Dw[i][j] = 1 ?
[
COw(ti|tj)
FREG(ti)
+
COw(tj |ti)
FREQ(tj)
]
2
(2)
COw(ti|tj) is the frequency of ti in the con-
text of tj , and FREQ(ti) is the frequency of
ti in the training corpus. We assume that the
distance between ti and tj is inversely propor-
tional to the semantic similarity between ti and
tj .
3. Using the distance matrix from (2), the con-
text terms are clustered using an agglomerative
clustering technique:
? Start by assigning each context term to a separate
cluster
? While stopping criterion is false: merge two clus-
ters whose distance 1 is the smallest.2
1There are several ways to define the distance between clus-
ters. Having experimented with three - Single Link, Complete
Link and Group Average, it was found that Complete Link def-
inition works best for the present task. (Complete Link distance
between clusters i and j is defined as the maximum distance be-
tween a term from cluster i and a term from cluster j).
2In the present study, the clusters are merged as long as the
84
The output of step (3) is a set of context term
clusters for the target word. Below are shown
select members for term clusters for the English
word bass:
Cluster 1: songwriter singer joined keyboardist
Cluster 2: waters fishing trout feet largemouth
4. Finally, the sentences containing the ambigu-
ous word are grouped using the context term
clusters from (3). Specifically, given a sen-
tence with the ambiguous word, we compute
the score of the sentence with respect to each
context word cluster in (3) and assign the sen-
tence to the cluster with the highest score. The
score of the sentence with respect to cluster c
is the sum of weights of sentence context terms
that are in c.
4 Experiments
The algorithm is evaluated on 9 ambiguous words
with two-sense distinctions. We select words that
(i) have the same two-sense distinction in all three
languages or (ii) are ambiguous in one of the lan-
guages, but each of their senses corresponds to an
unambiguous translation in the other two languages.
In the latter case, the translations are merged to-
gether to create an artificially ambiguous word. We
believe that this selection approach allows for a col-
lection of a comparable set of ambiguous words for
the three languages. An example of an ambiguous
word is the English word table, that corresponds to
two gross sense distinctions (tabular array, and a
piece of furniture). This word has two translations
into Russian and Hebrew, that correspond to the two
senses. The selected words are presented in Table 1.
The words display different types of ambigu-
ity. In particular, disambiguating the Hebrew word
gishah (access; approach) or the Russian word mir
(peace; world) would be useful in Machine Transla-
tion, while determining the sense of a word like lan-
guage would benefit an Information Retrieval sys-
tem. It should also be noted that several words pos-
sess additional senses, which were ignored because
they rarely occurred in the corpus. For example, the
Russian word yazyk (language) also has the meaning
of tongue (body part).
number of clusters exceeds the number of senses of the ambigu-
ous word in the test data.
The corpus for each language consists of 15M
word tokens, and for the same ambiguous word the
same number of training examples is selected from
each language. For each ambiguous word, a set of
100-150 examples together with 50 words of con-
text is selected from the section of the corpus not
used for training. These examples are manually an-
notated for senses and used as the test set for each
language.
5 Evaluation Methodology
The evaluation is conducted by comparing the in-
duced sentence clusters to clusters created manually.
We use three evaluation measures : cluster purity,
entropy, and FScore. 3
For a cluster Cr of size qr, where the size is the
number of examples in that cluster, the dominating
sense Si in that cluster is selected and cluster purity
is computed as follows:
P (Cr) =
nir
qr
, (3)
where nir is the number of examples in cluster Cr
with sense Si.
For an ambiguous word w, cluster purity P(w) is
the weighted average of purities of the clusters for
that word. 4. Higher cluster purity score corresponds
to a better clustering outcome.
Entropy and FScore measures are described in de-
tail in Zhao and Karypis (2005). Entropy indicates
how distinct senses are distributed between the two
clusters. The perfect distribution is the assignment
of all examples with sense 1 to one cluster and all
examples with sense 2 to the other cluster. In such
case, the entropy is 0. In general, a lower value in-
dicates a better cluster quality. Entropy is computed
for each cluster. Entropy for word w is the weighted
average of the entropies of the clusters for that word.
Finally, FScore considers both the coverage of the
algorithm and its ability to discriminate between the
two senses. FScore is computed as the harmonic
3Examples whose scores with respect to all clusters are zero
(examples that do not contain any terms found in the distance
matrix) are not assigned to any cluster, and thus do not affect
cluster purity and cluster entropy. This is captured by the FS-
core measure described below.
4In the present study, the number of clusters and the number
of senses for a word is always 2
85
Senses English Hebrew Russian
access;approach access;approach gishah dostup;podxod
actor;player actor;player saxqan akter;igrok
evidence; quarrel argument vikuax;nimuq argument
body part; chief head rosh golova;glava
world;peace world; peace shalom;olam mir
furniture; tabular array table shulxan;tavlah stol;tablitza
allow;resolve allow;resolve hershah;patar razreshat?
ambiance; air atmosphere avira;atmosfera atmosfera
human lang.;program. lang. language safah yazyk
Table 1: Ambiguous words for testing: The first column indicates the senses; unambiguous translations that
were merged to create an ambiguous word are indicated by a semicolon
mean of Precision and Recall, where recall and pre-
cision for sense Si with respect to cluster Cr are
computed by treating cluster Cr as the output of a
retrieval system for sense Si .
6 Results and Discussion
We show results for two experiments. Experi-
ment 1 compares the algorithm?s performance cross-
linguistically without morphological analysis ap-
plied to any of the languages. Experiment 2 com-
pares the performance for Russian in two settings:
with and without morphological processing per-
formed on the context terms.
Table 2 presents experimental results. Baseline is
computed by assigning the most common sense to
all occurrences of the ambiguous word. We observe
that English achieves the highest performance both
in terms of cluster purity and FScore, while Russian
performs most poorly among the three languages.
This behavior may be correlated with the average
frequency of the context terms that are used to con-
struct the distance matrix in the corpus (cf. 7 for
English and 4.2 for Russian). In particular, the dif-
ference in the frequencies can be attributed to the
morphological complexity of Russian, as compared
to English and Hebrew. Hebrew is more complex
than English morphologically, which would account
for a drop in performance for the Hebrew words vs.
the English words. Furthermore, one would expect
a higher degree of ambiguity for Hebrew due to the
absence of short vowels in text.
It is worth noting that while both Hebrew and
Russian possess features that might negatively af-
fect the performance, Hebrew performs better than
Russian. We hypothesize that cliticization and the
lack of vowels in text are not as significant factors
for the performance as the high inflectional nature
of a language, such as Russian. We observe that the
majotity of the context terms selected by the algo-
rithm for disambiguation belong to the noun cate-
gory. This seems intuitive, since nouns generally
provide more information content than other parts
of speech and thus should be useful for resolving
lexical ambiguity. While an English or a Hebrew
noun only has several wordforms, a Russian noun
may have up to 12 different forms due to various in-
flections.
The morphological complexity of Russian affects
the performance in two ways. First, cluster purity
is affected, since the counts of context terms are not
sufficiently reliable to accurately estimate term dis-
tances. Incorrect term distances subsequently affect
the quality of the term clusters. Second, the percent-
age of default occurrences (examples that have no
context terms occurring in the distance matrix) is the
least for English (0.22) and the highest for Russian
(0.27). The default occurrences affect the recall.
The results of experiment 2 support the fact that
morphological complexity of a language negatively
affects the performance. In that experiment, the in-
flections are removed from all the context terms. We
apply a morphological analyzer 5 to the corpus and
replace each word with its lemma. In 10% of the
word tokens, the analyzer gives more than one pos-
sible analysis, in which case the first analysis is se-
lected. As can be seen in Table 2 (last row), remov-
ing inflections produces a significant improvement
both in recall and precision, while preserving the
cluster purity and slightly reducing cluster entropy.
Moreover, the performance in terms of recall, pre-
cision, and coverage is better than for English and
5Available at http://www.aot.ru/
86
Language Baseline Coverage Precision Recall FScore Purity Entropy
English 0.73 0.78 0.77 0.61 0.68 0.79 0.61
Hebrew 0.72 0.79 0.76 0.58 0.66 0.82 0.59
Russian 0.72 0.73 0.70 0.54 0.61 0.81 0.62
Russian(lemma) 0.72 0.80 0.77 0.66 0.71 0.82 0.61
Table 2: Results: Baseline is the most frequent sense; coverage is the number of occurrences on which the
decision was made by the algorithm
Hebrew.
7 Conclusions and Future Work
We have described a cross-linguistic study of a
Word Sense Discrimination technique. An algo-
rithm based on context term clustering was applied
to ambiguous words from English, Hebrew, and
Russian, and a comparative analysis of the results
was presented. Several observations can be made.
First, the results suggest that the performance can
be affected by morphological complexity in the case
of a language, such as Russian, specifically, both in
terms of precision and recall. Second, removing in-
flectional morphology not only boosts the recall, but
significantly improves the precision. These results
support the view that morphological processing is
beneficial for WSD.
For future work, we plan to investigate more
thoroughly the role of morphological analysis
for WSD in Russian and Hebrew. In particular,
we will focus on the inflectional morphology of
Russian in order to determine whether removing
inflections consistently improves results for Russian
ambiguous words across different parts of speech.
Further, considering the complex structure of the
Hebrew language, we would like to determine what
kind of linguistic processing is useful for Hebrew in
the WSD context.
Acknowledgments
We are grateful to Roxana Girju and the anony-
mous reviewers for very useful suggestions and
comments. This work is funded in part by grants
from the National Security Agency and the National
Science Foundation.
References
Dmitry Davidov and Ari Rappoport 2006. Efficient Un-
supervised Discovery of Word Categories Using Sym-
metric Patterns and High Frequency Words. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, 297?304.
Sydney, Australia.
Richard O. Duda and Peter E. Hart. 1973. Pattern Classi-
fication and Scene Analysis. John Wiley & Sons, New
York.
Tanja Gaustad. 2004. A Lemma-Based Approach to a
Maximum Entropy Word Sense Disambiguation Sys-
tem for Dutch. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics (Col-
ing 2004), 778-784. Geneva.
Dmitry Levinson. 1999. Corpus-Based Method
for Unsupervised Word Sense Disambiguation.
www.stanford.edu/ dmitryle/acai99w1.ps.
Susan Weber McRoy. 1992. Using Multiple Knowledge
Sources for Word Sense Discrimination. Computa-
tional Linguistics, 18(1): 1?30.
Patrick Pantel and Dekang Lin. 2002. Discovering
Word Senses from Text In In Proceedings of ACM
SIGKDD, pages 613-619. Edmonton.
Ted Pedersen and Rebecca Bruce. 1997. Distinguishing
word senses in untagged text. In Proceedings of the
Second Conference on Empirical Methods in Natural
Language Processing, 197-207. Providence, RI, Au-
gust.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Richard Sproat and Jan van Santen. 1998. Automatic
ambiguity detection. In Proceedings of International
Conference on Spoken Language Processing . Sydney,
Australia, 1998.
Ying Zhao and George Karypis. 2005. Hierarchical
Clustering Algorithms for Document Datasets Data
Mining and Knowledge Discovery, 10(2):141?168.
87
Proceedings of NAACL HLT 2009: Tutorials, pages 15?16,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Writing Systems, Transliteration and Decipherment
Kevin Knight (USC/ISI)
Richard Sproat (CSLU/OHSU)
Description
Nearly all of the core data that computational linguists deal with is in the
form of text, which is to say that it consists of language data written (usually) in
the standard writing system for the language in question. Yet surprisingly little
is generally understood about how writing systems work. This tutorial will be
divided into three parts. In the first part we discuss the history of writing and
introduce a wide variety of writing systems, explaining their structure and how
they encode language. We end this section with a brief review of how some of
the properties of writing systems are handled in modern encoding systems, such
as Unicode, and some of the continued pitfalls that can occur despite the best
intentions of standardization. The second section of the tutorial will focus on the
problem of transcription between scripts (often termed ?transliteration?), and
how this problem?which is important both for machine translation and named
entity recognition?has been addressed. The third section is more theoretical
and, at the same time we hope, more fun. We will discuss the problem of
decipherment and how computational methods might be brought to bear on
the problem of unlocking the mysteries of as yet undeciphered ancient scripts.
We start with a brief review of three famous cases of decipherment. We then
discuss how techniques that have been used in speech recognition and machine
translation might be applied to the problem of decipherment. We end with a
survey of the as-yet undeciphered ancient scripts and give some sense of the
prospects of deciphering them given currently available data.
Outline
First hour:
? History of writing
? Survey of writing systems and how they work
? Modern encodings
Second hour:
? Problems of transcription (transliteration)
? Generative models of transcription
Break
15
? More on generative models of transcription
? Discriminative models
Third Hour
? Famous cases of decipherment
? Prospects for ?autodecipherment?
? What?s left to decipher?
Target Audience
This tutorial will be of interest to anyone who wishes to have a better under-
standing of how writing (the form of language that most computational linguists
deal with) works, and how such problems as transcription (transliteration) and
decipherment are approached computationally.
Bios
Kevin Knight is a Research Associate Professor in Computer Science at
the University of Southern California, a Senior Research Scientist and Fellow
at the USC/Information Sciences Institute, and Chief Scientist at Language
Weaver. Dr. Knight received a Ph.D. from Carnegie Mellon University in 1992,
and a bachelor?s degree from Harvard University. His current interests include
better statistical machine translation through linguistics, and he is also working
on exploiting cryptographic techniques to solve hard translation problems.
Richard Sproat received his Ph.D. in Linguistics from the Massachusetts
Institute of Technology in 1985. Since then he has worked at AT&T Bell Labs,
at Lucent?s Bell Labs and at AT&T Labs ? Research, before joining the faculty
of the University of Illinois, and subsequently the Oregon Health & Science Uni-
versity. Sproat has worked in numerous areas relating to language and compu-
tational linguistics, including syntax, morphology, computational morphology,
articulatory and acoustic phonetics, text processing, text-to-speech synthesis,
writing systems, and text-to-scene conversion.
16
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 109?117,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Knowing the Unseen: Estimating Vocabulary Size over Unseen Samples
Suma Bhat
Department of ECE
University of Illinois
spbhat2@illinois.edu
Richard Sproat
Center for Spoken Language Understanding
Oregon Health & Science University
rws@xoba.com
Abstract
Empirical studies on corpora involve mak-
ing measurements of several quantities for
the purpose of comparing corpora, creat-
ing language models or to make general-
izations about specific linguistic phenom-
ena in a language. Quantities such as av-
erage word length are stable across sam-
ple sizes and hence can be reliably esti-
mated from large enough samples. How-
ever, quantities such as vocabulary size
change with sample size. Thus measure-
ments based on a given sample will need
to be extrapolated to obtain their estimates
over larger unseen samples. In this work,
we propose a novel nonparametric estima-
tor of vocabulary size. Our main result is
to show the statistical consistency of the
estimator ? the first of its kind in the lit-
erature. Finally, we compare our proposal
with the state of the art estimators (both
parametric and nonparametric) on large
standard corpora; apart from showing the
favorable performance of our estimator,
we also see that the classical Good-Turing
estimator consistently underestimates the
vocabulary size.
1 Introduction
Empirical studies on corpora involve making mea-
surements of several quantities for the purpose of
comparing corpora, creating language models or
to make generalizations about specific linguistic
phenomena in a language. Quantities such as av-
erage word length or average sentence length are
stable across sample sizes. Hence empirical mea-
surements from large enough samples tend to be
reliable for even larger sample sizes. On the other
hand, quantities associated with word frequencies,
such as the number of hapax legomena or the num-
ber of distinct word types changes are strictly sam-
ple size dependent. Given a sample we can ob-
tain the seen vocabulary and the seen number of
hapax legomena. However, for the purpose of
comparison of corpora of different sizes or lin-
guistic phenomena based on samples of different
sizes it is imperative that these quantities be com-
pared based on similar sample sizes. We thus need
methods to extrapolate empirical measurements of
these quantities to arbitrary sample sizes.
Our focus in this study will be estimators of
vocabulary size for samples larger than the sam-
ple available. There is an abundance of estima-
tors of population size (in our case, vocabulary
size) in existing literature. Excellent survey arti-
cles that summarize the state-of-the-art are avail-
able in (Bunge and Fitzpatrick, 1993) and (Gan-
dolfi and Sastri, 2004). Of particular interest to
us is the set of estimators that have been shown
to model word frequency distributions well. This
study proposes a nonparametric estimator of vo-
cabulary size and evaluates its theoretical and em-
pirical performance. For comparison we consider
some state-of-the-art parametric and nonparamet-
ric estimators of vocabulary size.
The proposed non-parametric estimator for the
number of unseen elements assumes a regime
characterizing word frequency distributions. This
work is motivated by a scaling formulation to ad-
dress the problem of unlikely events proposed in
(Baayen, 2001; Khmaladze, 1987; Khmaladze and
Chitashvili, 1989; Wagner et al, 2006). We also
demonstrate that the estimator is strongly consis-
tent under the natural scaling formulation. While
compared with other vocabulary size estimates,
we see that our estimator performs at least as well
as some of the state of the art estimators.
2 Previous Work
Many estimators of vocabulary size are available
in the literature and a comparison of several non
109
parametric estimators of population size occurs in
(Gandolfi and Sastri, 2004). While a definite com-
parison including parametric estimators is lacking,
there is also no known work comparing methods
of extrapolation of vocabulary size. Baroni and
Evert, in (Baroni and Evert, 2005), evaluate the
performance of some estimators in extrapolating
vocabulary size for arbitrary sample sizes but limit
the study to parametric estimators. Since we con-
sider both parametric and nonparametric estima-
tors here, we consider this to be the first study
comparing a set of estimators for extrapolating vo-
cabulary size.
Estimators of vocabulary size that we compare
can be broadly classified into two types:
1. Nonparametric estimators- here word fre-
quency information from the given sample
alone is used to estimate the vocabulary size.
A good survey of the state of the art is avail-
able in (Gandolfi and Sastri, 2004). In this
paper, we compare our proposed estimator
with the canonical estimators available in
(Gandolfi and Sastri, 2004).
2. Parametric estimators- here a probabilistic
model capturing the relation between ex-
pected vocabulary size and sample size is the
estimator. Given a sample of size n, the
sample serves to calculate the parameters of
the model. The expected vocabulary for a
given sample size is then determined using
the explicit relation. The parametric esti-
mators considered in this study are (Baayen,
2001; Baroni and Evert, 2005),
(a) Zipf-Mandelbrot estimator (ZM);
(b) finite Zipf-Mandelbrot estimator (fZM).
In addition to the above estimators we consider
a novel non parametric estimator. It is the nonpara-
metric estimator that we propose, taking into ac-
count the characteristic feature of word frequency
distributions, to which we will turn next.
3 Novel Estimator of Vocabulary size
We observe (X1, . . . ,Xn), an i.i.d. sequence
drawn according to a probability distribution P
from a large, but finite, vocabulary ?. Our goal
is in estimating the ?essential? size of the vocabu-
lary ? using only the observations. In other words,
having seen a sample of size n we wish to know,
given another sample from the same population,
how many unseen elements we would expect to
see. Our nonparametric estimator for the number
of unseen elements is motivated by the character-
istic property of word frequency distributions, the
Large Number of Rare Events (LNRE) (Baayen,
2001). We also demonstrate that the estimator is
strongly consistent under a natural scaling formu-
lation described in (Khmaladze, 1987).
3.1 A Scaling Formulation
Our main interest is in probability distributions P
with the property that a large number of words in
the vocabulary ? are unlikely, i.e., the chance any
word appears eventually in an arbitrarily long ob-
servation is strictly between 0 and 1. The authors
in (Baayen, 2001; Khmaladze and Chitashvili,
1989; Wagner et al, 2006) propose a natural scal-
ing formulation to study this problem; specifically,
(Baayen, 2001) has a tutorial-like summary of the
theoretical work in (Khmaladze, 1987; Khmaladze
and Chitashvili, 1989). In particular, the authors
consider a sequence of vocabulary sets and prob-
ability distributions, indexed by the observation
size n. Specifically, the observation (X1, . . . ,Xn)
is drawn i.i.d. from a vocabulary ?n according to
probability Pn. If the probability of a word, say
? ? ?n is p, then the probability that this specific
word ? does not occur in an observation of size n
is
(1 ? p)n .
For ? to be an unlikely word, we would like this
probability for large n to remain strictly between
0 and 1. This implies that
c?
n ? p ?
c?
n , (1)
for some strictly positive constants 0 < c? < c? <
?. We will assume throughout this paper that c?
and c? are the same for every word ? ? ?n. This
implies that the vocabulary size is growing lin-
early with the observation size:
n
c? ? |?n| ?
n
c? .
This model is called the LNRE zone and its appli-
cability in natural language corpora is studied in
detail in (Baayen, 2001).
3.2 Shadows
Consider the observation string (X1, . . . ,Xn) and
let us denote the quantity of interest ? the number
110
of word types in the vocabulary ?n that are not
observed ? by On. This quantity is random since
the observation string itself is. However, we note
that the distribution of On is unaffected if one re-
labels the words in ?n. This motivates studying
of the probabilities assigned by Pn without refer-
ence to the labeling of the word; this is done in
(Khmaladze and Chitashvili, 1989) via the struc-
tural distribution function and in (Wagner et al,
2006) via the shadow. Here we focus on the latter
description:
Definition 1 Let Xn be a random variable on ?n
with distribution Pn. The shadow of Pn is de-
fined to be the distribution of the random variable
Pn({Xn}).
For the finite vocabulary situation we are con-
sidering, specifying the shadow is exactly equiv-
alent to specifying the unordered components of
Pn, viewed as a probability vector.
3.3 Scaled Shadows Converge
We will follow (Wagner et al, 2006) and sup-
pose that the scaled shadows, the distribution of
n ?Pn(Xn), denoted by Qn converge to a distribu-
tion Q. As an example, if Pn is a uniform distribu-
tion over a vocabulary of size cn, then n ? Pn(Xn)
equals 1c almost surely for each n (and hence it
converges in distribution). From this convergence
assumption we can, further, infer the following:
1. Since the probability of each word ? is lower
and upper bounded as in Equation (1), we
know that the distribution Qn is non-zero
only in the range [c?, c?].
2. The ?essential? size of the vocabulary, i.e.,
the number of words of ?n on which Pn
puts non-zero probability can be evaluated di-
rectly from the scaled shadow, scaled by 1n as
? c?
c?
1
y dQn(y). (2)
Using the dominated convergence theorem,
we can conclude that the convergence of the
scaled shadows guarantees that the size of the
vocabulary, scaled by 1/n, converges as well:
|?n|
n ?
? c?
c?
1
y dQ(y). (3)
3.4 Profiles and their Limits
Our goal in this paper is to estimate the size of the
underlying vocabulary, i.e., the expression in (2),
? c?
c?
n
y dQn(y), (4)
from the observations (X1, . . . ,Xn). We observe
that since the scaled shadow Qn does not de-
pend on the labeling of the words in ?n, a suf-
ficient statistic to estimate (4) from the observa-
tion (X1, . . . ,Xn) is the profile of the observation:
(?n1 , . . . , ?nn), defined as follows. ?nk is the num-
ber of word types that appear exactly k times in
the observation, for k = 1, . . . , n. Observe that
n
?
k=1
k?nk = n,
and that
V def=
n
?
k=1
?nk (5)
is the number of observed words. Thus, the object
of our interest is,
On = |?n| ? V. (6)
3.5 Convergence of Scaled Profiles
One of the main results of (Wagner et al, 2006) is
that the scaled profiles converge to a deterministic
probability vector under the scaling model intro-
duced in Section 3.3. Specifically, we have from
Proposition 1 of (Wagner et al, 2006):
n
?
k=1
?
?
?
?
k?k
n ? ?k?1
?
?
?
?
?? 0, almost surely, (7)
where
?k :=
? c?
c?
yk exp(?y)
k! dQ(y) k = 0, 1, 2, . . . .
(8)
This convergence result suggests a natural estima-
tor for On, expressed in Equation (6).
3.6 A Consistent Estimator of On
We start with the limiting expression for scaled
profiles in Equation (7) and come up with a natu-
ral estimator for On. Our development leading to
the estimator is somewhat heuristic and is aimed
at motivating the structure of the estimator for the
number of unseen words, On. We formally state
and prove its consistency at the end of this section.
111
3.6.1 A Heuristic Derivation
Starting from (7), let us first make the approxima-
tion that
k?k
n ? ?k?1, k = 1, . . . , n. (9)
We now have the formal calculation
n
?
k=1
?nk
n ?
n
?
k=1
?k?1
k (10)
=
n
?
k=1
? c?
c?
e?yyk?1
k! dQ(y)
?
? c?
c?
e?y
y
( n
?
k=1
yk
k!
)
dQ(y) (11)
?
? c?
c?
e?y
y (e
y ? 1) dQ(y) (12)
? |?n|n ?
? c?
c?
e?y
y dQ(y). (13)
Here the approximation in Equation (10) follows
from the approximation in Equation (9), the ap-
proximation in Equation (11) involves swapping
the outer discrete summation with integration and
is justified formally later in the section, the ap-
proximation in Equation (12) follows because
n
?
k=1
yk
k! ? e
y ? 1,
as n ? ?, and the approximation in Equa-
tion (13) is justified from the convergence in Equa-
tion (3). Now, comparing Equation (13) with
Equation (6), we arrive at an approximation for
our quantity of interest:
On
n ?
? c?
c?
e?y
y dQ(y). (14)
The geometric series allows us to write
1
y =
1
c?
?
?
?=0
(
1 ? yc?
)?
, ?y ? (0, c?) . (15)
Approximating this infinite series by a finite sum-
mation, we have for all y ? (c?, c?),
1
y ?
1
c?
M
?
?=0
(
1 ? yc?
)?
=
(
1 ? yc?
)M
y
?
(
1 ? c?c?
)M
c? . (16)
It helps to write the truncated geometric series as
a power series in y:
1
c?
M
?
?=0
(
1 ? yc?
)?
= 1c?
M
?
?=0
?
?
k=0
(
?
k
)
(?1)k
(y
c?
)k
= 1c?
M
?
k=0
( M
?
?=k
(
?
k
)
)
(?1)k
(y
c?
)k
=
M
?
k=0
(?1)k aMk yk, (17)
where we have written
aMk :=
1
c?k+1
( M
?
?=k
(
?
k
)
)
.
Substituting the finite summation approximation
in Equation 16 and its power series expression in
Equation (17) into Equation (14) and swapping the
discrete summation with the integral, we can con-
tinue
On
n ?
M
?
k=0
(?1)k aMk
? c?
c?
e?yyk dQ(y)
=
M
?
k=0
(?1)k aMk k!?k. (18)
Here, in Equation (18), we used the definition of
?k from Equation (8). From the convergence in
Equation (7), we finally arrive at our estimate:
On ?
M
?
k=0
(?1)k aMk (k + 1)! ?k+1. (19)
3.6.2 Consistency
Our main result is the demonstration of the consis-
tency of the estimator in Equation (19).
Theorem 1 For any ? > 0,
lim
n??
?
?
?
On ?
?M
k=0 (?1)
k aMk (k + 1)! ?k+1
?
?
?
n ? ?
almost surely, as long as
M ? c? log2 e + log2 (?c?)log2 (c?? c?) ? 1 ? log2 (c?)
. (20)
112
Proof: From Equation (6), we have
On
n =
|?n|
n ?
n
?
k=1
?k
n
= |?n|n ?
n
?
k=1
?k?1
k ?
n
?
k=1
1
k
(k?k
n ? ?k?1
)
. (21)
The first term in the right hand side (RHS) of
Equation (21) converges as seen in Equation (3).
The third term in the RHS of Equation (21) con-
verges to zero, almost surely, as seen from Equa-
tion (7). The second term in the RHS of Equa-
tion (21), on the other hand,
n
?
k=1
?k?1
k =
? c?
c?
e?y
y
( n
?
k=1
yk
k!
)
dQ(y)
?
? c?
c?
e?y
y (e
y ? 1) dQ(y), n ? ?,
=
? c?
c?
1
y dQ(y) ?
? c?
c?
e?y
y dQ(y).
The monotone convergence theorem justifies the
convergence in the second step above. Thus we
conclude that
lim
n??
On
n =
? c?
c?
e?y
y dQ(y) (22)
almost surely. Coming to the estimator, we can
write it as the sum of two terms:
M
?
k=0
(?1)k aMk k!?k (23)
+
M
?
k=0
(?1)k aMk k!
((k + 1)?k+1
n ? ?k
)
.
The second term in Equation (23) above is seen to
converge to zero almost surely as n ? ?, using
Equation (7) and noting that M is a constant not
depending on n. The first term in Equation (23)
can be written as, using the definition of ?k from
Equation (8),
? c?
c?
e?y
( M
?
k=0
(?1)k aMk yk
)
dQ(y). (24)
Combining Equations (22) and (24), we have that,
almost surely,
lim
n??
On ?
?M
k=0 (?1)k aMk (k + 1)! ?k+1
n =
? c?
c?
e?y
(
1
y ?
M
?
k=0
(?1)k aMk yk
)
dQ(y). (25)
Combining Equation (16) with Equation (17), we
have
0 < 1y ?
M
?
k=0
(?1)k aMk yk ?
(
1 ? c?c?
)M
c? . (26)
The quantity in Equation (25) can now be upper
bounded by, using Equation (26),
e?c?
(
1 ? c?c?
)M
c? .
For M that satisfy Equation (20) this term is less
than ?. The proof concludes.
3.7 Uniform Consistent Estimation
One of the main issues with actually employing
the estimator for the number of unseen elements
(cf. Equation (19)) is that it involves knowing the
parameter c?. In practice, there is no natural way to
obtain any estimate on this parameter c?. It would
be most useful if there were a way to modify the
estimator in a way that it does not depend on the
unobservable quantity c?. In this section we see that
such a modification is possible, while still retain-
ing the main theoretical performance result of con-
sistency (cf. Theorem 1).
The first step to see the modification is in ob-
serving where the need for c? arises: it is in writing
the geometric series for the function 1y (cf. Equa-
tions (15) and (16)). If we could let c? along with
the number of elements M itself depend on the
sample size n, then we could still have the geo-
metric series formula. More precisely, we have
1
y ?
1
c?n
Mn
?
?=0
(
1? yc?n
)?
= 1y
(
1 ? yc?n
)Mn
? 0, n ? ?,
as long as
c?n
Mn
? 0, n ? ?. (27)
This simple calculation suggests that we can re-
place c? and M in the formula for the estimator (cf.
Equation (19)) by terms that depend on n and sat-
isfy the condition expressed by Equation (27).
113
4 Experiments
4.1 Corpora
In our experiments we used the following corpora:
1. The British National Corpus (BNC): A cor-
pus of about 100 million words of written and
spoken British English from the years 1975-
1994.
2. The New York Times Corpus (NYT): A cor-
pus of about 5 million words.
3. The Malayalam Corpus (MAL): A collection
of about 2.5 million words from varied ar-
ticles in the Malayalam language from the
Central Institute of Indian Languages.
4. The Hindi Corpus (HIN): A collection of
about 3 million words from varied articles in
the Hindi language also from the Central In-
stitute of Indian Languages.
4.2 Methodology
We would like to see how well our estimator per-
forms in terms of estimating the number of unseen
elements. A natural way to study this is to ex-
pose only half of an existing corpus to be observed
and estimate the number of unseen elements (as-
suming the the actual corpus is twice the observed
size). We can then check numerically how well
our estimator performs with respect to the ?true?
value. We use a subset (the first 10%, 20%, 30%,
40% and 50%) of the corpus as the observed sam-
ple to estimate the vocabulary over twice the sam-
ple size. The following estimators have been com-
pared.
Nonparametric: Along with our proposed esti-
mator (in Section 3), the following canonical es-
timators available in (Gandolfi and Sastri, 2004)
and (Baayen, 2001) are studied.
1. Our proposed estimator On (cf. Section 3):
since the estimator is rather involved we con-
sider only small values of M (we see empir-
ically that the estimator converges for very
small values of M itself) and choose c? = M.
This allows our estimator for the number of
unseen elements to be of the following form,
for different values of M :
M On
1 2 (?1 ? ?2)
2 32 (?1 ? ?2) + 34?3
3 43 (?1 ? ?2) + 89
(
?3 ? ?43
)
Using this, the estimator of the true vocabu-
lary size is simply,
On + V. (28)
Here (cf. Equation (5))
V =
n
?
k=1
?nk . (29)
In the simulations below, we have considered
M large enough until we see numerical con-
vergence of the estimators: in all the cases,
no more than a value of 4 is needed for M .
For the English corpora, very small values of
M suffice ? in particular, we have considered
the average of the first three different estima-
tors (corresponding to the first three values
of M ). For the non-English corpora, we have
needed to consider M = 4.
2. Gandolfi-Sastri estimator,
VGS def=
n
n? ?1
(
V + ?1?2
)
, (30)
where
?2 = ?1 ? n? V2n +
?
5n2 + 2n(V ? 3?1) + (V ? ?1)2
2n ;
3. Chao estimator,
VChao def= V +
?21
2?2
; (31)
4. Good-Turing estimator,
VGT def=
V
(
1? ?1n
) ; (32)
5. ?Simplistic? estimator,
VSmpl def= V
(nnew
n
)
; (33)
here the supposition is that the vocabulary
size scales linearly with the sample size (here
nnew is the new sample size);
6. Baayen estimator,
VByn def= V +
(?1
n
)
nnew; (34)
here the supposition is that the vocabulary
growth rate at the observed sample size is
given by the ratio of the number of hapax
legomena to the sample size (cf. (Baayen,
2001) pp. 50).
114
% error of top 2 and Good?Turing estimates compared
%
 e
rro
r
?
40
?
30
?
20
?
10
0
10
Our GT ZM Our GT ZM Our GT ZM Our GT ZM
BNC NYT Malayalam Hindi
Figure 1: Comparison of error estimates of the 2
best estimators-ours and the ZM, with the Good-
Turing estimator using 10% sample size of all the
corpora. A bar with a positive height indicates
and overestimate and that with a negative height
indicates and underestimate. Our estimator out-
performs ZM. Good-Turing estimator widely un-
derestimates vocabulary size.
Parametric: Parametric estimators use the ob-
servations to first estimate the parameters. Then
the corresponding models are used to estimate the
vocabulary size over the larger sample. Thus the
frequency spectra of the observations are only in-
directly used in extrapolating the vocabulary size.
In this study we consider state of the art paramet-
ric estimators, as surveyed by (Baroni and Evert,
2005). We are aided in this study by the availabil-
ity of the implementations provided by the ZipfR
package and their default settings.
5 Results and Discussion
The performance of the different estimators as per-
centage errors of the true vocabulary size using
different corpora are tabulated in tables 1-4. We
now summarize some important observations.
? From the Figure 1, we see that our estima-
tor compares quite favorably with the best of
the state of the art estimators. The best of the
state of the art estimator is a parametric one
(ZM), while ours is a nonparametric estima-
tor.
? In table 1 and table 2 we see that our esti-
mate is quite close to the true vocabulary, at
all sample sizes. Further, it compares very fa-
vorably to the state of the art estimators (both
parametric and nonparametric).
? Again, on the two non-English corpora (ta-
bles 3 and 4) we see that our estimator com-
pares favorably with the best estimator of vo-
cabulary size and at some sample sizes even
surpasses it.
? Our estimator has theoretical performance
guarantees and its empirical performance is
comparable to that of the state of the art es-
timators. However, this performance comes
at a very small fraction of the computational
cost of the parametric estimators.
? The state of the art nonparametric Good-
Turing estimator wildly underestimates the
vocabulary; this is true in each of the four
corpora studied and at all sample sizes.
6 Conclusion
In this paper, we have proposed a new nonpara-
metric estimator of vocabulary size that takes into
account the LNRE property of word frequency
distributions and have shown that it is statistically
consistent. We then compared the performance of
the proposed estimator with that of the state of the
art estimators on large corpora. While the perfor-
mance of our estimator seems favorable, we also
see that the widely used classical Good-Turing
estimator consistently underestimates the vocabu-
lary size. Although as yet untested, with its com-
putational simplicity and favorable performance,
our estimator may serve as a more reliable alter-
native to the Good-Turing estimator for estimating
vocabulary sizes.
Acknowledgments
This research was partially supported by Award
IIS-0623805 from the National Science Founda-
tion.
References
R. H. Baayen. 2001. Word Frequency Distributions,
Kluwer Academic Publishers.
Marco Baroni and Stefan Evert. 2001. ?Testing the ex-
trapolation quality of word frequency models?, Pro-
ceedings of Corpus Linguistics , volume 1 of The
Corpus Linguistics Conference Series, P. Danielsson
and M. Wagenmakers (eds.).
J. Bunge and M. Fitzpatrick. 1993. ?Estimating the
number of species: a review?, Journal of the Amer-
ican Statistical Association, Vol. 88(421), pp. 364-
373.
115
Sample True % error w.r.t the true value
(% of corpus) value Our GT ZM fZM Smpl Byn Chao GS
10 153912 1 -27 -4 -8 46 23 8 -11
20 220847 -3 -30 -9 -12 39 19 4 -15
30 265813 -2 -30 -9 -11 39 20 6 -15
40 310351 1 -29 -7 -9 42 23 9 -13
50 340890 2 -28 -6 -8 43 24 10 -12
Table 1: Comparison of estimates of vocabulary size for the BNC corpus as percentage errors w.r.t the
true value. A negative value indicates an underestimate. Our estimator outperforms the other estimators
at all sample sizes.
Sample True % error w.r.t the true value
(% of corpus) value Our GT ZM fZM Smpl Byn Chao GS
10 37346 1 -24 5 -8 48 28 4 -8
20 51200 -3 -26 0 -11 46 22 -1 -11
30 60829 -2 -25 1 -10 48 23 1 -10
40 68774 -3 -25 0 -10 49 21 -1 -11
50 75526 -2 -25 0 -10 50 21 0 -10
Table 2: Comparison of estimates of vocabulary size for the NYT corpus as percentage errors w.r.t the
true value. A negative value indicates an underestimate. Our estimator compares favorably with ZM and
Chao.
Sample True % error w.r.t the true value
(% of corpus) value Our GT ZM fZM Smpl Byn Chao GS
10 146547 -2 -27 -5 -10 9 34 82 -2
20 246723 8 -23 4 -2 19 47 105 5
30 339196 4 -27 0 -5 16 42 93 -1
40 422010 5 -28 1 -4 17 43 95 -1
50 500166 5 -28 1 -4 18 44 94 -2
Table 3: Comparison of estimates of vocabulary size for the Malayalam corpus as percentage errors
w.r.t the true value. A negative value indicates an underestimate. Our estimator compares favorably with
ZM and GS.
Sample True % error w.r.t the true value
(% of corpus) value Our GT ZM fZM Smpl Byn Chao GS
10 47639 -2 -34 -4 -9 25 32 31 -12
20 71320 7 -30 2 -1 34 43 51 -7
30 93259 2 -33 -1 -5 30 38 42 -10
40 113186 0 -35 -5 -7 26 34 39 -13
50 131715 -1 -36 -6 -8 24 33 40 -14
Table 4: Comparison of estimates of vocabulary size for the Hindi corpus as percentage errors w.r.t the
true value. A negative value indicates an underestimate. Our estimator outperforms the other estimators
at certain sample sizes.
116
A. Gandolfi and C. C. A. Sastri. 2004. ?Nonparamet-
ric Estimations about Species not Observed in a
Random Sample?, Milan Journal of Mathematics,
Vol. 72, pp. 81-105.
E. V. Khmaladze. 1987. ?The statistical analysis of
large number of rare events?, Technical Report, De-
partment of Mathematics and Statistics., CWI, Am-
sterdam, MS-R8804.
E. V. Khmaladze and R. J. Chitashvili. 1989. ?Statis-
tical analysis of large number of rate events and re-
lated problems?, Probability theory and mathemati-
cal statistics (Russian), Vol. 92, pp. 196-245.
. P. Santhanam, A. Orlitsky, and K. Viswanathan, ?New
tricks for old dogs: Large alphabet probability es-
timation?, in Proc. 2007 IEEE Information Theory
Workshop, Sept. 2007, pp. 638?643.
A. B. Wagner, P. Viswanath and S. R. Kulkarni. 2006.
?Strong Consistency of the Good-Turing estimator?,
IEEE Symposium on Information Theory, 2006.
117
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 32?35,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Named Entity Transcription with Pair n-Gram Models
Martin Jansche
Google Inc.
mjansche@google.com
Richard Sproat
Google Inc. and OHSU
rws@google.com
Abstract
We submitted results for each of the eight
shared tasks. Except for Japanese name
kanji restoration, which uses a noisy channel
model, our Standard Run submissions were
produced by generative long-range pair n-
gram models, which we mostly augmented
with publicly available data (either from
LDC datasets or mined from Wikipedia) for
the Non-Standard Runs.
1 Introduction
This paper describes the work that we did at Google,
Inc. for the NEWS 2009 Machine Transliteration
Shared Task (Li et al, 2009b; Li et al, 2009a). Except
for the Japanese kanji task (which we describe be-
low), all models were pair n-gram language models.
Briefly, we took the training data, and ran an iterative
alignment algorithm using a single-state weighted
finite-state transducer (WFST).We then trained a lan-
guage model on the input-output pairs of the align-
ment, which was then converted into a WFST encod-
ing a joint model. For the Non-Standard runs, we use
additional data fromWikipedia or from the LDC, ex-
cept where noted below. In the few instances where
we used data not available from Wikipedia or LDC,
wewill be happy to share themwith other participants
of this competition.
2 Korean
For Korean, we created a mapping between each
Hangul glyph and its phonetic transcription inWorld-
Bet (Hieronymus, 1993) based on the tables from
Unitran (Yoon et al, 2007). Vowel-initial syllables
were augmented with a ?0? at the beginning of the
syllable, to avoid spurious resyllabifications: Abbott
should be ???, never ???. We also filtered the
set of possible Hangul syllable combinations, since
certain syllables are never used in transliterations, e.g.
any with two consonants in the coda. The mapping
between Hangul syllables and phonetic transcription
was handled with a simple FST.
The main transliteration model for the Standard
Run was a 10-gram pair language model trained on
an alignment of English letters to Korean phonemes.
All transliteration pairs observed in the training/
development data were cached, and made available
if those names should recur in the test data. We
also submitted a Non-Standard Run with English/
Korean pairs mined from Wikipedia. These were de-
rived from the titles of corresponding interlinked En-
glish and Korean articles. Obviously not all such
pairs are transliterations, so we filtered the raw list
by predicting, for each English word, and using the
trained transliteration model, what the ten most likely
transliterations were in Korean; and then accepting
any pair in Wikipedia where the string in Korean also
occurred in the set of predicted transliterations. This
resulted in 11,169 transliteration pairs. In addition a
dictionary of 9,047 English and Korean translitera-
tion pairs that we had obtained from another source
was added. These pairs were added to the cache, and
were also used to retrain the transliteration model,
along with the provided data.
3 Indian Languages
For the Indian languages Hindi, Tamil and Kannada,
the same basic approach as for Korean was used. We
created a reversible map between Devanagari, Tamil
or Kannada symbols and their phonemic values, us-
ing a modified version of Unitran. However, since
Brahmi-derived scripts distinguish between diacritic
and full vowel forms, in order to map back from
phonemic transcription into the script form, it is nec-
essary to know whether a vowel comes after a conso-
nant or not, in order to select the correct form. These
and other constraints were implementedwith a simple
hand-constructed WFST for each script.
The main transliteration model for the Standard
Run was a 6-gram pair language model trained on
an alignment of English letters to Hindi, Kannada
32
or Tamil phonemes in the training and development
sets. At test time, this WFST was composed with the
phoneme to letter WFST just described to produce a
WFST that maps directly between English letters and
Indian script forms. As with Korean, all observed
transliteration pairs from the training/development
data were cached, and made available if those names
should recur in the test data. For each Indian lan-
guage we also submitted a Non-Standard Run which
included English/Devanagari, English/Tamil and En-
glish/Kannada pairs mined from Wikipedia, and fil-
tered as described above for Korean. This resulted
in 11,674 pairs for English/Hindi, 10,957 pairs for
English/Tamil and 2,436 pairs for English/Kannada.
These pairs were then added to the cache, and were
also used to retrain the transliteration model, along
with the provided data.
4 Russian
For Russian, we computed a direct letter/letter cor-
respondences between the Latin representation of
English and the Cyrillic representation of Russian
words. This seemed to be a reasonable choice since
Russian orthography is fairly phonemic, at least at an
abstract level, and it was doubtful that any gain would
be had from trying to model the pronunciation better.
We note that many of the examples were, in fact, not
English to begin with, but a variety of languages, in-
cluding Polish and others, that happen to be written
in the Latin script.
We used a 6-gram pair language model for the
Standard Run. For the Non-Standard Runs we in-
cluded: (for NSR1) a list of 3,687 English/Russian
pairs mined from the Web; and (for NSR2), those,
plus a set of 1,826 mined fromWikipedia and filtered
as described above. In each case, the found pairs were
put in the cache, and were used to retrain the language
model.
5 Chinese
For Chinese, we built a direct stochastic model be-
tween strings of Latin characters representing the En-
glish names and strings of hanzi representing their
Chinese transcription. It is well known (Zhang et al,
2004) that the direct approach produces significantly
better transcription quality than indirect approaches
based on intermediate pinyin or phoneme represen-
tations. This observation is consistent with our own
experience during system development.
In our version of the direct approach, we first
aligned the English letter strings with their corre-
sponding Chinese hanzi strings using the same mem-
oryless monotonic alignment model as before. We
then built standard n-gram models over the align-
ments, which were then turned, for use at runtime,
into weighted FSTs computing a mapping from En-
glish to Chinese.
The transcription model we chose for the Stan-
dard Run is a 6-gram language model over align-
ments, built with Kneser-Ney smoothing and a mini-
mal amount of Seymore-Rosenfeld shrinking.
We submitted two Non-Standard Runs with addi-
tional names taken from the LDC Chinese/English
Name Entity Lists v 1.0 (LDC2005T34). The only list
from this collection we used was Propernames Peo-
ple EC, which contains 572,213 ?English? names (in
fact, names from many languages, all represented in
the Latin alphabet) with one or more Chinese tran-
scriptions for each name. Data of similar quality can
be easily extracted from theWeb as well. For the sake
of reproducible results, we deliberately chose to work
with a standard corpus. The LDC name lists have
all of the problems that are usually associated with
data extracted from the Web, including improbable
entries, genuine mistakes, character substitutions, a
variety of unspecified source languages, etc.
We removed names with symbols other than let-
ters ?a? through ?z? from the list and divided it into
a held-out portion, consisting of names that occur in
the development or test data of the Shared Task, and
a training portion, consisting of everything else, for a
total of 622,187 unique English/Chinese name pairs.
We then used the model from the Standard Run to
predict multiple pronunciations for each of the names
in the training portion of the LDC list and retained
up to 5 pronunciations for each English name where
the prediction from the Standard model agreed with
a pronunciation found in the LDC list.
For our first Non-Standard Run, we trained a 7-
gram language model based on the Shared Task train-
ing data (31,961 name pairs) plus an additional 95,576
name pairs from the intersection of the LDC list and
the Standard model predictions. Since the selection
of additional training data was, by design, very con-
servative, we got a small improvement over the Stan-
dard Run.
The reason for this cautious approach was that the
additional LDC data did not match the provided train-
ing and development data very well, partly due to
noise, partly due to different transcription conven-
tions. For example, the Pinyin syllable bo? is predom-
inantly written as? in the LDC data, but? does not
33
occur at all in the Shared Task training data:
Character Occurrences
Train LDC
? 0 13,110
? 1,547 3,709
We normalized the LDC data (towards the tran-
scription conventions implicit in the Shared Task
data) by replacing hanzi for frequent Pinyin syllables
with the predominant homophonous hanzi from the
Shared Task data. This resembles a related approach
to pronunciation extraction from the web (Ghoshal et
al., 2009), where extraction validation and pronunci-
ation normalization steps were found to be tremen-
dously helpful, even necessary, when using web-
derived pronunciations. One of the conclusions there
was that extracted pronunciations should be used di-
rectly when available.
This is what we did in our second Non-Standard
Run. We used the filtered and normalized LDC data
as a static dictionary in which to look up the transcrip-
tion of names in the test data. This is how the shared
task problem would be solved in practice and it re-
sulted in a huge gain in quality. Notice, however, that
doing so is non-trivial, because of the data quality and
data mismatch problems described above.
6 Japanese Katakana
The ?English? to Japanese katakana task suffered
from the usual problem that the Latin alphabet side
covered many languages besides English. It thus be-
came an exercise in guessing which one of many valid
ways of pronouncing the Latin letter string would be
chosen as the basis for the Japanese transcription. We
toyed with the idea of building mixture models before
deciding that this issue is more appropriate for a pro-
nunciation modeling shared task. In the end, we built
the same kinds of straightforward pair n-grammodels
as in the tasks described earlier.
For Japanese katakana we performed a similar
kind of preprocessing as for the Indian languages:
since it is possible (under minimal assumptions)
to construct an isomorphism between katakana and
Japanese phonemes, we chose to use phonemes as
the main level of representation in our model. This
is because Latin letters encode phonemes as opposed
to syllables or morae (to a first approximation) and
one pays a penalty (a loss of about 4% in accuracy on
the development data) for constructingmodels that go
from Latin letters directly to katakana.
For the Standard Run, we built a 5-grammodel that
maps from Latin letter strings to Japanese phoneme
strings. The model used the same kind of Kneser-
Ney smoothing and Seymore-Rosenfeld shrinking as
before. In addition, we restrict the model to only pro-
duce well-formed Japanese phoneme strings, by com-
posing it with an unweighted Japanese phonotactic
model that enforces the basic syllable structure.
7 Japanese Name Kanji
It is important to note that the Japanese name kanji
task is conceptually completely different from all of
the other tasks. We argue that this conceptual dif-
ference must translate into a different modeling and
system building approach.
The conceptual difference is this: In all other tasks,
we?re given well-formed ?English? names. For the
sake of argument, let?s say that they are indeed just
English names. These names have an English pro-
nunciation which is then mapped to a correspond-
ing Hindi or Korean pronunciation, and the resulting
Hindi or Korean ?words? (which do not look like or-
dinary Hindi or Korean words at all, except for su-
perficially following the phonology of the target lan-
guage) can be written down in Devanagari or Hangul.
Information is lost when distinct English sounds get
mapped to the same phonemes in the target language
andwhen semantic information (such as the gender of
the bearer of a name) is simply not transmitted across
the phonetic channel that produces the approximation
in the target language (transcription into Chinese is an
exception in this regard). We call this forward tran-
scription because we?re projecting the original repre-
sentation of a name onto an impoverished approxima-
tion.
In name kanji restoration, we?re moving in the op-
posite direction. The most natural, information-rich
form of a Japanese name is its kanji representation
(ja-Hani). When this gets transcribed into ro?maji (ja-
Latn), only the sound of the name is preserved. In
this task, we?re asked to recover the richer kanji form
from the impoverished ro?maji form. This is the op-
posite of the forward transcription tasks and just begs
to be described by a noisy channel model, which is
exactly what we did.
The noisy channel model is a factored generative
model that can be thought of as operating by drawing
an item (kanji string) from a source model over the
universe of Japanese names, and then, conditional on
the kanji, generating the observation (ro?maji string)
in a noisy, nondeterministic fashion, by drawing it at
random from a channel model (in this case, basically
a model of kanji readings).
To simplify things, we make the natural assump-
34
tion that there is a latent segmentation of the ro?maji
string into segments of one or more syllables and
that each individual kanji in a name generates exactly
one segment. For illustration, consider the example
abukawa ??, which has three possible segmenta-
tions: a+bukawa, abu+kawa, and abuka+wa. Note
that boundaries can fall into the middle of ambisyl-
labic long consonants, as in matto??.
Complicating this simple picture are several kinds
of noise in the training data: First, Chinese pinyin
mixed in with Japanese ro?maji, which we removed
mostly automatically from the training and develop-
ment data and for which we deliberately chose not to
produce guesses in the submitted runs on the test data.
Second, the seemingly arbitrary coalescence of cer-
tain vowel sequences. For example, o?numa ?? and
onuma?? appear as onuma, and kouda??? and
ko?da?? appear as koda in the training data. Severe
space limitations prevent us from going into further
details here: we will however discuss the issues dur-
ing our presentation at the workshop.
For the Standard Run, we built a trigram character
language model on the kanji names (16,182 from the
training data plus 3,539 from the development data,
discarding pinyin names). We assume a zero-order
channel model, where each kanji generates its portion
of the ro?maji observation independent of its kanji or
ro?maji context. We applied an EM algorithm to the
parallel ro?maji/kanji data (19,684 items) in order to
segment the ro?maji under the stated assumptions and
train the channel model. We pruned the model by re-
placing the last EM step with a Viterbi step, result-
ing in faster runtime with no loss in quality. NSR 1
uses more than 100k additional names (kanji only,
no additional parallel data) extracted from biograph-
ical articles in Wikipedia, as well as a list, found on
the Web, of the 10,000 most common Japanese sur-
names. A total of 117,782 names were used to train a
trigram source model. Everything else is identical to
the Standard Run. NSR 2 is like NSR 1 but adds dic-
tionary lookup. If we find the ro?maji name in a dictio-
nary of 27,358 names extracted from Wikipedia and
if a corresponding kanji name from the dictionary is
among the top 10 hypotheses produced by the model,
that hypothesis is promoted to the top (again, this per-
forms better than using the extracted names blindly).
NSR 3 is like NSR 1 but the channel model is trained
on a total of 108,172 ro?maji/kanji pairs consisting of
the training and development data plus data extracted
from biographies in Wikipedia. Finally NSR 4 is like
NSR 3 but adds the same kind of dictionary lookup as
in NSR 2. Note that the biggest gains are due first to
the richer source model in NSR 1 and second to the
richer channel model in NSR 3. The improvements
due to dictionary lookups in NSR 2 and 4 are small
by comparison.
8 Results
Results for the runs are summarized below. ?Rank?
is rank in SR/NSR as appropriate:
Run ACC F Rank
en/ta SR 0.436 0.894 2
NSR1 0.437 0.894 5
ja-Latn/ SR 0.606 0.749 2
ja-Hani NSR1 0.681 0.790 4
NSR2 0.703 0.805 3
NSR3 0.698 0.805 2
NSR4 0.717 0.818 1
en/ru SR 0.597 0.925 3
NSR1 0.609 0.928 2
NSR2 0.955 0.989 1
en/zh SR 0.646 0.867 6
NSR1 0.658 0.865 10
NSR2 0.909 0.960 1
en/hi SR 0.415 0.858 9
NSR1 0.424 0.862 8
en/ko SR 0.476 0.742 1
NSR1 0.794 0.894 1
en/kn SR 0.370 0.867 2
NSR1 0.374 0.868 4
en/ja-Kana SR 0.503 0.843 3
NSR1 0.564 0.862 n/a
Acknowledgments
The authors acknowledge the use of the English-Chinese
(EnCh) (Li et al, 2004), English-Japanese Katakana (EnJa),
English-Korean Hangul (EnKo), Japanese Name (in English)-
Japanese Kanji (JnJk) (http://www.cjk.org), and English-
Hindi (EnHi), English-Tamil (EnTa), English-Kannada (EnKa),
English-Russian (EnRu) (Kumaran and Kellner, 2007) corpora.
References
Arnab Ghoshal, Martin Jansche, Sanjeev Khudanpur, Michael
Riley, and Morgan E. Ulinksi. 2009. Web-derived pronunci-
ations. In ICASSP.
James L. Hieronymus. 1993. ASCII phonetic symbols for the
world?s languages: Worldbet. AT&T Bell Laboratories, tech-
nical memorandum.
A. Kumaran and Tobias Kellner. 2007. A generic framework for
machine transliteration. In SIGIR--30.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source chan-
nel model for machine transliteration. In ACL-42.
Haizhou Li, A. Kumaran, Vladimir Pervouchine, andMin Zhang.
2009a. Report on NEWS 2009 machine transliteration shared
task. In ACL-IJCNLP 2009 Named Entities Workshop, Singa-
pore.
Haizhou Li, A. Kumaran, Min Zhang, andVladimir Pervouchine.
2009b. Whitepaper of NEWS 2009 machine transliteration
shared task. In ACL-IJCNLP 2009 Named Entities Workshop,
Singapore.
Su-Youn Yoon, Kyoung-Young Kim, and Richard Sproat. 2007.
Multilingual transliteration using feature based phonetic
method. In ACL.
Min Zhang, Haizhou Li, and Jian Su. 2004. Direct orthographi-
cal mapping for machine transliteration. In COLING.
35
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 879?883,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Russian Stress Prediction using Maximum Entropy Ranking
Keith Hall Richard Sproat
Google, Inc
New York, NY, USA
{kbhall,rws}@google.com
Abstract
We explore a model of stress prediction
in Russian using a combination of lo-
cal contextual features and linguistically-
motivated features associated with the
word?s stem and suffix. We frame this
as a ranking problem, where the objec-
tive is to rank the pronunciation with the
correct stress above those with incorrect
stress. We train our models using a simple
Maximum Entropy ranking framework al-
lowing for efficient prediction. An empir-
ical evaluation shows that a model com-
bining the local contextual features and
the linguistically-motivated non-local fea-
tures performs best in identifying both
primary and secondary stress.
1 Introduction
In many languages, one component of accu-
rate word pronunciation prediction is predict-
ing the placement of lexical stress. While in
some languages (e.g. Spanish) the lexical stress
system is relatively simple, in others (e.g. En-
glish, Russian) stress prediction is quite compli-
cated. Much as with other work on pronuncia-
tion prediction, previous work on stress assign-
ment has fallen into two camps, namely systems
based on linguistically motivated rules (Church,
1985, for example) and more recently data-
driven techniques where the models are derived
directly from labeled training data (Dou et al,
2009). In this work, we present a machine-
learned system for predicting Russian stress
which incorporates both data-driven contextual
features as well as linguistically-motivated word
features.
2 Previous Work on Stress
Prediction
Pronunciation prediction, of which stress pre-
diction is a part, is important for many speech
applications including automatic speech recog-
nition, text-to-speech synthesis, and translit-
eration for, say, machine translation. While
there is by now a sizable literature on pro-
nunciation prediction from spelling (often
termed ?grapheme-to-phoneme? conversion),
work that specifically focuses on stress predic-
tion is more limited. One of the best-known
early pieces of work is (Church, 1985), which
uses morphological rules and stress pattern
templates to predict stress in novel words. An-
other early piece of work is (Williams, 1987).
The work we present here is closer in spirit to
data-driven approaches such as (Webster, 2004;
Pearson et al, 2000) and particularly (Dou et
al., 2009), whose features we use in the work
described below.
3 Russian Stress Patterns
Russian stress preserves many features of Indo-
European accenting patterns (Halle, 1997). In
order to know the stress of a morphologically
complex word consisting of a stem plus a suf-
fix, one needs to know if the stem has an accent,
and if so on what syllable; and similarly for the
suffix. For words where the stem is accented,
879
acc unacc postacc
Dat Sg ???'??? ?'????? ?????'?
gor?oxu g?orodu korolj?u
Dat Pl ???'???? ?????'?? ?????'??
gor?oxam gorod?am korolj?am
?pea? ?town? ?king?
Table 1: Examples of accented, unaccented and
postaccented nouns in Russian, for dative singular
and plural forms.
this accent overrides any accent that may oc-
cur on the suffix. With unaccented stems, if
the suffix has an accent, then stress for the
whole word will be on the suffix; if there is
also no stress on the suffix, then a default rule
places stress on the first syllable of the word.
In addition to these patterns, there are also
postaccented words, where accent is placed uni-
formly on the first syllable of the suffix ? an
innovation of East and South Slavic languages
(Halle, 1997). These latter cases can be handled
by assigning an accent to the stem, indicating
that it is associated with the syllable after the
stem. Some examples of each of these classes,
from (Halle, 1997, example 11), are given in
Table 1. According to Halle (1997), consid-
ering just nouns, 91.6% are accented (on the
stem), 6.6% are postaccented and 0.8% are un-
accented, with about 1.0% falling into other
patterns.
Stress placement in Russian is important for
speech applications since over and above the
phonetic effects of stress itself (prominence, du-
ration, etc.), the position of stress strongly in-
fluences vowel quality. To take an example
of the lexically unaccented noun ????? gorod
?city?, the genitive singular ?'????? g?oroda
/g"Or@d@/ contrasts with the nominative plural
?????'? gorod?a /g@r2d"a/. All non-stressed
/a/ are reduced to schwa ? or by most ac-
counts if before the stressed syllable to /2/; see
(Wade, 1992).
The stress patterns of Russian suggest that
useful features for predicting stress might in-
clude (string) prefix and suffix features of the
word in order to capture properties of the stem,
since some stems are (un)accented, or of the
suffix, since some suffixes are accented.
4 Maximum Entropy Rankers
Similarly to Dou et al (2009), we frame the
stress prediction problem as a ranking problem.
For each word, we identify stressable vowels and
generate a set of alternatives, each represent-
ing a different primary stress placement. Some
words also have secondary stress which, if it oc-
curs, always occurs before the primary stressed
syllable. For each primary stress alternative,
we generate all possible secondary stressed al-
ternatives, including an alternative that has no
secondary stress. (In the experiments reported
below we actually consider two conditions: one
where we ignore secondary stress in training
and evaluation; and one where we include it.)
Formally, we model the problem using a Max-
imum Entropy ranking framework similar to
that presented in Collins and Koo (2005). For
each example, xi, we generate the set of possible
stress patterns Yi. Our goal is to rank the items
in Yi such that all of the valid stress patterns
Y?i are above all of the invalid stress patterns.
Our objective function is the likelihood, L of
this conditional distribution:
L =
?
i
p(Y?i |Yi, xi) (1)
logL =
?
i
log p(Y?i |Yi, xi) (2)
=
?
i
log
?
y??Y?i
e
?
k ?kfk(y?,x)
Z
(3)
Z is defined as the sum of the conditional like-
lihood over all hypothesized stress predictions
for example xi:
Z =
?
y???Yi
e
?
k ?kfk(y??,x) (4)
The objective function in Equation 3 can be
optimized using a gradient-based optimization.
In our case, we use a variety of stochastic gra-
dient descent (SGD) which can be parallelized
for efficient training.
During training, we provide all plausibly cor-
rect primary stress patterns as the positive set
880
Y?i . At prediction-time, we evaluate all possi-
ble stress predictions and pick the one with the
highest score under the trained model ?:
argmax
y??Yi
p(y?|Yi) = argmax
y??Yi
?
k
?kfk(y?, x) (5)
The primary motivation for using Maximum
Entropy rather the ranking-SVM is for efficient
training and inference. Under the above Max-
imum Entropy model, we apply a linear model
to each hypothesis (i.e., we compute the dot-
product) and sort according to this score. This
makes inference (prediction) fast in comparison
to the ranking SVM-based approach proposed
in Dou et al (2009).
All experiments presented in this paper used
the Iterative Parameter Mixtures distributed
SGD training optimizer (Hall et al, 2010). Un-
der this training approach, per-iteration aver-
aging has a regularization-like effect for sparse
feature spaces. We also experimented with L1-
regularization, but it offered no additional im-
provements.
5 Features
The features used in (Dou et al, 2009) are
based on trigrams consisting of a vowel letter,
the preceding consonant letter (if any) and the
following consonant letter (if any). Attached
to each trigram is the stress level of the tri-
gram?s vowel ? 1, 2 or 0 (for no stress). For
the English word overdo with the stress pattern
2-0-1, the basic features would be ov:2, ver:0,
and do:1. Notating these pairs as si : ti, where
si is the triple, ti is the stress pattern and i is
the position in the word, the complete feature
set is given in Table 2, where the stress pat-
tern for the whole word is given in the last row
as t1t2...tN . Dou and colleagues use an SVM-
based ranking approach, so they generated fea-
tures for all possible stress assignments for each
word, assigning the highest rank to the correct
assignment. The ranker was then trained to
associate feature combinations to the correct
ranking of alternative stress possibilities.
Given the discussion in Section 3, plausible
additional features are all prefixes and suffixes
Substring si, ti
si, i, ti
Context si1, ti
si1si, ti
si+1, ti
sisi+1, ti
si1sisi+1, ti
Stress Pattern t1t2...tN
Table 2: Features used in (Dou et al, 2009, Table 2).
vowel ?,?,?,?,?,?,?,?,?
stop ?,?,?,?,?,?
nasal ?,?
fricative ?,?,?,?,?,?,?
hard/soft ?,?
yo ?
semivowel ?,?
liquid ?,?
affricate ?,?
Table 3: Abstract phonetic classes used for con-
structing ?abstract? versions of a word. Note that
etymologically, and in some ways phonologically, ?
v behaves like a semivowel in Russian.
of the word, which might be expected to better
capture some of the properties of Russian stress
patterns discussed above, than the much more
local features from (Dou et al, 2009). In this
case for all stress variants of the word we collect
prefixes of length 1 through the length of the
word, and similarly for suffixes, except that for
the stress symbol we treat that together with
the vowel it marks as a single symbol. Thus for
the word gorod?a, all prefixes of the word would
be g, go, gor, goro, gorod, gorod?a.
In addition, we include prefixes and suffixes
of an ?abstract? version of the word where most
consonants and vowels have been replaced by
a phonetic class. The mappings for these are
shown in Table 3.
Note that in Russian the vowel ? /jO/ is al-
ways stressed, but is rarely written in text: it
is usually spelled as ?, whose stressed pronun-
cation is /(j)E/. Since written ? is in general
ambiguous between ? and ?, when we compute
stress variants of a word for the purpose of rank-
881
ing, we include both variants that have ? and
?.
6 Data
Our data were 2,004,044 fully inflected words
with assigned stress expanded from Zaliznyak?s
Grammatical Dictionary of the Russian Lan-
guage (Zaliznyak, 1977). These were split ran-
domly into 1,904,044 training examples and
100,000 test examples. The 100,000 test ex-
amples obviously contain no forms that were
found in the training data, but most of them
are word forms that derive from lemmata from
which some training data forms are also de-
rived. Given the fact that Russian stress is lex-
ically determined as outlined in Section 3, this
is perfectly reasonable: in order to know how
to stress a form, it is often necessary to have
seen other words that share the same lemma.
Nonetheless, it is also of interest to know how
well the system works on words that do not
share any lemmata with words in the training
data. To that end, we collected a set of 248
forms that shared no lemmata with the train-
ing data. The two sets will be referred to in the
next section as the ?shared lemmata? and ?no
shared lemmata? sets.
7 Results
Table 4 gives word accuracy results for the dif-
ferent feature combinations, as follows: Dou et
al?s features (Dou et al, 2009); our affix fea-
tures; our affix features plus affix features based
on the abstract phonetic class versions of words;
Dou et als features plus our affix features; Dou
et als features plus our affix features plus the
abstract affix features.
When we consider only primary stress (col-
umn 2 in Table 4, for the shared-lemmata test
data, Dou et als features performed the worst
at 97.2% accuracy, with all feature combina-
tions that include the affix features performing
at the same level, 98.7%. For the no-shared-
lemmata test data, using Dou et als features
alone achieved an accuracy of 80.6%. The affix
features alone performed worse, at 79.8%, pre-
sumably because it is harder for them to gener-
Features 1 stress 1+2 stress
shared lemmata
Dou et al0.972 0.965
Aff 0.987 0.985
Aff+Abstr Aff 0.987 0.985
Dou et alAff 0.987 0.986
Dou et alAff+Abstr Aff 0.987 0.986
no shared lemmata
Dou et al0.806 0.798
Aff 0.798 0.782
Aff+Abstr 0.810 0.790
Dou et alAff 0.823 0.810
Dou et alAff+Abstr Aff 0.839 0.815
Table 4: Word accuracies for various feature combi-
nations for both shared lemmata and no-shared lem-
mata conditions. The second column reports results
where we consider only primary stress, the third col-
umn results where we also predict secondary stress.
alize to unseen cases, but using the abstract af-
fix features increased the performance to 81.0%,
better than that of using Dou et als features
alone. As can be seen combining Dou et als
features with various combinations of the affix
features improved the performance further.
For primary and secondary stress prediction
(column 3 in the table), the results are over-
all degraded for most conditions but otherwise
very similar in terms of ranking of the fea-
tures to what we find with primary stress alone.
Note though that for the shared-lemmata con-
dition the results with affix features are almost
as good as for the primary-stress-only case,
whereas there is a significant drop in perfor-
mance for the Dou et al features. For the
no-shared-lemmata condition, Dou et al?s fea-
tures fare rather better compared to the affix
features. On the other hand there is a sub-
stantial benefit to combining the features, as
the results for ?Dou et alAff? and ?Dou et
al+Aff+Abstr Aff? show. Note that in the
no-shared-lemmata condition, there is only one
word that is marked with a secondary stress,
and that stress is actually correctly predicted
by all methods. Much of the difference between
the Dou et al features and the affix condition
can be accounted for by three cases involving
the same root, which the affix condition misas-
882
signs secondary stress to.
For the shared-lemmata task however there
were a substantial number of differences, as
one might expect given the nature of the fea-
tures. Comparing just the Dou et al fea-
tures and the all-features condition, system-
atic benefit for the all-features condition was
found for secondary stress assignment for pro-
ductive prefixes where secondary stress is typ-
ically found. For example, the prefix ????
(?aero-?) as in ?`???????'???? (?aerodynam-
ics?) typically has secondary stress. This is usu-
ally missed by the Dou et al features, but is
uniformly correct for the all-features condition.
Since the no-shared-lemmata data set is
small, we tested significance using two permu-
tation tests. The first computed a distribu-
tion of scores for the test data where succes-
sive single test examples were removed. The
second randomly permuted the test data 248
times, after each random permutation, remov-
ing the first ten examples, and computing the
score. Pairwise t-tests between all conditions
for the primary-stress-only and for the primary
plus secondary stress predictions, were highly
significant in all cases.
We also experimented with a postaccent fea-
ture to model the postaccented class of nouns
described in Section 3. For each prefix of the
word, we record whether the following vowel
is stressed or unstressed. This feature yielded
only very slight improvements, and we do not
report these results here.
8 Discussion
In this paper we have presented a Maximum
Entropy ranking-based approach to Russian
stress prediction. The approach is similar in
spirit to the SVM-based ranking approach pre-
sented in (Dou et al, 2009), but incorporates
additional affix-based features, which are moti-
vated by linguistic analyses of the problem. We
have shown that these additional features gen-
eralize better than the Dou et al features in
cases where we have seen a related form of the
test word, and that combing the additional fea-
tures with the Dou et al features always yields
an improvement.
References
Kenneth Church. 1985. Stress assignment in letter
to sound rules for speech synthesis. In Associ-
ation for Computational Linguistics, pages 246?
253.
Michael Collins and Terry Koo. 2005. Discrim-
inative reranking for natural language parsing.
Computational Linguistics, 31:25?69, March.
Qing Dou, Shane Bergsma, Sittichai Jiampojamarn,
and Grzegorz Kondrak. 2009. A ranking ap-
proach to stress prediction for letter-to-phoneme
conversion. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP, pages
118?126, Suntec, Singapore, August. Association
for Computational Linguistics.
Keith B. Hall, Scott Gilpin, and Gideon Mann.
2010. Mapreduce/bigtable for distributed opti-
mization. In Neural Information Processing Sys-
tems Workshop on Leaning on Cores, Clusters,
and Clouds.
Morris Halle. 1997. On stress and accent in Indo-
European. Language, 73(2):275?313.
Steve Pearson, Roland Kuhn, Steven Fincke, and
Nick Kibre. 2000. Automatic methods for lexical
stress assignment and syllabification. In Interna-
tional Conference on Spoken Language Process-
ing, pages 423?426.
Terence Wade. 1992. A Comprehensive Russian
Grammar. Blackwell, Oxford.
Gabriel Webster. 2004. Improving letter-
to-pronunciation accuracy with automatic
morphologically-based stress prediction. In
International Conference on Spoken Language
Processing, pages 2573?2576.
Briony Williams. 1987. Word stress assignment
in a text-to-speech synthesis system for British
English. Computer Speech and Language, 2:235?
272.
Andrey Zaliznyak. 1977. Grammaticheskij slovar?
russkogo jazyka. Russkiy Yazik, Moscow.
883
Last Words
Ancient Symbols, Computational Linguistics,
and the Reviewing Practices of the General
Science Journals
Richard Sproat?
Center for Spoken Language
Understanding
1. Introduction
Few archaeological finds are as evocative as artifacts inscribed with symbols. Whenever
an archaeologist finds a potsherd or a seal impression that seems to have symbols
scratched or impressed on the surface, it is natural to want to ?read? the symbols. And
if the symbols come from an undeciphered or previously unknown symbol system it
is common to ask what language the symbols supposedly represent and whether the
system can be deciphered.
Of course the first question that really should be asked is whether the symbols are
in fact writing. A writing system, as linguists usually define it, is a symbol system that
is used to represent language. Familiar examples are alphabets such as the Latin, Greek,
Cyrillic, or Hangul alphabets, alphasyllabaries such as Devanagari or Tamil, syllabaries
such as Cherokee or Kana, and morphosyllabic systems like Chinese characters. But
symbol systems that do not encode language abound: European heraldry, mathematical
notation, labanotation (used to represent dance), and Boy Scout merit badges are all
examples of symbol systems that represent things, but do not function as part of a
system that represents language.
Whether an unknown system is writing or not is a difficult question to answer.
It can only be answered definitively in the affirmative if one can develop a verifiable
decipherment into some language or languages. Statistical techniques have been used in
decipherment for years, but these have always been used under the assumption that the
system one is dealing with is writing, and the techniques are used to uncover patterns or
regularities that might aid in the decipherment. Patterns of symbol distribution might
suggest that a symbol system is not linguistic: For example, odd repetition patterns
might make it seem that a symbol system is unlikely to be writing. But until recently
nobody had argued that statistical techniques could be used to determine that a system
is linguistic.1
It was therefore quite a surprise when, in April 2009, there appeared in Science
a short article by Rajesh Rao of the University of Washington and colleagues at two
research institutes in India that purported to provide such a measure (Rao et al 2009a).
Rao et al?s claim, which we will describe in more detail in the next section, was that
? Center for Spoken Language Understanding, Oregon Health & Science University, 20000 NW Walker Rd,
Beaverton, OR, 97006, USA. E-mail: rws@xoba.com.
1 People have used the existence of quasi-Zipfian distributions in symbol systems to argue for their status
as writing; such claims figure in the work of Rao and colleagues. But because it has been long known that
Zipfian distributions hold of many things besides language, such arguments are easy to dismiss.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
one could use conditional entropy as evidence that the famous symbol system of the third
millenium BCE Indus Valley civilization was most probably writing, and not some other
kind of system.
That the Indus symbols were writing is hardly a novel claim. Indeed, ever since the
first seal impression was found at Harappa (1872?1873 CE), it has been the standard
assumption that the symbols were part of a writing system and that the Indus Valley
civilization was literate. Over the years there have been literally hundreds of claims
of decipherment, the most well-known of these being the work of Asko Parpola and
colleagues over the last four decades (Parpola 1994). Parpola, who argues that the Indus
Valley people spoke an early form of Dravidian, has produced interpretations of a small
set of symbols, but nothing that can be characterized as a decipherment.
The first serious arguments against the idea that the Indus symbols were part of
a writing system were presented in work that Steve Farmer, Michael Witzel, and I
published in Farmer, Sproat, and Witzel (2004), which reviews extensive support for that
view from archaeological evidence and comparisons with other ancient symbol systems.
Although our arguments were certainly not universally acknowledged?least of all
among people who had spent most of their careers trying to decipher the symbols?
they have been accepted by many archaeologists and linguists, and established a viable
alternative view to the traditional view of these symbols. It was against this backdrop
that the Rao et al (2009a) paper appeared.
Taken at face value, Rao et al?s (2009a) paper would appear to have reestablished
the traditional view of the Indus symbols as the correct one, and indeed that is how the
paper was received by many who read it. A number of articles appeared in the popular
science press, with Wired declaring ?Artificial Intelligence Cracks Ancient Mystery?
(Keim 2009). The Indian press had a field day; they had studiously ignored the evidence
reported in our paper, presumably because it led to the unpalatable conclusion that
India?s earliest civilization was illiterate. But Rao et al?s paper, which appeared to
demonstrate the opposite, was widely reported.
The work has also apparently attracted attention beyond the popular science press
and those with some sort of axe to grind on the Indus Valley issue, for in March 2010
there appeared in the Proceedings of the Royal Society, Series A, a paper that used similar
techniques to Rao et al?s (2009a) in order to argue that ancient Pictish symbols, which
are found inscribed on about 300 standing stones in Scotland, are in fact a previously un-
recognized ancient writing system (Lee, Jonathan, and Ziman 2010). A trend, it seems,
has been established: We now have a set of statistical techniques that can distinguish
among ancient symbol systems and tell you which ones were writing and which ones
were not.
The only problem is that these techniques are in fact useless for this purpose, and
for reasons that are rather trivial and easy to demonstrate. The remainder of this article
will be devoted to two points. First, in Section 2, I review the techniques from the Rao
et al (2009a) and Lee, Jonathan, and Ziman (2010) papers, and show why they don?t
work. The demonstration will seem rather obvious to any reader of this journal. And
this in turn brings us to the second point: How is it that papers that are so trivially and
demonstrably wrong get published in journals such as Science or the Proceedings of the
Royal Society? Both papers relate to statistical language modeling, which is surely one
of the core techniques in computational linguistics, yet (apparently) no computational
linguists were asked to review these papers. Would a paper that made some blatantly
wrong claim about genetics be published in such venues? What does this say about our
field and its standing in the world? And what can we do about that? Those questions
are the topic of Section 3.
586
Sproat Ancient Symbols and Computational Linguistics
2. The Fallacies
Rao et al?s (2009a) paper is a typical short paper in Science consisting of a page of text
and figures, and a link to a longer description that details the techniques and data.
The main paper?which is presumably all that most people would read?contains a
convincing-looking plot, their Figure 1A, here reproduced as Figure 1. The plot purports
to show that bigram conditional entropy , defined as
H(Y|X) = ?
?
x?X ,y?Y
p(x, y)logp(y|x) (1)
can distinguish between non-linguistic symbol systems and linguistic symbol systems,
and that the Indus Valley symbols behave like linguistic symbol systems.
The plot looks very convincing indeed, but what does it mean?
Several aspects of the plot require explanation. First the horizontal axis, labeled
as ?number of tokens,? represents the bigram conditional entropy of subsets of each
corpus starting with the subset consisting of the 20 most common symbols, the 40
most common symbols, the 60 most common symbols, and so forth. What we see for
each corpus is that the conditional entropy grows over these successive subsets until it
approaches the conditional entropy of the corpus as a whole.
Second, the corpora represent small samples of various languages including En-
glish (sampled both as words and letters), Sumerian (cuneiform symbols), Old Tamil
(largely consonant?vowel combinations in the Tamil alphasyllabary), the Indus Valley
corpus due to Mahadevan (1977), and two types of non-linguistic systems (though see
subsequent discussion). The sample sizes are small because the Indus corpus against
which all other symbol systems are compared is very small. The average length of an
Indus ?inscription? (in Mahadevan?s corpus) is only about 4.5 symbols; the total size of
Figure 1
Conditional entropies for a variety of linguistic scripts and other symbol systems. From:
Rao, Rajesh, Nisha Yadav, Mayank Vahia, Hrishikesh Joglekar, R. Adhikari, and Iravatham
Mahadevan. 2009. Entropic evidence for linguistic structure in the Indus script. Science,
324(5931):1165. Figure 1A, page 1165. Reprinted with permission from AAAS. Poor quality
of the figure is due to poor quality in the original.
587
Computational Linguistics Volume 36, Number 3
the Mahadevan corpus is 7,000 tokens (and about 400 types). Though Rao et al (2009a)
make a point of stressing that they use sophisticated smoothing techniques (a modified
version of Kneser-Ney), one must remember that with such small data sets, smoothing
can only do so much for you.
Third, the curves labeled as ?Type 1? and ?Type 2? non-linguistic systems are
explained as follows:
Two major types of nonlinguistic systems are those that do not exhibit much sequential
structure (?Type 1? systems) and those that follow rigid sequential order (?Type 2?
systems). For example, the sequential order of signs in Vinc?a inscriptions appears to
have been unimportant. On the other hand, the sequences of deity signs in Near Eastern
inscriptions found on boundary stones (kudurrus) typically follow a rigid order that is
thought to reflect the hierarchical ordering of the deities. (Rao et al 2009a, page 1165)
On the face of it, it is not too surprising, given these descriptions, that the Type 1 system
shows rapid growth in the conditional entropy, whereas Type 2 stays close to zero. The
problem is that there is little evidence that either of these types accurately characterized
any ancient symbol system. So for example, the Vinc?a symbols of Old Europe were
certainly not random in their distribution according to the most authoritative source on
the topic (Winn 1981).2 Indeed, Gimbutas (1989) and Haarmann (1996) even proposed
that they represented a pre-Sumerian European script; although that is highly unlikely,
it is also unlikely they would have proposed the idea in the first place if the distribution
of symbols seemed random. Similarly, it is apparently not the case that the deity symbols
in kudurrus were arranged in a rigid order (see subsequent discussion): Clearly it is not
only computational linguists who should be bothered by the claims of this paper. In fact,
as one learns only if one reads the supplementary material for the paper, the data for
Type 1 and Type 2 were artificially generated from a rigid model (Type 2) and a random
and equiprobable model (Type 1).
Various on-line discussions, starting with Farmer, Sproat, and Witzel (2009), crit-
icized Rao et al (2009a) for their use of artificial data.3 So, in subsequent discussion,
including a recently published paper (Rao 2010) that largely rehashes the issues of
both the Science paper and another paper in PNAS (Rao et al 2009b),4 Rao backs off
from these claims and talks about the Type 1 and Type 2 curves as the limits of the
distribution. The take-home message appears to be that in principle symbol systems
could vary as widely as being completely rigid or completely random and equiprobable.
It is therefore surprising, the story goes, that the Indus symbols seem to fall right in
that narrow band that includes unequivocal writing systems. The problem with this
argument is that it is highly unlikely that there were ever any functional symbol sys-
tems that had either of these properties, and one can argue this point on basic infor-
mation theoretic grounds. A symbol system that was completely rigid?had an entropy
of 0?would convey no information whatsoever. If whenever symbol x occurred, sym-
2 Rao et al (2009a) mis-cite Winn to claim that the Vinc?a sequences were random.
3 We also summarized our criticisms of the paper in a letter to the editor of Science. This was rejected for
publication with the note ?we receive many more letters than we can accommodate.? This seemed an
odd excuse given that the letter would presumably be published online rather than in print?so space
would not be an issue, and the letter pertained directly to flows in a paper published in the magazine,
which one would think would be of importance.
4 Rao et al (2009b) has one advantage over Rao et al (2009a) in that they actually do show something: They
use Markov models to show that there is structure, which they term ?rich syntactic structure,? in the
Indus texts. That there is structure?the system is not random?has of course been known for decades;
see Farmer, Sproat, and Witzel (2004) for discussion of this point. And given the average length of the
Indus texts of around 4.5 glyphs, one wonders just how ?rich? the syntax could have been.
588
Sproat Ancient Symbols and Computational Linguistics
bol y always followed, there would be little point in having more than just symbol x,
except perhaps for decorative purposes. Even in language one finds pockets of such
predictability: The word sequence Monty Python?s Flying will hardly ever be followed
by anything other than Circus. For a whole system to be so rigid would be unexpected.
The other extreme?random and equiprobable?seems equally unlikely in general, if
only because symbols represent things, and the things they represent typically do not
occur with equal probability. So although Rao is technically correct that his Types 1 and 2
do represent the logical extremes of the distribution, it is not likely that any meaningful
symbol systems were ever created that had either of these properties.
In particular it is important to remember that random is not the same thing as random
and equiprobable: at least some of the discussion of Rao et al?s (2009a) paper (and the Lee,
Jonathan, and Ziman [2010] paper we examine subsequently) seems to depend upon
the confusion of these two quite distinct notions. If one allows that symbols have a
quasi-Zipfian distribution?something that is surely true of linguistic symbol systems,
but of many other things too?then one finds curves that look very similar to what
Rao et al find for their ?linguistic? systems in their Science paper. Thus, as I argued
in a contribution to Liberman (2009), one can ?get a very good fit to [Rao et al?s]
results for the Indus corpus with a model that has 400 elements with a perfect Zipf
distribution, with ? = 1.5, and conditional independence for the bigrams.? Similarly in
my invited talk at EMNLP?09 (Sproat 2009), I showed that one could replicate their
results with an artificially generated corpus that only matched the unigram frequencies
from the Mahadevan corpus and again had conditional independence for the bigrams.
It is not hard to understand why the plot for a randomly generated corpus with a
roughly Zipfian distribution should ?look like? language using Rao et al?s methods.
There are no constraints on what symbols can follow others, so for the n most frequent
symbols there is a large amount of uncertainty. But as one?s sample grows to the 2n most
frequent, the 3n most frequent, and so forth, the gain in uncertainty decreases simply
because the next n symbols have a smaller overall probability and thus their incremental
contribution to the uncertainty is smaller. Furthermore at no point will the entropy be
maximal: because the distribution of symbols is not equiprobable.
In subsequent discussions Rao?for example, Rao (2010)?has defended his posi-
tion by arguing that conditional entropy and other such measures are not intended to be
definitive, but merely suggestive and, when combined with other evidence that points
in the same direction, supportive of the conclusion that the Indus system is writing:
Simply put, it is an issue of weight of evidence. The problem is that for that argument to
work there must at least be some weight: If conditional entropy measures of a particular
form correlate more with language than they do with non-linguistic systems, if even
weakly, then that might count as evidence for the conclusion. In other words, one
wants a measure that can tell one, with better than chance accuracy, that the system
in question is (or is not) linguistic. But this has not been demonstrated: Nobody has
done the legwork of putting together the needed corpora of ancient linguistic and non-
linguistic symbol systems, and demonstrated that one can in fact use such measures to
do a better than chance job of classifying systems. The simple experiments involving
randomly generated texts discussed earlier do not leave one with much optimism that
this will be the case. But one has to admit that it is an open question. But it is the question
that has to be asked, and the fact that none of the reviewers of the Science article thought
to ask it speaks to the reviewing practices of that journal, at least as it relates to our field.
We turn now to Pictish symbols. The Picts were an Iron Age people (or possibly
several peoples) of Scotland who, among other things, left a few hundred standing
stones inscribed with symbols, with ?texts? ranging from one to a few symbols in
589
Computational Linguistics Volume 36, Number 3
length. Lee, Jonathan, and Ziman?s (2010) paper attempts to use measures derived
from entropy to ascertain whether these symbols are part of a linguistic writing system.
Similarly to Rao et al?s (2009a) work, they compare the symbols to a variety of known
writing systems, as well as symbol systems like Morse code, and European heraldry, and
randomly generated texts?by which, again, is meant random and equiprobable. As their
title ?Pictish symbols revealed as a written language through application of Shannon
entropy? suggests, they are much bolder than Rao et al (2009a) in what they think they
have demonstrated.
As with Rao et al?s (2009a) paper, there are a number of things in Lee, Jonathan, and
Ziman (2010) that should bother people other than computational linguists: They char-
acterize Egyptian hieroglyphs as a ?syllabic? writing system (it was a consonantal and
thus essentially a segmental writing system); they linearize their corpus of European
heraldry by reading bottom to top, which follows no conventions that I am aware of;
and they refer credulously to the supposed ?script? examples from Chinese Neolithic
pottery, which few Sinologists take seriously. But again, we focus here on the issues that
relate to computational linguistics.
Lee, Jonathan, and Ziman?s (2010) techniques are substantially more complicated
than Rao et al?s (2009a), and we do not have space to describe them fully here. One
reason for the complication is that they recognize the problem imposed by the very
small sample sizes of the corpora (a few hundred symbols in the case of Pictish), and
seek a method that is robust to such small sizes. They develop two measures, Ur and
and Cr, defined as follows. First, Ur is defined as
Ur =
F2
log2(Nd/Nu)
(2)
where F2 is the bigram entropy, Nd is the number of bigram types, and Nu is the number
of unigram types.5 Cr is defined as
Cr =
Nd
Nu
+ a
Sd
Td
(3)
where Nd and Nu are as before, a is a constant (for which, in their experiments, they
derive a value of 7, using cross-validation), Sd is the number of bigrams that occur once,
and Td is the total number of bigram tokens; this latter measure will be familiar as
n1
N ,
the Good-Turing estimate of the probability mass for unseen events. To illustrate the
components of Cr, Lee, Jonathan, and Ziman show a plot (their Figure 5.5), reproduced
here as Figure 2. According to their description this shows
[a p]lot of Sd/Td (degree of di-gram repetition) versus Nd/Nu (degree of di-gram lexicon
completeness). . . . Dashes, sematograms?heraldry; filled diamonds, letters?prose,
poetry and inscriptions; grey filled triangles, syllables?prose, poetry, inscriptions;
open squares, words?genealogical lists; crosses, code characters; open diamonds,
letters?genealogical lists; filled squares, words?prose, poetry and inscriptions. (Lee,
Jonathan, and Ziman 2010, page 8)
Note that the non-linguistic system of heraldry (given their assumptions of how to
?read? heraldic ?texts?) seems to have a much lower number of singleton bigrams than
would be expected given the corpus size, clearly separating it from linguistic systems.
5 Unfortunately, a complication in Lee, Jonathan, and Ziman?s (2010) paper is that their formulation of
bigram entropy in their Equation (2.2) is apparently wrong.
590
Sproat Ancient Symbols and Computational Linguistics
Figure 2
Reproduction of Figure 5.5, page 8, from Lee, Rob, Philip Jonathan, and Pauline Ziman. ?Pictish
symbols revealed as a written language through application of Shannon entropy.? Proceedings of
the Royal Society A: Mathematical, Physical & Engineering Sciences, pages 1?16, 31 March 2010. Used
with permission of the Royal Society. See text for explanation.
Lee, Jonathan, and Ziman (2010) use Cr and Ur to train a decision tree to classify
symbol systems. If Cr ? 4.89, the system is linguistic. Subsequent refinements use val-
ues of Ur to classify the system as segmental (Ur < 1.09), syllabic (Ur < 1.37), or else
logographic.
All very impressive looking, but does it really work? In order to put the Lee,
Jonathan, and Ziman (2010) theory to a serious test, I looked to another symbol system,
namely, Mesopotamian deity symbols from kudurrus (boundary stones) catalogued in
Seidl (1989). A small corpus was developed from the stones for which the depictions
in Seidl?s book were clear enough to read. The corpus contains only 545 tokens, with
59 types (the full set of types described by Seidl comprises 66). The Mesopotamian deity
symbols are pictographic, a property shared with many scripts, including Egyptian
and Luwian hieroglyphs and Mayan glyphs; and there are other script-like properties,
including the fact that the symbols are often arranged linearly (Figure 3), and some
symbols are ?ligatured? together. Yet we know that these symbols were not part of a
writing system.
Unfortunately the corpus is far too small for a meaningful comparison with the
results of Rao et al (2009a), though one point is clear from even a cursory examination
Figure 3
The linearly arranged symbols of the major deities of As?s?urnas.irpal II. From http://upload.
wikimedia.org/wikipedia/commons/8/87/Ashurnasirpal II stela british museam.jpg,
released under the GNU Free Documentation License, Version 1.2.
591
Computational Linguistics Volume 36, Number 3
of the texts: Rao et al?s claim that kudurru texts are rigidly ordered is clearly false
(which we also showed in Farmer, Sproat, and Witzel [2004]); if nothing else, some
symbols repeat within the same text, with different symbols following each repetition.
Turning now to Lee, Jonathan, and Ziman?s (2010) method, I computed Cr and Ur for
the kudurrus, yielding values of Cr = 8.0 and Ur = 1.55. For the Pictish symbols, Lee,
Jonathan, and Ziman computed values for Cr and Ur under various assumptions of
what the symbol type set was, with the largest values being Cr = 6.16 and Ur = 1.45.
The values for the kudurru texts are different than what they calculate for the Pictish
stones, but crucially they are different in the direction that, given their decision tree,
suggests that kudurrus are writing. In particular, Cr ? 4.89 and Ur ? 1.37, yielding
classification of the system as a logographic writing system. It is worth noting also that
the values for Nd/Nu and Sd/Td are 5.58 and 0.35, respectively, which puts them firmly
in the ?linguistic? range, as shown by the superimposed point in Figure 2.
More embarrassingly, a set of 75 ?texts? consisting of ?symbols? derived by succes-
sive tosses of seven six-sided dice, as suggested by Liberman (2010), with individual
text lengths ranging between 3 and 14, with a total of 638 ?symbols,? is revealed by
the application of Shannon entropy to be a syllabic writing system. For this system
Cr = 12.64 and Ur = 1.18.
Lee, Jonathan, and Ziman?s method thus fails a crucial test: It misclassifies as
writing systems whose true classification?as a non-linguistic system, as a randomly
generated and meaningless sequence?is known. Again, the reasons for this failure
seem clear enough. First, the tiny sample sizes of many of the texts they use make it
unlikely that one can derive reliable statistics in the first place. And second, even if we
allow that Lee, Jonathan, and Ziman?s measures reveal something about the structures
of the systems they are examining, the source of the structure could in principle be
many things. Perhaps it would have been too much to expect that a reviewer would
have known about the Mesopotamian deity symbols and suggested that Lee, Jonathan,
and Ziman should check those with their methods. But it would have been reasonable
to expect that someone should have asked them whether they can detect a truly random
but non-equiprobable system.
In summary, what neither the Rao et al work on the Indus symbols, nor the Lee,
Jonathan, and Ziman work on Pictish symbols have shown is that one can distinguish
structure that derives from linguistic constraints from structure that derives from some
other kind of constraints. Furthermore, they fail for rather trivial reasons?reasons that
should have been caught if competent reviewers had been assigned to these papers.
I must stress that I do not wish to argue that it is impossible that one could come up
with a sound statistical argument to show that a particular symbol system is not linguis-
tic. If one took a large sample of known linguistic and non-linguistic symbol systems,
and showed that a particular set of measures could reliably distinguish between them
with very high accuracy, then such measures could presumably be applied in the case of
unknown systems such as the Indus or Pictish systems. Then, and only then would one
have a clear and unequivocal demonstration of anything. But it is patently clear that the
papers we have critiqued here do not even come close to this.
3. What Can We Do about This?
The situation described in this article surely presents a problem for the field of computa-
tional linguistics. Although entropy and related concepts clearly predate computational
linguistics, they are central to statistical language processing and are used widely in
the field. Such measures certainly can tell us some things about a corpus of symbols,
592
Sproat Ancient Symbols and Computational Linguistics
but there is no evidence that they can tell us what Rao et al (2009a) or Lee, Jonathan,
and Ziman (2010) think they can tell us. Yet, with the publication of these papers, and
their promotion by the all-too-eager popular science press, non-specialists might easily
believe that ?artificial intelligence? methods can provide crucial evidence for a symbol
system?s status as writing. One can only expect that more such papers will appear.
Such work represents a misuse of the methods of the field of computational lin-
guistics, so in principle it should be of interest to practitioners in that field to try to
do something about this. At the very least, it would be useful if one could convince
general ?peer? reviewed publications such as Science or the Proceedings of the Royal
Society to include qualified computational linguists among the peer reviewers of any
such publications in the future. This was essentially Pereira?s plea (Pereira 2009). Such
a situation would hardly be tolerated in other fields, yet in publications like Science it
seems to be common when it comes to issues having to do with language.
Part of the problem may be that computational linguistics has relatively low visi-
bility. It is not clear that the editors of publications like Science even know that there are
people who spend their lives doing statistical and computational analyses of text; or, if
they do, that computational linguists have knowledge that is relevant to judging papers
like the ones under discussion here. The time is ripe for changing that. As the results
of computational linguistic research, in the form of things like machine translation or
automatic speech recognition systems, become more widely known and used, compu-
tational linguists have an opportunity to educate the wider community?and we should
take every opportunity to do so. For example the fact that n-gram language models are
used with a high degree of success in speech recognition systems depends upon the fact
that such language models are typically built from data consisting of millions or even
billions of tokens. Such points need to be stressed more fully in dealings with the press
or the science magazines, so that people do not get the impression that one can derive
reliable results by such techniques from corpora consisting of only a few hundred or few
thousand symbols. Despite a famous XKCD cartoon6 that characterizes computational
linguistics as a field that is ?so ill-defined? that people can ?subscribe to any of dozens
of contradictory models and still be taken seriously,? there are core methods that are
backed up by solid empirical data. Yet, as with any science, there are good ways and
bad ways to apply such methods.
Ultimately we may be fighting a losing battle. It is more exciting to learn that
a statistical method can tell you that such-and-such an ancient symbol system was
writing, than to learn that in fact the proposed methods do not work. But at least one
has a duty to try to set the record straight.
Acknowledgments
I thank Steve Farmer, Brian Roark, Robert
Dale, and a reviewer for Computational
Linguistics for useful comments on earlier
versions of this article.
References
Farmer, Steve, Richard Sproat, and Michael
Witzel. 2004. The collapse of the
Indus-script thesis: The myth of a literate
Harappan civilization. Electronic Journal of
Vedic Studies, 11(2):19?57.
Farmer, Steve, Richard Sproat, and Michael
Witzel. 2009. A refutation of the claimed
refutation of the nonlinguistic nature
of Indus symbols: Invented data sets
in the statistical paper of Rao et al
(Science, 2009). www.safarmer.com/
Refutation3.pdf.
Gimbutas, M. 1989. The Language of the
Goddess: Unearthing the Hidden Symbols of
6 http://xkcd.com/114/.
593
Computational Linguistics Volume 36, Number 3
Western Civilization. Thames and Hudson,
London.
Haarmann, Harald. 1996. Early Civilization
and Literacy in Europe: An Inquiry into
Cultural Continuity in the Ancient World.
Mouton de Gruyter, Berlin.
Keim, Brandon. 2009. Artificial intelligence
cracks 4,000-year-old mystery. Wired,
23 April. www.wired.com/wiredscience/
2009/04/indusscript/
Lee, Rob, Philip Jonathan, and Pauline
Ziman. 2010. Pictish symbols revealed as a
written language through application of
Shannon entropy. Proceedings of the Royal
Society A: Mathematical, Physical &
Engineering Sciences, pages 1?16,
31 March 2010.
Liberman, Mark. 2009. Conditional entropy
and the Indus script. Language Log,
26 April. http://languagelog.ldc.
upenn.edu/nll/?p=1374.
Liberman, Mark. 2010. Pictish writing?
Language Log, 2 April. http://
languagelog.ldc.upenn.edu/nll/
?p=2227.
Mahadevan, Iravatham. 1977. The Indus
Script: Texts, Concordance and Tables.
Archaeological Survey of India, Calcutta
and Delhi.
Parpola, Asko. 1994. Deciphering the Indus
Script. Cambridge University Press,
New York.
Pereira, Fernando. 2009. Falling for the
magic formula, April 26. http://
earningmyturns.blogspot.com/2009/04/
falling-for-magic-formula.html.
Rao, Rajesh. 2010. Probabilistic analysis of an
ancient undeciphered script. Computer,
April:76?80.
Rao, Rajesh, Nisha Yadav, Mayank Vahia,
Hrishikesh Joglekar, R. Adhikari, and
Iravatham Mahadevan. 2009a. Entropic
Evidence for Linguistic Structure in the
Indus Script. Science, 324(5931):1165.
Rao, Rajesh, Nisha Yadav, Mayank Vahia,
Hrishikesh Joglekar, R. Adhikari, and
Iravatham Mahadevan. 2009b. A Markov
model of the Indus script. Proceedings
of the National Academy of Sciences,
106(33):13685?13690.
Seidl, Ursula. 1989. Die babylonischen
Kudurru-Reliefs. Symbole mesopotamischer
Gottheiten. Universita?tsverlag Freiburg,
Freiburg.
Sproat, Richard. 2009. Symbols, meaning
and statistics. Invited talk at EMNLP,
Singapore. http://www.fask.uni-
mainz.de/lk/videoarchive/videos/
2009-08-06-emnlp-2009-richard-
sproat.html.
Winn, Shan M. M. 1981. Pre-writing in
Southeastern Europe: The Sign System of the
Vinc?a Culture, ca. 4000 B.C. Western
Publishers, Calgary.
594
Commentary and Discussion
Reply to Rao et al and Lee et al
Richard Sproat?
Center for Spoken Language Understanding
1. Introduction
In the last issue of this journal, I presented a piece that called into question some of the
techniques reported in two papers in high-profile journals that purported to provide
statistical evidence for the linguistic status of some ancient symbol systems (Sproat
2010a).
Not surprisingly, the authors of those two papers took issue with a number of
my claims, and have requested the opportunity to respond. The two responses, taken
together, are rather lengthy and as a result it is not possible, given limitations of space,
for me to address each and every one of their criticisms. I will therefore focus on what I
consider to be the most important objections.
2. Rao et al?s Response
The essential claim of Rao and his colleagues (henceforth ?Rao?) is that I have misrep-
resented the claims of Rao et al (2009), and painted an incomplete picture of their work.
They also take the opportunity to discuss at some length a wide range of evidence that
they feel supports the Indus script hypothesis.
I want to make two things clear at the outset. First, my critique was about the paper
that appeared in Science in 2009. Although I mentioned some of Rao?s later work, it was
not a critique of that later work. It seemed reasonable to assume that the paper in Science
was intended to be taken seriously and to stand on its own merits. If so, then I felt?
and still feel?that the paper had serious problems. A problematic paper that cannot
stand on its own does not become unproblematic even if subsequent research were to
show the conclusions to be correct. To give a stark example, if someone should even-
tually demonstrate rigorously that cottontop tamarins are capable of learning ?regular?
grammars, that would have no bearing on the questions currently surrounding Marc
Hauser?s 2002 publication in Cognition (Johnson 2010).
Second, it was not my purpose in my piece to use the pages of Computational
Linguistics as a forum for promoting the non-script theory for the Indus. Indeed I spent
exactly one short paragraph on this, where I discussed our work in Farmer, Sproat, and
Witzel (2004) as a way of setting the stage for the current discussion. In contrast, Rao
spends a significant portion of his response discussing the Indus symbols, the case for
the script theory, and the case against the non-script theory, and repeatedly refers to our
2004 paper and claimed problemswith our arguments. The desire to keepmy discussion
? Center for Spoken Language Understanding, Oregon Health & Science University, 20000 NWWalker Rd,
Beaverton, OR, 97006, USA. E-mail: rws@xoba.com.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 4
of the background short was what prompted me not to expand on my claim that our
arguments had ?been accepted by many archaeologists and linguists.? Rao notes that I
do not actually cite ?who these ?many archaeologists and linguists? are.? Perhaps they
do not exist? But they do: Andrew Lawler, a science reporter who in 2004 interviewed
a large number of people on both sides of the debate notes that ?many others are
convinced that Farmer, Witzel, and Sproat have found a way to move away from sterile
discussions of decipherment, and they find few flaws in their arguments? (Lawler
2004, page 2029), and quotes the Sanskrit scholar George Thompson and University
of Pennsylvania Professor Emeritus of Indian studies Frank Southworth.
2.1 Misunderstandings about Nonlinguistic Symbols
A large part of the discussion in Rao?s response centers, as it should, on the question
of the ?Type 1? (random) and ?Type 2? (rigidly ordered) models, which I argued did
not accurately characterize any nonlinguistic symbol systems. At the core of this debate
is the Old European sign system of the Vinc?a, which is claimed to be a good instance
of Type 1, and the Mesopotamian deity symbols found on kudurru stones, which are
claimed to be good instances of Type 2.
To support the claims for Vinc?a, Rao provides a quote from Winn (1990), which
notes that the signs do not seem to have any ordering, and are ?characteristically
disarranged.? But if one sees the broader context of this quote, it becomes clear that
Winn is talking here about a subset of the corpus, which is found on small vessels
and spindle whorls. He contrasts the inscriptions discussed in the quote given by
Rao with ?the more arranged format of tablets and seals? (page 270), and then again
(page 276) where he notes that ?[d]ifferences in complexity of sign usage is denoted
by sign ordering, which occurs on tablets and other objects, such as the plaque from
Grades?nica.? On page 263 he refers to the ?discovery of tablets with script-like content
at Tartaria in 1961,? a clear indicator that the Vinc?a tablets would seem to involve some
form of sign ordering. It is clear then, that Winn is not saying that the system is in
general unordered, only that a subset of the corpus is. Winn also suggests that the small
vessels in particular probably had a ritualistic function (page 276). Furthermore, the
text that was ellipted in Rao?s quote suggests that order was probably unimportant on
the spindle whorls ?since concepts or mnemonic aids could be inferred or interpreted
by an individual cognizant of the culture-specific content of the signs? (page 269),
suggesting a situation where the Vinc?a ?reader? could interpret the jumbled signs since
they knew the structure of the system. It is unfortunate that most of Winn?s interesting
work has been published in obscure locations, and is therefore not something the
average reader would run across. Fortunately though, Winn maintains a Web page at
http://www.prehistory.it/ftp/winn.htm, where one can see what the sign texts look
like and where Winn also notes (page 8) that ?the distinction between signs on pottery
and the more organized signs on tablets and other artifacts may signify functional
differences or different levels of usage and formality.?
Winn?s description does not support the claim that Vinc?a sign texts were generally
without structure. But suppose, counterfactually, it did. Would Rao et al?s ?Type 1? be
a good model of the system? Their Type 1 data is modeled by a random process that
generates texts ?based on the assumption that each sign has an equal probability of fol-
lowing any other? (Rao et al 2009, Supplement, page 2), or in other words a system that
is random and equiprobable. But where does Winn say that the system is equiprobable?
The most Rao?s excerpted quote could be taken to mean is that the system is random,
but Winn certainly does not say that the signs occur with equal frequency. Amuchmore
808
Sproat Reply to Rao et al and Lee et al
plausible model of a random ancient sign systemwould be one where the symbols have
a non-equiprobable distribution: as I argued in my article ?symbols represent things,
and the things they represent typically do not occur with equal probability.? Further, we
know that many things, from nonlinguistic systems such as kudurrus (see the following)
to populations of sets of species, have a Zipfian distribution, so a reasonable model
of such a system might well be one in which the unigram probability follows a Zipf-
Mandelbrot distribution. But to have presented Type 1 in this way would have been
problematic: As I discussed in my piece, such random texts easily ?fool? the bigram
conditional entropy measure that Rao et al proposed in their Science paper. Rao objects
to the statement in my piece that at least some of the discussion surrounding their
paper depends on the confusion between ?random? and ?random and equiprobable,?
deeming that statement ?unwarranted and not worthy of comment.? Yet their model of
Vinc?a as a random equiprobable system depends precisely on this confusion.
At the other extreme are the kudurru texts, which are claimed to be good examples
of Type 2, a rigidly ordered system. In support, Rao quotes Slanski (2003), who says
?to a certain extent, the divine symbols were deployed upon the Entitlement naru?s
(kudurrus) according to the deities? relative positions in the pantheon. The symbols
for the higher gods of the pantheon . . . are generally found upon or toward the top
and most prominent part of the monument.? Crucial phrases here are ?to a certain
extent? and ?the symbols for the higher gods . . . are generally found.? No doubt these
are partly true; indeed it is very often true that the ?sun disk,? ?crescent moon,? and
?star? symbols occur at the beginnings of texts?though ordering within those three
varies. But the kudurru texts are still far from rigid, as we showed in Farmer, Sproat, and
Witzel (2004) and as my own sample from Seidl shows. Over and above this, the quoted
excerpts would only seem to support a hierarchy, not a model ?based on the assumption
that each sign has a unique successor sign? or even one where based on ?variations of
this theme where each sign could be followed by, for example, 2 or 3 other signs? (Rao
et al 2009, Supplement, page 3). All the hierarchy implies is that a symbol Y that is
lower on the hierarchy should come after a symbol X that is higher on the hierarchy.
But for any given X there may be several symbols lower on the hierarchy, which means
that X could be followed directly by any of these, assuming that no concepts from the
intervening elements are ?mentioned? in the ?text.?
It is puzzling to me that, given these considerations and my own description of
my close examination of the kudurru corpus, Rao still suggests that ?we expect the
entropy of the kudurru sequences to be lower than linguistic systems and perhaps
slightly above the minimum entropy (Min Ent) range in Figure 1(b).? Surely one would
like to examine the data and see whether such a statement is likely to be supported.
In any case there seems to be little point in arguing about this. I have the data for the
kudurrus that I transcribed from Seidl and I will be happy to make them available to
anyone who wants them. As the reader will have seen in Lee et al?s reply, and as I
discuss in Section 3, Lee et al took a different attitude to the deity symbols than Rao
has. Rather than argue from ?first principles? about what the corpus might look like,
they actually went to the trouble of replicating my experiment and developed their own
corpus from Seidl?s work (Rob Lee, personal communication): They concluded that by
their measures kudurrus would also count as (logographic) writing (an issue I get back
to subsequently)?suggesting a much less rigid ordering than Rao et al seem to believe.
Rao contrasts the hierarchy that is certainly partly true of kudurru texts with the
situation in linguistic systems, which ?have no such hierarchy imposed on characters
or words.? Interestingly, although this is generally true, there are pockets of such
hierarchies that do occur in language. A good example is from Egyptian (Allen 2000),
809
Computational Linguistics Volume 36, Number 4
where there is a hierarchy that determines the order in which terms denoting human
or divine entities are written. Thus terms for gods will occur in writing before terms for
humans or other things, even if the divine terms would be spoken after the other terms.
This occurs a lot in royal names, so that for example Tutankhamen, whose name means
?living image of Amen?, is written in cartouches with the god Amen?s name first.
In my piece, I made the point that relevant nonlinguistic symbol systems abound,
a point that Rao seems to be calling into question. As a minor point, they object to
my use of Boy Scout merit badges and highway signs as relevant to the discussion,
inviting the reader to examine these and see ?whether such a comparison bears merit.?
Unfortunately this is due to a misinterpretation of what I meant by these comparisons,
and it is perfectly possible that I did not explain my point well enough. I was not
discussing the forms of the individual signs, but rather the fact that for merit badges
and highway signs the symbols can be combined into ?texts? that are invariably linearly
arranged. Boy Scout badges are worn on sashes, where they are neatly arranged in rows.
Informational highway signs, such as indicators for gas stations, accommodations, and
food, frequently occur together and when they do they are arranged linearly. Of course
the individual signs are not linearly composed any more than the Indus symbols are
linearly composed or, in general, the basic symbols in any script.
A more serious objection surrounds my claim that to have done the job properly,
Rao et al (2009) would have to have compared awider set of linguistic and nonlinguistic
systems, but I also suggested this would depend upon having a good set of corpora for
real nonlinguistic systems, something which I claimed nobody has done the legwork
of compiling. Rao claims that this ignores the work of Vidale (2007), who included a
description of ten ancient South Asian nonlinguistic systems in his rebuttal to Farmer,
Sproat, andWitzel (2004). This is misleading at best. If the reader will care to rereadwhat
I said in my article, it is clear that what I meant was that electronic corpora do not exist
onwhich one could carry out the kinds of statistical tests that Rao and colleagueswish to
perform. I believe that remains true. Vidale does indeed discuss ten systems, discusses
their distribution and use, and lists numbers of signs in each. But by ?legwork,? I was
not referring to discussion of such systems, but rather the development of corpora of such
systems whereby one could actually perform serious statistical analyses and compare
among a range of linguistic and nonlinguistic systems. As far as I am aware, there
is no such set of corpora. As Rao highlights, Vidale notes the ?systematic, large-scale
redundancy? of symbol distributions of some of the systems he examines. Perhaps, but
as we have seen in the discussion of Vinc?a symbols and kudurrus earlier, translating
such broad statements into precise statistical characterizations of the kind necessary for
Rao?s methods is tricky at best.
Rao also makes a point of noting Vidale?s observation that the number of distinct
signs in the systems he examines is 44, ?a far cry from the 400 or so signs in the Indus
script.? To that statistic one could add kudurru symbols, which number about 60 in
Seidl?s (1989) catalog. In fact, Vidale challenges Farmer, Witzel and me to ?identify
another South Asian system datable to the 3rd millennium B.C. of nonlinguistic signs
amounting to 400 basic signs or more? (page 344). Nonlinguistic systems with hundreds
of symbols certainly exist: heraldry (where the sign set is open-ended; see any text on
heraldry such as Slater [2002]) would be one such example, though of course that does
not meet the time and space restrictions required by Vidale. But then, what is the basis of
those restrictions? A symbol system has as many symbols as it needs. Chinese writing
has thousands of characters because it has chosen to represent linguistic units at the
level of the morpheme; the Greek alphabet is much smaller due to its decision to repre-
sent phonemic segments. If the Indus symbols are nonlinguistic, then the vocabulary
810
Sproat Reply to Rao et al and Lee et al
size of over 400 symbols would merely reflect that it represented a rich underlying
set of concepts. Proponents of the script thesis always like to discuss the advanced
state of Indus civilization, arguing that such an advanced civilization could easily have
invented writing, and would be hard pressed to do without it. The Indus certainly was
one of the most advanced civilizations of the third millenium BCE: Would it therefore
not be reasonable to suppose that they could have invented a rich nonlinguistic symbol
system, even if their less advanced neighbors could not?
I end the discussion of Type 1 and Type 2 systems by noting that Rao characterizes
the objection to the use of artificial data sets as ?a red herring? that ?does not change the
result that the Indus script is entropically similar to linguistic scripts.? But within the
first two paragraphs of the Science paper, Rao et al state: ?We compared the statistical
structure of sequences of signs in the Indus script with those from a representative
group of linguistic and nonlinguistic systems. Twomajor types of nonlinguistic systems
are those that do not exhibit much sequential structure (type 1 systems) and those that
follow rigid sequential order (type 2 systems).? These (artificial) systems are clearly
central to their argument yet, as I have argued, they are poor models of any relevant
nonlinguistic system. Objections hardly seem to constitute a red herring.
Rao takes strong issue with the fact that whereas I discuss Figure 1A from their
Science paper, nowhere do I discuss their Figure 1B, nor do I do more than mention their
later work. On the latter point, let me reiterate: My article was about the paper that
appeared in Science, not any later work. I was not, for example, making any claims about
the block entropy calculations in Rao (2010): I simply have not analyzed those. But as I
said, I assume that at the point the Science paper appeared, the methods proposed were
intended to be taken seriously on their own, and the evidence presented was intended
to be taken to support those methods. It is therefore perfectly reasonable for me to have
failed to discuss later work. What about Figure 1B? It is true that I did not present
this figure, which shows high entropy for DNA and proteins, includes Sanskrit among
the linguistic examples, and shows that Fortran has a somewhat lower entropy than the
natural language samples they include. My question about DNA and proteins would
be: Why are these relevant? Although they are possibly ?the two ?most ancient? non-
linguistic systems,? the topic of discussion surely is symbol systems that were products
of the human mind.
Fortran is of course a different matter, but here the question that has always puzzled
me is why Fortran was not included in Figure 1A: Why did Rao and colleagues not
show the entropy growth curve for Fortran in 1A, as Rao (2010) did and as they also
show in their Figure 1(b) in their response? I suspect I know: Using their original bigram
conditional entropymeasure, Figure 1A, Fortranwould have shown a growth curve that
overlapped significantly with the linguistic systems, compromising the visual impact of
the plot. But I had no direct evidence for this, so rather than speculate in my piece (as I
have here), I preferred to omit discussion. Unlike Rao, I did not view their Figure 1B as
including crucial data that I was somehow hiding.
While we are on this topic though, the block entropy in their new Figure 1(b) still
leaves one wondering: Fortran is lower than their linguistic examples, but not much
lower. The same is true of the Figure 1B in their Science paper. How would one expect
the plot to look if they had included a larger range of languages and text types? I return
to this point subsequently.
In Figure 1(b) in their response, Rao includes music (a Beethoven piece), which,
like DNA and protein, is very close to the maximum entropy curve. They later criticize
me for ignoring ?the fact that the results already include nonlinguistic systems: DNA
and protein sequences . . . as well as man-made sequences (Fortran code and music in
811
Computational Linguistics Volume 36, Number 4
Figure 1[b]).? DNA, protein, and Fortran we have just discussed. It seems odd to accuse
me of ignoring the case of music, as that data did not appear at all in the Science paper;
indeed it did not appear in Rao (2010), where he first introduced the block entropy
calculations. It would have required unusual prescience on my part to discuss data that,
until now, have not appeared in print.
Finally, while we are on the topic of types of symbol systems, I note that Rao makes
a point of enumerating the list of properties that they deem relevant for considering
the linguistic status of the Indus symbols. This is of course related to the issue of how
to interpret the claims of their work, which we take up in Section 2.2. Rather than
enumerate the properties they describe, I will simply present the script-like properties
of another system, namely, the Mesopotamian deity symbols, a system we know was
not linguistic. I also made some of these points, but more briefly, in my piece: (1) Deity
symbols are frequently linearly written and in the cases where linearity is less clear, the
symbols are written around the top of a stone in an apparent concentric circle pattern
(see examples in Seidl [1989]). One sees such non-linear arrangements with scripts too:
The Etruscan Magliano disk, and many rune stones, have text wrapped around the
border of the stone. (2) There is clear evidence for the directionality of deity symbols: To
the extent that ?more important? gods were depicted first, these occur at the left/top of
the text. (3) Deity symbols are often ligatured together: One symbol may be joined with
another. (4) The deity symbols obey a Zipfian distribution. (5) Deity symbols clearly
have language-like properties in that certain symbols display positional preference
(symbols for the more important gods coming earlier in the text), and certain glyphs
have an affinity for each other?for example, some glyphs such as the ?horned crown?
seem to like to be joined together with the ?symbol base.? (6) Deity symbols were used
largely on standing stones, but were also used in other contexts such as the ?necklace?
depicted in the As?s?urnas.irpal II bas relief shown in my piece. (7) Deity symbols are
pictographic, like many real scripts?Egyptian, Luwian, Mayan. This would seem like
a fairly compelling list of script-like properties: Contrary to what Rao suggests, it is
certainly possible to find very script-like nonlinguistic symbol systems.
2.2 Misunderstandings about Claims
Rao, in the response to my article, as well as elsewhere, makes the point that nowhere
do they claim that the conditional entropy measure(s) are sufficient by themselves
to demonstrate that the Indus symbols were linguistic. Indeed they do not. The last
sentence of the Science article reads: ?Given the prior evidence for syntactic structure in
the Indus script, our results increase the probability that the script represents language,
complementing other arguments that have been made explicitly or implicitly in favor
of the linguistic hypothesis.?
It is worth stressing at the outset that in my piece I do not claim that Rao and
colleagues make the claim that conditional entropy settles the matter?though it is
surely true that this was the way many people interpreted their results. The question is,
how is one supposed to interpret their claim that their results ?increase the probability
that the script represents language?? I can think of three possible interpretations. One
is as a loose general statement to the effect that there is already lots of evidence for the
script thesis, and this additional piece of evidence adds to that body.
The next two interpretations are more formal. A second interpretation is that they
are assuming some sort of ?deductive? model whereby for a set of features Fi, one is
computing the probability of the linguistic hypothesis (HL), versus the nonlinguistic
812
Sproat Reply to Rao et al and Lee et al
hypothesis (HNL), given these features. Implicitly then, we are combining the features
into a classifier that computes P(HL|Fi) versus P(HNL|Fi) for each feature, and produces
an overall prediction. It is worth noting that Lee et al?s paper does indeed use the
statistical measures Lee and colleagues develop in this deductive sense.
The third interpretation is the ?inductive? one that Rao presents in his response,
which he claims is the ?correct? way to interpret their work. In that interpretation, a
set of feature values are observed, and the task is to induce which of two underlying
models?HL orHNL?is the one likely to have generated the observation. The reader will
surely recognize that the second and third interpretations correspond pretty directly to
discriminative versus generative models. Both of these approaches are used in a variety
of classification tasks, and both are perfectly legitimate ways to view the problem.
The choice of the generative model as the correct interpretation of their work, and
its formalization as they have given it in their response to my piece, is the first time
this particular interpretation has been explicitly stated. At the very least, if they had
wanted people to understand this point, it would have been good to make it clear at
the outset by stating the model in the Science paper. One can hardly fault critics for
misunderstanding which of two equally viable interpretations was the one intended.
But nomatter: How aremy criticisms, and those of others Rao cites, relevant to what
they do claim, under the various interpretations? Do my criticisms, as Rao would claim,
miss the point? Am I building strawmen and destroying them? I do not think so. Under
the ?deductive? interpretation my critiques would appear to be highly relevant. I argue,
for example, that a memoryless random process with a non-equiprobable unigram
distribution is classified as ?linguistic? by the bigram conditional entropy feature from
the Science paper.1 This implies that there are distributions that are misclassified by the
feature. How many such misclassified systems are there among plausible nonlinguistic
symbol systems? We do not know, because Rao et al did not do a fair comparison of a
wide range of linguistic and nonlinguistic systems, and demonstrate statistically how
well the classifier works. So we do not know whether in fact this feature ?increases the
probability? of the Indus symbols being a linguistic script. Needless to say, this problem
with the deductive interpretation also implies the identical problemwith the looser first
interpretation I outlined above.
For the ?inductive? generative interpretation, the argument is a bit trickier because
we need to consider the probability of observing a feature, given one of the two hy-
potheses. For some features, such as the feature of linear arrangements of symbols, it
seems plausible that one has a higher expectation of seeing linearly arranged symbols
on the hypothesis that something is a true script, compared with the hypothesis of a
nonlinguistic system. For bigram conditional entropy, again we really do not know.
Again, not enough examples of human-created nonlinguistic systems were presented
in the Science paper to be able to estimate the probabilities. And there is another issue:
None of Rao and colleagues? work provides a statistical measure by which one can as-
sert that something is inside the ?linguistic? range. Rather, they present plots and invite
the reader to eyeball them, hardly a rigorous demonstration. As I noted herein, Fortran
does not appear to have a conditional entropy that is much lower than the linguistic
examples. What would one expect the ?linguistic range? to look like if a larger number
of languages were included and, equally importantly, a variety of genres ranging from
1 This was the ?simple experiment involving randomly generated texts? that I was referring to in my
article, a phrase I was misinterpreted to be using to characterize their work.
813
Computational Linguistics Volume 36, Number 4
prose, to poetry, to genealogies, to grafitti, to shopping lists? Again, we do not know, so
it is rather hard to evaluate the claim that for this feature P(Fi|HL) > P(Fi|HNL).
The bottom line is that if a proposal is to be taken seriously as a formal model, it
needs to meet certain criteria. When one proposes a new feature, one must demonstrate
that this feature adds information, and increases the discriminative power of one?s
model. I do not believe Rao et al?s paper in Science does this. And as we have already
noted, given several plausible interpretations of what one is trying to demonstrate, it is
usually best if one makes it clear at the outset what one means.
There is more that can be said. Aswe noted, Rao lists a number of features according
to which the Indus symbols look linguistic. But one can surely add to their list: Suppose
one considered features like ?is only used to write very short texts? or ?occurs in a
culture where there are no archaeological markers of manuscript production? (Farmer,
Sproat, and Witzel 2004)? Then the script thesis would not fare so well. As for Rao?s
claim that the prior probability P(HL)?the a priori probability of the linguistic hypoth-
esis being true?is ?higher than chance,? and the archaeological support for that claim,
I will have to pass over that point for lack of space. The interested reader may look at
Farmer, Sproat, and Witzel (2004), and the responses of our critics such as Vidale (2007),
and decide for themselves who makes the more convincing case.
2.3 Computational Linguistics, Decipherment, and the Wider World
Rao concludes by citing the impressive achievements of computational linguistics as a
field, and some of the important contributions to decipherment that work in the field has
afforded. They question my hypothesis that the editors of prominent science journals
may not ?even know that there are people who spend their lives doing statistical and
computational analyses of text,? and claim that ?[s]uch a statement fails to acknowledge
both the impressive achievements of the field of computational linguistics in recent
years and the wide coverage of these accomplishments in the popular press.?
I am of course aware of the impressive achievements of my field, and I even alluded
to them in the conclusion of my piece. I was trying to hypothesize why, apparently,
computational linguists were not asked to review the Science piece by Rao (or the Lee
et al piece in Proceedings of the Royal Society). It is entirely possible my hypothesis here
is wrong and that the editors of, say, Science are well aware of computational linguistics
as a field. That just makes this apparent oversight all the more disturbing.
I am also well aware of a whole range of work on applying computational tech-
niques to ancient scripts. My point, clearly, was not to claim that such work in general
is a ?misuse? of the methods of computational linguistics, as Rao seems to be implying
I do. I was rather focussing on two specific papers, the one published in Science, and the
one in the Proceedings of the Royal Society, to which I now turn.
3. Lee et al?s Response
Lee et al?s response (henceforth ?Lee?) is much shorter than Rao et al?s, and boils
down essentially to two points. The first point is that one can use randomizations of
a given corpus, compare the bigram conditional entropies of those randomized corpora
to the conditional entropies of the original, and see if the true conditional entropy is
uniformly smaller than any of the values computed from randomized corpora. For 80%
of script examples, and for the Pictish symbols, Lee found that this was the case. For sets
that were generated by random processes, the distribution of ?true? entropy vis-a`-vis
814
Sproat Reply to Rao et al and Lee et al
randomized entropy was more uniform. This method seems to make sense as a way
to spot memoryless non-equiprobable processes masquerading as structured systems,
such as the example of successive tosses of seven six-sided dice I discussed in my piece.
But this was not part of the methodology presented in the published paper by Lee,
which as I showed is fooled by such processes. Furthermore, it would still fail to spot
nonlinguistic systems that have structure, such as Mesopotamian deity symbols.
This leads directly into the second point, because Lee acknowledges my result
that texts from kudurrus look linguistic, noting that the values that I calculate ?for the
kudurrus data set places them in a similar level of communication? to Pictish symbols.
For them this is an acceptable result, and comes down to a ?difference in viewpoint
over terminology as to the definition of what constitutes ?writing?,? noting that I use
a ?stricter definition of writing than some other researchers, such as Powell (2009).?
They quote Powell as stating that ?writing is a system of markings with a conventional
reference that communicates information.? There is no doubt that some students of
writing systems have indeed adopted a broader definition of writing than that which
I assume. But these are by no means majority views. The problem with the broader
view expressed by the quoted sentence from Powell is that it would seem to classify
as writing any meaningful conventionalized symbol system. Mathematical equations,
dance notation, music, and (pace Lee et al) European heraldry would all count as
writing from that standpoint. After all, all of these are systems of markings, with
conventional reference, that communicate information. It is hard to believe that Powell
himself really wants this broad a definition: The subtitle of his book is ?Theory and His-
tory of the Technology of Civilization.? Although nonlinguistic notation systems such
as mathematics have been central in the history of the world, I submit that what makes
writing work as the ?technology of civilization? is precisely that it allows us to record
language (see Sproat 2010b). In any event if that is the breadth that Lee wants, then
certainly Pictish symbols (and Mesopotamian deity symbols) are writing, because they
are conventional systems of marks that (presumably) communicated information. But
the broader definition is also highly misleading. When lay people think of writing they
will undoubtedly think of the script and standard orthography they learned in school,
which allows them to write anything from letters, to shopping lists, to poetry, to journal
articles. Calling Pictish symbols ?writing? easily conjures up in the mind of the reader
the notion that such symbols could have been used by the Picts for writing treatises on
iron smelting techniques. No doubt Lee did not intend this interpretation, but once one
calls something ?writing? or ?language? this seems to be a natural consequence. In any
case the broad definition of writing Lee wants to assume reduces the claim that Pictish
symbols encode ?language? to near vacuity: Nobody ever doubted that the symbols
conveyed information of some kind.
4. Conclusion
It has been a useful exercise to read Rao et al?s and Lee et al?s responses, because this
has forced me to think even deeper about the issues than I had already done. I believe
my basic criticisms are still valid.
I end by noting that Rao et al, in particular, might feel grateful that they were given
an opportunity to respond in this forum. My colleagues and I were not so lucky: When
we wrote a letter to Science outlining our objections to the original paper, the journal
refused to publish our letter, citing ?space limitations.? Fortunately, Computational Lin-
guistics is still open for the exchange of critical discussion.
815
Computational Linguistics Volume 36, Number 4
References
Allen, James. 2000.Middle Egyptian: An
Introduction to the Language and Culture of
Hieroglyphs. Cambridge University Press,
New York.
Farmer, Steve, Richard Sproat, and
Michael Witzel. 2004. The collapse of the
Indus-script thesis: The myth of a literate
Harappan civilization. Electronic Journal
of Vedic Studies, 11(2):19?57.
Johnson, Carolyn. 2010. Journal editor
questions Harvard researcher?s
data. Boston Globe, August 27.
http://www.boston.com/news/
local/breaking news/2010/08/
journal editor.html.
Lawler, Andrew. 2004. The Indus script:
Write or wrong? Science, 306:2026?2029.
Powell, Barry. 2009.Writing: Theory and
History of the Technology of Civilization.
Wiley-Blackwell, Chichester.
Rao, Rajesh. 2010. Probabilistic analysis
of an ancient undeciphered script. IEEE
Computer, 43(4):76?80.
Rao, Rajesh, Nisha Yadav, Mayank Vahia,
Hrishikesh Joglekar, R. Adhikari,
and Iravatham Mahadevan. 2009.
Entropic evidence for linguistic
structure in the Indus script. Science,
324(5931):1165.
Seidl, Ursula. 1989. Die babylonischen
Kudurru- Reliefs. Symbole mesopotamischer
Gottheiten. Universita?tsverlag Freiburg,
Freiburg.
Slanski, Kathryn. 2003. The Babylonian
entitlement naru?s (kudurrus): a study in
their form and function. American Schools
of Oriental Research, Boston.
Slater, Stephen. 2002. The Complete Book of
Heraldry. Lorenz Books, London.
Sproat, Richard. 2010a. Ancient symbols,
computational linguistics, and the
reviewing practices of the general
science journals. Computational
Linguistics, 36(3):585?594.
Sproat, Richard. 2010b. Language, Technology,
and Society. Oxford University Press,
Oxford.
Vidale, Massimo. 2007. The collapse melts
down: A reply to Farmer, Sproat and
Witzel. East and West, 57:333?366.
Winn, Shan. 1990. A Neolithic sign system
in southeastern Europe. In M. L. Foster
and L. J. Botscharow, editors, The Life of
Symbols. Westview Press, Boulder, CO,
pages 269?271.
816
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 1?5,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Lexicographic Semirings for Exact Automata Encoding of Sequence Models
Brian Roark, Richard Sproat, and Izhak Shafran
{roark,rws,zak}@cslu.ogi.edu
Abstract
In this paper we introduce a novel use of the
lexicographic semiring and motivate its use
for speech and language processing tasks. We
prove that the semiring allows for exact en-
coding of backoff models with epsilon tran-
sitions. This allows for off-line optimization
of exact models represented as large weighted
finite-state transducers in contrast to implicit
(on-line) failure transition representations. We
present preliminary empirical results demon-
strating that, even in simple intersection sce-
narios amenable to the use of failure transi-
tions, the use of the more powerful lexico-
graphic semiring is competitive in terms of
time of intersection.
1 Introduction and Motivation
Representing smoothed n-gram language models as
weighted finite-state transducers (WFST) is most
naturally done with a failure transition, which re-
flects the semantics of the ?otherwise? formulation
of smoothing (Allauzen et al, 2003). For example,
the typical backoff formulation of the probability of
a word w given a history h is as follows
P(w | h) =
{
P(w | h) if c(hw) > 0
?hP(w | h?) otherwise
(1)
where P is an empirical estimate of the probabil-
ity that reserves small finite probability for unseen
n-grams; ?h is a backoff weight that ensures nor-
malization; and h? is a backoff history typically
achieved by excising the earliest word in the his-
tory h. The principle benefit of encoding the WFST
in this way is that it only requires explicitly storing
n-gram transitions for observed n-grams, i.e., count
greater than zero, as opposed to all possible n-grams
of the given order which would be infeasible in for
example large vocabulary speech recognition. This
is a massive space savings, and such an approach is
also used for non-probabilistic stochastic language
models, such as those trained with the perceptron
algorithm (Roark et al, 2007), as the means to ac-
cess all and exactly those features that should fire
for a particular sequence in a deterministic automa-
ton. Similar issues hold for other finite-state se-
quence processing problems, e.g., tagging, bracket-
ing or segmenting.
Failure transitions, however, are an implicit
method for representing a much larger explicit au-
tomaton ? in the case of n-gram models, all pos-
sible n-grams for that order. During composition
with the model, the failure transition must be inter-
preted on the fly, keeping track of those symbols
that have already been found leaving the original
state, and only allowing failure transition traversal
for symbols that have not been found (the semantics
of ?otherwise?). This compact implicit representa-
tion cannot generally be preserved when composing
with other models, e.g., when combining a language
model with a pronunciation lexicon as in widely-
used FST approaches to speech recognition (Mohri
et al, 2002). Moving from implicit to explicit repre-
sentation when performing such a composition leads
to an explosion in the size of the resulting trans-
ducer, frequently making the approach intractable.
In practice, an off-line approximation to the model
is made, typically by treating the failure transitions
as epsilon transitions (Mohri et al, 2002; Allauzen
et al, 2003), allowing large transducers to be com-
posed and optimized off-line. These complex ap-
proximate transducers are then used during first-pass
decoding, and the resulting pruned search graphs
(e.g., word lattices) can be rescored with exact lan-
guage models encoded with failure transitions.
Similar problems arise when building, say, POS-
taggers as WFST: not every pos-tag sequence will
have been observed during training, hence failure
transitions will achieve great savings in the size of
models. Yet discriminative models may include
complex features that combine both input stream
(word) and output stream (tag) sequences in a single
feature, yielding complicated transducer topologies
for which effective use of failure transitions may not
1
be possible. An exact encoding using other mecha-
nisms is required in such cases to allow for off-line
representation and optimization.
In this paper, we introduce a novel use of a semir-
ing ? the lexicographic semiring (Golan, 1999) ?
which permits an exact encoding of these sorts of
models with the same compact topology as with fail-
ure transitions, but using epsilon transitions. Unlike
the standard epsilon approximation, this semiring al-
lows for an exact representation, while also allow-
ing (unlike failure transition approaches) for off-line
composition with other transducers, with all the op-
timizations that such representations provide.
In the next section, we introduce the semiring, fol-
lowed by a proof that its use yields exact represen-
tations. We then conclude with a brief evaluation of
the cost of intersection relative to failure transitions
in comparable situations.
2 The Lexicographic Semiring
Weighted automata are automata in which the tran-
sitions carry weight elements of a semiring (Kuich
and Salomaa, 1986). A semiring is a ring that may
lack negation, with two associative operations? and
? and their respective identity elements 0 and 1. A
common semiring in speech and language process-
ing, and one that we will be using in this paper, is
the tropical semiring (R? {?},min,+,?, 0), i.e.,
min is the ? of the semiring (with identity?) and
+ is the ? of the semiring (with identity 0). This is
appropriate for performing Viterbi search using neg-
ative log probabilities ? we add negative logs along
a path and take the min between paths.
A ?W1,W2 . . .Wn?-lexicographic weight is a tu-
ple of weights where each of the weight classes
W1,W2 . . .Wn, must observe the path property
(Mohri, 2002). The path property of a semiring K
is defined in terms of the natural order on K such
that: a <K b iff a ? b = a. The tropical semiring
mentioned above is a common example of a semir-
ing that observes the path property, since:
w1 ? w2 = min{w1, w2}
w1 ? w2 = w1 + w2
The discussion in this paper will be restricted to
lexicographic weights consisting of a pair of tropi-
cal weights ? henceforth the ?T, T ?-lexicographic
semiring. For this semiring the operations ? and ?
are defined as follows (Golan, 1999, pp. 223?224):
?w1, w2? ? ?w3, w4? =
?
????
????
if w1 < w3 or
?w1, w2? (w1 = w3 &
w2 < w4)
?w3, w4? otherwise
?w1, w2? ? ?w3, w4? = ?w1 + w3, w2 + w4?
The term ?lexicographic? is an apt term for this
semiring since the comparison for ? is like the lexi-
cographic comparison of strings, comparing the first
elements, then the second, and so forth.
3 Language model encoding
3.1 Standard encoding
For language model encoding, we will differentiate
between two classes of transitions: backoff arcs (la-
beled with a ? for failure, or with  using our new
semiring); and n-gram arcs (everything else, labeled
with the word whose probability is assigned). Each
state in the automaton represents an n-gram history
string h and each n-gram arc is weighted with the
(negative log) conditional probability of the word w
labeling the arc given the history h. For a given his-
tory h and n-gram arc labeled with a word w, the
destination of the arc is the state associated with the
longest suffix of the string hw that is a history in the
model. This will depend on the Markov order of the
n-gram model. For example, consider the trigram
model schematic shown in Figure 1, in which only
history sequences of length 2 are kept in the model.
Thus, from history hi = wi?2wi?1, the word wi
transitions to hi+1 = wi?1wi, which is the longest
suffix of hiwi in the model.
As detailed in the ?otherwise? semantics of equa-
tion 1, backoff arcs transition from state h to a state
h?, typically the suffix of h of length |h| ? 1, with
weight (? log?h). We call the destination state a
backoff state. This recursive backoff topology ter-
minates at the unigram state, i.e., h = , no history.
Backoff states of order k may be traversed either
via ?-arcs from the higher order n-gram of order k+
1 or via an n-gram arc from a lower order n-gram of
order k?1. This means that no n-gram arc can enter
the zeroeth order state (final backoff), and full-order
states ? history strings of length n? 1 for a model
of order n ? may have n-gram arcs entering from
other full-order states as well as from backoff states
of history size n? 2.
3.2 Encoding with lexicographic semiring
For an LM machineM on the tropical semiring with
failure transitions, which is deterministic and has the
2
h i =wi-2wi-1 hi+1 =wi-1wiwi /-logP(wi |h i)
wi-1
?/-log ?hi
wi
?/-log ?h i+1
wi /-logP(wi|wi-1)
?/-log ?w i-1 wi /-logP(wi)
Figure 1: Deterministic finite-state representation of n-gram
models with negative log probabilities (tropical semiring). The
symbol ? labels backoff transitions. Modified from Roark and
Sproat (2007), Figure 6.1.
path property, we can simulate ?-arcs in a standard
LM topology by a topologically equivalent machine
M ? on the lexicographic ?T, T ? semiring, where ?
has been replaced with epsilon, as follows. For every
n-gram arc with label w and weight c, source state
si and destination state sj , construct an n-gram arc
with label w, weight ?0, c?, source state s?i, and des-
tination state s?j . The exit cost of each state is con-
structed as follows. If the state is non-final, ??,??.
Otherwise if it final with exit cost c it will be ?0, c?.
Let n be the length of the longest history string in
the model. For every ?-arc with (backoff) weight
c, source state si, and destination state sj repre-
senting a history of length k, construct an -arc
with source state s?i, destination state s
?
j , and weight
???(n?k), c?, where ? > 0 and ??(n?k) takes ? to
the (n ? k)th power with the ? operation. In the
tropical semiring, ? is +, so ??(n?k) = (n ? k)?.
For example, in a trigram model, if we are backing
off from a bigram state h (history length = 1) to a
unigram state, n ? k = 2 ? 0 = 2, so we set the
backoff weight to ?2?,? log?h) for some ? > 0.
In order to combine the model with another au-
tomaton or transducer, we would need to also con-
vert those models to the ?T, T ? semiring. For these
automata, we simply use a default transformation
such that every transition with weight c is assigned
weight ?0, c?. For example, given a word lattice
L, we convert the lattice to L? in the lexicographic
semiring using this default transformation, and then
perform the intersection L? ?M ?. By removing ep-
silon transitions and determinizing the result, the
low cost path for any given string will be retained
in the result, which will correspond to the path
achieved with ?-arcs. Finally we project the second
dimension of the ?T, T ? weights to produce a lattice
in the tropical semiring, which is equivalent to the
result of L ?M , i.e.,
C2(det(eps-rem(L? ?M ?))) = L ?M
where C2 denotes projecting the second-dimension
of the ?T, T ? weights, det(?) denotes determiniza-
tion, and eps-rem(?) denotes -removal.
4 Proof
We wish to prove that for any machine N ,
ShortestPath(M ? ? N ?) passes through the equiv-
alent states in M ? to those passed through in M for
ShortestPath(M ? N). Therefore determinization
of the resulting intersection after -removal yields
the same topology as intersection with the equiva-
lent ? machine. Intuitively, since the first dimension
of the ?T, T ? weights is 0 for n-gram arcs and > 0
for backoff arcs, the shortest path will traverse the
fewest possible backoff arcs; further, since higher-
order backoff arcs cost less in the first dimension of
the ?T, T ? weights in M ?, the shortest path will in-
clude n-gram arcs at their earliest possible point.
We prove this by induction on the state-sequence
of the path p/p? up to a given state si/s?i in the respec-
tive machines M/M ?.
Base case: If p/p? is of length 0, and therefore the
states si/s?i are the initial states of the respective ma-
chines, the proposition clearly holds.
Inductive step: Now suppose that p/p? visits
s0...si/s?0...s
?
i and we have therefore reached si/s
?
i
in the respective machines. Suppose the cumulated
weights of p/p? are W and ??,W ?, respectively. We
wish to show that whichever sj is next visited on p
(i.e., the path becomes s0...sisj) the equivalent state
s? is visited on p? (i.e., the path becomes s?0...s
?
is
?
j).
Let w be the next symbol to be matched leaving
states si and s?i. There are four cases to consider:
(1) there is an n-gram arc leaving states si and s?i la-
beled with w, but no backoff arc leaving the state;
(2) there is no n-gram arc labeled with w leaving the
states, but there is a backoff arc; (3) there is no n-
gram arc labeled with w and no backoff arc leaving
the states; and (4) there is both an n-gram arc labeled
with w and a backoff arc leaving the states. In cases
(1) and (2), there is only one possible transition to
take in either M or M ?, and based on the algorithm
for construction of M ? given in Section 3.2, these
transitions will point to sj and s?j respectively. Case
(3) leads to failure of intersection with either ma-
chine. This leaves case (4) to consider. In M , since
there is a transition leaving state si labeled with w,
3
the backoff arc, which is a failure transition, can-
not be traversed, hence the destination of the n-gram
arc sj will be the next state in p. However, in M ?,
both the n-gram transition labeled with w and the
backoff transition, now labeled with , can be tra-
versed. What we will now prove is that the shortest
path through M ? cannot include taking the backoff
arc in this case.
In order to emit w by taking the backoff arc out
of state s?i, one or more backoff () transitions must
be taken, followed by an n-gram arc labeled with
w. Let k be the order of the history represented
by state s?i, hence the cost of the first backoff arc
is ?(n? k)?,? log(?s?i)? in our semiring. If we
traverse m backoff arcs prior to emitting the w,
the first dimension of our accumulated cost will be
m(n? k+ m?12 )?, based on our algorithm for con-
struction of M ? given in Section 3.2. Let s?l be the
destination state after traversing m backoff arcs fol-
lowed by an n-gram arc labeled with w. Note that,
by definition, m ? k, and k ? m + 1 is the or-
der of state s?l. Based on the construction algo-
rithm, the state s?l is also reachable by first emit-
ting w from state s?i to reach state s
?
j followed by
some number of backoff transitions. The order of
state s?j is either k (if k is the highest order in the
model) or k + 1 (by extending the history of state
s?i by one word). If it is of order k, then it will re-
quire m? 1 backoff arcs to reach state s?l, one fewer
than the path to state s?l that begins with a back-
off arc, for a total cost of (m? 1)(n? k + m?12 )?
which is less than m(n? k + m?12 )?. If state
s?j is of order k + 1, there will be m backoff
arcs to reach state s?l, but with a total cost of
m(n? (k + 1) + m?12 )? = m(n? k +
m?3
2 )?
which is also less than m(n? k + m?12 )?. Hence
the state s?l can always be reached from s
?
i with a
lower cost through state s?j than by first taking the
backoff arc from s?i. Therefore the shortest path on
M ? must follow s?0...s
?
is
?
j . 2
This completes the proof.
5 Experimental Comparison of , ? and
?T, T ? encoded language models
For our experiments we used lattices derived from a
very large vocabulary continuous speech recognition
system, which was built for the 2007 GALE Ara-
bic speech recognition task, and used in the work
reported in Lehr and Shafran (2011). The lexico-
graphic semiring was evaluated on the development
set (2.6 hours of broadcast news and conversations;
18K words). The 888 word lattices for the develop-
ment set were generated using a competitive base-
line system with acoustic models trained on about
1000 hrs of Arabic broadcast data and a 4-gram lan-
guage model. The language model consisting of
122M n-grams was estimated by interpolation of 14
components. The vocabulary is relatively large at
737K and the associated dictionary has only single
pronunciations.
The language model was converted to the automa-
ton topology described earlier, and represented in
three ways: first as an approximation of a failure
machine using epsilons instead of failure arcs; sec-
ond as a correct failure machine; and third using the
lexicographic construction derived in this paper.
The three versions of the LM were evaluated by
intersecting them with the 888 lattices of the de-
velopment set. The overall error rate for the sys-
tems was 24.8%?comparable to the state-of-the-
art on this task1. For the shortest paths, the failure
and lexicographic machines always produced iden-
tical lattices (as determined by FST equivalence);
in contrast, 81% of the shortest paths from the ep-
silon approximation are different, at least in terms
of weights, from the shortest paths using the failure
LM. For full lattices, 42 (4.7%) of the lexicographic
outputs differ from the failure LM outputs, due to
small floating point rounding issues; 863 (97%) of
the epsilon approximation outputs differ.
In terms of size, the failure LM, with 5.7 mil-
lion arcs requires 97 Mb. The equivalent ?T, T ?-
lexicographic LM requires 120 Mb, due to the dou-
bling of the size of the weights.2 To measure speed,
we performed the intersections 1000 times for each
of our 888 lattices on a 2993 MHz Intel R? Xeon R?
CPU, and took the mean times for each of our meth-
ods. The 888 lattices were processed with a mean
of 1.62 seconds in total (1.8 msec per lattice) us-
ing the failure LM; using the ?T, T ?-lexicographic
LM required 1.8 seconds (2.0 msec per lattice), and
is thus about 11% slower. Epsilon approximation,
where the failure arcs are approximated with epsilon
arcs took 1.17 seconds (1.3 msec per lattice). The
1The error rate is a couple of points higher than in Lehr and
Shafran (2011) since we discarded non-lexical words, which are
absent in maximum likelihood estimated language model and
are typically augmented to the unigram backoff state with an
arbitrary cost, fine-tuned to optimize performance for a given
task.
2If size became an issue, the first dimension of the ?T, T ?-
weight can be represented by a single byte.
4
slightly slower speeds for the exact method using the
failure LM, and ?T, T ? can be related to the over-
head of computing the failure function at runtime,
and determinization, respectively.
6 Conclusion
In this paper we have introduced a novel applica-
tion of the lexicographic semiring, proved that it
can be used to provide an exact encoding of lan-
guage model topologies with failure arcs, and pro-
vided experimental results that demonstrate its ef-
ficiency. Since the ?T, T ?-lexicographic semiring
is both left- and right-distributive, other optimiza-
tions such as minimization are possible. The par-
ticular ?T, T ?-lexicographic semiring we have used
here is but one of many possible lexicographic en-
codings. We are currently exploring the use of a
lexicographic semiring that involves different semir-
ings in the various dimensions, for the integration of
part-of-speech taggers into language models.
An implementation of the lexicographic semir-
ing by the second author is already available as
part of the OpenFst package (Allauzen et al, 2007).
The methods described here are part of the NGram
language-model-training toolkit, soon to be released
at opengrm.org.
Acknowledgments
This research was supported in part by NSF Grant
#IIS-0811745 and DARPA grant #HR0011-09-1-
0041. Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of
the authors and do not necessarily reflect the views
of the NSF or DARPA. We thank Maider Lehr for
help in preparing the test data. We also thank the
ACL reviewers for valuable comments.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 40?47.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Twelfth International
Conference on Implementation and Application of Au-
tomata (CIAA 2007), Lecture Notes in Computer Sci-
ence, volume 4793, pages 11?23, Prague, Czech Re-
public. Springer.
Jonathan Golan. 1999. Semirings and their Applications.
Kluwer Academic Publishers, Dordrecht.
Werner Kuich and Arto Salomaa. 1986. Semirings,
Automata, Languages. Number 5 in EATCS Mono-
graphs on Theoretical Computer Science. Springer-
Verlag, Berlin, Germany.
Maider Lehr and Izhak Shafran. 2011. Learning a dis-
criminative weighted finite-state transducer for speech
recognition. IEEE Transactions on Audio, Speech, and
Language Processing, July.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech and Language,
16(1):69?88.
Mehryar Mohri. 2002. Semiring framework and algo-
rithms for shortest-distance problems. Journal of Au-
tomata, Languages and Combinatorics, 7(3):321?350.
Brian Roark and Richard Sproat. 2007. Computational
Approaches to Morphology and Syntax. Oxford Uni-
versity Press, Oxford.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech and Language, 21(2):373?392.
5
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 61?66,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
The OpenGrm open-source finite-state grammar software libraries
Brian Roark? Richard Sproat?? Cyril Allauzen? Michael Riley? Jeffrey Sorensen? & Terry Tai?
?Oregon Health & Science University, Portland, Oregon ?Google, Inc., New York
Abstract
In this paper, we present a new collection
of open-source software libraries that pro-
vides command line binary utilities and library
classes and functions for compiling regular
expression and context-sensitive rewrite rules
into finite-state transducers, and for n-gram
language modeling. The OpenGrm libraries
use the OpenFst library to provide an efficient
encoding of grammars and general algorithms
for building, modifying and applying models.
1 Introduction
The OpenGrm libraries1 are a (growing) collec-
tion of open-source software libraries for build-
ing and applying various kinds of formal gram-
mars. The C++ libraries use the OpenFst library2
for the underlying finite-state representation, which
allows for easy inspection of the resulting grammars
and models, as well as straightforward combination
with other finite-state transducers. Like OpenFst,
there are easy-to-use command line binaries for fre-
quently used operations, as well as a C++ library
interface, allowing library users to create their own
algorithms from the basic classes and functions pro-
vided.
The libraries can be used for a range of com-
mon string processing tasks, such as text normal-
ization, as well as for building and using large sta-
tistical models for applications like speech recogni-
tion. In the rest of the paper, we will present each of
the two libraries, starting with the Thrax grammar
compiler and then the NGram library. First, though,
we will briefly present some preliminary (infor-
mal) background on weighted finite-state transduc-
ers (WFST), just as needed for this paper.
1http://www.opengrm.org/
2http://www.openfst.org/
2 Informal WFST preliminaries
A weighted finite-state transducer consists of a set
of states and transitions between states. There is an
initial state and a subset of states are final. Each tran-
sition is labeled with an input symbol from an input
alphabet; an output symbol from an output alpha-
bet; an origin state; a destination state; and a weight.
Each final state has an associated final weight. A
path in the WFST is a sequence of transitions where
each transition?s destination state is the next transi-
tion?s origin state. A valid path through the WFST is
a path where the origin state of the first transition is
an initial state, and the the last transition is to a final
state. Weights combine along the path according to
the semiring of the WFST.
If every transition in the transducer has the same
input and output symbol, then the WFST represents
a weighted finite-state automaton. In the OpenFst
library, there are a small number of special sym-
bols that can be used. The  symbol represents the
empty string, which allows the transition to be tra-
versed without consuming any symbol. The ? (or
failure) symbol on a transition also allows it to be
traversed without consuming any symbol, but it dif-
fers from  in only allowing traversal if the symbol
being matched does not label any other transition
leaving the same state, i.e., it encodes the semantics
of otherwise, which is useful for language models.
For a more detailed presentation of WFSTs, see Al-
lauzen et al (2007).
3 The Thrax Grammar Compiler
The Thrax grammar compiler3 compiles grammars
that consist of regular expressions, and context-
dependent rewrite rules, into FST archives (fars) of
weighted finite state transducers. Grammars may
3The compiler is named after Dionysius Thrax (170?
90BCE), the reputed first Greek grammarian.
61
be split over multiple files and imported into other
grammars. Strings in the rules may be parsed
in one of three different ways: as a sequence of
bytes (the default), as utf8 encodings, or accord-
ing to a user-provided symbol table. With the
--save symbols flag, the transducers can be
saved out into fars with appropriate symbol tables.
The Thrax libraries provide full support for dif-
ferent weight (semiring) classes. The command-line
flag --semiring allows one to set the semiring,
currently to one of: tropical (default), log or log64
semirings.
3.1 General Description
Thrax revolves around rules which, typically, con-
struct an FST based on a given input. In the simplest
case, we can just provide a string that represents a
(trivial) transducer and name it using the assignment
operator:
pear = "pear";
In this example, we have an FST consisting of the
characters ?p?, ?e?, ?a?, and ?r? in a chain, assigned
to the identifier pear:
This identifier can be used later in order to build
further FSTs, using built-in operators or using cus-
tom functions:
kiwi = "kiwi";
fruits = pear | kiwi; # union
In Thrax, string FSTs are enclosed by double-quotes
(") whereas simple strings (often used as pathnames
for functions) are enclosed in single-quotes (?).
Thrax provides a set of built-in functions that
aid in the construction of more complex expres-
sions. We have already seen the disjunction ?|? in
the previous example. Other standard regular op-
erations are expr*, expr+, expr? and expr{m,n},
the latter repeating expr between m and n times,
inclusive. Composition is notated with ?@? so
that expr1 @ expr2 denotes the composition of
expr1 and expr2. Rewriting is denoted with ?:?
where expr1 : expr2 rewrites strings that match
expr1 into expr2. Weights can be added to expres-
sions using the notation ?<>?: thus, expr<2.4>
adds weight 2.4 to expr. Various operations on
FSTs are also provided by built-in functions, includ-
ing Determinize, Minimize, Optimize and
Invert, among many others.
3.2 Detailed Description
A Thrax grammar consists of a set of one or more
source files, each of which must have the extension
.grm. The compiler compiles each source file to a
single FST archive with the extension .far. Each
grammar file has sections: Imports and Body, each
of which is optional. The body section can include
statements interleaved with functions, as specified
below. Comments begin with a single pound sign
(#) and last until the next newline.
3.2.1 Imports
The Thrax compiler compiles source files (with
the extension .grm) into FST archive files (with
the extension .far). FST archives are an Open-
Fst storage format for a series of one or more FSTs.
The FST archive and the original source file then
form a pair which can be imported into other source
files, allowing a Python-esque include system that is
hopefully familiar to many. Instead of working with
a monolithic file, Thrax allows for a modular con-
struction of the final rule set as well as sharing of
common elements across projects.
3.2.2 Functions
Thrax has extensive support for functions that can
greatly augment the capabilities of the language.
Functions in Thrax can be specified in two ways.
The first is inline via the func keyword within grm
files. These functions can take any number of input
arguments and must return a single result (usually an
FST) to the caller via the return keyword:
func DumbPluralize[fst] {
# Concatenate with "s"...
result = fst "s";
# ...and then return to caller.
return result;
}
Alternatively, functions can be written C++ and
added to the language. Regardless of the func-
tion implementation method (inline in Thrax or
subclassed in C++), functions are integrated into
the Thrax environment and can be called directly
by using the function name and providing the
necessary arguments. Thus, assuming someone has
written a function called NetworkPluralize
that retrieves the plural of a word from some web-
site, one could write a grammar fragment as follows:
62
apple = "apple";
plural_apple = DumbPluralize[apple];
plural_tomato = NetworkPluralize[
"tomato",
?http://server:port/...?];
3.2.3 Statements
Functions can be interleaved with grammar state-
ments that generate the FSTs that are exported to the
FST archive as output. Each statement consists of an
assignment terminating with a semicolon:
foo = "abc";
export bar = foo | "xyz";
Statements preceded with the export keyword will
be written to the final output archive. Statements
lacking this keyword define temporaries that be used
later, but are themselves not output.
The basic elements of any grammar are string
FSTs, which, as mentioned earlier, are defined by
text enclosed by double quotes ("), in contrast to
raw strings, which are enclosed by single quotes (?).
String FSTs can be parsed in one of three ways,
which are denoted using a dot (.) followed by ei-
ther byte, utf8, or an identifier holding a symbol ta-
ble. Note that within strings, the backslash character
(\) is used to escape the next character. Of partic-
ular note, ?\n? translates into a newline, ?\r? into
a line feed, and ?\t? into the tab character. Literal
left and right square brackets also need escaping, as
they are used to generate symbols (see below). All
other characters following the backslash are unin-
terpreted, so that we can use \? and \? to insert an
actual quote (double) quote symbol instead of termi-
nating the string.
Strings, by default, are interpreted as sequences
of bytes, each transition of the resulting FST
corresponding to a single 1-byte character of the
input. This can be specified either by leaving off the
parse mode ("abc") or by explicitly using the byte
mode ("abc".byte). The second way is to use
UTF8 parsing by using the special keyword, e.g.:
Finally, we can load a symbol table and split
the string using the fst field separator flag
(found in fst/src/lib/symbol-table.cc)
and then perform symbol table lookups. Symbol ta-
bles can be loaded using the SymbolTable built-in
function:
arctic_symbol_table =
SymbolTable[?/path/to/bears.symtab?];
pb = "polar bear".arctic_symbol_table;
One can also create temporary symbols on the
fly by enclosing a symbol name inside brackets
within an FST string. All of the text inside the
brackets will be taken to be part of the symbol
name, and future encounters of the same symbol
name will map to the same label. By default, la-
bels use ?Private Use Area B? of the unicode ta-
ble (0x100000 - 0x10FFFD), except that the last two
code points 0x10FFFC and 0x10FFFD are reserved
for the ?[BOS]? and ?[EOS]? tags discussed below.
cross_pos = "cross" ("" : "_[s_noun]");
pluralize_nouns = "_[s_noun]" : "es";
3.3 Standard Library Functions and
Operations
Built-in functions are provided that operate on FSTs
and perform most of the operations that are available
in the OpenFst library. These include: closure, con-
catenation, difference, composition and union. In
most cases the notation of these functions follows
standard conventions. Thus, for example, for clo-
sure, the following syntax applies: fst* (accepts fst
0 or more times); fst+ (accepts fst 1 or more times);
fst? (accepts fst 0 or 1 times) fst{x,y} (accepts fst at
least x but no more than y times).
The operator ?@? is used for composition: a @
b denotes a composed with b. A ?:? is used to de-
note rewrite, where a : b denotes a transducer
that deletes a and inserts b. Most functions can also
be expressed using functional notation:
b = Rewrite["abc", "def"];
The delimiters< and> add a weight to an expres-
sion in the chosen semiring: a<3> adds the weight
3 (in the tropical semiring by default) to a.
Functions lacking operators (hence only called
by function name) include: ArcSort, Connect,
Determinize, RmEpsilon, Minimize,
Optimize, Invert, Project and Reverse.
Most of these call the obvious underlying OpenFst
function.
One function in particular, CDRewrite is worth
further discussion. This function takes a transducer
and two context acceptors (and the alphabet ma-
chine), and generates a new FST that performs a
context dependent rewrite everywhere in the pro-
vided contexts. The context-dependent rewrite algo-
rithm used is that of Mohri and Sproat (1996), and
63
see also Kaplan and Kay (1994). The fourth argu-
ment (sigma star) needs to be a minimized ma-
chine. The fifth argument selects the direction of
rewrite; we can either rewrite left-to-right or right-
to-left or simultaneously. The sixth argument selects
whether the rewrite is optional.
CDRewrite[tau, lambda, rho,
sigma_star,
?ltr?|?rtl?|?sim?,
?obl?|?opt?]
For context-dependent rewrite rules, two built-in
symbols ?[BOS]? and ?[EOS]? have a special mean-
ing in the context specifications: they refer to the
beginning and end of string, respectively.
There are also built-in functions that perform
other tasks. In the interest of space we concentrate
here on the StringFile function, which loads a
file consisting of a list of strings, or tab-separated
pairs of strings, and compiles them to an acceptor
that represents the union of the strings.
StringFile[?strings_file?]
While it is equivalent to the union of the individual
string (pairs), StringFile uses an efficient algo-
rithm for constructing a prefix tree (trie) from the
list and can be significantly more efficient than com-
puting a union for large lists. If a line consists of a
tab-separated pair of strings a, b, a transducer equiv-
alent to Rewrite[a, b] is compiled.
The optional keywords byte (default), utf8 or
the name of a symbol table can be used to specify
the parsing mode for the strings. Thus
StringFile[?strings_file?, utf8, my_symtab]
would parse a sequence of tab-separated pairs, using
utf8 parsing for the left-hand string, and the symbol
table my symtab for the right-hand string.
4 NGram Library
The OpenGrm NGram library contains tools for
building, manipulating and using n-gram language
models represented as weighted finite-state trans-
ducers. The same finite-state topology is used to en-
code raw counts as well as smoothed models. Here
we briefly present this structure, followed by details
on the operations manipulating it.
An n-gram is a sequence of n symbols: w1 . . . wn.
Each state in the model represents a prefix history
of the n-gram (w1 . . . wn?1), and transitions in the
model represent either n-grams or backoff transi-
tions following that history. Figure 1 lists conven-
tions for states and transitions used to encode the
n-grams as a WFST.
This representation is similar to that used in other
WFST-based n-gram software libraries, such as the
AT&T GRM library (Allauzen et al, 2005). One
key difference is the implicit representation of <s>
and </s>, as opposed to encoding them as symbols
in the grammar. This has the benefit of including all
start and stop symbol functionality while avoiding
common pitfalls that arise with explicit symbols.
Another difference from the GRM library repre-
sentation is explicit inclusion of failure links from
states to their backoff states even in the raw count
files. The OpenGrm n-gram FST format is consis-
tent through all stages of building the models, mean-
ing that model manipulation (e.g., merging of two
Figure 1: List of state and transition conventions used to encode collection of n-grams in WFST.
An n-gram is a sequence of n symbols: w1 . . . wn. Its proper prefixes include all sequences w1 . . . wk for k < n.
? There is a unigram state in every model, representing the empty string.
? Every proper prefix of every n-gram in the model has an associated state in the model.
? The state associated with an n-gram w1...wn has a backoff transition (labeled with ) to the state associated
with its suffix w2...wn.
? An n-gram w1...wn is represented as a transition, labeled with wn, from the state associated with its prefix
w1...wn?1 to a destination state defined as follows:
? If w1...wn is a proper prefix of an n-gram in the model, then the destination of the transition is the state
associated with w1...wn
? Otherwise, the destination of the transition is the state associated with the suffix w2...wn.
? Start and end of the sequence are not represented via transitions in the automaton or symbols in the symbol
table. Rather
? The start state of the automaton encodes the ?start of sequence? n-gram prefix (commonly denoted<s>).
? The end of the sequence (often denoted </s>) is included in the model through state final weights, i.e.,
for a state associated with an n-gram prefix w1...wn, the final weight of that state represents the weight
of the n-gram w1...wn</s>.
64
(a)
?
?
?
a/0
a/-1.1
b/-1.1
b/0
b/-0.69
a/-0.69
0
0
(b)
?/0.69
?/0.916
a/0.6
a/0.Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 364?369,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Hippocratic Abbreviation Expansion
Brian Roark and Richard Sproat
Google, Inc, 79 Ninth Avenue, New York, NY 10011
{roark,rws}@google.com
Abstract
Incorrect normalization of text can be par-
ticularly damaging for applications like
text-to-speech synthesis (TTS) or typing
auto-correction, where the resulting nor-
malization is directly presented to the user,
versus feeding downstream applications.
In this paper, we focus on abbreviation
expansion for TTS, which requires a ?do
no harm?, high precision approach yield-
ing few expansion errors at the cost of
leaving relatively many abbreviations un-
expanded. In the context of a large-
scale, real-world TTS scenario, we present
methods for training classifiers to establish
whether a particular expansion is apt. We
achieve a large increase in correct abbrevi-
ation expansion when combined with the
baseline text normalization component of
the TTS system, together with a substan-
tial reduction in incorrect expansions.
1 Introduction
Text normalization (Sproat et al, 2001) is an im-
portant initial phase for many natural language and
speech applications. The basic task of text normal-
ization is to convert non-standard words (NSWs)
? numbers, abbreviations, dates, etc. ? into stan-
dard words, though depending on the task and the
domain a greater or lesser number of these NSWs
may need to be normalized. Perhaps the most de-
manding such application is text-to-speech synthe-
sis (TTS) since, while for parsing, machine trans-
lation and information retrieval it may be accept-
able to leave such things as numbers and abbre-
viations unexpanded, for TTS all tokens need to
be read, and for that it is necessary to know how
to pronounce them. Which normalizations are re-
quired depends very much on the application.
What is also very application-dependent is the
cost of errors in normalization. For some applica-
tions, where the normalized string is an interme-
diate stage in a larger application such as trans-
lation or information retrieval, overgeneration of
normalized alternatives is often a beneficial strat-
egy, to the extent that it may improve the accu-
racy of what is eventually being presented to the
user. In other applications, such as TTS or typing
auto-correction, the resulting normalized string it-
self is directly presented to the user; hence errors
in normalization can have a very high cost relative
to leaving tokens unnormalized.
In this paper we concentrate on abbreviations,
which we define as alphabetic NSWs that it would
be normal to pronounce as their expansion. This
class of NSWs is particularly common in personal
ads, product reviews, and so forth. For example:
home health care svcs stat home health llc
osceola aquatic ctr stars rating write
audi vw repair ser quality and customer
Each of the examples above contains an abbrevi-
ation that, unlike, e.g., conventionalized state ab-
breviations such as ca for California, is either only
slightly standard (ctr for center) or not standard at
all (ser for service).
An important principle in text normalization for
TTS is do no harm. If a system is unable to re-
liably predict the correct reading for a string, it is
better to leave the string alone and have it default
to, say, a character-by-character reading, than to
expand it to something wrong. This is particularly
true in accessibility applications for users who rely
on TTS for most or all of their information needs.
Ideally a navigation system should read turn on
30N correctly as turn on thirty north; but if it can-
not resolve the ambiguity in 30N, it is far better to
read it as thirty N than as thirty Newtons, since lis-
teners can more easily recover from the first kind
of error than the second.
We present methods for learning abbreviation
expansion models that favor high precision (incor-
rect expansions < 2%). Unannotated data is used
to collect evidence for contextual disambiguation
and to train an abbreviation model. Then a small
amount of annotated data is used to build models
to determine whether to accept a candidate expan-
364
sion of an abbreviation based on these features.
The data we report on are taken from Google
Maps
TM
and web pages associated with its map en-
tries, but the methods can be applied to any data
source that is relatively abbreviation rich.
We note in passing that similar issues arise
in automatic spelling correction work (Wilcox-
O?Hearn et al, 2008), where it is better to leave
a word alone than to ?correct? it wrongly.
2 Related work
There has been a lot of interest in recent years on
?normalization? of social media such as Twitter,
but that work defines normalization much more
broadly than we do here (Xia et al, 2006; Choud-
hury et al, 2007; Kobus et al, 2008; Beaufort et
al., 2010; Kaufmann, 2010; Liu et al, 2011; Pen-
nell and Liu, 2011; Aw and Lee, 2012; Liu et al,
2012a; Liu et al, 2012b; Hassan and Menezes,
2013; Yang and Eisenstein, 2013). There is a good
reason for us to focus more narrowly. For Twit-
ter, much of the normalization task involves non-
standard language such as ur website suxx brah
(from Yang and Eisenstein (2013)). Expanding the
latter to your website sucks, brother certainly nor-
malizes it to standard English, but one could argue
that in so doing one is losing information that the
writer is trying to convey using an informal style.
On the other hand, someone who writes svc ctr
for service center in a product review is probably
merely trying to save time and so expanding the
abbreviations in that case is neutral with respect to
preserving the intent of the original text.
One other difference between the work we re-
port from much of the recent work cited above is
that that work focuses on getting high F scores,
whereas we are most concerned with getting high
precision. While this may seem like a trivial
trade off between precision and recall, our goal
motivates developing measures that minimize the
?risk? of expanding a term, something that is im-
portant in an application such as TTS, where one
cannot correct a misexpansion after it is spoken.
3 Methods
Since our target application is text-to-speech, we
define the task in terms of an existing TTS lexi-
con. If a word is already in the lexicon, it is left
unprocessed, since there is an existing pronuncia-
tion for it; if a word is out-of-vocabulary (OOV),
we consider expanding it to a word in the lexicon.
We consider a possible expansion for an abbrevi-
ation to be any word in the lexicon from which
the abbreviation can be derived by only deletion of
letters.
1
For present purposes we use the Google
English text-to-speech lexicon, consisting of over
430 thousand words. Given an OOV item (possi-
ble abbreviation) in context, we make use of fea-
tures of the context and of the OOV item itself to
enumerate and score candidate expansions.
Our data consists of 15.1 billion words of text
data from Google Maps
TM
, lower-cased and tok-
enized to remove punctuation symbols. We used
this data in several ways. First, we used it to boot-
strap a model for assigning a probability of an ab-
breviation/expansion pair. Second, we used it to
extract contextual n-gram features for predicting
possible expansions. Finally, we sampled just over
14 thousand OOV items in context and had them
manually labeled with a number of categories, in-
cluding ?abbreviation?. OOVs labeled as abbrevia-
tions were also labeled with the correct expansion.
We present each of these uses in turn.
3.1 Abbreviation modeling
We collect potential abbreviation/full-word pairs
by looking for terms that could be abbreviations
of full words that occur in the same context. Thus:
the svc/service center
heating clng/cooling system
dry clng/cleaning system
contributes evidence that svc is an abbreviation
of service. Similarly instances of clng in con-
texts that can contain cooling or cleaning are evi-
dence that clng could be an abbreviation of either
of these words. (The same contextual information
of course is used later on to disambiguate which
of the expansions is appropriate for the context.)
To compute the initial guess as to what can be a
possible abbreviation, a Thrax grammar (Roark et
al., 2012) is used that, among other things, speci-
fies that: the abbreviation must start with the same
letter as the full word; if a vowel is deleted, all ad-
jacent vowels should also be deleted; consonants
may be deleted in a cluster, but not the last one;
and a (string) suffix may be deleted.
2
We count
a pair of words as ?co-occurring? if they are ob-
served in the same context. For a given context C,
e.g., the center, letW
C
be the set of words found
in that context. Then, for any pair of words u, v,
we can assign a pair count based on the count of
contexts where both occur:
c(u, v) = |{C : u ?W
C
and v ?W
C
}|
1
We do not deal here with phonetic spellings in abbrevia-
tions such as 4get, or cases where letters have been transposed
due to typographical errors (scv).
2
This Thrax grammar can be found at
http://openfst.cs.nyu.edu/twiki/bin/
view/Contrib/ThraxContrib
365
blvd boulevard rd road yrs years
ca california fl florida ctr center
mins minutes def definitely ste suite
Table 1: Examples of automatically mined abbrevia-
tion/expansion pairs.
Let c(u) be defined as
?
v
c(u, v). From these
counts, we can define a 2?2 table and calculate
statistics such as the log likelihood statistic (Dun-
ning, 1993), which we use to rank possible abbre-
viation/expansion pairs. Scores derived from these
type (rather than token) counts highly rank pairs of
in-vocabulary words and OOV possible abbrevia-
tions that are substitutable in many contexts.
We further filter the potential abbreviations by
removing ones that have a lot of potential expan-
sions, where we set the cutoff at 10. This removes
mostly short abbreviations that are highly ambigu-
ous. The resulting ranked list of abbreviation ex-
pansion pairs is then thresholded before building
the abbreviation model (see below) to provide a
smaller but more confident training set. For this
paper, we used 5-gram contexts (two words on ei-
ther side) to extract abbreviations and their expan-
sions. See Table 1 for some examples.
Our abbreviation model is a pair character lan-
guage model (LM), also known as a joint multi-
gram model (Bisani and Ney, 2008), whereby
aligned symbols are treated as a single token and
a smoothed n-gram model is estimated. This de-
fines a joint distribution over input and output
sequences, and can be efficiently encoded as a
weighted finite-state transducer. The extracted
abbreviation/expansion pairs are character-aligned
and a 7-gram pair character LM is built over
the alignments using the OpenGrm n-gram library
(Roark et al, 2012). For example:
c:c :e :n t:t :e r:r
Note that, as we?ve defined it, the alignments from
abbreviation to expansion allow only identity and
insertion, no deletions or substitutions. The cost
from this LM, normalized by the length of the ex-
pansion, serves as a score for the quality of a pu-
tative expansion for an abbreviation.
For a small set of frequent, conventionalized
abbreviations (e.g., ca for California ? 63 pairs
in total ? mainly state abbreviations and similar
items), we assign an fixed pair LM score, since
these examples are in effect irregular cases, where
the regularities of the productive abbreviation pro-
cess do not capture their true cost.
3.2 Contextual features
To predict the expansion given the context, we ex-
tract n-gram observations for full words in the TTS
lexicon. We do this in two ways. First, we sim-
ply train a smoothed n-gram LM from the data.
Because of the size of the data set, this is heav-
ily pruned using relative entropy pruning (Stolcke,
1998). Second, we use log likelihood and log odds
ratios (this time using standardly defined n-gram
counts) to extract reliable bigram and trigram con-
texts for words. Space precludes a detailed treat-
ment of these two statistics, but, briefly, both can
be derived from contingency table values calcu-
lated from the frequencies of (1) the word in the
particular context; (2) the word in any context; (3)
the context with any word; and (4) all words in
the corpus. See Agresti (2002), Dunning (1993)
and Monroe et al (2008) for useful overviews of
how to calculate these and other statistics to de-
rive reliable associations. In our case, we use them
to derive associations between contexts and words
occuring in those contexts. The contexts include
trigrams with the target word in any of the three
positions, and bigrams with the target word in ei-
ther position. We filter the set of n-grams based on
both their log likelihood and log odds ratios, and
provide those scores as features.
3.3 Manual annotations
We randomly selected 14,434 OOVs in their full
context, and had them manually annotated as
falling within one of 8 categories, along with the
expansion if the category was ?abbreviation?. Note
that these are relatively lightweight annotations
that do not require extensive linguistics expertise.
The abbreviation class is defined as cases where
pronouncing as the expansion would be normal.
Other categories included letter sequence (expan-
sion would not be normal, e.g., TV); partial let-
ter sequence (e.g., PurePictureTV); misspelling;
leave as is (part of a URL or pronounced as a
word, e.g., NATO); foreign; don?t know; and junk.
Abbreviations accounted for nearly 23% of the
cases, and about 3/5 of these abbreviations were
instances from the set of 63 conventional abbrevi-
ation/expansion pairs mentioned in Section 3.1.
3.4 Abbreviation expansion systems
We have three base systems that we compare here.
The first is the hand-built TTS normalization sys-
tem. This system includes some manually built
patterns and an address parser to find common ab-
breviations that occur in a recognizable context.
For example, the grammar covers several hundred
city-state combinations, such as Fairbanks AK,
yielding good performance on such cases.
The other two systems were built using data ex-
tracted as described above. Both systems make
use of the pair LM outlined in Section 3.1, but
differ in how they model context. The first sys-
366
tem, which we call ?N-gram?, uses a pruned Katz
(1987) smoothed trigram model. The second sys-
tem, which we call ?SVM?, uses a Support Vec-
tor Machine (Cortes and Vapnik, 1995) to classify
candidate expansions as being correct or not. For
both systems, for any given input OOV, the pos-
sible expansion with the highest score is output,
along with the decision of whether to expand.
For the ?N-gram? system, n-gram negative log
probabilities are extracted as follows. Let w
i
be
the position of the target expansion. We extract the
part of the n-gram probability of the string that is
not constant across all competing expansions, and
normalize by the number of words in that window.
Thus the score of the word is:
S(w
i
) = ?
1
k + 1
i+k
?
j=i
log P(w
j
| w
j?1
w
j?2
)
In our experiments, k = 2 since we have a trigram
model, though in cases where the target word is the
last word in the string, k = 1, because there only
the end-of-string symbol must be predicted in ad-
dition to the expansion. We then take the Bayesian
fusion of this model with the pair LM, by adding
them in the log space, to get prediction from both
the context and abbreviation model.
For the ?SVM? model, we extract features from
the log likelihood and log odds scores associated
with contextual n-grams, as well as from the pair
LM probability and characteristics of the abbrevi-
ation itself. We train a linear model on a subset of
the annotated data (see section 4). Multiple con-
textual n-grams may be observed, and we take the
maximum log likelihood and log odds scores for
each candidate expansion in the observed context.
We then quantize these scores down into 16 bins,
using the histogram in the training data to define
bin thresholds so as to partition the training in-
stances evenly. We also create 16 bins for the pair
LM score. A binary feature is defined for each
bin that is set to 1 if the current candidate?s score
is less than the threshold of that bin, otherwise 0.
Thus multiple bin features can be active for a given
candidate expansion of the abbreviation.
We also have features that fire for each type of
contextual feature (e.g., trigram with expansion as
middle word, etc.), including ?no context?, where
none of the trigrams or bigrams from the current
example that include the candidate expansion are
present in our list. Further, we have features for
the length of the abbreviation (shorter abbrevia-
tions have more ambiguity, hence are more risky
to expand); membership in the list of frequent,
conventionalized abbreviations mentioned earlier;
and some combinations of these, along with bias
features. We train the model using standard op-
tions with Google internal SVM training tools.
Note that the number of n-grams in the two
models differs. The N-gram system has around
200M n-grams after pruning; while the SVM
model uses around a quarter of that. We also tried
a more heavily pruned n-gram model, and the re-
sults are only very slightly worse, certainly accept-
able for a low-resource scenario.
4 Experimental Results
We split the 3,209 labeled abbreviations into a
training set of 2,209 examples and a held aside de-
velopment set of 1,000 examples. We first evaluate
on the development set, then perform a final 10-
fold cross validation over the entire set of labeled
examples. We evaluate in terms of the percent-
age of abbreviations that were correctly expanded
(true positives, TP) and that were incorrectly ex-
panded (false positives, FP).
Results are shown in Table 2. The first two rows
show the baseline TTS system and SVM model.
On the development set, both systems have a false
positive rate near 3%, i.e., three abbreviations are
expanded incorrectly for every 100 examples; and
over 50% true positive rate, i.e., more than half of
the abbreviations are expanded correctly. To re-
port true and false positive rates for the N-gram
system we would need to select an arbitrary de-
cision threshold operating point, unlike the deter-
ministic TTS baseline and the SVM model with
its decision threshold of 0. Rather than tune such a
meta-parameter to the development set, we instead
present an ROC curve comparison of the N-gram
and SVM models, and then propose a method
for ?intersecting? their output without requiring a
tuned decision threshold.
Figure 1 presents an ROC curve for the N-gram
and SVM systems, and for the simple Bayesian
fusion (sum in log space) of their scores. We can
see that the SVM model has very high precision
for its highest ranked examples, yielding nearly
20% of the correct expansions without any in-
correct expansions. However the N-gram system
achieves higher true positive rates when the false
Percent of abbreviations
dev set full set
System TP FP TP FP
TTS baseline 55.0 3.1 40.0 3.0
SVM model 52.6 3.3 53.3 2.6
SVM ? N-gram 50.6 1.1 50.3 0.9
SVM ? N-gram, then TTS 73.5 1.9 74.5 1.5
Table 2: Results on held-out labeled data, and with final
10-fold cross-validation over the entire labeled set. Percent-
age of abbreviations expanded correctly (TP) and percentage
expanded incorrectly (FP) are reported for each system.
367
C
o
r
r
e
c
t
e
x
p
a
n
s
i
o
n
p
e
r
c
e
n
t
a
g
e
(
T
P
)
0 1 2 3 4
0
10
20
30
40
50
60
N-?gram SVM SVM ?+ ?N-?gram SVM ?intersect ?N-?gram
Incorrect ?expansion ?percentage ?(FP)
C
o
r
r
e
c
t
 ?
e
x
p
a
n
s
i
o
n
 ?
p
e
r
c
e
n
t
a
g
e
 ?
(
T
P
)
F
i
g
u
r
e
1
:
R
O
C
c
u
r
v
e
p
l
o
t
t
i
n
g
t
r
u
e
p
o
s
i
t
i
v
e
(
c
o
r
r
e
c
t
e
x
p
a
n
-
s
i
o
n
)
p
e
r
c
e
n
t
a
g
e
s
v
e
r
s
u
s
f
a
l
s
e
p
o
s
i
t
i
v
e
(
i
n
c
o
r
r
e
c
t
e
x
p
a
n
s
i
o
n
)
p
e
r
c
e
n
t
a
g
e
s
f
o
r
s
e
v
e
r
a
l
s
y
s
t
e
m
s
o
n
t
h
e
d
e
v
e
l
o
p
m
e
n
t
s
e
t
.
a
t
t
h
e
S
V
M
?
s
d
e
c
i
s
i
o
n
t
h
r
e
s
h
o
l
d
c
o
r
r
e
s
p
o
n
d
i
n
g
t
o
a
r
o
u
n
d
3
.
3
%
f
a
l
s
e
p
o
s
i
t
i
v
e
r
a
t
e
.
T
h
e
s
i
m
p
l
e
c
o
m
-
b
i
n
a
t
i
o
n
o
f
t
h
e
i
r
s
c
o
r
e
s
a
c
h
i
e
v
e
s
s
t
r
o
n
g
i
m
p
r
o
v
e
-
m
e
n
t
s
o
v
e
r
e
i
t
h
e
r
m
o
d
e
l
,
w
i
t
h
a
n
o
p
e
r
a
t
i
n
g
p
o
i
n
t
a
s
s
o
c
i
a
t
e
d
w
i
t
h
t
h
e
S
V
M
d
e
c
i
s
i
o
n
b
o
u
n
d
a
r
y
t
h
a
t
y
i
e
l
d
s
a
c
o
u
p
l
e
o
f
p
o
i
n
t
s
i
m
p
r
o
v
e
m
e
n
t
i
n
t
r
u
e
p
o
s
-
i
t
i
v
e
s
a
n
d
a
f
u
l
l
1
%
r
e
d
u
c
t
i
o
n
i
n
f
a
l
s
e
p
o
s
i
t
i
v
e
r
a
t
e
.
O
n
e
s
i
m
p
l
e
w
a
y
t
o
c
o
m
b
i
n
e
t
h
e
s
e
t
w
o
s
y
s
t
e
m
o
u
t
p
u
t
s
i
n
a
w
a
y
t
h
a
t
d
o
e
s
n
o
t
r
e
q
u
i
r
e
t
u
n
i
n
g
a
d
e
-
c
i
s
i
o
n
t
h
r
e
s
h
o
l
d
i
s
t
o
e
x
p
a
n
d
t
h
e
a
b
b
r
e
v
i
a
t
i
o
n
i
f
a
n
d
o
n
l
y
i
f
(
1
)
b
o
t
h
t
h
e
S
V
M
m
o
d
e
l
a
n
d
t
h
e
N
-
g
r
a
m
m
o
d
e
l
a
g
r
e
e
o
n
t
h
e
b
e
s
t
e
x
p
a
n
s
i
o
n
;
a
n
d
(
2
)
t
h
e
S
V
M
m
o
d
e
l
s
c
o
r
e
i
s
g
r
e
a
t
e
r
t
h
a
n
z
e
r
o
.
I
n
a
s
l
i
g
h
t
a
b
u
s
e
o
f
t
h
e
t
e
r
m
?
i
n
t
e
r
s
e
c
t
i
o
n
?
,
w
e
c
a
l
l
t
h
i
s
c
o
m
b
i
n
a
t
i
o
n
?
S
V
M
i
n
t
e
r
s
e
c
t
N
-
g
r
a
m
?
(
o
r
?
S
V
M
\
N
-
g
r
a
m
?
i
n
T
a
b
l
e
2
)
.
U
s
i
n
g
t
h
i
s
a
p
p
r
o
a
c
h
,
o
u
r
t
r
u
e
p
o
s
i
t
i
v
e
r
a
t
e
o
n
t
h
e
d
e
v
s
e
t
d
e
c
l
i
n
e
s
a
b
i
t
t
o
j
u
s
t
o
v
e
r
5
0
%
,
b
u
t
o
u
r
f
a
l
s
e
p
o
s
i
t
i
v
e
r
a
t
e
d
e
c
l
i
n
e
s
o
v
e
r
t
w
o
f
u
l
l
p
e
r
c
e
n
t
a
g
e
p
o
i
n
t
s
t
o
1
.
1
%
,
y
i
e
l
d
i
n
g
a
v
e
r
y
h
i
g
h
p
r
e
c
i
s
i
o
n
s
y
s
t
e
m
.
T
a
k
i
n
g
t
h
i
s
v
e
r
y
h
i
g
h
p
r
e
c
i
s
i
o
n
s
y
s
t
e
m
c
o
m
b
i
-
n
a
t
i
o
n
o
f
t
h
e
N
-
g
r
a
m
a
n
d
S
V
M
m
o
d
e
l
s
,
w
e
t
h
e
n
c
o
m
b
i
n
e
w
i
t
h
t
h
e
b
a
s
e
l
i
n
e
T
T
S
s
y
s
t
e
m
a
s
f
o
l
l
o
w
s
.
F
i
r
s
t
w
e
a
p
p
l
y
o
u
r
s
y
s
t
e
m
,
a
n
d
e
x
p
a
n
d
t
h
e
i
t
e
m
i
f
i
t
s
c
o
r
e
s
a
b
o
v
e
t
h
r
e
s
h
o
l
d
;
f
o
r
t
h
o
s
e
i
t
e
m
s
l
e
f
t
u
n
-
e
x
p
a
n
d
e
d
,
w
e
l
e
t
t
h
e
T
T
S
s
y
s
t
e
m
p
r
o
c
e
s
s
i
t
i
n
i
t
s
o
w
n
w
a
y
.
I
n
t
h
i
s
w
a
y
,
w
e
a
c
t
u
a
l
l
y
r
e
d
u
c
e
t
h
e
f
a
l
s
e
p
o
s
i
t
i
v
e
r
a
t
e
o
n
t
h
e
d
e
v
s
e
t
o
v
e
r
t
h
e
b
a
s
e
l
i
n
e
T
T
S
s
y
s
t
e
m
b
y
o
v
e
r
1
%
a
b
s
o
l
u
t
e
t
o
l
e
s
s
t
h
a
n
2
%
,
w
h
i
l
e
a
l
s
o
i
n
c
r
e
a
s
i
n
g
t
h
e
t
r
u
e
p
o
s
i
t
i
v
e
r
a
t
e
t
o
7
3
.
5
%
,
a
n
i
n
c
r
e
a
s
e
o
f
1
8
.
5
%
a
b
s
o
l
u
t
e
.
O
f
c
o
u
r
s
e
,
a
t
t
e
s
t
t
i
m
e
,
w
e
w
i
l
l
n
o
t
k
n
o
w
w
h
e
t
h
e
r
a
n
O
O
V
i
s
a
n
a
b
b
r
e
v
i
a
t
i
o
n
o
r
n
o
t
,
s
o
w
e
a
l
s
o
l
o
o
k
e
d
a
t
t
h
e
p
e
r
f
o
r
m
a
n
c
e
o
n
t
h
e
r
e
s
t
o
f
t
h
e
c
o
l
l
e
c
t
e
d
d
a
t
a
,
t
o
s
e
e
h
o
w
o
f
t
e
n
i
t
e
r
r
o
-
n
e
o
u
s
l
y
s
u
g
g
e
s
t
s
a
n
e
x
p
a
n
s
i
o
n
f
r
o
m
t
h
a
t
s
e
t
.
O
f
t
h
e
1
1
,
1
5
7
e
x
a
m
p
l
e
s
t
h
a
t
w
e
r
e
h
a
n
d
-
l
a
b
e
l
e
d
a
s
n
o
n
-
a
b
b
r
e
v
i
a
t
i
o
n
s
,
o
u
r
S
V
M
\
N
-
g
r
a
m
s
y
s
t
e
m
e
x
-
p
a
n
d
e
d
4
5
i
t
e
m
s
,
w
h
i
c
h
i
s
a
f
a
l
s
e
p
o
s
i
t
i
v
e
r
a
t
e
o
f
0
.
4
%
u
n
d
e
r
t
h
e
a
s
s
u
m
p
t
i
o
n
t
h
a
t
n
o
n
e
o
f
t
h
e
m
s
h
o
u
l
d
b
e
e
x
p
a
n
d
e
d
.
I
n
f
a
c
t
,
m
a
n
u
a
l
i
n
s
p
e
c
t
i
o
n
f
o
u
n
d
t
h
a
t
2
0
%
o
f
t
h
e
s
e
w
e
r
e
c
o
r
r
e
c
t
e
x
p
a
n
s
i
o
n
s
o
f
a
b
b
r
e
v
i
a
t
i
o
n
s
t
h
a
t
h
a
d
b
e
e
n
m
i
s
-
l
a
b
e
l
e
d
.
D
u
r
i
n
g
s
y
s
t
e
m
d
e
v
e
l
o
p
m
e
n
t
,
w
e
a
l
s
o
e
x
p
e
r
i
-
m
e
n
t
e
d
w
i
t
h
a
n
u
m
b
e
r
o
f
a
l
t
e
r
n
a
t
i
v
e
h
i
g
h
p
r
e
c
i
-
s
i
o
n
a
p
p
r
o
a
c
h
e
s
t
h
a
t
s
p
a
c
e
p
r
e
c
l
u
d
e
s
o
u
r
p
r
e
s
e
n
t
-
i
n
g
i
n
d
e
t
a
i
l
h
e
r
e
,
i
n
c
l
u
d
i
n
g
:
p
r
u
n
i
n
g
t
h
e
n
u
m
-
b
e
r
o
f
e
x
p
a
n
s
i
o
n
c
a
n
d
i
d
a
t
e
s
b
a
s
e
d
o
n
t
h
e
p
a
i
r
l
a
n
-
g
u
a
g
e
m
o
d
e
l
s
c
o
r
e
;
o
n
l
y
a
l
l
o
w
i
n
g
a
b
b
r
e
v
i
a
t
i
o
n
e
x
-
p
a
n
s
i
o
n
w
h
e
n
a
t
l
e
a
s
t
o
n
e
e
x
t
r
a
c
t
e
d
n
-
g
r
a
m
c
o
n
-
t
e
x
t
i
s
p
r
e
s
e
n
t
f
o
r
t
h
a
t
e
x
p
a
n
s
i
o
n
i
n
t
h
a
t
c
o
n
t
e
x
t
;
a
n
d
C
A
R
T
t
r
e
e
(
B
r
e
i
m
a
n
e
t
a
l
.
,
1
9
8
4
)
t
r
a
i
n
i
n
g
w
i
t
h
r
e
a
l
v
a
l
u
e
d
s
c
o
r
e
s
.
S
o
m
e
o
f
t
h
e
s
e
y
i
e
l
d
e
d
v
e
r
y
h
i
g
h
p
r
e
c
i
s
i
o
n
s
y
s
t
e
m
s
,
t
h
o
u
g
h
a
t
t
h
e
c
o
s
t
o
f
l
e
a
v
i
n
g
m
a
n
y
m
o
r
e
a
b
b
r
e
v
i
a
t
i
o
n
s
u
n
e
x
p
a
n
d
e
d
.
W
e
f
o
u
n
d
t
h
a
t
,
f
o
r
u
s
e
i
n
c
o
m
b
i
n
a
t
i
o
n
w
i
t
h
t
h
e
b
a
s
e
-
l
i
n
e
T
T
S
s
y
s
t
e
m
,
l
a
r
g
e
o
v
e
r
a
l
l
r
e
d
u
c
t
i
o
n
s
i
n
f
a
l
s
e
p
o
s
i
t
i
v
e
r
a
t
e
w
e
r
e
a
c
h
i
e
v
e
d
b
y
u
s
i
n
g
a
n
i
n
i
t
i
a
l
s
y
s
-
t
e
m
w
i
t
h
s
u
b
s
t
a
n
t
i
a
l
l
y
h
i
g
h
e
r
T
P
a
n
d
s
o
m
e
w
h
a
t
h
i
g
h
e
r
F
P
r
a
t
e
s
,
s
i
n
c
e
f
a
r
f
e
w
e
r
a
b
b
r
e
v
i
a
t
i
o
n
s
w
e
r
e
t
h
e
n
p
a
s
s
e
d
a
l
o
n
g
u
n
e
x
p
a
n
d
e
d
t
o
t
h
e
b
a
s
e
l
i
n
e
s
y
s
-
t
e
m
,
w
i
t
h
i
t
s
r
e
l
a
t
i
v
e
l
y
h
i
g
h
3
%
F
P
r
a
t
e
.
T
o
e
n
s
u
r
e
t
h
a
t
w
e
h
a
d
n
o
t
o
v
e
r
-
t
u
n
e
d
o
u
r
s
y
s
-
t
e
m
s
t
o
t
h
e
d
e
v
s
e
t
t
h
r
o
u
g
h
e
x
p
e
r
i
m
e
n
t
a
t
i
o
n
,
w
e
p
e
r
f
o
r
m
e
d
1
0
-
f
o
l
d
c
r
o
s
s
v
a
l
i
d
a
t
i
o
n
o
v
e
r
t
h
e
f
u
l
l
s
e
t
o
f
a
b
b
r
e
v
i
a
t
i
o
n
s
,
a
n
d
t
h
e
r
e
s
u
l
t
s
a
r
e
p
r
e
s
e
n
t
e
d
i
n
T
a
b
l
e
2
.
M
o
s
t
n
o
t
a
b
l
y
,
t
h
e
T
T
S
b
a
s
e
l
i
n
e
s
y
s
t
e
m
h
a
s
a
m
u
c
h
l
o
w
e
r
t
r
u
e
p
o
s
i
t
i
v
e
r
a
t
e
;
y
e
t
w
e
fi
n
d
o
u
r
s
y
s
t
e
m
s
a
c
h
i
e
v
e
p
e
r
f
o
r
m
a
n
c
e
v
e
r
y
c
l
o
s
e
t
o
t
h
a
t
f
o
r
t
h
e
d
e
v
e
l
o
p
m
e
n
t
s
e
t
,
s
o
t
h
a
t
o
u
r
fi
n
a
l
c
o
m
b
i
n
a
t
i
o
n
w
i
t
h
t
h
e
T
T
S
b
a
s
e
l
i
n
e
w
a
s
a
c
t
u
a
l
l
y
s
l
i
g
h
l
y
b
e
t
t
e
r
t
h
a
n
t
h
e
n
u
m
b
e
r
s
o
n
t
h
e
d
e
v
e
l
o
p
m
e
n
t
s
e
t
.
5
C
o
n
c
l
u
s
i
o
n
s
N
o
t
e
s
f
o
r
i
n
t
e
r
n
a
l
r
e
v
i
e
w
e
r
s
?
M
a
y
b
e
m
o
r
e
t
e
c
h
n
i
c
a
l
e
x
p
l
a
n
a
t
i
o
n
o
f
l
o
g
l
i
k
e
l
i
h
o
o
d
a
n
d
l
o
g
o
d
d
s
s
c
o
r
e
s
?
R
e
v
a
m
p
e
d
i
n
t
r
o
,
b
a
c
k
g
r
o
u
n
d
,
c
o
n
c
l
u
s
i
o
n
a
n
d
e
x
p
a
n
d
e
d
r
e
f
s
.
?
A
n
o
n
y
m
i
z
a
t
i
o
n
f
o
r
s
u
b
m
i
s
s
i
o
n
.
Incorrect expansion percentage (FP)
Figure 1: ROC curve plotting true positive (correct expan-
sion) percentages versus false positive (incorrect expansion)
percentages for several systems on the development set.
positive rate falls between 1 and 3 percent, though
both systems reach roughly the same performance
at the SVM?s decision threshold corresponding to
around 3.3% false positive rate. The simple com-
bination of their scores achieves strong improve-
ments over either model, with an operating point
associated with the SVM decision boundary that
yields a couple of points improvement in true pos-
itives and a full 1% reduction in false positive rate.
One simple way to combine these two system
outputs in a way that does not require tuning a de-
cision threshold is to expand the abbreviation if
and only if (1) both the SVM model and the N-
gram model agree on the best expansion; and (2)
the SVM model score is greater than zero. In a
slight abuse of the term ?intersection?, we call this
combination ?SVM intersect N-gram? (or ?SVM
? N-gram? in Table 2). Using this approach, our
true positive rate on the development set declines
a bit to just over 50%, but our false positive rate
declines over two full percentage points to 1.1%,
yielding a very high precision system.
Taking this very high precision system combi-
nation of the N-gram and SVM models, we then
combine with the baseline TTS system as follows.
First we apply our system, and expand the item if
it scores above threshold; for those items left un-
expanded, we let the TTS system process it in its
own way. In this way, we actually reduce the false
positive rate on the development set over the base-
line TTS system by over 1% absolute to less than
2%, while also increasing the true positive rate to
73.5%, an increase of 18.5% absolute.
Of course, at test time, we will not know
whether an OOV is an abbreviation or not, so
we also looked at the performance on the rest
of the collected data, to see how often it erro-
neously suggests an expansion from that set. Of
the 11,157 examples that were hand-labeled as
non-abbreviations, our SVM ?N-gram system ex-
panded 45 i ems, which is a false positive rate
of 0.4% under the assumption that none of them
should be expanded. In fact, m nual inspection
found that 20% of these were correct expansions
of abbreviations that had been mis-labeled.
We also experimented with a umber of alter-
native hig precision approaches tha space p e-
cludes our presenting in detail here, including:
pruning the number of expansion candidates based
on the pair LM score; only allowing abbreviation
expansion when at least one extracted n-gram con-
text is present for that expansion in that context;
and CART tree (Breiman et al, 1984) training
with real valued scores. Some of these yielded
very high precision systems, though at the cost
of leaving many more abbreviations unexpanded.
We found that, for use in combination with the
baseline TTS system, large overall reductions in
FP rate were achieved by using an initial system
with substantially higher TP and somewhat higher
FP rates, since far fewer abbreviations were then
passed along unexpanded to the baseline system,
with its relatively high 3% FP rate.
To ensure that we did not overtune our systems
to the development set through experimentation,
we performed 10-fold cross validation over the full
set of abbreviations. These results are presented
in Table 2. Most notably, the TTS baseline system
has a much lower true positive rate; yet we find our
systems achieve performance very close to that for
the development set, so that our final combination
with the TTS baseline was actually slighly better
than the numbers on the development set.
5 Conclusions
In this paper we have presented methods for high
precision abbreviation expansion for a TTS appli-
cation. The methods are largely self-organizing,
using in-domain unannotated data, and depend on
only a small amount of annotated data. Since the
SVM features relate to general properties of ab-
breviations, expansions and contexts, the classi-
fier parameters will likely carry over to new (En-
glish) domains. We demonstrate that in combi-
nation with a hand-built TTS baseline, the meth-
ods afford dramatic improvement in the TP rate
(to about 74% from a starting point of about 40%)
and a reduction of FP to below our goal of 2%.
Acknowledgments
We would like to thank Daan van Esch and the
Google Speech Data Operations team for their
work on preparing the annotated data. We also
thank the reviewers for their comments.
368
References
Alan Agresti. 2002. Categorical data analysis. John
Wiley & Sons, 2nd edition.
Ai Ti Aw and Lian Hau Lee. 2012. Personalized nor-
malization for a multilingual chat system. In Pro-
ceedings of the ACL 2012 System Demonstrations,
pages 31?36, Jeju Island, Korea, July. Association
for Computational Linguistics.
Richard Beaufort, Sophie Roekhaut, Louise-Am?elie
Cougnon, and C?edrick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normal-
izing SMS messages. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 770?779, Uppsala, Sweden, July.
Association for Computational Linguistics.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434?451.
Leo Breiman, Jerome H. Friedman, Richard A. Olshen,
and Charles J. Stone. 1984. Classification and Re-
gression Trees. Wadsworth & Brooks, Pacific Grove
CA.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Sudesha
Sarkar, and Anupam Basu. 2007. Investigation and
modeling of the structure of texting language. Int. J.
Doc. Anal. Recognit., 10:157?174.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Hany Hassan and Arul Menezes. 2013. Social text nor-
malization using contextual graph random walks. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1577?
1586.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recogniser. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3):400?401.
Max Kaufmann. 2010. Syntactic normalization of
Twitter messages. In International Conference on
NLP.
Catherine Kobus, Franc?ois Yvon, and G?eraldine
Damnati. 2008. Normalizing SMS: are two
metaphors better than one? In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 441?448, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution? Nor-
malizing text messages without pre-categorization
nor supervision. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 71?
76, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012a. A
broad-coverage normalization system for social me-
dia language. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1035?
1044, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou,
Zhongyang Fu, and Furu Wei. 2012b. Joint
inference of named entity recognition and nor-
malization for tweets. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
526?535, Jeju Island, Korea, July. Association for
Computational Linguistics.
Burt L Monroe, Michael P Colaresi, and Kevin M
Quinn. 2008. Fightin?words: Lexical feature se-
lection and evaluation for identifying the content of
political conflict. Political Analysis, 16(4):372?403.
Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of
SMS abbreviations. In IJCNLP. Papers/pennell-
liu3.pdf.
Brian Roark, Michael Riley, Cyril Allauzen, Terry Tai,
and Richard Sproat. 2012. The OpenGrm open-
source finite-state grammar software libraries. In
ACL, Jeju Island, Korea.
Richard Sproat, Alan Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287?333.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proc. DARPA Broad-
cast News Transcription and Understanding Work-
shop, pages 270?274.
Amber Wilcox-O?Hearn, Graeme Hirst, and Alexander
Budanitsky. 2008. Real-word spelling correction
with trigrams: A reconsideration of the Mays, Dam-
erau, and Mercer model. In CICLing 2008, volume
4919 of LNCS, pages 605?616, Berlin. Springer.
Yunqing Xia, Kam-Fai Wong, and Wenjie Li. 2006.
A phonetic-based approach to Chinese chat text nor-
malization. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 993?1000, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 61?72.
369
Collecting Semantic Data by Mechanical Turk for the Lexical
Knowledge Resource of a Text-to-Picture Generating System
Masoud Rouhizadeh* Margit Bowler* Richard Sproat*Bob Coyne**
*Center for Spoken Language Understanding, Oregon Health and Science University
**Department of Computer Science, Columbia University
Abstract
WordsEye is a system for automatically converting natural language text into 3D scenes repre-
senting the meaning of that text. At the core of WordsEye is the Scenario-Based Lexical Knowledge
Resource (SBLR), a unified knowledge base and representational system for expressing lexical and
real-world knowledge needed to depict scenes from text. To enrich a portion of the SBLR, we need to
fill out some contextual information about its objects, including information about their typical parts,
typical locations and typical objects located near them. This paper explores our proposed method-
ology to achieve this goal. First we try to collect some semantic information by using Amazon?s
Mechanical Turk (AMT). Then, we manually filter and classify the collected data and finally, we
compare the manual results with the output of some automatic filtration techniques which use several
WordNet similarity and corpus association measures.
1 Introduction
WordsEye (Coyne and Sproat, 2001), (Coyne et al, 2010) is a system for automatically converting natural
language text into 3D scenes representing the meaning of that text. A version of WordsEye has been
tested online (www.wordseye.com) with several thousand real-world users. The system works by first
parsing each input sentence into a dependency structure. These dependency structures are then processed
to resolve anaphora and other coreferences. The lexical items and dependency links are then converted
to semantic nodes and roles drawing on lexical valence patterns and other information in the Scenario-
Based Lexical Knowledge Resource (SBLR) (Coyne et al, 2010). The resulting semantic relations are
then converted to a final set of graphical constraints representing the position, orientation, size, color,
texture, and poses of objects in the scene. Finally, the scene is composed from these constraints and
rendered in OpenGL (http://www.opengl.org).
The SBLR is the core of the text-to-scene conversion mechanism. It is a unified knowledge base and
representational system for expressing lexical and real-world knowledge needed to depict scenes from
text. The SBLR will ultimately include information on the semantic categories of words; the semantic
relations between predicates (verbs, nouns, adjectives, and prepositions) and their arguments; the types
of arguments different predicates typically take; additional contextual knowledge about the visual scenes
various events and activities occur in; and the relationship between this linguistic information and the 3D
objects in our objects library.
To enrich a portion of the SBLR we need to fill out some contextual information about several
hundred objects in WordsEye?s database, including information about their typical parts, typical location
and typical objects nearby them. Such information can in principle be extracted from online corpora
(e.g. Sproat (2001)), but such data is invariably noisy and requires hand editing. Furthermore, precisely
because much of the information is common sense it is rarely explicitly stated in text. Ontologies of
common sense information such as Cyc are effectively useless for extracting such information.
This paper explores our proposed methodology to achieve this goal. First we try to collect some
semantic information by Amazon?s Mechanical Turk (AMT). Then, we manually filter and classify the
380
collected data and finally, we compare the manual results with the output of some automatic filtration
techniques which use WN similarity and corpus association measures.
2 Data collection from Amazon?s Mechanical Turk
Amazon?s Mechanical Turk is an online marketplace that provides a way to pay people small amounts
of money to perform tasks that are simple for humans but difficult for computers. Examples of these
Human Intelligence Tasks (HITs) range from labeling images to moderating blog comments to providing
feedback on the relevance of results for a search query. The highly accurate, cheap and efficient results
of several NLP tasks (Callison-Burch and Dredze, 2010) have encouraged us to explore using AMT.
We designed three separate tasks to collect information about typical nearby objects, typical location
and typical parts of the objects of our library. For task 1, we asked the workers to name 10 common
objects that they might typically find around or near a given object. For task 2, we asked the workers to
name 10 locations in which they might typically find a given object and in task 3, we asked the workers
to list 10 parts of a given object. Given that some objects might not consist of 10 parts, (i.e, they are
very simple objects), we wanted the worker to name as many parts as possible. We collected 17,200
responses from the AMT tasks and paid $106.90 overall for completion of the three tasks. Table 1 shows
a summary of the AMT tasks, payments, and completion time.
Task TW UI AA RPA EHR ACT
Objects 342 6850 2? $0.05 $1.54 5
Locations 342 6850 2? $0.05 $1.26 5
Parts 245 3500 1? $0.07 $2.29 5
TW: Number of Target Words; UI: Number of User Inputs; AA: Average Time Per Assignment;
RPA: Reward Per Assignment; EHR: Effective Hourly Rate; ACT: Approximate Completion Days
Table 1: Summary of AMT tasks, payments and the completion time
The data that we collected in this step was in raw format. The next step was filtering out undesirable
data entered by the workers and mapping it into entities and relations contained within the SBLR.
3 Manual filtering and classifying the data
Data collected from AMT tasks was manually filtered via removal of undesirable target item-response
item pairs and classified via definition of the relations between the remaining target item-response item
pairs. Response items given in their plural form were lemmatized to the singular form of the word.
A total of 34 relations were defined within the Amazon Mechanical Turk data. Defining relations was
completed manually and determined by pragmatic cues about the relationship held between the target
item-response item pair. Restricting AMT workers to those within the United States ensured that actions
or items which might differ in their typically found location by cultural or geographical context were
restricted to the location(s) generally agreed upon by English speakers within the United States.
Generic, widely applicable relations were used in the general case for all sets of Mechanical Turk data
(e.g. the containment relation containing.r was used for generic instances of containment; the next-to.r
relation was used for target item-response item pairs for which the orientation of the items with respect to
one another was not a defining characteristic of their relationship). Finer distinctions were made within
these generic relations, e.g. habitat.r and residence.r within the overarching containment relation, which
specified that the relation held between two items was that of habitat or residence, respectively. More
semantically explicit relations were used for target item-response item pairs that tended to occur in more
specific relations. Specific relations of this type included those spatial relations from the following target
item-response item-relation triples:
javelin - dirt - embedded-in.r
binoculars - case - true-containing.r
381
Another subsection of relations included functional relations such as those within the following
triples:
harmonica - hand - human-grip.r
earmuffs - head - wearing.r
Relation labels for meronymic (part-whole) relations were based off of already defined part-whole
classifications (Priss, 1996).
3.1 Data and results for each AMT task
Target item-response item pairs were usually rejected for misinterpretation of the potentially ambiguous
target item (e.g. misinterpreting mobile as a cell phone rather than as a decorative hanging structure,
prompting mobile - ear as an object-nearby object pair). Target item-response item pairs were also dis-
carded if the interpretation of the target item, though viable, was not contained within the SBLR library.
This was especially prevalent in instances where the target item was a plant or animal (e.g. crawfish)
that could be interpreted as either a live plant/animal or as food. With the exception of mushroom, the
SBLR does not contain the edible interpretation of these nouns; in the object-nearby object task, target
item-response item pairs such as crawfish - plate were discarded.
In the object-location task, the most common relation labels were derivatives of the generic spatial
containment relation. The containing.r relation accounted for 38.01% of all labeled target-response pairs;
habitat.r accounted for 11.02%, and on-surface.r accounted for 10.6%.
In the part-whole task, AMT workers provided responses that were predominantly labeled by part-
whole relations. When AMT responses were not relevant for part-whole relations, they tended to fall
under the generic containment relation. The object-part.r relation accounted for 79.12% of all labeled
target-response pairs; stuff-object.r accounted for 16.33%, and containing.r accounted for 1.48%.
As with the part-whole task, responses in the nearby objects task that were not relevant for the next-
to.r relation usually fell under the generic spatial containment relation. In the object-nearby object task,
the next-to.r relation was the most frequently utilized relation label, accounting for 75.66% of all target-
response pairs labeled. The on-surface.r relation was the second most common relation, with 5.69%,
and containing.r accounted for 4.44% of all labeled target-response pairs
4 Automatic filtering undesirable data
Manual processing of the data is a time-consuming and expensive approach. As a result, we are inves-
tigating different automatic techniques to filter out the undesirable responses from AMT, using current
manually annotated data as a gold standard for evaluation of automatic approaches.
4.1 WordNet Similarity measures
In the first approach, we computed some lexical similarity scores for the target and the response items
based on the followingWN similarity measures. (It should be noted that not all of the target and responses
were present in WN. For such words, we used their nearest hypernyms).
WN Path Distance Similarity between each target word and each received response for that target
word. This score denotes how similar two word senses are, based on the shortest path that connects
the senses in the is-a (hypernym/hypnoym) taxonomy. We selected the maximum similarity score of
different senses of the target and the respond words.
Resnik Similarity between each target word and each of the received responses for that target word.
This score denotes how similar the two word senses are, based on the Information Content (IC) of the
Least Common Subsumer (most specific ancestor node) (Resnik, 1999).
The Average Pairwise Similarity Score which is computed based on WN path distance similarity
score. If we assumeW1,W2...Wn to be n responses for target word T; and Sij to be theWN path distance
similarity between Wi and Wj , then the average pairwise similarity score for Wi will be Si1+Si2+...+Sinn .
This will provide us the average similarity of each response (i.e Wi) with the other responses (i.e. Wj
382
so that i6= j). In this way we will reward the responses that are more semantically related to each other
(regardless of their similarity to the target word).
The WN Matrix Similarity which is a bag of words similarity matrix based on WN path distance
similarities. For target word T we have the following similarity matrix:
1 + S12 + ...+ S1n
S21 + 1 + ...+ S2n
...
Sn1 + Sn2 + ...+ Snn
In this matrix row i is the similarity vector of Wi represented as ~Vi = [Si1 + Si2 + ...+ Sin]. We
use cosine similarity to calculate the similarity measure of two words. So, the similarity measure of
Wi and Wj is the cosine of ~Vi and ~Vj and is computed by CSij = cos(?) = Vi.Vj||Vi||.||Vj || . Then the WN
matrix similarity score of Wi will be CSi1+CSi2+...+CSinn . The more two words are semantically related
to similar set of words, the higher cosine similarity they will have. If a word is related to many different
words in the set, it will obtain higher WN matrix similarity score.
4.2 Corpus association measures
The next approach for filtering the raw data was finding association measures of target-response pairs
using Google?s 1-trillion 5-gram web corpus (LDC2006T13), by counting the frequency of each target
and response word in unigram and bigram portions of the corpus and then the number of times the two
words co-occur within a +/- 4-word window in the 5-gram portion of the corpus. We also computed the
sentential co-occurrences of each target-response pair (i.e. the number of sentences in which the target
or the response words appear and the number of sentences in which both words occur together) on the
English Gigaword corpus (LDC2007T07) which is a 1 billion word corpus of articles marked up from
English press texts (mainly the New York Times). Based on these counts, we used log-likelihood and
log-odds ratio (Dunning, 1993) to compute the association between the two words.
4.3 Discussion and evaluation of automatic filtaration techniques
The collected responses of each AMT task were ranked separately by each of the above similarity and
association measures. We classify the ranked responses into ?keep? (higher-scoring) and ?reject? (lower-
scoring) classes by defining a specific threshold for each list. Then we evaluated the accuracy of each
filtration approach by computing their precision and recall on correct ?keep? items (see table 2). In this
table the baseline score shows the accuracy of the responses of each AMT task before using automatic
filtration techniques. It should be added that collecting data by using AMT is rather cheap and fast, so
we are more interested in higher precision (achieving highly accurate data) than higher recall. Lower
recall means we lose some data, which is not too expensive to collect.
Baseline Log-likelihood Log-odds WN Path Dist sim Resnik sim WN Pairwise sim WN Matrix sim
Pre Rec Pre Rec Pre Rec Pre Rec Pre Rec Pre Rec Pre Rec
LOC 0.5527 1.0 0.7832 0.6690 0.7851 0.6684 0.5624 0.9724 0.5674 0.9784 0.6115 0.3657 0.4832 1.0
PAR 0.7887 1.0 0.7921 0.4523 0.8321 0.5022 0.8073 1.0 0.8234 1.0 0.9045 0.2859 0.9010 0.2516
OBJ 0.8934 1.0 0.9015 1.0 0.9286 0.9144 0.9123 1.0 0.9185 1.0 0.9855 0.3215 0.8925 1.0
Table 2: The accuracy of automatic filtering approaches
As can be seen in table 2, within the object-location data set, we gained the best precision (0.7832) by
using log-odds with relatively high recall (0.6690). Target-response pairs that were approved or rejected
contrary to automatic predictions were due primarily to the specificity of the response location.
In the part-whole task, the best precision (0.9010) was achieved by using WN matrix similarities
but again we lost a noticeable portion of data (recall= 0.2516). Rejected target-response pairs from the
higher-scoring part-whole set were often due to responses that named attributes, rather than parts, of
the target item (e.g. croissant - flaky). Many responses were too general (e.g, gong - material). Many
target-response pairs would have fallen under the next-to.r relation rather than any of the meronymic
383
relations. The majority of the approved target-response pairs from the lower-scoring part-whole set were
due to obvious, ?common sense responses that would usually be inferred rather than explicitly stated,
particularly body parts (e.g, bunny - brain).
The baseline accuracy of the nearby objects task is quite high (precision=0.8934, recall=1.0), and
we gain the best precision by using WN average pairwise similarity (0.9855) by removing lower-scoring
part of AMT responses (recall=0.3215). The high precision in all automatic techniques is due primarily
to the fact that the open-ended nature of the task resulted in a large number of target-response pairs that,
while not pertinent to the next-to.r relation, could be labeled by other relations. Again, the open-ended
nature of the nearby objects task resulted in the lowest percentage of rejected high-scoring pairs.
5 Conclusions
In this paper, we investigated the use Amazon?s Mechanical Turk for collecting semantic information for
a portion of our lexical knowledge resource. Manual evaluation of the AMT responses (baseline results
in table 2) confirms that we can collect highly accurate data in a cheap and efficient way by using AMT.
The accuracy of automatic filtration techniques sounds promising as we were able to filter out some
undesirable data, most of the time without loosing so much of collected responses.
Overall, we have shown a method which is very good in collecting semantic information and some
other methods which are very good at filtering out word pairs that are undesirable in this particular context
(i.e locations, nearby object and parts of our object library). This approach seems to have the potential
to be extended for more contexts. For the future work, we are planning to apply this methodology to
collect semantic information about action verbs, such as information about the locations of the action,
the participants, their relation to each other, the background objects and so on.
References
Callison-Burch, C. and M. Dredze (2010). Creating speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, Los Angeles, CA, USA, pp. 1?12.
Coyne, B., O. Rambow, J. Hirschberg, and R. Sproat (2010). Frame semantics in text-to-scene gen-
eration. In R. Setchi, I. Jordanov, R. Howlett, and L. Jain (Eds.), Knowledge-Based and Intelligent
Information and Engineering Systems, Volume 6279 of Lecture Notes in Computer Science, pp. 375?
384. Springer Berlin / Heidelberg.
Coyne, B. and R. Sproat (2001). Wordseye: An automatic text-to-scene conversion system. In Proceed-
ings of the 28th annual conference on Computer graphics and interactive techniques, Los Angeles,
CA, USA, pp. 487? 496.
Coyne, B., R. Sproat, and J. Hirschberg (2010). Spatial relations in text-to-scene conversion. In Compu-
tational Models of Spatial Language Interpretation, Workshop at Spatial Cognition 2010, Mt. Hood,
OR, USA, pp. 9?16.
Dunning, T. E. (1993). Accurate methods for the statistics of surprise and coincidence. Computational
Linguistics 19(1), 61?74.
Priss, U. (1996). Classification of meronymy by methods of relational concept analysis. In Online
proceedings of the 1996 Midwest Artificial Intelligence Conference, Bloomington, IN, USA.
Resnik, P. (1999). Semantic similarity in a taxonomy: An information-based measure and its application
to problems of ambiguity in natural language. Journal of Artificial Intelligence Research, 95?130.
Sproat, R. (2001). Inferring the environment in a text-to-scene conversion system. In Proceedings of The
First International Conference on Knowledge Capture, Victoria, BC, Canada, pp. 147?154.
384
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 22?31,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Towards technology-assisted co-construction with communication partners
Brian Roark
?
, Andrew Fowler
?
, Richard Sproat
?
, Christopher Gibbons
?
, Melanie Fried-Oken?
?Center for Spoken Language Understanding ?Child Development & Rehabilitation Center
Oregon Health & Science University
{roark,fowlera,sproatr}@cslu.ogi.edu {gibbons,mfo}@ohsu.edu
Abstract
In this paper, we examine the idea of
technology-assisted co-construction, where
the communication partner of an AAC user
can make guesses about the intended mes-
sages, which are included in the user?s word
completion/prediction interface. We run some
human trials to simulate this new interface
concept, with subjects predicting words as the
user?s intended message is being generated in
real time with specified typing speeds. Re-
sults indicate that people can provide substan-
tial keystroke savings by providing word com-
pletion or prediction, but that the savings are
not as high as n-gram language models. In-
terestingly, the language model and human
predictions are complementary in certain key
ways ? humans doing a better job in some
circumstances on contextually salient nouns.
We discuss implications of the enhanced co-
construction interface for real-time message
generation in AAC direct selection devices.
1 Introduction
Individuals who cannot use standard keyboards for
text entry because of physical disabilities have a
number of alternative text entry methods that per-
mit typing. Referred to as keyboard emulation
within augmentative and alternative communication
(AAC), there are many different access options for
the user, ranging from direct selection of letters with
any anatomical pointer (e.g., head, eyes) to use of a
binary switch ? triggered by button-press, eye-blink
or even through event related potentials (ERP) such
as the P300 detected in EEG signals. These options
allow the individual to indirectly select a symbol
based on some process for scanning through alter-
natives (Lesher et al, 1998). Typing speed is a chal-
lenge, yet is critically important for usability, and
as a result there is a significant line of research into
the utility of statistical language models for improv-
ing typing speed (McCoy et al, 2007; Koester and
Levine, 1996; Koester and Levine, 1997; Koester
and Levine, 1998). Methods of word, symbol,
phrase and message prediction via statistical lan-
guage models are widespread in both direct selec-
tion and scanning devices (Darragh et al, 1990; Li
and Hirst, 2005; Trost et al, 2005; Trnka et al,
2006; Trnka et al, 2007; Wandmacher and Antoine,
2007; Todman et al, 2008). To the extent that the
predictions are accurate, the number of keystrokes
required to type a message can be dramatically re-
duced, greatly speeding typing.
AAC devices for spontaneous and novel text gen-
eration are intended to empower the user of the sys-
tem, to place them in control of their own com-
munication, and reduce their reliance on others for
message formulation. As a result, all such devices
(much like standard personal computers) are built
for a single user, with a single keyboard and/or alter-
native input interface, which is driven by the user of
the system. The unilateral nature of these high tech-
nology solutions to AAC stands in contrast to com-
mon low technology solutions, which rely on collab-
oration between the individual formulating the mes-
sage and their communication partner. Many adults
with acquired neurological conditions rely on com-
munication partners for co-construction of messages
(Beukelman et al, 2007).
One key reason why low-tech co-construction
may be preferred to high-tech stand-alone AAC sys-
tem solutions is the resulting speed of communica-
tion. Whereas spoken language reaches more than
one hundred words per minute and an average speed
typist using standard touch typing will achieve ap-
proximately 35 words per minute, a user of an AAC
device will typically input text in the 3-10 words per
minute range. With a communication partner guess-
22
ing the intended message and requesting confirma-
tion, the communication rate can speed up dramati-
cally. For face-to-face communication ? a modality
that is currently very poorly served by AAC devices
? such a speedup is greatly preferred, despite any
potential authorship questions.
Consider the following low-tech scenario. Sandy
is locked-in, with just a single eye-blink serving to
provide binary yes/no feedback. Sandy?s commu-
nication partner, Kim, initiates communication by
verbally stepping through an imagined row/column
grid, first by number (to identify the row); then by
letter. In such a way, Sandy can indicate the first
desired symbol. Communication can continue in
this way until Kim has a good idea of the word that
Sandy intends and proposes the word. If Sandy says
yes, the word has been completed, much as auto-
matic word completion may occur within an AAC
device. But Kim doesn?t necessarily stop with word
completion; subsequent word prediction, phrase pre-
diction, in fact whole utterance prediction can fol-
low, driven by Kim?s intuitions derived from knowl-
edge of Sandy, true sensitivity to context, topic, so-
cial protocol, etc. It is no wonder that such methods
are often chosen over high-tech alternatives.
In this paper, we present some preliminary ideas
and experiments on an approach to providing tech-
nology support to this sort of co-construction during
typing. The core idea is to provide an enhanced in-
terface to the communication partner (Kim in the ex-
ample above), which does not allow them to directly
contribute to the message construction, but rather
to indirectly contribute, by predicting what they be-
lieve the individual will type next. Because most text
generation AAC devices typically already rely upon
symbol, word and phrase prediction from statistical
language models to speed text input, the predictions
of the conversation partner could be used to influ-
ence (or adapt) the language model. Such adaptation
could be as simple as assigning high probability to
words or symbols explicitly predicted by the com-
munication partner, or as complex as deriving the
topic or context from the partner?s predictions and
using that context to improve the model.
Statistical language models in AAC devices can
capture regularities in language, e.g., frequent word
collocations or phrases and names commonly used
by an individual. People, however, have access to
much more information than computational mod-
els, including rich knowledge of language, any rel-
evant contextual factors that may skew prediction,
familiarity with the AAC user, and extensive world
knowledge ? none of which can be easily included in
the kinds of simple statistical models that constitute
the current state of the art. People are typically quite
good at predicting what might come next in a sen-
tence, particularly if it is part of a larger discourse or
dialogue. Indeed, some of the earliest work looking
at statistical models of language established the en-
tropy of English by asking subjects to play a simple
language guessing game (Shannon, 1950). The so-
called ?Shannon game? starts with the subject guess-
ing the first letter of the text. Once they have guessed
correctly, it is uncovered, and the subject guesses
the next letter, and so on. A similar game could be
played with words instead of letters. The number of
guesses required is a measure of entropy in the lan-
guage. People are understandably very good at this
game, often correctly predicting symbols on the first
try for very long stretches of text. No purely com-
putational model can hope to match the contextual
sensitivity, partner familiarity, or world knowledge
that a human being brings to such a task.
A co-construction scenario differs from a Shan-
non game in terms of the time constraints under
which it operates. The communication partner in
such a scenario must offer completions and predic-
tions to the user in a way that actually speeds com-
munication relative to independent text generation.
Given an arbitrary amount of time, it is clear that
people have greater information at their disposal for
predicting subsequent content; what happens under
time constraints is less clear. Indeed, in this paper
we demonstrate that the time constraints put human
subjects at a strong disadvantage relative to language
models in the scenarios we simulated. While it is
far from clear that this disadvantage will also apply
in scenarios closer to the motivating example given
above, it is certainly the case that providing useful
input is a challenging task.
The principal benefit of technology-assisted co-
construction with communication partners is making
use of the partner?s knowledge of language and con-
text, as well as their familiarity with the AAC user
and the world, to yield better predictions of likely
continuations than are currently made by the kinds
23
of relatively uninformed (albeit state of the art) com-
putational language models. A secondary benefit is
that such an approach engages the conversation part-
ner in a high utility collaboration during the AAC
user?s turn, rather than simply sitting and waiting for
the reply to be produced. Lack of engagement is a
serious obstacle to successful conversation in AAC
(Hoag et al, 2004). The slow speed of AAC input is
itself a contributing factor to AAC user dissatisfac-
tion with face-to-face conversation, one of the most
critical modes of human social interaction, and the
one least served by current technology. Because of
the slow turnaround, the conversation partner tends
to lose focus and interest in the conversation, leading
to shorter and less satisfying exchanges than those
enjoyed by those using spoken language. A system
which leverages communication partner predictions
will more fully engage the conversation partner in
the process, rather than forcing them to wait for a
response with nothing to do.
Importantly, an enhanced interface such as that
proposed here provides predictive input from the
communication partner, but not direct compositional
input. The responsibility of selecting symbols and
words during text entry remains with the AAC user,
as the sole author of the text. In the preliminary
experiments presented later in the paper, we simu-
late a direct selection typing system with word pre-
diction, and measure the utility of human generated
word completions and predictions relative to n-gram
models. In such a scenario, n-gram predictions can
be replaced or augmented by human predictions.
This illustrates how easily technology assisted co-
construction with communication partners could po-
tentially be integrated into a user?s interface.
Despite the lack of speedup achieved versus n-
gram models in the results reported below, the po-
tential for capturing communication partner intu-
itions about AAC user intended utterances seems a
compelling topic for future research.
2 Background and Related Work
Over the past forty years, there has been a vast
array of technological solutions to aid AAC users
who present with severe speech and physical im-
pairments, from methods for generating possible
responses, to techniques for selecting among re-
sponses. The simplest methods to generate lan-
guage involve the use of pre-stored phrases, such as
?hello?, ?thank you?, ?I love you?, etc., which are
available on many AAC devices. Some studies have
indicated that use of such phrases improves the per-
ception of fluid communication (McCoy et al, 2007;
Hoag et al, 2008).
Prediction options vary in AAC devices, rang-
ing from letter-by-letter prediction ? see Higgin-
botham (1992) and Lesher et al (1998) for some
reviews ? to word-based prediction. Some systems
can be quite sophisticated, for example incorporat-
ing latent semantic analysis to aid in the better mod-
eling of discourse-level information (Wandmacher
and Antoine, 2007). The WebCrawler project in Jef-
frey Higginbotham?s lab uses topic-related wordlists
mined from the Web to populate a user?s AAC de-
vice with terminology that is likely to be of utility to
the current topic of conversation.
Going beyond word prediction, there has been
an increased interest in utterance-based approaches
(Todman et al, 2008), which extend prediction
from the character or word level to the level
of whole sentences. For example, systems like
FrameTalker/Contact (Higginbotham and Wilkins,
1999; Wilkins and Higginbotham, 2006) populate
the AAC device with pre-stored phrases that can be
organized in various ways. In a similar vein, re-
cent work reported in Wisenburn and Higginbotham
(2008; 2009) proposed a novel method that uses au-
tomatic speech recognition (ASR) on the speech of
the communication partner, extracts noun phrases
from the speech, and presents those noun phrases on
the AAC device, with frame sentences that the AAC
user can select. Thus if the communication partner
says ?Paris?, the AAC user will be able to select
from phrases like ?Tell me more about Paris? or ?I
want to talk about Paris?. This can speed up the con-
versation by providing topically-relevant responses.
Perhaps the most elaborate system of this kind is the
How Was School Today system (Reiter et al, 2009).
This system, which is geared towards children with
severe communication disabilities, uses data from
sensors, the Web, and other sources as input for a
natural language generation system. The system ac-
quires information about the child?s day in school:
which classes he or she attended, what activities
there were, information about visitors, food choices
at the cafeteria, and so forth. The data are then used
24
to generate natural language sentences, which are
converted to speech via a speech synthesizer. At the
end of the day, the child uses a menu to select sen-
tences that he or she wants the system to utter, and
thereby puts together a narrative that describes what
he/she did. The system allows for vastly more rapid
output than a system where the child constructs each
sentence from scratch.
Perhaps the closest work to what we are proposing
is the study of non-disabled adults in Cornish and
Higginbotham (No Date), where one of the adults
played the role of an AAC user, and the other a non-
disabled communication partner. The participants
completed a narrative, a map and a puzzle task. Of
interest was the relative amount of co-construction
of the other?s utterances by each partner, and in
particular its relation to which of the partners was
the one initiating the attempt to achieve a common
ground with the other speaker ? the ?grounded
contribution owner?. In all tasks both the commu-
nication partner and the AAC user co-constructed
each other?s contributions, but there was the great-
est asymmetry between the two users in the puzzle
task.
In what follows, we will first describe a prelim-
inary experiment of word completion for a simu-
lated AAC user, using sentences from the Enron
email corpus and the New York Times. We then
will present results for word completion and pre-
diction within the context of dialogs in the Switch-
board corpus. While we ultimately believe that
the potential for co-construction goes far beyond
simple word completion/prediction, these experi-
ments serve as a first indication of the challenges
to an enhanced technology-assisted interface for co-
construction with communication partners during
novel text generation.
3 Preliminary experiment
In this section, we present a preliminary experiment
to evaluate the potential utility of our technology-
assisted co-construction scenario. The experiment is
akin to a Shannon Game (Shannon, 1950), but with
a time limit for guesses imposed by the speed of typ-
ing. For the current experiment we chose 5 seconds
per keystroke as the simulated typing speed: target
sentences appeared one character at a time, every
five seconds. The subjects? task was to provide a
Figure 1: Preliminary experimental interface in terminal
window, with 4 predicted completions and cursor below
completion for the current word. If the correct word
is provided by the subject, it is selected by the sim-
ulated AAC user as the next keystroke.
For this preliminary experiment, we used a sim-
ple program running in the terminal window of a
Mac laptop. Figure 1 shows a screenshot from this
program in operation. The target string is displayed
at the top of the terminal window, one character at
a time, with the carat symbol showing white space
word boundaries. Predicted word completions are
made by typing with a standard qwerty keyboard;
and when the enter key is pressed, the word that has
been typed is aligned with the current incomplete
word. If it is consistent with the prefix of the word
that has been typed, it remains as a candidate for
completion. When the current five second interval
has passed, the set of accumulated predictions are
filtered to just those which are consistent with the
new letter that the user would have typed (e.g., ?i?
in Figure 1). If the correct word completion for the
target string is present, it is selected with the follow-
ing keystroke. Otherwise the following letter will
be typed (with the typical 5-second delay) and the
interface proceeds as before.
Three able-bodied, adult, literate subjects were
recruited for this initial experiment, and all three
completed trials with both Enron email and New
York Times target strings. The Enron data
comes from the Enron email dataset (http://www-
2.cs.cmu.edu/?enron/) and the NY Times data from
the English Gigaword corpus (LDC2007T07). Both
corpora were pre-processed to remove duplicate data
(e.g., spam or multiple recipient emails), tabular
data and other material that does not represent writ-
ten sentences. Details on this normalization can be
found in Roark (2009). Both corpora consist of writ-
ten sentences, one heavily edited (newspaper), the
other less formal (email); and both are large enough
to allow for robust statistical language modeling.
25
Ngram training Testing
Task sents words sents words chars
NYT 1.9M 35.6M 10 201 1199
Enron 0.6M 6.1M 10 102 528
Table 1: Statistics for each task of n-gram training corpus
size and test set size in terms of sentences, words and
characters (baseline keystrokes)
The two corpora were split into training and test-
ing sets, to allow for training of n-gram language
models to compare word completion performance.
To ensure fair comparison between n-gram and hu-
man word completion performance, no sentences in
the test sets were seen in the training data. From
each test corpus, we extracted sets of 10 contiguous
sentences at periodic intervals, to use as test or prac-
tice sets. Each subject used a 10 sentence practice
set from the NY Times to become familiar with the
task and interface; then performed the word com-
pletion task on one 10 sentence set from the NY
Times and one 10 sentence set from the Enron cor-
pus. Statistics of the training and test sets are given
in Table 1.
Language models were n-gram word-based mod-
els trained from the given corpora using Kneser-Ney
smoothing (Kneser and Ney, 1995). We performed
no pruning on the models.
We evaluate in terms of keystroke savings per-
centage. Let k be the baseline number of keystrokes
without word completion, which is the number of
characters in the sample, i.e., 1 keystroke per char-
acter. With a given word completion method, let c be
the number of keystrokes required to enter the text,
i.e., if the word completion method provides correct
words for selection, those will reduce the number of
keystrokes required1. Then keystroke savings per-
centage is 100?(k?c)/k, the percentage of original
keystrokes that were saved with word completion.
Table 2 shows the keystroke savings percentage on
our two tasks for three n-gram language models (un-
igram, bigram and trigram) and our three subjects.
It is clear from this table that the n-gram language
models are achieving much higher keystroke savings
than our three human subjects. Further, our three
subjects performed quite similarly, not only in com-
1Each word completion requires a selection keystroke, but
saves the keystrokes associated with the remaining characters
in the selected word.
N-gram Subject
Task 1g 2g 3g 1 2 3
NYT 47.4 54.5 56.0 36.5 32.0 32.9
Enron 54.4 61.4 64.4 34.5 32.0 34.1
Table 2: Keystroke savings percentage for test set across
models and subjects
parison with each other, but across the two tasks.
On the face of it, the relatively poor performance
of the human predictors might be surprising, given
that the original Shannon game was intended to es-
tablish a lower bound on the entropy of English. The
assumption has always been that people have better
language models than we can hope to learn automat-
ically. However, in contrast to the original Shannon
game, our predictions are carried out with a fairly
tight time limit, i.e., predictions need to be made
within a fairly short period in order to be made avail-
able to individuals for word completion. The time
limit within the current scenario is one factor that
seems to be putting the subjects at a disadvantage
compared to automated n-gram models on this task.
There are a couple of additional reasons why n-
gram models are performing better on these tasks.
First, they are specific domains with quite ample
training data for the language models. As the
amount of training data decreases ? which would
certainly be the case for individual AAC users ? the
efficacy of the n-gram models decrease. Second,
there is a 1-character advantage of n-gram models
relative to human predictions in this approach. To
see this point clearly, consider the position at the
start of the string. N-gram models can (for prac-
tical purposes) instantaneously provide predictions
for that word. But our subjects must begin typing
the words that they are predicting for this position
at the same time the individual is making their first
keystroke. Those predictions do not become opera-
tive until after that keystroke. Hence the time over-
head of prediction places a lag relative to what is
possible for the n-gram model. We will return to
this point in the discussion section at the end of the
paper.
There are some scenarios, however, where the
subjects did provide word completions prior to the
trigram language model in both domains. Interest-
ingly, a fairly large fraction of these words were
faster than n-gram for more than one of the three
26
NY Times Enron
company cranbury creditor hearing
creditors denied facility suggestions
foothill jamesway jamesways stairs
plan proposal sandler savings
stock stockholders warrants
Table 3: Words completed using subject suggestions with
fewer keystrokes than trigram model. Bold indicates
more than one subject was faster for that word.
subjects. Table 3 shows the list of these words for
our trials. These tended to be longer, open-class
words with high topical importance. In addition,
they tended to be words with common word pre-
fixes, which lead to higher confusability in the n-
gram model. Of course, common prefixes also lead
to higher confusability in our subjects, yet they ap-
pear to be able to leverage their superior context sen-
sitivity to yield effective disambiguation earlier than
the n-gram model in these cases.
Based on these results, we designed a second ex-
periment, with a few key changes from this prelim-
inary experiment, including an improved interface,
the ability to predict as well as complete, and a do-
main that is closer to a proposed model for this co-
construction task.
4 Switchboard experiment
Based on the preliminary experiment, we created a
new protocol and ran seven able-bodied, adult, lit-
erate subjects. We changed the interface and do-
main in ways that we believed would make a dif-
ference in the ability of subjects to compete with n-
gram models in keystroke savings. What remained
the same was the timing of the interface: characters
for target strings were displayed every five seconds.
Word completions were then evaluated for consis-
tency with what had been typed, and if the correct
word was present, the word was completed and re-
vealed, and typing continued.
Data Our primary motivating case for technology-
assisted co-construction comes from face-to-face di-
alog, yet the corpora from which target strings were
extracted in the preliminary experiments were from
large corpora of text produced under very different
conditions. One corpus that does represent a varied-
topic, conversational dialog scenario is the Switch-
board corpus (Godfrey et al, 1992), which contains
transcripts of both sides of telephone conversations.
The idea in using this data was to provide some num-
ber of utterances of dialog context (from the 10 pre-
vious dialog turns), and then ask subjects to provide
word completions for some number of subsequent
utterances.
While the Switchboard corpus does represent the
kind of conversational dialog we are interested in, it
is a spoken language corpus, yet we are modeling
written (typed) language. The difference between
written and spoken language does present something
of an issue for our task. To mitigate this mismatch
somewhat, we made use of the Switchboard section
of the Penn Treebank (Marcus et al, 1993), which
contains syntactic annotations of the Switchboard
transcripts, including explicit marking of disfluen-
cies (?EDITED? non-terminals in the treebank), in-
terjections or parentheticals such as ?I mean? or
?you know?. Using these syntactic annotations, we
produced edited transcripts that omit much of the
spoken language specific phenomena, thus provid-
ing a closer approximation to the kind of written di-
alogs we would like to simulate. In addition, we de-
cased the corpus and removed all characters except
the following: the 26 letters of the English alphabet,
the apostrophe, the space, and the dash.
Interface Figure 2 shows the graphical user inter-
face that was created for these trials. In the upper
box, ten utterances from the context of the dialog are
presented, with an indication of which speaker (A or
B) took the turn. Participants are asked to first read
this context and then press enter to begin the session.
Below this box, the current utterance is displayed,
along with which of the two participants is currently
producing the utterance. As in the previous experi-
ment, the string is displayed one character at a time
in this region. Below this is a text box where word
completions and predictions are entered. Finally, at
the bottom of the interface, Figure 2 shows two of
the five rows of current word completions (left col-
umn) and next word predictions (right column).
Perhaps the largest departure from the preliminary
experiment is the ability to not only complete the
current word but also to provide predictions about
the subsequent word. The subject uses a space de-
limiter to indicate whether predictions are for the
current word or for the subsequent word. Words
preceding a space are taken as current word com-
pletions; the first word after a space is taken as a
27
Figure 2: Experimental graphical user interface
subsequent word prediction. To just predict the sub-
sequent word, one can lead with a space, which re-
sults in no current word completion and whatever
comes after the space as next word prediction. Once
the current word is complete, any words on the sub-
sequent word prediction list are immediately shifted
to the word completion list. We limited current and
next word predictions to five.
We selected ten test dialogs, and subjects pro-
duced word completions and predictions for three
utterances per dialog, for a total of thirty utterances.
We selected the test dialogs to conform to the fol-
lowing characteristics:
1. Each group of three utterances was consecutive
and spoken by the same person.
2. Each utterance contained more than 15 charac-
ters of text.
3. Each group of three utterances began turn-
initially; the first of the three utterances was
always immediately after the other speaker in
the corpus had spoken at least two consecutive
utterances of 15 characters or more.
4. Each group of three utterances was far enough
into its respective conversation that there was
enough text to provide the ten lines of context
required above.
Language models used to contrast with human
performance on this task were trained separately for
every conversation in the test set. For each conver-
sation, Kneser-Ney smoothed n-gram models were
built using all other conversations in the normalized
Switchboard corpus. Thus no conversation is in its
own training data. Table 4 shows statistics of train-
ing and test sets.
Table 5 shows the results for n-gram models and
our seven subjects on this test. Despite the differ-
ences in the testing scenario from the preliminary
experiment, we can see that the results are very sim-
ilar to what was found in that experiment. Also sim-
ilar to the previous trial was the fact that a large per-
centage of tokens for which subjects provided faster
word completion than the trigram model were faster
for multiple subjects. Table 6 shows the nine words
that were completed faster by more than half of the
subjects than the trigram model. Thus, while there is
some individual variation in task performance, sub-
jects were fairly consistent in their ability to predict.
5 Discussion
In this paper we presented two experiments that
evaluated a new kind of technology-assisted co-
construction interface for communication partners
during time-constrained text generation. Results
Ngram training Testing
Task sents words sents words chars
SWBD 0.66M 3.7M 30 299 1501
Table 4: Statistics for the Switchboard task of n-gram
training corpus size and test set size in terms of utter-
ances, words and characters (baseline keystrokes)
28
N-gram Subject
Task 1g 2g 3g 1 2 3 4 5 6 7
Switchboard 51.0 59.0 60.0 28.7 33.1 28.4 28.6 34.1 31.8 32.5
Table 5: Keystroke savings percentage for Switchboard test set across models and subjects
applied can?t comes
every failure named
physics should supervisor
Table 6: Words completed in more than half of the
Switchboard trials using subject suggestions with fewer
keystrokes than trigram model.
from both experiments are negative, in terms of the
ability of our human subjects to speed up communi-
cation via word prediction under time constraints be-
yond what is achievable with n-gram language mod-
els. These results are somewhat surprising given
conventional wisdom about the superiority of hu-
man language models versus their simplified compu-
tational counterparts. One key reason driving the di-
vergence from conventional wisdom is the time con-
straint on production of predictions. Another is the
artificiality of the task and relative unfamiliarity of
the subjects with the individuals communicating.
While these results are negative, there are reasons
why they should not be taken as an indictment of
the approach as a whole, rather an indication of the
challenges faced by this task. First, we would stress
the fact that we have not yet tested the approach in a
situation where the user knows the speaker well, and
therefore can be presumed to have knowledge well
beyond general knowledge of English and general
topical knowledge. In future work we are planning
experiments based on interactions between people
who have a close relationship with each other. In
such a scenario, we can expect that humans would
have an advantage over statistical language models,
for which appropriate training data would not, in any
case, be available.
None of the domains that we evaluated were a per-
fect match to the application: the text data was not
dialog, and the dialogs were spoken rather than writ-
ten language. Further, the tasks that we evaluated in
this paper are quite rigid compared to what might
be considered acceptable in real use. For example,
our task required the prediction of a particular word
type, whereas in actual use synonyms or other ways
of phrasing the same information will likely be quite
acceptable to most AAC users. In such an applica-
tion, the task is not to facilitate production of a spe-
cific word string, rather production of an idea which
might be realized variously. We were interested in
the tasks reported here as a first step towards under-
standing the problem, and among the lessons learned
are the shortcomings of these very tasks.
Another take-away message relates to the util-
ity of the new interface itself. The subjects in
these trials had the difficult task of quickly pre-
dicting intended words; this is also a communica-
tion task that may be assisted. Providing access to
what n-gram models are predicting may allow the
communication partner to quickly select or winnow
down the options. Further, it is apparent that single
word completions or predictions is not where com-
munication partners are going to achieve order-of-
magnitude speedups in communication; rather such
speedups may be realized in facilitation of larger
phrase or whole utterance production, particularly
when the communication is between familiar part-
ners on known topics.
In summary, this paper presented preliminary re-
sults on the ability of human subjects to provide
word completion and prediction information to users
of AAC systems, through simulation of such a new
interface concept. While the subjects were not
able to match n-gram language models in terms
of keystroke reduction, we did see consistent per-
formance across many subjects and across several
domains, yielding real keystroke reductions on the
stimulus strings. Ultimately, the tasks were not as
representative of real co-construction scenarios a we
would have liked, but they serve to illustrate the
challenges of such an application.
Acknowledgments
This research was supported in part by NIH Grant
#1R01DC009834-01. Any opinions, findings, con-
clusions or recommendations expressed in this pub-
lication are those of the authors and do not necessar-
ily reflect the views of the NIH.
29
References
D.R. Beukelman, S. Fager, L. Ball, and A. Dietz. 2007.
AAC for adults with acquired neurological conditions:
A review. Augmentative and Alternative Communica-
tion, 23(3):230?242.
Jennifer Cornish and Jeffrey Higginbotham. No Date.
Assessing AAC interaction III: Effect of task type
on co-construction & message repair. AAC-
RERC, available from http:aac-rerc.psu.
edu/_userfiles/asha3.pdf.
J.J. Darragh, I.H. Witten, and M.L. James. 1990. The
reactive keyboard: A predictive typing aid. Computer,
23(11):41?49.
J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992.
Switchboard: A telephone speech corpus for research
and develpment. In Proceedings of ICASSP, volume I,
pages 517?520.
D. Jeffery Higginbotham and David Wilkins. 1999.
Frametalker: A system and method for utilizing com-
munication frames in augmented communication tech-
nologies. US Patent No. 5,956,667.
D. Jeffery Higginbotham. 1992. Evaluation of keystroke
savings across five assistive communication technolo-
gies. Augmentative and Alternative Communication,
8:258?272.
Linda A. Hoag, Jan L. Bedrosian, Kathleen F. McCoy,
and Dallas Johnson. 2004. Informativeness and speed
of message delivery trade-offs in augmentative and
alternative communication. Journal of Speech, Lan-
guage, and Hearing Research, 47:1270?1285.
Linda A. Hoag, Jan L. Bedrosian, Kathleen F. Mc-
Coy, and Dallas Johnson. 2008. Hierarchy of
conversational rule violations involving utterance-
based augmentative and alternative communication
systems. Augmentative and Alternative Communica-
tion, 24(2):149?161.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 181?184.
Heidi H. Koester and Simon Levine. 1996. Ef-
fect of a word prediction feature on user perfor-
mance. Augmentative and Alternative Communica-
tion, 12(3):155?168.
Heidi H. Koester and Simon Levine. 1997. Keystroke-
level models for user performance with word predic-
tion. Augmentative and Alternative Communication,
13(4):239257.
Heidi H. Koester and Simon Levine. 1998. Model
simulations of user performance with word predic-
tion. Augmentative and Alternative Communication,
14(1):25?36.
G.W. Lesher, B.J. Moulton, and D.J. Higginbotham.
1998. Techniques for augmenting scanning commu-
nication. Augmentative and Alternative Communica-
tion, 14:81?101.
J. Li and G. Hirst. 2005. Semantic knowledge in word
completion. In Proceedings of the 7th International
ACM Conference on Computers and Accessibility.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Kathleen F. McCoy, Jan L. Bedrosian, Linda A. Hoag,
and Dallas E. Johnson. 2007. Brevity and speed of
message delivery trade-offs in augmentative and alter-
native communication. Augmentative and Alternative
Communication, 23(1):76?88.
Ehud Reiter, Ross Turner, Norman Alm, Rolf Black,
Martin Dempster, and Annalu Waller. 2009. Us-
ing NLG to help language-impaired users tell stories
and participate in social dialogues. In 12th European
Workshop on Natural Language Generation, pages 1?
8. Association for Computational Linguistics.
B. Roark. 2009. Open vocabulary language modeling
for binary response typing interfaces. Technical
Report #CSLU-09-001, Center for Spoken Language
Processing, Oregon Health & Science University.
cslu.ogi.edu/publications/ps/roark09.pdf.
C.E. Shannon. 1950. Prediction and entropy of printed
English. Bell System Technical Journal, 30:50?64.
John Todman, Norman Alm, D. Jeffery Higginbotham,
and Portia File. 2008. Whole utterance approaches in
AAC. Augmentative and Alternative Communication,
24(3):235?254.
K. Trnka, D. Yarrington, K.F. McCoy, and C. Pennington.
2006. Topic modeling in fringe word prediction for
AAC. In Proceedings of the International Conference
on Intelligent User Interfaces, pages 276?278.
K. Trnka, D. Yarrington, J. McCaw, K.F. McCoy, and
C. Pennington. 2007. The effects of word predic-
tion on communication rate for AAC. In Proceed-
ings of HLT-NAACL; Companion Volume, Short Pa-
pers, pages 173?176.
H. Trost, J. Matiasek, and M. Baroni. 2005. The lan-
guage component of the FASTY text prediction sys-
tem. Applied Artificial Intelligence, 19(8):743?781.
T. Wandmacher and J.Y. Antoine. 2007. Methods to in-
tegrate a language model with semantic information
for a word prediction component. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 506?513.
David Wilkins and D. Jeffery Higginbotham. 2006. The
short story of Frametalker: An interactive AAC de-
vice. Perspectives on Augmentative and Alternative
Communication, 15(1):18?21.
30
Bruce Wisenburn and D. Jeffery Higginbotham. 2008.
An AAC application using speaking partner speech
recognition to automatically produce contextually rel-
evant utterances: Objective results. Augmentative and
Alternative Communication, 24(2):100?109.
Bruce Wisenburn and D. Jeffery Higginbotham. 2009.
Participant evaluations of rate and communication ef-
ficacy of an AAC application using natural language
processing. Augmentative and Alternative Communi-
cation, 25(2):78?89.
31
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 56?64,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Robust kaomoji detection in Twitter
Steven Bedrick, Russell Beckley, Brian Roark, Richard Sproat
Center for Spoken Language Understanding, Oregon Health & Science University
Portland, Oregon, USA
Abstract
In this paper, we look at the problem of robust
detection of a very productive class of Asian
style emoticons, known as facemarks or kao-
moji. We demonstrate the frequency and pro-
ductivity of these sequences in social media
such as Twitter. Previous approaches to detec-
tion and analysis of kaomoji have placed lim-
its on the range of phenomena that could be
detected with their method, and have looked
at largely monolingual evaluation sets (e.g.,
Japanese blogs). We find that these emoticons
occur broadly in many languages, hence our
approach is language agnostic. Rather than
relying on regular expressions over a prede-
fined set of likely tokens, we build weighted
context-free grammars that reward graphical
affinity and symmetry within whatever sym-
bols are used to construct the emoticon.
1 Introduction
Informal text genres, such as email, SMS or social
media messages, lack some of the modes used in
spoken language to communicate affect ? prosody
or laughter, for example. Affect can be provided
within such genres through the use of text format-
ting (e.g., capitalization for emphasis) or through the
use of extra-linguistic sequences such as the widely
used smiling, winking ;) emoticon. These sorts of
vertical face representations via ASCII punctuation
sequences are widely used in European languages,
but in Asian informal text genres another class of
emoticons is popular, involving a broader symbol set
and with a horizontal facial orientation. These go by
the name of facemarks or kaomoji. Figure 1 presents
|?_?|
57606710235369473: interesting use of u338 (diagonal overlay; in 
the "combining diacritical marks" section)
57807873274683393: RT @adamsbaldwin: THIS! --> | RT @MelissaTweets 
"By being afraid to go at ?bama politically, people display a soft 
racism." ~ #Repres ... (note peace sign in the "O" in Obama)
57577928942305280: Use of "ake" for "a que" in Spanish
57577937330913280: Example of tricky-to-tokenize tweet (irregular 
spacing): Sigan @TodoBiebs tiene la mejor informacion de Bieber,y 
es completamente cierta.Ellas son geniales. #LEGOO :) <3
57651140510224384: great English abbreviations and use of Unicode: 
Hehe? OK u r sexy, hottie ffw me naw, wud ya RT @Hawtbaeby: 
@enahoanagha Sha? M sad:-(
57581074657718272: You dnt never answer yo phone!
57583914226683904: IHate When Ppl Have Attitudes Wit Mee For No 
Reason. <---- repetition and also "With"->"Wit", "People"->"Ppl"
57850097320460289: Good example of shortening: @China_DollCris u 
lucky smh I'm going str8 thru !!
57610065699549184 <--- using Cyrillic characters to write in 
Spanish/portugese...
57577987641573376: hashtags used as parts of speech; also note "2" 
instead of "to"
57592260870668288: awareness of spelling issues
Fun smileys:
(?_?'!)
57746992926949376 (breve with combining lower line, makes a nice 
tear effect)
?
?
-?
( !!!)
"? ??#
"(?
?
?
? ? ? ? ??
?
?
)#
(?_?)
57592260870668288
57689354826547200: ?(?????)? as well as (!!! )))))))))?
57678625805320192
57675270324367360 (???!???)
57617464455991296
57603166048497664
57596757185544192 " ?[? ? ??]#, (????)
57584430105108480 (??"??)
57745965351837696 (*?m?*)
57745944376119296 (?
?
?
?
-?
?
?
?)
57745659142483968 (????`) 
57825858454433792 <-- uses a fun swirly Tamil letter: (?!?! `)
[o_-]
57606710235369473: interesting use of u338 (diagonal overlay; in 
the "combining diacritical marks" section)
57807873274683393: RT @adamsbaldwin: THIS! --> | RT @MelissaTweets 
"By being afraid to go at ?bama politically, people display a soft 
racism." ~ #Repres ... (note peace sign in the "O" in Obama)
57577928942305280: Use of "ake" for "a que" in Spanish
57577937330913280: Example of tricky-to-tokenize tweet (irregular 
spacing): Sigan @TodoBiebs tiene la mejor informacion de Bieber,y 
es completamente cierta.Ellas son geniales. #LEGOO :) <3
57651140510224384: great English abbreviations and use of Unicode: 
Hehe? OK u r sexy, hottie ffw me naw, wud ya RT @Hawtbaeby: 
@enahoanagha Sha? M sad:-(
57581074657718272: You dnt never answer yo phone!
57583914226683904: IHate When Ppl Have Attitudes Wit Mee For No 
Reason. <---- repetition and also "With"->"Wit", "People"->"Ppl"
57850097320460289: Good example of shortening: @China_DollCris u 
lucky smh I'm going str8 thru !!
57610065699549184 <--- using Cyrillic characters to write in 
Spanish/portugese...
57577987641573376: hashtags used as parts of speech; also note "2" 
instead of "to"
57592260870668288: awareness of spelling issues
Fun smileys:
(?_?'!)
57746992926949376 (breve with combining lower line, makes a nice 
tear effect)
?
?
-?
( !!!)
"? ??#
"(?
?
?
? ? ? ? ??
?
?
)#
(?_?)
57592260870668288
57689354826547200: ?(?????)? as well as (!!! )))))))))?
57678625805320192
57675270324367360 (???!???)
57617464455991296
6031 6048497664
5967 7185 4192 " ?[? ? ??]#, (????)
57584430105108480 (??"??)
57745965351837696 (*?m?*)
74594 376119  (?
?
?
?
-?
?
?
?)
57745659142483968 (????`) 
57825858454433792 <-- uses a fun swirly Tamil letter: (?!?! `)
\(?v?)/
57606710235369473: interesting use of u338 (diagonal overlay; in 
the "combining diacritical marks" section)
57807873274683393: RT @adamsbaldwin: THIS! --> | RT @MelissaTweets 
"By being afraid to go at ?bama politically, people display a soft 
racism." ~ #Repres ... (note peace sign in the "O" in Obama)
57577928942305280: Use of "ake" for "a que" in Spanish
57577937330913280: Example of tricky-to-tokenize tweet (irregular 
spacing): Sigan @TodoBiebs tiene la mejor informacion de Bieber,y 
es completamente cierta.Ellas son geniales. #LEGOO :) <3
57651140510224384: great English abbreviations and use of Unicode: 
Hehe? OK u r sexy, hottie ffw me naw, wud ya RT @Hawtbaeby: 
@enahoanagha Sha? M sad:-(
57581074657718272: You dnt never answer yo phone!
57583914226683904: IHate When Ppl Have Attitudes Wit Mee For No 
Reason. <---- repetition and also "With"->"Wit", "People"->"Ppl"
57850097320460289: Good example of shortening: @China_DollCris u 
lucky smh I'm going str8 thru !!
57610065699549184 <--- using Cyrillic characters to write in 
Spanish/portugese...
57577987641573376: hashtags used as parts of speech; also note "2" 
instead of "to"
57592260870668288: awareness of spelling issues
Fun smileys:
(?_?'!)
57746992926949376 (breve with combining lower line, makes a nice 
tear effect)
?
?
-?
( !!!)
"? ??#
"(?
?
?
? ? ? ? ??
?
?
)#
(?_?)
57592260870668288
57689354826547200: ?(?????)? as well as (!!! )))))))))?
57678625805320192
57675270324367360 (???!???)
57617464455991296
57603166048497664
57596757185544192 " ?[? ? ??]#, (????)
57584430105108480 (??"??)
57745965351837696 (*?m?*)
57745944376119296 (?
?
?
?
-?
?
?
?)
57745659142483968 (????`) 
57825858454433792 <-- uses a fun swirly Tamil letter: (?!?! `)
Figure 1: Some representative kaomoji emoticons
several examples of these sequences, including both
relatively common kaomoji as well as more exotic
and complex creations.
This class of emoticon is far more varied and pro-
ductive than the sideways European style emoticons,
and even lists of on the order of ten thousand emoti-
cons will fail to cover all instances in even a mod-
est sized sample of text. This relative productiv-
ity is due to several factors, including the horizon-
tal orientation, which allows for more flexibility in
configuring features both within the face and sur-
rounding the face (e.g., arms) than the vertical ori-
entation. Another important factor underlying kao-
moji productivity is historical in nature. kaomoji
were developed and popularized in Japan and other
Asian countries whose scripts have always required
multibyte character encodings, and whose users of
electronic communication systems have significant
experience working with characters beyond those
found in the standard ASCII set.
Linguistic symbols from various scripts can be
appropriated into the kaomoji for their resemblence
to facial features, such as a winking eye, and au-
thors of kaomoji sometimes use advanced Unicode
techniques to decorate glyphs with elaborate com-
binations of diacritic marks. For example, the kao-
56
moji in the top righthand corner of Figure 1, includes
an Arabic letter, and Thai vowel diacritics. Accu-
rate detection of these tokens ? and other common
sequences of extra-linguistic symbol sequences ? is
important for normalization of social media text for
downstream applications.
At the most basic level, the complex and unpre-
dictable combinations of characters found within
many kaomoji (often including punctuation and
whitespace, as well as irregularly-used Unicode
combining characters) can seriously confound sen-
tence and word segmentation algorithms that at-
tempt to operate on kaomoji-rich text; since segmen-
tation is typically the first step in any text process-
ing pipeline, issues here can cause a wide variety
of problems downstream. Accurately removing or
normalizing such sequences before attempting seg-
mentation can ensure that existing NLP tools are
able to effectively work with and analyze kaomoji-
including text.
At a higher level, the inclusion of a particular
kaomoji in a text represents a conscious decision
on the part of the text?s author, and fully interpret-
ing the text necessarily involves a degree of inter-
pretation of the kaomoji that they chose to include.
European-style emoticons form a relatively closed
set and are often fairly straightforward to interpret
(both in terms of computational, as well as human,
effort); kaomoji, on the other hand, are far more di-
verse, and interpretation is rarely simple.
In this paper, we present preliminary work on
defining robust models for detecting kaomoji in so-
cial media text. Prior work on detecting and classi-
fying these extra-linguistic sequences has relied on
the presence of fixed attested patterns (see discus-
sion in Section 2) for detection, and regular expres-
sions for segmentation. While such approaches can
capture the most common kaomoji and simple vari-
ants of them, the productive and creative nature of
the phenomenon results in a non-negligible out-of-
vocabulary problem. In this paper, we approach the
problem by examining a broader class of possible
sequences (see Section 4.2) for symmetry using a
robust probabilistic context-free grammar with rule
probabilities proportional to the symmetry or affin-
ity of matched terminal items in the rule. Our PCFG
is robust in the sense that every candidate sequence
is guaranteed to have a valid parse. We use the re-
sulting Viterbi best parse to provide a score to the
candidate sequence ? reranking our high recall list
to achieve, via thresholds, high precision. In addi-
tion, we investigate unsupervised model adaptation,
by incorporating Viterbi-best parses from a small set
of attested kaomoji scraped from websites; and in-
ducing grammars with a larger non-terminal set cor-
responding to regions of the face.
We present bootstrapping experiments for deriv-
ing highly functional, language independent models
for detecting kaomoji in text, on multilingual Twit-
ter data. Our approach can be used as part of a
stand-alone detection model, or as input into semi-
automatic kaomoji lexicon development. Before de-
scribing our approach, we will first present prior
work on this class of emoticon.
2 Prior Work
Nakamura et al (2003) presented a natural language
dialogue system that learned a model for generat-
ing kaomoji face marks within Japanese chat. They
trained a neural net to produce parts of the emoti-
con ? mouth, eyes, arms and ?optional things? as
observed in real world data. They relied on a hand-
constructed inventory of observed parts within each
of the above classes, and stitched together predicted
parts into a complete kaomoji using simple tem-
plates.
Tanaka et al (2005) presented a finite-state
chunking approach for detecting kaomoji in
Japanese on-line bulletin boards using SVMs with
simple features derived from a 7 character window.
Training was performed on kaomoji dictionaries
found online. They achieved precision and recall in
the mid-80s on their test set, which was a significant
recall improvement (17% absolute) and modest
precision improvement (1.5%) over exact match
within the dictionaries. They note certain kinds of
errors, e.g., ?(Thu)? which demonstrate that their
chunking models are (unsurprisingly) not capturing
the typical symmetry of kaomoji. In addition, they
perform classification of the kaomoji into 6 rough
categories (happy, sad, angry, etc.), achieving high
performance (90% accuracy) using a string kernel
within an SVM classifier.
Ptaszynski et al (2010) present work on a large
database of kaomoji, which makes use of an analy-
57
sis of the gestures conveyed by the emoticons and
their relation to a theory of non-verbal expressions.
They created an extensive (approximately 10,000
entry) lexicon with 10 emotion classes, and used
this database as the basis of both emoticon extrac-
tion from text and emotion classification. To detect
an emoticon in text, their system (named ?CAO?)
looked for three symbols in a row from a vocabulary
of the 455 most frequent symbols in their database.
Their approach led to a 2.4% false negative rate
when evaluated on 1,000 sentences extracted from
Japanese blogs. Once detected, the system extracts
the emoticon from the string using a gradual relax-
ation from exact match to approximate match, with
various regular expressions depending on specific
partial match criteria. A similar deterministic al-
gorithm based on sequenced relaxation from exact
match was used to assign affect to the emoticon.
Our work focuses on the emoticon detection
stage, and differs from the above systems in a num-
ber of ways. First, while kaomoji were popularized
in Asia, and are most prevalent in Asian languages,
they not only found in messages in those languages.
In Twitter, which is massively multilingual, we find
kaomoji with some frequency in many languages,
including European languages such as English and
Portuguese, Semitic languages and a range of Asian
languages. Our intent is to have a language inde-
pendent algorithm that looks for such sequences in
any message. Further, while we make use of online
dictionaries as development data, we appreciate the
productivity of the phenomenon and do not want to
restrict the emoticons that we detect to those con-
sisting of pre-observed characters. Hence we focus
instead on characteristics of kaomoji that have been
ignored in the above models: the frequent symmetry
of the strings. We make use of context-free mod-
els, built in such a way as to guarantee a parse for
any candidate sequence, which permits exploration
of a much broader space of potential candidates than
the prior approaches, using very general models and
limited assumptions about the key components of
the emoticons.
3 Data
Our starting resources consisted of a large, multi-
lingual corpus of Twitter data as well as a smaller
collection of kaomoji scraped from Internet sources.
Our Twitter corpus consists of approximately 80
million messages collected using Twitter?s ?Stream-
ing API? over a 50-day period from June through
August 2011. The corpus is extremely linguistically
diverse; human review of a small sample identified
messages written in >30 languages. The messages
themselves exhibit a wide variety of phenomena, in-
cluding substantial use of different types of Inter-
net slang and written dialect, as well as numerous
forms of non-linguistic content such as emoticons
and ?ASCII art.?
We took a two-pronged approach to developing
a set of ?gold-standard? kaomoji. Our first ap-
proach involved manually ?scraping? real-world ex-
amples from the Internet. Using a series of hand-
written scripts, we harvested 9,193 examples from
several human-curated Internet websites devoted to
collecting and exhibiting kaomoji. Many of these
consisted of several discrete sub-units, typically in-
cluding at least one ?face? element along with a
small amount of additional content. For example,
consider the following kaomoji, which appeared in
this exact form eight times in our Twitter corpus:
?(*???)??+.???????+.??
(???*)? 9
?(!"?" ? ?)??????#? 8
?(??????)??? 9
?(???`;)???? 52
??????? 5
?_(?_? ) 1
?/T?T)/??????? 4
???????????????????????? 1
?????? 1
????? 426
?????? 1
?(?????)/????? 1
?????????????? 4
??(?????)(??_ _)???? 10
????(???)??????? 1
????(T-T)?(^^ )???? 2
???Uo???oU??? 1
?????+.(???)(???)?+.?!! 1
. Note that, in this case, the
?face? is followed by a small amount of hiragana,
and that the message concludes with a dingbat in the
form of a ?heart? symbol.1
Of these 9,193 scraped examples, we observed
?3,700 to appear at least once in our corpus of
Twitter messages, and ?2,500 more than twice.
The most common kaomoji occurred with frequen-
cies in the low hundreds of thousands, although the
frequency with which individual kaomoji appeared
roughly followed a power-law distribution, meaning
that there were a small number that occurred with
great frequency and a much larger number that only
appeared rarely.
From this scraped corpus, we attempted to iden-
tify a subset that consisted solely of ?faces? to serve
as a high-precision training set. After observing
that nearly all of the faces involved a small number
of characters bracketed one of a small set of natu-
ral grouping characters (parentheses, ?curly braces,?
1Note as well that this kaomoji includes not only a wide va-
riety of symbols, but that some of those symbols are themselves
modified using combining diacritic marks. This is a common
practice in modern kaomoji, and one that complicates analysis.
58
etc.), we extracted approximately 6,000 substrings
matching a very simple regular expression pattern.
This approach missed many kaomoji, and of the
examples that it did detect, many were incom-
plete (in that they were missing any extra-bracketed
content? arms, ears, whiskers, etc.) However, the
contents of this ?just faces? sub-corpus offered de-
cent coverage of many of the core kaomoji phenom-
ena in a relatively noise-free manner. As such, we
found it to be useful as ?seed? data for the grammar
adaptation described in section 4.4.
In addition to our ?scraped? kaomoji corpus, we
constructed a smaller corpus of examples drawn di-
rectly from our Twitter corpus. The kaomoji phe-
nomenon is complex enough that capturing it in its
totality is difficult. However, it is possible to capture
a subset of kaomoji by looking for regions of per-
fect lexical symmetry. This approach will capture
many of the more regularly-formed and simple kao-
moji (for example, ?(-_-)?), although it will miss
many valid kaomoji. Using this approach, we iden-
tied 3,580 symmetrical candidate sequences; most
of these were indeed kaomoji, although there were
several false positives (for example, symmetrical se-
quences of repeated periods, question marks, etc.).
Using simple regular expressions, we were able to
remove 289 such false positives.
Interestingly, there was very little overlap be-
tween the corpus scraped from the Web and the sym-
metry corpus. A total of 39 kaomoji appeared in ex-
actly the same form in both sets. We noted, however,
that the kaomoji harvested from the Web tended to
be longer and more elaborate than those identified
from our Twitter corpus using the symmetry heuris-
tic (Mann-Whitney U, p < 0.001), and as previously
discussed, the Web kaomoji often contained one or
more face elements. Thus we expanded our defi-
nition of overlap, and counted sequences from the
symmetrical corpus that were substrings of scraped
kaomoji. Using this criterion, we identified 177 pos-
sibly intersecting kaomoji. The fact that so few indi-
vidual examples occurred in both corpora illustrates
the extremely productive nature of the phenomenon.
4 Methods
4.1 Graphical similarity
The use of particular characters in kaomoji is ul-
timately based on their graphical appearance. For
Figure 2: Ten example character pairs with imperfect
(but very high) symmetry identified by our algorithm.
Columns are: score, hex code point 1, hex code point
2, glyph 1, glyph 2.
example, good face delimiters frequently include
mated brackets or parentheses, since these elements
naturally look as if they delimit material. Further-
more, there are many characters which are not tech-
nically ?paired,? but look roughly more-or-less sym-
metrical. For example, the Arabic-Indic digits!???"and
!???" are commonly used as bracketing delimiters, for
example: !???". These characters can serve both as
?arms? as well as ?ears.?
Besides bracketing, symmetry plays an additional
role in kaomoji construction. Glyphs that make good
?eyes? are often round; ?noses? are often symmet-
ric about their central axis. Therefore a measure of
graphical similarity between characters is desirable.
To that end, we developed a very simple measure
of similarity. From online sources, we downloaded
a sample glyph for each code point in the Unicode
Basic Multilingual Plane, and extracted a bitmap for
each. In comparing two glyphs we first scale them
to have the same aspect ratio if necessary, and we
then compute the proportion of shared pixels be-
tween them, with a perfect match being 1 and the
worst match being 0. We can thus compute whether
two glyphs look similar; whether one glyph is a good
mirror image of the other (by comparing glyph A
with the mirror image of glyph B); and whether a
glyph is (vertically) symmetric (by computing the
similarity of the glyph and its vertical mirror image).
The method, while clearly simple-minded,
nonetheless produces plausible results, as seen in
Figure 2, which shows the best 10 candidates for
mirror image character pairs. We also calculate
the same score without flipping the image verti-
cally, which is also used to score possible symbol
matches, as detailed in Section 4.3.
59
4.2 Candidate extraction
We perform candidate kaomoji extraction via a very
simple hidden Markov model, which segments all
strings of Unicode graphemes into contiguous re-
gions that are either primarily linguistic (mainly
language encoding symbols2) or primarily non-
linguistic (mainly punctuation, or other symbols).
Our candidate emoticons, then, are this extensive
list of mainly non-linguistic symbol sequences. This
is a high recall approach, returning most sequences
that contain valid emoticons, but quite low precision,
since it includes many other sequences as well (ex-
tended runs of punctuation, etc.).
The simple HMM consists of 2 states: call them
A (mainly linguistic) and @ (mainly non-linguistic).
Since there are two emitted symbol classes (linguis-
tic L and non-linguistic N ), each HMM state must
have two emission probabilities, one for its domi-
nant symbol class (L in A and N in @) and one
for the other symbol class. Non-linguistic symbols
occur quite often in linguistic sequences, as punc-
tuation for example. However, sequences of, say,
3 or more in a row are not particularly frequent.
Similarly, linguistic symbols occur often in kaomoji,
though not often in sequences of, say, 3 or more.
Hence, to segment into contiguous sequences of a
certain number in a row, the probability of transition
from state A to state @ or vice versa must be signif-
icantly lower than the probability of emitting one or
two N from A states or L from @ states. We thus
have an 8 parameter HMM (four transition and four
emission probabilities) that was coarsely parameter-
ized to have the above properties, and used it to ex-
tract candidate non-linguistic sequences for evalua-
tion by our PCFG model.
Note that this approach does have the limitation
that it will trim off some linguistic symbols that oc-
cur on the periphery of an emoticon. Future versions
of this part of the system will address this issue by
extending the HMM. For this paper, we made use of
a slightly modified version of this simple HMM for
candidate extraction. The modifications involved the
addition of a special input state for whitespace and
full-stop punctuation, which helped prevent certain
very common classes of false-positive.
2Defined as a character having the Unicode ?letter? charac-
ter property.
rule score rule score
X? a X b S(a,b) X? a b S(a,b)
X? a X  X? X a 
X? a ? X? X X ?
Table 1: Rule schemata for producing PCFG
4.3 Baseline grammar induction
We perform a separate PCFG induction for ev-
ery candidate emoticon sequence, based on a small
set of rule templates methods for assigning rule
weights. By inducing small, example-specific
PCFGs, we ensure that every example has a valid
parse, without growing the grammar to the point that
the grammar constant would seriously impact parser
efficiency.
Table 1 shows the rule schemata that we used for
this paper. The resulting PCFG would have a single
non-terminal (X) and the variables a and b would be
instantiated with terminal items taken from the can-
didate sequence. Each instantiated rule receives a
probability proportional to the assigned score. For
the rules that ?pair? symbols a and b, a score is as-
signed in two ways, call them S1(a, b) and S2(a, b)
(they will be defined in a moment). Then S(a, b) =
max(S1(a, b) and S2(a, b)). If S(a, b) < ?, for some
threshold ?,3 then no rule is generated. S1 is the
graphical similarity of the first symbol with the verti-
cal mirror image of the second symbol, calculated as
presented in Section 4.1. This will give a high score
for things like balanced parentheses. S2 is the graph-
ical similarity of the first symbol with the second
symbol (not vertically flipped), which gives high
scores to the same or similar symbols. This permits
matches for, say, eyes that are not symmetric due to
an orientation of the face, e.g.,
!???"
(!#!). The other pa-
rameters (, ? and ?) are included to allow for, but
penalize, unmatched symbols in the sequence.
All possible rules for a given sequence are instan-
tiated using these templates, by placing each symbol
in the a slot with all subsequent symbols in the b slot
and scoring, as well as creating all rules with just a
alone for that symbol. For example, if we are given
the kaomoji (o o;) specific rules would be created
if the similarity scores were above threshold. For the
second symbol ?o?, the algorithm would evaluate the
3For this paper, ? was chosen to be 0.7.
60
similarity between ?o? and each of the four symbols
to its right , o, ; and ).
The resulting PCFG is normalized by summing
the score for each rule and normalizing by the score.
The grammar is then transformed to a weakly equiv-
alent CNF by binarizing the ternary rules and in-
troducing preterminal non-terminals. This grammar
is then provided to the parser4, which returns the
Viterbi best parse of the candidate emoticon along
with its probability. The score is then converted to
an approximate perplexity by dividing the negative
log probability by the number of unique symbols in
the sequence and taking the exponential.
4.4 Grammar enhancement and adaptation
The baseline grammar induction approach outlined
in the previous section can be improved in a cou-
ple of ways, without sacrificing the robustness of the
approach. One way is through grammar adaptation
based on automatic parses of attested kaomoji. The
other is by increasing the number of non-terminals
in the grammar, according to a prior understanding
of their typical (canonical) structure. We shall dis-
cuss each in turn.
Given a small corpus of attested emoticons (in our
case, the ?just faces? sub-corpus described in sec-
tion 3), we can apply the parser above to those ex-
amples, and extract the Viterbi best parses into an
automatically created treebank. From that treebank,
we extract counts of rule productions and use these
rule counts to inform our grammar estimation. The
benefit of this approach is that we will obtain addi-
tional probability mass for frequently observed con-
structions in that corpus, thus preferring commonly
associated pairs within the grammar. Of course, the
corpus only has a small fraction of the possible sym-
bols that we hope to cover in our robust approach, so
we want to incorporate this information in a way that
does not limit the kinds of sequences we can parse.
We can accomplish this by using simple Maxi-
mum a Posteriori (MAP) adaptation of the grammar
(Bacchiani et al, 2006). In this scenario, we will
first use our baseline method of grammar induction,
using the schemata shown in Table 1. The scores
derived in that process then serve as prior counts
4We used the BUBS parser (Bodenstab et al, 2011).
http://code.google.com/p/bubs-parser/
for the rules in the grammar, ensuring that all of
these rules continue to receive probability mass. We
then add in the counts for each of the rules from the
treebank. Many of the rules may have been unob-
served in the corpus, in which case they receive no
additional counts; observed rules, however, will re-
ceive extra weight proportional to their frequency in
that corpus. Note that these additional weights can
be scaled according to a given parameter. After in-
corporating these additional counts, the grammar is
normalized and parsing is performed as before. Of
course, this process can be iterated ? a new auto-
matic treebank can be produced based on an adapted
grammar, and so on.
In addition to grammar adaptation, we can en-
rich our grammars by increasing the non-terminal
sets. To do this, we created a nested hierarchy
of ?regions? of the emoticons, with constraints re-
lated to the canonical composition of the faces,
e.g., eyes are inside of faces, noses/mouths between
eyes, etc. These non-terminals replace our generic
non-terminal X in the rule schemata. For the cur-
rent paper, we included the following five ?region?
non-terminals: ITEM, OUT, FACE, EYES, NM. The
non-terminal ITEM is intended as a top-most non-
terminal to allow multiple emoticons in a single se-
quence, via an ITEM ? ITEM ITEM production.
None of the others non-terminals have repeating pro-
ductions of that sort ? so this replaces the X? X X
production from Table 1.
Every production (other than ITEM ? ITEM
ITEM) has zero or one non-terminals on the right-
hand side. In our new schemata, non-terminals on
the left-hand side can only have non-terminals on the
right-hand side at the same or lower levels. This en-
forces the nesting constraint, i.e., that eyes are inside
of the face. Levels can be omitted however ? e.g.,
eyes but no explicit face delimiter ? hence we can
?skip? a level using unary projections, e.g., FACE?
EYES. Those will come with a ?skip level? weight.
Categories can also rewrite to the same level (with a
?stay level? weight) or rewrite to the next level af-
ter emitting symbols (with a ?move to next level?
weight).
To encode a preference to move to the next level
rather than to stay at the same level, we assign a
weight of 1 to moving to the next level and a weight
of 0.5 to staying at the same level. The ?skip?
61
rule score
ITEM ? ITEM ITEM ?
ITEM ? OUT ?
OUT ? a OUT b S(a,b) + 0.5
OUT ? a OUT  + 0.5
OUT ? OUT a  + 0.5
OUT ? a FACE b S(a,b) + 1
OUT ? a FACE  +1
OUT ? FACE a  +1
OUT ? FACE 0.5
FACE ? a FACE b S(a,b) + 0.5
FACE ? a FACE  + 0.5
FACE ? FACE a  + 0.5
FACE ? a EYES b S(a,b) + 1
FACE ? a EYES  +1
FACE ? EYES a  +1
FACE ? EYES 0.1
EYES ? a EYES b S(a,b) + 0.5
EYES ? a EYES  + 0.5
EYES ? EYES a  + 0.5
EYES ? a NM b S(a,b) + 1
EYES ? a NM  +1
EYES ? NM a  +1
EYES ? NM 0.1
EYES ? a b S(a,b) + 1
NM ? a NM 
NM ? NM a 
NM ? a ?
Table 2: Rule schemata for expanded non-terminal set
weights depend on the level, e.g., skipping OUT
should be cheap (weight of 0.5), while skipping
the others more expensive (weight of 0.1). These
weights are like counts, and are added to the similar-
ity counts when deriving the probability of the rule.
Finally, there is a rule in the schemata in Table 1 with
a pair of symbols and no middle non-terminal. This
is most appropriate for eyes, hence will only be gen-
erated at that level. Similarly, the single symbol on
the right-hand side is for the NM (nose/mouth) re-
gion. Table 2 presents our expanded rule schemata.
Note that the grammar generated with this ex-
panded set of non-terminals is robust, just as the ear-
lier grammar is, in that every sequence is guaranteed
to have a parse. Further, it can be adapted using the
same methods presented earlier in this section.
5 Experimental Results
Using the candidate extraction methodology de-
scribed in section 4.2, we extracted 1.6 million dis-
tinct candidates from our corpus of 80 million Twit-
ter messages (candidates often appeared in multi-
ple messages). These candidates included genuine
emoticons, as well as extended strings of punc-
tuation and other ?noisy? chunks of text. Gen-
uine kaomoji were often picked up with some
amount of leading or trailing punctuation, for exam-
ple: ?..\(??`)/?; other times, kaomoji beginning
with linguistic characters were truncated: (^?*)?.
We provided these candidates to our parser un-
der four different conditions, each one producing
1.5 million parse trees: the single non-terminal ap-
proach described in section 4.3 or the enhanced mul-
tiple non-terminal approach described in section 4.4,
both with and without training via the Maximum A
Posteriori approach described in section 4.4.
Using the weighted-inside-score method de-
scribed in section 4.3, we produced a ranked list
of candidate emoticons from each condition?s out-
put. ?Well-scoring? candidates were ones for which
the parser was able to construct a low-cost parse.
We evaluated our approach in two ways. The first
way examined precision? how many of the best-
scoring candidate sequences actually contained kao-
moji? Manually reviewing all 1.6 million candidates
was not feasible, so we evaluated this aspect of our
system?s performance on a small subset of its out-
put. Computational considerations forced us to pro-
cess our large corpus in parallel, meaning that our set
of 1.6 million candidate kaomoji was already parti-
tioned into 160 sets of?10,000 candidates each. We
manually reviewed the top 1,000 sorted results from
one of these partitions, and flagged any entries that
did not contain or consist of a face-like kaomoji. The
results of each condition are presented in table 3.
The second evaluation approach we will exam-
ine looks at how our method compares with the
trigram-based approach described by (Yamada et al,
2007) (as described by (Ptaszynski et al, 2010)).
We trained both smoothed and unsmoothed lan-
guage models 5 on the ?just faces? sub-corpus used
for the A Posteriori grammar enhancement, and
computed perplexity measurements for the same
set ?10,000 candidates used previously. Table 3
presents these results; clearly, a smoothed trigram
model can achieve good results. The unsmoothed
model at first glance seems to have performed very
well; note, however, that only approximately 600
(out of nearly 10,000) candidates were ?matched?
by the unsmoothed model (i.e., they did not contain
any OOV symbols and therefore had finite perplex-
ity scores), yielding a very small but high-precision
set of emoticons.
Looking at precision, the model-based ap-
proaches outperformed our grammar approach. It
5Using the OpenGrm ngram language modeling toolkit.
62
Condition P@1000 MAP
Single Nonterm, Untrained 0.662 0.605
Single Nonterm, Trained 0.80 0.945
Multiple Nonterm, Untrained 0.795 0.932
Multiple Nonterm, Trained 0.885 0.875
Unsmoothed 3-gram 0.888 0.985
Smoothed 3-gram 0.905 0.956
Mixed, Single Nonterm, Untrained 0.662 0.902
Mixed, Single Nonterm, Trained 0.804 0.984
Mixed, Multiple Nonterm, Untrained 0.789 0.932
Mixed, Multiple Nonterm, Trained 0.878 0.977
Table 3: Experimental Results.
should be noted, however, that the trigram approach
was much less tolerant of certain non-standard for-
mulations involving novel characters or irregular
formulations ((?!?)?(?o?)?(??o??)
and
(?!?)?(?o?)?(??o??) are examples of kao-moji that our grammar-based approach ranked more
highly than did the trigram approach). The two
approaches also had different failure profiles. The
grammar approach?s false positives tended to be
symmetrical sequences of punctuation, whereas the
language models? were more variable. Were we to
review a larger selection of candidates, we believe
that the structure-capturing nature of the grammar
approach would enable it to outperform the more
simplistic approach.
We also attempted a hybrid ?mixed? approach in
which we used the language models to re-rank the
top 1,000 ?best? candidates from our parser?s output.
This generally resulted in improved performance,
and for some conditions the improvement was sub-
stantial. Future work will explore this approach in
greater detail and over larger amounts of data.
6 Discussion
We describe an almost entirely unsupervised ap-
proach to detecting kaomoji in irregular, real-world
text. In its baseline state, our system is able to ac-
curately identify a large number of examples using
a very simple set of templates, and can distinguish
kaomoji from other non-linguistic content (punctu-
ation, etc.). Using minimal supervision, we were
able to effect a dramatic increase in our system?s
performance. Visual comparison of the ?untrained?
results with the ?trained? results was instructive.
The untrained systems? results were very heavily in-
fluenced by their template rules? strong preference
for visual symmetry. Many instances of symmet-
rical punctuation sequences (e.g., ..?..) ended
up being ranked more highly than even fairly sim-
ple kaomoji, and in the absence of other informa-
tion, the length of the input strings also played a too-
important role in their rankings.
The MAP-ehanced systems? results, on the other
hand, retained their strong preference for symme-
try, but were also influenced by the patterns and
characters present in their training data. For ex-
ample, two of the top-ranked ?false positives? from
the enhanced system were the sequences >,< and
= =, both of which (while symmetrical) also con-
tain characters often seen in kaomoji. By using more
structurally diverse training data, we expect further
improvements in this area. Also, our system cur-
rently relies on a very small number of relatively
simplistic grammar templates; expanding these to
encode additional structure may also help.
Due to our current scoring mechanism, our parser
is biased against certain categories of kaomoji. Par-
ticularly poorly-scored are complex creations such
as (((| ???? ??|? ???=???| ???? ??|?))). In this example, the large number
of combining characters and lack of obvious nest-
ing therein confounded our templates and produced
expensive parse trees. Future work will involve im-
proved handling of such cases, either by modified
parsing schemes or additional templates.
One other area of future work is to match par-
ticular kaomoji, or fragments of kaomoji (e.g. par-
ticular eyes), to particular affective states, or other
features of the text. Some motifs are already well
known: for example, there is wide use of TT, or the
similar-looking Korean hangeul vowel yu, to repre-
sent crying eyes. We propose to do this initially by
computing the association between particular kao-
moji and words in the text. Such associations may
yield more than just information on the likely af-
fect associated with a kaomoji. So, for example,
using pointwise mutual information as a measure
of association, we found that in our Twitter corpus,
(*?_?*) seems to be highly associated with tweets
about Korean pop music, *-* with Brazilian post-
ings, and with Indonesian postings. Such
associations presumably reflect cultural preferences,
and could prove useful in identifying the provenance
of a message even if more conventional linguistic
techniques fail.
63
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian
Roark. 2011. Adaptive beam-width prediction for ef-
ficient cyk parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics, pages 440?449.
Junpei Nakamura, Takeshi Ikeda, Nobuo Inui, and
Yoshiyuki Kotani. 2003. Learning face mark for nat-
ural language dialogue system. In Proc. Conf. IEEE
Int?l Conf. Natural Language Processing and Knowl-
edge Eng, pages 180?185.
Michal Ptaszynski, Jacek Maciejewski, Pawel Dybala,
Rafal Rzepka, and Kenji Araki. 2010. Cao: A fully
automatic emoticon analysis system based on theory
of kinesics. IEEE Transactions on Affective Comput-
ing, 1:46?59.
Yuki Tanaka, Hiroya Takamura, and Manabu Okumura.
2005. Extraction and classification of facemarks with
kernel methods. In Proc. 10th Int?l Conf. Intelligent
User Interfaces.
T. Yamada, S. Tsuchiya, S. Kuroiwa, and F. Ren. 2007.
Classification of facemarks using n-gram. In Inter-
national Conference on Natural Language Processing
and Knowledge Engineering, pages 322?327.
64
NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 9?18,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
Discourse-Based Modeling for AAC
Margaret Mitchell Richard Sproat
Center for Spoken Language Understanding
Oregon Health & Science University
m.mitchell@abdn.ac.uk, rws@xoba.com
Abstract
This paper presents a method for an AAC sys-
tem to predict a whole response given features
of the previous utterance from the interlocu-
tor. It uses a large corpus of scripted dialogs,
computes a variety of lexical, syntactic and
whole phrase features for the previous utter-
ance, and predicts features that the response
should have, using an entropy-based measure.
We evaluate the system on a held-out portion
of the corpus. We find that for about 3.5% of
cases in the held-out corpus, we are able to
predict a response, and among those, over half
are either exact or at least reasonable substi-
tutes for the actual response. We also present
some results on keystroke savings. Finally
we compare our approach to a state-of-the-art
chatbot, and show (not surprisingly) that a sys-
tem like ours, tuned for a particular style of
conversation, outperforms one that is not.
Predicting possible responses automatically
by mining a corpus of dialogues is a
novel contribution to the literature on whole
utterance-based methods in AAC. Also useful,
we believe, is our estimate that about 3.5-4.0%
of utterances in dialogs are in principle pre-
dictable given previous context.
1 Introduction
One of the overarching goals of Augmentative and
Alternative Communication technology is to help
impaired users communicate more quickly and more
naturally. Over the past thirty years, solutions
that attempt to reduce the amount of effort needed
to input a sentence have include semantic com-
paction (Baker, 1990), and lexicon- or language-
model-based word prediction (Darragh et al, 1990;
Higginbotham, 1992; Li and Hirst, 2005; Trost et
al., 2005; Trnka et al, 2006; Trnka et al, 2007;
Wandmacher and Antoine, 2007), among others. In
recent years, there has been an increased interest
in whole utterance-based and discourse-based ap-
proaches (see Section 2). Such approaches have
been argued to be beneficial in that they can speed up
the conversation, thus making it appear more felici-
tous (McCoy et al, 2007). Most commercial tablets
sold as AAC devices contain an inventory of canned
phrases, comprising such items as common greet-
ings, polite phrases, salutations and so forth. Users
can also enter their own phrases, or indeed entire se-
quences of phrases (e.g., for a prepared talk).
The work presented here attempts to take whole
phrase prediction one step further by automatically
predicting appropriate responses to utterances by
mining conversational text. In an actual deploy-
ment, one would present a limited number of pre-
dicted phrases in a prominent location on the user?s
device, along with additional input options. The user
could then select from these phrases, or revert to
other input methods. In actual use, one would also
want such a system to incorporate speech recogni-
tion (ASR), but for the present we restrict ourselves
to typed text ? which is perfectly appropriate for
some modes of interaction such as on-line social me-
dia domains. Using a corpus of 72 million words
from American soap operas, we isolate features use-
ful in predicting an appropriate set of responses for
the previous utterance of an interlocutor. The main
results of this work are a method that can automati-
9
cally produce appropriate responses to utterances in
some cases, and an estimate of what percentage of
dialog may be amenable to such techniques.
2 Previous Work
Alm et al (1992) discuss how AAC technology can
increase social interaction by having the utterance,
rather than the letter or word, be the basic unit
of communication. Findings from conversational
analysis suggest a number of utterances common to
conversation, including short conversational openers
and closers (hello, goodbye), backchannel responses
(yeah?), and quickfire phrases (That?s too bad.). In-
deed ?small talk? is central to smooth-flowing con-
versation (King et al, 1995). Many modern AAC
systems therefore provide canned small-talk phrases
(Alm et al, 1993; Todman et al, 2008).
More complex conversational utterances are chal-
lenging to predict, and recent systems have used
a variety of approaches to generate longer phrases
from minimal user input. One approach relies on
telegraphic input, where full sentences are con-
structed from a set of uninflected words, as in the
Compansion system (McCoy et al, 1998). This
system employs a semantic parser to capture the
meaning of the input words and generates using
the Functional Unification Formalism (FUF) system
(Elhadad, 1991). One of the limitations of this ap-
proach is that information associated with each word
is primarily hand-coded on the basis of intuition; as
a result, the system cannot handle the problem of un-
restricted vocabulary. Similar issues arise in seman-
tic authoring systems (Netzer and Elhadad, 2006),
where at each step of the sentence creation process,
the system offers possible symbols for a small set of
concepts, and the user can select which is intended.
Recent work has also tried to handle the complex-
ity of conversation by providing full sentences with
slots that can be filled in by the user. Dempster et
al. (2010) define an ontology where pieces of hand-
coded knowledge are stored and realized within sev-
eral syntactic templates. Users can generate utter-
ances by entering utterance types and topics, and
these are filled into the templates. The Frametalker
system (Higginbotham et al, 1999) uses contextual
frames ? basic sentences for different contexts ?
with a set vocabulary for each. The intuition be-
hind this system is that there are typical linguistic
structures for different situations and the kinds of
words that the user will need to fill in will be se-
mantically related to the context. Wisenburn and
Higginbotham (2008) extend this technology using
ASR on the speech of the interlocutor. The system
extracts noun phrases from the speech and presents
those noun phrases on the AAC device, with frame
sentences that the user can then select. Thus, if the
interlocutor says Paris, the AAC user will be able to
select from phrases like Tell me more about Paris or
I want to talk about Paris.
Other approaches provide a way for users to
quickly find canned utterances. WordKeys (Langer
and Hickey, 1998) allows users to access stored
phrases by entering key words. This system ap-
proaches generation as a text retrieval task, using a
lexicon derived from WordNet to expand user input
to find possible utterances. Dye et al (1998) intro-
duce a system that utilizes scripts for specific situa-
tions. Although pre-stored scripts work reasonably
well for specific contexts, the authors find (not unex-
pectedly) that a larger number of scripts are needed
for the system to be generally effective.
3 The Soap Opera Corpus
In this work we attempt a different approach, devel-
oping a system that can learn appropriate responses
to utterances given a corpus of conversations.
Part of the difficulty in automatically generating
conversational utterances is that very large corpora
of naturally occurring dialogs are non-existent. The
closest such corpus is Switchboard (Godfrey and
Holliman, 1997), which contains 2,400 two-sided
conversations with about 1.4 million words. The in-
terlocutors in Switchboard are not acquainted with
each other and they are instructed to discuss a par-
ticular topic. While the dialogs are ?natural? to a
point, because they involve people who have never
previously met, they are not particularly reflective of
the kinds of conversations between intimates that we
are interested in helping impaired users with.
We thus look instead to a corpus of scripted di-
alogs taken from American soap operas. The web-
site tvmegasite.net contains soap opera scripts
that have been transcribed by aficionados of the var-
ious series. The scripts include utterances marked
10
with information on which character is speaking,
and a few dramatic cues. We downloaded 72 mil-
lion words of text, with 5.5 million utterances. Soap
opera series downloaded were: All my Children, As
the World Turns, The Bold and the Beautiful, Days
of our Lives, General Hospital, Guiding Light, One
Life to Live and The Young and the Restless. The text
was cleaned to remove HTML markup and other ex-
traneous material, and the result was a set of 550,000
dialogs, with alternating utterances by (usually) two
speakers. These dialogs were split 0.8/0.1/0.1 into
training, development testing and testing portions,
respectively. All results reported in this paper are on
the development test set.
While soap operas may not be very representative
of most people?s lives, the corpus nonetheless has
three advantages. First of all, the corpus is large.
Second, the language tends to be fairly colloquial.
Third, many of the dialogs take place between char-
acters who are supposed to know each other well,
often intimately; thus the topics might be more re-
flective of casual conversation between friends and
intimates than the dialogs one finds in Switchboard.
4 Data Analysis, Feature Extraction and
Utterance Prediction
Each dialog was processed using the Stanford Core
NLP tools. The Stanford tools perform part of
speech tagging (Toutanova et al, 2003), constituent
and dependency parsing (Klein and Manning, 2003),
named entity recognition (Finkel et al, 2005), and
coreference resolution (Lee et al, 2011). From
the output of the Stanford tools, the following fea-
tures were extracted for each utterance: word bi-
grams (pairs of adjacent words); dependency-head
relations, along with the type of dependency rela-
tion (basically, governors ? e.g., verbs ? and their
dependents ? e.g., nouns); named entities (per-
sons, organizations, etc.); and the whole utterance.
Extracted named entities include noun phrases that
were explicitly tagged as named entities, as well as
any phrases that were marked as coreferential with
named entities. Thus if the pronoun she occurred in
an utterance, and was marked as coreferential with a
previous or following named entity Amelia, then the
feature Amelia as a named entity was added for this
utterance. We also include the whole utterance as a
feature, which turns out to be the most useful predic-
tor for an appropriate response to an input utterance.
The dialogs were divided into turns, with each
turn consisting of one or more utterances. For our
experiments, we are interested in predicting the first
utterance of a turn (which in many cases may be the
whole turn) given features of all the utterances of
the previous turns ? the exception being that for
the whole sentence feature, only the last sentence of
the previous turn is used. The method of using fea-
tures of a turn to predict features of the next turn is
related to the work reported in Purandare and Lit-
man (2008), though their goal was to analyze dialog
coherence rather than to predict the next utterance.
We are particularly interested in feature values
that are highly skewed in their predictions, mean-
ing that if the turn has a given value, then the first
sentence of the next utterance is much more likely
to have some values than others. A useful measure
of this is the difference between the entropy of the
predicted feature values fi of a feature g:
H(g) = ?
n?
i=0
log(p(fi)) ? p(fi) (1)
and the maximum possible entropy of g given n pre-
dicted features, namely:
Hmax(g) = ?log(
1
n
) (2)
The larger the difference Hmax(g)?H(g), the more
skewed the distribution.
For the purposes of this experiment and to keep
the computation reasonably tractable, we computed
the entropic values described above for like features:
thus we used bigram features to predict bigram fea-
tures, dependency features to predict dependency
features, and so forth. We also filtered the output of
the process so that each feature of the prior context
had a minimum of 10 occurrences, and the entropy
of the feature was no greater than 0.9 of the max-
imum entropy as defined above. For each feature
value, the 2 most strongly associated values for the
predicted utterance were stored.
To take a simple example (Figure 1) the bigram ?m
fine has a strong association with the bigrams you ?re
and , I, these co-occurring 486 and 464 times in the
training corpus, respectively. For this feature, the
11
?m fine 8.196261 9.406976 you ?re 486
?m fine 8.196261 9.406976 , i 464
you?re kidding . __SENT 4.348040 4.852030
no. . __SENT 32
you?re kidding . __SENT 4.348040 4.852030
i wish . __SENT 7
Figure 1: Examples of bigram and full-sentence features.
entropy is 8.20 and the maximum entropy is 9.41.
Or consider a full-sentence feature You?re kidding.
This is strongly associated with the predicted sen-
tence features no.. and I wish..
Utterances in the training data were stored and as-
sociated with predicted features. In order to pro-
duce a rank-ordered list of possible responses to a
test utterance, the features of the test utterance are
extracted. For each of these features, the predicted
features and their entropies are retrieved. Those
training data utterances that match on one or more
of these predicted features are retrieved in this step,
and a score is assigned which is simply the sum of
the predicted feature entropies. However, since we
want to favor full-sentence matches, entropies for
full-sentence matches are multiplied by a positive
number (currently set to 100).
5 Experimental Results
5.1 Whole sentence prediction
The first question we were interested in is how of-
ten, based on the approach described here, one could
predict a sentence that is close to what the speaker
actually intended to say. For this purpose, we sim-
ply took as the gold standard the utterance that was
written in the script for the speaker, and considered
the prediction of the system described above, when
it was able to make one. The prediction could be
an exact match to what was actually said, something
close enough to be a reasonable substitute, some-
thing appropriate given the context but not the one
intended, or something that is wholly inappropriate.
In the ensuing discussion we will focus on whole
sentence features, since these were the most useful
for predicting reasonable whole sentences. We re-
turn to the use of other features in Section 5.2.
Some examples can be found in Figure 2. In
each case, we give the final sentence of the previous
turn, the actual utterance, and the two predicted ut-
PREV really ?
ACTUAL yeah .
PRED 232.3099 yeah . __SENT 4
PRED 230.9528 mm-hmm . __SENT 3
PREV love you .
ACTUAL i love you , too , baby doll .
PRED 83.4519 i love you , too . __SENT 3
PRED 74.1185 love you . __SENT 3
PREV ok ?
ACTUAL i?m sorry , laurie , about j.r. ,
about everything .
PRED 86.2623 yeah . __SENT 2
PRED 86.2623 ok . __SENT 2
Figure 2: Whole sentence prediction examples.
terances, along with the predicted utterances? scores
and the counts with which they co-occurred in the
training data with the previous utterance in question.
For the first example Really?, the actual response
was Yeah, and this was also the highest ranked re-
sponse of the system. In the second example, the ac-
tual response was I love you, too, baby doll, whereas
a response of the system was I love you too. While
not exact, this is arguably close enough, and could
be selected by an impaired user who did not wish to
type the whole message. In the third example, the
predictions Yeah. and Ok. do not substitute at all for
the actual response.
Of the 276,802 utterance-response pairs in the de-
velopment test data, the system was able to make
predictions for 9,794 cases, or 3.5%. Evaluating
9,794 responses is labor intensive, so two evalua-
tions based on random samples were performed.
In the first, the authors evaluated a random sam-
ple of 455 utterance pairs, assigning the following
scores to each response: 4 exact match; 3 equiva-
lent meaning; 2 good answer but not the right one;
1 inappropriate. The results are given in Table 1, for
the best score of the pair of responses generated. In
other words, if the first response has a score of 2 and
the second a score of 3, then the pair of responses
will receive a score of 3: in that pair, there was one
generated response that was close enough to use.
From Table 1, we see that between 38% to 40.7%
of the response pairs contained a response that was
exact, or close enough to have the same meaning.
59.3% to 62% had at best a reasonable answer, but
not the one intended. Finally, none contained only
12
Score Judge 1 Judge 2
Exact match 110 24.2% 109 24.0%
Equivalent meaning 63 13.8% 76 16.7%
Good answer (but wrong) 282 62.0% 270 59.3%
Inappropriate 0 0.0% 0 0.0%
Table 1: Judgments of a sample of 455 utterance pairs by
the authors.
inappropriate answers: this is not surprising, given
that all of the predicted responses were based on
what was found in the training data, which one may
assume involved largely felicitous interactions.
We also used Amazon?s Mechanical Turk (AMT)
to collect judgements from unbiased judges. Based
on our previous evaluation, we expanded the equiv-
alent meaning category into two more fine-grained
categories, essentially the same and similar mean-
ing, in order to capture phrases with slightly differ-
ent connotations. This results in the 4-point scale
in Table 2. Exact matches were found automatically
before giving response pairs to Turkers, and account
for a large portion of the data ? 2,330 of the 9,794
response pairs, or 23.8%. For the remaining 76.2%,
138 participants were asked to judge how close the
predicted response was to the actual response.
Each AMT participant was presented with six
prompts (three entropy-based conversational turns
and three chatbot-based conversational turns, dis-
cussed below). Each prompt listed the utterance,
actual response, and predicted response. Two ad-
ditional prompts with known answers were included
to automatically flag participants who were not fo-
cusing on the task. Evaluation results are given in
4 Essentially
the same:
They?re pretty close, and mean
basically the same thing.
3 Similar
meaning:
They?re similar, but the pre-
dicted response has a slightly
different connotation from the
actual response.
2 Good answer,
but not the
right one:
They?re different, but the pre-
dicted response is still a reason-
able response to the comment.
1 Inappropriate: Different, and the predicted re-
sponse is a totally unreasonable
response to the comment.
Table 2: Four-point scale for AMT evaluation. Exact
matches were found automatically.
Essentially the same 89 16.4%
Similar meaning 81 14.9%
Good answer (but wrong) 165 30.4%
Inappropriate 79 14.5%
Table 3: Evaluation results from AMT on a random
sample of 414 predicted utterances (excluding exact
matches).
Table 3. Percentages are multiplied by the propor-
tion of results they represent (.762). Of the evalu-
ated cases, we find that 31.3% of the predicted re-
sponses were judged to be essentially the same or
similar to the actual response. 30.4% were judged
to be a reasonable answer, and the remaining 14.5%
were judged to be inappropriate.
Evaluation by AMT judges was thus much more
favorable towards the prediction-based system than
the authors? evaluation. Where the authors found
13.8%-16.7% to be essentially the same or similar,
unbiased judges found just under a third of the data
to meet these criteria. Coupled with the automati-
cally detected exact matches, 55.1% of the predicted
responses were found to be a reasonable approxima-
tion of (or exactly) the intended response. A smaller
portion of the data was thought to be a good answer
(but wrong), or wholly inappropriate.
5.2 Prediction with features plus a prefix of the
intended utterance
It is of course not necessary for the system to predict
the whole response without any input from the user.
As with word prediction, the user might type a pre-
fix of the intended utterance, and the system could
then produce a small set of corresponding responses,
among which would often be the one desired.
In order to evaluate such a scenario, we consid-
ered the shortest prefix of the actual intended re-
sponse that would be consistent with a maximum
of five sentences predicted from the features of the
previous turn. Thus, we gathered the entire set of
sentences from the training data that matched one or
more of the predicted features, then began (virtually)
typing the actual response. There are two possible
outcomes. If the actual response is not in the set,
then at some point the typed prefix will be consistent
with none of the sentences in the set. In this worst
case, the user would simply have to type the whole
sentence (possibly using whatever word-completion
13
technology is already available on the device). But
if the intended response is in the set, then at some
point the set consistent with the prefix will be win-
nowed down to at most five members. The length of
the prefix at that point, subtracted from the length of
the intended sentence, is the keystroke savings.
Of the 276,802 utterances in the development test
responses, 11,665 (4.2%) had a keystroke savings
of greater than zero: thus, in 4.2% of cases, the in-
tended utterance was to be found among the set of
sentences consistent with the predicted features. The
total keystroke savings was 102,323 characters out
of a total of 8,725,508, or about 1%. While this is
clearly small, note that it is over and above whatever
keystroke savings one would gain by other methods,
such as language modeling.
5.3 ALICE
A final experiment involved using a chatbot to gen-
erate responses. Previous approaches have used
stored sentence templates that are generated based
on keyword input from the user; a similar approach
is used in a chatbot, where the input utterances are
themselves triggers for the generated content. For
this experiment, we used the publicly available AL-
ICE (Wallace, 2012), which won the Loebner Prize
(a Turing test) in 2000, 2001, and 2004. ALICE
makes use of a large library of pattern-action pairs
written in AIML (Artificial Intelligence Markup
Language): if an input sentence matches a partic-
ular pattern, a response is generated by a rule that is
associated with that pattern. ALICE follows conver-
sational context by using a notion of TOPIC (what
the conversation is currently about, based on key-
words) and of THAT (the bot?s previous utterance).
Both are used along with the input utterance when
selecting what next to say. In essence, ALICE is a
much more sophisticated version of the 1960s Eliza
program (Weizenbaum, 1966).
In order to use the chatbot for this task, we use an
AIML interpreter (Stratton, 2010) on the most recent
set of ALICE knowledge.1 ALICE was given the
utterances for each conversation in our development
testing set, which allows the system to store some
of the dialogue context under its THAT and TOPIC
1http://code.google.com/p/aiml-en-us-foundation-alice/, re-
trieved February 2012.
Essentially the same 45 10.7%
Similar meaning 96 22.9%
Good answer (but wrong) 135 32.1%
Inappropriate 138 32.9%
Table 4: Evaluation results from AMT on a random sam-
ple of 414 chatbot utterances (excluding exact matches).
variables.
Example responses are given in Figure 3. As with
the previous experiments, some responses are close
to the actual intended message (first example in Fig-
ure 3). In some other cases (second example), the
response is reasonable, though not the one intended.
But in many cases, the response is too ?cute?, as in
the examples on the righthand side.
Evaluation with AMT is given in Table 4, us-
ing the same scoring criteria as in Table 3. Exact
matches are again automatically removed, and ac-
count for 142 responses (1.5%). For the remaining
data, participants were asked to judge how close the
chatbot response was to the actual response. Per-
centages are multiplied by the proportion of results
they represent (.9855).
The chatbot is judged to produce less acceptable
utterances than the entropy-based approach, with
10.7% essentially the same as the actual response,
and 22.9% with similar meaning. Coupled with the
automatically detected exact matches, 35% of the
chatbot responses were found to be a reasonable ap-
proximation of (or exactly) the intended response.
Over half of the data (65%) was thought to be a good
answer (but wrong), or wholly inappropriate.
6 Discussion and Future Work
Of the two different methods for automatically gen-
erating responses, the entropy-based approach in
particular produces responses ranging from similar
to exact in a majority of the cases for which it can
make a prediction. These are promising findings
for expanding a system that populates possible re-
sponses from an input utterance.
An obvious limitation of the work presented here
is that it is based on scripted data. More to the point,
to build an effective dialog-based system for an im-
paired user, the best possible data to use would be
data involving the user him/herself along with his
or her interlocutors. Indeed, we are in the process
14
PREV okay .
ACTUAL okay .
ALICE Great.
PREV please don?t leave .
ACTUAL i?m right here .
ALICE I?m not going anywhere.
PREV you need to drink water .
ACTUAL no , stay .
ALICE I need to wait and see if the
opportunity presents itself.
PREV okay ?
ACTUAL you were so mad at me
lately , dixie .
ALICE Dude!
Figure 3: Some responses generated by ALICE.
of collecting a corpus of such interactions from a
small number of AAC users in the Portland, Ore-
gon area. But the resulting corpora will obviously
be tiny in comparison with the data used in the ex-
periments here, in no small measure because of the
extreme slowness with which most AAC users are
able to communicate. What can be done about this?
One thing would be to use the results of this work
directly even if it does not model the particular user:
even if it comes from soap opera dialogs, Are you
mad at me? No, I?m not mad at you, still makes
for a perfectly reasonable utterance/response pair.
This, to some extent, counters potential objections
that soap opera dialogs are not reflective of natural
interactions. These kinds of pairs could be supple-
mented by whatever data we are able to learn from a
particular user.
Even better, though, would be to collect large
amounts of data from users before they become im-
paired. Many disorders, such as ALS, are often de-
tected early, before they start to impair communi-
cation. In such cases, one could consider language-
banking the user?s interactions, and building a model
of the ways in which the user interacts with other
speakers, in order to get a good model of that par-
ticular user. While there are obviously privacy con-
cerns, a person who knows that they will lose the
ability to speak over time will likely be very moti-
vated to try to preserve samples of their speech and
language, assuming there exists technology that can
use those samples to provide more sophisticated as-
sistance when it becomes needed.
It may also be possible to use features from the
text to generate utterances, similar to the telegraphic
approaches to generation discussed in Section 2, but
automatically learning words that can be used to
generate appropriate responses to an utterance. As
a first look at the feasibility of this approach, we use
the Midge generator (Mitchell et al, 2012), rebuild-
ing its models from the soap dialogues. Midge re-
quires as input a set of nouns and then builds likely
syntactic structure around them, and so we use the
dialogues to predict possible nouns in response to
an input utterance. For each <utterance, response>
pair in the dialogues, we gather all utterance nouns
nu and all response nouns nr. We then compute nor-
malized pointwise mutual information (nPMI) for
each nu, nr pair type in the corpus. Given a novel in-
put utterance, we tag it to extract the nouns and cre-
ate the set of highest nPMI nouns from the model.
This is then input to Midge, which uses the set to
generate present-tense declarative sentences. Some
examples are given in Figure 4. We hope to expand
on this approach in future work.
A further improvement is to take advantage of
synonymy. There are many ways to convey the same
basic message: i am sick, i am not feeling well, i?m
under the weather, are all ways for a speaker to con-
vey that he or she is not in the best of health. In
the current system, these are all treated separately.
Clearly what is needed is a way of recognizing that
these are all paraphrases of each other. Fortunately,
there has been a lot of progress in recent years on
paraphrasing ? see Ganitkevitch et al (2011) for a
recent example ? and such work could in princi-
ple be adapted to the problem here. Indeed it seems
likely that incorporating paraphrasing into the sys-
tem will be a major source of improved coverage.
A limitation of the work described here is that
it only models turn-to-turn interactions. Clearly
discourse models need to have more memory than
this, so features that relate to earlier turns would be
needed. The downside is that this would quickly
lead to data sparsity.
There are a variety of machine learning tech-
niques that could also be tried, beyond the rather
15
Input: this is n?t the same . this is not like anything i have been
through before . i mean , how am i supposed to make it work with
somebody who ...
Pred. nouns: strength, somebody
Output: strength comes with somebody
Input: i ?ve been a little bit too busy to socialize . i did have an
interesting conversation with your sister , however .
Pred. nouns: bit, conversation, sister
Output: a bit about this conversation with sister
Figure 4: Generating with nPMI: Creating syntactic structure around likely nouns.
simple methods employed in this work. For exam-
ple, particular classes of response types, comprising
a variety of related utterances, may be predictable
using the extracted features.
Finally, we have assumed for this discussion that
the AAC system is only within the control of the im-
paired user. There is no reason to make that assump-
tion in general: many AAC situations in real life in-
volve a helper who will often co-construct with the
impaired user. Such helpers usually know the im-
paired user very well and can often make reasonable
guesses as to the whole utterance intended by the
impaired user. Recent work reported in Roark et al
(2011) suggests one way in which the results of a
language modeling system and those of a human co-
constructor may be integrated into a single system,
and such an approach could easily be applied here.
7 Conclusions
We have proposed and evaluated an approach to
whole utterance prediction for AAC. While the ap-
proach is fairly simple, it is able to generate correct
or at least reasonable responses in some cases. Such
a system could be used in conjunction with other
techniques, such as language-model-based predic-
tion, or co-construction. One of the potentially use-
ful side-effects of this work is an estimate of what
percentage of interactions in a dialog are likely to be
easily handled by such techniques. In other words,
how many interactions in dialog are sufficiently pre-
dictable that a system could have a reasonable guess
as to what a speaker is going to say given the pre-
vious context? A rough estimate based on what we
have found here is something on the order of 3.5%-
4.0%. Obviously this does not mean that the sys-
tem will always make the right prediction: a reason-
able response to congratulations on your promotion
would often be thank you, but a speaker may wish
to say something else. But what it does mean is that
in about 3.5%-4.0% of cases, one has a reasonable
chance of being able to guess. This percentage is
certainly small, and one might be inclined to con-
clude that the approach does not work. On the other
hand, it is important to bear in mind that not all per-
centages are created equal. Rapid responses to ba-
sic phrases (e.g. Are you mad at me? ? No, I?m
not mad at you), could help with the perceived flow
of conversation, even if they do not occur that fre-
quently.
As we noted at the outset, whole utterance pre-
diction is an area that has received increased inter-
est in recent years, because of its potential to speed
communication, and its contribution to increasing
the naturalness of conversational interactions. When
coupled with gains in utterance generation achieved
by other methods, automatically generating utter-
ances can further the range of comments and re-
sponses available to AAC users. The work reported
here is a small contribution towards this goal.
Acknowledgments
This work was supported under grant NIH-
K25DC011308. Sproat thanks his K25 mentor,
Melanie Fried-Oken, for discussion and support. We
also thank four anonymous reviewers, as well as the
audience at a Center for Spoken Language Under-
standing seminar, for their comments.
16
References
N. Alm, J. L. Arnott, and A. F. Newell. 1992. Predic-
tion and conversational momentum in an augmentative
communication system. Communications of the ACM,
35(5):46?57.
N. Alm, J. Todman, Leona Elder, and A. F. Newell. 1993.
Computer aided conversation for severely physically
impaired non-speaking people. Proceedings of IN-
TERCHI ?93, pages 236?241.
Bruce Baker. 1990. Semantic compaction: a basic tech-
nology for artificial intelligence in AAC. In 5th An-
nual Minspeak Conference.
J. J. Darragh, I. H. Witten, and M. L. James. 1990. The
reactive keyboard: A predictive typing aid. Computer,
23(11):41?49.
Martin Dempster, Norman Alm, and Ehud Reiter. 2010.
Automatic generation of conversational utterances and
narrative for augmentative and alternative communi-
cation: A prototype system. Proceedings of the Work-
shop on Speech and Language Processing for Assistive
Technologies (SLPAT), pages 10?18.
R. Dye, N. Alm, J. L. Arnott, G. Harper, and A Morrison.
1998. A script-based AAC system for transactional
interaction. Natural Language Engineering, 4(1):57?
71.
Michael Elhadad. 1991. FUF: The universal unifer-user
manual version 5.0. Technical report.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
363?370.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. Proceedings of Empirical
Methods in Natural Language Processing (EMNLP).
John Godfrey and Edward Holliman. 1997.
Switchboard-1 release 2. Linguistic Data Con-
sortium, Philadelphia.
D. J. Higginbotham, D. P. Wilkins, G. W. Lesher, and
B. J. Moulton. 1999. Frametalker: A communication
frame and utterance-based augmentative communica-
tion device. Technical Report.
D. Jeffery Higginbotham. 1992. Evaluation of keystroke
savings across five assistive communication technolo-
gies. Augmentative and Alternative Communication,
8:258?272.
Julia King, Tracie Spoeneman, Sheela Stuart, and David
Beukelman. 1995. Small talk in adult conversations:
Implications for AAC vocabulary selection. Augmen-
tative and Alternative Communication, 11:260?264.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. Proceedings of the 41st Meet-
ing of the Association for Computational Linguistics
(ACL), pages 423?430.
S. Langer and M. Hickey. 1998. Using semantic lexicons
for full text message retrieval in a communication aid.
Natural Language Engineering, 4(1):41?55.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. Proceedings of the
CoNLL-2011 Shared Task.
J. Li and G. Hirst. 2005. Semantic knowledge in word
completion. In Proceedings of the 7th International
ACM Conference on Computers and Accessibility.
K. McCoy, C. A. Pennington, and A. L. Badman. 1998.
Compansion: From research prototype to practical in-
tegration. Natural Language Engineering, 4(1):73?
95.
Kathleen F. McCoy, Jan L. Bedrosian, Linda A. Hoag,
and Dallas E. Johnson. 2007. Brevity and speed of
message delivery trade-offs in augmentative and alter-
native communication. Augmentative and Alternative
Communication, 23(1):76?88.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Sratos, Xufeng Han, Alysssa Mensch,
Alex Berg, Tamara L. Berg, and Hal Daume? III. 2012.
Midge: Generating image descriptions from computer
vision detections. Proceedings of EACL 2012.
Y. Netzer and M. Elhadad. 2006. Using semantic author-
ing for Blissymbols communication boards. Proceed-
ings of HLT 2006, pages 105?108.
Amruta Purandare and Diane Litman. 2008. Analyzing
dialog coherence using transition patterns in lexical
and semantic features. In FLAIRS Conference, pages
195?200.
Brian Roark, Andrew Fowler, Richard Sproat, Christo-
pher Gibbons, and Melanie Fried-Oken. 2011. To-
wards technology-assisted co-construction with com-
munication partners. Proceedings of the Workshop on
Speech and Language Processing for Assistive Tech-
nologies (SLPAT).
Cort Stratton. 2010. PyAIML, a Python AIML inter-
preter. http://pyaiml.sourceforge.net/.
J. Todman, A. Norman, J. Higginbotham, and P. File.
2008. Whole utterance approaches in AAC. Augmen-
tative and Alternative Communication, 24(3):235?
254.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. Proceed-
ings of HLT-NAACL, pages 252?259.
17
K. Trnka, D. Yarrington, K.F. McCoy, and C. Pennington.
2006. Topic modeling in fringe word prediction for
AAC. In Proceedings of the International Conference
on Intelligent User Interfaces, pages 276?278.
K. Trnka, D. Yarrington, J. McCaw, K.F. McCoy, and
C. Pennington. 2007. The effects of word predic-
tion on communication rate for AAC. In Proceed-
ings of HLT-NAACL; Companion Volume, Short Pa-
pers, pages 173?176.
H. Trost, J. Matiasek, and M. Baroni. 2005. The lan-
guage component of the FASTY text prediction sys-
tem. Applied Artificial Intelligence, 19(8):743?781.
Richard Wallace. 2012. A.L.I.C.E. (Artificial Linguistic
Internet Computer Entity). http://www.alicebot.org/.
T. Wandmacher and J.Y. Antoine. 2007. Methods to in-
tegrate a language model with semantic information
for a word prediction component. In Proceedings of
Empirical Methods in Natural Language Processing
(EMNLP), pages 506?513.
Joseph Weizenbaum. 1966. Eliza ? a computer program
for the study of natural language communication be-
tween man and machine. Proceedings of the ACM,
9(1).
Bruce Wisenburn and D. Jeffery Higginbotham. 2008.
An AAC application using speaking partner speech
recognition to automatically produce contextually rel-
evant utterances: Objective results. Augmentative and
Alternative Communication, 24(2):100?109.
18
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 46?50,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Detecting linguistic idiosyncratic interests in autism
using distributional semantic models
Masoud Rouhizadeh
?
, Emily Prud?hommeaux
?
, Jan van Santen
?
, Richard Sproat
?
?
Center for Spoken Language Understanding, Oregon Health & Science University
?
Center for Language Sciences, University of Rochester
?
Google, Inc.
{rouhizad,vansantj}@ohsu.edu, emilypx@gmail.com, rws@xoba.com
Abstract
Children with autism spectrum disorder
often exhibit idiosyncratic patterns of be-
haviors and interests. In this paper, we fo-
cus on measuring the presence of idiosyn-
cratic interests at the linguistic level in
children with autism using distributional
semantic models. We model the semantic
space of children?s narratives by calculat-
ing pairwise word overlap, and we com-
pare the overlap found within and across
diagnostic groups. We find that the words
used by children with typical development
tend to be used by other children with typ-
ical development, while the words used
by children with autism overlap less with
those used by children with typical devel-
opment and even less with those used by
other children with autism. These findings
suggest that children with autism are veer-
ing not only away from the topic of the
target narrative but also in idiosyncratic
semantic directions potentially defined by
their individual topics of interest.
1 Introduction
Autism spectrum disorder (ASD) is a neurode-
velopmental disorder characterized by impaired
communication and social behavior. One of the
core deficits associated with ASD is an intense
preoccupation with a restricted set of interests
(American Psychiatric Association, 2000; Amer-
ican Psychiatric Association, 2013), which can of-
ten be observed in an individual?s tendency to per-
severate on specific, idiosyncratic topics of con-
versation. Because this symptom is explicitly
mentioned among the diagnostic criteria for ASD
used in the DSM-IV and DSM-5, many diagnos-
tic instruments (Lord et al., 2002; Rutter et al.,
2003) require a qualitative assessment of this phe-
nomenon. Instances of perseveration on a partic-
ular topic in the spontaneous spoken language of
children with ASD, however, are not typically ex-
plicitly counted in a clinical setting, making com-
parisons with typically developing children diffi-
cult to quantify.
Expert manual analysis of conversations and
narratives of individuals with ASD has shown that
children and teenagers with autism include signif-
icantly more bizarre and irrelevant content in their
narratives (Loveland et al., 1990; Losh and Capps,
2003) and introduce more abrupt topic changes in
their conversations (Lam et al., 2012) than their
typically developing peers. Automatic detection
of poor topic maintenance has also been explored
using techniques originally developed for infor-
mation extraction (Rouhizadeh et al., 2013). There
has been little work, however, in annotating the
precise direction of the departure from a target
topic. Thus, it is not clear whether children with
ASD are instigating similar topic changes or pur-
suing idiosyncratic directions in their narratives
and conversations consistent with their restricted
interests.
In this paper, we attempt to automatically iden-
tify topic changes and idiosyncratic interests ex-
pressed in the language of children with ASD
by measuring the semantic similarity of narrative
retellings produced by children with and without
ASD. We first use word overlap measures to cal-
culate the semantic similarity between every pos-
sible pair of narratives. We then build three pair-
wise comparison matrices: one comparing pairs of
typically developing (TD) children; one compar-
ing pairs of children with ASD; and a third com-
46
paring pairs consisting of one child with ASD and
one child with TD. We calculate the significance
of the differences between the pairs in the three
matrices using the Monte Carlo method to shuffle
the diagnosis label of each child.
We find that TD children share the greatest
word overlap with one another, while children
with ASD have significantly less word overlap
with TD children and even less word overlap with
other ASD children. These results indicate that
TD children tend to adhere to the target topic in
the narrative retellings, while children with ASD
often stray from the target topic. Furthermore,
the fact that the word choices of an individual
child with ASD seem not to resemble the word
choices of other children with ASD suggests that
when a child with ASD chooses to abandon the
target topic, he or she does so in an idiosyncratic
way. Although these results are only indirect in-
dications of the presence of restricted interests,
the work presented here highlights the potential of
computational language analysis methods for im-
proving our understanding of the social and lin-
guistic deficits associated with the disorder.
2 Data
Participants in this study included 39 children with
typical development (TD) and 21 children with
autism spectrum disorder (ASD). ASD was di-
agnosed via clinical consensus according to the
DSM-IV-TR criteria (American Psychiatric Asso-
ciation, 2000) and the established threshold scores
on two diagnostic instruments: the Autism Di-
agnostic Observation Schedule (ADOS) (Lord et
al., 2002), a semi-structured series of activities de-
signed to allow an examiner to observe behaviors
associated with autism; and the Social Communi-
cation Questionnaire (SCQ) (Rutter et al., 2003),
a parental questionnaire. None of the children
in this study met the criteria for a language im-
pairment, and there were no significant between-
group differences in age (mean=6.3) or full-scale
IQ (mean=115.5).
The narrative retelling task analyzed here is the
Narrative Memory subtest of the NEPSY (Kork-
man et al., 1998), a large and comprehensive bat-
tery of tasks that test neurocognitive functioning in
children. The NEPSY Narrative Memory (NNM)
subtest is a narrative retelling test in which the sub-
ject listens to a brief narrative about a boy and his
dog and then must retell the narrative to the ex-
aminer. Under standard administration, the NNM
free recall score is calculated by counting how
many from a set of 17 story elements were used
in a retelling. Following the free recall portion of
the test is the cued recall task, in which the ex-
aminer then asks the subject to provide answers to
questions about all of the story elements that were
omitted in the retelling.
The NNM was administered to each participant
in the study, and each participant?s retelling was
recorded and transcribed. The responses for the
cued recall portion of the subtest were not in-
cluded in this work presented here. There was no
significant difference between the two diagnostic
groups in the standard NNM free recall score.
3 Methods
We expect that two different retellings of the same
source will lie in the same lexico-semantic space.
As a result, they should include high percentage
of overlapping words. When a pair of retellings
has a low word overlap measure, it could be that
one or both retellings include intrusions from un-
related topics. An alternative explanation is that
the subjects recalled a non-overlapping set of story
elements or simply a small set of story elements.
However, since we did not find any significant dif-
ference between the TD and ASD groups in the
standard narrative recall score, we infer that a low
percentage of word overlap indicates a difference
in topic between the two retellings.
3.1 Word overlap measures
In order to calculate the similarity between a pair
of narratives i and j, we use type and token over-
lap measures based on the Jaccard similarity coef-
ficient. Token similarity is defined as the size of
intersection of the words (i.e., the actual number
of tokens in common) in narratives i and j relative
to the size of the union of the words in the two
narratives (i.e., summing over all tokens in both
narratives, the maximum number of instances of
that token in either narrative). Type similarity is
defined as the size of intersection of the types (i.e.,
unique words) in narratives i and j relative to the
size of the union of the types in the two narratives.
For instance, for the following set of words i and
j:
i = {a, b, c, d, c}
j = {a, c, e, c, a, a},
the token intersection is equal to {a, c, c} and
47
Group Means
TD.TD TD.ASD ASD.ASD
Type Overlap .23 .17 .13
Token Overlap .19 .14 .11
Table 3: Word overlap pairwise group means
the token union is {a, a, a, c, c, b, e, d}. The token
overlap similarity between the two sets i and j is
therefore 3/8. The type intersection of i and j is
equal to {a, c} and the type union is {a, c, b, e, d},
yielding a type overlap similarity of 2/5.
3.2 Pairwise similarity matrix
We next build a similarity matrix for the type and
token overlap measures, comparing every possi-
ble pair of children. Every child in the TD and
ASD groups is compared to the children in his own
group (TD.TD and ASD.ASD), as well as the chil-
dren in the other group (TD.ASD). The pairwise
similarity matrix is diagonally symmetrical, and
we thus consider only the top right section of the
matrix above the diagonal in our analysis.
3.3 Monte Carlo permutation
Since we may not have enough information to
make an assumption that the pairwise similarity
measures of all children are from a particular dis-
tribution, we utilize a non-parametric procedure,
the Monte Carlo permutation approach, which is
widely used in non-standard significance testing
situations.
Given the three sub-matrices in the similarity
matrix described above (TD.TD, TD.ASD, and
ASD.ASD), we first calculate for each pair of sub-
matrices (e.g., TD.TD vs ASD.ASD) three statis-
tics that compare all cells in one submatrix with
the cells in other submatrices: the difference be-
tween the means, t-statistics (using the Welch
Two Sample t-test), and w-statistics (using the
Wilcoxon rank sum test). We label these observed
values observed-mean, observed-t, and observed-
w. We next take a large random sample with re-
placement from all possible permutations of the
data by shuffling the diagnosis labels of the chil-
dren 1000 times, and then calculate each of the
three above statistics for each shuffle. Finally, we
determine the number of times the observed values
exceed the values generated by the 1000 shuffles.
4 Results
The comparison of the group means of each of
the three sub-matrices described in Section 3.2
show that TD children have the greatest overlap
with each other; children with ASD have less
word overlap with TD children than TD children
have with one another and even less word over-
lap with other ASD children. The group means
of both type and token overlap are summarized
in Table 3. In addition, examples of overlapping
and non-overlapping terms between the groups are
provided in Tables 1 and 2 respectively.
The level plot of the pairwise token overlap
is shown in figure 1. We see that the TD.TD
sub-matrix has the lightest color, indicating higher
overlap, followed by TD.ASD. The ASD.ASD
submatrix has the darkest color, indicating low
word overlap.
In the next step, we determine the significance
of the group mean differences. As described in
Section 3.3, using the Monte Carlo permutation to
test the significance of the following comparisons:
TD.TD vs ASD.ASD, TD.TD vs TD.ASD, and
TD.ASD vs ASD.ASD. The results of these signif-
Group Top 10 overlapping words
TD.TD shoe, tree, climb, ladder, fall, Pepper, Jim, dog, sister, branch
TD.ASD shoe, tree, Jim, climb, dog, ladder, Pepper, fall, branch, sister
ASD.ASD shoe, tree, Jim, dog, climb, Pepper, ladder, branch, boy, run
Table 1: Top 10 overlapping words between the groups
Group Examples of non-overlapping words
TD.TD coconut, couch, jew, lie, picture, spike, stuff, t-rex, tight, watch
TD.ASD arm, bottom, cousin, doctor, eat, fruit, giant, meat, push, sense
ASD.ASD bite, bridge, crunch, donut, gadget, lizard, microphone, sell, table, vision
Table 2: Examples of non-overlapping words between the groups
48
??
??
??
Figure 1: Level plot of the pairwise token overlap
(lighter colors indicate higher overlap)
icance tests are summarized in table 4, and in all
cases the differences are significant at p < 0.05.
5 Conclusions and future work
The methods presented for comparing the lexical
choices made by children with and without ASD
while generating a narrative retelling demonstrate
the utility of language analysis for revealing diag-
nostically interesting information. The low rates
of word overlap between retellings produced by
children with ASD and those produced by typi-
cally developing children suggest that the children
with ASD are having difficulty maintaining the
target topic. Furthermore, the low overlap between
pairs of children with ASD suggests that children
with ASD are not straying from the topic in sim-
ilar ways but are instead exploring topics that are
of idiosyncratic interest.
These findings can be potentially used for
diagnostic purposes in combinations of other
applications of speech and language process-
ing for automated narrative retelling assessment
(Lehr et al., 2013), detection of off-topic words
(Rouhizadeh et al., 2013), and pragmatic deficits
(Prud?hommeaux and Rouhizadeh, 2012). From a
clinical standpoint, diagnostic measures utilizing
these methods for automated evaluation of disor-
dered language could be very useful in diagnosis
and planning interventions.
One major focus of our future work will be to
manually annotate the narrative retellings used in
this study to determine the frequency of topic de-
partures and the nature of these departures. Given
the vocabulary differences seen here, we expect
to find not only that children with ASD are aban-
doning the topic of the source narrative more fre-
quently than children with typical development
but also that the topics they choose to pursue are
related to their own individual specific interests.
A second area we hope to explore is the use
of external resources, such as WordNet, to ex-
pand the set of terms used to calculate word over-
lap. It is perfectly reasonable to expect that people
will use synonyms and paraphrases in their narra-
tive retellings. It is therefore possible that chil-
dren with autism are discussing the appropriate
topic but choosing unusual words within that topic
space in their retellings, which could be consis-
tent with the type of atypical language often ob-
served in children with ASD. By considering se-
mantic overlap rather than simple word overlap,
we may be able to distinguish instances of atypical
language from true examples of poor topic main-
tenance.
Third, we are also interested in applying the
analysis described above to a set of retellings from
seniors with and without mild cognitive impair-
ment, a frequent precursor to dementia. Like chil-
dren with ASD, seniors with dementia are also
more likely to include irrelevant information in
overlap statistic
p-values
TD.TD vs ASD.ASD TD.TD vs TD.ASD TD.ASD vs ASD.ASD
Type Overlap
Means .004 .042 .008
t.test .009 .012 .008
Wilcoxon test .004 .002 .002
Token Overlap
Means .012 .034 .028
t.test .014 .022 .022
Wilcoxon test .012 .002 .002
Table 4: Monte Carlo significance test results
49
their narrative retellings. These intrusions, how-
ever, are often informed by real-world knowledge,
and thus may not result in a decrease in measures
of word overlap with narratives produced by unim-
paired individuals.
Finally, we plan to apply our methods to the out-
put of an automatic speech recognition (ASR) sys-
tem rather than manual transcripts. Although the
ASR output is likely to contain many errors, the
fact that our methods focus on content words may
make them robust to the sorts of function word
recognition errors typically produced by ASR sys-
tems.
Acknowledgments
This work was supported in part by NSF grant
#BCS-0826654, and NIH NIDCD grants #R01-
DC007129 and #1R01DC012033-01. Any opin-
ions, findings, conclusions or recommendations
expressed in this publication are those of the au-
thors and do not necessarily reflect the views of
the NSF or the NIH.
References
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washing-
ton, DC.
American Psychiatric Association. 2013. Diagnostic
and statistical manual of mental disorders (5th ed.).
American Psychiatric Publishing, Washington, DC.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological as-
sessment. The Psychological Corporation, San An-
tonio.
Yan Grace Lam, Siu Sze, and Susanna Yeung. 2012.
Towards a convergent account of pragmatic lan-
guage deficits in children with high-functioning
autism: Depicting the phenotype using the prag-
matic rating scale. Research in Autism Spectrum
Disorders, 6(2):792?797.
Maider Lehr, Izhak Shafran, Emily Prud?hommeaux,
and Brian Roark. 2013. Discriminative joint model-
ing of lexical variation and acoustic confusion for
automated narrative retelling assessment. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Molly Losh and Lisa Capps. 2003. Narrative ability in
high-functioning children with autism or asperger?s
syndrome. Journal of Autism and Developmental
Disorders, 33(3):239?251.
Katherine Loveland, Robin McEvoy, and Belgin Tu-
nali. 1990. Narrative story telling in autism and
down?s syndrome. British Journal of Developmen-
tal Psychology, 8(1):9?23.
Emily Prud?hommeaux and Masoud Rouhizadeh.
2012. Automatic detection of pragmatic deficits
in children with autism. In Proceedings of the
3rd Workshop on Child, Computer and Interaction
(WOCCI).
Masoud Rouhizadeh, Emily Prud?hommeaux, Brian
Roark, and Jan van Santen. 2013. Distributional
semantic models for the evaluation of disordered
language. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Michael Rutter, Anthony Bailey, and Catherine Lord.
2003. Social Communication Questionnaire (SCQ).
Western Psychological Services, Los Angeles.
50
