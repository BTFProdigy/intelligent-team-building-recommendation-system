Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 17?24
Manchester, August 2008
On Robustness and Domain Adaptation using SVD
for Word Sense Disambiguation
Eneko Agirre and Oier Lopez de Lacalle
Informatika Fakultatea, University of the Basque Country
20018, Donostia, Basque Country
{e.agirre,oier.lopezdelacalle}@ehu.es
Abstract
In this paper we explore robustness and
domain adaptation issues for Word Sense
Disambiguation (WSD) using Singular
Value Decomposition (SVD) and unlabeled
data. We focus on the semi-supervised do-
main adaptation scenario, where we train
on the source corpus and test on the tar-
get corpus, and try to improve results us-
ing unlabeled data. Our method yields
up to 16.3% error reduction compared to
state-of-the-art systems, being the first to
report successful semi-supervised domain
adaptation. Surprisingly the improvement
comes from the use of unlabeled data from
the source corpus, and not from the target
corpora, meaning that we get robustness
rather than domain adaptation. In addition,
we study the behavior of our system on the
target domain.
1 Introduction
In many Natural Language Processing (NLP)
tasks we find that a large collection of manually-
annotated text is used to train and test supervised
machine learning models. While these models
have been shown to perform very well when tested
on the text collection related to the training data
(what we call the source domain), the performance
drops considerably when testing on text from other
domains (called target domains).
In order to build models that perform well in
new (target) domains we usually find two settings
(Daum?e III, 2007): In the semi-supervised setting
the goal is to improve the system trained on the
source domain using unlabeled data from the tar-
get domain, and the baseline is that of the system
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
trained on the source domain. In the supervised
setting, training data from both source and tar-
get domains are used, and the baseline is provided
by the system trained on the target domain. The
semi-supervised setting is the most attractive, as it
would save developers the need to hand-annotate
target corpora every time a new domain is to be
processed.
The main goal of this paper is to use unlabeled
data in order to get better domain-adaptation re-
sults for Word Sense Disambiguation (WSD) in
the semi-supervised setting. Singular Value De-
composition (SVD) has been shown to find corre-
lations between terms which are helpful to over-
come the scarcity of training data in WSD (Gliozzo
et al, 2005). This paper explores how this ability
of SVD can be applied to the domain-adaptation of
WSD systems, and we show that SVD and unla-
beled data improve the results of two state-of-the-
art WSD systems (k-NN and SVM). For the sake of
this paper we call this set of experiments the do-
main adaptation scenario.
In addition, we also perform some related exper-
iments on just the target domain. We use unlabeled
data in order to improve the results of a system
trained and tested in the target domain. These re-
sults are complementary to the domain adaptation
experiments, and also provide an upperbound for
semi-supervised domain adaptation. We call these
experiments the target domain scenario. Note
that both scenarios are semi-supervised, in that our
focus is on the use of unlabeled data in addition to
the available labeled data.
The experiments were performed on a publicly
available corpus which was designed to study the
effect of domain in WSD (Koeling et al, 2005). It
comprises 41 nouns closely related to the SPORTS
and FINANCES domains with 300 examples for
each. The 300 examples were drawn from the
British National Corpus (Leech, 1992) (BNC), the
SPORTS section of the Reuters corpus (Leech,
17
1992), and the FINANCES section of Reuters in
equal number.
The paper is structured as follows. Section 2 re-
views prior work in the area. Section 3 presents the
datasets used, and Section 4 the learning methods,
including the application of SVD. The experimen-
tal results are presented in Section 5, for the semi-
supervised domain adaptation scenario, and Sec-
tion 6, for the target scenario. Section 7 presents
the discussion and Section 8 the conclusions and
future work.
2 Prior Work
Domain adaptation is a subject attracting more
and more attention. In the semi-supervised set-
ting, Blitzer et al (2006) use Structural Corre-
spondence Learning and unlabeled data to adapt
a Part-of-Speech tagger. They carefully select so-
called ?pivot features? to learn linear predictors,
perform SVD on the weights learned by the pre-
dictor, and thus learn correspondences among fea-
tures in both source and target domains. Our tech-
nique also uses SVD, but we directly apply it to all
features, and thus avoid the need to define pivot
features. In preliminary work we unsuccessfully
tried to carry along the idea of pivot features to
WSD. Zelikovitz and Hirsh (2001) use unlabeled
data (so-called background knowledge) with La-
tent Semantic Indexing (also based on SVD) on a
Text Classification task with positive results. They
use related unlabeled text and include it in the
term-by-document matrix to expand it and capture
better the interesting properties of the data. Their
approach is similar to our SMA method in Section
4.2).
In the supervised setting, a recent paper by
Daum?e III (2007) shows that, using a very simple
feature augmentation method coupled with Sup-
port Vector Machines, he is able to effectively
use both labeled target and source data to pro-
vide the best results in a number of NLP tasks.
His method improves or equals over previously ex-
plored more sophisticated methods (Daum?e III and
Marcu, 2006; Chelba and Acero, 2004).
Regarding WSD, some initial works made ba-
sic analysis of the particular issues. Escudero et
al. (2000) tested the supervised adaptation set-
ting on the DSO corpus, which had examples from
the Brown corpus and Wall Street Journal cor-
pus. They found that the source corpus did not
help when tagging the target corpus, showing that
tagged corpora from each domain would suffice,
and concluding that hand tagging a large general
corpus would not guarantee robust broad-coverage
WSD. Agirre and Mart??nez (2000) also used the
DSO corpus in the supervised setting to show that
training on a subset of the source corpora that is
topically related to the target corpus does allow
for some domain adaptation. Their work used the
fact that the genre tags of Brown allowed to detect
which parts of the corpus were related to the target
corpus.
More recently, Koeling et al (2005) presented
an unsupervised system to learn the predominant
senses of particular domains. Their system was
based on the use of a similarity thesaurus induced
from the domain corpus and WordNet. They used
the same dataset as in this paper for evaluation.
Chan and Ng (2007) performed supervised domain
adaptation on a manually selected subset of 21
nouns from the DSO corpus. They used active
learning, count-merging, and predominant sense
estimation in order to save target annotation ef-
fort. They showed that adding just 30% of the tar-
get data to the source examples the same precision
as the full combination of target and source data
could be achieved. They also showed that using
the source corpus allowed to significantly improve
results when only 10%-30% of the target corpus
was used for training. No data was given about the
use of both tagged corpora.
Though not addressing domain adaptation, other
works on WSD also used SVD and are closely re-
lated to the present paper. Gliozzo et al (2005)
used SVD to reduce the space of the term-to-
document matrix, and then computed the similarity
between train and test instances using a mapping
to the reduced space (similar to our SMA method
in Section 4.2). They combined other knowledge
sources into a complex kernel using SVM. They
report improved performance on a number of lan-
guages in the Senseval-3 lexical sample dataset.
Our present paper differs from theirs in that we
propose an additional method to use SVD (the OMT
method, Section 4.2), and that we evaluate the con-
tribution of unlabeled data and SVD in isolation,
leaving combination for future work.
Ando (2006) used Alternative Structured Op-
timization, which is closely related to Structural
Learning (cited above). He first trained one lin-
ear predictor for each target word, and then per-
formed SVD on 7 carefully selected submatrices
18
of the feature-to-predictor matrix of weights. The
system attained small but consistent improvements
(no significance data was given) on the Senseval-
3 lexical sample datasets using SVD and unlabeled
data.
We have previously shown (Agirre et al, 2005;
Agirre and Lopez de Lacalle, 2007) that perform-
ing SVD on the feature-to-documents matrix is a
simple technique that allows to improve perfor-
mance with and without unlabeled data. The use
of several k-NN classifiers trained on a number of
reduced and original spaces was shown to rank first
in the Senseval-3 dataset and second in the Se-
mEval 2007 competition. The present work ex-
tends our own in that we present a comprehensive
study on a domain adaptation dataset, producing
additional insight on our method and the relation
between SVD, features and unlabeled data.
3 Data sets
The dataset we use was designed for domain-
related WSD experiments by Koeling et al (2005),
and is publicly available. The examples come
from the BNC (Leech, 1992) and the SPORTS and
FINANCES sections of the Reuters corpus (Rose
et al, 2002), comprising around 300 examples
(roughly 100 from each of those corpora) for each
of the 41 nouns. The nouns were selected be-
cause they were salient in either the SPORTS or
FINANCES domains, or because they had senses
linked to those domains. The occurrences were
hand-tagged with the senses from WordNet (WN)
version 1.7.1 (Fellbaum, 1998).
Compared to the DSO corpus used in prior work
(cf. Section 2) this corpus has been explicitly cre-
ated for domain adaptation studies. DSO con-
tains texts coming from the Brown corpus and the
Wall Street Journal, but the texts are not classi-
fied according to specific domains (e.g. Sports, Fi-
nances), which make DSO less suitable to study
domain adaptation.
In addition to the labeled data, we also use
unlabeled data coming from the three sources
used in the labeled corpus: the ?written? part of
the BNC (89.7M words), the FINANCES part of
Reuters (117,734 documents, 32.5M words), and
the SPORTS part (35,317 documents, 9.1M words).
4 Learning features and methods
In this section, we review the learning features, the
two methods to apply SVD, and the two learning
algorithms used in the experiments.
4.1 Learning features
We relied on the usual features used in previous
WSD work, grouped in three main sets. Local
collocations comprise the bigrams and trigrams
formed around the target word (using either lem-
mas, word-forms, and PoS tags
1
), those formed
with the previous/posterior lemma/word-form in
the sentence, and the content words in a ?4-word
window around the target. Syntactic dependen-
cies
2
use the object, subject, noun-modifier, prepo-
sition, and sibling lemmas, when available. Fi-
nally, Bag-of-words features are the lemmas of
the content words in the whole context, plus the
salient bigrams in the context (Pedersen, 2001).
4.2 Features from the reduced space
Apart from the original space of features, we have
the so called SVD features, obtained from the
projection of the feature vectors into the reduced
space (Deerwester et al, 1990). Basically, we set
a term-by-document or feature-by-example matrix
M from the corpus (see section below for more
details). SVD decomposes it into three matrices,
M = U?V
T
. If the desired number of dimensions
in the reduced space is p, we select p rows from ?
and V , yielding ?
p
and V
p
respectively. We can
map any feature vector
~
t (which represents either a
train or test example) into the p-dimensional space
as follows:
~
t
p
=
~
t
T
V
p
?
?1
p
. Those mapped vectors
have p dimensions, and each of the dimensions is
what we call a SVD feature. We can now use the
mapped vectors (
~
t
p
) to train and test any learning
method, as usual. We have explored two different
variants in order to build the reduced matrix and
obtain the SVD features, as follows.
Single Matrix for All target words (SVD-
SMA). The method comprises the following steps:
(i) extract bag-of-word features (terms in this case)
from unlabeled corpora, (ii) build the term-by-
document matrix, (iii) decompose it with SVD, and
(iv) project the labeled data (train/test). This tech-
nique is very similar to previous work on SVD
(Gliozzo et al, 2005; Zelikovitz and Hirsh, 2001).
The dimensionality reduction is performed once,
over the whole unlabeled corpus, and it is then ap-
plied to the labeled data of each word. The reduced
1
The PoS tagging was performed with the fnTBL toolkit
(Ngai and Florian, 2001)
2
This software was kindly provided by David Yarowsky?s
group, from Johns Hopkins University.
19
space is constructed only with terms, which corre-
spond to bag-of-words features, and thus discards
the rest of the features. Given that the WSD litera-
ture has shown that all features, including local and
syntactic features, are necessary for optimal per-
formance (Pradhan et al, 2007), we propose the
following alternative to construct the matrix.
One Matrix per Target word (SVD-OMT). For
each word: (i) construct a corpus with its occur-
rences in the labeled and, if desired, unlabeled cor-
pora, (ii) extract all features, (iii) build the feature-
by-example matrix, (iv) decompose it with SVD,
and (v) project all the labeled training and test data
for the word. Note that this variant performs one
SVD process for each target word separately, hence
its name. We proposed this technique in (Agirre et
al., 2005).
An important parameter when doing SVD is the
number of dimensions in the reduced space (p).
We tried two different values for p (25 and 200) in
the BNC domain, and the results were consistent
in that 25 performed better for SVD-OMT and 200
better for SVD-SMA. Those values were chosen for
testing in the SPORTS and FINANCES domains, i.e.
25 for SVD-OMT and 200 for SVD-SMA.
4.3 Building Matrices
The methods in the previous section can be applied
to the following matrices M :
? TRAIN: The matrix comprises features from
labeled train examples alone. This matrix can
only be used to obtain OMT features.
? TRAIN ? BNC: In addition to TRAIN, we
matrix also includes unlabeled examples from
the source corpus (BNC). Both OMT and SMA
features can be obtained.
? TRAIN ? {SPORTS,FINANCES}: Like the
previous, but using unlabeled examples from
one of the target corpora (FINANCES or
SPORTS) instead. Both OMT and SMA feature
can be obtained.
Based on previous work (Agirre et al, 2005), we
used 50% of the respective unlabeled corpora for
OMT features, and the whole corpora for SMA.
4.4 Learning methods
We used two well known classifiers, Support Vec-
tor Machines (SVM) and k-Nearest Neighbors (k-
NN). Regarding SVM we used linear kernels imple-
mented in SVM-Light (Joachims, 1999). We esti-
mated the soft margin (C) for each feature space
and each word using a greedy process in a prelim-
inary experiment on the source training data using
cross-validation. The same C value was used in the
rest of the settings.
k-NN is a memory based learning method,
where the neighbors are the k most similar la-
beled examples to the test example. The similarity
among instances is measured by the cosine of their
vectors. The test instance is labeled with the sense
obtaining the maximum the sum of the weighted
vote of the k most similar contexts. We set k to
5 based on previous results (Agirre and Lopez de
Lacalle, 2007).
5 Domain adaptation scenario
In this scenario we try to adapt a general purpose
supervised WSD system trained on the source cor-
pus (BNC) to a target corpus (either SPORTS or FI-
NANCES) using unlabeled corpora only.
5.1 Experimental results
Table 1 shows the precision results for this sce-
nario. Note that all methods have full coverage,
i.e. they return a sense for all test examples, and
therefore precision suffices to compare among sys-
tems. We have computed significance ranges for
all results in this paper using bootstrap resam-
pling (Noreen, 1989). F
1
scores outside of these
intervals are assumed to be significantly different
from the related F
1
score (p < 0.05).
The table has two main parts, each regarding
to one of the target domains, SPORTS and FI-
NANCES. The use of two target domains allows to
test whether the methods behave similarly in both
domains. The columns denote the classifier and
SVD method used: the MFS column corresponds
to the most frequent sense, k-NN-ORIG (SVM-
ORIG) corresponds to performing k-NN (SVM) on
the original feature space, k-NN-OMT (SVM-OMT)
corresponds to k-NN (SVM) on the reduced dimen-
sions of the OMT strategy, and k-NN-SMA (SVM-
SMA) corresponds to k-NN (SVM) on the reduced
dimensions of the SMA strategy (cf. Section 4.2).
The rows correspond to the matrix used for SVD
(cf. Section 4.3). Note that some of the cells have
no result, because that combination is not applica-
ble, e.g. using the TRAIN ? BNC in the original
space.
In the first row (TRAIN) of Table 1 we can
see that in both domains SVM on the original
space outperforms k-NN with statistical signifi-
20
BNC? SPORTS
matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMA
TRAIN 39.0?1.3 51.7?1.3 53.0?1.6 - 53.9?1.3 47.4?1.5 -
TRAIN ? SPORTS - - 47.8?1.5 49.7?1.5 - 51.8?1.5 53.8?1.5
TRAIN ? BNC - - 61.4?1.4 57.1?1.5 - 57.1?1.6 57.2?1.5
BNC? FINANCES
matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMA
TRAIN 51.2?1.6 60.4?1.6 62.5?1.4 - 62.9?1.6 59.4?1.5 -
TRAIN ? FINANCES - - 57.4?1.9 60.6?1.5 - 60.4?1.4 62.7?1.4
TRAIN ? BNC - - 65.9?1.5 68.3?1.4 - 67.0?1.3 66.8?1.5
Table 1: Precision for the domain adaptation scenario: training on labeled source corpus, plus unlabeled
corpora.
cance. Those are the baseline systems. On the
same row, working on the reduced space of the
TRAIN matrix with OMT allows to improve the re-
sults of k-NN, but not for SVM.
Contrary to our expectations, adding target unla-
beled corpora (TRAIN ? SPORTS and TRAIN ? FI-
NANCES rows respectively) does not improve the
results over the baseline. But using the source un-
labeled data (TRAIN ? BNC), we find that for both
domains and in all four columns the results are sig-
nificantly better than for the best baseline in both
SPORTS and FINANCES corpora.
The best results on the TRAIN ? BNC row de-
pend on the domain corpus. While k-NN-OMT ob-
tains the best results for SPORTS, in FINANCES
k-NN-SMA is best. k-NN, in principle a weaker
method that SVM, is able to attain the same or
superior performance than SVM on the reduced
spaces.
Table 3 summarizes the main results, and also
shows the error reduction figures, which range be-
tween 6.9% and 16.3%. As the most important
conclusion, we want to stress that, in this sce-
nario, we are able to build a very robust system
just adding unlabeled source material, and that we
fail to adapt to the domain using the target cor-
pus. These results are relevant to improve a generic
WSD system to be more robust when ported to new
domains.
5.2 Controlling size
In the original experiments reported in the previ-
ous sections, the size of the unlabeled corpora was
not balanced. Due to the importance of the amount
of unlabeled data, we performed two control ex-
periments for the OMT and SMA matrices on the
domain adaptation scenario, focusing on the k-NN
method. Regarding OMT, we used the minimum
number of instances per word between BNC and
each of the target domains. The system obtained
60.0 of precision using unlabeled data from BNC
and 49.5 for SPORTS data (compared to 61.4 and
47.8 in table 1, respectively). We did the same in
the FINANCES domain, and we obtained 65.6 of
precision for BNC and 54.4 for FINANCES (com-
pared to 65.7 and 57.4 in table 1, respectively). Al-
though the contribution of BNC unlabeled data is
slightly lower in this experiment, due to the smaller
amount of data, it still outperforms the target unla-
beled data by a large margin.
In the case of the SMA matrix, we used 25% of
the BNC, which is comparable to the SPORTS and
FINANCES sizes. The results, 56.9 of precision in
SPORTS domain and 68.1 in FINANCES (compared
to 57.1 and 68.3 in table 1, respectively), confirm
that the size is not an important factor for SMA ei-
ther.
6 Target scenario
In this second scenario we focus on the target do-
main. We train and test on the target domain, and
use unlabeled data in order to improve the result.
The goal of these experiments is to check the be-
havior of our method when applied to the target
domain, in order to better understand the results on
the domain adaptation scenario. They also provide
an upperbound for semi-supervised domain adap-
tation.
6.1 Experimental results
The results are presented in table 2. All experi-
ments in this section have been performed using
3-fold cross-validation. Again, we have full cover-
age in all cases, and the significance ranges corre-
spond to the 95% confidence level. The table has
two main parts, each regarding to one of the target
domains, SPORTS and FINANCES. As in Table 1,
the columns specify the classifier and SVD method
used, and the rows correspond to the matrices used
21
SPORTS? SPORTS (xval)
matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMA
TRAIN 77.8?1.2 84.5?1.0 85.0?1.1 - 85.1?1.0 81.0?1.5 -
TRAIN ? SPORTS - - 86.1?0.9 82.7?1.1 - 85.1?1.1 80.3?1.5
TRAIN ? BNC - - 84.4?1.0 80.4?1.5 - 84.3?0.9 79.8?1.2
FINANCES? FINANCES (xval)
matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMA
TRAIN 82.3?1.3 87.1?1.0 87.4?1.0 - 87.0?1.0 85.5?1.1 -
TRAIN ? SPORTS - - 87.8?0.8 84.3?1.4 - 86.4?0.9 82.9?1.1
TRAIN ? BNC - - 87.4?1.2 83.5?1.2 - 85.7?0.9 84.3?1.1
Table 2: Precision for the target scenario: training on labeled target corpora, plus unlabeled corpora.
to obtain the features.
Table 2 shows that k-NN-OMT using the tar-
get corpus (SPORTS and FINANCES, respectively)
slightly improves over the k-NN-ORIG and SVM-
ORIG classifiers, with significant difference in the
SPORTS domain. Contrary to the results on the
previous section, the source unlabeled corpus de-
grades performance, but the target corpus does al-
low for small improvements. Note that, in this sce-
nario, both SVM and k-NN perform similarly in the
original space, but only k-NN is able to profit from
the reduced space. Table 3 summarizes the best
result, alongside the error reduction.
The results of these experiments allow to con-
trast both scenarios, and to get deeper insight about
the relation between the labeled and unlabeled data
when performing SVD, as we will examine in the
next section.
7 Discussion
The main contribution of this paper is to show
that we obtain robustness when faced with do-
main shifts using a semi-supervised strategy. We
show that we can obtain it using a large, general,
unlabeled corpus. Note that our semi-supervised
method to attain robustness for domain shifts is
very cost-effective, as it does not require costly
hand-tagged material nor even large numbers of
unlabeled data from each target domain. These
results are more valuable given the lack of sub-
stantial positive results on the literature on semi-
supervised or supervised domain adaptation for
WSD (Escudero et al, 2000; Mart??nez and Agirre,
2000; Chan and Ng, 2007).
Compared to other settings, our semi-supervised
results improve over the completely unsupervised
system in (Koeling et al, 2005), which had 43.7%
and 49.9% precision for the SPORTS and FI-
NANCES domains respectively, but lag well behind
the target domain scenario, showing that there is
still room for improvement in the semi-supervised
setting.
While these results are based on a lexical sam-
ple, and thus not directly generalizable to an all-
words corpus, we think that they reflect the main
trends for nouns, as the 41 nouns where selected
among those exhibiting domain dependence (Koel-
ing et al, 2005). We can assume, though it would
be needed to be explored empirically, that other
nouns exhibiting domain independence would de-
grade less when moving to other domains, and thus
corroborate the robustness effect we have discov-
ered.
The fact that we attain robustness rather than do-
main adaptation proper deserves some analysis. In
the domain adaptation scenario only source unla-
beled data helped, but the results on the target sce-
nario show that it is the target unlabeled data which
is helping, and not the source one. Given that
SVD basically finds correlations among features,
it seems that constructing the term-by-document
(or feature-by-example) matrix with the training
data and the unlabeled corpus related to the train-
ing data is the key factor in play here.
The reasons for this can be traced back as fol-
lows. Our source corpus is the BNC, which is a
balanced corpus containing a variety of genres and
domains. The 100 examples for each word that
have been hand-tagged were gathered at random,
and thus cover several domains. For instance, the
OMT strategy for building the matrix extracts hun-
dreds of other examples from the BNC, and when
SVD collapses the features into a reduced space,
it effectively captures the most important corre-
lations in the feature-by-example matrix. When
faced with examples from a new domain, the re-
duced matrix is able to map some of the features
found in the test example to those in the train ex-
ample. Such overlap is more difficult if only 100
examples from the source domain are available.
22
SPORTS FINANCES sign. E.R (%) method
53.9?1.3 62.9?1.6 - - labeled source (SVM-ORIG: baseline )
57.1?1.5 68.3?1.4 ++ 6.9/14.5 labeled source + SVD on unlabeled source (k-NN-SMA)
61.4?1.4 65.9?1.5 ++ 16.3/8.1 labeled source + SVD on unlabeled source (k-NN-OMT)
85.1?1.0 87.0?1.0 - - labeled target (SVM-ORIG: baseline)
86.1?0.9 87.8?0.8 + 6.7/6.1 labeled target + SVD on unlabeled target (k-NN-OMT)
Table 3: Summary with the most important results for the two scenarios (best results for each in bold).
The significance column shows significance over baselines: ++ (significant in both target domains),
+ (significant in a single domain). The E.R column shows the error reduction in percentages over the
baseline methods.
The unlabeled data and SVD process allow to cap-
ture correlations among the features occurring in
the test data and those in the training data.
On the other hand, we are discarding all original
features, as we focus on the features from the re-
duced space alone. The newly found correlations
come at the price of possibly ignoring effective
original features, causing information loss. Only
when the correlations found in the reduced space
outweigh this information loss do we get better
performance on the reduced space than in the orig-
inal space. The experiment in Section 6 is impor-
tant in that it shows that the improvement is much
smaller and only significant in the target domain
scenario, which is in accordance with the hypothe-
sis above. This information loss is a motivation for
the combination of the features from the reduced
space with the original features, which will be the
focus of our future work.
Regarding the learning method and the two
strategies to apply SVD, the results show that k-
NN profits from the reduced spaces more than
SVM, even if its baseline performance is lower
than SVM. Regarding the matrix building system,
in the domain adaptation scenario, k-NN-OMT ob-
tains the best results (with statistical significance)
in the SPORTS corpus, and k-NN-SMA yields the
best results (with statistical significance) in the FI-
NANCES domain. Averaging over both domains,
k-NN-OMT is best. The target scenario results con-
firm this trend, as k-NN-OMT is superior to k-NN-
SMA in both domains. These results are in ac-
cordance with our previous experience on WSD
(Agirre et al, 2005), where our OMT method got
better results than SMA and those of (Gliozzo et
al., 2005) (who also use a method similar to SMA)
on the Senseval-3 lexical sample. While OMT re-
duces the feature-by-example matrix of each tar-
get word, SMA reduces a single term-by-document
matrix. SMA is able to find important correlations
among similar terms in the corpus, but it misses the
rich feature set used by WSD systems, as it focuses
on bag-of-words alone. OMT on the other hand is
able to find correlations between all features which
are relevant to the target word only.
8 Conclusions and Future Work
In this paper we explore robustness and domain
adaptation issues for Word Sense Disambiguation
using SVD and unlabeled data. We focus on the
semi-supervised scenario, where we train on the
source corpus (BNC), test on two target corpora
(SPORTS and FINANCES sections of Reuters), and
improve the results using unlabeled data.
Our method yields up to 16.3% error reduction
compared to SVM and k-NN on the labeled data
alone, showing the first positive results on domain
adaptation for WSD. In fact, we show that our re-
sults are due to the use of a large, general, unla-
beled corpus, and rather than domain-adaptation
proper we show robustness in face of a domain
shift. This kind of robustness is even more cost-
effective than semi-supervised domain adaptation,
as it does not require large unlabeled corpora and
repeating the computations for each new target do-
main.
This paper shows that the OMT technique to ap-
ply SVD that we proposed in (Agirre et al, 2005)
compares favorably to SMA, which has been previ-
ously used in (Gliozzo et al, 2005), and that k-NN
excels SVM on the features from the reduced space.
We also show that the unlabeled data needs to be
related to the training data, and that the benefits of
our method are larger when faced with a domain
shift (compared to test data coming from the same
domain as the training data).
In the future, we plan to combine the features
from the reduced space with the rest of features,
either using a combination of k-NN classifiers
(Agirre et al, 2005; Agirre and Lopez de Lacalle,
2007) or a complex kernel (Gliozzo et al, 2005).
23
A natural extension of our work would be to apply
our techniques to the supervised domain adapta-
tion scenario.
Acknowledgments
We wish to thank Diana McCarthy and Rob Koel-
ing for kindly providing us the Reuters tagged cor-
pora, David Mart??nez for helping us with the learn-
ing features, and Walter Daelemans for his ad-
vice on domain adaptation. Oier Lopez de La-
calle has a PhD grant from the Basque Govern-
ment. This work is partially funded by the Educa-
tion Ministry (KNOW TIN2006-15049, OpenMT
TIN2006-15307-C03-02) and the Basque Country
University (IT-397-07).
References
Agirre, E. and O. Lopez de Lacalle. 2007. UBC-ALM:
Combining k-NN with SVD for WSD. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007). Association for
Computational Linguistics.
Agirre, E., O. Lopez de Lacalle, and D. Mart??nez. 2005.
Exploring feature spaces with svd and unlabeled data
for Word Sense Disambiguation. In Proceedings of
the Conference on Recent Advances on Natural Lan-
guage Processing (RANLP?05).
Ando, R. Kubota. 2006. Applying alternating structure
optimization to word sense disambiguation. In Pro-
ceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL).
Blitzer, J., R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing.
Chan, Yee Seng and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics.
Chelba, C. and A. Acero. 2004. Adaptation of maxi-
mum entropy classifier: Little data can help a lot. In
Proceedings of of th Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Daum?e III, H. and D. Marcu. 2006. Domain adaptation
for statistical classifiers. Journal of Artificial Intelli-
gence Research, 26:101?126.
Daum?e III, H. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics.
Deerwester, S., S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by Latent Semantic
Analysis. Journal of the American Society for Infor-
mation Science.
Escudero, G., L. M?arquez, and G. Rigau. 2000. An
Empirical Study of the Domain Dependence of Su-
pervised Word Sense Didanbiguation Systems. Pro-
ceedings of the joint SIGDAT Conference on Empir-
ical Methods in Natural Language Processing and
Very Large Corpora, EMNLP/VLC.
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Gliozzo, A. M., C. Giuliano, and C. Strapparava. 2005.
Domain Kernels for Word Sense Disambiguation.
43nd Annual Meeting of the Association for Com-
putational Linguistics. (ACL-05).
Joachims, T. 1999. Making Large?Scale SVM Learn-
ing Practical. Advances in Kernel Methods ? Sup-
port Vector Learning, Cambridge, MA. MIT Press.
Koeling, R., D. McCarthy, and J. Carroll. 2005.
Domain-specific sense distributions and predomi-
nant sense acquisition. In Proceedings of the Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Process-
ing. HLT/EMNLP.
Leech, G. 1992. 100 million words of English: the
British National Corpus. Language Research.
Mart??nez, D. and E. Agirre. 2000. One Sense per Col-
location and Genre/Topic Variations. Conference on
Empirical Method in Natural Language.
Ngai, G. and R. Florian. 2001. Transformation-Based
Learning in the Fast Lane. Proceedings of the Sec-
ond Conference of the North American Chapter of
the Association for Computational Linguistics.
Noreen, E. W. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons.
Pedersen, T. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01).
Pradhan, S., E. Loper, D. Dligach, and M. Palmer.
2007. Semeval-2007 task-17: English lexical sam-
ple, srl and all words. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007).
Rose, T. G., M. Stevenson, and M. Whitehead. 2002.
The reuters corpus volumen 1 from yesterday?s news
to tomorrow?s language resources. In Proceedings
of the Third International Conference on Language
Resources and Evaluation (LREC-2002).
Zelikovitz, S. and H. Hirsh. 2001. Using LSI for text
classification in the presence of background text. In
Proceedings of CIKM-01, 10th ACM International
Conference on Information and Knowledge Manage-
ment. US.
24
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 42?50,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Supervised Domain Adaption for WSD
Eneko Agirre and Oier Lopez de Lacalle
IXA NLP Group
University of the Basque Country
Donostia, Basque Contry
{e.agirre,oier.lopezdelacalle}@ehu.es
Abstract
The lack of positive results on super-
vised domain adaptation for WSD have
cast some doubts on the utility of hand-
tagging general corpora and thus devel-
oping generic supervised WSD systems.
In this paper we show for the first time
that our WSD system trained on a general
source corpus (BNC) and the target corpus,
obtains up to 22% error reduction when
compared to a system trained on the tar-
get corpus alone. In addition, we show
that as little as 40% of the target corpus
(when supplemented with the source cor-
pus) is sufficient to obtain the same results
as training on the full target data. The key
for success is the use of unlabeled data
with SVD, a combination of kernels and
SVM.
1 Introduction
In many Natural Language Processing (NLP)
tasks we find that a large collection of manually-
annotated text is used to train and test supervised
machine learning models. While these models
have been shown to perform very well when tested
on the text collection related to the training data
(what we call the source domain), the perfor-
mance drops considerably when testing on text
from other domains (called target domains).
In order to build models that perform well in
new (target) domains we usually find two settings
(Daume? III, 2007). In the semi-supervised setting,
the training hand-annotated text from the source
domain is supplemented with unlabeled data from
the target domain. In the supervised setting, we
use training data from both the source and target
domains to test on the target domain.
In (Agirre and Lopez de Lacalle, 2008) we
studied semi-supervised Word Sense Disambigua-
tion (WSD) adaptation, and in this paper we fo-
cus on supervised WSD adaptation. We compare
the performance of similar supervised WSD sys-
tems on three different scenarios. In the source
to target scenario the WSD system is trained on
the source domain and tested on the target do-
main. In the target scenario the WSD system
is trained and tested on the target domain (using
cross-validation). In the adaptation scenario the
WSD system is trained on both source and target
domain and tested in the target domain (also using
cross-validation over the target data). The source
to target scenario represents a weak baseline for
domain adaptation, as it does not use any exam-
ples from the target domain. The target scenario
represents the hard baseline, and in fact, if the do-
main adaptation scenario does not yield better re-
sults, the adaptation would have failed, as it would
mean that the source examples are not useful when
we do have hand-labeled target examples.
Previous work shows that current state-of-the-
art WSD systems are not able to obtain better re-
sults on the adaptation scenario compared to the
target scenario (Escudero et al, 2000; Agirre and
Mart??nez, 2004; Chan and Ng, 2007). This would
mean that if a user of a generic WSD system (i.e.
based on hand-annotated examples from a generic
corpus) would need to adapt it to a specific do-
main, he would be better off throwing away the
generic examples and hand-tagging domain exam-
ples directly. This paper will show that domain
adaptation is feasible, even for difficult domain-
related words, in the sense that generic corpora
can be reused when deploying WSD systems in
specific domains. We will also show that, given
the source corpus, our technique can save up to
60% of effort when tagging domain-related occur-
rences.
We performed on a publicly available corpus
which was designed to study the effect of domains
in WSD (Koeling et al, 2005). It comprises 41
42
nouns which are highly relevant in the SPORTS
and FINANCES domains, with 300 examples for
each. The use of two target domains strengthens
the conclusions of this paper.
Our system uses Singular Value Decomposi-
tion (SVD) in order to find correlations between
terms, which are helpful to overcome the scarcity
of training data in WSD (Gliozzo et al, 2005).
This work explores how this ability of SVD and
a combination of the resulting feature spaces im-
proves domain adaptation. We present two ways
to combine the reduced spaces: kernel combina-
tion with Support Vector Machines (SVM), and k
Nearest-Neighbors (k-NN) combination.
The paper is structured as follows. Section 2 re-
views prior work in the area. Section 3 presents
the data sets used. In Section 4 we describe
the learning features, including the application of
SVD, and in Section 5 the learning methods and
the combination. The experimental results are pre-
sented in Section 6. Section 7 presents the discus-
sion and some analysis of this paper and finally
Section 8 draws the conclusions.
2 Prior work
Domain adaptation is a practical problem attract-
ing more and more attention. In the supervised
setting, a recent paper by Daume? III (2007) shows
that a simple feature augmentation method for
SVM is able to effectively use both labeled tar-
get and source data to provide the best domain-
adaptation results in a number of NLP tasks. His
method improves or equals over previously ex-
plored more sophisticated methods (Daume? III
and Marcu, 2006; Chelba and Acero, 2004). The
feature augmentation consists in making three ver-
sion of the original features: a general, a source-
specific and a target-specific versions. That way
the augmented source contains the general and
source-specific version and the augmented target
data general and specific versions. The idea be-
hind this is that target domain data has twice the
influence as the source when making predictions
about test target data. We reimplemented this
method and show that our results are better.
Regarding WSD, some initial works made a ba-
sic analysis of domain adaptation issues. Escud-
ero et al (2000) tested the supervised adaptation
scenario on the DSO corpus, which had examples
from the Brown corpus and Wall Street Journal
corpus. They found that the source corpus did
not help when tagging the target corpus, show-
ing that tagged corpora from each domain would
suffice, and concluding that hand tagging a large
general corpus would not guarantee robust broad-
coverage WSD. Agirre and Mart??nez (2000) used
the DSO corpus in the supervised scenario to show
that training on a subset of the source corpora that
is topically related to the target corpus does allow
for some domain adaptation.
More recently, Chan and Ng (2007) performed
supervised domain adaptation on a manually se-
lected subset of 21 nouns from the DSO corpus.
They used active learning, count-merging, and
predominant sense estimation in order to save tar-
get annotation effort. They showed that adding
just 30% of the target data to the source exam-
ples the same precision as the full combination of
target and source data could be achieved. They
also showed that using the source corpus allowed
to significantly improve results when only 10%-
30% of the target corpus was used for training.
Unfortunately, no data was given about the target
corpus results, thus failing to show that domain-
adaptation succeeded. In followup work (Zhong et
al., 2008), the feature augmentation approach was
combined with active learning and tested on the
OntoNotes corpus, on a large domain-adaptation
experiment. They reduced significantly the ef-
fort of hand-tagging, but only obtained domain-
adaptation for smaller fractions of the source and
target corpus. Similarly to these works we show
that we can save annotation effort on the target
corpus, but, in contrast, we do get domain adap-
tation when using the full dataset. In a way our
approach is complementary, and we could also ap-
ply active learning to further reduce the number of
target examples to be tagged.
Though not addressing domain adaptation,
other works on WSD also used SVD and are
closely related to the present paper. Ando (2006)
used Alternative Structured Optimization. She
first trained one linear predictor for each target
word, and then performed SVD on 7 carefully se-
lected submatrices of the feature-to-predictor ma-
trix of weights. The system attained small but
consistent improvements (no significance data was
given) on the Senseval-3 lexical sample datasets
using SVD and unlabeled data.
Gliozzo et al (2005) used SVD to reduce the
space of the term-to-document matrix, and then
computed the similarity between train and test
43
instances using a mapping to the reduced space
(similar to our SMA method in Section 4.2). They
combined other knowledge sources into a complex
kernel using SVM. They report improved perfor-
mance on a number of languages in the Senseval-
3 lexical sample dataset. Our present paper dif-
fers from theirs in that we propose an additional
method to use SVD (the OMT method), and that
we focus on domain adaptation.
In the semi-supervised setting, Blitzer et al
(2006) used Structural Correspondence Learning
and unlabeled data to adapt a Part-of-Speech tag-
ger. They carefully select so-called ?pivot fea-
tures? to learn linear predictors, perform SVD on
the weights learned by the predictor, and thus learn
correspondences among features in both source
and target domains. Our technique also uses SVD,
but we directly apply it to all features, and thus
avoid the need to define pivot features. In prelim-
inary work we unsuccessfully tried to carry along
the idea of pivot features to WSD. On the contrary,
in (Agirre and Lopez de Lacalle, 2008) we show
that methods closely related to those presented in
this paper produce positive semi-supervised do-
main adaptation results for WSD.
The methods used in this paper originated in
(Agirre et al, 2005; Agirre and Lopez de Lacalle,
2007), where SVD over a feature-to-documents
matrix improved WSD performance with and
without unlabeled data. The use of several k-
NN classifiers trained on a number of reduced and
original spaces was shown to get the best results
in the Senseval-3 dataset and ranked second in the
SemEval 2007 competition. The present paper ex-
tends this work and applies it to domain adapta-
tion.
3 Data sets
The dataset we use was designed for domain-
relatedWSD experiments by Koeling et al (2005),
and is publicly available. The examples come
from the BNC (Leech, 1992) and the SPORTS and
FINANCES sections of the Reuters corpus (Rose
et al, 2002), comprising around 300 examples
(roughly 100 from each of those corpora) for each
of the 41 nouns. The nouns were selected be-
cause they were salient in either the SPORTS or
FINANCES domains, or because they had senses
linked to those domains. The occurrences were
hand-tagged with the senses from WordNet (WN)
version 1.7.1 (Fellbaum, 1998). In our experi-
ments the BNC examples play the role of general
source corpora, and the FINANCES and SPORTS
examples the role of two specific domain target
corpora.
Compared to the DSO corpus used in prior work
(cf. Section 2) this corpus has been explicitly cre-
ated for domain adaptation studies. DSO con-
tains texts coming from the Brown corpus and the
Wall Street Journal, but the texts are not classi-
fied according to specific domains (e.g. Sports,
Finances), which make DSO less suitable to study
domain adaptation. The fact that the selected
nouns are related to the target domain makes
the (Koeling et al, 2005) corpus more demanding
than the DSO corpus, because one would expect
the performance of a generic WSD system to drop
when moving to the domain corpus for domain-
related words (cf. Table 1), while the performance
would be similar for generic words.
In addition to the labeled data, we also use
unlabeled data coming from the three sources
used in the labeled corpus: the ?written? part
of the BNC (89.7M words), the FINANCES part
of Reuters (32.5M words), and the SPORTS part
(9.1M words).
4 Original and SVD features
In this section, we review the features and two
methods to apply SVD over the features.
4.1 Features
We relied on the usual features used in previous
WSD work, grouped in three main sets. Local
collocations comprise the bigrams and trigrams
formed around the target word (using either lem-
mas, word-forms, or PoS tags) , those formed
with the previous/posterior lemma/word-form in
the sentence, and the content words in a ?4-word
window around the target. Syntactic dependen-
cies use the object, subject, noun-modifier, prepo-
sition, and sibling lemmas, when available. Fi-
nally, Bag-of-words features are the lemmas of
the content words in the whole context, plus the
salient bigrams in the context (Pedersen, 2001).
We refer to these features as original features.
4.2 SVD features
Apart from the original space of features, we have
used the so called SVD features, obtained from
the projection of the feature vectors into the re-
duced space (Deerwester et al, 1990). Basically,
44
we set a term-by-document or feature-by-example
matrix M from the corpus (see section below for
more details). SVD decomposes M into three ma-
trices, M = U?V T . If the desired number of
dimensions in the reduced space is p, we select p
rows from ? and V , yielding ?p and Vp respec-
tively. We can map any feature vector ~t (which
represents either a train or test example) into the
p-dimensional space as follows: ~tp = ~tTVp??1p .
Those mapped vectors have p dimensions, and
each of the dimensions is what we call a SVD fea-
ture. We have explored two different variants in
order to build the reduced matrix and obtain the
SVD features, as follows.
Single Matrix for All target words (SVD-
SMA). The method comprises the following steps:
(i) extract bag-of-word features (terms in this case)
from unlabeled corpora, (ii) build the term-by-
document matrix, (iii) decompose it with SVD, and
(iv) map the labeled data (train/test). This tech-
nique is very similar to previous work on SVD
(Gliozzo et al, 2005; Zelikovitz and Hirsh, 2001).
The dimensionality reduction is performed once,
over the whole unlabeled corpus, and it is then ap-
plied to the labeled data of each word. The re-
duced space is constructed only with terms, which
correspond to bag-of-words features, and thus dis-
cards the rest of the features. Given that the WSD
literature shows that all features are necessary for
optimal performance (Pradhan et al, 2007), we
propose the following alternative to construct the
matrix.
OneMatrix per Target word (SVD-OMT). For
each word: (i) construct a corpus with its occur-
rences in the labeled and, if desired, unlabeled cor-
pora, (ii) extract all features, (iii) build the feature-
by-example matrix, (iv) decompose it with SVD,
and (v) map all the labeled training and test data
for the word. Note that this variant performs one
SVD process for each target word separately, hence
its name.
When building the SVD-OMT matrices we can
use only the training data (TRAIN) or both the train
and unlabeled data (+UNLAB). When building the
SVD-SMA matrices, given the small size of the in-
dividual word matrices, we always use both the
train and unlabeled data (+UNLAB). Regarding the
amount of data, based also on previous work, we
used 50% of the available data for OMT, and the
whole corpora for SMA. An important parameter
when doing SVD is the number of dimensions in
the reduced space (p). We tried two different val-
ues for p (25 and 200) in the BNC domain, and
set a dimension for each classifier/matrix combi-
nation.
4.3 Motivation
The motivation behind our method is that although
the train and test feature vectors overlap suffi-
ciently in the usual WSD task, the domain dif-
ference makes such overlap more scarce. SVD
implicitly finds correlations among features, as it
maps related features into nearby regions in the re-
duced space. In the case of SMA, SVD is applied
over the joint term-by-document matrix of labeled
(and possibly unlabeled corpora), and it thus can
find correlations among closely related words (e.g.
cat and dog). These correlations can help reduce
the gap among bag-of-words features from the
source and target examples. In the case of OMT,
SVD over the joint feature-by-example matrix of
labeled and unlabeled examples of a word allows
to find correlations among features that show sim-
ilar occurrence patterns in the source and target
corpora for the target word.
5 Learning methods
k-NN is a memory based learning method, where
the neighbors are the k most similar labeled exam-
ples to the test example. The similarity among in-
stances is measured by the cosine of their vectors.
The test instance is labeled with the sense obtain-
ing the maximum sum of the weighted vote of the
k most similar contexts. We set k to 5 based on
previous results published in (Agirre and Lopez de
Lacalle, 2007).
Regarding SVM, we used linear kernels, but
also purpose-built kernels for the reduced spaces
and the combinations (cf. Section 5.2). We used
the default soft margin (C=0). In previous ex-
periments we learnt that C is very dependent on
the feature set and training data used. As we
will experiment with different features and train-
ing datasets, it did not make sense to optimize it
across all settings.
We will now detail how we combined the origi-
nal and SVD features in each of the machine learn-
ing methods.
5.1 k-NN combinations
Our k-NN combination method (Agirre et al,
2005; Agirre and Lopez de Lacalle, 2007) takes
45
advantage of the properties of k-NN classifiers and
exploit the fact that a classifier can be seen as
k points (number of nearest neighbor) each cast-
ing one vote. This makes easy to combine sev-
eral classifiers, one for each feature space. For in-
stance, taking two k-NN classifiers of k = 5, C1
andC2, we can combine them into a single k = 10
classifier, where five votes come from C1 and five
from C2. This allows to smoothly combine classi-
fiers from different feature spaces.
In this work we built three single k-NN classi-
fiers trained on OMT, SMA and the original fea-
tures, respectively. In order to combine them we
weight each vote by the inverse ratio of its position
in the rank of the single classifier, (k ? ri + 1)/k,
where ri is the rank.
5.2 Kernel combination
The basic idea of kernel methods is to find a suit-
able mapping function (?) in order to get a better
generalization. Instead of doing this mapping ex-
plicitly, kernels give the chance to do it inside the
algorithm. We will formalize it as follows. First,
we define the mapping function ? : X ? F . Once
the function is defined, we can use it in the kernel
function in order to become an implicit function
K(x, z) = ??(x) ? ?(z)?, where ??? denotes a in-
ner product between vectors in the feature space.
This way, we can very easily define mappings
representing different information sources and use
this mappings in several machine learning algo-
rithm. In our work we use SVM.
We defined three individual kernels (OMT, SMA
and original features) and the combined kernel.
The original feature kernel (KOrig) is given by
the identity function over the features ? : X ? X ,
defining the following kernel:
KOrig(xi,xj) =
?xi ? xj?
?
?xi ? xi? ?xj ? xj?
where the denominator is used to normalize and
avoid any kind of bias in the combination.
The OMT kernel (KOmt) and SMA kernel
(KSma) are defined using OMT and SMA projec-
tion matrices, respectively (cf. Section 4.2). Given
the OMT function mapping ?omt : Rm ? Rp,
where m is the number of the original features
and p the reduced dimensionality, then we define
KOmt(xi,xj) as follows (KSma is defined simi-
larly):
??omt(xi) ? ?omt(xj)?
?
??omt(xi) ? ?omt(xi)? ??omt(xj) ? ?omt(xj)?
BNC ? X SPORTS FINANCES
MFS 39.0 51.2
k-NN 51.7 60.4
SVM 53.9 62.9
Table 1: Source to target results: Train on BNC,
test on SPORTS and FINANCES.
Finally, we define the kernel combination:
KComb(xi,xj) =
n?
l=1
Kl(xi,xj)
?
Kl(xi,xi)Kl(xj,xj)
where n is the number of single kernels explained
above, and l the index for the kernel type.
6 Domain adaptation experiments
In this section we present the results in our two ref-
erence scenarios (source to target, target) and our
reference scenario (domain adaptation). Note that
all methods presented here have full coverage, i.e.
they return a sense for all test examples, and there-
fore precision equals recall, and suffices to com-
pare among systems.
6.1 Source to target scenario: BNC ? X
In this scenario our supervised WSD systems are
trained on the general source corpus (BNC) and
tested on the specific target domains separately
(SPORTS and FINANCES). We do not perform any
kind of adaptation, and therefore the results are
those expected for a generic WSD system when
applied to domain-specific texts.
Table 1 shows the results for k-NN and SVM
trained with the original features on the BNC. In
addition, we also show the results for the Most
Frequent Sense baseline (MFS) taken from the
BNC. The second column denotes the accuracies
obtained when testing on SPORTS, and the third
column the accuracies for FINANCES. The low ac-
curacy obtained with MFS, e.g. 39.0 of precision
in SPORTS, shows the difficulty of this task. Both
classifiers improve over MFS. These classifiers are
weak baselines for the domain adaptation system.
6.2 Target scenario X ? X
In this scenario we lay the harder baseline which
the domain adaptation experiments should im-
prove on (cf. next section). The WSD systems
are trained and tested on each of the target cor-
pora (SPORTS and FINANCES) using 3-fold cross-
validation.
46
SPORTS FINANCES
X ? X TRAIN +UNLAB TRAIN +UNLAB
MFS 77.8 - 82.3 -
k-NN 84.5 - 87.1 -
SVM 85.1 - 87.0 -
k-NN-OMT 85.0 86.1 87.3 87.6
SVM-OMT 82.9 85.1 85.3 86.4
k-NN-SMA - 81.1 - 83.2
SVM-SMA - 81.3 - 84.1
k-NN-COMB 86. 0 86.7 87.9 88.6
SVM-COMB - 86.5 - 88.5
Table 2: Target results: train and test on SPORTS,
train and test on FINANCES, using 3-fold cross-
validation.
Table 2 summarizes the results for this scenario.
TRAIN denotes that only tagged data was used to
train, +UNLAB denotes that we added unlabeled
data related to the source corpus when computing
SVD. The rows denote the classifier and the feature
spaces used, which are organized in four sections.
On the top rows we show the three baseline clas-
sifiers on the original features. The two sections
below show the results of those classifiers on the
reduced dimensions, OMT and SMA (cf. Section
4.2). Finally, the last rows show the results of the
combination strategies (cf. Sections 5.1 and 5.2).
Note that some of the cells have no result, because
that combination is not applicable (e.g. using the
train and unlabeled data in the original space).
First of all note that the results for the base-
lines (MFS, SVM, k-NN) are much larger than
those in Table 1, showing that this dataset is spe-
cially demanding for supervised WSD, and partic-
ularly difficult for domain adaptation experiments.
These results seem to indicate that the examples
from the source general corpus could be of little
use when tagging the target corpora. Note spe-
cially the difference in MFS performance. The pri-
ors of the senses are very different in the source
and target corpora, which is a well-known short-
coming for supervised systems. Note the high re-
sults of the baseline classifiers, which leave small
room for improvement.
The results for the more sophisticated methods
show that SVD and unlabeled data helps slightly,
except for k-NN-OMT on SPORTS. SMA de-
creases the performance compared to the classi-
fiers trained on original features. The best im-
provements come when the three strategies are
combined in one, as both the kernel and k-NN
combinations obtain improvements over the re-
spective single classifiers. Note that both the k-NN
BNC + X SPORTS FINANCES
? X TRAIN + UNLAB TRAIN + UNLAB
BNC ? X 53.9 - 62.9 -
X ? X 86.0 86.7 87.9 88.5
MFS 68.2 - 73.1 -
k-NN 81.3 - 86.0 -
SVM 84.7 - 87.5 -
k-NN-OMT 84.0 84.7 87.5 86.0
SVM-OMT 85.1 84.7 84.2 85.5
k-NN-SMA - 77.1 - 81.6
SVM-SMA - 78.1 - 80.7
k-NN-COMB 84.5 87.2 88.1 88.7
SVM-COMB - 88.4 - 89.7
SVM-AUG 85.9 - 88.1 -
Table 3: Domain adaptation results: Train on
BNC and SPORTS, test on SPORTS (same for FI-
NANCES).
and SVM combinations perform similarly.
In the combination strategy we show that unla-
beled data helps slightly, because instead of only
combining OMT and original features we have the
opportunity to introduce SMA. Note that it was not
our aim to improve the results of the basic classi-
fiers on this scenario, but given the fact that we are
going to apply all these techniques in the domain
adaptation scenario, we need to show these results
as baselines. That is, in the next section we will try
to obtain results which improve significantly over
the best results in this section.
6.3 Domain adaptation scenario
BNC + X ? X
In this last scenario we try to show that our WSD
system trained on both source (BNC) and tar-
get (SPORTS and FINANCES) data performs better
than the one trained on the target data alone. We
also use 3-fold cross-validation for the target data,
but the entire source data is used in each turn. The
unlabeled data here refers to the combination of
unlabeled source and target data.
The results are presented in table 3. Again, the
columns denote if unlabeled data has been used in
the learning process. The rows correspond to clas-
sifiers and the feature spaces involved. The first
rows report the best results in the previous scenar-
ios: BNC ? X for the source to target scenario,
and X ? X for the target scenario. The rest
of the table corresponds to the domain adaptation
scenario. The rows below correspond to MFS and
the baseline classifiers, followed by the OMT and
SMA results, and the combination results. The last
row shows the results for the feature augmentation
algorithm (Daume? III, 2007).
47
SPORTS FINANCES
BNC ? X
MFS 39.0 51.2
SVM 53.9 62.9
X ? X
MFS 77.8 82.3
SVM 85.1 87.0
k-NN-COMB (+UNLAB) 86.7 88.6
BNC +X ? X
MFS 68.2 73.1
SVM 84.7 87.5
SVM-AUG 85.9 88.1
SVM-COMB (+UNLAB) 88.4 89.7
Table 4: The most important results in each sce-
nario.
Focusing on the results, the table shows that
MFS decreases with respect to the target scenario
(cf. Table 2) when the source data is added, prob-
ably caused by the different sense distributions in
BNC and the target corpora. The baseline classi-
fiers (k-NN and SVM) are not able to improve over
the baseline classifiers on the target data alone,
which is coherent with past research, and shows
that straightforward domain adaptation does not
work.
The following rows show that our reduction
methods on themselves (OMT, SMA used by k-
NN and SVM) also fail to perform better than in
the target scenario, but the combinations using
unlabeled data (k-NN-COMB and specially SVM-
COMB) do manage to improve the best results for
the target scenario, showing that we were able to
attain domain adaptation. The feature augmenta-
tion approach (SVM-AUG) does improve slightly
over SVM in the target scenario, but not over the
best results in the target scenario, showing the dif-
ficulty of domain adaptation for WSD, at least on
this dataset.
7 Discussion and analysis
Table 4 summarizes the most important results.
The kernel combination method with unlabeled
data on the adaptation scenario reduces the error
on 22.1% and 17.6% over the baseline SVM on
the target scenario (SPORTS and FINANCES re-
spectively), and 12.7% and 9.0% over the k-NN
combination method on the target scenario. These
gains are remarkable given the already high base-
line, specially taking into consideration that the
41 nouns are closely related to the domains. The
differences, including SVM-AUG, are statistically
significant according to the Wilcoxon test with
%25 %32 %50 %62 %75 %82 %100sports (%)
80
82
84
86
88
accu
racy 
(%)
SVM-COMB (+UNLAB, BNC + SPORTS -> SPORTS)SVM-AUG (BNC + SPORTS -> SPORTS)SVM-ORIG (SPORTS -> SPORTS)y=85.1
Figure 1: Learning curves for SPORTS. The X
axis denotes the amount of SPORTS data and the
Y axis corresponds to accuracy.
%25 %32 %50 %62 %75 %82 %100finances (%)
84
86
88
90
accu
racy 
(%)
SVM-COMB (+UNLAB, BNC + FIN. -> FIN.)SVM-AUG (BNC + FIN. -> FIN.)SVM-ORIG (FIN. -> FIN.)y=87.0
Figure 2: Learning curves for FINANCES. The X
axis denotes the amount of FINANCES data and Y
axis corresponds to the accuracy.
p < 0.01.
In addition, we carried extra experiments to ex-
amine the learning curves, and to check, given
the source examples, how many additional ex-
amples from the target corpus are needed to ob-
tain the same results as in the target scenario us-
ing all available examples. We fixed the source
data and used increasing amounts of target data.
We show the original SVM on the target scenario,
and SVM-COMB (+UNLAB) and SVM-AUG as the
domain adaptation approaches. The results are
shown in figure 1 for SPORTS and figure 2 for FI-
NANCES. The horizontal line corresponds to the
performance of SVM on the target domain. The
point where the learning curves cross the horizon-
tal line show that our domain adaptation method
needs only around 40% of the target data in order
to get the same performance as the baseline SVM
on the target data. The learning curves also shows
48
that the domain adaptation kernel combination ap-
proach, no matter the amount of target data, is al-
ways above the rest of the classifiers, showing the
robustness of our approach.
8 Conclusion and future work
In this paper we explore supervised domain adap-
tation for WSD with positive results, that is,
whether hand-labeling general domain (source)
text is worth the effort when training WSD sys-
tems that are to be applied to specific domains (tar-
gets). We performed several experiments in three
scenarios. In the first scenario (source to target
scenario), the classifiers were trained on source
domain data (the BNC) and tested on the target do-
mains, composed by the SPORTS and FINANCES
sections of Reuters. In the second scenario (tar-
get scenario) we set the main baseline for our do-
main adaptation experiment, training and testing
our classifiers on the target domain data. In the last
scenario (domain adaptation scenario), we com-
bine both source and target data for training, and
test on the target data.
We report results in each scenario for k-NN and
SVM classifiers, for reduced features obtained us-
ing SVD over the training data, for the use of un-
labeled data, and for k-NN and SVM combinations
of all.
Our results show that our best domain adap-
tation strategy (using kernel combination of SVD
features and unlabeled data related to the training
data) yields statistically significant improvements:
up to 22% error reduction compared to SVM on
the target domain data alone. We also show that
our domain adaptation method only needs 40% of
the target data (in addition to the source data) in
order to get the same results as SVM on the target
alone.
We obtain coherent results in two target scenar-
ios, and consistent improvement at all levels of
the learning curves, showing the robustness or our
findings. We think that our dataset, which com-
prises examples for 41 nouns that are closely re-
lated to the target domains, is specially demand-
ing, as one would expect the performance of a
generic WSD system to drop when moving to
the domain corpus, specially on domain-related
words, while we could expect the performance to
be similar for generic or unrelated words.
In the future we would like to evaluate
our method on other datasets (e.g. DSO or
OntoNotes), to test whether the positive results are
confirmed. We would also like to study word-by-
word behaviour, in order to assess whether target
examples are really necessary for words which are
less related to the domain.
Acknowledgments
This work has been partially funded by the EU Commission
(project KYOTO ICT-2007-211423) and Spanish Research
Department (project KNOW TIN2006-15049-C03-01). Oier
Lopez de Lacalle has a PhD grant from the Basque Govern-
ment.
References
Eneko Agirre and Oier Lopez de Lacalle. 2007. Ubc-
alm: Combining k-nn with svd for wsd. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 342?
345, Prague, Czech Republic, June. Association for
Computational Linguistics.
Eneko Agirre and Oier Lopez de Lacalle. 2008. On
robustness and domain adaptation using SVD for
word sense disambiguation. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 17?24, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.
Eneko Agirre and David Mart??nez. 2004. The effect
of bias on an automatically-built word sense corpus.
Proceedings of the 4rd International Conference on
Languages Resources and Evaluations (LREC).
E. Agirre, O.Lopez de Lacalle, and David Mart??nez.
2005. Exploring feature spaces with svd and un-
labeled data for Word Sense Disambiguation. In
Proceedings of the Conference on Recent Advances
on Natural Language Processing (RANLP?05),
Borovets, Bulgaria.
Rie Kubota Ando. 2006. Applying alternating struc-
ture optimization to word sense disambiguation. In
Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL), pages
77?84, New York City.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120?128, Sydney, Australia, July.
Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2007. Do-
main adaptation with active learning for word sense
disambiguation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 49?56, Prague, Czech Republic,
June. Association for Computational Linguistics.
49
Ciprian Chelba and Alex Acero. 2004. Adaptation
of maximum entropy classifier: Little data can help
a lot. In Proceedings of of th Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Barcelona, Spain.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Scott Deerwester, Susan Dumais, Goerge Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal
of the American Society for Information Science,
41(6):391?407.
Gerard Escudero, Lluiz Ma?rquez, and German Rigau.
2000. An Empirical Study of the Domain Depen-
dence of Supervised Word Sense Didanbiguation
Systems. Proceedings of the joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora, EMNLP/VLC.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and
Carlo Strapparava. 2005. Domain Kernels for Word
Sense Disambiguation. 43nd Annual Meeting of the
Association for Computational Linguistics. (ACL-
05).
R. Koeling, D. McCarthy, and J. Carroll. 2005.
Domain-specific sense distributions and predomi-
nant sense acquisition. In Proceedings of the Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing. HLT/EMNLP, pages 419?426, Ann Ar-
bor, Michigan.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
David Mart??nez and Eneko Agirre. 2000. One Sense
per Collocation and Genre/Topic Variations. Con-
ference on Empirical Method in Natural Language.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), Pittsburgh, PA.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, srl and all words. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 87?92,
Prague, Czech Republic.
Tony G. Rose, Mark Stevenson, and Miles Whitehead.
2002. The reuters corpus volumen 1 from yester-
day?s news to tomorrow?s language resources. In
Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC-
2002), pages 827?832, Las Palmas, Canary Islands.
Sarah Zelikovitz and Haym Hirsh. 2001. Using LSI
for text classification in the presence of background
text. In Henrique Paques, Ling Liu, and David
Grossman, editors, Proceedings of CIKM-01, 10th
ACM International Conference on Information and
Knowledge Management, pages 113?118, Atlanta,
US. ACM Press, New York, US.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An
empirical study. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1002?1010, Honolulu, Hawaii,
October. Association for Computational Linguistics.
50
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 585?593,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Two graph-based algorithms for state-of-the-art WSD
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle and Aitor Soroa
IXA NLP Group
University of the Basque Country
Donostia, Basque Contry
a.soroa@si.ehu.es
Abstract
This paper explores the use of two graph
algorithms for unsupervised induction and
tagging of nominal word senses based on
corpora. Our main contribution is the op-
timization of the free parameters of those
algorithms and its evaluation against pub-
licly available gold standards. We present
a thorough evaluation comprising super-
vised and unsupervised modes, and both
lexical-sample and all-words tasks. The
results show that, in spite of the infor-
mation loss inherent to mapping the in-
duced senses to the gold-standard, the
optimization of parameters based on a
small sample of nouns carries over to all
nouns, performing close to supervised sys-
tems in the lexical sample task and yield-
ing the second-best WSD systems for the
Senseval-3 all-words task.
1 Introduction
Word sense disambiguation (WSD) is a key
enabling-technology. Supervised WSD tech-
niques are the best performing in public evalu-
ations, but need large amounts of hand-tagged
data. Existing hand-annotated corpora like Sem-
Cor (Miller et al, 1993), which is annotated with
WordNet senses (Fellbaum, 1998) allow for a
small improvement over the simple most frequent
sense heuristic, as attested in the all-words track of
the last Senseval competition (Snyder and Palmer,
2004). In theory, larger amounts of training data
(SemCor has approx. 700K words) would improve
the performance of supervised WSD, but no cur-
rent project exists to provide such an expensive re-
source.
Supervised WSD is based on the ?fixed-list of
senses? paradigm, where the senses for a tar-
get word are a closed list coming from a dic-
tionary or lexicon. Lexicographers and seman-
ticists have long warned about the problems of
such an approach, where senses are listed sepa-
rately as discrete entities, and have argued in fa-
vor of more complex representations, where, for
instance, senses are dense regions in a contin-
uum (Cruse, 2000).
Unsupervised WSD has followed this line of
thinking, and tries to induce word senses directly
from the corpus. Typical unsupervised WSD sys-
tems involve clustering techniques, which group
together similar examples. Given a set of induced
clusters (which represent word uses or senses1),
each new occurrence of the target word will be
compared to the clusters and the most similar clus-
ter will be selected as its sense.
Most of the unsupervised WSD work has been
based on the vector space model, where each
example is represented by a vector of features
(e.g. the words occurring in the context), and
the induced senses are either clusters of ex-
amples (Schu?tze, 1998; Purandare and Peder-
sen, 2004) or clusters of words (Pantel and Lin,
2002). Recently, Ve?ronis (Ve?ronis, 2004) has pro-
posed HyperLex, an application of graph models
to WSD based on the small-world properties of
cooccurrence graphs. Graph-based methods have
gained attention in several areas of NLP, including
knowledge-based WSD (Mihalcea, 2005; Navigli
and Velardi, 2005) and summarization (Erkan and
Radev, 2004; Mihalcea and Tarau, 2004).
The HyperLex algorithm presented in (Ve?ronis,
2004) is entirely corpus-based. It builds a cooccur-
rence graph for all pairs of words cooccurring in
the context of the target word. Ve?ronis shows that
this kind of graph fulfills the properties of small
world graphs, and thus possesses highly connected
1Unsupervised WSD approaches prefer the term ?word
uses? to ?word senses?. In this paper we use them inter-
changeably to refer to both the induced clusters, and to the
word senses from some reference lexicon.
585
components (hubs) in the graph. These hubs even-
tually identify the main word uses (senses) of the
target word, and can be used to perform word
sense disambiguation. These hubs are used as a
representation of the senses induced by the sys-
tem, the same way that clusters of examples are
used to represent senses in clustering approaches
to WSD (Purandare and Pedersen, 2004).
One of the problems of unsupervised systems
is that of managing to do a fair evaluation.
Most of current unsupervised systems are evalu-
ated in-house, with a brief comparison to a re-
implementation of a former system, leading to a
proliferation of unsupervised systems with little
ground to compare among them.
In preliminary work (Agirre et al, 2006), we
have shown that HyperLex compares favorably
to other unsupervised systems. We defined a
semi-supervised setting for optimizing the free-
parameters of HyperLex on the Senseval-2 En-
glish Lexical Sample task (S2LS), which con-
sisted on mapping the induced senses onto the
official sense inventory using the training part of
S2LS. The best parameters were then used on the
Senseval-3 English Lexical Sample task (S3LS),
where a similar semi-supervised method was used
to output the official sense inventory.
This paper extends the previous work in sev-
eral aspects. First of all, we adapted the PageR-
ank graph-based method (Brin and Page, 1998)
for WSD and compared it with HyperLex.
We also extend the previous evaluation scheme,
using measures in the clustering community which
only require a gold standard clustering and no
mapping step. This allows for having a purely
unsupervised WSD system, and at the same time
comparing supervised and unsupervised systems
according to clustering criteria.
We also include the Senseval-3 English All-
words testbed (S3AW), where, in principle, unsu-
pervised and semi-supervised systems have an ad-
vantage over purely supervised systems due to the
scarcity of training data. We show that our sys-
tem is competitive with supervised systems, rank-
ing second.
This paper is structured as follows. We first
present two graph-based algorithms, HyperLex
and PageRank. Section 3 presents the two evalu-
ation frameworks. Section 4 introduces parameter
optimization. Section 5 shows the experimental
setting and results. Section 6 analyzes the results
and presents related work. Finally, we draw the
conclusions and advance future work.
2 A graph algorithm for corpus-based
WSD
The basic steps for our implementation of Hyper-
Lex and its variant using PageRank are common.
We first build the cooccurrence graph, then we se-
lect the hubs that are going to represent the senses
using two different strategies inspired by Hyper-
Lex and PageRank. We are then ready to use the
induced senses to do word sense disambiguation.
2.1 Building cooccurrence graphs
For each word to be disambiguated, a text corpus
is collected, consisting of the paragraphs where
the word occurs. From this corpus, a cooccur-
rence graph for the target word is built. Vertices
in the graph correspond to words2 in the text (ex-
cept the target word itself). Two words appear-
ing in the same paragraph are said to cooccur, and
are connected with edges. Each edge is assigned
a weight which measures the relative frequency of
the two words cooccurring. Specifically, let wij be
the weight of the edge3 connecting nodes i and j,
then wij = 1 ? max[P (i | j), P (j | i)], where
P (i | j) = freqijfreqj and P (j | i) =
freqij
freqi .The weight of an edge measures how tightly
connected the two words are. Words which always
occur together receive a weight of 0. Words rarely
cooccurring receive weights close to 1.
2.2 Selecting hubs: HyperLex vs. PageRank
Once the cooccurrence graph is built, Ve?ronis pro-
poses a simple iterative algorithm to obtain its
hubs. At each step, the algorithm finds the ver-
tex with highest relative frequency4 in the graph,
and, if it meets some criteria, it is selected as a hub.
These criteria are determined by a set of heuristic
parameters, that will be explained later in Section
4. After a vertex is selected to be a hub, its neigh-
bors are no longer eligible as hub candidates. At
any time, if the next vertex candidate has a relative
frequency below a certain threshold, the algorithm
stops.
Another alternative is to use the PageRank algo-
rithm (Brin and Page, 1998) for finding hubs in the
2Following Ve?ronis, we only work on nouns.
3The cooccurrence graph is undirected, i.e. wij = wji
4In cooccurrence graphs, the relative frequency of a vertex
and its degree are linearly related, and it is therefore possible
to avoid the costly computation of the degree.
586
coocurrence graph. PageRank is an iterative algo-
rithm that ranks all the vertices according to their
relative importance within the graph following a
random-walk model. In this model, a link between
vertices v1 and v2 means that v1 recommends v2.
The more vertices recommend v2, the higher the
rank of v2 will be. Furthermore, the rank of a ver-
tex depends not only on how many vertices point
to it, but on the rank of these vertices as well.
Although PageRank was initially designed to
work with directed graphs, and with no weights
in links, the algorithm can be easily extended
to model undirected graphs whose edges are
weighted. Specifically, let G = (V, E) be an undi-
rected graph with the set of vertices V and set of
edges E. For a given vertex vi, let In(vi) be the set
of vertices pointing to it5. The rank of vi is defined
as:
P (vi) = (1? d) + d
?
j?In(vi)
wji
?
k?In(vj) wjk
P (vj)
where wij is the weight of the link between ver-
tices vi and vj , and 0 ? d ? 1. d is called the
damping factor and models the probability of a
web surfer standing at a vertex to follow a link
from this vertex (probability d) or to jump to a ran-
dom vertex in the graph (probability 1 ? d). The
factor is usually set at 0.85.
The algorithm initializes the ranks of the ver-
tices with a fixed value (usually 1N for a graph with
N vertices) and iterates until convergence below a
given threshold is achieved, or, more typically, un-
til a fixed number of iterations are executed. Note
that the convergence of the algorithms doesn?t de-
pend on the initial value of the ranks.
After running the algorithm, the vertices of the
graph are ordered in decreasing order according to
its rank, and a number of them are chosen as the
main hubs of the word. The hubs finally selected
depend again of some heuristics and will be de-
scribed in section 4.
2.3 Using hubs for WSD
Once the hubs that represent the senses of the word
are selected (following any of the methods pre-
sented in the last section), each of them is linked
to the target word with edges weighting 0, and
the Minimum Spanning Tree (MST) of the whole
graph is calculated and stored.
5As G is undirected, the in-degree of a vertex v is equal
to its out-degree.
The MST is then used to perform word sense
disambiguation, in the following way. For every
instance of the target word, the words surrounding
it are examined and looked up in the MST. By con-
struction of the MST, words in it are placed under
exactly one hub. Each word in the context receives
a set of scores s, with one score per hub, where all
scores are 0 except the one corresponding to the
hub where it is placed. If the scores are organized
in a score vector, all values are 0, except, say, the
i-th component, which receives a score d(hi, v),
which is the distance between the hub hi and the
node representing the word v. Thus, d(hi, v) as-
signs a score of 1 to hubs and the score decreases
as the nodes move away from the hub in the tree.
For a given occurrence of the target word, the
score vectors of all the words in the context are
added, and the hub that receives the maximum
score is chosen.
3 Evaluating unsupervised WSD systems
All unsupervised WSD algorithms need some ad-
dition in order to be evaluated. One alternative, as
in (Ve?ronis, 2004), is to manually decide the cor-
rectness of the hubs assigned to each occurrence
of the words. This approach has two main disad-
vantages. First, it is expensive to manually verify
each occurrence of the word, and different runs of
the algorithm need to be evaluated in turn. Sec-
ond, it is not an easy task to manually decide if
an occurrence of a word effectively corresponds
with the use of the word the assigned hub refers
to, specially considering that the person is given a
short list of words linked to the hub. Besides, it is
widely acknowledged that people are leaned not to
contradict the proposed answer.
A second alternative is to evaluate the system
according to some performance in an application,
e.g. information retrieval (Schu?tze, 1998). This is
a very attractive idea, but requires expensive sys-
tem development and it is sometimes difficult to
separate the reasons for the good (or bad) perfor-
mance.
A third alternative would be to devise a method
to map the hubs (clusters) returned by the system
to the senses in a lexicon. Pantel and Lin (2002)
automatically mapped the senses to WordNet, and
then measured the quality of the mapping. More
recently, tagged corpora have been used to map
the induced senses, and then compare the sys-
tems over publicly available benchmarks (Puran-
587
dare and Pedersen, 2004; Niu et al, 2005; Agirre
et al, 2006), which offers the advantage of com-
paring to other systems, but converts the whole
system into semi-supervised. See Section 5 for
more details on these systems. Note that the map-
ping introduces noise and information loss, which
is a disadvantage when comparing to other sys-
tems that rely on the gold-standard senses.
Yet another possibility is to evaluate the induced
senses against a gold standard as a clustering task.
Induced senses are clusters, gold standard senses
are classes, and measures from the clustering lit-
erature like entropy or purity can be used. In this
case the manually tagged corpus is taken to be the
gold standard, where a class is the set of examples
tagged with a sense.
We decided to adopt the last two alternatives,
since they allow for comparison over publicly
available systems of any kind.
3.1 Evaluation of clustering: hubs as clusters
In this setting the selected hubs are treated as
clusters of examples and gold standard senses are
classes. In order to compare the clusters with the
classes, hand annotated corpora are needed (for in-
stance Senseval). The test set is first tagged with
the induced senses. A perfect clustering solution
will be the one where each cluster has exactly the
same examples as one of the classes, and vice
versa. The evaluation is completely unsupervised.
Following standard cluster evaluation prac-
tice (Zhao and Karypis, 2005), we consider three
measures: entropy, purity and Fscore. The entropy
measure considers how the various classes of ob-
jects are distributed within each cluster. In gen-
eral, the smaller the entropy value, the better the
clustering algorithm performs. The purity mea-
sure considers the extent to which each cluster
contained objects from primarily one class. The
larger the values of purity, the better the cluster-
ing algorithm performs. The Fscore is used in a
similar fashion to Information Retrieval exercises,
with precision and recall defined as the percent-
age of correctly ?retrieved? examples for a clus-
ter (divided by total cluster size), and recall as the
percentage of correctly ?retrieved? examples for a
cluster (divided by total class size). For a formal
definition refer to (Zhao and Karypis, 2005). If the
clustering is identical to the original classes in the
datasets, FScore will be equal to one which means
that the higher the FScore, the better the clustering
is.
3.2 Evaluation as supervised WSD: mapping
hubs to senses
(Agirre et al, 2006) presents a straightforward
framework that uses hand-tagged material in or-
der to map the induced senses into the senses used
in a gold standard . The WSD system first tags the
training part of some hand-annotated corpus with
the induced hubs. The hand labels are then used
to construct a matrix relating assigned hubs to ex-
isting senses, simply counting the times an occur-
rence with sense sj has been assigned hub hi. In
the testing step we apply the WSD algorithm over
the test corpus, using the hubs-to-senses matrix to
select the sense with highest weights. See (Agirre
et al, 2006) for further details.
4 Tuning the parameters
The behavior of the original HyperLex algorithm
was influenced by a set of heuristic parameters,
which were set by Ve?ronis following his intuition.
In (Agirre et al, 2006) we tuned the parameters us-
ing the mapping strategy for evaluation. We set a
range for each of the parameters, and evaluated the
algorithm for each combination of the parameters
on a fixed set of words (S2LS), which was differ-
ent from the final test sets (S3LS and S3AW). This
ensures that the chosen parameter set can be used
for any noun, and is not overfitted to a small set of
nouns.
In this paper, we perform the parameter tuning
according to four different criteria, i.e., best su-
pervised performance and best unsupervised en-
tropy/purity/FScore performance. At the end, we
have four sets of parameters (those that obtained
the best results in S2LS for each criterion), and
each set is then selected to be run against the S3LS
and S3AW datasets.
The parameters of the graph-based algorithm
can be divided in two sets: those that affect how
the cooccurrence graph is built (p1?p4 below), and
those that control the way the hubs are extracted
from it (p5?p8 below).
p1 Minimum frequency of edges (occurrences)
p2 Minimum frequency of vertices (words)
p3 Edges with weights above this value are removed
p4 Context containing fewer words are not processed
p5 Minimum number of adjacent vertices in a hub
p6 Max. mean weight of the adjacent vertices of a hub
p7 Minimum frequency of hubs
p8 Number of selected hubs
588
Vr opt Pr fr (p7) and Pr fx (p8)
Vr Range Best Range Best
p1 5 1-3 1 1-3 2
p2 10 2-4 3 2-4 3
p3 .9 .3-.7 .4 .4-.5 .5
p4 4 4 4 4 4
p5 6 1-7 1 ? ?
p6 .8 .6-.95 .95 ? ?
p7 .001 .0009-.003 .001 .0015-.0025 .0016
p8 ? ? ? 50-65 55
Table 1: Parameters of the HyperLex algorithm
Both strategies to select hubs from the coocur-
rence graph (cf. Sect. 2.2) share parameters p1?
p4. The algorithm proposed by Ve?ronis uses p5?
p6 as requirements for hubs, and p7 as the thresh-
old to stop looking for more hubs: candidates with
frequency below p7 are not eligible to be hubs.
Regarding PageRank the original formulation
does not have any provision for determining which
are hubs and which not, it just returns a weighted
list of vertices. We have experimented with two
methods: a threshold for the frequency of the hubs
(as before, p7), and a fixed number of hubs for ev-
ery target word (p8). For a shorthand we use Vr for
Veronis? original formulation with default param-
eters, Vr opt for optimized parameters, and Pr fr
and Pr fx respectively for the two ways of using
PageRank.
Table 1 lists the parameters of the HyperLex al-
gorithm, with the default values proposed for them
in the original work (second column), the ranges
that we explored, and the optimal values according
to the supervised recall evaluation (cf. Sect. 3.1).
For Vr opt we tried 6700 combinations. PageRank
has less parameters, and we also used the previous
optimization of Vr opt to limit the range of p4, so
Pr fr and Pr fx get respectively 180 and 288 com-
binations.
5 Experiment setting and results
To evaluate the HyperLex algorithm in a standard
benchmark, we will first focus on a more exten-
sive evaluation of S3LS and then see the results
in S3AW (cf. Sec. 5.4). Following the design
for evaluation explained in Section 3, we use the
standard train-test split for the supervised evalua-
tion, while the unsupervised evaluation only uses
the test part.
Table 2 shows the results of the 4 variants of
our algorithm. Vr stands for the original Vero-
nis algorithm with default parameters, Vr opt to
our optimized version, and Pr fr and Pr fx to the
Sup. Unsupervised
Rec. Entr. Pur. FS
Vr 59.9 50.3 58.2 44.1
Vr opt 64.6 18.3 78.5 35.0
Pr fr 64.5 18.7 77.2 34.3
Pr fx 62.2 25.4 72.2 33.3
1ex-1hub 40.1 0.0 100.0 14.5
MFS 54.5 53.2 52.8 28.3
S3LS-best 72.9 19.9 67.3 63.8
kNN-all 70.6 21.2 64.0 60.6
kNN-BoW 63.5 22.6 61.1 57.1
Cymfony (10%-S3LS) 57.9 25.0 55.7 52.0
Prob0 (MFS-S3) 54.2 28.8 49.3 46.0
clr04 (MFS-Sc) 48.8 25.8 52.5 46.2
Ciaosenso (MFS-Sc) 48.7 28.0 50.3 48.8
duluth-senserelate 47.5 27.2 51.1 44.9
Table 2: Results for the nouns in S3LS using the 4 meth-
ods (Vr, Vr opt, Pr fr and Pr fx). Each of the methods was
optimized in S2LS using the 4 evaluation criteria (Supervised
recall, Entropy, Purity and Fscore) and evaluated on S3LS ac-
cording to the respective evaluation criteria (in the columns).
Two baselines, plus 3 supervised and 5 unsupervised systems
are also shown. Bold is used for best results in each category.
two variants of PageRank. In the columns we find
the evaluation results according to our 4 criteria.
For supervised evaluation we indicate only recall,
which in our case equals precision, as the cover-
age is 100% in all cases (values returned by the
official Senseval scorer). We also include 2 base-
lines, a system returning a single cluster (that of
the most frequent sense, MFS), and another re-
turning one cluster for each example (1ex-1hub).
The last rows list the results for 3 supervised and
5 unsupervised systems (see Sect. 5.1). We will
comment on the result of this table from different
perspectives.
5.1 Supervised evaluation
In this subsection we will focus in the first four
evaluation rows in Table 2. All variants of the al-
gorithm outperform by an ample margin the MFS
and the 1ex-1hub baselines when evaluated on
S3LS recall. This means that the method is able
to learn useful hubs. Note that we perform this su-
pervised evaluation just for comparison with other
systems, and to prove that we are able to provide
high performance WSD.
The default parameter setting (Vr) gets the
worst results, followed by the fixed-hub imple-
mentation of PageRank (Pr fx). Pagerank with
frequency threshold (Pr fr) and the optimized
Veronis (Vr opt) obtain a 10 point improvement
over the MFS baseline with very similar results
(the difference is not statistically significant ac-
cording to McNemar?s test at 95% confidence
589
level).
Table 2 also shows the results of three super-
vised systems. These results (and those of the
other unsupervised systems in the table) where ob-
tained from the Senseval website, and the only
processing we did was to filter nouns. S3LS-best
stands for the the winner of S3LS (Mihalcea et al,
2004), which is 8.3 points over our method. We
also include the results of two of our in-house sys-
tems. kNN-all is a state-of-the-art system (Agirre
et al, 2005) using wide range of local and top-
ical features, and only 2.3 points below the best
S3LS system. kNN-BoW which is the same super-
vised system, but restricted to bag-of-words fea-
tures only, which are the ones used by our graph-
based systems. The table shows that Vr opt and
Pr fr are one single point from kNN-BoW, which
is an impressive result if we take into account the
information loss of the mapping step and that we
tuned our parameters on a different set of words.
The last 5 rows of Table 2 show several un-
supervised systems, all of which except Cym-
fony (Niu et al, 2005) and (Purandare and Ped-
ersen, 2004) participated in S3LS (check (Mihal-
cea et al, 2004) for further details on the systems).
We classify them according to the amount of ?su-
pervision? they have: some have access to most-
frequent information (MFS-S3 if counted over
S3LS, MFS-Sc if counted over SemCor), some use
10% of the S3LS training part for mapping (10%-
S3LS). Only one system (Duluth) did not use in
any way hand-tagged corpora.
The table shows that Vr opt and Pr fr are more
than 6 points above the other unsupervised sys-
tems, but given the different typology of unsuper-
vised systems, it?s unfair to draw definitive con-
clusions from a raw comparison of results. The
system coming closer to ours is that described in
(Niu et al, 2005). They use hand tagged corpora
which does not need to include the target word to
tune the parameters of a rather complex clustering
method which does use local features. They do use
the S3LS training corpus for mapping. For every
sense of the target word, three of its contexts in
the train corpus are gathered (around 10% of the
training data) and tagged. Each cluster is then re-
lated with its most frequent sense. The mapping
method is similar to ours, but we use all the avail-
able training data and allow for different hubs to
be assigned to the same sense.
Another system similar to ours is (Purandare
and Pedersen, 2004), which unfortunately was
evaluated on Senseval 2 data and is not included
in the table. The authors use first and second or-
der bag-of-word context features to represent each
instance of the corpus. They apply several cluster-
ing algorithms based on the vector space model,
limiting the number of clusters to 7. They also
use all available training data for mapping, but
given their small number of clusters they opt for a
one-to-one mapping which maximizes the assign-
ment and discards the less frequent clusters. They
also discard some difficult cases, like senses and
words with low frequencies (10% of total occur-
rences and 90, respectively). The different test set
and mapping system make the comparison diffi-
cult, but the fact that the best of their combina-
tions beats MFS by 1 point on average (47.6% vs.
46.4%) for the selected nouns and senses make us
think that our results are more robust (nearly 10%
over MFS).
5.2 Clustering evaluation
The three columns corresponding to fully unsu-
pervised evaluation in Table 2 show that all our
3 optimized variants easily outperform the MFS
baseline. The best results are in this case for the
optimized Veronis, followed closely by Pagerank
with frequency threshold.
The comparison with the supervised and unsu-
pervised systems shows that our system gets better
entropy and purity values, but worse FScore. This
can be explained by the bias of entropy and purity
towards smaller and more numerous clusters. In
fact the 1ex-1hub baseline obtains the best entropy
and purity scores. Our graph-based system tends
to induce a large number of senses (with averages
of 60 to 70 senses). On the other hand FScore pe-
nalizes the systems inducing a different number of
clusters. As the supervised and unsupervised sys-
tems were designed to return the same (or similar)
number of senses as in the gold standard, they at-
tain higher FScores. This motivated us to compare
the results of the best parameters across evaluation
methods.
5.3 Comparison across evaluation methods
Table 3 shows all 16 evaluation possibilities for
each variant of the algorithm, depending of the
evaluation criteria used in S2LS (in the rows)
and the evaluation criteria used in S3LS (in the
columns). This table shows that the best results (in
bold for each variant) tend to be in the diagonal,
590
that is, when the same evaluation criterion is used
for optimization and test, but it is not decisive. If
we take the first row (supervised evaluation) as the
most credible criterion, we can see that optimiz-
ing according to entropy and purity get similar and
sometimes better result (Pr fr and Pr fx). On the
contrary the Fscore yields worse results by far.
This indicates that a purely unsupervised sys-
tem evaluated according to the gold standard
(based on entropy or purity) yields optimal param-
eters similar to the supervised (mapped) version.
This is an important result, as it shows that the
quality in performance does not come from the
mapping step, but from the algorithm and opti-
mal parameter setting. The table shows that op-
timization on purity and entropy criteria do corre-
late with good performance in the supervised eval-
uation.
The failure of FScore based optimization, in our
opinion, indicates that our clustering algorithm
prefers smaller and more numerous clusters, com-
pared to the gold standard. FScore prefers cluster-
ing solutions that have a similar number of clusters
to that of the gold standard, but it is unable to drive
the optimization or our algorithm towards good re-
sults in the supervised evaluation.
All in all, the best results are attained with
smaller and more numerous hubs, a kind of micro-
senses. This effect is the same for all three vari-
ants tried and all evaluation criteria, with Fscore
yielding less clusters. At first we were uncom-
fortable with this behavior, so we checked whether
HyperLex was degenerating into a trivial solution.
This was the main reason to include the 1ex-1hub
baseline, which simulates a clustering algorithm
returning one hub per example, and its precision
was 40.1, well below the MFS baseline. We also
realized that our results are in accordance with
some theories of word meaning, e.g. the ?indef-
initely large set of prototypes-within-prototypes?
envisioned in (Cruse, 2000). Ted Pedersen has
also observed a similar behaviour in his vector-
space model clustering experiments (PC). We now
think that the idea of having many micro-senses
is attractive for further exploration, specially if we
are able to organize them into coarser hubs in fu-
ture work.
5.4 S3AW task
In the Senseval-3 all-words task (Snyder and
Palmer, 2004) all words in three document ex-
Sup. Unsupervised
Alg. Opt. Rec. Entr. Pur. FS
Vr Sup 64.6 18.4 77.9 30.0
Ent 64.6 18.3 78.3 29.1
Pur 63.7 19.0 78.5 30.8
Fsc 60.4 38.2 63.5 35.0
Pr fr Sup 64.5 20.8 76.1 28.6
Ent 64.6 18.7 77.7 27.2
Pur 64.7 19.3 77.2 27.6
Fsc 61.2 36.0 65.2 34.3
Pr fx Sup 62.2 28.2 69.3 29.5
Ent 63.1 25.4 72.2 28.4
Pur 63.1 25.4 72.2 28.4
Fsc 54.5 32.9 66.5 33.3
Table 3: Cross-evaluation comparison. In the rows the eval-
uation method for optmizing over S2LS is shown, and in the
columns the result over S3LS according to the different eval-
uation methods.
recall
kuaw 70.9
Pr fr 70.7
Vr opt 70.1
GAMBL 70.1
MFS 69.9
LCCaw 68.6
Table 4: Results for the nouns in S3AW, compared to the
most frequent baseline and the top three supervised systems
cerpts need to be disambiguated. Given the
scarce amount of training data available in Sem-
cor (Miller et al, 1993), supervised systems barely
improve upon the simple most frequent heuris-
tic. In this setting the unsupervised evaluation
schemes are not feasible, as many of the target
words occur only once, so we used the map-
ping strategy with Semcor to produce the required
WordNet senses in the output.
Table 4 shows the results for our systems with
the best parameters according to the supervised
criterion on S2LS, plus the top three S3AW super-
vised systems and the most frequent sense heuris-
tic. In order to focus the comparison, we only kept
noun occurrences of all systems and filtered out
multiwords, target words with two different lem-
mas and unknown tags, leaving a total of 857 oc-
currences of nouns. We can see that Pr fr is only
0.2 from the S3AW winning system, demonstrat-
ing that our unsupervised graph-based systems
that use Semcor for mapping are nearly equivalent
to the most powerful supervised systems to date.
In fact, the differences in performance for the sys-
tems are not statistically significant (McNemar?s
test at 95% significance level).
591
6 Conclusions and further work
This paper has explored the use of two graph algo-
rithms for corpus-based disambiguation of nomi-
nal senses. We have shown that the parameter op-
timization learnt over a small set of nouns signifi-
cantly improves the performance for all nouns, and
produces a system which (1) in a lexical-sample
setting (Senseval 3 dataset) is 10 points over the
Most-Frequent-Sense baseline, 1 point over a su-
pervised system using the same kind of informa-
tion (i.e. bag-of-words features), and 8 points be-
low the best supervised system, and (2) in the all-
words setting is a` la par the best supervised sys-
tem. The performance of PageRank is statistically
the same as that of HyperLex, with the advantage
of PageRank of using less parameters.
In order to compete on the same test set as su-
pervised systems, we do use hand-tagged data, but
only to do the mapping from the induced senses
into the gold standard senses. In fact, we believe
that using our WSD system as a purely unsuper-
vised system (i.e. returning just hubs), the per-
fomance would be higher, as we would avoid the
information loss in the mapping. We would like
to test this on Information Retrieval, perhaps on a
setting similar to that of (Schu?tze, 1998), which
would allow for an indirect evaluation of the qual-
ity and a comparison with supervised WSD system
on the same grounds.
We have also shown that the optimization ac-
cording to purity and entropy values (which does
not need the supervised mapping step) yields very
good parameters, comparable to those obtained in
the supervised optimization strategy. This indi-
cates that we are able to optimize the algorithm
in a completely unsupervised fashion for a small
number of words, and then carry over to tag new
text with the induced senses.
Regarding efficiency, our implementation of
HyperLex is extremely fast. Trying the 6700 com-
binations of parameters takes 5 hours in a 2 AMD
Opteron processors at 2GHz and 3Gb RAM. A
single run (building the MST, mapping and tag-
ging the test sentences) takes only 16 sec. For this
reason, even if an on-line version would be in prin-
ciple desirable, we think that this batch version is
readily usable as a standalone word sense disam-
biguation system.
Both graph-based methods and vector-based
clustering methods rely on local information, typ-
ically obtained by the occurrences of neighbor
words in context. The advantage of graph-
based techniques over over vector-based cluster-
ing might come from the fact that the former are
able to measure the relative importance of a vertex
in the whole graph, and thus combine both local
and global cooccurrence information.
For the future, we would like to look more
closely the micro-senses induced by HyperLex,
and see if we can group them into coarser clus-
ters. We would also like to integrate different
kinds of information, specially the local or syn-
tactic features so successfully used by supervised
systems, but also more heterogeneous information
from knowledge bases.
Graph models have been very successful in
some settings (e.g. the PageRank algorithm of
Google), and have been rediscovered recently
for natural language tasks like knowledge-based
WSD, textual entailment, summarization and de-
pendency parsing. Now that we have set a ro-
bust optimization and evaluation framework we
would like to test other such algorithms (e.g.
HITS (Kleinberg, 1999)) in the same conditions.
Acknowledgements
Oier Lopez de Lacalle enjoys a PhD grant from the
Basque Government. We thank the comments of
the three anonymous reviewers.
References
E. Agirre, O. Lopez de Lacalle, and D. Martinez. 2005.
Exploring feature spaces with svd and unlabeled
data for word sense disambiguation. In Proc. of
RANLP.
E. Agirre, O. Lopez de Lacalle, D. Martinez, and
A. Soroa. 2006. Evaluating and optimizing the pa-
rameters of an unsupervised graph-based wsd algo-
rithm. In Proc. of the NAACL Texgraphs workshop.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1-7).
D. A. Cruse, 2000. Polysemy: Theoretical and Compu-
tational Approaches, chapter Aspects of the Micro-
structure of Word Meanings, pages 31?51. OUP.
G Erkan and D. R. Radev. 2004. Lexrank: Graph-
based centrality as salience in text summarization.
Journal of Artificial Intelligence Research (JAIR).
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
592
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604?632.
R. Mihalcea and P Tarau. 2004. Textrank: Bringing
order into texts. In Proc. of EMNLP2004.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The senseval-3 english lexical sample task. In
R. Mihalcea and P. Edmonds, editors, Senseval-3
proccedings, pages 25?28. ACL, July.
R. Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proc. of
EMNLP2005.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker.
1993. A semantic concordance. In Proc. of the
ARPA HLT workshop.
R. Navigli and P. Velardi. 2005. Structural seman-
tic interconnections: a knowledge-based approach
to word sense disambiguation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
7(27):1063?1074, June.
C. Niu, W. Li, R. K. Srihari, and H. Li. 2005. Word in-
dependent context pair classification model for word
sense disambiguation. In Proc. of CoNLL-2005.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proc. of KDD02.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proc. of CoNLL-2004, pages 41?
48.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Proc. of SENSEVAL.
J. Ve?ronis. 2004. Hyperlex: lexical cartography for in-
formation retrieval. Computer Speech & Language,
18(3):223?252.
Y Zhao and G Karypis. 2005. Hierarchical clustering
algorithms for document datasets. Data Mining and
Knowledge Discovery, 10(2):141?168.
593
Workshop on TextGraphs, at HLT-NAACL 2006, pages 89?96,
New York City, June 2006. c?2006 Association for Computational Linguistics
Evaluating and optimizing the parameters
of an unsupervised graph-based WSD algorithm
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle and Aitor Soroa
IXA NLP Group
University of Basque Country
Donostia, Basque Contry
a.soroa@ehu.es
Abstract
Ve?ronis (2004) has recently proposed
an innovative unsupervised algorithm for
word sense disambiguation based on
small-world graphs called HyperLex. This
paper explores two sides of the algorithm.
First, we extend Ve?ronis? work by opti-
mizing the free parameters (on a set of
words which is different to the target set).
Second, given that the empirical compar-
ison among unsupervised systems (and
with respect to supervised systems) is sel-
dom made, we used hand-tagged corpora
to map the induced senses to a standard
lexicon (WordNet) and a publicly avail-
able gold standard (Senseval 3 English
Lexical Sample). Our results for nouns
show that thanks to the optimization of
parameters and the mapping method, Hy-
perLex obtains results close to supervised
systems using the same kind of bag-of-
words features. Given the information
loss inherent in any mapping step and the
fact that the parameters were tuned for an-
other set of words, these are very interest-
ing results.
1 Introduction
Word sense disambiguation (WSD) is a key en-
abling technology. Supervised WSD techniques are
the best performing in public evaluations, but need
large amounts of hand-tagging data. Existing hand-
annotated corpora like SemCor (Miller et al, 1993),
which is annotated with WordNet senses (Fellbaum,
1998) allow for a small improvement over the simple
most frequent sense heuristic, as attested in the all-
words track of the last Senseval competition (Sny-
der and Palmer, 2004). In theory, larger amounts
of training data (SemCor has approx. 500M words)
would improve the performance of supervised WSD,
but no current project exists to provide such an ex-
pensive resource.
Supervised WSD is based on the ?fixed-list of
senses? paradigm, where the senses for a target word
are a closed list coming from a dictionary or lex-
icon. Lexicographers and semanticists have long
warned about the problems of such an approach,
where senses are listed separately as discrete enti-
ties, and have argued in favor of more complex rep-
resentations, where, for instance, senses are dense
regions in a continuum (Cruse, 2000).
Unsupervised WSD has followed this line of
thinking, and tries to induce word senses directly
from the corpus. Typical unsupervised WSD sys-
tems involve clustering techniques, which group to-
gether similar examples. Given a set of induced
clusters (which represent word uses or senses1),
each new occurrence of the target word will be com-
pared to the clusters and the most similar cluster will
be selected as its sense.
Most of the unsupervised WSD work has been
based on the vector space model (Schu?tze, 1998;
Pantel and Lin, 2002; Purandare and Pedersen,
2004), where each example is represented by a vec-
tor of features (e.g. the words occurring in the
context). Recently, Ve?ronis (Ve?ronis, 2004) has
1Unsupervised WSD approaches prefer the term ?word uses?
to ?word senses?. In this paper we use them interchangeably to
refer to both the induced clusters, and to the word senses from
some reference lexicon.
89
proposed HyperLex, an application of graph mod-
els to WSD based on the small-world properties
of cooccurrence graphs. Hand inspection of the
clusters (called hubs in this setting) by the author
was very positive, with hubs capturing the main
senses of the words. Besides, hand inspection of the
disambiguated occurrences yielded precisions over
95% (compared to a most frequent baseline of 73%)
which is an outstanding figure for WSD systems.
We noticed that HyperLex had some free param-
eters and had not been evaluated against a public
gold standard. Besides, we were struck by the few
works where supervised and unsupervised systems
were evaluated on the same test data. In this pa-
per we use an automatic method to map the induced
senses to WordNet using hand-tagged corpora, en-
abling the automatic evaluation against available
gold standards (Senseval 3 English Lexical Sam-
ple S3LS (Mihalcea et al, 2004)) and the automatic
optimization of the free parameters of the method.
The use of hand-tagged corpora for tagging makes
this algorithm a mixture of unsupervised and super-
vised: the method to induce senses in completely
unsupervised, but the mapping is supervised (albeit
very straightforward).
This paper is structured as follows. We first
present the graph-based algorithm as proposed by
Ve?ronis, reviewing briefly the features of small-
world graphs. Section 3 presents our framework for
mapping and evaluating the induced hubs. Section 4
introduces parameter optimization. Section 5 shows
the experiment setting and results. Section 6 ana-
lyzes the results and presents related work. Finally,
we draw the conclusions and advance future work.
2 HyperLex
Before presenting the HyperLex algorithm itself, we
briefly introduce small-world graphs.
2.1 Small world graphs
The small-world nature of a graph can be explained
in terms of its clustering coefficient and characteris-
tic path length. The clustering coefficient of a graph
shows the extent to which nodes tend to form con-
nected groups that have many edges connecting each
other in the group, and few edges leading out of
the group. On the other side, the characteristic path
length represents ?closeness? in a graph. See (Watts
and Strogatz, 1998) for further details on these char-
acteristics.
Randomly built graphs exhibit low clustering co-
efficients and are believed to represent something
very close to the minimal possible average path
length, at least in expectation. Perfectly ordered
graphs, on the other side, show high clustering coef-
ficients but also high average path length. According
to Watts and Strogatz (1998), small-world graphs lie
between these two extremes: they exhibit high clus-
tering coefficients, but short average path lengths.
Barabasi and Albert (1999) use the term ?scale-
free? to graphs whose degree probability follow a
power-law2. Specifically, scale free graphs follow
the property that the probability P (k) that a vertex
in the graph interacts with k other vertices decays as
a power-law, following P (k) ? k??. It turns out
that in this kind of graphs there exist nodes centrally
located and highly connected, called hubs.
2.2 The HyperLex algorithm for WSD
The HyperLex algorithm builds a cooccurrence
graph for all pairs of words cooccurring in the con-
text of the target word. Ve?ronis shows that this kind
of graph fulfills the properties of small world graphs,
and thus possess highly connected components in
the graph. The centers or prototypes of these com-
ponents, called hubs, eventually identify the main
word uses (senses) of the target word.
We will briefly introduce the algorithm here,
check (Ve?ronis, 2004) for further details. For each
word to be disambiguated, a text corpus is collected,
consisting of the paragraphs where the word occurs.
From this corpus, a cooccurrence graph for the tar-
get word is built. Nodes in the graph correspond to
the words3 in the text (except the target word itself).
Two words appearing in the same paragraph are said
to cooccur, and are connected with edges. Each edge
is assigned with a weight which measures the rela-
tive frequency of the two words cooccurring. Specif-
ically, let wij be the weight of the edge4 connecting
2Although scale-free graphs are not necessarily small
worlds, a lot of real world networks are both scale-free and
small worlds.
3Following Ve?ronis, we only work on nouns for the time
being.
4Note that the cooccurrence graph is undirected, i.e. wij =
wji
90
nodes i and j, then
wij = 1? max[P (i | j), P (j | i)]
P (i | j) =
freqij
freqj
and P (j | i) =
freqij
freqi
The weight of an edge measures how tightly con-
nected the two words are. Words which always oc-
cur together receive a weight of 0. Words rarely
cooccurring receive weights close to 1.
Once the cooccurrence graph is built, a simple it-
erative algorithm is executed to obtain its hubs. At
each step, the algorithm finds the vertex with high-
est relative frequency5 in the graph, and, if it meets
some criteria, it is selected as a hub. These criteria
are determined by a set of heuristic parameters, that
will be explained later in Section 4. After a vertex is
selected to be a hub, its neighbors are no longer eli-
gible as hub candidates. At any time, if the next ver-
tex candidate has a relative frequency below a cer-
tain threshold, the algorithm stops.
Once the hubs are selected, each of them is linked
to the target word with edges weighting 0, and the
Minimum Spanning Tree (MST) of the whole graph
is calculated and stored.
The MST is then used to perform word sense dis-
ambiguation, in the following way. For every in-
stance of the target word, the words surrounding it
are examined and confronted with the MST. By con-
struction of the MST, words in it are placed under
exactly one hub. Each word in the context receives
a set of scores s, with one score per hub, where all
scores are 0 except the one corresponding to the hub
where it is placed. If the scores are organized in a
score vector, all values are 0, except, say, the i-th
component, which receives a score d(hi, v), which
is the distance between the hub hi and the node rep-
resenting the word v. Thus, d(hi, v) assigns a score
of 1 to hubs and the score decreases as the nodes
move away from the hub in the tree.
For a given occurrence of the target word, the
score vectors of all the words in the context are
added, and the hub that receives the maximum score
is chosen.
5In cooccurrence graphs, the relative frequency of a vertex
and its degree are linearly related, and it is therefore possible to
avoid the costly computation of the degree.
Base
corpus
hyperLex_wsd hyperLex_wsd
hyperLex
Evaluator
Tagged
corpus
Test
corpus
Mapping
corpus
MST
matrix
Mapping
Figure 1: Design for the automatic mapping and evaluation
of HyperLex algorithm against a gold standard (test corpora).
3 Evaluating unsupervised WSD systems
All unsupervised WSD algorithms need some addi-
tion in order to be evaluated. One alternative, as in
(Ve?ronis, 2004), is to manually decide the correct-
ness of the hubs assigned to each occurrence of the
words. This approach has two main disadvantages.
First, it is expensive to manually verify each occur-
rence of the word, and different runs of the algo-
rithm need to be evaluated in turn. Second, it is not
an easy task to manually decide if an occurrence of
a word effectively corresponds with the use of the
word the assigned hub refers to, especially consid-
ering that the person is given a short list of words
linked to the hub. We also think that instead of judg-
ing whether the hub returned by the algorithm is cor-
rect, the person should have independently tagged
the occurrence with hubs, which should have been
then compared to the hub returned by the system.
A second alternative is to evaluate the system ac-
cording to some performance in an application, e.g.
information retrieval (Schu?tze, 1998). This is a very
attractive idea, but requires expensive system devel-
opment and it is sometimes difficult to separate the
reasons for the good (or bad) performance.
A third alternative would be to devise a method
to map the hubs (clusters) returned by the system
to the senses in a lexicon. Pantel and Lin (2002)
automatically map the senses to WordNet, and then
measure the quality of the mapping. More recently,
the mapping has been used to test the system on
publicly available benchmarks (Purandare and Ped-
91
Default p180 p1800 p6700
value Range Best Range Best Range Best
p1 5 2-3 2 1-3 2 1-3 1
p2 10 3-4 3 2-4 3 2-4 3
p3 0.9 0.7-0.9 0.7 0.5-0.7 0.5 0.3-0.7 0.4
p4 4 4 4 4 4 4 4
p5 6 6-7 6 3-7 3 1-7 1
p6 0.8 0.5-0.8 0.6 0.4-0.8 0.7 0.6-0.95 0.95
p7 0.001 0.0005-0.001 0.0009 0.0005-0.001 0.0009 0.0009-0.003 0.001
Table 1: Parameters of the HyperLex algorithm
ersen, 2004; Niu et al, 2005). See Section 6 for
more details on these systems.
Yet another possibility is to evaluate the induced
senses against a gold standard as a clustering task.
Induced senses are clusters, gold standard senses are
classes, and measures from the clustering literature
like entropy or purity can be used. As we wanted to
focus on the comparison against a standard data-set,
we decided to leave aside this otherwise interesting
option.
In this section we present a framework for au-
tomatically evaluating unsupervised WSD systems
against publicly available hand-tagged corpora. The
framework uses three data sets, called Base corpus,
Mapping corpus and Test corpus:
? The Base Corpus: a collection of examples of
the target word. The corpus is not annotated.
? The Mapping Corpus: a collection of examples
of the target word, where each corpus has been
manually annotated with its sense.
? The Test Corpus: a separate collection, also an-
notated with senses.
The evaluation framework is depicted in Figure 1.
The first step is to execute the HyperLex algorithm
over the Base corpus in order to obtain the hubs of
a target word, and the generated MST is stored. As
stated before, the Base Corpus is not tagged, so the
building of the MST is completely unsupervised.
In a second step (left part in Figure 1), we assign a
hub score vector to each of the occurrences of target
word in the Mapping corpus, using the MST calcu-
lated in the previous step (following the WSD al-
gorithm in Section 2.2). Using the hand-annotated
sense information, we can compute a mapping ma-
trix M that relates hubs and senses in the following
way. Suppose there are m hubs and n senses for the
target word. Then, M = {mij} 1 ? i ? m, 1 ?
j ? n, and each mij = P (sj |hi), that is, mij is the
probability of a word having sense j given that it has
been assigned hub i. This probability can be com-
puted counting the times an occurrence with sense
sj has been assigned hub hi.
This mapping matrix will be used to transform
any hub score vector h? = (h1, . . . , hm) returned
by the WSD algorithm into a sense score vector
s? = (s1, . . . , sn). It suffices to multiply the score
vector by M , i.e., s? = h?M .
In the last step (right part in Figure 1), we apply
the WSD algorithm over the Test corpus, using again
the MST generated in the first step, and returning a
hub score vector for each occurrence of the target
word in the test corpus. We then run the Evaluator,
which uses the M mapping matrix in order to con-
vert the hub score vector into a sense score vector.
The Evaluator then compares the sense with high-
est weight in the sense score vector to the sense that
was manually assigned, and outputs the precision
figures.
Preliminary experiments showed that, similar to
other unsupervised systems, HyperLex performs
better if it sees the test examples when building the
graph. We therefore decided to include a copy of the
training and test corpora in the base corpus (discard-
ing all hand-tagged sense information, of course).
Given the high efficiency of the algorithm this poses
no practical problem (see efficiency figures in Sec-
tion 6).
4 Tuning the parameters
As stated before, the behavior of the HyperLex algo-
rithm is influenced by a set of heuristic parameters,
that affect the way the cooccurrence graph is built,
the number of induced hubs, and the way they are
extracted from the graph. There are 7 parameters in
total:
p1 Minimum frequency of edges (occurrences)
p2 Minimum frequency of vertices (words)
p3 Edges with weights above this value are removed
p4 Context containing fewer words are not processed
92
word train test MFS default p180 p1800 p6700
argument 221 111 51.4 51.4 51.4 51.4 51.4
arm 266 133 82.0 82.0 80.5 82.0 82.7
atmosphere 161 81 66.7 67.9 70.4 70.4 67.9
audience 200 100 67.0 69.0 71.0 74.0 77.0
bank 262 132 67.4 69.7 75.0 76.5 75.0
degree 256 128 60.9 60.9 60.9 62.5 63.3
difference 226 114 40.4 40.4 41.2 46.5 49.1
difficulty 46 23 17.4 30.4 30.4 39.1 26.1
disc 200 100 38.0 66.0 75.0 70.0 76.0
image 146 74 36.5 63.5 62.2 67.6 64.9
interest 185 93 41.9 49.5 41.9 47.3 51.6
judgment 62 32 28.1 28.1 28.1 53.1 50.0
organization 112 56 73.2 73.2 73.2 71.4 73.2
paper 232 117 25.6 42.7 39.3 47.9 53.8
party 230 116 62.1 67.2 64.7 65.5 67.2
performance 172 87 32.2 44.8 46.0 54.0 59.8
plan 166 84 82.1 81.0 79.8 81.0 83.3
shelter 196 98 44.9 45.9 49.0 48.0 54.1
sort 190 96 65.6 64.6 64.6 65.6 64.6
source 64 32 65.6 59.4 56.2 62.5 62.5
Average: 54.5 59.9 60.3 63.0 64.6
(Over S2LS) 51.9 56.2 57.5 58.7 60.0
Table 2: Precision figures for nouns over the test corpus (S3LS). The second and third columns show the number of occurrences
in the train and test splits. The MFS column corresponds to the most frequent sense. The rest of columns correspond to different
parameter settings: default for the default setting, p180 for the best combination over 180, etc.. The last rows show the micro-
average over the S3LS run, and we also add the results on the S2LS dataset (different sets of nouns) to confirm that the same trends
hold in both datasets.
p5 Minimum number of adjacent vertices a hub must have
p6 Max. mean weight of the adjacent vertices of a hub
p7 Minimum frequency of hubs
Table 1 lists the parameters of the HyperLex al-
gorithm, and the default values proposed for them in
the original work (second column).
Given that we have devised a method to efficiently
evaluate the performance of HyperLex, we are able
to tune the parameters against the gold standard. We
first set a range for each of the parameters, and eval-
uated the algorithm for each combination of the pa-
rameters on a collection of examples of different
words (Senseval 2 English lexical-sample, S2LS).
This ensures that the chosen parameter set is valid
for any noun, and is not overfitted to a small set of
nouns.6 The set of parameters that obtained the best
results in the S2LS run is then selected to be run
against the S3LS dataset.
We first devised ranges for parameters amounting
to 180 possible combinations (p180 column in Ta-
ble 2), and then extended the ranges to amount to
1800 and 6700 combinations (columns p1800 and
p6700).
6In fact, previous experiments showed that optimizing the
parameters for each word did not yield better results.
5 Experiment setting and results
To evaluate the HyperLex algorithm in a standard
benchmark, we applied it to the 20 nouns in S3LS.
We use the standard training-test split. Following
the design in Section 3, we used both the training
and test sets as the Base Corpus (ignoring the sense
tags, of course). The Mapping Corpus comprised
the training split only, and the Test corpus the test
split only. The parameter tuning was done in a simi-
lar fashion, but on the S2LS dataset.
In Table 2 we can see the number of examples
of each word in the different corpus and the results
of the algorithm. We indicate only precision, as the
coverage is 100% in all cases. The left column,
named MFS, shows the precision when always as-
signing the most frequent sense (relative to the train
split). This is the baseline of our algorithm as our
algorithm does see the tags in the mapping step (see
Section 6 for further comments on this issue).
The default column shows the results for the Hy-
perLex algorithm with the default parameters as set
by Ve?ronis, except for the minimum frequency of
the vertices (p2 in Table 1), which according to some
preliminary experiments we set to 3. As we can see,
the algorithm with the default settings outperforms
93
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
 0.61
 0.62
 0.5  0.55  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1
Pr
ec
is
io
n
Similarity
Parameter space
Best fitting line
Figure 2: Dispersion plot of the parameter space for 6700
combinations. The horizontal axis shows the similarity of a pa-
rameter set w.r.t. the best parameter set using the cosine. The
vertical axis shows the precision in S2LS. The best fitting line
is also depicted.
the MFS baseline by 5.4 points average, and in al-
most all words (except plan, sort and source).
The results for the best of 180 combinations of the
parameters improve the default setting (0.4 overall),
Extending the parameter space to 1800 and 6700 im-
proves the precision up to 63.0 and 64.6, 10.1 over
the MFS (MFS only outperforms HyperLex in the
best setting for two words). The same trend can be
seen on the S2LS dataset, where the gain was more
modest (note that the parameters were optimized for
S2LS).
6 Discussion and related work
We first comment the results, doing some analysis,
and then compare our results to those of Ve?ronis. Fi-
nally we overview some relevant work and review
the results of unsupervised systems on the S3LS
benchmark.
6.1 Comments on the results
The results show clearly that our exploration of the
parameter space was successful, with the widest pa-
rameter space showing the best results.
In order to analyze whether the search in the pa-
rameter space was making any sense, we drew a dis-
persion plot (see Figure 2). In the top right-hand cor-
ner we have the point corresponding to the best per-
forming parameter set. If the parameters were not
conditioning the good results, then we would have
expected a random cloud of points. On the contrary,
we can see that there is a clear tendency for those
default p180 p1800 p6700
hubs defined 9.2 ?3.8 15.3 ?5.7 38.6 ?11.8 77.7?18.7
used 8.4 ?3.5 14.4 ?5.3 30.4 ?9.3 45.2?13.3
senses defined 5.4 ?1.5 5.4 ?1.5 5.4 ?1.5 5.4 ?1.5
used 2.6 ?1.2 2.5 ?1 3.1 ?1.1 3.2?1.2
senses in test 5.1 ?1.3 - - -
Table 3: Average number of hubs and senses (along with the
standard deviation) for three parameter settings. Defined means
the number of hubs induced, and used means the ones actually
returned by HyperLex when disambiguating the test set. The
same applies for senses, that is, defined means total number of
senses (equal for all columns), and used means the senses that
were actually used by HyperLex in the test set. The last row
shows the actual number of senses used by the hand-annotators
in the test set.
parameter sets most similar to the best one to obtain
better results, and in fact the best fitting line shows a
clearly ascending slope.
Regarding efficiency, our implementation of Hy-
perLex is extremely fast. Doing the 1800 combina-
tions takes 2 hours in a 2 AMD Opteron processors
at 2GHz and 3Gb RAM. A single run (building the
MST, mapping and tagging the test sentences) takes
only 16 sec. For this reason, even if an on-line ver-
sion would be in principle desirable, we think that
this batch version is readily usable.
6.2 Comparison to (Ve?ronis, 2004)
Compared to Ve?ronis we are inducing larger num-
bers of hubs (with different parameters), using less
examples to build the graphs and obtaining more
modest results (far from the 90?s). Regarding the lat-
ter, our results are in the range of other S3LS WSD
systems (see below), and the discrepancy can be ex-
plained by the way Ve?ronis performed his evaluation
(see Section 3).
Table 3 shows the average number of hubs for
the four parameter settings. The average number
of hubs for the default setting is larger than that of
Ve?ronis (which ranges between 4 and 9 per word),
but quite close to the average number of senses. The
exploration of the parameter space prefers parame-
ter settings with even larger number of hubs, and the
figures shows that most of them are actually used
for disambiguation. The table also shows that, after
the mapping, less than half of the senses are actu-
ally used, which seems to indicate that the mapping
tends to favor the most frequent senses.
Regarding the actual values of the parameters
used (c.f. Table 1), we had to reduce the value
94
of some parameters (e.g. the minimum frequency
of vertices) due to the smaller number of of exam-
ples (Ve?ronis used from 1900 to 8700 examples per
word). In theory, we could explore larger parame-
ter spaces, but Table 1 shoes that the best setting for
the 6700 combinations has no parameter in a range
boundary (except p5, which cannot be further re-
duced).
All in all, the best results are attained with smaller
and more numerous hubs, a kind of micro-senses.
A possible explanation for this discrepancy with
Ve?ronis could be that he was inspecting by hand
the hubs that he got, and perhaps was biased by the
fact that he wanted the hubs to look more like stan-
dard senses. At first we were uncomfortable with
this behavior, so we checked whether HyperLex was
degenerating into a trivial solution. We simulated
a clustering algorithm returning one hub per exam-
ple, and its precision was 40.1, well below the MFS
baseline. We also realized that our results are in
accordance with some theories of word meaning,
e.g. the ?indefinitely large set of prototypes-within-
prototypes? envisioned in (Cruse, 2000). We now
think that the idea of having many micro-senses is
very attractive for further exploration, especially if
we are able to organize them into coarser hubs.
6.3 Comparison to related work
Table 4 shows the performance of different systems
on the nouns of the S3LS benchmark. When not re-
ported separately, we obtained the results for nouns
running the official scorer program on the filtered
results, as available in the S3LS web page. The sec-
ond column shows the type of system (supervised,
unsupervised).
We include three supervised systems, the winner
of S3LS (Mihalcea et al, 2004), an in-house system
(kNN-all, CITATION OMITTED) which uses opti-
mized kNN, and the same in-house system restricted
to bag-of-words features only (kNN-bow), i.e. dis-
carding other local features like bigrams or trigrams
(which is what most unsupervised systems do). The
table shows that we are one point from the bag-of-
words classifier kNN-bow, which is an impressive
result if we take into account the information loss of
the mapping step and that we tuned our parameters
on a different set of words. The full kNN system is
state-of-the-art, only 4 points below the S3LS win-
System Type Prec. Cov.
S3LS-best Sup. 74.9 0.99
kNN-all Sup. 70.3 1.0
kNN-bow Sup. 65.7 1.0
HyperLex Unsup(S3LS) 64.6 1.0
Cymfony Unsup(10%-S3LS) 57.9 1.0
Prob0 Unsup. (MFS-S3) 55.0 0.98
MFS - 51.5 1.0
Ciaosenso Unsup (MFS-Sc) 53.95 0.90
clr04 Unsup (MFS-Sc) 48.86 1.0
duluth-senserelate Unsup 47.48 1.0
(Purandare and
Pedersen, 2004)
Unsup (S2LS) - -
Table 4: Comparison of HyperLex and MFS baseline to S3LS
systems for nouns. The last system was evaluated on S2LS.
ner.
Table 4 also shows several unsupervised systems,
all of which except Cymfony and (Purandare and
Pedersen, 2004) participated in S3LS (check (Mi-
halcea et al, 2004) for further details on the sys-
tems). We classify them according to the amount of
?supervision? they have: some have have access to
most-frequent information (MFS-S3 if counted over
S3LS, MFS-Sc if counted over SemCor), some use
10% of the S3LS training part for mapping (10%-
S3LS), and some use the full amount of S3LS train-
ing for mapping (S3LS). Only one system (Duluth)
did not use in any way hand-tagged corpora.
Given the different typology of unsupervised sys-
tems, it?s unfair to draw definitive conclusions from
a raw comparison of results. The system coming
closer to ours is that described in (Niu et al, 2005).
They use hand tagged corpora which does not need
to include the target word to tune the parameters of
a rather complex clustering method which does use
local information (an exception to the rule of unsu-
pervised systems). They do use the S3LS training
corpus for mapping. For every sense the target word,
three of its contexts in the train corpus are gathered
(around 10% of the training data) and tagged. Each
cluster is then related with its most frequent sense.
Only one cluster may be related to a specific sense,
so if two or more clusters map to the same sense,
only the largest of them is retained. The mapping
method is similar to ours, but we use all the avail-
able training data and allow for different hubs to be
assigned to the same sense.
Another system similar to ours is (Purandare and
Pedersen, 2004), which unfortunately was evaluated
on Senseval 2 data. The authors use first and second
95
order bag-of-word context features to represent each
instance of the corpus. They apply several clustering
algorithms based on the vector space model, limiting
the number of clusters to 7. They also use all avail-
able training data for mapping, but given their small
number of clusters they opt for a one-to-one map-
ping which maximizes the assignment and discards
the less frequent clusters. They also discard some
difficult cases, like senses and words with low fre-
quencies (10% of total occurrences and 90, respec-
tively). The different test set and mapping system
make the comparison difficult, but the fact that the
best of their combinations beats MFS by 1 point on
average (47.6% vs. 46.4%) for the selected nouns
and senses make us think that our results are more
robust (nearly 10% over MFS).
7 Conclusions and further work
This paper has explored two sides of HyperLex: the
optimization of the free parameters, and the empir-
ical comparison on a standard benchmark against
other WSD systems. We use hand-tagged corpora
to map the induced senses to WordNet senses.
Regarding the optimization of parameters, we
used a another testbed (S2LS) comprising different
words to select the best parameter. We consistently
improve the results of the parameters by Ve?ronis,
which is not perhaps so surprising, but the method
allows to fine-tune the parameters automatically to a
given corpus given a small test set.
Comparing unsupervised systems against super-
vised systems is seldom done. Our results indicate
that HyperLex with the supervised mapping is on
par with a state-of-the-art system which uses bag-
of-words features only. Given the information loss
inherent to any mapping, this is an impressive re-
sult. The comparison to other unsupervised systems
is difficult, as each one uses a different mapping
strategy and a different amount of supervision.
For the future, we would like to look more closely
the micro-senses induced by HyperLex, and see if
we can group them into coarser clusters. We also
plan to apply the parameters to the Senseval 3 all-
words task, which seems well fit for HyperLex: the
best supervised system only outperforms MFS by
a few points in this setting, and the training cor-
pora used (Semcor) is not related to the test corpora
(mainly Wall Street Journal texts).
Graph models have been very successful in some
settings (e.g. the PageRank algorithm of Google),
and have been rediscovered recently for natural lan-
guage tasks like knowledge-based WSD, textual en-
tailment, summarization and dependency parsing.
We would like to test other such algorithms in the
same conditions, and explore their potential to inte-
grate different kinds of information, especially the
local or syntactic features so successfully used by
supervised systems, but also more heterogeneous in-
formation from knowledge bases.
References
A. L. Barabasi and R. Albert. 1999. Emergence of scal-
ing in random networks. Science, 286(5439):509?512,
October.
D. A. Cruse, 2000. Polysemy: Theoretical and Com-
putational Approaches, chapter Aspects of the Micro-
structure of Word Meanings. OUP.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004. The
senseval-3 english lexical sample task. In R. Mihal-
cea and P. Edmonds, editors, Senseval-3 proceedings,
pages 25?28. ACL, July.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A semantic concordance. In Proc. of the ARPA HLT
workshop.
C. Niu, W. Li, R. K. Srihari, and H. Li. 2005. Word
independent context pair classification model for word
sense disambiguation. In Proc. of CoNLL-2005.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proc. of KDD02.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and simi-
larity spaces. In Proc. of CoNLL-2004.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Proc. of SENSEVAL.
J. Ve?ronis. 2004. HyperLex: lexical cartography for in-
formation retrieval. Computer Speech & Language,
18(3):223?252.
D. J. Watts and S. H. Strogatz. 1998. Collec-
tive dynamics of ?small-world? networks. Nature,
393(6684):440?442, June.
96
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 1?6,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 01: Evaluating WSD
on Cross-Language Information Retrieval
Eneko Agirre
IXA NLP group
University of the Basque Country
Donostia, Basque Counntry
e.agirre@ehu.es
Oier Lopez de Lacalle
IXA NLP group
University of the Basque Country
Donostia, Basque Country
jibloleo@ehu.es
German Rigau
IXA NLP group
University of the Basque Country
Donostia, Basque Country
german.rigau@ehu.es
Bernardo Magnini
ITC-IRST
Trento, Italy
magnini@itc.it
Arantxa Otegi
IXA NLP group
University of the Basque Country
Donostia, Basque Country
jibotusa@ehu.es
Piek Vossen
Irion Technologies
Delftechpark 26
2628XH Delft, Netherlands
Piek.Vossen@irion.nl
Abstract
This paper presents a first attempt of an
application-driven evaluation exercise of
WSD. We used a CLIR testbed from the
Cross Lingual Evaluation Forum. The ex-
pansion, indexing and retrieval strategies
where fixed by the organizers. The par-
ticipants had to return both the topics and
documents tagged with WordNet 1.6 word
senses. The organization provided training
data in the form of a pre-processed Semcor
which could be readily used by participants.
The task had two participants, and the orga-
nizer also provide an in-house WSD system
for comparison.
1 Introduction
Since the start of Senseval, the evaluation of Word
Sense Disambiguation (WSD) as a separate task is a
mature field, with both lexical-sample and all-words
tasks. In the first case the participants need to tag the
occurrences of a few words, for which hand-tagged
data has already been provided. In the all-words task
all the occurrences of open-class words occurring in
two or three documents (a few thousand words) need
to be disambiguated.
The community has long mentioned the neces-
sity of evaluating WSD in an application, in order
to check which WSD strategy is best, and more im-
portant, to try to show that WSD can make a differ-
ence in applications. The use of WSD in Machine
Translation has been the subject of some recent pa-
pers, but less attention has been paid to Information
Retrieval (IR).
With this proposal we want to make a first try to
define a task where WSD is evaluated with respect
to an Information Retrieval and Cross-Lingual Infor-
mation Retrieval (CLIR) exercise. From the WSD
perspective, this task will evaluate all-words WSD
systems indirectly on a real task. From the CLIR
perspective, this task will evaluate which WSD sys-
tems and strategies work best.
We are conscious that the number of possible con-
figurations for such an exercise is very large (in-
cluding sense inventory choice, using word sense in-
duction instead of disambiguation, query expansion,
WSD strategies, IR strategies, etc.), so this first edi-
tion focuses on the following:
? The IR/CLIR system is fixed.
? The expansion / translation strategy is fixed.
? The participants can choose the best WSD
strategy.
1
? The IR system is used as the upperbound for
the CLIR systems.
We think that it is important to start doing this
kind of application-driven evaluations, which might
shed light to the intricacies in the interaction be-
tween WSD and IR strategies. We see this as the
first of a series of exercises, and one outcome of this
task should be that both WSD and CLIR communi-
ties discuss together future evaluation possibilities.
This task has been organized in collabora-
tion with the Cross-Language Evaluation Forum
(CLEF1). The results will be analyzed in the CLEF-
2007 workshop, and a special track will be pro-
posed for CLEF-2008, where CLIR systems will
have the opportunity to use the annotated data
produced as a result of the Semeval-2007 task.
The task has a webpage with all the details at
http://ixa2.si.ehu.es/semeval-clir.
This paper is organized as follows. Section 2
describes the task with all the details regarding
datasets, expansion/translation, the IR/CLIR system
used, and steps for participation. Section 3 presents
the evaluation performed and the results obtained by
the participants. Finally, Section 4 draws the con-
clusions and mention the future work.
2 Description of the task
This is an application-driven task, where the appli-
cation is a fixed CLIR system. Participants disam-
biguate text by assigning WordNet 1.6 synsets and
the system will do the expansion to other languages,
index the expanded documents and run the retrieval
for all the languages in batch. The retrieval results
are taken as the measure for fitness of the disam-
biguation. The modules and rules for the expansion
and the retrieval will be exactly the same for all par-
ticipants.
We proposed two specific subtasks:
1. Participants disambiguate the corpus, the cor-
pus is expanded to synonyms/translations and
we measure the effects on IR/CLIR. Topics2 are
not processed.
1http://www.clef-campaign.org
2In IR topics are the short texts which are used by the sys-
tems to produce the queries. They usually provide extensive
information about the text to be searched, which can be used
both by the search engine and the human evaluators.
2. Participants disambiguate the topics per lan-
guage, we expand the queries to syn-
onyms/translations and we measure the effects
on IR/CLIR. Documents are not processed
The corpora and topics were obtained from the
ad-hoc CLEF tasks. The supported languages in the
topics are English and Spanish, but in order to limit
the scope of the exercise we decided to only use En-
glish documents. The participants only had to dis-
ambiguate the English topics and documents. Note
that most WSD systems only run on English text.
Due to these limitations, we had the following
evaluation settings:
IR with WSD of topics , where the participants
disambiguate the documents, the disam-
biguated documents are expanded to syn-
onyms, and the original topics are used for
querying. All documents and topics are in En-
glish.
IR with WSD of documents , where the partic-
ipants disambiguate the topics, the disam-
biguated topics are expanded and used for
querying the original documents. All docu-
ments and topics are in English.
CLIR with WSD of documents , where the partic-
ipants disambiguate the documents, the dis-
ambiguated documents are translated, and the
original topics in Spanish are used for query-
ing. The documents are in English and the top-
ics are in Spanish.
We decided to focus on CLIR for evaluation,
given the difficulty of improving IR. The IR results
are given as illustration, and as an upperbound of
the CLIR task. This use of IR results as a reference
for CLIR systems is customary in the CLIR commu-
nity (Harman, 2005).
2.1 Datasets
The English CLEF data from years 2000-2005 com-
prises corpora from ?Los Angeles Times? (year
1994) and ?Glasgow Herald? (year 1995) amounting
to 169,477 documents (579 MB of raw text, 4.8GB
in the XML format provided to participants, see Sec-
tion 2.3) and 300 topics in English and Spanish (the
topics are human translations of each other). The
relevance judgments were taken from CLEF. This
2
might have the disadvantage of having been pro-
duced by pooling the results of CLEF participants,
and might bias the results towards systems not using
WSD, specially for monolingual English retrieval.
We are considering the realization of a post-hoc
analysis of the participants results in order to ana-
lyze the effect on the lack of pooling.
Due to the size of the document collection, we de-
cided that the limited time available in the competi-
tion was too short to disambiguate the whole collec-
tion. We thus chose to take a sixth part of the corpus
at random, comprising 29,375 documents (874MB
in the XML format distributed to participants). Not
all topics had relevant documents in this 17% sam-
ple, and therefore only 201 topics were effectively
used for evaluation. All in all, we reused 21,797
relevance judgements that contained one of the doc-
uments in the 17% sample, from which 923 are pos-
itive3. For the future we would like to use the whole
collection.
2.2 Expansion and translation
For expansion and translation we used the publicly
available Multilingual Central Repository (MCR)
from the MEANING project (Atserias et al, 2004).
The MCR follows the EuroWordNet design, and
currently includes English, Spanish, Italian, Basque
and Catalan wordnets tightly connected through the
Interlingual Index (based on WordNet 1.6, but linked
to all other WordNet versions).
We only expanded (translated) the senses returned
by the WSD systems. That is, given a word like
?car?, it will be expanded to ?automobile? or ?railcar?
(and translated to ?auto? or ?vago?n? respectively) de-
pending on the sense in WN 1.6. If the systems re-
turns more than one sense, we choose the sense with
maximum weight. In case of ties, we expand (trans-
late) all. The participants could thus implicitly affect
the expansion results, for instance, when no sense
could be selected for a target noun, the participants
could either return nothing (or NOSENSE, which
would be equivalent), or all senses with 0 score. In
the first case no expansion would be performed, in
the second all senses would be expanded, which is
equivalent to full expansion. This fact will be men-
tioned again in Section 3.5.
3The overall figures are 125,556 relevance judgements for
the 300 topics, from which 5700 are positive
Note that in all cases we never delete any of the
words in the original text.
In addition to the expansion strategy used with the
participants, we tested other expansion strategies as
baselines:
noexp no expansion, original text
fullexp expansion (translation in the case of English
to Spanish expansion) to all synonyms of all
senses
wsd50 expansion to the best 50% senses as returned
by the WSD system. This expansion was tried
over the in-house WSD system of the organizer
only.
2.3 IR/CLIR system
The retrieval engine is an adaptation of the Twenty-
One search system (Hiemstra and Kraaij, 1998) that
was developed during the 90?s by the TNO research
institute at Delft (The Netherlands) getting good re-
sults on IR and CLIR exercises in TREC (Harman,
2005). It is now further developed by Irion technolo-
gies as a cross-lingual retrieval system (Vossen et al,
). For indexing, the TwentyOne system takes Noun
Phrases as an input. Noun Phases (NPs) are detected
using a chunker and a word form with POS lexicon.
Phrases outside the NPs are not indexed, as well as
non-content words (determiners, prepositions, etc.)
within the phrase.
The Irion TwentyOne system uses a two-stage re-
trieval process where relevant documents are first
extracted using a vector space matching and sec-
ondly phrases are matched with specific queries.
Likewise, the system is optimized for high-precision
phrase retrieval with short queries (1 up 5 words
with a phrasal structure as well). The system can be
stripped down to a basic vector space retrieval sys-
tem with an tf.idf metrics that returns documents for
topics up to a length of 30 words. The stripped-down
version was used for this task to make the retrieval
results compatible with the TREC/CLEF system.
The Irion system was also used for pre-
processing. The CLEF corpus and topics were con-
verted to the TwentyOne XML format, normalized,
and named-entities and phrasal structured detected.
Each of the target tokens was identified by an unique
identifier.
2.4 Participation
The participants were provided with the following:
3
1. the document collection in Irion XML format
2. the topics in Irion XML format
In addition, the organizers also provided some of
the widely used WSD features in a word-to-word
fashion4 (Agirre et al, 2006) in order to make partic-
ipation easier. These features were available for both
topics and documents as well as for all the words
with frequency above 10 in SemCor 1.6 (which can
be taken as the training data for supervised WSD
systems). The Semcor data is publicly available 5.
For the rest of the data, participants had to sign and
end user agreement.
The participants had to return the input files en-
riched with WordNet 1.6 sense tags in the required
XML format:
1. for all the documents in the collection
2. for all the topics
Scripts to produce the desired output from word-
to-word files and the input files were provided by
organizers, as well as DTD?s and software to check
that the results were conformant to the respective
DTD?s.
3 Evaluation and results
For each of the settings presented in Section 2 we
present the results of the participants, as well as
those of an in-house system presented by the orga-
nizers. Please refer to the system description papers
for a more complete description. We also provide
some baselines and alternative expansion (transla-
tion) strategies. All systems are evaluated accord-
ing to their Mean Average Precision 6 (MAP) as
computed by the trec eval software on the pre-
existing CLEF relevance-assessments.
3.1 Participants
The two systems that registered sent the results on
time.
PUTOP They extend on McCarthy?s predominant
sense method to create an unsupervised method
of word sense disambiguation that uses auto-
matically derived topics using Latent Dirichlet
4Each target word gets a file with all the occurrences, and
each occurrence gets the occurrence identifier, the sense tag (if
in training), and the list of features that apply to the occurrence.
5http://ixa2.si.ehu.es/semeval-clir/
6http://en.wikipedia.org/wiki/
Information retrieval
Allocation. Using topic-specific synset similar-
ity measures, they create predictions for each
word in each document using only word fre-
quency information. The disambiguation pro-
cess took aprox. 12 hours on a cluster of 48 ma-
chines (dual Xeons with 4GB of RAM). Note
that contrary to the specifications, this team
returned WordNet 2.1 senses, so we had to
map automatically to 1.6 senses (Daude et al,
2000).
UNIBA This team uses a a knowledge-based WSD
system that attempts to disambiguate all words
in a text by exploiting WordNet relations. The
main assumption is that a specific strategy for
each Part-Of-Speech (POS) is better than a sin-
gle strategy. Nouns are disambiguated basi-
cally using hypernymy links. Verbs are dis-
ambiguated according to the nouns surrounding
them, and adjectives and adverbs use glosses.
ORGANIZERS In addition to the regular partic-
ipants, and out of the competition, the orga-
nizers run a regular supervised WSD system
trained on Semcor. The system is based on
a single k-NN classifier using the features de-
scribed in (Agirre et al, 2006) and made avail-
able at the task website (cf. Section 2.4).
In addition to those we also present some com-
mon IR/CLIR baselines, baseline WSD systems, and
an alternative expansion:
noexp a non-expansion IR/CLIR baseline of the
documents or topics.
fullexp a full-expansion IR/CLIR baseline of the
documents or topics.
wsdrand a WSD baseline system which chooses a
sense at random. The usual expansion is ap-
plied.
1st a WSD baseline system which returns the sense
numbered as 1 in WordNet. The usual expan-
sion is applied.
wsd50 the organizer?s WSD system, where the 50%
senses of the word ranking according to the
WSD system are expanded. That is, instead of
expanding the single best sense, it expands the
best 50% senses.
3.2 IR Results
This section present the results obtained by the par-
ticipants and baselines in the two IR settings. The
4
IRtops IRdocs CLIR
no expansion 0.3599 0.3599 0.1446
full expansion 0.1610 0.1410 0.2676
UNIBA 0.3030 0.1521 0.1373
PUTOP 0.3036 0.1482 0.1734
wsdrand 0.2673 0.1482 0.2617
1st sense 0.2862 0.1172 0.2637
ORGANIZERS 0.2886 0.1587 0.2664
wsd50 0.2651 0.1479 0.2640
Table 1: Retrieval results given as MAP. IRtops
stands for English IR with topic expansion. IR-
docs stands for English IR with document expan-
sion. CLIR stands for CLIR results for translated
documents.
second and third columns of Table 1 present the re-
sults when disambiguating the topics and the docu-
ments respectively. Non of the expansion techniques
improves over the baseline (no expansion).
Note that due to the limitation of the search en-
gine, long queries were truncated at 50 words, which
might explain the very low results of the full expan-
sion.
3.3 CLIR results
The last column of Table 1 shows the CLIR results
when expanding (translating) the disambiguated
documents. None of the WSD systems attains the
performance of full expansion, which would be the
baseline CLIR system, but the WSD of the organizer
gets close.
3.4 WSD results
In addition to the IR and CLIR results we also pro-
vide the WSD performance of the participants on
the Senseval 2 and 3 all-words task. The documents
from those tasks were included alongside the CLEF
documents, in the same formats, so they are treated
as any other document. In order to evaluate, we had
to map automatically all WSD results to the respec-
tive WordNet version (using the mappings in (Daude
et al, 2000) which are publicly available).
The results are presented in Table 2, where we can
see that the best results are attained by the organizers
WSD system.
3.5 Discussion
First of all, we would like to mention that the WSD
and expansion strategy, which is very simplistic, de-
grades the IR performance. This was rather ex-
Senseval-2 all words
precision recall coverage
ORGANIZERS 0.584 0.577 93.61%
UNIBA 0.498 0.375 75.39%
PUTOP 0.388 0.240 61.92%
Senseval-3 all words
precision recall coverage
ORGANIZERS 0.591 0.566 95.76%
UNIBA 0.484 0.338 69.98%
PUTOP 0.334 0.186 55.68%
Table 2: English WSD results in the Senseval-2 and
Senseval-3 all-words datasets.
pected, as the IR experiments had an illustration
goal, and are used for comparison with the CLIR
experiments. In monolingual IR, expanding the top-
ics is much less harmful than expanding the docu-
ments. Unfortunately the limitation to 50 words in
the queries might have limited the expansion of the
topics, which make the results rather unreliable. We
plan to fix this for future evaluations.
Regarding CLIR results, even if none of the WSD
systems were able to beat the full-expansion base-
line, the organizers system was very close, which is
quite encouraging due to the very simplistic expan-
sion, indexing and retrieval strategies used.
In order to better interpret the results, Table 3
shows the amount of words after the expansion in
each case. This data is very important in order to un-
derstand the behavior of each of the systems. Note
that UNIBA returns 3 synsets at most, and therefore
the wsd50 strategy (select the 50% senses with best
score) leaves a single synset, which is the same as
taking the single best system (wsdbest). Regarding
PUTOP, this system returned a single synset, and
therefore the wsd50 figures are the same as the ws-
dbest figures.
Comparing the amount of words for the two par-
ticipant systems, we see that UNIBA has the least
words, closely followed by PUTOP. The organizers
WSD system gets far more expanded words. The
explanation is that when the synsets returned by a
WSD system all have 0 weights, the wsdbest expan-
sion strategy expands them all. This was not explicit
in the rules for participation, and might have affected
the results.
A cross analysis of the result tables and the num-
ber of words is interesting. For instance, in the IR
exercise, when we expand documents, the results in
5
English Spanish
No WSD noexp 9,900,818 9,900,818fullexp 93,551,450 58,491,767
UNIBA
wsdbest 19,436,374 17,226,104
wsd50 19,436,374 17,226,104
PUTOP wsdbest 20,101,627 16,591,485wsd50 20,101,627 16,591,485
Baseline 1st 24,842,800 20,261,081
WSD wsdrand 24,904,717 19,137,981
ORG. wsdbest 26,403,913 21,086,649wsd50 36,128,121 27,528,723
Table 3: Number of words in the document col-
lection after expansion for the WSD system and all
baselines. wsdbest stands for the expansion strategy
used with participants.
the third column of Table 1 show that the ranking for
the non-informed baselines is the following: best for
no expansion, second for random WSD, and third
for full expansion. These results can be explained
because of the amount of expansion: the more ex-
pansion the worst results. When more informed
WSD is performed, documents with more expansion
can get better results, and in fact the WSD system of
the organizers is the second best result from all sys-
tem and baselines, and has more words than the rest
(with exception of wsd50 and full expansion). Still,
the no expansion baseline is far from the WSD re-
sults.
Regarding the CLIR result, the situation is in-
verted, with the best results for the most productive
expansions (full expansion, random WSD and no ex-
pansion, in this order). For the more informed WSD
methods, the best results are again for the organizers
WSD system, which is very close to the full expan-
sion baseline. Even if wsd50 has more expanded
words wsdbest is more effective. Note the very high
results attained by random. These high results can
be explained by the fact that many senses get the
same translation, and thus for many words with few
translation, the random translation might be valid.
Still the wsdbest, 1st sense and wsd50 results get
better results.
4 Conclusions and future work
This paper presents the results of a preliminary at-
tempt of an application-driven evaluation exercise
of WSD in CLIR. The expansion, indexing and re-
trieval strategies proved too simplistic, and none of
the two participant systems and the organizers sys-
tem were able to beat the full-expansion baseline.
Due to efficiency reasons, the IRION system had
some of its features turned off. Still the results are
encouraging, as the organizers system was able to
get very close to the full expansion strategy with
much less expansion (translation).
For the future, a special track of CLEF-2008 will
leave the avenue open for more sophisticated CLIR
techniques. We plan to extend the WSD annotation
to all words in the CLEF English document collec-
tion, and we also plan to contact the best performing
systems of the SemEval all-words tasks to have bet-
ter quality annotations.
Acknowledgements
We wish to thank CLEF for allowing us to use their data, and the
CLEF coordinator, Carol Peters, for her help and collaboration.
This work has been partially funded by the Spanish education
ministry (project KNOW)
References
E. Agirre, O. Lopez de Lacalle, and D. Martinez. 2006.
Exploring feature set combinations for WSD. In Proc.
of the SEPLN.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The MEANING
Multilingual Central Repository. In Proceedings of the
2.nd Global WordNet Conference, GWC 2004, pages
23?30. Masaryk University, Brno, Czech Republic.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping Word-
Nets Using Structural Information. In Proc. of ACL,
Hong Kong.
D. Harman. 2005. Beyond English. In E. M. Voorhees
and D. Harman, editors, TREC: Experiment and Eval-
uation in Information Retrieval, pages 153?181. MIT
press.
D. Hiemstra and W. Kraaij. 1998. Twenty-One in ad-hoc
and CLIR. In E.M. Voorhees and D. K. Harman, ed-
itors, Proc. of TREC-7, pages 500?540. NIST Special
Publication.
P. Vossen, G. Rigau, I. Alegria, E. Agirre, D. Farwell,
and M. Fuentes. Meaningful results for Information
Retrieval in the MEANING project. In Proc. of the
3rd Global Wordnet Conference.
6
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 342?345,
Prague, June 2007. c?2007 Association for Computational Linguistics
UBC-ALM: Combining k-NN with SVD for WSD
Eneko Agirre and Oier Lopez de Lacalle
IXA NLP Group
University of the Basque Country
Donostia, Basque Country
{e.agirre,jibloleo}@ehu.es
Abstract
This work describes the University of the
Basque Country system (UBC-ALM) for
lexical sample and all-words WSD subtasks
of SemEval-2007 task 17, where it per-
formed in the second and fifth positions re-
spectively. The system is based on a com-
bination of k-Nearest Neighbor classifiers,
with each classifier learning from a distinct
set of features: local features (syntactic, col-
locations features), topical features (bag-of-
words, domain information) and latent fea-
tures learned from a reduced space using
Singular Value Decomposition.
1 Introduction
Our group (UBC-ALM) participated in the lexical
sample and all-words WSD subtasks of SemEval-
2007 task 17. We applied a combination of different
k-Nearest Neighbor (k-NN) classifiers. Each clas-
sifier manages different information sources (fea-
tures), making the combination a powerful solution.
This algorithm was previously tested on the datasets
from previous editions of Senseval (Agirre et al,
2005; Agirre et al, 2006). Before submission, the
performance of the system was tested on the Se-
mEval lexical sample training data. For learning we
use a rich set of features, including latent features
obtained from a reduced space using Singular Value
Decomposition (SVD).
This paper is organized as follows. The learning
features are presented in section 2, and the learning
algorithm and the combinations of single k-NNs are
given in section 3. Section 4 focuses on the tuning
experiments. Finally, section 5 summarizes the offi-
cial results and some conclusions.
2 Feature set
We relied on an extensive set of features of differ-
ent types, obtained by means of different tools and
resources. We defined two main groups: the origi-
nal features extracted directly from the text, and the
SVD features obtained after applying SVD decom-
position and projecting the original features into the
new semantic space (Agirre et al, 2005).
2.1 Original features
Local collocations: bigrams and trigrams formed
with the words around the target. These features are
constituted by lemmas, word-forms, or PoS tags1.
Other local features are those formed with the previ-
ous/posterior lemma/word-form in the context.
Syntactic dependencies: syntactic dependencies
were extracted using heuristic patterns, and regular
expressions defined with the PoS tags around the tar-
get2. The following relations were used: object, sub-
ject, noun-modifier, preposition, and sibling.
Bag-of-words features: we extract the lemmas
of the content words in the whole context, and in a
?4-word window around the target. We also obtain
salient bigrams in the context, with the methods and
the software described in (Pedersen, 2001).
Domain features: The WordNet Domains re-
source was used to identify the most relevant do-
mains in the context. Following the relevance for-
mula presented in (Magnini and Cavaglia?, 2000), we
defined 2 feature types: (1) the most relevant do-
main, and (2) a list of domains above a predefined
threshold3 .
1The PoS tagging was performed with the fnTBL toolkit
(Ngai and Florian, 2001).
2This software was kindly provided by David Yarowsky?s
group, from Johns Hopkins University.
3The software to obtain the relevant domains was kindly
provided by Gerard Escudero?s group, from Universitat Politec-
342
2.2 SVD features
Singular Value Decomposition (SVD) is an interest-
ing solution to the sparse data problem. This tech-
nique reduces the dimensions of the vectorial space
finding correlations and collapsing features. It also
gives the chance to use unlabeled data as an addi-
tional source of correlations.
M ? Rm?n, a matrix of features-by-document is
built from the training corpus and decomposed into
three matrices, as shown in Eq. (1). U and V , row
and column matrix, respectively, have orthonormal
columns and ? is a diagonal matrix which contains
k eigenvalues in descending order.
M = U?V T =
k=min{m,n}
?
i=1
?iuiviT (1)
We used the singular value matrix (?) and the
column matrix (U ) to create a projection matrix,
which is used to project the data (represented in fea-
tures vectors) from the original space to a reduced
space. Prior to that we selected the first p columns
from the ? and U matrices (p < k): ~tp = ~tTUp??1p
We have explored two different variants in order
to build a matrix, and obtain the SVD features:
SVD One Matrix per Target word (SVD-
OMT). For each word (i) we extracted all the fea-
tures from the given training (test) corpus, (ii) built
the feature-by-document matrix from training cor-
pus, (iii) decomposed it with SVD, and (iv) project
all the training (test) data. Note that this variant has
been only used in the lexical sample task due to its
costly computational requirements.
SVD Single Matrix for All target words (SVD-
SMA): (i) we extracted bag-of-words features from
the British National Corpus (BNC) (Leech, 1992),
(ii) built the feature-by-document matrix, (iii) de-
compose it with SVD, and (iv) project all the data
(train/test).
3 Learning Algorithm
The machine learning (ML) algorithm presented in
this section rely on the previously described fea-
tures. Each occurrence or instance is represented by
the features found in the context (fi). Given an oc-
currence of a word, the ML method below returns a
nica de Catalunya
weight for each sense (weight(sk)). The sense with
maximum weight will be selected.
We use a set of combination of the k-Nearest
Neighbor (k-NN) to tag the target words in both the
lexical sample and all-words tasks.
3.1 k-Nearest Neighbor
k-NN is a memory-based learning method, where
the neighbors are the k most similar contexts, repre-
sented by feature vectors (~ci), of the test vector (~f ).
The similarity among instances is measured by the
cosine of their vectors. The test instance is labeled
with the sense obtaining the maximum sum of the
weighted votes of the k most similar contexts. The
vote is weighted depending on its (neighbor) posi-
tion in the ordered rank, with the closest being first.
Eq. (2) formalizes k-NN, where Ci corresponds to
the sense label of the i-th closest neighbor.
arg max
Sj
=
k
?
i=1
{
1
i if Ci = Sj
0 otherwise (2)
3.2 k-NN combinations and feature splits
As seen in section 2 we use a variety of heteroge-
neous sets of features. Our previous experience has
shown that splitting the problem up into more co-
herent spaces, training different classifiers in each
feature space, and then combining them into a sin-
gle classifier is a good way to improve the results
(Agirre et al, 2005; Agirre et al, 2006). Depend-
ing on the feature type (original features or features
extracted from SVD projection) we split different
sets of feature spaces. In total we tried 10 features
spaces.
For the original features:
? all feats: Extracted all original features.
? all notdom: All original features except do-
main features.
? local: All the original features except domain
and bag-of-words features.
? topic: The sum of bag-of-words and domain
features.
? bow: Bag-of-word features.
? dom: Domain features.
343
Combination accuracy
all feats+topic+local+SVD-OMT[all feats]+SVD-OMT[topic]+SVD-OMT[local] 88.8
all feats+all notdom+topic+local+SVD-SMA+SVD-OMT[all feats]+SVD-OMT[topic]+SVD-OMT[local] 88.7
all feats+topic+local+SVD-SMA+SVD-OMT[all feats]+SVD-OMT[topic]+SVD-OMT[local] 88.5
all notdom+topic+local+SVD-SMA+SVD-OMT[all feats]+SVD-OMT[topic]+SVD-OMT[local] 88.5
all feats+all notdom+topic+local 88.4
all notdom+local+SVD-SMA 88.3
all feats+all notdom+local+SVD-SMA 88.2
all notdom+topic+local 88.1
all feats+topic+local 88.1
word-by-word optimization 89.5
Table 1: Result for the best k-NN combinations in 3 fold cross-validation SemEval lexical sample.
For the SVD features:
? SVD-OMT[all feats]: OMT matrix applied to
all original features.
? SVD-OMT[local]: OMT matrix to the local
original features.
? SVD-OMT[topic]: OMT matrix to the topic
original features.
? SVD-SMA: Features obtained from the projec-
tion of bow features with the SMA matrix.
Depending on the ML method one can try differ-
ent approaches to combine classifiers. In this work,
we exploited the fact that a k-NN classifier can be
seen as k points casting each one vote. The votes
are weigthed by the inverse ratio of its position in
the rank (k ? ri + 1)/k, where ri is the rank. Each
of the k-NN classifiers is trained on a different fea-
ture space and then combined.
4 Experiments on training data
We optimized and tuned the system differently for
each kind of tasks. We will examine each in turn.
4.1 Optimization for the lexical sample task
For the lexical sample task we only use the train-
ing data provided. We tuned the classifiers using 3
fold cross-validation on the SemEval lexical sample
training data. We tried to optimize several param-
eters: number of neighbors, SVD dimensions and
best combination of the single k-NNs. We set k as
one of 1, 3, 5 and 7, and the SVD dimension (d) as
one of 50, 100, 200 and 300. We also fixed the best
combination. This is the optimization procedure we
followed:
1. For each single classifier and feature set (see
section 2), check each parameter combination.
2. Fix the parameters for each single classifier. In
our case, k = 5 and k = 7 had similar results,
so we postponed the decision. d = 200 was the
best dimension for all classifiers, except SVD-
OMT[topic] which was d = 50.
3. For the best parameter settings (k = 5; k = 7
and d = 200; d = 50 when SVD-OMT[topic])
make a priori meaningful combinations (due
to CPU requirements, not all combination were
feasible).
4. Choose the x best combination overall, and op-
timize word by word among these combination.
We set x = 8 for this work, k was fixed in 5,
and d = 200 (except with SVD-OMT[topic]
which was d = 50).
Table 1 shows the best results for 3 fold cross-
validation in SemEval lexical sample training cor-
pus. The figures show that optimizing each word the
performance increases 0.7 percentage points over
the best combination.
4.2 Optimization for the all-words task
To train the classifiers for the all-words task we just
used Semcor (Miller et al, 1993). In (Agirre et
al., 2006) we already tested our approach on the
Senseval-3 all-words task. The best performance
for the Senseval-3 all-words task was obtained with
k = 5 and d = 200, but we decided to to perform
further experiments to search for the best combina-
tion. We tested the performance of the combination
of single k-NN training on Semcor and testing both
on the Senseval-3 all-words data (cf. Table 2) and on
the training data from SemEval-2007 lexical sample
(cf. Table 3).
Note that tables 2 and 3 show contradictory re-
sults. Given that in SemEval-2007 lexical sample
344
Combination rec. prec.
all feats+local+notbow 0.685 0.685
all feats+local+SVD-SMA 0.679 0.679
all feats+topic+local+SVD-SMA 0.689 0.689
Table 2: Results for the best k-NN combinations in
Senseval-3 all-words, using Semcor as training cor-
pus.
Combination rec. prec.
all feats+SVD-SMA 0.666 0.666
all feats+local+SVD-SMA 0.661 0.661
all feats+topic+local+SVD-SMA 0.664 0.664
Table 3: Results for the best k-NN combinations in
training part of SemEval lexical sample, using Sem-
cor as training corpus.
Task Method Rank rec. prec.
LS Best 1 0.887 0.887
LS UBC-ALM 2 0.869 0.869
LS Baseline - 0.780 0.780
AW Best 1 0.591 0.591
AW k-NN combination 5 0.544 0.544
AW Baseline - 0.514 0.514
Table 4: Official results for SemEval-2007 task 17
lexical sample and all-words subtasks.
the senses are more coarse grained, we decided to
take the best combination on Senseval-3 all-words
for the final submission.
5 Results and conclusions
Table 4 shows the performance obtained by our sys-
tem and the winning systems in the SemEval lexi-
cal sample and all-words evaluation. On the lexical
sample evaluation our system is 2.6 lower than the
cross-validation evaluation. This can be a sign of a
slight overfitting on the training data. All in all we
ranked second over 13 systems.
Our all-words system did not perform so well.
Our system is around 4.7 points below the winning
system, ranking 5th from a total of 14, and 3 points
above the baseline given by the organizers. This is
a disappointing result when compared to our previ-
ous work on Senseval-3 all-words where we were
able to beat the best official results (Agirre et al,
2006). Note that the test set was rather small, with
465 occurrences only, which might indicate that the
performance differences are not statistically signifi-
cant. We plan to further investigate the reasons for
our results.
Acknowledgments
We wish to thank to David Mart??nez for helping us
extracting learning features. This work has been
partially funded by the Spanish education ministry
(project KNOW). Oier Lopez de Lacalle is sup-
ported by a PhD grant from the Basque Government.
References
E. Agirre, O.Lopez de Lacalle, and David Mart??nez.
2005. Exploring feature spaces with svd and unlabeled
data for Word Sense Disambiguation. In Proceedings
of the Conference on Recent Advances on Natural Lan-
guage Processing (RANLP?05), Borovets, Bulgaria.
E. Agirre, O. Lopez de Lacalle, and D. Mart??nez. 2006.
Exploring feature spaces with svd and unlabeled data
for Word Sense Disambiguation. In Proceedings
of the XXII Conference of Sociedad Espaola para
el Procesamiento del Lenguaje Natural (SEPLN?06),
Zaragoza, Spain.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
B. Magnini and G. Cavaglia?. 2000. Integrating subject
field codes into WordNet. In Proceedings of the Sec-
ond International LREC Conference, Athens, Greece.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A Semantic Concordance. In Proceedings of the
ARPA Human Language Technology Workshop. Dis-
tributed as Human Language Technology by San Ma-
teo, CA: Morgan Kaufmann Publishers., pages 303?
308, Princeton, NJ.
G. Ngai and R. Florian. 2001. Transformation-Based
Learning in the Fast Lane. Proceedings of the Second
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 40-47,
Pittsburgh, PA, USA.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), Pittsburgh, PA.
345
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 350?353,
Prague, June 2007. c?2007 Association for Computational Linguistics
UBC-UMB: Combining unsupervised and supervised systems for all-words
WSD
David Martinez,Timothy Baldwin
LT Group, CSSE
University of Melbourne
Victoria 3010 Australia
{davidm,tim}@csse.unimelb.edu.au
Eneko Agirre, Oier Lopez de Lacalle
IXA NLP Group
Univ. of the Basque Country
Donostia, Basque Country
{e.agirre,jibloleo}@ehu.es
Abstract
This paper describes the joint submission
of two systems to the all-words WSD sub-
task of SemEval-2007 task 17. The main
goal of this work was to build a competitive
unsupervised system by combining hetero-
geneous algorithms. As a secondary goal,
we explored the integration of unsupervised
predictions into a supervised system by dif-
ferent means.
1 Introduction
This paper describes the joint submission of two sys-
tems to the all-words WSD subtask of SemEval-
2007 task 17. The systems were developed by the
University of the Basque Country (UBC), and the
University of Melbourne (UMB). The main goal of
this work was to build a competitive unsupervised
system by combining heterogeneous algorithms. As
a secondary goal, we explored the integration of
this method into a supervised system by different
means. Thus, this paper describes both the unsu-
pervised system (UBC-UMB-1), and the combined
supervised system (UBC-UMB-2) submitted to the
all-words task.
Our motivation in building unsupervised systems
comes from the difficulty of creating hand-tagged
data for all words and all languages, which is col-
loquially known as the knowledge acquisition bot-
tleneck. There have also been promising results in
recent work on the combination of unsupervised ap-
proaches that suggest the gap with respect to super-
vised systems is narrowing (Brody et al, 2006).
The remainder of the paper is organized as fol-
lows. First we describe the disambiguation algo-
rithms in Section 2. Next, the development exper-
iments are presented in Section 3, and our final sub-
missions and results in Section 4. Finally, we sum-
marize our conclusions in Section 5.
2 Algorithms
In this section, we will describe the standalone algo-
rithms (three unsupervised and one supervised) and
the combination schemes we explored. The unsu-
pervised methods are based on different intuitions
for disambiguation (topical features, local context,
and WordNet relations), which is a desirable charac-
teristic for combining algorithms.
2.1 Topic Signatures (TS)
Topic signatures (Agirre and de Lacalle, 2004) are
lists of words related to a particular sense. They can
be built from a variety of sources, and be used di-
rectly to perform WSD. Cuadros and Rigau (2006)
present a detailed evaluation of topic signatures built
from a variety of knowledge sources. In this work
we built those coming from the following:
? the relations in the Multilingual Central Repos-
itory (TS-MCR)
? the relations in the Extended WordNet (TS-
XWN)
In order to apply this resource for WSD, we sim-
ply measured the word-overlap between the target
context and each of the senses of the target word.
The sense with highest overlap is chosen as the cor-
rect sense.
350
2.2 Relatives in Context (RIC)
This is an unsupervised method presented in Mar-
tinez et al (2006). This algorithm makes use of
the WordNet relatives of the target word for disam-
biguation. The process is carried out in these steps:
(i) obtain a set of close relatives from WordNet for
each sense (the relatives can be polysemous); (ii) for
each test instance define all possible word sequences
that include the target word; (iii) for each word se-
quence, substitute the target word with each relative
and query a web search engine; (iv) rank queries ac-
cording to the following factors: length of the query,
distance of the relative to the target word, and num-
ber of hits; and (v) select the sense associated with
the highest ranked query.
The intuition behind this system is that we can
find related words that can be substituted for the tar-
get word in a given context, which are indicative of
its sense. The close relatives that can form more
common phrases from the target context determine
the target sense.
2.3 Relative Number (RNB)
This heuristic has been motivated as a way of identi-
fying rare senses of a word. An important disadvan-
tage of unsupervised systems is that rare senses can
be over-represented in the models, while supervised
systems are able to discard them because they have
access to token-level word sense distributions.
This simple algorithm relies on the number of
close relatives found in WordNet for each sense of
the word. The senses are ranked according to the
number of synonyms, direct hypernyms, and di-
rect hyponyms they have in WordNet. The highest
ranked sense is taken to be the most important for the
target word, and all occurrences of the target word
are tagged with that sense.
2.4 k-Nearest Neighbours (kNN)
As our supervised system, we relied on kNN. This is
a memory-based learning method where the neigh-
bours are the k most similar contexts, represented by
feature vectors (~ci) of the test vector (~f ). The sim-
ilarity among instances is measured by the cosine
of their vectors. The test instance is labeled with the
sense that obtains the maximum sum of the weighted
votes of the k most similar contexts. Each vote is
weighted depending on its (neighbour) position in
the ordered rank, with the closest being first. Equa-
tion 1 formalizes kNN, where Ci corresponds to the
sense label of the i-th closest neighbour.
arg max
Sj
=
k
?
i=1
{
1
i if Ci = Sj
0 otherwise (1)
The UBC group used a combination of kNN clas-
sifiers trained over a large set of features, and en-
hanced this method using Singular Value Decompo-
sition (SVD) for their supervised submission (UBC-
ALM) to the lexical-sample and all-words subtasks
(Agirre and Lopez de Lacalle, 2007). However, we
only used the basic implementation in this work, due
to time constraints.
2.5 Combination of systems
We explored two approaches to combine the stan-
dalone systems. The first consisted simply of adding
up the normalized weights that each system would
give to each sense. We tested this voting approach
both for the unsupervised and supervised settings.
The second method could only be applied in com-
bination with the supervised kNN system. The
idea was to include the unsupervised predictions as
weighted features for the supervised system. We re-
fer to this method as ?stacking?, and it has been pre-
viously used to integrate heterogeneous knowledge
sources for WSD (Stevenson and Wilks, 2001).
3 Development experiments
We tested the single algorithms and their combina-
tion over both Semcor and the training distribution
of the SemEval-2007 lexical-sample subtask of task
17 (S07LS for short). The goal of these experiments
was to obtain an estimate of the expected perfor-
mance, and submit the most promising configura-
tion. We present first the tests on the unsupervised
setting, and then the supervised setting. It is im-
portant to note that the hand-tagged corpora was not
used to fine-tune the parameters of the unsupervised
algorithms.
3.1 Unsupervised systems
For the first evaluation of our unsupervised systems,
we relied on Semcor, and tagged 43,063 instances
of the 329 word types occurring in SemEval-2007
351
System Recall
RNB 30.6
TS-MCR 57.5
TS-XWN 47.0
TS-MCR & TS-XWN 57.3
RBN & TS-MCR & TS-XWN 53.6
Table 1: Evaluation of standalone and combined
unsupervised systems over 43,063 instances from
Semcor
System Recall
TS-MCR 60.1
TS-XWN 54.3
TS-MCR & TS-XWN 61.1
TS-MCR & TS-XWN & RIC* 61.2
Table 2: Evaluation of standalone and combined
unsupervised systems over 8,518 instances from
S07LS training
all-words. Due to time constraints, we were not able
to test the RIC algorithm on this dataset. The re-
sults are shown in Table 1. We can see that the RNB
heuristic performs poorly, and that the best configu-
ration consists of applying the single TS-MCR algo-
rithm. From this experiment, we decided to remove
the RNB heuristic and focus on the topic signatures
and RIC.
We also used S07LS for extra experiments in
the unsupervised setting. From the training part of
the S07LS dataset, we extracted 8,518 instances of
words also occurring in SemEval-2007 all-words.
As S07LS used senses from OntoNotes, we relied
on the mapping provided by the task organisers to
link them to WordNet senses. We left RNB out of
this experiment due to its low performance in Sem-
cor, and regarding RIC, we only evaluated a sample
of 68 instances. Results are shown in Table 2. The
best scores are achieved when combining both sets
of topic signatures. The few cases that have been
disambiguated with RIC improve the overall perfor-
mance slightly.
3.2 Combined system
We could not rely on Semcor in the supervised set-
ting (we used it for training), and therefore tried to
use as much data as possible from the training com-
ponent of S07LS, wherein all the instances avail-
able (22,281) were disambiguated. We tested first
System Recall
kNN 87.4
kNN & TS-MCR 86.8
kNN & TS-XWN 86.4
kNN & TS-MCR & TS-XWN 86.0
Table 3: Evaluation of voting supervised systems in
22,281 instances from S07LS training
System Recall
kNN 71.7
kNN & TS-MCR & TS-XWN 71.8
Table 4: Evaluation of ?stacking? the unsupervised
systems on kNN over 8,518 instances from S07LS
training
the voting combination by adding the normalized
weights from the output of each system. Due to
time constraints we only evaluated the combination
of kNN with TS-MCR and TS-XWN. Results are
shown in Table 3, where we can see that combin-
ing the unsupervised systems with voting hurts the
performance of the kNN method.
Finally, we applied the second combination ap-
proach, consisting of including the predictions of the
unsupervised systems as features for kNN (?stack-
ing?). We performed this experiment on the training
part of S07LS, but only for the 8,518 instances of
the words occurring on the all-words dataset. The
results of this experiment are given in Table 4. We
observed a slight improvement in this case.
4 Final systems
For our final submissions, we chose the combination
?TS-MCR& TS-XWN&RIC? for the unsupervised
system (UBC-UMB-1), and the combination ?kNN
& TS-MCR & TS-XWN? via ?stacking? for our su-
pervised system (UBC-UMB-2). The results of all
the systems are given in Table 5.
We can see that our unsupervised system ranked
10th. Unfortunately, we do not know at the time of
writing which other systems are unsupervised, and
therefore are unable to compare to other unsuper-
vised systems.
Our ?stacking? supervised system performs
slightly lower than the kNN supervised systems by
UBC-ALM (which ranks 7th), showing that our sys-
tem was not able to profit from information from
352
System Precision Recall
1. 0.537 0.537
2. 0.527 0.527
3. 0.524 0.524
4. 0.522 0.486
5. 0.518 0.518
6. 0.514 0.514
7. 0.493 0.492
8. UBC-UMB-2 0.485 0.484
9. 0.420 0.420
10. UBC-UMB-1 0.362 0.362
11. 0.355 0.355
12. 0.337 0.337
13. 0.298 0.298
14. 0.120 0.118
Table 5: Official results for all systems in task #17
of SemEval-2007. Our systems are shown in bold.
UBC-UMB-1 stands for TS-MCR & TS-XWN &
RIC, and UBC-UMB-2 for kNN & TS-MCR & TS-
XWN.
System Precision Recall
TS-MCR 36.7 36.5
TS-XWN 33.1 32.9
RIC 30.6 30.4
TS-MCR & TS-XWN 37.5 37.3
TS-MCR & TS-XWN & RIC 36.2 36.2
Table 6: Our unsupervised systems in the SemEval-
2007 all words test data
the unsupervised systems. However, we cannot at-
tribute the decrease only to the unsupervised fea-
tures, as the kNN implementations were different
(UBC-ALM relied on SVD).
After the gold-standard data was released, we
were able to test the contribution of each of the un-
supervised systems in the ensemble, as well as two
additional combinations. The results are given in
Table 6. We can see that TS-MCR is the best per-
forming method, confirming our development ex-
periments (cf. Tables 1 and 2). In contrast, in-
cluding RIC decreased the performance by 0.7 per-
cent points, and had we used only TS-MCR and TS-
XWN our results would have been better.
5 Conclusions
In this submission we combined heterogeneous un-
supervised algorithms to obtain competitive perfor-
mance without relying on training data. However,
due to time constraints, we were only able to submit
a preliminary system, and some of the unsupervised
methods were not properly developed and tested.
For future work we plan to properly test these
methods, and deploy other unsupervised algorithms.
We also plan to explore more sophisticated combina-
tion strategies, using meta-learning to try to predict
which features of each word make a certain WSD
system succeed (or fail).
Acknowledgements
The first and second authors were supported by Aus-
tralian Research Council grant no. DP0663879. We
want to thank German Rigau from the University of
the Basque Country for kindly providing access to
the MCR.
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proceedings of the 4rd International
Conference on Language Resources and Evaluations
(LREC), pages 1123?6, Lisbon, Portugal.
Eneko Agirre and Oier Lopez de Lacalle. 2007. UBC-
ALM: Lexical-Sample and All-Words tasks. In
Proceedings of SemEval-2007 (forthcoming), Prague,
Czech Republic.
Samuel Brody, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised WSD. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the ACL, pages 97?104, Sydney, Australia.
Montse Cuadros and German Rigau. 2006. Quality as-
sessment of large scale knowledge resources. In Pro-
ceedings of the International Conference on Empirical
Methods in Natural Language Processing (EMNLP-
06), pages 534?41, Sydney, Australia.
David Martinez, Eneko Agirre, and Xinglong Wang.
2006. Word relatives in context for word sense dis-
ambiguation. In Proceedings of the 2006 Australasian
Language Technology Workshop, pages 42?50, Syd-
ney, Australia.
Mark Stevenson and YorickWilks. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321?49.
353
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 123?128,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 17: All-words Word Sense Disambiguation
on a Specific Domain
Eneko Agirre
IXA NLP group
UBC
Donostia, Basque Country
e.agirre@ehu.es
Oier Lopez de Lacalle
IXA NLP group
UBC
Donostia, Basque Country
oier.lopezdelacalle@ehu.es
Christiane Fellbaum
Department of Computer Science
Princeton University
Princeton, USA
fellbaum@princeton.edu
Andrea Marchetti
IIT
CNR
Pisa, Italy
andrea.marchetti@iit.cnr.it
Antonio Toral
ILC
CNR
Pisa, Italy
antonio.toral@ilc.cnr.it
Piek Vossen
Faculteit der Letteren
Vrije Universiteit Amsterdam
Amsterdam, Netherlands
p.vossen@let.vu.nl
Abstract
Domain portability and adaptation of NLP
components and Word Sense Disambiguation
systems present new challenges. The diffi-
culties found by supervised systems to adapt
might change the way we assess the strengths
and weaknesses of supervised and knowledge-
based WSD systems. Unfortunately, all ex-
isting evaluation datasets for specific domains
are lexical-sample corpora. With this paper
we want to motivate the creation of an all-
words test dataset for WSD on the environ-
ment domain in several languages, and present
the overall design of this SemEval task.
1 Introduction
Word Sense Disambiguation (WSD) competitions
have focused on general domain texts, as attested
in the last Senseval and Semeval competitions (Kil-
garriff, 2001; Mihalcea et al, 2004; Pradhan et al,
2007). Specific domains pose fresh challenges to
WSD systems: the context in which the senses occur
might change, distributions and predominant senses
vary, some words tend to occur in fewer senses in
specific domains, and new senses and terms might
be involved. Both supervised and knowledge-based
systems are affected by these issues: while the first
suffer from different context and sense priors, the
later suffer from lack of coverage of domain-related
words and information.
Domain adaptation of supervised techniques is a
hot issue in Natural Language Processing, includ-
ing Word Sense Disambiguation. Supervised Word
Sense Disambiguation systems trained on general
corpora are known to perform worse when applied
to specific domains (Escudero et al, 2000; Mart??nez
and Agirre, 2000), and domain adaptation tech-
niques have been proposed as a solution to this prob-
lem with mixed results.
Current research on applying WSD to specific do-
mains has been evaluated on three available lexical-
sample datasets (Ng and Lee, 1996; Weeber et al,
2001; Koeling et al, 2005). This kind of dataset
contains hand-labeled examples for a handful of se-
lected target words. As the systems are evaluated on
a few words, the actual performance of the systems
over complete texts can not be measured. Differ-
ences in behavior of WSD systems when applied to
lexical-sample and all-words datasets have been ob-
served on previous Senseval and Semeval competi-
tions (Kilgarriff, 2001; Mihalcea et al, 2004; Prad-
han et al, 2007): supervised systems attain results
on the high 80?s and beat the most frequent base-
line by a large margin for lexical-sample datasets,
but results on the all-words datasets were much more
modest, on the low 70?s, and a few points above the
most frequent baseline.
Thus, the behaviour of WSD systems on domain-
specific texts is largely unknown. While some words
could be supposed to behave in similar ways, and
thus be amenable to be properly treated by a generic
123
WSD algorithm, other words have senses closely
linked to the domain, and might be disambiguated
using purpose-built domain adaptation strategies (cf.
Section 4). While it seems that domain-specific
WSD might be a tougher problem than generic
WSD, it might well be that domain-related words
are easier to disambiguate.
The main goal of this task is to provide a mul-
tilingual testbed to evaluate WSD systems when
faced with full-texts from a specific domain, that of
environment-related texts. The paper is structured
as follows. The next section presents current lexi-
cal sample datasets for domain-specific WSD. Sec-
tion 3 presents some possible settings for domain
adaptation. Section 4 reviews the state-of-the art in
domain-specific WSD. Section 5 presents the design
of our task, and finally, Section 6 draws some con-
clusions.
2 Specific domain datasets available
We will briefly present the three existing datasets
for domain-related studies in WSD, which are all
lexical-sample.
The most commonly used dataset is the Defense
Science Organization (DSO) corpus (Ng and Lee,
1996), which comprises sentences from two differ-
ent corpora. The first is the Wall Street Journal
(WSJ), which belongs to the financial domain, and
the second is the Brown Corpus (BC) which is a bal-
anced corpora of English usage. 191 polysemous
words (nouns and verbs) of high frequency in WSJ
and BC were selected and a total of 192,800 occur-
rences of these words were tagged with WordNet 1.5
senses, more than 1,000 instances per word in aver-
age. The examples from BC comprise 78,080 oc-
currences of word senses, and examples from WSJ
consist on 114,794 occurrences. In domain adapta-
tion experiments, the Brown Corpus examples play
the role of general corpora, and the examples from
the WSJ play the role of domain-specific examples.
Koeling et al (2005) present a corpus were the
examples are drawn from the balanced BNC cor-
pus (Leech, 1992) and the SPORTS and FINANCES
sections of the newswire Reuters corpus (Rose et al,
2002), comprising around 300 examples (roughly
100 from each of those corpora) for each of the 41
nouns. The nouns were selected because they were
salient in either the SPORTS or FINANCES domains,
or because they had senses linked to those domains.
The occurrences were hand-tagged with the senses
from WordNet version 1.7.1 (Fellbaum, 1998). In
domain adaptation experiments the BNC examples
play the role of general corpora, and the FINANCES
and SPORTS examples the role of two specific do-
main corpora.
Finally, a dataset for biomedicine was developed
by Weeber et al (2001), and has been used as
a benchmark by many independent groups. The
UMLS Metathesaurus was used to provide a set of
possible meanings for terms in biomedical text. 50
ambiguous terms which occur frequently in MED-
LINE were chosen for inclusion in the test set. 100
instances of each term were selected from citations
added to the MEDLINE database in 1998 and man-
ually disambiguated by 11 annotators. Twelve terms
were flagged as ?problematic? due to substantial dis-
agreement between the annotators. In addition to the
meanings defined in UMLS, annotators had the op-
tion of assigning a special tag (?none?) when none
of the UMLS meanings seemed appropriate.
Although these three corpora are useful for WSD
research, it is difficult to infer which would be the
performance of a WSD system on full texts. The
corpus of Koeling et al, for instance, only includes
words which where salient for the target domains,
but the behavior of WSD systems on other words
cannot be explored. We would also like to note that
while the biomedicine corpus tackles scholarly text
of a very specific domain, the WSJ part of the DSO
includes texts from a financially oriented newspaper,
but also includes news of general interest which have
no strict relation to the finance domain.
3 Possible settings for domain adaptation
When performing supervised WSD on specific do-
mains the first setting is to train on a general domain
data set and to test on the specific domain (source
setting). If performance would be optimal, this
would be the ideal solution, as it would show that a
generic WSD system is robust enough to tackle texts
from new domains, and domain adaptation would
not be necessary.
The second setting (target setting) would be to
train the WSD systems only using examples from
124
the target domain. If this would be the optimal set-
ting, it would show that there is no cost-effective
method for domain adaptation. WSD systems would
need fresh examples every time they were deployed
in new domains, and examples from general do-
mains could be discarded.
In the third setting, the WSD system is trained
with examples coming from both the general domain
and the specific domain. Good results in this setting
would show that supervised domain adaptation is
working, and that generic WSD systems can be sup-
plemented with hand-tagged examples from the tar-
get domain.
There is an additional setting, where a generic
WSD system is supplemented with untagged exam-
ples from the domain. Good results in this setting
would show that semi-supervised domain adapta-
tion works, and that generic WSD systems can be
supplemented with untagged examples from the tar-
get domain in order to improve their results.
Most of current all-words generic supervised
WSD systems take SemCor (Miller et al, 1993) as
their source corpus, i.e. they are trained on SemCor
examples and then applied to new examples. Sem-
Cor is the largest publicly available annotated cor-
pus. It?s mainly a subset of the Brown Corpus, plus
the novel The Red Badge of Courage. The Brown
corpus is balanced, yet not from the general domain,
as it comprises 500 documents drawn from differ-
ent domains, each approximately 2000 words long.
Although the Brown corpus is balanced, SemCor is
not, as the documents were not chosen at random.
4 State-of-the-art in WSD for specific
domains
Initial work on domain adaptation for WSD sys-
tems showed that WSD systems were not able to
obtain better results on the source or adaptation set-
tings compared to the target settings (Escudero et
al., 2000), showing that a generic WSD system (i.e.
based on hand-annotated examples from a generic
corpus) would not be useful when moved to new do-
mains.
Escudero et al (2000) tested the supervised adap-
tation scenario on the DSO corpus, which had exam-
ples from the Brown Corpus and Wall Street Journal
corpus. They found that the source corpus did not
help when tagging the target corpus, showing that
tagged corpora from each domain would suffice, and
concluding that hand tagging a large general corpus
would not guarantee robust broad-coverage WSD.
Agirre and Mart??nez (2000) used the same DSO cor-
pus and showed that training on the subset of the
source corpus that is topically related to the target
corpus does allow for domain adaptation, obtaining
better results than training on the target data alone.
In (Agirre and Lopez de Lacalle, 2008), the au-
thors also show that state-of-the-art WSD systems
are not able to adapt to the domains in the context
of the Koeling et al (2005) dataset. While WSD
systems trained on the target domain obtained 85.1
and 87.0 of precision on the sports and finances do-
mains, respectively, the same systems trained on the
BNC corpus (considered as a general domain cor-
pus) obtained 53.9 and 62.9 of precision on sports
and finances, respectively. Training on both source
and target was inferior that using the target examples
alone.
Supervised adaptation
Supervised adaptation for other NLP tasks has been
widely reported. For instance, (Daume? III, 2007)
shows that a simple feature augmentation method
for SVM is able to effectively use both labeled tar-
get and source data to provide the best domain-
adaptation results in a number of NLP tasks. His
method improves or equals over previously explored
more sophisticated methods (Daume? III and Marcu,
2006; Chelba and Acero, 2004). In contrast, (Agirre
and Lopez de Lacalle, 2009) reimplemented this
method and showed that the improvement on WSD
in the (Koeling et al, 2005) data was marginal.
Better results have been obtained using purpose-
built adaptation methods. Chan and Ng (2007) per-
formed supervised domain adaptation on a manu-
ally selected subset of 21 nouns from the DSO cor-
pus. They used active learning, count-merging, and
predominant sense estimation in order to save tar-
get annotation effort. They showed that adding just
30% of the target data to the source examples the
same precision as the full combination of target and
source data could be achieved. They also showed
that using the source corpus significantly improved
results when only 10%-30% of the target corpus
was used for training. In followup work (Zhong et
125
Projections for 2100 suggest that temperature in Europe will have risen by between 2 to 6.3 C above 1990
levels. The sea level is projected to rise, and a greater frequency and intensity of extreme weather events are
expected. Even if emissions of greenhouse gases stop today, these changes would continue for many decades
and in the case of sea level for centuries. This is due to the historical build up of the gases in the atmosphere
and time lags in the response of climatic and oceanic systems to changes in the atmospheric concentration
of the gases.
Figure 1: Sample text from the environment domain.
al., 2008), the feature augmentation approach was
combined with active learning and tested on the
OntoNotes corpus, on a large domain-adaptation ex-
periment. They significantly reduced the effort of
hand-tagging, but only obtained positive domain-
adaptation results for smaller fractions of the target
corpus.
In (Agirre and Lopez de Lacalle, 2009) the au-
thors report successful adaptation on the (Koeling
et al, 2005) dataset on supervised setting. Their
method is based on the use of unlabeled data, re-
ducing the feature space with SVD, and combina-
tion of features using an ensemble of kernel meth-
ods. They report 22% error reduction when using
both source and target data compared to a classifier
trained on target the target data alone, even when the
full dataset is used.
Semi-supervised adaptation
There are less works on semi-supervised domain
adaptation in NLP tasks, and fewer in WSD task.
Blitzer et al (2006) used Structural Correspondence
Learning and unlabeled data to adapt a Part-of-
Speech tagger. They carefully select so-called pivot
features to learn linear predictors, perform SVD on
the weights learned by the predictor, and thus learn
correspondences among features in both source and
target domains. Agirre and Lopez de Lacalle (2008)
show that methods based on SVD with unlabeled
data and combination of distinct feature spaces pro-
duce positive semi-supervised domain adaptation re-
sults for WSD.
Unsupervised adaptation
In this context, we take unsupervised to mean
Knowledge-Based methods which do not require
hand-tagged corpora. The predominant sense acqui-
sition method was succesfully applied to specific do-
mains in (Koeling et al, 2005). The methos has two
steps: In the first, a corpus of untagged text from the
target domain is used to construct a thesaurus of sim-
ilar words. In the second, each target word is disam-
biguated using pairwise WordNet-based similarity
measures, taking as pairs the target word and each of
the most related words according to the thesaurus up
to a certain threshold. This method aims to obtain,
for each target word, the sense which is the most
predominant for the target corpus. When a general
corpus is used, the most predominant sense in gen-
eral is obtained, and when a domain-specific corpus
is used, the most predominant sense for that corpus
is obtained (Koeling et al, 2005). The main motiva-
tion of the authors is that the most frequent sense is a
very powerful baseline, but it is one which requires
hand-tagging text, while their method yields simi-
lar information automatically. The results show that
they are able to obtain good results. In related work,
(Agirre et al, 2009) report improved results using
the same strategy but applying a graph-based WSD
method, and highlight the domain-adaptation poten-
tial of unsupervised knowledge-based WSD systems
compared to supervised WSD.
5 Design of the WSD-domain task
This task was designed in the context of Ky-
oto (Piek Vossen and VanGent, 2008)1, an Asian-
European project that develops a community plat-
form for modeling knowledge and finding facts
across languages and cultures. The platform op-
erates as a Wiki system with an ontological sup-
port that social communities can use to agree on the
meaning of terms in specific domains of their inter-
est. Kyoto will focus on the environmental domain
because it poses interesting challenges for informa-
tion sharing, but the techniques and platforms will
be independent of the application domain. Kyoto
1http://www.kyoto-project.eu/
126
will make use of semantic technologies based on
ontologies and WSD in order to extract and repre-
sent relevant information for the domain, and is thus
interested on measuring the performance of WSD
techniques on this domain.
The WSD-domain task will comprise comparable
all-words test corpora on the environment domain.
Texts from the European Center for Nature Con-
servation2 and Worldwide Wildlife Forum3 will be
used in order to build domain specific test corpora.
We will select documents that are written for a gen-
eral but interested public and that involve specific
terms from the domain. The document content will
be comparable across languages. Figure 1 shows an
example in English related to global warming.
The data will be available in a number of lan-
guages: English, Dutch, Italian and Chinese. The
sense inventories will be based on wordnets of the
respective languages, which will be updated to in-
clude new vocabulary and senses. The test data will
comprise three documents of around 2000 words
each for each language. The annotation procedure
will involve double-blind annotation plus adjudica-
tion, and inter-tagger agreement data will be pro-
vided. The formats and scoring software will fol-
low those of Senseval-34 and SemEval-20075 En-
glish all-words tasks.
There will not be training data available, but par-
ticipants are free to use existing hand-tagged cor-
pora and lexical resources (e.g. SemCor and pre-
vious Senseval and SemEval data). We plan to make
available a corpus of documents from the same do-
main as the selected documents, as well as wordnets
updated to include the terms and senses in the se-
lected documents.
6 Conclusions
Domain portability and adaptation of NLP com-
ponents and Word Sense Disambiguation systems
present new challenges. The difficulties found by
supervised systems to adapt might change the way
we assess the strengths and weaknesses of super-
vised and knowledge-based WSD systems. Unfor-
tunately, all existing evaluation datasets for specific
2http://www.ecnc.org
3http://www.wwf.org
4http://www.senseval.org/senseval3
5http://nlp.cs.swarthmore.edu/semeval/
domains are lexical-sample corpora. With this paper
we have motivated the creation of an all-words test
dataset for WSD on the environment domain in sev-
eral languages, and presented the overall design of
this SemEval task.
Further details can be obtained from the Semeval-
20106 website, our task website7, and in our distri-
bution list8
7 Acknowledgments
The organization of the task is partially funded
by the European Commission (KYOTO FP7 ICT-
2007-211423) and the Spanish Research Depart-
ment (KNOW TIN2006-15049-C03-01).
References
Eneko Agirre and Oier Lopez de Lacalle. 2008. On ro-
bustness and domain adaptation using SVD for word
sense disambiguation. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 17?24, Manchester, UK, August.
Coling 2008 Organizing Committee.
Eneko Agirre and Oier Lopez de Lacalle. 2009. Super-
vised domain adaptation for wsd. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-09).
E. Agirre, O. Lopez de Lacalle, and A. Soroa. 2009.
Knowledge-based WSD and specific domains: Per-
forming over supervised WSD. In Proceedings of IJ-
CAI, Pasadena, USA.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
49?56, Prague, Czech Republic, June. Association for
Computational Linguistics.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy classifier: Little data can help a
lot. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Barcelona, Spain.
6http://semeval2.fbk.eu/
7http://xmlgroup.iit.cnr.it/SemEval2010/
8http://groups.google.com/groups/wsd-domain
127
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
Gerard Escudero, Lluiz Ma?rquez, and German Rigau.
2000. An Empirical Study of the Domain Dependence
of Supervised Word Sense Disambiguation Systems.
Proceedings of the joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora, EMNLP/VLC.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
A. Kilgarriff. 2001. English Lexical Sample Task De-
scription. In Proceedings of the Second International
Workshop on evaluating Word Sense Disambiguation
Systems, Toulouse, France.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense
acquisition. In Proceedings of the Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing.
HLT/EMNLP, pages 419?426, Ann Arbor, Michigan.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
David Mart??nez and Eneko Agirre. 2000. One Sense per
Collocation and Genre/Topic Variations. Conference
on Empirical Method in Natural Language.
R. Mihalcea, T. Chklovski, and Adam Killgariff. 2004.
The Senseval-3 English lexical sample task. In Pro-
ceedings of the 3rd ACL workshop on the Evaluation
of Systems for the Semantic Analysis of Text (SENSE-
VAL), Barcelona, Spain.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A Semantic Concordance. In Proceedings of the
ARPA Human Language Technology Workshop. Dis-
tributed as Human Language Technology by San Ma-
teo, CA: Morgan Kaufmann Publishers., pages 303?
308, Princeton, NJ.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
sense: An exemplar-based approach. In Proceedings
of the 34th Annual Meeting of the Association for
Computationla Linguistics (ACL), pages 40?47.
Nicoletta Calzolari Christiane Fellbaum Shu-kai Hsieh
Chu-Ren Huang Hitoshi Isahara Kyoko Kanzaki An-
drea Marchetti Monica Monachini Federico Neri
Remo Raffaelli German Rigau Maurizio Tescon
Piek Vossen, Eneko Agirre and Joop VanGent. 2008.
Kyoto: a system for mining, structuring and distribut-
ing knowledge across languages and cultures. In
European Language Resources Association (ELRA),
editor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, may.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: English
lexical sample, srl and all words. In Proceedings of
the Fourth International Workshop on Semantic Eval-
uations (SemEval-2007), pages 87?92, Prague, Czech
Republic.
Tony G. Rose, Mark Stevenson, and Miles Whitehead.
2002. The Reuters Corpus Volumen 1: from Yes-
terday?s News to Tomorrow?s Language Resources.
In Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC-2002),
pages 827?832, Las Palmas, Canary Islands.
Marc Weeber, James G. Mork, and Alan R. Aronson.
2001. Developing a test collection for biomedical
word sense disambiguation. In Proceedings of the
AMAI Symposium, pages 746?750, Washington, DC.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An em-
pirical study. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1002?1010, Honolulu, Hawaii, October.
Association for Computational Linguistics.
128
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 415?425,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Unsupervised Relation Extraction with General Domain Knowledge
Oier Lopez de Lacalle1,2 and Mirella Lapata1
1Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB
2IKERBASQUE, Basque Foundation for Science, Bilbao, Spain
oier.lopezdelacalle@ehu.es, mlap@inf.ed.ac.uk
Abstract
In this paper we present an unsupervised ap-
proach to relational information extraction.
Our model partitions tuples representing an
observed syntactic relationship between two
named entities (e.g., ?X was born in Y?
and ?X is from Y?) into clusters correspond-
ing to underlying semantic relation types
(e.g., BornIn, Located). Our approach incor-
porates general domain knowledge which we
encode as First Order Logic rules and auto-
matically combine with a topic model devel-
oped specifically for the relation extraction
task. Evaluation results on the ACE 2007
English Relation Detection and Categoriza-
tion (RDC) task show that our model outper-
forms competitive unsupervised approaches
by a wide margin and is able to produce clus-
ters shaped by both the data and the rules.
1 Introduction
Information extraction (IE) is becoming increas-
ingly useful as a form of shallow semantic analy-
sis. Learning relational facts from text is one of the
core tasks of IE and has applications in a variety
of fields including summarization, question answer-
ing, and information retrieval. Previous work (Sur-
deanu and Ciaramita, 2007; Culotta and Sorensen,
2004; Zhou et al, 2007) has traditionally relied on
extensive human involvement (e.g., hand-annotated
training instances, manual pattern extraction rules,
hand-picked seeds). Standard supervised techniques
can yield high performance when large amounts
of hand-labeled data are available for a fixed in-
ventory of relation types (e.g., Employment, Lo-
cated), however, extraction systems do not easily
generalize beyond their training domains and often
must be re-engineered for each application. Un-
supervised approaches offer a promising alternative
which could lead to significant resource savings and
more portable extraction systems.
It therefore comes as no surprise that latent topic
analysis methods have been used for a variety of
IE tasks. Yao et al (2011), for example, propose
a series of topic models which perform relation
discovery by clustering tuples representing an ob-
served syntactic relationship between two named en-
tities (e.g., ?X was born in Y? and ?X is from Y?).
The clusters correspond to semantic relations whose
number or type is not known in advance. Their mod-
els depart from standard Latent Dirichlet Allocation
(Blei et al, 2003) in that a document consists of re-
lation tuples rather than individual words; moreover,
tuples have features each of which is generated in-
dependently from a hidden relation (e.g., the words
corresponding to the first and second entities, the
type and order of the named entities). Since these
features are local, they cannot capture more global
constraints pertaining to the relation extraction task.
Such constraints may take the form of restrictions
on which tuples should be clustered together or
not. For instance, different types of named entities
may be indicative of different relations (ORG-LOC
entities often express a Location relation whereas
PER-PER entities express Business or Family rela-
tions) and thus tuples bearing these entities should
not be grouped together. Another example are tuples
with identical or similar features which intuitively
should be clustered together.
In this paper, we propose an unsupervised ap-
proach to relation extraction which does not re-
415
quire any relation-specific training data and allows
to incorporate global constraints general express-
ing domain knowledge. We encode domain knowl-
edge as First Order Logic (FOL) rules and automati-
cally integrate them with a topic model to produce
clusters shaped by the data and the constraints at
hand. Specifically, we extend the Fold-all (First-
Order Logic latent Dirichlet Allocation) framework
(Andrzejewski et al, 2011) to the relation extraction
task, explain how to incorporate meaningful con-
straints, and develop a scalable inference technique.
In the presence of multiple candidate relation de-
compositions for a given corpus, domain knowledge
can steer the model towards relations which are best
aligned with user and task modeling goals. We also
argue that a general mechanism for encoding addi-
tional modeling assumptions and side information
can lessen the need for ?custom? relation extraction
model variants. Experimental results on the ACE-
2007 Relation Detection and Categorization (RDC)
dataset show that our model outperforms competi-
tive unsupervised approaches by a wide margin and
is able to uncover meaningful relations with only
two general rule types.
Our contributions in this work are three-fold: a
new model that modifies the Fold-all framework and
extends it to the relation extraction task; a new for-
malization of the logic rules applicable to topic mod-
els defined over a rich set of features; and a proposal
for mining the logic rules automatically from a cor-
pus contrary to Andrzejewski et al (2011) who em-
ploy manually crafted seeds.
2 Related Work
A variety of learning paradigms have been applied
to relation extraction. As mentioned earlier, super-
vised methods have been shown to perform well in
this task. The reliance on manual annotation, which
is expensive to produce and thus limited in quantity,
has provided the impetus for semi-supervised and
purely unsupervised approaches. Semi-supervised
methods use a small number of seed instances or
patterns (per relation) to launch an iterative train-
ing process (Riloff and Jones, 1999; Agichtein and
Gravano, 2000; Bunescu and Mooney, 2007; Pan-
tel and Pennacchiotti, 2006). The seeds are used
to extract a new set of patterns from a large cor-
pus, which are then used to extract more instances,
and so on. Unsupervised relation extraction meth-
ods are not limited to a predefined set of target
relations, but discover all types of relations found
in the text. The relations represent clusters over
strings of words (Banko et al, 2007; Hasegawa et
al., 2004), syntactic patterns between entities (Yao
et al, 2011; Shinyama and Sekine, 2006), or logical
expressions (Poon and Domingos, 2009). Another
learning paradigm is distant supervision which does
not require labeled data but instead access to a rela-
tional database such as Freebase (Mintz et al, 2009).
The idea is to take entities that appear in some rela-
tion in the database, find the sentences that express
the relation in an unlabeled corpus, and use them to
train a relation classifier.
Our own work adds an additional approach into
the mix. We use a topic model to infer an arbi-
trary number of relations between named entities.
Although we do not have access to relation-specific
information (either as a relational database or manu-
ally annotated data), we impose task-specific con-
straints which inject domain knowledge into the
learning algorithm. We thus alleviate known prob-
lems with the interpretability of the clusters obtained
from topic models and are able to guide our model
towards reasonable relations. Andrzejewski et al
(2011) show how to integrate First-Order Logic with
vanilla LDA. We extend their formulation to relation
tuples rather than individual words. Our model gen-
erates a corpus of entity tuples which are in turn rep-
resented by features and uses automatically acquired
FOL rules. The idea of integrating topic modeling
with FOL builds on research in probabilistic logic
modeling such as Markov Logic Networks (Richard-
son and Domingos, 2006). Schoenmackers et al
(2010) learn Horn clauses from web-scale text with
aim of finding answers to a user?s query. Our work
is complementary to theirs. We could make use of
their rules to discover more accurate relations.
The general goal of assisting the learner in re-
covering the ?correct? clustering by supplying ad-
ditional domain knowledge is not new. Gondek and
Hofmann (2004) supply a known clustering they do
not want the learner to return, whereas Wagstaff
et al (2001) use pairwise labels for items indicat-
ing whether they belong in the same cluster. These
methods combine domain knowledge with statistical
learning in order to improve performance with re-
416
spect to the true target clustering. Although, the tar-
get labels are not available in our case, we are able to
show that the inclusion of domain knowledge yields
clustering improvements.
3 Learning Setting
Our relation extraction task broadly adheres to the
ACE specification guidelines. Our aim is to detect
and characterize the semantic relations between
two named entities. The input to our model is a
corpus of documents, where each document is a
bag of relation tuples which can be obtained from
the output of any dependency parser. Each tuple
represents a syntactic relationship between two
named entity (NE) mentions, and consists of three
components: the dependency path between the
two mentions, the source NE, and the target NE. A
dependency path is the concatenation of dependency
edges and nodes along a path in the dependency
tree. For example, the sentence ?George Bush
traveled to France on Thursday for a summit.?
would yield the tuple [SOURCE:George Bush(PER),
PATH:?nsubj?traveled?prep?to?pobj?,
DES:France(LOC)]. The tuple here expresses the
relation Located, however our model does not
observe any relation labels during training. The
model assigns tuples to clusters, corresponding to
an underlying relation type. Each tuple instance can
be then labeled with an identifier corresponding to
the cluster (aka relation) it has been assigned to.
4 Modeling Framework
Our model builds on the work of Yao et al (2011)
who develop a series of generative probabilistic
models for relation extraction. Specifically, we ex-
tend their relational LDA model by interfacing it
with FOL-rules. In the following, we first describe
their approach in more detail and then present our
extensions and modifications.
4.1 Relational LDA
Relational LDA is an extension to LDA with a sim-
ilar generative story. LDA models each document
as a mixture of topics, which are in turn character-
ized as distributions over words. In relational LDA,
each document is a mixture of relations over tuples
representing syntactic relations between two named
entities. The relation tuples are in turn generated a
by set of features drawn independently from the un-
derlying relation distribution.
More technically, a multinomial distribution over
relations ?di is drawn from a Dirichlet prior
(? ? Dir(?)) at the document level. Relation tuples
are generated from a multinomial distribution ?di
(zi|?di ? Mult(?di)) and are represented with k fea-
tures. Each feature is drawn (independently) from
a multinomial distribution selected by the relation
assigned to tuple i (fik|zi, ?zi ? Mult(?zi)). Rela-
tions are drawn from a Dirichlet prior (? ? Dir(?)).
In other words, each tuple in a document is assigned
a hidden relation (z = z1...zN ); each relation is
represented by a multinomial distribution over fea-
tures ?r (Dirichlet prior ?). ?r is a vector with F
dimensions each corresponding to a feature. Fi-
nally, documents (j = 1...D) are associated with a
multinomial distribution ?j over relations (Dirichlet
prior ?). ?j is a vector with R dimensions, one for
each relation.
Figure 1 represents relational LDA model as a an
undirected graphical model or factor graph (Bishop,
2006), ignoring for the moment the factor which
connects the d, z, f1...k and o variables. Directed
graphical models can be converted into undirected
ones by adding edges between co-parents (Koller
and Friedman, 2009). Each clique in the graph de-
fines a potential function which replaces the condi-
tional probabilities in the directed graph. Each max-
imal clique is associated with a special factor node
(the black squares) and clique members are con-
nected to that factor. The probability of any specific
configuration is calculated by multiplying the poten-
tial functions and normalizing them. We adopt the
factor graph representation as is it convenient for in-
troducing logic rules into the model. The joint prob-
ability of the model given the priors and the docu-
ments (P (p, z, ?, ?|?, ?,d)) is equivalent to:
R?
r
p(?r|?)
D?
j
p(?j |?)
N?
i
?di(zi)
?
k?pi
?zi(fk) (1)
where ?di(zi) is the zi-th element in the vector ?di
and ?zi(fk) is fk-th feature in the ?zi vector. Vari-
able pi is the i-th tuple containing k features. The
parameters of the latent variables (e.g., ?, ?) are
typically estimated using an approximate inference
algorithm such as Gibbs Sampling (Griffiths and
Steyvers, 2004).
417
Figure 1: Relational LDA as a factor graph. Filled
circles represent observed variables, empty circles are
associated with latent variables or model hyperparame-
ters, and plates indicate repeating structures. The black
squares are the factor nodes and are associated with the
potential functions corresponding to conditional indepen-
dence among the variables. The model observes D doc-
uments (d) consisting of N tuples (p), each represented
by a set of features f1,f2 . . . fk. z represents the relation
type assignment to a tuple, ? is the relation type propor-
tion for a given document, and ? the relation type dis-
tribution over the features. The logic factor (indicated
with the arrow) connects the KB with the relational LDA
model. Variable o is an observed variable which contains
the side information expressed in FOL.
As shown in Figure 1, the observed variables are
represented by filled circles. In our case, our model
sees the corpus (p, d), where d is the variable rep-
resenting the document and the tuples (p) are repre-
sented by a set of features f1,f2 . . . fk in the graph.
Empty circles are associated with latent variables to
be estimated: z represents the relation type assign-
ment to the tuple, ? is the relation type proportion
for the given document, and ? is the relation type
distribution over the features.
The features representing the tuples tap onto se-
mantic information expressed by different surface
forms and are an important part of the model. We
use a subset of the features proposed in Yao et al
(2011) which we briefly describe below:
SOURCE This feature corresponds to the first en-
tity mention of the tuple. In the sentence George
Bush traveled to France on Thursday for a summit.,
the value of this feature would be George Bush .
Value Predicate Description
zi = r Z(i, r) Latent relation type
fk = v F(k, v) feature of relation tuple
pi = i P(i, fk) tuple i contains feature fk
di = j D(i, j) observed document
Table 1: Logical variables for Relational LDA. The vari-
able i ranges over tuples in the corpus (i = [1 . . . N ]),
and k over features in the corpus (k = [1 . . . F ]).
DEST The feature corresponds to the second entity
mention and its value would be France in the previ-
ous example.
NEPAIR The feature indicates the type and order
of two entity mentions in the tuple. This would
be PER-ORG in our example.
PATH This feature refers to the dependency
path between two entity mentions. In our
sentence, the value of the feature would be
PATH:?nsubj?traveled?prep?to?pobj?.
TRIGGER Finally, trigger features are content
words occurring in the dependency path. The path
PATH:?nsubj?traveled?prep?to?pobj? con-
tains only one trigger word, namely traveled. The
intuition behind this feature is that paths sharing the
same set of trigger words should be grouped in the
same cluster.
4.2 First Order Logic and Relational LDA
We next couple relational LDA with global con-
straints, which we express using FOL rules. We
begin by representing relational LDA as a Markov
Logic Network (Richardson and Domingos, 2006).
We define a logical predicate for each model vari-
able. For example, assigned relation variable
(Z(i, r)) is true if zi = r and false otherwise. Table 1
shows the mapping of model variables onto logical
predicates. Logical rules are encoded in the form of
a weighted FOL knowledge base (KB) which is then
converted into Conjunctive Normal form:
KB = {(?1, ?1), ..., (?L, ?L)} (2)
The KB consists of L pairs, where each ?l rep-
resents a FOL rule and ?l ? 0 its weight. Rules
are soft preferences rather than hard constraints;
the weights represent the importance of ?l and are
418
set manually by the domain expert. The KB is
tied to the probabilistic model via its groundings
in the corpus. For each FOL rule ?l, let G(?l) be
the set of groundings, each mapping the free vari-
ables in ?l to a specific value. For example, in the
rule ?i, j, p : F(i, Obama) ? F(j,WhiteHouse) ?
P(p, i) ? P(p, j) ? Z(p, r)1, G consists of all the
rules where the free variables i, j and p are instanti-
ated. At grounding time, we parse the corpus search-
ing for the tuples that satisfy the logic rules and store
the indices of the tuples that ground the rule. The
stored indices are used to set ?l to a specific value.
For the (Obama, White House) example above, G
consists of F propositional rules for each observed
feature, where i ? [1 . . . F ]. For each grounding
(g ? G(?l)) we define an indicator function:
1g(z,p,d,o) =
?
??
??
1, if g is true under
z and p,d,o
0, otherwise
(3)
where z are relation assignments to tuples, p is the
set of features in tuples, d are documents, and o
the side information encoded in FOL. Contrary to
Andrzejewski et al (2011), we need to ground the
rules while taking into account if the feature speci-
fied in the rule is expressed by any tuple or the spe-
cific given tuple, since we are assigning relations to
tuples, and not directly to words.
Next, we define a Markov Random Field (MRF)
which combines relational LDA with the FOL
knowledge base. The MRF is defined over latent
relation tuple assignments z, relation feature multi-
nomials ?, and relation document multinomials ?
(the feature set, document, and external informa-
tion o are observed). Under this model the con-
ditional probability P (z, ?, ?|?, ?,p,d,o,KB) is
proportional to:
exp
?
?
L?
l
?
g?G(?l)
?l1g(z,p,d,o)
?
??
R?
r
p(?r|?)
D?
j
p(?j |?)
N?
i
?di(zi)
?
k?pi
?zi(fk)
(4)
The first term in Equation (4) corresponds to the
logic factor in Figure 1 that groups variables d, z,
1This rule translates as ?every tuple containing Obama and
White House as features should be in relation cluster r?.
f1, f2, . . . fk and o. The remaining terms in Equa-
tion (4) refer to relational LDA. The goal of the
model is to estimate the most likely ? and ? for the
given observed state. As z can not be marginalized
out, we proceed with MAP estimation of (z, ?, ?),
maximizing the log of the probability as in Andrze-
jewski et al (2011):
argmax
z,?,?
L?
l
?
g?G(?l)
?l1g(z,p,d,o)+
R?
r
log p(?r|?)+
N?
i
log ?di(zi)
?
k?pi
?zi(fk)
(5)
Once the parameters of the model are estimated
(see Section 4.3 for details), we use the ? proba-
bility distribution to assign a relation to a new test
tuple. We select the relation that maximizes the
probability argmaxr
?k
i P (fi|?r) where f1 . . . fk
are features representing the tuple and r the relation
index.
4.3 Inference
Exact inference is intractable for both relational
LDA and MLN models. In order to infer the most
likely multinomial parameters ? and ?, we applied
the Alternating Optimization with Mirror Descent
algorithm introduced in Andrzejewski et al (2011).
The algorithm alternates between optimizing the
multinomial parameters (?, ?), whilst holding the re-
lation assignments (z) fixed, and vice-versa. At each
iteration, the algorithm first finds the optimal (?, ?)
for a fixed z as the MAP estimate of the Dirichlet
posterior:
?r(f) ? nrf + ? ? 1 (6)
?j(r) ? njr + ?? 1 (7)
where nrf is the number of times feature f is
assigned to relation r in relation assignments z,
and njr is the number of times relation r is assigned
to document j. Next, z is optimized while keeping ?
and ? fixed. This step is divided into two parts. The
algorithm first deals with all zi which appear only in
trivial groundings, i.e., groundings whose indicator
functions 1g are not affected by the latent relation
assignment z. As zi only appears in the last term of
419
Equation (5), the algorithm needs only optimize the
following term:
zi = argmax
r=1...R
?di(r)
?
k?pi
?zi(fk) (8)
The second part deals with the remaining zi that ap-
pear in non-trivial groundings in the first term of
Equation (5). We follow Andrzejewski et al (2011)
in relaxing (5) into a continuous optimization prob-
lem and refer the reader to their paper for a more
in depth treatment. Suffice it to say that once the
binary variables zir ? {0, 1} are relaxed to contin-
uous values zir ? [0, 1], it is possible to introduce
the relational LDA term in the equation and com-
pute the gradient using the Entropic Mirror Descent
Algorithm (Beck and Teboulle, 2003):
argmax
z?[0,1]|KB|
L?
l
?
g?G(?l)
?l1g(z)+
?
i,r
zir log ?di(r)
?
k?pi
?zi(fk)
s.t zir ? 0 ,
?
i,r
zir = 1
(9)
In every iteration the approximation algorithm
randomly samples a term from the objective func-
tion (Equation (9)). The sampled term can be
a particular ground rule g or the relational LDA
term (
?
r zir log ?di(r)
?
k?pi
?zi(fk)) for some
uniformly sampled index i. The sampling of the
terms is weighted according to the rule weight (?l)
and the grounded value (G(?l)) in the case of logic
rules, and the size of corpus in tuples (|zKB|) for re-
lational LDA. Once we choose term f and take the
gradient, we can apply the Entropic Mirror Descent
update:
zir ?
zir exp(?Ozirf)?
r? zir? exp(?Ozir?f)
(10)
Finally, zi is recovered by rounding to argmaxr zir.
The main advantage of this approach is that it re-
quires only a means to sample groundings g for each
rule?l, and can avoid fully grounding the FOL rules.
4.4 Logic Rules
Our model assigns relations to tuples rather than top-
ics to words. Since our tuples are described in terms
of features our logic rules must reflect this too. For
our experiments we defined two very general types
of rules described below.
Must-link Tuple The motivation behind this rule
is that tuples which share features probably express
the same underlying relation. The rule must spec-
ify which feature has to be shared for the tuples
to be clustered together. For example, the rule be-
low states that tuples containing the dependency
path PATH:?appos?president?prep?of?pobj?
should go in the same cluster:
?i, j, k : F(i, PATH:is the president of ) ? P(j, fi)
?P(k, fi)? ?Z(j, t) ? Z(k, r)
Cannot-link Tuple We also define rules prohibit-
ing tuples to be clustered together because they do
not share any features. For example, tuples with
ORG-LOC entities, probably express a Location re-
lation and should not be clustered together with
PER-PER tuples, which in all likelihood express a
different relationship (e.g., Family). The rule below
expresses this constraint:
?i, j, k, l : F(i, NEPAIR:PER-PER)
?F(j, NEPAIR:ORG-LOC)
?P(k, fi) ? P(l, fj)? ?Z(k, r) ? ?Z(l, r)
The specification of the first order logic rules is
an integral part of the model. The rules express
knowledge about the task at hand, the domain in-
volved, and the way the relation extraction problem
is modeled (i.e., tuples expressed as features). So
far, we have abstractly formulated the rules without
explaining how they are specifically instantiated in
our model. We could write them down by hand after
inspecting some data or through consultation with a
domain expert. Instead, we obtain logic rules au-
tomatically from a corpus following the procedure
described in Section 5.
5 Experimental Setup
Data We trained our model on the New York
Times (years 2000?2007) corpus created by Yao et
al. (2011). The corpus contains approximately 2M
entity tuples. The latter were extracted from
428K documents. After post-processing (tokeniza-
tion, sentence-splitting, and part-of-speech tagging),
420
Must-link Tuple
F(i, NEPAIR:PER-PER, TRIGGER:wife) ? P(j, fi) ? P(k, fi)? ?Z(j, t) ? Z(k, r)
F(i, NEPAIR:PER-LOC, TRIGGER:die) ? P(j, fi) ? P(k, fi)? ?Z(j, t) ? Z(k, r)
F(i, PATH:?nsubj?die?prep?in?pobj?) ? P(j, fi) ? P(k, fi)? ?Z(j, t) ? Z(k, r)
F(i, SOURCE:Kobe, DEST:Lakers) ? P(j, fi) ? P(k, fi)? ?Z(j, t) ? Z(k, r)
Cannot-link Tuple
F(i, NEPAIR:ORG-LOC) ? F(j, NEPAIR:PER-PER) ? P(k, fi) ? P(l, fj)? ?Z(k, r) ? ?Z(l, r)
F(i, NEPAIR:LOC-LOC) ? F(j, TRIGGER:president) ? P(k, fi) ? P(l, fj)? ?Z(k, r) ? ?Z(l, r)
F(i, NEPAIR:PER-LOC) ? F(j, TRIGER:member) ? P(k, fi) ? P(l, fj)? ?Z(k, r) ? ?Z(l, r)
F(i, NEPAIR:PER-PER) ? F(j, TRIGER:sell) ? P(k, fi) ? P(l, fj)? ?Z(k, r) ? ?Z(l, r)
Table 2: Examples of automatically extracted Must-link and Cannot-link tuple rules.
named entities were automatically recognized and
labeled with PER, ORG, LOC, and MISC (Finkel
et al, 2005). Dependency paths for each pair of
named entity mentions were extracted from the out-
put of the MaltParser (Nivre et al, 2004). In our
experiments, we discarded tuples with paths longer
than 10 edges (Lin and Pantel, 2001). We evalu-
ated our model on the test partition of the ACE 2007
(English) RDC dataset which is labeled with gold
standard entity mentions and their relations. There
are six general relation types and 18 subtypes. We
used 25% of the ACE training partition as a devel-
opment set for parameter tuning.
Logic Rule Extraction We automatically ex-
tracted logic rules from the New York Times
(NYT) corpus as follows. The intuition behind
Must-link rules is that tuples with common features
should cluster together. Although we do not know
which features would yield the best rules, we
naively assume that good features are frequently
co-occurring features. Using the log-likelihood
ratio (Dunning, 1993), we first discarded low
confidence feature co-occurrences (p < 0.05). Two
features co-occur if they are both found within
the same sentence. We then sorted the remaining
co-occurrences by their frequency and retained the
N -best ones. We only considered unigram and
bigram features since higher-order ones tend to
be sparse. An example of a bigram feature would
be (PATH:?nsubj?grow?prep?in?pobj?,
DEST:Chicago).
The main intuition behind Cannot-link rules is
that tuples without any common features should
not cluster together. So, if two features never
co-occur, they probably express different relations.
For every unigram and bigram feature in the re-
spective N -best list, we find the features it does
not co-occur with in the NYT corpus. For ex-
ample, NEPAIR:PER?LOC does not co-occur with
DEST:Yankees and the bigram DEST:United Na-
tions, NEPAIR:PER?ORG does not co-occur with
SOURCE:Mr. Bush, NEPAIR:PER?LOC. Cannot-
link rules are then based on such non-co-occurring
feature pairs.
We optimized N empirically on the development
set. We experimented with values ranging from 20
to 500. We obtained 20 Must-link rules for coarse-
grained relations and 400 rules for their subtypes.
We extracted 1,814 Cannot-link rules for general re-
lations (N = 50) and 34,522 rules for subtypes
(N = 400). The number of features involved in the
Must-link rules was 25 for coarse-grained relations
and 422 for fine-grained relations. For Cannot-link
rules, 62 features were involved in coarse-grained
relations and 422 in fine-grained relations.
Examples of the rules we extracted are shown in
Table 2. The first rule in the upper half of the ta-
ble states that tuples must cluster together if their
source and target entities are PER and contain the
trigger word wife in their dependency path. The sec-
ond rule is similar, the source entity here is PER,
the target LOC and the trigger word is die. Ac-
cording to the third rule, tuples featuring the path
PATH:?nsubj?die?prep?in?pobj? should be
in the same cluster. The fourth rule forces tuples
whose source entity is Kobe and target entity is Lak-
ers to cluster together. The second half of the table
illustrates Cannot-link tuple rules. The first rule pre-
vents tuples with ORG-LOC entities to cluster to-
421
gether with PER-PER tuples. The second rule states
that we cannot link LOC-LOC tuples with those
whose trigger word is president, and so on.
Parameter Tuning Our framework has several
parameters that must be adjusted for an optimal clus-
tering solution. These include the hyperparame-
ters ? and ? as well as the number of clusters. In
addition, we have to assign a weight to each FOL
rule grounding. An exhaustive search on the hy-
perparameters and rule weights is not possible. We
therefore followed a step-wise approximation proce-
dure. First, we find the best ? and ? values, whilst
varying the number of clusters. Once we have the
best hyperparameters for each clustering, we set the
weights for the FOL rules. We varied the number
of relations from 5 to 50. We experimented with ?
values in the range of [0.05 ? 0.5] and ? values in
the range of [0.05 ? 0.5]. These values were opti-
mized separately for coarse- and fine-grained rela-
tions. Table 3 shows the optimal number of clusters
for different model variants and relation types.
The FOL weights can also make a difference in
the final output; the bigger the weight the more
times the rule will be sampled in the Mirror Descent
algorithm. We experimented with two weighting
schemes: (a) we gave a weight of 1 or 0.5 to each
rule grounding and (b) we scaled the weights so as
to make their contribution comparable to relational
LDA. We obtained best results on the development
set with the former scheme.
Baselines We compared our FOL relational LDA
model against standard LDA (Blei et al, 2003) and
relational LDA without the FOL component. In the
case of standard LDA, we estimated topics (rela-
tions) over words, and used the context of the en-
tity mentions pairs as a bag of words feature to se-
lect the most likely cluster at test time. Parameters
for LDA and relational LDA were optimized follow-
ing the same parameter tuning procedure described
above.
We also compared our model against the unsuper-
vised method introduced in Hasegawa et al (2004).
Their key idea is to cluster pairs of co-occurring
named entities according to the similarity of their
surrounding contexts. Following their approach, we
measured context similarity using the vector space
model and the cosine metric and grouped NE pairs
into clusters using a complete linkage hierarchical
clustering algorithm. We adopted the same parame-
ter values as detailed in their paper (e.g., cosine sim-
ilarity threshold, length of context vectors). At test
time, instances were assigned to the relation cluster
most similar to them (according to the cosine mea-
sure).
Evaluation We evaluated the clusters obtained by
our model and the comparison systems using the Fs-
core measure introduced in the SemEval 2007 task
(Agirre and Soroa, 2007); it is the harmonic mean
of precision and recall defined as the number of cor-
rect members of a cluster divided by the number of
items in the cluster and the number of items in the
gold-standard class, respectively.
6 Results
Our results are summarized in Table 3 which reports
Fscore for (Hasegawa et al, 2004), LDA, relational
LDA (RelLDA), and our model with the FOL com-
ponent. To assess the impact of the rules on the
clustering, we conducted several rule ablation stud-
ies. We thus present results with a model that in-
cludes both Must-link and Cannot-link tuple rules
(CLT+MLT), and models that include either Must-
link (MLT) or Cannot-link (CLT) rules but not both.
We show the performance of these models with the
entire feature set (see (ALL) in the table) and with a
subset consisting solely of NE pair related features
(see (NEPAIR) in the table). We report results against
coarse- and fine-grained relations (6 and 18 relation
types in ACE, respectively). The table shows the
optimal number of relation clusters (in parentheses)
per model and relation type.
We also wanted to examine the quality of the logic
rules. Recall that we learn these heuristically from
the NYT corpus. We thus trained an additional vari-
ant of our model with rules extracted from the ACE
training set (75%) which contains relation annota-
tions. The extraction procedure was similar to the
unsupervised case, save that the relation types were
known and thus informative features could be mined
more reliably. For Must-link rules, we extracted un-
igram and bigram feature frequencies for each re-
lation type and applied TF-IDF weighting in order
to discover the most discriminative ones. We cre-
ated logic rules for the 10 best feature combinations
in each relation type. Regarding Cannot-link rules,
we enumerated the features (unigrams and bigrams)
422
Model Subtype Type
HASEGAWA 26.1 (12) 34.7 (12)
LDA 23.4 (10) 29.0 (5)
RelLDA 30.4 (40) 38.6 (5)
U-MLT (ALL) 36.6 (10) 48.0 (5)
U-CLT (ALL) 30.5 (5) 39.3 (5)
U-CLT+MLT (ALL) 29.8 (5) 42.0 (5)
U-MLT (NEPAIR) 36.5 (10) 47.2 (5)
U-CLT (NEPAIR) 28.8 (50) 40.5 (5)
U-CLT+MLT (NEPAIR) 30.9 (10) 41.5 (5)
S-MLT (ALL) 37.0 (10) 47.0 (5)
S-CLT (ALL) 31.4 (50) 40.9 (5)
S-CLT+MLT (ALL) 32.3 (10) 42.5 (5)
S-MLT (NEPAIR) 37.0 (10) 47.6 (10)
S-CLT (NEPAIR) 31.4 (10) 40.1 (5)
S-CLT+MLT (NEPAIR) 37.1 (10) 46.0 (5)
Table 3: Model performance on the ACE 2007 test set
using Fscore. Results are shown for six main relation
types and their subtypes (18 in total). (ALL) models con-
tain rules extracted from the entire feature set. For (NE-
PAIR) models, rules were extracted from NEPAIR-related
features only. Prefix U- denotes models that use unsu-
pervised rules; prefix S- highlights models using super-
vised rules. The optimal number of relations per model
is shown in parentheses.
that did not co-occur in any relation type and applied
TF-IDF weighting. Again, we created rules for the
10 most discriminative features. We defined rules
over the entire feature set (466 Must-link and 26,074
Cannot-link rules) and a subset containing only NE
pairs. In Table 3, prefixes S- and U- indicate model
variants with supervised and unsupervised rules, re-
spectively.
Our results show that standard LDA is not suit-
able for relation extraction. The obtained clusters
are not informative enough to induce semantic re-
lations, whereas RelLDA yields substantially bet-
ter Fscores. This is not entirely surprising, given
that RelLDA is a relation extraction specific model.
Hasegawa et al?s (2004) model lies somewhere in
the middle between LDA and RelLDA. The com-
bination of RelLDA with automatically extracted
FOL rules improves over RelLDA across the board
(see the U- models in Table 3). MLT rules deliver
the largest improvement for both coarse and fine-
grained relation types. In general, CLT models per-
form worse as well as models using both types of
rules (MLT+CLT). The inferior performance of the
rule combination may be due to the fact that MLT
and CLT rules contain conflicting information and
to a certain extent cancel each other out. The use
of many rules might also negatively impact infer-
ence, i.e., discriminative rules are sampled less and
cannot influence the model towards a better solu-
tion. Restricting the number of features and rules
to named entity pairs only incurs a negligible drop
in performance. This is good news for scaling pur-
poses, since a small number of rules can greatly
speed-up inference. Interestingly, model variants
which use supervised FOL rules (see the prefix S-
in Table 3) perform on par with unsupervised mod-
els. Again, MLT rules perform best in the super-
vised case, whereas CLT rules marginally improve
over RelLDA.
We assessed whether differences in performance
are statistically significant (p < 0.05) using boot-
strap resampling (Noreen, 1989). All models across
all relation types are significantly better than LDA
and Hasegawa et al (2004). FOL-based models per-
form significantly better than RelLDA, with the ex-
ception of all CLT models and U-CLT+MLT (ALL).
MLT models are significantly better than any other
rule-based model, except those that only use NE-
PAIR features. We also measured whether differ-
ent models agree on their topic assignments using
Cohen?s Kappa.2 RelLDA agrees least with MLT
models and most with CLT models (i.e., ? = 0.50
for U-MLT (ALL) and ? = 0.65 for U-CLT (ALL)).
This suggests that the CLT rules do not affect the
output of RelLDA as much as MLT ones. Examples
of relation clusters discovered by the U-MLT (ALL)
model are shown in Table 4.
A last note on parameter selection. Our experi-
ments explored the parameter space extensively in
order to examine any interactions between the in-
duced relations and the logic rules. For most model
variants inferring subtype relations, the preferred
number of clusters is 10. For coarse-grained rela-
tions, the optimal number of clusters is five. Over-
all, we found that the quality of the output is highly
correlated with the quality of the logic rules and that
a few good rules are more important than the opti-
mal number of clusters. We consider these findings
robust enough to apply across domains and datasets.
2For all comparison models the number of relation clusters
was set to 10.
423
SOURCE PATH DEST
Republican president of Senate
Senate director of Yankees
House professor at Republican
Bush chairman of Congress
Democrat spokesman for House
Mr. Bush executive of Mets
Democrats director at U. of California
Republican analyst at United Nations
E
m
pl
oy
m
en
t
SOURCE PATH DEST
Yankees defeat World Series
Mets win Olympic
United States beat World Cup
Giants play Yankees
Jets win Super Bowl
Nets lose Olympics
Knicks sign Mets
Rangers victory over Giants
Sp
or
ts
Table 4: Clusters discovered by the U-MLT (ALL) model
indicating employment- and sports-type relations. For the
sake of readability, we do not display the syntactic depen-
dencies between words in a path.
7 Conclusions
In this paper we presented a new model for unsu-
pervised relation extraction which operates over tu-
ples representing a syntactic relationship between
two named entities. Our model clusters such tuples
into underlying semantic relations (e.g., Located,
Family) by incorporating general domain knowledge
which we encode as First Order Logic rules. Specif-
ically, we combine a topic model developed for the
relation extraction task with domain relevant rules,
and present an algorithm for estimating the param-
eters of this model. Evaluation results on the ACE
2007 (English) RDC task show that our model out-
performs competitive unsupervised approaches by a
wide margin and is able to produce clusters shaped
by both the data and the rules.
In the future, we would like to explore additional
types of rules such as seed rules, which would as-
sign tuples complying with the ?seed? information
to distinct relations. Aside from devising new rule
types, an obvious next step would be to explore dif-
ferent ways of extracting the rule set based on differ-
ent criteria (e.g., the most general versus most spe-
cific rules). Also note that in the current framework
rule weights are set manually by the domain expert.
An appealing direction would be to learn these auto-
matically e.g., via a procedure that optimizes some
clustering objective. Finally, it should be interesting
to use some form of distant supervision (Mintz et al,
2009) either as a means of obtaining useful rules or
to discard potentially noisy or uninformative rules.
Acknowledgments
We gratefully acknowledge financial support from
the Department of Education, Universities and Re-
search of the Basque Government (BFI-2011-442).
We also thank Limin Yao and Sebastian Riedel for
sharing their corpus with us and the members of the
Probabilistic Models reading group at the University
of Edinburgh for helpful feedback.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the 5th ACM International Confer-
ence on Digital Libraries, pages 85?94, San Antonio,
Texas.
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
task 02: Evaluating word sense induction and discrim-
ination systems. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, pages 7?12,
Prague, Czech Republic.
David Andrzejewski, Xiaojin Zhu, Mark Craven, and Ben
Recht. 2011. A framework for incorporating general
domain knowledge into latent Dirichlet alocation us-
ing first-order logic. In Proceedings of the 22nd In-
ternational Joint Conference on Artificial Intelligence,
pages 1171?1177, Barcelona, Spain.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Proceedings
of the 20th International Joint Conference on Artificial
Intelligence, pages 2670?2676, Hyderabad, India.
Amir Beck and Marc Teboulle. 2003. Mirror de-
scent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters,
31(3):167?175.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal su-
pervision. In Proceedings of the 45th Annual Meeting
424
of the Association of Computational Linguistics, pages
576?583, Prague, Czech Republic.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Meeting of the Association for Computational
Linguistics, Main Volume, pages 423?429, Barcelona,
Spain.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
363?370, Ann Arbor, Michigan.
David Gondek and Thomas Hofmann. 2004. Non-
redundant data clustering. In IEEE International Con-
ference on Data Mining, pages 75?82. IEEE Computer
Society.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(1):5228?5235.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics, pages 415?422, Barcelona, Spain.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discovery
of inference rules from text. In Proceedings of the 7th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 323?328, San
Francisco, California.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 1003?1011,
Suntec, Singapore.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of the 8th Conference on Computational Natural Lan-
guage Learning, pages 49?56, Boston, Massachusetts.
Eric W. Noreen. 1989. Computer-intensive Methods for
Testing Hypotheses: An Introduction. John Wiley and
Sons Inc.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 113?120, Sydney, Aus-
tralia.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10, Suntec, Singapore.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62(1?2):107?136.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction. In Proceedings of the
16th International Joint Conference on Artificial Intel-
ligence, pages 474?479, Stockholm, Sweden.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel Weld. 2010. Learning first-order Horn clauses
from web text. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1088?1098, Cambridge, MA, October. As-
sociation for Computational Linguistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference,
pages 304?311, New York City, USA.
Mihai Surdeanu and Massimiliano Ciaramita. 2007. Ro-
bust information extration with perceptrons. In Pro-
ceedings of the NIST 2007 Automatic Content Extrac-
tion Workshop.
Kiri Wagstaff, Claire Cardie, C Rogers, and S Schro?dl.
2001. Constrained k-means clustering with back-
ground knowledge. In International Conference on
Machine Learning, pages 577?584. Morgan Kauf-
mann.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1456?1466, Edinburgh, Scotland, UK.
GuoDong Zhou, Min Zhang, DongHong Ji, and QiaoM-
ing Zhu. 2007. Tree kernel-based relation extraction
with context-sensitive structured parse tree informa-
tion. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 728?736, Prague, Czech Republic.
425
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75?80,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 17: All-words Word Sense Disambiguation
on a Specific Domain
Eneko Agirre, Oier Lopez de Lacalle
IXA NLP group
UBC
Donostia, Basque Country
{e.agirre,oier.lopezdelacalle}@ehu.es
Christiane Fellbaum
Department of Computer Science
Princeton University
Princeton, USA
fellbaum@princeton.edu
Shu-Kai Hsieh
Department of English
National Taiwan Normal University
Taipei, Taiwan
shukai@ntnu.edu.tw
Maurizio Tesconi
IIT
CNR
Pisa, Italy
maurizio.tesconi@iit.cnr.it
Monica Monachini
ILC
CNR
Pisa, Italy
monica.monachini@ilc.cnr.it
Piek Vossen, Roxanne Segers
Faculteit der Letteren
Vrije Universiteit Amsterdam
Amsterdam, Netherlands
p.vossen@let.vu.nl,roxane.segers@gmail.com
Abstract
Domain portability and adaptation of NLP
components and Word Sense Disambigua-
tion systems present new challenges. The
difficulties found by supervised systems to
adapt might change the way we assess the
strengths and weaknesses of supervised
and knowledge-based WSD systems. Un-
fortunately, all existing evaluation datasets
for specific domains are lexical-sample
corpora. This task presented all-words
datasets on the environment domain for
WSD in four languages (Chinese, Dutch,
English, Italian). 11 teams participated,
with supervised and knowledge-based sys-
tems, mainly in the English dataset. The
results show that in all languages the par-
ticipants where able to beat the most fre-
quent sense heuristic as estimated from
general corpora. The most successful ap-
proaches used some sort of supervision in
the form of hand-tagged examples from
the domain.
1 Introduction
Word Sense Disambiguation (WSD) competitions
have focused on general domain texts, as attested
in previous Senseval and SemEval competitions
(Kilgarriff, 2001; Mihalcea et al, 2004; Snyder
and Palmer, 2004; Pradhan et al, 2007). Spe-
cific domains pose fresh challenges to WSD sys-
tems: the context in which the senses occur might
change, different domains involve different sense
distributions and predominant senses, some words
tend to occur in fewer senses in specific domains,
the context of the senses might change, and new
senses and terms might be involved. Both super-
vised and knowledge-based systems are affected
by these issues: while the first suffer from differ-
ent context and sense priors, the later suffer from
lack of coverage of domain-related words and in-
formation.
The main goal of this task is to provide a mul-
tilingual testbed to evaluate WSD systems when
faced with full-texts from a specific domain. All
datasets and related information are publicly avail-
able from the task websites
1
.
This task was designed in the context of Ky-
oto (Vossen et al, 2008)
2
, an Asian-European
project that develops a community platform for
modeling knowledge and finding facts across lan-
guages and cultures. The platform operates as a
Wiki system with an ontological support that so-
cial communities can use to agree on the mean-
ing of terms in specific domains of their interest.
Kyoto focuses on the environmental domain be-
cause it poses interesting challenges for informa-
tion sharing, but the techniques and platforms are
1
http://xmlgroup.iit.cnr.it/SemEval2010/
and http://semeval2.fbk.eu/
2
http://www.kyoto-project.eu/
75
independent of the application domain.
The paper is structured as follows. We first
present the preparation of the data. Section 3 re-
views participant systems and Section 4 the re-
sults. Finally, Section 5 presents the conclusions.
2 Data preparation
The data made available to the participants in-
cluded the test set proper, and background texts.
Participants had one week to work on the test set,
but the background texts where provided months
earlier.
2.1 Test datasets
The WSD-domain comprises comparable all-
words test corpora on the environment domain.
Three texts were compiled for each language by
the European Center for Nature Conservation
3
and
Worldwide Wildlife Forum
4
. They are documents
written for a general but interested public and in-
volve specific terms from the domain. The docu-
ment content is comparable across languages. Ta-
ble 1 shows the numbers for the datasets.
Although the original plan was to annotate mul-
tiword terms, and domain terminology, due to time
constraints we focused on single-word nouns and
verbs. The test set clearly marked which were
the words to be annotated. In the case of Dutch,
we also marked components of single-word com-
pounds. The format of the test set followed that of
previous all-word exercises, which we extended to
accommodate Dutch compounds. For further de-
tails check the datasets in the task website.
The sense inventory was based on publicly
available wordnets of the respective languages
(see task website for details). The annotation pro-
cedure involved double-blind annotation by ex-
perts plus adjudication, which allowed us to also
provide Inter Annotator Agreement (IAA) figures
for the dataset. The procedure was carried out us-
ing KAFnotator tool (Tesconi et al, 2010). Due
to limitations in resources and time, the English
dataset was annotated by a single expert annota-
tor. For the rest of languages, the agreement was
very good, as reported in Table 1.
Table 1 includes the results of the random base-
line, as an indication of the polysemy in each
dataset. Average polysemy is highest for English,
and lowest for Dutch.
3
http://www.ecnc.org
4
http://www.wwf.org
Total Noun Verb IAA Random
Chinese 3989 754 450 0.96 0.321
Dutch 8157 997 635 0.90 0.328
English 5342 1032 366 n/a 0.232
Italian 8560 1340 513 0.72 0.294
Table 1: Dataset numbers, including number of
tokens, nouns and verbs to be tagged, Inter-
Annotator Agreement (IAA) and precision of ran-
dom baseline.
Documents Words
Chinese 58 455359
Dutch 98 21089
English 113 2737202
Italian 27 240158
Table 2: Size of the background data.
2.2 Background data
In addition to the test datasets proper, we also pro-
vided additional documents on related subjects,
kindly provided by ECNC and WWF. Table 2
shows the number of documents and words made
available for each language. The full list with the
urls of the documents are available from the task
website, together with the background documents.
3 Participants
Eleven participants submitted more than thirty
runs (cf. Table 3). The authors classified their runs
into supervised (S in the tables, three runs), weakly
supervised (WS, four runs), unsupervised (no runs)
and knowledge-based (KB, the rest of runs)
5
. Only
one group used hand-tagged data from the domain,
which they produced on their own. We will briefly
review each of the participant groups, ordered fol-
lowing the rank obtained for English. They all par-
ticipated on the English task, with one exception
as noted below, so we report their rank in the En-
glish task. Please refer to their respective paper in
these proceedings for more details.
CFILT: They participated with a domain-
specific knowledge-based method based on Hop-
field networks (Khapra et al, 2010). They first
identify domain-dependant words using the back-
ground texts, use a graph based on hyponyms in
WordNet, and a breadth-first search to select the
most representative synsets within domain. In ad-
dition they added manually disambiguated around
one hundred examples from the domain as seeds.
5
Note that boundaries are slippery. We show the classifi-
cations as reported by the authors.
76
English
Rank Participant System ID Type P R R nouns R verbs
1 Anup Kulkarni CFILT-2 WS 0.570 0.555 ?0.024 0.594 ?0.028 0.445 ?0.047
2 Anup Kulkarni CFILT-1 WS 0.554 0.540 ?0.021 0.580 ?0.025 0.426 ?0.043
3 Siva Reddy IIITH1-d.l.ppr.05 WS 0.534 0.528 ?0.027 0.553 ?0.023 0.456 ?0.041
4 Abhilash Inumella IIITH2-d.r.l.ppr.05 WS 0.522 0.516 ?0.023 0.529 ?0.027 0.478 ?0.041
5 Ruben Izquierdo BLC20SemcorBackground S 0.513 0.513 ?0.022 0.534 ?0.026 0.454 ?0.044
- - Most Frequent Sense - 0.505 0.505 ?0.023 0.519 ?0.026 0.464 ?0.043
6 Ruben Izquierdo BLC20Semcor S 0.505 0.505 ?0.025 0.527 ?0.031 0.443 ?0.045
7 Anup Kulkarni CFILT-3 KB 0.512 0.495 ?0.023 0.516 ?0.027 0.434 ?0.048
8 Andrew Tran Treematch KB 0.506 0.493 ?0.021 0.516 ?0.028 0.426 ?0.046
9 Andrew Tran Treematch-2 KB 0.504 0.491 ?0.021 0.515 ?0.030 0.425 ?0.044
10 Aitor Soroa kyoto-2 KB 0.481 0.481 ?0.022 0.487 ?0.025 0.462 ?0.039
11 Andrew Tran Treematch-3 KB 0.492 0.479 ?0.022 0.494 ?0.028 0.434 ?0.039
12 Radu Ion RACAI-MFS KB 0.461 0.460 ?0.022 0.458 ?0.025 0.464 ?0.046
13 Hansen A. Schwartz UCF-WS KB 0.447 0.441 ?0.022 0.440 ?0.025 0.445 ?0.043
14 Yuhang Guo HIT-CIR-DMFS-1.ans KB 0.436 0.435 ?0.023 0.428 ?0.027 0.454 ?0.043
15 Hansen A. Schwartz UCF-WS-domain KB 0.440 0.434 ?0.024 0.434 ?0.029 0.434 ?0.044
16 Abhilash Inumella IIITH2-d.r.l.baseline.05 KB 0.496 0.433 ?0.024 0.452 ?0.023 0.390 ?0.044
17 Siva Reddy IIITH1-d.l.baseline.05 KB 0.498 0.432 ?0.021 0.463 ?0.026 0.344 ?0.038
18 Radu Ion RACAI-2MFS KB 0.433 0.431 ?0.022 0.434 ?0.027 0.399 ?0.049
19 Siva Reddy IIITH1-d.l.ppv.05 KB 0.426 0.425 ?0.026 0.434 ?0.028 0.399 ?0.043
20 Abhilash Inumella IIITH2-d.r.l.ppv.05 KB 0.424 0.422 ?0.023 0.456 ?0.025 0.325 ?0.044
21 Hansen A. Schwartz UCF-WS-domain.noPropers KB 0.437 0.392 ?0.025 0.377 ?0.025 0.434 ?0.043
22 Aitor Soroa kyoto-1 KB 0.384 0.384 ?0.022 0.382 ?0.024 0.391 ?0.047
23 Ruben Izquierdo BLC20Background S 0.380 0.380 ?0.022 0.385 ?0.026 0.366 ?0.037
24 Davide Buscaldi NLEL-WSD-PDB WS 0.381 0.356 ?0.022 0.357 ?0.027 0.352 ?0.049
25 Radu Ion RACAI-Lexical-Chains KB 0.351 0.350 ?0.015 0.344 ?0.017 0.368 ?0.030
26 Davide Buscaldi NLEL-WSD WS 0.370 0.345 ?0.022 0.352 ?0.027 0.328 ?0.037
27 Yoan Gutierrez Relevant Semantic Trees KB 0.328 0.322 ?0.022 0.335 ?0.026 0.284 ?0.044
28 Yoan Gutierrez Relevant Semantic Trees-2 KB 0.321 0.315 ?0.022 0.327 ?0.024 0.281 ?0.040
29 Yoan Gutierrez Relevant Cliques KB 0.312 0.303 ?0.021 0.304 ?0.024 0.301 ?0.041
- - Random baseline - 0.232 0.232 0.253 0.172
Chinese
Rank Participant System ID Type P R R nouns R verbs
- - Most Frequent Sense - 0.562 0.562 ?0.026 0.589 ?0.027 0.518 ?0.039
1 Meng-Hsien Shih HR KB 0.559 0.559 ?0.024 0.615 ?0.026 0.464 ?0.039
2 Meng-Hsien Shih GHR KB 0.517 0.517 ?0.024 0.533 ?0.035 0.491 ?0.038
- - Random baseline - 0.321 0.321 0.326 0.312
4 Aitor Soroa kyoto-3 KB 0.322 0.296 ?0.022 0.257 ?0.027 0.360 ?0.038
3 Aitor Soroa kyoto-2 KB 0.342 0.285 ?0.021 0.251 ?0.026 0.342 ?0.040
5 Aitor Soroa kyoto-1 KB 0.310 0.258 ?0.023 0.256 ?0.029 0.261 ?0.031
Dutch
Rank Participant System ID Type P R R nouns R verbs
1 Aitor Soroa kyoto-3 KB 0.526 0.526 ?0.022 0.575 ?0.029 0.450 ?0.034
2 Aitor Soroa kyoto-2 KB 0.519 0.519 ?0.022 0.561 ?0.027 0.454 ?0.034
- - Most Frequent Sense - 0.480 0.480 ?0.022 0.600 ?0.027 0.291 ?0.025
3 Aitor Soroa kyoto-1 KB 0.465 0.465 ?0.021 0.505 ?0.026 0.403 ?0.033
- - Random baseline - 0.328 0.328 0.350 0.293
Italian
Rank Participant System ID Type P R R nouns R verbs
1 Aitor Soroa kyoto-3 KB 0.529 0.529 ?0.021 0.530 ?0.024 0.528 ?0.038
2 Aitor Soroa kyoto-2 KB 0.521 0.521 ?0.018 0.522 ?0.023 0.519 ?0.035
3 Aitor Soroa kyoto-1 KB 0.496 0.496 ?0.019 0.507 ?0.020 0.468 ?0.037
- - Most Frequent Sense - 0.462 0.462 ?0.020 0.472 ?0.024 0.437 ?0.035
- - Random baseline - 0.294 0.294 0.308 0.257
Table 3: Overall results for the domain WSD datasets, ordered by recall.
This is the only group using hand-tagged data
from the target domain. Their best run ranked 1st.
IIITTH: They presented a personalized PageR-
ank algorithm over a graph constructed from
WordNet similar to (Agirre and Soroa, 2009),
with two variants. In the first (IIITH1), the vertices
of the graph are initialized following the rank-
ing scores obtained from predominant senses as in
(McCarthy et al, 2007). In the second (IIITH2),
the graph is initialized with keyness values as in
77
0.3 0.35 0.4 0.45 0.5 0.55
Rel. Cliques
Rel. Sem. Trees-2
Rel. Sem. Trees
NLEL-WSD
RACAI-Lexical-Chains
NLEL-WSD-PDB
BLC20BG
Kyoto-1
UCF-WS-domain.noPropers
IIITH2-d.r.l.ppv.05
IIITH1-d.l.ppv.05
RACAI-2MFS-BOW
IIITH1-d.l.baseline.05
IIITH2-d.r.l.baseline.05
UCF-WS-domain
HIT-CIR-DMFS
UCF-WS
RACAI-MFS
Treematch-3
Kyoto-2
Treematch-2
Treematch
CFILT-3
BLC20SC
BLC20SCBG
IIITH2-d.l.ppr.05
IIITH1-d.l.ppr.05
CFILT-1
CFILT-2
MFS
Figure 1: Plot for all the systems which participated in English domain WSD. Each point correspond
to one system (denoted in axis Y) according each recall and confidence interval (axis X ). Systems are
ordered depending on their rank.
(Rayson and Garside, 2000). Some of the runs
use sense statistics from SemCor, and have been
classified as weakly supervised. They submitted a
total of six runs, with the best run ranking 3rd.
BLC20(SC/BG/SCBG): This system is super-
vised. A Support Vector Machine was trained us-
ing the usual set of features extracted from con-
text and the most frequent class of the target word.
Semantic class-based classifiers were built from
SemCor (Izquierdo et al, 2009), where the classes
were automatically obtained exploiting the struc-
tural properties of WordNet. Their best run ranked
5th.
Treematch: This system uses a knowledge-
based disambiguation method that requires a dic-
tionary and untagged text as input. A previously
developed system (Chen et al, 2009) was adapted
to handle domain specific WSD. They built a
domain-specific corpus using words mined from
relevant web sites (e.g. WWF and ECNC) as
seeds. Once parsed the corpus, the used the de-
pendency knowledge to build a nodeset that was
used for WSD. The background documents pro-
vided by the organizers were only used to test how
exhaustive the initial seeds were. Their best run
ranked 8th.
Kyoto: This system participated in all four
languages, with a free reimplementation of
the domain-specific knowledge-based method for
WSD presented in (Agirre et al, 2009). It
uses a module to construct a distributional the-
saurus, which was run on the background text, and
a disambiguation module based on Personalized
PageRank over wordnet graphs. Different Word-
Net were used as the LKB depending on the lan-
guage. Their best run ranked 10th. Note that this
team includes some of the organizers of the task.
A strict separation was kept, in order to keep the
test dataset hidden from the actual developers of
the system.
RACAI: This participant submitted three differ-
ent knowledge-based systems. In the first, they use
the mapping to domains of WordNet (version 2.0)
in order to constraint the domains of the content
words of the test text. In the second, they choose
among senses using lexical chains (Ion and Ste-
fanescu, 2009). The third system combines the
previous two. Their best system ranked 12th.
HIT-CIR: They presented a knowledge-based
system which estimates predominant sense from
raw test. The predominant senses were calculated
with the frequency information in the provided
background text, and automatically constructed
78
thesauri from bilingual parallel corpora. The sys-
tem ranked 14.
UCFWS: This knowledge-based WSD system
was based on an algorithm originally described in
(Schwartz and Gomez, 2008), in which selectors
are acquired from the Web via searching with lo-
cal context of a given word. The sense is cho-
sen based on the similarity or relatedness between
the senses of the target word and various types
of selectors. In some runs they include predom-
inant senses(McCarthy et al, 2007). The best run
ranked 13th.
NLEL-WSD(-PDB): The system used for the
participation is based on an ensemble of different
methods using fuzzy-Borda voting. A similar sys-
tem was proposed in SemEval-2007 task-7 (Bus-
caldi and Rosso, 2007). In this case, the com-
ponent method used where the following ones:
1) Most Frequent Sense from SemCor; 2) Con-
ceptual Density ; 3) Supervised Domain Relative
Entropy classifier based on WordNet Domains;
4) Supervised Bayesian classifier based on Word-
Net Domains probabilities; and 5) Unsupervised
Knownet-20 classifiers. The best run ranked 24th.
UMCC-DLSI (Relevant): The team submitted
three different runs using a knowledge-based sys-
tem. The first two runs use domain vectors and
the third is based on cliques, which measure how
much a concept is correlated to the sentence by
obtaining Relevant Semantic Trees. Their best run
ranked 27th.
(G)HR: They presented a Knowledge-based
WSD system, which make use of two heuristic
rules (Li et al, 1995). The system enriched the
Chinese WordNet by adding semantic relations for
English domain specific words (e.g. ecology, en-
vironment). When in-domain senses are not avail-
able, the system relies on the first sense in the Chi-
nese WordNet. In addition, they also use sense
definitions. They only participated in the Chinese
task, with their best system ranking 1st.
4 Results
The evaluation has been carried out using the stan-
dard Senseval/SemEval scorer scorer2 as in-
cluded in the trial dataset, which computes preci-
sion and recall. Table 3 shows the results in each
dataset. Note that the main evaluation measure is
recall (R). In addition we also report precision (P)
and the recall for nouns and verbs. Recall mea-
sures are accompanied by a 95% confidence in-
terval calculated using bootstrap resampling pro-
cedure (Noreen, 1989). The difference between
two systems is deemed to be statistically signifi-
cant if there is no overlap between the confidence
intervals. We show graphically the results in Fig-
ure 1. For instance, the differences between the
highest scoring system and the following four sys-
tems are not statistically significant. Note that this
method of estimating statistical significance might
be more strict than other pairwise methods.
We also include the results of two baselines.
The random baseline was calculated analytically.
The first sense baseline for each language was
taken from each wordnet. The first sense baseline
in English and Chinese corresponds to the most
frequent sense, as estimated from out-of-domain
corpora. In Dutch and Italian, it followed the in-
tuitions of the lexicographer. Note that we don?t
have the most frequent sense baseline from the do-
main texts, which would surely show higher re-
sults (Koeling et al, 2005).
5 Conclusions
Domain portability and adaptation of NLP com-
ponents and Word Sense Disambiguation systems
present new challenges. The difficulties found by
supervised systems to adapt might change the way
we assess the strengths and weaknesses of super-
vised and knowledge-based WSD systems. With
this paper we have motivated the creation of an
all-words test dataset for WSD on the environ-
ment domain in several languages, and presented
the overall design of this SemEval task.
One of the goals of the exercise was to show
that WSD systems could make use of unannotated
background corpora to adapt to the domain and
improve their results. Although it?s early to reach
hard conclusions, the results show that in each of
the datasets, knowledge-based systems are able to
improve their results using background text, and
in two datasets the adaptation of knowledge-based
systems leads to results over the MFS baseline.
The evidence of domain adaptation of supervised
systems is weaker, as only one team tried, and the
differences with respect to MFS are very small.
The best results for English are obtained by a sys-
tem that combines a knowledge-based system with
some targeted hand-tagging. Regarding the tech-
niques used, graph-based methods over WordNet
and distributional thesaurus acquisition methods
have been used by several teams.
79
All datasets and related information are publicly
available from the task websites
6
.
Acknowledgments
We thank the collaboration of Lawrence Jones-Walters, Amor
Torre-Marin (ECNC) and Karin de Boom (WWF), com-
piling the test and background documents. This work
task is partially funded by the European Commission (KY-
OTO ICT-2007-211423), the Spanish Research Department
(KNOW-2 TIN2009-14715-C04-01) and the Basque Govern-
ment (BERBATEK IE09-262).
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proceedings of the
12th Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL09), pages 33?
41. Association for Computational Linguistics.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2009.
Knowledge-based wsd on specific domains: Performing
better than generic supervised wsd. In Proceedigns of IJ-
CAI. pp. 1501-1506.?.
Davide Buscaldi and Paolo Rosso. 2007. Upv-wsd : Com-
bining different wsd methods by means of fuzzy borda
voting. In Proceedings of the Fourth International Work-
shop on Semantic Evaluations (SemEval-2007), pages
434?437.
P. Chen, W. Ding, and D. Brown. 2009. A fully unsupervised
word sense disambiguation method and its evaluation on
coarse-grained all-words task. In Proceeding of the North
American Chapter of the Association for Computational
Linguistics (NAACL09).
Radu Ion and Dan Stefanescu. 2009. Unsupervised word
sense disambiguation with lexical chains and graph-based
context formalization. In Proceedings of the 4th Language
and Technology Conference: Human Language Technolo-
gies as a Challenge for Computer Science and Linguistics,
pages 190?194.
Rub?en Izquierdo, Armando Su?arez, and German Rigau.
2009. An empirical study on class-based word sense dis-
ambiguation. In EACL ?09: Proceedings of the 12th Con-
ference of the European Chapter of the Association for
Computational Linguistics, pages 389?397, Morristown,
NJ, USA. Association for Computational Linguistics.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak
Bhattacharyya. 2010. Domain-specific word sense dis-
ambiguation combining corpus based and wordnet based
parameters. In Proceedings of the 5th International Con-
ference on Global Wordnet (GWC2010).
A. Kilgarriff. 2001. English Lexical Sample Task Descrip-
tion. In Proceedings of the Second International Work-
shop on evaluating Word Sense Disambiguation Systems,
Toulouse, France.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense acqui-
sition. In Proceedings of the Human Language Technol-
ogy Conference and Conference on Empirical Methods in
6
http://xmlgroup.iit.cnr.it/SemEval2010/
and http://semeval2.fbk.eu/
Natural Language Processing. HLT/EMNLP, pages 419?
426, Ann Arbor, Michigan.
Xiaobin Li, Stan Szpakowicz, and Stan Matwin. 1995. A
wordnet-based algorithm for word sense disambiguation.
In Proceedings of The 14th International Joint Conference
on Artificial Intelligence (IJCAI95).
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2007. Unsupervised acquisition of predominant
word senses. Computational Linguistics, 33(4).
R. Mihalcea, T. Chklovski, and Adam Killgariff. 2004. The
Senseval-3 English lexical sample task. In Proceedings of
the 3rd ACL workshop on the Evaluation of Systems for the
Semantic Analysis of Text (SENSEVAL), Barcelona, Spain.
Eric W. Noreen. 1989. Computer-Intensive Methods for Test-
ing Hypotheses. John Wiley & Sons.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: English
lexical sample, srl and all words. In Proceedings of the
Fourth International Workshop on Semantic Evaluations
(SemEval-2007), pages 87?92, Prague, Czech Republic.
Paul Rayson and Roger Garside. 2000. Comparing corpora
using frequency profiling. In Proceedings of the workshop
on Comparing corpora, pages 1?6.
Hansen A. Schwartz and Fernando Gomez. 2008. Acquir-
ing knowledge from the web to be used as selectors for
noun sense disambiguation. In Proceedings of the Twelfth
Conference on Computational Natural Language Learn-
ing (CONLL08).
B. Snyder and M. Palmer. 2004. The English all-words task.
In Proceedings of the 3rd ACL workshop on the Evalua-
tion of Systems for the Semantic Analysis of Text (SENSE-
VAL), Barcelona, Spain.
M. Tesconi, F. Ronzano, S. Minutoli, C. Aliprandi, and
A. Marchetti. 2010. Kafnotator: a multilingual seman-
tic text annotation tool. In In Proceedings of the Second
International Conference on Global Interoperability for
Language Resources.
Piek Vossen, Eneko Agirre, Nicoletta Calzolari, Christiane
Fellbaum, Shu kai Hsieh, Chu-Ren Huang, Hitoshi Isa-
hara, Kyoko Kanzaki, Andrea Marchetti, Monica Mona-
chini, Federico Neri, Remo Raffaelli, German Rigau,
Maurizio Tescon, and Joop VanGent. 2008. Kyoto: a
system for mining, structuring and distributing knowl-
edge across languages and cultures. In Proceedings of the
Sixth International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may. European Lan-
guage Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
80
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 417?420,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
Kyoto: An Integrated System for Specific Domain WSD
Aitor Soroa, Eneko Agirre, Oier Lopez de Lacalle
University of the Basque Country
a.soroa@ehu.es
Monica Monachini
Istituto di Linguistica Computazionale
monica.monachini@ilc.cnr.it
Jessie Lo, Shu-Kai Hsieh
National Taiwan Normal University
shukai@ntnu.edu.tw
Wauter Bosma, Piek Vossen
Vrije Universiteit
{p.vossen,w.bosma}@let.vu.nl
Abstract
This document describes the prelimi-
nary release of the integrated Kyoto sys-
tem for specific domain WSD. The sys-
tem uses concept miners (Tybots) to ex-
tract domain-related terms and produces
a domain-related thesaurus, followed by
knowledge-based WSD based on word-
net graphs (UKB). The resulting system
can be applied to any language with a
lexical knowledge base, and is based on
publicly available software and resources.
Our participation in Semeval task #17 fo-
cused on producing running systems for
all languages in the task, and we attained
good results in all except Chinese. Due
to the pressure of the time-constraints in
the competition, the system is still under
development, and we expect results to im-
prove in the near future.
1 Introduction
In this paper we describe the participation of the
integrated Kyoto system on the ?SemEval-2010
task #17: All-words Word Sense Disambigua-
tion on a Specific Domain? task (Agirre et al,
2010). The goal of our participation was to eval-
uate the preliminary release of the integrated sys-
tem for specific domain WSD developed for the
Kyoto project
1
. Besides, we wanted to test the
performance of our domain specific WSD system
(Agirre et al, 2009) on this test set, and to inte-
grate the thesaurus construction software (Tybots)
developed for the project. The system can be run
for any language and domain if provided with a
lexical knowledge base and some background doc-
uments on the domain.
We will first present the components of our sys-
tem, followed by the experimental design and the
1
http://www.kyoto-project.eu
results. Finally, the conclusions are presented.
2 The Kyoto System for Domain Specific
WSD
We will present in turn UKB, the Tybots, and the
lexical knowledge-bases used.
2.1 UKB
UKB is a knowledge-based unsupervised WSD
system which exploits the structure of an under-
lying Language Knowledge Base (LKB) and finds
the most relevant concepts given an input con-
text (Agirre and Soroa, 2009). UKB starts by tak-
ing the LKB as a graph of concepts G = (V,E)
with a set of vertices V derived from LKB con-
cepts and a set of edges E representing relations
among them. Giving an input context, UKB ap-
plies the so called Personalized PageRank (Haveli-
wala, 2002) over it to obtain the most representa-
tive senses for the context.
PageRank (Brin and Page, 1998) is a method
for scoring the vertices V of a graph according
to each node?s structural importance. The algo-
rithm can be viewed as random walk process that
postulate the existence of a particle that randomly
traverses the graph, but at any time may jump to
a new vertex with a given damping factor (also
called teleport probability). After PageRank cal-
culation, the final weight of node i represents the
proportion of time that a random particle spends
visiting node i after a sufficiently long time. In
standard PageRank, the teleport vector is chosen
uniformly, whereas for Personalized PageRank it
is chosen from a nonuniform distribution of nodes,
specified by a teleport vector.
UKB concentrates the initial probability mass
of the teleport vector in the words occurring in
the context of the target word, causing all random
jumps on the walk to return to these words and
thus assigning a higher rank to the senses linked to
these words. Moreover, the high rank of the words
417
spreads through the links in the graph and make
all the nodes in its vicinity also receive high ranks.
Given a target word, the system checks which is
the relative ranking of its senses, and the WSD
system would output the one ranking highest.
UKB is very flexible and can be use to perform
WSD on different settings, depending on the con-
text used for disambiguating a word instance. In
this paper we use it to perform general and do-
main specific WSD, as shown in section 3. PageR-
ank is calculated by applying an iterative algo-
rithm until convergence below a given threshold
is achieved. Following usual practice, we used a
damping value of 0.85 and set the threshold value
at 0.001. We did not optimize these parameters.
2.2 Tybots
Tybots (Term Yielding Robots) are text mining
software that mine domain terms from corpus
(e.g. web pages), organizing them in a hierar-
chical structure, connecting them to wordnets and
ontologies to create a semantic model for the do-
main (Bosma and Vossen, 2010). The software is
freely available using Subversion
2
. Tybots try to
establish a view on the terminology of the domain
which is as complete as possible, discovering rela-
tions between terms and ranking terms by domain
relevance.
Preceding term extraction, we perform tok-
enization, part-of-speech tagging and lemmatiza-
tion, which is stored in Kyoto Annotation For-
mat (KAF) (Bosma et al, 2009). Tybots work
through KAF documents, acquire domain relevant
terms based on the syntactic features, gather co-
occurrence statistics to decide which terms are sig-
nificant in the domain and produce a thesaurus
with sets of related words. Section 3.3 describes
the specific settings that we used.
2.3 Lexical Knowledge bases
We used the following wordnets, as suggested by
the organizers:
WN30g: English WordNet 3.0 with gloss relations
(Fellbaum, 1998).
Dutch: The Dutch LKB is part of the Cor-
netto database version 1.3 (Vossen et al, 2008).
The Cornetto database can be obtained from
the Dutch/Flanders Taalunie
3
. Cornetto com-
prises taxonomic relations and equivalence rela-
2
http://kyoto.let.vu.nl/svn/kyoto/trunk
3
http://www.inl.nl/nl/lexica/780
#entries #synsets #rels. #WN30g
Monolingual
Chinese 8,186 14,243 20,433 20,584
Dutch 83,812 70,024 224,493 83,669
Italian 46,724 49,513 65,567 52,524
WN30g 147,306 117,522 525,351 n/a
Bilingual
Chinese-eng 8,186 141,561 566,368
Dutch-eng 83,812 188,511 833,513
Italian-eng 46,724 167,094 643,442
Table 1: Wordnets and their sizes (entries, synsets,
relations and links to WN30g).
tions from both WordNet 2.0 and 3.0. Cornetto
concepts are mapped to English WordNet 3.0.
Italian: Italwordnet (Roventini et al, 2003) was
created in the framework of the EuroWordNet,
employs the same set of semantic relations used
in EuroWordNet, and includes links to WordNet
3.0 synsets.
Chinese: The Chinese WordNet (Version 1.6) is
now partially open to the public
4
(Tsai et al,
2001). The Chinese WordNet is also mapped to
WordNet 3.0.
Table 1 shows the sizes of the graphs created
using each LKB as a source. The upper part shows
the number of lexical entries, synsets and relations
of each LKB. It also depicts the number of links to
English WordNet 3.0 synsets.
In addition, we also created bilingual graphs for
Dutch, Italian and Chinese, comprising the orig-
inal monolingual LKB, the links to WordNet 3.0
and WordNet 3.0 itself. We expected this richer
graphs to perform better performance. The sizes
of the bilingual graphs are shown in the lower side
of Table 1.
3 Experimental setting
All test documents were lemmatized and PoS-
tagged using the linguistic processors available
within the Kyoto project. In this section we de-
scribe the submitted runs.
3.1 UKB parameters
We use UKB with the default parameters. In par-
ticular, we don?t use dictionary weights, which in
the case of English come from annotated corpora.
This is done in order to make the system fully un-
supervised. It?s also worth mentioning that in the
default setting parts of speech were not used.
4
http://cwn.ling.sinica.edu.tw
418
RANK RUN P R R-NOUN R-VERB
Chinese
- 1sense 0.562 0.562 0.589 0.518
1 Best 0.559 0.559 - -
- Random 0.321 0.321 0.326 0.312
4 kyoto-3 0.322 0.296 0.257 0.360
3 kyoto-2 0.342 0.285 0.251 0.342
5 kyoto-1 0.310 0.258 0.256 0.261
Dutch
1 kyoto-3 0.526 0.526 0.575 0.450
2 kyoto-2 0.519 0.519 0.561 0.454
- 1sense 0.480 0.480 0.600 0.291
3 kyoto-1 0.465 0.465 0.505 0.403
- Random 0.328 0.328 0.350 0.293
English
1 Best 0.570 0.555 - -
- 1sense 0.505 0.505 0.519 0.454
10 kyoto-2 0.481 0.481 0.487 0.462
22 kyoto-1 0.384 0.384 0.382 0.391
- Random 0.232 0.232 0.253 0.172
Italian
1 kyoto-3 0.529 0.529 0.530 0.528
2 kyoto-2 0.521 0.521 0.522 0.519
3 kyoto-1 0.496 0.496 0.507 0.468
- 1sense 0.462 0.462 0.472 0.437
- Random 0.294 0.294 0.308 0.257
Table 2: Overall results of our runs, including pre-
cision (P) and recall (R), overall and for each PoS.
We include the First Sense (1sense) and random
baselines, as well as the best run, as provided by
the organizers.
3.2 Run1: UKB using context
The first run is an application of the UKB tool in
the standard setting, as described in (Agirre and
Soroa, 2009). Given the input text, we split it in
sentences, and we disambiguate each sentence at a
time. We extract the lemmas which have an entry
in the LKB and then apply Personalized PageR-
ank over all of them, obtaining a score for every
concept of the LKB. To disambiguate the words in
the sentence we just choose its associated concept
(sense) with maximum score.
In our experiments we build a context of at least
20 content words for each sentence to be disam-
biguated, taking the sentences immediately before
when necessary. UKB allows two main methods
of disambiguation, namely ppr and ppr w2w. We
used the latter method, as it has been shown to per-
form best.
In this setting we used the monolingual graphs
for each language (cf. section 2.3). Note that
in this run there is no domain adaptation, it thus
serves us as a baseline for assessing the benefits of
applying domain adaptation techniques.
3.3 Run2: UKB using related words
Instead of disambiguating words using their con-
text of occurrence, we follow the method de-
scribed in (Agirre et al, 2009). The idea is to first
obtain a list of related words for each of the tar-
get words, as collected from a domain corpus. On
a second step each target word is disambiguated
using the N most related words as context (see
below). For instance, in order to disambiguate
the word environment, we would not take into
account the context of occurrence (as in Section
3.2), but we would use the list of most related
words in the thesaurus (e.g. ?biodiversity, agri-
culture, ecosystem, nature, life, climate, . . .?). Us-
ing UKB over these contexts we obtain the most
predominant sense for each target word in the do-
main(McCarthy et al, 2007), which is used to la-
bel all occurrences of the target word in the test
dataset.
In order to build the thesaurus with the lists of
related words, we used Tybots (c.f. section 2.2),
one for each corpus of the evaluation dataset, i.e.
Chinese, Dutch, English, and Italian. We used the
background documents provided by the organiz-
ers, which we processed using the linguistic pro-
cessors of the project to obtain the documents in
KAF. We used the Tybots with the following set-
tings. We discarded co-occurring words with fre-
quencies below 10
5
. Distributional similarity was
computed using (Lin, 1998). Finally, we used up
to 50 related words for each target word.
As in run1, we used the monolingual graphs for
the LKBs in each language.
3.4 Run3: UKB using related words and
bilingual graphs
The third run is exactly the same as run2, except
that we used bilingual graphs instead of monolin-
gual ones for all languages other than English (cf.
section 2.3). There is no run3 for English.
4 Results
Table 2 shows the results of our system on the
different languages. We will analyze different as-
pects of the results in turn.
Domain adaptation: Using Personalized Pager-
ank over related words (run2 and run3) con-
sistently outperforms the standard setting (run1)
in all languages. This result is consistent with
5
In the case of Dutch we did not use any threshold due to
the small size of the background corpus.
419
our previous work on English (Agirre et al,
2009), and shows that domain adaptation works
for knowledge-based systems.
Monolingual vs. Bilingual graphs: As ex-
pected, we obtained better results using the bilin-
gual graphs (run3) than with monolingual graphs
(run2), showing that the English WordNet has a
richer set of relations, and that those relations can
be successfully ported to other languages. This
confirms that aligning different wordnets at the
synset level is highly beneficial.
Overall results: the results of our runs are highly
satisfactory. In two languages (Dutch and Ital-
ian) our best runs perform better than the first
sense baseline, which is typically hard to beat for
knowledge-based systems. In English, our system
performs close but below the first sense baseline,
and in Chinese our method performed below the
random baseline.
The poor results obtained for Chinese can be
due the LKB topology; an analysis over the graph
shows that it is formed by a large number of
small components, unrelated with each other. This
?flat? structure heavily penalizes the graph based
method, which is many times unable to discrimi-
nate among the concepts of a word. We are cur-
rently inspecting the results, and we don?t discard
bugs, due to the preliminary status of our software.
In particular, we need to re-examine the output of
the Tybot for Chinese.
5 Conclusions
This paper describes the results of the prelimi-
nary release of he integrated Kyoto system for do-
main specific WSD. It comprises Tybots to con-
struct a domain-related thesaurus, and UKB for
knowledge-based WSD based on wordnet graphs.
We applied our system to all languages in the
dataset, obtaining good results. In fact, our sys-
tem can be applied to any language with a lexical
knowledge base, and is based on publicly available
software and resources. We used the wordnets and
background texts provided by the organizers of the
task.
Our results show that we were succesful in
adapting our system to the domain, as we man-
aged to beat the first sense baseline in two lan-
guages. Our results also show that adding the En-
glish WordNet to the other language wordnets via
the available links is beneficial.
Our participation focused on producing running
systems for all languages in the task, and we at-
tained good results in all except Chinese. Due to
the pressure and the time-constraints in the com-
petition, the system is still under development. We
are currently revising our system for bugs and fine-
tuning it.
Acknowledgments
This work task is partially funded by the Eu-
ropean Commission (KYOTO ICT-2007-211423),
the Spanish Research Department (KNOW-2
TIN2009-14715-C04-01) and the Basque Govern-
ment (BERBATEK IE09-262).
References
E. Agirre and A. Soroa. 2009. Personalizing pagerank for
word sense disambiguation. In Proceedings of EACL09,
pages 33?41. Association for Computational Linguistics.
E. Agirre, O. L?opez de Lacalle, and A. Soroa. 2009.
Knowledge-based wsd on specific domains: Performing
better than generic supervised wsd. In Proceedigns of IJ-
CAI. pp. 1501-1506.?.
E. Agirre, O. L?opez de Lacalle, C. Fellbaum, S.K. Hsieh,
M. Tesconi, M. Monachini, P. Vossen, and R. Segers.
2010. Semeval-2010 task 17: All-words word sense dis-
ambiguation on a specific domain. In Same volume.
W. E. Bosma and P. Vossen. 2010. Bootstrapping language
neutral term extraction. In Proceedings of LREC2010,
May.
W. E. Bosma, P. Vossen, A. Soroa, G. Rigau, M. Tesconi,
A. Marchetti, M. Monachini, and C. Aliprandi. 2009.
KAF: a generic semantic annotation format. In Proceed-
ings of the GL2009 Workshop on Semantic Annotation.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks and
ISDN Systems, 30(1-7).
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database. The MIT Press, Cambridge, MA.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In WWW
?02: Proceedings of the 11th international conference on
WWW, pages 517?526, New York, NY, USA. ACM.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of ACL98, Montreal, Canada.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2007.
Unsupervised acquisition of predominant word senses.
Computational Linguistics, 33(4).
A. Roventini, A. Alonge, F. Bertagna, N. Calzolari, J. Can-
cila, C. Girardi, B. Magnini, R. Marinelli, M. Speranza,
and A. Zampolli. 2003. Italwordnet: building a large
semantic database for the automatic treatment of Italian.
Linguistica Computazionale, Special Issue (XVIII-XIX),
pages 745?791.
B.S. Tsai, C.R. Huang, S.c. Tseng, J.Y. Lin, K.J. Chen, and
Y.S. Chuang. 2001. Definition and tests for lexical se-
mantic relations in Chinese. In Proceedings CLSW 2001.
P. Vossen, I. Maks, R. Segers, H. van der Vliet, and H. van
Zutphen. 2008. The cornetto database: the architecture
and alignment issues. In Proceedings GWC 2008, pages
485?506.
420
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 580?584, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
EHU-ALM: Similarity-Feature Based Approach for Student Response
Analysis
Itziar Aldabe, Montse Maritxalar
IXA NLP Group
University of Basque Country (UPV-EHU)
itziar.aldabe@ehu.es
montse.maritxalar@ehu.es
Oier Lopez de Lacalle
University of Edinburgh
IKERBASQUE,
Basque Foundation for Science
oier.lopezdelacalle@gmail.com
Abstract
We present a 5-way supervised system based
on syntactic-semantic similarity features. The
model deploys: Text overlap measures,
WordNet-based lexical similarities, graph-
based similarities, corpus-based similarities,
syntactic structure overlap and predicate-
argument overlap measures. These measures
are applied to question, reference answer and
student answer triplets. We take into account
the negation in the syntactic and predicate-
argument overlap measures. Our system uses
the domain-specific data as one dataset to
build a robust system. The results show that
our system is above the median and mean on
all the evaluation scenarios of the SemEval-
2013 task #7.
1 Introduction
In this paper we describe our participation with a
feature-based supervised system to the SemEval-
2013 task #7: The Joint Student Response Analy-
sis and 8th Recognizing Textual Entailment Chal-
lenge (Dzikovska et al, 2013). The goal of our
participation is to build a generic system that is
robust enough across domains and scenarios. A
domain-specific system requires new training ex-
amples when shifting to a new domain. However,
domain-specific data is difficult to obtain and creat-
ing new resources is expensive.
We seek robustness by mixing the instances from
BEETLE and SCIENTSBANK. We show our strategy
is suitable to build a generic system that performs
competitively on any domain in the 5-way task.
The paper proceeds as follows. Section 2 de-
scribes the system presenting the learning features
and the runs. In Section 3 we show the optimiza-
tion details, followed by the results (Section 4) and
a preliminary error analysis (Section 5).
2 System description
Our system aims for robustness using the domain-
specific training data as one dataset. Therefore,
we do not differentiate between examples from the
given domains (BEETLE and SCIENTSBANK) when
training the system. In contrast, our approach dintin-
guishes between new questions (unseen answer vs.
unseen question) as well as question types (how,
what and why) by means of simple heuristics.
The runs are organized according to different sys-
tem designs. Although all the runs use the same fea-
ture set, we split the training set to build more spe-
cialized classifiers. Training examples are grouped
depending on: i) the answer is unseen; ii) the ques-
tion is unseen; and iii) the question type (i.e. what,
how, why). Each run defines a framework to explore
the different ways to approach the problem. While
the first run is the simplest and is the most generic
in nature, the third tries to split the task into simpler
problems and creates more specialized classifiers.
2.1 Similarity learning features
Our model is based on various text similarity fea-
tures. Almost all of the measures are computed be-
tween question, reference answer and student an-
swer triplets. The measures based on syntactic struc-
ture and predicate-argument overlaps are only ap-
plied to the student and reference answer pairs. In
580
total, we defined 30 features which can be grouped
as follows:
Text overlapmeasures The similarity of two texts
is computed based on the number of overlapping
words. We obtain the similarity of two texts based
on the F-Measure, the Dice Coefficient, The Cosine,
and the Lesk measures. For that, we use the imple-
mentation available in the Text::Similarity package1.
WordNet-based lexical similarities All the simi-
larity metrics based on WordNet (Miller, 1995) fol-
low the methodology proposed in (Mihalcea et al,
2006). For each open-class word in one of the in-
put texts, we obtain the maximun semantic similar-
ity or relatedness value matching the same open-
class words in the other input text. The values of
each matching are summed up and normalized by
the length of the two input texts as explained in
(Mihalcea et al, 2006). We compute the measures
of Resnik, Lin, Jiang-Conrath, Leacock-Chodorow,
Wu-Palmer, Banerjee-Pedersen, and Patwardhan-
Pedersen provided in the WordNet::Similarity pack-
age (Patwardhan et al, 2003).
Graph-based similarities The similarity of two
texts is based on a graph-based representation
(Agirre and Soroa, 2009) of WordNet. The method
is a two-step process: first the personalized PageR-
ank over WordNet is computed for each text. This
produces a probability distribution over WordNet.
Then, the probability distributions are encoded as
vectors and the cosine similarity between those vec-
tors is calculated.
Corpus-based similarities We compute two
corpus-based similarity measures: Latent Semantic
Analysis (Deerwester et al, 1990) and Latent
Dirichlet Allocation (Blei et al, 2003). We estimate
100 dimensions for LSA and 50 topics for LDA.
Both models are obtained from a subset of the En-
glish Wikipedia following the hierarchy of science
categories. We started with a small set of categories
and recovered the articles below the sub-hierarchy.
We only went 3 levels down to avoid noisy articles
as the category system is rather flat. The similarity
of two texts is the cosine similarity between the
1http://www.d.umn.edu/ tpederse/text-similarity.html
resulting vectors associated with each text in the
latent space.
Syntactic structure overlap The role of syntax is
studied by the use of graph subsumption based on
the approach proposed in (McCarthy et al, 2008).
The text is mapped into a graph with nodes rep-
resenting words and links indicating syntactic de-
pendencies between them. The similarity of two
texts is computed based on the overlap of the syn-
tactic structures. Negation is handled explicitly in
the graph.
Predicate-argument overlap The similarity of
two texts is computed by analyzing the overlap of
the predicates and their associated semantic argu-
ments. The system looks for verbal and nominal
predicates. The similarity is also based on the ap-
proach proposed in (McCarthy et al, 2008). The
graph is represented with words as nodes and the
semantic role of arguments as links. First, the ver-
bal propositions and their arguments are automat-
ically obtained (Bjo?rkelund et al, 2009) as repre-
sented in PropBank (Palmer et al, 2005). Second,
a generalization of the predicates is obtained based
on VerbNet (Kipper, 2005) and NomBank (Meyers
et al, 2004). Finally, the similarity of two texts
is computed based on the overlap of the predicate-
argument relations.
2.2 Architecture of the runs
Generic Framework RUN1 This is the simplest
framework for the assessment of student answers.
The system relies on a single classifier, which has
been optimized on the unseen question scenario.
The scenario is simulated by splitting the training
set so that each question and its answers are in the
same fold.
Unseen Framework RUN2 This framework relies
on two classifiers. The first is tuned on an unseen
answer scenario and the second is prepared for the
question scenario (cf. RUN1). In order to build the
unseen answer classifier, we split the training set so
that answers to the same question can occur in dif-
ferent folders. In test time, the instance is classified
depending on whether it is an unseen answer or an
581
BEETLE SCIENTSBANK OVERALL
Uns-answ Uns-qst All Uns-answ Uns-qst Uns-dom All All
RUN1 0.499 (6) 0.352 (7) 0.404 0.396 (7) 0.283 (4) 0.345 (3) 0.348 0.406
RUN2 0.526 (4) 0.352 (7) 0.413 0.418 (6) 0.283 (4) 0.345 (3) 0.350 0.414
RUN3 0.502 (5) 0.370 (6) 0.415 0.424 (5) 0.260 (8) 0.337 (5) 0.340 0.403
LOWEST 0.170 0.173 - 0.089 0.095 0.121 - -
BEST 0.619 0.552 - 0.478 0.307 0.380 - -
MEAN 0.435 0.343 - 0.341 0.240 0.267 - -
MEDIAN 0.437 0.326 - 0.376 0.259 0.268 - -
Table 1: 5-way results of the runs in F1 macro-average on BEETLE and SCIENTSBANK domains across different
scenarios. Along with the runs, the LOWEST and the BEST system in each scenario are shown. The MEAN and
MEDIAN of the dataset are also presented. Finally, the OVERALL results are showed summing up both domains. Uns-
answ refers to unseen answers scenario, Uns-qst stands for unseen question, Uns-dom unseen domain and All refers
to the sum of all scenarios. The run results are presented together with the ranked position in the task.
unseen question2.
Question-type Framework RUN3 The run con-
sists of a set of question-type expert classifiers. We
divided the training set based on whether an instance
reflected a what, how or why question. We then par-
titioned each question type into unseen answer and
unseen question scenarios. In total, the framework
deploys 6 classifiers, i.e. a test instance is classified
according to the question type and scenario. We set
heuristics to automatically distinguish the instance
type.
3 Optimization on training set
We set a heuristic to create the training instances.
For each student answer, if the matching reference
answer is indicated in it, we create a triplet with the
question, the student answer, and the matching ref-
erence answer. If there is no matching answer, the
reference answer is randomly selected giving pref-
erence to the best reference answers.
Once we have a training set, we split it into dif-
ferent ways to simulate the scenarios described in
Section 2.2. All the models are optimized using 10-
fold cross-validation of the pertaining training set.
For the classifiers in RUN1 and RUN2 we used 8910
training instances. For RUN3 the instances were di-
vided as follows: 1235 instances for how questions,
3089 for what questions and 4589 for why ques-
tions. In total, we obtained 8 models which were
distributed through the runs.
2We treat unseen-domain instances as unseen-question in-
stances.
Our approach uses Support Vector Ma-
chine (Chang and Lin, 2011) to build the classifiers.
As the number of features is not high, we used the
gaussian kernel in order to solve the non-linear
problem. The main parameters of the kernel (? and
C) were tuned using grid search over the parameter
in the cross-validation setting. We focused on
optimizing the F1 macro average of the classifier
in order to avoid a bias towards the major classes.
Each of the 8 classifiers were tuned independently.
The triplets of question, student answer and ref-
erence answer of the test instances were always cre-
ated selecting the first reference answer of the given
set of answers.
4 Results
A total of 8 teams participated in the 5-way task,
submitting a total of 16 system runs (Dzikovska et
al., 2013). Table 1 shows the performance obtained
by our systems across domains and different scenar-
ios. Our three runs ranked differently based on the
evaluation scenario: beetle-uns-answ (6,4,5 rank for
RUN1, RUN2, RUN3, respectively); beetle-uns-qst
(7,7,6); scientsbank-uns-answ (7,6,5); scientsbank-
uns-qst (4,4,8) and scientsbank-uns-dom (3,3,5). We
also evaluated our runs on the entire domain (All
columns) and on the whole test set (OVERALL).
The results show we built robust systems. Despite
being below the best system of each evaluation sce-
nario, the results show that the runs are competitive.
All our runs are above the median and outperform
the average results on each evaluation. Overall, the
results attained in SCIENTSBANK are lower than in
582
BEETLE. This might be due to the questions and
answers being longer in SCIENTSBANK, making it
difficult to obtain good patterns.
As regards our runs, there is no significant overall
difference. While RUN3 performs better in BEETLE
unseen question and SCIENTSBANK unseen answer,
in the rest of scenarios RUN2 outperforms the rest
of the runs. As expected, RUN2 outperforms RUN1
in the unseen answer scenario since the former has
a module specializing in unseen answers. However,
although RUN3 is an ensemble of six classifiers, it is
not the best run. This is probably because the train-
ing sets are not big enough.
Unseen framework (RUN2)
Prec Rec F1
correct 0.552 0.677 0.608
partially correct 0.324 0.323 0.323
contradictory 0.239 0.121 0.160
irrelevant 0.472 0.377 0.419
non domain 0.415 0.849 0.557
Macro average 0.400 0.469 0.414
Micro average 0.443 0.464 0.446
Table 2: results of the RUN2 system on a entire test set.
Table 2 shows the detailed results of the RUN2
system on the entire test set. It is noticeable the
low results obtained on the contradictory class. This
might be because the defined features are not able
to model negation properly and do not deal with
antonymy. Surprisingly, the non domain class is not
the most problematic, even if the system was trained
on a low number of instances.
5 Preliminary Error Analysis
We conducted a preliminary error analysis and stud-
ied some of the misclassified test instances to detect
some problematic issues and to define improvements
to our approach.
Example 5.1 Sam and Jasmine were sitting on a
park bench eating their lunches. A mosquito landed
on Sam?s arm and Sam began slapping at it. When
he did that, he knocked Jasmine?s soda into her lap,
causing her to jump up. What was Sam?s response?
R: Sam?s response was to slap the mosquito.
S1: Sam?s response was to say sorry
S2: To smack the bee.
Some of the detected errors suggest that our use
of syntax and lexical overlap is not sufficient to iden-
tify the correct class. Our system marks the student
answer S1 from Example 5.13 as correct. The ref-
erence answer and the student answer share a great
number of words and the dependency trees are al-
most identical, but not the meanings. In addition, the
question contains additional information that may
require other types of features to correctly classify
the instance.
The predicate-argument overlap feature tries to
generalize the predicate information to find similar-
ities between verbs with the same meaning. How-
ever, our system does not always work in a correct
way. The verb smack in the student answer S2 and
the verb slap in the reference answer mean the same.
Our system classifies the answer incorrectly. If we
look at PropBank and VerbNet, we find that there
is not mapping between PropBank and VerbNet for
these particular verbs.
Example 5.2 Why do you think the other terminals
are being held in a different electrical state than that
of the negative terminal?
R: Terminals 4, 5 and 6 are not connected to the
negative battery terminal
S1: They are connected to the positive battery ter-
minal
We consider the negation as part of the syntac-
tic and predicate-argument overlap measures. How-
ever, our system does not characterize the similar-
ity between not connected to the negative and con-
nected to the positive (Example 5.2). This type of
examples suggest that the system needs to model the
negation and antonyms with additional features.
In the future, further error analysis will be car-
ried out to design features to better model the prob-
lem. We also anticipate creating a specialized fea-
ture space for each question type.
Acknowledgments
This research was partially funded by the Ber2Tek
project (IE12-333), the SKaTeR project (TIN2012-
38584-C06-02) and the NewsReader project (FP7-
ICT-2011-8-316404).
3R refers to the reference answer and S1 and S2 to student
answers.
583
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of the 12th conference of the European chapter of
the Association for Computational Linguistics (EACL-
2009), Athens, Greece.
Anders Bjo?rkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of The Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL-2009),
pages 43?48.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1?27:27, May.
Scott Deerwester, Susan Dumais, Goerge Furnas,
Thomas Landauer, and Richard Harshman. 1990. In-
dexing by Latent Semantic Analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, University of
Pennsylvania.
Philip M. McCarthy, Vasile Rus, Scott A. Crossley,
Arthur C. Graesser, and Danielle S. McNamara. 2008.
Assessing forward-, reverse-, and average-entailment
indices on natural language input from the intelligent
tutoring system, iSTART. In D. Wilson and G. Sut-
cliffe, editors, Proceedings of the 21st International
Florida Artificial Intelligence Research Society Con-
ference, pages 201?206, Menlo Park, CA: The AAAI
Press.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The nombank project: An interim
report. In A. Meyers, editor, HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24?31,
Boston, Massachusetts, USA, May 2 - May 7. Associ-
ation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings the
American Association for Artificial Intelligence (AAAI
2006), Boston.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38(11):39?41.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic role. Computational Linguistics, 31(1):71?
106.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2003. Using measures of semantic related-
ness for word sense disambiguation. In Proceedings
of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics.
584
