Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 1?6,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Classifier Combination Techniques Applied to Coreference Resolution
Smita Vemulapalli1, Xiaoqiang Luo2, John F. Pitrelli2 and Imed Zitouni2
1Center for Signal and Image Processing (CSIP) 2IBM T. J. Watson Research Center
School of ECE, Georgia Institute of Technology 1101 Kitchawan Road
Atlanta, GA 30332, USA Yorktown Heights, NY 10598, USA
smita@ece.gatech.edu {xiaoluo,pitrelli,izitouni}@us.ibm.com
Abstract
This paper examines the applicability of clas-
sifier combination approaches such as bagging
and boosting for coreference resolution. To
the best of our knowledge, this is the first ef-
fort that utilizes such techniques for corefer-
ence resolution. In this paper, we provide ex-
perimental evidence which indicates that the
accuracy of the coreference engine can po-
tentially be increased by use of bagging and
boosting methods, without any additional fea-
tures or training data. We implement and eval-
uate combination techniques at the mention,
entity and document level, and also address is-
sues like entity alignment, that are specific to
coreference resolution.
1 Introduction
Coreference resolution is the task of partitioning a
set of mentions (i.e. person, organization and loca-
tion) into entities. A mention is an instance of textual
reference to an object, which can be either named
(e.g. Barack Obama), nominal (e.g. the president) or
pronominal (e.g. he, his, it). An entity is an aggre-
gate of all the mentions (of any level) which refer to
one conceptual entity. For example, in the following
sentence:
John said Mary was his sister.
there are four mentions: John, Mary, his, and
sister.
John and his belong to the one entity since they
refer to the same person; Mary and sister both
refer to another person entity. Furthermore, John
and Mary are named mentions, sister is a nomi-
nal mention and his is a pronominal mention.
In this paper, we present a potential approach for
improving the performance of coreference resolu-
tion by using classifier combination techniques such
as bagging and boosting. To the best of our knowl-
edge, this is the first effort that utilizes classifier
combination for improving coreference resolution.
Combination methods have been applied to many
problems in natural-language processing (NLP). Ex-
amples include the ROVER system (Fiscus, 1997)
for speech recognition, the Multi-Engine Machine
Translation (MEMT) system (Jayaraman and Lavie,
2005), and part-of-speech tagging (Brill and Wu,
1998; Halteren et al, 2001). Most of these tech-
niques have shown a considerable improvement over
the performance of a single classifier and, therefore,
lead us to consider implementing such a multiple-
classifier system for coreference resolution as well.
Using classifier combination techniques one can
potentially achieve a classification accuracy that is
superior to that of the single best classifier. This
is based on the assumption that the errors made by
each of the classifiers are not identical, and there-
fore if we intelligently combine multiple classifier
outputs, we may be able to correct some of these er-
rors.
The main contributions of this paper are:
? Demonstrating the potential for improvement in
the baseline ? By implementing a system that
behaves like an oracle, we have shown that the
output of the combination of multiple classifiers
has the potential to be significantly higher in ac-
curacy than any of the individual classifiers.
? Adapting traditional bagging techniques ? Mul-
tiple classifiers, generated using bagging tech-
niques, were combined using an entity-level sum
1
rule and mention-level majority voting.
? Implementing a document-level boosting algo-
rithm ? A boosting algorithm was implemented
in which a coreference resolution classifier was
iteratively trained using a re-weighted training
set, where the reweighting was done at the doc-
ument level.
? Addressing the problem of entity alignment ?
In order to apply combination techniques to
multiple classifiers, we need to address entity-
alignment issues, explained later in this paper.
The baseline coreference system we use is sim-
ilar to the one described by Luo et al (Luo et al,
2004). In such a system, mentions are processed
sequentially, and at each step, a mention is either
linked to one of existing entities, or used to create a
new entity. At the end of this process, each possible
partition of the mentions corresponds to a unique se-
quence of link or creation actions, each of which is
scored by a statistical model. The one with the high-
est score is output as the final coreference result.
2 Classifier Combination Techniques
2.1 Bagging
One way to obtain multiple classifiers is via bagging
or bootstrap aggregating (Breiman, 1996). These
classifiers, obtained using randomly-sampled train-
ing sets, may be combined to improve classification.
We generated several classifiers by two tech-
niques. In the first technique, we randomly sample
the set of documents (training set) to generate a few
classifiers. In the second technique, we need to re-
duce the feature set and this is not done in a random
fashion. Instead, we use our understanding of the in-
dividual features and also their relation to other fea-
tures to decide which features may be dropped.
2.2 Oracle
In this paper, we refer to an oracle system which
uses knowledge of the truth. Here, truth, called the
gold standard henceforth, refers to mention detec-
tion and coreference resolution done by a human for
each document. It is possible that the gold standard
may have errors and is not perfect truth, but, as in
most NLP systems, it is considered the reference for
evaluating computer-based coreference resolution.
To understand the oracle, consider an example in
which the outputs of two classifiers for the same in-
put document are C1 and C2, as shown in Figure 1.
 
    C2-EP 
C2-EQ 
C2-ER 
C2-ES 
C1-EA 
C1-EB 
C1-EC 
C1-ED 
G-E1 
G-E2 
G-E3 
G-E4 
C1-EA 
C2-ER 
C1-ED 
C2-ES 
0.72 0.66 
1.0 0.85 
0.88 
0.78 
0.75 
Gold 
G 
Classifier C2 
File X File X File X File X 
Classifier C1 Oracle Output 
Figure 1: Working of the oracle
The number of entities in C1 and C2 may not be the
same and even in cases where they are, the number
of mentions in corresponding entities may not be the
same. In fact, even finding the corresponding entity
in the other classifier output or in the gold standard
output G is not a trivial problem and requires us to
be able to align any two classifier outputs.
The alignment between any two coreference la-
belings, say C1 and G, for a document is the best
one-to-one map (Luo, 2005) between the entities of
C1 and G. To align the entities of C1 with those of
G, under the assumption that an entity in C1 may
be aligned with at most only one entity in G and
vice versa, we need to generate a bipartite graph
between the entities of C1 and G. Now the align-
ment task is a maximum bipartite matching prob-
lem. This is solved by using the Kuhn-Munkres al-
gorithm (Kuhn, 1955; Munkres, 1957). The weights
of the edges of the graph are entity-level alignment
measures. The metric we use is a relative mea-
sure of the similarity between the two entities. To
compute the similarity metric ? (Luo, 2005) for the
entity pair (R,S), we use the formula shown in
Equation 1, where (?) represents the commonal-
ity with attribute-weighted partial scores. Attributes
are things such as (ACE) entity type, subtype, entity
class, etc.
?(R,S) = 2 |R ? S||R|+ |S| (1)
The oracle output is a combination of the entities
in C1 and C2 with the highest entity-pair alignment
measures with the entities in G.1 We can see in Fig-
ure 1 that the entity G-E1 is aligned with entities C1-
EA and C2-EP. We pick the entity with the highest
entity-pair alignment measure (highlighted in gray)
which, in this case, is C1-EA. This is repeated for
1A mention may be repeated across multiple output entities,
which is not an unwarranted advantage as the scorer insists on
one-to-one entity alignment. So if there are two entities con-
taining mention A, at most one mention A is credited and the
other will hurt the score.
2
    F-E1 
F-E2 
F-E3 
F-E4 
C2-EP 
C2-EQ 
C2-ER 
C2-ES 
C1-EA 
C1-EB 
C1-EC 
C1-ED 
0.72 0.6
1.0 0.85 
0.88 0.78 
0.75 
Full F 
File X File X File X 
Classifier C1 
             C2-ES F-E4 
C1-ED C2-EQ F-E3 
C1-EB  C2-ER F-E2 
C1-EA  C2-EP F-E1 
Entity-level 
Alignment Table 
Classifier C2 
Figure 2: Entity alignment between classifier outputs
every entity in G. The oracle output can be seen in
the right-hand side of Figure 1. This technique can
be scaled up to work for any number of classifiers.
2.3 Preliminary Combination Approaches
Imitating the oracle. Making use of the existing
framework of the oracle, we implement a combina-
tion technique that imitates the oracle except that in
this case, we do not have the gold standard. If we
have N classifiers Ci, i = 1 to N , then we replace
the gold standard by each of theN classifiers in suc-
cession, to get N outputs Combi, i = 1 to N .
The task of generating multiple classifier combi-
nation outputs that have a higher accuracy than the
original classifiers is often considered to be easier
than the task of determining the best of these out-
puts. We used the formulas in Equations 2, 3 and 4
to assign a score Si to each of the N combination
outputs Combi, and then we pick the one with the
highest score. The function Sc (which corresponds
to the function ? in Equation 1) gives the similarity
between the entities in the pair (R,S).
Si = 1N ? 1
?
j = 1 to N
j 6= i
Sc(Combi, Cj) (2)
Si = Sc(Combi, Ci) (3)
Si = 1N ? 1
?
j = 1 to N
j 6= i
Sc(Combi, Combj) (4)
Entity-level sum-rule. We implemented a basic sum-
rule at the entity level, where we generate only one
combination classifier output by aligning the entities
in the N classifiers and picking only one entity at
each level of alignment. In the oracle, the reference
for entity-alignment was the gold standard. Here,
we use the baseline/full system (generated using the
entire training and feature set) to do this. The entity-
level alignment is represented as a table in Figure 2.
Let Ai, i = 1 to M be the aligned entities in one
row of the table in Figure 2. Here, M ? N if
 
 
A A1   A2   A3   A4  ? 
B B1   B2           B4  ? 
C C1   C2   C3   C4  ? 
D        D2   D3   D4  ? 
3 
0 
1 
0 
A{m1,m2,m6} 
B{ m3} 
C{ m4,m5} 
D{m7 } 
Entity-level Alignment Table 
Mention m1 
Mention Count for m1 Output Majority Voting for mention m1 
Figure 3: Mention-level majority voting
we exclude the baseline from the combination and
M ? N + 1 if we include it. To pick one entity
out of these M entities, we use the traditional sum
rule (Tulyakov et al, 2008), shown in Equation 5, to
compute the S(Ai) for each Ai and pick the entity
with the highest S(Ai) value.
S(Ai) =
?
j = 1 to N
j 6= i
Sc(Ai, Aj) (5)
2.4 Mention-level Majority Voting
In the previous techniques, entities are either picked
or rejected as a whole but never broken down fur-
ther. In the mention-level majority voting technique,
we work at the mention level, so the entities created
after combination may be different from the entities
of all the classifiers that are being combined.
In the entity-level alignment table (shown in Fig-
ure 3), A, B, C and D refer to the entities in the base-
line system and A1, A2, ..., D4 represent the enti-
ties of the input classifiers that are aligned with each
of the baseline classifier entities. Majority voting is
done by counting the number of times a mention is
found in a set of aligned entities. So for every row
in the table, we have a mention count. The row with
the highest mention count is assigned the mention in
the output. This is repeated for each mention in the
document. In Figure 3, we are voting for the men-
tion m1, which is found to have a voting count of 3
(the majority vote) at the entity-level A and a count
of 1 at the entity-level C, so the mention is assigned
to the entity A. It is important to note that some clas-
sifier entities may not align with any baseline clas-
sifier entity as we allow only a one-to-one mapping
during alignment. Such entities will not be a part of
the alignment table. If this number is large, it may
have a considerable effect on the combination.
2.5 Document-level Boosting
Boosting techniques (Schapire, 1999) combine mul-
tiple classifiers, built iteratively and trained on
re-weighted data, to improve classification accu-
racy. Since coreference resolution is done for a
whole document, we can not split a document fur-
3
Test 
docu
ment
s
with 
perce
ntile 
< P th
resh
and F
-mea
sure 
< F th
resh
bc bn cts nw un wl
# Training documents : # Test documents 
ratio for every genre is maintained
Train Test
Train Test
Train Test
Train Test
Train Test
Docu
ment
s
to bo
ost
Train
ing S
et
Shuf
fle
Boos
ting o
f Tra
ining
 Set
Train
ing S
et
Figure 4: Document-level boosting
ther. So when we re-weight the training set, we
are actually re-weighting the documents (hence the
name document-level boosting). Figure 4 shows an
overview of this technique.
The decision of which documents to boost is
made using two thresholds: percentile threshold
Pthresh and the F-measure threshold Fthresh. Doc-
uments in the test set that are in the lowest Pthresh
percentile and that have a document F-measure less
than Fthresh will be boosted in the training set for
the next iteration. We shuffle the training set to cre-
ate some randomness and then divide it into groups
of training and test sets in a round-robin fashion such
that a predetermined ratio of the number of training
documents to the number of test documents is main-
tained. In Figure 4, the light gray regions refer to
training documents and the dark gray regions refer
to test documents. Another important consideration
is that it is difficult to achieve good coreference res-
olution performance on documents of some genres
compared to others, even if they are boosted signif-
icantly. In an iterative process, it is likely that doc-
uments of such genres will get repeatedly boosted.
Also our training set has more documents of some
genres and fewer of others. So we try to maintain, to
some extent, the ratio of documents from different
genres in the training set while splitting this training
set further into groups of training and test sets.
3 Evaluation
This section describes the general setup used to con-
duct the experiments and presents an evaluation of
the combination techniques that were implemented.
Experimental setup. The coreference resolution
system used in our experiments makes use of a Max-
imum Entropy model which has lexical, syntacti-
cal, semantic and discourse features (Luo et al,
Table 1: Statistics of ACE 2005 data
DataSet #Docs #Words #Mentions #Entities
Training 499 253771 46646 16102Test 100 45659 8178 2709Total 599 299430 54824 18811
Table 2: Accuracy of generated and baseline classifiers
Classifier Accuracy (%)
C1 ? C15 Average 77.52Highest 79.16Lowest 75.81C0 Baseline 78.53
2004). Experiments are conducted on ACE 2005
data (NIST, 2005), which consists of 599 documents
from rich and diversified sources. We reserve the
last 16% documents of each source as the test set,
and use the rest of the documents as the training set.
The ACE 2005 data split is tabulated in Table 1.
Bagging A total of 15 classifiers (C1 to C15) were
generated, 12 of which were obtained by sampling
the training set and the remaining 3 by sampling
the feature set. We also make use of the base-
line classifier C0. The accuracy of C0 to C15 has
been summarized in Table 2. The agreement be-
tween the classifiers? output was found to be in the
range of 93% to 95%. In this paper, the metric used
to compute the accuracy of the coreference resolu-
tion is the Constrained Entity-Alignment F-Measure
(CEAF) (Luo, 2005) with the entity-pair similarity
measure in Equation 1.
Oracle. To conduct the oracle experiment, we train
1 to 15 classifiers and align their output to the gold
standard. For all entities aligned with a gold entity,
we pick the one with the highest score as the output.
We measure the performance for varying number of
classifiers, and the result is plotted in Figure 5.
First, we observe a steady and significant increase
in CEAF for every additional classifier, because ad-
ditional classifiers can only improve the alignment
score. Second, we note that the oracle accuracy is
87.58% for a single input classifier C1, i.e. an abso-
lute gain of 9% compared to C0. This is because the
availability of gold entities makes it possible to re-
move many false-alarm entities. Finally, the oracle
accuracy when all 15 classifiers are used as input is
94.59%, a 16.06% absolute improvement.
This experiment helps us to understand the perfor-
mance bound of combining multiple classifiers and
the contribution of every additional classifier.
Preliminary combination approaches. While the
oracle results are encouraging, a natural question is
4
 
75
 
80
 
85
 
90
 
95
 
100
 
0
 
2
 
4
 
6
 
8
 
10
 
12
 
14
 
16
Accuracy (%)
Num
ber
 of C
lass
ifier
s
bas
elin
e
Figure 5: Oracle performance vs. number of classifiers
     
 
 
 
 
 
 
 
 
7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56 
Classifier 
C1 
Classifier  
C2 
Combination 
Output Classifier  C3 
7-10 7-17 7-18 7-19 7-27 7-30 15-22 20-33 20-68 37-56 
    7-10 7-17 7-27 
Legend: 
Type I  
mentions 
Type II  
mentions 
Type III  
mentions 
Type IV  
mentions 
7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56 
Baseline 
C   7-17 7-27 7-61 7-63 7-64 20-39 20-62 20-66  
Figure 6: A real example showing the working of
mention-level majority voting
how much performance gain can be attained if the
gold standard is not available. To answer this ques-
tion, we replace the gold standard with one of the
classifiers C1 to C15, and align the classifiers. This
is done in a round robin fashion as described in Sec-
tion 2.3. The best performance of this procedure is
77.93%. The sum-rule combination output had an
accuracy of 78.65% with a slightly different base-
line of 78.81%. These techniques do not yield a sta-
tistically significant increase in CEAF but this is not
surprising as C1 to C15 are highly correlated.
Mention-level majority voting. This experiment is
conducted to evaluate the mention-level majority
voting technique. The results are not statistically
better than the baseline, but they give us valuable
insight into the working of the combination tech-
nique. The example in Figure 6 shows a single
entity-alignment level for the baselineC0 and 3 clas-
sifiers C1, C2, and C3 and the combination output
by mention-level majority voting. The mentions are
denoted by the notation ?EntityID - MentionID?, for
example 7-10 is the mention with EntityID=7 and
MentionID=10. Here, we use the EntityID in the
gold file. The mentions with EntityID=7 are ?cor-
rect? i.e. they belong in this entity, and the others
are ?wrong? i.e. they do not belong in this entity.
The aligned mentions are of four types:
? Type I mentions ? These mentions have a highest
voting count of 2 or more at the same entity-level
alignment and hence appear in the output.
? Type II mentions ? These mentions have a high-
est voting count of 1. But they are present in
more than one input classifier and there is a tie
between the mention counts at different entity-
level alignments. The rule to break the tie is
that mentions are included if they are also seen
in the full system C0. As can been seen, this rule
brings in correct mentions such as 7-61, 7-63,
7-64, but it also admits 20-33,20-39 and 20-62.
In the oracle, the gold standard helps to remove
entities with false-alarm mentions, whereas the
full system output is noisy and it is not strong
enough to reliably remove undesired mentions.
? Type III mentions ? There is only one mention
20-66 which is of this type. It is selected in the
combination output since it is present in C2 and
the baseline C0, although it has been rejected as
a false-alarm in C1 and C3.
? Type IV mentions ? These false-alarm mentions
(relative to C0) are rejected in the output. As can
be seen, this correctly rejects mentions such as
15-22 and 20-68, but it also rejects correct men-
tions 7-18, 7-19 and 7-30.
In summary, the current implementation of this
technique has a limited ability to distinguish correct
mentions from wrong ones due to the noisy nature
of C0 which is used for alignment. We also observe
that mentions spread across different alignments of-
ten have low-count and they are often tied in count.
Therefore, it is important to set a minimum thresh-
old for accepting these low-count majority votes and
also investigate better tie-breaking techniques.
Document-level Boosting This experiment is con-
ducted to evaluate the document-level boosting tech-
nique. Table 3 shows the results with the ratio
of the number of training documents to the num-
ber of test documents equal to 80:20, F-measure
threshold Fthresh = 74% and percentile threshold
Pthresh = 25%. The accuracy increases by 0.7%,
relative to the baseline. Due to computational com-
plexity considerations, we used fixed values for the
parameters. Therefore, these values may be sub-
optimal and may not correspond to the best possible
increase in accuracy.
4 Related Work
A large body of literature related to statistical meth-
ods for coreference resolution is available (Ng and
Cardie, 2003; Yang et al, 2003; Ng, 2008; Poon and
5
Table 3: Results of document-level boosting
Iteration Accuracy (%)
1 78.532 78.823 79.084 78.37
Domingos, 2008; McCallum and Wellner, 2003).
Poon and Domingos (Poon and Domingos, 2008)
use an unsupervised technique based on joint infer-
ence across mentions and Markov logic as a repre-
sentation language for their system on both MUC
and ACE data. Ng (Ng, 2008) proposed a genera-
tive model for unsupervised coreference resolution
that views coreference as an EM clustering process.
In this paper, we make use of a coreference engine
similar to the one described by Luo et al (Luo et al,
2004), where a Bell tree representation and a Maxi-
mum entropy framework are used to provide a natu-
rally incremental framework for coreference resolu-
tion. To the best of our knowledge, this is the first ef-
fort that utilizes classifier combination techniques to
improve coreference resolution. Combination tech-
niques have earlier been applied to various applica-
tions including machine translation (Jayaraman and
Lavie, 2005), part-of-speech tagging (Brill and Wu,
1998) and base noun phrase identification (Sang et
al., 2000). However, the use of these techniques for
coreference resolution presents a unique set of chal-
lenges, such as the issue of entity alignment between
the multiple classifier outputs.
5 Conclusions and Future Work
In this paper, we examined and evaluated the ap-
plicability of bagging and boosting techniques to
coreference resolution. We also provided empir-
ical evidence that coreference resolution accuracy
can potentially be improved by using multiple clas-
sifiers. In future, we plan to improve (1) the entity-
alignment strategy, (2) the majority voting technique
by setting a minimum threshold for the majority-
vote and better tie-breaking, and (3) the boosting
algorithm to automatically optimize the parameters
that have been manually set in this paper. Another
possible avenue for future work would be to test
these combination techniques with other coreference
resolution systems.
Acknowledgments
The authors would like to acknowledge Ganesh N.
Ramaswamy for his guidance and support in con-
ducting the research presented in this paper.
References
L. Breiman. 1996. Bagging predictors. In Machine
Learning.
E. Brill and J. Wu. 1998. Classifier combination for im-
proved lexical disambiguation. In Proc. of COLING.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recogniser output voting error
reduction (rover). In Proc. of ASRU.
H. V. Halteren et al 2001. Improving accuracy in
word class tagging through the combination of ma-
chine learning systems. Computational Linguistics,
27.
S. Jayaraman and A. Lavie. 2005. Multi-engine machine
translation guided by explicit word matching. In Proc.
of ACL.
H. W. Kuhn. 1955. The hungarian method for the assign-
ment problem. Naval Research Logistics Quarterly, 2.
X. Luo et al 2004. A mention-synchronous coreference
resolution algorithm based on the bell tree. In Proc. of
ACL.
X. Luo. 2005. On coreference resolution performance
metrics. In Proc. of EMNLP.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In Proc. of IJCAI/IIWeb.
J. Munkres. 1957. Algorithms for the assignment and
transportation problems. Journal of the Society of In-
dustrial and Applied Mathematics, 5(1).
V. Ng and C. Cardie. 2003. Bootstrapping coreference
classifiers with multiple machine learning algorithms.
In Proc. of EMNLP.
V. Ng. 2008. Unsupervised models for coreference reso-
lution. In Proc. of EMNLP.
NIST. 2005. ACE?05 evaluation. www.nist.gov/
speech/tests/ace/ace05/index.html.
H. Poon and P. Domingos. 2008. Joint unsupervised
coreference resolution with Markov Logic. In Proc.
of EMNLP.
E. F. T. K. Sang et al 2000. Applying system combi-
nation to base noun phrase identification. In Proc. of
COLING 2000.
R.E. Schapire. 1999. A brief introduction to boosting. In
Proc. of IJCAI.
S. Tulyakov et al 2008. Review of classifier combi-
nation methods. In Machine Learning in Document
Analysis and Recognition.
X. Yang et al 2003. Coreference resolution using com-
petition learning approach. In Proc. of ACL.
6
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 335?345,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Improving Mention Detection Robustness to Noisy Input
Radu Florian, John F. Pitrelli, Salim Roukos and Imed Zitouni
IBM T.J. Watson Research Center
Yorktown Heights, NY, U.S.A.
{raduf,pitrelli,roukos,izitouni}us.ibm.com
Abstract
Information-extraction (IE) research typically
focuses on clean-text inputs. However, an IE
engine serving real applications yields many
false alarms due to less-well-formed input.
For example, IE in a multilingual broadcast
processing system has to deal with inaccu-
rate automatic transcription and translation.
The resulting presence of non-target-language
text in this case, and non-language mate-
rial interspersed in data from other applica-
tions, raise the research problem of making
IE robust to such noisy input text. We ad-
dress one such IE task: entity-mention de-
tection. We describe augmenting a statistical
mention-detection system in order to reduce
false alarms from spurious passages. The di-
verse nature of input noise leads us to pursue
a multi-faceted approach to robustness. For
our English-language system, at various miss
rates we eliminate 97% of false alarms on in-
puts from other Latin-alphabet languages. In
another experiment, representing scenarios in
which genre-specific training is infeasible, we
process real financial-transactions text con-
taining mixed languages and data-set codes.
On these data, because we do not train on data
like it, we achieve a smaller but significant im-
provement. These gains come with virtually
no loss in accuracy on clean English text.
1 Introduction
Information-extraction (IE) research is typically per-
formed on clean text in a predetermined language.
Lately, IE has improved to the point of being usable
for some real-world tasks whose accuracy require-
ments are reachable with current technology. These
uses include media monitoring, topic alerts, sum-
marization, population of databases for advanced
search, etc. These uses often combine IE with tech-
nologies such as speech recognition, machine trans-
lation, topic clustering, and information retrieval.
The propagation of IE technology from isolated
use to aggregates with such other technologies, from
NLP experts to other types of computer scientists,
and from researchers to users, feeds back to the IE
research community the need for additional inves-
tigation which we loosely refer to as ?information-
extraction robustness? research. For example:
1. Broadcast monitoring demands that IE handle
as input not only clean text, but also the tran-
scripts output by speech recognizers.
2. Multilingual applications, and the imperfection
of translation technology, require IE to contend
with non-target-language text input (Pitrelli et
al., 2008).
3. Naive users at times input to IE other material
which deviates from clean text, such as a PDF
file that ?looks? like plain text.
4. Search applications require IE to deal with
databases which not only possess clean text but
at times exhibit other complications like mark-
up codes particular to narrow, application-
specific data-format standards, for example, the
excerpt from a financial-transactions data set
shown in Figure 1.
Legacy industry-specific standards, such as il-
lustrated in this example, are part of long-
established processes which are cumbersome
to convert to a more-modern database format.
Transaction data sets typically build up over a
period of years, and as seen here, can exhibit
335
:54D://121000358
BANK OF BOSTON
:55D:/0148280005
NEVADA DEPT.OF VET.94C RECOV.FD
-5:MAC:E19DECA8CHK:641EB09B8968
USING OF FIELD 59: ONLY /INS/ WHEN
FOLLOWED BY BCC CODE IN CASE
OF QUESTIONS DONT HESITATE TO
CONTACT US QUOTING REFERENCE
NON-STC CHARGES OR VIA E-MAIL:
YOVANKA(UL)BRATASOVA(AT)BOA.CZ.
BEST REGARDS
BANKA OBCHODNIKA, A.S. PRAGUE, CZ
:58E::ADTX//++ ADDITIONAL
INFORMATION ++ PLEASE BE
INFORMED THAT AS A RESULT OF
THE PURCHASE OFFER ENDED ON 23
MAR 2008 CALDRADE LTD. IS
POSSESSING WITH MORE THEN 90
PER CENT VOTING RIGHT OF SLICE.
THEREFOR CALDRADE LTD. IS
EXERCISING PURCHASE RIGHTS
FOR ALL SLICE SHARES WHICH ARE
CURRENTLY NOT INHIS OWN.
PURCHASE PRICE: HUF 1.940 PER
SHARE. PLEASE :58E::ADTX//NOTE
THAT THOSE SHARES WHICH WILL
NOT BE PRESENTED TO THE OFFER
WILL BE CANCELLED AND INVALID.
:58:SIE SELBST
TRN/REF:515220 035
:78:RUECKGABE DES BETRAGES LT.
ANZBA43 M ZWECKS RUECKGABE IN
AUD. URSPR. ZU UNSEREM ZA MIT
REF. 0170252313279065 UND IHRE
RUECKG. :42:/BNF/UNSERE REF:
Figure 1: Example application-specific text, in this
case from financial transactions.
peculiar mark-up interspersed with meaning-
ful text. They also suffer complications arising
from limited-size entry fields and a diversity
of data-entry personnel, leading to effects like
haphazard abbreviation and improper spacing,
as shown. These issues greatly complicate the
IE problem, particularly considering that adapt-
ing IE to such formats is hampered by the exis-
tence of a multitude of such ?standards? and by
lack of sufficient annotated data in each one.
A typical state-of-the-art statistical IE engine will
happily process such ?noisy? inputs, and will typ-
ically provide garbage-in/garbage-out performance,
embarrassingly reporting spurious ?information? no
human would ever mistake. Yet it is also inappro-
priate to discard such documents wholesale: even
poor-quality inputs may have relevant information
interspersed. This information can include accurate
speech-recognition output, names which are recog-
nizable even in wrong-language material, and clean
target-language passages interleaved with the mark-
up. Thus, here we address methods to make IE ro-
bust to such varied-quality inputs. Specifically, our
overall goals are
? to skip processing non-language material such
as standard or database-specific mark-up,
? to process all non-target-language text cau-
tiously, catching interspersed target-language
text as well as text which is compatible with
the target language, e.g. person names which
are the same in the target- and non-target lan-
guage, and
? to degrade gracefully when processing anoma-
lous target-language material,
while minimizing any disruption of the processing
of clean, target-language text, and avoiding any ne-
cessity for explicit pre-classification of the genre of
material being input to the system. Such explicit
classification would be impractical in the presence
of the interleaving and the unconstrained data for-
mats from unpredetermined sources.
We begin our robustness work by addressing an
important and basic IE task: mention detection
(MD). MD is the task of identifying and classifying
textual references to entities in open-domain texts.
Mentions may be of type ?named? (e.g. John, Las
Vegas), ?nominal? (e.g. engineer, dentist)
or ?pronominal? (e.g. they, he). A mention also
336
has a specific class which describes the type of en-
tity it refers to. For instance, consider the following
sentence:
Julia Gillard, prime
minister of Australia,
declared she will enhance
the country?s economy.
Here we see three mentions of one person en-
tity: Julia Gillard, prime minister, and
she; these mentions are of type named, nominal,
and pronominal, respectively. Australia and
country are mentions of type named and nominal,
respectively, of a single geopolitical entity. Thus, the
MD task is a more general and complex task than
named-entity recognition, which aims at identifying
and classifying only named mentions.
Our approach to IE has been to use language-
independent algorithms, in order to facilitate reuse
across languages, but we train them with language-
specific data, for the sake of accuracy. Therefore, in-
put is expected to be predominantly in a target lan-
guage. However, real-world data genres inevitably
include some mixed-language/non-linguistic input.
Genre-specific training is typically infeasible due
to such application-specific data sets being unanno-
tated, motivating this line of research. Therefore, the
goal of this study is to investigate schemes to make a
language-specific MD engine robust to the types of
interspersed non-target material described above. In
these initial experiments, we work with English as
the target language, though we aim to make our ap-
proach to robustness as target-language-independent
as possible.
While our ultimate goal is a language-
independent approach to robustness, in these
initial experiments, English is the target language.
However, we process mixed-language material
including real-world data with its own peculiar
mark-up, text conventions including abbreviations,
and mix of languages, with the goal of English MD.
We approach robust MD using a multi-stage strat-
egy. First, non-target-character-set passages (here,
non-Latin-alphabet) are identified and marked for
non-processing. Then, following word-tokenization,
we apply a language classifier to a sliding variable-
length set of windows in order to generate fea-
tures for each word indicative of how much the text
around that word resembles good English, primar-
ily in comparison to other Latin-alphabet languages.
These features are used in a separate maximum-
entropy classifier whose output is a single feature to
add to the MD classifier. Additional features, pri-
marily to distinguish English from non-language in-
put, are added to MD as well. An example is the
minimum of the number of letters and the number of
digits in the ?word?, which when greater than zero
often indicates database detritus. Then we run the
MD classifier enhanced with these new robustness-
oriented features. We evaluate using a detection-
error-trade-off (DET) (Martin et al, 1997) anal-
ysis, in addition to traditional precision/recall/F -
measure.
This paper is organized as follows. Section 2 dis-
cusses previous work. Section 3 describes the base-
line maximum-entropy-based MD system. Section 4
introduces enhancements to the system to achieve
robustness. Section 5 describes databases used for
experiments, which are discussed in Section 6, and
Section 7 draws conclusions and plots future work.
2 Previous work on mention detection
The MD task has close ties to named-entity recog-
nition, which has been the focus of much recent re-
search (Bikel et al, 1997; Borthwick et al, 1998;
Tjong Kim Sang, 2002; Florian et al, 2003; Bena-
jiba et al, 2009), and has been at the center of sev-
eral evaluations: MUC-6, MUC-7, CoNLL?02 and
CoNLL?03 shared tasks. Usually, in computational-
linguistics literature, a named entity represents an
instance of either a location, a person, an organi-
zation, and the named-entity-recognition task con-
sists of identifying each individual occurrence of
names of such an entity appearing in the text. As
stated earlier, in this paper we are interested in
identification and classification of textual references
to object/abstraction mentions, which can be either
named, nominal or pronominal. This task has been
a focus of interest in ACE since 2003. The recent
ACE evaluation campaign was in 2008.
Effort to handle noisy data is still limited, espe-
cially for scenarios in which the system at decoding
time does not have prior knowledge of the input data
source. Previous work dealing with unstructured
data assumes the knowledge of the input data source.
As an example, E. Minkov et al (Minkov et al,
2005) assume that the input data is text from e-mails,
and define special features to enhance the detection
of named entities. Miller et al (Miller et al, 2000)
assume that the input data is the output of a speech
or optical character recognition system, and hence
extract new features for better named-entity recog-
nition. In a different research problem, L. Yi et al
eliminate the noisy text from the document before
337
performing data mining (Yi et al, 2003). Hence,
they do not try to process noisy data; instead, they
remove it. The approach we propose in this paper
does not assume prior knowledge of the data source.
Also we do not want to eliminate the noisy data, but
rather attempt to detect the appropriate mentions, if
any, that appear in that portion of the data.
3 Mention-detection algorithm
Similarly to classical NLP tasks such as base phrase
chunking (Ramshaw and Marcus, 1999) and named-
entity recognition (Tjong Kim Sang, 2002), we for-
mulate the MD task as a sequence-classification
problem, by assigning to each word token in the
text a label indicating whether it starts a specific
mention, is inside a specific mention, or is out-
side any mentions. We also assign to every non-
outside label a class to specify entity type e.g. per-
son, organization, location, etc. We are interested
in a statistical approach that can easily be adapted
for several languages and that has the ability to
integrate easily and make effective use of diverse
sources of information to achieve high system per-
formance. This is because, similar to many NLP
tasks, good performance has been shown to depend
heavily on integrating many sources of informa-
tion (Florian et al, 2004). We choose a Maximum
Entropy Markov Model (MEMM) as described pre-
viously (Florian et al, 2004; Zitouni and Florian,
2009). The maximum-entropy model is trained us-
ing the sequential conditional generalized iterative
scaling (SCGIS) technique (Goodman, 2002), and it
uses a Gaussian prior for regularization (Chen and
Rosenfeld, 2000)1.
3.1 Mention detection: standard features
The featues used by our mention detection systems
can be divided into the following categories:
1. Lexical Features Lexical features are imple-
mented as token n-grams spanning the current
token, both preceding and following it. For a
token xi, token n-gram features will contain the
previous n?1 tokens (xi?n+1, . . . xi?1) and the
following n? 1 tokens (xi+1, . . . xi+n?1). Set-
ting n equal to 3 turned out to be a good choice.
2. Gazetteer-based Features The gazetteer-
based features we use are computed on tokens.
1Note that the resulting model cannot really be called a
maximum-entropy model, as it does not yield the model which
has the maximum entropy (the second term in the product), but
rather is a maximum-a-posteriori model.
The gazetteers consist of several class of
dictionaries: including person names, country
names, company names, etc. Dictionar-
ies contain single names such as John or
Boston, and also phrases such as Barack
Obama, New York City, or The United
States. During both training and decoding,
when we encounter in the text a token or a
sequence of tokens that completely matches an
entry in a dictionary, we fire its corresponding
class.
The use of this framework to build MD systems
for clean English text has given very competitive re-
sults at ACE evaluations (Florian et al, 2006). Try-
ing other classifiers is always a good experiment,
which we didn?t pursue here for two reasons: first,
the MEMM system used here is state-of-the-art, as
proven in evaluations and competitions ? while it is
entirely possible that another system might get better
results, we don?t think the difference would be large.
Second, we are interested in ways of improving per-
formance on noisy data, and we expect any system
to observe similar degradation in performance when
presented with unexpected input ? showing results
for multiple classifier types might very well dilute
the message, so we stuck to one classifier type.
4 Enhancements for robustness
As stated above, our goal is to skip spans of charac-
ters which do not lend themselves to target-language
MD, while minimizing impact on MD for target-
language text, with English as the initial target lan-
guage for our experiments. More specifically, our
task is to process data automatically in any unprede-
termined format from any source, during which we
strive to avoid outputting spurious mentions on:
? non-language material, such as mark-up tags
and other data-set detritus, as well as non-text
data such as code or binaries likely mistakenly
submitted to the MD system,
? non-target-character-set material, here, non-
Latin-alphabet material, such as Arabic and
Chinese in their native character sets, and
? target-character-set material not in the target
language, here, Latin-alphabet languages other
than English.
It is important to note that this is not merely
a document-classification problem; this non-target
data is often interspersed with valid input text.
338
Mark-up is the obvious example of interspersing;
however, other categories of non-target data can also
interleave tightly with valid input. A few examples:
? English text is sometimes infixed right in a Chi-
nese sentence, such as
? some translation algorithms will leave un-
changed an untranslatable word, or will
transliterate it into the target language using a
character convention which may not be a stan-
dard known to the MD engine, and
? some target-alphabet-but-non-target-language
material will be compatible with the target
language, particularly people?s names. An
example with English as the target lan-
guage is Barack Obama in the Spanish
text ...presidente de Estados
Unidos, Barack Obama, dijo el
da 24 que ....
Therefore, to minimize needless loss of process-
able material, a robustness algorithm ideally does a
sliding analysis, in which, character-by-character or
word-by-word, material may be deemed to be suit-
able to process. Furthermore, a variety of strategies
will be needed to contend with the diverse nature of
non-target material and the patterns in which it will
appear among valid input.
Accordingly, the following is a summary of algo-
rithmic enhancements to MD:
1. detection of standard file formats, such as
SGML, and associated detagging,
2. segmentation of the file into target- vs. non-
target-character-set passages, such that the lat-
ter not be processed further,
3. tokenization to determine word and sentence
units, and
4. MD, augmented as follows:
? Sentence-level categorization of likeli-
hood of good English.
? If ?clean? English was detected, run the
same clean baseline model as described in
Section 3.
? If the text is determined to be a
bad fit to English, run an alternate
maximum-entropy model that is heavily
based on gazetteers, using only context-
independent (e.g. primarily gazetteer-
based) features, to catch isolated ob-
vious English/English-compatible names
embedded in otherwise-foreign text.
? If in between ?clean? and ?bad?, use
a ?mixed? maximum-entropy MD model
whose training data and feature set are
augmented to handle interleaving of En-
glish with mark-up and other languages.
These MD-algorithm enhancements will be de-
scribed in the following subsections.
4.1 Detection and detagging for standard file
formats
Some types of mark-up are well-known standards,
such as SGML (Warmer and van Egmond, 1989).
Clearly the optimal way of dealing with them is to
apply detectors of these specific formats, and associ-
ated detaggers, as done previously (Yi et al, 2003).
For this reason, standard mark-up is not a subject of
the current study; rather, our concern is with mark-
up peculiar to specific data sets, as described above,
and so while this step is part of our overall strategy,
it is not employed in the present experiments.
4.2 Character-set segmentation
Some entity mentions may be recognizable in a non-
target language which shares the target-language?s
character set, for example, a person?s name recog-
nizable by English speakers in an otherwise-not-
understandable Spanish sentence. However, non-
target character sets, such as Arabic and Chinese
when processing English, represent pure noise for
an IE system. Therefore, deterministic character-
set segmentation is applied, to mark non-target-
character-set passages for non-processing by the re-
mainder of the system, or, in a multilingual system,
to be diverted to a subsystem suited to process that
character set. Characters which can be ambiguous
with regard to character set, such as some punctua-
tion marks, are attached to target-character-set pas-
sages when possible, but are not considered to break
non-target-character-set passages surrounding them
on both sides.
4.3 Tokenization
Subsequent processing is based on determination of
the language of target-alphabet text. The fundamen-
tal unit of such processing is target-alphabet word,
necessitating tokenization at this point into word-
level units. This step includes punctuation sepa-
339
ration as well as the detction of sentence bound-
ary (Zimmerman et al, 2006).
4.4 Robust mention detection
After preprocessing steps presented earlier, we de-
tect mentions using a cascaded approach that com-
bines several MD classifiers. Our goal is to select
among maximum-entropy MD classifiers trained
separately to represent different degrees of ?nois-
iness? occurring in many genres of data, includ-
ing machine-translation output, informal communi-
cations, mixed-language material, varied forms of
non-standard database mark-up, etc. We somewhat-
arbitrarily choose to employ three classifiers as de-
scribed below. We select a classifier based on a
sentence-level determination of the material?s fit to
the target language. First, we build an n-gram lan-
guage model on clean target-language training text.
This language model is used to compute the perplex-
ity (PP ) of each sentence during decoding. The
PP indicates the quality of the text in the target-
language (i.e. English) (Brown et al, 1992); the
lower the PP , the cleaner the text. A sentence
with a PP lower than a threshold ?1 is considered
?clean? and hence the ?clean? baseline MD model
described in Section 3 is used to detect mentions
of this sentence. The clean MD model has access
to standard features described in Section 3.1. In
the case where a sentence looks particularly badly
matched to the target language, defined as PP > ?2,
we use a ?gazetteer-based? model based on a dic-
tionary look-up to detect mentions; we retreat to
seeking known mentions in a context-independent
manner reflecting that most of the context consists
of out-of-vocabulary words. The gazetteer-based
MD model has access only to gazetteer information
and does not look to lexical context during decod-
ing, reflecting the likelihood that in this poor ma-
terial, words surrounding any recognizable mention
are foreign and therefore unusable. In the case of an
in-between determination, that is, a sentence with
?1 < PP < ?2, we use a ?mixed? MD model, based
on augmenting the training data set and the feature
set as described in the next section. The values of ?1
and ?2 are estimated empirically on a separate devel-
opment data set that is also used to tune the Gaussian
prior (Chen and Rosenfeld, 2000). This set contains
a mix of clean English and Latin-alphabet-but-non-
English text that is not used for traning and evalua-
tion.
The advantage of this combination strategy is that
we do not need pre-defined knowledge of the text
source in order to apply an appropriate model. The
selection of the appropriate model to use for de-
coding is done automatically based on PP value of
the sentence. We will show in the experiments sec-
tion how this combination strategy is effective not
only in maintaining good performance on a clean
English text but also in improving performance on
non-English data when compared to other source-
specific MD models.
4.5 Mixed mention detection model
The mixed MD model is designed to process ?sen-
tences? mixing English with non-English, whether
foreign-language or non-language material. Our
approach is to augment model training compared
to the clean baseline by adding non-English,
mixed-language, and non-language material, and
to augment the model?s feature set with language-
identification features more localized than the
sentence-level perplexity described above, as well as
other features designed primarily to distinguish non-
language material such as mark-up codes.
4.5.1 Language-identification features
We apply an n-gram-based language classi-
fier (Prager, 1999) to variable-length sliding win-
dows as follows. For each word, we run 1- through
6-preceding-word windows through the classifier,
and 1- through 6-word windows beginning with the
word, for a total of 12 windows, yielding for each
window a result like:
0.235 Swedish
0.148 English
0.134 French
...
For each of the 12 results, we extract three fea-
tures: the identity of the top-scoring language, here,
Swedish; the confidence score in the top-scoring
language, here, 0.235; and the score difference be-
tween the target language (English for these ex-
periments) and the top-scoring non-target language,
here, 0.148 ? 0.235 = ?0.087. Thus we have
a 36-feature vector for each word. We bin these
and use them as input to a maximum-entropy clas-
sifier (separate from the MD classifier) which out-
puts ?English? or ?Non-English?, and a confidence
score. These scores in turn are binned into six cate-
gories to serve as a ?how-English-is-it? feature in the
augmented MD model. The language-identification
classifier and the maximum-entropy ?how-English?
classifier are each trained on text data separate from
340
each other and from the training and test sets for
MD.
4.5.2 Additional features
The following features are designed to capture
evidence of whether a ?word? is in fact linguistic
material or not: number of alphabetic characters,
number of characters, maximum consecutive rep-
etitions of a character, numbers of non-alphabetic
and non-alphanumeric characters, fraction of char-
acters which are alphabetic, fraction alphanumeric,
and number of vowels. These features are part of the
augmentation of the mixed MD model relative to the
clean MD model.
5 Data sets
Four data sets are used for our initial experiments.
One, ?English?, consists of 367 documents total-
ing 170,000 words, drawn from web news stories
from various sources and detagged to be plain text.
This set is divided into 340 documents as a train-
ing set and 27 for testing, annotated as described in
more detail elsewhere (Han, 2010). These data av-
erage approximately 21 annotated mentions per 100
words.
The second set, ?Latin?, consists of 23 detagged
web news articles from 11 non-English Latin-
alphabet languages totaling 31,000 words. Of these
articles, 12 articles containing 19,000 words are
used as a training set, with the remaining used for
testing, and each set containing all 11 languages.
They are annotated using the same annotation con-
ventions as ?English?, and from the perspective of
English; that is, only mentions which would be clear
to an English speaker are labeled, such as Barack
Obama in the Spanish example in Section 4. For
this reason, these data average only approximately 5
mentions per 100 words.
The third, ?Transactions?, consists of approxi-
mately 60,000 words drawn from a text data set
logging real financial transactions. Figure 1 shows
example passages from this database, anonymized
while preserving the character of the content.
This data set logs transactions by a staff of
customer-service representatives. English is the pri-
mary language, but owing to international clientele,
occasionally representatives communicate in other
languages, such as the German here, or in English
but mentioning institutions in other countries, here, a
Czech bank. Interspersed among text are codes spe-
cific to this application which delineate and identify
various information fields and punctuate long pas-
sages. The application also places constraints on
legal characters, leading to the unusual representa-
tion of underline and the ?at? sign as shown, mak-
ing for an e-mail address which is human-readable
but likely not obvious to a machine. Abbreviations
represent terms particularly common in this appli-
cation area, though they may not be obvious with-
out adapting to the application; these include stan-
dards like HUF, a currency code which stands for
Hungarian forint, and financial-transaction peculiar-
ities like BNF for ?beneficiary? as seen in Figure 1.
In short, good English is interspersed with non-
language content, foreign-language text, and rough
English like data-entry errors and haphazard abbre-
viations. These data average 4 mentions per 100
words.
Data sets with peculiarities analogous to those in
this Transactions set are commonplace in a variety
of settings. Training specific to data sets like this is
often infeasible due to lack of labeled data, insuffi-
cient data for training, and the multitude of such data
formats. For this reason, we do not train on Transac-
tions, letting our testing on this data set serve as an
example of testing on such data formats unseen.
6 Experiments
MD systems were trained to recognize the 116
entity-mention types shown in Table 1, annotated as
described previously (Han, 2010). The clean-data
classifier was trained on the English training data us-
ing the feature set described in Section 3.1. The clas-
sifier for ?mixed?-quality data and the ?gazetteer?
model were each trained on that set plus the ?Latin?
training set and the supplemental set. In addition,
?mixed? training included the additional features de-
scribed in Section 4.5. The framework used to build
the baseline MD system is similar to the one we used
in the ACE evaluation2. This system has achieved
competitive results with an F -measure of 82.7 when
trained on the seven main types of ACE data with
access to wordnet and part-of-speech-tag informa-
tion as well as output of other MD and named-entity
recognizers (Zitouni and Florian, 2008).
It is instructive to evaluate on the individual com-
ponent systems as well as the combination, despite
the fact that the individual components are not well-
suited to all the data sets, for example, the mixed
and gazetteer systems being a poorer fit to the En-
glish task than the baseline, and vice versa for the
2NIST?s ACE evaluation plan:
http://www.nist.gov/speech/tests/ace/index.htm
341
age event-custody facility people date
animal event-demonstration food percent duration
award event-disaster geological-object person e-mail-address
cardinal event-legal geo-political product measure
disease event-meeting law substance money
event event-performance location title-of-a-work phone-number
event-award event-personnel ordinal vehicle ticker-symbol
event-communication event-sports organ weapon time
event-crime event-violence organization web-address
Table 1: Entity-type categories used in these experiments. The eight in the right-most column are not
further distinguished by mention type, while the remaining 36 are further classified as named, nominal or
pronominal, for a total of 36 ? 3 + 8 = 116 mention labels.
English Latin Transactions
P R F P R F P R F
Clean 78.7 73.6 76.1 16.0 40.0 22.9 19.5 32.2 24.3
Mixed 77.9 69.7 73.6 78.5 55.9 65.3 37.1 47.8 41.7
Gazetteer 76.9 66.2 71.1 77.8 55.5 64.8 36.5 47.5 41.3
Combination 78.1 73.2 75.6 80.4 56.0 66.0 38.5 49.1 43.2
Table 2: Performance of clean, mixed, and gazetteer-based mention detection systems as well as their com-
bination. Performance is presented in terms of Precision (P), Recall (R), and F -measure (F).
non-target data sets. Precision/recall/F -measure re-
sults are shown in Table 2. Not surprisingly, the
baseline system, intended for clean data, performs
poorly on noisy data. The mixed and gazetteer sys-
tems, having a variety of noisy data in their train-
ing set, perform much better on the noisy conditions,
particularly on Latin-alphabet-non-English data be-
cause that is one of the conditions included in its
training, while Transactions remains a condition not
covered in the training set and so shows less im-
provement. However, because the mixed classifier,
and moreso the gazetteer classifier, are oriented to
noisy data, on clean data they suffer in performance
by 2.5 and 5 F -measure points, respectively. But
system combination serves us well: it recovers all
but 0.5 F -measure point of this loss, while also ac-
tually performing better on the noisy data sets than
the two classifiers specifically targeted toward them,
as can be seen in Table 2. It is important to note
that the major advantage of using the combination
model is the fact that we do not have to know the
data source in order to select the appropriate MD
model to use. We assume that the data source is
unknown, which is our claim in this work, and we
show that we obtain better performance than using
source-specific MD models. This reflects the fact
that a noisy data set will in fact have portions with
varying degrees of ?noise?, so the combination out-
performs any single model targeted to a single par-
ticular level of noise, enabling the system to con-
tend with such variability without the need for pre-
segregating sub-types of data for noise level. The
obtained improvement from the system combination
over all other models is statistically significant based
on the stratified bootstrap re-sampling significance
test (Noreen, 1989). We consider results statistically
significant when p < 0.05, which is the case in this
paper. This approach was used in the named-entity-
recognition shared task of CoNNL-20023.
It should be noted that some completely-non-
target types of data, such as non-target-character set
data, have been omitted from analysis here. In-
cluding them would make our system look compar-
atively stronger, as they would have only spurious
mentions and so generate false alarms but no correct
mentions in the baseline system, while our system
deterministically removes them.
As mentioned above, we view MD robustness pri-
marily as an effort to eliminate, relative to a base-
line system, large volumes of spurious ?mentions?
detected in non-target input content, while minimiz-
3http://www.cnts.ua.ac.be/conll2002/ner/
342
(a) DET plot for clean (baseline), mixed, gazetteer,
and combination MD systems on the Latin-alphabet-
non-English text. The clean system (upper curve)
performs far worse than the other three systems de-
signed to provide robustness; these systems in turn
perform nearly indistinguishably.
(b) DET plot for clean (baseline), mixed, gazetteer,
and combination MD systems on the Transactions
data set. The clean system (upper/longer curve)
reaches far higher false-alarm rates, while never ap-
proaching the lower miss rates achievable by any of
the other three systems, which in turn perform com-
parably to each other.
Figure 2: DET plots for Latin-alphabet-non-English and Transactions data sets
ing disruption of detection in target input. A sec-
ondary goal is recall in the event of occasional valid
mentions in such non-target material. Thus, as in-
put material degrades, precision increases in impor-
tance relative to recall. As such, we view precision
and recall asymmetrically on this task, and so rather
than evaluating purely in terms of F -measure, we
perform a detection-error-trade-off (DET) (Martin
et al, 1997) analysis, in which we plot a curve of
miss rate on valid mentions vs. false-alarm rate, with
the curve traced by varying a confidence threshold
across its range. We measure false-alarm and miss
rates relative to the number of actual mentions anno-
tated in the data set:
FA rate = # false alarms# annotated mentions (1)
Miss rate = # misses# annotated mentions (2)
where false alarms are ?mentions? output by the sys-
tem but not appearing in annotation, while misses
are mentions which are annotated but do not ap-
pear in the system output. Each mention is treated
equally in this analysis, so frequently-recurring en-
tity/mention types weigh on the results accordingly.
Figure 2a shows a DET plot for the clean, mixed,
gazetteer, and combination systems on the ?Latin?
data set, while Figure 2b shows the analogous plot
for the ?Transactions? data set. The drastic gains
made over the baseline system by the three experi-
mental systems are evident in the plots. For exam-
ple, on Latin, choosing an operating point of a miss
rate of 0.6 (nearly the best achievable by the clean
system), we find that the robustness-oriented sys-
tems eliminate 97% of the false alarms of the clean
baseline system, as the plot shows false-alarm rates
near 0.07 compared to the baseline?s of 2.08. Gains
on Transaction data are more modest, owing to this
case representing a data genre not included in train-
ing. It should be noted that the jaggedness of the
Transaction curves traces to the repetitive nature of
some of the terms in this data set.
In making a system more oriented toward robust-
ness in the face of non-target inputs, it is important
to quantify the effect of these systems being less-
oriented toward clean, target-language text. Figure 3
shows the analogous DET plot for the English test
set, showing that achieving robustness through the
combination system comes at a small cost to accu-
racy on the text the original system is trained to pro-
cess.
7 Conclusions
For information-extraction systems to be useful,
their performance must degrade gracefully when
confronted with inputs which deviate from ideal
and/or derive from unknown sources in unknown
formats. Imperfectly-translated, mixed-language,
marked-up text and non-language material must not
343
Figure 3: DET plot for clean (baseline), mixed,
gazetteer, and combination MD systems on clean English
text, verifying that performance by the clean system (low-
est curve) is very closely approximated by the combina-
tion system (second-lowest curve), while the mixed sys-
tem performs somewhat worse and the gazetteer system
(top curve), worse still, reflecting that these systems are
increasingly oriented toward noisy inputs.
be processed in a garbage-in-garbage-out fashion
merely because the system was designed only to
handle clean text in one language. Thus we have em-
barked on information-extraction-robustness work,
to improve performance on imperfect inputs while
minimizing disruption of processing of clean text.
We have demonstrated that for one IE task, mention
detection, a multi-faceted approach, motivated by
the diversity of input data imperfections, can elimi-
nate a large proportion of the spurious outputs com-
pared to a system trained on the target input, at a
relatively small cost of accuracy on that target input.
This outcome is achieved by a system-combination
approach in which a perplexity-based measure of
how well the input matches the target language is
used to select among models designed to deal with
such varying levels of noise. Rather than relying on
explicit recognition of genre of source data, the ex-
perimental system merely does its own assessment
of how much each sentence-sized chunk matches the
target language, an important feature in the case of
unknown text sources.
Chief among directions for further work is to con-
tinue to improve performance on noisy data, and to
strengthen our findings via larger data sets. Addi-
tionally, we look forward to expanding analysis to
different types of imperfect input, such as machine-
translation output, different types of mark-up, and
different genres of real data. Further work should
also explore the degree to which the approach to
achieving robustness must vary according to the tar-
get language. Finally, robustness work should be ex-
panded to other information-extraction tasks.
Acknowledgements
The authors thank Ben Han, Anuska Renta,
Veronique Baloup-Kovalenko and Owais Akhtar for
their help with annotation. This work was supported
in part by DARPA under contract HR0011-08-C-
0110.
References
Y. Benajiba, M. Diab, and P. Rosso. 2009. Arabic named
entity recognition: A feature-driven study. In the spe-
cial issue on Processing Morphologically Rich Lan-
guages of the IEEE Transaction on Audio, Speech and
Language.
D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings of ANLP-97, pages 194?201.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, J. C.
Lai, and R. L. Mercer. 1992. An estimate of an up-
per bound for the entropy of English. Computational
Linguistics, 18(1), March.
S. Chen and R. Rosenfeld. 2000. A survey of smooth-
ing techniques for ME models. IEEE Transaction on
Speech and Audio Processing.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In Conference on Computational Natural Lan-
guage Learning - CoNLL-2003, Edmonton, Canada,
May.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proceedings of HLT-NAACL 2004, pages
1?8.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006.
Factorizing complex models: A case study in men-
tion detection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 473?480, Sydney, Australia,
July. Association for Computational Linguistics.
J. Goodman. 2002. Sequential conditional generalized
iterative scaling. In Proceedings of ACL?02.
D. B. Han. 2010. Klue annotation guidelines - version
2.0. Technical Report RC25042, IBM Research, Au-
gust.
344
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocki. 1997. The DET curve in assessment
of detection task performance. In Proceedings of the
European Conference on Speech Communication and
Technology (Eurospeech), pages 1895?1898. Rhodes,
Greece.
D. Miller, S. Boisen, R. Schwartz, R. Stone, and
R. Weischedel. 2000. Named entity extraction from
noisy input: speech and OCR. In Proceedings of the
sixth conference on Applied natural language process-
ing, pages 316?324, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-
tracting personal names from email: Applying named
entity recognition to informal text. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 443?450, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
E. W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley Sons.
J. F. Pitrelli, B. L. Lewis, E. A. Epstein, M. Franz,
D. Kiecza, J. L. Quinn, G. Ramaswamy, A. Srivas-
tava, and P. Virga. 2008. Aggregating Distributed
STT, MT, and Information Extraction Engines: The
GALE Interoperability-Demo System. In Interspeech.
Brisbane, NSW, Australia.
J. M. Prager. 1999. Linguini: Language identification for
multilingual documents. In Journal of Management
Information Systems, pages 1?11.
L. Ramshaw and M. Marcus. 1999. Text chunking using
transformation-based learning. In S. Armstrong, K.W.
Church, P. Isabelle, S. Manzi, E. Tzoukermann, and
D. Yarowsky, editors, Natural Language Processing
Using Very Large Corpora, pages 157?176. Kluwer.
E. F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independentnamed entity
recognition. In Proceedings of CoNLL-2002, pages
155?158. Taipei, Taiwan.
J. Warmer and S. van Egmond. 1989. The implementa-
tion of the Amsterdam SGML parser. Electron. Publ.
Origin. Dissem. Des., 2(2):65?90.
L. Yi, B. Liu, and X. Li. 2003. Eliminating noisy in-
formation in web pages for data mining. In KDD ?03:
Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 296?305, New York, NY, USA. ACM.
M. Zimmerman, D. Hakkani-Tur, J. Fung, N. Mirghafori,
L. Gottlieb, E. Shriberg, and Y. Liu. 2006. The
ICSI+ multilingual sentence segmentation system. In
Interspeech, pages 117?120, Pittsburgh, Pennsylvania,
September.
I. Zitouni and R. Florian. 2008. Mention detection
crossing the language barrier. In Proceedings of
EMNLP?08, Honolulu, Hawaii, October.
I. Zitouni and R. Florian. 2009. Cross-language informa-
tion propagation for Arabic mention detection. ACM
Transactions on Asian Language Information Process-
ing (TALIP), 8(4):1?21.
345
