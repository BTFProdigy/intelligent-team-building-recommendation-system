Proceedings of NAACL HLT 2007, Companion Volume, pages 1?4,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Comparing User Simulation Models For Dialog Strategy Learning
Hua Ai
University of Pittsburgh
Intelligent Systems Program
Pittsburgh PA, 15260, USA
hua@cs.pitt.edu
Joel R. Tetreault
University of Pittsburgh
LRDC
Pittsburgh PA, 15260, USA
tetreaul@pitt.edu
Diane J. Litman
University of Pittsburgh
Dept. of Computer Science
LRDC
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Abstract
This paper explores what kind of user sim-
ulation model is suitable for developing
a training corpus for using Markov Deci-
sion Processes (MDPs) to automatically
learn dialog strategies. Our results sug-
gest that with sparse training data, a model
that aims to randomly explore more dialog
state spaces with certain constraints actu-
ally performs at the same or better than a
more complex model that simulates real-
istic user behaviors in a statistical way.
1 Introduction
Recently, user simulation has been used in the de-
velopment of spoken dialog systems. In contrast to
experiments with human subjects, which are usually
expensive and time consuming, user simulation gen-
erates a large corpus of user behaviors in a low-cost
and time-efficient manner. For example, user sim-
ulation has been used in evaluation of spoken dia-
log systems (Lo?pez-Co?zar et al, 2003) and to learn
dialog strategies (Scheffler, 2002). However, these
studies do not systematically evaluate how helpful a
user simulation is. (Schatzmann et al, 2005) pro-
pose a set of evaluation measures to assess the re-
alness of the simulated corpora (i.e. how similar
are the simulated behaviors and human behaviors).
Nevertheless, how realistic a simulated corpus needs
to be for different tasks is still an open question.
We hypothesize that for tasks like system eval-
uation, a more realistic simulated corpus is prefer-
able. Since the system strategies are evaluated and
adapted based on the analysis of these simulated dia-
log behaviors, we would expect that these behaviors
are what we are going to see in the test phase when
the systems interact with human users. However,
for automatically learning dialog strategies, it is not
clear how realistic versus how exploratory (Singh et
al., 2002) the training corpus should be. A train-
ing corpus needs to be exploratory with respect to
the chosen dialog system actions because if a cer-
tain action is never tried at certain states, we will
not know the value of taking that action in that state.
In (Singh et al, 2002), their system is designed to
randomly choose one from the allowed actions with
uniform probability in the training phase in order to
explore possible dialog state spaces. In contrast,we
use user simulation to generate exploratory training
data because in the tutoring system we work with,
reasonable tutor actions are largely restricted by stu-
dent performance. If certain student actions do not
appear, this system would not be able to explore a
state space randomly .
This paper investigates what kind of user simula-
tion is good for using Markov Decision Processes
(MDPs) to learn dialog strategies. In this study,
we compare three simulation models which differ in
their efforts on modeling the dialog behaviors in a
training corpus versus exploring a potentially larger
dialog space. In addition, we look into the impact of
different state space representations and different re-
ward functions on the choice of simulation models.
2 System and Corpus
Our system is a speech-enabled Intelligent Tutor-
ing System that helps students understand qualita-
1
tive physics questions. The dialog policy was deter-
ministic and hand-crafted in a finite state paradigm
(Ai et al, 2006). We collected 130 dialogs (1019
student utterances) with 26 human subjects. Cor-
rectness (correct(c), incorrect(ic)) is automatically
judged by the system1 and kept in the system?s logs.
Percent incorrectness (ic%) is also automatically
calculated and logged. Each student utterance was
manually annotated for certainty (certain, uncer-
tain, neutral, mixed) in a previous study2 based on
both lexical and prosodic information. In this study,
we use a two-way classification (certain(cert), not-
certain(ncert)), where we collapse uncertain, neu-
tral, and mixed to be ncert to balance our data. An
example of coded dialog between the tutor (T) and a
student (S) is given in Table 1.
3 Experimental Setup
3.1 Learning Task
Our current system can only respond to the cor-
rectness of a student?s utterances; the system thus
ignores other underlying information, for exam-
ple, certainty which is believed to provide use-
ful information for the tutor. In our corpus, the
strength of the tutor?s minimal feedback (defined be-
low) is in fact strongly correlated with the percent-
age of student certainty (chi-square test, p<0.01).
Strong Feedback (SF) is when the tutor clearly states
whether the student?s answer is correct or incor-
rect (i.e., ?This is great!?); Weak Feedback (WF)
is when the tutor does not comment on the correct-
ness of a student?s answer or gives slightly negative
feedback such as ?well?. Our goal is to learn how
to manipulate the strength of the tutor minimal feed-
back in order to maximize student?s overall certainty
in the entire dialog. We keep the other parts of the
tutor feedback (e.g. explanations, questions) so the
system?s original design of maximizing the percent-
age of student correct answers is utilized.
3.2 Simulation Models
All three models we describe below are trained from
the real corpus we collected. We simulate on the
word level because generating student?s dialog acts
alone does not provide sufficient information for
1Kappa of 0.79 is gained comparing to human judgements.
2Kappa of 0.68 is gained in a preliminary agreement study.
T1: Which law of motion would you use?
S1: Newton?s second law? [ic, ic%=1, ncert]
T2: Well... The best law to use is Newton?s
third law. Do you recall what it says?
S2: For every action there is an equal and
opposite reaction? [c, ic%=50%, ncert]
Table 1: Sample coded dialog excerpt.
our tutoring system to decide the next system?s ac-
tion. Thus, the output of the three models is a stu-
dent utterance along with the student certainty (cert,
ncert). Since it is hard to generate a natural lan-
guage utterance for each tutor?s question, we use the
student answers in the real corpus as the candidate
answers for the simulated students (Ai et al, 2006).
In addition, we simulate student certainty in a very
simple way: the simulation models output the cer-
tainty originally associated with that utterance.
Probabilistic Model (PM) is meant to capture re-
alistic student behavior in a probabilistic way. Given
a certain tutor question along with a tutor feedback,
it will first compute the probabilities of the four
types of student answers from the training corpus: c
and cert, c and ncert, ic and cert, and ic and ncert.
Then, following this distribution, the model selects
the type of student answers to output, and then it
picks an utterance that satisfies the correctness and
certainty constraints of the chosen answer type from
the candidate answer set and outputs that utterance.
We implement a back-off mechanism to count pos-
sible answers that do not appear in the real corpus.
Total Random Model (TRM) ignores what the
current question is or what feedback is given. It ran-
domly picks one utterance from all the utterances in
the entire candidate answer set. This model tries to
explore all the possible dialog states.
Restricted Random Model (RRM) differs from
the PM in that given a certain tutor question and a
tutor feedback, it chooses to give a c and cert, c and
ncert, ic and cert, or ic and ncert answer with equal
probability. This model is a compromise between
the exploration of the dialog state space and the re-
alness of generated user behaviors.
3.3 MDP Configuration
A MDP has four main components: states, actions,
a policy, and a reward function. In this study, the ac-
tions allowed in each dialog state are SF and WF;
2
the policy we are trying to learn is in every state
whether the tutor should give SF and WF in order
to maximize the percent certainty in the dialog.
Since different state space representations and re-
ward functions have a strong impact on the MDP
policy learning, we investigate different configura-
tions to avoid possible bias introduced by certain
configurations. We use two state space representa-
tions: SSR1 uses the correctness of current student
turn and percent incorrectness so far; and SSR2 adds
in the certainty of the current student turn on top of
SSR1. Two reward functions are investigated: in
RF1, we assign +100 to every dialog that has a per-
cent certainty higher than the median from the train-
ing corpus, and -100 to every dialog that has a per-
cent certainty below the median; in RF2, we assign
different rewards to every different dialog by multi-
plying the percent certainty in that dialog with 100.
Other MDP parameter settings are the same as de-
scribed in (Tetreault et al, 2006).
3.4 Methodology
We first let the three simulation models interact with
the original system to generate different training cor-
pora. Then, we learn three MDP policies in a fixed
configuration from the three training corpora sep-
arately. For each configuration, we run the sim-
ulation models until we get enough training data
such that the learned policies on that corpus do not
change anymore (40,000 dialogs are generated by
each model). After that, the learned new policies are
implemented into the original system respectively 3.
Finally, we use our most realistic model, the PM,
to interact with each new system 500 times to eval-
uate the new systems? performances. We use two
evaluation measures. EM1 is the number of dialogs
that would be assigned +100 using the old median
split. EM2 is the average of percent certainty in ev-
ery single dialog from the newly generated corpus.
A policy is considered better if it can improve the
percentage of certainty more than other policies, or
has more dialogs that will be assigned +100. The
baseline for EM1 is 250, since half of the 500 di-
alogs would be assigned +100 using the old median
3For example, the policy learned from the training corpus
generated by the RRM with SSR1 and RF1 is: give SF when
the current student answer is ic and ic%>50%, otherwise give
WF.
split. The baseline for EM2 is 35.21%, which is
obtained by calculating the percent certainty in the
corpus generated by the 40,000 interactions between
the PM and the original system.
4 Results and Discussion
Table 2 summarizes our results. There are two
columns under each ?state representation+reward
function? configuration, presenting the results using
the two evaluation approaches. EM1 measures ex-
actly what RF1 tries to optimize; while EM2 mea-
sures exactly what RF2 tries to optimize. However,
we show the results evaluated by both EM1 and
EM2 for all configurations since the two evaluation
measures have their own practical values and can
be deployed under different design requirements.
All results that significantly4 outperform the corre-
sponding baselines are marked with ?.
When evaluating using EM1, the RRM signifi-
cantly4 outperforms the other two models in all con-
figurations (in bold in Table 2). Also, the PM per-
forms better (but not statistically significantly) than
the TRM. When evaluating on EM2, the RRM sig-
nificantly4 outperforms the other two when using
SSR1 and RF1 (in bold in Table 2). In all other
configurations, the three models do not differ signif-
icantly. It is not surprising that the RRM outper-
forms the PM in most of the cases even when we
test on the PM. (Schatzmann et al, 2005) also ob-
serve that a good model can still perform well when
tested on a poor model.
We suspect that the performance of the PM is
harmed by the data sparsity issue in the real cor-
pus that we trained the model on. Consider the case
of SSR1: 25.8% of the potentially possible dialog
states do not exist in the real corpus. Although we
implement a back-off mechanism, the PM will still
have much less chance to transition to the states that
are not observed in the real corpus. Thus, when we
learn the MDP policy from the corpus generated by
this model, the actions to take in these less-likely
states are not fully learned. In contrast, the RRM
transitions from one state to each of the next possible
states with equal probability, which compensates for
the data sparsity problem. We further examine the
results obtained using SSR1 and RF1 and evaluated
4Using 2-sided t-test with Bonferroni correction, p<0.05.
3
Model Name SSR1+RF1 SSR2+RF1 SSR1+RF2 SSR2+RF2
EM1 EM2 EM1 EM2 EM1 EM2 EM1 EM2
Probabilistic Model 222 36.30% 217 37.63% 197 40.78%? 197 40.01%?
Total Random Model 192 36.30% 211 38.57% 188 40.21%? 179 40.21%?
Restricted Random Model 390? 46.11%? 368? 37.27% 309 40.21%? 301 40.21%?
Table 2: Evaluation of the new policies trained with the three simulation models
by EM1 to confirm our hypothesis. When looking
into the frequent states5, 70.1% of them are seen fre-
quently in the training corpus generated by the PM,
while 76.3% are seen frequently in the training cor-
pus generated by the RRM. A higher percentage in-
dicates the policy might be better trained with more
training instances. This explains why the RRM out-
performs the PM in this case.
While the TRM also tries to explore dialog state
space, only 65.2% of the frequent states in testing
phase are observed frequently in the training phase.
This is because the Total Random Model answers
90% of the questions incorrectly and often goes
deeply down the error-correction paths. It does ex-
plore some states that are at the end of the paths,
but since these are the infrequent states in the test
phase, exploring these states does not actually im-
prove the model?s performance much. On the other
hand, while the student correctness rate in the real
corpus is 60%, the RRM prevents itself from being
trapped in the less-likely states on incorrect answer
paths by keeping its correctness rate to be 50%.
Our results are preliminary but suggest interest-
ing points in building simulation models: 1. When
trained from a sparse data set, it may be better to
use a RRM than a more realistic PM or a more ex-
ploratory TRM; 2. State space representation may
not impact evaluation results as much as reward
functions and evaluation measures, since when us-
ing RF2 and evaluating with EM2, the differences
we see using RF1 or EM1 become less significant.
In our future work, we are going to further investi-
gate whether the trends shown in this paper general-
ize to on-line MDP policy learning. We also want to
explore other user simulations that are designed for
sparse training data (Henderson et al, 2005). More
5We define frequent states to be those that comprise at least
1% of the entire corpus. These frequent states add up to more
than 80% of the training/testing corpus. However, deciding the
threshold of the frequent states in training/testing is an open
question.
importantly, we are going to test the new policies
with the other simulations and human subjects to
validate the learning process.
Acknowledgements
NSF (0325054, 0328431) supports this research.
The authors wish to thank Tomas Singliar for his
valuable suggestions, Scott Silliman for his support
on building the simulation system, and the anony-
mous reviewers for their insightful comments.
References
H. Ai and D. Litman. 2006. Comparing Real-Real,
Simulated-Simulated, and Simulated-Real Spoken Di-
alogue Corpora. In Proc. AAAI Workshop on Statis-
tical and Empirical Approaches for SDS.
J. Henderson, O.Lemon, and K.Georgila. 2005. Hybrid
reinforcement/supervised learning for dialogue poli-
cies from COMMUNICATOR data. In Proc. IJCAI
workshop on Knowledge and Reasoning in Practical
Dialogue Systems.
R. Lo?pez-Co?zar, A. De la Torre, J. Segura, and A. Ru-
bio. 2003. Assessment of dialog systems by means of
a new simulation technique. Speech Communication
(40): 387-407.
K. Scheffler. 2002. Automatic Design of Spoken Dialog
Systems. Ph.D. diss., Cambridge University.
J. Schatzmann, K. Georgila, and S. Young. 2005. Quan-
titative Evaluation of User Simulation Techniques for
Spoken Dialog Systems. In Proc. of 6th SIGdial.
J. Schatzmann, M. N. Stuttle, K. Weilhammer and
S. Young. 2005. Effects of the User Model on
Simulation-based Learning of Dialogue Strategies. In
Proc. of ASRU05.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing Dialog Managment with Reinforcement
Learning: Experiments with the NJFun System. Jour-
nal of Artificial Intelligence Research, (16):105-133.
J. Tetreault and D. Litman. 2006. Comparing the Utility
of State Features in Spoken Dialogue Using Reinforce-
ment Learning.. In Proc. NAACL06.
4
Proceedings of ACL-08: HLT, pages 622?629,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Assessing Dialog System User Simulation Evaluation Measures Using
Human Judges
Hua Ai
University of Pittsburgh
Pittsburgh PA, 15260, USA
hua@cs.pitt.edu
Diane J. Litman
University of Pittsburgh
Pittsburgh, PA 15260, USA
litman@cs.pitt.edu
Abstract
Previous studies evaluate simulated dialog
corpora using evaluation measures which can
be automatically extracted from the dialog
systems? logs. However, the validity of these
automatic measures has not been fully proven.
In this study, we first recruit human judges
to assess the quality of three simulated dia-
log corpora and then use human judgments
as the gold standard to validate the conclu-
sions drawn from the automatic measures. We
observe that it is hard for the human judges
to reach good agreement when asked to rate
the quality of the dialogs from given perspec-
tives. However, the human ratings give con-
sistent ranking of the quality of simulated cor-
pora generated by different simulation mod-
els. When building prediction models of hu-
man judgments using previously proposed au-
tomatic measures, we find that we cannot reli-
ably predict human ratings using a regression
model, but we can predict human rankings by
a ranking model.
1 Introduction
User simulation has been widely used in different
phases in spoken dialog system development. In
the system development phase, user simulation is
used in training different system components. For
example, (Levin et al, 2000) and (Scheffler, 2002)
exploit user simulations to generate large corpora
for using Reinforcement Learning to develop dia-
log strategies, while (Chung, 2004) implement user
simulation to train the speech recognizer and under-
standing components.
While user simulation is considered to be more
low-cost and time-efficient than experiments with
human subjects, one major concern is how well the
state-of-the-art user simulations can mimic human
user behaviors and how well they can substitute for
human users in a variety of tasks. (Schatzmann
et al, 2005) propose a set of evaluation measures
to assess the quality of simulated corpora. They
find that these evaluation measures are sufficient
to discern simulated from real dialogs. Since this
multiple-measure approach does not offer a easily
reportable statistic indicating the quality of a user
simulation, (Williams, 2007) proposes a single mea-
sure for evaluating and rank-ordering user simula-
tions based on the divergence between the simulated
and real users? performance. This new approach also
offers a lookup table that helps to judge whether an
observed ordering of two user simulations is statisti-
cally significant.
In this study, we also strive to develop a prediction
model of the rankings of the simulated users? per-
formance. However, our approach use human judg-
ments as the gold standard. Although to date there
are few studies that use human judges to directly as-
sess the quality of user simulation, we believe that
this is a reliable approach to assess the simulated
corpora as well as an important step towards devel-
oping a comprehensive set of user simulation evalu-
ation measures. First, we can estimate the difficulty
of the task of distinguishing real and simulated cor-
pora by knowing how hard it is for human judges to
reach an agreement. Second, human judgments can
be used as the gold standard of the automatic evalua-
tion measures. Third, we can validate the automatic
622
measures by correlating the conclusions drawn from
the automatic measures with the human judgments.
In this study, we recruit human judges to assess
the quality of three user simulation models. Judges
are asked to read the transcripts of the dialogs be-
tween a computer tutoring system and the simula-
tion models and to rate the dialogs on a 5-point scale
from different perspectives. Judges are also given
the transcripts between human users and the com-
puter tutor. We first assess human judges? abilities
in distinguishing real from simulated users. We find
that it is hard for human judges to reach good agree-
ment on the ratings. However, these ratings give
consistent ranking on the quality of the real and the
simulated user models. Similarly, when we use pre-
viously proposed automatic measures to predict hu-
man judgments, we cannot reliably predict human
ratings using a regression model, but we can consis-
tently mimic human judges? rankings using a rank-
ing model. We suggest that this ranking model can
be used to quickly assess the quality of a new simu-
lation model without manual efforts by ranking the
new model against the old models.
2 Related Work
A lot of research has been done in evaluating differ-
ent components of Spoken Dialog Systems as well
as overall system performance. Different evaluation
approaches are proposed for different tasks. Some
studies (e.g., (Walker et al, 1997)) build regression
models to predict user satisfaction scores from the
system log as well as the user survey. There are also
studies that evaluate different systems/system com-
ponents by ranking the quality of their outputs. For
example, (Walker et al, 2001) train a ranking model
that ranks the outputs of different language genera-
tion strategies based on human judges? rankings. In
this study, we build both a regression model and a
ranking model to evaluate user simulation.
(Schatzmann et al, 2005) summarize some
broadly used automatic evaluation measures for user
simulation and integrate several new automatic mea-
sures to form a comprehensive set of statistical eval-
uation measures. The first group of measures inves-
tigates how much information is transmitted in the
dialog and how active the dialog participants are.
The second group of measures analyzes the style of
the dialog and the last group of measures examines
the efficiency of the dialogs. While these automatic
measures are handy to use, these measures have not
been validated by humans.
There are well-known practices which validate
automatic measures using human judgments. For
example, in machine translation, BLEU score (Pa-
pineni et al, 2002) is developed to assess the quality
of machine translated sentences. Statistical analysis
is used to validate this score by showing that BLEU
score is highly correlated with the human judgment.
In this study, we validate a subset of the automatic
measures proposed by (Schatzmann et al, 2005) by
correlating the measures with human judgments. We
follow the design of (Linguistic Data Consortium,
2005) in obtaining human judgments. We call our
study an assessment study.
3 System and User Simulation Models
In this section, we describe our dialog system (IT-
SPOKE) and the user simulation models which
we use in the assessment study. ITSPOKE is
a speech-enabled Intelligent Tutoring System that
helps students understand qualitative physics ques-
tions. In the system, the computer tutor first presents
a physics question and the student types an essay
as the answer. Then, the tutor analyzes the essay
and initiates a tutoring dialog to correct misconcep-
tions and to elicit further explanations. A corpus
of 100 tutoring dialogs was collected between 20
college students (solving 5 physics problems each)
and the computer tutor, yielding 1388 student turns.
The correctness of student answers is automatically
judged by the system and kept in the system?s logs.
Our previous study manually clustered tutor ques-
tions into 20 clusters based on the knowledge (e.g.,
acceleration, Newton?s 3rd Law) that is required to
answer each question (Ai and Litman, 2007).
We train three simulation models from the real
corpus: the random model, the correctness model,
and the cluster model. All simulation models gener-
ate student utterances on the word level by picking
out the recognized student answers (with potential
speech recognition errors) from the human subject
experiments with different policies. The random
model (ran) is a simple unigram model which ran-
domly picks a student?s utterance from the real cor-
623
pus as the answer to a tutor?s question, neglecting
which question it is. The correctness model (cor)
is designed to give a correct/incorrect answer with
the same probability as the average of real students.
For each tutor?s question, we automatically compute
the average correctness rate of real student answers
from the system logs. Then, a correct/incorrect an-
swer is randomly chosen from the correct/incorrect
answer sets for this question. The cluster model
(clu) tries to model student learning by assuming
that a student will have a higher chance to give a
correct answer to the question of a cluster in which
he/she mostly answers correctly before. It computes
the conditional probability of whether a student an-
swer is correct/incorrect given the content of the tu-
tor?s question and the correctness of the student?s an-
swer to the last previous question that belongs to the
same question cluster. We also refer to the real stu-
dent as the real student model (real) in the paper.
We hypothesize that the ranking of the four student
models (from the most realistic to the least) is: real,
clu, cor, and ran.
4 Assessment Study Design
4.1 Data
We decided to conduct a middle-scale assessment
study that involved 30 human judges. We conducted
a small pilot study to estimate how long it took a
judge to answer all survey questions (described in
Section 4.2) in one dialog because we wanted to con-
trol the length of the study so that judges would not
have too much cognitive load and would be consis-
tent and accurate on their answers. Based on the pi-
lot study, we decided to assign each judge 12 dialogs
which took about an hour to complete. Each dialog
was assigned to two judges. We used three out of the
five physics problems from the original real corpus
to ensure the variety of dialog contents while keep-
ing the corpus size small. Therefore, the evaluation
corpus consisted of 180 dialogs, in which 15 dialogs
were generated by each of the 4 student models on
each of the 3 problems.
4.2 Survey Design
4.2.1 Survey questions
We designed a web survey to collect human judg-
ments on a 5-point scale on both utterance and di-
Figure 1: Utterance level questions.
alog levels. Each dialog is separated into pairs of
a tutor question and the corresponding student an-
swer. Figure 1 shows the three questions which
are asked for each tutor-student utterance pair. The
three questions assess the quality of the student an-
swers from three aspects of Grice?s Maxim (Grice,
1975): Maxim of Quantity (u QNT), Maxim of Rel-
evance (u RLV), and Maxim of Manner (u MNR).
We do not include the Maxim of Quality because in
our task domain the correctness of the student an-
swers depends largely on students? physics knowl-
edge, which is not a factor we would like to consider
when evaluating the realness of the students? dialog
behaviors.
In Figure 2, we show the three dialog level ques-
tions which are asked at the end of each dialog.
The first question (d TUR) is a Turing test type of
question which aims to obtain an impression of the
student?s overall performance. The second ques-
tion (d QLT) assesses the dialog quality from a
tutoring perspective. The third question (d PAT)
sets a higher standard on the student?s performance.
Unlike the first two questions which ask whether
the student ?looks? good, this question further asks
whether the judges would like to partner with the
particular student.
4.2.2 Survey Website
We display one tutor-student utterance pair and
the three utterance level questions on each web page.
After the judges answer the three questions, he/she
will be led to the next page which displays the next
pair of tutor-student utterances in the dialog with
the same three utterance level questions. The judge
624
Figure 2: Dialog level questions.
reads through the dialog in this manner and answers
all utterance level questions. At the end of the di-
alog, three dialog level questions are displayed on
one webpage. We provide a textbox under each di-
alog level question for the judge to type in a brief
explanation on his/her answer. After the judge com-
pletes the three dialog level questions, he/she will be
led to a new dialog. This procedure repeats until the
judge completes all of the 12 assigned dialogs.
4.3 Assessment Study
30 college students are recruited as human judges
via flyers. Judges are required to be native speak-
ers of American English to make correct judgments
on the language use and fluency of the dialog. They
are also required to have taken at least one course
on Newtonian physics to ensure that they can under-
stand the physics tutoring dialogs and make judg-
ments about the content of the dialogs. We follow
the same task assigning procedure that is used in
(Linguistic Data Consortium, 2005) to ensure a uni-
form distribution of judges across student models
and dialogs while maintaining a random choice of
judges, models, and dialogs. Judges are instructed to
work as quickly as comfortably possible. They are
encouraged to provide their intuitive reactions and
not to ponder their decisions.
5 Assessment Study Results
In the initial analysis, we observe that it is a difficult
task for human judges to rate on the 5-point scale
and the agreements among the judges are fairly low.
Table 1 shows for each question, the percentages of
d TUR d QLT d PAT u QNT u RLV u MNR
22.8% 27.8% 35.6% 39.2% 38.4% 38.7%
Table 1: Percent agreements on 5-point scale
pairs of judges who gave the same ratings on the 5-
point scale. For the rest of the paper, we collapse
the ?definitely? types of answers with its adjacent
?probably? types of answers (more specifically, an-
swer 1 with 2, and 4 with 5). We substitute scores 1
and 2 with a score of 1.5, and scores 4 and 5 with a
score of 4.5. A score of 3 remains the same.
5.1 Inter-annotator agreement
Table 2 shows the inter-annotator agreements on the
collapsed 3-point scale. The first column presents
the question types. In the first row, ?diff? stands
for the differences between human judges? ratings.
The column ?diff=0? shows the percent agreements
on the 3-point scale. We can see the improvements
from the original 5-point scale when comparing with
Table 1. The column ?diff=1? shows the percentages
of pairs of judges who agree with each other on a
weaker basis in that one of the judges chooses ?can-
not tell?. The column ?diff=2? shows the percent-
ages of pairs of judges who disagree with each other.
The column ?Kappa? shows the un-weighted kappa
agreements and the column ?Kappa*? shows the lin-
ear weighted kappa. We construct the confusion ma-
trix for each question to compute kappa agreements.
Table 3 shows the confusion matrix for d TUR. The
first three rows of the first three columns show the
counts of judges? ratings on the 3-point scale. For
example, the first cell shows that there are 20 cases
where both judges give 1.5 to the same dialog. When
calculating the linear weighted kappa, we define the
distances between the adjacent categories to be one1.
Note that we randomly picked two judges to rate
each dialog so that different dialogs are rated by dif-
ferent pairs of judges and one pair of judges only
worked on one dialog together. Thus, the kappa
agreements here do not reflect the agreement of one
pair of judges. Instead, the kappa agreements show
the overall observed agreement among every pair of
1We also calculated the quadratic weighted kappa in which
the distances are squared and the kappa results are similar to the
linear weighted ones. For calculating the two weighted kappas,
see http://faculty.vassar.edu/lowry/kappa.html for details.
625
Q diff=0 diff=1 diff=2 Kappa Kappa*
d TUR 35.0% 45.6% 19.4% 0.022 0.079
d QLT 46.1% 28.9% 25.0% 0.115 0.162
d PAT 47.2% 30.6% 22.2% 0.155 0.207
u QNT 66.8% 13.9% 19.3% 0.377 0.430
u RLV 66.6% 17.2% 16.2% 0.369 0.433
u MNR 67.5% 15.4% 17.1% 0.405 0.470
Table 2: Agreements on 3-point scale
score=1.5 score=3 score=4.5 sum
score=1.5 20 26 20 66
score=3 17 11 19 47
score=4.5 15 20 32 67
sum 52 57 71 180
Table 3: Confusion Matrix on d TUR
judges controlling for the chance agreement.
We observe that human judges have low agree-
ment on all types of questions, although the agree-
ments on the utterance level questions are better
than the dialog level questions. This observation
indicates that assessing the overall quality of sim-
ulated/real dialogs on the dialog level is a difficult
task. The lowest agreement appears on d TUR.
We investigate the low agreements by looking into
judges? explanations on the dialog level questions.
21% of the judges find it hard to rate a particular
dialog because that dialog is too short or the stu-
dent utterances mostly consist of one or two words.
There are also some common false beliefs among
the judges. For example, 16% of the judges think
that humans will say longer utterances while 9% of
the judges think that only humans will admit the ig-
norance of an answer.
5.2 Rankings of the models
In Table 4, the first column shows the name of the
questions; the second column shows the name of
the models; the third to the fifth column present the
percentages of judges who choose answer 1 and 2,
can?t tell, and answer 4 and 5. For example, when
looking at the column ?1 and 2? for d TUR, we
see that 22.2% of the judges think a dialog by a
real student is generated probably or definitely by
a computer; more judges (25.6%) think a dialog by
the cluster model is generated by a computer; even
more judges (32.2%) think a dialog by the correct-
ness model is generated by a computer; and even
Question model 1 and 2 can?t tell 4 and 5
d TUR
real 22.2% 28.9% 48.9%
clu 25.6% 31.1% 43.3%
cor 32.2% 26.7% 41.1%
ran 51.1% 28.9% 20.0%
d QLT
real 20.0% 10.0% 70.0%
clu 21.1% 20.0% 58.9%
cor 24.4% 15.6% 60.0%
ran 60.0% 18.9% 21.1%
d PAT
real 28.9% 21.1% 50.0%
clu 41.1% 17.8% 41.1%
cor 43.3% 18.9% 37.8%
ran 82.2% 14.4% 3.4%
Table 4: Rankings on Dialog Level Questions
more judges (51.1%) think a dialog by the random
model is generated by a computer. When looking at
the column ?4 and 5? for d TUR, we find that most
of the judges think a dialog by the real student is
generated by a human while the fewest number of
judges think a dialog by the random model is gen-
erated by a human. Given that more human-like is
better, both rankings support our hypothesis that the
quality of the models from the best to the worst is:
real, clu, cor, and ran. In other words, although it is
hard to obtain well-agreed ratings among judges, we
can combine the judges? ratings to produce the rank-
ing of the models. We see consistent ranking orders
on d QLT and d PAT as well, except for a disorder
of cluster and correctness model on d QLT indicated
by the underlines.
When comparing two models, we can tell which
model is better from the above rankings. Neverthe-
less, we also want to know how significant the dif-
ference is. We use t-tests to examine the significance
of differences between every two models. We aver-
age the two human judges? ratings to get an aver-
aged score for each dialog. For each pair of models,
we compare the two groups of the averaged scores
for the dialogs generated by the two models using
2-tail t-tests at the significance level of p < 0.05.
In Table 5, the first row presents the names of the
models in each pair of comparison. Sig means that
the t-test is significant after Bonferroni correction;
question mark (?) means that the t-test is signifi-
cant before the correction, but not significant after-
wards, we treat this situation as a trend; not means
that the t-test is not significant at all. The table shows
626
real- real- real- ran- ran- cor-
ran cor clu cor clu clu
d TUR sig not not sig sig not
d QLT sig not not sig sig not
d PAT sig ? ? sig sig not
u QNT sig not not sig sig not
u RLV sig not not sig sig not
u MNR sig not not sig sig not
Table 5: T-Tests Results
that only the random model is significantly different
from all other models. The correctness model and
the cluster model are not significantly different from
the real student given the human judges? ratings, nei-
ther are the two models significantly different from
each other.
5.3 Human judgment accuracy on d TUR
We look further into d TUR in Table 4 because it is
the only question that we know the ground truth. We
compute the accuracy of human judgment as (num-
ber of ratings 4&5 on real dialogs + number of rat-
ings of 1&2 on simulated dialogs)/(2*total number
of dialogs). The accuracy is 39.44%, which serves
as further evidence that it is difficult to discern hu-
man from simulated users directly. A weaker accu-
racy is calculated to be 68.35% when we treat ?can-
not tell? as a correct answer as well.
6 Validating Automatic Measures
Since it is expensive to use human judges to rate
simulated dialogs, we are interested in building pre-
diction models of human judgments using auto-
matic measures. If the prediction model can re-
liably mimic human judgments, it can be used to
rate new simulation models without collecting hu-
man ratings. In this section, we use a subset of the
automatic measures proposed in (Schatzmann et al,
2005) that are applicable to our data to predict hu-
man judgments. Here, the human judgment on each
dialog is calculated as the average of the two judges?
ratings. We focus on predicting human judgments
on the dialog level because these ratings represent
the overall performance of the student models. We
use six high-level dialog feature measures including
the number of student turns (Sturn), the number of
tutor turns (Tturn), the number of words per stu-
dent turn (Swordrate), the number of words per tu-
tor turn (Twordrate), the ratio of system/user words
per dialog (WordRatio), and the percentage of cor-
rect answers (cRate).
6.1 The Regression Model
We use stepwise multiple linear regression to model
the human judgments using the set of automatic fea-
tures we listed above. The stepwise procedure au-
tomatically selects measures to be included in the
model. For example, d TUR is predicted as 3.65 ?
0.08 ? WordRatio ? 3.21 ? Swordrate, with an
R-square of 0.12. The prediction models for d QLT
and d PAT have similar low R-square values of 0.08
and 0.17, respectively. This result is not surprising
because we only include the surface level automatic
measures here. Also, these measures are designed
for comparison between models instead of predic-
tion. Thus, in Section 6.2, we build a ranking model
to utilize the measures in their comparative manner.
6.2 The Ranking Model
We train three ranking models to mimic human
judges? rankings of the real and the simulated stu-
dent models on the three dialog level questions using
RankBoost, a boosting algorithm for ranking ((Fre-
und et al, 2003), (Mairesse et al, 2007)). We briefly
explain the algorithm using the same terminologies
and equations as in (Mairesse et al, 2007), by build-
ing the ranking model for d TUR as an example.
In the training phase, the algorithm takes as input
a group of dialogs that are represented by values of
the automatic measures and the human judges? rat-
ings on d TUR. The RankBoost algorithm treats the
group of dialogs as ordered pairs:
T = {(x, y)| x, y are two dialog samples,
x has a higher human rated score than y }
Each dialog x is represented by a set of m indica-
tor functions hs(x) (1 ? s ? m). For example:
hs(x) =
{ 1 if WordRatio(x) ? 0.47
0 otherwise
Here, the threshold of 0.47 is calculated by Rank-
Boost. ? is a parameter associated with each indi-
cator function. For each dialog, a ranking score is
627
calculated as:
F (x) =
?
s
?shs(x) (1)
In the training phase, the human ratings are used
to set ? by minimizing the loss function:
LOSS = 1|T |
?
(x,y)?T
eval(F (x) ? F (y)) (2)
The eval function returns 0 if (x, y) pair is ranked
correctly, and 1 otherwise. In other words, LOSS
score is the percentage of misordered pairs where
the order of the predicted scores disagree with the
order indicated by human judges. In the testing
phase, the ranking score for every dialog is cal-
culated by Equation 1. A baseline model which
ranks dialog pairs randomly produces a LOSS of 0.5
(lower is better).
While LOSS indicates how many pairs of dialogs
are ranked correctly, our main focus here is to rank
the performance of the four student models instead
of individual dialogs. Therefore, we propose another
Averaged Model Ranking (AMR) score. AMR is
computed as the sum of the ratings of all the dialogs
generated by one model averaged by the number of
the dialogs. The four student models are then ranked
based on their AMR scores. The chance to get the
right ranking order of the four student models by
random guess is 1/(4!).
Table 6 shows a made-up example to illustrate the
two measures. real 1 and real 2 are two dialogs gen-
erated by the real student model; ran 1 and ran 2
are two dialogs by the random model. The second
and third column shows the human-rated score as the
gold standard and the machine-predicted score in the
testing phase respectively. The LOSS in this exam-
ple is 1/6, because only the pair of real 2 and ran 1
is misordered out of all the 6 possible pair combina-
tions. We then compute the AMR of the two models.
According to human-rated scores, the real model is
scored 0.75 (=(0.9+0.6)/2) while the random model
is scored 0.3. When looking at the predicted scores,
the real model is scored 0.65, which is also higher
than the random model with a score of 0.4. We thus
conclude that the ranking model ranks the two stu-
dent models correctly according to the overall rating
measure. We use both LOSS and AMR to evaluate
the ranking models.
Dialog Human-rated Score Predicted Score
real 1 0.9 0.9
real 2 0.6 0.4
ran 1 0.4 0.6
ran 2 0.2 0.2
Table 6: A Made-up Example of the Ranking Model
Cross Validation d TUR d QLT d PAT
Regular 0.176 0.155 0.151
Minus-one-model 0.224 0.180 0.178
Table 7: LOSS scores for Regular and Minus-one-model
(during training) Cross Validations
First, we use regular 4-fold cross validation where
we randomly hold out 25% of the data for testing
and train on the remaining 75% of the data for 4
rounds. Both the training and the testing data consist
of dialogs equally distributed among the four student
models. However, since the practical usage of the
ranking model is to rank a new model against sev-
eral old models without collecting additional human
ratings, we further test the algorithm by repeating
the 4 rounds of testing while taking turns to hold out
the dialogs from one model in the training data, as-
suming that model is the new model that we do not
have human ratings to train on. The testing corpus
still consists of dialogs from all four models. We call
this approach the minus-one-model cross validation.
Table 7 shows the LOSS scores for both cross val-
idations. Using 2-tailed t-tests, we observe that the
ranking models significantly outperforms the ran-
dom baseline in all cases after Bonferroni correction
(p < 0.05). When comparing the two cross vali-
dation results for the same question, we see more
LOSS in the more difficult minus-one-model case.
However, the LOSS scores do not offer a direct
conclusion on whether the ranking model ranks the
four student models correctly or not. To address
this question, we use AMR scores to re-evaluate all
cross validation results. Table 8 shows the human-
rated and predicted AMR scores averaged over four
rounds of testing on the regular cross validation re-
sults. We see that the ranking model gives the
same rankings of the student models as the human
judges on all questions. When applying AMR on
the minus-one-model cross validation results, we see
similar results that the ranking model reproduces hu-
628
real clu cor ran
human predicted human predicted human predicted human predicted
d TUR 0.68 0.62 0.65 0.59 0.63 0.52 0.51 0.49
d QLT 0.75 0.71 0.71 0.63 0.69 0.61 0.48 0.50
d PAR 0.66 0.65 0.60 0.60 0.58 0.57 0.31 0.32
Table 8: AMR Scores for Regular Cross Validation
man judges? rankings. Therefore, we suggest that
the ranking model can be used to evaluate a new
simulation model by ranking it against several old
models. Since our testing corpus is relatively small,
we would like to confirm this result on a large corpus
and on other dialog systems in the future.
7 Conclusion and Future Work
Automatic evaluation measures are used in evaluat-
ing simulated dialog corpora. In this study, we inves-
tigate a set of previously proposed automatic mea-
sures by comparing the conclusions drawn by these
measures with human judgments. These measures
are considered as valid if the conclusions drawn by
these measures agree with human judgments. We
use a tutoring dialog corpus with real students, and
three simulated dialog corpora generated by three
different simulation models trained from the real
corpus. Human judges are recruited to read the di-
alog transcripts and rate the dialogs by answering
different utterance and dialog level questions. We
observe low agreements among human judges? rat-
ings. However, the overall human ratings give con-
sistent rankings on the quality of the real and sim-
ulated user models. Therefore, we build a ranking
model which successfully mimics human judgments
using previously proposed automatic measures. We
suggest that the ranking model can be used to rank
new simulation models against the old models in or-
der to assess the quality of the new model.
In the future, we would like to test the ranking
model on larger dialog corpora generated by more
simulation models. We would also want to include
more automatic measures that may be available in
the richer corpora to improve the ranking and the
regression models.
Acknowledgments
This work is supported by NSF 0325054. We thank
J. Tereault, M. Rotaru, K. Forbes-Riley and the
anonymous reviewers for their insightful sugges-
tions, F. Mairesse for helping with RankBoost, and
S. Silliman for his help in the survey experiment.
References
H. Ai and D. Litman. 2007. Knowledge Consistent User
Simulations for Dialog Systems. In Proc. of Inter-
speech 2007.
G. Chung. 2004. Developing a Flexible Spoken Dialog
System Using Simulation. In Proc. of ACL 04.
Y. Freund, R. Iyer, R.E. Schapire, and Y. Singer. 2003.
An Efficient Boosting Algorithm for Combining Pref-
erences. Journal of Machine Learning Research.
H. P. Grice 1975. Logic and Conversation. Syntax and
Semantics III: Speech Acts, 41-58.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A Stochas-
tic Model of Human-Machine Interaction For learning
Dialog Strategies. IEEE Trans. On Speech and Audio
Processing, 8(1):11-23.
Linguistic Data Consortium. 2005. Linguistic Data An-
notation Specification: Assessment of Fluency and Ad-
equacy in Translations.
F. Mairesse, M. Walker, M. Mehl and R. Moore. 2007.
Using Linguistic Cues for the Automatic Recognition
of Personality in Conversation and Text. Journal of
Artificial Intelligence Research, Vol 30, pp 457-501.
K.A. Papineni, S. Roukos, R.T. Ward, and W-J. Zhu.
2002. Bleu: A Method for Automatic Evaluation of
Machine Translation. In Proc. of 40th ACL.
J. Schatzmann, K. Georgila, and S. Young. 2005. Quan-
titative Evaluation of User Simulation Techniques for
Spoken Dialog Systems. In Proc. of 6th SIGdial.
K. Scheffler. 2002. Automatic Design of Spoken Dialog
Systems. Ph.D. diss., Cambridge University.
J. D. Williams. 2007. A Method for Evaluating and Com-
paring User Simulations: The Cramer-von Mises Di-
vergence. Proc IEEE Workshop on Automatic Speech
Recognition and Understanding (ASRU).
M. Walker, D. Litman, C. Kamm, and A. Abella. 1997.
PARADISE: A Framework for Evaluating Spoken Dia-
log Agents. In Proc. of ACL 97.
M. Walker, O. Rambow, and M. Rogati. 2001. SPoT: A
Trainable Sentence Planner. In Proc. of NAACL 01.
629
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 888?896,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Setting Up User Action Probabilities in User Simulations for Dialog
System Development
Hua Ai
University of Pittsburgh
Pittsburgh PA, 15260, USA
hua@cs.pitt.edu
Diane Litman
University of Pittsburgh
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Abstract
User simulations are shown to be useful in
spoken dialog system development. Since
most current user simulations deploy prob-
ability models to mimic human user be-
haviors, how to set up user action proba-
bilities in these models is a key problem
to solve. One generally used approach is
to estimate these probabilities from human
user data. However, when building a new
dialog system, usually no data or only a
small amount of data is available. In this
study, we compare estimating user proba-
bilities from a small user data set versus
handcrafting the probabilities. We discuss
the pros and cons of both solutions for dif-
ferent dialog system development tasks.
1 Introduction
User simulations are widely used in spoken di-
alog system development. Recent studies use
user simulations to generate training corpora to
learn dialog strategies automatically ((Williams
and Young, 2007), (Lemon and Liu, 2007)), or to
evaluate dialog system performance (Lo?pez-Co?zar
et al, 2003). Most studies show that using user
simulations significantly improves dialog system
performance as well as speeds up system devel-
opment. Since user simulation is such a useful
tool, dialog system researchers have studied how
to build user simulations from a variety of perspec-
tives. Some studies look into the impact of training
data on user simulations. For example, (Georgila
et al, 2008) observe differences between simu-
lated users trained from human users of different
age groups. Other studies explore different simu-
lation models, i.e. the mechanism of deciding the
next user actions given the current dialog context.
(Schatzmann et al, 2006) give a thorough review
of different types of simulation models. Since
most of these current user simulation techniques
use probabilistic models to generate user actions,
how to set up the probabilities in the simulations
is another important problem to solve.
One general approach to set up user action prob-
abilities is to learn the probabilities from a col-
lected human user dialog corpus ((Schatzmann et
al., 2007b), (Georgila et al, 2008)). While this
approach takes advantage of observed user behav-
iors in predicting future user behaviors, it suffers
from the problem of learning probabilities from
one group of users while potentially using them
with another group of users. The accuracy of the
learned probabilities becomes more questionable
when the collected human corpus is small. How-
ever, this is a common problem in building new
dialog systems, when often no data1 or only a
small amount of data is available. An alterna-
tive approach is to handcraft user action proba-
bilities ((Schatzmann et al, 2007a), (Janarthanam
and Lemon, 2008)). This approach is less data-
intensive, but requires nontrivial work by domain
experts. What is more, as the number of proba-
bilities increases, it is hard even for the experts to
set the probabilities. Since both handcrafting and
training user action probabilities have their own
pros and cons, it is an interesting research ques-
tion to investigate which approach is better for a
certain task given the amount of data that is avail-
able.
In this study, we investigate a manual and a
trained approach in setting up user action proba-
bilities, applied to building the same probabilis-
tic simulation model. For the manual user simula-
tions, we look into two sets of handcrafted proba-
bilities which use the same expert knowledge but
differ in individual probability values. This aims
to take into account small variations that can possi-
1When no human user data is collected with the dialog
system, Wizard-of-Oz experiments can be conducted to col-
lect training data for building user simulations.
888
bly be introduced by different domain experts. For
the trained user simulations, we examine two sets
of probabilities trained from user corpora of dif-
ferent sizes, since the amount of training data will
impact the quality of the trained probability mod-
els. We compare the trained and the handcrafted
simulations on three tasks. We observe that in our
task settings, the two manual simulations do not
differ significantly on any tasks. In addition, there
is no significant difference among the trained and
the manual simulations in generating corpus level
dialog behaviors as well as in generating training
corpora for learning dialog strategies. When com-
paring on a dialog system evaluation task, the sim-
ulation trained from more data significantly out-
performs the two manual simulations, which again
outperforms the simulation trained from less data.
Based on our observations, we answer the orig-
inal question of how to design user action proba-
bilities for simulations that are similar to ours in
terms of the complexity of the simulations2. We
suggest that handcrafted user simulations can per-
form reasonably well in building a new dialog sys-
tem, especially when we are not sure that there is
enough data for training simulation models. How-
ever, once we have a dialog system, it is use-
ful to collect human user data in order to train a
new user simulation model since the trained sim-
ulations perform better than the handcrafted user
simulations on more tasks. Since how to decide
whether enough data is available for simulation
training is another research question to answer, we
will further discuss the impact of our results later
in Section 6.
2 Related Work
Most current simulation models are probabilistic
models in which the models simulate user actions
based on dialog context features (Schatzmann et
al., 2006). We represent these models as:
P (user action|feature1, . . .,featuren) (1)
The number of probabilities involved in this
model is:
(# of possible actions-1) ?
n?
k=1
(# of feature values). (2)
Some studies handcraft these probabilities. For
example, (Schatzmann et al, 2007a) condition the
2The number of user action probabilities and the simu-
lated user behaviors will impact the design choice.
user actions on user?s goals and the agenda to
reach those goals. They manually author the prob-
abilities in the user?s agenda update model and the
goal update model, and then calculate the user ac-
tion probabilities based on the two models. (Ja-
narthanam and Lemon, 2008) handcraft 15 proba-
bilities in simulated users? initial profiles and then
author rules to update these probabilities during
the dialogs.
Other studies use a human user corpus as the
training corpus to learn user action probabilities
in user simulations. Since the human user cor-
pus often does not include all possible actions that
users may take during interactions with the dialog
system, different strategies are used to account for
user actions that do not appear in the training cor-
pus but may be present when testing the user sim-
ulations. For example, (Schatzmann et al, 2007b)
introduce a summary space approach to map the
actual dialog context space into a more tractable
summary space. Then, they use forward and back-
ward learning algorithms to learn the probabili-
ties from a corpus generated by 40 human users
(160 dialogs). (Rieser and Lemon, 2006) use a
two step approach in computing the probabilities
from a corpus consisting of dialogs from 24 hu-
man users (70 dialogs). They first cluster dialog
contexts based on selected features and then build
conditional probability models for each cluster.
In our study, we build a conditional probability
model which will be described in detail in Sec-
tion 3.2.1. There are 40 probabilities to set up in
this model3. We will explain different approaches
to assign these probabilities later in Section 3.2.2.
3 System and User Simulations
In this section, we describe the dialog system, the
human user corpus we collected with the system,
and the user simulation we used.
3.1 System and Corpus
The ITSPOKE system (Litman and Silliman,
2004) is an Intelligent Tutoring System which
teaches Newtonian physics. It is a speech-
enhanced version of the Why2-Atlas tutoring sys-
tem (Vanlehn et al, 2002). During the interac-
tion with students, the system initiates a spoken
tutoring dialog to correct misconceptions and to
3There are 2 possible actions in our model, 20 possible
values for the first feature qCluster and 2 possible values for
the second feature prevCorrectness as described later in Sec-
tion 3.2.1. Using Equation 2, 40=(2-1)*20*2.
889
SYSTEM1: Do you recall what Newton?s
third law says? [3rdLaw]
Student1: Force equals mass times
acceleration. [ic, c%=0, ncert]
SYSTEM2: Newton?s third law says ...
If you hit the wall harder, is the
force of your fist acting on the
wall greater or less? [3rdLaw]
Student2: Greater. [c, c%=50%,cert]
Dialog goes on
Table 1: Sample coded dialog excerpt.
elicit further explanation. A pretest is given before
the interaction and a posttest is given afterwards.
We calculate a Normalized Learning Gain for each
student to evaluate the performance of the system
in terms of the student?s knowledge gain:
NLG = posttest score - pretest score1-pretest score (3)
The current tutoring dialog strategy was hand-
crafted in a finite state paradigm by domain ex-
perts, and the tutor?s response is based only on the
correctness of the student?s answer4. However, tu-
toring research (Craig et al, 2004) suggests that
other underlying information in student utterances
(e.g., student certainty) is also useful in improving
learning. Therefore, we are working on learning
a dialog strategy to also take into account student
certainty.
In our prior work, a corpus of 100 dialogs (1388
student turns) was collected between 20 human
subjects (5 dialogs per subject) and the ITSPOKE
system. Correctness (correct(c), incorrect(ic)) is
automatically judged by the system and is kept in
the system?s logs. We also computed the student?s
correctness rate (c%) and labeled it after every
student turn. Each student utterance was manu-
ally annotated for certainty (certain(cert), notcer-
tain(ncert)) in a previous study based on both lex-
ical and prosodic information5. In addition, we
manually clustered tutor questions into 20 clusters
based on the knowledge that is required to answer
that question, e.g. questions on Newton?s Third
Law are put into a cluster labeled as (3rdLaw).
There are other clusters such as gravity, acceler-
ation, etc. An example of a coded dialog between
the system and a student is given in Table 1.
4Despite the limitation of the current system, students
learn significantly after interacting with the system.
5Kappa of 0.68 is gained in the agreement study.
3.2 User Simulation Model and Model
Probabilities Set-up
3.2.1 User Simulation Model
We build a Knowledge Consistency Model6 (KC
Model) to simulate consistent student behaviors
while interacting with a tutoring system. Ac-
cording to learning literature (Cen et al, 2006),
once a student acquires certain knowledge, his/her
performance on similar problems that require
the same knowledge (i.e. questions from the
same cluster we introduced in Section 3.1) will
become stable. Therefore, in the KC Model,
we condition the student action stuAction based
on the cluster of tutor question (qCluster) and
the student?s correctness when last encountering
a question from that cluster (prevCorrectness):
P (stuAction|qCluster, prevCorrectness). For
example, in Table 1, when deciding the student?s
answer after the second tutor question, the simu-
lation looks back into the dialog and finds out that
the last time (in Student1) the student answered
a question from the same cluster 3rdLaw incor-
rectly. Therefore, this time the simulation gives
a correct student answer based on the probability
P (c|3rdLaw, ic).
Since different groups of students often have
different learning abilities, we examine such dif-
ferences among our users by grouping the users
based on Normalized Learning Gains (NLG),
which is an important feature to describe user be-
haviors in tutoring systems. By dividing our hu-
man users into high/low learners based on the me-
dian of NLG, we find a significant difference in the
NLG of the two groups based on 2-tailed t-tests
(p < 0.05). Therefore, we construct a simula-
tion to represent low learners and another simula-
tion to represent high learners to better character-
ize the differences in high/low learners? behaviors.
Similar approaches are adopted in other studies in
building user simulations for dialog systems (e.g.,
(Georgila et al, 2008) simulate old versus young
users separately).
Our simulation models work on the word level 7
because generating student dialog acts alone does
not provide sufficient information for our tutoring
system to decide the next system action. Since it
is hard to generate a natural language utterance for
each tutor?s question, we use the student answers
6This is the best model we built in our previous studies
(Ai and Litman, 2007).
7See (Ai and Litman, 2006) for more details.
890
in the human user corpus as the candidate answers
for the simulated students.
3.2.2 Model Probabilities Set-up
Now we discuss how to set up user action prob-
abilities in the KC Model. We compare learning
probabilities from human user data to handcrafting
probabilities based on expert knowledge. Since we
represent high/low learners using different mod-
els, we build simulation models with separate user
action probabilities to represent the two groups of
learners.
When learning the probabilities in the Trained
KC Models, we calculate user action probabilities
for high/low learners in our human corpus sepa-
rately. We use add-one smoothing to account for
user actions that do not appear in the human user
corpus. For the first time the student answers a
question in a certain cluster, we back-off the user
action probability to P(stuAction | average cor-
rectness rate of this question in human user cor-
pus). We first train a KC model using the data
from all 20 human users to build the TrainedMore
(Tmore) Model. Then, in order to investigate the
impact of the amount of training data on the qual-
ity of trained simulations, we randomly pick 5 out
of the 10 high learners and 5 out of the 10 low
learners to get an even smaller human user corpus.
We train the TrainedLess (Tless) Model from this
small corpus .
When handcrafting the probabilities in the Man-
ual KC Models8, the clusters of questions are
first grouped into three difficulty groups (Easy,
Medium, Hard). Based on expert knowledge,
we assume on average 70% of students can cor-
rectly answer the tutor questions from the Easy
group, while for the Medium group only 60%
and for the hard group 50%. Then, we assign
a correctness rate higher than the average for
the high learners and a corresponding correctness
rate lower than the average for the low learners.
For the first Manual KC model (M1), within the
same difficulty group, the same two probabilities
P1(stuAction|qClusteri, prevCorrectness = c) and
P2(stuAction|qClusteri, prevCorrectness = ic) are
assigned to each clusteri as the averages for the
corresponding high/low learners. Since a different
human expert will possibly provide a slightly dif-
ferent set of probabilities even based on the same
mechanism, we also design another set of prob-
8The first author of the paper acts as the domain expert.
abilities to account for such variations. For the
second Manual KC model (M2), we allow dif-
ferences among the clusters within the same dif-
ficulty group. For the clusters in each difficulty
group, we randomly assign a probability that dif-
fers no more than 5% from the average. For exam-
ple, for the easy clusters, we assign average proba-
bilities of high/low learners between [65%, 75%].
Although human experts may differ to some ex-
tent in assigning individual probability values, we
hypothesize that in general a certain amount of ex-
pertise is required in assigning these probabilities.
To investigate this, we build a baseline simula-
tion with no expert knowledge, which is a Ran-
dom Model (Ran) that randomly assigns values
for these user action probabilities.
4 Evaluation Measures
In this section, we introduce the evaluation mea-
sures for comparing the simulated corpora gen-
erated by different simulation models to the hu-
man user corpus. In Section 4.1, we use a set of
widely used domain independent features to com-
pare the simulated and the human user corpora
on corpus-level dialog behaviors. These compar-
isons give us a direct impression of how similar
the simulated dialogs are to human user dialogs.
Then, we compare the simulations in task-oriented
contexts. Since simulated user corpora are often
used as training corpora for using MDPs to learn
new dialog strategies, in Section 4.2 we estimate
how different the learned dialog strategies would
be when trained from different simulated corpora.
Another way to use user simulation is to test dialog
systems. Therefore, in Section 4.3, we compare
the user actions predicted by the various simula-
tion models with actual human user actions.
4.1 Measures on Corpus Level Dialog
Behaviors
We compare the dialog corpora generated by user
simulations to our human user corpus using a com-
prehensive set of corpus level measures proposed
by (Schatzmann et al, 2005). Here, we use a sub-
set of the measures which describe high-level dia-
log features that are applicable to our data. The
measures we use include the number of student
turns (Sturn), the number of tutor turns (Tturn), the
number of words per student turn (Swordrate), the
number of words per tutor turn (Twordrate), the ra-
tio of system/user words per dialog (WordRatio),
891
and the percentage of correct answers (cRate).
4.2 Measures on Dialog Strategy Learning
In this section, we introduce two measures to com-
pare the simulations based on their performance
on a dialog strategy learning task. In recent stud-
ies (e.g., (Janarthanam and Lemon, 2008)), user
simulations are built to generate a large corpus
to build MDPs in using Reinforcement Learning
(RL) to learn new dialog strategies. When building
an MDP from a training corpus9, we compute the
transition probabilities P (st+1|st, a) (the proba-
bility of getting from state st to the next state st+1
after taking action a), and the reward of this transi-
tion R(st, a, st+1). Then, the expected cumulative
value (V-value) of a state s can be calculated using
this recursive function:
V (s) =
?
st+1
P (st+1|st, a)[R(st, a, st+1) + ?V (st+1)]
(4)
? is a discount factor which ranges between 0 and
1.
For our evaluation, we first compare the tran-
sition probabilities calculated from all simulated
corpora. The transition probabilities are only de-
termined by the states and user actions presented
by the training corpus, regardless of the rest of the
MDP configuration. Since the MDP configuration
has a big impact on the learned strategies, we want
to first factor this impact out and estimate the dif-
ferences in learned strategies that are brought in
by the training corpora alone. As a second evalua-
tion measure, we apply reinforcement learning to
the MDP representing each simulated corpus sep-
arately to learn dialog strategies. We compare the
Expected Cumulative Rewards (ECRs)(Williams
and Young, 2007) of these dialog strategies, which
show the expectation of the rewards we can obtain
by applying the learned strategies.
The MDP learning task in our study is to max-
imize student certainty during tutoring dialogs.
The dialog states are characterized using the cor-
rectness of the current student answer and the stu-
dent correctness rate so far. We represent the cor-
rectness rate as a binary feature: lc if it is below
the training corpus average and hc if it is above the
average. The end of dialog reward is assigned to
be +100 if the dialog has a percent certainty higher
9In this paper, we use off-line model-based RL (Paek,
2006) rather than learning an optimal strategy online during
system-user interactions.
than the median from the training corpus and -100
otherwise. The action choice of the tutoring sys-
tem is to give a strong (s) or weak (w) feedback.
A strong feedback clearly indicates the correctness
of the current student answer while the weak feed-
back does not. For example, the second system
turn in Table 1 contains a weak feedback. If the
system says ?Your answer is incorrect? at the be-
ginning of this turn, that would be a strong feed-
back. In order to simulate student certainty, we
simply output the student certainty originally asso-
ciated in each student utterance. Thus, the output
of the KC Models here is a student utterance along
with the student certainty (cert, ncert). In a pre-
vious study (Ai et al, 2007), we investigated the
impact of different MDP configurations by com-
paring the ECRs of the learned dialog strategies.
Here, we use one of the best-performing MDP
configurations, but vary the simulated corpora that
we train the dialog strategies on. Our goal is to see
which user simulation performs better in generat-
ing a training corpus for dialog strategy learning.
4.3 Measures on Dialog System Evaluation
In this section, we introduce two ways to com-
pare human user actions with the actions predicted
by the simulations. The aim of this comparison
is to assess how accurately the simulations can
replicate human user behaviors when encounter-
ing the same dialog situation. A simulated user
that can accurately predict human user behaviors
is needed to replace human users when evaluating
dialog systems.
We randomly divide the human user dialog cor-
pus into four parts: each part contains a balanced
amount of high/low learner data. Then we perform
four fold cross validation by always using 3 parts
of the data as our training corpus for user simula-
tions, and the remaining one part of the data as
testing data to compare with simulated user ac-
tions. We always compare high human learners
only with simulation models that represent high
learners and low human learners only with simu-
lation models that represent low learners. Compar-
isons are done on a turn by turn basis. Every time
the human user takes an action in the dialogs in the
testing data, the user simulations are used to pre-
dict an action based on related dialog information
from the human user dialog. For a KC Model, the
related dialog information includes qCluster and
prevCorrectness . We first compare the simulation
892
predicted user actions directly with human user ac-
tions. We define simulation accuracy as:
Accuracy = Correctly predicted human user actionsTotal number of human user actions (5)
However, since our simulation model is a prob-
abilistic model, the model will take an action
stochastically after the same tutor turn. In other
words, we need to take into account the probabil-
ity for the simulation to predict the right human
user action. If the simulation outputs the right ac-
tion with a small probability, it is less likely that
this simulation can correctly predict human user
behaviors when generating a large dialog corpus.
We consider a simulated action associated with a
higher probability to be ranked higher than an ac-
tion with a lower probability. Then, we use the re-
ciprocal ranking from information retrieval tasks
(Radev et al, 2002) to assess the simulation per-
formance10. Mean Reciprocal Ranking is defined
as:
MRR = 1A
A?
k=1
1
ranki (6)
In Equation 6, A stands for the total number of
human user actions, ranki stands for the ranking
of the simulated action which matches the i-th hu-
man user action.
Table 2 shows an example of comparing simu-
lated user actions with human user actions in the
sample dialog in Table 1. In the first turn Stu-
dent1, a simulation model has a 60% chance to
output an incorrect answer and a 40% chance to
output a correct answer while it actually outputs
an incorrect answer. In this case, we consider the
simulation ranks the actions in the order of: ic, c.
Since the human user gives an incorrect answer at
this time, the simulated action matches with this
human user action and the reciprocal ranking is
1. However, in the turn Student2, the simulation?s
output does not match the human user action. This
time, the correct simulated user action is ranked
second. Therefore, the reciprocal ranking of this
simulation action is 1/2.
We hypothesize that the measures introduced
in this section have larger power in differentiat-
ing different simulated user behaviors since every
10(Georgila et al, 2008) use Precision and Recall to cap-
ture similar information as our accuracy, and Expected Pre-
cision and Expected Recall to capture similar information as
our reciprocal ranking.
simulated user action contributes to the compar-
ison between different simulations. In contrast,
the measures introduced in Section 4.1 and Sec-
tion 4.2 have less differentiating power since they
compare at the corpus level.
5 Results
We let al user simulations interact with our dia-
log system, where each simulates 250 low learners
and 250 high learners. In this section, we report
the results of applying the evaluation measures we
discuss in Section 4 on comparing simulated and
human user corpora. When we talk about signifi-
cant results in the statistics tests below, we always
mean that the p-value of the test is ? 0.05.
5.1 Comparing on Corpus Level Dialog
Behavior
Figure 1 shows the results of comparisons using
domain independent high-level dialog features of
our corpora. The x-axis shows the evaluation mea-
sures; the y-axis shows the mean for each corpus
normalized to the mean of the human user cor-
pus. Error bars show the standard deviations of
the mean values. As we can see from the figure,
the Random Model performs differently from the
human and all the other simulated models. There
is no difference in dialog behaviors among the hu-
man corpus, the trained and the manual simulated
corpora.
In sum, both the Trained KC Models and
the Manual KC Models can generate human-like
high-level dialog behaviors while the Random
Model cannot.
5.2 Comparing on Dialog Strategy Learning
Task
Next, we compare the difference in dialog strategy
learning when training on the simulated corpora
using similar approaches in (Tetreault and Litman,
2008). Table 3 shows the transition probabilities
starting from the state (c, lc). For example, the
first cell shows in the Tmore corpus, the probabil-
ity of starting from state (c, lc), getting a strong
feedback, and transitioning into the same state is
24.82%. We calculate the same table for the other
three states (c, hc), (ic, lc), and (ic, hc). Using
paired-sample t-tests with bonferroni corrections,
the only significant differences are observed be-
tween the random simulated corpus and each of
the other simulated corpora.
893
i-th Turn human Simulation Model Simulation Output CorrectlyPredictedActions ReciprocalRanking
Student1 ic 60% ic, 40% c ic 1 1
Student2 c 70% ic, 30% c ic 0 1/2
Average / / / (1+0)/2 (1+1/2)/2
Table 2: An Example of Comparing Simulated Actions with Human User Actions.
Figure 1: Comparison of human and simulated dialogs by high-level dialog features.
Tmore Tless M1 M2 Ran
s?c lc 24.82 31.42 25.64 22.70 13.25
w?c lc 17.64 12.35 16.62 18.85 9.74
s?ic lc 2.11 7.07 1.70 1.63 19.31
w?ic lc 1.80 2.17 2.05 3.25 21.06
s?c hc 29.95 26.46 22.23 31.04 10.54
w?c hc 13.93 9.50 22.73 15.10 11.29
s?ic hc 5.52 2.51 4.29 0.54 7.13
w?ic hc 4.24 9.08 4.74 6.89 7.68
Table 3: Comparisons of MDP transition proba-
bilities at state (c, lc) (Numbers in this table are
percentages).
Tmore Tless M1 M2 Ran
ECR 15.10 11.72 15.24 15.51 7.03
CI ?2.21 ?1.95 ?2.07 ?3.46 ?2.11
Table 4: Comparisons of ECR of learned dialog
strategies.
We also use a MDP toolkit to learn dialog strate-
gies from all the simulated corpora and then com-
pute the Expected Cumulative Reward (ECR) for
the learned strategies. In Table 4, the upper part
of each cell shows the ECR of the learned dialog
strategy; the lower part of the cell shows the 95%
Confidence Interval (CI) of the ECR. We can see
from the overlap of the confidence intervals that
the only significant difference is observed between
the dialog strategy trained from the random simu-
lated corpus and the strategies trained from each
of the other simulated corpora. Also, it is inter-
esting to see that the CI of the two manual simu-
lations overlap more with the CI of Tmore model
than with the CI of the Tless model.
In sum, the manual user simulations work as
well as the trained user simulation when being
used to generate a training corpus to apply MDPs
to learn new dialog strategies.
Tmore Tless M1 M2 Ran
Accu- 0.78 0.60 0.70 0.72 0.41
racy (?0.01) (?0.02) (?0.02) (?0.02) (?0.02)
MRR 0.72 0.52 0.63 0.64 0.32(?0.02) (?0.02) (?0.02) (?0.01) (?0.02)
Table 5: Comparisons of correctly predicted hu-
man user actions.
5.3 Comparisons in Dialog System
Evaluation
Finally, we compare how accurately the user sim-
ulations can predict human user actions given the
same dialog context. Table 5 shows the averages
and CIs (in parenthesis) from the four fold cross
validations. The second row shows the results
based on direct comparisons with human user ac-
tions, and the third row shows the mean recipro-
cal ranking of simulated actions. We observe that
in terms of both the accuracy and the reciprocal
ranking, the performance ranking from the high-
est to the lowest (with significant difference be-
tween adjacent ranks) is: the Tmore Model, both
of the manual models (no significant differences
between these two models), the Tless Model, and
the Ran Model. Therefore, we suggest that the
handcrafted user simulation is not sufficient to be
used in evaluating dialog systems because it does
not generate user actions that are as similar to hu-
man user actions. However, the handcrafted user
simulation is still better than a user simulation
trained with not enough training data. This re-
sult also indicates that this evaluation measure has
more differentiating power than the previous mea-
sures since it captures significant differences that
are not shown by the previous measures.
In sum, the Tmore simulation performs the best
in predicting human user actions.
894
6 Conclusion and Future Work
Setting up user action probabilities in user sim-
ulation is a non-trivial task, especially when no
training data or only a small amount of data is
available. In this study, we compare several ap-
proaches in setting up user action probabilities
for the same simulation model: training from all
available human user data, training from half of
the available data, two handcrafting approaches
which use the same expert knowledge but differ
slightly in individual probability assignments, and
a baseline approach which randomly assigns all
user action probabilities. We compare the built
simulations from different aspects. We find that
the two trained simulations and the two hand-
crafted simulations outperform the random simu-
lation in all tasks. No significant difference is ob-
served among the trained and the handcrafted sim-
ulations when comparing their generated corpora
on corpus-level dialog features as well as when
serving as the training corpora for learning dialog
strategies. However, the simulation trained from
all available human user data can predict human
user actions more accurately than the handcrafted
simulations, which again perform better than the
model trained from half of the human user corpus.
Nevertheless, no significant difference is observed
between the two handcrafted simulations.
Our study takes a first step in comparing the
choices of handcrafting versus training user simu-
lations when only limited or even no training data
is available, e.g., when constructing a new dialog
system. As shown for our task setting, both types
of user simulations can be used in generating train-
ing data for learning new dialog strategies. How-
ever, we observe (as in a prior study by (Schatz-
mann et al, 2007b)) that the simulation trained
from more user data has a better chance to outper-
form the simulation trained from less training data.
We also observe that a handcrafted user simulation
with expert knowledge can reach the performance
of the better trained simulation. However, a cer-
tain level of expert knowledge is needed in hand-
crafting user simulations since a random simula-
tion does not perform well in any tasks. Therefore,
our results suggest that if an expert is available for
designing a user simulation when not enough user
data is collected, it may be better to handcraft the
user simulation than training the simulation from
the small amount of human user data. However,
it is another open research question to answer how
much data is enough for training a user simulation,
which depends on many factors such as the com-
plexity of the user simulation model. When using
simulations to test a dialog system, our results sug-
gest that once we have enough human user data, it
is better to use the data to train a new simulation
to replace the handcrafted simulation.
In the future, we will conduct follow up stud-
ies to confirm our current findings since there are
several factors that can impact our results. First
of all, our current system mainly distinguishes the
student answers as correct and incorrect. We are
currently looking into dividing the incorrect stu-
dent answers into more categories (such as par-
tially correct answers, vague answers, or over-
specific answers) which will increase the number
of simulated user actions. Also, although the size
of the human corpus which we build the trained
user simulations from is comparable to other stud-
ies (e.g., (Rieser and Lemon, 2006), (Schatzmann
et al, 2007b)), using a larger human corpus may
improve the performance of the trained simula-
tions. We are in the process of collecting another
corpus which will consist of 60 human users (300
dialogs). We plan to re-train a simulation when
this new corpus is available. Also, we would be
able to train more complex models (e.g., a simula-
tion model which takes into account a longer dia-
log history) with the extra data. Finally, although
we add some noise into the current manual simula-
tion designed by our domain expert to account for
variations of expert knowledge, we would like to
recruit another human expert to construct a new
manual simulation to compare with the existing
simulations. It would also be interesting to repli-
cate our experiments on other dialog systems to
see whether our observations will generalize. Our
long term goal is to provide guidance of how to ef-
fectively build user simulations for different dialog
system development tasks given limited resources.
Acknowledgments
The first author is supported by Mellon Fellow-
ship from the University of Pittsburgh. This work
is supported partially by NSF 0325054. We thank
K. Forbes-Riley, P. Jordan and the anonymous re-
viewers for their insightful suggestions.
References
H. Ai and D. Litman. 2006. Comparing Real-Real,
Simulated-Simulated, and Simulated-Real Spoken
895
Dialogue Corpora. In Proc. of the AAAI Workshop
on Statistical and Empirical Approaches for Spoken
Dialogue Systems.
H. Ai and D. Litman. 2007. Knowledge Consistent
User Simulations for Dialog Systems. In Proc. of
Interspeech 2007.
H. Ai, J. Tetreault, and D. Litman. 2007. Comparing
User Simulation Models for Dialog Strategy Learn-
ing. In Proc. of NAACL-HLT 2007.
H. Cen, K. Koedinger and B. Junker. 2006. Learn-
ing Factors Analysis-A General Method for Cogni-
tive Model Evaluation and Improvement. In Proc. of
8th International Conference on ITS.
S. Craig, A. Graesser, J. Sullins, and B. Gholson. 2004.
Affect and learning: an exploratory look into the
role of affect in learning with AutoTutor. Journal
of Educational Media 29(3), 241250.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning User Simulations for Information State
Update Dialogue Systems. In Proc. of Interspeech
2005.
K. Georgila, M. Wolters, and J. Moore. 2008. Simu-
lating the Behaviour of Older versus Younger Users
when Interacting with Spoken Dialogue Systems. In
Proc. of 46th ACL.
S. Janarthanam and O. Lemon. 2008. User simulations
for online adaptation and knowledge-alignment in
Troubleshooting dialogue systems. In Proc. of the
12th SEMdial Workshop on on the Semantics and
Pragmatics of Dialogues.
O. Lemon and X. Liu. 2007. Dialogue Policy Learn-
ing for combinations of Noise and User Simulation:
transfer results. In Proc. of 8th SIGdial.
D. Litman and S. Silliman. 2004. ITSPOKE: An Intel-
ligent Tutoring Spoken Dialogue System. In Com-
panion Proc. of the Human Language Technology:
NAACL.
R. Lo?pez-Co?zar, A. De la Torre, J. C. Segura and A.
J. Rubio. 2003. Assessment of dialogue systems by
means of a new simulation technique. Speech Com-
munication (40): 387-407.
T. Paek. 2006. Reinforcement learning for spo-
ken dialogue systems: Comparing strengths and
weaknesses for practical deployment. In Proc.
of Interspeech-06 Workshop on ?Dialogue on Dia-
logues - Multidisciplinary Evaluation of Advanced
Speech-based Interacive Systems?.
D. Radev, H. Qi, H. Wu, and W. Fan. 2002. Evaluating
web-based question answering systems. In Proc. of
LREC 2002.
V. Rieser and O. Lemon. 2006. Cluster-based User
Simulations for Learning Dialogue Strategies. In
Proc. of Interspeech 2006.
J. Schatzmann, K. Georgila, and S. Young. 2005.
Quantitative Evaluation of User Simulation Tech-
niques for Spoken Dialogue Systems. In Proc. of 6th
SIGDial.
J. Schatzmann, K. Weilhammer, M. Stuttle, and S.
Young. 2006. A Survey of Statistical User Simula-
tion Techniques for Reinforcement-Learning of Di-
alogue Management Strategies. Knowledge Engi-
neering Review 21(2): 97-126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. Young. 2007a. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proc. of HLT/NAACL 2007.
J. Schatzmann, B. Thomson and S. Young. 2007b. Sta-
tistical User Simulation with a Hidden Agenda. In
Proc. of 8th SIGdial.
J. Tetreault and D. Litman. 2008. A Reinforcement
Learning Approach to Evaluating State Representa-
tions in Spoken Dialogue Systems. Speech Commu-
nication (Special Issue on Evaluating new methods
and models for advanced speech-based interactive
systems), 50(8-9): 683-696.
K. VanLehn, P. Jordan, C. Rose?, D. Bhembe, M.
Bo?ttner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler, R.
Srivastava, and R. Wilson. 2002. The architecture
of Why2-Atlas: A coach for qualitative physics es-
say writing. In Proc. Intelligent Tutoring Systems
Conference..
J. Williams and S. Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language 21(2): 231-
422.
896
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 164?171,
Columbus, June 2008. c?2008 Association for Computational Linguistics
 
Abstract 
We propose to use user simulation for testing 
during the development of a sophisticated dia-
log system. While the limited behaviors of the 
state-of-the-art user simulation may not cover 
important aspects in the dialog system testing, 
our proposed approach extends the functional-
ity of the simulation so that it can be used at 
least for the early stage testing before the sys-
tem reaches stable performance for evaluation 
involving human users. The proposed ap-
proach includes a set of evaluation measures 
that can be computed automatically from the 
interaction logs between the user simulator 
and the dialog system. We first validate these 
measures on human user dialogs using user 
satisfaction scores. We also build a regression 
model to estimate the user satisfaction scores 
using these evaluation measures. Then, we 
apply the evaluation measures on a simulated 
dialog corpus trained from the real user cor-
pus. We show that the user satisfaction scores 
estimated from the simulated corpus are not 
statistically different from the real users? satis-
faction scores.  
1 Introduction 
 Spoken dialog systems are being widely used in 
daily life. The increasing demands of such systems 
require shorter system development cycles and 
better automatic system developing techniques. As 
a result, machine learning techniques are applied to 
learn dialog strategies automatically, such as rein-
forcement learning (Singh et al, 2002; Williams & 
Young, 2007), supervised learning (Henderson et 
                                                          
* This study was conducted when the author was an intern at 
Bosch RTC. 
al., 2005), etc. These techniques require a signifi-
cant amount of training data for the automatic 
learners to sufficiently explore the vast space of 
possible dialog states and strategies. However, it is 
always hard to obtain training corpora that are 
large enough to ensure that the learned strategies 
are reliable. User simulation is an attempt to solve 
this problem by generating synthetic training cor-
pora using computer simulated users. The simu-
lated users are built to mimic real users' behaviors 
to some extent while allowing them to be pro-
grammed to explore unseen but still possible user 
behaviors. These simulated users can interact with 
the dialog systems to generate large amounts of 
training data in a low-cost and time-efficient man-
ner. Many previous studies (Scheffler, 2002; 
Pietquin, 2004) have shown that the dialog strate-
gies learned from the simulated training data out-
perform the hand-crafted strategies. There are also 
studies that use user simulation to train speech rec-
ognition and understanding components (Chung, 
2004). 
    While user simulation is largely used in dialog 
system training, it has only been used in limited 
scope for testing specific dialog system compo-
nents in the system evaluation phase (L?pez-C?zar 
et al, 2003; Filisko and Seneff, 2006). This is 
partly because the state-of-the-art simulated users 
have quite limited abilities in mimicking human 
users' behaviors and typically over-generate possi-
ble dialog behaviors. This is not a major problem 
when using simulated dialog corpus as the training 
corpus for dialog strategy learning because the 
over-generated simulation behaviors would only 
provide the machine learners with a broader dialog 
state space to explore (Ai et al, 2007). However, 
realistic user behaviors are highly desired in the 
testing phase because the systems are evaluated 
and adjusted based on the analysis of the dialogs 
generated in this phase. Therefore, we would ex-
User Simulation as Testing for Spoken Dialog Systems 
 
Hua Ai* Fuliang Weng 
Intelligent Systems Program Research and Technology Center 
University of Pittsburgh Robert Bosch LLC 
210 S. Bouquet St., Pittsburg, PA 15260 4009 Miranda Ave., Palo Alto, CA 94304 
Hua@cs.pitt.edu Fuliang.weng@us.bosch.com 
 
164
pect that these user behaviors are what we will see 
in the final evaluation with human users. In this 
case, any over-generated dialog behaviors may 
cause the system to be blamed for untargeted func-
tions. What is more, the simulated users cannot 
provide subjective user satisfaction feedback 
which is also important for improving the systems. 
Since it is expensive and time-consuming to test 
every version of the system with a significant 
amount of paid subjects, the testing during the de-
velopment is typically constrained to a limited 
number of users, and often, to repeated users who 
are colleagues or developers themselves. Thus, the 
system performance is not always optimized for 
the intended users.  
Our ultimate goal is to supplement human test-
ing with simulated users during the development to 
speed up the system development towards desired 
performance. This would be especially useful in 
the early development stage, since it would avoid 
conducting tests with human users when they may 
feel extremely frustrated due to the malfunction of 
the unstable system. 
As a first attempt, we try to extend the state-of-
the-art user simulation by incorporating a set of 
new but straightforward evaluation measures for 
automatically assessing the dialog system perform-
ance. These evaluation measures focus on three 
basic aspects of task-oriented dialog systems: un-
derstanding ability, efficiency, and the appropri-
ateness of the system actions. They are first 
applied on a corpus generated between a dialog 
system and a group of human users to demonstrate 
the validity of these measures with the human us-
ers' satisfaction scores. Results show that these 
measures are significantly correlated with the hu-
man users' satisfactions. Then, a regression model 
is built to predict the user satisfaction scores using 
these evaluation measures. We also apply the re-
gression model on a simulated dialog corpus 
trained from the above real user corpus, and show 
that the user satisfaction scores estimated from the 
simulated dialogs do not differ significantly from 
the real users? satisfaction scores. Finally, we con-
clude that these evaluation measures can be used to 
assess the system performance based on the esti-
mated user satisfaction. 
2 User Simulation Techniques  
Most user simulation models are trained from dia-
log corpora generated by human users. Earlier 
models predict user actions based on simple rela-
tions between the system actions and the following 
user responses. (Eckert et al, 1997) first suggest a 
bigram model to predict the next user's action 
based on the previous system's action. (Levin et al, 
2000) add constraints to the bigram model to ac-
cept the expected dialog acts only. However, their 
basic assumption of making the next user's action 
dependent only on the system's previous action is 
oversimplified. Later, many studies model more 
comprehensive user behaviors by adding user goals 
to constrain the user actions (Scheffler, 2002; Piet-
quin, 2004). These simulated users mimic real user 
behaviors in a statistical way, conditioning the user 
actions on the user goals and the dialog contexts. 
More recent research defines agenda for simulated 
users to complete a set of settled goals (Schatz-
mann et al, 2007). This type of simulated user up-
dates the agenda and the current goal based on the 
changes of the dialog states. 
In this study, we build a simulated user similar 
to (Schatzmann et al, 2007) in which the simulated 
user keeps a list of its goals and another agenda of 
actions to complete the goals. In our restaurant se-
lection domain, the users? tasks are to find a de-
sired restaurant based on several constraints 
specified by the task scenarios. We consider these 
restaurant constraints as the goals for the simulated 
user. At the beginning of the dialog, the simulated 
user randomly generates an agenda for the list of 
the ordered goals corresponding to the three con-
straints in requesting a restaurant. An agenda con-
tains multiple ordered items, each of which 
consists of the number of constraints and the spe-
cific constraints to be included in each user utter-
ance. During the dialog, the simulated user updates 
its list of goals by removing the constraints that 
have been understood by the system. It also re-
moves from its agenda the unnecessary actions that 
are related to the already filled goals while adding 
new actions. New actions are added according to 
the last system?s question (such as requesting the 
user to repeat the last utterance) as well as the 
simulated user?s current goals. The actions that 
address the last system?s question are given higher 
priorities then other actions in the agenda. For ex-
ample, if the dialog system fails to understand the 
last user utterance and thus requests a clarification, 
the simulated user will satisfy the system?s request 
165
before moving on to discuss a new constraint. The 
simulated user updated the agenda with the new 
actions after each user turn.  
The current simulated user interacts with the 
system on the word level. It generates a string of 
words by instantiating its current action using pre-
defined templates derived from previously col-
lected corpora with real users. Random lexical 
errors are added to simulate a spoken language 
understanding performance with a word error rate 
of 15% and a semantic error rate of 11% based on 
previous experience (Weng et al, 2006). 
3 System and Corpus  
CHAT (Conversational Helper for Automotive 
Tasks) is a spoken dialog system that supports na-
vigation, restaurant selection and mp3 player ap-
plications. The system is specifically designed for 
users to interact with devices and receive services 
while performing other cognitive demanding, or 
primary tasks such as driving (Weng et al, 2007). 
CHAT deploys a combination of off-the-shelf 
components, components used in previous lan-
guage applications, and components specifically 
developed as part of this project. The core compo-
nents of the system include a statistical language 
understanding (SLU) module with multiple under-
standing strategies for imperfect input, an informa-
tion-state-update dialog manager (DM) that 
handles multiple dialog threads and mixed initia-
tives (Mirkovic and Cavedon, 2005), a knowledge 
manager (KM) that controls access to ontology-
based domain knowledge, and a content optimizer 
that connects the DM and the KM for resolving 
ambiguities from the users' requests, regulating the 
amount of information to be presented to the user, 
as well as providing recommendations to users. In 
addition, we use Nuance 8.51 with dynamic gram-
mars and classbased n-grams, for speech recogni-
tion, and Nuance Vocalizer 3.0 for text-to-speech 
synthesis (TTS). However, the two speech compo-
nents, i.e., the recognizer and TTS are not used in 
the version of the system that interacts with the 
simulated users.  
The CHAT system was tested for the navigation 
domain, the restaurant selection and the MP3 mu-
sic player. In this study, we focus on the dialog 
corpus collected on the restaurant domain only. A 
                                                          
1 See http://www.nuance.com for details. 
small number of human users were used as dry-run 
tests for the system development from November, 
2005 to January, 2006. We group the adjacent dry-
runs to represent system improvement stages on a 
weekly basis. Table 1 shows the improvement 
stages, the dry-run dates which each stage in-
cludes, and the number of subjects tested in each 
stage. A final evaluation was conducted during 
January 19-31, 2006, without any further system 
modifications. This final evaluation involved 20 
paid subjects who were recruited via internet ad-
vertisement. 
Only the users in the final evaluation completed 
user satisfaction surveys after interacting with the 
system. In the survey, users were asked to rate the 
conversation from 6 perspectives, each on a 5-
point scale: whether the system was easy to use, 
whether the system understood the user well, 
whether the interaction pattern was natural, 
whether the system's actions were appropriate, 
whether the system acted as expected, and whether 
the user was willing to use the system on a regular 
base. A user satisfaction score was computed as 
the average of the 6 ratings. 
 
 
Nine tasks of restaurant selections were used in 
both dry-runs and the final evaluation using 12 
constraints in total (e.g., cuisine type, price level, 
location). These 12 constraints are spread across 
the nine tasks evenly with three constraints per 
task. In addition, each task is carefully worded 
based on the task-constrained and language-
unconstrained guideline. In other words, we want 
the users to form an intended mental context while 
trying to prevent them from copying the exact 
phrasing in the task description. During the dry-
runs, the users randomly pick three to four tasks to  
Stage Dry-run Dates Users
1 11/21/05, 11/22/05 2 
2 11/30/05, 12/1/05, 12/2/05 3 
3 12/7/05, 12/8/05 2 
4 12/13/05, 12/14/05, 12/15/05 5 
5 12/19/05, 12/20/05, 12/21/05 4 
6 12/27/05, 12/28/05 2 
7 1/4/06, 1/5/06 2 
8 1/10/06, 1/11/06, 1/13/06 4 
9 1/16/06, 1/17/06 3 
Table 1: Dry-runs 
166
test the system, while in the final evaluation each 
user is required to complete all of the 9 tasks. As a  
result of the final evaluation in the restaurant do-
main with 2500 restaurants, we reached a task 
completion rate of 94% with a word recognition 
rate of 85%, and a semantic accuracy rate of 89%. 
4 Evaluation Measures  
 In this section, we describe in detail the evaluation 
measures covering three basic aspects of task-
oriented dialog systems: understanding ability, ef-
ficiency, and the appropriateness of the system 
actions.  
4.1 Understanding Ability Measures 
Human-human dialog is a process to reach mutual 
understandings between the dialog partners by ex-
changing information through the dialog. This in-
formation exchanging process also takes place in 
the interaction between users and spoken dialog 
systems. In a task-oriented conversation, the dialog 
system's major task is to understand the users' 
needs in order to provide the right service. In the 
information-state update framework, the system 
continuously updates its information-states during 
the dialog while the users are conveying their re-
quirements. If a misunderstanding occurs, there 
would be a mismatch between the users? require-
ments and the system?s understandings. Thus, the 
error recovery dialog is needed to fix the mis-
matches. The error recovery dialog can be initiated 
either by the system by asking the user to rephrase 
or to repeat the previous utterance, or by the user 
to restate the previous request. 
We use the percent of agreement between the 
system's and the user's understandings (under-
standingAgreement) to measure how well the 
system understands the user. The computation of 
this measure is illustrated through the example dia-
log in Table 2. In this table, the first column shows 
the system utterances and the user utterances re-
ceived by the system. The correct words are shown 
in square brackets immediately after the misunder-
stood words (E.g., in Utterance ?User1?). The sec-
ond column represents semantic content from the 
users? utterances in the form of constraint-value 
pairs based on the system?s understandings. This 
information can be automatically retrieved from 
the system logs. The third column includes the ac-
tion types of the current system/user utterances. 
Since the dialog manager is an information-
updating dialog manager that manages information 
in the format of slots, this column also shows the 
number of slots that are exchanged in the utterance 
and the number of matched slots. In our task do-
main, the user can request information (req_info), 
request the same information again (rereq_info), 
answer an explicit confirmation (exp_confirm), 
and close a dialog (close_dialog). The system can 
provide information (pro_info) or explicitly con-
firms (exp_confirm) the information. Another 
Utterances Constraints Understood by the System 
Action Types & Under-
standing Status 
User1: I'm looking for a five star Thai [Italian] 
restaurant accept credit cards  
Action: req_info 
Slots conveyed: 3 slots 
System1: There are 7 restaurants that have 
a five star rating, accept credit cards, and 
serve Thai food in my database. Here are 
the first few: TeJUS, Slam Corner Cate, 
Setha's Restaurant. Should I continue? 
service level=five star 
cuisine type=Thai 
payment type=credit card 
Action: pro_info 
Slots accepted: 3 slots 
Completely matched: 2 slots 
 
User2: I need to look for Italian restaurant  Action: rereq_info Slot  conveyed: 1 slot 
System2: Are you looking for a restaurant 
that serves Italian food? 
service level=five star 
cuisine type=Italian (unconfirmed) 
payment type=credit card 
Action: exp_confirm 
Slot  updated: 1 slot 
Completely matched: 2.5 slot 
User3: yes  Action: exp_confirm Slot conveyed: 1 slot 
System3: I found 38 restaurants that serve 
Italian food, accept credit cards and have a 
five star rating, for example, Toto Caf? ? 
service level=five star 
cuisine type=Italian (confirmed) 
payment type=credit card 
Action: pro_info 
Slots accepted: 3 slots 
Completely matched: 3 slots 
User4: I'm done.  Action: close_dialog 
Table 2: A Sample Dialog with Annotations. 
      
167
available system action that is not shown in this 
example is to ask the user to repeat/rephrase (re-
phrase), where the user can respond by providing 
the information again (repro_info).  
In our experiment, we measure the understand-
ings between the users and the system by compar-
ing the values of the constraints that are specified 
by the users with their values understood by the 
system. In this dialog, the user specified all con-
straints in the first utterance: 
Service level = Five star 
           Cuisine type = Italian 
  Payment type = Credit card 
    The first system utterance shows that the system 
understood two constraints but misunderstood the 
cuisine type, thus the percent agreement of mutual 
understandings is 2/3 at this time. Then, the user 
restated the cuisine type and the second system 
utterance confirmed this information. Since the 
system only asks for explicit information when its 
confidence is low, we count the system's under-
standing on the cuisine type as a 50% match with 
the user's. Therefore, the total percent agreement is 
2.5/3. The user then confirmed that the system had 
correctly understood all constraints. Therefore, the 
system provided the restaurant information in the 
last utterance. The system's understanding matches 
100% with the user's at this point.  
    The percent agreement of system/user under-
standings over the entire dialog is calculated by 
averaging the percent agreement after each turn. In 
this example, understandingAgreement is (2/3 + 
2.5/3 + 1)/3 =83.3%. We hypothesize that the 
higher the understandingAgreement is, the better 
the system performs, and thus the more the user is 
satisfied. The matches of understandings can be 
calculated automatically from the user simulation 
and the system logs. However, since we work with 
human users' dialogs in the first part of this study, 
we manually annotated the semantic contents (e.g., 
cuisine name) in the real user corpus.  
Previous studies (E.g., Walker et al, 1997) use a 
corpus level semantic accuracy measure (semanti-
cAccuracy) to capture the system?s understanding 
ability. SemanticAccuracy is defined in the stan-
dard way as the total number of correctly under-
stood constraints divided by the total number of 
constraints mentioned in the entire dialog. The un-
derstandingAgreement measure we introduce here 
is essentially the averaged per-sentence semantic 
accuracy, which emphasizes the utterance level 
perception rather than a single corpus level aver-
age. The intuition behind this new measure is that 
it is better for the system to always understand 
something to keep a conversation going than for 
the system to understand really well sometimes but 
really bad at other times. We compute both meas-
ures in our experiments for comparison.  
4.2 Efficiency Measure 
Efficiency is another important measure of the sys-
tem performance. A standard efficiency measure is 
the number of dialog turns. However, we would 
like to take into account the user's dialog strategy 
because how the user specifies the restaurant selec-
tion constraints has a certain impact on the dialog 
pace. Comparing two situations where one user 
specifies the three constraints of selecting a restau-
rant in three separate utterances, while another user 
specifies all the constraints in one utterance, we 
will find that the total number of dialog turns in the 
second situation is smaller assuming perfect under-
standings. Thus, we propose to use the ratio be-
tween the number of turns in the perfect 
understanding situation and the number of turns in 
practice (efficiencyRatio) to measure the system 
efficiency. The larger the efficiencyRatio is, the 
closer the actual number of turns is to the perfect 
understanding situation. In the example in Table 2, 
because the user chose to specify all the constraints 
in one utterance, the dialog length would be 2 turns 
in perfect understanding situation (excluding the 
last user turn which is always "I'm done"). How-
ever, the actual dialog length is 6 turns. Thus, the 
efficiencyRatio is 2/6. 
Since our task scenarios always contain three 
constraints, we can calculate the length of the er-
ror-free dialogs based on the user?s strategy. When 
the user specifies all constraints in the first utter-
ance, the ideal dialog will have only 2 turns; when 
the user specifies two constraints in one utterance 
and the other constraints in a separate utterance, 
the ideal dialog will have 4 turns; when the user 
specifies all constraints one by one, the ideal dia-
log will have 6 turns. Thus, in the simulation envi-
ronment, the length of the ideal dialog can be 
calculated from the simulated users? agenda. Then, 
the efficiencyRatio can be calculated automati-
cally. We manually computed this measure for the 
real users? dialogs. 
168
Similarly, in order to compare with previous 
studies, we also investigate the total number of 
dialog turns (dialogTurns) proposed as the effi-
ciency measure (E.g., M?ller et al, 2007).  
4.3 Action Appropriateness Measure  
This measure aims to evaluate the appropriateness 
of the system actions. The definition of appropri-
ateness can vary on different tasks and different 
system design requirements. For example, some 
systems always ask users to explicitly confirm 
their utterances due to high security needs. In this 
case, an explicit confirmation after each user utter-
ance is an appropriate system action. However, in 
other cases, frequent explicit confirmations may be 
considered as inappropriate because they may irri-
tate the users. In our task domain, we define the 
only inappropriate system action to be providing 
information based on misunderstood user require-
ments. In this situation, the system is not aware of 
its misunderstanding error. Instead of conducting 
an appropriate error-recovering dialog, the system 
provides wrong information to the user which we 
hypothesize will decrease the user?s satisfaction.  
We use the percentage of appropriate system ac-
tions out of the total number of system actions 
(percentAppropriate) to measure the appropriate-
ness of system actions. In the example in Table 2, 
only the first system action is inappropriate in all 3 
system actions. Thus, the percent system action 
appropriateness is 2/3. Since we can detect the sys-
tem?s misunderstanding and the system?s action in 
the simulated dialog environment, this measure can 
be calculated automatically for the simulated dia-
logs. For the real user corpus, we manually coded 
the inappropriate system utterances.  
Note that the definition of appropriate action we 
use here is fairly loose. This is partly due to the 
simplicity of our task domain and the limited pos-
sible system/user actions. Nevertheless, there is 
also an advantage of the loose definition: we do 
not bias towards one particular dialog strategy 
since our goal here is to find some general and eas-
ily measurable system performance factors that are 
correlated with the user satisfaction. 
5 Investigating Evaluation Measures on 
the Real User Corpus  
In this section, we first validate the proposed 
measures using real users? satisfaction scores, and 
then show the differentiating power of these meas-
ures through the improvement curves plotted on 
the dry-run data. 
5.1 Validating Evaluation Measures 
To validate the evaluation measures introduced in 
Section 4, we use Pearson?s correlation to examine 
how well these evaluation measures can predict the 
user satisfaction scores. Here, we only look at the 
dialog corpus in final evaluation because only 
these users filled out the user satisfaction surveys. 
For each user, we compute the average value of the 
evaluation measures across all dialogs generated 
by that user. 
 
Table 3 lists the correlation between the evalua-
tion measures and the user satisfaction scores, as 
well as the p-value for each correlation. The corre-
lation describes a linear relationship between these 
measures and the user satisfaction scores. For the 
measures that describe the system?s understanding 
abilities and the measures that describe the sys-
tem?s efficiency, our newly proposed measures 
show higher correlations with the user satisfaction 
scores than their counterparts. Therefore, in the 
rest of the study, we drop the two measures used 
by the previous studies, i.e., semanticAccuracy and 
dialogTurns.  
We observe that the user satisfaction scores are 
significantly positively correlated with all the three 
proposed measures. These correlations confirms 
our expectations: user satisfaction is higher when 
the system?s understanding matches better with the 
users? requirements; when the dialog efficiency is 
closer to the situation of perfect understanding; or 
when the system's actions are mostly appropriate. 
We suggest that these measures can serve as indi-
cators for user satisfaction.  
    We further use all the measures to build a re-
gression model to predict the user satisfaction 
score. The prediction model is: 
Evaluation Measure Correlation P-value 
understandingAgreement 0.354 0.05 
semanticAccuracy 0.304 0.08 
efficiencyRatio 0.406 0.02 
dialogTurns -0.321 0.05 
percentAppropriate 0.454 0.01 
Table3: Correlations with User Satisfaction Scores. 
169
User Satisfaction  
   = 6.123*percentAppropriate 
  +2.854*efficiencyRatio                         --- (1) 
      +0.864*understandingAgreement - 4.67 
 
The R-square is 0.655, which indicates that 
65.5% of the user satisfaction scores can be ex-
plained by this model. While this prediction model 
has much room for improvement, we suggest that 
it can be used to estimate the users? satisfaction 
scores for simulated users in the early system test-
ing stage to quickly assess the system's perform-
ance. Since the weights are tuned based on the data 
from this specific application, the prediction model 
may not be used directly for other domains.  
5.2 Assessing the Differentiating Power of the 
Evaluation Measures 
Since this set of evaluation measures intends to 
evaluate the system's performance in the develop-
ment stage, we would like the measures to be able 
to reflect small changes made in the system and to 
indicate whether these changes show the right 
trend of increased user satisfaction in reality. A set 
of good evaluation measures should be sensible to 
subtle system changes. 
We assess the differentiating power of the eval-
uation measures using the dialog corpus collected 
during the dry-runs. The system was tested on a 
weekly basis as explained in Table 1. For each im-
provement stage, we compute the values for the 
three evaluation measures averaging across all dia-
logs from all users. Figure 1 shows the three im-
provement curves based on these three measures. 
The x-axis shows the first date of each improve-
ment stage; the y-axis shows the value of the eval-
uation measures. We observe that all three curves 
show the right trends that indicate the system?s 
improvements over the development stages.  
6 Applying the Evaluation Measures on 
the Simulated Corpus  
We train a goal and agenda driven user simulation 
model from the final evaluation dialog corpus with 
the real users. The simulation model interacts with 
the dialog system 20 times (each time the simula-
tion model represents a different simulated user), 
generating nine dialogs on all of the nine tasks 
each time. In each interaction, the simulated users 
generate their agenda randomly based on a uniform 
distribution. The simulated corpus consists of 180 
dialogs from 20 simulated users, which is of the 
same size as the real user corpus. The values of the 
evaluation measures are computed automatically at 
the end of each simulated dialog. 
   We compute the estimated user satisfaction score 
using Equation 1 for each simulated user. We then 
compare the user satisfaction scores of the 20 si-
mulated users with the satisfaction scores of the 20 
real users. The average and the standard deviation 
of the user satisfaction scores for real users are 
(3.79, 0.72), and the ones for simulated users are 
(3.77, 1.34). Using two-tailed t-test at significance 
level p<0.05, we observe that there are no statisti-
cally significant differences between the two pools 
of scores. Therefore, we suggest that the user satis-
faction estimated from the simulated dialog corpus 
can be used to assess the system performance. 
However, these average scores only offer us one 
perspective in comparing the real with the simu-
lated user satisfaction. In the future, we would like 
to look further into the differences between the 
distributions of these user satisfaction scores. 
7 Conclusions and Future Work  
User simulation has been increasingly used in gen-
erating large corpora for using machine learning 
techniques to automate dialog system design. 
However, user simulation has not been used much 
in testing dialog systems. There are two major con-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
11/21/05 11/30/05 12/05/05 12/13/05 12/19/05 12/27/05 01/04/06 01/10/06 01/16/06
understandingAgreement eff iciencyRatio percentAppropriate
Figure 1: The Improvement Curves on Dry-run Data 
170
cerns: 1. we are not sure how well the state-of-the-
art user simulation can mimic realistic user behav-
iors; 2. we do not get important feedback on user 
satisfaction when replacing human users with 
simulated users. In this study, we suggest that 
while the simulated users might not be mature to 
use in the final system evaluation stage, they can 
be used in the early testing stages of the system 
development cycle to make sure that the system is 
functioning in the desired way. We further propose 
a set of evaluation measures that can be extracted 
from the simulation logs to assess the system per-
formance. We validate these evaluation measures 
on human user dialogs and examine the differenti-
ating power of these measures. We suggest that 
these measures can be used to guide the develop-
ment of the system towards improving user satis-
faction. We also apply the evaluation measures on 
a simulation corpus trained from the real user dia-
logs. We show that the user satisfaction scores es-
timated on the simulated dialogs do not 
significantly differ statistically from the real users? 
satisfaction scores. Therefore, we suggest that the 
estimated user satisfaction can be used to assess 
the system performance while testing with simu-
lated users.  
In the future, we would like to confirm our pro-
posed evaluation measures by testing them on dia-
log systems that allows more complicated dialog 
structures and systems on other domains.  
Acknowledgments 
The authors would like to thank Zhongchao 
Fei, Zhe Feng, Junkuo Cao, and Baoshi Yan 
for their help during the simulation system de-
velopment and the three anonymous reviewers 
for their insightful suggestions. All the remain-
ing errors are ours.  
References  
H. Ai, J. Tetreault, and D. Litman. 2007. Comparing 
User Simulation Models for Dialog Strategy Learn-
ing. In Proc. NAACL-HLT (short paper session). 
G. Chung. 2004. Developing a Flexible Spoken Dialog 
System Using Simulation. In Proc. of ACL 04. 
W. Eckert, E. Levin, and R. Pieraccini. 1997. User 
Modeling for Spoken Dialogue System Evaluation. In 
Proc. of IEEE workshop on ASRU. 
E. Filisko and S. Seneff. 2006. Learning Decision Mod-
els in Spoken Dialogue Systems Via User Simulation. 
In Proc. of AAAI Workshop on Statistical and Em-
pirical Approaches for Spoken Dialogue Systems. 
J. Henderson, O. Lemon, and K. Georgila. 2005. Hybrid 
Reinforcement/Supervised Learning for Dialogue 
Policies from COMMUNICATOR data. In IJCAI 
workshop on Knowledge and Reasoning in Practical 
Dialogue Systems. 
E. Levin, R. Pieraccini, and W. Eckert. 2000. A Stochas-
tic Model of Human-Machine Interaction For learn-
ing Dialogue Strategies. IEEE Trans. On Speech and 
Audio Processing, 8(1):11-23. 
R. L?pez-C?zar, A. De la Torre, J. C. Segura and A. J. 
Rubio. (2003). Assessment of dialogue systems by 
means of a new simulation technique. Speech Com-
munication (40): 387-407. 
D. Mirkovic and L. Cavedon. 2005. Practical multi-
domain, multi-device dialogue management, 
PACLING'05: 6th Meeting of the Pacific Association 
for Computational Linguistics. 
Sebastian M?ller, Jan Krebber and Paula Smeele. 2006. 
Evaluating the speech output component of a smart-
home system. Speech Communication (48): 1-27. 
O. Pietquin, O. 2004. A Framework for Unsupervised 
Learning of Dialog Strategies. Ph.D. diss., Faculte 
Polytechnique de Mons. 
K. Scheffler. 2002. Automatic Design of Spoken Dialog 
Systems. Ph.D. diss., Cambridge University. 
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002. 
Optimizing DialogueManagement with Reinforce-
ment Learning: Experiments with the NJFun System. 
Journal of Artificial Intelligence Research (JAIR), 
vol. 16. 
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, 
and Young. S. 2007. Agenda-Based User Simulation 
for Bootstrapping a POMDP Dialogue System. In 
Proc. of NAACL-HLT (short paper session). 
F. Weng, S. Varges, B. Raghunathan, F. Ratiu, H. Pon-
Barry, B. Lathrop, Q. Zhang, H. Bratt, T. Scheideck, 
R. Mishra, K. Xu, M. Purvey, A. Lien, M. Raya, S. 
Peters, Y. Meng, J. Russell,  L. Cavedon, E. Shri-
berg, and H. Schmidt. 2006. CHAT: A Conversa-
tional Helper for Automotive Tasks. In Proc. of 
Interspeech. 
F. Weng, B. Yan, Z. Feng, F. Ratiu, M. Raya, B. Lath-
rop, A. Lien, S. Varges, R. Mishra, F. Lin, M. Purver, 
H. Bratt, Y. Meng, S. Peters, T. Scheideck, B. Rag-
hunathan and Z. Zhang. 2007. CHAT to your destina-
tion. In Proc. Of 8th SIGdial workshop on Discourse 
and Dialogue. 
J. Williams and S. Young. 2006. Partially Observable 
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language. 
M. Walker, D. Litman, C. Kamm, and A. Abella. 1997. 
PARADISE: A Framework for Evaluating Spoken 
Dialogue Agents. In Proceedings of the 35th ACL. 
171
