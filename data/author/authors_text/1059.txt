Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 49?56,
Sydney, July 2006. c?2006 Association for Computational Linguistics
N Semantic Classes are Harder than Two
Ben Carterette?
CIIR
University of Massachusetts
Amherst, MA 01003
carteret@cs.umass.edu
Rosie Jones
Yahoo! Research
3333 Empire Ave.
Burbank, CA 91504
jonesr@yahoo-inc.com
Wiley Greiner?
Los Angeles Software Inc.
1329 Pine Street
Santa Monica, CA 90405
w.greiner@lasoft.com
Cory Barr
Yahoo! Research
3333 Empire Ave.
Burbank, CA 91504
barrc@yahoo-inc.com
Abstract
We show that we can automatically clas-
sify semantically related phrases into 10
classes. Classification robustness is im-
proved by training with multiple sources
of evidence, including within-document
cooccurrence, HTML markup, syntactic
relationships in sentences, substitutability
in query logs, and string similarity. Our
work provides a benchmark for automatic
n-way classification into WordNet?s se-
mantic classes, both on a TREC news cor-
pus and on a corpus of substitutable search
query phrases.
1 Introduction
Identifying semantically related phrases has been
demonstrated to be useful in information retrieval
(Anick, 2003; Terra and Clarke, 2004) and spon-
sored search (Jones et al, 2006). Work on seman-
tic entailment often includes lexical entailment as
a subtask (Dagan et al, 2005).
We draw a distinction between the task of iden-
tifying terms which are topically related and iden-
tifying the specific semantic class. For example,
the terms ?dog?, ?puppy?, ?canine?, ?schnauzer?,
?cat? and ?pet? are highly related terms, which
can be identified using techniques that include
distributional similarity (Lee, 1999) and within-
document cooccurrence measures such as point-
wise mutual information (Turney et al, 2003).
These techniques, however, do not allow us to dis-
tinguish the more specific relationships:
? hypernym(dog,puppy)
?This work was carried out while these authors were at
Yahoo! Research.
? hyponym(dog,canine)
? coordinate(dog,cat)
Lexical resources such as WordNet (Miller,
1995) are extremely useful, but are limited by be-
ing manually constructed. They do not contain se-
mantic class relationships for the many new terms
we encounter in text such as web documents, for
example ?mp3 player? or ?ipod?. We can use
WordNet as training data for such classification to
the extent that the training on pairs found in Word-
Net and testing on pairs found outside WordNet
provides accurate generalization.
We describe a set of features used to train n-
way supervised machine-learned classification of
semantic classes for arbitrary pairs of phrases. Re-
dundancy in the sources of our feature informa-
tion means that we are able to provide coverage
over an extremely large vocabulary of phrases. We
contrast this with techniques that require parsing
of natural language sentences (Snow et al, 2005)
which, while providing reasonable performance,
can only be applied to a restricted vocabulary of
phrases cooccuring in sentences.
Our contributions are:
? Demonstration that binary classification re-
moves the difficult cases of classification into
closely related semantic classes
? Demonstration that dependency parser paths
are inadequate for semantic classification into
7 WordNet classes on TREC news corpora
? A benchmark of 10-class semantic classifica-
tion over highly substitutable query phrases
? Demonstration that training a classifier us-
ing WordNet for labeling does not generalize
well to query pairs
? Demonstration that much of the performance
in classification can be attained using only
49
syntactic features
? A learning curve for classification of query
phrase pairs that suggests the primary bottle-
neck is manually labeled training instances:
we expect our benchmark to be surpassed.
2 Relation to Previous Work
Snow et al (2005) demonstrated binary classi-
fication of hypernyms and non-hypernyms using
WordNet (Miller, 1995) as a source of training la-
bels. Using dependency parse tree paths as fea-
tures, they were able to generalize from WordNet
labelings to human labelings.
Turney et al (2003) combined features to an-
swer multiple-choice synonym questions from the
TOEFL test and verbal analogy questions from
the SAT college entrance exam. The multiple-
choice questions typically do not consist of mul-
tiple closely related terms. A typical example is
given by Turney:
? hidden:: (a) laughable (c) ancient
(b) veiled (d) revealed
Note that only (b) and (d) are at all related to the
term, so the algorithm only needs to distinguish
antonyms from synonyms, not synonyms from say
hypernyms.
We use as input phrase pairs recorded in query
logs that web searchers substitute during search
sessions. We find much more closely related
phrases:
? hidden::
(a) secret (e) hiden
(b) hidden camera (f) voyeur
(c) hidden cam (g) hide
(d) spy
This set contains a context-dependent synonym,
topically related verbs and nouns, and a spelling
correction. All of these could cooccur on web
pages, so simple cooccurrence statistics may not
be sufficient to classify each according to the se-
mantic type.
We show that the techniques used to perform
binary semantic classification do not work as well
when extended to a full n-way semantic classifi-
cation. We show that using a variety of features
performs better than any feature alone.
3 Identifying Candidate Phrases for
Classification
In this section we introduce the two data sources
we use to extract sets of candidate related phrases
for classification: a TREC-WordNet intersection
and query logs.
3.1 Noun-Phrase Pairs Cooccuring in TREC
News Sentences
The first is a data-set derived from TREC news
corpora and WordNet used in previous work for
binary semantic class classification (Snow et al,
2005). We extract two sets of candidate-related
pairs from these corpora, one restricted and one
more complete set.
Snow et al obtained training data from the inter-
section of noun-phrases cooccuring in sentences in
a TREC news corpus and those that can be labeled
unambiguously as hypernyms or non-hypernyms
using WordNet. We use a restricted set since in-
stances selected in the previous work are a subset
of the instances one is likely to encounter in text.
The pairs are generally either related in one type
of relationship, or completely unrelated.
In general we may be able to identify related
phrases (for example with distributional similarity
(Lee, 1999)), but would like to be able to automat-
ically classify the related phrases by the type of
the relationship. For this task we identify a larger
set of candidate-related phrases.
3.2 Query Log Data
To find phrases that are similar or substitutable for
web searchers, we turn to logs of user search ses-
sions. We look at query reformulations: a pair
of successive queries issued by a single user on
a single day. We collapse repeated searches for
the same terms, as well as query pair sequences
repeated by the same user on the same day.
3.2.1 Substitutable Query Segments
Whole queries tend to consist of several con-
cepts together, for example ?new york | maps? or
?britney spears | mp3s?. We identify segments or
phrases using a measure over adjacent terms sim-
ilar to mutual information. Substitutions occur at
the level of segments. For example, a user may
initially search for ?britney spears | mp3s?, then
search for ?britney spears | music?. By aligning
query pairs with a single substituted segment, we
generate pairs of phrases which a user has substi-
tuted. In this example, the phrase ?mp3s? was sub-
stituted by the phrase ?music?.
Aggregating substitutable pairs over millions of
users and millions of search sessions, we can cal-
culate the probability of each such rewrite, then
50
test each pair for statistical significance to elim-
inate phrase rewrites which occurred in a small
number of sessions, perhaps by chance. To test
for statistical significance we use the pair inde-
pendence likelihood ratio, or log-likelihood ratio,
test. This metric tests the hypothesis that the prob-
ability of phrase ? is the same whether phrase ?
has been seen or not by calculating the likelihood
of the observed data under a binomial distribution
using probabilities derived using each hypothesis
(Dunning, 1993).
log? = logL (P (?|?) = P (?|??))L (P (?|?) 6= P (?|??))
A high negative value for ? suggests a strong
dependence between query ? and query ?.
4 Labeling Phrase Pairs for Supervised
Learning
We took a random sample of query segment sub-
stitutions from our query logs to be labeled. The
sampling was limited to pairs that were frequent
substitutions for each other to ensure a high prob-
ability of the segments having some relationship.
4.1 WordNet Labeling
WordNet is a large lexical database of English
words. In addition to defining several hun-
dred thousand words, it defines synonym sets, or
synsets, of words that represent some underly-
ing lexical concept, plus relationships between
synsets. The most frequent relationships between
noun-phrases are synonym, hyponym, hypernym,
and coordinate, defined in Table 1. We also may
use meronym and holonym, defined as the PART-OF
relationship.
We used WordNet to automatically label the
subset of our sample for which both phrases occur
in WordNet. Any sense of the first segment having
a relationship to any sense of the second would re-
sult in the pair being labeled. Since WordNet con-
tains many other relationships in addition to those
listed above, we group the rest into the other cate-
gory. If the segments had no relationship in Word-
Net, they were labeled no relationship.
4.2 Segment Pair Labels
Phrase pairs passing a statistical test are com-
mon reformulations, but can be of many seman-
tic types. Rieh and Xie (2001) categorized types
of query reformulations, defining 10 general cat-
egories: specification, generalization, synonym,
parallel movement, term variations, operator us-
age, error correction, general resource, special re-
source, and site URLs. We redefine these slightly
to apply to query segments. The summary of the
definitions is shown in Table 1, along with the dis-
tribution in the data of pairs passing the statistical
test.
4.2.1 Hand Labeling
More than 90% of phrases in query logs do not
appear in WordNet due to being spelling errors,
web site URLs, proper nouns of a temporal nature,
etc. Six annotators labeled 2, 463 segment pairs
selected randomly from our sample. Annotators
agreed on the label of 78% of pairs, with a Kappa
statistic of .74.
5 Automatic Classification
We wish to perform supervised classification of
pairs of phrases into semantic classes. To do this,
we will assign features to each pair of phrases,
which may be predictive of their semantic rela-
tionship, then use a machine-learned classifier to
assign weights to these features. In Section 7 we
will look at the learned weights and discuss which
features are most significant for identifying which
semantic classes.
5.1 Features
Features for query substitution pairs are extracted
from query logs and web pages.
5.1.1 Web Page / Document Features
We submit the two segments to a web search
engine as a conjunctive query and download the
top 50 results. Each result is converted into an
HTML Document Object Model (DOM) tree and
segmented into sentences.
Dependency Tree Paths The path from the first
segment to the second in a dependency parse
tree generated by MINIPAR (Lin, 1998)
from sentences in which both segments ap-
pear. These were previously used by Snow
et al (2005). These features were extracted
from web pages in all experiments, except
where we identify that we used TREC news
stories (the same data as used by Snow et al).
HTML Paths The paths from DOM tree nodes
the first segment appears in to nodes the sec-
ond segment appears in. The value is the
number of times the path occurs with the pair.
51
Class Description Example %
synonym one phrase can be used in place of the other without loss in meaning low cost; cheap 4.2
hypernym X is a hypernym of Y if and only if Y is a X muscle car; mustang 2.0
hyponym X is a hyponym of Y if and only if X is a Y (inverse of hypernymy) lotus; flowers 2.0
coordinate there is some Z such that X and Y are both Zs aquarius; gemini 13.9
generalization X is a generalization of Y if X contains less information about the topic lyrics; santana lyrics 4.8
specialization X is a specification of Y if X contains more information about the topic credit card; card 4.7
spelling change spelling errors, typos, punctuation changes, spacing changes peopl; people 14.9
stemmed form X and Y have the same lemmas ant; ants 3.4
URL change X and Y are related and X or Y is a URL alliance; alliance.com 29.8
other relationship X and Y are related in some other way flagpoles; flags 9.8
no relationship X and Y are not related in any obvious way crypt; tree 10.4
Table 1: Semantic relationships between phrases rewritten in query reformulation sessions, along with their prevalence in our
data.
Lexico-syntactic Patterns (Hearst, 1992) A sub-
string occurring between the two segments
extracted from text in nodes in which both
segments appear. In the example fragment
?authors such as Shakespeare?, the feature
is ?such as? and the value is the number of
times the substring appears between ?author?
and ?Shakespeare?.
5.1.2 Query Pair Features
Table 2 summarizes features that are induced
from the query strings themselves or calculated
from query log data.
5.2 Additional Training Pairs
We can double our training set by adding for each
pair u1, u2 a new pair u2, u1. The class of the new
pair is the same as the old in all cases but hyper-
nym, hyponym, specification, and generalization,
which are inverted. Features are reversed from
f(u1, u2) to f(u2, u1).
A pair and its inverse have different sets of fea-
tures, so splitting the set randomly into training
and testing sets should not result in resubstitution
error. Nonetheless, we ensure that a pair and its
inverse are not separated for training and testing.
5.3 Classifier
For each class we train a binary one-vs.-all linear-
kernel support vector machine (SVM) using the
optimization algorithm of Keerthi and DeCoste
(2005).
5.3.1 Meta-Classifier
For n-class classification, we calibrate SVM
scores to probabilities using the method described
by Platt (2000). This gives us P (class|pair) for
each pair. The final classification for a pair is
argmaxclassP (class|pair).
Source Snow (NIPS 2005) Experiment
Task binary hypernym binary hypernym
Data WordNet-TREC WordNet-TREC
Instance Count 752,311 752,311
Features minipar paths minipar paths
Feature Count 69,592 69,592
Classifier logistic Regression linear SVM
maxF 0.348 0.453
Table 3: Snow et als (2005) reported performance using lin-
ear regression, and our reproduction of the same experiment,
using a support vector machine (SVM).
5.3.2 Evaluation
Binary classifiers are evaluated by ranking in-
stances by classification score and finding the Max
F1 (the harmonic mean of precision and recall;
ranges from 0 to 1) and area under the ROC curve
(AUC; ranges from 0.5 to 1 with at least 0.8 being
?good?). The meta-classifier is evaluated by pre-
cision and recall of each class and classification
accuracy of all instances.
6 Experiments
6.1 Baseline Comparison to Snow et al?s
Previous Hypernym Classification on
WordNet-TREC data
Snow et al (2005) evaluated binary classifi-
cation of noun-phrase pairs as hypernyms or
non-hypernyms. When training and testing on
WordNet-labeled pairs from TREC sentences,
they report classifier Max F of 0.348, using de-
pendency path features and logistic regression. To
justify our choice of an SVM for classification, we
replicated their work. Snow et al provided us with
their data. With our SVM we achieved a Max F of
0.453, 30% higher than they reported.
6.2 Extending Snow et al?s WordNet-TREC
Binary Classification to N Classes
Snow et al select pairs that are ?Known Hyper-
nyms? (the first sense of the first word is a hy-
52
Feature Description
Levenshtein Distance # character insertions/deletions/substitutions to change query ? to query ? (Levenshtein, 1966).
Word Overlap Percent # words the two queries have in common, divided by num. words in the longer query.
Possible Stem 1 if the two segments stem to the same root using the Porter stemmer.
Substring Containment 1 if the first segment is a substring of the second.
Is URL 1 if either segment matches a handmade URL regexp.
Query Pair Frequency # times the pair was seen in the entire unlabeled corpus of query pairs.
Log Likelihood Ratio The Log Likelihood Ratio described in Section 3.2.1 Formula 3.2.1
Dice and Jaccard Coefficients Measures of the similarity of substitutes for and by the two phrases.
Table 2: Syntactic and statistical features over pairs of phrases.
ponym of the first sense of the second and both
have no more than one tagged sense in the Brown
corpus) and ?Known Non-Hypernyms? (no sense
of the first word is a hyponym of any sense of the
second). We wished to test whether making the
classes less cleanly separable would affect the re-
sults, and also whether we could use these features
for n-way classification.
From the same TREC corpus we extracted
known synonym, known hyponym, known coordi-
nate, known meronym, and known holonym pairs.
Each of these classes is defined analogously to the
known hypernym class; we selected these six rela-
tionships because they are the six most common.
A pair is labeled known no-relationship if no sense
of the first word has any relationship to any sense
of the second word. The class distribution was se-
lected to match as closely as possible that observed
in query logs. We labeled 50,000 pairs total.
Results are shown in Table 4(a). Although AUC
is fairly high for all classes, MaxF is low for all
but two. MaxF has degraded quite a bit for hyper-
nyms from Table 3. Removing all instances except
hypernym and no relationship brings MaxF up to
0.45, suggesting that the additional classes make it
harder to separate hypernyms.
Metaclassifier accuracy is very good, but this is
due to high recall of no relationship and coordi-
nate pairs: more than 80% of instances with some
relationship are predicted to be coordinates, and
most of the rest are predicted no relationship. It
seems that we are only distinguishing between no
vs. some relationship.
The size of the no relationship class may be bi-
asing the results. We removed those instances, but
performance of the n-class classifier did not im-
prove (Table 4(b)). MaxF of binary classifiers did
improve, even though AUC is much worse.
6.3 N-Class Classification of Query Pairs
We now use query pairs rather than TREC pairs.
6.3.1 Classification Using Only Dependency
Paths
We first limit features to dependency paths in
order to compare to the prior results. Dependency
paths cannot be obtained for all query phrase pairs,
since the two phrases must appear in the same sen-
tence together. We used only the pairs for which
we could get path features, about 32% of the total.
Table 5(a) shows results of binary classification
and metaclassification on those instances using de-
pendency path features only. We can see that de-
pendency paths do not perform very well on their
own: most instances are assigned to the ?coordi-
nate? class that comprises a plurality of instances.
A comparison of Tables 5(a) and 4(a) suggests
that classifying query substitution pairs is harder
than classifying TREC phrases.
Table 5(b) shows the results of binary clas-
sification and metaclassification on the same in-
stances using all features. Using all features im-
proves performance dramatically on each individ-
ual binary classifier as well as the metaclassifier.
6.3.2 Classification on All Query Pairs Using
All Features
We now expand to all of our hand-labeled pairs.
Table 6(a) shows results of binary and meta classi-
fication; Figure 1 shows precision-recall curves for
10 binary classifiers (excluding URLs). Our clas-
sifier does quite well on every class but hypernym
and hyponym. These two make up a very small
percentage of the data, so it is not surprising that
performance would be so poor.
The metaclassifier achieved 71% accuracy. This
is significantly better than random or majority-
class baselines, and close to our 78% interanno-
tator agreement. Thresholding the metaclassifier
to pairs with greater than .5 max class probability
(68% of instances) gives 85% accuracy.
Next we wish to see how much of the perfor-
mance can be maintained without using the com-
53
binary n-way data
class maxF AUC prec rec %
no rel .980 .986 .979 .985 80.0
synonym .028 .856 0 0 0.3
hypernym .185 .888 .512 .019 2.1
hyponym .193 .890 .462 .016 2.1
coordinate .808 .971 .714 .931 14.8
meronym .158 .905 .615 .050 0.3
holonym .120 .883 .909 .062 0.3
metaclassifier accuracy .927
(a) All seven WordNet classes. The high accuracy is
mostly due to high recall of no rel and coordinate classes.
binary n-way data
maxF AUC prec rec %
? ? ? ? 0
.086 .683 0 0 1.7
.337 .708 .563 .077 10.6
.341 .720 .527 .080 10.6
.857 .737 .757 .986 74.1
.251 .777 .500 .068 1.5
.277 .767 .522 .075 1.5
? .749
(b) Removing no relationship instances
improves MaxF and recall of all classes,
but performance is generally worse.
Table 4: Performance of 7 binary classifier and metaclassifiers on phrase-pairs cooccuring in TREC data labeled with WordNet
classes, using minipar dependency features. These features do not seem to be adequate for distinguishing classes other than
coordinate and no-relationship.
binary n-way
class maxf auc prec rec
no rel .281 .611 .067 .006
synonym .269 .656 .293 .167
hypernym .140 .626 0 0
hyponym .121 .610 0 0
coordinate .506 .760 .303 .888
spelling .288 .677 .121 .022
stemmed .571 .834 .769 .260
URL .742 .919 .767 .691
generalization .082 .547 0 0
specification .085 .528 0 0
other .393 .681 .384 .364
metaclassifier accuracy .385
(a) Dependency tree paths only.
binary n-way data
maxf auc prec rec % % full
.602 .883 .639 .497 10.6 3.5
.477 .851 .571 .278 4.5 1.5
.167 .686 .125 .017 3.7 1.2
.136 .660 0 0 3.7 1.2
.747 .935 .624 .862 21.0 6.9
.814 .970 .703 .916 11.0 3.6
.781 .972 .788 .675 4.8 1.6
1 1 1 1 16.2 5.3
.490 .883 .489 .393 3.5 1.1
.584 .854 .600 .589 3.5 1.1
.641 .895 .603 .661 17.5 5.7
? .692 ?
(b) All features.
Table 5: Binary and metaclassifier performance on the 32% of hand-labeled instances with dependency path features. Adding
all our features significantly improves performance over just using dependency paths.
putationally expensive syntactic parsing of depen-
dency paths. To estimate the marginal gain of the
other features over the dependency paths, we ex-
cluded the latter features and retrained our clas-
sifiers. Results are shown in Table 6(b). Even
though binary and meta-classifier performance de-
creases on all classes but generalizations and spec-
ifications, much of the performance is maintained.
Because URL changes are easily identifiable by
the IsURL feature, we removed those instances
and retrained the classifiers. Results are shown in
Table 6(c). Although overall accuracy is worse,
individual class performance is still high, allow-
ing us to conclude our results are not only due to
the ease of classifying URLs.
We generated a learning curve by randomly
sampling instances, training the binary classifiers
on that subset, and training the metaclassifier on
the results of the binary classifiers. The curve is
shown in Figure 2. With 10% of the instances, we
have a metaclassifier accuracy of 59%; with 100%
of the data, accuracy is 71%. Accuracy shows no
sign of falling off with more instances.
6.4 Training on WordNet-Labeled Pairs Only
Figure 2 implies that more labeled instances will
lead to greater accuracy. However, manually la-
beled instances are generally expensive to obtain.
Here we look to other sources of labeled instances
for additional training pairs.
6.4.1 Training and Testing on WordNet
We trained and tested five classifiers using 10-
fold cross validation on our set of WordNet-
labeled query segment pairs. Results for each class
are shown in Table 7. We seem to have regressed
to predicting no vs. some relationship.
Because these results are not as good as the
human-labeled results, we believe that some of our
performance must be due to peculiarities of our
data. That is not unexpected: since words that ap-
pear in WordNet are very common, features are
much noisier than features associated with query
entities that are often structured within web pages.
54
binary n-way
class maxf auc prec rec
no rel .531 .878 .616 .643
synonym .355 .820 .506 .212
hypernym .173 .821 .100 .020
hyponym .173 .797 .059 .010
coordinate .635 .921 .590 .703
spelling .778 .960 .625 .904
stemmed .703 .973 .786 .589
URL 1 1 1 1
generalization .565 .916 .575 .483
specification .661 .926 .652 .506
other .539 .898 .575 .483
metaclassifier accuracy .714
(a) All features.
binary n-way data
maxf auc prec rec %
.466 .764 .549 .482 10.4
.351 .745 .493 .178 4.2
.133 .728 0 0 2.0
.163 .733 0 0 2.0
.539 .832 .565 .732 13.9
.723 .917 .628 .902 14.9
.656 .964 .797 .583 3.4
1 1 1 1 29.8
.492 .852 .604 .604 4.8
.578 .869 .670 .644 4.7
.436 .790 .550 .444 9.8
? .714
(b) Dependency path features removed.
binary n-way
maxf auc prec rec
.512 .808 .502 .486
.350 .759 .478 .212
.156 .710 .250 .020
.187 .739 .125 .020
.634 .885 .587 .706
.774 .939 .617 .906
.717 .967 .802 .601
? ? ? ?
.581 .885 .598 .634
.665 .906 .657 .468
.529 .847 .559 .469
? .587
(c) URL class removed.
Table 6: Binary and metaclassifier performance on all classes and all hand-labeled instances. Table (a) provides a benchmark
for 10-class classification over highly substitutable query phrases. Table (b) shows that a lot of our performance can be achieved
without computationally-expensive parsing.
binary meta data
class maxf auc prec rec %
no rel .758 .719 .660 .882 57.8
synonym .431 .901 .617 .199 2.4
hypernym .284 .803 .367 .061 1.8
hyponym .212 .804 .415 .056 1.6
coordinate .588 .713 .615 .369 35.5
other .206 .739 .375 .019 0.8
metaclassifier accuracy .648
Table 7: Binary and metaclassifier performance on WordNet-
labeled instances with all features.
binary meta data
class maxf auc prec rec %
no rel .525 .671 .485 .354 31.9
synonym .381 .671 .684 .125 13.0
hypernym .211 .605 0 0 6.2
hyponym .125 .501 0 0 6.2
coordinate .623 .628 .485 .844 42.6
metaclassifier accuracy .490
Table 8: Training on WordNet-labeled pairs and testing on
hand-labeled pairs. Classifiers trained on WordNet do not
generalize well.
6.4.2 Training on WordNet, Testing on
WordNet and Hand-Labeled Pairs
We took the five classes for which human and
WordNet definitions agreed (synonyms, coordi-
nates, hypernyms, hyponyms, and no relationship)
and trained classifiers on all WordNet-labeled in-
stances. We tested the classifiers on human-
labeled instances from just those five classes. Re-
sults are shown in Table 8. Performance was
not very good, reinforcing the idea that while our
features can distinguish between query segments,
they cannot distinguish between common words.
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
 0  500  1000  1500  2000  2500  3000  3500  4000  4500  5000
M
et
ac
la
ss
ifi
er
 a
cc
ur
ac
y
Number of query pairs
Figure 2: Meta-classifier accuracy as a function of number of
labeled instances for training.
7 Discussion
Almost all high-weighted features are either
HTML paths or query log features; these are the
ones that are easiest to obtain. Many of the
highest-weight HTML tree features are symmet-
ric, e.g. both words appear in cells of the same ta-
ble, or as items in the same list. Here we note a
selection of the more interesting predictors.
synonym ??X or Y? expressed as a dependency
path was a high-weight feature.
hyper/hyponym ??Y and other X? as a depen-
dency path has highest weight. An interesting
feature is X in a table cell and Y appearing in
text outside but nearby the table.
sibling ?many symmetric HTML features. ?X to
the Y? as in ?80s to the 90s?. ?X and Y?, ?X,
Y, and Z? highly-weighted minipar paths.
general/specialization ?the top three features
are substring containment, word subset dif-
ference count, and prefix overlap.
spelling change ?many negative features, indi-
55
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
isi
on
Recall
  F=0.531
  F=0.634
  F=0.354
F=0.172
  F=0.173
no relationship
sibling
synonym
hyponym
hypernym
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
isi
on
Recall
  F=0.777
  F=0.538
  F=0.702
  F=0.565
  F=0.661
spelling change
related in some other way
stemmed form
generalization
specification
Figure 1: Precision-recall curves for 10 binary classifiers on all hand-labeled instances with all features.
cating that two words that cooccur in a web
page are not likely to be spelling differences.
other ?many symmetric HTML features. Two
words emphasized in the same way (e.g. both
bolded) may indicate some relationship.
none ?many asymmetric HTML features, e.g.
one word in a blockquote, the other bolded
in a different paragraph. Dice coefficient is a
good negative features.
8 Conclusion
We have provided the first benchmark for n-
class semantic classification of highly substi-
tutable query phrases. There is much room for im-
provement, and we expect that this baseline will
be surpassed.
Acknowledgments
Thanks to Chris Manning and Omid Madani for
helpful comments, to Omid Madani for providing
the classification code, to Rion Snow for providing
the hypernym data, and to our labelers.
This work was supported in part by the CIIR
and in part by the Defense Advanced Research
Projects Agency (DARPA) under contract number
HR001-06-C-0023. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect those of the sponsor.
References
Peter G. Anick. 2003. Using terminological feedback for
web search refinement: a log-based study. In SIGIR 2003,
pages 88?95.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The pascal recognising textual entailment challenge. In
PASCAL Challenges Workshop on Recognising Textual
Entailment.
Ted E. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguistics,
19(1):61?74.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of Coling 1992,
pages 539?545.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In 15th
International World Wide Web Conference (WWW-2006),
Edinburgh.
Sathiya Keerthi and Dennis DeCoste. 2005. A modified fi-
nite newton method for fast solution of large scale linear
svms. Journal of Machine Learning Research, 6:341?361,
March.
Lillian Lee. 1999. Measures of distributional similarity. In
37th Annual Meeting of the Association for Computational
Linguistics, pages 25?32.
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Cybernetics
and Control Theory, 10(8):707?710. Original in Doklady
Akademii Nauk SSSR 163(4): 845?848 (1965).
Dekang Lin. 1998. Dependency-based evaluation of mini-
par. In Workshop on the Evaluation of Parsing Systems.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38(11):39?41.
J. Platt. 2000. Probabilistic outputs for support vector ma-
chines and comparison to regularized likelihood methods.
pages 61?74.
Soo Young Rieh and Hong Iris Xie. 2001. Patterns and se-
quences of multiple query reformulations in web search-
ing: A preliminary study. In Proceedings of the 64th An-
nual Meeting of the American Society for Information Sci-
ence and Technology Vol. 38, pages 246?255.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2005. Learn-
ing syntactic patterns for automatic hypernym discovery.
In Proceedings of the Nineteenth Annual Conference on
Neural Information Processing Systems (NIPS 2005).
Egidio Terra and Charles L. A. Clarke. 2004. Scoring miss-
ing terms in information retrieval tasks. In CIKM 2004,
pages 50?58.
P.D Turney, M.L. Littman, J. Bigham, and V. Shnayder, 2003.
Recent Advances in Natural Language Processing III: Se-
lected Papers from RANLP 2003, chapter Combining in-
dependent modules in lexical multiple-choice problems,
pages 101?110. John Benjamins.
56
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 148?154, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
MayoClinicNLP?CORE: Semantic representations for textual similarity
Stephen Wu
Mayo Clinic
Rochester, MN 55905
wu.stephen@mayo.edu
Dongqing Zhu & Ben Carterette
University of Delaware
Newark, DE 19716
{zhu,carteret}@cis.udel.edu
Hongfang Liu
Mayo Clinic
Rochester, MN 55905
liu.hongfang@mayo.edu
Abstract
The Semantic Textual Similarity (STS) task
examines semantic similarity at a sentence-
level. We explored three representations of
semantics (implicit or explicit): named enti-
ties, semantic vectors, and structured vectorial
semantics. From a DKPro baseline, we also
performed feature selection and used source-
specific linear regression models to combine
our features. Our systems placed 5th, 6th, and
8th among 90 submitted systems.
1 Introduction
The Semantic Textual Similarity (STS) task (Agirre
et al, 2012; Agirre et al, 2013) examines semantic
similarity at a sentence-level. While much work has
compared the semantics of terms, concepts, or doc-
uments, this space has been relatively unexplored.
The 2013 STS task provided sentence pairs and a
0?5 human rating of their similarity, with training
data from 5 sources and test data from 4 sources.
We sought to explore and evaluate the usefulness
of several semantic representations that have had
recent significance in research or practice. First,
information extraction (IE) methods often implic-
itly consider named entities as ad hoc semantic rep-
resentations, for example, in the clinical domain.
Therefore, we sought to evaluate similarity based on
named entity-based features. Second, in many appli-
cations, an effective means of incorporating distri-
butional semantics is Random Indexing (RI). Thus
we consider three different representations possi-
ble within Random Indexing (Kanerva et al, 2000;
Sahlgren, 2005). Finally, because compositional
distributional semantics is an important research
topic (Mitchell and Lapata, 2008; Erk and Pado?,
2008), we sought to evaluate a principled compo-
sition strategy: structured vectorial semantics (Wu
and Schuler, 2011).
The remainder of this paper proceeds as follows.
Section 2 overviews our similarity metrics, and Sec-
tion 3 overviews the systems that were defined on
these metrics. Competition results and additional
analyses are in Section 4. We end with discussion
on the results in Section 5.
2 Similarity measures
Because we expect semantic similarity to be multi-
layered, we expect that we will need many similar-
ity measures to approximate human similarity judg-
ments. Rather than reinvent the wheel, we have cho-
sen to introduce features that complement existing
successful feature sets. We utilized 17 features from
DKPro Similarity and 21 features from TakeLab,
i.e., the two top-performing systems in the 2012 STS
task, as a solid baseline.
These are summarized in Table 1. We introduce 3
categories of new similarity metrics, 9 metrics in all.
2.1 Named entity measures
Named entity recognition provides a common ap-
proximation of semantic content for the informa-
tion extraction perspective. We define three simple
similarity metrics based on named entities. First,
we computed the named entity overlap (exact string
matches) between the two sentences, where NEk
was the set of named entities found in sentence
Sk. This is the harmonic mean of how closely S1
148
Table 1: Full feature pool in MayoClinicNLP systems. The proposed MayoClinicNLP metrics are meant to comple-
ment DKPro (Ba?r et al, 2012) and TakeLab (?Saric? et al, 2012) metrics.
DKPro metrics (17) TakeLab metrics (21) Custom MayoClinicNLP metrics (9)
n-grams/WordNGramContainmentMeasure 1 stopword-filtered t ngram/UnigramOverlap
n-grams/WordNGramContainmentMeasure 2 stopword-filtered t ngram/BigramOverlap
n-grams/WordNGramJaccardMeasure 1 t ngram/TrigramOverlap
n-grams/WordNGramJaccardMeasure 2 stopword-filtered t ngram/ContentUnigramOverlap
n-grams/WordNGramJaccardMeasure 3 t ngram/ContentBigramOverlap
n-grams/WordNGramJaccardMeasure 4 t ngram/ContentTrigramOverlap
n-grams/WordNGramJaccardMeasure 4 stopword-filtered
t words/WeightedWordOverlap custom/StanfordNerMeasure overlap.txt
t words/GreedyLemmaAligningOverlap custom/StanfordNerMeasure aligngst.txt
t words/WordNetAugmentedWordOverlap custom/StanfordNerMeasure alignlcs.txt
esa/ESA Wiktionary t vec/LSAWordSimilarity NYT custom/SVSePhrSimilarityMeasure.txt
esa/ESA WordNet t vec/LSAWordSimilarity weighted NYT custom/SVSeTopSimilarityMeasure.txt
t vec/LSAWordSimilarity weighted Wiki custom/SemanticVectorsSimilarityMeasure d200 wr0.txt
custom/SemanticVectorsSimilarityMeasure d200 wr6b.txt
custom/SemanticVectorsSimilarityMeasure d200 wr6d.txt
custom/SemanticVectorsSimilarityMeasure d200 wr6p.txt
n-grams/CharacterNGramMeasure 2 t other/RelativeLengthDifference
n-grams/CharacterNGramMeasure 3 t other/RelativeInfoContentDifference
n-grams/CharacterNGramMeasure 4 t other/NumbersSize
string/GreedyStringTiling 3 t other/NumbersOverlap
string/LongestCommonSubsequenceComparator t other/NumbersSubset
string/LongestCommonSubsequenceNormComparator t other/SentenceSize
string/LongestCommonSubstringComparator t other/CaseMatches
t other/StocksSize
t other/StocksOverlap
matches S2, and how closely S2 matches S1:
simneo(S1, S2) = 2 ? ?NE1 ?NE2??NE1? + ?NE2? (1)
Additionally, we relax the constraint of requiring
exact string matches between the two sentences by
using the longest common subsequence (Allison and
Dix, 1986) and greedy string tiling (Wise, 1996) al-
gorithms. These metrics give similarities between
two strings, rather than two sets of strings as we
have with NE1 and NE2. Thus, we follow previ-
ous work in greedily aligning these named entities
(Lavie and Denkowski, 2009; ?Saric? et al, 2012) into
pairs. Namely, we compare each pair (nei,1, nej,2)
of named entity strings in NE1 and NE2. The
highest-scoring pair is entered into a set of pairs, P .
Then, the next highest pair is added to P if neither
named entity is already in P , and discarded other-
wise; this continues until there are no more named
entities in either NE1 or NE2.
We then define two named entity aligning mea-
sures that use the longest common subsequence
(LCS) and greedy string tiling (GST) fuzzy string
matching algorithms:
simnea(S1, S2) =
?
(ne1,ne2)?P
f(ne1, ne2)
max (?NE1?, ?NE2?)
(2)
where f(?) is either the LCS or GST algorithm.
In our experiments, we performed named entity
recognition with the Stanford NER tool using the
standard English model (Finkel et al, 2005). Also,
we used UKP?s existing implementation of LCS and
GST (?Saric? et al, 2012) for the latter two measures.
2.2 Random indexing measures
Random indexing (Kanerva et al, 2000; Sahlgren,
2005) is another distributional semantics framework
for representing terms as vectors. Similar to LSA
(Deerwester et al, 1990), an index is created that
represents each term as a semantic vector. But
in random indexing, each term is represented by
an elemental vector et with a small number of
randomly-generated non-zero components. The in-
tuition for this means of dimensionality reduction is
that these randomly-generated elemental vectors are
like quasi-orthogonal bases in a traditional geomet-
ric semantic space, rather than, e.g., 300 fully or-
thogonal dimensions from singular value decompo-
sition (Landauer and Dumais, 1997). For a standard
model with random indexing, a contextual term vec-
tor ct,std is the the sum of the elemental vectors cor-
responding to tokens in the document. All contexts
for a particular term are summed and normalized to
produce a final term vector vt,std.
Other notions of context can be incorporated into
149
this model. Local co-occurrence context can be ac-
counted for in a basic sliding-window model by con-
sidering words within some window radius r (in-
stead of a whole document). Each instance of the
term t will have a contextual vector ct,win = et?r +
? + et?1 + et+1 +? + et+r; context vectors for each
instance (in a large corpus) would again be added
and normalized to create the overall vector vt,win.
A directional model doubles the dimensionality of
the vector and considers left- and right-context sepa-
rately (half the indices for left-context, half for right-
context), using a permutation to achieve one of the
two contexts. A permutated positional model uses a
position-specific permutation function to encode the
relative word positions (rather than just left- or right-
context) separately. Again, vt would be summed
and normalized over all instances of ct.
Sentence vectors from any of these 4 Random
Indexing-based models (standard, windowed, direc-
tional, positional) are just the sum of the vectors for
each term vS = ?t?S vt. We define 4 separate simi-
larity metrics for STS as:
simRI(S1, S2) = cos(vS1,vS2) (3)
We used the semantic vectors package (Widdows
and Ferraro, 2008; Widdows and Cohen, 2010) in
the default configuration for the standard model. For
the windowed, directional, and positional models,
we used a 6-word window radius with 200 dimen-
sions and a seed length of 5. All models were
trained on the raw text of the Penn Treebank Wall
Street Journal corpus and a 100,075-article subset of
Wikipedia.
2.3 Semantic vectorial semantics measures
Structured vectorial semantics (SVS) composes dis-
tributional semantic representations in syntactic
context (Wu and Schuler, 2011). Similarity met-
rics defined with SVS inherently explore the quali-
ties of a fully interactive syntax?semantics interface.
While previous work evaluated the syntactic contri-
butions of this model, the STS task allows us to eval-
uate the phrase-level semantic validity of the model.
We summarize SVS here as bottom-up vector com-
position and parsing, then continue on to define the
associated similarity metrics.
Each token in a sentence is modeled generatively
as a vector e? of latent referents i? in syntactic con-
text c? ; each element in the vector is defined as:
e?[i?] = P(x? ? lci?), for preterm ? (4)
where l? is a constant for preterminals.
We write SVS vector composition between two
word (or phrase) vectors in linear algebra form,1 as-
suming that we are composing the semantics of two
children e? and e? in a binary syntactic tree into
their parent e? :
e? = M? (L??? ? e?)? (L??? ? e?) ? 1 (5)
M is a diagonal matrix that encapsulates probabilis-
tic syntactic information; the L matrices are linear
transformations that capture how semantically rele-
vant child vectors are to the resulting vector (e.g.,
L??? defines the the relevance of e? to e?). These
matrices are defined such that the resulting e? is a
semantic vector of consistent P(x? ? lci?) probabil-
ities. Further detail is in our previous work (Wu,
2010; Wu and Schuler, 2011).
Similarity metrics can be defined in the SVS
space by comparing the distributions of the com-
posed e? vectors ? i.e., our similarity metric is
a comparison of the vector semantics at different
phrasal nodes. We define two measures, one cor-
responding to the top node c? (e.g., with a syntactic
constituent c? = ?S?), and one corresponding to the
left and right largest child nodes (e.g.,, c? = ?NP?
and c ?= ?VP? for a canonical subject?verb?object
sentence in English).
simsvs-top(S1, S2) = cos(e?(S1),e?(S2)) (6)
simsvs-phr(S1, S2) =max(
avgsim(e?(S1),e?(S2);e ?(S1),e ?(S2)),
avgsim(e?(S1),e ?(S2);e ?(S1),e?(S2))) (7)
where avgsim() is the harmonic mean of the co-
sine similarities between the two pairs of arguments.
Top-level similarity comparisons in (6) amounts to
comparing the semantics of a whole sentence. The
phrasal similarity function simsvs-phr(S1, S2) in (7)
thus seeks to semantically align the two largest sub-
trees, and weight them. Compared to simsvs-top,
1We define the operator ? as point-by-point multiplication
of two diagonal matrices and 1 as a column vector of ones, col-
lapsing a diagonal matrix onto a column vector.
150
the phrasal similarity function simsvs-phr(S1, S2) as-
sumes there might be some information captured in
the child nodes that could be lost in the final compo-
sition to the top node.
In our experiments, we used the parser described
in Wu and Schuler (2011) with 1,000 headwords
and 10 relational clusters, trained on the Wall Street
Journal treebank.
3 Feature combination framework
The similarity metrics of Section 2 were calculated
for each of the sentence pairs in the training set, and
later the test set. In combining these metrics, we ex-
tended a DKPro Similarity baseline (3.1) with fea-
ture selection (3.2) and source-specific models and
classification (3.3).
3.1 Linear regression via DKPro Similarity
For our baseline (MayoClinicNLPr1wtCDT), we
used the UIMA-based DKPro Similarity system
from STS 2012 (Ba?r et al, 2012). Aside from the
large number of sound similarity measures, this pro-
vided linear regression through the WEKA package
(Hall et al, 2009) to combine all of the disparate
similarity metrics into a single one, and some pre-
processing. Regression weights were determined on
the whole training set for each source.
3.2 Feature selection
Not every feature was included in the final linear re-
gression models. To determine the best of the 47
(DKPro?17, TakeLab?21, MayoClinicNLP?9) fea-
tures, we performed a full forward-search on the
space of similarity measures. In forward-search, we
perform 10-fold cross-validation on the training set
for each measure, and pick the best one; in the next
round, that best metric is retained, and the remaining
metrics are considered for addition. Rounds con-
tinue until all the features are exhausted, though a
stopping-point is noted when performance no longer
increases.
3.3 Subdomain source models and
classification
There were 5 sources of data in the training set:
paraphrase sentence pairs (MSRpar), sentence pairs
from video descriptions (MSRvid), MT evaluation
sentence pairs (MTnews and MTeuroparl) and gloss
pairs (OnWN). In our submitted runs, we trained
a separate, feature-selected model based on cross-
validation for each of these data sources. In train-
ing data on cross-validation tests, training domain-
specific models outperformed training a single con-
glomerate model.
In the test data, there were 4 sources, with 2
appearing in training data (OnWN, SMT) and 2
that were novel (FrameNet/Wordnet sense defini-
tions (FNWN), European news headlines (head-
lines)). We examined two different strategies for ap-
plying the 5-source trained models on these 4 test
sets. Both of these strategies rely on a multiclass
random forest classifier, which we trained on the 47
similarity metrics.
First, for each sentence pair, we considered the
final similarity score to be a weighted combination
of the similarity score from each of the 5 source-
specific similarity models. The combination weights
were determined by utilizing the classifier?s confi-
dence scores. Second, the final similarity was cho-
sen as the single source-specific similarity score cor-
responding to the classifier?s output class.
4 Evaluation
The MayoClinicNLP team submitted three systems
to the STS-Core task. We also include here a post-
hoc run that was considered as a possible submis-
sion.
r1wtCDT This run used the 47 metrics from
DKPro, TakeLab, and MayoClinicNLP as a
feature pool for feature selection. Source-
specific similarity metrics were combined with
classifier-confidence-score weights.
r2CDT Same feature pool as run 1. Best-match (as
determined by classifier) source-specific simi-
larity metric was used rather than a weighted
combination.
r3wtCD TakeLab features were removed from the
feature pool (before feature selection). Same
source combination as run 1.
r4ALL Post-hoc run using all 47 metrics, but train-
ing a single linear regression model rather than
source-specific models.
151
Table 2: Performance comparison.
TEAM NAME headlines rank OnWNrank FNWNrank SMT rank mean rank
UMBC EBIQUITY-ParingWords 0.7642 0.7529 0.5818 0.3804 0.6181 1
UMBC EBIQUITY-galactus 0.7428 0.7053 0.5444 0.3705 0.5927 2
deft-baseline 0.6532 0.8431 0.5083 0.3265 0.5795 3
MayoClinicNLP-r4ALL 0.7275 0.7618 0.4359 0.3048 0.5707
UMBC EBIQUITY-saiyan 0.7838 0.5593 0.5815 0.3563 0.5683 4
MayoClinicNLP-r3wtCD 0.6440 43 0.8295 2 0.3202 47 0.3561 17 0.5671 5
MayoClinicNLP-r1wtCDT 0.6584 33 0.7775 4 0.3735 26 0.3605 13 0.5649 6
CLaC-RUN2 0.6921 0.7366 0.3793 0.3375 0.5587 7
MayoClinicNLP-r2CDT 0.6827 23 0.6612 20 0.396 17 0.3946 5 0.5572 8
NTNU-RUN1 0.7279 0.5952 0.3215 0.4015 0.5519 9
CLaC-RUN1 0.6774 0.7667 0.3793 0.3068 0.5511 10
4.1 Competition performance
Table 2 shows the top 10 runs of 90 submitted in
the STS-Core task are shown, with our three sys-
tems placing 5th, 6th, and 8th. Additionally, we can
see that run 4 would have placed 4th. Notice that
there are significant source-specific differences be-
tween the runs. For example, while run 4 is better
overall, runs 1?3 outperform it on all but the head-
lines and FNWN datasets, i.e., the test datasets that
were not present in the training data. Thus, it is
clear that the source-specific models are beneficial
when the training data is in-domain, but a combined
model is more beneficial when no such training data
is available.
4.2 Feature selection analysis
0 10 20 30 40
0.
60
0.
65
0.
70
0.
75
0.
80
0.
85
0.
90
Step
Pe
a
rs
o
n
?s
 C
or
re
la
tio
n 
Co
ef
fic
ie
nt
MSRpar
MSRvid
SMTeuroparl
OnWN
SMTnews
ALL
Figure 1: Performance curve of feature selection for
r1wtCDT, r2CDT, and r4ALL
Due to the source-specific variability among the
runs, it is important to know whether the forward-
search feature selection performed as expected. For
source specific models (runs 1 and 3) and a com-
bined model (run 4), Figure 1 shows the 10-fold
cross-validation scores on the training set as the next
feature is added to the model. As we would ex-
pect, there is an initial growth region where the first
features truly complement one another and improve
performance significantly. A plateau is reached for
each of the models, and some (e.g., SMTnews) even
decay if too many noisy features are added.
The feature selection curves are as expected. Be-
cause the plateau regions are large, feature selection
could be cut off at about 10 features, with gains in
efficiency and perhaps little effect on accuracy.
The resulting selected features for some of the
trained models are shown in Table 3.
4.3 Contribution of MayoClinicNLP metrics
We determined whether including MayoClinicNLP
features was any benefit over a feature-selected
DKPro baseline. Table 4 analyzes this question
by adding each of our measures in turn to a base-
line feature-selected DKPro (dkselected). Note that
this baseline was extremely effective; it would have
ranked 4th in the STS competition, outperforming
our run 4. Thus, metrics that improve this baseline
must truly be complementary metrics. Here, we see
that only the phrasal SVSmeasure is able to improve
performance overall, largely by its contributions to
the most difficult categories, FNWN and SMT. In
fact, that system (dkselected + SVSePhrSimilari-
tyMeasure) represents the best-performing run of
any that was produced in our framework.
152
Table 3: Top retained features for several linear regression models.
OnWN - r1wtCDT and r2CDT (15 shown/19 selected) SMTnews - r1wtCDT and r2CDT (15 shown/17 selected) All - r4ALL (29 shown/29 selected)
t ngram/ContentUnigramOverlap t other/RelativeInfoContentDifference t vec/LSAWordSimilarity weighted NYT
t other/RelativeInfoContentDifference n-grams/CharacterNGramMeasure 2 n-grams/CharacterNGramMeasure 2
t vec/LSAWordSimilarity weighted NYT t other/CaseMatches string/LongestCommonSubstringComparator
esa/ESA Wiktionary string/GreedyStringTiling 3 t other/NumbersOverlap
t ngram/ContentBigramOverlap custom/RandomIndexingMeasure d200 wr6p t words/WordNetAugmentedWordOverlap
n-grams/CharacterNGramMeasure 2 custom/StanfordNerMeasure overlap n-grams/WordNGramJaccardMeasure 1
t words/WordNetAugmentedWordOverlap t vec/LSAWordSimilarity weighted NYT n-grams/CharacterNGramMeasure 3
t ngram/BigramOverlap t other/SentenceSize t other/SentenceSize
string/GreedyStringTiling 3 custom/RandomIndexingMeasure d200 wr0 t other/RelativeInfoContentDifference
string/LongestCommonSubsequenceNormComparator custom/SVSePhrSimilarityMeasure t ngram/ContentBigramOverlap
custom/RandomIndexingMeasure d200 wr0 esa/ESA Wiktionary n-grams/WordNGramJaccardMeasure 4
custom/StanfordNerMeasure aligngst string/LongestCommonSubstringComparator t other/NumbersSize
custom/StanfordNerMeasure alignlcs t other/NumbersSize t other/NumbersSubset
custom/StanfordNerMeasure overlap n-grams/WordNGramContainmentMeasure 2 stopword-filtered custom/SVSePhrSimilarityMeasure
custom/SVSePhrSimilarityMeasure custom/SVSeTopSimilarityMeasure custom/SemanticVectorsSimilarityMeasure d200 wr6p
esa/ESA WordNet
OnWN - r3wtCD (7 shown/7 selected) SMTnews - r3wtCD (15 shown/23 selected) esa/ESA Wiktionary
esa/ESA Wiktionary string/GreedyStringTiling 3 string/LongestCommonSubsequenceComparator
string/LongestCommonSubsequenceComparator custom/StanfordNerMeasure overlap string/LongestCommonSubsequenceNormComparator
string/GreedyStringTiling 3 n-grams/CharacterNGramMeasure 2 n-grams/WordNGramContainmentMeasure 1 stopword-filtered
string/LongestCommonSubsequenceNormComparator custom/RandomIndexingMeasure d200 wr6p word-sim/MCS06 Resnik WordNet
string/LongestCommonSubstringComparator n-grams/CharacterNGramMeasure 3 t ngram/ContentUnigramOverlap
word-sim/MCS06 Resnik WordNet string/LongestCommonSubsequenceComparator n-grams/WordNGramContainmentMeasure 2 stopword-filtered
n-grams/WordNGramContainmentMeasure 2 stopword-filtered custom/StanfordNerMeasure aligngst n-grams/WordNGramJaccardMeasure 2 stopword-filtered
custom/SVSePhrSimilarityMeasure t ngram/UnigramOverlap
esa/ESA Wiktionary t ngram/BigramOverlap
esa/ESA WordNet t other/StocksSize
n-grams/WordNGramContainmentMeasure 2 stopword-filtered t words/GreedyLemmaAligningOverlap
n-grams/WordNGramJaccardMeasure 1 t other/StocksOverlap
string/LongestCommonSubstringComparator
custom/RandomIndexingMeasure d200 wr6d
custom/RandomIndexingMeasure d200 wr0
Table 4: Adding customized features one at a time into optimized DKPro feature set. Models are trained across all
sources.
headlines OnWN FNWN SMT mean
dkselected 0.70331 0.79752 0.38358 0.31744 0.571319
dkselected + SVSePhrSimilarityMeasure 0.70178 0.79644 0.38685 0.32332 0.572774
dkselected + RandomIndexingMeasure d200 wr0 0.70054 0.79752 0.38432 0.31615 0.570028
dkselected + SVSeTopSimilarityMeasure 0.69873 0.79522 0.38815 0.31723 0.569533
dkselected + RandomIndexingMeasure d200 wr6d 0.69944 0.79836 0.38416 0.31397 0.569131
dkselected + RandomIndexingMeasure d200 wr6b 0.69992 0.79788 0.38435 0.31328 0.568957
dkselected + RandomIndexingMeasure d200 wr6p 0.69878 0.79848 0.37876 0.31436 0.568617
dkselected + StanfordNerMeasure aligngst 0.69446 0.79502 0.38703 0.31497 0.567212
dkselected + StanfordNerMeasure overlap 0.69468 0.79509 0.38703 0.31466 0.567200
dkselected + StanfordNerMeasure alignlcs 0.69451 0.79486 0.38657 0.31394 0.566807
(dk + all custom) selected 0.70311 0.79887 0.37477 0.31665 0.570586
Also, we see some source-specific behavior. None
of our introduced measures are able to improve the
headlines similarities. However, random indexing
improves OnWN scores, several strategies improve
the FNWN metric, and simsvs-phr is the only viable
performance improvement on the SMT corpus.
5 Discussion
Mayo Clinic?s submissions to Semantic Textual
Similarity 2013 performed well, placing 5th, 6th,
and 8th among 90 submitted systems. We intro-
duced similarity metrics that used different means
to do compositional distributional semantics along
with some named entity-based measures, finding
some improvement especially for phrasal similar-
ity from structured vectorial semantics. Through-
out, we utilized forward-search feature selection,
which enhanced the performance of the models. We
also used source-based linear regression models and
considered unseen sources as mixtures of existing
sources; we found that in-domain data is neces-
sary for smaller, source-based models to outperform
larger, conglomerate models.
Acknowledgments
Thanks to the developers of the UKP DKPro sys-
tem and the TakeLab system for making their code
available. Also, thanks to James Masanz for initial
implementations of some similarity measures.
153
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, pages 385?393. Association for Computational
Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Lloyd Allison and Trevor I Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23(5):305?310.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics-Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, pages 435?
440. Association for Computational Linguistics.
Scott Deerwester, Susan Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP 2008.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363?370.
Association for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Pentti Kanerva, Jan Kristofersson, and Anders Holst.
2000. Random indexing of text samples for latent se-
mantic analysis. In Proceedings of the 22nd annual
conference of the cognitive science society, volume
1036. Citeseer.
T.K. Landauer and S.T. Dumais. 1997. A Solution to
Plato?s Problem: The Latent Semantic Analysis The-
ory of Acquisition, Induction, and Representation of
Knowledge. Psychological Review, 104:211?240.
Alon Lavie and Michael J Denkowski. 2009. The meteor
metric for automatic evaluation of machine translation.
Machine translation, 23(2-3):105?115.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, OH.
M. Sahlgren. 2005. An introduction to random index-
ing. In Methods and Applications of Semantic Index-
ing Workshop at the 7th International Conference on
Terminology and Knowledge Engineering, TKE, vol-
ume 5.
Frane ?Saric?, Goran Glavas?, Mladen Karan, Jan ?Snajder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Dominic Widdows and Trevor Cohen. 2010. The seman-
tic vectors package: New algorithms and public tools
for distributional semantics. In Semantic Computing
(ICSC), 2010 IEEE Fourth International Conference
on, pages 9?15. IEEE.
D. Widdows and K. Ferraro. 2008. Semantic vec-
tors: a scalable open source package and online tech-
nology management application. Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08), pages 1183?1190.
Michael J Wise. 1996. Yap3: Improved detection of sim-
ilarities in computer program and other texts. In ACM
SIGCSE Bulletin, volume 28, pages 130?134. ACM.
StephenWu andWilliam Schuler. 2011. Structured com-
position of semantic vectors. In Proceedings of the In-
ternational Conference on Computational Semantics.
Stephen Tze-Inn Wu. 2010. Vectorial Representations
of Meaning for a Computational Model of Language
Comprehension. Ph.D. thesis, Department of Com-
puter Science and Engineering, University of Min-
nesota.
154
