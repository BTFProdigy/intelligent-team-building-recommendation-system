Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 31?38,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
 1
Modifying a Natural Language Processing System for  
European Languages to Treat Arabic in Information Processing  
and Information Retrieval Applications 
 
 
Gregory Grefenstette, Nasredine Semmar, Fa?za Elkateb-Gara 
Multilingual Multimedia Knowledge Engineering Laboratory (LIC2M)  
Commissariat ? l?Energie Atomique, Laboratoire d?Int?gration des Syst?mes et des Technologies 
(CEA LIST) 
B.P. 6, 92265 Fontenay-aux-Roses Cedex, France  
{gregory.grefenstette,nasredine.semmar,faiza.gara}@cea.fr  
 
 
 
 
Abstract 
The goal of many natural language proc-
essing platforms is to be able to someday 
correctly treat all languages. Each new 
language, especially one from a new lan-
guage family, provokes some modifica-
tion and design changes. Here we present 
the changes that we had to introduce into 
our platform designed for European lan-
guages in order to handle a Semitic lan-
guage. Treatment of Arabic was 
successfully integrated into our cross lan-
guage information retrieval system, which 
is visible online. 
1 Introduction 
When a natural language processing (NLP) system 
is created in a modular fashion, it can be relatively 
easy to extend treatment to new languages (May-
nard, et al 2003) depending on the depth and 
completeness desired. We present here lessons 
learned from the extension of our NLP system that 
was originally implemented for Romance and 
Germanic European1 languages to a member of the 
Semitic language family, Arabic. Though our sys-
tem was designed modularly, this new language 
posed new problems. We present our answers to 
                                                          
1
 European languages from non indo-European families 
(Basque, Finnish and Hungarian) pose some of the same prob-
lems that Arabic does. 
these problems encountered in the creation of an 
Arabic processing system, and illustrate its integra-
tion into an online cross language information re-
trieval (CLIR) system dealing with documents 
written in Arabic, English French and Spanish. 
 
2 The LIMA natural language processor 
Our NLP system (Besan?on et al, 2003), called 
LIMA2, was built using a traditional architecture 
involving separate modules for  
1. Morphological analysis:  
a. Tokenization (separating the input 
stream into a graph of words). 
b. Simple word lookup (search for 
words in a full form lexicon). 
c. Orthographical alternative lookup 
(looking for differently accented 
forms, alternative hyphenisation, 
concatenated words, abbreviation 
recognition), which might alter the 
original non-cyclic word graph by 
adding alternative paths. 
d. Idiomatic expressions recognizer 
(detecting and considering them as 
single words in the word graph). 
e. Unknown word analysis. 
2. Part-of-Speech and Syntactic analysis: 
a. After the morphological analysis, 
which has augmented the original 
graph with as many nodes as there 
                                                          
2
 LIMA stands for the LIC2M Multilingual Analyzer. 
31
 2
are interpretations for the tokens, 
part-of-speech analysis using lan-
guage models from a hand-tagged 
corpus reduces the number of pos-
sible readings of the input. 
b. Named entity recognizer. 
c. Recognition of nominal and verbal 
chains in the graph. 
d. Dependency relation extraction. 
3. Information retrieval application: 
a. Subgraph indexing. 
b. Query reformulation (monolingual 
reformulation for paraphrases and 
synonymy; multilingual for cross 
language information retrieval). 
c. Retrieval scoring comparing par-
tial matches on subgraphs and en-
tities. 
 
Our LIMA NLP system (Besan?on et al, 2003) 
was first implemented for English, French, German 
and Spanish, with all data coded in UTF8.  When 
we extended the system to Arabic, we found that a 
number of modifications had to be introduced. We 
detail these modifications in the next sections. 
3 Changes specific to Semitic languages 
Two new problems posed by Arabic (and common 
to most Semitic languages) that forced us to alter 
our NLP system are the problem of incomplete 
vowelization of printed texts3 and the problem of 
agglutinative clitics. We discuss how these new 
problems influenced our lexical resources and lan-
guage processing steps.  
Lexical Resources 
The first task for introducing a new language is to 
create the lexical resources for this language. Since 
Arabic presents agglutination of articles, preposi-
tions and conjunctions at the beginning of words as 
well as pronouns at the end of words, and these 
phenomena were not treated in our existing Euro-
                                                          
3
 Since the headwords of our monolingual and cross-lingual 
reference dictionaries for Arabic possess voweled entries, we 
hope to attain greater precision by treating this problem. An 
alternative but noisy approach (Larkey et al 2002) is to reduce 
to unvoweled text throughout the NLP application. 
pean languages4, we had to decide how this feature 
would be handled in the lexicon. Solutions to this 
problem have been proposed, ranging from genera-
tion and storage of all agglutinated words forms 
(Debili and Zouari, 1985) to the compilation of 
valid sequences of proclitics, words and enclitics 
into finite-state machines (Beesley, 1996). Our 
system had already addressed the problem of com-
pounds for German in the following way: if an in-
put word is not present in the dictionary, a 
compound-searching module returns all complete 
sequences of dictionary words (a list of possible 
compound joining "fogemorphemes" is passed to 
this module) as valid decompositions of the input 
word. Though theoretically this method could be 
used to treat Arabic clitics, we decided against us-
ing this existing process for two reasons: 
1. Contrary to German, in which any noun 
may theoretically be the first element of 
a compound, Arabic clitics belong to a 
small closed set of articles, conjunc-
tions, prepositions and pronouns. Al-
lowing any word to appear at the 
beginning or end of an agglutinated 
word would generate unnecessary noise. 
2. Storing all words with all possible cli-
tics would multiply the size of lexicon 
proportionally to the number of legal 
possible combinations. We decided that 
this would take up too much space, 
though others have adopted this ap-
proach as mentioned above. 
 
We decided to create three lexicons: two additional 
(small) lists of proclitic and enclitic combinations, 
and one large lexicon of full form5 voweled words 
(with no clitics), the creation of the large lexicon 
from a set of lemmas using classic conjugation 
rules did not require any modification of the exist-
ing dictionary building and compilation compo-
nent. Since our NLP system already possessed a 
mechanism for mapping unaccented words to ac-
cented entries, and we decided to use this existing 
                                                          
4
 Spanish, of course, possesses enclitic pronouns for some 
verb forms but these were not adequately treated until the 
solution for Arabic was implemented in our system.  
5
 Our dictionary making process generates all full form ver-
sions of non compound and unagglutinated words. These are e 
then compiled into a finite-state automaton. Every node corre-
sponding to a full word is flagged, and an index corresponding 
to the automaton path points to the lexical data for that word. 
32
 3
mechanism for later matching of voweled and un-
voweled versions of Arabic words in applications. 
Thus the only changes for lexical resources involve 
adding two small clitic lexicons. 
Processing Steps: Morphological analysis 
Going back to the NLP processing steps listed in 
section 2, we now discuss new processing changes 
needed for treating Arabic. Tokenization (1a) and 
simple word lookup (2a) of the tokenized strings in 
the dictionary were unchanged as LIMA was 
coded for UTF8. If the word was not found, an 
existing orthographical alternative lookup (1c) was 
also used without change (except for the addition 
of the language specific correspondence table be-
tween accented and unaccented characters) in order 
to find lexical entries for unvoweled or partially 
voweled words. Using this existing mechanism for 
treating the vowelization problem does not allow 
us to exploit partial vowelization as we explain in a 
later section. 
 
At this point in the processing, a word that contains 
clitics will not have been found in the dictionary 
since we had decided not to include word forms 
including clitics. We introduced, here, a new proc-
essing step for Arabic: a clitic stemmer. This 
stemmer uses the following linguistic resources: 
? The full form dictionary, containing for 
each word form its possible part-of-speech 
tags and linguistic features (gender, num-
ber, etc.). We currently have 5.4 million 
entries in this dictionary6. 
? The proclitic dictionary and the enclitic 
dictionary, having the same structure of 
the full form dictionary with voweled and 
unvoweled versions of each valid combi-
nation of clitics. There are 77 and 65 en-
tries respectively in each dictionary. 
 
The clitic stemmer proceeds as follows on tokens 
unrecognized after step 1c: 
? Several vowel form normalizations are 
performed (?  ?  ?  ?  ?  ?  are removed,  ?  ?  	  
are replaced by  ?  and  final  ?  ?  ?  or ? 
are replaced by  ?  ??  ??  or  ). 
                                                          
6
 If we generated all forms including appended clitics, we 
would generate an estimated 60 billion forms (Attia, 1999). 
? All clitic possibilities are computed by us-
ing proclitics and enclitics dictionaries. 
? A radical, computed by removing these 
clitics, is checked against the full form 
lexicon. If it does not exist in the full form 
lexicon, re-write rules (such as those de-
scribed in Darwish (2002)) are applied, 
and the altered form is checked against the 
full form dictionary. For example, consider 
the token  ???? and the included clitics (?, 
?), the computed radical ?? does not exist 
in the full form lexicon but after applying 
one of the dozen re-write rules, the modi-
fied radical ?? is found the dictionary and 
the input token is segmented into root and 
clitics as:  ? + ?? + ? = ???? . 
? The compatibility of the morpho-syntactic 
tags of the three components (proclitic, 
radical, enclitic) is then checked. Only 
valid segmentations are kept and added 
into the word graph. Table 1 gives some 
examples of segmentations7 of words in 
the sentence  ?