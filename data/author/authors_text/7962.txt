An Empirical Investigation of the Relation Between Discourse Structure and 
Co-Reference 
Dan Cristea 
Department of Computer Science 
University "A.I. Cuza" 
Ia~i, Romania 
dcristea @ infoiasi, m 
Daniel Marcu 
In fo rmat ion  Sc iences  Inst itute and 
Depar tment  of  Computer  Sc ience 
Univers i ty  of  Southern Cal i forn ia  
Los  Angeles ,  CA, USA 
ma twig @ isi. edu 
Nancy Ide 
Department ofComputer Science 
Vassar College 
Poughkeepsie, NY, USA 
ide @ cs. vassal: edu 
Valentin Tablan* 
Department of Computer Science 
University of Sheffield 
United Kingdom 
v. tablan @ sheJ.'field, ac. uk 
Abstract 
We compare the potential of two classes el' linear and hi- 
erarchical models of discourse to determine co-reference 
links and resolve anaphors. The comparison uses a co l  
pus of thirty texts, which were manually annotated for 
co-reference and discourse structure. 
1 Introduction 
Most current anaphora resolution systems implelnent a
pipeline architecture with three modules (Lappin and Le- 
ass, 1994; Mitkov, 1997; Kameyama, 1997). 
1. A COLLF.CT module determines a list of potential 
antecedents (LPA) for each anaphor (l~ronourl, deli- 
nile noun, proper name, etc.) that have the potential 
to resolve it. 
2. A FILTI~,P, module eliminates referees incompatible 
with the anaphor fi'om the LPA. 
3. A PP, EFERENCE module determines the most likely 
antecedent on the basis of an ordering policy. 
In most cases, the COLLECT module determines an LPA 
by enumerating all antecedents in a window o1' text that 
precedes the anaphor under scrutiny (Hobbs, 1978; Lap- 
pin and Leass, 1994; Mitkov, 1997; Kameyama, 1997; 
Ge et al, 1998). This window can be as small as two 
or three sentences or as large as the entire preceding 
text. The FILTEP, module usually imposes emantic on- 
straints by requiring that the anaphor and potential an- 
tecedents have the same number and gendm; that selec- 
tional restrictions are obeyed, etc. The PREFERENCE 
module imposes preferences on potential antecedents 
on the basis of their grammatical roles, parallelism, 
fi'equency, proximity, etc. In some cases, anaphora 
resolution systems implement hese modules explic- 
itly (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 
* On leave fi'om lhe Faculty of Computer Science, University "AI. I. 
Cuza" of lasi. 
1997; Kameyama, 1997). In other cases, these modules 
are integrated by means of statistical (Ge et al, 1998) or 
uncertainty reasoning teclmiques (Mitkov, 1997). 
The fact that current anaphora resolution systems rely 
exclusively on the linear nature of texts in order to de- 
termine the LPA of an anaphor seems odd, given flint 
several studies have claimed that there is a strong rela- 
tion between discourse structure and reference (Sidner, 
1981 ; Grosz and Sidner, 1986; Grosz et al, 1995; Fox, 
1987; Vonk ct al., 1992; Azzam el al., 1998; Hitzcman 
and Pocsio, 1998). These studies claim, on the one hand, 
that the use of referents in naturally occurring texts im- 
poses constraints on the interpretation f discourse; and, 
on the other, that the structure of discourse constrains the 
LPAs to which anaphors can be resolved. The oddness 
of the situation can be explained by lho fac! that both 
groups seem primafilcie m be right. Empirical exper- 
iments studies that employ linear techniques for deter- 
mining the LPAs o1' almphol's report recall and precision 
anaphora resolution results in the range of 80% (Lappin 
and Leass, 1994; Ge ct al., 1998). Empirical experiments 
that investigated the relation between discourse structure 
and reference also claim that by exploiting the structure 
of discourse one has the potential of determining correct 
co-referential links for more than 80% of the referential 
expressions (Fox, 1987; Cristca et al, 1998) although to 
date, no discourse-based anaphora resolution system has 
been implemented. Since no direct comparison of these 
two classes of approaches has been made, it is difficult to 
determine which group is right, and what method is the 
best. 
In this paper, we attempt o Iill this gap by empiri- 
cally comparing the potential of linear and hierarchical 
models el' discourse to correctly establish co-referential 
links in texts, and hence, their potential to correctly re- 
solve anaphors. Since it is likely that both linear- and 
discourse-based anaphora resolution systems can imple- 
ment similar FILTER and PI~,I~FERENCE strategies, we fo- 
cus here only on the strategies that can be used to COL- 
208 
LECT lists of potential antecedents. Specilically, we fo- 
ct, s on deterlnining whether discourse llteories can help 
an anaphora resolution system detemfine LPAs that are 
"better" than the LPAs that can be contputed from a lin- 
ear interpretation f texts. Section 2 outlines the theoreti- 
cal assumptions of otu" empirical investigation. Section 3 
describes our experiment. We conclude with a discussion 
of tile results. 
2 Background 
2.1 Assumptions 
Our approach is based on the following assmnptions: 
I. For each anaphor in a text, an anaphora resolution 
system must produce an LPA that contains a refer- 
ent to which 111e anaphor can be resolvcd. The size 
of this LPA wuies fronl system to system, depend- 
ing on tile theory a system implements. 
2. The smaller the I,PA (while retaining a correct an- 
tecedent), the less likely that errors ill the \]7tI,TH{ 
:(lid PI',V,I;F, RI;,NCI ~, modules wil l  affect the ability of 
a system to select the appropriate referent. 
3. Theory A is better than lheory B for the task of rel- 
erence resolution if theory A produces I J 'As that 
contain more antecedents o which amtphors can be 
corrcclly resolved than theory B, and if the l,l~As 
produced by theory A arc smaller than those pro- 
duccd by theory B. l;or cxaml)lc, if for a given 
anaphor, theory A produces an I,PA thai contains a 
referee to which the anaphor can be resolved, while 
theory B produces an IJ~A that does not contain 
such a re\[eree, theory A is better than theory B. 
Moreover, if for a given anaphor, theory A produces 
an Lt)A wilh two referees and theory B produces an 
LPA with seven rel'crees (each LPA containing a ref- 
eree to which tile anal)her can be resolved), lheory 
A is considered better than theory 11 because it has a 
higher probability of solving that anaphor correctly. 
We consider two classes of models for determining the 
LPAs of anaphors ill a text: 
Linear-k models. This is at class of linear models in 
which the LPAs include all the references foulad in the 
discourse unit under scrutiny and the k discourse units 
that immediately precede it. Linear-0 models an ap- 
proach that assumes that :tll anaphors can be resolved 
intra-unit; Linear- 1 models an approach that cor,'esponds 
roughly to centering (Grosz et al, 1995). Linear-k is con- 
sistent with the asslunl)tions that underlie most current 
anaphora resohltion systems, which look back h units in 
order to resolve an anaphor. 
l) iscourse-V1:k models. In |his class ()1'models, LPAs 
include all lhe refcrentM expressions fotmd in the dis- 
course unit under scrutiny and the k discourse units that 
hierarchically precede it. The units that hierarchically 
precede a given unit are determined according to Veins 
Theory (VT) (Cristea et al, 1998), which is described 
brielly below. 
2.2 Veins Theory 
VT extends and formalizes the relation between dis- 
course  s t ruc ture  and reference proposed by Fox (1987). 
It identilies "veins", i.e., chains of elementary discourse 
units, over discourse structure trees that are built accord- 
ing to the requirements put forth in Rhetorical Structure 
Theo,y (RST) (Mann and Thompson, 1988). 
One of the conjectures ()1' VT is that the vein expres- 
sion of an elementary discourse unit provides a coher- 
ent "abstract" of the discourse fi'agmcnt hat contains 
that unit. As an internally coherent discottrse fragment, 
most ()1' the anaphors and referentM expressions (REst 
in a unit must be resolved to referees that oceul" in the 
text subs:used by the units in tile vein. This conjec- 
ture is consistent with Fox's view (1987) that the units 
that contain referees to which anaphors can be resolved 
are determined by the nuclearity of the discourse units 
thal precede the anaphors and the overall structure of dis- 
course. According to V'I; REs of both satellites and nu- 
clei can access referees of hierarchically preceding nt,- 
cleus nodes. REs of nuclei can mainly access referees of 
preceding nuclei nodes and of directly subordinated, pre- 
ceding satellite nodes. And the interposition ()1' a nucleus 
after a satellite blocks tim accessibility of the satellite for 
all nodes that att'e lovcer in the corresponding discourse 
structure (see (Cristea et al, 1998) for a full delinition). 
Hence, the fundamental intuition unde,lying VT is 
that the RST-spceilie distinction between nuclei and 
satellites constrains the range of referents to which 
anaphors can 19e resolved; in other words, the nucleus- 
satellite distinction induces for each anaphor (and each 
referential expression) a Do,naita of Refcrenlial Acces- 
sibility (DRA). For each anaphor a in a discourse unit 
~z, VT hypothesizes that a can be resolved by examin- 
ing referential expressions that were used in a subset o1' 
the discourse units that precede it; this subset is called 
the DRA of u. For any elcntentary unit u in a text, the 
corresponding DRA is computed autonmtically from the 
rhetorical representation f that text in two steps: 
1. lteads for each node are computed bottom-up over 
the rhetorical representation tree. Heads ()1" elemen- 
tary discottrse traits are the traits themselves. Heads 
of internal nodes, i.e., discourse spans, are con> 
pt, ted by taking the union of the heads of the im- 
mediate child nodes that :ire nuclei. For example, 
for the text in Figure I, whose rhetorical structure is 
shown in Figure 2, the head ()1' span 115,711 is unit 5 
because the head ()t' the inmmdiate nucleus, the ele- 
mentary unit 5, is 5. However, the head of span 116,7\] 
is the list (6,7) because both immediate children are 
nuclei of a multinuclear relation. 
2. Using the results of step 1, Vein expressions are 
eOmlmted top-down lbr each node in the tree. The 
vein of the root is its head. Veins of child nodes 
209 
i. \[Michael D. Casey,\[a top Johnson&Johnson 
...... get, moved teCGe~ Therapy In~,  
a small biotechnology concern here, 
2. to become_~_t>president and ch ieC  
operating officer. \[ 
3. \[Mr. Casey, 46 years old,\] was\[ president of 
J&J's McNeil Pharmaceutical subsidiary,\] 
*t. which was merged with another J&J unit, 
Ortho Pharmaceutical Corp., this year in 
a cost-cutting move. 
5. I,Ir. Casev\[ succeeds M. James Barrett, 50, 
as\[president of ~,~netic Ther-ap~. 
6. Mr. Barrett remains chief executive officer 
7. and becomes chainaan. 
8. \[Mr-\] r. Casey\] said 
9. ~made the move te {\]{e smaller compan~ 
i0. because~saw health care moving toward 
technologies like 
products. 
ll. Ube l ieve  that the field is emerging and i~ 
prepared to break loose, 
12.\[he\[said. 
Figure 1: An example of text and its elementary units. 
The |'eferential expressions surrounded by boxes and el- 
lipses correspond to two distinct co-referential equiv- 
alence classes. Referential expressions urrounded by 
boxes refer to Mr: Casey; those surrounded by ellipses 
refer to Genetic Thercq~y Inc. 
are computed recursively according to tile rules de- 
scribed by Cristea et al(1998). The DRA ot" a unit u 
is given by the units that precede u in 1t~e vein. 
For example, for the text and RST tree in Figures 1 
and 2, the vein expression of unit 3, which contains 
units 1 and 3, suggests that anaphors from unit 3 
should be resolved only to referential expressions 
in units 1 and 3. Because unit 2 is a satellite to 
unit 1, it is considered to be "blocked" to referen- 
tial links fi'om trait 3. In contrast, tile DRA of unit 
9, consisting o1' units 1, 8, and 9, reflects the intu- 
ition that anaphors l?om unit 9 can be resolved only 
to referential ext)ressions fi'om unit 1, which is the 
most important trait in span \[1,7\], and to unit 8, a 
satellite that immediately precedes unit 9. Figure 2 
shows the heads and veins of all internal nodes in 
the rhetorical representation. 
2.3 Comparing models 
The premise underlying out" experiment is that there are 
potentially significant differences in the size of the search 
space rcquired to resolve referential cxpressions when 
using Linear models vs. Discou|'se-VT models. For ex- 
ample, for text and tile RST tree in Figures 1 and 2, the 
Discourse-VT model narrows tlle search space required 
to resolve the a|mphor the smaller company in unit 9. 
According to VT, we look lbr potential antecede|Us for 
the smaller company in the DRA of unit 9, which lists 
units I, 8, and 9. The antecedent Genetic Therapy Inc. 
appears in unit 1 ; therefore, using VT we search back 2 
units (units 8 and 1) to lind a correct antecedent. In con- 
trast, to resolve the same reference using a linear model, 
four units (units 8, 7, 6, and 5) must he examined be- 
fore Genetic The;zq?y is found. Assuming that referen- 
tial links are established as tile text is processed, Genetic 
Therapy would be linked back to pronottn its in unit 2, 
which would in turn be linked to the first occurrence of 
the antecedent,Genetic Therapy Inc., in unit 1, tile an- 
tecedent determined irectly by using V'E 
In general, when hierarchical adjacency is considered, 
an anaphor may be resolved to a referent hat is not the 
closest in a linear interpretation of a text. Simihu'ly, aref- 
erential expression can be linked to a referee that is not 
the closest in a linear interpretation of a text. However, 
this does not create problems because we are focusing 
here only on co-referential relations of identity (see sec- 
tion 3). Since these relations induce equivalence classes 
over tile set of referential expressions in a text, it is suf\[i- 
cient that an anaphor or referential expression is resolved 
to any of the members of the relevant equiw|lence class. 
For example, according to V'I, the referential expression 
MI: Casey in unit 5 in Figm'e 1 can be linked directly 
only to the referee Mr Casey in unit 1, because the DRA 
o1' unit 5 is { 1,5}. By considering the co-referential links 
of the REs in the other units, tile full equivalence class 
can be determined. This is consistent with tile distinction 
between "direct" and "indirect" references discussed by 
Cristea, et al(1998). 
3 The Experiment 
3.1 Materials 
We used thirty newspaper texts whose lengths varied 
widely; the mean o- is 408 words and tile standard e- 
viation/t is 376. Tile texts were annotated manually for 
co-reference r lations of identity (Hirschman and Chin- 
chef, 1997). Tile co-reference relations define equiv- 
alence classes oil the set of all marked referents in a 
text. Tile texts were also manually annotated by Marcu 
et al (1999) with disconrse structures built in the style 
of Mann and Thompson (1988). Each discourse analy- 
sis yielded an average of 52 elementary discourse units. 
See (Hirschman and Chinchor, 1997) and (Marcu et al, 
1999) for details of tile annotation processes. 
210 
H = 1 9 * 
V=lg*  
t I= l  
"?r ... = 1 9 * . . . . . . .  
H=I  L -  - - -  V=lg*  _ . . . . . . . . . .  
H= i |  ~{:-.,, - 
V 1 9 :+:~f- . . . . .  ~'}'-~%l. 3 5 9 * 
1 2 3 4 
H=3 
V=1359 
DF~,= 1 3 
___ - - - - -  - - ~  
_ _ - - -  - - - ___  
I - _  I 
H=5 
- - - - - __  _ 
';,~ = 1 59* 
q _}L___= 6 7 
._~-"-"- \; =xt,,5 67 9 * 
5 //%,,\ 
6 7 
H=9 
"?" = 1 9 * 
I H=9 i . . . .  i la---- . . . . . . . . . . . .  
m 
V= 1 9"  ~ ~---- . . . .  -- i 21-25 
\ [ -  
13-213 
9 191011*  
\" = 1 (g)9  
DRA = 1 11 12 
Figure 2: The I),ST analysis of the text in ligure I. The trcc is rcprescnted using the conventions proposed by Mann 
and Thompson (1988). 
3.2 Compar ing  potent ia l  to es tab l i sh  co - re ferent ia l  
l inks  
3.2.1 Method  
The annotations for co-reference rchttions and rhetorical 
struclure trues for the thirty texts were fused, yielding 
representations that rcllect not  only tile discourse strut- 
lure, but also the co-reference quivalencc lasses spe- 
citic to each tcxl. Based on this information, we cval- 
ualed the potential of each of the two classes (51" mod- 
els discussed in secdon 2 (Linear-k and Discourse-VT-k) 
to correctly establish co-referential links as follows: For 
each model, each k, and each marked referential expres- 
sion o., we determined whether or not tlle corresponding 
LPA (delined over k elementary units) contained a ref- 
eree from the same equiwdence class. For example, for 
the Linear-2 model and referential expression lhe .vmaller 
company in t, nit 9, we estimated whether a co-refercntial 
link could be established between the smaller company 
and another referential expression in units 7, 8, or 9. 
For the Discourse-VT-2 model and the same referential 
expression, we estimated whether a co-referential link 
could bE established between the smaller company and 
another eferential expression in units 1, 8, or 9, which 
correspond to the DRA of unit 9. 
qb enable a fair comparison of the two models, when k 
is la,'ger than the size of the DRA of a given unit, WE ex- 
tend thatDRA using the closest units that precede the unit 
under scrutiny and are not ah'eady in the DRA. Hence, 
for the Linear-3 model and the referential expression the 
smaller conq~any in trait 9, we estimate whether a co- 
referential link can be established between the xmaller 
company and another eferential expression in units 9, 8, 
7, or 6. For tile Discourse-VT-3 model and tile same rcf- 
ermltial expression, we estimate whclher a co-referential 
link can be established between the smaller company and 
another eferential expression in units 9, 8, 1, or 7, which 
correspond I(5 the DRA of mill 9 (unfls 9, 8, and 1) and to 
unit 7, the closest unit preceding unit 9 that is not  ill ils 
I)RA. 
For the l)iscottrse-VT-k models, we assume Ihat the 
Fxtended DRA (EDRA) of size \]c of a unit ~t. (EDRAz: ('~)) 
is given by the lh'st 1 _< k units of a sequence that 
lists, in reverse order, the units of the DRA of '~z plus 
the /c - l units that precede tt but arc not in its DRA. 
For example, \['or the text in Figure 1, the follow- 
ing relations hold: EDRAc,(!)) = 9; EDRAI(C.)) = 
9, 8; EDRA.,(9) = .q,8, I; EI)RA3(9) := 9 ,8 ,1 ,7 ;  
I'~DRA.,I(!)) = !),8, 1,7,6. For Linear-k inodels, the 
EDRAz:(u) is given by u and the k units that immedi- 
ately precede ~t. 
The potential p(M,  a, EDRA,~) (5t' a model M to de- 
termine correct co-referential links with respect o a ref- 
Erential expression a in unit u, given a corresponding 
EDRA of size k (EDRAt.(u)), is assigned the value 1 if 
the EDRA contains a co-referent from the same equiwt- 
lence class as a. Otherwise, p(M, ,, EDRAt~) is assigned 
the value O. The potential p(k4, 6', k) of a model M 
to determine correct co-rEferential links for all referen- 
tial expressions in a corpus of texts C, using EDRAs 
of size k, is computed as the SUlll oF the potentials 
p(M, a.,EI)RA#) of  all referential expressions ct in C'. 
This potential is normalized to a wdue bEtweEn 0 and 
1 by dividing p(k/l, C, k) by the number ot' referential 
211 
expressions in the corpus that have an antecedent. 
By examining the potential of each model to correctly 
determine co-referential expressions for each k, it is pos- 
sible to determine the degree to which an implementa- 
tion of a given approach can contribute to the overall 
efficiency of anaphora resolution systems. That is, if a 
given model has the potential to correctly determine a
significant percentage of co-referential expressions with 
small DRAs, an anaphora resolution system implement- 
ing that model will have to consider fewer options over- 
all. Hence, the probability of error is reduced. 
3.2.2 Results 
The graph in Figure 3 shows the potentials of the Linear- 
k and Discourse-VT-k models to correctly determine co- 
referential links for each k from 1 to 20. The graph in 
Figure 4 represents he same potefftials but focuses only 
on ks in the interval \[2,9\]. As these two graphs how, the 
potentials increase monotonically with k, the VT-k mod- 
els always doing better than the Linear-k models. Even- 
tually, for large ks, the potential performance of the two 
models converges to 100%. 
The graphs in Figures 3 and 4 also suggest resolution 
strategies for implemented systems. For example, the 
graphs suggests that by choosing to work with EDRAs 
of size 7, a discourse-based system has the potential of 
resolving more than 90% of the co-referential links in 
a text correctly. To achieve the same potential, a linear- 
based system needs to look back 8 units. I fa system does 
not look back at all and attempts to resolve co-referential 
links only within the unit under scrutiny (k = 0), it has 
the potential to correctly resolve about 40% of the co- 
referential links. 
To provide a clearer idea of how the two models differ, 
Figure 5 shows, for each k, the value of the Discourse- 
VT-k potentials divided by the value of the Linear-k po- 
tentials. For k = 0, the potentials of both models are 
equal because both use only the unit in focus in order to 
determine co-referential links. For k = 1, the Discourse- 
VT-1 model is about 7% better than the Linear-! model. 
As the value of k increases, the value Discourse-VT- 
k/Linear-k converges to 1. 
In Figures 6 and 7, we display the number of excep- 
tions, i.e., co-referential links that Discourse-VT-k and 
Linear-k models cannot determine correctly. As one 
can see, over the whole corpus, for each k _< 3, the 
Discourse-VT-k models have the potential to determine 
correctly about 100 more co-referential links than the 
Linear-k models. As k increases, the performance of the 
two models converges. 
3,2,3 Statistical significance 
In order to assess the statistical significance of the differ- 
ence between the potentials of the two models to estab- 
lish correct co-referential links, we carried out a Paired- 
Samples T Test for each k. In general, a Paired-Samples 
T Test checks whether the mean of casewise differences 
between two variables differs from 0. For each text in 
I O0 00% 
__~____x.~-.,:..-~'.. . . . . . .  
a0~? oo~??'~ ?- .~:..- ...... 
70.00% 
60 00% ~' 
5000% . 
40O0% ? 
o 
EDRA s i ze  
- - - -  VT-k  . . . . . . .  tmeaf .k  
Figure 3: The potential of Linear-k and Discourse-VT- 
k models to determine correct co-referential links (0 < 
k < 20). 
9"5.00% 
90.00"/,, 
85.00% 
800O% 
75.(X~% 
70.007~ 
~ ~ . - ' " ' "  -"'"" --"" .... .. 
J 
1 2 3 4 5 6 7 9 
EDI1A Bt=~ 
? ---13---  VT-k - - . I f , , .  Linear-k 
Figure 4: The potential of Linear-k and Discourse-VT- 
k models to determine correct co-referential links (2 < 
k _< 9). 
the corpus and each k, we determined the potentials of 
both VT-k and Linear-k models to establish correct co- 
referential links in that text. For ks smaller than 4, the 
difference in potentials was statistically significant. For 
example, for k = 3, t = 3.345, df = 29, P = 0.002. For 
values of k larger than or equal to 4, the difference was no 
longer significant. These results are consistent with the 
graphs shown in Figure 3 to 7, which all show that the 
potentials of Discourse-VT-k and Linear-k models con- 
verges to the same value as the value of k increases. 
3.3 Comparing the effort required to establish 
co-referential links 
3.3.1 Method 
The method escribed in section 3.2.1 estimates the po- 
tential of Linear-k and Discourse-VT-k models to deter- 
mine correct co-referential links by treating EDRAs as 
sets. However, from a computational perspective (and 
212 
L'07 / Q 
t .a6  '" 
. . . .  \[ / ", 
t l ' . ,  u 
0.9~ 
o.97 I 
~,oo I "' 
700 c , " ? 
"', o 
'500 " - . -  "~-~ 
400 \ ' - - ,  "'*x ? "--- 12~:~: ...... 
2OO 
~00 
2 3 4 5 6 7 t3 10 
L 
Figure 5: A direct comparison of Discourse-VT-k 
and Linear-VT-k potentials to correctly determine co- 
referential links (0 < k < 20). 
Figure 7: The number of co-referential links that caunot 
be correctly determined by Discourse-VT-k and Linear-k 
models (1 < k < 10). 
14130 
1200 
0 lOOO 
600 
401) 
200 
\ 
- - -~u ,~:  ?~:  :g~: \[t..:~g<_ .~, ~ g :~. . . t~_~.~_~_ , .=~r ,~ ~ 
EORA ~lz~ 
Figure 6: The number of co-referential links that cannot 
be correctly determined by Discourse-VT-k and Linear-k 
models (0 < /~' < 20). 
presumably, from a psycholinguistic perspective as well) 
it also makes sense to compare the effort required by the 
two classes of models to establish correct co-referential 
links. We estimate this effort using a very simple metric 
that assumes that the closer an antecedent is to a cor- 
responding referential expression in the EDRA, the bet- 
ter. Hence, in estimating the effort to establish a co- 
referential link, we treat EDRAs as ordered lists. For ex- 
ample, using the Linear-9 model, to determine the correct 
antecedent of the referential expression the smaller com- 
pany in unit 9 of Figure 1, it is necessary to search back 
through 4 units (to unit 5, which contains the referent Ge- 
netic Therapy). Had unit 5 been Ml: Cassey succeeds M. 
James Barrett, 50, we would have had to go back 8 units 
(to unit 1) in order to correctly resolve the RE the smaller 
company. In contrast, in the Discourse-VT-9 model, we 
go back only 2 units because unit 1 is two units away 
fi'om unit 9 (EDRA:~ (9) = 9, 8, 1,7, 6, 5,4, 3, 2). 
We consider that the effort e(AJ, a, EDRAa.) of a 
model M to determine correct co-referential links with 
respect o one referential, in unit u, given a correspond- 
ing EDRA of size L" (EDRA~.(,)) is given by the number 
of units between u and the first unit in EDRAk(u) that 
contains a co.-referential expression of a. 
The effort e(M, C, k) of a model M to determine cor- 
rect co-referential links for all referential expressions in 
a corpus of texts C using EDRAs of size k was computed 
as the sum of the efforts e(M, a, EDRAk) of all referen- 
tia ! expressions a in C. 
3.3.2 Results 
Figure 8 shows the Discourse-VT-k and Linear-k efforts 
computed over all referential expressions in the corpus 
and all ks. It is possible, for a given referent a and a 
given k, that no co-referential link exists in the units of 
the corresponding EDRAa.. In this case, we consider that 
the effort is equal to k. As a consequence, for small ks 
the effort required to establish co-referential links is sim- 
ilar for both theories, because both can establish only a 
limited number of links. However, as k increases, the 
effort computed over the entire corpus diverges dramat- 
ically: using the Discourse-VT model, the search space 
for co-referential links is reduced by about 800 units for a 
corpus containing roughly 1200 referential expressions. 
3.3.3 Statistical significance 
A Paired-Samples T Test was performed for each k. For 
each text in the corpus and each k, we determined the 
effort of both VT-k and Linear& models to establish cor- 
rect co-referential links in that text. For all ks the dif- 
ference in effort was statistically significant. For exam- 
ple, for k = T, we obtained the values t = 3.5l, df = 
29, P = 0.001. These results are intuitive: because 
EDRAs are treated as ordered lists and not as sets, the 
effect of the discourse structure on establishing correct 
co-referential links is not diminished as/,' increases. 
4 Conclusion 
We analyzed empirically the potentials of discourse and 
linear models of text to determine co-referential links. 
Our analysis suggests that by exploiting the hierarchi- 
cal structure of texts, one can increase the potential 
213 
7000 . . . . . . . . . .  i - ; ; ;~ : ' : - ' ; ;  . . . .  ~ . . . . .  
~ 0 0 -  
k . . . . . . . . . . . . . . . . . . . . . . . . . .  
I000 
tORA ~ z Q  
- - V T ~ e s s  . . . . . . .  ~ o s s  
Figure 8: The effort required by Linear-k and Discourse- 
VT-k models to determine correct co-referential links 
(0< h< 100). 
q 
of natural anguage systems to correctly determine co- 
referential links, which is a requirement for correctly re- 
solving anaphors. If one treats all discourse units in the 
preceding discourse qually, the increase is statistically 
significant only when a discourse-based coreference sys- 
tem looks back at most four discourse units in order to 
establish co-referential links. However, if one assumes 
that proximity plays an important role in establishing co- 
referential links and that referential expressions are more 
likely to be linked to referees that were used recently in 
discourse, the increase is statistically significant no mat- 
ter how many units a discourse-based co-reference sys~ 
tern looks back in order to establish co-referential links. 
Acknowledgements. We are grateful to Lynette 
Hirschman and Nancy Chinchor for making available 
their corpora of co-reference annotations. We are also 
grateful to Graeme Hirst for comments and feedback on 
a previous draft of this paper. 
References 
Saliha Azzam, Kevin Humphreys, and Robert 
Gaizauskas. 1998. Evaluating a focus-based ap- 
proach to anaphora resolution. In Proceedings of 
the 361h Ammal Meeting of the Associatiot~ for 
Computational Linguistics and of the 17th Inter- 
national Conference on Computational Linguistics 
(COLlNG/ACL'98), pages 74-78, Montreal, Canada, 
August 10-14. 
Dan Cristea, Nancy Ide, and Laurent Romary. 1998. 
Veins theory: A model of global discourse cohesion 
and coherence. In Proceedings of the 36th Ammal 
Meeting of the Association Jot" Computational Lin- 
guistics attd of the 17th lntertmtional Conference on 
Computational Linguistics (COLING/ACL'98), pages 
281-285, Montreal, Canada, August. 
Barbara Fox. 1987. Discourse Structure and Anaphora. 
Cambridge Studies in Linguistics; 48. Cambridge Uni- 
versity Press. 
Niyu Oe, John Hale, and Eugene Charniak. 1998. A sta- 
tistical approach to anaphora resolution. In Proceed- 
ings of the Sixth Worksho t) on Vet 3, Large Corpora, 
pages 161-170, Montreal, Canada, August 15-16. 
Barbara J. Grosz and Candace L. Sidner. 1986. At- 
tention, intentions, and the structure of discourse. 
Computational Linguistics, 12(3): 175-204, July- 
September. 
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 
1995. Centering: A framework tbr modeling the lo- 
cal coherence of discourse. Computational Linguis- 
tics, 21 (2):203-226, June. 
Lynette Hirschman and Nancy Chinchor, 1997. MUC-7 
Coreference Task Definition, July 13. 
Janet Hitzeman and Massimo Poesio. 1998. Long dis- 
tance pronominalization a d global focus. In Ptv- 
ceedings of the 36th Ammal Meeting of the Associ- 
ation for Computational Linguistics attd of the 17th 
hzternational Conference on Computational Linguis- 
tics (COLING/ACL'98), pages 550-556, Montreal, 
Canada, August. 
Jerry H. Hobbs. 1978. Resolving pronoun references. 
Lingua, 44:311-338. 
Megumi Kameyama. 1997. Recognizing referential 
links: An information extraction perspective. In Pro- 
ceedings of the ACL/EACL'97 Workshop on Opera- 
tional Factors in Practical, Robust Anaphora Resoht- 
tion, pages 46-53. 
Shalom Lappin and Herbert J. Leass. 1994, An algo- 
rithm for pronominal anaphora resolution. Computa- 
tional Linguistics, 20(4):535-561. 
William C. Mann and Sandra A. Thompson. 1988. 
Rhetorical structure theory: Toward a functional the- 
ory of text organization. Text, 8(3):243-28 i. 
Daniel Marcu, Estibaliz Amorrortu, and Magdalena 
Romera. 1999. Experiments in constructing a cor- 
pus of discourse trees. In Ptvceedings of the ACL'99 
Worksho t)on Standards and 7bols for Discout:re Tag- 
ging, pages 48-57, University of Maryland, June 22. 
Ruslan Mitkov. 1997. Factors in anaphora resolution: 
They are not the only things that matter, a case study 
based on two different approaches. In Proceedings of 
the ACL/EACL'97 Workshop on Operational Factors 
in Practical, Robust Anaphora Resolution, pages 14- 
21. 
Candace L. Sidner. 1981. Focusing for interpretation f
pronouns. Computational Linguistics, 7(4):217-231, 
October-December. 
Wietske Vonk, Lettica G.M.M. Hustinx, and Wire tt.G. 
Simons. 1992. The use of referential expressions in 
structuring discourse, l~mguage and Cognitive Pro- 
cesses, 7(3,4):301-333. 
214 
 
	
Using GATE as an Environment for Teaching NLP
Kalina Bontcheva, Hamish Cunningham, Valentin Tablan, Diana Maynard, Oana Hamza
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
{kalina,hamish,valyt,diana,oana}@dcs.shef.ac.uk
Abstract
In this paper we argue that the GATE
architecture and visual development
environment can be used as an effec-
tive tool for teaching language engi-
neering and computational linguistics.
Since GATE comes with a customis-
able and extendable set of components,
it allows students to get hands-on ex-
perience with building NLP applica-
tions. GATE also has tools for cor-
pus annotation and performance eval-
uation, so students can go through the
entire application development process
within its graphical development en-
vironment. Finally, it offers com-
prehensive Unicode-compliant multi-
lingual support, thus allowing stu-
dents to create components for lan-
guages other than English. Unlike
other NLP teaching tools which were
designed specifically and only for this
purpose, GATE is a system developed
for and used actively in language en-
gineering research. This unique dual-
ity allows students to contribute to re-
search projects and gain skills in em-
bedding HLT in practical applications.
1 Introduction
When students learn programming, they have
the benefit of integrated development environ-
ments, which support them throughout the en-
tire application development process: from writ-
ing the code, through testing, to documenta-
tion. In addition, these environments offer sup-
port and automation of common tasks, e.g., user
interfaces can be designed easily by assembling
them visually from components like menus and
windows. Similarly, NLP and CL students can
benefit from the existence of a graphical devel-
opment environment, which allows them to get
hands-on experience in every aspect of develop-
ing and evaluating language processing modules.
In addition, such a tool would enable students to
see clearly the practical relevance and need for
language processing, by allowing them to exper-
iment easily with building NLP-powered (Web)
applications.
This paper shows how an existing infrastruc-
ture for language engineering research ? GATE
(Cunningham et al, 2002a; Cunningham, 2002)
? has been used successfully as an NLP teach-
ing environment, in addition to being a suc-
cessful vehicle for building NLP applications
and reusable components (Maynard et al, 2002;
Maynard et al, 2001). The key features of
GATE which make it particularly suitable for
teaching are:
? The system is designed to separate cleanly
low-level tasks such as data storage, data
visualisation, location and loading of com-
ponents and execution of processes from the
data structures and algorithms that actu-
ally process human language. In this way,
the students can concentrate on studying
and/or modifying the NLP data and algo-
rithms, while leaving the mundane tasks to
GATE.
                     July 2002, pp. 54-62.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
? Automating measurement of performance
of language processing components and fa-
cilities for the creation of the annotated cor-
pora needed for that.
? Providing a baseline set of language pro-
cessing components that can be extended
and/or replaced by students as required.
These modules typically separate clearly
the linguistic data from the algorithms that
use it, thus allowing teachers to present
them separately and the students to adapt
the modules to new domains/languages by
just modifying the linguistic data.
? It comes with exhaustive documenta-
tion, tutorials, and online movie demon-
strations, available on its Web site
(http://gate.ac.uk).
GATE and its language processing modules
were developed to promote robustness and scala-
bility of NLP approaches and applications, with
an emphasis on language engineering research.
Therefore, NLP/LE courses based on GATE
offer students the opportunity to learn from
non-toy applications, running on big, realistic
datasets (e.g., British National corpus or news
collected by a Web crawler). This unique re-
search/teaching duality also allows students to
contribute to research projects and gain skills in
embedding HLT in practical applications.
2 GATE from a Teaching
Perspective
GATE (Cunningham et al, 2002a) is an archi-
tecture, a framework and a development envi-
ronment for human language technology mod-
ules and applications. It comes with a set of
reusable modules, which are able to perform ba-
sic language processing tasks such as POS tag-
ging and semantic tagging. These eliminate the
need for students to re-implement useful algo-
rithms and modules, which are pre-requisites
for completing their assignments. For exam-
ple, Marin Dimitrov from Sofia University suc-
cessfully completed his masters? degree by im-
plementing a lightweight approach to pronom-
inal coreference resolution for named entities1,
which uses GATE?s reusable modules for the
earlier processing and builds upon their results
(see Section 4).
For courses where the emphasis is more on
linguistic annotation and corpus work, GATE
can be used as a corpus annotation environment
(see http://gate.ac.uk/talks/tutorial3/). The
annotation can be done completely manually
or it can be bootstrapped by running some
of GATE?s processing resources over the cor-
pus and then correcting/adding new annota-
tions manually. These facilities can also be used
in courses and assignments where the students
need to learn how to create data for quantitative
evaluation of NLP systems.
If evaluated against the requirements for
teaching environments discussed in (Loper and
Bird, 2002), GATE covers them all quite well.
The graphical development environment and
the JAPE language facilitate otherwise difficult
tasks. Inter-module consistency is achieved by
using the annotations model to hold language
data, while extensibility and modularity are the
very reason why GATE has been successfully
used in many research projects (Maynard et al,
2000). In addition, GATE also offers robustness
and scalability, which allow students to experi-
ment with big corpora, such as the British Na-
tional Corpus (approx. 4GB). In the following
subsections we will provide further detail about
these aspects of GATE.
2.1 GATE?s Graphical Development
Environment
GATE comes with a graphical development en-
vironment (or GATE GUI) that facilitates stu-
dents in inspecting the language processing re-
sults and debugging the modules. The envi-
ronment has facilities to view documents, cor-
pora, ontologies (including the popular Prote?ge?
editor (Noy et al, 2001)), and linguistic data
(expressed as annotations, see below), e.g., Fig-
ure 1 shows the document viewer with some
annotations highlighted. It also shows the re-
source panel on the left with all loaded appli-
1The thesis is available at
http://www.ontotext.com/ie/thesis-m.pdf
Figure 1: GATE?s visual development environment
cations, language resources, and processing re-
sources (i.e., modules). There are also view-
ers/editors for complex linguistic data like coref-
erence chains (Figure 2) and syntax trees (Fig-
ure 3). New graphical components can be in-
tegrated easily, thus allowing lecturers to cus-
tomise the environment as necessary. The
GATE team is also developing new visualisation
modules, especially a visual JAPE rule develop-
ment tool.
2.2 GATE API and Data Model
The central concept that needs to be learned by
the students before they start using GATE is
the annotation data model, which encodes all
linguistic data and is used as input and out-
put for all modules. GATE uses a single uni-
fied model of annotation - a modified form of
the TIPSTER format (Grishman, 1997) which
has been made largely compatible with the Atlas
format (Bird and Liberman, 1999). Annotations
are characterised by a type and a set of features
represented as attribute-value pairs. The anno-
tations are stored in structures called annotation
sets which constitute independent layers of an-
notation over the text content. The annotations
format is independent of any particular linguis-
tic formalism, in order to enable the use of mod-
ules based on different linguistic theories. This
generality enables the representation of a wide-
variety of linguistic information, ranging from
very simple (e.g., tokeniser results) to very com-
Figure 2: The coreference chains viewer
plex (e.g., parse trees and discourse representa-
tion: examples in (Saggion et al, 2002)). In
addition, the annotation format allows the rep-
resentation of incomplete linguistic structures,
e.g., partial-parsing results. GATE?s tree view-
ing component has been written especially to
be able to display such disconnected and incom-
plete trees.
GATE is implemented in Java, which makes
it easier for students to use it, because typi-
cally they are already familiar with this lan-
guage from their programming courses. The
GATE API (Application Programming Inter-
face) is fully documented in Javadoc and also
examples are given in the comprehensive User
Guide (Cunningham et al, 2002b). However,
students generally do not need to familiarise
themselves with Java and the API at all, be-
cause the majority of the modules are based on
GATE?s JAPE language, so customisation of ex-
isting and development of new modules only re-
quires knowledge of JAPE and the annotation
model described above.
JAPE is a version of CPSL (Common Pattern
Specification Language) (Appelt, 1996) and is
used to describe patterns to match and annota-
tions to be created as a result (for further de-
tails see (Cunningham et al, 2002b)). Once fa-
miliar with GATE?s data model, students would
not find it difficult to write the JAPE pattern-
based rules, because they are effectively regular
expressions, which is a concept familiar to most
Figure 3: The syntax tree viewer, showing a par-
tial syntax tree for a sentence from a telecom
news text
CS students.
An example rule from an existing named en-
tity recognition grammar is:
Rule: Company1
Priority: 25
(
({Token.orthography == upperInitial})+
{Lookup.kind == companyDesignator}
):companyMatch
-->
:companyMatch.NamedEntity =
{kind = "company", rule = "Company1"}
The rule matches a pattern consisting of any
kind of word, which starts with an upper-cased
letter (recognised by the tokeniser), followed by
one of the entries in the gazetteer list for com-
pany designators (words which typically indi-
cate companies, such as ?Ltd.? and ?GmBH?). It
then annotates this pattern with the entity type
?NamedEntity?, and gives it a feature ?kind?
with value company and another feature ?rule?
with value ?Company1?. The rule feature is
simply used for debugging purposes, so it is clear
which particular rule has fired to create the an-
notation.
The grammars (which are sets of rules) do not
need to be compiled by the students, because
they are automatically analysed and executed by
the JAPE Transducer module, which is a finite-
Figure 4: The visual evaluation tool
state transducer over the annotations in the doc-
ument. Since the grammars are stored in files in
a plain text format, they can be edited in any
text editor such as Notepad or Vi. The rule de-
velopment process is performed by the students
using GATE?s visual environment (see Figure 1)
to execute the grammars and visualise the re-
sults. The process is actually a cycle, where the
students write one or more rules, re-initialise the
transducer in the GATE GUI by right-clicking
on it, then run it on the test data, check the re-
sults, and go back to improving the rules. The
evaluation part of this cycle is performed using
GATE?s visual evaluation tools which also pro-
duce precision, recall, and f-measure automati-
cally (see Figure 4).
The advantage of using JAPE for the student
assignments is that once learned by the students,
it enables them to experiment with a variety
of NLP tasks from tokenisation and sentence
splitter, to chunking, to template-based infor-
mation extraction. Because it does not need to
be compiled and supports incremental develop-
ment, JAPE is ideal for rapid prototyping, so
students can experiment with alternative ideas.
Students who are doing bigger projects, e.g., a
final year project, might want to develop GATE
modules which are not based on the finite-state
machinery and JAPE. Or the assignment might
require the development of more complex gram-
mars in JAPE, in which case they might have
to use Java code on the right-hand side of the
rule. Since such GATE modules typically only
access and manipulate annotations, even then
the students would need to learn only that part
of GATE?s API (i.e., no more than 5 classes).
Our experience with two MSc students ? Partha
Lal and Marin Dimitrov ? has shown that they
do not have significant problems with using that
either.
2.3 Some useful modules
The tokeniser splits text into simple tokens,
such as numbers, punctuation, symbols, and
words of different types (e.g. with an initial capi-
tal, all upper case, etc.). The tokeniser does not
generally need to be modified for different ap-
plications or text types. It currently recognises
many types of words, whitespace patterns, num-
bers, symbols and punctuation and should han-
dle any language from the Indo-European group
without modifications. Since it is available as
open source, one student assignment could be
to modify its rules to cope with other languages
or specific problems in a given language. The to-
keniser is based on finite-state technology, so the
rules are independent from the algorithm that
executes them.
The sentence splitter is a cascade of finite-
state transducers which segments the text into
sentences. This module is required for the tag-
ger. Both the splitter and tagger are domain-
and application-independent. Again, the split-
ter grammars can be modified as part of a stu-
dent project, e.g., to deal with specifically for-
matted texts.
The tagger is a modified version of the Brill
tagger, which assigns a part-of-speech tag to
each word or symbol. To modify the tagger?s
behaviour, students will have to re-train it on
relevant annotated texts.
The gazetteer consists of lists such as cities,
organisations, days of the week, etc. It not only
consists of entities, but also of names of useful
indicators, such as typical company designators
(e.g. ?Ltd.?), titles, etc. The gazetteer lists are
compiled into finite state machines, which anno-
tate the occurrence of the list items in the given
document. Students can easily extend the exist-
ing lists and add new ones by double-clicking on
the Gazetteer processing resource, which brings
up the gazetteer editor if it has been installed,
or using GATE?s Unicode editor.
The JAPE transducer is the module that
runs JAPE grammars, which could be doing
tasks like chunking, named entity recognition,
etc. By default, GATE is supplied with an NE
transducer which performs named entity recog-
nition for English and a VP Chunker which
shows how chunking can be done using JAPE.
An even simpler (in terms of grammar rules
complexity) and somewhat incomplete NP chun-
ker can be obtained by request from the first
author.
The orthomatcher is a module, whose pri-
mary objective is to perform co-reference, or en-
tity tracking, by recognising relations between
entities, based on orthographically matching
their names. It also has a secondary role in im-
proving named entity recognition by assigning
annotations to previously unclassified names,
based on relations with existing entities.
2.4 Support for languages other than
English
GATE uses Unicode (Unicode Consortium,
1996) throughout, and has been tested on a va-
riety of Slavic, Germanic, Romance, and Indic
languages. The ability to handle Unicode data,
along with the separation between data and al-
gorithms, allows students to perform easily even
small-scale experiments with porting NLP com-
ponents to new languages. The graphical devel-
opment environment supports fully the creation,
editing, and visualisation of linguistic data, doc-
uments, and corpora in Unicode-supported lan-
guages (see (Tablan et al, 2002)). In order to
make it easier for foreign students to use the
GUI, we are planning to localise its menus, er-
ror messages, and buttons which currently are
only in English.
2.5 Installation and Programming
Languages Support
Since GATE is 100% Java, it can run on any
platform that has a Java support. To make it
easier to install and maintain, GATE comes with
installation wizards for all major platforms. It
also allows the creation and use of a site-wide
GATE configuration file, so settings need only
be specified once and all copies run by the stu-
dents will have the same configuration and mod-
ules available. In addition, GATE allows stu-
dents to have their own configuration settings,
e.g., specify modules which are available only
to them. The personal settings override those
from GATE?s default and site-wide configura-
tions. Students can also easily install GATE
on their home computers using the installation
program. GATE also allows applications to be
saved and moved between computers and plat-
forms, so students can easily work both at home
and in the lab and transfer their data and ap-
plications between the two.
GATE?s graphical environment comes config-
ured by default to save its own state on exit,
so students will automatically get their applica-
tions, modules, and data restored automatically
the next time they load GATE.
Although GATE is Java-based, modules writ-
ten in other languages can also be integrated
and used. For example, Prolog modules are eas-
ily executable using the Jasper Java-Prolog link-
ing library. Other programming languages can
be used if they support Java Native Interface
(JNI).
3 Existing Uses of GATE for
Teaching
Postgraduates in locations as diverse as Bul-
garia, Copenhagen and Surrey are using the
system in order to avoid having to write sim-
ple things like sentence splitters from scratch,
and to enable visualisation and management
of data. For example, Partha Lal at Impe-
rial College is developing a summarisation sys-
tem based on GATE and ANNIE as a final-
year project for an MEng Degree in Comput-
ing (http://www.doc.ic.ac.uk/? pl98/). His site
includes the URL of his components and once
given this URL, GATE loads his software over
the network. Another student project will be
discussed in more detail in Section 4.
Our colleagues in the Universities of Ed-
inburgh, UMIST in Manchester, and Sussex
(amongst others) have reported using previous
versions of the system for teaching, and the Uni-
versity of Stuttgart produced a tutorial in Ger-
man for the same purposes. Educational users of
early versions of GATE 2 include Exeter Univer-
sity, Imperial College, Stuttgart University, the
University of Edinburgh and others. In order to
facilitate the use of GATE as a teaching tool,
we have provided a number of tutorials, online
demonstrations, and exhaustive documentation
on GATE?s Web site (http://gate.ac.uk).
4 An Example MSc Project
The goal of this work was to develop a corefer-
ence resolution module to be integrated within
the named entity recognition system provided
with GATE. This required a number of tasks to
be performed by the student: (i) corpus anal-
ysis; (ii) implementation and integration; (iii)
testing and quantitative evaluation.
The student developed a lightweight approach
to resolving pronominal coreference for named
entities, which was implemented as a GATE
module and run after the existing NE modules
provided with the framework. This enabled him
also to use an existing annotated corpus from
an Information Extraction evaluation competi-
tion and the GATE evaluation tools to establish
how his module compared with results reported
in the literature. Finally, the testing process was
made simple, thanks to GATE?s visualisation fa-
cilities, which are already capable of displaying
coreference chains in documents.
GATE not only allowed the student to achieve
verifiable results quickly, but it also did not in-
cur substantial integration overheads, because
it comes with a bootstrap tool which automates
the creation of GATE-compliant NLP modules.
The steps that need to be followed are:2
? use the bootstrap tool to create an empty
Java module, then add the implementation
to it. A JAVA development environment
like JBuilder and VisualCafe can be used
for this and the next stages, if the students
are familiar with them;
? compile the class, and any others that it
uses, into a Java Archive (JAR) file (GATE
2For further details and an example see (Cunningham
et al, 2002b).
Figure 5: BootStrap Wizard Dialogue
generates automatically a Makefile too, to
facilitate this process);
? write some XML configuration data for the
new resource;
? tell GATE the URL of the new JAR and
XML files.
5 Example Topics
Since GATE has been used for a wide range of
tasks, it can be used for the teaching of a number
of topics. Topics that can be covered in (part of)
a course, based on GATE are:
? Language Processing, Language Engineer-
ing, and Computational Linguistics: differ-
ences, methodologies, problems.
? Architectures, portability, robustness, cor-
pora, and the Web.
? Corpora, annotation, and evaluation: tools
and methodologies.
? Basic modules: tokenisation, sentence split-
ting, gazetteer lookup.
? Part-of-speech tagging.
? Information Extraction: issues, tasks, rep-
resenting linguistic data in the TIPSTER
annotation format, MUC, results achieved.
? Named Entity Recognition.
? Coreference Resolution
? Template Elements and Relations
? Scenario Templates
? Parsing and chunking
? Document summarisation
? Ontologies and discourse interpretation
? Language generation
While language generation, parsing, summari-
sation, and discourse interpretation modules are
not currently distributed with GATE, they can
be obtained by contacting the authors. Modules
for text classification and learning algorithms in
general are to be developed in the near future.
A lecturer willing to contribute any such mod-
ules to GATE will be very welcome to do so and
will be offered integration support.
6 Example Assignments
The availability of example modules for a vari-
ety of NLP tasks allows students to use them
as a basis for the development of an entire NLP
application, consisting of separate modules built
during their course. For example, let us consider
two problems: recognising chemical formulae in
texts and making an IE system that extracts
information from dialogues. Both tasks require
students to make changes in a number of existing
components and also write some new grammars.
Some example assignments for the chemical
formulae recognition follow:
? tokeniser : while it will probably work well
for the dialogues, the first assignment would
be to make modifications to its regular ex-
pression grammar to tokenise formulae like
H4ClO2 and Al-Li-Ti in a more suitable
way.
? gazetteer : create new lists containing new
useful clues and types of data, e.g., all
chemical elements and their abbreviations.
? named entity recognition: write a new
grammar to be executed by a new JAPE
transducer module for the recognition of the
chemical formulae.
Some assignments for the dialogue application
are:
? sentence splitter : modify it so that it splits
correctly dialogue texts, by taking into ac-
count the speaker information (because dia-
logues often do not have punctuation). For
example:
A: Thank you, can I have your full name?
C: Err John Smith
A: Can you also confirm your postcode and
telephone number for security?
C: Erm it?s 111 111 11 11
A: Postcode?
C: AB11 1CD
? corpus annotation and evaluation: use the
default named entity recogniser to boot-
strap the manual annotation of the test
data for the dialogue application; evaluate
the performance of the default NE gram-
mars on the dialogue texts; suggest possi-
ble improvements on the basis of the infor-
mation about missed and incorrect anno-
tations provided by the corpus benchmark
tool.
? named entity recognition: implement the
improvements proposed at the previous
step, by changing the default NE grammar
rules and/or by introducing rules specific to
your dialogue domain.
Finally, some assignments which are not con-
nected to any particular domain or application:
? chunking : implement an NP chunker using
JAPE. Look at the VP chunker grammars
for examples.
? template-based IE : experiment with ex-
tracting information from the dialogues us-
ing templates and JAPE (an example im-
plementation will be provided soon).
? (for a group of students) building NLP-
enabled Web applications: embed one of the
IE applications developed so far into a Web
application, which takes a Web page and
returns it annotated with the entities. Use
http://gate.ac.uk/annie/index.jsp as an ex-
ample.
In the near future it will be also possible to
have assignments on summarisation and genera-
tion, but these modules are still under develop-
ment. It will be possible to demonstrate parsing
and discourse interpretation, but because these
modules are implemented in Prolog and some-
what difficult to modify, assignments based on
them are not recommended. However, other
such modules, e.g., those from NLTK (Loper
and Bird, 2002), can be used for such assign-
ments.
7 Conclusions
In this paper we have outlined the GATE sys-
tem and its key features that make it an effective
tool for teaching NLP and CL. The main advan-
tage is that GATE is a framework and a graph-
ical development environment which is suitable
both for research and teaching, thus making it
easier to connect the two, e.g., allow a student to
carry out a final-year project which contributes
to novel research, carried out by their lectur-
ers. The development environment comes with
a comprehensive set of tools, which cover the
entire application development cycle. It can be
used to provide students with hands-on experi-
ence in a wide variety of tasks. Universities will-
ing to use GATE as a teaching tool will benefit
from the comprehensive documentation, several
tutorials, and online demonstrations.
References
D.E. Appelt. 1996. The Common Pattern Specifi-
cation Language. Technical report, SRI Interna-
tional, Artificial Intelligence Center.
S. Bird and M. Liberman. 1999. A Formal Frame-
work for Linguistic Annotation. Technical Re-
port MS-CIS-99-01, Department of Computer and
Information Science, University of Pennsylvania.
http://xxx.lanl.gov/abs/cs.CL/9903003.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002a. GATE: A framework and
graphical development environment for robust
NLP tools and applications. In Proceedings of the
40th Anniversary Meeting of the Association for
Computational Linguistics.
H. Cunningham, D. Maynard, K. Bontcheva,
V. Tablan, and C. Ursu. 2002b. The GATE User
Guide. http://gate.ac.uk/.
H. Cunningham. 2002. GATE, a General Archi-
tecture for Text Engineering. Computers and the
Humanities, 36:223?254.
R. Grishman. 1997. TIPSTER Architec-
ture Design Document Version 2.3. Techni-
cal report, DARPA. http://www.itl.nist.gov/-
div894/894.02/related projects/tipster/.
E. Loper and S. Bird. 2002. NLTK: The Natural
Language Toolkit. In ACL Workshop on Effective
Tools and Methodologies in Teaching NLP.
D. Maynard, H. Cunningham, K. Bontcheva,
R. Catizone, George Demetriou, Robert
Gaizauskas, Oana Hamza, Mark Hepple, Patrick
Herring, Brian Mitchell, Michael Oakes, Wim
Peters, Andrea Setzer, Mark Stevenson, Valentin
Tablan, Christian Ursu, and Yorick Wilks. 2000.
A Survey of Uses of GATE. Technical Report
CS?00?06, Department of Computer Science,
University of Sheffield.
D. Maynard, V. Tablan, C. Ursu, H. Cunningham,
and Y. Wilks. 2001. Named Entity Recognition
from Diverse Text Types. In Recent Advances
in Natural Language Processing 2001 Conference,
Tzigov Chark, Bulgaria.
D. Maynard, V. Tablan, H. Cunningham, C. Ursu,
H. Saggion, K. Bontcheva, and Y. Wilks. 2002.
Architectural elements of language engineering ro-
bustness. Journal of Natural Language Engineer-
ing ? Special Issue on Robust Methods in Analysis
of Natural Language Data. forthcoming.
N.F. Noy, M. Sintek, S. Decker, M. Crubzy, R.W.
Fergerson, and M.A. Musen. 2001. Creating Se-
mantic Web Contents with Prote?ge?-2000. IEEE
Intelligent Systems, 16(2):60?71.
H. Saggion, H. Cunningham, K. Bontcheva, D. May-
nard, C. Ursu, O. Hamza, and Y. Wilks. 2002.
Access to Multimedia Information through Mul-
tisource and Multilanguage Information Extrac-
tion. In 7th Workshop on Applications of Natural
Language to Information Systems (NLDB 2002),
Stockholm, Sweden.
V. Tablan, C. Ursu, K. Bontcheva, H. Cunningham,
D. Maynard, O. Hamza, Tony McEnery, Paul
Baker, and Mark Leisher. 2002. A unicode-based
environment for creation and use of language re-
sources. In Proceedings of 3rd Language Resources
and Evaluation Conference. forthcoming.
Unicode Consortium. 1996. The Unicode Standard,
Version 2.0. Addison-Wesley, Reading, MA.
OLLIE: On-Line Learning for Information Extraction
Valentin Tablan, Kalina Bontcheva, Diana Maynard, Hamish Cunningham
University of Sheffield
Regent Court, 211 Portobello St.
Sheffield S1 4DP, UK
{V.Tablan,diana,kalina,hamish}@dcs.shef.ac.uk
Abstract
This paper reports work aimed at develop-
ing an open, distributed learning environ-
ment, OLLIE, where researchers can ex-
periment with different Machine Learning
(ML) methods for Information Extraction.
Once the required level of performance is
reached, the ML algorithms can be used to
speed up the manual annotation process.
OLLIE uses a browser client while data
storage and ML training is performed on
servers. The different ML algorithms use
a unified programming interface; the inte-
gration of new ones is straightforward.
1 Introduction
OLLIE is an on-line application for corpus annota-
tion that harnesses the power of Machine Learning
(ML) and Information Extraction (IE) in order to
make the annotator?s task easier and more efficient.
A normal OLLIE working session starts with the
user uploading a set of documents, selecting which
ML method to use from the several supplied by the
system, choosing the parameters for the learning
module and starting to annotate the texts. During
the initial phase of the manual annotation process,
the system learns in the background (i.e. on the
server) from the user?s actions and, when a certain
degree of confidence is reached, it starts making sug-
gestions by pre-annotating the documents. Initially,
some of these suggestions may be erroneous but, as
the user makes the necessary corrections, the system
will learn from its mistakes and the performance will
increase leading to a reduction in the amount of hu-
man input required.
The implementation is based on a client-server ar-
chitecture where the client is any Java-enabled web
browser and the server is responsible for storing
data, training ML models and providing access ser-
vices for the users.
The client side of OLLIE is implemented as a set
of Java Server Pages (JSPs) and a small number of
Java applets are used for tasks where the user inter-
face capabilities provided by HTML are not enough.
The server side comprises a JSP/servlet server,
a relational database server and an instance of the
GATE architecture for language engineering which
is used for driving all the language-related process-
ing. The general architecture is presented in Figure
1.
The next section describes the client side of the
OLLIE system while Section 3 details the imple-
mentation of the server with a subsection on the inte-
gration of Machine Learning. Section 4 talks about
security; Section 6 about future improvements and
Section 7 concludes the paper.
2 The OLLIE client
The OLLIE client consists of several web pages,
each of them giving the user access to a particular
service provided by the server.
One such page provides facilities for uploading
documents in the system from a URL, a local file,
or created from text pasted in a form. A variety
of formats including XML, HTML, email and plain
text are supported. When a document is created, the
Figure 1: The general architecture of OLLIE
original markup ?if present? is separated from tex-
tual content to prevent it from interfering with the
subsequent language processing.
Another page lets the user choose which machine
learning algorithm is to be used, the attributes that
characterise the instances (e.g., orthography, part-
of-speech), and parameters for the chosen learning
method (e.g., thresholds, smoothing values). The
classes to be learnt (e.g., Person, Organisation) are
provided as part of the user profile, which can be
edited on a dedicated page. All ML methods com-
patible with OLLIE have a uniform way of describ-
ing attributes and classes (see Section 3.1 for more
details on the ML integration); this makes possible
the use of a single user interface for all the ML al-
gorithms available. The fine-tuning parameters are
specific to each ML method and, although the ML
methods can be run with their default parameters, as
established by (Daelemans and Hoste, 2002), sub-
stantial variation in performance can be obtained by
changing algorithm options.
Since OLLIE needs to support the users with the
annotation process by learning in the background
and suggesting annotations, it offers control over
the accuracy threshold for these suggestions. This
avoids annoying the users with wrong suggestions
while assuring that suggestions the system is confi-
dent about are used to pre-annotate the data, reduc-
ing the workload of the user.
The document editor can then be used to annotate
the text (see Figure 2). The right-hand side shows
the classes of annotations (as specified in the user
profile) and the user selects the text to be annotated
(e.g., ?McCarthy?) and clicks on the desired class
(e.g., Person). The new annotation is added to the
document and the server is updated immediately (so
the new data becomes available to the ML algorithm
too). The document editor also provides facilities
for deleting wrong annotations, which are then prop-
agated to the server, in a similar way.
The graphical interface facilities provided by a
web browser could be used to design an interface for
annotating documents but that would mean stretch-
ing them beyond their intended use and it is hard to
believe that such an interface would rate very high
on a usability scale. In order to provide a more er-
gonomic interface, OLLIE uses a Java applet that
integrates seamlessly with the page displayed by the
browser. Apart from better usability, this allows for
greater range of options for the user.
The communication between the editor applet and
the server is established using Java Remote Method
Invocation (a protocol similar to the C++ Remote
Procedure Call ? RPC) which allows the instant no-
tification when updates are needed for the document
stored on the server. The continuous communication
between the client and the server adds the benefit of
data security in case of network failure. The data
on the server always reflects the latest version of the
document so no data loss can occur. The session
data stored by the server expires automatically after
an idle time of one hour. This releases the resources
used on the server in case of persistent network fail-
ures.
The data structures used to store documents on the
server are relatively large because of the numerous
indices stored to allow efficient access to the annota-
tions. The copy downloaded by the client when the
annotation process is initiated is greatly reduced by
filtering out all the unnecessary information. Most
of the data transferred during the client-server com-
munication is also compressed, which reduces the
level of network traffic ? always a problem in client
server architectures that run over the Internet.
Figure 2: Annotating text in the OLLIE client
Another utility provided by the client is a page
that lets the user specify the access rights to the doc-
ument/corpus, which determine whether it can be
shared for viewing or collaborative annotation (see
Section 4 for details on security).
3 Implementation of the OLLIE server
While the client side of the OLLIE application is
presented as set of web pages, the server part is
based on the open source GATE architecture.
GATE is an infrastructure for developing and de-
ploying software components that process human
language (Cunningham et al, 2002). It is written
in Java and exploits component-based software de-
velopment, object orientation and mobile code. One
quality of GATE is that it uses Unicode through-
out (Tablan et al, 2002). Its Unicode capabilities
have been tested on a variety of Slavic, Germanic,
Romance, and Indic languages (Gamba?ck and Ols-
son, 2000; Baker et al, 2002). This allows OL-
LIE to handle documents in languages other than
English. The back-end of OLLIE uses the GATE
framework to provide language processing compo-
nents, services for persistent storage of user data,
security, and application management.
When a document is loaded in the OLLIE client
and subsequently uploaded to the server, its format
is analysed and converted into a GATE document
which consists of textual content and one or more
layers of annotation. The annotation format is a
modified form of the TIPSTER format (Grishman,
1997), is largely isomorphic with the Atlas format
(Bird and Liberman, 1999) and successfully sup-
ports I/O to/from XCES and TEI (Ide et al, 2000).1
An annotation has a type, a pair of nodes pointing
to positions inside the document content, and a set
of attribute-values, encoding further linguistic infor-
mation. Attributes are strings; values can be any
Java object. An annotation layer is organised as a
Directed Acyclic Graph on which the nodes are par-
ticular locations in the document content and the
arcs are made out of annotations. All the markup
contained in the original document is automatically
extracted into a special annotation layer and can be
used for processing or for exporting the document
back to its original format.
1The American National Corpus is using GATE for a large
TEI-based project.
Linguistic data (i.e., annotated documents and
corpora) is stored in a database on the server (see
Figure 1), in order to achieve optimal performance,
concurrent data access, and persistence between
working sessions.
One of the most important tasks for the OLLIE
server is the execution and control of ML algo-
rithms. In order to be able to use ML in OLLIE,
a new processing resource was designed that adds
ML support to GATE.
3.1 Machine Learning Support
Our implementation for ML uses classification al-
gorithms for which annotations of a given type are
instances while the attributes for them are collected
from the context in which the instances occur in the
documents.
Three types of attributes are defined: nominal,
boolean and numeric. The nominal attributes can
take a value from a specified set of possible values
while the boolean and numeric ones have the usual
definitions.
When collecting training data, all the annotations
of the type specified as instances are listed, and for
each of them, the set of attribute values is deter-
mined. All attribute values for an instance refer to
characteristics of a particular instance annotation,
which may be either the current instance or one sit-
uated at a specified relative position.
Boolean attributes refer to the presence (or ab-
sence) of a particular type of annotation overlapping
at least partially with the required instance. Nominal
and numeric attributes refer to features on a partic-
ular type of annotation that (partially) overlaps the
instance in scope.
One of the boolean or nominal attributes is
marked as the class attribute, and the values which
that attribute can take are the labels for the classes
to be learnt by the algorithm. Figure 3 depicts some
types of attributes and the values they would take
in a particular example. The boxes represent an-
notations, Token annotations are used as instances,
the one in the centre being the current instance for
which attribute values are being collected.
Since linguistic information, such as part-of-
speech and gazetteer class, is often used as at-
tributes for ML, OLLIE provides support for iden-
tifying a wide range of linguistic information - part-
of-speech, sentence boundaries, gazetteer lists, and
named entity class. This information, together with
tokenisation information (kind, orthography, and to-
ken length) is obtained by using the language pro-
cessing components available with GATE, as part of
the ANNIE system (Cunningham et al, 2002). See
Section 5 for more details on the types of linguistic
features that can be used. The user chooses which of
this information is to be used as attributes.
An ML implementation has two modes of func-
tioning: training ? when the model is being built,
and application ? when the built model is used to
classify new instances. Our implementation consists
of a GATE processing resource that handles both the
training and application phases. It is responsible for
detecting all the instances in a document and col-
lecting the attribute values for them. The data thus
obtained can then be forwarded to various external
implementations of ML algorithms.
Depending on the type of the attribute that is
marked as class, different actions will be performed
when a classification occurs. For boolean attributes,
a new annotation of the type specified in the attribute
definition will be created. Nominal attributes trigger
the addition of the feature specified in the attribute
definition on an annotation of the required type sit-
uated at the position of the classified instance. If no
such annotation is present, it will be created.
Once an ML model is built it can be stored as part
of the user profile and reloaded for use at a later time.
The execution of the ML processing resource is
controlled through configuration data that selects the
type of annotation to be used as instances, defines all
the attributes and selects which ML algorithm will
be used and with what parameters.
One good source of implementations for many
well-known ML algorithms is the WEKA library
(Witten and Frank, 1999).2 It also provides a wealth
of tools for performance evaluation, testing, and at-
tribute selection, which were used during the devel-
opment process.
OLLIE uses the ML implementations provided by
WEKA which is accessed through a simple wrap-
per that translates the requests from GATE into API
calls ?understood? by WEKA. The main types of re-
quests dealt with by the wrapper are the setting of
2WEKA homepage: http://www.cs.waikato.ac.nz/ml/weka/
Figure 3: Example of attributes and their values.
configuration data, the addition of a new training in-
stance and the classification of an application-time
instance.
4 Security
Because OLLIE is deployed as a web application
? accessible by anyone with Internet access, secu-
rity is an important issue. Users store documents on
the server and the system also keeps some personal
data about the users for practical reasons.3 All users
need to be provided with a mechanism to authen-
ticate themselves to the system and they need to be
able to select who else, apart from them, will be able
to see or modify the data they store on the server.
Every user has a username and a password, used
to retrieve their profiles and determine which doc-
uments they can access. The profiles also contain
information specifying the types of annotations that
they will be creating during the annotation process.
For example, in the case of a basic named entity
3Storing email addresses for instance is useful for sending
password reminders.
recognition task, the profile will specify Person, Or-
ganisation, and Location. These tags will then be
provided in the document annotation pages.
The access rights for the documents are handled
by GATE which implements a security model simi-
lar to that used in Unix file systems. Table 1 shows
the combination of rights that are possible. They
give a good granularity for access rights, ranging
from private to world readable.
The set of known users is shared between GATE
and OLLIE and, once a user is authenticated with
the system, the login details are kept in session data
which is stored on the OLLIE server. This allows for
automatic logins to the underlying GATE platform
and transparent management of GATE sessions.
5 ML Experiments and Evaluation
To the end of evaluating the suitability of the ML
algorithms provided by WEKA for use in OLLIE
we performed several experiments for named entity
recognition on the MUC-7 corpus (SAIC, 1998). We
concentrated on the recognition of ENAMEX enti-
User User?s Group Other Users
Mode Read Write Read Write Read Write
?World Read/Group Write? + + + + + -
?Group Read/Group Write? + + + + - -
?Group Read/Owner Write? + + + - - -
?Owner Read/Owner Write? + + - - - -
Table 1: Security model ? the access rights
ties, i.e., Person, Organisation, and Location. The
MUC-7 corpus contains 1880 Organisation (46%),
1324 Location (32%), and 887 Person (22%) an-
notations in 100 documents. The task has two ele-
ments: recognition of the entity boundaries and clas-
sification of the entities in the three classes. The re-
sults are summarised below.
We first tested the ability of the learners to iden-
tify correctly the boundaries of named entities. Us-
ing 10-fold cross-validation on the MUC 7 corpus
described above, we experimented with different
machine learning algorithms and parameters (using
WEKA), and using different attributes for training.
5 different algorithms have been evaluated: Zero
R and OneR ? as baselines, Naive Bayes, IBK (an
implementation of K Nearest Neighbour) and J48
(an implementation of a C4.5 decision tree).
As expected, the baseline algorithms performed
very poorly (at around 1%). For IBK small windows
gave low results, while large windows were very in-
efficient. The best results (f-measure of around 60%)
were achieved using the J48 algorithm.
The types of linguistic data used for the attribute
collection included part of speech information, or-
thography (upper case, lower case, initial upper case
letter, mixture of upper and lower case), token kind
(word, symbol, punctuation or number), sentence
boundary, the presence of certain known names and
keywords from the gazetteer lists provided by the
ANNIE system. Tokens were used as instance an-
notations and, for each token, the window used for
collecting the attributes was of size 5 (itself plus two
other tokens in each direction).
Additional information, such as features on a
wider window of tokens, tended to improve the re-
call marginally, but decreased the precision substan-
tially, resulting in a lower F-measure, and therefore
the trade off was not worthwhile.
We also tested the algorithms on a smaller news
corpus (which contained around 68,000 instances as
opposed to 300,000 for the MUC7 corpus). Again,
the J48 algorithm scored highest, with the decision
table and the K nearest neighbour algorithms both
scoring approximately 1 percentage point lower than
the J48.
The second set of experiments was to classify the
named entities identified into the three ENAMEX
categories: Organisations, Persons and Locations.
Using 10-fold cross-validation on the MUC 7 corpus
described above, we experimented with the WEKA
machine learning algorithms and parameters, and
using attributes for training similar to those used for
boundary detection. The best results were achieved
again with the J48 algorithm, and, for this easier
task, they were situated at around 90%. The at-
tributes were chosen on the basis of their informa-
tion gain, calculated using WEKA?s attribute selec-
tion facilities.
The named entity recognition experiments were
performed mainly to evaluate the WEKA ML algo-
rithms on datasets of different sizes, ranging from
small to fairly large ones (300,000 instances). The
different ML algorithms had different memory re-
quirements and execution speed, tested on a PIII
1.5GHz PC running Windows 2000 with 1GB RAM.
From all algorithms tested, the decision table and
decision tree were the slowest (325 and 122 seconds
respectively on 68,000 instances) and required most
memory - up to 800MB on the big datasets. Naive
Bayes was very fast (only 0.25 seconds) with 1R fol-
lowing closely (0.28 seconds).
6 Further Work
OLLIE is very much work-in-progress and there are
several possible improvements we are considering.
When dealing with a particular corpus, it is rea-
sonable to assume that the documents may be quite
similar in terms of subject, genre or style. Because
of that, it is possible that the quality of the user ex-
perience can be improved by simply using a list of
positive and negative examples. This would allow
the system not to make the same mistakes by always
missing a particular example or always annotating a
false positive ? which can be very annoying for the
user.
The big difference in execution time for differ-
ent ML algorithms shows that there are practical
advantages that can be gained from having more
than one ML algorithm integrated in OLLIE, when
it comes to supporting the user with the annotation
task. Since the two experiments showed that Naive
Bayes performs only slightly worse than the best,
but slower algorithms, it may be feasible to train
both a fast Naive Bayes classifier and a slower, but
more precise one. In this way, while the slower ML
algorithm is being re-trained on the latest data, OL-
LIE can choose between using the older model of
this algorithm or the newly re-trained faster base-
line, depending on which ones gives better results,
and suggest annotations for the current document.
As with other such environments, this performance
is measured with respect to the latest annotated doc-
ument.
We hope to be able to integrate more learning
methods, e.g., TiMBL (Daelemans et al, 1998) and
we will also provide support for other people willing
to integrate theirs and make them available from our
OLLIE server or run their own server.
We plan to experiment with other NLP tasks, e.g,
relation extraction, coreference resolution and text
planning for language generation.
Finally, we are working on a Web service imple-
mentation of OLLIE for other distributed, Grid and
e-science applications.
7 Conclusion
OLLIE is an advanced collaborative annotation en-
vironment, which allows users to share and annotate
distributed corpora, supported by adaptive informa-
tion extraction that trains in the background and pro-
vides suggestions.
The option of sharing access to documents with
other users gives several users the possibility to en-
gage in collaborative annotation of documents. For
example, one user can annotate a text with organi-
sations, then another annotate it with locations. Be-
cause the documents reside on the shared server one
user can see errors or questionable markup intro-
duced by another user and initiate a discussion. Such
collaborative annotation is useful in the wider con-
text of creating and sharing language resources (Ma
et al, 2002).
A number of Machine Learning approaches for
Information Extraction have been developed re-
cently, e.g., (Collins, 2002; Bikel et al, 1999), in-
cluding some that use active learning, e.g., (Thomp-
son et al, 1999) or offer automated support, e.g,
(Ciravegna et al, 2002), in order to lower the over-
head of annotating training data. While there ex-
ist corpora used for comparative evaluation, (e.g.,
MUC or the CMU seminar corpus), there is no easy
way to test those ML algorithms on other data, eval-
uate their portability to new domains, or experiment
with different parameters of the models. While some
of the algorithms are available for experimentation,
they are implemented in different languages, require
different data formats, and run on different plat-
forms. All of this makes it hard to ensure experimen-
tal repeatability and eliminate site-specific skew ef-
fects. Also, since not all systems are freely available,
we propose an open, distributed environment where
researchers can experiment with different learning
methods on their own data.
Another advantage of OLLIE is that it defines
a simple API (Application Programming Interface)
which is used by the different ML algorithms to ac-
cess the training data (see Section 3.1). Therefore,
the integration of a new machine learning algorithm
in OLLIE amounts to providing a wrapper that im-
plements this API (a straightforward process). We
have already provided a wrapper for the ML algo-
rithms provided by the WEKA toolkit which can be
used as an example.
Although OLLIE shares features with other adap-
tive IE environments (e.g., (Ciravegna et al, 2002))
and collaborative annotation tools (e.g., (Ma et al,
2002)), it combines them in a unique fashion. In ad-
dition, OLLIE is the only adaptive IE system that al-
lows users to choose which ML approach they want
to use and to comparatively evaluate different ap-
proaches.
References
[Baker et al2002] P. Baker, A. Hardie, T. McEnery,
H. Cunningham, and R. Gaizauskas. 2002. EMILLE,
A 67-Million Word Corpus of Indic Languages: Data
Collection, Mark-up and Harmonisation. In Proceed-
ings of 3rd Language Resources and Evaluation Con-
ference (LREC?2002), pages 819?825.
[Bikel et al1999] D. Bikel, R. Schwartz, and R.M.
Weischedel. 1999. An Algorithm that Learns What?s
in a Name. Machine Learning, Special Issue on Natu-
ral Language Learning, 34(1-3), Feb.
[Bird and Liberman1999] S. Bird and M. Liberman.
1999. A Formal Framework for Linguistic Anno-
tation. Technical Report MS-CIS-99-01, Depart-
ment of Computer and Information Science, Uni-
versity of Pennsylvania. http://xxx.lanl.gov/-
abs/cs.CL/9903003.
[Ciravegna et al2002] F. Ciravegna, A. Dingli, D. Pe-
trelli, and Y. Wilks. 2002. User-System Coop-
eration in Document Annotation Based on Informa-
tion Extraction. In 13th International Conference on
Knowledge Engineering and Knowledge Management
(EKAW02), pages 122?137, Siguenza, Spain.
[Collins2002] M. Collins. 2002. Ranking algorithms for
named entity extraction: Boosting and the voted per-
ceptron. In Proceedings of the 40th Annual Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia,PA.
[Cunningham et al2002] H. Cunningham, D. Maynard,
K. Bontcheva, and V. Tablan. 2002. GATE: A Frame-
work and Graphical Development Environment for
Robust NLP Tools and Applications. In Proceedings
of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
[Daelemans and Hoste2002] Walter Daelemans and
Ve?ronique Hoste. 2002. Evaluation of Machine
Learning Methods for Natural Language Processing
Tasks. In LREC 2002 Third International Conference
on Language Resources and Evaluation, pages 755?
760, CNTS Language Technology Group, University
of Antwerp, UIA, Universiteitsplein 1 (bldng A),
B-2610 Antwerpen, Belgium.
[Daelemans et al1998] W. Daelemans, J. Zavrel,
K. van der Sloot, and A. van den Bosch. 1998. Timbl:
Tilburg memory based learner version 1.0. Technical
Report Technical Report 98-03, ILK.
[Gamba?ck and Olsson2000] B. Gamba?ck and F. Olsson.
2000. Experiences of Language Engineering Algo-
rithm Reuse. In Second International Conference on
Language Resources and Evaluation (LREC), pages
155?160, Athens, Greece.
[Grishman1997] R. Grishman. 1997. TIPSTER Ar-
chitecture Design Document Version 2.3. Techni-
cal report, DARPA. http://www.itl.nist.gov/-
div894/894.02/related projects/tipster/.
[Ide et al2000] N. Ide, P. Bonhomme, and L. Romary.
2000. XCES: An XML-based Standard for Linguis-
tic Corpora. In Proceedings of the Second Interna-
tional Language Resources and Evaluation Confer-
ence (LREC), pages 825?830, Athens, Greece.
[Ma et al2002] X. Ma, H. Lee, S. Bird, and K. Maeda.
2002. Models and tools for collaborative annotation.
In Proceedings of 3rd Language Resources and Evalu-
ation Conference (LREC?2002), Gran Canaria, Spain.
[SAIC1998] SAIC. 1998. Proceedings of the Sev-
enth Message Understanding Conference (MUC-
7). http://www.itl.nist.gov/iaui/894.02/-
related projects/muc/index.html.
[Tablan et al2002] V. Tablan, C. Ursu, K. Bontcheva,
H. Cunningham, D. Maynard, O. Hamza, Tony
McEnery, Paul Baker, and Mark Leisher. 2002. A
unicode-based environment for creation and use of
language resources. In Proceedings of 3rd Language
Resources and Evaluation Conference.
[Thompson et al1999] C. A. Thompson, M. E. Califf, and
R. J. Mooney. 1999. Active learning for natural lan-
guage parsing and information extraction. In Pro-
ceedings of the International Conference on Machine
Learning, pages 406?414.
[Witten and Frank1999] I. H. Witten and E. Frank. 1999.
Data Mining: Practical Machine Learning Tools and
Techniques with Java Implementations. Morgan Kauf-
mann.
  	
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 19?24,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
AnnoMarket: An Open Cloud Platform for NLP
Valentin Tablan, Kalina Bontcheva
Ian Roberts, Hamish Cunningham
University of Sheffield,
Department of Computer Science
211 Portobello, Sheffield, UK
Initial.Surname@dcs.shef.ac.uk
Marin Dimitrov
Ontotext AD
47A Tsarigradsko Shosse, Sofia, Bulgaria
marin.dimitrov@ontotext.com
Abstract
This paper presents AnnoMarket, an open
cloud-based platform which enables re-
searchers to deploy, share, and use lan-
guage processing components and re-
sources, following the data-as-a-service
and software-as-a-service paradigms. The
focus is on multilingual text analysis re-
sources and services, based on an open-
source infrastructure and compliant with
relevant NLP standards. We demonstrate
how the AnnoMarket platform can be used
to develop NLP applications with little
or no programming, to index the results
for enhanced browsing and search, and
to evaluate performance. Utilising Anno-
Market is straightforward, since cloud in-
frastructural issues are dealt with by the
platform, completely transparently to the
user: load balancing, efficient data upload
and storage, deployment on the virtual ma-
chines, security, and fault tolerance.
1 Introduction
Following the Software-as-a-Service (SaaS)
paradigm from cloud computing (Dikaiakos et al,
2009), a number of text processing services have
been developed, e.g. OpenCalais1 and Alchemy
API2. These provide information extraction ser-
vices, accessible programmatically and charged
per number of documents processed.
However, they suffer from two key technical
drawbacks. Firstly, document-by-document pro-
cessing over HTTP is inefficient on large datasets
and is also limited to within-document text pro-
cessing algorithms. Secondly, the text process-
ing algorithms are pre-packaged: it is not pos-
sible for researchers to extend the functional-
1http://www.opencalais.com
2http://www.alchemyapi.com
ity (e.g. adapt such a service to recognise new
kinds of entities). Additionally, these text pro-
cessing SaaS sites come with daily rate limits,
in terms of number of API calls or documents
that can be processed. Consequently, using these
services for research is not just limited in terms
of text processing functionality offered, but also
quickly becomes very expensive on large-scale
datasets. A moderately-sized collection of tweets,
for example, comprises small but numerous docu-
ments, which can lead to unfeasibly high process-
ing costs.
Platform-as-a-Service (PaaS) (Dikaiakos et al,
2009) are a type of cloud computing service which
insulates developers from the low-level issues of
utilising cloud infrastructures effectively, while
providing facilities for efficient development, test-
ing, and deployment of software over the Inter-
net, following the SaaS model. In the context
of traditional NLP research and development, and
pre-dating cloud computing, similar needs were
addressed through NLP infrastructures, such as
GATE (Cunningham et al, 2013) and UIMA (Fer-
rucci and Lally, 2004). These infrastructures ac-
celerated significantly the pace of NLP research,
through reusable algorithms (e.g. rule-based pat-
tern matching engines, machine learning algo-
rithms), free tools for low-level NLP tasks, and
support for multiple input and output document
formats (e.g. XML, PDF, DOC, RDF, JSON).
This demonstration introduces the AnnoMar-
ket3 open, cloud-based platform, which has
been developed following the PaaS paradigm.
It enables researchers to deploy, share, and
use language processing components and re-
sources, following the Data-as-a-Service (DaaS)
and Software-as-a-Service (SaaS) paradigms. It
gives researchers access to an open, standard-
compliant NLP infrastructure and enables them
3At the time of writing, a beta version of AnnoMarket is
available at http://annomarket.com
19
to carry out large-scale NLP experiments by har-
nessing the vast, on-demand compute power of
the Amazon cloud. It supports not only NLP al-
gorithm development and execution, but also on-
demand collaborative corpus annotation and per-
formance evaluation. Important infrastructural is-
sues are dealt with by the platform, completely
transparently for the researcher: load balancing,
efficient data upload and storage, deployment on
the virtual machines, security, and fault tolerance.
AnnoMarket differs from previous work (e.g.
(Zhou et al, 2010; Ramakrishnan et al, 2010))
in that it requires no programming in order to
run a GATE-compliant NLP application on a large
dataset. In that sense, it combines the ease of
use of an NLP SaaS with the openness and com-
prehensive facilities of the GATE NLP infras-
tructure. AnnoMarket offers a growing number
of pre-packaged services, in multiple languages.
Additionally, as a specialised NLP PaaS, it also
supports a bring-your-own-pipeline option, which
can be built easily by reusing pre-existing GATE-
compatible NLP components and adding some
new ones. Moreover, in addition to offering entity
extraction services like OpenCalais, our NLP PaaS
also supports manual corpus annotation, semantic
indexing and search, and performance evaluation.
The contributions of this paper are as follows:
1. A demonstration of running AnnoMarket
multilingual NLP services on large datasets,
without programming. The new service
deployment facilities will also be shown,
including how services can optionally be
shared with others.
2. A demonstration on shared research corpora
via the AnnoMarket platform, following the
data-as-a-service model (the sharer is respon-
sible for ensuring no copyright violations).
3. A demonstration of the large-scale search and
browsing interface, which uses the results of
the NLP SaaS to offer enhanced, semantic-
based functionality.
2 The AnnoMarket NLP PaaS
This section first discusses the methodology
underpinning the AnnoMarket platform, then
presents its architecture and key components.
2.1 Development and Deployment
Methodology
The development of text analysis algorithms and
pipelines typically follows a certain methodolog-
ical pattern, or lifecycle. A central problem is
to define the NLP task, such that human anno-
tators can perform it with a high level of agree-
ment and to create high quality training and evalu-
ation datasets. It is common to use double or triple
annotation, where several people perform the an-
notation task independently and we then measure
their level of agreement (Inter-Annotator Agree-
ment, or IAA) to quantify and control the quality
of this data (Hovy, 2010).
The AnnoMarket platform was therefore de-
signed to offer full methodological support for all
stages of the text analysis development lifecycle:
1. Create an initial prototype of the NLP
pipeline, testing on a small document collec-
tion, using the desktop-based GATE user in-
terface (Cunningham et al, 2002);
2. If required, collect a gold-standard corpus for
evaluation and/or training, using the GATE
Teamware collaborative corpus annotation
service (Bontcheva et al, 2013), running in
AnnoMarket;
3. Evaluate the performance of the automatic
pipeline on the gold standard (either locally
in the GATE development environment or on
the cloud). Return to step 1 for further devel-
opment and evaluation cycles, as needed.
4. Upload the large datasets and deploy the NLP
pipeline on the AnnoMarket PaaS;
5. Run the large-scale NLP experiment and
download the results as XML or a standard
linguistic annotation format (Ide and Ro-
mary, 2004). AnnoMarket alo offers scal-
able semantic indexing and search over the
linguistic annotations and document content.
6. Analyse any errors, and if required, iterate
again over the earlier steps.
AnnoMarket is fully compatible with the GATE
open-source architecture (Cunningham et al,
2002), in order to benefit from GATE?s numerous
reusable and multilingual text processing compo-
nents, and also from its infrastructural support for
linguistic standards and diverse input formats.
2.2 Architecture
The architecture of the AnnoMarket PaaS com-
prises of four layers (see Figure 1), combining
20
Figure 1: The AnnoMarket Architecture
components with related capabilities. Addition-
ally, we have identified three aspects, which span
across multiple layers.
The Data Layer is described in Section 2.3, the
Platform Layer ? in Section 2.4, and the Annota-
tion Services ? in Section 2.5.
The fourth, web user interface layer, contains a
number of UI components that allow researchers
to use the AnnoMarket platform in various ways,
e.g. to run an already deployed text annotation ser-
vice on a large dataset, to deploy and share a new
service on the platform, or to upload (and option-
ally share) a document collection (i.e. a corpus).
There is also support for finding relevant services,
deployed on the AnnoMarket platform. Lastly,
due to the platform running on the Amazon cloud
infrastructure, there are account management in-
terfaces, including billing information, payments,
and usage reports.
The first vertical aspect is cloud deployment on
Amazon. This covers support for automatic up and
down-scaling of the allocated Amazon resources,
detection of and recovery from Amazon infras-
tructure failures and network failures, and data
backup.
Usage monitoring and billing is the second
key vertical aspect, since fine-grained pay-as-
you-go ability is essential. Even in the case of
freely-available annotations services, Amazon us-
age charges are incurred and thus such function-
ality is needed. Various usage metrics are mon-
itored and metered so that proper billing can be
guaranteed, including: storage space required by
language resources and data sets; CPU utilisation
of the annotation services; number and size of doc-
uments processed.
Security aspects also have impact on all the lay-
ers of the AnnoMarket platform:
? Data Layer ? data encryption and access con-
trol;
? Platform Layer ? data encryption, authentica-
tion and access control;
? Service layer ? authentication and transport
level encryption;
? User Interface layer ? authentication and
transport level encryption.
In addition, we have implemented a REST pro-
gramming API for AnnoMarket, so that data up-
load and download and running of annotation ser-
vices can all be done automatically, outside of
the web interface. This allows tighter integration
within other applications, as well as support for
synchronous (i.e. document-by-document) calling
of the annotation services.
2.3 The Data Layer
The Data Layer stores various kinds of content,
e.g. crawled web content, users? own corpora (pri-
vate or shared with others), results from running
the annotation services, etc.
Input documents can be in all major formats
(e.g., XML, HTML, JSON, PDF, DOC), based
on GATE?s comprehensive format support. In all
cases, when a document is being processed by An-
noMarket, the format is analysed and converted
into a single unified, graph-based model of an-
notation: the one of the GATE NLP framework
(Cunningham et al, 2002). Then this internal an-
notation format is also used by the collaborative
corpus annotation web tool, and for annotation in-
dexing and search. Annotations produced can be
exported as in-line or stand-off XML, including
XCES (Ide and Romary, 2004).
In implementation terms, Amazon S3 is used to
store content on the platform. S3 provides a REST
service for content access, as well as direct HTTP
access, which provides an easy way for AnnoMar-
ket users to upload and download content.
While stored on the cloud, data is protected by
Amazon?s security procedures. All transfers be-
tween the cloud storage, the annotation services,
and the user?s computer are done via an encrypted
channel, using SSL.
2.4 The Platform Layer
The AnnoMarket platform provides an environ-
ment where text processing applications can be de-
ployed as annotation services on the cloud. It al-
lows processing pipelines that were produced on a
21
Figure 2: Web-based Job Editor
developer?s stand-alone computer to be deployed
seamlessly on distributed hardware resources (the
compute cloud) with the aim of processing large
amounts of data in a timely fashion. This process
needs to be resilient in the face of failures at the
level of the cloud infrastructure, the network com-
munication, errors in the processing pipeline and
in the input data.
The platform layer determines the optimal num-
ber of virtual machines for running a given NLP
application, given the size of the document collec-
tion to be processed and taking into account the
overhead in starting up new virtual machines on
demand. The implementation is designed to be ro-
bust in the face of hardware failures and process-
ing errors. For technical details on the way this
was implemented on Amazon EC2 see (Tablan et
al., 2013).
The GATE plugin-based architecture (Cunning-
ham et al, 2002) is the basis for the platform en-
vironment. Users can upload any pipelines com-
pliant with the GATE Processing Resource (PR)
model and these are automatically deployed as an-
notation services on the AnnoMarket platform.
2.5 Annotation Services
As discussed above, the platform layer in An-
noMarket addresses most of the technical and
methodological requirements towards the NLP
PaaS, making the deployment, execution, and
sharing of annotation services (i.e. pipelines and
algorithms) a straightforward task. From a re-
searcher?s perspective, executing an annotation
service on a dataset involves a few simple steps:
? Upload the document collection to be pro-
cessed or point the system to a shared dataset
on the platform;
? Upload a GATE-based processing pipeline to
be used (or choose an already deployed anno-
tation service);
? Set any required parameter values;
? Press the ?Start? button.
While the job is running, a regularly updated
execution log is made available in the user?s dash-
board. Upon job completion, an email notification
is also sent. Most of the implementation details are
hidden away from the user, who interacts with the
system through a web-based job editor, depicted
in Figure 2, or through a REST API.
The number of already deployed annotation ser-
vices on the platform is growing continuously.
Figure 3 shows a subset of them, as well as the
metadata tags associated with these services, so
that users can quickly restrict which types of ser-
vices they are after and then be shown only the
relevant subset. At the time of writing, there are
services of the following kinds:
? Part-of-Speech-Taggers for English, German,
Dutch, and Hungarian.
? Chunking: the GATE NP and VP chunkers
and the OpenNLP ones;
? Parsing: currently the Stanford Parser 4, but
more are under integration;
? Stemming in 15 languages, via the Snowball
stemmer;
? Named Entity Recognition: in English, Ger-
man, French, Arabic, Dutch, Romanian, and
Bulgarian;
? Biomedical taggers: the PennBio5 and the
AbGene (Tanabe and Wilbur, 2002) taggers;
? Twitter-specific NLP: language detection, to-
kenisation, normalisation, POS tagging, and
4http://nlp.stanford.edu/software/lex-parser.shtml
5http://www.seas.upenn.edu/?strctlrn/BioTagger/BioTagger.html
22
Figure 3: Pre-deployed Text Annotation Services
Figure 4: Creating a New Annotation Service
NER.
The deployment of new annotation services is
done via a web interface (see Figure 4), where an
administrator needs to configure some basic de-
tails related to the utilisation of the platform layer
and provide a self-contained GATE-compatible
application. Platform users can only publish their
own annotation services by contacting an adminis-
trator, who can validate the provided pipeline be-
fore making it publicly available to the other users.
This step is intended to protect the users commu-
nity from malicious or poor quality pipelines.
3 Search and Browsing of Annotated
Corpora
The AnnoMarket platform also includes a service
for indexing and searching over a collection of se-
mantically annotated documents. The output of an
annotation service (see Figure 2) can be fed di-
rectly into a search index, which is created as the
service is run on the documents. This provides fa-
cilities for searching over different views of doc-
ument text, for example one can search the docu-
ment?s words, the part-of-speech of those words,
or their morphological roots. As well as searching
the document text, we also support searches over
the documents? semantic annotations, e.g. named
entity types or semantic roles.
Figure 5 shows a semantic search over 80,000
news web pages from the BBC. They have
first been pre-processed with the POS tagging,
morphological analysis, and NER services on
the platform and the output indexed automat-
ically. The search query is for documents,
where entities of type Person are followed by
any morphological form of the verb say, i.e.
{Person} root:say.
4 Conclusion
This paper described a cloud-based open platform
for text mining, which aims to assist the develop-
ment and deployment of robust, large-scale text
processing applications. By supporting the shar-
ing of annotation pipelines, AnnoMarket alo pro-
23
Figure 5: Example Semantic Search Results
motes reuse and repeatability of experiments.
As the number of annotation services offered by
the platform has grown, we identified a need for
service search, so that users can locate useful NLP
services more effectively. We are currently devel-
oping a new UI, which offers search and brows-
ing functionality, alongside various criteria, such
as functionality (e.g. POS tagger, named entity
recogniser), user ratings, natural language sup-
ported). In the medium- to long-term we have
also planned to support UIMA-based pipelines,
via GATE?s UIMA compatibility layer.
A beta version is currently open to researchers
for experimentation. Within the next six months
we plan to to solicit more shared annotation
pipelines to be deployed on the platform by other
researchers.
Acknowledgments
This work was supported by the European Union
under grant agreement No. 296322 AnnoMarket,6
and a UK EPSRC grant No. EP/I004327/1.
References
Kalina Bontcheva, Hamish Cunningham, Ian Roberts,
Angus. Roberts, Valentin. Tablan, Niraj Aswani, and
Genevieve Gorrell. 2013. GATE Teamware: A
Web-based, Collaborative Text Annotation Frame-
work. Language Resources and Evaluation.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. Gate: an
architecture for development of robust hlt applica-
tions. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, 7?12
July 2002, ACL ?02, pages 168?175, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Hamish Cunningham, Valentin Tablan, Angus Roberts,
and Kalina Bontcheva. 2013. Getting more out of
biomedical documents with gate?s full lifecycle open
6See http://www.annomarket.eu/.
source text analytics. PLoS Computational Biology,
9(2):e1002854, 02.
Marios D Dikaiakos, Dimitrios Katsaros, Pankaj
Mehra, George Pallis, and Athena Vakali. 2009.
Cloud computing: Distributed internet computing
for IT and scientific research. IEEE Internet Com-
puting, 13(5):10?13.
David Ferrucci and Adam Lally. 2004. UIMA: An
Architectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
Eduard Hovy. 2010. Annotation. In Tutorial Abstracts
of ACL.
Nancy Ide and Laurent Romary. 2004. Standards for
language resources. Natural Language Engineer-
ing, 10:211?225.
C. Ramakrishnan, W. A. Baumgartner, J. A. Blake,
G. A. P. C. Burns, K. Bretonnel Cohen, H. Drabkin,
J. Eppig, E. Hovy, C. N. Hsu, L. E. Hunter, T. Ingulf-
sen, H. R. Onda, S. Pokkunuri, E. Riloff, C. Roeder,
and K. Verspoor. 2010. Building the scientific
knowledge mine (SciKnowMine): a community-
driven framework for text mining tools in direct ser-
vice to biocuration. In New Challenges for NLP
Frameworks (NLPFrameworks 2010), LREC 2010,
pages 9?14, Valletta, Malta, May. ELRA.
Valentin Tablan, Ian Roberts, Hamish Cunningham,
and Kalina Bontcheva. 2013. GATECloud.net: a
Platform for Large-Scale, Open-Source Text Pro-
cessing on the Cloud. Philosophical Transactions
of the Royal Society A: Mathematical, Physical &
Engineering Sciences, 371(1983):20120071.
Lorraine Tanabe and W. John Wilbur. 2002. Tag-
ging Gene and Protein Names in Full Text Articles.
In Proceedings of the ACL-02 workshop on Natural
Language Processing in the biomedical domain, 7?
12 July 2002, volume 3, pages 9?13, Philadelphia,
PA. Association for Computational Linguistics.
Bin Zhou, Yan Jia, Chunyang Liu, and Xu Zhang.
2010. A distributed text mining system for online
web textual data analysis. In Cyber-Enabled Dis-
tributed Computing and Knowledge Discovery (Cy-
berC), 2010 International Conference on, pages 1?
4, Los Alamitos, CA, USA, October. IEEE Com-
puter Society.
24
