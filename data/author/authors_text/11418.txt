Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 51?56,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Acquiring High Quality Non-Expert Knowledge from                         
On-demand Workforce 
 
Donghui Feng           Sveva Besana          Remi Zajac  
AT&T Interactive Research 
Glendale, CA, 91203 
{dfeng, sbesana, rzajac}@attinteractive.com 
 
  
 
Abstract 
Being expensive and time consuming, human 
knowledge acquisition has consistently been a 
major bottleneck for solving real problems. In 
this paper, we present a practical framework 
for acquiring high quality non-expert knowl-
edge from on-demand workforce using Ama-
zon Mechanical Turk (MTurk). We show how 
to apply this framework to collect large-scale 
human knowledge on AOL query classifica-
tion in a fast and efficient fashion. Based on 
extensive experiments and analysis, we dem-
onstrate how to detect low-quality labels from 
massive data sets and their impact on collect-
ing high-quality knowledge. Our experimental 
findings also provide insight into the best 
practices on balancing cost and data quality for 
using MTurk. 
1 Introduction 
Human knowledge acquisition is critical for 
training intelligent systems to solve real prob-
lems, both for industry applications and aca-
demic research. For example, many machine 
learning and natural language processing tasks 
require non-trivial human labeled data for super-
vised learning-based approaches. Traditionally 
this has been collected from domain experts, 
which we refer to as expert knowledge. 
However, acquiring in-house expert knowl-
edge is usually very expensive, time consuming, 
and has consistently been a major bottleneck for 
many research problems. For example, tremen-
dous efforts have been put into creating TREC 
corpora (Voorhees, 2003).  
As a result, several research projects spon-
sored by NSF and DARPA aim to construct 
valuable data resources via human labeling; these 
are exemplified by PennTree Bank (Marcus et 
al., 1993), FrameNet (Baker et al, 1998), and 
OntoNotes (Hovy et al, 2006).  
In addition, there are projects such as Open 
Mind Common Sense (OMCS) (Stork, 1999; 
Singh et al, 2002), ISI LEARNER (Chklovski, 
2003), and the Fact Entry Tool by Cycorp (Be-
lasco et al, 2002) where knowledge is gathered 
from volunteers. 
One interesting approach followed by von 
Ahn and Dabbish (2004), applied to image label-
ing on the Web, is to collect valuable input from 
entertained labelers. Turning label acquisition 
into a computer game addresses tediousness, 
which is one of the main reasons that it is hard to 
gather large quantities of data from volunteers.     
More recently researchers have begun to ex-
plore approaches for acquiring human knowl-
edge from an on-demand workforce such as 
Amazon Mechanical Turk1. MTurk is a market-
place for jobs that require human intelligence. 
There has been an increase in demand for 
crowdsourcing prompted by both the academic 
community and industry needs. For instance, 
Microsoft/Powerset uses MTurk for search rele-
vance evaluation and other companies are lever-
aging turkers to clean their data sources. 
However, while it is cheap and fast to obtain 
large-scale non-expert labels using MTurk, it is 
still unclear how to leverage its capability more 
efficiently and economically to obtain sufficient 
useful and high-quality data for solving real 
problems.  
In this paper, we present a practical frame-
work for acquiring high quality non-expert 
knowledge using MTurk. As a case study we 
have applied this framework to obtain human 
classifications on AOL queries (determining 
whether a query might be a local search or not). 
Based on extensive experiments and analysis, we 
show how to detect bad labelers/labels from 
massive data sets and how to build high-quality 
labeling sets. Our experiments also provide in-
                                                 
1 Amazon Mechanical Turk:  http://www.mturk.com/ 
51
sight into the best practices for balancing cost 
and data quality when using MTurk. 
The remainder of this paper is organized as 
follows: In Section 2, we review related work 
using MTurk. We describe our methodology in 
Section 3 and in Section 4 we present our ex-
perimental results and further analysis. In Sec-
tion 5 we draw conclusions and discuss our plans 
for future work. 
2 Related Work 
It is either infeasible or very time and cost con-
suming to acquire in-house expert human knowl-
edge. To obtain valuable human knowledge (e.g., 
in the format of labeled data), many research 
projects in the natural language community have 
been funded to create large-scale corpora and 
knowledge bases, such as PenTreeBank (Marcus 
et al, 1993), FrameNet (Baker et al, 1998), 
PropBank (Palmer et al, 2005), and OntoNotes 
(Hovy et al, 2006). 
MTurk has been attracting much attention 
within several research areas since its release. Su 
et al (2007) use MTurk to collect large-scale 
review data. Kaisser and Lowe (2008) report 
their work on generating research collections of 
question-answering pairs using MTurk. Sorokin 
and Forsyth (2008) outsource image-labeling 
tasks to MTurk. Kittur et al (2008) use MTurk 
as the paradigm for user studies. In the natural 
language community Snow et al (2008) report 
their work on collecting linguistic annotation for 
a variety of natural language tasks including 
word sense disambiguation, word similarity, and 
textual entailment recognition. 
However, most of the reported work focuses 
on how to apply data collected from MTurk to 
their applications. In our work, we concentrate 
on presenting a practical framework for using 
MTurk by separating the process into a valida-
tion phase and a large-scale submission phase.  
By analyzing workers? behavior and their data 
quality, we investigate how to detect low-quality 
labels and their impact on collected human 
knowledge; in addition, during the validation 
step we study how to best use MTurk to balance 
payments and data quality. Although our work is 
based on the submission of a classification task, 
the framework and approaches can be adapted 
for other types of tasks. 
In the next section, we will discuss in more 
detail our practical framework for using MTurk. 
3 Methodology 
3.1 Amazon Mechanical Turk 
Amazon launched their MTurk service in 2005. 
This service was initially used for internal pro-
jects and eventually fulfilled the demand for us-
ing human intelligence to perform various tasks 
that computers currently cannot do or do very 
well.  
MTurk users naturally fall into two roles: a re-
quester and a turker. As a requester, you can de-
fine your Human Intelligent Tasks (HITs), de-
sign suitable templates, and submit your tasks to 
be completed by turkers. A turker may choose 
from HITs that she is eligible to work on and get 
paid after the requester approves her work. The 
work presented in this paper is mostly from the 
perspective of a requester. 
3.2 Key Issues 
While it is quite easy to start using MTurk, re-
questers have to confront the following: how can 
we obtain sufficient useful and high-quality data 
for solving real problems efficiently and eco-
nomically?  
In practice, there are three key issues to con-
sider when answering this question. 
Key Issues Description 
Data    
Quality 
Is the labeled data good enough for 
practical use? 
Cost What is the sweet spot for payment? 
Scale How efficiently can MTurk be used 
when handling large-scale data sets? 
Can the submitted job be done in a 
timely manner?  
Table 1. Key issues for using MTurk. 
Requesters want to obtain high-quality data on 
a large scale without overpaying turkers. Our 
proposed framework will address these key is-
sues.  
3.3 Approaches 
Since not all tasks collecting non-expert knowl-
edge share the same characteristics and suitable 
applications, there is not a one-size-fits-all solu-
tion as the best practice when using MTurk.  
In our approach, we divide the process into 
two phases:  
? Validation Phase.  
? Large-scale Submission Phase.  
The first phase gives us information used to 
determine if MTurk is a valid approach for a 
given problem and what the optimal parameters 
for high quality and a short turn-around time are. 
52
We have to determine the right cost for the task 
and the optimal number of labels. We empiri-
cally determine these parameters with an MTurk 
submission using a small amount of data. These 
optimal parameters are then used for the large-
scale submission phase.  
Most data labeling tasks require subjective 
judgments. One cannot expect labeling results 
from different labelers to always be the same. 
The degree of agreement among turkers varies 
depending on the complexity and ambiguity of 
individual tasks. Typically we need to obtain 
multiple labels for each HIT by assigning multi-
ple turkers to the same task. 
Researchers mainly use the following two 
quantitative measures to assess inter-agreement: 
observed agreement and kappa statistics.  
P(A) is the observed agreement among anno-
tators. It represents the portion where annotators 
produce identical labels. This is very natural and 
straightforward. However, people argue this may 
not necessarily reflect the exact degree of agree-
ment due to chance agreement.  
P(E)  is the hypothetical probability of chance 
agreement. In other words, P(E)  represents the 
degree of agreement if both annotators conduct 
annotations randomly (according to their own 
prior probability).  
We can also use the kappa coefficient as a 
quantitative measure of inter-person agreement. 
It is a commonly used measure to remove the 
effect of chance agreement. It was first intro-
duced in statistics (Cohen, 1960) and has been 
widely used in the language technology commu-
nity, especially for corpus-driven approaches 
(Carletta, 1996; Krippendorf, 1980). Kappa is 
defined with the following equation:  
kappa = P(A) ? P(E)
1? P(E)  
Generally it is viewed more robust than ob-
served agreement P(A)  because it removes 
chance agreement P(E) . 
DetectOutlier( P) 
    for each turker p ? P  
collect the label set L  from p 
for each label l ? L  
    /* compared with others? majority voting */ 
    compute its agreement with others 
compute P(A) p  (or kappa p ) 
    analyze the distribution of P(A) 
    return outlier turkers 
Figure 1. Outlier detection algorithm. 
We use these measures to automatically detect 
outlier turkers producing low-quality results. 
Figure 1 shows our algorithm for automatically 
detecting outlier turkers.  
4 Experiments 
Based on our proposed framework and ap-
proaches, as a case study we conducted experi-
ments on a classification task using MTurk.  
The classification task requires the turker to 
determine whether a web query is a local search 
or not. For example, is the user typing this query 
looking for a local business or not? The labeled 
data set can be used to train a query classifier for 
a web search system. 
This capability will make search systems able 
to distinguish local search queries from other 
types of queries and to apply specific search al-
gorithms and data resources to better serve users? 
information needs.  
For example, if a person types ?largest biomed 
company in San Diego? and the web search sys-
tems can recognize this query as a local search 
query, it will apply local search algorithms on 
listing data instead of or as well as generating a 
general web search request.  
4.1 Validation Phase 
We downloaded the publicly available AOL 
query log2 and used this as our corpus. We first 
scanned all queries with geographic locations 
(including states, cities, and neighborhoods) and 
then randomly selected a set of queries for our 
experiments. 
For the validation phase, 700 queries were 
first labeled in-house by domain experts and we 
refer to this set as expert labels. To obtain the 
optimal parameters including the desired number 
of labels and payment price, we designed our 
HITs and experiments in the following way:  
We put 10 queries into one HIT, requested 15 
labels for each query/HIT, and varied payment 
for each HIT in four separate runs. Our payments 
include $0.01, $0.02, $0.05, and $0.10 per HIT. 
The goal is to have HITs completed in a timely 
fashion and have them yield high-quality data.  
We submitted our HITs to MTurk in four dif-
ferent runs with the following prices: $0.01, 
$0.02, $0.03, and $0.10. According to our pre-
defined evaluation measures and our outlier de-
tection algorithm, we investigated how to obtain 
the optimal parameters. Figure 2. shows the task 
completion statistics for the four different runs. 
                                                 
2 AOL Log Data: http://www.gregsadetsky.com/aol-data/ 
53
 
Figure 2. Task completion statistics. 
As shown in Figure 2, with the increase of 
payments, the average hourly rate increases from 
$0.72 to $9.73 and the total turn-around time 
dramatically decreases from more than 47 hours 
to about 1.5 hours. In the meantime, people tend 
to become more focused on the tasks and spend 
less time per HIT. 
In addition, as we increase payment, more 
people tend to stay with the task and take it more 
seriously as evidenced by the quality of the la-
beled data. This results in fewer numbers of 
workers overall as well as fewer outliers as 
shown in Figure 3.  
 
Figure 3. Total number of workers and outliers. 
We investigate two types of agreements, inter-
turker agreement and agreement between turkers 
and our in-house experts. For inter non-expert 
agreements, we compute each turker?s agreement 
with all others? majority voting results.  
Payment 
(USD) 0.01 0.02 0.05 0.10 
Median of 
inter-
turker 
agreement 0.8074 0.8583 0.9346 0.9028 
Table 2. Median of inter-turker agreements. 
As in our outlier detection algorithm, we ana-
lyzed the distribution of inter-turker agreements. 
Table 2 shows the median values of inter-turker 
agreement as we vary the payment prices. The 
median value keeps on increasing when the price 
increases from $0.01, to $0.02 and $0.05. How-
ever, it drops as the price increases from $0.05 to 
$0.10. This implies that turkers do not necessar-
ily improve their work quality as they get paid 
more. One of the possible explanations for this 
phenomenon is that when the reward is high 
people tend to work towards completing the task 
as fast as possible instead of focusing on submit-
ting high-quality data. This trend may be intrin-
sic to the task we have submitted and further ex-
periments will show if this turker behavior is 
task-independent.    
 
Figure 4. Agreement with experts. 
 
Figure 5. Inter non-expert agreement. 
We also analyzed agreement between non-
experts and experts. Figure 4 depicts the trend of 
the agreement scores with the increase of number 
of labels and payments. For example, given 
seven labels per query, in the experiment with 
the $0.05 payment, the majority voting of non-
expert labels has an agreement of 0.9465 with 
expert labeling. As explained earlier we do not 
necessarily obtain the best data qual-
ity/agreement with the $0.10 payment. Instead, 
we get the highest agreement with the $0.05 
payment. We have determined this rate to be the 
54
sweet spot in terms of cost. Also, seven labels 
per query produce a very high agreement with no 
further significant improvement when we in-
crease the number of labels.  
For inter non-expert agreements, we found 
similar trends in terms of different payments and 
number of labels as shown in Figure 5. 
As mentioned above, our algorithm is able to 
detect turkers producing low-quality data. One 
natural question is: how will their labels affect 
the overall data quality? 
We studied this problem in two different 
ways. We evaluated the data quality by removing 
either all polluted queries or only outliers? labels. 
Here polluted queries refer to those queries re-
ceiving at least one label from outliers. By re-
moving polluted queries, we only investigate the 
clean data set without any outlier labels. The 
other alternative is to only remove outliers? la-
bels for specific queries but others? labels for 
those queries will be kept. Both the agreement 
between experts and non-experts and inter-non-
experts agreement show similar trends: data 
quality without outliers? labels is slightly better 
since there is less noise. However, as outliers? 
labels may span a large number of queries, it 
may not be feasible to remove all polluted que-
ries. For example, in one of our experiments, 
outliers? labels pollute more than half of all the 
records. We cannot simply remove all the queries 
with outliers? labels due to consideration of cost.  
On the other hand, the effect of outliers? labels 
is not that significant if a certain number of re-
quested labels per query are collected. As shown 
in Figure 6, noisy data from outliers can be over-
ridden by assigning more labelers. 
 
Figure 6. Agreement with Experts (removing 
outliers? labels (payment = $0.05)).  
From the validation phase of the query classi-
fication task, we determine that the optimal pa-
rameters are paying $0.05 per HIT and request-
ing seven labels per query. Given this number of 
labels, the effect of outliers? labels can be over-
ridden for the final result.  
4.2 Large-scale Submission Phase 
Having obtained the optimal parameters from the 
validation phase, we are then ready to make a 
large-scale submission.  
For this phase, we paid $0.05 per HIT and re-
quested seven labels per query/HIT. Following 
similar filtering and sampling approaches as in 
the validation phase, we selected 22.5k queries 
from the AOL search log. Table 3 shows the de-
tected outliers for this large-scale submission.  
Total Number of Turkers 228 
Number of Outlier Turkers 23 
Outlier Ratio 10.09% 
Table 3. Number of turkers and outliers. 
Based on the distribution of inter-turker 
agreement, any turkers with agreement less than 
0.6501 are recognized as outliers. For a total 
number of 15,750 HITs, 228 turkers contributed 
to the labeling effort and 10.09% of them were 
recognized as outliers.  
Table 4 shows the number of labels from the 
outliers and the approval ratio of collected data. 
About 10.08% of labels are from outlier turkers 
and rejected.  
Total Number of Labels 157,500 
Number of Outlier Labels 15,870 
Approval Ratio 89.92% 
Table 4. Total number of labels. 
We have experimented using MTurk for a web 
query classification task. With learned optimal 
parameters from the validation phase, we col-
lected large-scale high-quality non-expert labels 
in a fast and economical way. These data will be 
used to train query classifiers to enhance web 
search systems handling local search queries. 
5 Conclusions and Future Work 
In this paper, we presented a practical framework 
for acquiring high quality non-expert knowledge 
from an on-demand and scalable workforce. Us-
ing Amazon Mechanical Turk, we collected 
large-scale human classification knowledge on 
web search queries.  
To learn the best practices when using MTurk, 
we presented a two-phase approach, a validation 
phase and a large-scale submission phase. We 
conducted extensive experiments to obtain the 
optimal parameters on the number of labelers 
and payments in the validation phase. We also 
presented an algorithm to automatically detect 
55
outlier turkers based on the agreement analysis 
and investigated the effect of removing an inac-
curately labeled set.  
Acquiring high-quality human knowledge will 
remain a major concern and a bottleneck for in-
dustry applications and academic problems. Un-
like traditional ways of collecting in-house hu-
man knowledge, MTurk provides an alternative 
way to acquire non-expert knowledge. As shown 
in our experiments, given appropriate quality 
control, we have been able to acquire high-
quality data in a very fast and efficient way. We 
believe MTurk will attract more attention and 
usage in broader areas. 
In the future, we are planning to investigate 
how this framework can be applied to different 
types of human knowledge acquisition tasks and 
how to leverage large-scale labeled data sets for 
solving natural language processing problems.  
References  
Baker, C.F., Fillmore, C.J., and Lowe, J.B. 1998. The 
Berkeley FrameNet Project. In Proc. of COLING-
ACL-1998.  
Belasco, A., Curtis, J., Kahlert, R., Klein, C., Mayans, 
C., and Reagan, P. 2002. Representing Knowledge 
Gaps Effectively. In Practical Aspects of Knowl-
edge Management, (PAKM).  
Carletta, J. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics. 22(2):249?254. 
Chklovski, T. 2003. LEARNER: A System for Ac-
quiring Commonsense Knowledge by Analogy. In 
Proc. of Second International Conference on 
Knowledge Capture (KCAP 2003).  
Cohen, J. 1960. A coefficient of agreement for nomi-
nal scales. Educational and Psychological Meas-
urement. Vol.20, No.1, pp.37-46.  
Colowick, S.M. and Pool, J. 2007. Disambiguating for 
the web: a test of two methods. In Proc. of the 4th 
international Conference on Knowledge Capture 
(K-CAP 2007).  
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., and 
Weischedel, R. 2006. OntoNotes: The 90% Solu-
tion. In Proc. of HLT-NAACL-2006.  
Kaisser, M. and Lowe, J.B. 2008. Creating a Research 
Collection of Question Answer Sentence Pairs with 
Amazon's Mechanical Turk. In Proc. of the Fifth 
International Conference on Language Resources 
and Evaluation (LREC-2008).  
Kittur, A., Chi, E. H., and Suh, B. 2008. Crowdsourc-
ing user studies with Mechanical Turk. In Proc. of 
the 26th Annual ACM Conference on Human Fac-
tors in Computing Systems (CHI-2008). 
Krippendorf, K. 1980. Content Analysis: An introduc-
tion to its methodology. Sage Publications.  
Marcus, M., Marcinkiewicz, M.A., and Santorini, B. 
1993. Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics. 
19:2, June 1993.  
Nakov, P. 2008. Paraphrasing Verbs for Noun Com-
pound Interpretation. In Proc. of the Workshop on 
Multiword Expressions (MWE-2008).  
Palmer, M., Gildea, D., and Kingsbury, P. 2005. The 
Proposition Bank: A Corpus Annotated with Se-
mantic Roles. Computational Linguistics. 31:1.  
Sheng, V.S., Provost, F., and Ipeirotis, P.G. 2008. Get 
another label? improving data quality and data 
mining using multiple, noisy labelers. In Proc. of 
the 14th ACM SIGKDD international Conference 
on Knowledge Discovery and Data Mining (KDD-
2008).  
Singh, P., Lin, T., Mueller, E., Lim, G., Perkins, T., 
and Zhu, W. 2002. Open Mind Common Sense: 
Knowledge acquisition from the general public. In 
Meersman, R. and Tari, Z. (Eds.), LNCS: Vol. 
2519. On the Move to Meaningful Internet Sys-
tems: DOA/CoopIS/ODBASE (pp. 1223-1237). 
Springer-Verlag.  
Snow, R., O?Connor, B., Jurafsky, D., and Ng, A.Y. 
2008. Cheap and Fast ? But is it Good? Evaluating 
Non-Expert Annotations for Natural Language 
Tasks . In Proc. of EMNLP-2008.  
Sorokin, A. and Forsyth, D. 2008. Utility data annota-
tion with Amazon Mechanical Turk. In Proc. of the 
First IEEE Workshop on Internet Vision at CVPR-
2008.  
Stork, D.G. 1999. The Open Mind Initiative. IEEE 
Expert Systems and Their Applications. pp. 16-20, 
May/June 1999.  
Su, Q., Pavlov, D., Chow, J., and Baker, W.C. 2007. 
Internet-scale collection of human-reviewed data. 
In Proc. of the 16th international Conference on 
World Wide Web (WWW-2007).  
Von Ahn, L. and Dabbish, L. 2004. Labeling Images 
with a Computer Game. In Proc. of ACM Confer-
ence on Human Factors in Computing Systmes 
(CHI). pp. 319-326. 
Voorhees, E.M. 2003. Overview of TREC 2003. In 
Proc. of TREC-2003.  
56
Coling 2010: Demonstration Volume, pages 17?20,
Beijing, August 2010
Have2eat: a Restaurant Finder with Review Summarization
for Mobile Phones
Giuseppe Di Fabbrizio and Narendra Gupta
AT&T Labs - Research, Inc.
{pino,ngupta}@research.att.com {sbesana,pmani}@attinteractive.com
Sveva Besana and Premkumar Mani
AT&T Interactive - Applied Research
Abstract
Have2eat is a popular mobile application
available for iPhone and Android-based de-
vices that helps users to find and assess
nearby restaurants. It lists restaurants lo-
cated around the device and provides a quick
highlight about the opinions expressed by
online reviewers. Have2eat summarizes tex-
tual reviews by extracting relevant sentences
and by automatically generating detailed rat-
ings about specific aspects of the restaurant.
A compact one-screen digest allows users to
quickly access the information they need, ex-
pand to full review pages, and report their ex-
perience online by entering ratings and com-
ments.
1 Introduction
Bloggers, professional reviewers, and consumers
continuously create opinion-rich web reviews about
products and services, with the result that textual re-
views are now abundant on the web and often con-
vey a useful overall rating. However, an overall rat-
ing cannot express the multiple or conflicting opin-
ions that might be contained in the text and screen-
ing the content of a large number of reviews could
be a daunting task. For example, a restaurant might
receive a great evaluation overall, while the service
might be rated below-average due to slow and dis-
courteous wait staff. Pinpointing opinions in doc-
uments, and the entities being referenced, would
provide a finer-grained sentiment analysis and bet-
ter summarize users? opinions. In addition, select-
ing salient sentences from the reviews to textually
summarize opinions would add useful details to con-
sumers that are not expressed by numeric ratings.
This is especially true for so-called road warriors and
mobile users ?on the run? who are often dealing with
limited time and display real estate in searching for a
restaurant to make a decision.
Have2eat1 is a popular2 mobile application avail-
able for iPhone and Android-based devices that ad-
dresses these challenges. Have2eat uses the geo-
location information either from the GPS device or
explicitly entered by the user to produce a list of
restaurants sorted by distance and located within a
specific radius from the originating location. In addi-
tion, when restaurant reviews are available, a compact
one-screen digest displays a summary of the reviews
posted on the web by other customers. Customers
can expand to read a full review page and also enter
their own ratings, comments and feedback. The re-
view summaries are visualized on the mobile screen:
? graphically by thumbs-up (positive reviews)
and thumbs-down (negative reviews) for differ-
ent aspects of the restaurant;
? textually by a few sentences selected from re-
view texts that best summarize the opinions
about various aspects of the restaurant expressed
in the reviews;
Extracting opinions from text presents many nat-
ural language processing challenges. Prior work on
sentiment analysis has been focusing on binary clas-
sification of positive and negative opinions (Turney,
2002; Pang et al, 2002; Yu and Hatzivassiloglou,
2003), while aspect rating inference (e.g., the task
of determining the opinion polarity in a multi-point
scale) has been previously analyzed in Pang and
Lee (2005); Goldberg and Zhu (2006); Leung et al
(2006). More recently, Snyder and Barzilay (2007);
Shimada and Endo (2008) extended the inference
process to multi-aspect ratings where reviews include
numerical ratings from mutually dependent aspects.
Snyder and Barzilay (2007) shows that modeling the
dependencies between aspect ratings in the same re-
views helps to reduce the rank-loss (Crammer and
Singer, 2001).
1www.have2eat.com
2More than 400,000 downloads to-date for the iPhone
version alone
17
There are similar mobile applications obtainable
either on the Apple iPhone App Store or as web-
based mobile application, such as Zagat3, UrbanS-
poon4, YP Mobile5, and Yelp6, but, to the extent of
our knowledge, most of them are only focused on
finding the restaurant location based on proximity
and some restaurant filtering criterion. When avail-
able, restaurant reviews are simply visualized as con-
tiguous list of text snippets with the overall experi-
ence rating. None of the listed applications include
extended rating predictions and reviews summariza-
tion.
2 System Description
The have2eat system architecture is composed of two
parts: 1) predictive model training ? illustrated in Fig-
ure 1 and described in section 2.1, and 2) graphical
and textual summarization ? shown in Figure 2 and
described in section 2.2.
2.1 Graphical summarization by thumbs
up/down
The majority of textual reviews available online are
accompanied by a single overall rating of the restau-
rant. To predict consistent ratings for different as-
pects, namely food, service, atmosphere, value, and
overall experience, we use machine learning tech-
niques to train predictive models, one for each as-
pect; see Figure 1. More specifically, we used ap-
proximately 6,000 restaurant reviews scraped from a
restaurant review website7. On this website, besides
textual reviews, users have also provided numerical
ratings for the five aspects mentioned above. Ratings
are given on a scale of 1 to 5, 1 being poor and 5
excellent. We experimented with different regression
and classification models using a host of syntactic and
semantic features. We evaluated these models using
rank-loss metrics which measure the average differ-
ence between predicted and actual ratings. We found
that a maximum entropy (Nigam et al, 1999) model
combined with a re-ranking method that keeps in con-
sideration the interdependence among aspect ratings,
provided the best predictive model with an average
rank-loss of 0.617 (Gupta et al, 2010). This results
is better than previous work on the same task as de-
scribed in Snyder and Barzilay (2007).
To cope with the limited real estate on mobile
phones for displaying and allowing users to input
their opinions, the predicted ratings were mapped
onto thumbs?up and thumbs?down. For each restau-
3mobile.zagat.com
4www.urbanspoon.com
5m.yp.com
6m.yelp.com
7www.we8there.com
rant the proportion of reviews with rating of 1 and 2
was considered thumbs down and ratings of 4 and 5
were mapped to thumbs up. Table 1 shows an exam-
ple of this mapping.
Reviews Thumbs
a b c Up Down
Atmosphere 3 2 4 50% 50%
Food 4 4 5 100% 0
Value 3 2 4 50% 50%
Service 5 5 5 100% 0
Overall 4 4 5 100% 0
Table 1: Mapping example between ratings and
thumbs up/down. Ratings of 3 are considered neutral
and ignored in this mapping
2.2 Textual summaries by sentence selection
Figure 2 shows how summary sentences are selected
from textual reviews. As described in the previous
section, we trained predictive models for each aspect
of the restaurant. To select summary sentences we
split the review text into sentences8. Using the pre-
dictive models and iterating over the restaurant list-
ings, sentences in the reviews are classified by aspect
ratings and confidence score. As a result, for each
sentence we get 5 ratings and confidence scores for
those ratings. We then select a few sentences that
have extreme ratings and high confidence and present
them as summary text.
We evaluated these summaries using the following
metrics.
1. Aspect Accuracy: How well selected sentences
represent the aspect they are supposed to.
2. Coverage: How many of the aspects present in
the textual reviews are represented in the se-
lected sentences.
8For this purpose we used a sentence splitter based on
statistical models which besides n-grams also uses word
part-of-speech as features. This sentence splitter was
trained on email data and is 97% accurate.
Figure 1: Predictive model training
18
Figure 2: Graphical and textual summarization
3. Rating Consistency: How consistent the se-
lected sentences with the summarizing aspect
ratings are.
4. Summary quality: Subjective human judgments
as to how good the summaries are and automatic
multi-document summarization to how good the
summaries are compared to a manually created
GOLD standard using ROUGE-based (Lin, 2004)
metrics.
A detailed description of the summarization task
evaluation will be published elsewhere.
3 Demonstration
When launching the application, users are presented
with a list of twenty nearby restaurants. The user can
browse more restaurants by tapping on a link at the
bottom of the page. For each listing we show the dis-
tance from the current location and, if available, we
provide a thumbs-up or thumbs-down, price informa-
tion and the summary sentence with the highest confi-
dence score across aspects. Figure 3 shows an exam-
ple of the List page. If users want a list of restaurants
for a different location they can tap the Change but-
ton at the top of the page. This action will bring up
the Location page where the user can enter city and
state and/or a street address.
Users can select a restaurant in the list to view the
details, see Figure 4. Details include address, phone
number and thumbs up/down for the overall, food,
service, value and atmosphere aspects. The user can
provide feedback by tapping on the thumbs-up or
thumbs-down buttons, as well as by leaving a com-
ment at the bottom of the screen. This page also in-
cludes a few summary sentences with extreme ratings
and high confidence scores. An example of selected
sentences with their polarity is shown in Table 2. By
tapping on any of the sentences the users can view
the full text of the review from which the sentence
was selected. Users can also add a new restaurant by
tapping the Add icon in the tab bar.
Figure 3: Have2eat listings screen shot on iPhone
Figure 5 displays the review selected in the Details
page along with any other reviews which exist for the
restaurant. Users can give feedback on whether they
found the review helpful or not by using a thumbs-up
or a thumbs-down respectively. Users can also add a
review by tapping on a link at the bottom of the page.
4 Conclusion
This demonstration has shown a restaurant finder ap-
plication for mobile phones, which makes use of
summarization techniques to predict aspect ratings
from review text and select salient phrases express-
ing users? opinions about specific restaurant aspects.
Users can directly contribute with their feedback by
tapping on the aspect thumbs buttons or by directly
typing comments.
19
Figure 4: Have2eat automatically predicted aspect
ratings and summary
Restaurant 1 (3 reviews)
+ The soups are GREAT! Everything that we have ever ordered has
exceeded the ex...
+ Delivery is prompt and credit cards are welcome
+ Their chicken fried rice is the second best in Southern California.
Restaurant 2 (8 reviews)
+ Great tasting burgers, friendly fast service!
+ The inside is warm and even though the chairs looked uncomfort-
able, they were not at all.
- Too many other places to try to worry about getting mediocre food
as a high price.
Restaurant 3 (4 reviews)
+ The salads are tasty, the breadsticks are to die for.
- We waited approximate 10 more minutes and then asked how
much longer.
+ A fun place to go with faimily or a date.
+ If you like salt then this is the place to go, almost everything is full
of s...
Table 2: Example of extracted summaries
Acknowledgments
We thank Jay Lieske, Kirk Boydston, Amy Li, Gwen
Christian, and Remi Zajac for their contributions and
great enthusiasm.
References
Crammer, Koby and Yoram Singer. 2001. Pranking with
ranking. In Thomas G. Dietterich, Suzanna Becker, and
Zoubin Ghahramani, editors, Neural Information Pro-
cessing Systems: Natural and Synthetic (NIPS). MIT
Press, Vancouver, British Columbia, Canada, pages
641?647.
Goldberg, Andrew B. and Jerry Zhu. 2006. Seeing
stars when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
TextGraphs: HLT/NAACL Workshop on Graph-based
Algorithms for Natural Language Processing.
Gupta, Narendra, Giuseppe Di Fabbrizio, and Patrick
Haffner. 2010. Capturing the stars: Predicting ratings
for service and product reviews. In Proceedings of the
HLT-NAACL Workshop on Semantic Search (Semantic-
Search 2010). Los Angeles, CA, USA.
Figure 5: Have2eat reviews
Leung, Cane Wing-ki, Stephen Chi-fai Chan, and Fu-lai
Chung. 2006. Integrating collaborative filtering and sen-
timent analysis: A rating inference approach. In Pro-
ceedings of The ECAI 2006 Workshop on Recommender
Systems. Riva del Garda, I, pages 62?66.
Lin, Chin-Yew. 2004. ROUGE: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out. page 10.
Nigam, Kamal, John Lafferty, and Andrew Mccallum.
1999. Using maximum entropy for text classification.
In IJCAI-99 Workshop on Machine Learning for Infor-
mation Filtering. pages 61?67.
Pang, Bo and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with re-
spect to rating scales. In Proceedings of the Association
for Computational Linguistics (ACL). pages 115?124.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Pro-
cessing (EMNLP). pages 79?86.
Shimada, Kazutaka and Tsutomu Endo. 2008. Seeing
several stars: A rating inference task for a document
containing several evaluation criteria. In Advances in
Knowledge Discovery and Data Mining, 12th Pacific-
Asia Conference, PAKDD 2008. Springer, Osaka, Japan,
volume 5012 of Lecture Notes in Computer Science,
pages 1006?1014.
Snyder, Benjamin and Regina Barzilay. 2007. Multiple
aspect ranking using the Good Grief algorithm. In
Proceedings of the Joint Human Language Technol-
ogy/North American Chapter of the ACL Conference
(HLT-NAACL). pages 300?307.
Turney, Peter. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification of
reviews. In Proceedings of the Association for Compu-
tational Linguistics (ACL). pages 417?424.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
20
