Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923?931,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Better Word Alignments with Supervised ITG Models
Aria Haghighi, John Blitzer, John DeNero and Dan Klein
Computer Science Division, University of California at Berkeley
{ aria42,blitzer,denero,klein }@cs.berkeley.edu
Abstract
This work investigates supervised word align-
ment methods that exploit inversion transduc-
tion grammar (ITG) constraints. We con-
sider maximum margin and conditional like-
lihood objectives, including the presentation
of a new normal form grammar for canoni-
calizing derivations. Even for non-ITG sen-
tence pairs, we show that it is possible learn
ITG alignment models by simple relaxations
of structured discriminative learning objec-
tives. For efficiency, we describe a set of prun-
ing techniques that together allow us to align
sentences two orders of magnitude faster than
naive bitext CKY parsing. Finally, we intro-
duce many-to-one block alignment features,
which significantly improve our ITG models.
Altogether, our method results in the best re-
ported AER numbers for Chinese-English and
a performance improvement of 1.1 BLEU over
GIZA++ alignments.
1 Introduction
Inversion transduction grammar (ITG) con-
straints (Wu, 1997) provide coherent structural
constraints on the relationship between a sentence
and its translation. ITG has been extensively
explored in unsupervised statistical word align-
ment (Zhang and Gildea, 2005; Cherry and
Lin, 2007a; Zhang et al, 2008) and machine
translation decoding (Cherry and Lin, 2007b;
Petrov et al, 2008). In this work, we investigate
large-scale, discriminative ITG word alignment.
Past work on discriminative word alignment
has focused on the family of at-most-one-to-one
matchings (Melamed, 2000; Taskar et al, 2005;
Moore et al, 2006). An exception to this is the
work of Cherry and Lin (2006), who discrim-
inatively trained one-to-one ITG models, albeit
with limited feature sets. As they found, ITG
approaches offer several advantages over general
matchings. First, the additional structural con-
straint can result in superior alignments. We con-
firm and extend this result, showing that one-to-
one ITG models can perform as well as, or better
than, general one-to-one matching models, either
using heuristic weights or using rich, learned fea-
tures.
A second advantage of ITG approaches is that
they admit a range of training options. As with
general one-to-one matchings, we can optimize
margin-based objectives. However, unlike with
general matchings, we can also efficiently com-
pute expectations over the set of ITG derivations,
enabling the training of conditional likelihood
models. A major challenge in both cases is that
our training alignments are often not one-to-one
ITG alignments. Under such conditions, directly
training to maximize margin is unstable, and train-
ing to maximize likelihood is ill-defined, since the
target algnment derivations don?t exist in our hy-
pothesis class. We show how to adapt both margin
and likelihood objectives to learn good ITG align-
ers.
In the case of likelihood training, two innova-
tions are presented. The simple, two-rule ITG
grammar exponentially over-counts certain align-
ment structures relative to others. Because of this,
Wu (1997) and Zens and Ney (2003) introduced a
normal form ITG which avoids this over-counting.
We extend this normal form to null productions
and give the first extensive empirical comparison
of simple and normal form ITGs, for posterior de-
coding under our likelihood models. Additionally,
we show how to deal with training instances where
the gold alignments are outside of the hypothesis
class by instead optimizing the likelihood of a set
of minimum-loss alignments.
Perhaps the greatest advantage of ITG mod-
els is that they straightforwardly permit block-
923
structured alignments (i.e. phrases), which gen-
eral matchings cannot efficiently do. The need for
block alignments is especially acute in Chinese-
English data, where oracle AERs drop from 10.2
without blocks to around 1.2 with them. Indeed,
blocks are the primary reason for gold alignments
being outside the space of one-to-one ITG align-
ments. We show that placing linear potential func-
tions on many-to-one blocks can substantially im-
prove performance.
Finally, to scale up our system, we give a com-
bination of pruning techniques that allows us to
sum ITG alignments two orders of magnitude
faster than naive inside-outside parsing.
All in all, our discriminatively trained, block
ITG models produce alignments which exhibit
the best AER on the NIST 2002 Chinese-English
alignment data set. Furthermore, they result in
a 1.1 BLEU-point improvement over GIZA++
alignments in an end-to-end Hiero (Chiang, 2007)
machine translation system.
2 Alignment Families
In order to structurally restrict attention to rea-
sonable alignments, word alignment models must
constrain the set of alignments considered. In this
section, we discuss and compare alignment fami-
lies used to train our discriminative models.
Initially, as in Taskar et al (2005) and Moore
et al (2006), we assume the score a of a potential
alignment a) decomposes as
s(a) = ?
(i,j)?a
sij +
?
i/?a
si +
?
j /?a
sj (1)
where sij are word-to-word potentials and si and
sj represent English null and foreign null poten-
tials, respectively.
We evaluate our proposed alignments (a)
against hand-annotated alignments, which are
marked with sure (s) and possible (p) alignments.
The alignment error rate (AER) is given by,
AER(a, s,p) = 1? |a ? s|+ |a ? p||a|+ |s|
2.1 1-to-1 Matchings
The class of at most 1-to-1 alignment match-
ings, A1-1, has been considered in several works
(Melamed, 2000; Taskar et al, 2005; Moore et al,
2006). The alignment that maximizes a set of po-
tentials factored as in Equation (1) can be found
in O(n3) time using a bipartite matching algo-
rithm (Kuhn, 1955).1 On the other hand, summing
over A1-1 is #P -hard (Valiant, 1979).
Initially, we consider heuristic alignment poten-
tials given by Dice coefficients
Dice(e, f) = 2CefCe + Cf
where Cef is the joint count of words (e, f) ap-
pearing in aligned sentence pairs, and Ce and Cf
are monolingual unigram counts.
We extracted such counts from 1.1 million
French-English aligned sentence pairs of Hansards
data (see Section 6.1). For each sentence pair in
the Hansards test set, we predicted the alignment
from A1-1 which maximized the sum of Dice po-
tentials. This yielded 30.6 AER.
2.2 Inversion Transduction Grammar
Wu (1997)?s inversion transduction grammar
(ITG) is a synchronous grammar formalism in
which derivations of sentence pairs correspond to
alignments. In its original formulation, there is a
single non-terminal X spanning a bitext cell with
an English and foreign span. There are three rule
types: Terminal unary productions X ? ?e, f?,
where e and f are an aligned English and for-
eign word pair (possibly with one being null);
normal binary rules X ? X(L)X(R), where the
English and foreign spans are constructed from
the children as ?X(L)X(R), X(L)X(R)?; and in-
verted binary rules X ; X(L)X(R), where the
foreign span inverts the order of the children
?X(L)X(R), X(R)X(L)?.2 In general, we will call
a bitext cell a normal cell if it was constructed with
a normal rule and inverted if constructed with an
inverted rule.
Each ITG derivation yields some alignment.
The set of such ITG alignments,AITG, are a strict
subset of A1-1 (Wu, 1997). Thus, we will view
ITG as a constraint on A1-1 which we will ar-
gue is generally beneficial. The maximum scor-
ing alignment from AITG can be found in O(n6)
time with synchronous CFG parsing; in practice,
we can make ITG parsing efficient using a variety
of pruning techniques. One computational advan-
tage of AITG over A1-1 alignments is that sum-
mation overAITG is tractable. The corresponding
1We shall use n throughout to refer to the maximum of
foreign and English sentence lengths.
2The superscripts on non-terminals are added only to in-
dicate correspondence of child symbols.
924
In
d
o
n
e
s
i
a
'
s
p
a
r
l
i
a
m
e
n
t
s
p
e
a
k
e
r
a
r
r
a
i
g
n
e
d
i
n
c
o
u
r
t
 ?
 ?
 ??
 ??
 ??
 ??
 ?
 ?
 ??
 ??
 ??
 ??
I
n
d
o
n
e
s
i
a
'
s
p
a
r
l
i
a
m
e
n
t
s
p
e
a
k
e
r
a
r
r
a
i
g
n
e
d
i
n
c
o
u
r
t
(a) Max-Matching Alignment (b) Block ITG Alignment
Figure 1: Best alignments from (a) 1-1 matchings and (b) block ITG (BITG) families respectively. The 1-1
matching is the best possible alignment in the model family, but cannot capture the fact that Indonesia is rendered
as two words in Chinese or that in court is rendered as a single word in Chinese.
dynamic program allows us to utilize likelihood-
based objectives for learning alignment models
(see Section 4).
Using the same heuristic Dice potentials on
the Hansards test set, the maximal scoring align-
ment from AITG yields 28.4 AER?2.4 better
than A1-1 ?indicating that ITG can be beneficial
as a constraint on heuristic alignments.
2.3 Block ITG
An important alignment pattern disallowed by
A1-1 is the many-to-one alignment block. While
not prevalent in our hand-aligned French Hansards
dataset, blocks occur frequently in our hand-
aligned Chinese-English NIST data. Figure 1
contains an example. Extending A1-1 to include
blocks is problematic, because finding a maximal
1-1 matching over phrases is NP-hard (DeNero
and Klein, 2008).
With ITG, it is relatively easy to allow contigu-
ous many-to-one alignment blocks without added
complexity.3 This is accomplished by adding ad-
ditional unary terminal productions aligning a for-
eign phrase to a single English terminal or vice
versa. We will use BITG to refer to this block
ITG variant and ABITG to refer to the alignment
family, which is neither contained in nor contains
A1-1. For this alignment family, we expand the
alignment potential decomposition in Equation (1)
to incorporate block potentials sef and sef which
represent English and foreign many-to-one align-
ment blocks, respectively.
One way to evaluate alignment families is to
3In our experiments we limited the block size to 4.
consider their oracle AER. In the 2002 NIST
Chinese-English hand-aligned data (see Sec-
tion 6.2), we constructed oracle alignment poten-
tials as follows: sij is set to +1 if (i, j) is a sure
or possible alignment in the hand-aligned data, -
1 otherwise. All null potentials (si and sj) are
set to 0. A max-matching under these potentials is
generally a minimal loss alignment in the family.
The oracle AER computed in this was is 10.1 for
A1-1 and 10.2 for AITG. The ABITG alignment
family has an oracle AER of 1.2. These basic ex-
periments show that AITG outperforms A1-1 for
heuristic alignments, and ABITG provide a much
closer fit to true Chinese-English alignments than
A1-1.
3 Margin-Based Training
In this and the next section, we discuss learning
alignment potentials. As input, we have a training
set D = (x1,a?1), . . . , (xn,a?n) of hand-aligned
data, where x refers to a sentence pair. We will as-
sume the score of a alignment is given as a linear
function of a feature vector ?(x,a). We will fur-
ther assume the feature representation of an align-
ment, ?(x,a) decomposes as in Equation (1),
?
(i,j)?a
?ij(x) +
?
i/?a
?i(x) +
?
j /?a
?j(x)
In the framework of loss-augmented margin
learning, we seek a w such that w ? ?(x,a?) is
larger than w ? ?(x,a) + L(a,a?) for all a in an
alignment family, where L(a,a?) is the loss be-
tween a proposed alignment a and the gold align-
ment a?. As in Taskar et al (2005), we utilize a
925
loss that decomposes across alignments. Specif-
ically, for each alignment cell (i, j) which is not
a possible alignment in a?, we incur a loss of 1
when aij 6= a?ij ; note that if (i, j) is a possible
alignment, our loss is indifferent to its presence in
the proposal alignment.
A simple loss-augmented learning pro-
cedure is the margin infused relaxed algo-
rithm (MIRA) (Crammer et al, 2006). MIRA
is an online procedure, where at each time step
t+ 1, we update our weights as follows:
wt+1 = argminw||w ?wt||22 (2)
s.t. w ? ?(x,a?) ? w ? ?(x, a?) + L(a?,a?)
where a? = argmax
a?A
wt ? ?(x,a)
In our data sets, many a? are not in A1-1 (and
thus not in AITG), implying the minimum in-
family loss must exceed 0. Since MIRA oper-
ates in an online fashion, this can cause severe
stability problems. On the Hansards data, the
simple averaging technique described by Collins
(2002) yields a reasonable model. On the Chinese
NIST data, however, where almost no alignment
is in A1-1, the update rule from Equation (2) is
completely unstable, and even the averaged model
does not yield high-quality results.
We instead use a variant of MIRA similar to
Chiang et al (2008). First, rather than update
towards the hand-labeled alignment a?, we up-
date towards an alignment which achieves mini-
mal loss within the family.4 We call this best-
in-class alignment a?p. Second, we perform loss-
augmented inference to obtain a?. This yields the
modified QP,
wt+1 = argminw||w ?wt||22 (3)
s.t. w ? ?(x,a?p) ? w ? ?(x, a?) + L(a,a?p)
where a? = argmax
a?A
wt ? ?(x,a) + ?L(a,a?p)
By setting ? = 0, we recover the MIRA update
from Equation (2). As ? grows, we increase our
preference that a? have high loss (relative to a?p)
rather than high model score. With this change,
MIRA is stable, but still performs suboptimally.
The reason is that initially the score for all align-
ments is low, so we are biased toward only using
very high loss alignments in our constraint. This
slows learning and prevents us from finding a use-
ful weight vector. Instead, in all the experiments
4There might be several alignments which achieve this
minimal loss; we choose arbitrarily among them.
we report here, we begin with ? = 0 and slowly
increase it to ? = 0.5.
4 Likelihood Objective
An alternative to margin-based training is a likeli-
hood objective, which learns a conditional align-
ment distribution Pw(a|x) parametrized as fol-
lows,
logPw(a|x)=w??(x,a)?log
?
a??A
exp(w??(x,a?))
where the log-denominator represents a sum over
the alignment family A. This alignment probabil-
ity only places mass on members ofA. The likeli-
hood objective is given by,
max
w
?
(x,a?)?A
logPw(a?|x)
Optimizing this objective with gradient methods
requires summing over alignments. ForAITG and
ABITG, we can efficiently sum over the set of ITG
derivations inO(n6) time using the inside-outside
algorithm. However, for the ITG grammar pre-
sented in Section 2.2, each alignment has multiple
grammar derivations. In order to correctly sum
over the set of ITG alignments, we need to alter
the grammar to ensure a bijective correspondence
between alignments and derivations.
4.1 ITG Normal Form
There are two ways in which ITG derivations dou-
ble count alignments. First, n-ary productions are
not binarized to remove ambiguity; this results in
an exponential number of derivations for diagonal
alignments. This source of overcounting is con-
sidered and fixed by Wu (1997) and Zens and Ney
(2003), which we briefly review here. The result-
ing grammar, which does not handle null align-
ments, consists of a symbol N to represent a bi-
text cell produced by a normal rule and I for a cell
formed by an inverted rule; alignment terminals
can be either N or I . In order to ensure unique
derivations, we stipulate that a N cell can be con-
structed only from a sequence of smaller inverted
cells I . Binarizing the rule N ? I2+ introduces
the intermediary symbolN (see Figure 2(a)). Sim-
ilarly for inverse cells, we insist an I cell only be
built by an inverted combination of N cells; bina-
rization of I ; N2+ requires the introduction of
the intermediary symbol I (see Figure 2(b)).
Null productions are also a source of double
counting, as there are many possible orders in
926
N ? I2+N ? IN
N ? I
}
N ? IN
I
I
I
N
N
N
(a) Normal Domain Rules
} I ! N2+
I ! NI
I ! NI
I ! N N
N
N
I
I
I
(b) Inverted Domain Rules
N11 ? ??, f?N11
N11 ? N10
N10 ? N10?e, ??
N10 ? N00
}N11 ? ??, f?
?N10
}N10 ? N00?e, ???
}
N00 ? I11N
N ? I11N
N ? I00
N00 ? I+11I00
N00 N10 N10
N11
N
NI11
I11
I00
N00
N11
(c) Normal Domain with Null Rules
}
}
}
I11 ! ??, f?I11
I11 ! I10 I11 ! ??, f?
?I10
I10 ! I10?e, ??
I10 ! I00 I10 ! I00?e, ??
?
I00 ! N+11N00 I
I
N00
N11
N11
I00 ! N11I
I ! N11I
I ! N00
I00
I00 I10 I10
I11
I11
(d) Inverted Domain with Null Rules
Figure 2: Illustration of two unambiguous forms of ITG grammars: In (a) and (b), we illustrate the normal grammar
without nulls (presented in Wu (1997) and Zens and Ney (2003)). In (c) and (d), we present a normal form grammar
that accounts for null alignments.
which to attach null alignments to a bitext cell;
we address this by adapting the grammar to force
a null attachment order. We introduce symbols
N00, N10, and N11 to represent whether a normal
cell has taken no nulls, is accepting foreign nulls,
or is accepting English nulls, respectively. We also
introduce symbols I00, I10, and I11 to represent
inverse cells at analogous stages of taking nulls.
As Figures 2 (c) and (d) illustrate, the directions
in which nulls are attached to normal and inverse
cells differ. The N00 symbol is constructed by
one or more ?complete? inverted cells I11 termi-
nated by a no-null I00. By placing I00 in the lower
right hand corner, we allow the larger N00 to un-
ambiguously attach nulls. N00 transitions to the
N10 symbol and accepts any number of ?e, ?? En-
glish terminal alignments. Then N10 transitions to
N11 and accepts any number of ??, f? foreign ter-
minal alignments. An analogous set of grammar
rules exists for the inverted case (see Figure 2(d)
for an illustration). Given this normal form, we
can efficiently compute model expectations over
ITG alignments without double counting.5 To our
knowledge, the alteration of the normal form to
accommodate null emissions is novel to this work.
5The complete grammar adds sentinel symbols to the up-
per left and lower right, and the root symbol is constrained to
be a N00.
4.2 Relaxing the Single Target Assumption
A crucial obstacle for using the likelihood objec-
tive is that a given a? may not be in the alignment
family. As in our alteration to MIRA (Section 3),
we could replace a? with a minimal loss in-class
alignment a?p. However, in contrast to MIRA, the
likelihood objective will implicitly penalize pro-
posed alignments which have loss equal to a?p. We
opt instead to maximize the probability of the set
of alignmentsM(a?) which achieve the same op-
timal in-class loss. Concretely, let m? be the min-
imal loss achievable relative to a? in A. Then,
M(a?) = {a ? A|L(a,a?) = m?}
When a? is an ITG alignment (i.e., m? is 0),
M(a?) consists only of alignments which have all
the sure alignments in a?, but may have some sub-
set of the possible alignments in a?. See Figure 3
for a specific example where m? = 1.
Our modified likelihood objective is given by,
max
w
?
(x,a?)?D
log ?
a?M(a?)
Pw(a|x)
Note that this objective is no longer convex, as it
involves a logarithm of a summation, however we
still utilize gradient-based optimization. Summing
and obtaining feature expectations over M(a?)
can be done efficiently using a constrained variant
927
MIRA Likelihood
1-1 ITG ITG-S ITG-N
Features P R AER P R AER P R AER P R AER
Dice,dist 85.9 82.6 15.6 86.7 82.9 15.0 89.2 85.2 12.6 87.8 82.6 14.6
+lex,ortho 89.3 86.0 12.2 90.1 86.4 11.5 92.0 90.6 8.6 90.3 88.8 10.4
+joint HMM 95.8 93.8 5.0 96.0 93.2 5.2 95.5 94.2 5.0 95.6 94.0 5.1
Table 1: Results on the French Hansards dataset. Columns indicate models and training methods. The rows
indicate the feature sets used. ITG-S uses the simple grammar (Section 2.2). ITG-N uses the normal form grammar
(Section 4.1). For MIRA (Viterbi inference), the highest-scoring alignment is the same, regardless of grammar.
T
h
a
t
i
s
n
o
t
g
o
o
d
e
n
o
u
g
h
 Se
ne
est
pas
 suffisant
a?Gold Alignment Target AlignmentsM(a?)
Figure 3: Often, the gold alignment a? isn?t in our
alignment family, here ABITG. For the likelihood ob-
jective (Section 4.2), we maximize the probability of
the setM(a?) consisting of alignments ABITG which
achieve minimal loss relative to a?. In this example,
the minimal loss is 1, and we have a choice of remov-
ing either of the sure alignments to the English word
not. We also have the choice of whether to include the
possible alignment, yielding 4 alignments inM(a?).
of the inside-outside algorithm where sure align-
ments not present in a? are disallowed, and the
number of missing sure alignments is appended to
the state of the bitext cell.6
One advantage of the likelihood-based objec-
tive is that we can obtain posteriors over individual
alignment cells,
Pw((i, j)|x) =
?
a?A:(i,j)?a
Pw(a|x)
We obtain posterior ITG alignments by including
all alignment cells (i, j) such that Pw((i, j)|x) ex-
ceeds a fixed threshold t. Posterior thresholding
allows us to easily trade-off precision and recall in
our alignments by raising or lowering t.
5 Dynamic Program Pruning
Both discriminative methods require repeated
model inference: MIRA depends upon loss-
augmented Viterbi parsing, while conditional like-
6Note that alignments that achieve the minimal loss would
not introduce any alignments not either sure or possible, so it
suffices to keep track only of the number of sure recall errors.
lihood uses the inside-outside algorithm for com-
puting cell posteriors. Exhaustive computation
of these quantities requires an O(n6) dynamic
program that is prohibitively slow even on small
supervised training sets. However, most of the
search space can safely be pruned using posterior
predictions from a simpler alignment models. We
use posteriors from two jointly estimated HMM
models to make pruning decisions during ITG in-
ference (Liang et al, 2006). Our first pruning tech-
nique is broadly similar to Cherry and Lin (2007a).
We select high-precision alignment links from the
HMM models: those word pairs that have a pos-
terior greater than 0.9 in either model. Then, we
prune all bitext cells that would invalidate more
than 8 of these high-precision alignments.
Our second pruning technique is to prune all
one-by-one (word-to-word) bitext cells that have a
posterior below 10?4 in both HMM models. Prun-
ing a one-by-one cell also indirectly prunes larger
cells containing it. To take maximal advantage of
this indirect pruning, we avoid explicitly attempt-
ing to build each cell in the dynamic program. In-
stead, we track bounds on the spans for which we
have successfully built ITG cells, and we only iter-
ate over larger spans that fall within those bounds.
The details of a similar bounding approach appear
in DeNero et al (2009).
In all, pruning reduces MIRA iteration time
from 175 to 5 minutes on the NIST Chinese-
English dataset with negligible performance loss.
Likelihood training time is reduced by nearly two
orders of magnitude.
6 Alignment Quality Experiments
We present results which measure the quality of
our models on two hand-aligned data sets. Our
first is the English-French Hansards data set from
the 2003 NAACL shared task (Mihalcea and Ped-
ersen, 2003). Here we use the same 337/100
train/test split of the labeled data as Taskar et al
928
MIRA Likelihood
1-1 ITG BITG BITG-S BITG-N
Features P R AER P R AER P R AER P R AER P R AER
Dice, dist,
blcks, dict, lex 85.7 63.7 26.8 86.2 65.8 25.2 85.0 73.3 21.1 85.7 73.7 20.6 85.3 74.8 20.1
+HMM 90.5 69.4 21.2 91.2 70.1 20.3 90.2 80.1 15.0 87.3 82.8 14.9 88.2 83.0 14.4
Table 2: Word alignment results on Chinese-English. Each column is a learning objective paired with an alignment
family. The first row represents our best model without external alignment models and the second row includes
features from the jointly trained HMM. Under likelihood, BITG-S uses the simple grammar (Section 2.2). BITG-N
uses the normal form grammar (Section 4.1).
(2005); we compute external features from the
same unlabeled data, 1.1 million sentence pairs.
Our second is the Chinese-English hand-aligned
portion of the 2002 NIST MT evaluation set. This
dataset has 491 sentences, which we split into a
training set of 150 and a test set of 191. When we
trained external Chinese models, we used the same
unlabeled data set as DeNero and Klein (2007), in-
cluding the bilingual dictionary.
For likelihood based models, we set the L2 reg-
ularization parameter, ?2, to 100 and the thresh-
old for posterior decoding to 0.33. We report re-
sults using the simple ITG grammar (ITG-S, Sec-
tion 2.2) where summing over derivations dou-
ble counts alignments, as well as the normal form
ITG grammar (ITG-N,Section 4.1) which does
not double count. We ran our annealed loss-
augmented MIRA for 15 iterations, beginning
with ? at 0 and increasing it linearly to 0.5. We
compute Viterbi alignments using the averaged
weight vector from this procedure.
6.1 French Hansards Results
The French Hansards data are well-studied data
sets for discriminative word alignment (Taskar et
al., 2005; Cherry and Lin, 2006; Lacoste-Julien
et al, 2006). For this data set, it is not clear
that improving alignment error rate beyond that of
GIZA++ is useful for translation (Ganchev et al,
2008). Table 1 illustrates results for the Hansards
data set. The first row uses dice and the same dis-
tance features as Taskar et al (2005). The first
two rows repeat the experiments of Taskar et al
(2005) and Cherry and Lin (2006), but adding ITG
models that are trained to maximize conditional
likelihood. The last row includes the posterior of
the jointly-trained HMM of Liang et al (2006)
as a feature. This model alone achieves an AER
of 5.4. No model significantly improves over the
HMM alone, which is consistent with the results
of Taskar et al (2005).
6.2 Chinese NIST Results
Chinese-English alignment is a much harder task
than French-English alignment. For example, the
HMM aligner achieves an AER of 20.7 when us-
ing the competitive thresholding heuristic of DeN-
ero and Klein (2007). On this data set, our block
ITG models make substantial performance im-
provements over the HMM, and moreover these
results do translate into downstream improve-
ments in BLEU score for the Chinese-English lan-
guage pair. Because of this, we will briefly de-
scribe the features used for these models in de-
tail. For features on one-by-one cells, we con-
sider Dice, the distance features from (Taskar et
al., 2005), dictionary features, and features for the
50 most frequent lexical pairs. We also trained an
HMM aligner as described in DeNero and Klein
(2007) and used the posteriors of this model as fea-
tures. The first two columns of Table 2 illustrate
these features for ITG and one-to-one matchings.
For our block ITG models, we include all of
these features, along with variants designed for
many-to-one blocks. For example, we include the
average Dice of all the cells in a block. In addi-
tion, we also created three new block-specific fea-
tures types. The first type comprises bias features
for each block length. The second type comprises
features computed from N-gram statistics gathered
from a large monolingual corpus. These include
features such as the number of occurrences of the
phrasal (multi-word) side of a many-to-one block,
as well as pointwise mutual information statistics
for the multi-word parts of many-to-one blocks.
These features capture roughly how ?coherent? the
multi-word side of a block is.
The final block feature type consists of phrase
shape features. These are designed as follows: For
each word in a potential many-to-one block align-
ment, we map an individual word to X if it is not
one of the 25 most frequent words. Some example
features of this type are,
929
? English Block: [the X, X], [in X of, X]
? Chinese Block: [ X, X] [X|, X]
For English blocks, for example, these features
capture the behavior of phrases such as in spite
of or in front of that are rendered as one word in
Chinese. For Chinese blocks, these features cap-
ture the behavior of phrases containing classifier
phrases like? orP, which are rendered as
English indefinite determiners.
The right-hand three columns in Table 2 present
supervised results on our Chinese English data set
using block features. We note that almost all of
our performance gains (relative to both the HMM
and 1-1 matchings) come from BITG and block
features. The maximum likelihood-trained nor-
mal form ITG model outperforms the HMM, even
without including any features derived from the
unlabeled data. Once we include the posteriors
of the HMM as a feature, the AER decreases to
14.4. The previous best AER result on this data set
is 15.9 from Ayan and Dorr (2006), who trained
stacked neural networks based on GIZA++ align-
ments. Our results are not directly comparable
(they used more labeled data, but did not have the
HMM posteriors as an input feature).
6.3 End-To-End MT Experiments
We further evaluated our alignments in an end-to-
end Chinese to English translation task using the
publicly available hierarchical pipeline JosHUa
(Li and Khudanpur, 2008). The pipeline extracts
a Hiero-style synchronous context-free grammar
(Chiang, 2007), employs suffix-array based rule
extraction (Lopez, 2007), and tunes model pa-
rameters with minimum error rate training (Och,
2003). We trained on the FBIS corpus using sen-
tences up to length 40, which includes 2.7 million
English words. We used a 5-gram language model
trained on 126 million words of the Xinhua section
of the English Gigaword corpus, estimated with
SRILM (Stolcke, 2002). We tuned on 300 sen-
tences of the NIST MT04 test set.
Results on the NIST MT05 test set appear in
Table 3. We compared four sets of alignments.
The GIZA++ alignments7 are combined across di-
rections with the grow-diag-final heuristic, which
outperformed the union. The joint HMM align-
ments are generated from competitive posterior
7We used a standard training regimen: 5 iterations of
model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3
iterations of Model 4.
Alignments Translations
Model Prec Rec Rules BLEU
GIZA++ 62 84 1.9M 23.22
Joint HMM 79 77 4.0M 23.05
Viterbi ITG 90 80 3.8M 24.28
Posterior ITG 81 83 4.2M 24.32
Table 3: Results on the NIST MT05 Chinese-English
test set show that our ITG alignments yield improve-
ments in translation quality.
thresholding (DeNero and Klein, 2007). The ITG
Viterbi alignments are the Viterbi output of the
ITG model with all features, trained to maximize
log likelihood. The ITG Posterior alignments
result from applying competitive thresholding to
alignment posteriors under the ITG model. Our
supervised ITG model gave a 1.1 BLEU increase
over GIZA++.
7 Conclusion
This work presented the first large-scale applica-
tion of ITG to discriminative word alignment. We
empirically investigated the performance of con-
ditional likelihood training of ITG word aligners
under simple and normal form grammars. We
showed that through the combination of relaxed
learning objectives, many-to-one block alignment
potential, and efficient pruning, ITG models can
yield state-of-the art word alignments, even when
the underlying gold alignments are highly non-
ITG. Our models yielded the lowest published er-
ror for Chinese-English alignment and an increase
in downstream translation performance.
References
Necip Fazil Ayan and Bonnie Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In ACL.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In ACL.
Colin Cherry and Dekang Lin. 2007a. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In NAACL-HLT 2007.
Colin Cherry and Dekang Lin. 2007b. A scalable in-
version transduction grammar for joint phrasal trans-
lation modeling. In SSST Workshop at ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
930
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Koby Crammer, Ofer Dekel, Shai S. Shwartz, and
Yoram Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
John DeNero and Dan Klein. 2008. The complexity
of phrase alignment problems. In ACL Short Paper
Track.
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient parsing for transducer gram-
mars. In NAACL.
Kuzman Ganchev, Joao Graca, and Ben Taskar. 2008.
Better alignments = better translations? In ACL.
H. W. Kuhn. 1955. The Hungarian method for the as-
signment problem. Naval Research Logistic Quar-
terly.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael Jordan. 2006. Word alignment via
quadratic assignment. In NAACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
SSST Workshop at ACL.
Percy Liang, Dan Klein, and Dan Klein. 2006. Align-
ment by agreement. In NAACL-HLT.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics.
Rada Mihalcea and Ted Pedersen. 2003. An evalua-
tion exercise for word alignment. In HLT/NAACL
Workshop on Building and Using Parallel Texts.
Robert C. Moore, Wen tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word
alignment. In ACL-COLING.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Empirical Methods in Nat-
ural Language Processing.
Andreas Stolcke. 2002. Srilm: An extensible language
modeling toolkit. In ICSLP 2002.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In NAACL-HLT.
L. G. Valiant. 1979. The complexity of computing the
permanent. Theoretical Computer Science, 8:189?
201.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In ACL.
Hao Zhang and Dan Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment.
In ACL.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL.
931
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1075?1083,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Exploiting Bilingual Information to Improve Web Search
Wei Gao1, John Blitzer2, Ming Zhou3, and Kam-Fai Wong1
1The Chinese University of Hong Kong, Shatin, N.T., Hong Kong, China
{wgao,kfwong}@se.cuhk.edu.hk
2Computer Science Division, University of California at Berkeley, CA 94720-1776, USA
blitzer@cs.berkeley.edu
3Microsoft Research Asia, Beijing 100190, China
mingzhou@microsoft.com
Abstract
Web search quality can vary widely across
languages, even for the same information
need. We propose to exploit this variation
in quality by learning a ranking function
on bilingual queries: queries that appear in
query logs for two languages but represent
equivalent search interests. For a given
bilingual query, along with correspond-
ing monolingual query log and monolin-
gual ranking, we generate a ranking on
pairs of documents, one from each lan-
guage. Then we learn a linear ranking
function which exploits bilingual features
on pairs of documents, as well as standard
monolingual features. Finally, we show
how to reconstruct monolingual ranking
from a learned bilingual ranking. Us-
ing publicly available Chinese and English
query logs, we demonstrate for both lan-
guages that our ranking technique exploit-
ing bilingual data leads to significant im-
provements over a state-of-the-art mono-
lingual ranking algorithm.
1 Introduction
Web search quality can vary widely across lan-
guages, even for a single query and search en-
gine. For example, we might expect that rank-
ing search results for the query Wj? ?Y?
(Thomas Hobbes) to be more difficult in Chinese
than it is in English, even while holding the ba-
sic ranking function constant. At the same time,
ranking search results for the query Han Feizi (8
:) is likely to be harder in English than in Chi-
nese. A large portion of web queries have such
properties that they are originated in a language
different from the one they are searched.
This variance in problem difficulty across lan-
guages is not unique to web search; it appears in
a wide range of natural language processing prob-
lems. Much recent work on bilingual data has fo-
cused on exploiting these variations in difficulty
to improve a variety of monolingual tasks, includ-
ing parsing (Hwa et al, 2005; Smith and Smith,
2004; Burkett and Klein, 2008; Snyder and Barzi-
lay, 2008), named entity recognition (Chang et al,
2009), and topic clustering (Wu and Oard, 2008).
In this work, we exploit a similar intuition to im-
prove monolingual web search.
Our problem setting differs from cross-lingual
web search, where the goal is to return machine-
translated results from one language in response to
a query from another (Lavrenko et al, 2002). We
operate under the assumption that for many mono-
lingual English queries (e.g., Han Feizi), there ex-
ist good documents in English. If we have Chinese
information as well, we can exploit it to help find
these documents. As we will see, machine trans-
lation can provide important predictive informa-
tion in our setting, but we do not wish to display
machine-translated output to the user.
We approach our problem by learning a rank-
ing function for bilingual queries ? queries that
are easily translated (e.g., with machine transla-
tion) and appear in the query logs of two languages
(e.g., English and Chinese). Given query logs
in both languages, we identify bilingual queries
with sufficient clickthrough statistics in both sides.
Large-scale aggregated clickthrough data were
proved useful and effective in learning ranking
functions (Dou et al, 2008). Using these statis-
tics, we can construct a ranking over pairs of docu-
ments, one from each language. We use this rank-
ing to learn a linear scoring function on pairs of
documents given a bilingual query.
We find that our bilingual rankings have good
monolingual ranking properties. In particular,
given an optimal pairwise bilingual ranking, we
show that simple heuristics can effectively approx-
imate the optimal monolingual ranking. Using
1075
1 10 100 1,000 10,000 50,0000
5
10
15
20
25
30
35
40
45
50
Frequency (# of times that queries are issued)
Pro
por
tion
 of 
bilin
gua
l qu
erie
s (%
)
 
 
English
Chinese
Figure 1: Proportion of bilingual queries in the
query logs of different languages.
these heuristics and our learned pairwise scoring
function, we can derive a ranking for new, unseen
bilingual queries. We develop and test our bilin-
gual ranker on English and Chinese with two large,
publicly available query logs from the AOL search
engine1 (English query log) (Pass et al, 2006)
and the Sougou search engine2 (Chinese query
log) (Liu et al, 2007). For both languages, we
achieve significant improvements over monolin-
gual Ranking SVM (RSVM) baselines (Herbrich
et al, 2000; Joachims, 2002), which exploit a va-
riety of monolingual features.
2 Bilingual Query Statistics
We designate a query as bilingual if the concept
has been searched by users of both two languages.
As a result, not only does it occur in the query log
of its own language, but its translation also appears
in the log of the second language. So a bilingual
query yields reasonable queries in both languages.
Of course, most queries are not bilingual. For ex-
ample, our English log contains map of Alabama,
but not our Chinese log. In this case, we wouldn?t
expect the Chinese results for the query?s transla-
tion, ?n?j?C, to be helpful in ranking the
English results.
In total, we extracted 4.8 million English
queries from AOL log, of which 1.3% of their
translations appear in Sogou log. Similarly, of our
3.1 million Chinese queries from Sogou log, 2.3%
of their translations appear in AOL log. By to-
tal number of queries issued (i.e., counting dupli-
1http://search.aol.com
2http://www.sogou.com
cates), the proportion of bilingual queries is much
higher. As Figure 1 shows as the number of times
a query is issued increases, so does the chance of
it being bilingual. In particular, nearly 45% of the
highest-frequency English queries and 35% of the
highest-frequency Chinese queries are bilingual.
3 Learning to Rank Using Bilingual
Information
Given a set of bilingual queries, we now de-
scribe how to learn a ranking function for mono-
lingual data that exploits information from both
languages. Our procedure has three steps: Given
two monolingual rankings, we construct a bilin-
gual ranking on pairs of documents, one from each
language. Then we learn a linear scoring function
for pairs of documents that exploits monolingual
information (in both languages) and bilingual in-
formation. Finally, given this ranking function on
pairs and a new bilingual query, we reconstruct a
monolingual ranking for the language of interest.
This section addresses these steps in turn.
3.1 Creating Bilingual Training Data
Without loss of generality, suppose we rank En-
glish documents with constraints from Chinese
documents. Given an English log Le and a Chi-
nese log Lc, our ranking algorithm takes as input
a bilingual query pair q = (qe, qc) where qe ? Le
and qc ? Lc, a set of returned English documents
{ei}Ni=1 from qe, and a set of constraint Chinese
documents {cj}nj=1 from qc. In order to create
bilingual ranking data, we first generate monolin-
gual ranking data from clickthrough statistics. For
each language-query-document triple, we calcu-
late the aggregated click count across all users and
rank documents according to this statistic. We de-
note the count of a page as C(ei) or C(cj).
The use of clickthrough statistics as feedback
for learning ranking functions is not without con-
troversy, but recent empirical results on large
data sets suggest that the aggregated user clicks
provides an informative indicator of relevance
preference for a query. Joachims et al (2007)
showed that relative feedback signals generated
from clicks correspond well with human judg-
ments. Dou et al (2008) revealed that a straight-
forward use of aggregated clicks can achieve a bet-
ter ranking than using explicitly labeled data be-
cause clickthrough data contain fine-grained dif-
ferences between documents useful for learning an
1076
Table 1: Clickthrough data of a bilingual query
pair extracted from query logs.
Bilingual query pair (Mazda,jH)
doc URL click #
e1 www.mazda.com 229
e2 www.mazdausa.com 185
e3 www.mazda.co.uk 5
e4 www.starmazda.com 2
e5 www.mazdamotosports.com 2
. . . . . .
c1 www.faw-mazda.com 50
c2 price.pcauto.com.cn/brand.
jsp?bid=17
43
c3 auto.sina.com.cn/salon/
FORD/MAZDA.shtml
20
c4 car.autohome.com.cn/brand/
119/
18
c5 jsp.auto.sohu.com/view/
brand-bid-263.html
9
. . . . . .
accurate and reliable ranking. Therefore, we lever-
age aggregated clicks for comparing the relevance
order of documents. Note that there is nothing
specific to our technique that requires clickthrough
statistics. Indeed, our methods could easily be em-
ployed with human annotated data. Table 1 gives
an example of a bilingual query pair and the ag-
gregated click count of each result page.
Given two monolingual documents, a prefer-
ence order can be inferred if one document is
clicked more often than another. To allow for
cross-lingual information, we extend the order of
individual documents into that of bilingual docu-
ment pairs: given two bilingual document pairs,
we will write
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
to indi-
cate that the pair of
(
e(1)i , c
(1)
j
)
is ranked higher
than the pair of
(
e(2)i , c
(2)
j
)
.
Definition 1
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
if and
only if one of the following relations hold:
1. C(e(1)i ) > C(e
(2)
i ) and C(c
(1)
j ) ? C(c
(2)
j )
2. C(e(1)i ) ? C(e
(2)
i ) and C(c
(1)
j ) > C(c
(2)
j )
Note, however, that from a purely monolingual
perspective, this definition introduces orderings on
documents that should not initially have existed.
For English ranking, for example, we may have(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
even when C(e(1)i ) =
C(e(2)i ). This leads us to the following asymmet-
ric definition of  that we use in practice:
Definition 2
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
if and
only if C(e(1)i ) > C(e
(2)
i ) and C(c
(1)
j ) ? C(c
(2)
j )
With this definition, we can unambiguously
compare the relevance of bilingual document pairs
based on the order of monolingual documents.
The advantages are two-fold: (1) we can treat mul-
tiple cross-lingual document similarities the same
way as the commonly used query-document fea-
tures in a uniform manner of learning; (2) with the
similarities, the relevance estimation on bilingual
document pairs can be enhanced, and this in return
can improve the ranking of documents.
3.2 Ranking Model
Given a pair of bilingual queries (qe, qc), we
can extract the set of corresponding bilin-
gual document pairs and their click counts
{(ei, cj), (C(ei), C(cj))}, where i = 1, . . . , N
and j = 1, . . . , n. Based on that, we produce a
set of bilingual ranking instances S = {?ij, zij},
where each ?ij = {xi;yj; sij} is the feature
vector of (ei, cj) consisting of three components:
xi = f(qe, ei) is the vector of monolingual rele-
vancy features of ei, yi = f(qc, cj) is the vector
of monolingual relevancy features of cj , and sij =
sim(ei, cj) is the vector of cross-lingual similari-
ties between ei and cj , and zij = (C(ei), C(cj))
is the corresponding click counts.
The task is to select the optimal function that
minimizes a given loss with respect to the order
of ranked bilingual document pairs and the gold.
We resort to Ranking SVM (RSVM) (Herbrich et
al., 2000; Joachims, 2002) learning for classifica-
tion on pairs of instances. Compared the base-
line RSVM (monolingual), our algorithm learns
to classify on pairs of bilingual document pairs
rather than on pairs of individual documents.
Let f being a linear function:
f~w(ei, cj) = ~wx ? xi + ~wy ? yj + ~ws ? sij (1)
where ~w = {~wx; ~wy; ~ws} denotes the weight vec-
tor, in which the elements correspond to the rele-
vancy features and similarities. For any two bilin-
gual document pairs, their preference relation is
measured by the difference of the functional val-
ues of Equation 1:
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
?
f~w
(
e(1)i , c
(1)
j
)
? f~w
(
e(2)i , c
(2)
j
)
> 0 ?
~wx ?
(
x(1)i ? x
(2)
i
)
+ ~wy ?
(
y(1)j ? y
(2)
j
)
+
~ws ?
(
s(1)ij ? s
(2)
ij
)
> 0
1077
We then create a new training corpus based on the
preference ordering of any two such pairs: S? =
{??ij, z
?
ij}, where the new feature vector becomes
??ij =
{
x(1)i ? x
(2)
i ;y
(1)
j ? y
(2)
j ; s
(1)
ij ? s
(2)
ij
}
,
and the class label
z?ij =
?
?
?
+1, if
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
;
?1, if
(
e(2)i , c
(2)
j
)

(
e(1)i , c
(1)
j
)
is a binary preference value depending on the or-
der of bilingual document pairs. The problem is to
solve SVM objective: min
~w
1
2?~w?
2 + ?
?
i
?
j ?ij
subject to bilingual constraints: z?ij ? (~w ? ?
?
ij) ?
1? ?ij and ?ij ? 0.
There are potentially ? = nN bilingual docu-
ment pairs for each query, and the number of com-
parable pairs may be much larger due to the com-
binatorial nature (but less than ?(? ? 1)/2). To
speed up training, we resort to stochastic gradient
descent (SGD) optimizer (Shalev-Shwartz et al,
2007) to approximate the true gradient of the loss
function evaluated on a single instance (i.e., per
constraint). The parameters are then adjusted by
an amount proportional to this approximate gradi-
ent. For large data set, SGD-RSVM can be much
faster than batch-mode gradient descent.
3.3 Inference
The solution ~w forms a vector orthogonal to the
hyper-plane of RSVM. To predict the order of
bilingual document pairs, the ranking score can
be simply calculated by Equation 1. However, a
prominent problem is how to derive the full order
of monolingual documents for output from the or-
der of bilingual document pairs. To our knowl-
edge, there is no precise conversion algorithm in
polynomial time. We thus adopt two heuristics for
approximating the true document score:
? H-1 (max score): Choose the maximum
score of the pair as the score of document,
i.e., score(ei) = maxj(f(ei, cj)).
? H-2 (mean score): Average over all the
scores of pairs associated with the ranked
document as the score of this document, i.e.,
score(ei) = 1/n
?
j f(ei, cj).
Intuitively, for the rank score of a single docu-
ment, H-2 combines the ?voting? scores from its n
constraint documents weighted equally, while H-1
simply chooses the maximum one. A formal ap-
proach to the problem is to leverage rank aggre-
gation formalism (Dwork et a., 2001; Liu et al,
2007), which will be left for our future work. The
two simple heuristics are employed here because
of their simplicity and efficiency. The time com-
plexity of the approximation is linear to the num-
ber of ranked documents given n is constant.
4 Features and Similarities
Standard features for learning to rank include vari-
ous query-document features, e.g., BM25 (Robert-
son, 1997), as well as query-independent features,
e.g., PageRank (Brin and Page, 1998). Our feature
space consists of both these standard monolingual
features and cross-lingual similarities among doc-
uments. The cross-lingual similarities are valu-
ated using different translation mechanisms, e.g.,
dictionary-based translation or machine transla-
tion, or even without any translation at all.
4.1 Monolingual Relevancy Features
In learning to rank, the relevancy between query
and documents and the measures based on link
analysis are commonly used as features. The dis-
cussion on their details is beyond the scope of this
paper. Readers may refer to (Liu et al, 2007)
for the definitions of many such features. We im-
plement six of these features that are considered
the most typical shown as Table 2. These include
sets of measures such as BM25, language-model-
based IR score, and PageRank. Because most con-
ventional IR and web search relevancy measures
fall into this category, we call them altogether IR
features in what follows. Note that for a given
bilingual document pair (e, c), the monolingual IR
features consist of relevance score vectors f(qe, e)
in English and f(qc, c) in Chinese.
4.2 Cross-lingual Document Similarities
To measure the document similarity across dif-
ferent languages, we define the similarity vector
sim(e, c) as a series of functions mapping a bilin-
gual document pair to positive real numbers. In-
tuitively, a good similarity function is one which
maps cross-lingual relevant documents into close
scores and maintains a large distance between ir-
relevant and relevant documents. Four categories
of similarity measures are employed.
Dictionary-based Similarity (DIC): For
dictionary-based document translation, we use
1078
Table 2: List of monolingual relevancy measures
used as IR features in our model.
IR Feature Description
BM25 Okapi BM25 score (Robertson, 1997)
BM25 PRF Okapi BM25 score with pseudo-
relevance feedback (Robertson and
Jones, 1976)
LM DIR Language-model-based IR score with
Dirichlet smoothing (Zhai and Lafferty,
2001)
LM JM Language-model-based IR score with
Jelinek-Mercer smoothing (Zhai and
Lafferty, 2001)
LM ABS Language-model-based IR score with
absolute discounting (Zhai and Lafferty,
2001)
PageRank PageRank score (Brin and Page, 1998)
the similarity measure proposed by Mathieu et
al. (2004). Given a bilingual dictionary, we let
T (e, c) denote the set of word pairs (we, wc) such
that we is a word in English document e, and wc
is a word in Chinese document c, and we is the
English translation of wc. We define tf(we, e)
and tf(wc, c) to be the term frequency of we in
e and that of wc in c, respectively. Let df(we)
and df(wc) be the English document frequency
for we and Chinese document frequency for
wc. If ne (nc) is the total number of English
(Chinese), then the bilingual idf is defined as
idf(we, wc) = log ne+ncdf(we)+df(wc) . Then the
cross-lingual document similarity is calculated by
sim(e, c) =
?
(we,wc)?T (e,c)
tf(we,e)tf(wc,c)idf(we,wc)2
?
Z
where Z is a normalization coefficient (see Math-
ieu et al (2004) for detail). This similarity func-
tion can be understood as the cross-lingual coun-
terpart of the monolingual cosine similarity func-
tion (Salton, 1998).
Similarity Based on Machine Translation
(MT): For machine translation, the cross-lingual
measure actually becomes a monolingual similar-
ity between one document and another?s transla-
tion. We therefore adopt cosine function for it di-
rectly (Salton, 1998).
Translation Ratio (RATIO): Translation ratio
is defined as two sets of ratios of translatable terms
using a bilingual dictionary: RATIO FOR ? what
percent of words in e can be translated to words in
c; RATIO BACK ? what percent of words in c can
be translated back to words in e.
URL LCS Ratio (URL): The ratio of longest
common subsequence (Cormen et al, 2001) be-
tween the URLs of two pages being compared.
This measure is useful to capture pages in different
languages but with similar URLs such as www.
airbus.com, www.airbus.com.cn, etc.
Note that each set of similarities above except
URL includes 3 values based on different fields of
web page: title, body, and title+body.
5 Experiments and Results
This section presents evaluation metric, data sets
and experiments for our proposed ranker.
5.1 Evaluation Metric
Commonly adopted metrics for ranking, such as
mean average precision (Buckley and Voorhees,
2000) and Normalized Discounted Cumulative
Gain (Ja?rvelin and Keka?la?inen, 2000), is designed
for data sets with human relevance judgment,
which is not available to us. Therefore, we
use the Kendall?s tau coefficient (Kendall, 1938;
Joachims, 2002) to measure the degree of correla-
tion between two rankings. For simplicity, let?s as-
sume strict orderings of any given ranking. There-
fore we ignore all the pairs with ties (instances
with the identical click count). Kendall?s tau is
defined as ?(ra, rb) = (P ? Q)/(P + Q), where
P is the number of concordant pairs and Q is the
number of disconcordant pairs in the given order-
ings ra and rb. The value is a real number within
[?1,+1], where ?1 indicates a complete inver-
sion, and +1 stands for perfect agreement, and a
value of zero indicates no correlation.
Existing ranking techniques heavily depend on
human relevance judgment that is very costly to
obtain. Similar to Dou et al(2008), our method
utilizes the automatically aggregated click count in
query logs as the gold for deriving the true order
of relevancy, but we use the clickthrough of dif-
ferent languages. We average Kendall?s tau values
between the algorithm output and the gold based
on click frequency for all test queries.
5.2 Data Sets
Query logs can be the basis for constructing high
quality ranking corpus. Due to the proprietary
issue of log, no public ranking corpus based on
real-world search engine log is currently avail-
able. Moreover, to build a predictable bilingual
ranking corpus, the logs of different languages are
needed and have to meet certain conditions: (1)
they should be sufficiently large so that a good
number of bilingual query pairs could be identi-
1079
Table 3: Statistics on AOL and Sogou query logs.
AOL(EN) Sogou(CH)
# sessions 657,426 5,131,000
# unique queries 10,154,743 3,117,902
# clicked queries 4,811,650 3,117,590
# clicked URLs 1,632,788 8,627,174
time span 2006/03-05 2006/08
size 2.12GB 1.56GB
fied; (2) for the identified query pairs, there should
be sufficient statistics of associated clickthrough
data; (3) The click frequency should be well dis-
tributed at both sides so that the preference order
between bilingual document pairs can be derived
for SVM learning.
For these reasons, we use two independent and
publicly accessible query logs to construct our
bilingual ranking corpus: English AOL log3 and
Chinese Sogou log4. Table 3 shows some statis-
tics of these two large query logs.
We automatically identify 10,544 bilingual
query pairs from the two logs using the Java
API for Google Translate5, in which each query
has certain number of clicked URLs. To bet-
ter control the bilingual equivalency of queries,
we make sure the bilingual queries in each of
these pairs are bi-directional translations. Then
we download all their clicked pages, which re-
sults in 70,180 English6 and 111,197 Chinese doc-
uments. These documents form two independent
collections, which are indexed separately for re-
trieval and feature calculation.
For good quality, it is necessary to have suffi-
cient clickthrough data for each query. So we fur-
ther identify 1,084 out of 10,544 bilingual query
pairs, in which each query has at least 10 clicked
and downloadable documents. This smaller col-
lection is used for learning our model, which con-
tains 21,711 English and 28,578 Chinese docu-
ments7. In order to compute cross-lingual doc-
ument similarities based on machine translation
3http://gregsadetsky.com/aol-data/
4http://www.sogou.com/labs/dl/q.html
5http://code.google.com/p/
google-api-translate-java/
6AOL log only records the domain portion of the clicked
URLs, which misleads document downloading. We use the
?search within site or domain? function of a major search en-
gine to approximate the real clicked URLs by keeping the
first returned result for each query.
7Because Sogou log has a lot more clicked URLs, for bal-
ancing with the number of English pages, we kept at most 50
pages per Chinese query.
Table 4: Kendall?s tau values of English ranking.
The significant improvements over baseline (99%
confidence) are bolded with the p-values given in
parenthesis. * indicates significant improvement
over IR (no similarity). n = 5.
Models Pair H-1 (max) H-2 (mean)
RSVM (baseline) n/a 0.2424 0.2424
IR (no similarity) 0.2783 0.2445 0.2445
IR+DIC 0.2909 0.2453 0.2496
IR+MT 0.2858
0.2488* 0.2494*
(p=0.0003) (p=0.0004)
IR+DIC+MT 0.2901 0.2481
0.2514*
(p=0.0009)
IR+DIC+RATIO 0.2946 0.2466
0.2519*
(p=0.0004)
IR+DIC+MT
+RATIO
0.2940
0.2473* 0.2539*
(p=0.0009) (p=1.5e-5)
IR+DIC+MT
+RATIO+URL
0.2979
0.2533* 0.2577*
(p=2.2e-5) (p=4.4e-7)
(see Section 4.2), we automatically translate all
these 50,298 documents using Google Translate,
i.e., English to Chinese and vice versa. Then the
bilingual document pairs are constructed, and all
the monolingual features and cross-lingual simi-
larities are computed (see Section 4.1&4.2).
5.3 English Ranking Performance
Here we examine the ranking performance of our
English ranker under different similarity settings.
We use traditional RSVM (Herbrich et al, 2000;
Joachims, 2002) without any bilingual considera-
tion as the baseline, which uses only English IR
features. We conduct this experiment using all the
1,084 bilingual query pairs with 4-fold cross vali-
dation (each fold with 271 query pairs). The num-
ber of constraint documents n is empirically set as
5. The results are shown in Table 4.
Clearly, bilingual constraints are helpful to
improve English ranking. Our pairwise set-
tings unanimously outperforms the RSVM base-
line. The paired two-tailed t-test (Smucker et
al., 2007) shows that most improvements resulted
from heuristic H-2 (mean score) are statistically
significant at 99% confidence level (p<0.01). Rel-
atively fewer significant improvements can be
made by heuristic H-1 (max score). This is be-
cause the maximum score on pair is just a rough
approximation to the optimal document score. But
this simple scheme works surprisingly well and
still consistently outperforms the baseline.
Note that our bilingual model with only IR fea-
tures, i.e., IR (no similarity), also outperforms the
baseline. The reason is that in this setting there are
1080
1 2 3 4 5 6 7 8 9 100.23
0.235
0.24
0.245
0.25
0.255
0.26
# of constraint documents in a different language
Ke
nda
ll?s 
tau
 
 
RSVM (baseline)
IR+DIC
IR+MT
IR+DIC+MT
IR+DIC+RAIO+MT
IR+DIC+RAIO+MT+URL
Figure 2: English ranking results vary with the
number of constraint Chinese documents.
IR features of n Chinese documents introduced in
addition to the IR features of English documents
in the baseline.
The DIC similarity does not work as effectively
as MT. This may be due to the limitation of bilin-
gual dictionary alone for translating documents,
where the issues like out-of-vocabulary words and
translation ambiguity are common but can be bet-
ter dealt with by MT. When DIC is combined with
RATIO, which considers both forward and back-
ward translation of words, it can capture the corre-
lation between bilingually very similar pages, thus
performs better.
We find that the URL similarity, although sim-
ple, is very useful and improves 1.5?2.4% of
Kendall?s tau value than not using it. This is be-
cause the URLs of the top Chinese (constraint)
documents are often similar to many of returned
English URLs which are generally more regu-
lar. For example, in query pair (Toyota Camry,
TProceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1051?1055,
Prague, June 2007. c?2007 Association for Computational Linguistics
Frustratingly Hard Domain Adaptation for Dependency Parsing
Mark Dredze1 and John Blitzer1 and Partha Pratim Talukdar1 and
Kuzman Ganchev1 and Joa?o V. Grac?a2 and Fernando Pereira1
1CIS Dept., University of Pennsylvania, Philadelphia, PA 19104
{mdredze|blitzer|partha|kuzman|pereira}@seas.upenn.edu
2L2F ? INESC-ID Lisboa/IST, Rua Alves Redol 9, 1000-029, Lisboa, Portugal
javg@l2f.inesc-id.pt
Abstract
We describe some challenges of adaptation
in the 2007 CoNLL Shared Task on Domain
Adaptation. Our error analysis for this task
suggests that a primary source of error is
differences in annotation guidelines between
treebanks. Our suspicions are supported by
the observation that no team was able to im-
prove target domain performance substan-
tially over a state of the art baseline.
1 Introduction
Dependency parsing, an important NLP task, can be
done with high levels of accuracy. However, adapt-
ing parsers to new domains without target domain
labeled training data remains an open problem. This
paper outlines our participation in the 2007 CoNLL
Shared Task on Domain Adaptation (Nivre et al,
2007). The goal was to adapt a parser trained on
a single source domain to a new target domain us-
ing only unlabeled data. We were given around
15K sentences of labeled text from the Wall Street
Journal (WSJ) (Marcus et al, 1993; Johansson and
Nugues, 2007) as well as 200K unlabeled sentences.
The development data was 200 sentences of labeled
biomedical oncology text (BIO, the ONCO portion
of the Penn Biomedical Treebank), as well as 200K
unlabeled sentences (Kulick et al, 2004). The two
test domains were a collection of medline chem-
istry abstracts (pchem, the CYP portion of the Penn
Biomedical Treebank) and the Child Language Data
Exchange System corpus (CHILDES) (MacWhin-
ney, 2000; Brown, 1973). We used the second or-
der two stage parser and edge labeler of McDonald
et al (2006), which achieved top results in the 2006
CoNLL-X shared task. Preliminary experiments in-
dicated that the edge labeler was fairly robust to do-
main adaptation, lowering accuracy by 3% in the de-
velopment domain as opposed to 2% in the source,
so we focused on unlabeled dependency parsing.
Our system did well, officially coming in 3rd
place out of 12 teams and within 1% of the top sys-
tem (Table 1). 1 In unlabeled parsing, we scored
1st and 2nd on CHILDES and pchem respectively.
However, our results were obtained without adap-
tation. Given our position in the ranking, this sug-
gests that no team was able to significantly improve
performance on either test domain beyond that of a
state-of-the-art parser.
After much effort in developing adaptation meth-
ods, it is critical to understand the causes of these
negative results. In what follows, we provide an er-
ror analysis that attributes domain loss for this task
to a difference in annotation guidelines between do-
mains. We then overview our attempts to improve
adaptation. While we were able to show limited
adaptation on reduced training data or with first-
order features, no modifications improved parsing
with all the training data and second-order features.
2 Parsing Challenges
We begin with an error analysis for adaptation be-
tween WSJ and BIO. We divided the available WSJ
data into a train and test set, trained a parser on
the train set and compared errors on the test set
and BIO. Accuracy dropped from 90% on WSJ to
84% on BIO. We then computed the fraction of er-
rors involving each POS tag. For the most common
1While only 8 teams participated in the closed track with us,
our score beat all of the teams in the open track.
1051
pchem l pchem ul childes ul bio ul
Ours 80.22 83.38 61.37 83.93
Best 81.06 83.42 61.37 -
Mean 73.03 76.42 57.89 -
Rank 3rd 2nd 1st -
Table 1: Official labeled (l) and other unlabeled (ul)
submitted results for the two test domains (pchem
and childes) and development data accuracy (bio).
The parser was trained on the provided WSJ data.
POS types, the loss (difference in source and tar-
get error) was: verbs (2%), conjunctions (5%), dig-
its (23%), prepositions (4%), adjectives (3%), de-
terminers (4%) and nouns (9%). 2 Two POS types
stand out: digits and nouns. Digits are less than
4% of the tokens in BIO. Errors result from the BIO
annotations for long sequences of digits which do
not appear in WSJ. Since these annotations are new
with respect to the WSJ guidelines, it is impossi-
ble to parse these without injecting knowledge of
the annotation guidelines. 3 Nouns are far more
common, comprising 33% of BIO and 30% of WSJ
tokens, the most popular POS tag by far. Addi-
tionally, other POS types listed above (adjectives,
prepositions, determiners, conjunctions) often attach
to nouns. To confirm that nouns were problem-
atic, we modified a first-order parser (no second or-
der features) by adding a feature indicating correct
noun-noun edges, forcing the parser to predict these
edges correctly. Adaptation performance rose on
BIO from 78% without the feature to 87% with the
feature. This indicates that most of the loss comes
from missing these edges.
The primary problem for nouns is the difference
between structures in each domain. The annota-
tion guidelines for the Penn Treebank flattened noun
phrases to simplify annotation (Marcus et al, 1993),
so there is no complex structure to NPs. Ku?bler
(2006) showed that it is difficult to compare the
Penn Treebank to other treebanks with more com-
plex noun structures, such as BIO. Consider theWSJ
phrase ?the New York State Insurance Department?.
The annotation indicates a flat structure, where ev-
2We measured these drops on several other dependency
parsers and found similar results.
3For example, the phrase ?(R = 28% (10/26); K=10% (3/29);
chi2 test: p=0.014).?
ery token is headed by ?Department?. In contrast,
a similar BIO phrase has a very different structure,
pursuant to the BIO guidelines. For ?the detoxi-
cation enzyme glutathione transferase P1-1?, ?en-
zyme? is the head of the NP, ?P1-1? is the head of
?transferase?, and ?transferase? is the head of ?glu-
tathione?. Since the guidelines differ, we observe no
corresponding structure in the WSJ. It is telling that
the parser labels this BIO example by attaching ev-
ery token to the final proper noun ?P1-1?, exactly as
the WSJ guidelines indicate. Unlabeled data cannot
indicate that BIO uses a different standard.
Another problem concerns appositives. For ex-
ample, the phrase ?Howard Mosher, president and
chief executive officer,? has ?Mosher? as the head
of ?Howard? and of the appositive NP delimited by
commas. While similar constructions occur in BIO,
there are no commas to indicate this. An example is
the above BIO NP, in which the phrase ?glutathione
transferase P1-1? is an appositive indicating which
?enzyme? is meant. However, since there are no
commas, the parser thinks ?P1-1? is the head. How-
ever, there are not many right to left attaching nouns.
In addition to a change in the annotation guide-
lines for NPs, we observed an important difference
in the distribution of POS tags. NN tags were almost
twice as likely in the BIO domain (14% in WSJ and
25% in BIO). NNP tags, which are close to 10% of
the tags in WSJ, are nonexistent in BIO (.24%). The
cause for this is clear when the annotation guide-
lines are considered. The proper nouns in WSJ are
names of companies, people and places, while in
BIO they are names of genes, proteins and chemi-
cals. However, for BIO these nouns are labeled NN
instead of NNP. This decision effectively removes
NNP from the BIO domain and renders all features
that depend on the NNP tag ineffective. In our above
BIO NP example, all nouns are labeled NN, whereas
the WSJ example contains NNP tags. The largest
tri-gram differences involve nouns, such as NN-NN-
NN, NNP-NNP-NNP, NN-IN-NN, and IN-NN-NN.
However, when we examine the coarse POS tags,
which do not distinguish between nouns, these dif-
ferences disappear. This indicates that while the
overall distribution of POS tags is similar between
the domains, the fine grained tags differ. These fine
grained tags provide more information than coarse
tags; experiments that removed fine grained tags
1052
hurt WSJ performance but did not affect BIO.
Finally, we examined the effect of unknown
words. Not surprisingly, the most significant dif-
ferences in error rates concerned dependencies be-
tween words of which one or both were unknown
to the parser. For two words that were seen in the
training data loss was 4%, for a single unknown
word loss was 15%, and 26% when both words were
unknown. Both words were unknown only 5% of
the time in BIO, while one of the words being un-
known was more common, reflecting 27% of deci-
sions. Upon further investigation, the majority of
unknown words were nouns, which indicates that
unknown word errors were caused by the problems
discussed above.
Recent theoretical work on domain adapta-
tion (Ben-David et al, 2006) attributes adaptation
loss to two sources: the difference in the distribu-
tion between domains and the difference in label-
ing functions. Adaptation techniques focus on the
former since it is impossible to determine the lat-
ter without knowledge of the labeling function. In
parsing adaptation, the former corresponds to a dif-
ference between the features seen in each domain,
such as new words in the target domain. The de-
cision function corresponds to differences between
annotation guidelines between two domains. Our er-
ror analysis suggests that the primary cause of loss
from adaptation is from differences in the annotation
guidelines themselves. Therefore, significant im-
provements cannot be made without specific knowl-
edge of the target domain?s annotation standards. No
amount of source training data can help if no rele-
vant structure exists in the data. Given the results
for the domain adaptation track, it appears no team
successfully adapted a state-of-the-art parser.
3 Adaptation Approaches
We survey the main approaches we explored for this
task. While some of these approaches provided a
modest performance boost to a simple parser (lim-
ited data and first-order features), no method added
any performance to our best parser (all data and
second-order features).
3.1 Features
A natural approach to improving parsing is to mod-
ify the feature set, both by removing features less
likely to transfer and by adding features that are
more likely to transfer. We began with the first ap-
proach and removed a large number of features that
we believed transfered poorly, such as most features
for noun-noun edges. We obtained a small improve-
ment in BIO performance on limited data only. We
then added several different types of features, specif-
ically designed to improve noun phrase construc-
tions, such as features based on the lexical position
of nouns (common position in NPs), frequency of
occurrence, and NP chunking information. For ex-
ample, trained on in-domain data, nouns that occur
more often tend to be heads. However, none of these
features transfered between domains.
A final type of feature we added was based on
the behavior of nouns, adjectives and verbs in each
domain. We constructed a feature representation
of words based on adjacent POS and words and
clustered words using an algorithm similar to that
of Saul and Pereira (1997). For example, our clus-
tering algorithm grouped first names in one group
and measurements in another. We then added the
cluster membership as a lexical feature to the parser.
None of the resulting features helped adaptation.
3.2 Diversity
Training diversity may be an effective source for
adaptation. We began by adding information from
multiple different parsers, which has been shown
to improve in-domain parsing. We added features
indicating when an edge was predicted by another
parser and if an edge crossed a predicted edge, as
well as conjunctions with edge types. This failed
to improve BIO accuracy since these features were
less reliable at test time. Next, we tried instance
bagging (Breiman, 1996) to generate some diversity
among parsers. We selected with replacement 2000
training examples from the training data and trained
three parsers. Each parser then tagged the remain-
ing 13K sentences, yielding 39K parsed sentences.
We then shuffled these sentences and trained a final
parser. This failed to improve performance, possibly
because of conflicting annotations or because of lack
of sufficient diversity. To address conflicting annota-
1053
tions, we added slack variables to the MIRA learn-
ing algorithm (Crammer et al, 2006) used to train
the parsers, without success. We measured diversity
by comparing the parses of each model. The dif-
ference in annotation agreement between the three
instance bagging parsers was about half the differ-
ence between these parsers and the gold annotations.
While we believe this is not enough diversity, it was
not feasible to repeat our experiment with a large
number of parsers.
3.3 Target Focused Learning
Another approach to adaptation is to favor training
examples that are similar to the target. We first mod-
ified the weight given by the parser to each training
sentence based on the similarity of the sentence to
target domain sentences. This can be done by mod-
ifying the loss to limit updates in cases where the
sentence does not reflect the target domain. We tried
a number of criteria to weigh sentences without suc-
cess, including sentence length and number of verbs.
Next, we trained a discriminative model on the pro-
vided unlabeled data to predict the domain of each
sentence based on POS n-grams in the sentence.
Training sentences with a higher probability of be-
ing in the target domain received higher weights,
also without success. Further experiments showed
that any decrease in training data hurt parser perfor-
mance. It would seem that the parser has no dif-
ficulty learning important training sentences in the
presence of unimportant training examples.
A related idea focused on words, weighing highly
tokens that appeared frequently in the target domain.
We scaled the loss associated with a token by a fac-
tor proportional to its frequency in the target do-
main. We found certain scaling techniques obtained
tiny improvements on the target domain that, while
significant compared to competition results, are not
statistically significant. We also attempted a sim-
ilar approach on the feature level. A very predic-
tive source domain feature is not useful if it does
not appear in the target domain. However, limiting
the feature space to target domain features had no
effect. Instead, we scaled each feature?s value by a
factor proportional to its frequency in the target do-
main and trained the parser on these scaled feature
values. We obtained small improvements on small
amounts of training data.
4 Future Directions
Given our pessimistic analysis and the long list of
failed methods, one may wonder if parser adapta-
tion is possible at all. We believe that it is. First,
there may be room for adaptation with our domains
if a common annotation scheme is used. Second,
we have stressed that typical adaptation, modifying
a model trained on the source domain, will fail but
there may be unsupervised parsing techniques that
improve performance after adaptation, such as a rule
based NP parser for BIO based on knowledge of the
annotations. However, this approach is unsatisfying
as it does not allow general purpose adaptation.
5 Acknowledgments
We thank Joel Wallenberg and Nikhil Dinesh for
their informative and helpful linguistic expertise,
Kevin Lerman for his edge labeler code, and Koby
Crammer for helpful conversations. Dredze is sup-
ported by a NDSEG fellowship; Ganchev and Taluk-
dar by NSF ITR EIA-0205448; and Blitzer by
DARPA under Contract No. NBCHD03001. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
author(s) and do not necessarily reflect the views of
the DARPA or the Department of Interior-National
Business Center (DOI-NBC).
References
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In NIPS.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585, Mar.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Sandra Ku?bler. 2006. How do treebank annotation
schemes influence parsing results? or how not to com-
pare apples and oranges. In RANLP.
1054
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency parsing with a two-
stage discriminative parser. In Conference on Natural
Language Learning (CoNLL).
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
Lawrence Saul and Fernando Pereira. 1997. Aggre-
gate and mixed-order markov models for statistical
language modeling. In EMNLP.
1055
Evaluation challenges in large-scale document summarization
Dragomir R. Radev
U. of Michigan
radev@umich.edu
Wai Lam
Chinese U. of Hong Kong
wlam@se.cuhk.edu.hk
Arda C?elebi
USC/ISI
ardax@isi.edu
Simone Teufel
U. of Cambridge
simone.teufel@cl.cam.ac.uk
John Blitzer
U. of Pennsylvania
blitzer@seas.upenn.edu
Danyu Liu
U. of Alabama
liudy@cis.uab.edu
Horacio Saggion
U. of Sheffield
h.saggion@dcs.shef.ac.uk
Hong Qi
U. of Michigan
hqi@umich.edu
Elliott Drabek
Johns Hopkins U.
edrabek@cs.jhu.edu
Abstract
We present a large-scale meta evaluation
of eight evaluation measures for both
single-document and multi-document
summarizers. To this end we built a
corpus consisting of (a) 100 Million auto-
matic summaries using six summarizers
and baselines at ten summary lengths in
both English and Chinese, (b) more than
10,000 manual abstracts and extracts, and
(c) 200 Million automatic document and
summary retrievals using 20 queries. We
present both qualitative and quantitative
results showing the strengths and draw-
backs of all evaluation methods and how
they rank the different summarizers.
1 Introduction
Automatic document summarization is a field that
has seen increasing attention from the NLP commu-
nity in recent years. In part, this is because sum-
marization incorporates many important aspects of
both natural language understanding and natural lan-
guage generation. In part it is because effective auto-
matic summarization would be useful in a variety of
areas. Unfortunately, evaluating automatic summa-
rization in a standard and inexpensive way is a diffi-
cult task (Mani et al, 2001). Traditional large-scale
evaluations are either too simplistic (using measures
like precision, recall, and percent agreement which
(1) don?t take chance agreement into account and (2)
don?t account for the fact that human judges don?t
agree which sentences should be in a summary) or
too expensive (an approach using manual judge-
ments can scale up to a few hundred summaries but
not to tens or hundreds of thousands).
In this paper, we present a comparison of six
summarizers as well as a meta-evaluation including
eight measures: Precision/Recall, Percent Agree-
ment, Kappa, Relative Utility, Relevance Correla-
tion, and three types of Content-Based measures
(cosine, longest common subsequence, and word
overlap). We found that while all measures tend
to rank summarizers in different orders, measures
like Kappa, Relative Utility, Relevance Correlation
and Content-Based each offer significant advantages
over the more simplistic methods.
2 Data, Annotation, and Experimental
Design
We performed our experiments on the Hong Kong
News corpus provided by the Hong Kong SAR of
the People?s Republic of China (LDC catalog num-
ber LDC2000T46). It contains 18,146 pairs of par-
allel documents in English and Chinese. The texts
are not typical news articles. The Hong Kong News-
paper mainly publishes announcements of the local
administration and descriptions of municipal events,
such as an anniversary of the fire department, or sea-
sonal festivals. We tokenized the corpus to iden-
tify headlines and sentence boundaries. For the En-
glish text, we used a lemmatizer for nouns and verbs.
We also segmented the Chinese documents using the
tool provided at http://www.mandarintools.com.
Several steps of the meta evaluation that we per-
formed involved human annotator support. First, we
Cluster 2 Meetings with foreign leaders
Cluster 46 Improving Employment Opportunities
Cluster 54 Illegal immigrants
Cluster 60 Customs staff doing good job.
Cluster 61 Permits for charitable fund raising
Cluster 62 Y2K readiness
Cluster 112 Autumn and sports carnivals
Cluster 125 Narcotics Rehabilitation
Cluster 199 Intellectual Property Rights
Cluster 241 Fire safety, building management concerns
Cluster 323 Battle against disc piracy
Cluster 398 Flu results in Health Controls
Cluster 447 Housing (Amendment) Bill Brings Assorted Improvements
Cluster 551 Natural disaster victims aided
Cluster 827 Health education for youngsters
Cluster 885 Customs combats contraband/dutiable cigarette operations
Cluster 883 Public health concerns cause food-business closings
Cluster 1014 Traffic Safety Enforcement
Cluster 1018 Flower shows
Cluster 1197 Museums: exhibits/hours
Figure 1: Twenty queries created by the LDC for
this experiment.
asked LDC to build a set of queries (Figure 1). Each
of these queries produced a cluster of relevant doc-
uments. Twenty of these clusters were used in the
experiments in this paper.
Additionally, we needed manual summaries or ex-
tracts for reference. The LDC annotators produced
summaries for each document in all clusters. In or-
der to produce human extracts, our judges also la-
beled sentences with ?relevance judgements?, which
indicate the relevance of sentence to the topic of the
document. The relevance judgements for sentences
range from 0 (irrelevant) to 10 (essential). As in
(Radev et al, 2000), in order to create an extract of
a certain length, we simply extract the top scoring
sentences that add up to that length.
For each target summary length, we produce an
extract using a summarizer or baseline. Then we
compare the output of the summarizer or baseline
with the extract produced from the human relevance
judgements. Both the summarizers and the evalua-
tion measures are described in greater detail in the
next two sections.
2.1 Summarizers and baselines
This section briefly describes the summarizers we
used in the evaluation. All summarizers take as input
a target length (n%) and a document (or cluster) split
into sentences. Their output is an n% extract of the
document (or cluster).
? MEAD (Radev et al, 2000): MEAD is
a centroid-based extractive summarizer that
scores sentences based on sentence-level and
inter-sentence features which indicate the qual-
ity of the sentence as a summary sentence. It
then chooses the top-ranked sentences for in-
clusion in the output summary. MEAD runs on
both English documents and on BIG5-encoded
Chinese. We tested the summarizer in both lan-
guages.
? WEBS (Websumm (Mani and Bloedorn,
2000)): can be used to produce generic and
query-based summaries. Websumm uses a
graph-connectivity model and operates under
the assumption that nodes which are connected
to many other nodes are likely to carry salient
information.
? SUMM (Summarist (Hovy and Lin, 1999)):
an extractive summarizer based on topic signa-
tures.
? ALGN (alignment-based): We ran a sentence
alignment algorithm (Gale and Church, 1993)
for each pair of English and Chinese stories.
We used it to automatically generate Chinese
?manual? extracts from the English manual ex-
tracts we received from LDC.
? LEAD (lead-based): n% sentences are chosen
from the beginning of the text.
? RAND (random): n% sentences are chosen at
random.
The six summarizers were run at ten different tar-
get lengths to produce more than 100 million sum-
maries (Figure 2). For the purpose of this paper, we
only focus on a small portion of the possible experi-
ments that our corpus can facilitate.
3 Summary Evaluation Techniques
We used three general types of evaluation measures:
co-selection, content-based similarity, and relevance
correlation. Co-selection measures include preci-
sion and recall of co-selected sentences, relative util-
ity (Radev et al, 2000), and Kappa (Siegel and
Castellan, 1988; Carletta, 1996). Co-selection meth-
ods have some restrictions: they only work for ex-
tractive summarizers. Two manual summaries of the
same input do not in general share many identical
sentences. We address this weakness of co-selection
Lengths #dj
05W 05S 10W 10S 20W 20S 30W 30S 40W 40S FD
E-FD - - - - - - - - - - x 40
E-LD X X X X x x X X X X - 440
E-RA X X X X x x X X X X - 440
E-MO x x X x x x X x X x - 540
E-M2 - - - - - X - - - - - 20
E-M3 - - - - - X - - - - - 8
E-S2 - - - - - X - - - - - 8
E-WS - X - X x x - X - X - 160
E-WQ - - - - - X - - - - - 10
E-LC - - - - - - x - - - - 40
E-CY - X - X - x - X - X - 120
E-AL X X X X X X X X X X - 200
E-AR X X X X X X X X X X - 200
E-AM X X X X X X X X X X - 200
C-FD - - - - - - - - - - x 40
C-LD X X X X x x X X X X - 240
C-RA X X X X x x X X X X - 240
C-MO X x X x x x X x X x - 320
C-M2 - - - - - X - - - - - 20
C-CY - X - X - x - X - X - 120
C-AL X X X X X X X X X X - 180
C-AR X X X X X X X X X X - 200
C-AM - X X X X X X X X - 120
X-FD - - - - - - - - - - x 40
X-LD X X X X x x X X X X - 240
X-RA X X X X x x X X X X - 240
X-MO X x X x x x X x X x - 320
X-M2 - - - - - X - - - - - 20
X-CY - X - X - x - X - X - 120
X-AL X X X X X X X X X X - 140
X-AR X X X X X X X X X X - 160
X-AM - X X X X X X X - X - 120
Figure 2: All runs performed (X = 20 clusters, x = 10 clusters). Language: E = English, C = Chinese,
X = cross-lingual; Summarizer: LD=LEAD, RA=RAND, WS=WEBS, WQ=WEBS-query based, etc.; S =
sentence-based, W = word-based; #dj = number of ?docjudges? (ranked lists of documents and summaries).
Target lengths above 50% are not shown in this table for lack of space. Each run is available using two
different retrieval schemes. We report results using the cross-lingual retrievals in a separate paper.
measures with several content-based similarity mea-
sures. The similarity measures we use are word
overlap, longest common subsequence, and cosine.
One advantage of similarity measures is that they
can compare manual and automatic extracts with
manual abstracts. To our knowledge, no system-
atic experiments about agreement on the task of
summary writing have been performed before. We
use similarity measures to measure interjudge agree-
ment among three judges per topic. We also ap-
ply the measures between human extracts and sum-
maries, which answers the question if human ex-
tracts are more similar to automatic extracts or to
human summaries.
The third group of evaluation measures includes
relevance correlation. It shows the relative perfor-
mance of a summary: how much the performance
of document retrieval decreases when indexing sum-
maries rather than full texts.
Task-based evaluations (e.g., SUMMAC (Mani
et al, 2001), DUC (Harman and Marcu, 2001), or
(Tombros et al, 1998) measure human performance
using the summaries for a certain task (after the
summaries are created). Although they can be a
very effective way of measuring summary quality,
task-based evaluations are prohibitively expensive at
large scales. In this project, we didn?t perform any
task-based evaluations as they would not be appro-
priate at the scale of millions of summaries.
3.1 Evaluation by sentence co-selection
For each document and target length we produce
three extracts from the three different judges, which
we label throughout as J1, J2, and J3.
We used the rates 5%, 10%, 20%, 30%, 40% for
most experiments. For some experiments, we also
consider summaries of 50%, 60%, 70%, 80% and
90% of the original length of the documents. Figure
3 shows some abbreviations for co-selection that we
will use throughout this section.
3.1.1 Precision and Recall
Precision and recall are defined as:
PJ2 (J1) =
A
A+ C
,RJ2 (J1) =
A
A+ B
J2
Sentence in
Extract
Sentence not
in Extract
Sentence in
Extract
A B A+ B
J1 Sentence not
in Extract
C D C +D
A+ C B +D N = A +
B+C+D
Figure 3: Contingency table comparing sentences
extracted by the system and the judges.
In our case, each set of documents which is com-
pared has the same number of sentences and also
the same number of sentences are extracted; thus
P = R.
The average precision Pavg(SY STEM) and re-
call Ravg(SY STEM) are calculated by summing
over individual judges and normalizing. The aver-
age interjudge precision and recall is computed by
averaging over all judge pairs.
However, precision and recall do not take chance
agreement into account. The amount of agreement
one would expect two judges to reach by chance de-
pends on the number and relative proportions of the
categories used by the coders. The next section on
Kappa shows that chance agreement is very high in
extractive summarization.
3.1.2 Kappa
Kappa (Siegel and Castellan, 1988) is an evalua-
tion measure which is increasingly used in NLP an-
notation work (Krippendorff, 1980; Carletta, 1996).
Kappa has the following advantages over P and R:
? It factors out random agreement. Random
agreement is defined as the level of agreement
which would be reached by random annotation
using the same distribution of categories as the
real annotators.
? It allows for comparisons between arbitrary
numbers of annotators and items.
? It treats less frequent categories as more im-
portant (in our case: selected sentences), simi-
larly to precision and recall but it also consid-
ers (with a smaller weight) more frequent cate-
gories as well.
The Kappa coefficient controls agreement P (A)
by taking into account agreement by chance P (E) :
K =
P (A)? P (E)
1? P (E)
No matter how many items or annotators, or how
the categories are distributed, K = 0 when there is
no agreement other than what would be expected by
chance, and K = 1 when agreement is perfect. If
two annotators agree less than expected by chance,
Kappa can also be negative.
We report Kappa between three annotators in the
case of human agreement, and between three hu-
mans and a system (i.e. four judges) in the next sec-
tion.
3.1.3 Relative Utility
Relative Utility (RU) (Radev et al, 2000) is tested
on a large corpus for the first time in this project.
RU takes into account chance agreement as a lower
bound and interjudge agreement as an upper bound
of performance. RU allows judges and summarizers
to pick different sentences with similar content in
their summaries without penalizing them for doing
so. Each judge is asked to indicate the importance
of each sentence in a cluster on a scale from 0 to
10. Judges also specify which sentences subsume or
paraphrase each other. In relative utility, the score
of an automatic summary increases with the impor-
tance of the sentences that it includes but goes down
with the inclusion of redundant sentences.
3.2 Content-based Similarity measures
Content-based similarity measures compute the sim-
ilarity between two summaries at a more fine-
grained level than just sentences. For each automatic
extract S and similarity measure M we compute the
following number:
sim(M,S, {J1, J2, J3}) =
M(S, J1) +M(S, J2) +M(S, J3)
3
We used several content-based similarity mea-
sures that take into account different properties of
the text:
Cosine similarity is computed using the follow-
ing formula (Salton, 1988):
cos(X,Y ) =
?
xi ? yi
??
(xi)2 ?
??
(yi)2
where X and Y are text representations based on
the vector space model.
Longest Common Subsequence is computed as
follows:
lcs(X,Y ) = (length(X) + length(Y )? d(X,Y ))/2
where X and Y are representations based on
sequences and where lcs(X,Y ) is the length of
the longest common subsequence between X and
Y , length(X) is the length of the string X , and
d(X,Y ) is the minimum number of deletion and in-
sertions needed to transform X into Y (Crochemore
and Rytter, 1994).
3.3 Relevance Correlation
Relevance correlation (RC) is a new measure for as-
sessing the relative decrease in retrieval performance
when indexing summaries instead of full documents.
The idea behind it is similar to (Sparck-Jones and
Sakai, 2001). In that experiment, Sparck-Jones and
Sakai determine that short summaries are good sub-
stitutes for full documents at the high precision end.
With RC we attempt to rank all documents given a
query.
Suppose that given a queryQ and a corpus of doc-
uments Di, a search engine ranks all documents in
Di according to their relevance to the query Q. If
instead of the corpus Di, the respective summaries
of all documents are substituted for the full docu-
ments and the resulting corpus of summaries Si is
ranked by the same retrieval engine for relevance to
the query, a different ranking will be obtained. If
the summaries are good surrogates for the full docu-
ments, then it can be expected that rankings will be
similar.
There exist several methods for measuring the
similarity of rankings. One such method is Kendall?s
tau and another is Spearman?s rank correlation. Both
methods are quite appropriate for the task that we
want to perform; however, since search engines pro-
duce relevance scores in addition to rankings, we
can use a stronger similarity test, linear correlation
between retrieval scores. When two identical rank-
ings are compared, their correlation is 1. Two com-
pletely independent rankings result in a score of 0
while two rankings that are reverse versions of one
another have a score of -1. Although rank correla-
tion seems to be another valid measure, given the
large number of irrelevant documents per query re-
sulting in a large number of tied ranks, we opted for
linear correlation. Interestingly enough, linear cor-
relation and rank correlation agreed with each other.
Relevance correlation r is defined as the linear
correlation of the relevance scores (x and y) as-
signed by two different IR algorithms on the same
set of documents or by the same IR algorithm on
different data sets:
r =
?
i
(xi ? x)(yi ? y)
??
i
(xi ? x)2
??
i
(yi ? y)2
Here x and y are the means of the relevance scores
for the document sequence.
We preprocess the documents and use Smart to
index and retrieve them. After the retrieval process,
each summary is associated with a score indicating
the relevance of the summary to the query. The
relevance score is actually calculated as the inner
product of the summary vector and the query vec-
tor. Based on the relevance score, we can produce a
full ranking of all the summaries in the corpus.
In contrast to (Brandow et al, 1995) who run 12
Boolean queries on a corpus of 21,000 documents
and compare three types of documents (full docu-
ments, lead extracts, and ANES extracts), we mea-
sure retrieval performance under more than 300 con-
ditions (by language, summary length, retrieval pol-
icy for 8 summarizers or baselines).
4 Results
This section reports results for the summarizers and
baselines described above. We relied directly on the
relevance judgements to create ?manual extracts? to
use as gold standards for evaluating the English sys-
tems. To evaluate Chinese, we made use of a ta-
ble of automatically produced alignments. While
the accuracy of the alignments is quite high, we
have not thoroughly measured the errors produced
when mapping target English summaries into Chi-
nese. This will be done in future work.
4.1 Co-selection results
Co-selection agreement (Section 3.1) is reported in
Figures 4, and 5). The tables assume human perfor-
mance is the upper bound, the next rows compare
the different summarizers.
Figure 4 shows results for precision and recall.
We observe the effect of a dependence of the nu-
merical results on the length of the summary, which
is a well-known fact from information retrieval eval-
uations.
Websumm has an advantage over MEAD for
longer summaries but not for 20% or less. Lead
summaries perform better than all the automatic
summarizers, and better than the human judges.
This result usually occurs when the judges choose
different, but early sentences. Human judgements
overtake the lead baseline for summaries of length
50% or more.
5% 10% 20% 30% 40%
Humans .187 .246 .379 .467 .579
MEAD .160 .231 .351 .420 .519
WEBS .310 .305 .358 .439 .543
LEAD .354 .387 .447 .483 .583
RAND .094 .113 .224 .357 .432
Figure 4: Results in precision=recall (averaged over
20 clusters).
Figure 5 shows results using Kappa. Random
agreement is 0 by definition between a random pro-
cess and a non-random process.
While the results are overall rather low, the num-
bers still show the following trends:
? MEAD outperforms Websumm for all but the
5% target length.
? Lead summaries perform best below 20%,
whereas human agreement is higher after that.
? There is a rather large difference between the
two summarizers and the humans (except for
the 5% case for Websumm). This numerical
difference is relatively higher than for any other
co-selection measure treated here.
? Random is overall the worst performer.
? Agreement improves with summary length.
Figures 6 and 7 summarize the results obtained
through Relative Utility. As the figures indicate,
random performance is quite high although all non-
random methods outperform it significantly. Fur-
ther, and in contrast with other co-selection evalua-
tion criteria, in both the single- and multi-document
5% 10% 20% 30% 40%
Humans .127 .157 .194 .225 .274
MEAD .109 .136 .168 .192 .230
WEBS .138 .128 .146 .159 .192
LEAD .180 .198 .213 .220 .261
RAND .064 .081 .097 .116 .137
Figure 5: Results in kappa, averaged over 20 clus-
ters.
case MEAD outperforms LEAD for shorter sum-
maries (5-30%). The lower bound (R) represents the
average performance of all extracts at the given sum-
mary length while the upper bound (J) is the inter-
judge agreement among the three judges.
5% 10% 20% 30% 40%
R 0.66 0.68 0.71 0.74 0.76
RAND 0.67 0.67 0.71 0.75 0.77
WEBS 0.72 0.73 0.76 0.79 0.82
LEAD 0.72 0.73 0.77 0.80 0.83
MEAD 0.78 0.79 0.79 0.81 0.83
J 0.80 0.81 0.83 0.85 0.87
Figure 6: RU per summarizer and summary length
(Single-document).
5% 10% 20% 30% 40%
R 0.64 0.66 0.69 0.72 0.74
RAND 0.63 0.65 0.71 0.72 0.74
LEAD 0.71 0.71 0.76 0.79 0.82
MEAD 0.73 0.75 0.78 0.79 0.81
J 0.76 0.78 0.81 0.83 0.85
Figure 7: RU per summarizer and summary length
(Multi-document).
4.2 Content-based results
The results obtained for a subset of target lengths
using content-based evaluation can be seen in Fig-
ures 8 and 9. In all our experiments with tf ? idf -
weighted cosine, the lead-based summarizer ob-
tained results close to the judges in most of the target
lengths while MEAD is ranked in second position.
In all our experiments using longest common sub-
sequence, no system obtained better results in the
majority of the cases.
10% 20% 30% 40%
LEAD 0.55 0.65 0.70 0.79
MEAD 0.46 0.61 0.70 0.78
RAND 0.31 0.47 0.60 0.69
WEBS 0.52 0.60 0.68 0.77
Figure 8: Cosine (tf?idf ). Average over 10 clusters.
10% 20% 30% 40%
LEAD 0.47 0.55 0.60 0.70
MEAD 0.37 0.52 0.61 0.70
RAND 0.25 0.38 0.50 0.58
WEBS 0.39 0.45 0.53 0.64
Figure 9: Longest Common Subsequence. Average
over 10 clusters.
The numbers obtained in the evaluation of Chi-
nese summaries for cosine and longest common sub-
sequence can be seen in Figures 10 and 11. Both
measures identify MEAD as the summarizer that
produced results closer to the ideal summaries (these
results also were observed across measures and text
representations).
10% 20% 30% 40%
SUMM 0.44 0.65 0.71 0.78
LEAD 0.54 0.63 0.68 0.77
MEAD 0.49 0.65 0.74 0.82
RAND 0.31 0.50 0.65 0.71
Figure 10: Chinese Summaries. Cosine (tf ? idf ).
Average over 10 clusters. Vector space of Words as
Text Representation.
10% 20% 30% 40%
SUMM 0.32 0.53 0.57 0.65
LEAD 0.42 0.49 0.54 0.64
MEAD 0.35 0.50 0.60 0.70
RAND 0.21 0.35 0.49 0.54
Figure 11: Chinese Summaries. Longest Common
Subsequence. Average over 10 clusters. Chinese
Words as Text Representation.
We have based this evaluation on target sum-
maries produced by LDC assessors, although other
alternatives exist. Content-based similarity mea-
sures do not require the target summary to be a sub-
set of sentences from the source document, thus,
content evaluation based on similarity measures
can be done using summaries published with the
source documents which are in many cases available
(Teufel and Moens, 1997; Saggion, 2000).
4.3 Relevance Correlation results
We present several results using Relevance Correla-
tion. Figures 12 and 13 show how RC changes de-
pending on the summarizer and the language used.
RC is as high as 1.0 when full documents (FD) are
compared to themselves. One can notice that even
random extracts get a relatively high RC score. It is
also worth observing that Chinese summaries score
lower than their corresponding English summaries.
Figure 14 shows the effects of summary length and
summarizers on RC. As one might expect, longer
summaries carry more of the content of the full doc-
ument than shorter ones. At the same time, the rel-
ative performance of the different summarizers re-
mains the same across compression rates.
C112 C125 C241 C323 C551 AVG10
FD 1.00 1.00 1.00 1.00 1.00 1.000
MEAD 0.91 0.92 0.93 0.92 0.90 0.903
WEBS 0.88 0.82 0.89 0.91 0.88 0.843
LEAD 0.80 0.80 0.84 0.85 0.81 0.802
RAND 0.80 0.78 0.87 0.85 0.79 0.800
SUMM 0.77 0.79 0.85 0.88 0.81 0.775
Figure 12: RC per summarizer (English 20%).
C112 C125 C241 C323 C551 AVG10
FD 1.00 1.00 1.00 1.00 1.00 1.000
MEAD 0.78 0.87 0.93 0.66 0.91 0.850
SUMM 0.76 0.75 0.85 0.84 0.75 0.755
RAND 0.71 0.75 0.85 0.60 0.74 0.744
ALGN 0.74 0.72 0.83 0.95 0.72 0.738
LEAD 0.72 0.71 0.83 0.58 0.75 0.733
Figure 13: RC per summarizer (Chinese, 20%).
5% 10% 20% 30% 40%
FD 1.000 1.000 1.000 1.000 1.000
MEAD 0.724 0.834 0.916 0.946 0.962
WEBS 0.730 0.804 0.876 0.912 0.936
LEAD 0.660 0.730 0.820 0.880 0.906
SUMM 0.622 0.710 0.820 0.848 0.862
RAND 0.554 0.708 0.818 0.884 0.922
Figure 14: RC per summary length and summarizer.
5 Conclusion
This paper describes several contributions to text
summarization:
First, we observed that different measures rank
summaries differently, although most of them
showed that ?intelligent? summarizers outperform
lead-based summaries which is encouraging given
that previous results had cast doubt on the ability of
summarizers to do better than simple baselines.
Second, we found that measures like Kappa, Rel-
ative Utility, Relevance Correlation and Content-
Based, each offer significant advantages over more
simplistic methods like Precision, Recall, and Per-
cent Agreement with respect to scalability, applica-
bility to multidocument summaries, and ability to
include human and chance agreement. Figure 15
Property Prec, recall Kappa Normalized RU Word overlap, cosine, LCS Relevance Correlation
Intrinsic (I)/extrinsic (E) I I I I E
Agreement between human extracts X X X X X
Agreement human extracts and automatic extracts X X X X X
Agreement human abstracts and human extracts X
Non-binary decisions X X
Takes random agreement into account by design X X
Full documents vs. extracts X X
Systems with different sentence segmentation X X
Multidocument extracts X X X X
Full corpus coverage X X
Figure 15: Properties of evaluation measures used in this project.
presents a short comparison of all these evaluation
measures.
Third, we performed extensive experiments using
a new evaluation measure, Relevance Correlation,
which measures how well a summary can be used
to replace a document for retrieval purposes.
Finally, we have packaged the code used for this
project into a summarization evaluation toolkit and
produced what we believe is the largest and most
complete annotated corpus for further research in
text summarization. The corpus and related software
is slated for release by the LDC in mid 2003.
References
Ron Brandow, Karl Mitze, and Lisa F. Rau. 1995. Auto-
matic Condensation of Electronic Publications by Sen-
tence Selection. Information Processing and Manage-
ment, 31(5):675?685.
Jean Carletta. 1996. Assessing Agreement on Classifica-
tion Tasks: The Kappa Statistic. CL, 22(2):249?254.
Maxime Crochemore and Wojciech Rytter. 1994. Text
Algorithms. Oxford University Press.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Donna Harman and Daniel Marcu, editors. 2001. Pro-
ceedings of the 1st Document Understanding Confer-
ence. New Orleans, LA, September.
Eduard Hovy and Chin Yew Lin. 1999. Automated Text
Summarization in SUMMARIST. In Inderjeet Mani
and Mark T. Maybury, editors, Advances in Automatic
Text Summarization, pages 81?94. The MIT Press.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Sage Publications, Bev-
erly Hills, CA.
Inderjeet Mani and Eric Bloedorn. 2000. Summariz-
ing Similarities and Differences Among Related Doc-
uments. Information Retrieval, 1(1).
Inderjeet Mani, The?re`se Firmin, David House, Gary
Klein, Beth Sundheim, and Lynette Hirschman. 2001.
The TIPSTER SUMMAC Text Summarization Evalu-
ation. In Natural Language Engineering.
Dragomir R. Radev, Hongyan Jing, and Malgorzata
Budzikowska. 2000. Centroid-Based Summarization
of Multiple Documents: Sentence Extraction, Utility-
Based Evaluation, and User Studies. In Proceedings
of the Workshop on Automatic Summarization at the
6th Applied Natural Language Processing Conference
and the 1st Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Seattle, WA, April.
Horacio Saggion. 2000. Ge?ne?ration automatique
de re?sume?s par analyse se?lective. Ph.D. the-
sis, De?partement d?informatique et de recherche
ope?rationnelle. Faculte? des arts et des sciences. Uni-
versite? de Montre?al, August.
Gerard Salton. 1988. Automatic Text Processing.
Addison-Wesley Publishing Company.
Sidney Siegel and N. John Jr. Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, Berkeley, CA, 2nd edition.
Karen Sparck-Jones and Tetsuya Sakai. 2001. Generic
Summaries for Indexing in IR. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 190?198, New Orleans, LA, September.
Simone Teufel and Marc Moens. 1997. Sentence Ex-
traction as a Classification Task. In Proceedings of the
Workshop on Intelligent Scalable Text Summarization
at the 35th Meeting of the Association for Computa-
tional Linguistics, and the 8th Conference of the Eu-
ropean Chapter of the Assocation for Computational
Linguistics, Madrid, Spain.
Anastasios Tombros, Mark Sanderson, and Phil Gray.
1998. Advantages of Query Biased Summaries in In-
formation Retrieval. In Eduard Hovy and Dragomir R.
Radev, editors, Proceedings of the AAAI Symposium
on Intelligent Text Summarization, pages 34?43, Stan-
ford, California, USA, March 23?25,. The AAAI
Press.
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440?447,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification
John Blitzer Mark Dredze
Department of Computer and Information Science
University of Pennsylvania
{blitzer|mdredze|pereria@cis.upenn.edu}
Fernando Pereira
Abstract
Automatic sentiment classification has been
extensively studied and applied in recent
years. However, sentiment is expressed dif-
ferently in different domains, and annotating
corpora for every possible domain of interest
is impractical. We investigate domain adap-
tation for sentiment classifiers, focusing on
online reviews for different types of prod-
ucts. First, we extend to sentiment classifi-
cation the recently-proposed structural cor-
respondence learning (SCL) algorithm, re-
ducing the relative error due to adaptation
between domains by an average of 30% over
the original SCL algorithm and 46% over
a supervised baseline. Second, we identify
a measure of domain similarity that corre-
lates well with the potential for adaptation
of a classifier from one domain to another.
This measure could for instance be used to
select a small set of domains to annotate
whose trained classifiers would transfer well
to many other domains.
1 Introduction
Sentiment detection and classification has received
considerable attention recently (Pang et al, 2002;
Turney, 2002; Goldberg and Zhu, 2004). While
movie reviews have been the most studied domain,
sentiment analysis has extended to a number of
new domains, ranging from stock message boards
to congressional floor debates (Das and Chen, 2001;
Thomas et al, 2006). Research results have been
deployed industrially in systems that gauge market
reaction and summarize opinion from Web pages,
discussion boards, and blogs.
With such widely-varying domains, researchers
and engineers who build sentiment classification
systems need to collect and curate data for each new
domain they encounter. Even in the case of market
analysis, if automatic sentiment classification were
to be used across a wide range of domains, the ef-
fort to annotate corpora for each domain may be-
come prohibitive, especially since product features
change over time. We envision a scenario in which
developers annotate corpora for a small number of
domains, train classifiers on those corpora, and then
apply them to other similar corpora. However, this
approach raises two important questions. First, it
is well known that trained classifiers lose accuracy
when the test data distribution is significantly differ-
ent from the training data distribution 1. Second, it is
not clear which notion of domain similarity should
be used to select domains to annotate that would be
good proxies for many other domains.
We propose solutions to these two questions and
evaluate them on a corpus of reviews for four differ-
ent types of products from Amazon: books, DVDs,
electronics, and kitchen appliances2. First, we show
how to extend the recently proposed structural cor-
1For surveys of recent research on domain adaptation, see
the ICML 2006 Workshop on Structural Knowledge Transfer
for Machine Learning (http://gameairesearch.uta.
edu/) and the NIPS 2006 Workshop on Learning when test
and training inputs have different distribution (http://ida.
first.fraunhofer.de/projects/different06/)
2The dataset will be made available by the authors at publi-
cation time.
440
respondence learning (SCL) domain adaptation al-
gorithm (Blitzer et al, 2006) for use in sentiment
classification. A key step in SCL is the selection of
pivot features that are used to link the source and tar-
get domains. We suggest selecting pivots based not
only on their common frequency but also according
to their mutual information with the source labels.
For data as diverse as product reviews, SCL can
sometimes misalign features, resulting in degrada-
tion when we adapt between domains. In our second
extension we show how to correct misalignments us-
ing a very small number of labeled instances.
Second, we evaluate the A-distance (Ben-David
et al, 2006) between domains as measure of the loss
due to adaptation from one to the other. The A-
distance can be measured from unlabeled data, and it
was designed to take into account only divergences
which affect classification accuracy. We show that it
correlates well with adaptation loss, indicating that
we can use the A-distance to select a subset of do-
mains to label as sources.
In the next section we briefly review SCL and in-
troduce our new pivot selection method. Section 3
describes datasets and experimental method. Sec-
tion 4 gives results for SCL and the mutual informa-
tion method for selecting pivot features. Section 5
shows how to correct feature misalignments using a
small amount of labeled target domain data. Sec-
tion 6 motivates the A-distance and shows that it
correlates well with adaptability. We discuss related
work in Section 7 and conclude in Section 8.
2 Structural Correspondence Learning
Before reviewing SCL, we give a brief illustrative
example. Suppose that we are adapting from re-
views of computers to reviews of cell phones. While
many of the features of a good cell phone review are
the same as a computer review ? the words ?excel-
lent? and ?awful? for example ? many words are to-
tally new, like ?reception?. At the same time, many
features which were useful for computers, such as
?dual-core? are no longer useful for cell phones.
Our key intuition is that even when ?good-quality
reception? and ?fast dual-core? are completely dis-
tinct for each domain, if they both have high correla-
tion with ?excellent? and low correlation with ?aw-
ful? on unlabeled data, then we can tentatively align
them. After learning a classifier for computer re-
views, when we see a cell-phone feature like ?good-
quality reception?, we know it should behave in a
roughly similar manner to ?fast dual-core?.
2.1 Algorithm Overview
Given labeled data from a source domain and un-
labeled data from both source and target domains,
SCL first chooses a set ofm pivot features which oc-
cur frequently in both domains. Then, it models the
correlations between the pivot features and all other
features by training linear pivot predictors to predict
occurrences of each pivot in the unlabeled data from
both domains (Ando and Zhang, 2005; Blitzer et al,
2006). The `th pivot predictor is characterized by
its weight vector w`; positive entries in that weight
vector mean that a non-pivot feature (like ?fast dual-
core?) is highly correlated with the corresponding
pivot (like ?excellent?).
The pivot predictor column weight vectors can be
arranged into a matrix W = [w`]n`=1. Let ? ? R
k?d
be the top k left singular vectors of W (here d indi-
cates the total number of features). These vectors are
the principal predictors for our weight space. If we
chose our pivot features well, then we expect these
principal predictors to discriminate among positive
and negative words in both domains.
At training and test time, suppose we observe a
feature vector x. We apply the projection ?x to ob-
tain k new real-valued features. Now we learn a
predictor for the augmented instance ?x, ?x?. If ?
contains meaningful correspondences, then the pre-
dictor which uses ? will perform well in both source
and target domains.
2.2 Selecting Pivots with Mutual Information
The efficacy of SCL depends on the choice of pivot
features. For the part of speech tagging problem
studied by Blitzer et al (2006), frequently-occurring
words in both domains were good choices, since
they often correspond to function words such as
prepositions and determiners, which are good indi-
cators of parts of speech. This is not the case for
sentiment classification, however. Therefore, we re-
quire that pivot features also be good predictors of
the source label. Among those features, we then
choose the ones with highest mutual information to
the source label. Table 1 shows the set-symmetric
441
SCL, not SCL-MI SCL-MI, not SCL
book one <num> so all a must a wonderful loved it
very about they like weak don?t waste awful
good when highly recommended and easy
Table 1: Top pivots selected by SCL, but not SCL-
MI (left) and vice-versa (right)
differences between the two methods for pivot selec-
tion when adapting a classifier from books to kitchen
appliances. We refer throughout the rest of this work
to our method for selecting pivots as SCL-MI.
3 Dataset and Baseline
We constructed a new dataset for sentiment domain
adaptation by selecting Amazon product reviews for
four different product types: books, DVDs, electron-
ics and kitchen appliances. Each review consists of
a rating (0-5 stars), a reviewer name and location,
a product name, a review title and date, and the re-
view text. Reviews with rating > 3 were labeled
positive, those with rating < 3 were labeled neg-
ative, and the rest discarded because their polarity
was ambiguous. After this conversion, we had 1000
positive and 1000 negative examples for each do-
main, the same balanced composition as the polarity
dataset (Pang et al, 2002). In addition to the labeled
data, we included between 3685 (DVDs) and 5945
(kitchen) instances of unlabeled data. The size of the
unlabeled data was limited primarily by the number
of reviews we could crawl and download from the
Amazon website. Since we were able to obtain la-
bels for all of the reviews, we also ensured that they
were balanced between positive and negative exam-
ples, as well.
While the polarity dataset is a popular choice in
the literature, we were unable to use it for our task.
Our method requires many unlabeled reviews and
despite a large number of IMDB reviews available
online, the extensive curation requirements made
preparing a large amount of data difficult 3.
For classification, we use linear predictors on un-
igram and bigram features, trained to minimize the
Huber loss with stochastic gradient descent (Zhang,
3For a description of the construction of the polarity
dataset, see http://www.cs.cornell.edu/people/
pabo/movie-review-data/.
2004). On the polarity dataset, this model matches
the results reported by Pang et al (2002). When we
report results with SCL and SCL-MI, we require that
pivots occur in more than five documents in each do-
main. We set k, the number of singular vectors of the
weight matrix, to 50.
4 Experiments with SCL and SCL-MI
Each labeled dataset was split into a training set of
1600 instances and a test set of 400 instances. All
the experiments use a classifier trained on the train-
ing set of one domain and tested on the test set of
a possibly different domain. The baseline is a lin-
ear classifier trained without adaptation, while the
gold standard is an in-domain classifier trained on
the same domain as it is tested.
Figure 1 gives accuracies for all pairs of domain
adaptation. The domains are ordered clockwise
from the top left: books, DVDs, electronics, and
kitchen. For each set of bars, the first letter is the
source domain and the second letter is the target
domain. The thick horizontal bars are the accura-
cies of the in-domain classifiers for these domains.
Thus the first set of bars shows that the baseline
achieves 72.8% accuracy adapting from DVDs to
books. SCL-MI achieves 79.7% and the in-domain
gold standard is 80.4%. We say that the adaptation
loss for the baseline model is 7.6% and the adapta-
tion loss for the SCL-MImodel is 0.7%. The relative
reduction in error due to adaptation of SCL-MI for
this test is 90.8%.
We can observe from these results that there is a
rough grouping of our domains. Books and DVDs
are similar, as are kitchen appliances and electron-
ics, but the two groups are different from one an-
other. Adapting classifiers from books to DVDs, for
instance, is easier than adapting them from books
to kitchen appliances. We note that when transfer-
ring from kitchen to electronics, SCL-MI actually
outperforms the in-domain classifier. This is possi-
ble since the unlabeled data may contain information
that the in-domain classifier does not have access to.
At the beginning of Section 2 we gave exam-
ples of how features can change behavior across do-
mains. The first type of behavior is when predictive
features from the source domain are not predictive
or do not appear in the target domain. The second is
442
657075
808590
D->B E->B K->B B->D E->D K->D
baseline SCL SCL-MIbooks
72.8 76.8
79.7
70.7 75.4 75.4 70.9 66.1 68.6
80.4 82.477.2 74.0 75.8 70.6 74.3 76.2 72.7 75.4 76.9
dvd
6570
7580
8590
B->E D->E K->E B->K D->K E->K
electronics kitchen
70.8 77.5 75.9 73.0 74.1 74.1
82.7 83.7 86.884.4
87.7
74.5 78.7 78.9 74.079.4
81.4 84.0 84.4 85.9
Figure 1: Accuracy results for domain adaptation between all pairs using SCL and SCL-MI. Thick black
lines are the accuracies of in-domain classifiers.
domain\polarity negative positive
books plot <num> pages predictable reader grisham engaging
reading this page <num> must read fascinating
kitchen the plastic poorly designed excellent product espresso
leaking awkward to defective are perfect years now a breeze
Table 2: Correspondences discovered by SCL for books and kitchen appliances. The top row shows features
that only appear in books and the bottom features that only appear in kitchen appliances. The left and right
columns show negative and positive features in correspondence, respectively.
when predictive features from the target domain do
not appear in the source domain. To show how SCL
deals with those domain mismatches, we look at the
adaptation from book reviews to reviews of kitchen
appliances. We selected the top 1000 most infor-
mative features in both domains. In both cases, be-
tween 85 and 90% of the informative features from
one domain were not among the most informative
of the other domain4. SCL addresses both of these
issues simultaneously by aligning features from the
two domains.
4There is a third type, features which are positive in one do-
main but negative in another, but they appear very infrequently
in our datasets.
Table 2 illustrates one row of the projection ma-
trix ? for adapting from books to kitchen appliances;
the features on each row appear only in the corre-
sponding domain. A supervised classifier trained on
book reviews cannot assign weight to the kitchen
features in the second row of table 2. In con-
trast, SCL assigns weight to these features indirectly
through the projection matrix. When we observe
the feature ?predictable? with a negative book re-
view, we update parameters corresponding to the
entire projection, including the kitchen-specific fea-
tures ?poorly designed? and ?awkward to?.
While some rows of the projection matrix ? are
443
useful for classification, SCL can also misalign fea-
tures. This causes problems when a projection is
discriminative in the source domain but not in the
target. This is the case for adapting from kitchen
appliances to books. Since the book domain is
quite broad, many projections in books model topic
distinctions such as between religious and political
books. These projections, which are uninforma-
tive as to the target label, are put into correspon-
dence with the fewer discriminating projections in
the much narrower kitchen domain. When we adapt
from kitchen to books, we assign weight to these un-
informative projections, degrading target classifica-
tion accuracy.
5 Correcting Misalignments
We now show how to use a small amount of target
domain labeled data to learn to ignore misaligned
projections from SCL-MI. Using the notation of
Ando and Zhang (2005), we can write the supervised
training objective of SCL on the source domain as
min
w,v
?
i
L
(
w?xi + v??xi, yi
)
+ ?||w||2 + ?||v||2 ,
where y is the label. The weight vector w ? Rd
weighs the original features, while v ? Rk weighs
the projected features. Ando and Zhang (2005) and
Blitzer et al (2006) suggest ? = 10?4, ? = 0, which
we have used in our results so far.
Suppose now that we have trained source model
weight vectors ws and vs. A small amount of tar-
get domain data is probably insufficient to signif-
icantly change w, but we can correct v, which is
much smaller. We augment each labeled target in-
stance xj with the label assigned by the source do-
main classifier (Florian et al, 2004; Blitzer et al,
2006). Then we solve
minw,v
?
j L (w
?xj + v??xj , yj) + ?||w||2
+?||v ? vs||2 .
Since we don?t want to deviate significantly from the
source parameters, we set ? = ? = 10?1.
Figure 2 shows the corrected SCL-MI model us-
ing 50 target domain labeled instances. We chose
this number since we believe it to be a reasonable
amount for a single engineer to label with minimal
effort. For reasons of space, for each target domain
dom \ model base base scl scl-mi scl-mi
+targ +targ
books 8.9 9.0 7.4 5.8 4.4
dvd 8.9 8.9 7.8 6.1 5.3
electron 8.3 8.5 6.0 5.5 4.8
kitchen 10.2 9.9 7.0 5.6 5.1
average 9.1 9.1 7.1 5.8 4.9
Table 3: For each domain, we show the loss due to transfer
for each method, averaged over all domains. The bottom row
shows the average loss over all runs.
we show adaptation from only the two domains on
which SCL-MI performed the worst relative to the
supervised baseline. For example, the book domain
shows only results from electronics and kitchen, but
not DVDs. As a baseline, we used the label of the
source domain classifier as a feature in the target, but
did not use any SCL features. We note that the base-
line is very close to just using the source domain
classifier, because with only 50 target domain in-
stances we do not have enough data to relearn all of
the parameters inw. As we can see, though, relearn-
ing the 50 parameters in v is quite helpful. The cor-
rected model always improves over the baseline for
every possible transfer, including those not shown in
the figure.
The idea of using the regularizer of a linear model
to encourage the target parameters to be close to the
source parameters has been used previously in do-
main adaptation. In particular, Chelba and Acero
(2004) showed how this technique can be effective
for capitalization adaptation. The major difference
between our approach and theirs is that we only pe-
nalize deviation from the source parameters for the
weights v of projected features, while they work
with the weights of the original features only. For
our small amount of labeled target data, attempting
to penalize w using ws performed no better than
our baseline. Because we only need to learn to ig-
nore projections that misalign features, we can make
much better use of our labeled data by adapting only
50 parameters, rather than 200,000.
Table 3 summarizes the results of sections 4 and
5. Structural correspondence learning reduces the
error due to transfer by 21%. Choosing pivots by
mutual information allows us to further reduce the
error to 36%. Finally, by adding 50 instances of tar-
get domain data and using this to correct the mis-
aligned projections, we achieve an average relative
444
657075
808590
E->B K->B B->D K->D B->E D->E B->K E->K
base+50-targ SCL-MI+50-targbooks kitchen
70.9 76.0 70.7 76.8
78.5 72.7
80.4 87.776.6 70.8 76.6 73.0 77.9 74.3
80.7 84.3
dvd electronics82.4 84.4
73.2
85.9
Figure 2: Accuracy results for domain adaptation with 50 labeled target domain instances.
reduction in error of 46%.
6 Measuring Adaptability
Sections 2-5 focused on how to adapt to a target do-
main when you had a labeled source dataset. We
now take a step back to look at the problem of se-
lecting source domain data to label. We study a set-
ting where an engineer knows roughly her domains
of interest but does not have any labeled data yet. In
that case, she can ask the question ?Which sources
should I label to obtain the best performance over
all my domains?? On our product domains, for ex-
ample, if we are interested in classifying reviews
of kitchen appliances, we know from sections 4-5
that it would be foolish to label reviews of books or
DVDs rather than electronics. Here we show how to
select source domains using only unlabeled data and
the SCL representation.
6.1 The A-distance
We propose to measure domain adaptability by us-
ing the divergence of two domains after the SCL
projection. We can characterize domains by their
induced distributions on instance space: the more
different the domains, the more divergent the distri-
butions. Here we make use of the A-distance (Ben-
David et al, 2006). The key intuition behind the
A-distance is that while two domains can differ in
arbitrary ways, we are only interested in the differ-
ences that affect classification accuracy.
Let A be the family of subsets of Rk correspond-
ing to characteristic functions of linear classifiers
(sets on which a linear classifier returns positive
value). Then theA distance between two probability
distributions is
dA(D,D
?) = 2 sup
A?A
|PrD [A] ? PrD? [A]| .
That is, we find the subset in A on which the distri-
butions differ the most in the L1 sense. Ben-David
et al (2006) show that computing the A-distance for
a finite sample is exactly the problem of minimiz-
ing the empirical risk of a classifier that discrimi-
nates between instances drawn fromD and instances
drawn from D?. This is convenient for us, since it al-
lows us to use classification machinery to compute
the A-distance.
6.2 Unlabeled Adaptability Measurements
We follow Ben-David et al (2006) and use the Hu-
ber loss as a proxy for the A-distance. Our proce-
dure is as follows: Given two domains, we compute
the SCL representation. Then we create a data set
where each instance ?x is labeled with the identity
of the domain from which it came and train a linear
classifier. For each pair of domains we compute the
empirical average per-instance Huber loss, subtract
it from 1, and multiply the result by 100. We refer
to this quantity as the proxy A-distance. When it is
100, the two domains are completely distinct. When
it is 0, the two domains are indistinguishable using a
linear classifier.
Figure 3 is a correlation plot between the proxy
A-distance and the adaptation error. Suppose we
wanted to label two domains out of the four in such a
445
024
6810
1214
60 65 70 75 80 85 90 95 100Proxy A-distanceAd
aptation Loss EK BD DE
DK BE, BK
Figure 3: The proxy A-distance between each do-
main pair plotted against the average adaptation loss
of as measured by our baseline system. Each pair of
domains is labeled by their first letters: EK indicates
the pair electronics and kitchen.
way as to minimize our error on all the domains. Us-
ing the proxy A-distance as a criterion, we observe
that we would choose one domain from either books
or DVDs, but not both, since then we would not be
able to adequately cover electronics or kitchen appli-
ances. Similarly we would also choose one domain
from either electronics or kitchen appliances, but not
both.
7 Related Work
Sentiment classification has advanced considerably
since the work of Pang et al (2002), which we use
as our baseline. Thomas et al (2006) use discourse
structure present in congressional records to perform
more accurate sentiment classification. Pang and
Lee (2005) treat sentiment analysis as an ordinal
ranking problem. In our work we only show im-
provement for the basic model, but all of these new
techniques also make use of lexical features. Thus
we believe that our adaptation methods could be also
applied to those more refined models.
While work on domain adaptation for senti-
ment classifiers is sparse, it is worth noting that
other researchers have investigated unsupervised
and semisupervised methods for domain adaptation.
The work most similar in spirit to ours that of Tur-
ney (2002). He used the difference in mutual in-
formation with two human-selected features (the
words ?excellent? and ?poor?) to score features in
a completely unsupervised manner. Then he clas-
sified documents according to various functions of
these mutual information scores. We stress that our
method improves a supervised baseline. While we
do not have a direct comparison, we note that Tur-
ney (2002) performs worse on movie reviews than
on his other datasets, the same type of data as the
polarity dataset.
We also note the work of Aue and Gamon (2005),
who performed a number of empirical tests on do-
main adaptation of sentiment classifiers. Most of
these tests were unsuccessful. We briefly note their
results on combining a number of source domains.
They observed that source domains closer to the tar-
get helped more. In preliminary experiments we
confirmed these results. Adding more labeled data
always helps, but diversifying training data does not.
When classifying kitchen appliances, for any fixed
amount of labeled data, it is always better to draw
from electronics as a source than use some combi-
nation of all three other domains.
Domain adaptation alone is a generally well-
studied area, and we cannot possibly hope to cover
all of it here. As we noted in Section 5, we are
able to significantly outperform basic structural cor-
respondence learning (Blitzer et al, 2006). We also
note that while Florian et al (2004) and Blitzer et al
(2006) observe that including the label of a source
classifier as a feature on small amounts of target data
tends to improve over using either the source alone
or the target alne, we did not observe that for our
data. We believe the most important reason for this
is that they explore structured prediction problems,
where labels of surrounding words from the source
classifier may be very informative, even if the cur-
rent label is not. In contrast our simple binary pre-
diction problem does not exhibit such behavior. This
may also be the reason that the model of Chelba and
Acero (2004) did not aid in adaptation.
Finally we note that while Blitzer et al (2006) did
combine SCL with labeled target domain data, they
only compared using the label of SCL or non-SCL
source classifiers as features, following the work of
Florian et al (2004). By only adapting the SCL-
related part of the weight vector v, we are able to
make better use of our small amount of unlabeled
data than these previous techniques.
446
8 Conclusion
Sentiment classification has seen a great deal of at-
tention. Its application to many different domains
of discourse makes it an ideal candidate for domain
adaptation. This work addressed two important
questions of domain adaptation. First, we showed
that for a given source and target domain, we can
significantly improve for sentiment classification the
structural correspondence learning model of Blitzer
et al (2006). We chose pivot features using not only
common frequency among domains but also mutual
information with the source labels. We also showed
how to correct structural correspondence misalign-
ments by using a small amount of labeled target do-
main data.
Second, we provided a method for selecting those
source domains most likely to adapt well to given
target domains. The unsupervised A-distance mea-
sure of divergence between domains correlates well
with loss due to adaptation. Thus we can use the A-
distance to select source domains to label which will
give low target domain error.
In the future, we wish to include some of the more
recent advances in sentiment classification, as well
as addressing the more realistic problem of rank-
ing. We are also actively searching for a larger and
more varied set of domains on which to test our tech-
niques.
Acknowledgements
We thank Nikhil Dinesh for helpful advice through-
out the course of this work. This material is based
upon work partially supported by the Defense Ad-
vanced Research Projects Agency (DARPA) un-
der Contract No. NBCHD03001. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA or
the Department of Interior-National BusinessCenter
(DOI-NBC).
References
Rie Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks and
unlabeled data. JMLR, 6:1817?1853.
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case study.
http://research.microsoft.com/ anthaue/.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In Neural Information Processing
Systems (NIPS).
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In EMNLP.
Sanjiv Das and Mike Chen. 2001. Yahoo! for ama-
zon: Extracting market sentiment from stock message
boards. In Proceedings of Athe Asia Pacific Finance
Association Annual Conference.
R. Florian, H. Hassan, A.Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In of HLT-NAACL.
Andrew Goldberg and Xiaojin Zhu. 2004. Seeing
stars when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Processing.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of Association
for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of Empiri-
cal Methods in Natural Language Processing.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of Association for
Computational Linguistics.
Tong Zhang. 2004. Solving large scale linear predic-
tion problems using stochastic gradient descent al-
gorithms. In International Conference on Machine
Learning (ICML).
447
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 120?128,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Domain Adaptation with Structural Correspondence Learning
John Blitzer Ryan McDonald Fernando Pereira
{blitzer|ryantm|pereira}@cis.upenn.edu
Department of Computer and Information Science, University of Pennsylvania
3330 Walnut Street, Philadelphia, PA 19104, USA
Abstract
Discriminative learning methods are
widely used in natural language process-
ing. These methods work best when their
training and test data are drawn from the
same distribution. For many NLP tasks,
however, we are confronted with new
domains in which labeled data is scarce
or non-existent. In such cases, we seek
to adapt existing models from a resource-
rich source domain to a resource-poor
target domain. We introduce structural
correspondence learning to automatically
induce correspondences among features
from different domains. We test our tech-
nique on part of speech tagging and show
performance gains for varying amounts
of source and target training data, as well
as improvements in target domain parsing
accuracy using our improved tagger.
1 Introduction
Discriminative learning methods are ubiquitous in
natural language processing. Discriminative tag-
gers and chunkers have been the state-of-the-art
for more than a decade (Ratnaparkhi, 1996; Sha
and Pereira, 2003). Furthermore, end-to-end sys-
tems like speech recognizers (Roark et al, 2004)
and automatic translators (Och, 2003) use increas-
ingly sophisticated discriminative models, which
generalize well to new data that is drawn from the
same distribution as the training data.
However, in many situations we may have a
source domain with plentiful labeled training data,
but we need to process material from a target do-
main with a different distribution from the source
domain and no labeled data. In such cases, we
must take steps to adapt a model trained on the
source domain for use in the target domain (Roark
and Bacchiani, 2003; Florian et al, 2004; Chelba
and Acero, 2004; Ando, 2004; Lease and Char-
niak, 2005; Daume? III and Marcu, 2006). This
work focuses on using unlabeled data from both
the source and target domains to learn a common
feature representation that is meaningful across
both domains. We hypothesize that a discrimi-
native model trained in the source domain using
this common feature representation will general-
ize better to the target domain.
This representation is learned using a method
we call structural correspondence learning (SCL).
The key idea of SCL is to identify correspon-
dences among features from different domains by
modeling their correlations with pivot features.
Pivot features are features which behave in the
same way for discriminative learning in both do-
mains. Non-pivot features from different domains
which are correlated with many of the same pivot
features are assumed to correspond, and we treat
them similarly in a discriminative learner.
Even on the unlabeled data, the co-occurrence
statistics of pivot and non-pivot features are likely
to be sparse, and we must model them in a com-
pact way. There are many choices for modeling
co-occurrence data (Brown et al, 1992; Pereira
et al, 1993; Blei et al, 2003). In this work we
choose to use the technique of structural learn-
ing (Ando and Zhang, 2005a; Ando and Zhang,
2005b). Structural learning models the correla-
tions which are most useful for semi-supervised
learning. We demonstrate how to adapt it for trans-
fer learning, and consequently the structural part
of structural correspondence learning is borrowed
from it.1
SCL is a general technique, which one can ap-
ply to feature based classifiers for any task. Here,
1Structural learning is different from learning with struc-
tured outputs, a common paradigm for discriminative nat-
ural language processing models. To avoid terminologi-
cal confusion, we refer throughout the paper to a specific
structural learning method, alternating structural optimiza-
tion (ASO) (Ando and Zhang, 2005a).
120
(a) Wall Street Journal
DT JJ VBZ DT NN IN DT JJ NN
The clash is a sign of a new toughness
CC NN IN NNP POS JJ JJ NN .
and divisiveness in Japan ?s once-cozy financial circles .
(b) MEDLINE
DT JJ VBN NNS IN DT NN NNS VBP
The oncogenic mutated forms of the ras proteins are
RB JJ CC VBP IN JJ NN NN .
constitutively active and interfere with normal signal transduction .
Figure 1: Part of speech-tagged sentences from both corpora
we investigate its use in part of speech (PoS) tag-
ging (Ratnaparkhi, 1996; Toutanova et al, 2003).
While PoS tagging has been heavily studied, many
domains lack appropriate training corpora for PoS
tagging. Nevertheless, PoS tagging is an impor-
tant stage in pipelined language processing sys-
tems, from information extractors to speech syn-
thesizers. We show how to use SCL to transfer a
PoS tagger from the Wall Street Journal (financial
news) to MEDLINE (biomedical abstracts), which
use very different vocabularies, and we demon-
strate not only improved PoS accuracy but also
improved end-to-end parsing accuracy while using
the improved tagger.
An important but rarely-explored setting in do-
main adaptation is when we have no labeled
training data for the target domain. We first
demonstrate that in this situation SCL significantly
improves performance over both supervised and
semi-supervised taggers. In the case when some
in-domain labeled training data is available, we
show how to use SCL together with the classifier
combination techniques of Florian et al (2004) to
achieve even greater performance.
In the next section, we describe a motivating
example involving financial news and biomedical
data. Section 3 describes the structural correspon-
dence learning algorithm. Sections 6 and 7 report
results on adapting from the Wall Street Journal to
MEDLINE. We discuss related work on domain
adaptation in section 8 and conclude in section 9.
2 A Motivating Example
Figure 1 shows two PoS-tagged sentences, one
each from the Wall Street Journal (hereafter WSJ)
and MEDLINE. We chose these sentences for two
reasons. First, we wish to visually emphasize the
difference between the two domains. The vocab-
ularies differ significantly, and PoS taggers suf-
fer accordingly. Second, we want to focus on the
(a) An ambiguous instance
JJ vs. NN
with normal signal transduction
(b) MEDLINE occurrences of
signal, together with pivot
features
the signal required to
stimulatory signal from
essential signal for
(c) Corresponding WSJ
words, together with pivot
features
of investment required
of buyouts from buyers
to jail for violating
Figure 2: Correcting an incorrect biomedical tag.
Corresponding words are in bold, and pivot fea-
tures are italicized
phrase ?with normal signal transduction? from the
MEDLINE sentence, depicted in Figure 2(a). The
word ?signal? in this sentence is a noun, but a tag-
ger trained on the WSJ incorrectly classifies it as
an adjective. We introduce the notion of pivot fea-
tures. Pivot features are features which occur fre-
quently in the two domains and behave similarly
in both. Figure 2(b) shows some pivot features
that occur together with the word ?signal? in our
biomedical unlabeled data. In this case our pivot
features are all of type <the token on the
right>. Note that ?signal? is unambiguously a
noun in these contexts. Adjectives rarely precede
past tense verbs such as ?required? or prepositions
such as ?from? and ?for?.
We now search for occurrences of the pivot fea-
tures in the WSJ. Figure 2(c) shows some words
that occur together with the pivot features in the
WSJ unlabeled data. Note that ?investment?,
?buy-outs?, and ?jail? are all common nouns in the
financial domain. Furthermore, since we have la-
beled WSJ data, we expect to be able to label at
least some of these nouns correctly.
This example captures the intuition behind
structural correspondence learning. We want to
use pivot features from our unlabeled data to put
domain-specific words in correspondence. That is,
121
Input: labeled source data {(xt, yt)Tt=1},
unlabeled data from both domains {xj}
Output: predictor f : X ? Y
1. Choose m pivot features. Create m binary
prediction problems, p`(x), ` = 1 . . . m
2. For ` = 1 to m
w?` = argmin
w
?
P
j L(w ? xj , p`(xj))+
?||w||2
?
end
3. W = [w?1| . . . |w?m], [U D V T ] = SVD(W ),
? = UT[1:h,:]
4. Return f , a predictor trained
on
(
??
xt
?xi
?
, yt
?T
t=1
)
Figure 3: SCL Algorithm
we want the pivot features to model the fact that in
the biomedical domain, the word signal behaves
similarly to the words investments, buyouts and
jail in the financial news domain. In practice, we
use this technique to find correspondences among
all features, not just word features.
3 Structural Correspondence Learning
Structural correspondence learning involves a
source domain and a target domain. Both domains
have ample unlabeled data, but only the source do-
main has labeled training data. We refer to the task
for which we have labeled training data as the su-
pervised task. In our experiments, the supervised
task is part of speech tagging. We require that the
input x in both domains be a vector of binary fea-
tures from a finite feature space. The first step of
SCL is to define a set of pivot features on the unla-
beled data from both domains. We then use these
pivot features to learn a mapping ? from the orig-
inal feature spaces of both domains to a shared,
low-dimensional real-valued feature space. A high
inner product in this new space indicates a high de-
gree of correspondence.
During supervised task training, we use both
the transformed and original features from the
source domain. During supervised task testing, we
use the both the transformed and original features
from the target domain. If we learned a good map-
ping ?, then the classifier we learn on the source
domain will also be effective on the target domain.
The SCL algorithm is given in Figure 3, and the
remainder of this section describes it in detail.
3.1 Pivot Features
Pivot features should occur frequently in the un-
labeled data of both domains, since we must esti-
mate their covariance with non-pivot features ac-
curately, but they must also be diverse enough
to adequately characterize the nuances of the su-
pervised task. A good example of this tradeoff
are determiners in PoS tagging. Determiners are
good pivot features, since they occur frequently
in any domain of written English, but choosing
only determiners will not help us to discriminate
between nouns and adjectives. Pivot features cor-
respond to the auxiliary problems of Ando and
Zhang (2005a).
In section 2, we showed example pivot fea-
tures of type <the token on the right>.
We also use pivot features of type <the token
on the left> and <the token in the
middle>. In practice there are many thousands
of pivot features, corresponding to instantiations
of these three types for frequent words in both do-
mains. We choose m pivot features, which we in-
dex with `.
3.2 Pivot Predictors
From each pivot feature we create a binary clas-
sification problem of the form ?Does pivot fea-
ture ` occur in this instance??. One such ex-
ample is ?Is <the token on the right>
required?? These binary classification problems
can be trained from the unlabeled data, since they
merely represent properties of the input. If we rep-
resent our features as a binary vector x, we can
solve these problems using m linear predictors.
f`(x) = sgn(w?` ? x), ` = 1 . . . m
Note that these predictors operate on the original
feature space. This step is shown in line 2 of Fig-
ure 3. Here L(p, y) is a real-valued loss func-
tion for binary classification. We follow Ando and
Zhang (2005a) and use the modified Huber loss.
Since each instance contains features which are
totally predictive of the pivot feature (the feature
itself), we never use these features when making
the binary prediction. That is, we do not use any
feature derived from the right word when solving
a right token pivot predictor.
The pivot predictors are the key element in SCL.
The weight vectors w?` encode the covariance of
the non-pivot features with the pivot features. If
the weight given to the z?th feature by the `?th
122
pivot predictor is positive, then feature z is posi-
tively correlated with pivot feature `. Since pivot
features occur frequently in both domains, we ex-
pect non-pivot features from both domains to be
correlated with them. If two non-pivot features are
correlated in the same way with many of the same
pivot features, then they have a high degree of cor-
respondence. Finally, observe that w?` is a linear
projection of the original feature space onto R.
3.3 Singular Value Decomposition
Since each pivot predictor is a projection onto R,
we could create m new real-valued features, one
for each pivot. For both computational and statis-
tical reasons, though, we follow Ando and Zhang
(2005a) and compute a low-dimensional linear ap-
proximation to the pivot predictor space. Let W
be the matrix whose columns are the pivot pre-
dictor weight vectors. Now let W = UDV T be
the singular value decomposition of W , so that
? = UT[1:h,:] is the matrix whose rows are the top
left singular vectors of W .
The rows of ? are the principal pivot predictors,
which capture the variance of the pivot predictor
space as best as possible in h dimensions. Further-
more, ? is a projection from the original feature
space onto Rh. That is, ?x is the desired mapping
to the (low dimensional) shared feature represen-
tation. This is step 3 of Figure 3.
3.4 Supervised Training and Inference
To perform inference and learning for the super-
vised task, we simply augment the original fea-
ture vector with features obtained by applying the
mapping ?. We then use a standard discrimina-
tive learner on the augmented feature vector. For
training instance t, the augmented feature vector
will contain all the original features xt plus the
new shared features ?xt. If we have designed the
pivots well, then ? should encode correspondences
among features from different domains which are
important for the supervised task, and the classi-
fier we train using these new features on the source
domain will perform well on the target domain.
4 Model Choices
Structural correspondence learning uses the tech-
niques of alternating structural optimization
(ASO) to learn the correlations among pivot and
non-pivot features. Ando and Zhang (2005a) de-
scribe several free paramters and extensions to
ASO, and we briefly address our choices for these
here. We set h, the dimensionality of our low-rank
representation to be 25. As in Ando and Zhang
(2005a), we observed that setting h between 20
and 100 did not change results significantly, and a
lower dimensionality translated to faster run-time.
We also implemented both of the extensions de-
scribed in Ando and Zhang (2005a). The first is
to only use positive entries in the pivot predictor
weight vectors to compute the SVD. This yields
a sparse representation which saves both time and
space, and it also performs better. The second is to
compute block SVDs of the matrix W , where one
block corresponds to one feature type. We used
the same 58 feature types as Ratnaparkhi (1996).
This gave us a total of 1450 projection features for
both semisupervised ASO and SCL.
We found it necessary to make a change to the
ASO algorithm as described in Ando and Zhang
(2005a). We rescale the projection features to al-
low them to receive more weight from a regular-
ized discriminative learner. Without any rescaling,
we were not able to reproduce the original ASO
results. The rescaling parameter is a single num-
ber, and we choose it using heldout data from our
source domain. In all our experiments, we rescale
our projection features to have average L1 norm on
the training set five times that of the binary-valued
features.
Finally, we also make one more change to make
optimization faster. We select only half of the
ASO features for use in the final model. This
is done by running a few iterations of stochas-
tic gradient descent on the PoS tagging problem,
then choosing the features with the largest weight-
variance across the different labels. This cut in
half training time and marginally improved perfor-
mance in all our experiments.
5 Data Sets and Supervised Tagger
5.1 Source Domain: WSJ
We used sections 02-21 of the Penn Treebank
(Marcus et al, 1993) for training. This resulted in
39,832 training sentences. For the unlabeled data,
we used 100,000 sentences from a 1988 subset of
the WSJ.
5.2 Target Domain: Biomedical Text
For unlabeled data we used 200,000 sentences that
were chosen by searching MEDLINE for abstracts
pertaining to cancer, in particular genomic varia-
123
company
transaction
investors
officials yourpretty
short-term
political
receptors mutation
assays
lesions functional
transientneuronal
metastatic
WSJ Only
MEDLINE Only
Figure 4: An example projection of word features onto R. Words on the left (negative valued) behave
similarly to each other for classification, but differently from words on the right (positive valued). The
projection distinguishes nouns from adjectives and determiners in both domains.
tions and mutations. For labeled training and test-
ing purposes we use 1061 sentences that have been
annotated by humans as part of the Penn BioIE
project (PennBioIE, 2005). We use the same 561-
sentence test set in all our experiments. The part-
of-speech tag set for this data is a superset of
the Penn Treebank?s including the two new tags
HYPH (for hyphens) and AFX (for common post-
modifiers of biomedical entities such as genes).
These tags were introduced due to the importance
of hyphenated entities in biomedical text, and are
used for 1.8% of the words in the test set. Any
tagger trained only on WSJ text will automatically
predict wrong tags for those words.
5.3 Supervised Tagger
Since SCL is really a method for inducing a set
of cross-domain features, we are free to choose
any feature-based classifier to use them. For
our experiments we use a version of the discrim-
inative online large-margin learning algorithm
MIRA (Crammer et al, 2006). MIRA learns and
outputs a linear classification score, s(x,y;w) =
w ? f(x,y), where the feature representation f can
contain arbitrary features of the input, including
the correspondence features described earlier. In
particular, MIRA aims to learn weights so that
the score of correct output, yt, for input xt is
separated from the highest scoring incorrect out-
puts2, with a margin proportional to their Ham-
ming losses. MIRA has been used successfully for
both sequence analysis (McDonald et al, 2005a)
and dependency parsing (McDonald et al, 2005b).
As with any structured predictor, we need to
factor the output space to make inference tractable.
We use a first-order Markov factorization, allow-
ing for an efficient Viterbi inference procedure.
2We fix the number of high scoring incorrect outputs to 5.
6 Visualizing ?
In section 2 we claimed that good representations
should encode correspondences between words
like ?signal? from MEDLINE and ?investment?
from the WSJ. Recall that the rows of ? are pro-
jections from the original feature space onto the
real line. Here we examine word features under
these projections. Figure 4 shows a row from
the matrix ?. Applying this projection to a word
gives a real value on the horizontal dashed line
axis. The words below the horizontal axis occur
only in the WSJ. The words above the axis occur
only in MEDLINE. The verticle line in the mid-
dle represents the value zero. Ticks to the left or
right indicate relative positive or negative values
for a word under this projection. This projection
discriminates between nouns (negative) and adjec-
tives (positive). A tagger which gives high pos-
itive weight to the features induced by applying
this projection will be able to discriminate among
the associated classes of biomedical words, even
when it has never observed the words explicitly in
the WSJ source training set.
7 Empirical Results
All the results we present in this section use the
MIRA tagger from Section 5.3. The ASO and
structural correspondence results also use projec-
tion features learned using ASO and SCL. Sec-
tion 7.1 presents results comparing structural cor-
respondence learning with the supervised baseline
and ASO in the case where we have no labeled
data in the target domain. Section 7.2 gives results
for the case where we have some limited data in
the target domain. In this case, we use classifiers
as features as described in Florian et al (2004).
Finally, we show in Section 7.3 that our SCL PoS
124
(a)
100  500  1k 5k 40k75
80
85
90
Results for 561 MEDLINE Test Sentences
Number of WSJ Training Sentences
Ac
cu
ra
cy
supervised
semi?ASO
SCL
(b) Accuracy on 561-sentence test set
Words
Model All Unknown
Ratnaparkhi (1996) 87.2 65.2
supervised 87.9 68.4
semi-ASO 88.4 70.9
SCL 88.9 72.0
(c) Statistical Significance (McNemar?s)
for all words
Null Hypothesis p-value
semi-ASO vs. super 0.0015
SCL vs. super 2.1 ? 10?12
SCL vs. semi-ASO 0.0003
Figure 5: PoS tagging results with no target labeled training data
(a)
50 100 200 500
86
88
90
92
94
96
Number of MEDLINE Training Sentences
Ac
cu
ra
cy
Results for 561 MEDLINE Test Sentences
40k?SCL
40k?super
1k?SCL
1k?super
nosource
(b) 500 target domain training sentences
Model Testing Accuracy
nosource 94.5
1k-super 94.5
1k-SCL 95.0
40k-super 95.6
40k-SCL 96.1
(c) McNemar?s Test (500 training sentences)
Null Hypothesis p-value
1k-super vs. nosource 0.732
1k-SCL vs. 1k-super 0.0003
40k-super vs. nosource 1.9 ? 10?12
40k-SCL vs. 40k-super 6.5 ? 10?7
Figure 6: PoS tagging results with no target labeled training data
tagger improves the performance of a dependency
parser on the target domain.
7.1 No Target Labeled Training Data
For the results in this section, we trained a
structural correspondence learner with 100,000
sentences of unlabeled data from the WSJ and
100,000 sentences of unlabeled biomedical data.
We use as pivot features words that occur more
than 50 times in both domains. The supervised
baseline does not use unlabeled data. The ASO
baseline is an implementation of Ando and Zhang
(2005b). It uses 200,000 sentences of unlabeled
MEDLINE data but no unlabeled WSJ data. For
ASO we used as auxiliary problems words that oc-
cur more than 500 times in the MEDLINE unla-
beled data.
Figure 5(a) plots the accuracies of the three
models with varying amounts of WSJ training
data. With one hundred sentences of training
data, structural correspondence learning gives a
19.1% relative reduction in error over the super-
vised baseline, and it consistently outperforms
both baseline models. Figure 5(b) gives results
for 40,000 sentences, and Figure 5(c) shows cor-
responding significance tests, with p < 0.05 be-
ing significant. We use a McNemar paired test for
labeling disagreements (Gillick and Cox, 1989).
Even when we use all the WSJ training data avail-
able, the SCL model significantly improves accu-
racy over both the supervised and ASO baselines.
The second column of Figure 5(b) gives un-
known word accuracies on the biomedical data.
125
Of thirteen thousand test instances, approximately
three thousand were unknown. For unknown
words, SCL gives a relative reduction in error of
19.5% over Ratnaparkhi (1996), even with 40,000
sentences of source domain training data.
7.2 Some Target Labeled Training Data
In this section we give results for small amounts of
target domain training data. In this case, we make
use of the out-of-domain data by using features of
the source domain tagger?s predictions in training
and testing the target domain tagger (Florian et al,
2004). Though other methods for incorporating
small amounts of training data in the target domain
were available, such as those proposed by Chelba
and Acero (2004) and by Daume? III and Marcu
(2006), we chose this method for its simplicity and
consistently good performance. We use as features
the current predicted tag and all tag bigrams in a
5-token window around the current token.
Figure 6(a) plots tagging accuracy for varying
amounts of MEDLINE training data. The two
horizontal lines are the fixed accuracies of the
SCL WSJ-trained taggers using one thousand and
forty thousand sentences of training data. The five
learning curves are for taggers trained with vary-
ing amounts of target domain training data. They
use features on the outputs of taggers from sec-
tion 7.1. The legend indicates the kinds of features
used in the target domain (in addition to the stan-
dard features). For example, ?40k-SCL? means
that the tagger uses features on the outputs of an
SCL source tagger trained on forty thousand sen-
tences of WSJ data. ?nosource? indicates a tar-
get tagger that did not use any tagger trained on
the source domain. With 1000 source domain sen-
tences and 50 target domain sentences, using SCL
tagger features gives a 20.4% relative reduction
in error over using supervised tagger features and
a 39.9% relative reduction in error over using no
source features.
Figure 6(b) is a table of accuracies for 500 tar-
get domain training sentences, and Figure 6(c)
gives corresponding significance scores. With
1000 source domain sentences and 500 target do-
main sentences, using supervised tagger features
gives no improvement over using no source fea-
tures. Using SCL features still does, however.
7.3 Improving Parser Performance
We emphasize the importance of PoS tagging in a
pipelined NLP system by incorporating our SCL
100  500  1k 5k 40k
58
62
66
70
74
78
82
Dependency Parsing for 561 Test Sentences
Number of WSJ Training Sentences
Ac
cu
ra
cy
supervised
SCL
gold
Figure 7: Dependency parsing results using differ-
ent part of speech taggers
tagger into a WSJ-trained dependency parser and
and evaluate it on MEDLINE data. We use the
parser described by McDonald et al (2005b). That
parser assumes that a sentence has been PoS-
tagged before parsing. We train the parser and PoS
tagger on the same size of WSJ data.
Figure 7 shows dependency parsing accuracy on
our 561-sentence MEDLINE test set. We parsed
the sentences using the PoS tags output by our
source domain supervised tagger, the SCL tagger
from subsection 7.1, and the gold PoS tags. All
of the differences in this figure are significant ac-
cording to McNemar?s test. The SCL tags consis-
tently improve parsing performance over the tags
output by the supervised tagger. This is a rather in-
direct method of improving parsing performance
with SCL. In the future, we plan on directly incor-
porating SCL features into a discriminative parser
to improve its adaptation properties.
8 Related Work
Domain adaptation is an important and well-
studied area in natural language processing. Here
we outline a few recent advances. Roark and Bac-
chiani (2003) use a Dirichlet prior on the multi-
nomial parameters of a generative parsing model
to combine a large amount of training data from a
source corpus (WSJ), and small amount of train-
ing data from a target corpus (Brown). Aside
from Florian et al (2004), several authors have
also given techniques for adapting classification to
new domains. Chelba and Acero (2004) first train
a classifier on the source data. Then they use max-
imum a posteriori estimation of the weights of a
126
maximum entropy target domain classifier. The
prior is Gaussian with mean equal to the weights
of the source domain classifier. Daume? III and
Marcu (2006) use an empirical Bayes model to es-
timate a latent variable model grouping instances
into domain-specific or common across both do-
mains. They also jointly estimate the parameters
of the common classification model and the do-
main specific classification models. Our work fo-
cuses on finding a common representation for fea-
tures from different domains, not instances. We
believe this is an important distinction, since the
same instance can contain some features which are
common across domains and some which are do-
main specific.
The key difference between the previous four
pieces of work and our own is the use of unlabeled
data. We do not require labeled training data in
the new domain to demonstrate an improvement
over our baseline models. We believe this is essen-
tial, since many domains of application in natural
language processing have no labeled training data.
Lease and Charniak (2005) adapt a WSJ parser
to biomedical text without any biomedical tree-
banked data. However, they assume other labeled
resources in the target domain. In Section 7.3 we
give similar parsing results, but we adapt a source
domain tagger to obtain the PoS resources.
To the best of our knowledge, SCL is the first
method to use unlabeled data from both domains
for domain adaptation. By using just the unlabeled
data from the target domain, however, we can view
domain adaptation as a standard semisupervised
learning problem. There are many possible ap-
proaches for semisupservised learning in natural
language processing, and it is beyond the scope
of this paper to address them all. We chose to
compare with ASO because it consistently outper-
forms cotraining (Blum and Mitchell, 1998) and
clustering methods (Miller et al, 2004). We did
run experiments with the top-k version of ASO
(Ando and Zhang, 2005a), which is inspired by
cotraining but consistently outperforms it. This
did not outperform the supervised method for do-
main adaptation. We speculate that this is because
biomedical and financial data are quite different.
In such a situation, bootstrapping techniques are
likely to introduce too much noise from the source
domain to be useful.
Structural correspondence learning is most sim-
ilar to that of Ando (2004), who analyzed a
situation with no target domain labeled data.
Her model estimated co-occurrence counts from
source unlabeled data and then used the SVD of
this matrix to generate features for a named en-
tity recognizer. Our ASO baseline uses unlabeled
data from the target domain. Since this consis-
tently outperforms unlabeled data from only the
source domain, we report only these baseline re-
sults. To the best of our knowledge, this is the first
work to use unlabeled data from both domains to
find feature correspondences.
One important advantage that this work shares
with Ando (2004) is that an SCL model can be
easily combined with all other domain adaptation
techniques (Section 7.2). We are simply induc-
ing a feature representation that generalizes well
across domains. This feature representation can
then be used in all the techniques described above.
9 Conclusion
Structural correspondence learning is a marriage
of ideas from single domain semi-supervised
learning and domain adaptation. It uses unla-
beled data and frequently-occurring pivot features
from both source and target domains to find corre-
spondences among features from these domains.
Finding correspondences involves estimating the
correlations between pivot and non-pivot feautres,
and we adapt structural learning (ASO) (Ando and
Zhang, 2005a; Ando and Zhang, 2005b) for this
task. SCL is a general technique that can be ap-
plied to any feature-based discriminative learner.
We showed results using SCL to transfer a PoS
tagger from the Wall Street Journal to a corpus
of MEDLINE abstracts. SCL consistently out-
performed both supervised and semi-supervised
learning with no labeled target domain training
data. We also showed how to combine an SCL
tagger with target domain labeled data using the
classifier combination techniques from Florian et
al. (2004). Finally, we improved parsing perfor-
mance in the target domain when using the SCL
PoS tagger.
One of our next goals is to apply SCL directly
to parsing. We are also focusing on other po-
tential applications, including chunking (Sha and
Pereira, 2003), named entity recognition (Florian
et al, 2004; Ando and Zhang, 2005b; Daume? III
and Marcu, 2006), and speaker adaptation (Kuhn
et al, 1998). Finally, we are investigating more
direct ways of applying structural correspondence
127
learning when we have labeled data from both
source and target domains. In particular, the la-
beled data of both domains, not just the unlabeled
data, should influence the learned representations.
Acknowledgments
We thank Rie Kubota Ando and Tong Zhang
for their helpful advice on ASO, Steve Carroll
and Pete White of The Children?s Hospital of
Philadelphia for providing the MEDLINE data,
and the PennBioIE annotation team for the anno-
tated MEDLINE data used in our test sets. This
material is based upon work partially supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. NBCHD030010.
Any opinions, findings, and conclusions or rec-
ommendations expressed in this material are those
of the author(s) and do not necessarily reflect
the views of the DARPA or the Department
of Interior-National Business Center (DOI-NBC).
Additional support was provided by NSF under
ITR grant EIA-0205448.
References
R. Ando and T. Zhang. 2005a. A framework for learn-
ing predictive structures from multiple tasks and un-
labeled data. JMLR, 6:1817?1853.
R. Ando and T. Zhang. 2005b. A high-performance
semi-supervised learning method for text chunking.
In ACL.
R. Ando. 2004. Exploiting unannotated corpora for
tagging and chunking. In ACL. Short paper.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. JMLR, 3:993?1022.
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Workshop
on Computational Learning Theory.
P. Brown, V. Della Pietra, P. deSouza, J. Lai, and
R. Mercer. 1992. Class-based n-gram models
of natural language. Computational Linguistics,
18(4):467?479.
C. Chelba and A. Acero. 2004. Adaptation of maxi-
mum entropy capitalizer: Little data can help a lot.
In EMNLP.
K. Crammer, Dekel O, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
H. Daum e? III and D. Marcu. 2006. Domain adaptation
for statistical classifiers. JAIR.
R. Florian, H. Hassan, A.Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.
2004. A statistical model for multilingual entity de-
tection and tracking. In of HLT-NAACL.
L. Gillick and S. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
ICASSP.
R. Kuhn, P. Nguyen, J.C. Junqua, L. Goldwasser,
N. Niedzielski, S. Fincke, K. Field, and M. Con-
tolini. 1998. Eigenvoices for speaker adaptation.
In ICSLP.
M. Lease and E. Charniak. 2005. Parsing biomedical
literature. In IJCNLP.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
R. McDonald, K. Crammer, and F. Pereira. 2005a.
Flexible text segmentation with structured multil-
abel classification. In HLT-EMNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005b. On-
line large-margin training of dependency parsers. In
ACL.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative train-
ing. In HLT-NAACL.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL.
PennBioIE. 2005. Mining The Bibliome Project.
http://bioie.ldc.upenn.edu/.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of english words. In ACL.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In EMNLP.
B. Roark and M. Bacchiani. 2003. Supervised and
unsupervised PCFG adaptation to novel domains. In
HLT-NAACL.
B. Roark, M. Saraclar, M. Collins, and M. Johnson.
2004. Discriminative language modeling with con-
ditional random fields and the perceptron algorithm.
In ACL.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In HLT-NAACL.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In NAACL.
128
Tutorial Abstracts of ACL-08: HLT, page 3,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Semi-supervised Learning for Natural Language Processing
John Blitzer
Natural Language Computing Group
Microsoft Research Asia
Beijing, China
blitzer@cis.upenn.edu
Xiaojin Jerry Zhu
Department of Computer Science
University of Wisconsin, Madison
Madison, WI, USA
jerryzhu@cs.wisc.edu
1 Introduction
The amount of unlabeled linguistic data available
to us is much larger and growing much faster than
the amount of labeled data. Semi-supervised learn-
ing algorithms combine unlabeled data with a small
labeled training set to train better models. This
tutorial emphasizes practical applications of semi-
supervised learning; we treat semi-supervised learn-
ing methods as tools for building effective models
from limited training data. An attendee will leave
our tutorial with
1. A basic knowledge of the most common classes
of semi-supervised learning algorithms and where
they have been used in NLP before.
2. The ability to decide which class will be useful
in her research.
3. Suggestions against potential pitfalls in semi-
supervised learning.
2 Content Overview
Self-training methods Self-training methods use
the labeled data to train an initial model and then
use that model to label the unlabeled data and re-
train a new model. We will examine in detail the co-
training method of Blum and Mitchell [2], includ-
ing the assumptions it makes, and two applications
of co-training to NLP data. Another popular self-
training method treats the labels of the unlabeled
data as hidden and estimates a single model from
labeled and unlabeled data. We explore new meth-
ods in this framework that make use of declarative
linguistic side information to constrain the solutions
found using unlabeled data [3].
Graph regularization methods Graph regulariza-
tion methods build models based on a graph on in-
stances, where edges in the graph indicate similarity.
The regularization constraint is one of smoothness
along this graph. We wish to find models that per-
form well on the training data, but we also regularize
so that unlabeled nodes which are similar according
to the graph have similar labels. For this section, we
focus in detail on the Gaussian fields method of Zhu
et al [4].
Structural learning Structural learning [1] uses un-
labeled data to find a new, reduced-complexity hy-
pothesis space by exploiting regularities in feature
space via unlabeled data. If this new hypothesis
space still contains good hypotheses for our super-
vised learning problem, we may achieve high accu-
racy with much less training data. The regularities
we use come in the form of lexical features that func-
tion similarly for prediction. This section will fo-
cus on the assumptions behind structural learning, as
well as applications to tagging and sentiment analy-
sis.
References
[1] Rie Ando and Tong Zhang. A Framework for Learn-
ing Predictive Structures from Multiple Tasks and Unla-
beled Data. JMLR 2005.
[2] Avrim Blum and Tom Mitchell. Combining Labeled
and Unlabeled Data with Co-training. COLT 1998.
[3] Aria Haghighi and Dan Klein. Prototype-driven
Learning for Sequence Models. HLT/NAACL 2006.
[4] Xiaojin Zhu, Zoubin Ghahramani, and John Laf-
ferty. Semi-supervised Learning using Gaussian Fields
and Harmonic Functions. ICML 2003.
3
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 127?135,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Joint Parsing and Alignment with Weakly Synchronized Grammars
David Burkett John Blitzer Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,blitzer,klein}@cs.berkeley.edu
Abstract
Syntactic machine translation systems extract
rules from bilingual, word-aligned, syntacti-
cally parsed text, but current systems for pars-
ing and word alignment are at best cascaded
and at worst totally independent of one an-
other. This work presents a unified joint model
for simultaneous parsing and word alignment.
To flexibly model syntactic divergence, we de-
velop a discriminative log-linear model over
two parse trees and an ITG derivation which
is encouraged but not forced to synchronize
with the parses. Our model gives absolute
improvements of 3.3 F1 for English pars-
ing, 2.1 F1 for Chinese parsing, and 5.5 F1
for word alignment over each task?s indepen-
dent baseline, giving the best reported results
for both Chinese-English word alignment and
joint parsing on the parallel portion of the Chi-
nese treebank. We also show an improvement
of 1.2 BLEU in downstream MT evaluation
over basic HMM alignments.
1 Introduction
Current syntactic machine translation (MT) sys-
tems build synchronous context free grammars from
aligned syntactic fragments (Galley et al, 2004;
Zollmann et al, 2006). Extracting such grammars
requires that bilingual word alignments and mono-
lingual syntactic parses be compatible. Because of
this, much recent work in both word alignment and
parsing has focused on changing aligners to make
use of syntactic information (DeNero and Klein,
2007; May and Knight, 2007; Fossum et al, 2008)
or changing parsers to make use of word align-
ments (Smith and Smith, 2004; Burkett and Klein,
2008; Snyder et al, 2009). In the first case, how-
ever, parsers do not exploit bilingual information.
In the second, word alignment is performed with a
model that does not exploit syntactic information.
This work presents a single, joint model for parsing
and word alignment that allows both pieces to influ-
ence one another simultaneously.
While building a joint model seems intuitive,
there is no easy way to characterize how word align-
ments and syntactic parses should relate to each
other in general. In the ideal situation, each pair
of sentences in a bilingual corpus could be syntacti-
cally parsed using a synchronous context-free gram-
mar. Of course, real translations are almost always
at least partially syntactically divergent. Therefore,
it is unreasonable to expect perfect matches of any
kind between the two sides? syntactic trees, much
less expect that those matches be well explained at
a word level. Indeed, it is sometimes the case that
large pieces of a sentence pair are completely asyn-
chronous and can only be explained monolingually.
Our model exploits synchronization where pos-
sible to perform more accurately on both word
alignment and parsing, but also allows indepen-
dent models to dictate pieces of parse trees and
word alignments when synchronization is impossi-
ble. This notion of ?weak synchronization? is pa-
rameterized and estimated from data to maximize
the likelihood of the correct parses and word align-
ments. Weak synchronization is closely related to
the quasi-synchronous models of Smith and Eis-
ner (2006; 2009) and the bilingual parse reranking
model of Burkett and Klein (2008), but those models
assume that the word alignment of a sentence pair is
known and fixed.
To simultaneously model both parses and align-
127
ments, our model loosely couples three separate
combinatorial structures: monolingual trees in the
source and target languages, and a synchronous ITG
alignment that links the two languages (but is not
constrained to match linguistic syntax). The model
has no hard constraints on how these three struc-
tures must align, but instead contains a set of ?syn-
chronization? features that are used to propagate
influence between the three component grammars.
The presence of synchronization features couples
the parses and alignments, but makes exact inference
in the model intractable; we show how to use a vari-
ational mean field approximation, both for comput-
ing approximate feature expectations during train-
ing, and for performing approximate joint inference
at test time.
We train our joint model on the parallel, gold
word-aligned portion of the Chinese treebank.
When evaluated on parsing and word alignment, this
model significantly improves over independently-
trained baselines: the monolingual parser of Petrov
and Klein (2007) and the discriminative word
aligner of Haghighi et al (2009). It also improves
over the discriminative, bilingual parsing model
of Burkett and Klein (2008), yielding the highest
joint parsing F1 numbers on this data set. Finally,
our model improves word alignment in the context
of translation, leading to a 1.2 BLEU increase over
using HMM word alignments.
2 Joint Parsing and Alignment
Given a source-language sentence, s, and a target-
language sentence, s?, we wish to predict a source
tree t, a target tree t?, and some kind of alignment
a between them. These structures are illustrated in
Figure 1.
To facilitate these predictions, we define a condi-
tional distribution P(t, a, t?|s, s?). We begin with a
generic conditional exponential form:
P(t, a, t?|s, s?) ? exp ?>?(t, a, t?, s, s?) (1)
Unfortunately, a generic model of this form is in-
tractable, because we cannot efficiently sum over
all triples (t, a, t?) without some assumptions about
how the features ?(t, a, t?, s, s?) decompose.
One natural solution is to restrict our candidate
triples to those given by a synchronous context free
grammar (SCFG) (Shieber and Schabes, 1990). Fig-
ure 1(a) gives a simple example of generation from
a log-linearly parameterized synchronous grammar,
together with its features. With the SCFG restric-
tion, we can sum over the necessary structures using
the O(n6) bitext inside-outside algorithm, making
P(t, a, t?|s, s?) relatively efficient to compute expec-
tations under.
Unfortunately, an SCFG requires that all the con-
stituents of each tree, from the root down to the
words, are generated perfectly in tandem. The re-
sulting inability to model any level of syntactic di-
vergence prevents accurate modeling of the individ-
ual monolingual trees. We will consider the run-
ning example from Figure 2 throughout the paper.
Here, for instance, the verb phrase established in
such places as Quanzhou, Zhangzhou, etc. in En-
glish does not correspond to any single node in the
Chinese tree. A synchronous grammar has no choice
but to analyze this sentence incorrectly, either by ig-
noring this verb phrase in English or postulating an
incorrect Chinese constituent that corresponds to it.
Therefore, instead of requiring strict synchroniza-
tion, our model treats the two monolingual trees and
the alignment as separate objects that can vary arbi-
trarily. However, the model rewards synchronization
appropriately when the alignment brings the trees
into correspondence.
3 Weakly Synchronized Grammars
We propose a joint model which still gives probabil-
ities on triples (t, a, t?). However, instead of using
SCFG rules to synchronously enforce the tree con-
straints on t and t?, we only require that each of t
and t? be well-formed under separate monolingual
CFGs.
In order to permit efficient enumeration of all pos-
sible alignments a, we also restrict a to the set of
unlabeled ITG bitrees (Wu, 1997), though again we
do not require that a relate to t or t? in any particular
way. Although this assumption does limit the space
of possible word-level alignments, for the domain
we consider (Chinese-English word alignment), the
reduced space still contains almost all empirically
observed alignments (Haghighi et al, 2009).1 For
1See Section 8.1 for some new terminal productions re-
quired to make this true for the parallel Chinese treebank.
128
NP VP
S
NP
VP
IP
b
0
b
1
b
2
Features
?( (IP, b
0
, S), s, s? )
?( (NP, b
1
, NP), s, s? )
?( (VP, b
2
, VP), s, s? )
NP VP
S
NP
IP
b
0
b
1
b
2
VP
AP
Features
   
   
(IP, s)     
   
(b
0
, s, s?)
   
   
(NP, s)     
   
(b
1
, s, s?)
   
   
(VP, s)     
   
(b
2
, s, s?)
   
   
(S, s?)
      (IP, b
0
)
   
   
(NP, s?)
      (b
0
, S)
   
   
(AP, s?)
      (b
1
, NP)
   
   
(VP, s?)
      (IP, b
0
, S)
Parsing
Alignment
Synchronization
?E
?E
?E
?E
?F
?F
?F
?A
?A
?A
?!
?!
?!"
?!
(a) Synchronous Rule (b) Asynchronous Rule
Figure 1: Source trees, t (right), alignments, a (grid), and target trees, t? (top), and feature decompositions for syn-
chronous (a) and weakly synchronous (b) grammars. Features always condition on bispans and/or anchored syntactic
productions, but weakly synchronous grammars permit more general decompositions.
example, in Figure 2, the word alignment is ITG-
derivable, and each of the colored rectangles is a bi-
span in that derivation.
There are no additional constraints beyond the
independent, internal structural constraints on t, a,
and t?. This decoupling permits derivations like that
in Figure 1(b), where the top-level syntactic nodes
align, but their children are allowed to diverge. With
the three structures separated, our first model is a
completely factored decomposition of (1).
Formally, we represent a source tree t as a set of
nodes {n}, each node representing a labeled span.
Likewise, a target tree t? is a set of nodes {n?}.2 We
represent alignments a as sets of bispans {b}, indi-
cated by rectangles in Figure 1.3 Using this notation,
the initial model has the following form:
P(t, a, t?|s, s?) ? exp
?
?
?
n?t
?>?F (n, s)+
?
b?a
?>?A(b, s, s?)+
?
n??t?
?>?E(n?, s?)
?
?
(2)
Here ?F (n, s) indicates a vector of source node fea-
tures, ?E(n?, s?) is a vector of target node features,
and ?A(b, s, s?) is a vector of alignment bispan fea-
tures. Of course, this model is completely asyn-
2For expositional clarity, we describe n and n? as labeled
spans only. However, in general, features that depend on n or
n? are permitted to depend on the entire rule, and do in our final
system.
3Alignments a link arbitrary spans of s and s? (including
non-constituents and individual words). We discuss the relation
to word-level alignments in Section 4.
chronous so far, and fails to couple the trees and
alignments at all. To permit soft constraints between
the three structures we are modeling, we add a set of
synchronization features.
For n ? t and b ? a, we say that n b if n and b
both map onto the same span of s. We define b n?
analogously for n? ? t?. We now consider three
different types of synchronization features. Source-
alignment synchronization features ?(n, b) are ex-
tracted whenever n  b. Similarly, target-alignment
features ?(b, n?) are extracted if b  n?. These
features capture phenomena like that of bispan b7
in Figure 2. Here the Chinese noun? synchronizes
with the ITG derivation, but the English projection
of b7 is a distituent. Finally, we extract source-target
features ?./(n, b, n?) whenever nbn?. These fea-
tures capture complete bispan synchrony (as in bi-
span b8) and can be expressed over triples (n, b, n?)
which happen to align, allowing us to reward syn-
chrony, but not requiring it. All of these licensing
conditions are illustrated in Figure 1(b).
With these features added, the final form of the
model is:
P(t, a, t?|s, s?) ? exp
?
?
?
n?t
?>?F (n, s)+
?
b?a
?>?A(b, s, s?)+
?
n??t?
?>?E(n?, s?)+
?
nb
?>?(n, b)+
?
bn?
?>?(b, n?)+
?
nbn?
?>?./(n, b, n?)
?
?
(3)
129
We emphasize that because of the synchronization
features, this final form does not admit any known
efficient dynamic programming for the exact com-
putation of expectations. We will therefore turn to a
variational inference method in Section 6.
4 Features
With the model?s locality structure defined, we
just need to specify the actual feature function,
?. We divide the features into three types: pars-
ing features (?F (n, s) and ?E(n?, s?)), alignment
features (?A(b, s, s?)) and synchronization features
(?(n, b), ?(b, n?), and ?./(n, b, n?)). We detail
each of these in turn here.
4.1 Parsing
The monolingual parsing features we use are sim-
ply parsing model scores under the parser of Petrov
and Klein (2007). While that parser uses heavily re-
fined PCFGs with rule probabilities defined at the
refined symbol level, we interact with its posterior
distribution via posterior marginal probabilities over
unrefined symbols. In particular, to each unrefined
anchored production iAj ? iBkCj , we associate a
single feature whose value is the marginal quantity
log P(iBkCj |iAj , s) under the monolingual parser.
These scores are the same as the variational rule
scores of Matsuzaki et al (2005).4
4.2 Alignment
We begin with the same set of alignment features
as Haghighi et al (2009), which are defined only for
terminal bispans. In addition, we include features on
nonterminal bispans, including a bias feature, fea-
tures that measure the difference in size between
the source and target spans, features that measure
the difference in relative sentence position between
the source and target spans, and features that mea-
sure the density of word-to-word alignment poste-
riors under a separate unsupervised word alignment
model.
4Of course the structure of our model permits any of the
additional rule-factored monolingual parsing features that have
been described in the parsing literature, but in the present work
we focus on the contributions of joint modeling.
4.3 Synchronization
Our synchronization features are indicators for the
syntactic types of the participating nodes. We de-
termine types at both a coarse (more collapsed
than Treebank symbols) and fine (Treebank sym-
bol) level. At the coarse level, we distinguish be-
tween phrasal nodes (e.g. S, NP), synthetic nodes
introduced in the process of binarizing the grammar
(e.g. S?, NP?), and part-of-speech nodes (e.g. NN,
VBZ). At the fine level, we distinguish all nodes
by their exact label. We use coarse and fine types
for both partially synchronized (source-alignment or
target-alignment) features and completely synchro-
nized (source-alignment-target) features. The inset
of Figure 2 shows some sample features. Of course,
we could devise even more sophisticated features by
using the input text itself. As we shall see, however,
our model gives significant improvements with these
simple features alone.
5 Learning
We learn the parameters of our model on the paral-
lel portion of the Chinese treebank. Although our
model assigns probabilities to entire synchronous
derivations of sentences, the parallel Chinese tree-
bank gives alignments only at the word level (1 by
1 bispans in Figure 2). This means that our align-
ment variable a is not fully observed. Because of
this, given a particular word alignment w, we max-
imize the marginal probability of the set of deriva-
tions A(w) that are consistent with w (Haghighi et
al., 2009).5
L(?)=log
?
a?A(wi)
P(ti, a, t?i|si, s?i)
We maximize this objective using standard gradient
methods (Nocedal and Wright, 1999). As with fully
visible log-linear models, the gradient for the ith sen-
tence pair with respect to ? is a difference of feature
expectations:
?L(?) =EP(a|ti,wi,t?i,si,s?i)
[
?(ti, a, t?i, si, s?i)
]
? EP(t,a,t?|si,s?i)
[
?(t, a, t?, si, s?i)
] (4)
5We also learn from non-ITG alignments by maximizing the
marginal probability of the set of minimum-recall error align-
ments in the same way as Haghighi et al (2009)
130
NP
NP
IN
PP
NPIN
PPVBN
VPVBD
VPNP
S
JJ NNS
...
were established in such places as Quanzhou Zhangzhou etc.
?
??
??
?
?
??
?
...
NP
P
NN
NP
PP
VP
VV
AS
NP
VP
b
8
b
7
b
4
Sample Synchronization Features
NP, b8,NP
NN, b7
?!"( ) = CoarseSourceTarget?phrasal, phrasal? : 1
FineSourceTarget?NP,NP? : 1
?!( ) = CoarseSourceAlign?pos? : 1
FineSourceAlign?NN? : 1
Figure 2: An example of a Chinese-English sentence pair with parses, word alignments, and a subset of the full optimal
ITG derivation, including one totally unsynchronized bispan (b4), one partially synchronized bispan (b7), and and fully
synchronized bispan (b8). The inset provides some examples of active synchronization features (see Section 4.3) on
these bispans. On this example, the monolingual English parser erroneously attached the lower PP to the VP headed by
established, and the non-syntactic ITG word aligner misaligned? to such instead of to etc. Our joint model corrected
both of these mistakes because it was rewarded for the synchronization of the two NPs joined by b8.
We cannot efficiently compute the model expecta-
tions in this equation exactly. Therefore we turn next
to an approximate inference method.
6 Mean Field Inference
Instead of computing the model expectations from
(4), we compute the expectations for each sentence
pair with respect to a simpler, fully factored distri-
bution Q(t, a, t?) = q(t)q(a)q(t?). Rewriting Q in
log-linear form, we have:
Q(t, a, t?) ? exp
?
?
?
n?t
?n +
?
b?a
?b +
?
n??t?
?n?
?
?
Here, the ?n, ?b and ?n? are variational parameters
which we set to best approximate our weakly syn-
chronized model from (3):
?? = argmin
?
KL
(
Q?||P?(t, a, t?|s, s?)
)
Once we have found Q, we compute an approximate
gradient by replacing the model expectations with
expectations under Q:
EQ(a|wi)
[
?(ti, a, t?i, si, s?i)
]
? EQ(t,a,t?|si,s?i)
[
?(t, a, t?, si, s?i)
]
Now, we will briefly describe how we compute Q.
First, note that the parameters ? of Q factor along
individual source nodes, target nodes, and bispans.
The combination of the KL objective and our par-
ticular factored form of Q make our inference pro-
cedure a structured mean field algorithm (Saul and
Jordan, 1996). Structured mean field techniques are
well-studied in graphical models, and our adaptation
in this section to multiple grammars follows stan-
dard techniques (see e.g. Wainwright and Jordan,
2008).
Rather than derive the mean field updates for ?,
we describe the algorithm (shown in Figure 3) pro-
cedurally. Similar to block Gibbs sampling, we it-
eratively optimize each component (source parse,
target parse, and alignment) of the model in turn,
conditioned on the others. Where block Gibbs sam-
pling conditions on fixed trees or ITG derivations,
our mean field algorithm maintains uncertainty in
131
Input: sentence pair (s, s?)
parameter vector ?
Output: variational parameters ?
1. Initialize
?0n ? ?
>?F (n, s)
?0b??
>?A(b, s, s?)
?0n???
>?E(n?, s?)
?0n ?
?
t q?0(t)I(n ? t), etc for ?
0
b , ?
0
n?
2. While not converged, for each n, n?, b in
the monolingual and ITG charts
?in ? ?
>
(
?F (n, s) +
?
b,nb ?
i?1
b ?(n, b)+
?
b,nb
?
n?,bn? ?
i?1
b ?
i?1
n? ?./(n, b, n
?)
)
?in ?
?
t q?(t)I(n ? t) (inside-outside)
?ib ? ?
>
(
?A(b, s, s?) +
?
n,nb ?
i?1
n ?(n, b)+
?
n?,bn? ?
i?1
n? ?(b, n
?)+
?
n,nb
?
n?,bn? ?
i?1
n ?
i?1
n? ?./(n, b, n
?)
)
?b ?
?
a q?(a)I(b ? a) (bitext inside-outside)
updates for ?in? , ?
i
n? analogous to ?
i
n, ?
i
n
3. Return variational parameters ?
Figure 3: Structured mean field inference for the weakly
synchronized model. I(n ? t) is an indicator value for
the presence of node n in source tree t.
the form of monolingual parse forests or ITG forests.
The key components to this uncertainty are the
expected counts of particular source nodes, target
nodes, and bispans under the mean field distribution:
?n =
?
t
q?(t)I(n ? t)
?n? =
?
t?
q?(t?)I(n? ? t?)
?b =
?
a
q?(a)I(b ? a)
Since dynamic programs exist for summing over
each of the individual factors, these expectations can
be computed in polynomial time.
6.1 Pruning
Although we can approximate the expectations from
(4) in polynomial time using our mean field distribu-
tion, in practice we must still prune the ITG forests
and monolingual parse forests to allow tractable in-
ference. We prune our ITG forests using the same
basic idea as Haghighi et al (2009), but we em-
ploy a technique that allows us to be more aggres-
sive. Where Haghighi et al (2009) pruned bispans
based on how many unsupervised HMM alignments
were violated, we first train a maximum-matching
word aligner (Taskar et al, 2005) using our super-
vised data set, which has only half the precision er-
rors of the unsupervised HMM. We then prune ev-
ery bispan which violates at least three alignments
from the maximum-matching aligner. When com-
pared to pruning the bitext forest of our model with
Haghighi et al (2009)?s HMM technique, this new
technique allows us to maintain the same level of ac-
curacy while cutting the number of bispans in half.
In addition to pruning the bitext forests, we also
prune the syntactic parse forests using the mono-
lingual parsing model scores. For each unrefined
anchored production iAj ? iBkCj , we com-
pute the marginal probability P(iAj ,i Bk,k Cj |s) un-
der the monolingual parser (these are equivalent to
the maxrule scores from Petrov and Klein 2007). We
only include productions where this probability is
greater than 10?20. Note that at training time, we are
not guaranteed that the gold trees will be included
in the pruned forest. Because of this, we replace the
gold trees ti, t?i with oracle trees from the pruned for-
est, which can be found efficiently using a variant of
the inside algorithm (Huang, 2008).
7 Testing
Once the model has been trained, we still need to
determine how to use it to predict parses and word
alignments for our test sentence pairs. Ideally, given
the sentence pair (s, s?), we would find:
(t?, w?, t??) = argmax
t,w,t?
P(t, w, t?|s, s?)
= argmax
t,w,t?
?
a?A(w)
P(t, a, t?|s, s?)
Of course, this is also intractable, so we once again
resort to our mean field approximation. This yields
the approximate solution:
(t?, w?, t??) = argmax
t,w,t?
?
a?A(w)
Q(t, a, t?)
However, recall that Q incorporates the model?s mu-
tual constraint into the variational parameters, which
132
factor into q(t), q(a), and q(t?). This allows us to
simplify further, and find the maximum a posteriori
assignments under the variational distribution. The
trees can be found quickly using the Viterbi inside
algorithm on their respective qs. However, the sum
for computing w? under q is still intractable.
As we cannot find the maximum probability word
alignment, we provide two alternative approaches
for finding w?. The first is to just find the Viterbi
ITG derivation a? = argmaxa q(a) and then set w?
to contain exactly the 1x1 bispans in a?. The second
method, posterior thresholding, is to compute poste-
rior marginal probabilities under q for each 1x1 cell
beginning at position i, j in the word alignment grid:
m(i, j) =
?
a
q(a)I((i, i+ 1, j, j + 1) ? a)
We then include w(i, j) in w? if m(w(i, j)) > ? ,
where ? is a threshold chosen to trade off precision
and recall. For our experiments, we found that the
Viterbi alignment was uniformly worse than poste-
rior thresholding. All the results from the next sec-
tion use the threshold ? = 0.25.
8 Experiments
We trained and tested our model on the translated
portion of the Chinese treebank (Bies et al, 2007),
which includes hand annotated Chinese and English
parses and word alignments. We separated the data
into three sets: train, dev, and test, according to the
standard Chinese treebank split. To speed up train-
ing, we only used training sentences of length ? 50
words, which left us with 1974 of 2261 sentences.
We measured the results in two ways. First, we
directly measured F1 for English parsing, Chinese
parsing, and word alignment on a held out section of
the hand annotated corpus used to train the model.
Next, we further evaluated the quality of the word
alignments produced by our model by using them as
input for a machine translation system.
8.1 Dataset-specific ITG Terminals
The Chinese treebank gold word alignments include
significantly more many-to-many word alignments
than those used by Haghighi et al (2009). We are
able to produce some of these many-to-many align-
ments by including new many-to-many terminals in
t
h
e
e
n
t
i
r
e
c
o
u
n
t
r
y
i
n
r
e
c
e
n
t
y
e
a
r
s
b
o
t
h
s
i
d
e
s
?
?
?
?
??
?
(a) 2x2
t
h
e
e
n
t
i
r
e
c
o
u
n
t
r
y
i
n
r
e
c
e
n
t
y
e
a
r
s
b
o
t
h
s
i
d
e
s
?
?
?
?
??
?
(b) 2x3
t
h
e
e
n
t
i
r
e
c
o
u
n
t
r
y
i
n
r
e
c
e
n
t
y
e
a
r
s
b
o
t
h
s
i
d
e
s
?
?
?
?
??
?
(c) Gapped 2x3
Figure 4: Examples of phrasal alignments that can be rep-
resented by our new ITG terminal bispans.
our ITG word aligner, as shown in Figure 4. Our
terminal productions sometimes capture non-literal
translation like both sides or in recent years. They
also can allow us to capture particular, systematic
changes in the annotation standard. For example,
the gapped pattern from Figure 4 captures the stan-
dard that English word the is always aligned to the
Chinese head noun in a noun phrase. We featurize
these non-terminals with features similar to those
of Haghighi et al (2009), and all of the alignment
results we report in Section 8.2 (both joint and ITG)
employ these features.
8.2 Parsing and Word Alignment
To compute features that depend on external models,
we needed to train an unsupervised word aligner and
monolingual English and Chinese parsers. The un-
supervised word aligner was a pair of jointly trained
HMMs (Liang et al, 2006), trained on the FBIS cor-
pus. We used the Berkeley Parser (Petrov and Klein,
2007) for both monolingual parsers, with the Chi-
nese parser trained on the full Chinese treebank, and
the English parser trained on a concatenation of the
Penn WSJ corpus (Marcus et al, 1993) and the En-
glish side of train.6
We compare our parsing results to the mono-
lingual parsing models and to the English-Chinese
bilingual reranker of Burkett and Klein (2008),
trained on the same dataset. The results are in
Table 1. For word alignment, we compare to
6To avoid overlap in the data used to train the monolingual
parsers and the joint model, at training time, we used a separate
version of the Chinese parser, trained only on articles 400-1151
(omitting articles in train). For English parsing, we deemed it
insufficient to entirely omit the Chinese treebank data from the
monolingual parser?s training set, as otherwise the monolingual
parser would be trained entirely on out-of-domain data. There-
fore, at training time we used two separate English parsers: to
compute model scores for the first half of train, we used a parser
trained on a concatenation of the WSJ corpus and the second
half of train, and vice versa for the remaining sentences.
133
Test Results
Ch F1 Eng F1 Tot F1
Monolingual 83.6 81.2 82.5
Reranker 86.0 83.8 84.9
Joint 85.7 84.5 85.1
Table 1: Parsing results. Our joint model has the highest
reported F1 for English-Chinese bilingual parsing.
Test Results
Precision Recall AER F1
HMM 86.0 58.4 30.0 69.5
ITG 86.8 73.4 20.2 79.5
Joint 85.5 84.6 14.9 85.0
Table 2: Word alignment results. Our joint model has the
highest reported F1 for English-Chinese word alignment.
the baseline unsupervised HMM word aligner and
to the English-Chinese ITG-based word aligner
of Haghighi et al (2009). The results are in Table 2.
As can be seen, our model makes substantial im-
provements over the independent models. For pars-
ing, we improve absolute F1 over the monolingual
parsers by 2.1 in Chinese, and by 3.3 in English.
For word alignment, we improve absolute F1 by 5.5
over the non-syntactic ITG word aligner. In addi-
tion, our English parsing results are better than those
of the Burkett and Klein (2008) bilingual reranker,
the current top-performing English-Chinese bilin-
gual parser, despite ours using a much simpler set
of synchronization features.
8.3 Machine Translation
We further tested our alignments by using them to
train the Joshua machine translation system (Li and
Khudanpur, 2008). Table 3 describes the results of
our experiments. For all of the systems, we tuned
Rules Tune Test
HMM 1.1M 29.0 29.4
ITG 1.5M 29.9 30.4?
Joint 1.5M 29.6 30.6
Table 3: Tune and test BLEU results for machine transla-
tion systems built with different alignment tools. ? indi-
cates a statistically significant difference between a sys-
tem?s test performance and the one above it.
on 1000 sentences of the NIST 2004 and 2005 ma-
chine translation evaluations, and tested on 400 sen-
tences of the NIST 2006 MT evaluation. Our train-
ing set consisted of 250k sentences of newswire dis-
tributed with the GALE project, all of which were
sub-sampled to have high Ngram overlap with the
tune and test sets. All of our sentences were of
length at most 40 words. When building the trans-
lation grammars, we used Joshua?s default ?tight?
phrase extraction option. We ran MERT for 4 itera-
tions, optimizing 20 weight vectors per iteration on
a 200-best list.
Table 3 gives the results. On the test set, we also
ran the approximate randomization test suggested by
Riezler and Maxwell (2005). We found that our joint
parsing and alignment system significantly outper-
formed the HMM aligner, but the improvement over
the ITG aligner was not statistically significant.
9 Conclusion
The quality of statistical machine translation mod-
els depends crucially on the quality of word align-
ments and syntactic parses for the bilingual training
corpus. Our work presented the first joint model
for parsing and alignment, demonstrating that we
can improve results on both of these tasks, as well
as on downstream machine translation, by allowing
parsers and word aligners to simultaneously inform
one another. Crucial to this improved performance
is a notion of weak synchronization, which allows
our model to learn when pieces of a grammar are
synchronized and when they are not. Although ex-
act inference in the weakly synchronized model is
intractable, we developed a mean field approximate
inference scheme based on monolingual and bitext
parsing, allowing for efficient inference.
Acknowledgements
We thank Adam Pauls and John DeNero for their
help in running machine translation experiments.
We also thank the three anonymous reviewers for
their helpful comments on an earlier draft of this
paper. This project is funded in part by NSF
grants 0915265 and 0643742, an NSF graduate re-
search fellowship, the CIA under grant HM1582-09-
1-0021, and BBN under DARPA contract HR0011-
06-C-0022.
134
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese translation treebank v 1.0.
Web download. LDC2007T02.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In EMNLP.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment for syntax-
based statistical machine translation. In ACL MT
Workshop.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In ACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
SSST.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuki Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Jon May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In EMNLP.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
Optimization. Springer.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Workshop on Intrinsic and Extrinsic Eval-
uation Methods for MT and Summarization, ACL.
Lawrence Saul and Michael Jordan. 1996. Exploit-
ing tractable substructures in intractable networks. In
NIPS.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In ACL.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In HLT-NAACL.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In EMNLP.
David A. Smith and Noah A. Smith. 2004. Bilin-
gual parsing with factored estimation: using English
to parse Korean. In EMNLP.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In ACL.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In EMNLP.
Martin J Wainwright and Michael I Jordan. 2008.
Graphical Models, Exponential Families, and Varia-
tional Inference. Now Publishers Inc., Hanover, MA,
USA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Andreas Zollmann, Ashish Venugopal, Stephan Vogel,
and Alex Waibel. 2006. The CMU-AKA syntax aug-
mented machine translation system for IWSLT-06. In
IWSLT.
135
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 46?54,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Learning Better Monolingual Models with Unannotated Bilingual Text
David Burkett? Slav Petrov? John Blitzer? Dan Klein?
?University of California, Berkeley ?Google Research
{dburkett,blitzer,klein}@cs.berkeley.edu slav@google.com
Abstract
This work shows how to improve state-of-the-art
monolingual natural language processing models
using unannotated bilingual text. We build a mul-
tiview learning objective that enforces agreement
between monolingual and bilingual models. In
our method the first, monolingual view consists of
supervised predictors learned separately for each
language. The second, bilingual view consists of
log-linear predictors learned over both languages
on bilingual text. Our training procedure estimates
the parameters of the bilingual model using the
output of the monolingual model, and we show how
to combine the two models to account for depen-
dence between views. For the task of named entity
recognition, using bilingual predictors increases F1
by 16.1% absolute over a supervised monolingual
model, and retraining on bilingual predictions
increases monolingual model F1 by 14.6%. For
syntactic parsing, our bilingual predictor increases
F1 by 2.1% absolute, and retraining a monolingual
model on its output gives an improvement of 2.0%.
1 Introduction
Natural language analysis in one language can be
improved by exploiting translations in another lan-
guage. This observation has formed the basis for
important work on syntax projection across lan-
guages (Yarowsky et al, 2001; Hwa et al, 2005;
Ganchev et al, 2009) and unsupervised syntax
induction in multiple languages (Snyder et al,
2009), as well as other tasks, such as cross-lingual
named entity recognition (Huang and Vogel, 2002;
Moore, 2003) and information retrieval (Si and
Callan, 2005). In all of these cases, multilingual
models yield increased accuracy because differ-
ent languages present different ambiguities and
therefore offer complementary constraints on the
shared underlying labels.
In the present work, we consider a setting where
we already possess supervised monolingual mod-
els, and wish to improve these models using unan-
notated bilingual parallel text (bitext). We cast this
problem in the multiple-view (multiview) learning
framework (Blum and Mitchell, 1998; Collins and
Singer, 1999; Balcan and Blum, 2005; Ganchev et
al., 2008). Our two views are a monolingual view,
which uses the supervised monolingual models but
not bilingual information, and a bilingual view,
which exploits features that measure agreement
across languages. The parameters of the bilin-
gual view are trained to reproduce the output of
the monolingual view. We show that by introduc-
ing weakened monolingual models into the bilin-
gual view, we can optimize the parameters of the
bilingual model to improve monolingual models.
At prediction time, we automatically account for
the between-view dependence introduced by the
weakened monolingual models with a simple but
effective view-combination heuristic.
We demonstrate the performance of this method
on two problems. The first is named en-
tity recognition (NER). For this problem, our
method automatically learns (a variation on) ear-
lier hand-designed rule-based bilingual NER pre-
dictors (Huang and Vogel, 2002; Moore, 2003),
resulting in absolute performance gains of up to
16.1% F1. The second task we consider is statis-
tical parsing. For this task, we follow the setup
of Burkett and Klein (2008), who improved Chi-
nese and English monolingual parsers using par-
allel, hand-parsed text. We achieve nearly iden-
tical improvements using a purely unlabeled bi-
text. These results carry over to machine transla-
tion, where we can achieve slightly better BLEU
improvements than the supervised model of Bur-
kett and Klein (2008) since we are able to train
our model directly on the parallel data where we
perform rule extraction.
Finally, for both of our tasks, we use our bilin-
gual model to generate additional automatically
labeled monolingual training data. We compare
46
this approach to monolingual self-training and
show an improvement of up to 14.4% F1 for entity
recognition. Even for parsing, where the bilingual
portion of the treebank is much smaller than the
monolingual, our technique still can improve over
purely monolingual self-training by 0.7% F1.
2 Prior Work on Learning from
Bilingual Text
Prior work in learning monolingual models from
bitexts falls roughly into three categories: Unsu-
pervised induction, cross-lingual projection, and
bilingual constraints for supervised monolingual
models. Two recent, successful unsupervised
induction methods are those of Blunsom et al
(2009) and Snyder et al (2009). Both of them es-
timate hierarchical Bayesian models and employ
bilingual data to constrain the types of models that
can be derived. Projection methods, on the other
hand, were among the first applications of parallel
text (after machine translation) (Yarowsky et al,
2001; Yarowsky and Ngai, 2001; Hwa et al, 2005;
Ganchev et al, 2009). They assume the existence
of a good, monolingual model for one language
but little or no information about the second lan-
guage. Given a parallel sentence pair, they use the
annotations for one language to heavily constrain
the set of possible annotations for the other.
Our work falls into the final category: We wish
to use bilingual data to improve monolingual mod-
els which are already trained on large amounts of
data and effective on their own (Huang and Vo-
gel, 2002; Smith and Smith, 2004; Snyder and
Barzilay, 2008; Burkett and Klein, 2008). Proce-
durally, our work is most closely related to that
of Burkett and Klein (2008). They used an an-
notated bitext to learn parse reranking models for
English and Chinese, exploiting features that ex-
amine pieces of parse trees in both languages. Our
method can be thought of as the semi-supervised
counterpart to their supervised model. Indeed, we
achieve nearly the same results, but without anno-
tated bitexts. Smith and Smith (2004) consider
a similar setting for parsing both English and Ko-
rean, but instead of learning a joint model, they
consider a fixed combination of two parsers and
a word aligner. Our model learns parameters for
combining two monolingual models and poten-
tially thousands of bilingual features. The result
is that our model significantly improves state-of-
the-art results, for both parsing and NER.
3 A Multiview Bilingual Model
Given two input sentences x = (x1, x2) that
are word-aligned translations of each other, we
consider the problem of predicting (structured)
labels y = (y1, y2) by estimating conditional
models on pairs of labels from both languages,
p(y1, y2|x1, x2). Our model consists of two views,
which we will refer to as monolingual and bilin-
gual. The monolingual view estimates the joint
probability as the product of independent marginal
distributions over each language, pM (y|x) =
p1(y1|x1)p2(y2|x2). In our applications, these
marginal distributions will be computed by state-
of-the-art statistical taggers and parsers trained on
large monolingual corpora.
This work focuses on learning parameters for
the bilingual view of the data. We parameterize
the bilingual view using at most one-to-one match-
ings between nodes of structured labels in each
language (Burkett and Klein, 2008). In this work,
we use the term node to indicate a particular com-
ponent of a label, such as a single (multi-word)
named entity or a node in a parse tree. In Fig-
ure 2(a), for example, the nodes labeled NP1 in
both the Chinese and English trees are matched.
Since we don?t know a priori how the components
relate to one another, we treat these matchings as
hidden. For each matching a and pair of labels
y, we define a feature vector ?(y1, a, y2) which
factors on edges in the matching. Our model is
a conditional exponential family distribution over
matchings and labels:
p?(y, a|x) = exp
[
?>?(y1, a, y2)?A(?;x)
]
,
where ? is a parameter vector, and A(?;x) is the
log partition function for a sentence pair x. We
must approximate A(?;x) because summing over
all at most one-to-one matchings a is #P-hard. We
approximate this sum using the maximum-scoring
matching (Burkett and Klein, 2008):
A?(?;x) = log
?
y
max
a
(
exp
[
?>?(y1, a, y2)
])
.
In order to compute the distribution on labels y, we
must marginalize over hidden alignments between
nodes, which we also approximate by using the
maximum-scoring matching:
q?(y|x)
def
= max
a
exp
[
?>?(y1, a, y2)?A?(?;x)
]
.
47
the reports of European Court
ORG1
of Auditors
die Berichte des Europ?ischen Rechnungshofes
ORG1
the
Figure 1: An example where English NER can be
used to disambiguate German NER.
We further simplify inference in our model by
working in a reranking setting (Collins, 2000;
Charniak and Johnson, 2005), where we only con-
sider the top k outputs from monolingual models
in both languages, for a total of k2 labels y. In
practice, k2 ? 10, 000 for our largest problem.
3.1 Including Weakened Models
Now that we have defined our bilingual model, we
could train it to agree with the output of the mono-
lingual model (Collins and Singer, 1999; Ganchev
et al, 2008). As we will see in Section 4, however,
the feature functions ?(y1, a, y2) make no refer-
ence to the input sentences x, other than through a
fixed word alignment. With such limited monolin-
gual information, it is impossible for the bilingual
model to adequately capture all of the information
necessary for NER or parsing. As a simple ex-
ample, a bilingual NER model will be perfectly
happy to label two aligned person names as ORG
instead of PER: both labelings agree equally well.
We briefly illustrate how poorly such a basic bilin-
gual model performs in Section 10.
One way to solve this problem is to include the
output of the full monolingual models as features
in the bilingual view. However, we are training the
bilingual view to match the output of these same
models, which can be trivially achieved by putting
weight on only the monolingual model scores and
never recruiting any bilingual features. There-
fore, we use an intermediate approach: we intro-
duce the output of deliberately weakened mono-
lingual models as features in the bilingual view.
A weakened model is from the same class as the
full monolingual models, but is intentionally crip-
pled in some way (by removing feature templates,
for example). Crucially, the weakened models will
make predictions that are roughly similar to the
full models, but systematically worse. Therefore,
model scores from the weakened models provide
enough power for the bilingual view to make accu-
Feat. types Examples
Algn Densty INSIDEBOTH=3 INENONLY=0
Indicators LBLMATCH=true BIAS=true
Table 1: Sample features used for named entity
recognition for the ORG entity in Figure 1.
rate predictions, but ensure that bilingual features
will be required to optimize the training objective.
Let `W1 = log p
W
1 (y1|x1), `
W
2 = log p
W
2 (y2|x2)
be the log-probability scores from the weakened
models. Our final approximation to the marginal
distribution over labels y is:
q?1,?2,?(y|x)
def
= max
a
exp
h
?1`
W
1 + ?2`
W
2 +
?>?(y1, a, y2)? A?(?1, ?2,?;x)
i
.
(1)
Where
A?(?1, ?2,?;x) =
log
X
y
max
a
exp
h
?1`
W
1 + ?2`
W
2 + ?
>?(y1, a, y2)
i
is the updated approximate log partition function.
4 NER and Parsing Examples
Before formally describing our algorithm for find-
ing the parameters [?1, ?2,?], we first give exam-
ples of our problems of named entity recognition
and syntactic parsing, together with node align-
ments and features for each. Figure 1 depicts a
correctly-labeled sentence fragment in both En-
glish and German. In English, the capitalization of
the phrase European Court of Auditors helps iden-
tify the span as a named entity. However, in Ger-
man, all nouns are capitalized, and capitalization
is therefore a less useful cue. While a monolin-
gual German tagger is likely to miss the entity in
the German text, by exploiting the parallel English
text and word alignment information, we can hope
to improve the German performance, and correctly
tag Europa?ischen Rechnungshofes.
The monolingual features are standard features
for discriminative, state-of-the-art entity recogniz-
ers, and we can produce weakened monolingual
models by simply limiting the feature set. The
bilingual features, ?(y1, a, y2), are over pairs of
aligned nodes, where nodes of the labels y1 and
y2 are simply the individual named entities. We
use a small bilingual feature set consisting of two
types of features. First, we use the word alignment
density features from Burkett and Klein (2008),
which measure how well the aligned entity pair
matches up with alignments from an independent
48
Input: full and weakened monolingual models:
p1(y1|x1), p2(y2|x2), p
w
1 (y1|x1), p
w
2 (y2|x2)
unannotated bilingual data: U
Output: bilingual parameters: ??, ??1, ??2
1. Label U with full monolingual models:
?x ? U, y?M = argmaxy p1(y1|x1)p2(y2|x2).
2. Return argmax?1,?2,?
Q
x?U q?,?1,?2 (y?M |x),
where q?,?1,?2 has the form in Equation 1.
Figure 3: Bilingual training with multiple views.
word aligner. We also include two indicator fea-
tures: a bias feature that allows the model to learn
a general preference for matched entities, and a
feature that is active whenever the pair of nodes
has the same label. Figure 1 contains sample val-
ues for each of these features.
Another natural setting where bilingual con-
straints can be exploited is syntactic parsing. Fig-
ure 2 shows an example English prepositional
phrase attachment ambiguity that can be resolved
bilingually by exploiting Chinese. The English
monolingual parse mistakenly attaches to to the
verb increased. In Chinese, however, this ambi-
guity does not exist. Instead, the word ?, which
aligns to to, has strong selectional preference for
attaching to a noun on the left.
In our parsing experiments, we use the Berke-
ley parser (Petrov et al, 2006; Petrov and Klein,
2007), a split-merge latent variable parser, for our
monolingual models. Our full model is the re-
sult of training the parser with five split-merge
phases. Our weakened model uses only two. For
the bilingual model, we use the same bilingual fea-
ture set as Burkett and Klein (2008). Table 2 gives
some examples, but does not exhaustively enumer-
ate those features.
5 Training Bilingual Models
Previous work in multiview learning has focused
on the case of agreement regularization (Collins
and Singer, 1999; Ganchev et al, 2008). If we had
bilingual labeled data, together with our unlabeled
data and monolingual labeled data, we could ex-
ploit these techniques. Because we do not possess
bilingual labeled data, we must train the bilingual
model in another way. Here we advocate train-
ing the bilingual model (consisting of the bilin-
gual features and weakened monolingual models)
to imitate the full monolingual models. In terms
of agreement regularization, our procedure may be
thought of as ?regularizing? the bilingual model to
be similar to the full monolingual models.
Input: full and weakened monolingual models:
p1(y1|x1), p2(y2|x2), p
w
1 (y1|x1), p
w
2 (y2|x2)
bilingual parameters: ??, ??1, ??2
bilingual input: x = (x1, x2)
Output: bilingual label: y?
Bilingual w/ Weak Bilingual w/ Full
1a. l1 = log
`
pw1 (y1|x1)
?
1b. l1 = log
`
p1(y1|x1)
?
2a. l2 = log
`
pw2 (y2|x2)
?
2b. l2 = log
`
p2(y2|x2)
?
3. Return argmaxy maxa ??1l1 + ??2l2+??
>
?(y1, a, y2)
Figure 4: Prediction by combining monolingual
and bilingual models.
Our training algorithm is summarized in Fig-
ure 3. For each unlabeled point x = (x1, x2), let
y?M be the joint label which has the highest score
from the independent monolingual models (line
1). We then find bilingual parameters ??, ??1, ??2
that maximize q??,??1,??2(y?x|x) (line 2). This max-
likelihood optimization can be solved by an EM-
like procedure (Burkett and Klein, 2008). This
procedure iteratively updates the parameter esti-
mates by (a) finding the optimum alignments for
each candidate label pair under the current pa-
rameters and then (b) updating the parameters to
maximize a modified version of Equation 1, re-
stricted to the optimal alignments. Because we re-
strict alignments to the set of at most one-to-one
matchings, the (a) step is tractable using the Hun-
garian algorithm. With the alignments fixed, the
(b) step just involves maximizing likelihood under
a log-linear model with no latent variables ? this
problem is convex and can be solved efficiently
using gradient-based methods. The procedure has
no guarantees, but is observed in practice to con-
verge to a local optimum.
6 Predicting with Monolingual and
Bilingual Models
Once we have learned the parameters of the bilin-
gual model, the standard method of bilingual pre-
diction would be to just choose the y that is most
likely under q??,??1,??2 :
y? = argmax
y
q??,??1,??2(y|x) . (2)
We refer to prediction under this model as ?Bilin-
gual w/ Weak,? to evoke the fact that the model is
making use of weakened monolingual models in
its feature set.
Given that we have two views of the data,
though, we should be able to leverage additional
information in order to make better predictions. In
49
VB 
NP1 
NP 
VP 
S 
These measures increased the attractiveness of Tianjin to Taiwanese merchants 
(a) 
NP PP PP 
These measures increased the attractiveness of Tianjin to Taiwanese merchants 
VB 
NP 
NP 
VP1 
S 
NP PP PP 
?? ? ?? ? ? ? ?? ? ?? ?? ?
S 
NP 
VB NNP 
PP 
DE NN 
NP1 
VP 
?? ? ?? ? ? ? ?? ? ?? ?? ?
S 
NP 
VB NNP 
PP 
DE NN 
NP1 
VP 
(b) 
Figure 2: An example of PP attachment that is ambiguous in English, but simple in Chinese. In (a) the
correct parses agree (low PP attachment), whereas in (b) the incorrect parses disagree.
Feature Types Feature Templates
Examples
Correct Incorrect
Alignment Density INSIDEBOTH, INSIDEENONLY INSIDEENONLY=0 INSIDEENONLY=1
Span Difference ABSDIFFERENCE ABSDIFFERENCE=3 ABSDIFFERENCE=4
Syntactic Indicators LABEL?E,C?, NUMCHILDREN?E,C? LABEL?NP,NP?=true LABEL?VP,NP?=true
Table 2: Sample bilingual features used for parsing. The examples are features that would be extracted
by aligning the parents of the PP nodes in Figure 2(a) (Correct) and Figure 2(b) (Incorrect).
particular, the monolingual view uses monolingual
models that are known to be superior to the mono-
lingual information available in the bilingual view.
Thus, we would like to find some way to incorpo-
rate the full monolingual models into our predic-
tion method. One obvious choice is to choose the
labeling that maximizes the ?agreement distribu-
tion? (Collins and Singer, 1999; Ganchev et al,
2008). In our setting, this amounts to choosing:
y? = argmax
y
pM (y|x) q??,??1??2(y|x) . (3)
This is the correct decision rule if the views are
independent and the labels y are uniformly dis-
tributed a priori,1 but we have deliberately in-
troduced between-view dependence in the form
of the weakened monolingual models. Equa-
tion 3 implicitly double-counts monolingual infor-
mation.
One way to avoid this double-counting is to
simply discard the weakened monolingual models
when making a joint prediction:
y? = argmax
y
max
a
pM (y|x)
exp
[
??
>
?(y1, a, y2)
]
.
(4)
1See, e.g. Ando & Zhang(Ando and Zhang, 2007) for a
derivation of the decision rule from Equation 3 under these
assumptions.
This decision rule uniformly combines the two
monolingual models and the bilingual model.
Note, however, that we have already learned non-
uniform weights for the weakened monolingual
models. Our final decision rule uses these weights
as weights for the full monolingual models:
y? = argmax
y
max
a
exp
[
??1 log
(
p1(y1|x1)
)
+
??2 log
(
p2(y2|x2)
)
+??
>
?(y1, a, y2)
]
. (5)
As we will show in Section 10, this rule for com-
bining the monolingual and bilingual views per-
forms significantly better than the alternatives, and
comes close to the optimal weighting for the bilin-
gual and monolingual models.
We will refer to predictions made with Equa-
tion 5 as ?Bilingual w/ Full?, to evoke the use of
the full monolingual models alongside our bilin-
gual features. Prediction using ?Bilingual w/
Weak? and ?Bilingual w/ Full? is summarized in
Figure 4.
7 Retraining Monolingual Models
Although bilingual models have many direct ap-
plications (e.g. in machine translation), we also
wish to be able to apply our models on purely
monolingual data. In this case, we can still take
50
Input: annotated monolingual data: L1, L2
unannotated bilingual data: U
monolingual models: p1(y1|x1), p2(y2|x2)
bilingual parameters: ??, ??1, ??2
Output: retrained monolingual models:
pr1(y1|x1), p
r
2(y2|x2)
?x = (x1, x2) ? U:
Self-Retrained Bilingual-Retrained
1a. y?x1 = argmaxy1 p1(y1|x1) 1b. Pick y?x, Fig. 4
y?x2 = argmaxy2 p2(y2|x2) (Bilingual w/ Full)
2. Add (x1, y?x1 ) to L1 and add (x2, y?x2 ) to L2.
3. Return full monolingual models pr1(y1|x1),
pr2(y2|x2) trained on newly enlarged L1, L2.
Figure 5: Retraining monolingual models.
advantage of parallel corpora by using our bilin-
gual models to generate new training data for the
monolingual models. This can be especially use-
ful when we wish to use our monolingual models
in a domain for which we lack annotated data, but
for which bitexts are plentiful.2
Our retraining procedure is summarized in Fig-
ure 5. Once we have trained our bilingual param-
eters and have a ?Bilingual w/ Full? predictor (us-
ing Equation 5), we can use that predictor to an-
notate a large corpus of parallel data (line 1b). We
then retrain the full monolingual models on a con-
catenation of their original training data and the
newly annotated data (line 3). We refer to the new
monolingual models retrained on the output of the
bilingual models as ?Bilingual-Retrained,? and we
tested such models for both NER and parsing. For
comparison, we also retrained monolingual mod-
els directly on the output of the original full mono-
lingual models, using the same unannotated bilin-
gual corpora for self-training (line 1a). We refer to
these models as ?Self-Retrained?.
We evaluated our retrained monolingual mod-
els on the same test sets as our bilingual mod-
els, but using only monolingual data at test time.
The texts used for retraining overlapped with the
bitexts used for training the bilingual model, but
both sets were disjoint from the test sets.
8 NER Experiments
We demonstrate the utility of multiview learn-
ing for named entity recognition (NER) on En-
glish/German sentence pairs. We built both our
full and weakened monolingual English and Ger-
man models from the CoNLL 2003 shared task
2Of course, unannotated monolingual data is even more
plentiful, but as we will show, with the same amount of data,
our method is more effective than simple monolingual self-
training.
training data. The bilingual model parameters
were trained on 5,000 parallel sentences extracted
from the Europarl corpus. For the retraining
experiments, we added an additional 5,000 sen-
tences, for 10,000 in all. For testing, we used
the Europarl 2006 development set and the 2007
newswire test set. Neither of these data sets were
annotated with named entities, so we manually an-
notated 200 sentences from each of them.
We used the Stanford NER tagger (Finkel et
al., 2005) with its default configuration as our full
monolingual model for each language. We weak-
ened both the English and German models by re-
moving several non-lexical and word-shape fea-
tures. We made one more crucial change to our
monolingual German model. The German entity
recognizer has extremely low recall (44 %) when
out of domain, so we chose y?x from Figure 3 to
be the label in the top five which had the largest
number of named entities.
Table 3 gives results for named entity recogni-
tion. The first two rows are the full and weak-
ened monolingual models alone. The second two
are the multiview trained bilingual models. We
first note that for English, using the full bilin-
gual model yields only slight improvements over
the baseline full monolingual model, and in prac-
tice the predictions were almost identical. For this
problem, the monolingual German model is much
worse than the monolingual English model, and so
the bilingual model doesn?t offer significant im-
provements in English. The bilingual model does
show significant German improvements, however,
including a 16.1% absolute gain in F1 over the
baseline for parliamentary proceedings.
The last two rows of Table 3 give results for
monolingual models which are trained on data that
was automatically labeled using the our models.
English results were again mixed, due to the rel-
atively weak English performance of the bilin-
gual model. For German, though, the ?Bilingual-
Retrained? model improves 14.4% F1 over the
?Self-Retrained? baseline.
9 Parsing Experiments
Our next set of experiments are on syntactic pars-
ing of English and Chinese. We trained both our
full and weakened monolingual English models
on the Penn Wall Street Journal corpus (Marcus
et al, 1993), as described in Section 4. Our full
and weakened Chinese models were trained on
51
Eng Parliament Eng Newswire Ger Parliament Ger Newswire
Prec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1
Monolingual Models (Baseline)
Weak Monolingual 52.6 65.9 58.5 67.7 83.0 74.6 71.3 36.4 48.2 80.0 51.5 62.7
Full Monolingual 65.7 71.4 68.4 80.1 88.7 84.2 69.8 44.0 54.0 73.0 56.4 63.7
Multiview Trained Bilingual Models
Bilingual w/ Weak 56.2 70.8 62.7 71.4 86.2 78.1 70.1 66.3 68.2 76.5 76.1 76.3
Bilingual w/ Full 65.4 72.4 68.7 80.6 88.7 84.4 70.1 70.1 70.1 74.6 77.3 75.9
Retrained Monolingual Models
Self-Retrained 71.7 74.0 72.9 79.9 87.4 83.5 70.4 44.0 54.2 79.3 58.9 67.6
Bilingual-Retrained 68.6 70.8 69.7 80.7 89.3 84.8 74.5 63.6 68.6 77.9 69.3 73.4
Table 3: NER Results. Rows are grouped by data condition. We bold all entries that are best in their
group and beat the strongest monolingual baseline.
Chinese English
Monolingual Models (Baseline)
Weak Monolingual 78.3 67.6
Full Monolingual 84.2 75.4
Multiview Trained Bilingual Models
Bilingual w/ Weak 80.4 70.8
Bilingual w/ Full 85.9 77.5
Supervised Trained Bilingual Models
Burkett and Klein (2008) 86.1 78.2
Retrained Monolingual Models
Self-Retrained 83.6 76.7
Bilingual-Retrained 83.9 77.4
Table 4: Parsing results. Rows are grouped by data
condition. We bold entries that are best in their
group and beat the the Full Monolingual baseline.
the Penn Chinese treebank (Xue et al, 2002) (ar-
ticles 400-1151), excluding the bilingual portion.
The bilingual data consists of the parallel part of
the Chinese treebank (articles 1-270), which also
includes manually parsed English translations of
each Chinese sentence (Bies et al, 2007). Only
the Chinese sentences and their English transla-
tions were used to train the bilingual models ? the
gold trees were ignored. For retraining, we used
the same data, but weighted it to match the sizes
of the original monolingual treebanks. We tested
on the standard Chinese treebank development set,
which also includes English translations.
Table 4 gives results for syntactic parsing. For
comparison, we also show results for the super-
vised bilingual model of Burkett and Klein (2008).
This model uses the same features at prediction
time as the multiview trained ?Bilingual w/ Full?
model, but it is trained on hand-annotated parses.
We first examine the first four rows of Table 4. The
?Bilingual w/ Full? model significantly improves
performance in both English and Chinese relative
to the monolingual baseline. Indeed, it performs
Phrase-Based System
Moses (No Parser) 18.8
Syntactic Systems
Monolingual Parser 18.7
Supervised Bilingual (Treebank Bi-trees) 21.1
Multiview Bilingual (Treebank Bitext) 20.9
Multiview Bilingual (Domain Bitext) 21.2
Table 5: Machine translation results.
only slightly worse than the supervised model.
The last two rows of Table 4 are the results of
monolingual parsers trained on automatically la-
beled data. In general, gains in English, which
is out of domain relative to the Penn Treebank,
are larger than those in Chinese, which is in do-
main. We also emphasize that, unlike our NER
data, this bitext was fairly small relative to the an-
notated monolingual data. Therefore, while we
still learn good bilingual model parameters which
give a sizable agreement-based boost when doing
bilingual prediction, we don?t expect retraining to
result in a coverage-based boost in monolingual
performance.
9.1 Machine Translation Experiments
Although we don?t have hand-labeled data for our
largest Chinese-English parallel corpora, we can
still evaluate our parsing results via our perfor-
mance on a downstream machine translation (MT)
task. Our experimental setup is as follows: first,
we used the first 100,000 sentences of the English-
Chinese bitext from Wang et al (2007) to train
Moses (Koehn et al, 2007), a phrase-based MT
system that we use as a baseline. We then used the
same sentences to extract tree-to-string transducer
rules from target-side (English) trees (Galley et al,
2004). We compare the single-reference BLEU
scores of syntactic MT systems that result from
using different parsers to generate these trees.
52
0.0 0.2 
0.4 0.6 
0.8 1.0 
1.2 1.4 
0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 
68-71 65-68 62-65 59-62 56-59 
English Weight 
German
 Weigh
t 
German F1 
70.3 70.1 59.1 
* + * + 
(a) 
0.0 0.2 
0.4 0.6 
0.8 1.0 
1.2 1.4 
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 
81.8-82.1 81.5-81.8 81.2-81.5 80.9-81.2 80.6-80.9 
English Weight 
Chines
e Weig
ht 
Combined F1 
82.1 82.0 81.4 
* + ? 
* + 
? 
(b) 
Figure 6: (a) NER and (b) parsing results for different values of ?1 and ?2 (see Equation 6). ?*? shows
optimal weights, ?+? shows our learned weights, and ?-? shows uniform combination weights.
For our syntactic baseline, we used the mono-
lingual English parser. For our remaining experi-
ments, we parsed both English and Chinese simul-
taneously. The supervised model and the first mul-
tiview trained model are the same Chinese tree-
bank trained models for which we reported pars-
ing results. We also used our multiview method to
train an additional bilingual model on part of the
bitext we used to extract translation rules.
The results are shown in Table 5. Once again,
our multiview trained model yields comparable re-
sults to the supervised model. Furthermore, while
the differences are small, our best performance
comes from the model trained on in-domain data,
for which no gold trees exist.
10 Analyzing Combined Prediction
In this section, we explore combinations of the full
monolingual models, p1(y1|x1) and p2(y2|x2),
and the bilingual model, max
a
??
>
?(y1, a, y2). For
parsing, the results in this section are for combined
F1. This simply computes F1 over all of the sen-
tences in both the English and Chinese test sets.
For NER, we just use German F1, since English is
relatively constant across runs.
We begin by examining how poorly our model
performs if we do not consider monolingual in-
formation in the bilingual view. For parsing, the
combined Chinese and English F1 for this model
is 78.7%. When we combine this model uniformly
with the full monolingual model, as in Equation 4,
combined F1 improves to 81.2%, but is still well
below our best combined score of 82.1%. NER
results for a model trained without monolingual
information show an even larger decline.
Now let us consider decision rules of the form:
y? = argmax
y
max
a
exp[?1 log
`
p1(y1|x1)
?
+
?2 log
`
p2(y2|x2)
?
+??
>
?(y1, a, y2)] .
Note that when ?1 = ?2 = 1, this is exactly
the uniform decision rule (Equation 4). When
?1 = ??1 and ?2 = ??2, this is the ?Bilingual w/
Full? decision rule (Equation 5). Figure 6 is a
contour plot of F1 with respect to the parameters
?1 and ?2. Our decision rule ?Bilingual w/ Full?
(Equation 5, marked with a ?+?) is near the opti-
mum (?*?), while the uniform decision rule (?-?)
performs quite poorly. This is true for both NER
(Figure 6a) and parsing (Figure 6b).
There is one more decision rule which we have
yet to consider: the ?conditional independence?
decision rule from Equation 3. While this rule can-
not be shown on the plots in Figure 6 (because
it uses both the full and weakened monolingual
models), we note that it also performs poorly in
both cases (80.7% F1 for parsing, for example).
11 Conclusions
We show for the first time that state-of-the-art,
discriminative monolingual models can be signifi-
cantly improved using unannotated bilingual text.
We do this by first building bilingual models that
are trained to agree with pairs of independently-
trained monolingual models. Then we combine
the bilingual and monolingual models to account
for dependence across views. By automatically
annotating unlabeled bitexts with these bilingual
models, we can train new monolingual models that
do not rely on bilingual data at test time, but still
perform substantially better than models trained
using only monolingual resources.
Acknowledgements
This project is funded in part by NSF grants
0915265 and 0643742, an NSF graduate research
fellowship, the DNI under grant HM1582-09-1-
0021, and BBN under DARPA contract HR0011-
06-C-0022.
53
References
Rie Kubota Ando and Tong Zhang. 2007. Two-view
feature generation model for semi-supervised learn-
ing. In ICML.
Maria-Florina Balcan and Avrim Blum. 2005. A pac-
style model for learning from labeled and unlabeled
data. In COLT.
Ann Bies, Martha Palmer, Justin Mott, and Colin
Warner. 2007. English chinese translation treebank
v 1.0. Web download. LDC2007T02.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009.
Bayesian synchronous grammar induction. In NIPS.
David Burkett and Dan Klein. 2008. Two lan-
guages are better than one (for syntactic parsing). In
EMNLP.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In ACL.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
EMNLP.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Kuzman Ganchev, Joao Graca, John Blitzer, and Ben
Taskar. 2008. Multi-view learning over structured
and non-identical outputs. In UAI.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In ACL.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In ICMI.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Special Issue of the Journal of Natural Language
Engineering on Parallel Texts, 11(3):311?325.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Robert Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In EACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In COLING-ACL.
Luo Si and Jamie Callan. 2005. Clef 2005: Multi-
lingual retrieval by combining multiple multilingual
ranked lists. In CLEF.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: using english to
parse korean. In EMNLP.
Benjamin Snyder and Regina Barzilay. 2008. Cross-
lingual propagation for morphological analysis. In
AAAI.
Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In ACL.
Wen Wang, Andreas Stolcke, and Jing Zheng. 2007.
Reranking machine translation hypotheses with
structured and web-based language models. In IEEE
ASRU Workshop.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In COLING.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In NAACL.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Human Language Technologies.
54
