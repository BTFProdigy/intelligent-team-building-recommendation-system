Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 917?926,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Cross-Language Document Summarization Based on Machine 
Translation Quality Prediction 
 
Xiaojun Wan, Huiying Li and Jianguo Xiao 
Institute of Compute Science and Technology, Peking University, Beijing 100871, China 
Key Laboratory of Computational Linguistics (Peking University), MOE, China 
{wanxiaojun,lihuiying,xiaojianguo}@icst.pku.edu.cn 
 
 
Abstract 
 
Cross-language document summarization is a 
task of producing a summary in one language 
for a document set in a different language. Ex-
isting methods simply use machine translation 
for document translation or summary transla-
tion. However, current machine translation 
services are far from satisfactory, which re-
sults in that the quality of the cross-language 
summary is usually very poor, both in read-
ability and content.  In this paper, we propose 
to consider the translation quality of each sen-
tence in the English-to-Chinese cross-language 
summarization process. First, the translation 
quality of each English sentence in the docu-
ment set is predicted with the SVM regression 
method, and then the quality score of each sen-
tence is incorporated into the summarization 
process. Finally, the English sentences with 
high translation quality and high informative-
ness are selected and translated to form the 
Chinese summary. Experimental results dem-
onstrate the effectiveness and usefulness of the 
proposed approach.  
 
1 Introduction 
Given a document or document set in one source 
language, cross-language document summariza-
tion aims to produce a summary in a different 
target language. In this study, we focus on Eng-
lish-to-Chinese document summarization for the 
purpose of helping Chinese readers to quickly 
understand the major content of an English docu-
ment or document set. This task is very impor-
tant in the field of multilingual information ac-
cess.  
Till now, most previous work focuses on 
monolingual document summarization, but 
cross-language document summarization has re-
ceived little attention in the past years. A 
straightforward way for cross-language docu-
ment summarization is to translate the summary 
from the source language to the target language 
by using machine translation services. However, 
though machine translation techniques have been 
advanced a lot, the machine translation quality is 
far from satisfactory, and in many cases, the 
translated texts are hard to understand. Therefore, 
the translated summary is likely to be hard to 
understand by readers, i.e., the summary quality 
is likely to be very poor. For example, the trans-
lated Chinese sentence for an ordinary English 
sentence (?It is also Mr Baker who is making the 
most of presidential powers to dispense lar-
gesse.?) by using Google Translate is ?????
????????????????????. 
The translated sentence is hard to understand 
because it contains incorrect translations and it is 
very disfluent. If such sentences are selected into 
the summary, the quality of the summary would 
be very poor.  
In order to address the above problem, we 
propose to consider the translation quality of the 
English sentences in the summarization process. 
In particular, the translation quality of each Eng-
lish sentence is predicted by using the SVM re-
gression method, and then the predicted MT 
quality score of each sentence is incorporated 
into the sentence evaluation process, and finally 
both informative and easy-to-translate sentences 
are selected and translated to form the Chinese 
summary.  
An empirical evaluation is conducted to evalu-
ate the performance of machine translation qual-
ity prediction, and a user study is performed to 
evaluate the cross-language summary quality. 
The results demonstrate the effectiveness of the 
proposed approach.  
The rest of this paper is organized as follows: 
Section 2 introduces related work. The system is 
overviewed in Section 3. In Sections 4 and 5, we 
present the detailed algorithms and evaluation 
917
results of machine translation quality prediction 
and cross-language summarization, respectively. 
We discuss in Section 6 and conclude this paper 
in Section 7.  
2 Related Work 
2.1 Machine Translation Quality Prediction 
Machine translation evaluation aims to assess the 
correctness and quality of the translation. Usu-
ally, the human reference translation is provided, 
and various methods and metrics have been de-
veloped for comparing the system-translated text 
and the human reference text. For example, the 
BLEU metric, the NIST metric and their relatives 
are all based on the idea that the more shared 
substrings the system-translated text has with the 
human reference translation, the better the trans-
lation is. Blatz et al (2003) investigate training 
sentence-level confidence measures using a vari-
ety of fuzzy match scores. Albrecht and Hwa 
(2007) rely on regression algorithms and refer-
ence-based features to measure the quality of 
sentences.  
Transition evaluation without using reference 
translations has also been investigated. Quirk 
(2004) presents a supervised method for training 
a sentence level confidence measure on transla-
tion output using a human-annotated corpus. 
Features derived from the source sentence and 
the target sentence (e.g. sentence length, perplex-
ity, etc.) and features about the translation proc-
ess are leveraged. Gamon et al (2005) investi-
gate the possibility of evaluating MT quality and 
fluency at the sentence level in the absence of 
reference translations, and they can improve on 
the correlation between language model perplex-
ity scores and human judgment by combing these 
perplexity scores with class probabilities from a 
machine-learned classifier. Specia et al (2009) 
use the ICM theory to identify the threshold to 
map a continuous predicted score into ?good? or 
?bad? categories. Chae and Nenkova (2009) use 
surface syntactic features to assess the fluency of 
machine translation results.   
In this study, we further predict the translation 
quality of an English sentence before the ma-
chine translation process, i.e., we do not leverage 
reference translation and the target sentence.  
2.2 Document Summarization  
Document summarization methods can be gener-
ally categorized into extraction-based methods 
and abstraction-based methods. In this paper, we 
focus on extraction-based methods. Extraction-
based summarization methods usually assign 
each sentence a saliency score and then rank the 
sentences in a document or document set.  
  For single document summarization, the sen-
tence score is usually computed by empirical 
combination of a number of statistical and lin-
guistic feature values, such as term frequency, 
sentence position, cue words, stigma words, 
topic signature (Luhn 1969; Lin and Hovy, 2000). 
The summary sentences can also be selected by 
using machine learning methods (Kupiec et al, 
1995; Amini and Gallinari, 2002) or graph-based 
methods (ErKan and Radev, 2004; Mihalcea and 
Tarau, 2004). Other methods include mutual re-
inforcement principle (Zha 2002; Wan et al, 
2007). 
  For multi-document summarization, the cen-
troid-based method (Radev et al, 2004) is a typi-
cal method, and it scores sentences based on 
cluster centroids, position and TFIDF features. 
NeATS (Lin and Hovy, 2002) makes use of new 
features such as topic signature to select impor-
tant sentences. Machine Learning based ap-
proaches have also been proposed for combining 
various sentence features (Wong et al, 2008).  
The influences of input difficulty on summariza-
tion performance have been investigated in 
(Nenkova and Louis, 2008). Graph-based meth-
ods have also been used to rank sentences in a 
document set. For example, Mihalcea and Tarau 
(2005) extend the TextRank algorithm to com-
pute sentence importance in a document set. 
Cluster-level information has been incorporated 
in the graph model to better evaluate sentences 
(Wan and Yang, 2008). Topic-focused or query 
biased multi-document summarization has also 
been investigated (Wan et al, 2006). Wan et al 
(2010) propose the EUSUM system for extract-
ing easy-to-understand English summaries for 
non-native readers.  
Several pilot studies have been performed for 
the cross-language summarization task by simply 
using document translation or summary transla-
tion. Leuski et al (2003) use machine translation 
for English headline generation for Hindi docu-
ments. Lim et al (2004) propose to generate a 
Japanese summary without using a Japanese 
summarization system, by first translating Japa-
nese documents into Korean documents, and 
then extracting summary sentences by using Ko-
rean summarizer, and finally mapping Korean 
summary sentences to Japanese summary sen-
tences. Chalendar et al (2005) focuses on se-
mantic analysis and sentence generation tech-
niques for cross-language summarization. Orasan 
918
and Chiorean (2008) propose to produce summa-
ries with the MMR method from Romanian news 
articles and then automatically translate the 
summaries into English. Cross language query 
based summarization has been investigated in 
(Pingali et al, 2007), where the query and the 
documents are in different languages. Other re-
lated work includes multilingual summarization 
(Lin et al, 2005), which aims to create summa-
ries from multiple sources in multiple languages. 
Siddharthan and McKeown (2005) use the in-
formation redundancy in multilingual input to 
correct errors in machine translation and thus 
improve the quality of multilingual summaries.  
3 The Proposed Approach 
Previous methods for cross-language summariza-
tion usually consist of two steps: one step for 
summarization and one step for translation. Dif-
ferent order of the two steps can lead to the fol-
lowing two basic English-to-Chinese summariza-
tion methods:   
Late Translation (LateTrans): Firstly, an 
English summary is produced for the English 
document set by using existing summarization 
methods. Then, the English summary is auto-
matically translated into the corresponding Chi-
nese summary by using machine translation ser-
vices.  
Early Translation (EarlyTrans): Firstly, the 
English documents are translated into Chinese 
documents by using machine translation services. 
Then, a Chinese summary is produced for the 
translated Chinese documents.  
Generally speaking, the LateTrans method has 
a few advantages over the EarlyTrans method: 
1) The LateTrans method is much more effi-
cient than the EarlyTrans method, because only a 
very few summary sentences are required to be 
translated in the LateTrans method, whereas all 
the sentences in the documents are required to be 
translated in the EarlyTrans method.  
2)  The LateTrans method is deemed to be 
more effective than the EarlyTrans method, be-
cause the translation errors of the sentences have 
great influences on the summary sentence extrac-
tion in the EarlyTrans method. 
Thus in this study, we adopt the LateTrans 
method as our baseline method. We also adopt 
the late translation strategy for our proposed ap-
proach. 
In the baseline method, a translated Chinese 
sentence is selected into the summary because 
the original English sentence is informative. 
However, an informative and fluent English sen-
tence is likely to be translated into an uninforma-
tive and disfluent Chinese sentence, and there-
fore, this sentence cannot be selected into the 
summary.  
In order to address the above problem of exist-
ing methods, our proposed approach takes into 
account a novel factor of each sentence for cross-
language summary extraction. Each English sen-
tence is associated with a score indicating its 
translation quality. An English sentence with 
high translation quality score is more likely to be 
selected into the original English summary, and 
such English summary can be translated into a 
better Chinese summary.   Figure 1 gives the ar-
chitecture of our proposed approach.  
 
 
Figure 1: Architecture of our proposed ap-
proach 
Seen from the figure, our proposed approach 
consists of four main steps: 1) The machine 
translation quality score of each English sentence 
is predicted by using regression methods; 2) The 
informativeness score of each English sentence is 
computed by using existing methods; 3) The 
English summary is produced by making use of 
both the machine translation quality score and 
the informativeness score; 4) The extracted Eng-
lish summary is translated into Chinese summary 
by using machine translation services.  
In this study, we adopt Google Translate1 for 
English-to-Chinese translation. Google Translate 
is one of the state-of-the-art commercial machine 
translation systems used today. It applies statisti-
cal learning techniques to build a translation 
                                                 
1 http://translate.google.com/translate_t 
English 
Sentences 
Sentence    
MT Quality 
Prediction
Sentence      
Informativeness 
Evaluation 
English 
Summary 
Extraction 
EN-to-CN 
Machine 
Translation 
Chinese Summary 
Informativeness score 
English summary 
MT quality score
919
model based on both monolingual text in the tar-
get language and aligned text consisting of ex-
amples of human translations between the lan-
guages. 
The first step and the evaluation results will be 
described in Section 4, and the other steps and 
the evaluation results will be described together 
in Section 5.  
4 Machine Translation Quality Predic-
tion  
4.1 Methodology 
In this study, machine translation (MT) quality 
reflects both the translation accuracy and the flu-
ency of the translated sentence. An English sen-
tence with high MT quality score is likely to be 
translated into an accurate and fluent Chinese 
sentence, which can be easily read and under-
stand by Chinese readers.  The MT quality pre-
diction is a task of mapping an English sentence 
to a numerical value corresponding to a quality 
level. The larger the value is, the more accurately 
and fluently the sentence can be translated into 
Chinese sentence.  
As introduced in Section 2.1, several related 
work has used regression and classification 
methods for MT quality prediction without refer-
ence translations. In our approach, the MT qual-
ity of each sentence in the documents is also pre-
dicted without reference translations. The differ-
ence between our task and previous work is that 
previous work can make use of both features in 
source sentence and features in target sentence, 
while our task only leverages features in source 
sentence, because in the late translation strategy, 
the English sentences in the documents have not 
been translated yet at this step.  
In this study, we adopt the ?-support vector re-
gression (?-SVR) method (Vapnik 1995) for the 
sentence-level MT quality prediction task.  The 
SVR algorithm is firmly grounded in the frame-
work of statistical learning theory (VC theory). 
The goal of a regression algorithm is to fit a flat 
function to the given training data points. 
Formally, given a set of training data points 
D={(xi,yi)| i=1,2,?,n} ? Rd?R,  where xi is input 
feature vector and yi is associated score, the goal 
is to fit a function f which approximates the rela-
tion inherited between the data set points. The 
standard form is:  
??
==
++
n
i
i
n
i
i
T
bw
CCww
1
*
1,,, 2
1  min
*
??
??
 
Subject to 
iii
T ybxfw ?? +??+)(  
*)( ii
T
i bxfwy ?? +???  
.,...,1  ,0,, * niii =????  
  The constant C>0 is a parameter for determin-
ing the trade-off between the flatness of f and the 
amount up to which deviations larger than ? are 
tolerated.   
  In the experiments, we use the LIBSVM tool 
(Chang and Lin, 2001) with the RBF kernel for 
the task, and we use the parameter selection tool 
of 10-fold cross validation via grid search to find 
the best parameters on the training set with re-
spect to mean squared error (MSE), and then use 
the best parameters to train on the whole training 
set.   
  We use the following two groups of features 
for each sentence: the first group includes several 
basic features, and the second group includes 
several parse based features2. They are all de-
rived based on the source English sentence.  
  The basic features are as follows: 
1) Sentence length:  It refers to the number of 
words in the sentence.   
2) Sub-sentence number: It refers to the num-
ber of sub-sentences in the sentence. We 
simply use the punctuation marks as indica-
tors of sub-sentences. 
3) Average sub-sentence length: It refers to 
the average number of words in the sub-
sentences within the sentence.   
4) Percentage of nouns and adjectives: It re-
fers to the percentage of noun words or ad-
jective words in the in the sentence. 
5) Number of question words: It refers to the 
number of question words (who, whom, 
whose, when, where, which, how, why, what) 
in the sentence. 
  We use the Stanford Lexicalized Parser (Klein 
and Manning, 2002) with the provided English 
PCFG model to parse a sentence into a parse tree. 
The output tree is a context-free phrase structure 
grammar representation of the sentence. The 
parse features are then selected as follows: 
1) Depth of the parse tree:  It refers to the 
depth of the generated parse tree.  
2) Number of SBARs in the parse tree:  
SBAR is defined as a clause introduced by a 
(possibly empty) subordinating conjunction. 
It is an indictor of sentence complexity.  
                                                 
2  Other features, including n-gram frequency, perplexity 
features, etc., are not useful in our study. MT features are 
not used because Google Translate is used as a black box.  
920
3) Number of NPs in the parse tree:  It refers 
to the number of noun phrases in the parse 
tree.   
4) Number of VPs in the parse tree:  It refers 
to the number of verb phrases in the parse 
tree.    
  All the above feature values are scaled by us-
ing the provided svm-scale program.   
At this step, each English sentence si can be 
associated with a MT quality score TransScore(si) 
predicted by the ?-SVR method. The score is fi-
nally normalized by dividing by the maximum 
score. 
4.2 Evaluation  
4.2.1 Evaluation Setup 
In the experiments, we first constructed the gold-
standard dataset in the following way:  
DUC2001 provided 309 English news articles 
for document summarization tasks, and the arti-
cles were grouped into 30 document sets. The 
news articles were selected from TREC-9. We 
chose five document sets (d04, d05, d06, d08, 
d11) with 54 news articles out of the DUC2001 
document sets. The documents were then split 
into sentences and we used 1736 sentences for 
evaluation. All the sentences were automatically 
translated into Chinese sentences by using the 
Google Translate service. 
Two Chinese college students were employed 
for data annotation. They read the original Eng-
lish sentence and the translated Chinese sentence, 
and then manually labeled the overall translation 
quality score for each sentence, separately. The 
translation quality is an overall measure for both 
the translation accuracy and the readability of the 
translated sentence.  The score ranges between 1 
and 5, and 1 means ?very bad?, and 5 means 
?very good?, and 3 means ?normal?. The correla-
tion between the two sets of labeled scores is 
0.646. The final translation quality score was the 
average of the scores provided by the two anno-
tators.  
After annotation, we randomly separated the 
labeled sentence set into a training set of 1428 
sentences and a test set of 308 sentences. We 
then used the LIBSVM tool for training and test-
ing. 
Two metrics were used for evaluating the pre-
diction results. The two metrics are as follows: 
Mean Square Error (MSE): This metric is a 
measure of how correct each of the prediction 
values is on average, penalizing more severe er-
rors more heavily. Given the set of prediction 
scores for the test sentences: },...1|?{? niyY i == , and 
the manually assigned scores for the sentences: 
},...1|{ niyY i == , the MSE of the prediction result 
is defined as  
?
=
?=
n
i
ii yyn
YMSE
1
2)?(1)?(  
Pearson?s Correlation Coefficient (?):  This 
metric is a measure of whether the trends of pre-
diction values matched the trends for human-
labeled data. The coefficient between Y and Y?  is 
defined as  
yy
n
i
ii
sns
yyyy
?
1
)??)((?
=
??
=?  
where y and y?  are the sample means of Y and 
Y? , ys and ys ? are the sample standard deviations 
of Y and Y? . 
4.2.2 Evaluation Results 
Table 1 shows the prediction results. We can see 
that the overall results are promising. And the 
correlation is moderately high. The results are 
acceptable because we only make use of the fea-
tures derived from the source sentence. The re-
sults guarantee that the use of MT quality scores 
in the summarization process is feasible.  
We can also see that both the basic features 
and the parse features are beneficial to the over-
all prediction results.   
  
Feature Set MSE ? 
Basic features 0.709 0.399 
Parse features 0.702 0.395 
All features 0.683 0.433 
Table 1: Prediction results 
5 Cross-Language Document Summari-
zation  
5.1 Methodology 
In this section, we first compute the informative-
ness score for each sentence. The score reflect 
how the sentence expresses the major topic in the 
documents. Various existing methods can be 
used for computing the score. In this study, we 
adopt the centroid-based method. 
The centroid-based method is the algorithm 
used in the MEAD system. The method uses a 
heuristic and simple way to sum the sentence 
scores computed based on different features. The 
score for each sentence is a linear combination of 
921
the weights computed based on the following 
three features: 
  Centroid-based Weight. The sentences close 
to the centroid of the document set are usually 
more important than the sentences farther away. 
And the centroid weight C(si) of a sentence si is 
calculated as the cosine similarity  between the 
sentence text and the concatenated text for the 
whole document set D. The weight is then nor-
malized by dividing the maximal weight. 
  Sentence Position. The leading several sen-
tences of a document are usually important. So 
we calculate for each sentence a weight to reflect 
its position priority as P(si)=1-(i-1)/n, where i is 
the sequence of the sentence si and n is the total 
number of sentences in the document. Obviously, 
i ranges from 1 to n.  
  First Sentence Similarity. Because the first 
sentence of a document is very important, a sen-
tence similar to the first sentence is also impor-
tant. Thus we use the cosine similarity value be-
tween a sentence and the corresponding first sen-
tence in the same document as the weight F(si) 
for sentence si. 
  After all the above weights are calculated for 
each sentence, we sum all the weights and get the 
overall score for the sentence as follows: 
)()()()( iiii sFsPsCsInfoScore ?+?+?= ???  
where ?, ? and ? are parameters reflecting the 
importance of different features. We empirically 
set ?=?=?=1.  
  After the informativeness scores for all sen-
tences are computed, the score of each sentence 
is normalized by dividing by the maximum score.  
After we obtain the MT quality score and the 
informativeness score of each sentence in the 
document set, we linearly combine the two 
scores to get the overall score of each sentence.  
Formally, let TransScore(si)?[0,1] and Info-
Score(si)?[0,1] denote the MT quality score and 
the informativeness score of sentence si, the 
overall score of the sentence is: 
where ??[0,1] is a parameter controlling the 
influences of the two factors. If ? is set to 0, the 
summary is extracted without considering the 
MT quality factor. In the experiments, we em-
pirically set the parameter to 0.3 in order to bal-
ance the two factors of content informativeness 
and translation quality.   
For multi-document summarization, some sen-
tences are highly overlapping with each other, 
and thus we apply the same greedy algorithm in 
(Wan et al, 2006) to penalize the sentences 
highly overlapping with other highly scored sen-
tences, and finally the informative, novel, and 
easy-to-translate sentences are chosen into the 
English summary. 
  Finally, the sentences in the English summary 
are translated into the corresponding Chinese 
sentences by using Google Translate, and the 
Chinese summary is formed.   
5.2 Evaluation 
5.2.1 Evaluation Setup 
In this experiment, we used the document sets 
provided by DUC2001 for evaluation. As men-
tioned in Section 4.2.1, DUC2001 provided 30 
English document sets for generic multi-
document summarization. The average document 
number per document set was 10. The sentences 
in each article have been separated and the sen-
tence information has been stored into files. Ge-
neric reference English summaries were pro-
vided by NIST annotators for evaluation. In our 
study, we aimed to produce Chinese summaries 
for the English document sets. The summary 
length was limited to five sentences, i.e. each 
summary consisted of five sentences. 
The DUC2001 dataset was divided into the 
following two datasets:  
Ideal Dataset: We have manually labeled the 
MT quality scores for the sentences in five 
document sets (d04-d11), and we directly used 
the manually labeled scores in the summarization 
process. The ideal dataset contained these five 
document sets. 
Real Dataset: The MT quality scores for the 
sentences in the remaining 25 document sets 
were automatically predicted by using the 
learned SVM regression model. And we used the 
automatically predicted scores in the summariza-
tion process. The real dataset contained these 25 
document sets. 
  We performed two evaluation procedures: one 
based on the ideal dataset to validate the 
feasibility of the proposed approach, and 
the other based on the real dataset to 
demonstrate the effectiveness of the proposed 
approach in real applications.  
To date, various methods and metrics have 
been developed for English summary evaluation 
by comparing system summary with reference 
summary, such as the pyramid method (Nenkova 
et al, 2007) and the ROUGE metrics (Lin and 
Hovy, 2003). However, such methods or metrics 
cannot be directly used for evaluating Chinese 
summary without reference Chinese summary.  
)()()1()( iii sTransScoresInfoScoresreOverallSco ?+??= ??
922
Instead, we developed an evaluation protocol as 
follows: 
The evaluation was based on human scoring. 
Four Chinese college students participated in the 
evaluation as subjects. We have developed a 
friendly tool for helping the subjects to evaluate 
each Chinese summary from the following three 
aspects: 
Content: This aspect indicates how much a 
summary reflects the major content of the docu-
ment set. After reading a summary, each user can 
select a score between 1 and 5 for the summary. 
1 means ?very uninformative? and 5 means 
?very informative?. 
Readability:  This aspect indicates the read-
ability level of the whole summary. After reading 
a summary, each user can select a score between 
1 and 5 for the summary. 1 means ?hard to read?, 
and 5 means ?easy to read?. 
Overall:  This aspect indicates the overall 
quality of a summary. After reading a summary, 
each user can select a score between 1 and 5 for 
the summary. 1 means ?very bad?, and 5 means 
?very good?. 
We performed the evaluation procedures on 
the ideal dataset and the read dataset, separately. 
During each evaluation procedure, we compared 
our proposed approach (?=0.3) with the baseline 
approach without considering the MT quality 
factor (?=0). And the two summaries produced 
by the two systems for the same document set 
were presented in the same interface, and then 
the four subjects assigned scores to each sum-
mary after they read and compared the two 
summaries.  And the assigned scores were finally 
averaged across the documents sets and across 
the subjects.  
5.2.2 Evaluation Results 
Table 2 shows the evaluation results on the ideal 
dataset with 5 document sets. We can see that 
based on the manually labeled MT quality scores, 
the Chinese summaries produced by our pro-
posed approach are significantly better than that 
produced by the baseline approach over all three 
aspects. All subjects agree that our proposed ap-
proach can produce more informative and easy-
to-read Chinese summaries than the baseline ap-
proach.   
Table 3 shows the evaluation results on the 
real dataset with 25 document sets. We can see 
that based on the automatically predicted MT 
quality scores, the Chinese summaries produced 
by our proposed approach are significantly better 
than that produced by the baseline approach over 
the readability aspect and the overall aspect. Al-
most all subjects agree that our proposed ap-
proach can produce more easy-to-read and high-
quality Chinese summaries than the baseline ap-
proach.   
Comparing the evaluation results in the two 
tables, we can find that the performance differ-
ence between the two approaches on the ideal 
dataset is bigger than that on the real dataset, es-
pecially on the content aspect. The results dem-
onstrate that the more accurate the MT quality 
scores are, the more significant the performance 
improvement is.  
   Overall, the proposed approach is effective to 
produce good-quality Chinese summaries for 
English document sets. 
 
 Baseline Approach Proposed Approach 
 content readability overall content readability overall 
Subject1 3.2 2.6 2.8 3.4 3.0 3.4 
Subject2 3.0 3.2 3.2 3.4 3.6 3.4 
Subject3 3.4 2.8 3.2 3.6 3.8 3.8 
Subject4 3.2 3.0 3.2 3.8 3.8 3.8 
Average 3.2 2.9 3.1 3.55* 3.55* 3.6* 
Table 2: Evaluation results on the ideal dataset (5 document sets) 
 Baseline Approach Proposed Approach 
 content readability overall content readability overall 
Subject1 2.64 2.56 2.60 2.80 3.24 2.96 
Subject2 3.60 2.76 3.36 3.52 3.28 3.64 
Subject3 3.52 3.72 3.44 3.56 3.80 3.48 
Subject4 3.16 2.96 3.12 3.16 3.44 3.52 
Average 3.23 3.00 3.13 3.26 3.44* 3.40* 
Table 3: Evaluation results on the real dataset (25 document sets) 
(* indicates the difference between the average score of the proposed approach and that of the baseline approach 
is statistically significant by using t-test.) 
923
 5.2.3 Example Analysis 
In this section, we give two running examples to 
better show the effectiveness of our proposed 
approach. The Chinese sentences and the original 
English sentences in the summary are presented 
together. The normalized MT quality score for 
each sentence is also given at the end of the Chi-
nese sentence.  
 
Document set 1: D04 from the ideal dataset 
Summary by baseline approach: 
s1: ?????????????????????73???
?37???????????????-?????????
????????(0.56) 
(US INSURERS expect to pay out an estimated Dollars 7.3bn 
(Pounds 3.7bn) in Florida as a result of Hurricane Andrew - by far 
the costliest disaster the industry has ever faced. ) 
s2: ?????????????????????????
???????????????????????????
?????????(0.67) 
(THERE are growing signs that Hurricane Andrew, unwelcome as 
it was for the devastated inhabitants of Florida and Louisiana, may 
in the end do no harm to the re-election campaign of President 
George Bush.) 
s3: ?????????????????????????
???????????????4000???&#39;? (0.44) 
(GENERAL ACCIDENT said yesterday that insurance claims 
arising from Hurricane Andrew could 'cost it as much as Dollars 
40m'.) 
s4: ???????????????4??????????
??? (0.56) 
(In the Bahamas, government spokesman Mr Jimmy Curry said 
four deaths had been reported on outlying eastern islands.) 
s5: ??????1.6??????????????????
???????????????????????????
???(0.44) 
(New Orleans, with a population of 1.6m, is particularly vulnerable 
because the city lies below sea level, has the Mississippi River 
running through its centre and a large lake immediately to the north.) 
 
Summary by proposed approach: 
s1: ?????????????????????73???
?37???????????????-?????????
????????(0.56) 
(US INSURERS expect to pay out an estimated Dollars 7.3bn 
(Pounds 3.7bn) in Florida as a result of Hurricane Andrew - by far 
the costliest disaster the industry has ever faced.) 
s2: ?????????????????????????
???????????????????????????
?????????(0.67) 
(THERE are growing signs that Hurricane Andrew, unwelcome as 
it was for the devastated inhabitants of Florida and Louisiana, may 
in the end do no harm to the re-election campaign of President 
George Bush.) 
s3: ???????????????4??????????
???(0.56) 
(In the Bahamas, government spokesman Mr Jimmy Curry said 
four deaths had been reported on outlying eastern islands.) 
s4: ?????????????????????????
??????? (0.89) 
(The brunt of the losses are likely to be concentrated among US 
insurers, industry analysts said yesterday.) 
s5: ?????????????(1.0) 
(In north Miami, damage is minimal.) 
 
Document set 2: D54 from the real dataset 
Summary by baseline approach: 
s1: ????11?6???????????????????
???????(0.57) 
(Two propositions on California's Nov. 6 ballot would, among other 
things, limit the terms of statewide officeholders and state legisla-
tors.) 
s2: ?????????????????????????
?????????(0.36) 
(One reason is that term limits would open up politics to many 
people now excluded from office by career incumbents.) 
s3: ?????????????????????????
??????????(0.20) 
(Proposals to limit the terms of members of Congress and of state 
legislators are popular and getting more so, according to the pundits 
and the polls.) 
s4: ?????????????????????????
?????????????????(0.24) 
(State statutes that bar first-time candidates from running for Con-
gress have been held to add to the qualifications set forth in the 
Constitution and have been invalidated.) 
s5: ?????????????????????????
?????????????????????????(0.20) 
(Another argument is that a citizen Congress with its continuing 
flow of fresh faces into Washington would result in better govern-
ment than that provided by representatives with lengthy tenure.) 
Summary by proposed approach: 
s1: ???? 11? 6??????????????????
????????(0.57) 
(Two propositions on California's Nov. 6 ballot would, among other 
things, limit the terms of statewide officeholders and state legisla-
tors.) 
s2: ?????????????????????????
?????????(0.36) 
(One reason is that term limits would open up politics to many 
people now excluded from office by career incumbents.) 
s3: ?????????????????????????
?????????????????????????(0.20) 
(Another argument is that a citizen Congress with its continuing 
flow of fresh faces into Washington would result in better govern-
ment than that provided by representatives with lengthy tenure.) 
s4: ????????????????????????
?????????????(0.39) 
(There are two solid reasons for congressional term limitation that 
economists, at least those of the public-choice persuasion, should 
fully appreciate.) 
s5: ?????????????????????????
?????(0.47) 
(The root of the problems with Congress is that, barring major 
scandal, it is almost impossible to defeat an incumbent.) 
6 Discussion  
In this study, we adopt the late translation strat-
egy for cross-document summarization. As men-
tioned earlier, the late translation strategy has 
some advantages over the early translation strat-
egy. However, in the early translation strategy, 
we can use the features derived from both the 
source English sentence and the target Chinese 
sentence to improve the MT quality prediction 
results.  
Overall, the framework of our proposed ap-
proach can be easily adapted for cross-document 
summarization with the early translation strategy. 
924
And an empirical comparison between the two 
strategies is left as our future work. 
Though this study focuses on English-to-
Chinese document summarization, cross-
language summarization tasks for other lan-
guages can also be solved by using our proposed 
approach.  
7 Conclusion and Future Work  
In this study we propose a novel approach to ad-
dress the cross-language document summariza-
tion task. Our proposed approach predicts the 
MT quality score of each English sentence and 
then incorporates the score into the summariza-
tion process. The user study results verify the 
effectiveness of the approach. 
In future work, we will manually translate 
English reference summaries into Chinese refer-
ence summaries, and then adopt the ROUGE 
metrics to perform automatic evaluation of the 
extracted Chinese summaries by comparing them 
with the Chinese reference summaries. Moreover, 
we will further improve the sentence?s MT qual-
ity by using sentence compression or sentence 
reduction techniques.  
Acknowledgments 
This work was supported by NSFC (60873155), 
Beijing Nova Program (2008B03), NCET 
(NCET-08-0006), RFDP (20070001059) and 
National High-tech R&D Program 
(2008AA01Z421). We thank the students for 
participating in the user study. We also thank the 
anonymous reviewers for their useful comments. 
References  
J. Albrecht and R. Hwa. 2007. A re-examination of 
machine learning approaches for sentence-level mt 
evaluation. In Proceedings of ACL2007. 
M. R. Amini, P. Gallinari. 2002. The Use of Unla-
beled Data to Improve Supervised Learning for 
Text Summarization. In Proceedings of SIGIR2002. 
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. 
Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 
2003. Confidence estimation for statistical machine 
translation. Johns Hopkins Summer Workshop Fi-
nal Report. 
J. Chae and A. Nenkova. 2009. Predicting the fluency 
of text with shallow structural features: case studies 
of machine translation and human-written text. In 
Proceedings of EACL2009. 
G. de Chalendar, R. Besan?on, O. Ferret, G. Grefen-
stette, and O. Mesnard. 2005. Crosslingual summa-
rization with thematic extraction, syntactic sen-
tence simplification, and bilingual generation. In 
Workshop on Crossing Barriers in Text Summari-
zation Research, 5th International Conference on 
Recent Advances in Natural Language Processing  
(RANLP2005). 
C.-C. Chang and C.-J. Lin. 2001. LIBSVM : a library 
for support vector machines. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
G. ErKan, D. R. Radev. LexPageRank. 2004. Prestige 
in Multi-Document Text Summarization. In Pro-
ceedings of EMNLP2004. 
M. Gamon, A. Aue, and M. Smets. 2005. Sentence-
level MT evaluation without reference translations: 
beyond language modeling. In Proceedings of 
EAMT2005. 
D. Klein and C. D. Manning. 2002. Fast Exact Infer-
ence with a Factored Model for Natural Language 
Parsing. In Proceedings of NIPS2002. 
J. Kupiec, J. Pedersen, F. Chen. 1995. A.Trainable 
Document Summarizer. In Proceedings of 
SIGIR1995. 
A. Leuski, C.-Y. Lin, L. Zhou, U. Germann, F. J. Och, 
E. Hovy. 2003. Cross-lingual C*ST*RD: English 
access to Hindi information. ACM Transactions on 
Asian Language Information Processing, 2(3): 
245-269. 
J.-M. Lim, I.-S. Kang, J.-H. Lee. 2004. Multi-
document summarization using cross-language 
texts. In Proceedings of NTCIR-4.  
C. Y. Lin, E. Hovy. 2000. The Automated Acquisition 
of Topic Signatures for Text Summarization. In 
Proceedings of the 17th Conference on Computa-
tional Linguistics. 
C..-Y. Lin and E.. H. Hovy. 2002. From Single to 
Multi-document Summarization: A Prototype Sys-
tem and its Evaluation. In Proceedings of ACL-02. 
C.-Y. Lin and E.H. Hovy. 2003. Automatic Evalua-
tion of Summaries Using N-gram Co-occurrence 
Statistics. In Proceedings of HLT-NAACL -03. 
C.-Y. Lin, L. Zhou, and E. Hovy. 2005. Multilingual 
summarization evaluation 2005: automatic evalua-
tion report. In Proceedings of MSE (ACL-2005 
Workshop). 
H. P. Luhn. 1969. The Automatic Creation of litera-
ture Abstracts. IBM Journal of Research and De-
velopment, 2(2). 
R. Mihalcea, P. Tarau. 2004. TextRank: Bringing 
Order into Texts. In Proceedings of EMNLP2004. 
R. Mihalcea and P. Tarau. 2005. A language inde-
pendent algorithm for single and multiple docu-
ment summarization. In Proceedings of IJCNLP-05. 
A. Nenkova and A. Louis. 2008. Can you summarize 
this? Identifying correlates of input difficulty for 
generic multi-document summarization. In Pro-
ceedings of ACL-08:HLT. 
A. Nenkova, R. Passonneau, and K. McKeown. 2007. 
The Pyramid method: incorporating human content 
selection variation in summarization evaluation. 
925
ACM Transactions on Speech and Language Proc-
essing (TSLP), 4(2). 
C. Orasan, and O. A. Chiorean. 2008. Evaluation of a 
Crosslingual Romanian-English Multi-document 
Summariser. In Proceedings of 6th Language Re-
sources and Evaluation Conference (LREC2008). 
P. Pingali, J. Jagarlamudi and V. Varma. 2007. Ex-
periments in cross language query focused multi-
document summarization. In Workshop on Cross 
Lingual Information Access Addressing the Infor-
mation Need of Multilingual Societies in 
IJCAI2007. 
C. Quirk. 2004. Training a sentence-level machine 
translation confidence measure. In Proceedings of 
LREC2004. 
D. R. Radev, H. Y. Jing, M. Stys and D. Tam. 2004. 
Centroid-based summarization of multiple docu-
ments. Information Processing and Management, 
40: 919-938. 
A. Siddharthan and K. McKeown. 2005. Improving 
multilingual summarization: using redundancy in 
the input to correct MT errors. In Proceedings of 
HLT/EMNLP-2005. 
L. Specia, Z. Wang, M. Turchi, J. Shawe-Taylor, C. 
Saunders. 2009. Improving the Confidence of Ma-
chine Translation Quality Estimates. In MT Summit 
2009 (Machine Translation Summit XII). 
V. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer. 
X. Wan, H. Li and J. Xiao. 2010. EUSUM: extracting 
easy-to-understand English summaries for non-
native readers. In Proceedings of  SIGIR2010. 
X. Wan, J. Yang and J. Xiao. 2006. Using cross-
document random walks for topic-focused multi-
documetn summarization. In Proceedings of 
WI2006. 
X. Wan and J. Yang. 2008. Multi-document summari-
zation using cluster-based link analysis. In Pro-
ceedings of SIGIR-08. 
X. Wan, J. Yang and J. Xiao. 2007. Towards an Itera-
tive Reinforcement Approach for Simultaneous 
Document Summarization and Keyword Extraction. 
In Proceedings of ACL2007.  
K.-F. Wong, M. Wu and W. Li. 2008. Extractive sum-
marization using supervised and semi-supervised 
learning. In Proceedings of COLING-08. 
H. Y. Zha. 2002. Generic Summarization and Key-
phrase Extraction Using Mutual Reinforcement 
Principle and Sentence Clustering. In Proceedings 
of SIGIR2002. 
 
926
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 467?473,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Structured Distributional Semantic Model for Event Co-reference
Kartik Goyal? Sujay Kumar Jauhar? Huiying Li?
Mrinmaya Sachan? Shashank Srivastava? Eduard Hovy
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{kartikgo,sjauhar,huiyingl,mrinmays,shashans,hovy}@cs.cmu.edu
Abstract
In this paper we present a novel ap-
proach to modelling distributional seman-
tics that represents meaning as distribu-
tions over relations in syntactic neighbor-
hoods. We argue that our model approxi-
mates meaning in compositional configu-
rations more effectively than standard dis-
tributional vectors or bag-of-words mod-
els. We test our hypothesis on the problem
of judging event coreferentiality, which in-
volves compositional interactions in the
predicate-argument structure of sentences,
and demonstrate that our model outper-
forms both state-of-the-art window-based
word embeddings as well as simple ap-
proaches to compositional semantics pre-
viously employed in the literature.
1 Introduction
Distributional Semantic Models (DSM) are popu-
lar in computational semantics. DSMs are based
on the hypothesis that the meaning of a word or
phrase can be effectively captured by the distribu-
tion of words in its neighborhood. They have been
successfully used in a variety of NLP tasks includ-
ing information retrieval (Manning et al, 2008),
question answering (Tellex et al, 2003), word-
sense discrimination (Sch?tze, 1998) and disam-
biguation (McCarthy et al, 2004), semantic sim-
ilarity computation (Wong and Raghavan, 1984;
McCarthy and Carroll, 2003) and selectional pref-
erence modeling (Erk, 2007).
A shortcoming of DSMs is that they ignore the
syntax within the context, thereby reducing the
distribution to a bag of words. Composing the
?*Equally contributing authors
distributions for ?Lincoln?, ?Booth?, and ?killed?
gives the same result regardless of whether the in-
put is ?Booth killed Lincoln? or ?Lincoln killed
Booth?. But as suggested by Pantel and Lin (2000)
and others, modeling the distribution over prefer-
ential attachments for each syntactic relation sep-
arately yields greater expressive power. Thus, to
remedy the bag-of-words failing, we extend the
generic DSM model to several relation-specific
distributions over syntactic neighborhoods. In
other words, one can think of the Structured DSM
(SDSM) representation of a word/phrase as sev-
eral vectors defined over the same vocabulary,
each vector representing the word?s selectional
preferences for its various syntactic arguments.
We argue that this representation not only cap-
tures individual word semantics more effectively
than the standard DSM, but is also better able to
express the semantics of compositional units. We
prove this on the task of judging event coreference.
Experimental results indicate that our model
achieves greater predictive accuracy on the task
than models that employ weaker forms of compo-
sition, as well as a baseline that relies on state-
of-the-art window based word embeddings. This
suggests that our formalism holds the potential of
greater expressive power in problems that involve
underlying semantic compositionality.
2 Related Work
Next, we relate and contrast our work to prior re-
search in the fields of Distributional Vector Space
Models, Semantic Compositionality and Event
Co-reference Resolution.
2.1 DSMs and Compositionality
The underlying idea that ?a word is characterized
by the company it keeps? was expressed by Firth
467
(1957). Several works have defined approaches to
modelling context-word distributions anchored on
a target word, topic, or sentence position. Collec-
tively these approaches are called Distributional
Semantic Models (DSMs).
While DSMs have been very successful on a va-
riety of tasks, they are not an effective model of
semantics as they lack properties such as compo-
sitionality or the ability to handle operators such
as negation. In order to model a stronger form of
semantics, there has been a recent surge in stud-
ies that phrase the problem of DSM composition-
ality as one of vector composition. These tech-
niques derive the meaning of the combination of
two words a and b by a single vector c = f(a, b).
Mitchell and Lapata (2008) propose a framework
to define the composition c = f(a, b, r,K) where
r is the relation between a and b, and K is the
additional knowledge used to define composition.
While this framework is quite general, the actual
models considered in the literature tend to disre-
gard K and r and mostly perform component-wise
addition and multiplication, with slight variations,
of the two vectors. To the best of our knowledge
the formulation of composition we propose is the
first to account for both K and r within this com-
positional framework.
Dinu and Lapata (2010) and S?aghdha and Ko-
rhonen (2011) introduced a probabilistic model
to represent word meanings by a latent variable
model. Subsequently, other high-dimensional ex-
tensions by Rudolph and Giesbrecht (2010), Ba-
roni and Zamparelli (2010) and Grefenstette et
al. (2011), regression models by Guevara (2010),
and recursive neural network based solutions by
Socher et al (2012) and Collobert et al (2011)
have been proposed. However, these models do
not efficiently account for structure.
Pantel and Lin (2000) and Erk and Pad? (2008)
attempt to include syntactic context in distribu-
tional models. A quasi-compositional approach
was attempted in Thater et al (2010) by a com-
bination of first and second order context vectors.
But they do not explicitly construct phrase-level
meaning from words which limits their applicabil-
ity to real world problems. Furthermore, we also
include structure into our method of composition.
Prior work in structure aware methods to the best
of our knowledge are (Weisman et al, 2012) and
(Baroni and Lenci, 2010). However, these meth-
ods do not explicitly model composition.
2.2 Event Co-reference Resolution
While automated resolution of entity coreference
has been an actively researched area (Haghighi
and Klein, 2009; Stoyanov et al, 2009; Raghu-
nathan et al, 2010), there has been relatively lit-
tle work on event coreference resolution. Lee
et al (2012) perform joint cross-document entity
and event coreference resolution using the two-
way feedback between events and their arguments.
We, on the other hand, attempt a slightly different
problem of making co-referentiality judgements
on event-coreference candidate pairs.
3 Structured Distributional Semantics
In this paper, we propose an approach to incorpo-
rate structure into distributional semantics (more
details in Goyal et al (2013)). The word distribu-
tions drawn from the context defined by a set of
relations anchored on the target word (or phrase)
form a set of vectors, namely a matrix for the tar-
get word. One axis of the matrix runs over all
the relations and the other axis is over the distri-
butional word vocabulary. The cells store word
counts (or PMI scores, or other measures of word
association). Note that collapsing the rows of the
matrix provides the standard dependency based
distributional representation.
3.1 Building Representation: The PropStore
To build a lexicon of SDSM matrices for a given
vocabulary we first construct a proposition knowl-
edge base (the PropStore) created by parsing the
Simple English Wikipedia. Dependency arcs are
stored as 3-tuples of the form ?w1, r, w2?, denot-
ing an occurrence of words w1, word w2 related
by r. We also store sentence indices for triples
as this allows us to achieve an intuitive technique
to achieve compositionality. In addition to the
words? surface-forms, the PropStore also stores
their POS tags, lemmas, and Wordnet supersenses.
This helps to generalize our representation when
surface-form distributions are sparse.
The PropStore can be used to query for the ex-
pectations of words, supersenses, relations, etc.,
around a given word. In the example in Figure 1,
the query (SST(W1) = verb.consumption, ?, dobj)
i.e. ?what is consumed? might return expectations
[pasta:1, spaghetti:1, mice:1 . . . ]. Relations and
POS tags are obtained using a dependency parser
Tratz and Hovy (2011), supersense tags using sst-
light Ciaramita and Altun (2006), and lemmas us-
468
Figure 1: Sample sentences & triples
ing Wordnet Fellbaum (1998).
3.2 Mimicking Compositionality
For representing intermediate multi-word phrases,
we extend the above word-relation matrix symbol-
ism in a bottom-up fashion using the PropStore.
The combination hinges on the intuition that when
lexical units combine to form a larger syntactically
connected phrase, the representation of the phrase
is given by its own distributional neighborhood
within the embedded parse tree. The distributional
neighborhood of the net phrase can be computed
using the PropStore given syntactic relations an-
chored on its parts. For the example in Figure
1, we can compose SST(w1) = Noun.person and
Lemma(W1) = eat appearing together with a nsubj
relation to obtain expectations around ?people eat?
yielding [pasta:1, spaghetti:1 . . . ] for the object
relation, [room:2, restaurant:1 . . .] for the location
relation, etc. Larger phrasal queries can be built to
answer queries like ?What do people in China eat
with??, ?What do cows do??, etc. All of this helps
us to account for both relation r and knowledge K
obtained from the PropStore within the composi-
tional framework c = f(a, b, r,K).
The general outline to obtain a composition
of two words is given in Algorithm 1, which
returns the distributional expectation around the
composed unit. Note that the entire algorithm can
conveniently be written in the form of database
queries to our PropStore.
Algorithm 1 ComposePair(w1, r, w2)
M1 ? queryMatrix(w1) (1)
M2 ? queryMatrix(w2) (2)
SentIDs?M1(r) ?M2(r) (3)
return ((M1? SentIDs) ? (M2? SentIDs)) (4)
For the example ?noun.person nsubj eat?, steps
(1) and (2) involve querying the PropStore for the
individual tokens, noun.person and eat. Let the re-
sulting matrices be M1 and M2, respectively. In
step (3), SentIDs (sentences where the two words
appear with the specified relation) are obtained by
taking the intersection between the nsubj compo-
nent vectors of the two matrices M1 and M2. In
step (4), the entries of the original matrices M1
and M2 are intersected with this list of common
SentIDs. Finally, the resulting matrix for the com-
position of the two words is simply the union of
all the relationwise intersected sentence IDs. Intu-
itively, through this procedure, we have computed
the expectation around the words w1 and w2 when
they are connected by the relation ?r?.
Similar to the two-word composition process,
given a parse subtree T of a phrase, we obtain
its matrix representation of empirical counts over
word-relation contexts (described in Algorithm 2).
Let the E = {e1 . . . en} be the set of edges in T ,
ei = (wi1, ri, wi2)?i = 1 . . . n.
Algorithm 2 ComposePhrase(T )
SentIDs? All Sentences in corpus
for i = 1? n do
Mi1 ? queryMatrix(wi1)
Mi2 ? queryMatrix(wi2)
SentIDs? SentIDs ?(M1(ri) ?M2(ri))
end for
return ((M11? SentIDs) ? (M12? SentIDs)
? ? ? ? (Mn1? SentIDs) ? (Mn2? SentIDs))
The phrase representations becomes sparser as
phrase length increases. For this study, we restrict
phrasal query length to a maximum of three words.
3.3 Event Coreferentiality
Given the SDSM formulation and assuming no
sparsity constraints, it is possible to calculate
469
SDSM matrices for composed concepts. However,
are these correct? Intuitively, if they truly capture
semantics, the two SDSM matrix representations
for ?Booth assassinated Lincoln? and ?Booth shot
Lincoln with a gun" should be (almost) the same.
To test this hypothesis we turn to the task of pre-
dicting whether two event mentions are coreferent
or not, even if their surface forms differ. It may be
noted that this task is different from the task of full
event coreference and hence is not directly compa-
rable to previous experimental results in the liter-
ature. Two mentions generally refer to the same
event when their respective actions, agents, pa-
tients, locations, and times are (almost) the same.
Given the non-compositional nature of determin-
ing equality of locations and times, we represent
each event mention by a triple E = (e, a, p) for
the event, agent, and patient.
In our corpus, most event mentions are verbs.
However, when nominalized events are encoun-
tered, we replace them by their verbal forms. We
use SRL Collobert et al (2011) to determine the
agent and patient arguments of an event mention.
When SRL fails to determine either role, its empir-
ical substitutes are obtained by querying the Prop-
Store for the most likely word expectations for
the role. It may be noted that the SDSM repre-
sentation relies on syntactic dependancy relations.
Hence, to bridge the gap between these relations
and the composition of semantic role participants
of event mentions we empirically determine those
syntactic relations which most strongly co-occur
with the semantic relations connecting events,
agents and patients. The triple (e, a, p) is thus the
composition of the triples (a, relationsetagent, e)
and (p, relationsetpatient, e), and hence a com-
plex object. To determine equality of this complex
composed representation we generate three levels
of progressively simplified event constituents for
comparison:
Level 1: Full Composition:
Mfull = ComposePhrase(e, a, p).
Level 2: Partial Composition:
Mpart:EA = ComposePair(e, r, a)
Mpart:EP = ComposePair(e, r, p).
Level 3: No Composition:
ME = queryMatrix(e)
MA = queryMatrix(a)
MP = queryMatrix(p)
To judge coreference between
events E1 and E2, we compute pair-
wise similarities Sim(M1full,M2full),
Sim(M1part:EA,M2part:EA), etc., for each
level of the composed triple representation. Fur-
thermore, we vary the computation of similarity
by considering different levels of granularity
(lemma, SST), various choices of distance
metric (Euclidean, Cityblock, Cosine), and
score normalization techniques (Row-wise, Full,
Column-collapsed). This results in 159 similarity-
based features for every pair of events, which are
used to train a classifier to decide conference.
4 Experiments
We evaluate our method on two datasets and com-
pare it against four baselines, two of which use
window based distributional vectors and two that
employ weaker forms of composition.
4.1 Datasets
IC Event Coreference Corpus: The dataset
(Hovy et al, 2013), drawn from 100 news articles
about violent events, contains manually created
annotations for 2214 pairs of co-referent and non-
coreferent events each. Where available, events?
semantic role-fillers for agent and patient are an-
notated as well. When missing, empirical substi-
tutes were obtained by querying the PropStore for
the preferred word attachments.
EventCorefBank (ECB) corpus: This corpus
(Bejan and Harabagiu, 2010) of 482 documents
from Google News is clustered into 45 topics,
with event coreference chains annotated over each
topic. The event mentions are enriched with se-
mantic roles to obtain the canonical event struc-
ture described above. Positive instances are ob-
tained by taking pairwise event mentions within
each chain, and negative instances are generated
from pairwise event mentions across chains, but
within the same topic. This results in 11039 posi-
tive instances and 33459 negative instances.
4.2 Baselines
To establish the efficacy of our model, we compare
SDSM against a purely window-based baseline
(DSM) trained on the same corpus. In our exper-
iments we set a window size of seven words. We
also compare SDSM against the window-based
embeddings trained using a recursive neural net-
work (SENNA) (Collobert et al, 2011) on both
datsets. SENNA embeddings are state-of-the-art
for many NLP tasks. The second baseline uses
470
IC Corpus ECB Corpus
Prec Rec F-1 Acc Prec Rec F-1 Acc
SDSM 0.916 0.929 0.922 0.906 0.901 0.401 0.564 0.843
Senna 0.850 0.881 0.865 0.835 0.616 0.408 0.505 0.791
DSM 0.743 0.843 0.790 0.740 0.854 0.378 0.524 0.830
MVC 0.756 0.961 0.846 0.787 0.914 0.353 0.510 0.831
AVC 0.753 0.941 0.837 0.777 0.901 0.373 0.528 0.834
Table 1: Cross-validation Performance on IC and ECB dataset
SENNA to generate level 3 similarity features for
events? individual words (agent, patient and ac-
tion). As our final set of baselines, we extend two
simple techniques proposed by (Mitchell and Lap-
ata, 2008) that use element-wise addition and mul-
tiplication operators to perform composition. We
extend it to our matrix representation and build
two baselines AVC (element-wise addition) and
MVC (element-wise multiplication).
4.3 Discussion
Among common classifiers, decision-trees (J48)
yielded best results in our experiments. Table 1
summarizes our results on both datasets.
The results reveal that the SDSM model con-
sistently outperforms DSM, SENNA embeddings,
and the MVC and AVC models, both in terms
of F-1 score and accuracy. The IC corpus com-
prises of domain specific texts, resulting in high
lexical overlap between event mentions. Hence,
the scores on the IC corpus are consistently higher
than those on the ECB corpus.
The improvements over DSM and SENNA em-
beddings, support our hypothesis that syntax lends
greater expressive power to distributional seman-
tics in compositional configurations. Furthermore,
the increase in predictive accuracy over MVC and
AVC shows that our formulation of composition
of two words based on the relation binding them
yields a stronger form of compositionality than
simple additive and multiplicative models.
Next, we perform an ablation study to deter-
mine the most predictive features for the task of
event coreferentiality. The forward selection pro-
cedure reveals that the most informative attributes
are the level 2 compositional features involving
the agent and the action, as well as their individ-
ual level 3 features. This corresponds to the in-
tuition that the agent and the action are the prin-
cipal determiners for identifying events. Features
involving the patient and level 1 features are least
useful. This is probably because features involv-
ing full composition are sparse, and not as likely
to provide statistically significant evidence. This
may change as our PropStore grows in size.
5 Conclusion and Future Work
We outlined an approach that introduces structure
into distributed semantic representations gives us
an ability to compare the identity of two repre-
sentations derived from supposedly semantically
identical phrases with different surface realiza-
tions. We employed the task of event coreference
to validate our representation and achieved sig-
nificantly higher predictive accuracy than several
baselines.
In the future, we would like to extend our model
to other semantic tasks such as paraphrase detec-
tion, lexical substitution and recognizing textual
entailment. We would also like to replace our syn-
tactic relations to semantic relations and explore
various ways of dimensionality reduction to solve
this problem.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their valuable comments and sugges-
tions to improve the quality of the paper. This
work was supported in part by the following
grants: NSF grant IIS-1143703, NSF award IIS-
1147810, DARPA grant FA87501220342.
References
Marco Baroni and Alessandro Lenci. 2010. Distri-
butional memory: A general framework for corpus-
based semantics. Comput. Linguist., 36(4):673?721,
December.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
471
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 594?602, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Ronan Collobert, Jason Weston, L?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 999888:2493?2537,
November.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10, pages
1162?1172, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 897?906, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
John R. Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. Studies in Linguistic Analysis, pages 1?
32.
Kartik. Goyal, Sujay Kumar Jauhar, Mrinmaya Sachan,
Shashank Srivastava, Huiying Li, and Eduard Hovy.
2013. A structured distributional semantic model
: Integrating structure with semantics. In Proceed-
ings of the 1st Continuous Vector Space Models and
their Compositionality Workshop at the conference
of ACL 2013.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distri-
butional models of meaning. In Proceedings of the
Ninth International Conference on Computational
Semantics, IWCS ?11, pages 125?134, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, GEMS ?10, pages 33?37, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ?09, pages 1152?
1161, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
E.H. Hovy, T. Mitamura, M.F. Verdejo, J. Araki, and
A. Philpot. 2013. Events are not simple: Iden-
tity, non-identity, and quasi-identity. In Proceedings
of the 1st Events Workshop at the conference of the
HLT-NAACL 2013.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 489?500, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Comput.
Linguist., 29(4):639?654, December.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, ACL ?04, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244.
Patrick Pantel and Dekang Lin. 2000. Word-for-word
glossing with contextually similar words. In Pro-
ceedings of the 1st North American chapter of the
Association for Computational Linguistics confer-
ence, NAACL 2000, pages 78?85, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
472
pages 492?501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 907?916, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hinrich Sch?tze. 1998. Automatic word sense dis-
crimination. Comput. Linguist., 24(1):97?123.
Diarmuid ? S?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1047?1057, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 1201?1211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2,
ACL ?09, pages 656?664, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In SIGIR, pages 41?47.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ?10, pages
948?957, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1257?1268, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Hila Weisman, Jonathan Berant, Idan Szpektor, and Ido
Dagan. 2012. Learning verb inference rules from
linguistically-motivated evidence. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 194?204, Stroudsburg, PA, USA. Association
for Computational Linguistics.
S. K. M. Wong and Vijay V. Raghavan. 1984. Vector
space model of information retrieval: a reevaluation.
In Proceedings of the 7th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?84, pages 167?185,
Swinton, UK. British Computer Society.
473
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 20?29,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
A Structured Distributional Semantic Model : Integrating Structure with
Semantics
Kartik Goyal? Sujay Kumar Jauhar? Huiying Li?
Mrinmaya Sachan? Shashank Srivastava? Eduard Hovy
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{kartikgo,sjauhar,huiyingl,mrinmays,shashans,hovy}@cs.cmu.edu
Abstract
In this paper we present a novel approach
(SDSM) that incorporates structure in dis-
tributional semantics. SDSM represents
meaning as relation specific distributions
over syntactic neighborhoods. We em-
pirically show that the model can effec-
tively represent the semantics of single
words and provides significant advantages
when dealing with phrasal units that in-
volve word composition. In particular, we
demonstrate that our model outperforms
both state-of-the-art window-based word
embeddings as well as simple approaches
for composing distributional semantic rep-
resentations on an artificial task of verb
sense disambiguation and a real-world ap-
plication of judging event coreference.
1 Introduction
With the advent of statistical methods for NLP,
Distributional Semantic Models (DSMs) have
emerged as powerful method for representing
word semantics. In particular, the distributional
vector formalism, which represents meaning by a
distribution over neighboring words, has gained
the most popularity.
DSMs are widely used in information re-
trieval (Manning et al, 2008), question answer-
ing (Tellex et al, 2003), semantic similarity com-
putation (Wong and Raghavan, 1984; McCarthy
and Carroll, 2003), automated dictionary building
(Curran, 2003), automated essay grading (Lan-
dauer and Dutnais, 1997), word-sense discrimina-
tion and disambiguation (McCarthy et al, 2004;
?*Equally contributing authors
Sch?tze, 1998), selectional preference model-
ing (Erk, 2007) and identification of translation
equivalents (Hjelm, 2007).
Systems that use DSMs implicitly make a bag
of words assumption: that the meaning of a phrase
can be reasonably estimated from the meaning of
its constituents. However, semantics in natural
language is a compositional phenomenon, encom-
passing interactions between syntactic structures,
and the meaning of lexical constituents. It fol-
lows that the DSM formalism lends itself poorly
to composition since it implicitly disregards syn-
tactic structure. For instance, the distributions for
?Lincoln?, ?Booth?, and ?killed? when merged
produce the same result regardless of whether the
input is ?Booth killed Lincoln? or ?Lincoln killed
Booth?. As suggested by Pantel and Lin (2000)
and others, modeling the distribution over prefer-
ential attachments for each syntactic relation sep-
arately can yield greater expressive power.
Attempts have been made to model linguistic
composition of individual word vectors (Mitchell
and Lapata, 2009), as well as remedy the inher-
ent failings of the standard distributional approach
(Erk and Pad?, 2008). The results show vary-
ing degrees of efficacy, but have largely failed to
model deeper lexical semantics or compositional
expectations of words and word combinations.
In this paper we propose an extension to the
traditional DSM model that explicitly preserves
structural information and permits the approxima-
tion of distributional expectation over dependency
relations. We extend the generic DSM model by
representing a word as distributions over relation-
specific syntactic neighborhoods. One can think
of the Structured DSM (SDSM) representation
of a word/phrase as several vectors defined over
the same vocabulary, each vector representing the
20
word?s selectional preferences for a different syn-
tactic argument. We argue that this represen-
tation captures individual word semantics effec-
tively, and is better able to express the semantics
of composed units.
The overarching theme of our framework of
evaluation is to explore the semantic space of the
SDSM. We do this by measuring its ability to dis-
criminate between varying surface forms of the
same underlying concept. We perform the follow-
ing set of experiments to evaluate its expressive
power, and conclude the following:
1. Experiments with single words on similar-
ity scoring and substitute selection: SDSM
performs at par with window-based distribu-
tional vectors.
2. Experiments with phrasal units on two-word
composition: state-of-the-art results are pro-
duced on the dataset from Mitchell and Lap-
ata (2008) in terms of correlation with human
judgment.
3. Experiments with larger structures on the
task of judging event coreferentiality: SDSM
shows superior performance over state-of-
the-art window-based word embeddings, and
simple models for composing distributional
semantic representations.
2 Related Work
Distributional Semantic Models are based on the
intuition that ?a word is characterized by the com-
pany it keeps? (Firth, 1957). While DSMs have
been very successful on a variety of NLP tasks,
they are generally considered inappropriate for
deeper semantics because they lack the ability to
model composition, modifiers or negation.
Recently, there has been a surge in studies to
model a stronger form of semantics by phrasing
the problem of DSM compositionality as one of
vector composition. These techniques derive the
meaning of the combination of two words a and
b by a single vector c = f(a, b). Mitchell and
Lapata (2008) propose a framework to define the
composition c = f(a, b, r,K) where r is the re-
lation between a and b, and K is the additional
knowledge used to define composition.
While the framework is quite general, most
models in the literature tend to disregard K and
r and are generally restricted to component-wise
addition and multiplication on the vectors to be
composed, with slight variations. Dinu and Lap-
ata (2010) and S?aghdha and Korhonen (2011) in-
troduced a probabilistic model to represent word
meanings by a latent variable model. Subse-
quently, other high-dimensional extensions by
Rudolph and Giesbrecht (2010), Baroni and Zam-
parelli (2010) and Grefenstette et al (2011), re-
gression models by Guevara (2010), and recursive
neural network based solutions by Socher et al
(2012) and Collobert et al (2011) have been pro-
posed.
Pantel and Lin (2000) and Erk and Pad? (2008)
attempted to include syntactic context in distri-
butional models. However, their approaches do
not explicitly construct phrase-level meaning from
words which limits their applicability to real world
problems. A quasi-compositional approach was
also attempted in Thater et al (2010) by a system-
atic combination of first and second order context
vectors. To the best of our knowledge the formu-
lation of composition we propose is the first to ac-
count for K and r within the general framework
of composition c = f(a, b, r,K).
3 Structured Distributional Semantics
In this section, we describe our Structured Distri-
butional Semantic framework in detail. We first
build a large knowledge base from sample english
texts and use it to represent basic lexical units.
Next, we describe a technique to obtain the repre-
sentation for larger units by composing their con-
stituents.
3.1 The PropStore
To build a lexicon of SDSM representations for
a given vocabulary we construct a proposition
knowledge base (the PropStore) by processing the
text of Simple English Wikipedia through a de-
pendency parser. Dependency arcs are stored as
3-tuples of the form ?w1, r, w2?, denoting occur-
rences of words w1 and word w2 related by the
syntactic dependency r. We also store sentence
identifiers for each triple for reasons described
later. In addition to the words? surface-forms, the
PropStore also stores their POS tags, lemmas, and
Wordnet supersenses.
The PropStore can be used to query for pre-
ferred expectations of words, supersenses, re-
lations, etc., around a given word. In the
example in Figure 1, the query (SST(W1)
21
Figure 1: Sample sentences & triples
= verb.consumption, ?, dobj) i.e., ?what is
consumed?, might return expectations [pasta:1,
spaghetti:1, mice:1 . . . ]. In our implementation,
the relations and POS tags are obtained using the
Fanseparser (Tratz and Hovy, 2011), supersense
tags using sst-light (Ciaramita and Altun, 2006),
and lemmas are obtained from Wordnet (Miller,
1995).
3.2 Building the Representation
Next, we describe a method to represent lexical
entries as structured distributional matrices using
the PropStore.
The canonical form of a concept C (word,
phrase etc.) in the SDSM framework is a matrix
MC , whose entry MCij is a list of sentence identi-
fiers obtained by querying the PropStore for con-
texts in which C appears in the syntactic neigh-
borhood of the word j linked by the dependency
relation i. As with other distributional models in
the literature, the content of a cell is the frequency
of co-occurrence of its concept and word under the
given relational constraint.
This canonical matrix form can be interpreted
in several different ways. Each interpretation is
based on a different normalization scheme.
1. Row Norm: Each row of the matrix is inter-
preted as a distribution over words that attach
to the target concept with the given depen-
dency relation.
MCij =
Mij
?jMij
?i
2. Full Norm: The entire matrix is interpreted
as a distribution over the word-relation pairs
which can attach to the target concept.
MCij =
Mij
?i,jMij
?i, j
Figure 2: Mimicking composition of two words
3. Collapsed Vector Norm: The columns of
the matrix are collapsed to form a standard
normalized distributional vector trained on
dependency relations rather than sliding win-
dows.
MCj =
?iMij
?i,jMij
?j
3.3 Mimicking Compositionality
For representing intermediate multi-word phrases,
we extend the above word-relation matrix sym-
bolism in a bottom-up fashion. The combina-
tion hinges on the intuition that when lexical units
combine to form a larger syntactically connected
phrase, the representation of the phrase is given
by its own distributional neighborhood within the
embedded parse tree. The distributional neighbor-
hood of the net phrase can be computed using the
PropStore given syntactic relations anchored on its
parts. For the example in Figure 1, we can com-
pose SST(w1) = Noun.person and Lemma(W1)
= eat with relation ?nsubj? to obtain expectations
around ?people eat? yielding [pasta:1, spaghetti:1
. . . ] for the object relation ([dining room:2, restau-
rant:1 . . .] for the location relation, etc.) (See Fig-
ure 2). Larger phrasal queries can be built to an-
swer questions like ?What do people in China eat
with??, ?What do cows do??, etc. All of this helps
22
us to account for both relation r and knowledgeK
obtained from the PropStore within the composi-
tional framework c = f(a, b, r,K).
The general outline to obtain a composition of
two words is given in Algorithm 1. Here, we
first determine the sentence indices where the two
words w1 and w2 occur with relation r. Then,
we return the expectations around the two words
within these sentences. Note that the entire algo-
rithm can conveniently be written in the form of
database queries to our PropStore.
Algorithm 1 ComposePair(w1, r, w2)
M1 ? queryMatrix(w1)
M2 ? queryMatrix(w2)
SentIDs?M1(r) ?M2(r)
return ((M1? SentIDs) ? (M2? SentIDs))
Similar to the two-word composition process,
given a parse subtree T of a phrase, we obtain
its matrix representation of empirical counts over
word-relation contexts. This procedure is de-
scribed in Algorithm 2. Let the E = {e1 . . . en}
be the set of edges in T , ei = (wi1, ri, wi2)?i =
1 . . . n.
Algorithm 2 ComposePhrase(T )
SentIDs? All Sentences in corpus
for i = 1? n do
Mi1 ? queryMatrix(wi1)
Mi2 ? queryMatrix(wi2)
SentIDs? SentIDs ?(M1(ri) ?M2(ri))
end for
return ((M11? SentIDs) ? (M12? SentIDs)
? ? ? ? (Mn1? SentIDs) ? (Mn2? SentIDs))
3.4 Tackling Sparsity
The SDSM model reflects syntactic properties of
language through preferential filler constraints.
But by distributing counts over a set of relations
the resultant SDSM representation is compara-
tively much sparser than the DSM representation
for the same word. In this section we present some
ways to address this problem.
3.4.1 Sparse Back-off
The first technique to tackle sparsity is to back
off to progressively more general levels of lin-
guistic granularity when sparse matrix represen-
tations for words or compositional units are en-
countered or when the word or unit is not in the
lexicon. For example, the composition ?Balthazar
eats? cannot be directly computed if the named en-
tity ?Balthazar? does not occur in the PropStore?s
knowledge base. In this case, a query for a su-
persense substitute ? ?Noun.person eat? ? can be
issued instead. When supersenses themselves fail
to provide numerically significant distributions for
words or word combinations, a second back-off
step involves querying for POS tags. With coarser
levels of linguistic representation, the expressive
power of the distributions becomes diluted. But
this is often necessary to handle rare words. Note
that this is an issue with DSMs too.
3.4.2 Densification
In addition to the back-off method, we also pro-
pose a secondary method for ?densifying? distri-
butions. A concept?s distribution is modified by
using words encountered in its syntactic neighbor-
hood to infer counts for other semantically similar
words. In other terms, given the matrix represen-
tation of a concept, densification seeks to popu-
late its null columns (which each represent a word-
dimension in the structured distributional context)
with values weighted by their scaled similarities to
words (or effectively word-dimensions) that actu-
ally occur in the syntactic neighborhood.
For example, suppose the word ?play? had an
?nsubj? preferential vector that contained the fol-
lowing counts: [cat:4 ; Jane:2]. One might then
populate the column for ?dog? in this vector with
a count proportional to its similarity to the word
cat (say 0.8), thus resulting in the vector [cat:4 ;
Jane:2 ; dog:3.2]. These counts could just as well
be probability values or PMI associations (suitably
normalized). In this manner, the k most similar
word-dimensions can be densified for each word
that actually occurs in a syntactic context. As with
sparse back-off, there is an inherent trade-off be-
tween the degree of densification k and the expres-
sive power of the resulting representation.
3.4.3 Dimensionality Reduction
The final method tackles the problem of sparsity
by reducing the representation to a dense low-
dimensional word embedding using singular value
decomposition (SVD). In a typical term-document
matrix, SVD finds a low-dimensional approxima-
tion of the original matrix where columns become
latent concepts while similarity structure between
rows are preserved. The PropStore, as described in
Section 3.1, is an order-3 tensor with w1, w2 and
23
rel as its three axes. We explore the following two
possibilities to perform dimensionality reduction
using SVD.
Word-word matrix SVD. In this experiment,
we preserve the axes w1 and w2 and ignore the re-
lational information. Following the SVD regime (
W = U?V T ) where ? is a square diagonal ma-
trix of k largest singular values, and U and V are
m? k and n? k matrices respectively. We adopt
matrixU as the compacted concept representation.
Tensor SVD. To remedy the relation-agnostic
nature of the word-word SVD matrix represen-
tation, we use tensor SVD (Vasilescu and Ter-
zopoulos, 2002) to preserve the structural infor-
mation. The mode-n vectors of an order-N tensor
A?RI1?I2?...?IN are the In-dimensional vectors
obtained from A by varying index in while keep-
ing other indices fixed. The matrix formed by all
the mode-n vectors is a mode-n flattening of the
tensor. To obtain the compact representations of
concepts we thus first apply mode w1 flattening
and then perform SVD on the resulting tensor.
4 Single Word Evaluation
In this section we describe experiments and re-
sults for judging the expressive power of the struc-
tured distributional representation for individual
words. We use a similarity scoring task and a lexi-
cal substitute selection task for the purpose of this
evaluation. We compare the SDSM representa-
tion to standard window-based distributional vec-
tors trained on the same corpus (Simple English
Wikipedia). We also experiment with different
normalization techniques outlined in Section 3.2,
which effectively lead to structured distributional
representations with distinct interpretations.
We experimented with various similarity met-
rics and found that the normalized cityblock dis-
tance metric provides the most stable results.
CityBlock(X,Y ) =
ArcTan(d(X,Y ))
d(X,Y )
d(X,Y ) =
1
|R|
?
r?R
d(Xr, Yr)
Results in the rest of this section are thus reported
using the normalized cityblock metric. We also
report experimental results for the two methods
of alleviating sparsity discussed in Section 3.4,
namely, densification and SVD.
4.1 Similarity Scoring
On this task, the different semantic representations
were used to compute similarity scores between
two (out of context) words. We used a dataset
from Finkelstein et al (2002) for our experiments.
It consists of 353 pairs of words along with an av-
eraged similarity score on a scale of 1.0 to 10.0
obtained from 13?16 human judges.
4.2 Lexical Substitute Selection
In the second task, the same set of semantic repre-
sentations was used to produce a similarity rank-
ing on the Turney (2002) ESL dataset. This dataset
comprises 50 words that appear in a context (we
discarded the context in this experiment), along
with 4 candidate lexical substitutions. We eval-
uate the semantic representations on the basis of
their ability to discriminate the top-ranked candi-
date.1
4.3 Results and Discussion
Table 1 summarizes the results for the window-
based baseline and each of the structured distri-
butional representations on both tasks. It shows
that our representations for single words are com-
petitive with window based distributional vectors.
Densification in certain conditions improves our
results, but no consistent pattern is discernible.
This can be attributed to the trade-off between the
gain from generalization and the noise introduced
by semantic drift.
Hence we resort to dimensionality reduction as
an additional method of reducing sparsity. Table
2 gives correlation scores on the Finkelstein et al
(2002) dataset when SVD is performed on the rep-
resentations, as described in Section 3.4.3. We
give results when 100 and 500 principal compo-
nents are preserved for both SVD techniques.
These experiments suggest that though afflicted
by sparsity, the proposed structured distributional
paradigm is competitive with window-based dis-
tributional vectors. In the following sections we
show that that the framework provides consid-
erably greater power for modeling composition
when dealing with units consisting of more than
one word.
1While we are aware of the standard lexical substitution
corpus from McCarthy and Navigli (2007) we chose the one
mentioned above for its basic vocabulary, lower dependence
on context, and simpler evaluation framework.
24
Model Finklestein (Corr.) ESL (% Acc.)
DSM 0.283 0.247
Collapsed 0.260 0.178
FullNorm 0.282 0.192
RowNorm 0.236 0.264
Densified RowNorm 0.259 0.267
Table 1: Single Word Evaluation
Model Correlation
matSVD100 0.207
matSVD500 0.221
tenSVD100 0.267
tenSVD500 0.315
Table 2: Finklestein: Correlation using SVD
5 Verb Sense Disambiguation using
Composition
In this section, we examine how well our model
performs composition on a pair of words. We
derive the compositional semantic representations
for word pairs from the M&L dataset (Mitchell
and Lapata, 2008) and compare our performance
with M&L?s additive and multiplicative models of
composition.
5.1 Dataset
The M&L dataset consists of polysemous intransi-
tive verb and subject pairs that co-occur at least 50
times in the BNC corpus. Additionally two land-
mark words are given for every polysemous verb,
each corresponding to one of its senses. The sub-
ject nouns provide contextual disambiguation for
the senses of the verb. For each [subject, verb,
landmark] tuple, a human assigned score on a 7-
point scale is provided, indicating the compatibil-
ity of the landmark with the reference verb-subj
pair. For example, for the pair ?gun bomb?, land-
mark ?thunder? is more similar to the verb than
landmark ?prosper?. The corpus contains 120 tu-
ples and altogether 3600 human judgments. Re-
liability of the human ratings is examined by cal-
culating inter-annotator Spearman?s ? correlation
coefficient.
5.2 Experiment procedure
For each tuple in the dataset, we derive the com-
posed word-pair matrix for the reference verb-subj
pair based on the algorithm described in Section
3.3 and query the single-word matrix for the land-
mark word. A few modifications are made to ad-
just the algorithm for the current task:
1. In our formulation, the dependency relation
needs to be specified in order to compose
a pair of words. Hence, we determine the
five most frequent relations between w1 and
w2 by querying the PropStore. We then use
the algorithm in Section 3.3 to compose the
verb-subj word pair using these relations, re-
sulting in five composed representations.
2. The word pairs in M&L corpus are ex-
tracted from a parsed version of the BNC cor-
pus, while our PropStore is built on Simple
Wikipedia texts, whose vocabulary is signif-
icantly different from that of the BNC cor-
pus. This causes null returns in our PropStore
queries, in which case we back-off to retriev-
ing results for super-sense tags of both the
words. Finally, the composed matrix and the
landmark matrix are compared against each
other by different matrix distance measures,
which results in a similarity score. For a [sub-
ject, verb, landmark] tuple, we average the
similarity scores yielded by the relations ob-
tained in 1.
The Spearman Correlation ? between our sim-
ilarity ratings and the ones assigned by human
judges is computed over all the tuples. Follow-
ing M&L?s experiments, the inter-annotator agree-
ment correlation coefficient serves an upper bound
on the task.
5.3 Results and Discussion
As in Section 4, we choose the cityblock mea-
sure as the similarity metric of choice. Table 3
shows the evaluation results for two word compo-
sition. Except for row normalization, both forms
of normalization in the structured distributional
paradigm show significant improvement over the
results reported by M&L. The results are statisti-
cally significant at p-value = 0.004 and 0.001 for
Full Norm and Collapsed Vector Norm, respec-
tively.
Model ?
M&L combined 0.19
Row Norm 0.134
Full Norm 0.289
Collapsed Vector Norm 0.259
UpperBound 0.40
Table 3: Two Word Composition Evaluation
These results validate our hypothesis that the in-
tegration of structure into distributional semantics
25
as well as our framing of word composition to-
gether outperform window-based representations
under simplistic models of composition such as
addition and multiplication. This finding is further
re-enforced in the following experiments on event
coreferentiality judgment.
6 Event Coreference Judgment
Given the SDSM formulation and assuming no
sparsity constraints, it is possible to calculate
SDSM matrices for composed concepts. However,
are these correct? Intuitively, if they truly capture
semantics, the two SDSM matrix representations
for ?Booth assassinated Lincoln? and ?Booth shot
Lincoln with a gun" should be (almost) the same.
To test this hypothesis we turn to the task of pre-
dicting whether two event mentions are coreferent
or not, even if their surface forms differ.
While automated resolution of entity coref-
erence has been an actively researched area
(Haghighi and Klein, 2009; Stoyanov et al, 2009;
Raghunathan et al, 2010), there has been rela-
tively little work on event coreference resolution.
Lee et al (2012) perform joint cross-document
entity and event coreference resolution using the
two-way feedback between events and their argu-
ments.
In this paper, however, we only consider coref-
erentiality between pairs of events. Formally,
two event mentions generally refer to the same
event when their respective actions, agents, pa-
tients, locations, and times are (almost) the same.
Given the non-compositional nature of determin-
ing equality of locations and times, we represent
each event mention by a triple E = (e, a, p) for
the event, agent, and patient.
While linguistic theory of argument realiza-
tion is a debated research area (Levin and Rap-
paport Hovav, 2005; Goldberg, 2005), it is com-
monly believed that event structure (Moens and
Steedman, 1988) centralizes on the predicate,
which governs and selects its role arguments
(Jackendoff, 1987). In the corpora we use for
our experiments, most event mentions are verbs.
However, when nominalized events are encoun-
tered, we replace them by their verbal forms. We
use SRL Collobert et al (2011) to determine the
agent and patient arguments of an event mention.
When SRL fails to determine either role, its empir-
ical substitutes are obtained by querying the Prop-
Store for the most likely word expectations for the
role. The triple (e, a, p) is thus the composition
of the triples (a, relagent, e) and (p, relpatient, e),
and hence a complex object. To determine equal-
ity of this complex composed representation we
generate three levels of progressively simplified
event constituents for comparison:
Level 1: Full Composition:
Mfull = ComposePhrase(e, a, p).
Level 2: Partial Composition:
Mpart:EA = ComposePair(e, r, a)
Mpart:EP = ComposePair(e, r, p).
Level 3: No Composition:
ME = queryMatrix(e)
MA = queryMatrix(a)
MP = queryMatrix(p).
To judge coreference between
events E1 and E2, we compute pair-
wise similarities Sim(M1full,M2full),
Sim(M1part:EA,M2part:EA), etc., for each
level of the composed triple representation. Fur-
thermore, we vary the computation of similarity
by considering different levels of granularity
(lemma, SST), various choices of distance metric
(Euclidean, Cityblock, Cosine), and score nor-
malization techniques (Row-wise, Full, Column
collapsed). This results in 159 similarity-based
features for every pair of events, which are used
to train a classifier to make a binary decision for
coreferentiality.
6.1 Datasets
We evaluate our method on two datasets and com-
pare it against four baselines, two of which use
window based distributional vectors and two that
employ weaker forms of composition.
IC Event Coreference Corpus: The dataset
(citation suppressed), drawn from 100 news arti-
cles about violent events, contains manually cre-
ated annotations for 2214 pairs of co-referent
and non-coreferent events each. Where available,
events? semantic role-fillers for agent and patient
are annotated as well. When missing, empirical
substitutes were obtained by querying the Prop-
Store for the preferred word attachments.
EventCorefBank (ECB) corpus: This corpus
(Bejan and Harabagiu, 2010) of 482 documents
from Google News is clustered into 45 topics,
with event coreference chains annotated over each
topic. The event mentions are enriched with se-
mantic roles to obtain the canonical event struc-
ture described above. Positive instances are ob-
26
IC Corpus ECB Corpus
Prec Rec F-1 Acc Prec Rec F-1 Acc
SDSM 0.916 0.929 0.922 0.906 0.901 0.401 0.564 0.843
Senna 0.850 0.881 0.865 0.835 0.616 0.408 0.505 0.791
DSM 0.743 0.843 0.790 0.740 0.854 0.378 0.524 0.830
MVC 0.756 0.961 0.846 0.787 0.914 0.353 0.510 0.831
AVC 0.753 0.941 0.837 0.777 0.901 0.373 0.528 0.834
Table 4: Cross-validation Performance on IC and ECB dataset
tained by taking pairwise event mentions within
each chain, and negative instances are generated
from pairwise event mentions across chains, but
within the same topic. This results in 11039 posi-
tive instances and 33459 negative instances.
6.2 Baselines:
To establish the efficacy of our model, we com-
pare SDSM against a purely window-based base-
line (DSM) trained on the same corpus. In our ex-
periments we set a window size of three words to
either side of the target. We also compare SDSM
against the window-based embeddings trained us-
ing a recursive neural network (SENNA) (Col-
lobert et al, 2011) on both datsets. SENNA em-
beddings are state-of-the-art for many NLP tasks.
The second baseline uses SENNA to generate
level 3 similarity features for events? individual
words (agent, patient and action). As our final
set of baselines, we extend two simple techniques
proposed by Mitchell and Lapata (2008) that use
element-wise addition and multiplication opera-
tors to perform composition. The two baselines
thus obtained are AVC (element-wise addition)
and MVC (element-wise multiplication).
6.3 Results and Discussion:
We experimented with a number of common clas-
sifiers, and selected decision-trees (J48) as they
give the best classification accuracy. Table 4 sum-
marizes our results on both datasets.
The results reveal that the SDSM model con-
sistently outperforms DSM, SENNA embeddings,
and the MVC and AVC models, both in terms
of F-1 score and accuracy. The IC corpus com-
prises of domain specific texts, resulting in high
lexical overlap between event mentions. Hence,
the scores on the IC corpus are consistently higher
than those on the ECB corpus.
The improvements over DSM and SENNA em-
beddings, support our hypothesis that syntax lends
greater expressive power to distributional seman-
tics in compositional configurations. Furthermore,
the increase in predictive accuracy over MVC and
AVC shows that our formulation of composition
of two words based on the relation binding them
yields a stronger form of composition than simple
additive and multiplicative models.
Next, we perform an ablation study to deter-
mine the most predictive features for the task of
determining event coreferentiality. The forward
selection procedure reveals that the most informa-
tive attributes are the level 2 compositional fea-
tures involving the agent and the action, as well as
their individual level 3 features. This corresponds
to the intuition that the agent and the action are the
principal determiners for identifying events. Fea-
tures involving the patient and level 1 features are
least useful. The latter involves full composition,
resulting in sparse representations and hence have
low predictive power.
7 Conclusion and Future Work
In this paper we outlined an approach that intro-
duces structure into distributional semantics. We
presented a method to compose distributional rep-
resentations of individual units into larger com-
posed structures. We tested the efficacy of our
model on several evaluation tasks. Our model?s
performance is competitive for tasks dealing with
semantic similarity of individual words, even
though it suffers from the problem of sparsity.
Additionally, it outperforms window-based ap-
proaches on tasks involving semantic composi-
tion. In future work we hope to extend this for-
malism to other semantic tasks like paraphrase de-
tection and recognizing textual entailment.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their valuable comments and sugges-
tions to improve the quality of the paper. This
work was supported in part by the following
grants: NSF grant IIS-1143703, NSF award IIS-
1147810, DARPA grant FA87501220342.
27
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 594?602, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Ronan Collobert, Jason Weston, L?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 999888:2493?2537,
November.
James Richard Curran. 2003. From distributional to
semantic similarity. Technical report.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10, pages
1162?1172, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 897?906, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The con-
cept revisited. In ACM Transactions on Information
Systems, volume 20, pages 116?131, January.
John R. Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. Studies in Linguistic Analysis, pages 1?
32.
Adele E. Goldberg. 2005. Argument Realization: Cog-
nitive Grouping and Theoretical Extensions.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distri-
butional models of meaning. In Proceedings of the
Ninth International Conference on Computational
Semantics, IWCS ?11, pages 125?134, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, GEMS ?10, pages 33?37, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ?09, pages 1152?
1161, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Hans Hjelm. 2007. Identifying cross language term
equivalents using statistical machine translation and
distributional association measures. In Proceedings
of NODALIDA, pages 97?104. Citeseer.
Ray Jackendoff. 1987. The status of thematic roles in
linguistic theory. Linguistic Inquiry, 18(3):369?411.
Thomas K Landauer and Susan T. Dutnais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
pages 211?240.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 489?500, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment Realization. Cambridge University Press.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Comput.
Linguist., 29(4):639?654, December.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task.
In Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007), Prague,
Czech Republic, pages 48?53.
28
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, ACL ?04, Stroudsburg, PA, USA. Association
for Computational Linguistics.
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39?41, Novem-
ber.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In In Proceedings
of ACL-08: HLT, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 1 - Volume
1, EMNLP ?09, pages 430?439, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational lin-
guistics, 14(2):15?28.
Patrick Pantel and Dekang Lin. 2000. Word-for-word
glossing with contextually similar words. In Pro-
ceedings of the 1st North American chapter of the
Association for Computational Linguistics confer-
ence, NAACL 2000, pages 78?85, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 492?501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 907?916, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hinrich Sch?tze. 1998. Automatic word sense dis-
crimination. Comput. Linguist., 24(1):97?123.
Diarmuid ? S?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1047?1057, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 1201?1211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2,
ACL ?09, pages 656?664, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In SIGIR, pages 41?47.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ?10, pages
948?957, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1257?1268, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Peter D. Turney. 2002. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. CoRR.
M. Alex O. Vasilescu and Demetri Terzopoulos. 2002.
Multilinear analysis of image ensembles: Tensor-
faces. In In Proceedings of the European Confer-
ence on Computer Vision, pages 447?460.
S. K. M. Wong and Vijay V. Raghavan. 1984. Vector
space model of information retrieval: a reevaluation.
In Proceedings of the 7th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?84, pages 167?185,
Swinton, UK. British Computer Society.
29
