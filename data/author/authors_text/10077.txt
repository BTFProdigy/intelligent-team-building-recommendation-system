R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 177 ? 187, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Linguistically-Motivated Grammar Extraction, 
Generalization and Adaptation 
Yu-Ming Hsieh, Duen-Chi Yang, and Keh-Jiann Chen 
Institute of Information Science, Academia Sinica, Taipei 
{morris, ydc, kchen}@iis.sinica.edu.tw 
Abstract. In order to obtain a high precision and high coverage grammar, we 
proposed a model to measure grammar coverage and designed a PCFG parser to 
measure efficiency of the grammar. To generalize grammars, a grammar binari-
zation method was proposed to increase the coverage of a probabilistic context-
free grammar. In the mean time linguistically-motivated feature constraints 
were added into grammar rules to maintain precision of the grammar. The gen-
eralized grammar increases grammar coverage from 93% to 99% and bracket-
ing F-score from 87% to 91% in parsing Chinese sentences. To cope with error 
propagations due to word segmentation and part-of-speech tagging errors, we 
also proposed a grammar blending method to adapt to such errors. The blended 
grammar can reduce about 20~30% of parsing errors due to error assignment of 
pos made by a word segmentation system.  
Keywords: Grammar Coverage, Ambiguity, Sentence Parsing, Grammar  
Extraction. 
1   Introduction 
Treebanks provide instances of phrasal structures and their statistical distributions. 
However none of treebanks provide sufficient amount of samples which cover all types 
of phrasal structures, in particular, for the languages without inflectional markers, such 
as Chinese. It results that grammars directly extracted from treebanks suffer low cover-
age rate and low precision [7]. However arbitrarily generalizing applicable rule patterns 
may cause over-generation and increase ambiguities. It may not improve parsing per-
formance [7]. Therefore a new approach of grammar binarization was proposed in this 
paper. The binarized grammars were derived from probabilistic context-free grammars 
(PCFG) by rule binarization. The approach was motivated by the linguistic fact that 
adjuncts could be arbitrarily occurred or not occurred in a phrase. The binarized gram-
mars have better coverage than the original grammars directly extracted from treebank. 
However they also suffer problems of over-generation and structure-ambiguity. Con-
temporary grammar formalisms, such as GPSG, LFG, HPSG, take phrase structure rules 
as backbone for phrase structure representation and adding feature constraints to elimi-
nate illegal or non-logical structures. In order to achieve higher coverage, the backbone 
grammar rules (syntactic grammar) are allowed to be over-generation and the feature 
constraints (semantic grammar for world knowledge) eliminate superfluous structures 
178 Y.-M. Hsieh, D.-C. Yang, and K.-J. Chen 
and increase the precision of grammar representation. Recently, probabilistic prefer-
ences for grammar rules were incorporated to resolve structure-ambiguities and had 
great improvements on parsing performances [2, 6, 10]. Regarding feature constrains, it 
was shown that contexture information of categories of neighboring nodes, mother 
nodes, or head words are useful for improving grammar precision and parsing perform-
ances [1, 2, 7, 10, 12]. However tradeoffs between grammar coverage and grammar 
precision are always inevitable. Excessive grammatical constraints will reduce grammar 
coverage and hence reduce parsing performances. On the other hand, loosely con-
strained grammars cause structure-ambiguities and also reduce parsing performances. In 
this paper, we consider grammar optimization in particular for Chinese language. Lin-
guistically-motivated feature constraints were added to the grammar rules and evaluated 
to maintain both grammar coverage and precision. In section 2, the experimental envi-
ronments were introduced. Grammar generalization and specialization methods were 
discussed in section 3. Grammars adapting to pos-tagging errors were discussed in sec-
tion 4. Conclusions and future researches were stated in the last section. 
2   Research Environments 
The complete research environment, as shown in the figure 1, comprises of the fol-
lowing five modules and functions. 
a) Word segmentation module: identify words including out-of-vocabulary word 
and provide their syntactic categories. 
b) Grammar construction module: extract and derive (perform rule generalization, 
specialization and adaptation processes) probabilistic grammars from tree-
banks. 
c) PCFG parser: parse input sentences. 
d) Evaluation module: evaluate performances of parsers and grammars. 
e) Semantic role assignment module: resolve semantic relations for constituents. 
 
Fig. 1. The system diagram of CKIP parsing environment 
 Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 179 
2.1   Grammar Extraction Module  
Grammars are extracted from Sinica Treebank [4, 5]. Sinica Treebank version 2.0 
contains 38,944 tree-structures and 230,979 words. It provides instances of phrasal 
structures and their statistical distributions. In Sinica Treebank, each sentence is anno-
tated with its syntactic structure and semantic roles for constituents in a dependency 
framework. Figure 2 is an example. 
e.g. ? ? ?? ? ?. 
 Ta  jiao  Li-si  jian  qiu. 
 ?He asked Lisi to pick up the ball.? 
Tree-structure:  
S(agent:NP(Head:Nh:?)|Head:VF:?|goal:NP(Head:Nb:??)|theme:VP(Head:VC:?| 
goal:NP(Head:Na:?))) 
Fig. 2. A sample tree-structure 
Since the Treebank cannot provide sufficient amount of samples which cover all 
types of phrasal structures, it results that grammars directly extracted from treebanks 
suffer low coverage rate [5]. Therefore grammar generalization and specialization 
processes are carried out to obtain grammars with better coverage and precision. The 
detail processes will be discussed in section 3. 
2.2   PCFG Parser and Grammar Performance Evaluation 
The probabilistic context-free parsing strategies were used as our parsing model [2, 6, 
8]. Calculating probabilities of rules from a treebank is straightforward and we use 
maximum likelihood estimation to estimate the rule probabilities, as in [2]. The parser 
adopts an Earley?s Algorithm [8]. It is a top-down left-to-right algorithm. The results 
of binary structures will be normalized into a regular phrase structures by removing 
intermediate nodes, if used grammars are binarized grammars. Grammar efficiency 
will be evaluated according to its parsing performance. 
2.3   Experiments and Performance Evaluation 
Three sets of testing data were used in our performance evaluation. Their basic statis-
tics are shown in Table 1. Each set of testing data represents easy, hard and moderate 
respectively.  
Table 1. Three sets of testing data were used in our experiments 
Testing data Sources hardness
# of short 
sentence 
(1-5 words) 
# of normal 
sentences 
(6-10 words)
# of long 
sentences 
(>11 words) 
Total 
sentences 
Sinica Balanced corpus moderate 612 385 124 1,121 
Sinorama Magazine harder 428 424 104 956 
Textbook Elementary school easy 1,159 566 25 1,750 
180 Y.-M. Hsieh, D.-C. Yang, and K.-J. Chen 
The following parser and grammar performance evaluation indicators were used in 
our experiments: 
z LP(Labeled Precision) 
parser by the labeled phrases of #
parser by the labeled phrasescorrect  of #LP =  
z LR(Labeled Recall) 
data  testing thein phrases of #
parser by the labeled phrasescorrect  of #LR =  
z LF(Labeled F-measure) 
LR  LP
2* LR * LPLF
+
=
 
z BP(Bracketed Precision) 
parser by the made brackets of pairs of #
parser by the madecorrectly  brackets of pairs of #BP =  
z BR(Bracketed Recall) 
data  testing theof standard gold  thein brackets of pairs of #
parser by the madecorrectly  brackets of pairs of #BR =  
z BF(Bracketed F-measure) 
BR  BP
2* BR * BPBF
+
=
 
Additional indicators regarding coverage of grammars?  
z RC-Type?type coverage of rules 
data  testingin  typesrule of #
rulesgrammar  anddata   testingboth in  typesrules of #Type-RC =
 
z RC-Token?token coverage of rules 
data  testingin  tokensrule of #
rulesgrammar  anddata   testingboth in  tokensrules of #Token-RC =
 
The token coverage of a set of rules is the ceiling of parsing algorithm to achieve. 
Tradeoff effects between grammar coverage and parsing F-score can be examined for 
each set of rules. 
3   Grammar Generalization and Specialization 
By using above mentioned research environment, we intend to find out most effec-
tive grammar generalization method and specialization features for Chinese lan-
guage. To extend an existing or extracted grammar, there are several different ap-
proaches. A na?ve approach is to generalize a fine-grained rule to a coarse-grained 
rule. The approach does not generate new patterns. Only the applicable patterns for 
each word were increased. However it was shown that arbitrarily increasing the 
applicable rule patterns does increase the coverage rates of grammars, but degrade 
parsing performance [5]. A better approach is to generalizing and specializing rules 
under linguistically-motivated way. 
 Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 181 
3.1   Binary Grammar Generation, Generalization, and Specialization 
The length of a phrase in Treebank is variable and usually long phrases suffer from 
low probability. Therefore most PCFG approaches adopt the binary equivalence 
grammar, such as Chomsky normal form (CNF). For instance, a grammar rule of S? 
NP Pp Adv V can be replaced by the set of equivalent rules of {S?Np R0, R0?Pp 
R1, R1?Adv V}. The binarization method proposed in our system is different from 
CNF. It generalizes the original grammar to broader coverage. For instance, the above 
rule after performing right-association binarization 1  will produce following three 
binary rules {S?Np S?, S??Pp S?, S??Adv V}. It results that constituents (adjuncts 
and arguments) can be occurred or not occurred at almost any place in the phrase. It 
partially fulfilled the linguistic fact that adjuncts in a phrase are arbitrarily occurred. 
However it also violated the fact that arguments do not arbitrarily occur. Experimental 
results of the Sinica testing data showed that the grammar token coverage increased 
from 92.8% to 99.4%, but the labeling F-score dropped from 82.43% to 82.11% [7]. 
Therefore feature constraints were added into binary rules to limit over-generation 
caused by recursively adding constituents into intermediate-phrase types, such as S? at 
above example. 
Feature attached rules will look like following: 
S?
-left:Adv-head:V? Adv V; 
S?
-left:Pp-head:V?Pp S?-left:Adv-head:V; 
The intermediated node S?
-left:Pp-head:V says that it is a partial S structure with left-
most constituent Pp and a phrasal head V. Here the leftmost feature constraints linear 
order of constituents and the head feature implies that the structure patterns are head 
word dependent. Both constraints are linguistically plausible. Another advantage of 
the feature-constraint binary grammar is that in addition to rule probability it is easy 
to implement association strength of modifier word and head word to evaluate plausi-
bility of derived structures. 
3.2   Feature Constraints for Reducing Ambiguities of Generalized Grammars 
Adding feature constraints into grammar rules attempts to increase precision of gram-
mar representation. However the side-effect is that it also reduces grammar coverage. 
Therefore grammar design is balanced between its precision and coverage. We are 
looking for a grammar with highest coverage and precision. The tradeoff depends on 
the ambiguity resolution power of adopted parser. If the ambiguity resolution power 
of adopted parser is strong and robust, the grammar coverage might be more impor-
tant than grammar precision. On the other hand a weak parser had better to use 
grammars with more feature constraints. In our experiments, we consider grammars 
suited for PCFG parsing. The follows are some of the most important linguistically-
motivated features which have been tested. 
                                                          
1
 The reason for using right-association binarization instead of left-association or head-first 
association binarization is that our parsing process is from left to right. It turns out that pars-
ing speed of right associated grammars is much faster than left-associated grammars for left-
to-right parsing. 
182 Y.-M. Hsieh, D.-C. Yang, and K.-J. Chen 
Head (Head feature): Pos of phrasal head will propagate to all intermediate nodes 
within the constituent. 
Example:S(NP(Head:Nh:?)|S?
-VF(Head:VF:?|S?-VF(NP(Head:Nb:??)| 
VP(Head:VC:?| NP(Head:Na:?))))) 
Linguistic motivations: Constrain sub-categorization frame. 
Left (Leftmost feature): The pos of the leftmost constitute will propagate one?level to 
its intermediate mother-node only. 
Example:S(NP(Head:Nh:?)|S?
-Head:VF(Head:VF:?|S?-NP(NP(Head:Nb:??)| 
VP(Head:VC:?| NP(Head:Na:?))))) 
Linguistic motivation: Constraint linear order of constituents. 
Mother (Mother-node): The pos of mother-node assigns to all daughter nodes. 
Example:S(NP
-S(Head:Nh:?)|S?(Head:VF:?|S?(NP-S(Head:Nb:??)|VP-S(Head:VC:
?| NP
-VP(Head:Na: ? ))))) 
Linguistic motivation: Constraint syntactic structures for daughter nodes. 
Head0/1 (Existence of phrasal head): If phrasal head exists in intermediate node, the 
nodes will be marked with feature 1; otherwise 0. 
Example:S(NP(Head:Nh:? )|S?
-1(Head:VF:? |S?-0(NP(Head:Nb:?? )|VP(Head:VC:
?| NP(Head:Na: ? ))))) 
Linguistic motivation: Enforce unique phrasal head in each phrase. 
Table 2. Performance evaluations for different features 
(a)Binary rules without features (b)Binary+Left 
 Sinica Snorama Textbook Sinica Sinorama Textbook 
RC-Type 95.632 94.026 94.479 95.074 93.823 94.464 
RC-Token 99.422 99.139 99.417 99.012 98.756 99.179 
LP 81.51 77.45 84.42 86.27 80.28 86.67 
LR 82.73 77.03 85.09 86.18 80.00 87.23 
LF 82.11 77.24 84.75 86.22 80.14 86.94 
BP 87.73 85.31 89.66 90.43 86.71 90.84 
BR 89.16 84.91 90.52 90.46 86.41 91.57 
BF 88.44 85.11 90.09 90.45 86.56 91.20 
(c)Binary+Head (d)Binary+Mother 
 Sinica Snorama Textbook Sinica Sinorama Textbook 
RC-Type 94.595 93.474 94.480 94.737 94.082 92.985 
RC-Token 98.919 98.740 99.215 98.919 98.628 98.857 
LP 83.68 77.96 85.52 81.87 78.00 83.77 
LR 83.75 77.83 86.10 82.83 76.95 84.58 
LF 83.71 77.90 85.81 82.35 77.47 84.17 
BP 89.49 85.29 90.17 87.85 85.44 88.47 
BR 89.59 85.15 90.91 88.84 84.66 89.57 
BF 89.54 85.22 90.54 88.34 85.05 89.01 
 Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 183 
Each set of feature constraint added grammar is tested and evaluated. Table 2 
shows the experimental results. Since all features have their own linguistic motiva-
tions, the result feature constrained grammars maintain high coverage and have im-
proving grammar precision. Therefore each feature more or less improves the parsing 
performance and the feature of leftmost daughter node, which constrains the linear 
order of constituents, is the most effective feature. The Left-constraint-added gram-
mar reduces grammar token-coverage very little and significantly increases label and 
bracket f-scores. 
It is shown that all linguistically-motivated features are more or less effective. The 
leftmost constitute feature, which constraints linear order of constituents, is the most 
effective feature. The mother-node feature is the least effective feature, since syntactic 
structures do not vary too much for each phrase type while playing different gram-
matical functions in Chinese. 
Table 3. Performances of grammars with different feature combinations 
(a) Binary+Left+Head1/0 (b) Binary+Left+Head 
 Sinica Sinorama Textbook Sinica Sinorama Textbook 
RC-Type 94.887 93.745 94.381 92.879 91.853 92.324 
RC-Token 98.975 98.740 99.167 98.173 98.022 98.608 
LF 86.54 79.81 87.68 86.00 79.53 86.86 
BF 90.69 86.16 91.39 90.10 86.06 90.91 
LF-1 86.71 79.98 87.73 86.76 79.86 87.16 
BF-1 90.86 86.34 91.45 90.89 86.42 91.22 
Table 4. Performances of the grammar with most feature constraints 
Binary+Left+Head+Mother+Head1/0  
Sinica Sinorama Textbook 
RC-Type 90.709 90.460 90.538 
RC-Token 96.906 96.698 97.643 
LF 86.75 78.38 86.19 
BF 90.54 85.20 90.07 
LF-1 88.56 79.55 87.84 
BF-1 92.44 86.46 91.80 
Since all the above features are effective, we like to see the results of multi-feature 
combinations. Many different feature combinations were tested. The experimental 
results show that none of the feature combinations outperform the binary grammars 
with Left and Head1/0 features, even the grammar combining all features, as shown in 
the Table 3 and 4. Here LF-1 and BF-1 measure the label and bracket f-scores only on 
the sentences with parsing results (i.e. sentences failed of producing parsing results 
are ignored). The results show that grammar with all feature constraints has better LF-
1 and BF-1 scores, since the grammar has higher precision. However the total per-
formances, i.e. Lf and BF scores, are not better than the simpler grammar with feature 
184 Y.-M. Hsieh, D.-C. Yang, and K.-J. Chen 
constraints of Left and Head1/0, since the higher precision grammar losses slight edge 
on the grammar coverage. The result clearly shows that tradeoffs do exist between 
grammar precision and coverage. It also suggests that if a feature constraint can im-
prove grammar precision a lot but also reduce grammar coverage a lot, it is better to 
treat such feature constraints as a soft constraint instead of hard constraint. Probabilis-
tic preference for such feature parameters will be a possible implementation of soft 
constraint.  
3.3   Discussions 
Feature constraints impose additional constraints between constituents for phrase 
structures. However different feature constraints serve for different functions and 
have different feature assignment principles. Some features serve for local constraints, 
such as Left, Head, and Head0/1. Those features are only assigned at local intermedi-
ate nodes. Some features are designed for external effect such as Mother Feature, 
which is assigned to phrase nodes and their daughter intermediate nodes. For in-
stances, NP structures for subject usually are different from NP structures for object 
in English sentences [10]. NP attached with Mother-feature can make the difference. 
NPS rules and NPVP rules will be derived each respectively from subject NP and ob-
ject NP structures. However such difference seems not very significant in Chinese. 
Therefore feature selection and assignment should be linguistically-motivated as 
shown in our experiments. 
In conclusion, linguistically-motivated features have better effects on parsing per-
formances than arbitrarily selected features, since they increase grammar precision, 
but only reduce grammar coverage slightly. The feature of leftmost daughter, which 
constraints linear order of constituents, is the most effective feature for parsing. Other 
sub-categorization related features, such as mother node and head features, do not 
contribute parsing F-scores very much. Such features might be useful for purpose of 
sentence generation instead of parsing. 
4   Adapt to Pos Errors Due to Automatic Pos Tagging 
Perfect testing data was used for the above experiments without considering word 
segmentation and pos tagging errors. However in real life word segmentation and pos 
tagging errors will degenerate parsing performances. The real parsing performances 
of accepting input from automatic word segmentation and pos tagging system are 
shown in the Table 5. 
Table 5. Parsing performances of inputs produced by the automatic word segmentation and  
pos tagging 
Binary+Left+Head1/0  
Sinica Sinorama Textbook 
LF 76.18 64.53 73.61 
BF 84.01 75.95 84.28 
 Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 185 
The na?ve approach to overcome the pos tagging errors was to delay some of the 
ambiguous pos resolution for words with lower confidence tagging scores and leave 
parser to resolve the ambiguous pos until parsing stage. The tagging confidence of 
each word is measured by the following value. 
Confidence value= 
)c(P)c(P
)c(P
w,2w,1
w,1
+
, where P(c1,w) and P(c2,w) are probabilities  
assigned by the tagging model for the best candidate c1,w and the second best candi-
date c2,w. 
The experimental results, Table 6, show that delaying ambiguous pos resolution 
does not improve parsing performances, since pos ambiguities increase structure am-
biguities and the parser is not robust enough to select the best tagging sequence.  The 
higher confidence values mean that more words with lower confidence tagging will 
leave ambiguous pos tags and the results show the worse performances. Charniak et al
[3] experimented with using multiple tags per word as input to a treebank parser, and 
came to a similar conclusion. 
Table 6. Parsing performances for different confidence level of pos ambiguities 
Confidence value=0.5  
Sinica Sinorama Textbook 
LF 75.92 64.14 74.66 
BF 83.48 75.22 83.65 
Confidence value=0.8  
Sinica Sinorama Textbook 
LF 75.37 63.17 73.76 
BF 83.32 74.50 83.33 
Confidence value=1.0  
Sinica Sinorama Textbook 
LF 74.12 61.25 69.44 
BF 82.57 73.17 81.17 
4.1   Blending Grammars 
A new approach of grammar blending method was proposed to cope with pos tagging 
errors. The idea is to blend the original grammar with a newly extracted grammar 
derived from the Treebank in which pos categories are tagged by the automatic pos 
tagger. The blended grammars contain the original rules and the extended rules due to 
pos tagging errors. A 5-fold cross-validation was applied on the testing data to tune 
the blending weight between the original grammar and the error-adapted grammar. 
The experimental results show that the blended grammar of weights 8:2 between the 
original grammar and error-adapted grammar achieves the best results. It reduces 
about 20%~30% parsing errors due to pos tagging errors, shown in the Table 7. The 
pure error-adapted grammar, i.e. 0:10 blending weight, does not improve the parsing 
performance very much 
186 Y.-M. Hsieh, D.-C. Yang, and K.-J. Chen 
Table 7. Performances of the blended grammars 
Error-adapted grammar i.e. 
blending weight (0:10) 
Blending weight 8:2  
Sinica Sinirama Textbook Sinica Sinirama Textbook 
LF 75.99 66.16 71.92 78.04 66.49 74.69 
BF 85.65 77.89 85.04 86.06 77.82 85.91 
5   Conclusion and Future Researches 
In order to obtain a high precision and high coverage grammar, we proposed a model 
to measure grammar coverage and designed a PCFG parser to measure efficiency of 
the grammar. Grammar binarization method was proposed to generalize rules and to 
increase the coverage of context-free grammars. Linguistically-motivated feature 
constraints were added into grammar rules to maintain grammar rule precision. It is 
shown that the feature of leftmost daughter, which constraints linear order of constitu-
ents, is the most effective feature. Other sub-categorization related features, such as 
mother node and head features, do not contribute parsing F-scores very much. Such 
features might be very useful for purpose of sentence generation instead of parsing. 
The best performed feature constraint binarized grammar increases the grammar cov-
erage of the original grammar from 93% to 99% and bracketing F-score from 87% to 
91% in parsing moderate hard testing data. To cope with error propagations due to 
word segmentation and part-of-speech tagging errors, a grammar blending method 
was proposed to adapt to such errors. The blended grammar can reduce about 20~30% 
of parsing errors due to error assignment of a pos tagging system.  
In the future, we will study more effective way to resolve structure ambiguities. In 
particular, consider the tradeoff effect between grammar coverage and precision. The 
balance between soft constraints and hard constraints will be focus of our future re-
searches. In addition to rule probability, word association probability will be another 
preference measure to resolve structure ambiguity, in particular for conjunctive  
structures.  
Acknowledgement 
This research was supported in part by National Science Council under a Center Ex-
cellence Grant NSC 93-2752-E-001-001-PAE and National Digital Archives Program 
Grant NSC93-2422-H-001-0004.  
References 
1. E. Charniak, and G. Carroll, ?Context-sensitive statistics for improved grammatical lan-
guage models.? In Proceedings of the 12th National Conference on Artificial Intelligence, 
AAAI Press, pp. 742-747, Seattle, WA, 1994, 
2. E. Charniak, ?Treebank grammars.? In Proceedings of the Thirteenth National Conference 
on Artificial Intelligence, pp. 1031-1036. AAAI Press/MIT Press, 1996. 
 Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 187 
3. E. Charniak, and G. Carroll, J. Adcock, A. Cassanda, Y. Gotoh, J. Katz, M. Littman, J. 
Mccann, "Taggers for Parsers", Artificial Intelligence, vol. 85, num. 1-2, 1996. 
4. Feng-Yi Chen, Pi-Fang Tsai, Keh-Jiann Chen, and Huang, Chu-Ren, ?Sinica Treebank.? 
Computational Linguistics and Chinese Language Processing, 4(2):87-103, 2000. 
5. Keh-Jiann Chen and, Yu-Ming Hsieh, ?Chinese Treebanks and Grammar Extraction.? the 
First International Joint Conference on Natural Language Processing (IJCNLP-04), March 
2004. 
6. Michael Collins, ?Head-Driven Statistical Models for Natural Language parsing.? Ph.D. 
thesis, Univ. of Pennsylvania, 1999. 
7. Yu-Ming Hsieh, Duen-Chi Yang and Keh-Jiann Chen, ?Grammar extraction, generaliza-
tion and specialization. ( in Chinese)?Proceedings of ROCLING 2004. 
8. Christopher D. Manning and Hinrich Schutze, ?Foundations of Statistical Natural Lan-
guage Processing.? the MIT Press, Cambridge, Massachusetts, 1999. 
9. Mark Johnson, ?PCFG models of linguistic tree representations.? Computational Linguis-
tics, Vol.24, pp.613-632, 1998. 
10. Dan Klein and Christopher D. Manning, ?Accurate Unlexicalized Parsing.? Proceeding of 
the 4lst Annual Meeting of the Association for Computational Linguistics, pp. 423-430, 
July 2003. 
11. Honglin Sun and Daniel Jurafsky, ?Shallow Semantic Parsing of Chinese.? Proceedings of 
NAACL 2004. 
12. 12.Hao Zhang, Qun Liu, Kevin Zhang, Gang Zou and Shuo Bai, ?Statistical Chinese 
Parser ICTPROP.? Technology Report, Institute of Computing Technology, 2003. 
 
Resolving Ambiguities of Chinese Conjunctive Structures by Divide-
and-conquer Approaches 
Duen-Chi Yang, Yu-Ming Hsieh, Keh-Jiann Chen  
Institute of Information Science, Academia Sinica, Taipei 
{ydc, morris, kchen}@iis.sinica.edu.tw 
 
 
Abstract 
This paper presents a method to enhance a 
Chinese parser in parsing conjunctive 
structures. Long conjunctive structures 
cause long-distance dependencies and tre-
mendous syntactic ambiguities. Pure syn-
tactic approaches hardly can determine 
boundaries of conjunctive phrases properly. 
In this paper, we propose a divide-and-
conquer approach which overcomes the dif-
ficulty of data-sparseness of the training 
data and uses both syntactic symmetry and 
semantic reasonableness to evaluate am-
biguous conjunctive structures. In compar-
ing with the performances of the PCFG 
parser without using the divide-and-
conquer approach, the precision of the con-
junctive boundary detection is improved 
from 53.47% to 83.17%, and the bracketing 
f-score of sentences with conjunctive struc-
tures is raised up about 11 %. 
1 Introduction 
Parsing a sentence with long conjunctive structure 
is difficult, since it is inadequate for a context-free 
grammar to represent context-sensitive-like coordi-
nation structures, such as ?a b c? and a? b? c?? ?.  
It causes long-distance dependencies and tremen-
dous syntactic ambiguities (a large number of al-
ternatives). Pure syntactic approaches cannot de-
termine boundaries of conjunctive phrases properly. 
It is obvious that both syntactic and semantic in-
formation are necessary for resolving ambiguous 
boundaries of conjunctive structures. 
Some analysis methods of the detection of con-
junctive structures have been studied for a while. 
Despite of using different resources and tools, these 
methods mainly make use of the similarity of 
words or word categories on both sides of conjunc-
tive structure (Agarwal et al, 1992; Kurohashi et 
al., 1994; Delden, 2002; Steiner 2003). They as-
sumed that two sides of conjuncts should have 
similar syntactic and semantic structures. Some 
papers also suggest that certain key word patterns 
can be used to decide the boundaries (Wu 2003). 
Agarwal et al (1992) used a semantic tagger and a 
syntactic chunker to label syntactic and semantic 
chunks. And then they defined multi-level (cate-
gory to category or semantic type to semantic type) 
similarity matching to find the structure boundaries. 
Delden (2002) included semantic analysis by 
applying WordNet (Miller 1993) information. 
These presented methods used similarity measures 
heuristically according to the property of the lan-
guages. However detecting conjunctive boundaries 
with a similar method in Chinese may meet some 
problems, since a Chinese word may play different 
syntactic functions without inflection. It results that 
syntactic symmetry is not enough to resolve ambi-
guities of conjunctive structures and semantic rea-
sonableness is hard to be evaluated. Therefore we 
propose a divide-and-conquer approach which 
takes the advantage of using structure information 
of partial sentences located at both sides of con-
junction. Furthermore we believe that simple cases 
can be solved by simple methods which are effi-
cient and only complex cases require deep syntac-
tic and semantic analysis. Therefore we develop an 
algorithm to discriminate simple cases and com-
plex cases first. We then use a sophisticated algo-
rithm to handle complex cases only.  
For simple cases, we use conventional pattern 
matching approach to speedup process. For com-
plex conjunctive structures, we propose a divide-
and-conquer approach to resolve the problem. An 
input sentence with complex conjunctive structure 
715
is first divided into two parts, one to the left of the 
conjunctive and one to the right, and then parsed 
independently to detect possible candidates of two 
conjuncts. The particular property of complex con-
junctive structures of Chinese language allows us 
to parse and to produce syntactic structures of two 
partial sentences, since according to our observa-
tions and experiments the syntactic structures of 
partial sentences at either side of a complex con-
junctive construction are grammatical most of the 
times. Figure 1 shows an instance. The parsing re-
sults not only reduce the possible ambiguous 
boundaries but also provide global structural in-
formation for checking the properness of both sides 
of conjunctive structure. Another important point 
worth mentioning is that since the size of available 
Treebank is small, a two-stage approach is pro-
posed to resolve the data sparseness problems in 
evaluating syntactic symmetry and semantic rea-
sonableness. At the first stage, a Conditional Ran-
dom Fields model is trained and used to generate a 
set of candidate boundaries. At the second stage, a 
word-association model is trained from a giga-
word corpus to evaluate the semantic properness of 
candidates. The proposed divide-and-conquer algo-
rithm avoids parsing full complex conjunctive 
structures and handles conjunctive structures with 
deep structural and semantic analysis. 
The extraction method for context-dependent 
rules is described in Section 2 and detail of the di-
vide-and-conquer approach is stated in Section 3. 
In Section 4, we introduce our experimental envi-
ronment and show the results of our experiment. 
We also make some discussions about our observa-
tions in Section 4. Finally, we offer our conclusion 
and future work in Section 5. 
2 Boundary Detection for Simple Con-
junctive Phrases 
The aim of this phase of approach is to determine if 
simple conjunctive phrases exist in input sentences 
and then identify their boundaries by matching 
context-dependent rules. To derive a set of context-
dependent rules for conjunctive phrases, a na?ve 
approach is to extract all conjunctive patterns with 
their contextual constraints from Treebank. How-
ever such a set of extracted rules suffers a low cov-
erage rate, since limited size of training data causes 
zero frequency of long n-gram PoS patterns. 
2.1 Rule extraction and generalization 
Agarwal et al, (1992), Kurohashi et al, (1994), 
and Delden (2002) had shown that the properties of 
likeness and symmetry in both syntactic types and 
lengths for example, exist in most conjunctive 
cases. Hence we use both properties as the condi-
tions in deciding boundaries of conjunctive phrases. 
When we observe Sinica Treebank (Chen et al, 
2003), we also find that this property is more obvi-
ous in simple conjunctive cases than in complex 
cases. 
First, we use a simple algorithm to detect the 
boundaries of completely symmetric conjunctive 
phrases. If PoS patterns of ?A B C and A B C? or 
?A B and A B? occurred in the input sentence, we 
consider patterns of such structures are legitimate 
conjunctive structures regardless whether the PoS 
sequences ?A B C and A B C? or ?A B and A B? 
ever occurred in the Treebank. For other cases we 
use context-dependent rule patterns to determine 
boundaries of conjunctive structures. 
Statistical context-dependent PoS-based rule pat-
terns are extracted automatically from Sinica Tree-
bank. Each rule contains the PoS pattern of a con-
junctive phrase and its left/right contextual con-
straints. The occurrence frequency of the rule and 
its correct identification rate are also associated. e.g. 
[VC] (Na Caa Nc) [DE]1 ;  12; 11 
This rule says that PoS sequence Na Caa Nc 
forms a conjunctive phrase when its left context is 
a VC and its right context is a DE. Such pattern 
occurred 12 times in the training corpus and 11 out 
of 12 times (Na Caa Nc) are correct conjunctive 
phrases. 
Context-dependent rule patterns are generated 
and generalized by the following procedure. 
Rule Generation and Generalization 
For each conjunctive structure in the Treebank, we 
consider a window pattern of at most 9 words. This 
pattern contains conjunction in the center and at 
most 4 words at each side of the conjunction. The 
PoS sequence of these 9 words forms a context-
dependent rule. For instance, the conjunctive struc-
ture shown in Figure 1 will generate the pattern (1). 
(1) [Vc DM] (VH  Na  Caa Neu Na) [DE Na] 
The long pattern has low applicability and hardly 
                                                 
1 Caa is a PoS for coordinate conjunction. Na is a common 
noun; Nc denotes place noun, and Vc is a transitive verb. DE 
denotes the relativizer ???. 
716
can evaluate its precision. Therefore a rule gener-
alization process is applied. Two kinds of generali-
zations are available. One is reducing the length of 
contextual constrains and the other is to reduce a 
fine-grained PoS constraint to a coarse-grained PoS. 
Some instances, shown in (2), are the generalized 
patterns of (1). 
(2)  [DM] (VH  Na  Caa Neu Na) [DE];1;1 
(VH  Na  Caa Neu Na); 10; 5 
[DM] (V  N  Caa N N) [DE]; 3; 2 
Then the applicability and precision of rules higher 
than threshold values will be selected. The threshold 
values for the rule selection are determined by test-
ing results on the development data. 
3 Resolution of Complex Conjunctive 
Structures 
Complex structures are cases whose boundaries can 
not be identified by the pattern matching at phase-1. 
We propose a divide-and-conquer approach to re-
solve the problem. An input sentence with complex 
conjunctive structure was first divided into two 
parts with each part containing one of the conjuncts 
and then parsed independently to produce their 
syntactic structures for detecting possible bounda-
ries of two conjuncts. Then ambiguous candidate 
structures are generated and the best conjunctive 
structure is selected by evaluating syntactic sym-
metry and semantic reasonableness of the candi-
dates. Since the two parts of the partial sentences 
are simple without conjunctive structure and nor-
mally grammatical 2 , hence they can be easily 
parsed by a PCFG parser. 
Here we illustrate the divide-and-conquer algo-
rithm by the following example. For instance, the 
example shown in Figure 1 has complex conjunc-
tive structure and it was first split into two parts (1a) 
and (1b) at conjunction marker ? ??. 
(1a) ?? if (Cbb) ? I (Nh) ?? invent (VC) ?? a 
kind (DM) ? low (VH) ?? pollution (Na) 
(1b) ? null (Neu) ?? accident (Na) ?(DE) ??
car (Na)  
The two parts of partial sentences are then 
parsed to produce their syntactic structures as 
shown in Figure 1. Then a CRF model trained from 
Sinica Treebank for checking syntactic symmetry 
                                                 
2 According to our experiments only 0.8% of the complex 
testing data and development data are failed to parse their 
partial structures at both sides of conjunction. 
was derived to pick the top-N candidates according 
to the syntactic information of both sides of partial 
sentences. Then at the second stage, a semantic 
evaluation model is proposed to select the best 
candidate. The detail of the semantic evaluation 
model is described in the section 3.2. The reason 
for using a two-stage approach is that the size of 
the Treebank is limited, but the semantic evaluation 
model requires the values of association strengths 
between words. The current Treebank cannot pro-
vide enough coverage and reliable values of word-
association strengths.  
3.1  Derive and evaluate possible candidates 
CRF is a well-known probabilistic framework for 
segmenting and labeling sequence data (Lafferty, et 
al. 2001). In our experiments, we regard the prob-
lem of boundary detection as a chunking-like prob-
lem (Lee et al, 2005). Due to this reason, we use 
CRF model to generate candidates and their ranks. 
The features used in CRF model included some 
global syntactic information, such as syntactic 
category of a partial structure and its phrasal head. 
Such global syntactic information is crucial for the 
success of boundary detection and is not available 
if without the step of parsing process. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. The syntactic structures of 5(a) and 5(b) 
produced by a PCFG parser. 
The features used are: 
WL,i ; CL,i; WR,j ; CR,j : The left(i)/right(j) most word 
and its pos category of the left/right conjunct. 
PL, ; PR,: The phrasal category of the left/right con-
junct. 
HwL ; HcL ; HwR ; HcR: The phrasal head and its pos 
category of the left/right conjunct. 
DL ; DR: The length of the left/right conjunct.  
Three types of feature patterns are used for CRF. 
The first type is feature patterns regarding individ-
WL,i+1 (WLi WL,i-1 ?.       WL1    W0   WR1 ?WR,j )WR,j+1, 
 
Some example feature values of the above hypothesis boundaries.  
WLi  = ?; CLi  =Nh; WR,j  =??; CR,j  =Na; 
PL, =S; PR, =NP; 
HwL=??; HcL= VC; HwR =??; HcR=Na; 
DL = 5; DR = 2;
717
ual conjuncts. The second type is feature patterns 
regarding symmetry between two conjuncts. The 
third type is feature patterns regarding contextual 
properness of a conjunctive structure. 
Type1: WLi, WLi-1, WLi+1, CLi, CLi-1, CLi-2, CLi-1CLi-2, CLi+1, CLi+2, 
CLi+1CLi+2, CLiCLi-1CLi-2, CLi-1CLiCLi+1, CLiCLi+1CLi+2, 
WLiHwL, CLiHcL, and WRj, WRj-1, WRj+1, CRj, CRj-1, CRj-2, CRj-
1CRj-2, CRj+1, CRj+2, CRj+1CRj+2, CRjCRj-1CRj-2, CRj-1CRjCRj+1, 
CRjCRj+1CRj+2, WRjHwR, CRjHcR.. 
Type 2: PL PR, HwLHwR, HcLHcR, DLDR. 
Type 3: WL,i+1HwRj, WR,j+1HwLi, WL,1WR,j, WR,1WL,j, 
WL,1WR,j+1, WR,1WL,j+1, WL,1WR,jWR,j+1, 
WR,1WL,iWL,i+1, WL,1WR,j-1WR,j, WR,1WL,i-1WL,i,  
CL,i-1HcRj, CR,j-1HcLi, CL,i+1HcRj, CR,j+1HcLi, 
CL,iCL,i+1HcRj, CR,jCR,j+1HcLi, CL,1CR,j, CR,1CL,j, 
CL,1CR,j+1, CR,1CL,j+1, CL,1CR,jCR,j+1, CR,1CL,iCL,i+1, 
CL,1CR,j-1CR,j, CR,1CL,i-1CL,i. 
A CRF model is trained from the Sinica Tree-
bank and estimated the probabilities of hypothesis 
conjunctive boundary pairs by the feature patterns 
listed above. The top ranked candidates are se-
lected according to the CRF model. In general, for 
further improvement, a final step of semantic 
evaluation will be performed to select the best can-
didate from top-N boundary structures ranked by 
the CRF model, which is described in the next sec-
tion.  
3.2 The word-association evaluation model 
For the purpose of selecting the best candidates of 
complex conjunctive structures, a word association 
evaluation model is adopted (Hsieh et al 2007). 
The word-to-word association data is learned 
automatically by parsing texts from the Taiwan 
Central News Agency corpus (traditional charac-
ters), which contains 735 million characters. The 
syntactically dependent words-pairs are extracted 
from the parsed trees. The word-pairs are phrasal 
heads and their arguments or modifiers. Though the 
data is imperfect (due to some errors produced by 
auto-tagging system and parser), the amount of 
data is large enough to compensate parsing errors 
and reliably exhibit strength between two 
words/concepts. 
37,489,408 sentences in CNA (Central News 
Agency) corpus are successfully parsed and the 
number of extracted word associations is 
221,482,591. The word association probabilities is 
estimated by eq.(1). 
)(
),(
)|(
Headfreq
ModifyHeadfreq
HeadModifyP =        (1) 
?freq(Head)? means Head word frequency in the 
corpus and ?freq(Head,Modify)? is the cooccur-
rence frequency of Head and Modify/Argument.  
The final evaluation is done by combining three 
scores, i.e. (1) the probability produced by PCFG 
parser, (2) the scores of CRF classifier and (3) the 
scores of semantic evaluation. The detail is de-
scribed in Section 4.2. 
4 Experiments 
3,484 sentences of the Sinica Treebank are used as 
training data. The development data and testing 
data are extracted from three different set of cor-
pora the Sinica corpus, Sinorama magazines and 
textbooks of elementary school (Hsieh et al 2005). 
They are totally 202 sentences (244 conjunctions) 
with 6-10 words and 107 sentences (159 conjunc-
tions) with more than 11 words. We only test the 
sentences which contain the coordinate conjunction 
category or categories.  
We adopt the standard PARSEVAL metrics 
(Manning et al, 1999) including bracket f-score to 
evaluate the performance of the tree structures of 
sentences and accuracies of boundary detection of 
conjunction structures. 
4.1 Phase-1 experimental results 
For the phase-1 experiments, the context-
dependent rules are extracted and generalized from 
Sinica treebank. We then use the development data 
to evaluate the performances for different sets of 
rules selected by different threshold values. The 
results show that the threshold values of occurrence 
once and precision 70% performed best. This 
means any context-dependent rule with precision 
greater than or equal to 70% is used for the future 
processes. 39941 rules are in the set. In Table 1, we 
compare the phase-1 result with the baseline model 
on test data. It is shown that the boundary detection 
precision is very high, but the recall rate is com-
paratively low, since the phase-1 process cannot 
handle the complex cases. We also compare the 
processing time between the baseline model and 
the phase-1 parsing processes in Table 2. Marking 
conjunctive boundaries before parsing can limit the 
search range for parser and save processing time. 
The effect is more obvious when parsing long sen-
tences. Because long sentences generate more am-
718
biguous paths than shorter sentences, these surely 
spend much more time. 
6-10 words more than 11 words Test data  
Baseline phase1 Baseline phase1
C-boundary  
f-score 
55.74 84.43 50.0 63.75 
S-bracket        
f-score 
72.67 84.44 71.20 79.40 
Table 1. The comparison between the baseline 
PCFG model and the phase1 parsing process . 
6-10 words more than 11 words unit: second 
Baseline  phase1  Baseline  phase1
development data 14 12 34 23 
test data 14 11 34 24 
Table 2. The comparison of processing time be-
tween the baseline model and the phase1 parsing 
process. 
4.2 Phase-2 experimental results 
Complex cases cannot be matched by context-
dependent rules at the phrase-1 which will be han-
dled by the phase-2 algorithms mentioned in Sec-
tion 3. We use the CRF++ tool (Kudo, 2006) to 
train our CRF model. The CRF model can produce 
the N-best candidates for an input conjunctive sen-
tence. We experiment on the models of Top1-CRF 
and TopN-CRF where the Top1-CRF algorithm 
means that the final output is the best candidate 
produced by CRF model and the TopN-CRF means 
that the final output is the best candidate produced 
by the structure evaluation process described below. 
For each N-best candidate structure, three 
evaluation scores is derived: (a) the probability 
score generated from the PCFG parser, i.e. 
RuleScore, (b) the probability score generated from 
the CRF classifier, i.e. CRF-Score, and (c) the 
word association score, i.e. WA-Score. We normal-
ize each of the three scores by eq.(2): 
minmax
min)(
ScoreScore
ScoreScore
Scorenormal ii ?
?=                 (2) 
Scorei means the score of the i-th candidate, and 
Scoremin and Scoremax mean the worst and the best 
score in the candidate set for a target conjunctive 
sentence. The normalized scores are between 0 and 
1. After normalization, we combine the three 
scores with different weights: 
Total Score = w1*RuleScore + w2*CRF-Score + 
w3*WA-Score                                   (3) 
The w1, w2 and w3 are regarded as the degree of 
importance of the three types of information. We 
use development data to determine the best combi-
nation of w1, w2, w3. Due to limit amount of de-
velopment data, many local maximum and global 
maximum are achieved by different values of w1, 
w2, w3. Therefore we use a clustering algorithm to 
cluster the grid points of (w1, w2, w3) which pro-
duce the best performance. We then pick the larg-
est cluster and calculate its centroid as our final 
weights which are shown at Table 3.  
 Top N w1 w2 w3 
6-10words N = 3 0.11 0.64 0.25 
11- words N = 3 0.18 0.76 0.06 
Table 3. The best weights determined by the devel-
opment data for the sentences with different 
lengths using the best-3 candidates.  
The performance results of the testing data are 
shown in Table 4. In comparing with the results of 
the baseline model shown in Table 1, the conjunc-
tion boundary f-score increased from about 53% to 
83% for the testing data. The processes also im-
prove the overall parsing f-scores from 72% to 
83%. The results of Table 4 also show that the 
evaluation function indeed improves the perform-
ances but marginally. However the experiments are 
done under the condition that the input sentences 
are perfectly word segmented and pos tagged. In 
real practices, parser may accept sentences with 
ambiguous word segmentation and pos tagging to 
avoid the error accumulation due to early commit-
ment on word segmentation and pos tagging. 
Therefore parsers require much more information 
to resolve much more ambiguous conditions. A 
robust evaluation function may play a very impor-
tant role. We will do more researches in the future. 
 Top1CRF TopNCRF 
C-boundary f-score 85.57 89.55 Develop-
ment data S-bracket f-score 80.10 82.34 
C-boundary f-score 82.18 83.17 Test data 
S-bracket f-score 83.15 83.45 
Table 4. The final results of our overall processes.  
Another point worth mentioning, the perform-
ances of ?CRF? (using CRF model without phase-1) 
and ?phase1+CRF? (using CRF model after phase-
1) algorithms are comparable. However ?phase1+ 
CRF? algorithm is much more efficient, since 
?phase1+CRF? algorithm can determine the simple 
conjunctive structures by pattern matching and 
most of conjunctive structures are simple. On the 
other hand, the ?CRF? model requires twice partial 
sentence parsing, generates candidates with CRF 
719
classifier and evaluates structure with three syntac-
tic and semantic scores. 
5 Conclusion 
Conjunctive boundary detection is not a simple 
task. It is not only time consuming but also knowl-
edge intensive. Therefore we propose a context-
dependent rules matching approach to handle sim-
ple cases to get fast returns.  For complex cases, we 
use a knowledge intensive divide-and-conquer ap-
proach. To resolve the problems of inadequate 
knowledge and data sparseness due to limit amount 
of structure annotated training data, we extract 
word/concept associations from CNA corpus.  
In our experiments, the proposed model works 
well. Most conjunctive phrases are simple cases 
and can be matched by context-dependent rules and 
indeed avoid unnecessary calculation. Compared 
with the baseline method of straight forward PCFG 
parsing, the f-score of conjunctive boundary detec-
tion can be raised about 22%. For the complex 
cases, the boundaries f-score is further raised about 
7% after phase-2 processes. The experimental re-
sults show that the method not only works well on 
boundary resolution for conjunctive phrases but 
also improves the total performances of syntactic 
parsing. 
Our solutions include the rule-based method and 
cooperate with semantic and syntactic analyses. 
Therefore in the future we will try to enhance the 
syntactic and semantic analyses. For syntactic 
analysis, we still need to find more effective meth-
ods to improve the performance of our parser. For 
the semantic analysis, we will try to refine the word 
association data and discover a better semantic 
evaluation model. 
Acknowledgements 
This research was supported in part by National 
Digital Archives Program (NDAP, Taiwan) spon-
sored by the National Science Council of Taiwan 
under NSC Grants: NSC95-2422-H-001-031-. 
References 
Agarwal, Rajeev and Boggess, Lois. 1992. A Simple but 
Useful Approach to Conjunct Identification. In Pro-
ceedings of 30th Annual Meeting of Association for 
Computational Linguistics, pages 15-21. 
Chen, Keh-Jiann, Huang, Chu-Ren, Chen, Feng-Yi, Luo, 
Chi-Ching, Chang, Ming-Chung, Chen, Chao-Jan and 
Gao, Zhao-Ming. 2003. Sinica Treebank: design cri-
teria, representational issues and implementation. In 
Anne Abeille, (ed.): Building and Using Parsed Cor-
pora. Text, Speech and Language Technology. 
20:231-248, pages 231-248. 
Hsieh,Yu-Min, Yang, Duen-Chi and Chen, Keh-Jiann. 
2005. Linguistically-motivated grammar extraction, 
generalization and adaptation. In Proceedings of the 
Second International Join Conference on Natural 
Language Processing (IJCNLP2005), pages 177-187, 
Jeju Island, Republic of Korea. 
Hsieh, Yu-Ming, Duen-Chi Yang and Keh-Jiann Chen. 
2007. Improve Parsing Performance by Self-Learning. 
International Journal of Computational Linguistics 
and Chinese Language Processing, Vol. 12, #2, 
pages 195-216. 
Kurohashi, Sadao, and Nagao, Makoto. 1994. A Syntac-
tic Analysis Method of Long Japanese Sentences 
Based on the Detection of Conjunctive Structure. 
Computational Linguistics 20(4), pages 507-534. 
Kudo, Taku. 2006. (software)CRF++: Yet Another CRF 
toolkit http://chasen.org/~taku/software/CRF++/. 
Lafferty, John, McCallum, Andrew, Pereira, Fernando. 
2001. Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence Data. 
In Proceedings of the 18th International Conference 
on Machine Learning (ICML-01), pages 282-289. 
Lee, Yong-Hun, Kim, Mi-Young and Lee, Jong-Hyeok. 
2005. Chunking Using Conditional Random Fields in 
Korea Texts. In Proceedings of the Second Interna-
tional Join Conference on Natural Language Proc-
essing (IJCNLP2005), pages 155-164, Jeju Island, 
Republic of Korea. 
Manning, Christopher D., and Schutze, Hinrich. 1999. 
Foundations of Statistical Natural Language process-
ing. The MIT Press, Cambridge, Massachusetts.  
Miller, Geroge, 1993. Introduction to WordNet: An 
Online Lexical Database. Princeton, CSL Report 43. 
Steiner, Ilona. 2003. Parsing Syntactic Redundancies in 
Coordinate Structures. Poster presentation at the 
European Cognitive Science Conference (Euro-
CogSci03). 
Van Delden, Sebastian. 2002. A Hybrid Approach to 
Pre-Conjunct Identification. In Proceedings of the 
2002 Language Engineering Conference (LEC 2002), 
pages 72-77, University of Hyderabad, India. 
Wu, Yunfang. 2003. Contextual Information of Coordi-
nate Structure. Advances on the Research of Machine 
Translation, pages 103-109, Publishing house of 
Electronics Industry. 
720
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 928?937,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Ambiguity Resolution for Vt-N Structures in Chinese 
 
 
Yu-Ming Hsieh1,2       Jason S. Chang2       Keh-Jiann Chen1 
1 Institute of Information Science, Academia Sinica, Taiwan 
2 Department of Computer Science, National Tsing-Hua University, Taiwan 
morris@iis.sinica.edu.tw, jason.jschang@gmail.com 
kchen@iis.sinica.edu.tw 
 
  
 
Abstract 
The syntactic ambiguity of a transitive 
verb (Vt) followed by a noun (N) has 
long been a problem in Chinese parsing. 
In this paper, we propose a classifier to 
resolve the ambiguity of Vt-N structures. 
The design of the classifier is based on 
three important guidelines, namely, 
adopting linguistically motivated features, 
using all available resources, and easy in-
tegration into a parsing model. The lin-
guistically motivated features include 
semantic relations, context, and morpho-
logical structures; and the available re-
sources are treebank, thesaurus, affix da-
tabase, and large corpora. We also pro-
pose two learning approaches that resolve 
the problem of data sparseness by auto-
parsing and extracting relative 
knowledge from large-scale unlabeled 
data. Our experiment results show that 
the Vt-N classifier outperforms the cur-
rent PCFG parser. Furthermore, it can be 
easily and effectively integrated into the 
PCFG parser and general statistical pars-
ing models. Evaluation of the learning 
approaches indicates that world 
knowledge facilitates Vt-N disambigua-
tion through data selection and error cor-
rection. 
1 Introduction 
In Chinese, the structure of a transitive verb (Vt) 
followed by a noun (N) may be a verb phrase 
(VP), a noun phrase (NP), or there may not be a 
dependent relation, as shown in (1) below. In 
general, parsers may prefer VP reading because a 
transitive verb followed by a noun object is nor-
mally a VP structure. However, Chinese verbs 
can also modify nouns without morphological 
inflection, e.g., ?? /farming ? /pond. Conse-
quently, parsing Vt-N structures is difficult be-
cause it is hard to resolve such ambiguities with-
out prior knowledge. The following are some 
typical examples of various Vt-N structures:  
1) 
??/solve ??/problem ? VP 
??/solving ??/method ? NP 
??/solve ??/mankind (??/problem)?None 
To find the most effective disambiguation fea-
tures, we need more information about the Vt-N 
? NP construction and the semantic relations 
between Vt and N. Statistical data from the Sini-
ca Treebank (Chen et al., 2003) indicates that 
58% of Vt-N structures are verb phrases, 16% 
are noun phrases, and 26% do not have any de-
pendent relations. It is obvious that the semantic 
relations between a Vt-N structure and its con-
text information are very important for differen-
tiating between dependent relations. Although 
the verb-argument relation of VP structures is 
well understood, it is not clear what kind of se-
mantic relations result in NP structures. In the 
next sub-section, we consider three questions: 
What sets of nouns accept verbs as their modifi-
ers? Is it possible to identify the semantic types 
of such pairs of verbs and nouns? What are their 
semantic relations? 
1.1 Problem Analysis 
Analysis of the instances of NP(Vt-N) structures 
in the Sinica Treebank reveals the following four 
types of semantic structures, which are used in 
the design of our classifier. 
 
Type 1. Telic(Vt) + Host(N): Vt denotes the 
telic function (purpose) of the head noun N, e.g., 
928
?? /research ?? /tool; ?? /explore ?
/machine; ?/gamble ?/house; ??/search ?
?/program. The telic function must be a salient 
property of head nouns, such as tools, buildings, 
artifacts, organizations and people. To identify 
such cases, we need to know the types of nouns 
which take telic function as their salient property. 
Furthermore, many of the nouns are monosyl-
labic words, such as ?/people, ?/instruments, 
?/machines. 
Type 2. Host-Event(Vt) + Attribute(N): 
Head nouns are attribute nouns that denote the 
attributes of the verb, e.g., ??/research ??
/method (method of research); ??/attack ??
/strategy (attacking strategy); ??/write ??
/context (context of writing); ?/gamble ?/rule 
(gambling rules). An attribute noun is a special 
type of noun. Semantically, attribute nouns de-
note the attribute types of objects or events, such 
as weight, color, method, and rule. Syntactically, 
attribute nouns do not play adjectival roles (Liu, 
2008). By contrast, object nouns may modify 
nouns. The number of attributes for events is 
limited. If we could discover all event-attribute 
relations, then we can solve this type of construc-
tion. 
Type 3. Agentive + Host: There is only a lim-
ited number of such constructions and the results 
of the constructions are usually ambiguous, e.g., 
??/fried rice (NP), ??/shouting sound. The 
first example also has the VP reading. 
Type 4. Apposition + Affair: Head nouns are 
event nouns and modifiers are verbs of apposi-
tion events, e.g. ??/collide ??/accident, ?
? /destruct ?? /movement, ?? /hate ??
/behavior. There is finite number of event nouns.  
 
Furthermore, when we consider verbal modi-
fiers, we find that verbs can play adjectival roles 
in Chinese without inflection, but not all verbs 
play adjectival roles. According to Chang et al. 
(2000) and our observations, adjectival verbs are 
verbs that denote event types rather than event 
instances; that is, they denote a class of events 
which that are concepts in an upper-level ontolo-
gy. One important characteristic of adjectival 
verbs is that they have conjunctive morphologi-
cal structures, i.e., the words are conjunct with 
two nearly synonymous verbs, e.g., ?/study ?
/search (research), ? /explore ? /detect (ex-
plore), and ?/search ?/find (search). Therefore, 
we need a morphological classifier that can de-
tect the conjunctive morphological structure of a 
verb by checking the semantic parity of two 
morphemes of the verb. 
Based on our analysis, we designed a Vt-N 
classifier that incorporates the above features to 
solve the problem. However, there is a data 
sparseness problem because of the limited size of 
the current Treebank. In other words, Treebank 
cannot provide enough training data to train a 
classifier properly. To resolve the problem, we 
should mine useful information from all availa-
ble resources. 
The remainder of this paper is organized as 
follows. Section 2 provides a review of related 
works. In Section 3, we describe the disambigua-
tion model with our selected features, and intro-
duce a strategy for handling unknown words. We 
also propose a learning approach for a large-
scale unlabeled corpus. In Section 4, we report 
the results of experiments conducted to evaluate 
the proposed Vt-N classifier on different feature 
combinations and learning approaches. Section 5 
contains our concluding remarks. 
2 Related Work 
Most works on V-N structure identification focus 
on two types of relation classification: modifier-
head relations and predicate-object relations (Wu, 
2003; Qiu, 2005; Chen, 2008; Chen et al., 2008; 
Yu et al., 2008). They exclude the independent 
structure and conjunctive head-head relation, but 
the cross-bracket relation does exist between two 
adjacent words in real language. For example, if 
???/all over  ??/world ? was included in the 
short sentence ???/all over  ??/world ??
/countries?, it would be an independent structure. 
A conjunctive head-head relation between a verb 
and a noun is rare. However, in the sentence ??
? ?? ? ? ??? (Both service and equip-
ment are very thoughtful.), there is a conjunctive 
head-head relation between the verb ? ?
/service and the noun ??/equipment. Therefore, 
we use four types of relations to describe the V-
N structures in our experiments. The symbol 
?H/X? denotes a predicate-object relation; ?X/H? 
denotes a modifier-head relation; ?H/H? denotes 
a conjunctive head-head relation; and ?X/X? de-
notes an independent relation. 
Feature selection is an important task in V-N 
disambiguation. Hence, a number of studies have 
suggested features that may help resolve the am-
biguity of V-N structures (Zhao and Huang, 1999; 
Sun and Jurafsky, 2003; Chiu et al., 2004; Qiu, 
2005; Chen, 2008). Zhao and Huang used lexi-
cons, semantic knowledge, and word length in-
929
formation to increase the accuracy of identifica-
tion. Although they used the Chinese thesaurus 
CiLin (Mei et al., 1983) to derive lexical seman-
tic knowledge, the word coverage of CiLin is 
insufficient. Moreover, none of the above papers 
tackle the problem of unknown words. Sun and 
Jurafsky exploit the probabilistic rhythm feature 
(i.e., the number of syllables in a word or the 
number of words in a phrase) in their shallow 
parser. Their results show that the feature im-
proves the parsing performance, which coincides 
with our analysis in Section 1.1. Chiu et al.?s 
study shows that the morphological structure of 
verbs influences their syntactic behavior. We 
follow this finding and utilize the morphological 
structure of verbs as a feature in the proposed Vt-
N classifier. Qiu?s approach uses an electronic 
syntactic dictionary and a semantic dictionary to 
analyze the relations of V-N phrases. However, 
the approach suffers from two problems: (1) low 
word coverage of the semantic dictionary and (2) 
the semantic type classifier is inadequate. Finally, 
Chen proposed an automatic VN combination 
method with features of verbs, nouns, context, 
and the syllables of words. The experiment re-
sults show that the method performs reasonably 
well without using any other resources. 
Based on the above feature selection methods, 
we extract relevant knowledge from Treebank to 
design a Vt-N classifier. However we have to 
resolve the common problem of data sparseness. 
Learning knowledge by analyzing large-scale 
unlabeled data is necessary and proved useful in 
previous works (Wu, 2003; Chen et al., 2008; Yu 
et al., 2008). Wu developed a machine learning 
method that acquires verb-object and modifier-
head relations automatically. The mutual infor-
mation scores are then used to prune verb-noun 
whose scores are below a certain threshold. The 
author found that accurate identification of the 
verb-noun relation improved the parsing perfor-
mance by 4%. Yu et al. learned head-modifier 
pairs from parsed data and proposed a head-
modifier classifier to filter the data. The filtering 
model uses the following features: a PoS-tag pair 
of the head and the modifier; the distance be-
tween the head and the modifier; and the pres-
ence or absence of punctuation marks (e.g., 
commas, colons, and semi-colons) between the 
head and the modifier. Although the method im-
proves the parsing performance by 2%, the filter-
ing model obtains limited data; the recall rate is 
only 46.35%. The authors also fail to solve the 
problem of Vt-N ambiguity. 
Our review of previous works and the obser-
vations in Section 1.1 show that lexical words, 
semantic information, the syllabic length of 
words, neighboring PoSs and the knowledge 
learned from large-scale data are important for 
Vt-N disambiguation. We consider more features 
for disambiguating Vt-N structures than previous 
studies. For example, we utilize (1) four relation 
classification in a real environment, including 
?X/H?, ?H/X?, ?X/X? and ?H/H? relations; (2) un-
known word processing of Vt-N words (includ-
ing semantic type predication and morph-
structure predication); (3) unsupervised data se-
lection (a simple and effective way to extend 
knowledge); and (4) supervised knowledge cor-
rection, which makes the extracted knowledge 
more useful. 
3 Design of the Disambiguation Model 
The disambiguation model is a Vt-N relation 
classifier that classifies Vt-N relations into ?H/X? 
(predicate-object relations), ?X/H? (modifier-
head relations), ?H/H? (conjunctive head-head 
relations), or ?X/X? (independent relations). We 
use the Maximum Entropy toolkit (Zhang, 2004) 
to construct the classifier. The advantage of us-
ing the Maximum Entropy model is twofold: (1) 
it has the flexibility to adjust features; and (2) it 
provides the probability values of the classifica-
tion, which can be easily integrated into our 
PCFG parsing model. 
In the following sections, we discuss the de-
sign of our model for feature selection and ex-
traction, unknown word processing, and world 
knowledge learning. 
3.1 Feature Selection and Extraction 
We divide the selected features into five groups: 
PoS tags of Vt and N, PoS tags of the context, 
words, semantics, and additional information. 
Table 1 shows the feature types and symbol nota-
tions. We use symbols of t1 and t2 to denote the 
PoS of Vt and N respectively. The context fea-
ture is neighboring PoSs of Vt and N: the sym-
bols of t-2 and t-1 represent its left PoSs, and the 
symbol t3 and t4 represent its right PoSs. The se-
mantic feature is the lexicon?s semantic type ex-
tracted from E-HowNet sense expressions 
(Huang et al., 2008). For example, the E-
HowNet expression of ? ? ? /vehicles? is 
{LandVehicle|? :quantity={mass|? }}, so its 
semantic type is {LandVehicle|?}. We discuss 
the model?s performance with different feature 
combinations in Section 4. 
930
Feature  Feature Description 
PoS PoS of Vt and N 
t1; t2 
Context Neighboring PoSs 
t-2; t-1; t3; t4 
Word Lexical word 
w1; w2 
Semantic Semantic type of word 
st1; st2 
Additional 
Information 
Morphological structure of verb 
Vmorph 
 Syllabic length of noun 
Nlen 
 
Table 1. The features used in the Vt-N classifier 
 
The example in Figure 1 illustrates feature la-
beling of a Vt-N structure. First, an instance of a 
Vt-N structure is identified from Treebank. Then, 
we assign the semantic type of each word with-
out considering the problem of sense ambiguity 
for the moment. This is because sense ambigui-
ties are partially resolved by PoS tagging, and 
the general problem of sense disambiguation is 
beyond the scope of this paper. Furthermore, 
Zhao and Huang (1999) demonstrated that the 
retained ambiguity does not have an adverse im-
pact on identification. Therefore, we keep the 
ambiguous semantic type for future processing. 
 
zhe        zaochen     xuexi  zhongwen    DE    fongchao 
this         cause        learn     Chinese                    trend 
?This causes the trend of learning Chinese.? 
 
Figure 1. An example of a tree with a Vt-N struc-
ture 
 
Table 2 shows the labeled features for ???
/learn  ??/Chinese? in Figure 1. The column x  
and y describe relevant features in ???/learn? 
and ???/Chinese? respectively. Some features 
are not explicitly annotated in the Treebank, e.g., 
the semantic types of words and the morphologi-
cal structure of verbs. We propose labeling 
methods for them in the next sub-section. 
Feature Type x y 
Word w1=?? w2=?? 
PoS t1=VC t2=Na 
Semantic st1=study|?? st2=language|?? 
Context t-2=Nep; t-1=VK; t3=DE; t4=Na 
Additional 
Information 
Vmorph=VV Nlen=2 
Relation Type  rt = H/X 
 
Table 2. The feature labels of Vt-N pair in Figure 
1 
3.2 Unknown Word Processing 
In Chinese documents, 3% to 7% of the words 
are usually unknown (Sproat and Emerson, 
2003). By ?unknown words?, we mean words not 
listed in the dictionary. More specifically, in this 
paper, unknown words means words without se-
mantic type information (i.e., E-HowNet expres-
sions) and verbs without morphological structure 
information. Therefore, we propose a method for 
predicting the semantic types of unknown words, 
and use an affix database to train a morph-
structure classifier to derive the morphological 
structure of verbs. 
 
Morph-Structure Predication of Verbs: We 
use data analyzed by Chiu et al. (2004) to devel-
op a classifier for predicating the morphological 
structure of verbs. There are four types of mor-
phological structures for verbs: the coordinating 
structure (VV), the modifier-head structure (AV), 
the verb-complement structure (VR), and the 
verb-object structure (VO). To classify verbs 
automatically, we incorporate three features in 
the proposed classifier, namely, the lexeme itself, 
the prefix and the suffix, and the semantic types 
of the prefix and the suffix. Then, we use train-
ing data from the affix database to train the clas-
sifier. Table 3 shows an example of the unknown 
verb ???? /disseminate? and the morph-
structure classifier shows that it is a ?VR? type. 
 
Feature Feature Description 
Word=??? Lexicon 
PW=?? Prefix word 
PWST={disseminate|??} Semantic Type of 
Prefix Word ?? 
SW=? Suffix Word 
SWST={Vachieve|??} Semantic Type of 
Suffix Word ? 
 
Table 3. An example of an unknown verb and 
feature templates for morph-structure predication 
931
 
Semantic Type Provider: The system ex-
ploits WORD, PoS, affix and E-HowNet infor-
mation to obtain the semantic types of words (see 
Figure 2). If a word is known and its PoS is giv-
en, we can usually find its semantic type by 
searching the E-HowNet database. For an un-
known word, the semantic type of its head mor-
pheme is its semantic type; and the semantic type 
of the head morpheme is obtained from E-
HowNet1. For example, the unknown word ??
?? /disseminate?, its prefix word is ???
/disseminate? and we learn that its semantic type 
is {disseminate|??} from E-HowNet. There-
fore, we assign {disseminate|??} as the se-
mantic type of ???? /disseminate?. If the 
word or head morpheme does not exist in the 
affix database, we assign a general semantic type 
based on its PoS, e.g., nouns are {thing|??} 
and verbs are {act|??}. In this matching pro-
cedure, we may encounter multiple matching 
data of words and affixes. Our strategy is to keep 
the ambiguous semantic type for future pro-
cessing. 
 
Input: WORD, PoS 
Output: Semantic Type (ST) 
procedure STP(WORD, PoS) 
 (* Initial Step *) 
 ST := null; 
 (* Step 1: Known word *) 
 if WORD already in E-HowNet then 
  ST := EHowNet(WORD, PoS); 
 else if WORD in Affix database then 
  ST := EHowNet(affix of WORD, PoS); 
 (* Step 2 : Unknown word *) 
 if ST is null and PoS is ?Vt? then 
  ST := EHowNet(prefix of WORD, PoS);  
 else if ST is null and PoS is ?N? then 
  ST := EHowNet(suffix of WORD, PoS);  
 (* Step 3 : default *) 
 if ST is null and PoS is ?Vt? then 
  ST := ?act|???; 
 else if ST is null and PoS is ?N? then 
  ST := ?thing|??? 
 (* Finally *) 
 STP := ST; 
end; 
 
Figure 2. The Pseudo-code of the Semantic Type 
Predication Algorithm. 
 
1 The E-HowNet function in Figure 2 will return a null ST 
value where words do not exist in E-HowNet or Affix data-
base. 
3.3 Learning World Knowledge 
Based on the features discussed in the previous 
sub-section, we extract prior knowledge from 
Treebank to design the Vt-N classifier. However, 
the training suffers from the data sparseness 
problem. Furthermore most ambiguous Vt-N 
relations are resolved by common sense 
knowledge that makes it even harder to construct 
a well-trained system. An alternative way to ex-
tend world knowledge is to learn from large-
scale unlabeled data (Wu, 2003; Chen et al., 
2008; Yu et al., 2008). However, the unsuper-
vised approach accumulates errors caused by 
automatic annotation processes, such as word 
segmentation, PoS tagging, syntactic parsing, 
and semantic role assignment. Therefore, how to 
extract useful knowledge accurately is an im-
portant issue. 
To resolve the error accumulation problem, we 
propose two methods: unsupervised NP selection 
and supervised error correction. The NP selec-
tion method exploits the fact that an intransitive 
verb followed by a noun can only be interpreted 
as an NP structure, not a VP structure. It is easy 
to find such instances with high precision by 
parsing a large corpus. Based on the selection 
method, we can extend contextual knowledge 
about NP(V+N) and extract nouns that take ad-
jectival verbs as modifiers. The error correction 
method involves a small amount of manual edit-
ing in order to make the data more useful and 
reduce the number of errors in auto-extracted 
knowledge. The rationale is that, in general, high 
frequency Vt-N word-bigram is either VP or NP 
without ambiguity. Therefore, to obtain more 
accurate training data, we simply classify each 
high frequency Vt-N word bigram into a unique 
correct type without checking all of its instances. 
We provide more detailed information about the 
method in Section 4.3. 
4 Experiments and Results 
4.1 Experimental Setting 
We classify Vt-N structures into four types of 
syntactic structures by using the bracketed in-
formation (tree structure) and dependency rela-
tion (head-modifier) to extract the Vt-N relations 
from treebank automatically. The resources used 
in the experiments as follows. 
Treebank: The Sinica Treebank contains 
61,087 syntactic tree structures with 361,834 
words. We extracted 9,017 instances of Vt-N 
structures from the corpus. Then, we randomly 
                                                 
932
selected 1,000 of the instances as test data and 
used the remainder (8,017 instances) as training 
data. Labeled information of word segmentation 
and PoS-tagging were retained and utilized in the 
experiments. 
E-HowNet: E-HowNet contains 99,525 lexi-
cal semantic definitions that provide information 
about the semantic type of words. We also im-
plement the semantic type predication algorithm 
in Figure 2 to generate the semantic types of all 
Vt and N words, including unknown words. 
Affix Data: The database includes 13,287 ex-
amples of verbs and 27,267 examples of nouns, 
each example relates to an affix. The detailed 
statistics of the verb morph-structure categoriza-
tion are shown in Table 4. The data is used to 
train a classifier to predicate the morph-structure 
of verbs. We found that verbs with a conjunctive 
structure (VV) are more likely to play adjectival 
roles than the other three types of verbs. The 
classifier achieved 87.88% accuracy on 10-fold 
cross validation of the above 13,287 verbs. 
 
 VV VR AV VO 
Prefix 920 2,892 904 662 
Suffix 439 7,388 51 31 
 
Table 4. The statistics of verb morph-structure 
categorization 
 
Large Corpus: We used a Chinese parser to 
analyze sentence structures automatically. The 
auto-parsed tree structures are used in Experi-
ment 2 (described in the Sub-section 4.3). We 
obtained 1,262,420 parsed sentences and derived 
237,843 instances of Vt-N structure as our da-
taset (called as ASBC). 
4.2 Experiment 1: Evaluation of the Vt-N 
Classifier 
In this experiment, we used the Maximum En-
tropy Toolkit (Zhang, 2004) to develop the Vt-N 
classifier. Based on the features discussed in Sec-
tion 3.1, we designed five models to evaluate the 
classifier?s performance on different feature 
combinations.  
The features and used in each model are de-
scribed below. The feature values shown in 
brackets refer to the example in Figure 1. 
? M1 is the baseline model. It uses PoS-tag 
pairs as features, such as (t1=VC, t2=Na). 
? M2 extends the M1 model by adding con-
text features of (t-1=VK, t1=VC), (t2=Na, 
t3=DE), (t-2=Nep, t-1=VK, t1=VC), (t2=Na, 
t3=DE, t4=Na) and (t-1=VK, t3=DE). 
? M3 extends the M2 model by adding lexi-
con features of (w1=??, t1=VK, w2=?
?, t2=Na), (w1???, w2=??), (w1=?
?) and (w2=??). 
? M4 extends the M3 model by adding se-
mantic features of (st1=study|??, t1=VK , 
st2=language|?? , t2=Na), (st1=study|?
? , t1=VK) and (st2=language| ? ? , 
t2=Na). 
? M5 extends the M4 model by adding two 
features: the morph-structure of verbs; and 
the syllabic length of nouns 
(Vmorph=?VV?) and (Nlen=2). 
Table 5 shows the results of using different 
feature combinations in the models. The symbol 
P1(%) is the 10-fold cross validation accuracy of 
the training data, and the symbol P2(%) is the 
accuracy of the test data. By adding contextual 
features, the accuracy rate of M2 increases from 
59.10% to 72.30%. The result shows that contex-
tual information is the most important feature 
used to disambiguate VP, NP and independent 
structures. The accuracy of M2 is approximately 
the same as the result of our PCFG parser be-
cause both systems use contextual information. 
By adding lexical features (M3), the accuracy 
rate increases from 72.30% to 80.20%. For se-
mantic type features (M4), the accuracy rate in-
creases from 80.20% to 81.90%. The 1.7% in-
crease in the accuracy rate indicates that seman-
tic generalization is useful. Finally, in M5, the 
accuracy rate increases from 81.90% to 83.00%. 
The improvement demonstrates the benefits of 
using the verb morph-structure and noun length 
features. 
 
Models Feature for Vt-N P1(%) P2(%) 
M1 (t1,t2) 61.94 59.10 
M2 + (t-1,t1) (t2,t3) (t-2,t-
1,t1) (t2,t3,t4) (t-1,t3) 
76.59 72.30 
M3 + (w1,t1,w2,t2) (w1,w2) 
(w2) (w1) 
83.55 80.20 
M4 + (st1,t1,st2,t2) (st1,t1) 
(st2, t2) 
84.63 81.90 
M5 + (Vmorph) (Nlen) 85.01 83.00 
 
Table 5. The results of using different feature 
combinations 
 
933
Next, we consider the influence of unknown 
words on the Vt-N classifier. The statistics shows 
that 17% of the words in Treebank lack semantic 
type information, e.g., ??/StayIn, ??/fill, ?
?/posted, and ??/tied. The accuracy of the 
Vt-N classifier declines by 0.7% without seman-
tic type information for unknown words. In other 
words, lexical semantic information improves the 
accuracy of the Vt-N classifier. Regarding the 
problem of unknown morph-structure of words, 
we observe that over 85% of verbs with more 
than 2 characters are not found in the affix data-
base. If we exclude unknown words, the accura-
cy of the Vt-N prediction decreases by 1%. 
Therefore, morph-structure information has a 
positive effect on the classifier. 
4.3 Experiment 2: Using Knowledge Ob-
tained from Large-scale Unlabeled Data 
by the Selection and Correction Meth-
ods. 
In this experiment, we evaluated the two 
methods discussed in Section 3, i.e., unsuper-
vised NP selection and supervised error correc-
tion. We applied the data selection method (i.e., 
distance=1, with an intransitive verb (Vi) fol-
lowed by an object noun (Na)) to select 46,258 
instances from the ASBC corpus and compile a 
dataset called Treebank+ASBC-Vi-N. Table 6 
shows the performance of model 5 (M5) on the 
training data derived from Treebank and Tree-
bank+ASBC-Vi-N. The results demonstrate that 
learning more nouns that accept verbal modifiers 
improves the accuracy. 
 
 
Treebank+ 
ASBC-Vi-N 
Treebank 
size of training 
instances 
46,258 8,017 
M5 - P2(%) 83.90 83.00 
 
Table 6. Experiment results on the test data for 
various knowledge sources 
 
We had also try to use the auto-parsed results 
of the Vt-N structures from the ASBC corpus as 
supplementary training data for train M5. It de-
grades the model?s performance by too much 
error when using the supplementary training data. 
To resolve the problem, we utilize the supervised 
error correction method, which manually correct 
errors rapidly because high frequency instances 
(w1, w2) rarely have ambiguous classifications in 
different contexts. So we designed an editing tool 
to correct errors made by the parser in the classi-
fication of high frequency Vt-N word pairs. After 
the manual correction operation, which takes 40 
man-hours, we assign the correct classifications 
(w1, t1, w2, t2, rt) for 2,674 Vt-N structure types 
which contains 10,263 instances to creates the 
ASBC+Correction dataset. Adding the corrected 
data to the original training data increases the 
precision rate to 88.40% and reduces the number 
of errors by approximately 31.76%, as shown in 
the Treebank+ASBC+Correction column of Ta-
ble 7. 
 
 
Treebank+ 
ASBC+Correction 
Treebank+ 
ASBC-Vi-N 
Treebank 
size of train-
ing instances 
56,521 46,258 8,017 
M5 - P2(%) 88.40 83.90 83.00 
 
Table 7. Experiment results of classifiers with 
different training data 
 
We also used the precision and recall rates to 
evaluate the performance of the models on each 
type of relation. The results are shown in Table 8. 
Overall, the Treebank+ASBC+Correction meth-
od achieves the best performance in terms of the 
precision rate. The results for Treebank+ASBC-
Vi-N show that the unsupervised data selection 
method can find some knowledge to help identi-
fy NP structures. In addition, the proposed mod-
els achieve better precision rates than the PCFG 
parser. The results demonstrate that using our 
guidelines to design a disambiguation model to 
resolve the Vt-N problem is successful. 
 
 H/X X/H X/X 
Treebank 
R(%) 91.11 67.90 74.62 
P(%) 84.43 78.57 81.86 
Treebank+ 
ASBC-Vi-N 
R(%) 91.00 72.22 71.54 
P(%) 84.57 72.67 85.71 
Treebank+ 
ASBC+Correction 
R(%) 98.62 60.49 83.08 
P(%) 86.63 88.29 93.51 
PCFG 
R(%) 90.54 23.63 80.21 
P(%) 78.24 73.58 75.00 
 
Table 8. Performance comparison of different 
classification models. 
 
4.4 Experiment 3: Integrating the Vt-N 
classifier with the PCFG Parser 
Identifying Vt-N structures correctly facilitates 
statistical parsing, machine translation, infor-
934
mation retrieval, and text classification. In this 
experiment, we develop a baseline PCFG parser 
based on feature-based grammar representation 
by Hsieh et al. (2012) to find the best tree struc-
tures (T) of a given sentence (S). The parser then 
selects the best tree according to the evaluation 
score Score(T,S) of all possible trees. If there are 
n PCFG rules in the tree T, the Score(T,S) is the 
accumulation of the logarithmic probabilities of 
the i-th grammar rule (RPi). Formula 1 shows the 
baseline PCFG parser. 
 
?
=
=
n
i
iRPSTScore
1
)(),(  (1)
 
 
The Vt-N models can be easily integrated into 
the PCFG parser. Formula 2 represents the inte-
grated structural evaluation model. We combine 
RPi and VtNPi with the weights w1 and w2 re-
spectively, and set the value of w2 higher than 
that of w1. VtNPi is the probability produced by 
the Vt-N classifier for the type of the relation 
between Vt-N bigram determined by the PCFG 
parsing. The classifier is triggered when a [Vt, N] 
structure is encountered; otherwise, the Vt-N 
model is not processed. 
 
?
=
?+?=
n
i
ii VtNPwRPwSTScore
1
21 )(),(  (2)
 
 
The results of evaluating the parsing model in-
corporated with the Vt-N classifier (see Formula 
2) are shown in Table 9 and Table 10. The P2 is 
the accuracy of Vt-N classification on the test 
data. The bracketed f-score (BF2) is the parsing 
performance metric. Based on these results, the 
integrated model outperforms the PCFG parser in 
terms of Vt-N classification. Because the Vt-N 
classifier only considers sentences that contain 
Vt-N structures, it does not affect the parsing 
accuracies of other sentences.  
 
 
PCFG +  
M5 (Treebank) PCFG 
P2(%) 80.68 77.09 
BF(%) 83.64 82.80 
 
Table 9. The performance of the PCFG parser 
with and without model M5 from Treebank. 
 
2 The evaluation formula is (BP*BR*2) / (BP+BR), where 
BP is the precision and BR is the recall. 
 
PCFG +  
M5 (Treebank+ASBC+Correction) PCFG 
P2(%) 87.88 77.09 
BF(%) 84.68 82.80 
 
Table 10. The performance of the PCFG parser 
with and without model M5 from Tree-
bank+ASBC+Correction data set. 
 
4.5 Experiment 4: Comparison of Various 
Chinese Parsers 
In this experiment, we give some comparison 
results in various parser: ?PCFG Parser? (base-
line), ?CDM Parser? (Hsieh et al., 2012), and 
?Berkeley Parser? (Petrov et al., 2006). The CDM 
parser achieves the best score in Traditional Chi-
nese Parsing task of SIGHAN Bake-offs 2012 
(Tseng et al., 2012). Petrov?s parser (as Berkeley, 
version is 2009 1.1) is the best PCFG parser for 
non-English language and it is an open source. In 
our comparison, we use the same training data 
for training models and parse the same test da-
taset based on the gold standard word segmenta-
tion and PoS tags. We have already discussed the 
PCFG parser in Section 4.4. As for CDM parser, 
we retrain relevant model in our experiments. 
And since Berkeley parser take different tree 
structure (Penn Treebank format), we transform 
the experimental data to Berkeley CoNLL format 
and re-train a new model with parameters ?-
treebank CHINESE -SMcycles 4? 3 from training 
data. Moreover we use ?-useGoldPOS? parame-
ters to parse test data and further transform them 
to Sinica Treebank style from the Berkeley par-
ser?s results. The different tree structure formats 
of Sinica Treebank and Penn Treebank are as 
follow: 
 
Sinica Treebank:  
S(NP(Head:Nh:??)|Head:VC:??
|NP(Head:Na:??)) 
 
Penn Treebank:  
( (S (NP (Head:Nh (Nh ??))) (Head:VC 
(VC ??)) (NP (Head:Na (Na ??))))) 
 
The evaluation results on the testing data, i.e. 
in P2 metric, are as follows. The accuracy of 
PCFG parser is 77.09%; CDM parser reaches 
78.45% of accuracy; and Berkeley parser is 
70.68%. The results show that the problem of Vt-
3 The ?-treebank CHINESE -SMcycles 4? is the best train-
ing parameter in Traditional Chinese Parsing task of 
SIGHAN Bake-offs 2012. 
                                                 
                                                 
935
N cannot be well solved by any general parser 
including CDM parser and Berkeley?s parser. It 
is necessary to have a different approach aside 
from the general model. So we set the target for a 
better model for Vt-N classification which can be 
easily integrated into the existing parsing model. 
So far our best model achieved the P2 accuracy 
of 87.88%.  
5 Concluding Remarks 
We have proposed a classifier to resolve the am-
biguity of Vt-N structures. The design of the 
classifier is based on three important guidelines, 
namely, adopting linguistically motivated fea-
tures, using all available resources, and easy in-
tegration into parsing model. After analyzing the 
Vt-N structures, we identify linguistically moti-
vated features, such as lexical words, semantic 
knowledge, the morphological structure of verbs, 
neighboring parts-of-speech, and the syllabic 
length of words. Then, we design a classifier to 
verify the usefulness of each feature. We also 
resolve the technical problems that affect the 
prediction of the semantic types and morph-
structures of unknown words. In addition, we 
propose a framework for unsupervised data se-
lection and supervised error correction for learn-
ing more useful knowledge. Our experiment re-
sults show that the proposed Vt-N classifier sig-
nificantly outperforms the PCFG Chinese parser 
in terms of Vt-N structure identification. Moreo-
ver, integrating the Vt-N classifier with a parsing 
model improves the overall parsing performance 
without side effects. 
In our future research, we will exploit the pro-
posed framework to resolve other parsing diffi-
culties in Chinese, e.g., N-N combination. We 
will also extend the Semantic Type Predication 
Algorithm (Figure 2) to deal with all Chinese 
words. Finally, for real world knowledge learn-
ing, we will continue to learn more useful 
knowledge by auto-parsing to improve the pars-
ing performance. 
Acknowledgments 
We thank the anonymous reviewers for their val-
uable comments. This work was supported by 
National Science Council under Grant NSC99-
2221-E-001-014-MY3. 
Reference 
Li-li Chang, Keh-Jiann Chen, and Chu-Ren Huang. 
2000. Alternation Across Semantic Fields: A Study 
on Mandarin Verbs of Emotion. Internal Journal of 
Computational Linguistics and Chinese Language 
Processing (IJCLCLP), 5(1):61-80. 
Keh-Jiann Chen, Chu-Ren Huang, Chi-Ching Luo, 
Feng-Yi Chen, Ming-Chung Chang, Chao-Jan 
Chen, , and Zhao-Ming Gao. 2003. Sinica Tree-
bank: Design Criteria, Representational Issues and 
Implementation. In (Abeille 2003) Treebanks: 
Building and Using Parsed Corpora, pages 231-
248. Dordrecht, the Netherlands: Kluwer. 
Li-jiang Chen. 2008. Autolabeling of VN Combina-
tion Based on Multi-classifier. Journal of Comput-
er Engineering, 34(5):79-81. 
Wenliang Chen, Daisuke Kawahara, Kiyotaka 
Uchimoto, Yujjie Zhang, and Hitoshi Isahara. 2008. 
Dependency Parsig with Short Dependency Rela-
tions in Unlabeled Data.  In Proceedings of the 
third International Joint Conference on Natural 
Language Processing (IJCNLP). pages 88-94.. 
Chih-ming Chiu, Ji-Chin Lo, and Keh-Jiann Chen. 
2004. Compositional Semantics of Mandarin Affix 
Verbs. In Proceedings of the Research on Compu-
tational Linguistics Conference (ROCLING), pages 
131-139. 
Yu-Ming Hsieh, Ming-Hong Bai, Jason S. Chang, and 
Keh-Jiann Chen. 2012. Improving PCFG Chinese 
Parsing with Context-Dependent Probability Re-
estimation, In Proceedings of the Second CIPS-
SIGHAN Joint Conference on Chinese Language 
Processing, pages 216?221. 
Shu-Ling Huang, You-Shan Chung, Keh-Jiann Chen. 
2008. E-HowNet: the Expansion of HowNet. In 
Proceedings of the First National HowNet work-
shop, pages 10-22, Beijing, China. 
Chunhi Liu, Xiandai Hanyu Shuxing Fanchou Yianjiu 
(??????????). Chengdu: Bashu Books, 
2008. 
Jiaju Mei, Yiming Lan, Yunqi Gao, and Yongxian 
Ying. 1983. A Dictionary of Synonyms. Shanghai 
Cishu Chubanshe.  
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceesings of 
COLING/ACL, pages 433-400. 
Likun Qiu. 2005. Constitutive Relation Analysis for 
V-N Phrases. Journal of Chinese Language and 
Computing, 15(3):173-183. 
Richard Sproat and Thomas Emerson, 2003. The first 
International Chinese Word Segmentation Bakeoff. 
In Proceedings of the Second SIGHAN Workshop 
on Chinese Language Processing, pages 133-143. 
Honglin Sun and Dan Jurafsky. 2003. The Effect of 
Rhythm on Structural Disambiguation in Chinese. 
In Proceedings of the Second SIGHAN Workshop 
on Chinese Language Processing, pages 39-46. 
936
Yuen-Hsieh Tseng, Lung-Hao Lee, and Liang-Chih 
Yu. 2012. Tranditional Chinese Parsing Evaluation 
at SIGHAN Bake-offs 2012. In Proceedings of the 
Second CIPS-SIGHAN Joint Conference on Chi-
nese Language Processing, pages 199-205. 
Andi Wu. 2003. Learning Verb-Noun Relations to 
Improve Parsing. In Proceedings of the Second 
SIGHAN workshop on Chinese Language Pro-
cessing, pages 119-124. 
Kun Yu, Daisuke Kawahara, and Sadao Kurohashi. 
2008. Chinese Dependency Parsing with Large 
Scale Automatically Constructed Case Structures, 
In Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING2008), 
pages 1049-1056. 
Jun Zhao and Chang-ning Huang. 1999. The Com-
plex-feature-based Model for Acquisition of VN-
construction Structure Templates. Journal of Soft-
ware, 10(1):92-99. 
Le Zhang. 2004. Maximum Entropy Modeling 
Toolkit for Python and C++. Reference Manual. 
937
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 55?60,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
DOMCAT: A Bilingual Concordancer for Domain-Specific Computer 
Assisted Translation 
Ming-Hong Bai1,2 Yu-Ming Hsieh1,2 Keh-Jiann Chen1 Jason S. Chang2 
1 Institute of Information Science, Academia Sinica, Taiwan 
2 Department of Computer Science, National Tsing-Hua University, Taiwan 
mhbai@sinica.edu.tw, morris@iis.sinica.edu.tw, 
kchen@iis.sinica.edu.tw, jason.jschang@gmail.com 
 
Abstract 
In this paper, we propose a web-based 
bilingual concordancer, DOMCAT 1 , for 
domain-specific computer assisted 
translation. Given a multi-word expression 
as a query, the system involves retrieving 
sentence pairs from a bilingual corpus, 
identifying translation equivalents of the 
query in the sentence pairs (translation 
spotting) and ranking the retrieved sentence 
pairs according to the relevance between 
the query and the translation equivalents. 
To provide high-precision translation 
spotting for domain-specific translation 
tasks, we exploited a normalized 
correlation method to spot the translation 
equivalents. To ranking the retrieved 
sentence pairs, we propose a correlation 
function modified from the Dice coefficient 
for assessing the correlation between the 
query and the translation equivalents. The 
performances of the translation spotting 
module and the ranking module are 
evaluated in terms of precision-recall 
measures and coverage rate respectively. 
1 Introduction 
A bilingual concordancer is a tool that can retrieve 
aligned sentence pairs in a parallel corpus whose 
source sentences contain the query and the 
translation equivalents of the query are identified 
in the target sentences. It helps not only on finding 
translation equivalents of the query but also 
presenting various contexts of occurrence. As a 
result, it is extremely useful for bilingual 
                                                          
1 http://ckip.iis.sinica.edu.tw/DOMCAT/ 
lexicographers, human translators and second 
language learners (Bowker and Barlow 2004; 
Bourdaillet et al, 2010; Gao 2011).  
Identifying the translation equivalents, 
translation spotting, is the most challenging part of 
a bilingual concordancer. Recently, most of the 
existing bilingual concordancers spot translation 
equivalents in terms of word alignment-based 
method. (Jian et al, 2004; Callison-Burch et al, 
2005; Bourdaillet et al, 2010). However, word 
alignment-based translation spotting has some 
drawbacks. First, aligning a rare (low frequency) 
term may encounter the garbage collection effect 
(Moore, 2004; Liang et al, 2006) that cause the 
term to align to many unrelated words. Second, the 
statistical word alignment model is not good at 
many-to-many alignment due to the fact that 
translation equivalents are not always correlated in 
lexical level. Unfortunately, the above effects will 
be intensified in a domain-specific concordancer 
because the queries are usually domain-specific 
terms, which are mostly multi-word low-frequency 
terms and semantically non-compositional terms. 
Wu et al (2003) employed a statistical 
association criterion to spot translation equivalents 
in their bilingual concordancer. The association-
based criterion can avoid the above mentioned 
effects. However, it has other drawbacks in 
translation spotting task. First, it will encounter the 
contextual effect that causes the system incorrectly 
spot the translations of the strongly collocated 
context. Second, the association-based translation 
spotting tends to spot the common subsequence of 
a set of similar translations instead of the full 
translations. Figure 1 illustrates an example of 
contextual effect, in which ?Fan K'uan? is 
incorrectly spotted as part of the translation of the 
query term ?????? ? (Travelers Among 
Mountains and Streams), which is the name of the 
55
painting painted by ?Fan K'uan/?? ? since the 
painter?s name is strongly collocated with the 
name of the painting. 
 
Sung , Travelers Among Mountains and Streams , Fan 
K'uan 
???????? 
Figure 1. ?Fan K'uan? may be incorrectly spotted as 
part of the translation of ???????, if pure 
association method is applied. 
 
Figure 2 illustrates an example of common 
subsequence effect, in which ??????? (the 
River During the Qingming Festival/ Up the River 
During Qingming) has two similar translations as 
quoted, but the Dice coefficient tends to spot the 
common subsequences of the translations. 
(Function words are ignored in our translation 
spotting.) 
 
Expo 2010 Shanghai-Treasures of Chinese Art Along 
the River During the Qingming Festival 
2010?????????????????? 
Oversized Hanging Scrolls and Handscrolls Up the 
River During Qingming 
???????????? 
Figure 2. The Dice coefficient tends to spot the common 
subsequences ?River During Qingming?. 
Bai et al (2009) proposed a normalized 
frequency criterion to extract translation 
equivalents form sentence aligned parallel corpus. 
This criterion takes lexical-level contexture effect 
into account, so it can effectively resolve the above 
mentioned effect. But the goal of their method is to 
find most common translations instead of spotting 
translations, so the normalized frequency criterion 
tends to ignore rare translations. 
In this paper, we propose a bilingual 
concordancer, DOMCAT, for computer assisted 
domain-specific term translation. To remedy the 
above mentioned effects, we extended the 
normalized frequency of Bai et al (2009) to a 
normalized correlation criterion to spot translation 
equivalents. The normalized correlation inherits 
the characteristics of normalized frequency and is 
adjusted for spotting rare translations. These 
characteristics are especially important for a 
domain-specific bilingual concordancer to spot 
translation pairs of low-frequency and semantically 
non-compositional terms.  
The remainder of this paper is organized as 
follows.  Section 2 describes the DOMCAT system. 
In Section 3, we describe the evaluation of the 
DOMCAT system. Section 4 contains some 
concluding remarks. 
2 The DOMCAT System 
Given a query, the DOMCAT bilingual 
concordancer retrieves sentence pairs and spots 
translation equivalents by the following steps: 
 
1. Retrieve the sentence pairs whose source 
sentences contain the query term. 
2. Extract translation candidate words from the 
retrieved sentence pairs by the normalized 
correlation criterion. 
3. Spot the candidate words for each target 
sentence and rank the sentences by 
normalized the Dice coefficient criterion. 
 
In step 1, the query term can be a single word, a 
phrase, a gapped sequence and even a regular 
expression. The parallel corpus is indexed by the 
suffix array to efficiently retrieve the sentences.  
The step 2 and step 3 are more complicated and 
will be described from Section 2.1 to Section 2.3. 
2.1 Extract Translation Candidate Words 
After the queried sentence pairs retrieved from the 
parallel corpus, we can extract translation 
candidate words from the sentence pairs. We 
compute the local normalized correlation with 
respect to the query term for each word e in each 
target sentence. The local normalized correlation 
is defined as follows: 
 
?
?
??
??
??
???
f
q
f
qfeq
j
i
f j
f i
fep
fepelnc ||)|(
||)|(),,;(      (1) 
 
where q denotes the query term, f denotes the 
source sentence and e denotes the target sentence,  
? is a small smoothing factor. The probability p(e|f) 
is the word translation probability derived from the 
entire parallel corpus by IBM Model 1 (Brown et 
al., 1993). The sense of local normalized 
correlation of e can be interpreted as the 
probability of word e being part of translation of 
the query term q under the condition of sentence 
pair (e, f). 
56
Once the local normalized correlation is 
computed for each word in retrieved sentences, we 
compute the normalized correlation on the 
retrieved sentences. The normalized correlation is 
the average of all lnc values and defined as follows:  
 
?
?
?
n
i
iielncnenc 1
)()( ),,;(1);( feqq            (2) 
 
where n is the number of retrieved sentence pairs.  
After the nc values for the words of the retrieved 
target sentences are computed, we can obtain a 
translation candidate list by filtering out the words 
with lower nc values. 
To compare with the association-based method, 
we also sorted the word list by the Dice coefficient 
defined as follows: 
 
)()(
),(2),( q
qq freqefreq
efreqedice ??           (3) 
 
where freq is frequency function which  computes 
frequencies from the parallel corpus. 
 
Candidate words NC 
mountain 0.676 
stream 0.442 
traveler 0.374 
among 0.363 
sung 0.095 
k'uan 0.090 
Figure 3(a). Candidate words sorted by nc values. 
 
Candidate words Dice 
traveler 0.385 
reduced 0.176 
stream 0.128 
k'uan 0.121 
fan 0.082 
among 0.049 
mountain 0.035 
Figure 3(b). Candidate words sorted by Dice coefficient 
values. 
 
Figure 3(a) and (b) illustrate examples of 
translation candidate words of the query term ??
???? ? (Travelers Among Mountains and 
Streams) sorted by the nc values, NC, and the Dice 
coefficients respectively. The result shows that the 
normalized correlation separated the related words 
from unrelated words much better than the Dice 
coefficient. 
The rationale behind the normalized correlation 
is that the nc value is the strength of word e 
generated by the query compared to that of 
generated by the whole sentence. As a result, the 
normalized correlation can easily separate the 
words generated by the query term from the words 
generated by the context. On the contrary, the Dice 
coefficient counts the frequency of a co-occurred 
word without considering the fact that it could be 
generated by the strongly collocated context.  
 
2.2 Translation Spotting 
Once we have a translation candidate list and 
respective nc values, we can spot the translation 
equivalents by the following spotting algorithm. 
For each target sentence, first, spot the word with 
highest nc value. Then extend the spotted sequence 
to the neighbors of the word by checking their nc 
values of neighbor words but skipping function 
words. If the nc value is greater than a threshold ?, 
add the word into spotted sequence. Repeat the 
extending process until no word can be added to 
the spotted sequence. 
The following is the pseudo-code for the 
algorithm: 
 
S is the target sentence 
H is the spotted word sequence 
?is the threshold of translation candidate words 
 
Initialize: 
H? ?
emax?S[0] Foreach ei in S: If nc(ei) > nc(emax):  
emax ??ei 
If nc(emax )??:?
add?emax?to?H 
Repeat until no word add to H 
ej?left?neighbor?of?H?
If?nc(ej?)??:?
? ???add?ej?to?H?
ek?right?neighbor?of?H?
If nc(?ek?)???:?
? ???add?ek?to?H?
Figure 4: Pseudo-code of translation spotting process. 
 
57
2.3 Ranking 
The ranking mechanism of a bilingual 
concordancer is used to provide the most related 
translation of the query on the top of the outputs 
for the user. So, an association metric is needed to 
evaluate the relations between the query and the 
spotted translations. The Dice coefficient is a 
widely used measure for assessing the association 
strength between a multi-word expression and its 
translation candidates. (Kupiec, 1993; Smadja et 
al., 1996; Kitamura and Matsumoto, 1996; 
Yamamoto and Matsumoto, 2000; Melamed, 2001)  
The following is the definition of the Dice 
coefficient: 
 
)()(
),(2),( qt
qtqt freqfreq
freqdice ??            (4) 
 
where q denotes a multi-word expression to be 
translated, t denotes a translation candidate of q. 
However, the Dice coefficient has the common 
subsequence effect (as mentioned in Section 1) due 
to the fact that the co-occurrence frequency of the 
common subsequence is usually larger than that of 
the full translation; hence, the Dice coefficient 
tends to choose the common subsequence. 
To remedy the common subsequence effect, we 
introduce a normalized frequency for a spotted 
sequence defined as follows: 
 
?
?
?
n
i
iilnfnf
1
)()( ),,;(),( feqtqt            (5) 
 
where lnf is a function which compute normalized 
frequency locally in each sentence. The following 
is the definition of lnf: 
 
?
???
??
tH
feqfeqt
e
elnclnf )),,;(1(),,;(      (6) 
 
where H is the spotted sequence of the sentence 
pair (e,f), H-t are the words in H but not in t. The 
rationale behind lnf function is that: when counting 
the local frequency of t in a sentence pair, if t is a 
subsequence of H, then the count of t should be 
reasonably reduced by considering the strength of 
the correlation between the words in H-t and the 
query. 
Then, we modify the Dice coefficient by 
replacing the co-occurrence frequency with 
normalized frequency as follows: 
 
)()(
),(2),( qt
qtqt freqfreq
nfnf_dice ??        (7) 
 
The new scoring function, nf_dice(t,q), is 
exploited as our criterion for assessing the 
association strength between the query and the 
spotted sequences. 
3 Experimental Results 
3.1 Experimental Setting 
We use the Chinese/English web pages of the 
National Palace Museum 2  as our underlying 
parallel corpus. It contains about 30,000 sentences 
in each language. We exploited the Champollion 
Toolkit (Ma et al, 2006) to align the sentence pairs. 
The English sentences are tokenized and 
lemmatized by using the NLTK (Bird and Loper, 
2004) and the Chinese sentences are segmented by 
the CKIP Chinese segmenter (Ma and Chen, 2003). 
To evaluate the performance of the translation 
spotting, we selected 12 domain-specific terms to 
query the concordancer. Then, the returned spotted 
translation equivalents are evaluated against a 
manually annotated gold standard in terms of recall 
and precision metrics. We also build two different 
translation spotting modules by using the GIZA++ 
toolkit (Och and Ney, 2000) with the 
intersection/union of the bidirectional word 
alignment as baseline systems. 
To evaluate the performance of the ranking 
criterion, we compiled a reference translation set 
for each query by collecting the manually 
annotated translation spotting set and selecting 1 to 
3 frequently used translations. Then, the outputs of 
each query are ranked by the nf_dice function and 
evaluated against the reference translation set. We 
also compared the ranking performance with the 
Dice coefficient. 
3.2 Evaluation of Translation Spotting 
We evaluate the translation spotting in terms of the 
Recall and Precision metrics defined as follows: 
 
                                                          
2 http://www.npm.gov.tw 
58
||
||
1
)(
1
)()(
?
?
?
? ?? n
i
i
g
n
i
ii
g
H
HHRecall                     (8) 
||
||
1
)(
1
)()(
?
?
?
? ?? n
i
i
n
i
ii
g
H
HHPrecision                     (9) 
 
where i denotes the index of the retrieved 
sentence, )(iH  is the spotted sequences of the ith 
sentence returned by the concordancer,  and )(igH is 
the gold standard spotted sequences of the ith 
sentence. Table 1 shows the evaluation of 
translation spotting for normalized correlation, NC, 
compared with the intersection and union of 
GIZA++ word alignment. The F-score of the 
normalized correlation is much higher than that of 
the word alignment methods. It is noteworthy that 
the normalized correlation increased the recall rate 
without losing the precision rate. This may indicate 
that the normalized correlation can effectively 
conquer the drawbacks of the word alignment-
based translation spotting and the association-
based translation spotting mentioned in Section 1. 
 
 Recall Precision F-score 
Intersection 0.4026 0.9498 0.5656 
Union 0.7061 0.9217 0.7996 
NC 0.8579 0.9318 0.8933 
Table 1. Evaluation of the translation spotting 
queried by 12 domain-specific terms. 
 
We also evaluate the queried results of each 
term individually (as shown in Table 2). As it 
shows, the normalized correlation is quite stable 
for translation spotting. 
 
Query terms GIZA Intersection GIZA Union NC R P F R P F R P F 
??? (Maogong cauldron) 0.27 0.86 0.41 0.87 0.74 0.80  0.92 0.97 0.94 
????(Jadeite cabbage) 0.48 1.00 0.65 1.00 0.88 0.94  0.98 0.98 0.98 
?????(Travelers Among Mountains and Streams) 0.28 0.75 0.41 1.00 0.68 0.81 0.94 0.91 0.92
?????(Up the River During Qingming) 0.22 0.93 0.35 0.97 0.83 0.89  0.99 0.91 0.95
???(Ching-te-chen) 0.50 0.87 0.63 0.73 0.31 0.44 1.00 0.69 0.82
??(porcelain) 0.53 0.99 0.69 0.93 0.64 0.76 0.78 0.96 0.86
??(cobalt blue glaze) 0.12 1.00 0.21 0.85 0.58 0.69 0.94 0.86 0.90
??(inscription) 0.20 0.89 0.32 0.71 0.34 0.46  0.88 0.95 0.91
????(Three Friends and a Hundred Birds) 0.58 0.99 0.73 1.00 0.97 0.99 1.00 0.72 0.84
??(wild cursive script) 0.42 1.00 0.59 0.63 0.80 0.71 0.84 1.00 0.91
???(Preface to the Orchid Pavilion Gathering) 0.33 0.75 0.46 0.56 0.50 0.53 0.78 1.00 0.88
????(Latter Odes to the Red Cliff) 0.19 0.50 0.27 0.75 0.46 0.57 0.94 0.88 0.91
Table 2. Evaluation of the translation spotting for each term
3.3 Evaluation of Ranking 
To evaluate the performance of a ranking function, 
we ranked the retrieved sentences of the queries by 
the function. Then, the top-n sentences of the 
output are evaluated in terms of the coverage rate 
defined as follows: 
?coverage  
queries of #
top-nin on  translatia findcan  queries of #   (10) 
 
The meaning of the coverage rate can be 
interpreted as: how many percent of the query can 
find an acceptable translation in the top-n results.  
We use the reference translations, as described in 
Section 3.1, as acceptable translation set for each 
query of our experiment. Table 3 shows the 
coverage rate of the nf_dice function compared 
with the Dice coefficient. As it shows, in the 
outputs ranked by the Dice coefficient, uses 
usually have to look up more than 3 sentences to 
find an acceptable translation; while in the outputs 
ranked by the nf_dice function, users can find an 
acceptable translation in top-2 sentences. 
 
59
 
 dice nf_dice 
top-1 0.42  0.92 
top-2 0.75  1.00 
top-3 0.92  1.00 
Table 3. Evaluation of the ranking criteria. 
4 Conclusion and Future Works 
In this paper, we proposed a bilingual 
concordancer, DOMCAT, designed as a domain-
specific computer assisted translation tool. We 
exploited a normalized correlation which 
incorporate lexical level information into 
association-based method that effectively avoid the 
drawbacks of the word alignment-based translation 
spotting as well as the association-based translation 
spotting. 
In the future, it would be interesting to extend 
the parallel corpus to the internet to retrieve more 
rich data for the computer assisted translation. 
References  
Bai, Ming-Hong, Jia-Ming You, Keh-Jiann Chen, Jason 
S. Chang. 2009. Acquiring Translation Equivalences 
of Multiword Expressions by Normalized Correlation 
Frequencies. In Proceedings of EMNLP, pages 478-
486. 
Bird, Steven and Edward Loper. 2004. NLTK: The 
Natural Language Toolkit. In Proceedings of ACL, 
pages 214-217. 
Bourdaillet, Julien, St?phane Huet, Philippe Langlais 
and Guy Lapalme. 2010. TRANSSEARCH: from a 
bilingual concordancer to a translation finder. 
Machine Translation, 24(3-4): 241?271. 
Bowker, Lynne, Michael Barlow. 2004. Bilingual 
concordancers and translation memories: A 
comparative evaluation. In Proceedings of the 
Second International Workshop on Language 
Resources for Translation Work, Research and 
Training , pages. 52-61. 
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2):263-311. 
Callison-Burch, Chris, Colin Bannard and Josh 
Schroeder. 2005. A Compact Data Structure for 
Searchable Translation Memories. In Proceedings of 
EAMT. 
Gao, Zhao-Ming. 2011. Exploring the effects and use of 
a Chinese?English parallel concordancer. Computer-
Assisted Language Learning 24.3 (July 2011): 255-
275. 
Jian, Jia-Yan, Yu-Chia Chang and Jason S. Chang. 2004. 
TANGO: Bilingual Collocational Concordancer. In 
Proceedings of ACL, pages 166-169. 
Kitamura, Mihoko and Yuji Matsumoto. 1996. 
Automatic Extraction of Word Sequence 
Correspondences in Parallel Corpora. In Proceedings 
of WVLC-4 pages 79-87. 
Kupiec, Julian. 1993. An Algorithm for Finding Noun 
Phrase Correspondences in Bilingual Corpora. In 
Proceedings of ACL, pages 17-22. 
Liang, Percy, Ben Taskar, Dan Klein. 2006. Alignment 
by Agreement. In Proceedings of HLT-NAACL 2006, 
pages 104-111, New York, USA. 
Ma, Wei-Yun and Keh-Jiann Chen. 2003. Introduction 
to CKIP Chinese word segmentation system for the 
first international Chinese word segmentation 
bakeoff. In Proceedings of the second SIGHAN 
workshop on Chinese language processing, pages 
168-171. 
Ma, Xiaoyi. 2006. Champollion: A Robust Parallel Text 
Sentence Aligner. In Proceedings of the Fifth 
International Conference on Language Resources 
and Evaluation. 
Melamed, Ilya Dan. 2001. Empirical Methods for 
Exploiting parallel Texts. MIT press. 
Moore, Robert C. 2004. Improving IBM Word-
Alignment Model 1. In Proceedings of ACL, pages 
519-526, Barcelona, Spain. 
Och, Franz J., Hermann Ney., 2000, Improved 
Statistical Alignment Models, In Proceedings of ACL, 
pages 440-447. Hong Kong. 
Smadja, Frank, Kathleen R. McKeown, and Vasileios 
Hatzivassiloglou. 1996. Translating Collocations for 
Bilingual Lexicons: A Statistical Approach. 
Computational Linguistics, 22(1):1-38. 
Wu, Jian-Cheng, Kevin C. Yeh, Thomas C. Chuang, 
Wen-Chi Shei, Jason S. Chang.  2003. TotalRecall: A 
Bilingual Concordance for Computer Assisted 
Translation and Language Learning. In Proceedings 
of ACL, pages 201-204. 
Yamamoto, Kaoru, Yuji Matsumoto. 2000. Acquisition 
of Phrase-level Bilingual Correspondence using 
Dependency Structure. In Proceedings of COLING, 
pages 933-939. 
60
