Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 643?654, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Concurrent Acquisition of Word Meaning and Lexical Categories
Afra Alishahi
a.alishahi@uvt.nl
Communication and Information Sciences
Tilburg University, The Netherlands
Grzegorz Chrupa?a
gchrupala@lsv.uni-saarland.de
Spoken Language Systems
Saarland University, Germany
Abstract
Learning the meaning of words from ambigu-
ous and noisy context is a challenging task for
language learners. It has been suggested that
children draw on syntactic cues such as lexical
categories of words to constrain potential ref-
erents of words in a complex scene. Although
the acquisition of lexical categories should be
interleaved with learning word meanings, it
has not previously been modeled in that fash-
ion. In this paper, we investigate the inter-
play of word learning and category induction
by integrating an LDA-based word class learn-
ing module with a probabilistic word learning
model. Our results show that the incremen-
tally induced word classes significantly im-
prove word learning, and their contribution is
comparable to that of manually assigned part
of speech categories.
1 Learning the Meaning of Words
For young learners of a natural language, mapping
each word to its correct meaning is a challenging
task. Words are often used as part of an utterance
rather than in isolation. The meaning of an utter-
ance must be inferred from among numerous pos-
sible interpretations that the (usually complex) sur-
rounding scene offers. In addition, the linguistic and
visual context in which words are heard and used
is often noisy and highly ambiguous. Particularly,
many words in a language are polysemous and have
different meanings.
Various learning mechanisms have been proposed
for word learning. One well-studied mechanism
is cross-situational learning, a bottom-up strategy
based on statistical co-occurrence of words and ref-
erents across situations (Quine 1960, Pinker 1989).
Several experimental studies have shown that adults
and children are sensitive to cross-situational evi-
dence and use this information for mapping words to
objects, actions and properties (Smith and Yu 2007,
Monaghan and Mattock 2009). A number of com-
putational models have been developed based on this
principle, demonstrating that cross-situational learn-
ing is a powerful and efficient mechanism for learn-
ing the correct mappings between words and mean-
ings from noisy input (e.g. Siskind 1996, Yu 2005,
Fazly et al010).
Another potential source of information that can
help the learner to constrain the relevant aspects of a
scene is the sentential context of a word. It has been
suggested that children draw on syntactic cues pro-
vided by the linguistic context in order to guide word
learning, a hypothesis known as syntactic bootstrap-
ping (Gleitman 1990). There is substantial evidence
that children are sensitive to the structural regular-
ities of language from a very young age, and that
they use these structural cues to find the referent of
a novel word (e.g. Naigles and Hoff-Ginsberg 1995,
Gertner et al006). In particular, young children
have robust knowledge of some of the abstract lexi-
cal categories such as nouns and verbs (e.g. Gelman
and Taylor 1984, Kemp et al005).
Recent studies have examined the interplay of
cross-situational learning and sentence-level learn-
ing mechanisms, showing that adult learners of an
artificial language can successfully and simultane-
ously apply cues and constraints from both sources
of information when mapping words to their refer-
ents (Gillette et al999, Lidz et al010, Koehne
and Crocker 2010; 2011). Several computational
models have also investigated this interaction by
adding manually annotated part-of-speech tags as
643
input to word learning algorithms, and suggesting
that integration of lexical categories can boost the
performance of a cross-situational model (Yu 2006,
Alishahi and Fazly 2010).
However, none of the existing experimental or
computational studies have examined the acquisition
of word meanings and lexical categories in paral-
lel. They all make the simplifying assumption that
prior to the onset of word learning, the categoriza-
tion module has already formed a relatively robust
set of lexical categories. This assumption can be jus-
tified in the case of adult learners of a second or ar-
tificial language. But children?s acquisition of cate-
gories is most probably interleaved with the acquisi-
tion of word meaning, and these two processes must
ultimately be studied simultaneously.
In this paper, we investigate concurrent acquisi-
tion of word meanings and lexical categories. We
use an online version of the LDA algorithm to
induce a set of word classes from child-directed
speech, and integrate them into an existing prob-
abilistic model of word learning which combines
cross-situational evidence with cues from lexical
categories. Through a number of simulations of a
word learning scenario, we show that our automat-
ically and incrementally induced categories signifi-
cantly improve the performance of the word learning
model, and are closely comparable to a set of gold-
standard, manually-annotated part of speech tags.
2 A Word Learning Model
We want to investigate whether lexical categories
(i.e. word classes) that are incrementally induced
from child-directed speech can improve the perfor-
mance of a cross-situational word learning model.
For this purpose, we use the model of Alishahi and
Fazly (2010). This model uses a probabilistic learn-
ing algorithm for combining evidence from word?
referent co-occurrence statistics and the meanings
associated with a set of pre-defined categories. They
use child-directed utterances, manually annotated
with a small set of part of speech tags, from the
Manchester corpus (Theakston et al001) in the
CHILDES database (MacWhinney 1995). Their ex-
perimental results show that integrating these gold-
standard categories into the algorithm boosts its per-
formance over a pure cross-situational version.
The model of Alishahi and Fazly (2010) has the
suitable architecture for our goal: it provides an in-
tegrated learning mechanism which combines evi-
dence from word-referent co-occurrence with cues
from the meaning representation associated with
word categories. However, the model has two ma-
jor shortcomings. First, it assumes that lexical cate-
gories are formed and finalized prior to the onset of
word learning and that a correct and unique category
for a target word can be identified at each point in
time, assumptions that are highly unlikely. Second,
it does not handle any ambiguity in the meaning of
a word. Instead, each word is assumed to have only
one correct meaning. Considering the high level of
lexical ambiguity in most natural languages, this as-
sumption unreasonably simplifies the word learning
problem.
To investigate the plausibility of integrating word
and category learning, we use an online algorithm
for automatically and incrementally inducing a set
of lexical categories. Moreover, we use each word in
its original form instead of lemmatizing them, which
implies that categories contain different morpholog-
ical forms of the same word. By applying these
changes, we are able to study the contribution of lex-
ical categories to word learning in a more realistic
scenario.
Representation of input. The input to the model
consists of a sequence of utterances, each paired
with a representation of an observed scene. We rep-
resent an utterance as a set of words, U = {w}
(e.g. {she, went, home, ...}), and the corresponding
scene as a set of semantic features, S = {f} (e.g.
{ANIMATE, HUMAN, FEMALE, ...}).
Word and category meaning. We represent the
meaning of a word as a time-dependent probability
distribution p(t)(?|w) over all the semantic features,
where p(t)(f |w) is the probability of feature f be-
ing associated with word w at time t. In the absence
of any prior knowledge, the model assumes a uni-
form distribution over all features as the meaning of
a novel word. Also, a function cat(t)(w) gives us
the category to which a word w in utterance U (t) be-
longs.
At each point in time, a category c contains a set
of word tokens. We assign a meaning to each cat-
644
egory as a weighted sum of the meaning learned
so far for each of its members, or p(t)(f |c) =
(1/|c|)
?
w?c p
(t)(f |w), where |c| is the number of
word tokens in c at the current moment.
Learning algorithm. Given an utterance-scene pair
(U (t), S(t)) received at time t, the model first calcu-
lates an alignment score a for each word w ? U (t)
and each semantic feature f ? S(t). A semantic fea-
ture can be aligned to a word according to the mean-
ing acquired for that word from previous observa-
tions (word-based alignment, or aw). Alternatively,
distributional clues of the word can be used to de-
termine its category, and the semantic features can
be aligned to the word according to the meaning as-
sociated to its category (category-based alignment,
or ac). We combine these two sources of evidence
when estimating an alignment score:
a(w|f, U (t), S(t)) = ?(w)? aw(w|f, U
(t), S(t)) (1)
+(1? ?(w))? ac(w|f, U
(t), S(t))
where the word-based and category-based alignment
scores are estimated based on the acquired meanings
of the word and its category, respectively:
aw(w|f, U
(t), S(t)) =
p(t?1)(f |w)
?
wk?U(t)
p(t?1)(f |wk)
ac(w|f, U
(t), S(t)) =
p(t?1)(f |cat(w))
?
wk?U(t)
p(t?1)(f |cat(wk))
The relative contribution of the word-based versus
the category-based alignment is determined by the
weight function ?(w). Cross-situational evidence
is a reliable cue for frequent words; on the other
hand, the category-based score is most informative
when the model encounters a low-frequency word
(See Alishahi and Fazly (2010) for a full analysis of
the frequency effect). Therefore, we define ?(w) as
a function of the frequency of the word n(w):
?(w) = n(w)/(n(w) + 1)
Once an alignment score is calculated for each
word w ? U (t) and each feature f ? S(t), the model
revises the meanings of all the words in U (t) and
their corresponding categories as follows:
assoc(t)(w, f) = assoc(t?1)(w, f) + a(w|f,U(t),S(t))
where assoc(t?1)(w, f) is zero if w and f have not
co-occurred before. These association scores are
then used to update the meaning of the words in the
current input:
p(t)(f |w) =
assoc(t)(f, w)
?
fj?F
assoc(t)(fj , w)
(2)
where F is the set of all features seen so far. We use
a smoothed version of this formula to accommodate
noisy or rare input. This process is repeated for all
the input pairs, one at a time.
Uniform categories. Adding the category-based
alignment as a new factor to Eqn. (1) might im-
ply that the role of categories in this model is noth-
ing more than smoothing the cross-situational-based
alignment of words and referents. In order to in-
vestigate this issue, we use the following alignment
formula as an informed baseline in our experiments,
where we replace ac(?|f, U (t), S(t)) with a uniform
distribution:1
a(w|f, U (t), S(t)) = ?(w)? aw(w|f, U
(t), S(t)) (3)
+(1? ?(w))?
1
|U (t)|
where aw(w|f, U (t), S(t)) and ?(w) are estimated as
before. In our experiments in Section 4, we refer to
this baseline as the ?uniform? condition.
3 Online induction of word classes with
LDA
Empirical findings suggest that young children form
their knowledge of abstract categories, such as
verbs, nouns, and adjectives, gradually (e.g. Gel-
man and Taylor 1984, Kemp et al005). In ad-
dition, several unsupervised computational mod-
els have been proposed for inducing categories of
words which resemble part-of-speech categories, by
1We thank an anonymous reviewers for suggesting this con-
dition as an informed baseline.
645
drawing on distributional properties of their con-
text (see for example Redington et al998, Clark
2000, Mintz 2003, Parisien et al008, Chrupa?a
and Alishahi 2010). However, explicit accounts of
how such categories can be integrated in a cross-
situational model of word learning have been rare.
Here we adopt an online version of the model pro-
posed in Chrupa?a (2011), a method of soft word
class learning using Latent Dirichlet Allocation. The
approach is much more efficient than the commonly
used alternative (Brown clustering, (Brown et al
1992)) while at the same time matching or outper-
forming it when the word classes are used as au-
tomatically learned features for supervised learning
of various language understanding tasks. Here we
adopt this model as our approach to learning lexical
categories.
In Section 3.1 we describe the LDA model for
word classes; in Section 3.2 we discuss the online
Gibbs sampler we use for inference.
3.1 Word class learning with LDA
Latent Dirichlet Allocation (LDA) was introduced
by Blei et al2003) and is most commonly used
for modeling the topic structure in document collec-
tions. It is a generative, probabilistic hierarchical
Bayesian model that induces a set of latent variables,
which correspond to the topics. The topics them-
selves are multinomial distributions over words.
The generative structure of the LDA model is the
following:
?k ? Dirichlet(?), k ? [1,K]
?d ? Dirichlet(?), d ? [1, D]
znd ? Categorical(?d), nd ? [1, Nd]
wnd ? Categorical(?znd ), nd ? [1, Nd]
(4)
Chrupa?a (2011) reinterprets the LDA model in
terms of word classes as follows: K is the number
of classes, D is the number of unique word types,
Nd is the number of context features (such as right or
left neighbor) associated with word type d, znd is the
class of word type d in the nthd context, andwnd is the
nthd context feature of word type d. Hyperparameters
? and ? control the sparseness of the vectors ?d and
?k.
Wordtype Features
How doR
do HowL youR youL
you doL doR
Table 1: Matrix of context features
1.8M words (CHILDES) 100M words (BNC)
train car can will
give bring June March
shoes clothes man woman
book hole black white
monkey rabbit business language
Table 2: Most similar word pairs
As an example consider the small corpus consist-
ing of the single sentence How do you do. The rows
in Table 1 show the features w1 . . . wNd for each
word type d if we use each word?s left and right
neighbors as features, and subscript words with L
and R to indicate left and right.
After inference, the ?d parameters correspond to
word class probability distributions given a word
type while the ?k correspond to feature distributions
given a word class: the model provides a probabilis-
tic representation for word types independently of
their context, and also for contexts independently of
the word type.
Probabilistic, soft word classes are more expres-
sive than hard categories. First, they make it
easy and efficient to express shared ambiguities:
Chrupa?a (2011) gives an example of words used
as either first names or surnames, and this shared
ambiguity is reflected in the similarity of their word
class distributions. Second, with soft word classes it
becomes easy to express graded similarity between
words: as an example, Table 2 shows a random se-
lection out of the 100 most similar word pairs ac-
cording to the Jensen-Shannon divergence between
their word class distributions, according to a word
class model with 25 classes induced from (i) 1.8 mil-
lion words of the CHILDES corpus or (ii) 100 mil-
lion word of the BNC corpus. The similarities were
measured between each of the 1000 most frequent
CHILDES or BNC words.
646
3.2 Online Gibbs sampling for LDA
There have been a number of attempts to develop
online inference algorithms for topic modeling with
LDA. A simple modification of the standard Gibbs
sampler (o-LDA) was proposed by Song et al
(2005) and Banerjee and Basu (2007).
Canini et al2009) experiment with three sam-
pling algorithms for online topic inference: (i) o-
LDA, (ii) incremental Gibbs sampler, and (iii) a par-
ticle filter. Only o-LDA is truly online in the sense
that it does not revisit previously seen documents.
The other two, the incremental Gibbs sampler and
the particle filter, keep seen documents and periodi-
cally resample them. In Canini et al experiments
all of the online algorithms perform worse than the
standard batch Gibbs sampler on a document clus-
tering task.
Hoffman et al2010) develop an online version
of the variational Bayes (VB) optimization method
for inference for topic modeling with LDA. Their
method achieves good empirical results compared
to batch VB as measured by perplexity on held-
out data, especially when used with large minibatch
sizes.
Online VB for LDA is appropriate when stream-
ing documents: with online VB documents are rep-
resented as word count tables. In our scenario where
we apply LDA to modeling word classes we need to
process context features from sentences arriving in
a stream: i.e. we need to sample entries from a ta-
ble like Table 1 in order of arrival rather than row
by row. This means that online VB is not directly
applicable to online word-class induction.
However it also means that one issue with o-LDA
identified by Canini et al2009) is ameliorated.
When sampling in a topic modeling setting, docu-
ments are unique and are never seen again. Thus,
the topics associated with old documents get stale
and need to be periodically rejuvenated (i.e. resam-
pled). This is the reason why the incremental Gibbs
sampler and the particle filter algorithms in Canini
et al2009) need to keep old documents around and
cannot run in a true online fashion. Since for word
class modeling we stream context features as they
arrive, we will continue to see features associated
with the seen word types, and will automatically re-
sample their class assignments. In exploratory ex-
periments we have seen that this narrows the per-
formance gap between the o-LDA sampler and the
batch collapsed Gibbs sampler.
We present our version of the o-LDA sampler in
Algorithm 1. For each incoming sentence twe run J
passes of sampling, updating the counts tables after
each sampling step. We sample the class assignment
zti for feature wti according to:
P (zt|zt?1,wt,dt) ?
(nzt,dtt?1 + ?)? (n
zt,wt
t?1 + ?)
?Vt?1
j=1 n
zt,wj
t?1 + ?
,
(5)
where nz,dt stands for the number of times class z
co-occurred with word type d up to step t, and sim-
ilarly nz,wt is the number of times feature w was as-
signed to class z. Vt is the number of unique features
seen up to step t, while ? and ? are the LDA hyper-
parameters. There are two differences between the
original o-LDA and our version: we do not initialize
the algorithm with a batch run over a prefix of the
data, and we allow more than one sampling pass per
sentence.2 Exploratory experiments have shown that
batch initialization is unnecessary, and that multiple
passes typically improve the quality of the induced
word classes.
Algorithm 1 Online Gibbs sampler for word class
induction with LDA
for t = 1?? do
for j = 1? J do
for i = 1? It do
sample zti ? P (zti |zti?1,wti ,dti)
increment n
zti ,wti
t and n
zti ,dti
t
Figure 1 shows the top 10 words for each of the
10 word classes induced with our online Gibbs sam-
pler from 1.8 million words of CHILDES. Similarly,
Figure 2 shows the top 10 words for 5 randomly cho-
sen topics out of 50, learned online from 100 million
words of the BNC.
The topics are relatively coherent and at these lev-
els of granularity express mostly part of speech and
subcategorization frame information.
Note that for each word class we show the words
most frequently assigned to it while Gibbs sampling.
2Note that we do not allow multiple passes over the stream
of sentences. Rather, while processing the current sentence, we
allow the words in this sentence to be sampled more than once.
647
do are have can not go put did get play
is that it what not there he was where put
you not I the we what it they your a
to you we and I will not can it on
it a that the not he this right got she
are do is have on in can want did going
one I not shall there then you are we it
is in are on oh with and of have do
the a your of that it this some not very
going want bit go have look got will at little
Figure 1: Top 10 words for 10 classes learned from
CHILDES
I you he it they we she , You He
a the more some all no The other I two
as if when that where how because If before what
was is ?s had , has are would did said
the his her their this an that its your my
Figure 2: Top 10 words of 5 randomly chosen classes
learned from BNC
Since we are dealing with soft classes, most word-
types have non-zero assignment probabilities for
many classes. Thus frequently occurring words such
as not will typically be listed for several classes.
4 Evaluation
4.1 Experimental setup
As training data, we extract utterances from the
Manchester corpus (Theakston et al001) in the
CHILDES database (MacWhinney 1995), a corpus
that contains transcripts of conversations with chil-
dren between the ages of 1 year, 8 months and 3
years. We use the mother?s speech from transcripts
of 12 children (henceforth referred to by children?s
names).
We run word class induction while simultane-
ously outputting the highest scoring word-class la-
bel for each word: for a new sentence, we sam-
ple class assignments for each feature (doing J
passes), update the counts, and then for each word
dti output the highest scoring class label according
to argmaxz n
z,dti
t (where n
z,dti
t stands for the num-
ber of times class z co-occurred with word type dti
up to step t).
During development we ran the online word class
induction module on data for Aran, Becky, Carl and
Anne and then started the word learning module for
the Anne portion while continuing inducing cate-
gories. We then evaluated word learning on Anne.
We chose the parameters of the word class induc-
tion module based on those development results:
?K
1=1 ? = 10, ? = 0.1, K = 10 and J = 20.
We used cross-validation for the final evaluation.
For each of six data files (Anne, Aran, Becky, Carl,
Dominic and Gail), we ran word-class induction on
the whole corpus with the chosen file last, and then
started applying the word-learning algorithm on this
last chosen file (while continuing with category in-
duction). We evaluated how well word meanings
were learned in those six cases.
We follow Alishahi and Fazly (2010) in the con-
struction of the input. We need a semantic represen-
tation paired with each utterance. Such a represen-
tation is not available from the corpus and has to be
constructed. We automatically construct a gold lexi-
con for all nouns and verbs in this corpus as follows.
For each word, we extract all hypernyms for its first
sense in the appropriate (verb or noun) hierarchy in
WordNet (Fellbaum 1998), and add the first word in
the synset of each hypernym to the set of semantic
features for the target word. For verbs, we also ex-
tract features from VerbNet (Kipper et al006). A
small subset of words (pronouns and frequent quan-
tifiers) are also manually added. This lexicon repre-
sents the true meaning of each word, and is used in
generating the scene representations in the input and
in evaluation.
For each utterance in the input corpus, we form
the union of the feature representations of all its
words. Words not found in the lexicon (i.e. for which
we could not extract a semantic representation from
WordNet and VerbNet) are removed from the utter-
ance (only for the word learning module).
In order to simulate the high level of noise that
children receive from their environment, we follow
Alishahi and Fazly (2010) and pair each utterance
with a combination of its own scene representation
and the scene representation for the following utter-
ance. This decision was based on the intuition that
consequent utterances are more likely to be about re-
648
Utterance: { mommy, ate, broccoli }
Scene: { ANIMATE, HUMAN, ...,
CONSUMPTION, ACTION, ...
BROCCOLI, VEGETABLE, ...
PLATE, OBJECT, ... }
Figure 3: A sample input item to the word learning model
lated topics and scenes. This results in a (roughly)
200% ambiguity. In addition, we remove the mean-
ing of one random word from the scene representa-
tion of every second utterance in an attempt to sim-
ulate cases where the referent of an uttered word is
not within the perception field (such as ?daddy is not
home yet?). A sample utterance and its correspond-
ing scene are shown in Figure 3.
As mentioned before, many words in our input
corpus are polysemous. For such words, we extract
different sets of features depending on their manu-
ally tagged part of speech and keep them in the lex-
icon (e.g. the lexicon contains two different entries
for set:N and set:V). When constructing a scene rep-
resentation for an utterance which contains an am-
biguous word, we choose the correct sense from our
lexicon according to the word?s part of speech tag in
Manchester corpus.
In the experiments reported in the next section,
we assess the performance of our model on learning
words at each point in time: for each target word,
we compare its set of features in the lexicon with
its probability distribution over the semantic fea-
tures that the model has learned. We use mean aver-
age precision (MAP) to measure how well p(t)(?|w)
ranks the features of w.
4.2 Learning curves
To understand whether our categories contribute to
learning of word?meaning mappings, we compare
the pattern of word learning over time in four con-
ditions. The first condition represents our baseline,
in which we do not use category-based alignment
in the word learning model by setting ?(w) = 1
in Eqn. (1). In the second condition we use a set
of uniformly distributed categories for alignment,
as estimated by Eqn. (3) on page 3 (this condition
is introduced to examine whether categories act as
more than a simple smoothing factor in the align-
Category Avg. MAP Std. Dev.
None 0.626 0.032
Uniform 0.633 0.032
LDA 0.659 0.029
POS 0.672 0.030
Table 3: Final Mean Average Precision scores
ment process.) In the third condition we use the cat-
egories induced by online LDA in the word learning
model. The fourth condition represents the perfor-
mance ceiling, in which we use the pre-defined and
manually annotated part of speech categories from
the Manchester corpus.
Table 3 shows the average and the standard devia-
tion of the final MAP scores across the six datasets,
for the four conditions (no categories, uniform cat-
egories, LDA categories and gold part-of-speech
tags). The differences between LDA and None, and
between LDA and Uniform are statistically signif-
icant according to the paired t test (p < 0.01),
while the difference between LDA and POS is not
(p = 0.16).
Figure 4 shows the learning curves in each con-
dition, averaged over the six splits explained in the
previous section. The top panel shows the average
learning curve over the minimum number of sen-
tences across the six sub-corpora (8800 sentences).
The curves show that our LDA categories signifi-
cantly improve the performance of the model over
both baselines. That means that using these cate-
gories can improve word learning compared to not
using them and relying on cross-situational evidence
alone. Moreover, LDA-induced categories are not
merely acting as a smoothing function the way the
?uniform? categories are. Our results show that they
are bringing relevant information to the task at hand,
that is, improving word learning by using the sen-
tential context. In fact, this improvement is compa-
rable to the improvement achieved by integrating the
?gold-standard? POS categories.
The middle and bottom panels of Figure 4 zoom
in on shorter time spans (5000 and 1000 sentences,
respectively). These diagrams suggest that the pat-
tern of improvement over baseline is relatively con-
stant, even at very early stages of learning. In fact,
once the model receives enough input data, cross-
situational evidence becomes stronger (since fewer
649
words in the input are encountered for the first time)
and the contribution of the categories becomes less
significant.
4.3 Class granularity
In Figure 5 we show the influence of the number of
word classes used on the performance in word learn-
ing. It is evident that in the range between 5 to 20
classes the performance of the word learning module
is quite stable and insensitive to the exact class gran-
ularity. Even with only 5 classes the model can still
roughly distinguish noun-like words from verb-like
words from pronoun-like words, and this will help
learn the meaning elements derived from the higher
levels of WordNet hierarchy. Notwithstanding that,
ideally we would like to avoid having to pre-specify
the number of classes for the word class induction
module: we thus plan to investigate non-parametric
models such as Hierarchical Dirichlet Process for
this purpose.
5 Related Work
This paper investigates the interplay between two
language learning tasks which have so far been stud-
ied in isolation: the acquisition of lexical categories
from distributional clues, and learning the mapping
between words and meanings. Previous models
have shown that lexical categories can be learned
from unannotated text, mainly drawing on distri-
butional properties of words (e.g. Redington et al
1998, Clark 2000, Mintz 2003, Parisien et al008,
Chrupa?a and Alishahi 2010).
Independently, several computational models
have exploited cross-situational evidence in learning
the correct mappings between words and meanings,
using rule-based inference (Siskind 1996), neural
networks (Li et al004, Regier 2005), hierarchical
Bayesian models (Frank et al007) and probabilis-
tic alignment inspired by machine translation mod-
els (Yu 2005, Fazly et al010).
There are only a few existing computational mod-
els that explore the role of syntax in word learning.
Maurits et al2009) investigates the joint acquisi-
tion of word meaning and word order using a batch
model. This model is tested on an artificial language
with a simple first order predicate representation of
meaning, and limited built-in possibilities for word
0 2000 4000 6000 80000.
35
0.45
0.55
0.65
Input Items
Mean
 Aver
age P
recisi
on
POSLDAUniformNone
(a) all sentences
0 1000 2000 3000 4000 50000.
35
0.45
0.55
0.65
Input Items
Mean
 Aver
age P
recisi
on
POSLDAUniformNone
(b) first 5000 sentences
0 200 400 600 800 10000.
35
0.45
0.55
0.65
Input Items
Mean
 Aver
age P
recisi
on
POSLDAUniformNone
(c) first 1000 sentences
Figure 4: Mean average precision for all observed words
at each point in time for four conditions: with gold
POS categories, with LDA categories, with uniform cate-
gories, and without using categories. Each panel displays
a different time span.
650
0 2000 4000 6000 80000.
40
0.45
0.50
0.55
0.60
0.65
0.70
Input Items
Mean
 Aver
age P
recisi
on
20 LDA classes10 LDA classes5 LDA classesNo categories
Figure 5: Mean average precision for all observed words
at each point in time in four conditions: using online LDA
categories of varying numbers of 20, 10 and 5, and with-
out using categories.
order. The model of Niyogi (2002) simulates the
mutual bootstrapping effects of syntactic and seman-
tic knowledge in verb learning, that is the use of syn-
tax to aid in inducing the semantics of a verb, and the
use of semantics to narrow down possible syntactic
frames in which a verb can participate. However,
this model relies on manually assigned priors for as-
sociations between syntactic and semantic features,
and is tested on a toy language with very limited vo-
cabulary and a constrained syntax.
Yu (2006) integrates automatically induced syn-
tactic word categories into his model of cross-
situational word learning, showing that they can im-
prove the model?s performance. Yu?s model also
processes input utterances in a batch mode, and its
evaluation is limited to situations in which only a
coarse distinction between referring words (words
that could potentially refer to objects in a scene, e.g.
concrete nouns) and non-referring words (words that
cannot possibly refer to objects, e.g. function words)
is sufficient. It is thus not clear whether information
about finer-grained categories (e.g. verbs and nouns)
can indeed help word learning in a more naturalistic
incremental setting.
On the other hand, the model of Alishahi and
Fazly (2010) integrates manually annotated part-of-
speech tags into an incremental word learning al-
gorithm, and shows that these tags boost the over-
all word learning performance, especially for infre-
quent words.
In a different line of research, a number of mod-
els have been proposed which study the acquisition
of the link between syntax and semantics within the
Combinatory Categorial Grammar (CCG) frame-
work (Briscoe 1997, Villavicencio 2002, Buttery
2006, Kwiatkowski et al012). These approaches
set the parameters of a semantic parser on a cor-
pus of utterances paired with a logical form as their
meaning.
These models bring in extensive and detailed prior
assumptions about the nature of the syntactic repre-
sentation (i.e. atomic categories such as S and NP,
and built-in rules which govern their combination),
as well as about the representation of meaning via
the formalism of lambda calculus.
This is fundamentally different than the approach
taken in this paper, which in comparison only as-
sumes very simple syntactic and semantic represen-
tations of syntax. We view word and category learn-
ing as stand-alone cognitive tasks with independent
representations (word meanings as probabilistic col-
lections of properties or features as opposed to sin-
gle symbols; categories as sets of word tokens with
similar context distribution) and we do not bring in
any prior knowledge of specific atomic categories.
6 Conclusion
In this paper, we show the plausibility of using
automatically and incrementally induced categories
while learning word meanings. Our results suggest
that the sentential context that a word appears in
across its different uses can be used as a complemen-
tary source of guidance for mapping it to its featural
meaning representation.
In Section 4 we show that the improvement
achieved by our categories is comparable to that
gained by integrating gold POS categories. This re-
sult is very encouraging, since manually assigned
POS tags are typically believed to set the upper
bound on the usefulness of category information.
We believe that it automatically induced cate-
gories have the potential to do even better: Chrupa?a
and Alishahi (2010) have shown that categories in-
duced from usage data in an unsupervised fashion
can be used more effectively than POS categories in
651
a number of tasks. In our experiments here on the
development data we observed some improvements
over POS categories. This advantage can result from
the fact that our categories are more fine-grained (if
also more noisy) than POS categories, which some-
times yields more accurate predictions.
One important characteristic of the category in-
duction algorithm we have used in this paper is that
it provides a soft categorization scheme, where each
word is associated with a probability distribution
over all categories. In future, we plan to exploit this
feature: when estimating the category-based align-
ment, we can interpolate predictions of multiple cat-
egories to which a word belongs, weighted by its
probabilities associated with membership in each
category.
Acknowledgements
Grzegorz Chrupa?a was funded by the German Fed-
eral Ministry of Education and Research (BMBF)
under grant number 01IC10S01O as part of
the Software-Cluster project EMERGENT (www.
software-cluster.org).
References
Alishahi, A. and Fazly, A. (2010). Integrating
Syntactic Knowledge into a Model of Cross-
situational Word Learning. In Proceedings of the
32nd Annual Conference of the Cognitive Science
Society.
Banerjee, A. and Basu, S. (2007). Topic models over
text streams: A study of batch and online unsuper-
vised learning. In SIAM Data Mining.
Blei, D., Ng, A., and Jordan, M. (2003). La-
tent dirichlet alation. The Journal of Machine
Learning Research, 3:993?1022.
Briscoe, T. (1997). Co-evolution of language and
of the language acquisition device. In Proceed-
ings of the eighth conference on European chap-
ter of the Association for Computational Linguis-
tics, pages 418?427. Association for Computa-
tional Linguistics.
Brown, P. F., Mercer, R. L., Della Pietra, V. J., and
Lai, J. C. (1992). Class-based n-gram models
of natural language. Computational Linguistics,
18(4):467?479.
Buttery, P. (2006). Computational models for first
language acquisition. Computer Laboratory, Uni-
versity of Cambridge, Tech. Rep. UCAM-CLTR-
675.
Canini, K., Shi, L., and Griffiths, T. (2009). Online
inference of topics with latent dirichlet alation.
In Proceedings of the International Conference on
Artificial Intelligence and Statistics.
Chrupa?a, G. (2011). Efficient induction of proba-
bilistic word classes with LDA. In International
Joint Conference on Natural Language Process-
ing.
Chrupa?a, G. and Alishahi, A. (2010). Online
Entropy-based Model of Lexical Category Acqui-
sition. In CoNLL 2010.
Clark, A. (2000). Inducing syntactic categories by
context distribution clustering. In Proceedings of
the 2nd workshop on Learning Language in Logic
and the 4th conference on Computational Natural
Language Learning, pages 91?94. Association for
Computational Linguistics Morristown, NJ, USA.
Fazly, A., Alishahi, A., and Stevenson, S. (2010).
A Probabilistic Computational Model of Cross-
Situational Word Learning. Cognitive Science,
34(6):1017?1063.
Fellbaum, C., editor (1998). WordNet, An Electronic
Lexical Database. MIT Press.
Frank, M. C., Goodman, N. D., and Tenenbaum,
J. B. (2007). A Bayesian framework for cross-
situational word-learning. In Advances in Neural
Information Processing Systems, volume 20.
Gelman, S. and Taylor, M. (1984). How two-year-
old children interpret proper and common names
for unfamiliar objects. Child Development, pages
1535?1540.
Gertner, Y., Fisher, C., and Eisengart, J. (2006).
Learning words and rules: Abstract knowledge of
word order in early sentence comprehension. Psy-
chological Science, 17(8):684?691.
Gillette, J., Gleitman, H., Gleitman, L., and Led-
erer, A. (1999). Human simulations of vocabulary
learning. Cognition, 73(2):135?76.
Gleitman, L. (1990). The structural sources of verb
meanings. Language Acquisition, 1:135?176.
652
Hoffman, M., Blei, D., and Bach, F. (2010). On-
line learning for latent dirichlet alation. In
Advances in Neural Information Processing Sys-
tems.
Kemp, N., Lieven, E., and Tomasello, M. (2005).
Young Children?s Knowledge of the? Deter-
miner? and? Adjective? Categories. Journal
of Speech, Language and Hearing Research,
48(3):592?609.
Kipper, K., Korhonen, A., Ryant, N., and Palmer,
M. (2006). Extensive classifications of english
verbs. In Proceedings of the 12th EURALEX In-
ternational Congress.
Koehne, J. and Crocker, M. W. (2010). Sen-
tence processing mechanisms influence cross-
situational word learning. In Proceedings of the
Annual Conference of the Cognitive Science Soci-
ety.
Koehne, J. and Crocker, M. W. (2011). The inter-
play of multiple mechanisms in word learning. In
Proceedings of the Annual Conference of the Cog-
nitive Science Society.
Kwiatkowski, T., Goldwater, S., Zettelmoyer, L.,
and Steedman, M. (2012). A probabilistic model
of syntactic and semantic acquisition from child-
directed utterances and their meanings. In Pro-
ceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics.
Li, P., Farkas, I., and MacWhinney, B. (2004). Early
lexical development in a self-organizing neural
network. Neural Networks, 17:1345?1362.
Lidz, J., Bunger, A., Leddon, E., Baier, R., and Wax-
man, S. R. (2010). When one cue is better than
two: lexical vs . syntactic cues to verb learning.
Unpublished manuscript.
MacWhinney, B. (1995). The CHILDES Project:
Tools for Analyzing Talk. Hillsdale, NJ: Lawrence
Erlbaum Associates, second edition.
Maurits, L., Perfors, A. F., and Navarro, D. J. (2009).
Joint acquisition of word order and word refer-
ence. In Proceedings of the 31st Annual Confer-
ence of the Cognitive Science Society.
Mintz, T. (2003). Frequent frames as a cue for gram-
matical categories in child directed speech. Cog-
nition, 90(1):91?117.
Monaghan, P. and Mattock, K. (2009). Cross-
situational language learning: The effects of
grammatical categories as constraints on referen-
tial labeling. In Proceedings of the 31st Annual
Conference of the Cognitive Science Society.
Naigles, L. and Hoff-Ginsberg, E. (1995). Input to
Verb Learning: Evidence for the Plausibility of
Syntactic Bootstrapping. Developmental Psychol-
ogy, 31(5):827?37.
Niyogi, S. (2002). Bayesian learning at the syntax-
semantics interface. In Proceedings of the 24th
annual conference of the Cognitive Science Soci-
ety, pages 697?702.
Parisien, C., Fazly, A., and Stevenson, S. (2008). An
incremental bayesian model for learning syntactic
categories. In Proceedings of the Twelfth Confer-
ence on Computational Natural Language Learn-
ing.
Pinker, S. (1989). Learnability and Cognition: The
Acquisition of Argument Structure. Cambridge,
MA: MIT Press.
Quine, W. (1960). Word and Object. Cambridge
University Press, Cambridge, MA.
Redington, M., Crater, N., and Finch, S. (1998). Dis-
tributional information: A powerful cue for ac-
quiring syntactic categories. Cognitive Science:
A Multidisciplinary Journal, 22(4):425?469.
Regier, T. (2005). The emergence of words: Atten-
tional learning in form and meaning. Cognitive
Science, 29:819?865.
Siskind, J. M. (1996). A computational study of
cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61:39?91.
Smith, L. and Yu, C. (2007). Infants rapidly learn
words from noisy data via cross-situational statis-
tics. In Proceedings of the 29th Annual Confer-
ence of the Cognitive Science Society.
Song, X., Lin, C., Tseng, B., and Sun, M. (2005).
Modeling and predicting personal information
dissemination behavior. In Proceedings of the
eleventh ACM SIGKDD international conference
on Knowledge discovery in data mining, pages
479?488. ACM.
653
Theakston, A. L., Lieven, E. V., Pine, J. M., and
Rowland, C. F. (2001). The role of performance
limitations in the acquisition of verb-argument
structure: An alternative account. Journal of
Child Language, 28:127?152.
Villavicencio, A. (2002). The acquisition of a
unification-based generalised categorial grammar.
In Proceedings of the Third CLUK Colloquium,
pages 59?66.
Yu, C. (2005). The emergence of links between
lexical acquisition and object categorization: A
computational study. Connection Science, 17(3?
4):381?397.
Yu, C. (2006). Learning syntax?semantics mappings
to bootstrap word learning. In Proceedings of the
28th Annual Conference of the Cognitive Science
Society.
654
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 613?622,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Learning from evolving data streams: online triage of bug reports
Grzegorz Chrupala
Spoken Language Systems
Saarland University
gchrupala@lsv.uni-saarland.de
Abstract
Open issue trackers are a type of social me-
dia that has received relatively little atten-
tion from the text-mining community. We
investigate the problems inherent in learn-
ing to triage bug reports from time-varying
data. We demonstrate that concept drift is
an important consideration. We show the
effectiveness of online learning algorithms
by evaluating them on several bug report
datasets collected from open issue trackers
associated with large open-source projects.
We make this collection of data publicly
available.
1 Introduction
There has been relatively little research to date
on applying machine learning and Natural Lan-
guage Processing techniques to automate soft-
ware project workflows. In this paper we address
the problem of bug report triage.
1.1 Issue tracking
Large software projects typically track defect re-
ports, feature requests and other issue reports us-
ing an issue tracker system. Open source projects
tend to use trackers which are open to both devel-
opers and users. If the product has many users its
tracker can receive an overwhelming number of
issue reports: Mozilla was receiving almost 300
reports per day in 2006 (Anvik et al2006). Some-
one has to monitor those reports and triage them,
that is decide which component they affect and
which developer or team of developers should be
responsible for analyzing them and fixing the re-
ported defects. An automated agent assisting the
staff responsible for such triage has the potential
to substantially reduce the time and cost of this
task.
1.2 Issue trackers as social media
In a large software project with a loose, not
strictly hierarchical organization, standards and
practices are not exclusively imposed top-down
but also tend to spontaneously arise in a bottom-
up fashion, arrived at through interaction of in-
dividual developers, testers and users. The indi-
viduals involved may negotiate practices explic-
itly, but may also imitate and influence each other
via implicitly acquired reputation and status. This
process has a strong emergent component: an in-
formal taxonomy may arise and evolve in an is-
sue tracker via the use of free-form tags or labels.
Developers, testers and users can attach tags to
their issue reports in order to informally classify
them. The issue tracking software may give users
feedback by informing them which tags were fre-
quently used in the past, or suggest tags based
on the content of the report or other information.
Through this collaborative, feedback driven pro-
cess involving both human and machine partici-
pants, an evolving consensus on the label inven-
tory and semantics typically arises, without much
top-down control (Halpin et al2007).
This kind of emergent taxonomy is known as
a folksonomy or collaborative tagging and is
very common in the context of social web appli-
cations. Large software projects, especially those
with open policies and little hierarchical struc-
tures, tend to exhibit many of the same emergent
social properties as the more prototypical social
applications. While this is a useful phenomenon,
it presents a special challenge from the machine-
learning point of view.
613
1.3 Concept drift
Many standard supervised approaches in
machine-learning assume a stationary distribution
from which training examples are independently
drawn. The set of training examples is processed
as a batch, and the resulting learned decision
function (such as a classifier) is then used on test
items, which are assumed to be drawn from the
same stationary distribution.
If we need an automated agent which uses hu-
man labels to learn to tag objects the batch learn-
ing approach is inadequate. Examples arrive one-
by-one in a stream, not as a batch. Even more
importantly, both the output (label) distribution
and the input distribution from which the exam-
ples come are emphatically not stationary. As a
software project progresses and matures, the type
of issues reported is going to change. As project
members and users come and go, the vocabulary
they use to describe the issues will vary. As the
consensus tag folksonomy emerges, the label and
training example distribution will evolve. This
phenomenon is sometimes referred to as concept
drift (Widmer and Kubat 1996, Tsymbal 2004).
Early research on learning to triage tended to
either not notice the problem (C?ubranic? and Mur-
phy 2004), or acknowledge but not address it (An-
vik et al2006): the evaluation these authors used
assigned bug reports randomly to training and
evaluation sets, discarding the temporal sequenc-
ing of the data stream.
Bhattacharya and Neamtiu (2010) explicitly
address the issue of online training and evalua-
tion. In their setup, the system predicts the out-
put for an item based only on items preceding it
in time. However, their approach to incremen-
tal learning is simplistic: they use a batch clas-
sifier, but retrain it from scratch after receiving
each training example. A fully retrained batch
classifier will adapt only slowly to changing data
stream, as more recent example have no more in-
fluence on the decision function that less recent
ones.
Tamrawi et al(2011) propose an incremental
approach to bug triage: the classes are ranked
according to a fuzzy set membership function,
which is based on incrementally updated fea-
ture/class co-occurrence counts. The model is ef-
ficient in online classification, but also adapts only
slowly.
1.4 Online learning
This paucity of research on online learning from
issue tracker streams is rather surprising, given
that truly incremental learners have been well-
known for many years. In fact one of the first
learning algorithms proposed was Rosenblatt?s
perceptron, a simple mistake-driven discrimina-
tive classification algorithm (Rosenblatt 1958). In
the current paper we address this situation and
show that by using simple, standard online learn-
ing methods we can improve on batch or pseudo-
online learning. We also show that when using
a sophisticated state-of-the-art stochastic gradient
descent technique the performance gains can be
quite large.
1.5 Contributions
Our main contributions are the following: Firstly,
we explicitly show that concept-drift is pervasive
and serious in real bug report streams. We then
address this problem by leveraging state-of-the-
art online learning techniques which automati-
cally track the evolving data stream and incremen-
tally update the model after each data item. We
also adopt the continuous evaluation paradigm,
where the learner predicts the output for each ex-
ample before using it to update the model. Sec-
ondly, we address the important issue of repro-
ducibility in research in bug triage automation
by making available the data sets which we col-
lected and used, in both their raw and prepro-
cessed forms.
2 Open issue-tracker data
Open source software repositories and their as-
sociated issue trackers are a naturally occurring
source of large amounts of (partially) labeled data.
There seems to be growing interest in exploiting
this rich resource as evidenced by existing publi-
cations as well as the appearance of a dedicated
workshop (Working Conference on Mining Soft-
ware Repositories).
In spite of the fact that the data is publicly avail-
able in open repositories, it is not possible to di-
rectly compare the results of the research con-
ducted on bug triage so far: authors use non-
trivial project-specific filtering, re-labeling and
pre-processing heuristics; these steps are usually
not specified in enough detail that they could be
easily reproduced.
614
Field Meaning
Identifier Issue ID
Title Short description of issue
Description Content of issue report, which
may include steps to reproduce,
error messages, stack traces etc.
Author ID of report submitter
CCS List of IDs of people CC?d on
the issue report
Labels List of tags associated with is-
sue
Status Label describing the current sta-
tus of the issue (e.g. Invalid,
Fixed, Won?t Fix)
Assigned To ID of person who has been as-
signed to deal with the issue
Published Date on which issue report was
submitted
Table 1: Issue report record
To help remedy this situation we decided to col-
lect data from several open issue trackers, use the
minimal amount of simple preprocessing and fil-
ter heuristics to get useful input data, and publicly
share both the raw and preprocessed data.
We designed a simple record type which acts
as a common denominator for several tracker for-
mats. Thus we can use a common representation
for issue reports from various trackers. The fields
in our record are shown in Table 1.
Below we describe the issue trackers used
and the datasets we build from them. As dis-
cussed above (and in more detail in Section 4.1),
we use progressive validation rather than a split
into training and test set. However, in order
to avoid developing on the test data, we split
each data stream into two substreams, by assign-
ing odd-numbered examples to the test stream
and the even-numbered ones to the development
stream. We can use the development stream for
exploratory data analysis and feature and param-
eter tuning, and then use progressive validation to
evaluate on entirely unseen test data. Below we
specify the size and number of unique labels in
the development sets; the test sets are very similar
in size.
Chromium Chromium is the open source-
project behind Google?s Chrome browser
(http://code.google.com/p/
chromium/). We retrieved all the bugs
from the issue tracker, of which 66,704 have one
of the closed statuses. We generated two data sets
from the Chromium issues:
? Chromium SUBCOMPONENT. Chromium
uses special tags to help triage the bug re-
ports. Tags prefixed with Area- specify
which subcomponent of the project the bug
should be routed to. In some cases more
than one Area- tag is present. Since this
affects less than 1% of reports, for simplic-
ity we treat these as single, compound labels.
The development set contains 31,953 items,
and 75 unique output labels.
? Chromium ASSIGNED. In this dataset the
output is the value of the assignedTo
field. We discarded issues where the
field was left empty, as well as the
ones which contained the placeholder value
all-bugs-test.chromium.org. The
development set contains 16,154 items and
591 unique output labels.
Android Android is a mobile operating sys-
tem project (http://code.google.com/
p/android/). We retrieved all the bugs reports,
of which 6,341 had a closed status. We generated
two datasets:
? Android SUBCOMPONENT. The reports
which are labeled with tags prefixed with
Component-. The development set con-
tains 888 items and 12 unique output labels.
? Android ASSIGNED. The output label is the
value of the assignedTo field. We dis-
carded issues with the field left empty. The
development set contains 718 items and 72
unique output labels.
Firefox Firefox is the well-known web-browser
project (https://bugzilla.mozilla.
org).
We obtained a total of 81,987 issues with a
closed status.
? Firefox ASSIGNED. We discarded issues
where the field was left empty, as well as
the ones which contained a placeholder value
(nobody). The development set contains
12,733 items and 503 unique output labels.
Launchpad Launchpad is an issue tracker
run by Canonical Ltd for mostly Ubuntu-related
projects (https://bugs.launchpad.
615
net/). We obtained a total of 99,380 issues with
a closed status.
? Launchpad ASSIGNED. We discarded issues
where the field was left empty. The devel-
opment set contains 18,634 items and 1,970
unique output labels.
3 Analysis of concept drift
In the introduction we have hypothesized that in
issue tracker streams concept drift would be an
especially acute problem. In this section we show
how class distributions evolve over time in the
data we collected.
A time-varying distribution is difficult to sum-
marize with a single number, but it is easy to ap-
preciate in a graph. Figures 1 and 2 show concept
drift for several of our data streams. The horizon-
tal axis indexes the position in the data stream.
The vertical axis shows the class proportions at
each position, averaged over a window containing
7% of all the examples in the stream, i.e. in each
thin vertical bar the proportion of colors used cor-
responds to the smoothed class distribution at a
particular position in the stream.
Consider the plot for Chromium SUBCOMPO-
NENT. We can see that a bit before the middle
point in the stream class proportions change quite
dramatically: The orange BROWSERUI and vio-
let MISC almost disappears, while blue INTER-
NALS, pink UI and dark red UNDEFINED take
over. This likely corresponds to an overhaul in the
label inventory and/or recommended best practice
for triage in this project. There are also more
gradual and smaller scale changes throughout the
data stream.
The Android SUBCOMPONENT stream con-
tains much less data so the plot is less smooth, but
there are clear transitions in this image also. We
see that light blue GOOGLE all but disappears after
about two thirds point and the proportion of vio-
let TOOLS and light-green DALVIK dramatically
increases.
In Figure 2 we see the evolution of class pro-
portions in the ASSIGNED datasets. Each plot?s
idiosyncratic shape illustrates that there is wide
variation in the amount and nature of concept drift
in different software project issue trackers.
Figure 1: SUBCOMPONENT class distribution change
over time
4 Experimental results
In an online setting it is important to use an evalu-
ation regime which closely mimics the continuous
use of the system in a real-life situation.
4.1 Progressive validation
When learning from data streams the standard
evaluation methodology where data is split into a
separate training and test set is not applicable. An
evaluation regime know as progressive validation
has been used to accurately measure the general-
ization performance of online algorithms (Blum
et al1999). Under progressive evaluation, an in-
put example from a temporally ordered sequence
is sent to the learner, which returns the prediction.
The error incurred on this example is recorded,
and the true output is only then sent to the learner
which may update its model based on it. The fi-
nal error is the mean of the per-example errors.
Thus even though there is no separate test set, the
prediction for each input is generated based on a
model trained on examples which do not include
it.
In previous work on bug report triage, Bhat-
tacharya and Neamtiu (2010) and Tamrawi et al
(2011) used an evaluation scheme (close to) pro-
616
Figure 2: ASSIGNED class distribution change over time
gressive validation. They omit the initial 111
th
of
the examples from the mean.
4.2 Mean reciprocal rank
A bug report triaging agent is most likely to be
used in a semi-automatic workflow, where a hu-
man triager is presented with a ranked list of
possible outputs (component labels or developer
IDs). As such it is important to evaluate not only
accuracy of the top ranking suggesting, but rather
the quality of the whole ranked list.
Previous research (Bhattacharya and Neamtiu
2010, Tamrawi et al2011) made an attempt at
approximating this criterion by reporting scores
which indicate whether the true output is present
in the top n elements of the ranking, for several
values of n. Here we suggest borrowing the mean
reciprocal rank (MRR) metric from the informa-
tion retrieval domain (Voorhees 2000). It is de-
fined as the mean of the reciprocals of the rank at
which the true output is found:
MRR =
1
N
N?
i=1
rank(i)?1
where rank(i) indicates the rank of the ith true
output. MRR has the advantage of providing a
single number which summarizes the quality of
whole rankings for all the examples. MRR is also
a special case of Mean Average Precision when
there is only one true output per item.
4.3 Input representation
Since in this paper we focus on the issues related
to concept drift and online learning, we kept the
feature set relatively simple. We preprocess the
text in the issue report title and description fields
by removing HTML markup, tokenizing, lower-
casing and removing most punctuation. We then
extracted the following feature types:
? Title unigram and bigram counts
? Description unigram and bigram counts
? Author ID (binary indicator feature)
? Year, month and day of submission (binary
indicator features)
4.4 Models
We tested a simple online baseline, a pseudo-
online algorithm which uses a batch model and
repeatedly retrains it, an online model used in pre-
vious research on bug triage and two generic on-
line learning algorithms.
Window Frequency Baseline This baseline
does not use any input features. It outputs the
617
ranked list of labels for the current item based
on the relative frequencies of output labels in the
window of k previous items. We tested windows
of size 100 and 1000 and report the better result.
SVM Minibatch This model uses the mul-
ticlass linear Support Vector Machine model
(Crammer and Singer 2002) as implemented in
SVM Light (Joachims 1999). SVM is known
as a state-of-the-art batch model in classification
in general and in text categorization in particu-
lar. The output classes for an input example are
ranked according to the value of the discriminant
values returned by the SVM classifier. In order
to adapt the model to an online setting we retrain
it every n examples on the window of k previous
examples. The parameters n and k can have large
influence on the prediction, but it is not clear how
to set them when learning from streams. Here we
chose the values (100,1000) based on how feasi-
ble the run time was and on the performance dur-
ing exploratory experiments on Chromium SUB-
COMPONENT. Interestingly, keeping the window
parameter relatively small helps performance: a
window of 1,000 works better than a window of
5,000.
Perceptron We implemented a single-pass on-
line multiclass Perceptron with a constant learn-
ing rate. It maintains a weight vector for each
output seen so far: the prediction function ranks
outputs according to the inner product of the cur-
rent example with the corresponding weight vec-
tor. The update function takes the true output and
the predicted output. If they are not equal, the
current input is subtracted from the weight vector
corresponding to the predicted output and added
to the weight vector corresponding to the true out-
put (see Algorithm 1). We hash each feature to an
integer value and use it as the feature?s index in
the weight vectors in order to bound memory us-
age in an online setting (Weinberger et al2009).
The Perceptron is a simple but strong baseline for
online learning.
Bugzie This is the model described in Tamrawi
et al(2011). The output classes are ranked ac-
cording to the fuzzy set membership function de-
fined as follows:
?(y,X) = 1?
?
x?X
(
1?
n(y, x)
n(y) + n(x)? n(y, x)
)
Algorithm 1 Multiclass online perceptron
function PREDICT(Y,W,x)
return {(y,WTy x) | y ? Y }
procedure UPDATE(W,x, y?, y)
if y? 6= y then
Wy? ?Wy? ? x
Wy ?Wy + x
where y is the output label, X the set of features
in the input issue report, n(y, x) the number of ex-
amples labeled as y which contain feature x, n(y)
number of examples labeled y and n(x) number
of examples containing feature x. The counts are
updated online. Tamrawi et al(2011) also use
two so called caches: the label cache keeps the
j% most recent labels and the term cache the k
most significant features for each label. Since
in Tamrawi et al(2011)?s experiments the label
cache did not affect the results significantly, here
we always set j to 100%. We select the optimal
k parameter from {100, 1000, 5000} based on the
development set.
Regression with Stochastic Gradient Descent
This model performs online multiclass learning
by means of a reduction to regression. The re-
gressor is a linear model trained using Stochastic
Gradient Descent (Zhang 2004). SGD updates the
current parameter vector w(t) based on the gradi-
ent of the loss incurred by the regressor on the
current example (x(t), y(t)):
w(t+1) = w(t) ? ?(t)?L(y(t),w(t)
T
x(t))
The parameter ?(t) is the learning rate at time t,
and L is the loss function. We use the squared
loss:
L(y, y?) = (y ? y?)2
We reduce multiclass learning to regression us-
ing a one-vs-all-type scheme, by effectively trans-
forming an example (x, y) ? X ? Y into |Y |
(x?, y?) ? X ? ? {0, 1} examples, where Y is the
set of labels seen so far. The transform T is de-
fined as follows:
T (x, y) = {(x?, I(y = y?)) | y? ? Y, x?h(i,y?) = xi}
where h(i, y?) composes the index i with the label
y? (by hashing).
For a new input x the ranking of the outputs
y ? Y is obtained according to the value of the
618
prediction of the base regressor on the binary ex-
ample corresponding to each class label.
As our basic regression learner we use the ef-
ficient implementation of regression via SGD,
Vowpal Wabbit (VW) (Langford et al2011). VW
implements setting adaptive individual learning
rates for each feature as proposed by Duchi et al
(2010), McMahan and Streeter (2010).
This is appropriate when there are many sparse
features, and is especially useful in learning from
text from fast evolving data. The features such
as unigram and bigram counts that we rely on are
notoriously sparse, and this is exacerbated by the
change over time in bug report streams.
4.5 Results
Figures 3 and 4 show the progressive validation
results on all the development data streams. The
horizontal lines indicate the mean MRR scores for
the whole stream. The curves show a moving av-
erage of MRR in a window comprised of 7% of
the total number of items. In most of the plots it is
evident how the prediction performance depends
on the concept drift illustrated in the plots in Sec-
tion 3: for example on Chromium SUBCOMPO-
NENT the performance of all the models drops a
bit before the midpoint in the stream while the
learners adapt to the change in label distribution
that is happening at this time. This is especially
pronounced for Bugzie, since it is not able to learn
from mistakes and adapt rapidly, but simply accu-
mulates counts.
For five out of the six datasets, Regression SGD
gives the best overall performance. On Launch-
pad ASSIGNED, Bugzie scores higher ? we inves-
tigate this anomaly below.
Another observation is that the window-based
frequency baseline can be quite hard to beat:
In three out of the six cases, the minibatch
SVM model is no better than the baseline.
Bugzie sometimes performs quite well, but for
Chromium SUBCOMPONENT and Firefox AS-
SIGNED it scores below the baseline.
Regarding the quality of the different datasets,
an interesting indicator is the relative error reduc-
tion by the best model over the baseline (see Ta-
ble 2). It is especially hard to extract meaning-
ful information about the labeling from the inputs
on the Firefox ASSIGNED dataset. One possible
cause of this can be that the assignment labeling
practices in this project are not consistent: this im-
Dataset RER
Chromium SUB 0.36
Android SUB 0.38
Chromium AS 0.21
Android AS 0.19
Firefox AS 0.16
Launchpad AS 0.49
Table 2: Best model?s error relative to baseline on the
development set
Task Model MRR Acc
Chromium Window 0.5747 0.3467
SVM 0.5766 0.4535
Perceptron 0.5793 0.4393
Bugzie 0.4971 0.2638
Regression 0.7271 0.5672
Android Window 0.5209 0.3080
SVM 0.5459 0.4255
Perceptron 0.5892 0.4390
Bugzie 0.6281 0.4614
Regression 0.7012 0.5610
Table 3: SUBCOMPONENT evaluation results on test
set.
pression seems to be born out by informal inspec-
tion.
On the other hand as the scores in Table 2
indicate, Chromium SUBCOMPONENT, Android
SUBCOMPOMENT and Launchpad ASSIGNED
contain enough high-quality signal for the best
model to substantially outperform the label fre-
quency baseline.
On Launchpad ASSIGNED Regression SGD
performs worse than Bugzie. The concept drift
plot for these data suggests one reason: there is
very little change in class distribution over time
as compared to the other datasets. In fact, even
though the issue reports in Launchpad range from
year 2005 to 2011, the more recent ones are heav-
ily overrepresented: 84% of the items in the de-
velopment data are from 2011. Thus fast adap-
tation is less important in this case and Bugzie is
able to perform well.
On the other hand, the reason for the less than
stellar score achieved with Regression SGD is due
to another special feature of this dataset: it has
by far the largest number of labels, almost 2,000.
This degrades the performance for the one-vs-all
scheme we use with SGD Regression. Prelim-
inary investigation indicates that the problem is
mostly caused by our application of the ?hash-
619
Figure 3: SUBCOMPONENT evaluation results on the
development set
ing trick? to feature-label pairs (see section 4.4),
which leads to excessive collisions with very large
label sets. Our current implementation can use at
most 29 bit-sized hashes which is insufficient for
datasets like Launchpad ASSIGNED. We are cur-
rently removing this limitation and we expect it
will lead to substantial gains on massively multi-
class problems.
In Tables 3 and 4 we present the overall MRR
results on the test data streams. The picture is sim-
ilar to the development data discussed above.
5 Discussion and related work
Our results show that by choosing the appropri-
ate learner for the scenario of learning from data
streams, we can achieve much better results than
by attempting to twist batch algorithm to fit the
online learning setting. Even a simple and well-
know algorithm such as Perceptron can be effec-
tive, but by using recent advances in research on
SGD algorithms we can obtain substantial im-
provements on the best previously used approach.
Below we review the research on bug report triage
most relevant to our work.
C?ubranic? and Murphy (2004) seems to be the
first attempt to automate bug triage. The authors
cast bug triage as a text classification task and use
Task Model MRR Acc
Chromium Window 0.0999 0.0472
SVM 0.0908 0.0550
Perceptron 0.1817 0.1128
Bugzie 0.2063 0.0960
Regression 0.3074 0.2157
Android Window 0.3198 0.1684
SVM 0.2541 0.1684
Perceptron 0.3225 0.2057
Bugzie 0.3690 0.2086
Regression 0.4446 0.2951
Firefox Window 0.5695 0.4426
SVM 0.4604 0.4166
Perceptron 0.5191 0.4306
Bugzie 0.5402 0.4100
Regression 0.6367 0.5245
Launchpad Window 0.0725 0.0337
SVM 0.1006 0.0704
Perceptron 0.3323 0.2607
Bugzie 0.5271 0.4339
Regression 0.4702 0.3879
Table 4: ASSIGNED evaluation results on test set
the data representation (bag of words) and learn-
ing algorithm (Naive Bayes) typical for text clas-
sification at the time. They collect over 15,000
bug reports from the Eclipse project. The max-
imum accuracy they report is 30% which was
achieved by using 90% of the data for training.
In Anvik et al(2006) the authors experiment
with three learning algorithms: Naive Bayes,
SVM and Decision Tree: SVM performs best in
their experiments. They evaluate using precision
and recall rather than accuracy. They report re-
sults on the Eclipse and Firefox projects, with pre-
cision 57% and 64% respectively, but very low re-
call (7% and 2%).
Matter et al(2009) adopt a different approach
to bug triage. In addition to the project?s issue
tracker data, they use also the source-code ver-
sion control data. They build an expertise model
for each developer which is a word count vec-
tor of the source code changes committed. They
also build a word count vector for each bug report,
and use the cosine between the report and the ex-
pertise model to rank developers. Using this ap-
proach (with a heuristic term weighting scheme)
they report 33.6% accuracy on Eclipse.
Bhattacharya and Neamtiu (2010) acknowl-
edge the evolving nature of bug report streams
and attempt to apply incremental learning meth-
ods to bug triage. They use a two-step approach:
620
Figure 4: ASSIGNED evaluation results on the development set
first they predict the most likely developer to as-
sign to a bug using a classifier. In a second step
they rank candidate developers according to how
likely they were to take over a bug from the de-
veloper predicted in the first step. Their approach
to incremental learning simply involves fully re-
training a batch classifier after each item in the
data stream. They test their approach on fixed
bugs in Mozilla and Eclipse, reporting accuracies
of 27.5% and 38.2% respectively.
Tamrawi et al(2011) propose the Bugzie
model where developers are ranked according to
the fuzzy set membership function as defined
in section 4.4. They also use the label (devel-
oper) cache and term cache to speed up pro-
cessing and make the model adapt better to the
evolving data stream. They evaluate Bugzie and
compare its performance to the models used in
Bhattacharya and Neamtiu (2010) on seven issue
trackers: Bugzie has superior performance on all
of them ranging from 29.9% to 45.7% for top-1
output. They do not use separate validation sets
for system development and parameter tuning.
In comparison to Bhattacharya and Neamtiu
(2010) and Tamrawi et al(2011), here we focus
much more on the analysis of concept drift in data
streams and on the evaluation of learning under its
constraints. We also show that for evolving issue
tracker data, in a large majority of cases SGD Re-
gression handily outperforms Bugzie.
6 Conclusion
We demonstrate that concept drift is a real, perva-
sive issue for learning from issue tracker streams.
We show how to adapt to it by leveraging recent
research in online learning algorithms. We also
make our dataset collection publicly available to
enable direct comparisons between different bug
triage systems.1
We have identified a good learning framework
for mining bug reports: in future we would like
to explore smarter ways of extracting useful sig-
nals from the data by using more linguistically
informed preprocessing and higher-level features
such as word classes.
Acknowledgments
This work was carried out in the context of
the Software-Cluster project EMERGENT and was
partially funded by BMBF under grant number
01IC10S01O.
1Available from http://goo.gl/ZquBe
621
References
Anvik, J., Hiew, L., and Murphy, G. (2006). Who
should fix this bug? In Proceedings of the 28th
international conference on Software engineer-
ing, pages 361?370. ACM.
Bhattacharya, P. and Neamtiu, I. (2010). Fine-
grained incremental learning and multi-feature
tossing graphs to improve bug triaging. In
International Conference on Software Mainte-
nance (ICSM), pages 1?10. IEEE.
Blum, A., Kalai, A., and Langford, J. (1999).
Beating the hold-out: Bounds for k-fold and
progressive cross-validation. In Proceedings
of the twelfth annual conference on Computa-
tional learning theory, pages 203?208. ACM.
Crammer, K. and Singer, Y. (2002). On the al-
gorithmic implementation of multiclass kernel-
based vector machines. The Journal of Ma-
chine Learning Research, 2:265?292.
Duchi, J., Hazan, E., and Singer, Y. (2010). Adap-
tive subgradient methods for online learning
and stochastic optimization. Journal of Ma-
chine Learning Research.
Halpin, H., Robu, V., and Shepherd, H. (2007).
The complex dynamics of collaborative tag-
ging. In Proceedings of the 16th international
conference on World Wide Web, pages 211?
220. ACM.
Joachims, T. (1999). Making large-scale svm
learning practical. In Scho?lkopf, B., Burges,
C., and Smola, A., editors, Advances in Kernel
Methods-Support Vector Learning. MIT-Press.
Langford, J., Hsu, D., Karampatziakis, N.,
Chapelle, O., Mineiro, P., Hoffman, M.,
Hofman, J., Lamkhede, S., Chopra, S.,
Faigon, A., Li, L., Rios, G., and Strehl,
A. (2011). Vowpal wabbit. https:
//github.com/JohnLangford/
vowpal_wabbit/wiki.
Matter, D., Kuhn, A., and Nierstrasz, O. (2009).
Assigning bug reports using a vocabulary-
based expertise model of developers. In Sixth
IEEE Working Conference on Mining Software
Repositories.
McMahan, H. and Streeter, M. (2010). Adap-
tive bound optimization for online convex op-
timization. Arxiv preprint arXiv:1002.4908.
Rosenblatt, F. (1958). The perceptron: A prob-
abilistic model for information storage and or-
ganization in the brain. Psychological review,
65(6):386.
Tamrawi, A., Nguyen, T., Al-Kofahi, J., and
Nguyen, T. (2011). Fuzzy set and cache-based
approach for bug triaging. In Proceedings of
the 19th ACM SIGSOFT symposium and the
13th European conference on Foundations of
software engineering, pages 365?375. ACM.
Tsymbal, A. (2004). The problem of concept
drift: definitions and related work. Computer
Science Department, Trinity College Dublin.
Voorhees, E. (2000). The TREC-8 question an-
swering track report. NIST Special Publication,
pages 77?82.
Weinberger, K., Dasgupta, A., Langford, J.,
Smola, A., and Attenberg, J. (2009). Feature
hashing for large scale multitask learning. In
Proceedings of the 26th Annual International
Conference on Machine Learning, pages 1113?
1120. ACM.
Widmer, G. and Kubat, M. (1996). Learning in the
presence of concept drift and hidden contexts.
Machine learning, 23(1):69?101.
Zhang, T. (2004). Solving large scale linear
prediction problems using stochastic gradient
descent algorithms. In Proceedings of the
twenty-first international conference on Ma-
chine learning, page 116. ACM.
C?ubranic?, D. and Murphy, G. C. (2004). Auto-
matic bug triage using text categorization. In
In SEKE 2004: Proceedings of the Sixteenth In-
ternational Conference on Software Engineer-
ing & Knowledge Engineering, pages 92?97.
KSI Press.
622
