A Hybrid Feature Set based Maximum Entropy Hindi Named Entity
Recognition
Sujan Kumar Saha
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
sujan.kr.saha@gmail.com
Sudeshna Sarkar
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
shudeshna@gmail.com
Pabitra Mitra
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
pabitra@gmail.com
Abstract
We describe our effort in developing a
Named Entity Recognition (NER) system
for Hindi using Maximum Entropy (Max-
Ent) approach. We developed a NER an-
notated corpora for the purpose. We have
tried to identify the most relevant features
for Hindi NER task to enable us to develop
an efficient NER from the limited corpora
developed. Apart from the orthographic and
collocation features, we have experimented
on the efficiency of using gazetteer lists as
features. We also worked on semi-automatic
induction of context patterns and experi-
mented with using these as features of the
MaxEnt method. We have evaluated the per-
formance of the system against a blind test
set having 4 classes - Person, Organization,
Location and Date. Our system achieved a
f-value of 81.52%.
1 Introduction
Named Entity Recognition involves locating and
classifying the names in text. NER is an important
task, having applications in Information Extraction
(IE), question answering, machine translation and in
most other NLP applications.
NER systems have been developed for English
and few other languages with high accuracies. These
systems take advantage of large amount of Named
Entity (NE) annotated corpora and other NER re-
sources. However when we started working on a
NER system for Hindi, we did not have any NER
annotated corpora for Hindi, neither did we have ac-
cess to any comprehensive gazetteer list.
In this work we have identified suitable features
for the Hindi NER task. Orthography features, the
suffix and prefix information, as well as information
about the sorrounding words and their tags are used
to develop a Maximum Entropy (MaxEnt) based
Hindi NER system. Additionally, we have acquired
gazetteer lists for Hindi and used these gazetteers in
the Maximum Entropy (MaxEnt) based Hindi NER
system. We also worked on semi-automatically
learning of context pattern for identifying names.
These context pattern rules have been integrated into
the MaxEnt based NER system, leading to a high ac-
curacy.
The paper is organized as follows. A brief survey
of different techniques used for the NER task in dif-
ferent languages and domains are presented in Sec-
tion 2. The MaxEnt based NER system is described
in Section 3. Various features used in NER are then
discussed. Next we present the experimental results
and related discussions. Finally Section 8 concludes
the paper.
2 Previous Work
A variety of techniques has been used for NER. The
two major approaches to NER are:
1. Linguistic approaches.
2. Machine Learning based approaches.
The linguistic approaches typically use rules man-
ually written by linguists. There are several rule-
based NER systems, containing mainly lexicalized
343
grammar, gazetteer lists, and list of trigger words,
which are capable of providing 88%-92% f-measure
accuracy for English (Grishman, 1995; McDonald,
1996; Wakao et al, 1996).
The main disadvantages of these rule-based tech-
niques are that these require huge experience and
grammatical knowledge of the particular language
or domain and these systems are not transferable to
other languages or domains.
Machine Learning (ML) based techniques for
NER make use of a large amount of NE anno-
tated training data to acquire high level language
knowledge. Several ML techniques have been suc-
cessfully used for the NER task of which Hidden
Markov Model (Bikel et al, 1997), Maximum En-
tropy (Borthwick, 1999), Conditional Random Field
(Li and Mccallum, 2004) are most common. Com-
binations of different ML approaches are also used.
Srihari et al (2000) combines Maximum Entropy,
Hidden Markov Model and handcrafted rules to
build an NER system.
NER systems use gazetteer lists for identifying
names. Both the linguistic approach (Grishman,
1995; Wakao et al, 1996) and the ML based ap-
proach (Borthwick, 1999; Srihari et al, 2000) use
gazetteer lists.
The linguistic approach uses hand-crafted rules
which needs skilled linguistics. Some recent ap-
proaches try to learn context patterns through ML
which reduce amount of manual labour. Talukder et
al.(2006) combined grammatical and statistical tech-
niques to create high precision patterns specific for
NE extraction. An approach to lexical pattern learn-
ing for Indian languages is described by Ekbal and
Bandopadhyay (2007). They used seed data and an-
notated corpus to find the patterns for NER.
The NER task for Hindi has been explored by
Cucerzan and Yarowsky in their language indepen-
dent NER work which used morphological and con-
textual evidences (Cucerzan and Yarowsky, 1999).
They ran their experiment with 5 languages - Roma-
nian, English, Greek, Turkish and Hindi. Among
these the accuracy for Hindi was the worst. For
Hindi the system achieved 41.70% f-value with a
very low recall of 27.84% and about 85% preci-
sion. A more successful Hindi NER system was
developed by Wei Li and Andrew Mccallum (2004)
using Conditional Random Fields (CRFs) with fea-
ture induction. They were able to achieve 71.50%
f-value using a training set of size 340k words. In
Hindi the maximum accuracy is achieved by (Kumar
and Bhattacharyya, 2006). Their Maximum Entropy
Markov Model (MEMM) based model gives 79.7%
f-value.
3 Maximum Entropy Based Model
We have used a Maximum Entropy model to build
the NER in Hindi. MaxEnt is a flexible statistical
model which assigns an outcome for each token
based on its history and features. MaxEnt computes
the probability p(o|h) for any o from the space of
all possible outcomes O, and for every h from the
space of all possible histories H . A history is all
the conditioning data that enables one to assign
probabilities to the space of outcomes. In NER,
history can be viewed as all information derivable
from the training corpus relative to the current
token. The computation of p(o|h) in MaxEnt
depends on a set of features, which are helpful in
making predictions about the outcome. The features
may be binary-valued or multi-valued. For instance,
one of our features is: the current token is a part
of the surname list; how likely is it to be part of
a person name. Formally, we can represent this
feature as follows:
f(h, o) =
{
1 if wi in surname list and o = person
0 otherwise
(1)
Given a set of features and a training corpus,
the MaxEnt estimation process produces a model
in which every feature fi has a weight ?i. We can
compute the conditional probability as (Pietra et al,
1997):
p(o|h) = 1Z(h)
?
i
?ifi(h,o) (2)
Z(h) =
?
o
?
i
?ifi(h,o) (3)
So the conditional probability of the outcome is
the product of the weights of all active features, nor-
malized over the products of all the features. For
our development we have used a Java based open-
nlp MaxEnt toolkit1 to get the probability values of
1www.maxent.sourceforge.net.
344
a word belonging to each class. That is, given a se-
quence of words, the probability of each class is ob-
tained for each word. To find the most probable tag
corresponding to each word of a sequence, we can
choose the tag having the highest class conditional
probability value. But this method is not good as it
might result in an inadmissible output tag.
Some tag sequences should never happen. To
eliminate these inadmissible sequences we have
made some restrictions. Then we used a beam
search algorithm with a beam of length 3 with these
restrictions.
The training data for this task is composed of
about 243K words which is collected from the
popular daily Hindi newspaper ?Dainik Jagaran?.
This corpus has been manually annotated and has
about 16,482 NEs. In this development we have
considered 4 types of NEs, these are Person(P),
Location(L), Organization(O) and Date(D). To
recognize entity boundaries each name class N
is subdivided into 4 sub-classes, i.e., N Begin,
N Continue, N End, and N Unique. Hence,
there are a total of 17 classes including 1 class for
not-name. The corpus contains 6, 298 Person, 4, 696
Location, 3, 652 Organization and 1, 845 Date enti-
ties.
4 Features for Hindi NER
Machine learning approaches like MaxEnt, CRF etc.
make use of different features for identifying the
NEs. Orthographic features (like capitalization, dec-
imal, digits), affixes, left and right context (like pre-
vious and next words), NE specific trigger words,
gazetteer features, POS and morphological features
etc. are generally used for NER. In English and
some other languages, capitalization features play
an important role as NEs are generally capitalized
for these languages. Unfortunately this feature is not
applicable for Hindi. Also Indian person names are
more diverse, lots of common words having other
meanings are also used as person names. These
make difficult to develop a NER system on Hindi.
Li and Mccallum (2004) used the entire word text,
character n-grams (n = 2, 3, 4), word prefix and suf-
fix of lengths 2, 3 and 4, and 24 Hindi gazetteer lists
as atomic features in their Hindi NER. Kumar and
Bhattacharyya (2006) used word features (suffixes,
digits, special characters), context features, dictio-
nary features, NE list features etc. in their MEMM
based Hindi NER system. In the following we have
discussed about the features we have identified and
used to develop the Hindi NER system.
4.1 Feature Description
The features which we have identified for Hindi
Named Entity Recognition are:
Static Word Feature: The previous and next
words of a particular word are used as features. The
previous m words (wi?m...wi?1) to next n words
(wi+1...wi+n) can be treated. During our experi-
ment different combinations of previous 4 to next
4 words are used.
Context Lists: Context words are defined as the
frequent words present in a word window for a par-
ticular class. We compiled a list of the most frequent
words that occur within a window of wi?3...wi+3
of every NE class. For example, location con-
text list contains the words like ?jAkara2? (go-
ing to), ?desha? (country), ?rAjadhAnI? (capital)
etc. and person context list contains ?kahA? (say),
?prdhAnama.ntrI? (prime minister) etc. For a
given word, the value of this feature correspond-
ing to a given NE type is set to 1 if the window
wi?3...wi+3 around the wi contains at last one word
from this list.
Dynamic NE tag: Named Entity tags of the pre-
vious words (ti?m...ti?1) are used as features.
First Word: If the token is the first word of a
sentence, then this feature is set to 1. Otherwise, it
is set to 0.
Contains Digit: If a token ?w? contains digit(s)
then the feature ContainsDigit is set to 1. This
feature is helpful for identifying company product
names (e.g. 06WD1992), house number (e.g. C226)
etc.
Numerical Word: For a token ?w? if the word
is a numerical word i.e. a word denoting a number
(e.g. eka (one), do (two), tina (three) etc.) then the
feature NumWord is set to 1.
Word Suffix: Word suffix information is helpful
to identify the named NEs. Two types of suffix fea-
tures have been used. Firstly a fixed length word
suffix of the current and surrounding words are used
2All Hindi words are written in italics using the ?Itrans?
transliteration.
345
as features. Secondly we compiled lists of common
suffixes of person and place names in Hindi. For ex-
ample, ?pura?, ?bAda?, ?nagara? etc. are location
suffixes. We used two binary features correspond-
ing to the lists - whether a given word has a suffix
from the list.
Word Prefix: Prefix information of a word may
be also helpful in identifying whether it is a NE. A
fixed length word prefix of current and surrounding
words are treated as a features.
Parts-of-Speech (POS) Information: The POS
of the current word and the surrounding words may
be useful feature for NER. We have access to a Hindi
POS pagger developed at IIT Kharagpur which has
an accuracy about 90%. The tagset of the tagger
contains 28 tags. We have used the POS values of
the current and surrounding tokens as features.
We realized that the detailed POS tagging is not
very relevant. Since NEs are noun phrases, the noun
tag is very relevant. Further the postposition follow-
ing a name may give a clue to the NE type. So we de-
cided to use a coarse-grained tagset with only three
tags - nominal (Nom), postposition (PSP) and other
(O).
The POS information is also used by defining sev-
eral binary features. An example is the NomPSP
binary feature. The value of this feature is defined
to be 1 if the current token is nominal and the next
token is a PSP.
5 Enhancement using Gazetteer Feature
Lists of names of various types are helpful in name
identification. We have compiled some specialized
name lists from different web sources. But the
names in these lists are in English, not in Hindi.
So we have transliterated these English name lists
to make them useful for our Hindi NER task.
For the transliteration we have build a 2-phase
transliteration module. We have defined an inter-
mediate alphabet containing 34 characters. English
names are transliterated to this intermediate form us-
ing a map-table. Hindi strings are also transliter-
ated to the intermediate alphabet form using a dif-
ferent map-table. For a English-Hindi string pair,
if transliterations of the both strings are same, then
we conclude that one string is the transliteration of
the other. This transliteration module works with
91.59% accuracy.
Using the transliteration approach we have con-
structed 8 lists. Which are, month name and days of
the week (40)3, organization end words list (92), per-
son prefix words list (123), list of common locations
(80), location names list (17,600), first names list
(9722), middle names list (35), surnames list (1800).
The lists can be used in name identification in var-
ious ways. One way is to check whether a token is
in any list. But this approach is not good as it has
some limitations. Some words may present in two or
more gazetteer lists. For example, ?bangAlora? is in
surnames list and also in location names list. Confu-
sions arise to make decisions for these words. Some
words are in gazetteer lists but sometimes these are
used in text as not-name entity. For example, ?gayA?
is in location list but sometimes the word is used as
verb in text and makes confusion. These limitations
might be reduced if the contexts are considered.
We have used these gazetteer lists as features
of MaxEnt. We have prepared several binary fea-
tures which are defined as whether a given word is
in a particular list. For example, a binary feature
FirstName is 1 for a particular token ?t? if ?t? is in
the first name list.
6 Context Pattern based Features
Context patterns are helpful for identifying NEs. As
manual identification of context patterns takes much
manual labour and linguistic knowledge, we have
developed a module for semi-automatically learning
of context pattern. The summary of the context pat-
tern learning module is given follows:
1. Collect some seed entities (E) for each class.
2. For each seed entity e in E, from the corpus
find context string(C) comprised of n tokens
before e, a placeholder for the class instance
and n tokens after e. [We have used n = 3]
This set of tokens form initial pattern.
3. Search the pattern in the corpus and find the
coverage and precision.
4. Discard the patterns having low precision.
3The italics integers in brackets indicate the size of the lists.
346
5. Generalize the patterns by dropping one or
more tokens to increase coverage.
6. Find best patterns having good precision and
coverage.
The quality of a pattern is measured by precision
and coverage. Precision is the ratio of correct iden-
tification and the total identification, when the par-
ticular pattern is used to identify of NEs of a spe-
cific type from a raw text. Coverage is the amount
of total identification. We have given more impor-
tance to precision and we have marked a pattern as
effective if the precision is more than 95%. The
method is applied on an un-annotated text having
4887011 words collected from ?Dainik Jagaran? and
context patterns are learned. These context patterns
are used as features of MaxEnt in the Hindi NER
system. Some example patterns are:
1. mukhyama.ntrI <PER> Aja
2. <PER> ne kahA ki
3. rAjadhAnI <LOC> me
7 Evaluation
We have evaluated the system using a blind test cor-
pus of 25K words, which is distinct from the training
corpus. The accuracies are measured in terms of the
f-measure, which is the weighted harmonic mean of
precision and recall. Here we can mention that we
have evaluated the performance of the system on ac-
tual NEs. That means the system annotates the test
data using 17 tags, similar to the training data. Dur-
ing evaluation we have merged the sub-tags of a par-
ticular entity to get a complete NEs and calculated
the accuracies. At the end of section 7.1 we have
also mentioned the accuracies if evaluated on the
tags. A number of experiments are conducted con-
sidering various combinations of features to identify
the best feature set for the Hindi NER task.
7.1 Baseline
The baseline performance of the system without us-
ing gazetteer and context patterns are presented in
Table 1. They are summarized below.
While experimenting with static word features,
we have observed that a window of previous two
Feature Class F-value
f1 = Word, NE Tag
PER 63.33
LOC 69.56
ORG 58.58
DAT 91.76
TOTAL 69.64
f2 = Word, NE Tag,
PER 69.75
LOC 75.8
ORG 59.31
Suffix (? 2) DAT 89.09
TOTAL 73.42
f3 = Word, NE Tag,
PER 70.61
LOC 71
ORG 59.31
Suffix (? 2), Prefix DAT 89.09
TOTAL 72.5
f4 = Word, NE Tag,
PER 70.61
LOC 75.8
ORG 60.54
Digit, Suffix (? 2) DAT 93.8
TOTAL 74.26
f5 = Word, NE Tag, POS
PER 64.25
LOC 71
ORG 60.54
DAT 89.09
TOTAL 70.39
Suffix (? 2), Digit,
PER 72.26
f6 = Word, NE Tag, LOC 78.6
ORG 51.36
NomPSP DAT 92.82
TOTAL 75.6
Table 1: F-values for different features
words to next two words (Wi?2...Wi+2) gives best
results. But when several other features are com-
bined then single word window (Wi?1...Wi+1) per-
forms better. Similarly we have experimented with
suffixes of different lengths and observed that the
suffixes of length ? 2 gives the best result for the
Hindi NER task. In using POS information, we
have observed that the coarse-grained POS tagger
information is more effective than the finer-grained
POS values. A feature set, combining finer-grained
POS values, surrounding words and previous NE
tag, gives a f-value of 70.39%. But when the
coarse-grained POS values are used instead of the
347
finer-grained POS values, the f-value is increased
to 74.16%. The most interesting fact we have ob-
served that more complex features do not guaran-
tee to achieve better results. For example, a feature
set combined with current and surrounding words,
previous NE tag and fixed length suffix information,
gives a f-value 73.42%. But when prefix information
are added the f-value decreased to 72.5%. The high-
est accuracy achieved by the system is 75.6% f-value
without using gazetteer information and context pat-
terns.
The results in Table 1 are obtained by evaluating
on the actual NEs. But when the system is evaluated
on the tags the f-value increases. For f6, the accu-
racy achieved on actual NEs is 75.6%, but if eval-
uated on tags, the value increased to 77.36%. Sim-
ilarly, for f2, the accuracy increased to 75.91% if
evaluated on tags. The reason is the NEs contain-
ing 3 or more words, are subdivided to N-begin, N-
continue (1 or more) and N-end. So if there is an
error in any of the subtags, the total NE becomes
an error. We observed many cases where NEs are
partially identified by the system, but these are con-
sidered as error during evaluation.
7.2 Using Gazetteer Lists and Context Patterns
Next we add gazetteer and context patterns as fea-
tures in our MaxEnt based NER system. In Ta-
ble 2 we have compared the results after addition
of gazetteer information and context patterns with
previous results. While experimenting we have ob-
served that gazetteer lists and context patterns are
capable of increasing the performance of our base-
line system. That is tested on all the baseline feature
sets. In Table 2 the comparison is shown for only
two features - f2 and f6 which are defined in Table 1.
It may be observed that the relative advantage of us-
ing both gazetteer and context patterns together over
using them individually is not much. For example,
when gazetteer information are added with f2, the f-
value is increased by 6.38%, when context patterns
are added the f-value is increased by 6.64%., but
when both are added the increment is 7.27%. This
may be due to the fact that both gazetteer and con-
text patterns lead to the same identifications. Using
the comprehensive feature set (using gazetteer infor-
mation and context patterns) the MaxEnt based NER
system achieves the maximum f-value of 81.52%.
F-value
Fea-
ture
Class No
Gaz
or
Pat
With
Gaz
With
Pat
With
Gaz
and
Pat
f2
PER 69.75 74.2 75.61 76.03
LOC 75.8 82.02 79.94 82.02
ORG 59.31 72.61 73.4 74.63
DAT 89.09 94.29 95.32 95.32
TOTAL 73.42 79.8 80.06 80.69
f6
PER 72.26 76.03 75.61 78.41
LOC 78.6 82.02 80.49 83.26
ORG 51.36 72.61 74.1 75.43
DAT 92.82 94.28 95.87 96.5
TOTAL 75.6 80.24 80.37 81.52
Table 2: F-values for different features with
gazetteers and context patterns
8 Conclusion
We have shown that our MaxEnt based NER sys-
tem is able to achieve a f-value of 81.52%, using a
hybrid set of features including traditional NER fea-
tures augmented with gazetteer lists and extracted
context patterns. The system outperforms the exist-
ing NER systems in Hindi.
Feature selection and feature clustering might
lead to further improvement of performance and is
under investigation.
9 Acknowledgement
The work is partially funded by Microsoft Research
India.
References
Bikel Daniel M., Miller Scott, Schwartz Richard and
Weischedel Ralph. 1997. Nymble: A High Perfor-
mance Learning Name-finder. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing, pages 194?201.
Borthwick Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
Computer Science Department, New York University.
Cucerzan Silviu and Yarowsky David. 1999. Language
Independent Named Entity Recognition Combining
348
Morphological and Contextual Evidence. In Proceed-
ings of the Joint SIGDAT Conference on EMNLP and
VLC 1999, pages 90?99.
Ekbal A. and Bandyopadhyay S. 2007. Lexical Pattern
Learning from Corpus Data for Named Entity Recog-
nition. In Proceedings of International Conference on
Natural Language Processing (ICON), 2007.
Grishman Ralph. 1995. The New York University Sys-
tem MUC-6 or Where?s the syntax? In Proceedings of
the Sixth Message Understanding Conference.
Kumar N. and Bhattacharyya Pushpak. 2006. Named
Entity Recognition in Hindi using MEMM. In Techni-
cal Report, IIT Bombay, India..
Li Wei and McCallum Andrew. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Con-
ditional Random Fields and Feature Induction (Short
Paper). ACM Transactions on Computational Logic.
McDonald D. 1996. Internal and external evidence in the
identification and semantic categorization of proper
names. In B. Boguraev and J. Pustejovsky, editors,
Corpus Processing for Lexical Acquisition, pages 21?
39.
Pietra Stephen Della, Pietra Vincent Della and Lafferty
John. 1997. Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 19(4):380?393.
Srihari R., Niu C. and Li W. 2000. A Hybrid Approach
for Named Entity and Sub-Type Tagging. In Proceed-
ings of the sixth conference on Applied natural lan-
guage processing.
Talukdar Pratim P., Brants T., Liberman M., and Pereira
F. 2006. A context pattern induction method
for named entity extraction. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X).
Wakao T., Gaizauskas R. and Wilks Y. 1996. Evaluation
of an algorithm for the recognition and classification
of proper names. In Proceedings of COLING-96.
349
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 17?24,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
A Hybrid Approach for Named Entity Recognition in Indian Languages
Sujan Kumar Saha Sanjay Chatterji Sandipan Dandapat
Indian Institute of Technology Indian Institute of Technology Indian Institute of Technology
Kharagpur, West Bengal Kharagpur, West Bengal Kharagpur, West Bengal
India - 721302 India - 721302 India - 721302
sujan.kr.saha@gmail.com sanjay chatter@yahoo.com sandipan@cse.iitkgp.ernet.in
Sudeshna Sarkar Pabitra Mitra
Indian Institute of Technology Indian Institute of Technology
Kharagpur, West Bengal Kharagpur, West Bengal
India - 721302 India - 721302
shudeshna@gmail.com pabitra@gmail.com
Abstract
In this paper we describe a hybrid system
that applies Maximum Entropy model (Max-
Ent), language specific rules and gazetteers
to the task of Named Entity Recognition
(NER) in Indian languages designed for the
IJCNLP NERSSEAL shared task. Starting
with Named Entity (NE) annotated corpora
and a set of features we first build a base-
line NER system. Then some language spe-
cific rules are added to the system to recog-
nize some specific NE classes. Also we have
added some gazetteers and context patterns
to the system to increase the performance.
As identification of rules and context pat-
terns requires language knowledge, we were
able to prepare rules and identify context
patterns for Hindi and Bengali only. For the
other languages the system uses the MaxEnt
model only. After preparing the one-level
NER system, we have applied a set of rules
to identify the nested entities. The system
is able to recognize 12 classes of NEs with
65.13% f-value in Hindi, 65.96% f-value in
Bengali and 44.65%, 18.74%, and 35.47%
f-value in Oriya, Telugu and Urdu respec-
tively.
1 Introduction
Named entity recognition involves locating and clas-
sifying the names in text. NER is an important
task, having applications in Information Extraction
(IE), Question Answering (QA), Machine Transla-
tion (MT) and in most other NLP applications.
This paper presents a Hybrid NER system for In-
dian languages which is designed for the IJCNLP
NERSSEAL shared task competition, the goal of
which is to perform NE recognition on 12 types
of NEs - person, designation, title-person, organiza-
tion, abbreviation, brand, title-object, location, time,
number, measure and term.
In this work we have identified suitable features
for the Hindi NER task. Orthography features, suf-
fix and prefix information, morphology informa-
tion, part-of-speech information as well as informa-
tion about the surrounding words and their tags are
used to develop a MaxEnt based Hindi NER sys-
tem. Then we realized that the recognition of some
classes will be better if we apply class specific lan-
guage rules in addition to the MaxEnt model. We
have defined rules for time, measure and number
classes. We made gazetteers based identification for
designation, title-person and some terms. Also we
have used person and location gazetteers as features
of MaxEnt for better identification of these classes.
Finally we have built a module for semi-automatic
extraction of context patterns and extracted context
patterns for person, location, organization and title-
object classes and these are added to the baseline
NER system.
The shared task was defined to build the NER sys-
tems for 5 Indian languages - Hindi, Bengali, Oriya,
Telugu and Urdu for which training data was pro-
17
vided. Among these 5 languages only Bengali and
Hindi are known to us but we have no knowledge for
other 3 languages. So we are unable to build rules
and extract context patterns for these languages. The
NER systems for these 3 languages contain only
the baseline system i.e. the MaxEnt system. Also
our baseline MaxEnt NER system uses morphologi-
cal and parts-of-speech (POS) information as a fea-
ture. Due to unavailability of morphological ana-
lyzer and POS tagger for these 3 languages, these in-
formation are not added to the systems. Among the
3 languages, only for Oriya NER system we have
used small gazetteers for person, location and des-
ignation extracted from the training data. For Ben-
gali and Hindi the developed systems are complete
hybrid systems containing rules, gazetteers, context
patterns and the MaxEnt model.
The paper is organized as follows. A brief sur-
vey of different techniques used for the NER task
in different languages and domains are presented in
Section 2. Also a brief survey on nested NE recog-
nition system is presented here. A discussion on
the training data is given in Section 3. The MaxEnt
based NER system is described in Section 4. Vari-
ous features used in NER are then discussed. Next
we present the experimental results and related dis-
cussions in Section 8. Finally Section 9 concludes
the paper.
2 Previous Work
A variety of techniques has been used for NER. The
two major approaches to NER are:
1. Linguistic approaches.
2. Machine Learning (ML) based approaches.
The linguistic approaches typically use rules man-
ually written by linguists. There are several rule-
based NER systems, containing mainly lexicalized
grammar, gazetteer lists, and list of trigger words,
which are capable of providing 88%-92% f-measure
accuracy for English (Grishman, 1995; McDonald,
1996; Wakao et al, 1996).
The main disadvantages of these rule-based tech-
niques are that these require huge experience and
grammatical knowledge of the particular language
or domain and these systems are not transferable to
other languages or domains.
ML based techniques for NER make use of a
large amount of NE annotated training data to ac-
quire high level language knowledge. Several ML
techniques have been successfully used for the NER
task of which Hidden Markov Model (HMM) (Bikel
et al, 1997), Maximum Entropy (MaxEnt) (Borth-
wick, 1999), Conditional Random Field (CRF) (Li
and Mccallum, 2004) are most common. Combina-
tions of different ML approaches are also used. Sri-
hari et al (2000) combines MaxEnt, Hidden Markov
Model (HMM) and handcrafted rules to build an
NER system.
NER systems use gazetteer lists for identifying
names. Both the linguistic approach (Grishman,
1995; Wakao et al, 1996) and the ML based ap-
proach (Borthwick, 1999; Srihari et al, 2000) use
gazetteer lists.
Linguistic approach uses handcrafted rules which
needs skilled linguistics. Some recent approaches
try to learn context patterns through ML which re-
duce amount of manual labour. Talukder et al(2006)
combined grammatical and statistical techniques to
create high precision patterns specific for NE extrac-
tion. An approach to lexical pattern learning for In-
dian languages is described by Ekbal and Bandopad-
hyay (2007). They used seed data and annotated cor-
pus to find the patterns for NER.
The NER task for Hindi has been explored by
Cucerzan and Yarowsky in their language indepen-
dent NER work which used morphological and con-
textual evidences (Cucerzan and Yarowsky, 1999).
They ran their experiment with 5 languages - Roma-
nian, English, Greek, Turkish and Hindi. Among
these the accuracy for Hindi was the worst. For
Hindi the system achieved 41.70% f-value with a
very low recall of 27.84% and about 85% precision.
A more successful Hindi NER system was devel-
oped by Wei Li and Andrew Mccallum (2004) using
Conditional Random Fields (CRFs) with feature in-
duction. They were able to achieve 71.50% f-value
using a training set of size 340k words. In Hindi
the maximum accuracy is achieved by Kumar and
Bhattacharyya, (2006). Their Maximum Entropy
Markov Model (MEMM) based model gives 79.7%
f-value.
All the NER systems described above are able
to detect one-level NEs. In recent years, the inter-
est in detection of nested NEs has increased. Here
18
we mention few attempts for nested NE detection.
Zhou et al (2004) described an approach to iden-
tify cascaded NEs from biomedical texts. They de-
tected the innermost NEs first and then they derived
rules to find the other NEs containing these as sub-
strings. Another approach, described by McDonald
et al (2005), uses structural multilevel classifica-
tion to deal with overlapping and discontinuous enti-
ties. B. Gu (2006) has treated the task of identifying
the nested NEs a binary classification problem and
solved it using support vector machines. For each
token in nested NEs, they used two schemes to set
its class label: labeling as the outermost entity or the
inner entities.
3 Training Data
The data used for the training of the systems was
provided. The annotated data uses Shakti Standard
Format (SSF). For our development we have con-
verted the SSF format data into the IOB formatted
text in which a B ? XXX tag indicates the first
word of an entity type XXX and I?XXX is used
for subsequent words of an entity. The tag O indi-
cates the word is outside of a NE. The training data
for Hindi contains more than 5 lakh words, for Ben-
gali about 160K words and about 93K, 64K and 36K
words for Oriya, Telugu and Urdu respectively.
In time of development we have observed that
the training data, provided by the organizers of the
shared task, contains several types of errors in NE
tagging. These errors in the training corpora affects
badly to the machine learning (ML) based models.
But we have not made corrections of the errors in
the training corpora in time of our development. All
the results shown in the paper are obtained using the
provided corpora without any modification in NE
annotation.
4 Maximum Entropy Based Model
We have used MaxEnt model to build the baseline
NER system. MaxEnt is a flexible statistical model
which assigns an outcome for each token based on
its history and features. Given a set of features and a
training corpus, the MaxEnt estimation process pro-
duces a model. For our development we have used
a Java based open-nlp MaxEnt toolkit1 to get the
1www.maxent.sourceforge.net
probability values of a word belonging to each class.
That is, given a sequence of words, the probability
of each class is obtained for each word. To find the
most probable tag corresponding to each word of a
sequence, we can choose the tag having the highest
class conditional probability value. But this method
is not good as it might result in an inadmissible as-
signment.
Some tag sequences should never happen. To
eliminate these inadmissible sequences we have
made some restrictions. Then we used a beam
search algorithm with a beam of length 3 with these
restrictions.
4.1 Features
MaxEnt makes use of different features for identify-
ing the NEs. Orthographic features (like capitaliza-
tion, decimal, digits), affixes, left and right context
(like previous and next words), NE specific trigger
words, gazetteer features, POS and morphological
features etc. are generally used for NER. In En-
glish and some other languages, capitalization fea-
tures play an important role as NEs are generally
capitalized for these languages. Unfortunately this
feature is not applicable for the Indian languages.
Also Indian person names are more diverse, lots of
common words having other meanings are also used
as person names. Li and Mccallum (2004) used the
entire word text, character n-grams (n = 2, 3, 4),
word prefix and suffix of lengths 2, 3 and 4, and 24
Hindi gazetteer lists as atomic features in their Hindi
NER. Kumar and Bhattacharyya (2006) used word
features (suffixes, digits, special characters), context
features, dictionary features, NE list features etc. in
their MEMM based Hindi NER system. In the fol-
lowing we have discussed about the features we have
identified and used to develop the Indian language
NER systems.
Static Word Feature: The previous and next
words of a particular word are used as features. The
previous m words (wi?m...wi?1) to next n words
(wi+1...wi+n) can be considered. During our exper-
iment different combinations of previous 4 to next 4
words are used.
Context Lists: Context words are defined as the
frequent words present in a word window for a par-
ticular class. We compiled a list of the most frequent
words that occur within a window of wi?3...wi+3
19
of every NE class. For example, location con-
text list contains the words like ?jAkara2? (go-
ing to), ?desha? (country), ?rAjadhAnI? (capital)
etc. and person context list contains ?kahA? (say),
?pradhAnama.ntrI? (prime minister) etc. For a
given word, the value of this feature correspond-
ing to a given NE type is set to 1 if the window
wi?3...wi+3 around the wi contains at last one word
from this list.
Dynamic NE tag: Named Entity tags of the pre-
vious words (ti?m...ti?1) are used as features.
First Word: If the token is the first word of a
sentence, then this feature is set to 1. Otherwise, it
is set to 0.
Contains Digit: If a token ?w? contains digit(s)
then the feature ContainsDigit is set to 1.
Numerical Word: For a token ?w? if the word
is a numerical word i.e. a word denoting a number
(e.g. eka (one), do (two), tina (three) etc.) then the
feature NumWord is set to 1.
Word Suffix: Word suffix information is helpful
to identify the NEs. Two types of suffix features
have been used. Firstly a fixed length word suffix of
the current and surrounding words are used as fea-
tures. Secondly we compiled lists of common suf-
fixes of person and place names in Hindi. For ex-
ample, ?pura?, ?bAda?, ?nagara? etc. are location
suffixes. We used binary features corresponding to
the lists - whether a given word has a suffix from a
particular list.
Word Prefix: Prefix information of a word may
also be helpful in identifying whether it is a NE. A
fixed length word prefix of current and surrounding
words are treated as features.
Root Information of Word: Indian languages
are morphologically rich. Words are inflected in var-
ious forms depending on its number, tense, person,
case etc. Identification of NEs becomes difficult for
these inflections. The task becomes easier if instead
of the inflected words, corresponding root words are
checked whether these are NE or not. For that task
we have used morphological analyzers for Hindi and
Bengali which are developed at IIT kharagpur.
Parts-of-Speech (POS) Information: The POS
of the current word and the surrounding words may
2All Hindi words are written in italics using the ?Itrans?
transliteration
be useful feature for NER. We have accessed to
Hindi and Bengali POS taggers developed at IIT
Kharagpur which has accuracy about 90%. The
tagset of the tagger contains 28 tags. We have used
the POS values of the current and surrounding to-
kens as features.
We realized that the detailed POS tagging is not
very relevant. Since NEs are noun phrases, the noun
tag is very relevant. Further the postposition follow-
ing a name may give a clue to the NE type for Hindi.
So we decided to use a coarse-grained tagset with
only three tags - nominal (Nom), postposition (PSP)
and other (O).
The POS information is also used by defining sev-
eral binary features. An example is the NomPSP
binary feature. The value of this feature is defined
to be 1 if the current token is nominal and the next
token is a PSP.
5 Language Specific Rules
After building of the MaxEnt model we have ob-
served that only a small set of rules are able to iden-
tify the classes like number, measure, time, more ef-
ficiently than the MaxEnt based model. Then we
have tried to define the rules for these classes. The
rule identification is done manually and requires lan-
guage knowledge. We have defined the required
rules for Bengali and Hindi but we are unable to do
the same for other 3 languages as the languages are
unknown to us. In the following we have mentioned
some example rules which are defined and used in
our system.
? IF ((Wi is a number or numeric word) AND
(Wi+1 is an unit))
THEN (Wi Wi+1) bigram is a measure NE.
? IF ((Wi is a number or numeric word) AND
(Wi+1 is a month-name) AND (Wi+2 is a 4
digit number))
THEN (Wi Wi+1 Wi+2) trigram is a time NE.
? IF ((Wi denotes a day of a week) AND (Wi+1
is a number or numeric word) AND (Wi+2 is a
month name))
THEN (Wi Wi+1 Wi+2) trigram is a time NE.
We have defined 36 rules in total for time, mea-
sure and number classes. These rules use some lists
20
which are built. These lists contain correspond-
ing entries both in the target language and in En-
glish. For example the months names list contains
the names according to the English calender and the
names according to the Indian calender. In the fol-
lowing we have mentioned the lists we have pre-
pared for the rule-based module.
? Names of months.
? Names of seasons.
? Days of a week.
? Names of units.
? Numerical words.
5.1 Semi-automatic Extraction of Context
Patterns
Similar to the rules defined for time, measure and
date classes, if efficient context patterns (CP) can
be extracted for a particular class, these can help
in identification of NEs of the corresponding class.
But extraction of CP requires huge labour if done
manually. We have developed a module for semi-
automatically extraction of context patterns. This
module makes use of the most frequent entities of
a particular class as seed for that class and finds the
surrounding tokens of the seed to extract effective
patterns. We mark a pattern as ?effective? if the pre-
cision of the pattern is very high. Precision of a pat-
tern is defined as the ratio of correct identification
and the total identification when the pattern is used
to identify NEs of a particular type from a text.
For our task we have extracted patterns for per-
son, location, organization and title-object classes.
These patterns are able to identify the NEs of a spe-
cific classes but detection of NE boundary is not
done properly by the patterns. For boundary detec-
tion we have added some heuristics and used POS
information of the surrounding words. The patterns
for a particular class may identify the NEs of other
classes also. For example the patterns for identify-
ing person names may also identify the designation
or title-persons. These need to be handled carefully
at the time of using patterns. In the following some
example patterns are listed which are able to identify
person names for Hindi.
? <PER> ne kahA ki
? <PER> kA kathana he.n
? mukhyama.ntrI <PER> Aja
? <PER> ne apane gra.ntha
? <PER> ke putra <PER>
6 Use of Gazetteer Lists
Lists of names of various types are helpful in name
identification. Firstly we have prepared the lists us-
ing the training corpus. But these are not sufficient.
Then we have compiled some specialized name lists
from different web sources. But the names in these
lists are in English, not in Indian languages. So we
have transliterated these English name lists to make
them useful for our NER task.
Using transliteration we have constructed several
lists. Which are, month name and days of the week,
list of common locations, location names list, first
names list, middle names list, surnames list etc.
The lists can be used in name identification in var-
ious ways. One way is to check whether a token is in
any list. But this approach is not good as it has some
limitations. Some words may present in two or more
gazetteer lists. Confusions arise to make decisions
for these words. Some words are in gazetteer lists
but sometimes these are used in text as not-name en-
tity. We have used these gazetteer lists as features of
MaxEnt. We have prepared several binary features
which are defined as whether a given word is in a
particular list.
7 Detection of Nested Entities
The training corpora used for the models, are not
annotated as nested. The maximal entities are an-
notated in the training corpus. For detection of the
nested NEs, we have derived some rules. For exam-
ple, if a particular word is a number or numeric word
and is a part of a NE type other than ?number?, then
we have made the nesting. Again, if any common lo-
cation identifier word like, jilA (district), shahara
(town) etc. is a part of a ?location? entity then we
have nested there. During one-level NE identifica-
tion, we have generated lists for all the identified lo-
cation and person names. Then we have searched
other NEs containing these as substring to make the
21
nesting. After preparing the one-level NER system,
we have applied the derived rules on it to identify
the nested entities.
8 Evaluation
The accuracies of the system are measured in terms
of the f-measure, which is the weighted harmonic
mean of precision and recall. Nested, maximal and
lexical accuracies are calculated separately. The
test data for all the five languages are provided.
The size of the shared task test files are: Hindi
- 38,704 words, Bengali - 32,796 words, Oriya -
26,988 words, Telugu - 7,076 words and Urdu -
12,805 words.
We have already mentioned that after preparing
a one-level NER system, the rule-based module is
used to modify it to a nested one. A number of ex-
periments are conducted considering various combi-
nations of features to identify the best feature set for
Indian language NER task. It is very difficult and
time consuming to conduct experiments for all the
languages. During the development we have con-
ducted all the experiments on Hindi and Bengali. We
have prepared a development test data composed of
24,265 words for Hindi and 10,902 word for Ben-
gali and accuracies of the system are tested on the
development data. The details of the experiments on
Hindi data for the best feature selection is described
in the following section.
8.1 Best Feature Set Selection
The performance of the system on the Hindi data
using various features are presented in Table 1.
They are summarized below. While experimenting
with static word features, we have observed that a
window of previous two words to next two words
(Wi?2...Wi+2) gives best results. But when sev-
eral other features are combined then smaller win-
dow (Wi?1...Wi+1) performs better. Similarly we
have experimented with suffixes of different lengths
and observed that the suffixes of length ? 2 gives
the best result for the Hindi NER task. In using
POS information, we have observed that the coarse-
grained POS tagger information is more effective
than the finer-grained POS values. The most in-
teresting fact we have observed that more complex
features do not guarantee to achieve better results.
For example, a feature set combined with current
and surrounding words, previous NE tag and fixed
length suffix information, gives a f-value 64.17%.
But when prefix information are added the f-value
decreased to 63.73%. Again when the context lists
are added to the feature set containing words, previ-
ous tags, suffix information, digit information and
the NomPSP binary feature, the accuracy has de-
creased to 67.33% from 68.0%.
Feature Overall
F-value
Word, NE Tag 58.92
Word, NE Tag, Suffix (? 2) 64.17
Word, NE Tag, Suffix (? 2),
Prefix
63.73
Word, NE Tag, Digit, Suffix 66.61
Word, NE Tag, Context List 63.57
Word, NE Tag, POS (full) 61.28
Word, NE Tag, Suffix (? 2),
Digit, NomPSP
68.60
Word, NE Tag, Suffix (? 2),
Digit, Context List, NomPSP
67.33
Word, NE Tag, Suffix (?
2), Digit, NomPSP, Linguis-
tic Rules
73.40
Word, NE Tag, Suffix (? 2),
Digit, NomPSP, Gazetteers
72.08
Word, NE Tag, Suffix (?
2), Digit, NomPSP, Linguis-
tic Rules, Gazetteers
74.53
Table 1: Hindi development set f-values for different
features
The feature set containing words, previous
tags, suffix information, digit information and the
NomPSP binary feature is the identified best feature
set without linguistic rules and gazetteer informa-
tion. Then we have added the linguistic rules, pat-
terns and gazetteer information to the system and the
changes in accuracies are shown in the table.
8.2 Results on the Test Data
The best identified feature set is used for the de-
velopment of the NER systems for all the five lan-
guages. We have already mentioned that for only
for Bengali and Hindi we have added linguistic rules
22
and gazetteer lists in the MaxEnt based NER sys-
tems. The accuracy of the system on the shared task
test data for all the languages are shown in Table 2.
Lan-
guage
Type Preci-
sion
Recall F-
measure
Bengali
Maximal 52.92 68.07 59.54
Nested 55.02 68.43 60.99
Lexical 62.30 70.07 65.96
Hindi
Maximal 75.19 58.94 66.08
Nested 79.58 58.61 67.50
Lexical 82.76 53.69 65.13
Oriya
Maximal 21.17 26.92 23.70
Nested 27.73 28.13 27.93
Lexical 51.51 39.40 44.65
Telugu
Maximal 10.47 9.64 10.04
Nested 22.05 13.16 16.48
Lexical 25.23 14.91 18.74
Urdu
Maximal 26.12 29.69 27.79
Nested 27.99 29.21 28.59
Lexical 37.58 33.58 35.47
Table 2: Accuracy of the system for all languages
The accuracies of Oriya, Telugu and Urdu lan-
guages are poor compared to the other two lan-
guages. The reasons are POS information, mor-
phological information, language specific rules and
gazetteers are not used for these languages. Also the
size of training data for these languages are smaller.
To mention, for Urdu, size of the training data is only
about 36K words which is very small to train a Max-
Ent model.
It is mentioned that we have prepared a set of rules
which are capable of identifying the nested NEs.
Once the one-level NER system has built, we have
applied the rules on it. In Table 3 we have shown
the f-values of each class after addition of the nested
rules. The detailed results for all languages are not
shown. In the table we have shown only the results
of Bengali and Hindi.
For both the languages ?title-person? and ?desig-
nation? classes are suffering from poor accuracies.
The reason is, in the training data and also in the
annotated test data, these classes contains many an-
notation errors. Also the classes being closely re-
lated to each other, the system fails to distinguish
them properly. The detection of the ?term? class is
Hindi Bengali
Class Maximal Nested Maximal Nested
Person 70.87 71.00 77.45 79.09
Desig-
nation
48.98 59.81 26.32 26.32
Organi-
zation
47.22 47.22 41.43 71.43
Abbre-
viation
- 72.73 51.61 51.61
Brand - - - -
Title-
person
- 60.00 5.19 47.61
Title-
object
41.32 40.98 72.97 72.97
Location 86.02 87.02 76.27 76.27
Time 67.42 67.42 56.30 56.30
Number 84.59 85.13 40.65 40.65
Measure 59.26 55.17 62.50 62.50
Term 48.91 50.51 43.67 43.67
Table 3: Comparison of maximal and nested f-
values for different classes of Hindi and Bengali
very difficult. In the test files amount of ?term? en-
tity is large, for Bengali - 434 and for Hindi - 1080,
so the poor accuracy of the class affects badly to the
overall accuracy. We have made rule-based identi-
fication for ?number?, ?measure? and ?time? classes;
the accuracies of these classes proves that the rules
need to be modified to achieve better accuracy for
these classes. Also the accuracy of the ?organiza-
tion? class is not high, because amount of organiza-
tion entities is not sufficient in the training corpus.
We have achieved good results for other two main
classes - ?person? and ?location?.
8.3 Comparison with Other Shared Task
Systems
The comparison of the accuracies of our system
and other shared task systems is given in Table 4.
From the comparison we can see that our system
has achieved the best accuracies for most of the lan-
guages.
9 Conclusion
We have prepared a MaxEnt based system for the
NER task in Indian languages. We have also added
23
Lan-
guage
Our S2 S6 S7
Bengali 65.96 39.77 40.63 59.39
Hindi 65.13 46.84 50.06 33.12
Oriya 44.65 45.84 39.04 28.71
Telugu 18.74 46.58 40.94 4.75
Urdu 35.47 44.73 43.46 35.52
Table 4: Comparison of our lexical f-measure accu-
racies with the systems : S2 - Praveen P.(2008), S6 -
Gali et al(2008) and S7 - Ekbal et al(2008)
rules and gazetteers for Bengali and Hindi. Also our
derived rules need to be modified for improvement
of the system. We have not made use of rules and
gazetteers for Oriya, Telugu and Urdu. As the size
of training data is not much for these 3 languages,
rules and gazetteers would be effective. We have
experimented with MaxEnt model only, other ML
methods like HMM, CRF or MEMM may be able
to give better accuracy. We have not worked much
on the detection of nested NEs. Proper detection of
nested entities may lead to further improvement of
performance and is under investigation.
References
Bikel Daniel M., Miller Scott, Schwartz Richard and
Weischedel Ralph. 1997. Nymble: A High Perfor-
mance Learning Name-finder. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing, 194?201.
Borthwick Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
Computer Science Department, New York University.
Cucerzan Silviu and Yarowsky David. 1999. Language
Independent Named Entity Recognition Combining
Morphological and Contextual Evidence. In Proceed-
ings of the Joint SIGDAT Conference on EMNLP and
VLC 1999, 90?99.
Ekbal A. and Bandyopadhyay S. 2007. Lexical Pattern
Learning from Corpus Data for Named Entity Recog-
nition. In Proceedings of International Conference on
Natural Language Processing (ICON), 2007.
Ekbal A., Haque R., Das A., Poka V. and Bandyopad-
hyay S. 2008. Language Independent Named Entity
Recognition in Indian Languages In Proceedings of
IJCNLP workshop on NERSSEAL. (Accepted)
Gali K., Surana H., Vaidya A., Shishtla P. and Misra
Sharma D. 2008. Named Entity Recognition through
CRF Based Machine Learning and Language Specific
Heuristics In Proceedings of IJCNLP workshop on
NERSSEAL. (Accepted)
Grishman Ralph. 1995. The New York University Sys-
tem MUC-6 or Where?s the syntax? In Proceedings of
the Sixth Message Understanding Conference.
Gu B. 2006. Recognizing Nested Named Entities in GE-
NIA corpus. In Proceedings of the BioNLP Workshop
on Linking Natural Language Processing and Biology
at HLT-NAACL 06, pages 112-113.
Kumar N. and Bhattacharyya Pushpak. 2006. Named
Entity Recognition in Hindi using MEMM. In Techni-
cal Report, IIT Bombay, India..
Li Wei and McCallum Andrew. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Condi-
tional Random Fields and Feature Induction (Short Pa-
per). In ACM Transactions on Computational Logic.
McDonald D. 1996. Internal and external evidence in the
identification and semantic categorization of proper
names. In B. Boguraev and J. Pustejovsky, editors,
Corpus Processing for Lexical Acquisition, 21?39.
McDonald R., Crammer K. and Pereira F. 2005. Flexible
text segmentation with structured multilabel classifica-
tion. In Proceedings of EMNLP05.
Praveen P. 2008. Hybrid Named Entity Recogni-
tion System for South-South East Indian Languages.
InProceedings of IJCNLP workshop on NERSSEAL.
(Accepted)
Srihari R., Niu C. and Li W. 2000. A Hybrid Approach
for Named Entity and Sub-Type Tagging. In Proceed-
ings of the sixth conference on Applied natural lan-
guage processing.
Talukdar Pratim P., Brants T., Liberman M., and Pereira
F. 2006. A context pattern induction method
for named entity extraction. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X).
Wakao T., Gaizauskas R. and Wilks Y. 1996. Evaluation
of an algorithm for the recognition and classification
of proper names. In Proceedings of COLING-96.
Zhou G., Zhang J., Su J., Shen D. and Tan C. 2004.
Recognizing Names in Biomedical Texts: a Machine
Learning Approach. Bioinformatics, 20(7):1178-
1190.
24
Gazetteer Preparation for Named Entity Recognition in Indian Languages
Sujan Kumar Saha
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
sujan.kr.saha@gmail.com
Sudeshna Sarkar
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
shudeshna@gmail.com
Pabitra Mitra
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
pabitra@gmail.com
Abstract
This paper describes our approaches for the
preparation of gazetteers for named entity
recognition (NER) in Indian languages. We
have described two methodologies for the
preparation of gazetteers1. Since the rel-
evant gazetteer lists are more easily avail-
able in English we have used a translitera-
tion based approach to convert available En-
glish name lists to Indian languages. The
second approach is a context pattern induc-
tion based domain specific gazetteer prepa-
ration. This approach uses a domain specific
raw corpus and a few seed entities to learn
context patterns and then the corresponding
name lists are generated by using bootstrap-
ping.
1 Introduction
Named entity recognition involves locating and clas-
sifying the names in text. NER is an important task,
having applications in information extraction (IE),
question answering (QA), machine translation and
in most other NLP applications.
NER systems have been developed for resource-
rich languages like English with very high accura-
cies. But constructing an NER for a resource-poor
language is very challenging due to unavailability of
proper resources. Name-dictionaries or gazetteers
are very helpful NER resources and in most Indian
1Specialized list of names for a particular class of Named
Entity (NE). For Example, India is in the location gazetteer,
Sachin is in the person first name gazetteer.
languages there is no reasonable size publicly avail-
able list. The web contains lots of such resources,
which can be used for Indian language NER devel-
opment. But most of the web resources are in En-
glish. Our approach is to transliterate the relevant
English resources and name dictionaries into Indian
languages to make them useful for Indian language
NER task. But direct transliteration from English to
an Indian languages is not easy. Few attempts are
taken to build English to Indian language transliter-
ation systems but the word agreement ratio (WAR)
reached is upto 69.3% (Ekbal et al, 2006).
We have attempted to build a transliteration sys-
tem which uses an intermediate alphabet. Both
the English and the Indian language strings are
transliterated to the intermediate alphabet and for a
English-Indian language pair, if the transliterated in-
termediate alphabet strings are same then we have
concluded that the strings are the transliteration of
one another. We have transliterated the available En-
glish name lists into the intermediate alphabet and
these might be used as gazetteers. The Indian lan-
guage words need to be transliterated to the inter-
mediate format to check whether the word is in a
gazetteer or not. This system does not transliter-
ate the English name lists into Indian languages but
makes them useful in Indian languages NER task.
Transliteration based approaches are useful when
there is availability of English name lists. But when
relevant English name lists are not available then
also we can prepare gazetteers from raw corpus.
We have defined a semi-automatic context pattern
(CP) extraction based gazetteer preparation frame-
work. This framework uses bootstrapping to prepare
The 6th Workshop on Asian Languae Resources, 2008
9
the gazetteers from a large raw corpus starting from
few seed entities. Firstly fixed length patterns are
formed using the surrounding words of the seeds.
Depending on the pattern precision, the patterns are
discarded or generalized by dropping tokens from
the patterns. This set of high precision patterns ex-
tracts other named entities (NEs) which are added to
the seed list for the next iteration of the process. Fi-
nally we are able to prepare the required gazetteers.
To prove the effectiveness of the gazetteer prepa-
ration approach, we have prepared some gazetteers
like names of cricketers, names of tennis players etc.
from a raw Hindi sports domain corpus. The de-
tails of the approaches are given in the following
sections.
The paper is organized as follows. Usefulness
of gazetteers in NER, transliteration approaches in
general and specific for Indian languages and gen-
eral pattern extraction methodologies are discussed
in section 2. Section 3 presents the architecture of
the 2-phase transliteration system and preparation of
gazetteers using that. In section 4 context pattern
extraction based gazetteer preparation is discussed.
Finally section 5 concludes the paper.
2 Previous Work
The main approaches to NER are Linguistic ap-
proaches and Machine Learning (ML) based ap-
proaches. The linguistic approach typically uses
rule-based models manually written by linguists.
ML based techniques make use of a large amount
of annotated training data to acquire high-level lan-
guage knowledge. Several ML techniques like Hid-
den Markov Model (HMM)(Bikel et al, 1997),
Maximum Entropy Model(MaxEnt) (Borthwick,
1999), Conditional Random Field(CRF) (Li and
McCallum, 2004) etc. have been successfully used
for the NER task. Both the approaches may make
use of gazetteer information to build systems. There
are many systems which use gazetteers to improve
the accuracy.
Ralph Grishman has developed a rule-based NER
system which uses some specialized name dictionar-
ies including names of all countries, names of major
cities, names of companies, common first names etc
(Grishman, 1995). Another rule based NER system
is developed by Wakao et al (1996) which has used
several gazetteers like organization names, location
names, person names, human titles etc.
We will now mention some ML based sys-
tems. MENE is a MaxEnt based system de-
veloped by Borthwick. This system has used 8
dictionaries (Borthwick, 1999), which are: First
names (1,245), Corporate names (10,300), Corpo-
rate names without suffix (10,300), Colleges and
Universities (1,225), Corporate suffixes (244), Date
and Time (51) etc. The italics numbers in bracket
indicates the size of the dictionaries. The hy-
brid system developed by Srihari et al(2000) com-
bines several modules built by using MaxEnt, HMM
and handcrafted rules. This system uses the fol-
lowing gazetteers: First name (8,000), Family
name (14,000) and a big gazetteer of Locations
(250,000). There are many other systems which
have used name dictionaries to improve the accu-
racy. Kozareva (2006) described a methodology
to generate gazetteer lists automatically for Spanish
and to build NER system with labeled and unlabeled
data. The location gazetteer is built by finding loca-
tion patterns which looks for specific prepositions.
And the person gazetteer is constructed with graph
exploration algorithm.
Transliteration is also a very important topic
and lots of transliteration systems for different lan-
guages have been developed using different ap-
proaches. The basic approaches for transliteration
are phoneme based or spelling-based. A phoneme-
based statistical transliteration system from Ara-
bic to English was developed by Knight and
Graehl(1998). This system uses a finite state
transducer that implements transformation rules to
do back-transliteration. A spelling-based model
that directly maps English letter sequences into
Arabic letters was developed by Al-Onaizan and
Knight(2002). Several transliteration systems ex-
ist for English-Japanese, English-Chinese, English-
Spanish and many other languages to English. But
very few attempts have been reported on the de-
velopment of transliteration systems between Indian
languages and English. We can mention a transliter-
ation system for Bengali-English transliteration de-
veloped by Ekbal et al(2006). They have proposed
different models modifying the joint source chan-
nel model. In that system a Bengali string is di-
vided into transliteration units containing a vowel
The 6th Workshop on Asian Languae Resources, 2008
10
modifier or matra at the end of each unit. Sim-
ilarly the English string is also divided into units.
Then they defined various unigram, bigram or tri-
gram models depending on the consideration of the
contexts of the units. They have also considered lin-
guistic knowledge in the form of possible conjuncts
and diphthongs in Bengali and their representations
in English. This system is capable of transliterat-
ing mainly person names. The highest transliteration
accuracy achieved by them is 69.3% Word Agree-
ment Ratio (WAR) for Bengali to English and 67.9%
WAR for English to Bengali transliteration.
In the field of IE, patterns play a key role in
identifying relevant pieces of information. Soder-
land et al(1995), Rillof and Jones(1999), Lin et
al.(2003), Downey et al(2004), Etzioni et al(2005)
described different approaches to context pattern in-
duction. Talukder et al(2006) combined grammati-
cal and statistical techniques to create high precision
patterns specific for NE extraction. An approach to
lexical pattern learning for Indian languages is de-
scribed by Ekbal and Bandopadhyay (2007). They
used seed data and annotated corpus to find the pat-
terns for NER.
3 Transliteration based Gazetteer
Preparation
Gazetteers or name dictionaries are helpful in NER.
We have already discussed about some English NER
systems where the usefulness of the gazetteers have
been established. However while developing NER
systems in Indian languages, we tried to find relevant
gazetteers. But we could not obtain openly available
gazetteer lists for these languages. But we found
that there are a lot of resources of names of Indian
persons, Indian places, organizations etc. in English
available in the web. In Table 1 we have mentioned
some of the sources which contains relevant name
lists.
But it is not possible to use the available name
lists directly in the Indian language NER task as
these are in English. We have decided to translit-
erate the English lists into Indian languages to make
them useful in the Indian language NER task.
List Web Sources
http://www.bsnl.co.in/ onlinedi-
rectory.htm
First
Name
http://web1.mtnl.net.in/ direc-
tory/
http://www.eci.gov.in/
http://hiren.info/indian-baby-
names/
http://www.indiaexpress.com/
specials/babynames/
http://surnamedirectory.com/
surname-index.html
Surname http://web1.mtnl.net.in/ direc-
tory/
http://en.wikipedia.org
India Lo-
cation
http://indiavilas.com/ indi-
ainfo/pincodes.asp
http://www.indiapost.gov.in
http://www.eci.gov.in/
World
Location
http://www.maxmind.com/
app/worldcities
http://en.wikipedia.org/wiki
Table 1: Web sources for some relevant name lists
3.1 Transliteration
The transliteration from English to Hindi is quite
difficult. English alphabet contains 26 characters
whereas the Hindi alphabet contains 52 characters.
So the mapping is not trivial. We have already men-
tioned that for Bengali a transliteration system was
developed by Ekbal et al Similar approach can be
used to develop transliteration systems for other In-
dian languages. But this approach uses a bilingual
transliteration corpus, which requires much efforts
to built, is unavailable in proper size in all Indian
languages. Also using this approach the word agree-
ment ratio obtained is below 70%.
To make the transliteration process easier and
more accurate, we have decided to build a 2-phase
transliteration module. Our goal is to make deci-
sion that a particular Indian language string is in an
English gazetteer or not. We need not transliterate
directly from Indian language strings to English or
English name lists into Indian languages. Our idea is
to define an intermediate alphabet and both English
and Indian language strings will be transliterated to
The 6th Workshop on Asian Languae Resources, 2008
11
the intermediate alphabet. For two English-Hindi
string pair, if the intermediate alphabet is same then
we can conclude that one string is the transliteration
of the other.
First of all we need to decide the size of the inter-
mediate alphabet. Preserving the phonetic proper-
ties we have defined our intermediate alphabet con-
sisting of 34 characters. To indicate these 34 charac-
ters, we have given unique character-id to each char-
acter.
3.2 English to Intermediate Alphabet
Transliteration
For transliterating English strings into the interme-
diate state, we have built a phonetic map table. This
phonetic map table maps an English n-gram into an
intermediate character. A part of the map table is
given in Table 2. In the map table, the mapping is
from strings of varying length in the English to one
character in the intermediate alphabet. In our table
the length of the left hand side varies from 1 to 3.
English Intermediate
A a?
EE, I, II ??
OO, U u?
B, W ?b
BH, V v?
CH c?
R, RH r?
SH, S s?
Table 2: A part of the map table
In the following we have described the procedure
of transliteration.
Procedure 1: Transliteration
Source string - English, Output string - Intermediate.
1. Scan the source string (S) from left to right.
2. Extract the first n-gram (G) from the string.
(n = 3)
3. Find if it is in the map-table.
4. If yes, insert its corresponding intermediate
state entity into target string T.
Remove the n-gram from S.
S = S ? G.
Go to step 2.
5. Else set n = n? 1.
Go to step 3.
Here we can take an Indian language name,
?surabhi?, as example to explain the procedure in
details. When the name is written in English, it
may be written in several ways like ?suravi, ?shu-
ravi?, ?surabhee?, ?shurabhi? etc. The English string
?surabhi? is transliterated to ?s?u?r?a?v???? by the translit-
erator. Again if we see the transliteration for ?shu-
ravi?, then also the intermediate transliterated string
is same as the previous one.
3.3 Indian Language to Intermediate Alphabet
Transliteration
This is a 2-phase process. The first phase transliter-
ates the Indian language string into itrans. Itrans is
representation of Indian language alphabets in terms
of ASCII. Since Indian text is composed of syllabic
units rather than individual alphabetic letters, itrans
uses combinations of two or more letters of En-
glish alphabet to represent an Indian language syl-
lable. However, there being multiple sounds in In-
dian languages corresponding to the same English
letter, not all Indian syllables can be represented by
logical combinations of English alphabet. Hence,
itrans uses some non-alphabetic special characters
also in some of the syllables. A map table2, with
some heuristic knowledge, is used for the translit-
eration. For example, the Hindi word ?surabhi? is
converted ?sUrabhI? in itrans.
In the second phase the itrans string is translit-
erated into the intermediate state using the similar
procedure described section 3.2. Here also we use
a map-table containing the mappings from itrans to
intermediate alphabet. This procedure transliterates
the example itrans word ?sUrabhI? to ?s?u?r?a?v????.
3.4 Evaluation
In section 3.2 and 3.3 we have described two phase
transliteration with an example word. We have
shown that our transliteration system transliterates
the Indian language name ?surabhi? and the corre-
sponding English strings into the same intermediate
2The map table is available at www.aczoom.com/itrans.
The 6th Workshop on Asian Languae Resources, 2008
12
string. The system has limitations like sometimes
two different strings can be mapped into a same in-
termediate alphabet string.
For the evaluation of the system we have applied
the transliteration system for two languages - Hindi
and Bengali. For evaluating the system for Hindi
we have created a bi-lingual corpus containing 1070
English-Hindi word pair most of which are names.
980 of them are transliterated correctly by the sys-
tem. The system accuracy is 980 ? 100/1070 =
91.59%. For evaluating the system for Bengali, we
have used a similar bi-lingual corpus and the system
transliterates with 89.3% accuracy.
3.5 Prepared Gazetteer Lists
Previously we have mentioned the web sources
where some name lists are available. Names of
a particular category are collected from different
sources and merged to build a English name list of
that category. Then we have applied our transliter-
ation procedure on the list and transliterated the list
into the intermediate alphabet. This intermediate al-
phabet list acts as a gazetteer in NER task in Indian
languages. When an Indian language NER system
needs to access the gazetteer lists, it transliterates the
Indian language strings into the intermediate alpha-
bet, and searches into the list. In the following we
have described the prepared gazetteer lists which are
useful for a general domain Indian language NER
system.
First Name List: This list contains 10,200 first
names collected from the web. Most of the collected
first names are of Indian origin. Apart from the In-
dian names, we have also collected some non-Indian
names. These non-Indian names are generally the
names of some famous persons, like sports stars,
film stars, scientists, politicians, who are likely to
come in Indian language texts. In our first name list
1500 such names are included.
Surname List: This is a very important list which
contains common surnames. We have prepared the
surname list from different sources containing about
1500 Indian surnames and 400 other surnames.
Indian Locations: This list contains about
14,000 entities. The names of states, cities and
towns, districts, important places in different cities
and even lots of village names are collected in the
list. The list needs to be processed into a list of un-
igrams (e.g., kolakAtA3 (Kolkata), bihAra (Bihar)),
bigrams (e.g., nayI dillI (New Delhi), pashchima
bA.nglA (West Bengal)) and trigrams (e.g. uttaara
chabisha paraganA (North 24 Pargana)). The words
are matched with unigrams, sequences of two con-
secutive words are matched against bigrams and
sequences of three consecutive words are matched
against trigrams.
World Location: The list contains the names of
the countries, different state and city names in world
and also the names of important rivers, mountains
etc. The list contains about 4,000 location names.
Similar to the Indian location list, this list also needs
to be processed as unigram, bigrams and trigrams.
4 Context Pattern Extraction based
Gazetteer Preparation
Gazetteers can also be prepared by extracting con-
text patterns. Transliteration based gazetteer prepa-
ration is applicable while there is availability of En-
glish or parallel language name list. But if such
relevant name lists are not available, but a large
raw corpus is available, then we can use the con-
text pattern extraction based methodology to prepare
the gazetteers. This method seeks some high pre-
cision context patterns by using some seed entities
and hits the patterns to the raw corpus to prepare the
gazetteers.
The overall methodology of extracting context
patterns from a raw corpus is summarized as fol-
lows:
1. Find a large raw corpus and some seed entities
(E) for each class of NEs.
2. For each seed entity e in E, from the corpus
find context string(C) comprised of n tokens
before e, a placeholder for the class instance
and n tokens after e. [We have used n = 3]
This set of words form initial pattern.
3. Search the pattern to the corpus and find the
coverage and precision.
4. Discard the patterns having low precision.
5. Generalize the patterns by dropping one or
more tokens to increase coverage.
3The Indian languages strings are written in italics font and
using itrans transliteration.
The 6th Workshop on Asian Languae Resources, 2008
13
6. Find best patterns having good precision and
coverage.
The details of the context pattern extraction
based gazetteer preparation methodology is de-
scribed in the following subsections. We have taken
a Hindi sports domain raw corpus and prepared
some gazetteers like names of cricketers, names of
tennis players to prove the effectiveness of the pro-
posed methodology.
4.1 Selection of Seed Entity
Context pattern extraction based gazetteer prepa-
ration methodology is applied to a sports domain
corpus which contains about 20 lakhs words col-
lected from the popular Hindi newspaper ?Dainik
Jagaran?. We have worked on preparing the lists
of cricket players, list of tennis players. We have
collected the most frequent names to build the seed
list. For preparing the list of tennis players, we
have taken 5 names as seed entities : Andre Agassi,
Steffi Graf, Serena Williams, Roger Federer and Jus-
tine Henin. Similarly the seed list of cricket players
name list contains only 3 names: Sachin Tendulkar,
Brian Lara and Glenn McGrath.
4.2 Context Extraction
To extract the patterns for a particular category, we
select a part of the corpus where the target seeds will
be available with high frequency. For example to
get the patterns for the names of the cricketers, we
select a part of the corpus where most of the sen-
tences are cricket related. To select the cricket re-
lated sentences, we prepared a list containing the
most frequent words related to cricket like, rAna
(run), ballebAja (batsman), gedabAja (bowler) etc.
Depending upon the presence of such words we have
selected the ?part?. In our development the cricket
?part? contains 120K words. Similar ?part? is devel-
oped for other gazetteers. For a particular seed, we
find the occurrences of the seed entity in the corre-
sponding raw ?part? corpus. Then we extracted three
tokens immediately preceding the seed and three to-
kens immediately following the seed. A placeholder
(CRIC for cricketers, TENS for tennis players)
replaces the seed. The placeholder and the surround-
ing tokens t?3 t?2 t?1 placeholder t+1 t+2 t+3)
form the initial set of patterns.
For the seed sachina tedulkara (Sachin Ten-
dulkar) we extract 92 initial patterns. Some of which
are:
? ki mAsTara blAsTara CRIC ko Tima ke
? dravi.Da aura CRIC 241 nAbAda ke
? bhAratiya ballebAja CRIC ne 100 rana
? mere vichAra se CRIC ko Takkara dene
4.3 Pattern Quality Measure
We measured the quality of a pattern depending on
its precision and coverage. Precision is the ratio of
correct identification and the total identification. If
the precision is high then also we have assumed that
the pattern is a good pattern. In our development
we have marked a pattern as good if the precision is
100%.
We search the initial patterns in the correspond-
ing ?part? corpus to measure the precision and cov-
erage. If the precision is less than 100% for a pat-
tern then we have rejected the pattern. Otherwise we
have tried to make it more generalized to increase
the coverage. To make the generalization we have
dropped the left most and right most tokens one by
one and checked the pattern quality. If for a initial
pattern, several patterns presents with 100% preci-
sion then we have selected those patterns for which
no subset of those is a good pattern. By this way we
have prepared a list of good patterns for a particular
gazetteer type.
In time of ?good? pattern selection we have made
some interesting observations.
? There are some patterns which satisfy the 100%
precision criteria but the coverage is very poor
in terms of new entity extraction. For exam-
ple, ?mAsTara blAsTara CRIC ko? is a pattern
with 100% precision. The pattern has 24 in-
stances in the ?part? corpus, but all the extracted
entities are ?sachina tedulkara?. We have also
examined the pattern in the total raw corpus.
It is capable of extracting ?sachina tedulkara?
only. So in spite of fulfilling all the criteria the
pattern is not a ?good? pattern.
? Another interesting observation is, that there
are some patterns which are ?good? patterns in
The 6th Workshop on Asian Languae Resources, 2008
14
the context of the ?part? corpus, but when used
in the total raw corpus, it extracts non-relevant
entities. For example ?mere vichAra se CRIC
ko Takkara? is a ?good? pattern so it should ex-
tract the names of the cricketers. But when
this is used in the total raw corpus it extracts
non-cricketer entities (e.g. tennis players, chess
players) also. To make such patterns useful we
have extracted all the cricket related sentences
in the similar way which was used for selecting
the ?part? corpus and then the patterns are used
to extract entities from these sentences.
? There present are patterns with very high cover-
age but precision is just below 100%. We have
analyzed these patterns and manually identified
the wrongly extracted entities. If the wrong en-
tities can be grouped together and are having
some specific properties then we have added
these entities in a ?pattern exception list?. Then
the pattern is used as a ?good? pattern and the
exception list is used to detect the wrong iden-
tifications.
In the following we have given some example of
?good? patterns which are useful in identification of
the names of the cricket players.
? ballebAja CRIC ko Tima ke
? ballebAja CRIC ne
? CRIC kA arddhashataka
4.4 Gazetteer Preparation
The extracted ?good? patterns are capable of iden-
tifying NEs from a raw corpus. These patterns are
then used to prepare the gazetteers. The seeds form
the initial gazetteer list for a particular gazetteer
type. The ?good? patterns are used to extract entities
from the total raw corpus. The entities identified by
the patterns are added to the corresponding gazetteer
list. In that way we can add more entities in our first
phase gazetteer list. These new entities are taken as
seeds for the next phase. Then the same procedure
is followed repeatedly to develop a large gazetteer.
We have already mentioned that we have worked
with a sports domain corpus and prepared some
gazetteers. This gazetteers are prepared just to prove
the efficiency of our approach. By using only 3 seed
entities we become able to prepare a gazetteer which
contains 412 names of the cricketers. Even using
this approach only one seed ?Sachin Tendualkar? ex-
tracts 297 names after the second iteration. Similarly
we have collected 245 names of tennis players from
5 seed entities.
5 Conclusion
In this paper we have described our approaches for
the preparation of gazetteers. We have also prepared
some gazetteers using both the approaches to show
their effectiveness. These approaches are very use-
ful for the NER task in resource-poor languages and
also in domain specific NER task.
References
Al-Onaizan Y. and Knight K. 2002. Machine Translit-
eration of Names in Arabic Text. Proceedings of
the ACL Workshop on Computational Approaches to
Semitic Languages.
Bikel Daniel M., Miller Scott, Schwartz Richard and
Weischedel Ralph. 1997. Nymble: A high perfor-
mance learning name-finder. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing, peges 194?201.
Borthwick Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
Computer Science Department, New York University.
Downey D., Etzioni O., Soderland S., Weld D.S. 2004.
Learning text patterns for Web information extraction
and assessment. In AAAI-04 Workshop on Adaptive
Text Extraction and Mining, pages 50?55.
Ekbal A. and Bandyopadhyay S. 2007. Lexical Pattern
Learning from Corpus Data for Named Entity Recog-
nition. In Proceedings of International Conference on
Natural Language Processing (ICON), 2007.
Ekbal A., Naskar S. and Bandyopadhyay S. 2006. A
Modified Joint Source Channel Model for Translitera-
tion. In Proceedings of the COLING/ACL 2006, Aus-
tralia, pages 191?198.
Etzioni Oren, Cafarella Michael, Downey Doug, Popescu
Ana-Maria, Shaked Tal, Soderland Stephen, Weld
Daniel S. and Yates Alexander. 2005. Unsupervised
named-entity extraction from the Web: An experimen-
tal study. In Artificial Intelligence, 165(1): 91-134.
Grishman Ralph. 1995. The New York University Sys-
tem MUC-6 or Where?s the syntax? In Proceedings of
the Sixth Message Understanding Conference.
The 6th Workshop on Asian Languae Resources, 2008
15
Knight K. and Graehl J. 1998. Machine Transliteration.
Computational Linguistics, 24(4): 599?612.
Kozareva Zornitsa. 2006. Bootstrapping Named Entity
Recognition with Automatically Generated Gazetteer
Lists. In Proceedings of EACL student session (EACL
2006).
Li Wei and McCallum Andrew. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Condi-
tional Random Fields and Feature Induction (Short Pa-
per). In ACM Transactions on Computational Logic.
Lin Winston, Yangarber Roman and Grishman Ralph.
2003. Bootstrapped learning of semantic classes from
positive and negative examples. In Proceedings of
ICML-2003 Workshop on The Continuum from La-
beled to Unlabeled Data.
Riloff E. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Articial Intelli-
gence, pages 1044?1049.
Srihari R., Niu C. and Li W. 2000. A Hybrid Approach
for Named Entity and Sub-Type Tagging. In Proceed-
ings of the sixth conference on Applied natural lan-
guage processing.
Soderland Stephen, Fisher David, Aseltine Jonathan,
Lehnert Wendy. 1995. CRYSTAL: Inducing a Con-
ceptual Dictionary. In Proceedings of the Fourteenth
International Joint Conference on Artificial Intelli-
gence.
Talukdar P. Pratim, T. Brants, M. Liberman and F.
Pereira. 2006. A context pattern induction method
for named entity extraction. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X).
Wakao T., Gaizauskas R. and Wilks Y. 1996. Evaluation
of an algorithm for the recognition and classification
of proper names. In Proceedings of COLING-96.
The 6th Workshop on Asian Languae Resources, 2008
16
Proceedings of ACL-08: HLT, pages 488?495,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Word Clustering and Word Selection based Feature Reduction for MaxEnt
based Hindi NER
Sujan Kumar Saha
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
sujan.kr.saha@gmail.com
Pabitra Mitra
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
pabitra@gmail.com
Sudeshna Sarkar
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
shudeshna@gmail.com
Abstract
Statistical machine learning methods are em-
ployed to train a Named Entity Recognizer
from annotated data. Methods like Maxi-
mum Entropy and Conditional Random Fields
make use of features for the training purpose.
These methods tend to overfit when the avail-
able training corpus is limited especially if the
number of features is large or the number of
values for a feature is large. To overcome
this we proposed two techniques for feature
reduction based on word clustering and se-
lection. A number of word similarity mea-
sures are proposed for clustering words for
the Named Entity Recognition task. A few
corpus based statistical measures are used for
important word selection. The feature reduc-
tion techniques lead to a substantial perfor-
mance improvement over baseline Maximum
Entropy technique.
1 Introduction
Named Entity Recognition (NER) involves locat-
ing and classifying the names in a text. NER is
an important task, having applications in informa-
tion extraction, question answering, machine trans-
lation and in most other Natural Language Process-
ing (NLP) applications. NER systems have been de-
veloped for English and few other languages with
high accuracy. These belong to two main cate-
gories based on machine learning (Bikel et al, 1997;
Borthwick, 1999; McCallum and Li, 2003) and lan-
guage or domain specific rules (Grishman, 1995;
Wakao et al, 1996).
In English, the names are usually capitalized
which is an important clue for identifying a name.
Absence of capitalization makes the Hindi NER task
difficult. Also, person names are more diverse in In-
dian languages, many common words being used as
names.
A pioneering work on Hindi NER is by Li and
McCallum (2003) where they used Conditional Ran-
dom Fields (CRF) and feature induction to auto-
matically construct only the features that are impor-
tant for recognition. In an effort to reduce overfit-
ting, they use a combination of a Gaussian prior and
early-stopping.
In their Maximum Entropy (MaxEnt) based ap-
proach for Hindi NER development, Saha et al
(2008) also observed that the performance of the
MaxEnt based model often decreases when huge
number of features are used in the model. This is
due to overfitting which is a serious problem in most
of the NLP tasks in resource poor languages where
annotated data is scarce.
This paper is a study on effectiveness of word
clustering and selection as feature reduction tech-
niques for MaxEnt based NER. For clustering we
use a number of word similarities like cosine sim-
ilarity among words and co-occurrence, along with
the k-means clustering algorithm. The clusters are
then used as features instead of words. For impor-
tant word selection we use corpus based statistical
measurements to find the importance of the words in
the NER task. A significant performance improve-
ment over baseline MaxEnt was observed after using
the above feature reduction techniques.
The paper is organized as follows. The MaxEnt
488
based NER system is described in Section 2. Vari-
ous approaches for word clustering are discussed in
Section 3. Next section presents the procedure for
selecting the important words. In Section 5 experi-
mental results and related discussions are given. Fi-
nally Section 6 concludes the paper.
2 Maximum Entropy Based Model for
Hindi NER
Maximum Entropy (MaxEnt) principle is a com-
monly used technique which provides probability of
belongingness of a token to a class. MaxEnt com-
putes the probability p(o|h) for any o from the space
of all possible outcomes O, and for every h from
the space of all possible histories H . In NER, his-
tory can be viewed as all information derivable from
the training corpus relative to the current token. The
computation of probability (p(o|h)) of an outcome
for a token in MaxEnt depends on a set of features
that are helpful in making predictions about the out-
come. The features may be binary-valued or multi-
valued. Given a set of features and a training corpus,
the MaxEnt estimation process produces a model in
which every feature fi has a weight ?i. We can
compute the conditional probability as (Berger et al,
1996):
p(o|h) =
1
Z(h)
?
i
?i
fi(h,o) (1)
Z(h) =
?
o
?
i
?i
fi(h,o) (2)
The conditional probability of the outcome is the
product of the weights of all active features, normal-
ized over the products of all the features. For our
development we have used a Java based open-nlp
MaxEnt toolkit1. A beam search algorithm is used
to get the most probable class from the probabilities.
2.1 Training Corpus
The training data for the Hindi NER task is com-
posed of about 243K words which is collected
from the popular daily Hindi newspaper ?Dainik
Jagaran?. This corpus has been manually anno-
tated and contains about 16,491 Named Entities
(NEs). In this study we have considered 4 types
1http://sourceforge.net/projects/maxent/
Type Features
Word wi, wi?1, wi?2, wi+1, wi+2
NE Tag ti?1, ti?2
Digit infor-
mation
Contains digit, Only digit, Four
digit, Numerical word
Affix infor-
mation
Fixed length suffix, Suffix list,
Fixed length prefix
POS infor-
mation
POS of words, Coarse-grained
POS, POS based binary features
Table 1: Features used in the MaxEnt based Hindi NER
system
of NEs, these are Person (Per), Location (Loc),
Organization (Org) and Date (Dat). To recognize
entity boundaries each name class N has 4 types
of labels: N Begin, N Continue, N End and
N Unique. For example, Kharagpur is annotated
as Loc Unique and Atal Bihari Vajpeyi is annotated
as Per Begin Per Continue Per End. Hence,
there are a total of 17 classes including one class for
not-name. The corpus contains 6298 person, 4696
location, 3652 organization and 1845 date entities.
2.2 Feature Description
We have identified a number of candidate features
for the Hindi NER task. Several experiments were
conducted with the identified features, individually
and in combination. Some of the features are men-
tioned below. They are summarized in Table 1.
Static Word Feature: Recognition of NE is
highly dependent on contexts. So the surrounding
words of a particular word (wi) are used as fea-
tures. During our experiments different combina-
tions of previous 3 words (wi?3...wi?1) to next 3
words (wi+1...wi+3) are treated as features. This is
represented by L binary features where L is the size
of lexicon.
Dynamic NE tag: NE tags of the previous words
(ti?m...ti?1) are used as features. During decoding,
the value of this feature for a word (wi) is obtained
only after the computation of the NE tag for the pre-
vious word (wi?1).
Digit Information: If a word (wi) contains
digit(s) then the feature ContainsDigit is set to 1.
This feature is used with some modifications also.
OnlyDigit, which is set to 1 if the word contains
489
Feature Id Feature Per Loc Org Dat Total
F1 wi, wi?1, wi+1 61.36 68.29 52.12 88.9 67.26
F2 wi, wi?1, wi?2, wi+1, wi+2 64.10 67.81 58 92.30 69.09
F3 wi, wi?1, wi?2, wi?3, wi+1,
wi+2, wi+3
60.42 67.81 51.48 90.18 66.84
F4 wi, wi?1, wi?2, wi+1, wi+2,
ti?1, ti?2, Suffix
66.67 73.36 58.58 89.09 71.2
F5 wi, wi?1, wi+1, ti?1, Suffix 69.65 75.8 59.31 89.09 73.42
F6 wi, wi?1, wi+1, ti?1, Prefix 66.67 71 58.58 87.8 70.02
F7 wi, wi?1, wi+1, ti?1, Prefix,
Suffix
70.61 71 59.31 89.09 72.5
F8 wi, wi?1, wi+1, ti?1, Suffix,
Digit
70.61 75.8 60.54 93.8 74.26
F9 wi, wi?1, wi+1, ti?1, POS (28
tags)
64.25 71 60.54 89.09 70.39
F10 wi, wi?1, wi+1, ti?1, POS
(coarse grained)
69.65 75.8 59.31 92.82 74.16
F11 wi, wi?1, wi+1, Ti?1, Suffix,
Digit, NomPSP
72.26 78.6 61.36 92.82 75.6
F12 wi, wi?1, wi+1, wi?2, wi+2,
Ti?1, Prefix, Suffix, Digit,
NomPSP
65.26 78.01 52.12 93.33 72.65
Table 2: F-values for different features in the MaxEnt based Hindi NER system
only digits, 4Digit, which is set to 1 if the word
contains only 4 digits, etc. are some modifications
of the feature which are helpful.
Numerical Word: For a word (wi) if it is a nu-
merical word i.e. word denoting a number (e.g. eka2
(one), do (two), tina (three) etc.) then the feature
NumWord is set to 1.
Word Suffix: Word suffix information is helpful
to identify the NEs. Two types of suffix features
have been used. Firstly a fixed length word suffix
(set of characters occurring at the end of the word) of
the current and surrounding words are used as fea-
tures. Secondly we compiled list of common suf-
fixes of place names in Hindi. For example, pura,
bAda, nagara etc. are location suffixes. We used
binary feature corresponding to the list - whether a
given word has a suffix from the list.
Word Prefix: Prefix information of a word may
be also helpful in identifying whether it is a NE. A
2All Hindi words are written in italics using the ?Itrans?
transliteration.
fixed length word prefix (set of characters occur-
ring at the beginning of the word) of current and
surrounding words are treated as features. List of
important prefixes, which are used frequently in the
NEs, are also effective.
Parts-of-Speech (POS) Information: The POS
of the current word and the surrounding words are
used as feature for NER. We have used a Hindi POS
tagger developed at IIT Kharagpur, India which has
an accuracy about 90%. We have used the POS val-
ues of the current and surrounding words as features.
We realized that the detailed POS tagging is not
very relevant. Since NEs are noun phrases, the noun
tag is very relevant. Further the postposition follow-
ing a name may give a clue to the NE type. So we de-
cided to use a coarse-grained tagset with only three
tags - nominal (Nom), postposition (PSP) and other
(O).
The POS information is also used by defining sev-
eral binary features. An example is the NomPSP
binary feature. The value of this feature is defined
to be 1 if the current word is nominal and the next
490
word is a PSP.
2.3 Performance of Hindi NER using MaxEnt
Method
The performance of the MaxEnt based Hindi NER
using the above mentioned features is reported here
as a baseline. We have evaluated the system us-
ing a blind test corpus of 25K words. The test
corpus contains 521 person, 728 location, 262 or-
ganization and 236 date entities. The accuracies
are measured in terms of the f-measure, which is
the weighted harmonic mean of precision and re-
call. Precision is the fraction of the correct anno-
tations and recall is the fraction of the total NEs
that are successfully annotated. The general formula
for measuring the f-measure or f-value is, F? =
(1+?2) . (precision . recall) \ (?2 . precision +
recall). Here the value of ? is taken as 1. In Table 2
we have shown the accuracy values for few feature
sets.
While experimenting with static word features,
we have observed that a window of previous and
next two words (wi?2...wi+2) gives best result
(69.09) using the word features only. But whenwi?3
and wi+3 are added with it, the f-value is reduced
to 66.84. Again when wi?2 and wi+2 are deducted
from the feature set (i.e. only wi?1 and wi+1 as fea-
ture), the f-value is reduced to 67.26. This demon-
strates thatwi?2 andwi+2 are helpful features in NE
identification.
When suffix, prefix and digit information are
added to the feature set, the f-value is increased upto
74.26. The value is obtained using the feature set
F8 [wi, wi?1, wi+1, ti?1, Suffix, Digit]. It is ob-
served that when wi?2 and wi+2 are added with the
feature, the accuracy decreases by 2%. It contra-
dicts the results using the word features only. An-
other interesting observation is that prefix informa-
tion are helpful features in NE identification as these
increase accuracy when separately added with the
word features (F6). Similarly the suffix information
helps in increasing the accuracy. But when both the
suffix and prefix information are used in combina-
tion along with the word features, the f-value is de-
creased. From Table 2, a f-value of 73.42 is obtained
using F5 [wi, wi?1, wi+1, ti?1, Suffix] but when
prefix information are added with it (F7), the f-value
is reduced to 72.5.
POS information are important features in NER.
In general it is observed that coarse grained POS
information performs better than the finer grained
POS information. The best accuracy (75.6 f-value)
of the baseline system is obtained using the binary
NomPSP feature along with word feature (wi?1,
wi+1), suffix and digit information. It is noted that
when wi?2, wi+2 and prefix information are added
with the best feature, the f-value is reduced to 72.65.
From the above discussion it is clear that the sys-
tem suffers from overfitting if a large number of fea-
tures are used to train the system. Note that the sur-
rounding word (wi?2, wi?1, wi+1, wi+2 etc.) fea-
tures can take any value from the lexicon and hence
are of high dimensionality. These cause the degra-
dation of performance of the system. However it is
obvious that few words in the lexicon are important
in identification of NEs.
To solve the problem of high dimensionality we
use clustering to group the words present in the cor-
pus into much smaller number of clusters. Then
the word clusters are used as features instead of
the word features (for surrounding words). For ex-
ample, our Hindi corpus contains 17,456 different
words, which are grouped into N (say 100) clusters.
Then for a particular word, it is assigned to a cluster
and the corresponding cluster-id is used as feature.
Hence the number of features is reduced to 100 in-
stead of 17,456.
Similarly, selection of important words can also
solve the problem of high dimensionality. As some
of the words in the lexicon play important role in
the NE identification process, we aim to select these
particular words. Only these important words are
used in NE identification instead of all words in the
corpus.
3 Word Clustering
Clustering is the process of grouping together ob-
jects based on their similarity. The measure of sim-
ilarity is critical for good quality clustering. We
have experimented with some approaches to com-
pute word-word similarity. These are described in
details in the following section.
491
3.1 Cosine Similarity based on Sentence Level
Co-occurrence
A word is represented by a binary vector of dimen-
sion same as the number of sentences in the cor-
pus. A component of the vector is 1 if the word
occurs in the corresponding sentence and zero oth-
erwise. Then we measure cosine similarity between
the word vectors. The cosine similarity between two
word vectors ( ~A and ~B) with dimension d is mea-
sured as:
CosSim( ~A, ~B) =
?
dAdBd
(
?
dA
2
d)
1
2 ? (
?
dB
2
d)
1
2
(3)
This measures the number of co-occurring sen-
tences.
3.2 Cosine Similarity based on Proximal
Words
In this measure a word is represented by a vector
having dimension same as the lexicon size. For
ease of implementation we have taken a dimen-
sion of 2 ? 200, where each component of the vec-
tor corresponds to one of the 200 most frequent
preceding and following words of a token word.
List Prev containing the most frequent (top 200)
previous words (wi?1 or wi?2 if wi is the first word
of a NE) and List Next contains 200 most frequent
next words (wi+1 or wi+2 if wi is the last word of a
NE). A particular word wk may occur several times
(say n) in the corpus. For each occurrence of wk
find if its previous word (wk?1 or wk?2) matches
any element of List Prev. If matches, then set 1 to
the corresponding position of the vector and set zero
to all other positions related to List Prev. Sim-
ilarly check the next word (wk+1 or wk+2) in the
List Next and find the values of the corresponding
positions. The final word vector ~Wk is obtained by
taking the average of all occurrences of wk. Then
the cosine similarity is measured between the word
vectors. This measures the similarity of the contexts
of the occurrences of the word in terms of the prox-
imal words.
3.3 Similarity based on Proximity to NE
Categories
Here, for each word (wi) in the corpus four binary
vectors are defined corresponding to two preceding
and two following positions (i-1, i-2, i+1, i+2). Each
binary vector is of dimension five corresponding
to four NE classes (Cj) and one for the not-name
class. For a particular word wk, find all the words
occur in a particular position (say, +1). Measure
the fraction (Pj(wk)) of these words belonging to a
class Cj . The component of the word vector ~Wk for
the position corresponding to Cj is Pj(wk).
Pj(wk) =
No. of times wk+1 is a NE of class Cj
Total occurrence of wk in corpus
The Euclidean distance is used to find the simi-
larity between the above word vectors as a similar-
ity measure. Some of the word vectors for the +1
position are given in Table 3. In this table we have
given the word vectors for a few Hindi words, which
are, sthita (located), shahara (city), jAkara (go), na-
gara (township), gA.nva (village), nivAsI (resident),
mishrA (a surname) and limiTeDa (ltd.). From the
table we observe that the word vectors are close for
sthita [0 0.478 0 0 0.522], shahara [0 0.585 0.001
0.024 0.39], nagara [0 0.507 0.019 0 0.474] and
gA.nva [0 0.551 0 0 0.449]. So these words are con-
sidered as close.
Word Per Loc Org Dat Not
sthita 0 0.478 0 0 0.522
shahara 0 0.585 0.001 0.024 0.39
jAkara 0 0.22 0 0 0.88
nagara 0 0.507 0.019 0 0.474
gA.nva 0 0.551 0 0 0.449
nivAsI 0.108 0.622 0 0 0.27
mishrA 0.889 0 0 0 0.111
limiTeDa 0 0 1 0 0
Table 3: Example of some word vectors for next (+1)
position (see text for glosses)
3.4 K-means Clustering
Using the above similarity measures we have used
the k-means algorithm. The seeds were randomly
selected. The value of k (number of clusters) was
varied till the best result is obtained.
4 Important Word Selection
It is noted that not all words are equally important
in determining the NE category. Some of the words
492
in the lexicon are typically associated with a partic-
ular NE category and hence have important role to
play in the classification process. We describe be-
low a few statistical techniques that has been used to
identify the important words.
4.1 Class Independent Important Word
Selection
We define context words as those which occur in
proximity of a NE. In other words, context words
are the words present in the wi?2, wi?1, wi+1
or wi+2 position if wi is a NE. Note that only a
subset of the lexicon are context words. For all
the context words, its N weight is calculated as
the ratio between the occurrence of the word as a
context word and its total number of occurrence in
the corpus. The context words having the higher
N weight are considered as important words for
NER. For our experiments we have considered top
500 words as important words.
N weight(wi) =
Occurrence of wi as context word
Total occurrence of wi in corpus
4.2 Important Words for Each Class
Similar to the class independent important word se-
lection from the contexts, important words are se-
lected for individual classes also. This is an exten-
sion of the previous context word considering only
NEs of a particular class. For person, location, or-
ganization and date classes we have considered top
150, 120, 50 and 50 words respectively as impor-
tant words. Four binary features are also defined for
these four classes. These are defined as having value
1 if any of the context words belongs to the impor-
tant words list for a particular class.
4.3 Important Words for Each Position
Position based important words are also selected
from the corpus. Here instead of context, particu-
lar positions are considered. Four lists are compiled
for two preceding and two following positions (-2,
-1, +1 and +2).
5 Evaluation of NE Recognition
The following subsections contain the experimental
results using word clustering and important word se-
lection. The results demonstrate the effectiveness of
k Per Loc Org Dat Total
20 66.33 74.57 43.64 91.30 69.54
50 64.13 76.35 52 93.62 71.7
80 66.33 74.57 53.85 93.62 72.08
100 70.1 73.1 57.7 96.62 72.78
120 66.15 73.43 54.9 93.62 71.52
150 66.88 74.94 53.06 95.65 72.33
200 66.09 73.82 52 92 71.13
Table 4: Variation of MaxEnt based system accuracy de-
pending on number of clusters (k)
word clustering and important word selection over
the baseline MaxEnt model.
5.1 Using Word Clusters
To evaluate the effectiveness of the clustering ap-
proaches in Hindi NER, we have used cluster fea-
tures instead of word features. For the surrounding
words, corresponding cluster-ids are used as feature.
Choice of k : We have already mentioned that,
for k-means clustering number of classes (k) should
be determined initially. To find suitable k we had
conducted the following experiments. We have se-
lected a feature F1 (mentioned in Table 2) and ap-
plied the clusters with different k as features replac-
ing the word features. In Table 4 we have summa-
rized the experimental results, in order to find a suit-
able k for clustering, the word vectors obtained us-
ing the procedure described in Section 3.3. From
the table we observe that the best result is obtained
when k is 100. We have used k = 100 for the sub-
sequent experiments for comparing the effectiveness
of the features. Similarly when we deal with all the
words in the corpus (17,465 words), we got best re-
sults when the words are clustered into 1100 clus-
ters. ?
The details of the comparison between the base-
line word features and the reduced features obtained
using clustering are given in Table 5. In general it
is observed that clustering has improved the perfor-
mance over baseline features. Using only cluster
features the system provides a maximum f-value of
74.26 where the corresponding word features give
f-value of 69.09.
Among the various similarity measures of clus-
tering, improved results are obtained using the clus-
493
Feature Using
Word
Features
Using
Clusters
(C1)
Using
Clusters
(C2)
Using
Clusters
(C3)
wi, window(-1, +1) 67.26 69.67 72.05 72.78
wi, window(-2, +2) 69.09 71.52 72.65 74.26
wi, window(-1, +1), Suffix 73.42 74.24 75.44 75.84
wi, window(-1, +1), Prefix, Suffix 72.5 74.76 75.7 76.33
wi, window(-1, +1), Prefix, Suffix, Digit 74.26 75.09 75.91 76.41
wi, window(-1, +1), Prefix, Suffix, Digit,
NomPSP
75.6 77.2 77.39 77.61
wi, window(-2, +2), Prefix, Suffix, Digit,
NomPSP
72.65 77.86 78.61 79.03
Table 5: F-values for different features in a MaxEnt based Hindi NER with clustering based feature reduction
[window(?m,+n) refers to the cluster or word features corresponding to previous m positions and next n posi-
tions; C1 is the clusters which use sentence level co-occurrence based cosine similarity (3.1), C2 denotes the clusters
which use proximal word based cosine similarity (3.2), C3 denotes the clusters for each positions related to NE (3.3)]
ters which uses the similarity measurement based on
proximity of the words to NE categories (defined in
Section 3.3).
Using clustering features the best f-value (79.03)
is obtained using clusters for previous two and next
two words along with the suffix, prefix, digit and
POS information.
It is observed that the prefix information increases
the accuracy if applied along with suffix informa-
tion when cluster features are used. More interest-
ingly, addition of cluster features for positions ?2
and +2 over the feature [window(-1, +1), Suffix,
Prefix, Digit, NomPSP] increase the f-value from
77.61 to 79.03. But in the baseline system addition
of word features (wi?2 and wi+2) over the same fea-
ture decrease the f-value from 75.6 to 72.65.
5.2 Using Important Word Selection
The details of the comparison between the word fea-
ture and the reduced features based on important
word selection are given in Table 6. For the sur-
rounding word features, find whether the particular
word (e.g. at position -1, -2 etc.) presents in the
important words list (corresponding to the particu-
lar position if position based important words are
considered). If the word occurs in the list then the
word is used as features. In general it is observed
that word selection also improves performance over
baseline features. Among the different approaches,
the best result is obtained when important words for
two preceding and two following positions (defined
in Section 4.3) are selected. Using important word
based features, the highest f-value of 79.85 is ob-
tained by using the important words for previous two
and next two positions along with the suffix, prefix,
digit and POS information.
5.3 Relative Effectiveness of Clustering and
Word Selection
In most of the cases clustering based features per-
form better then the important word based feature
reduction. But the best f-value (79.85) of the sys-
tem (using the clustering based and important word
based features separately) is obtained by using im-
portant word based features.
Next we have made an experiment by consider-
ing both the clusters and important words combined.
We have defined the combined feature as, if the word
(wi) is in the corresponding important word list then
the word is used as feature otherwise the correspond-
ing cluster-id (in which wi belongs to) is considered
as feature. Using the combined feature, we have
achieved further improvement. Here we are able to
achieve the highest f-value of 80.01.
6 Conclusion
A hierarchical word clustering technique, where
clusters are driven automatically from large unan-
494
Feature Using
Word
Features
Using
Words
(I1)
Using
Words
(I2)
Using
Words
(I3)
wi, window(-1, +1) 67.26 66.31 67.53 66.8
wi, window(-2, +2) 69.09 72.04 72.9 73.34
wi, window(-1, +1), Suffix 73.42 73.85 73.12 74.61
wi, window(-1, +1), Prefix, Suffix 72.5 73.52 73.94 74.87
wi, window(-1, +1), Prefix, Suffix, Digit 74.26 73.97 74.13 74.7
wi, window(-1, +1), Prefix, Suffix, Digit,
NomPSP
75.6 75.84 76.6 77.22
wi, window(-2, +2), Prefix, Suffix, Digit,
NomPSP
72.65 76.69 77.42 79.85
Table 6: F-values for different features in a MaxEnt based Hindi NER with important word based feature reduction
[window(?m,+n) refers to the important word or baseline word features corresponding to previous m positions and
next n positions; I1 is the class independent important words (4.1), I2 denotes the important words for each class (4.2),
I3 denotes the important words for each positions (4.3)]
notated corpus, is used by Miller et al (2004) for
augmenting annotated training data. Note that our
clustering approach is different, where the clusters
are obtained using some statistics derived from the
annotated corpus, and also the purpose is different
as we have used the clusters for feature reduction.
In this paper we propose two feature reduction
techniques for Hindi NER based on word cluster-
ing and word selection. A number of word similar-
ity measures are used for clustering. A few statisti-
cal approaches are used for the selection of impor-
tant words. It is observed that significant enhance-
ment of accuracy over the baseline system which use
word features is obtained. This is probably due to
reduction of overfitting. This is more important for
a resource poor languages like Hindi where there is
scarcity in annotated training data and other NER
resources (like, gazetteer lists).
7 Acknowledgement
The work is partially funded by Microsoft Research
India.
References
Berger A L, Pietra S D and Pietra V D 1996. A Maxi-
mum Entropy Approach to Natural Language Process-
ing. Computational Linguistic, 22(1):39?71.
Bikel D M, Miller S, Schwartz R and W Ralph. 1997.
Nymble: A High Performance Learning Name-finder.
In Proceedings of the Fifth Conference on Applied Nat-
ural Language Processing, pages 194?201.
Borthwick A. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, Computer
Science Department, New York University.
Grishman R. 1995. The New York University System
MUC-6 or Where?s the syntax? In Proceedings of the
Sixth Message Understanding Conference.
Li W and McCallum A. 2003. Rapid Development of
Hindi Named Entity Recognition using Conditional
Random Fields and Feature Induction. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 2(3):290?294.
McCallum A and Li W. 2003. Early Results for Named
Entity Recognition with Conditional Random fields,
feature induction and web-enhanced lexicons. In Pro-
ceedings of the Seventh Conference on Natural Lan-
guage Learning at HLT-NAACL.
Miller S, Guinness J and Zamanian A. 2004. Name Tag-
ging with Word Clusters and Discriminative Training.
In Proceedings of the HLT-NAACL 2004, pages 337?
342.
Saha S K, Sarkar S and Mitra P. 2008. A Hybrid Fea-
ture Set based Maximum Entropy Hindi Named En-
tity Recognition. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP-08), pages 343?349.
Wakao T, Gaizauskas R and Wilks Y. 1996. Evaluation
of an algorithm for the recognition and classification
of proper names. In Proceedings of COLING-96.
495
