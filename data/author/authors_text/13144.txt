Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2031?2041, Dublin, Ireland, August 23-29 2014.
Query Lattice for Translation Retrieval
Meiping Dong
?
, Yong Cheng
?
, Yang Liu
?
, Jia Xu
?
, Maosong Sun
?
,
Tatsuya Izuha

, Jie Hao
#
?
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
hellodmp@163.com, {liuyang2011,sms}@tsinghua.edu.cn
?
Institute for Interdisciplinary Information Sciences
Tsinghua University, Beijing, China
chengyong3001@gmail.com, xu@tsinghua.edu.cn

Toshiba Corporation Corporate Research & Development Center
tatsuya.izuha@toshiba.co.jp
#
Toshiba (China) R&D Center
haojie@toshiba.com.cn
Abstract
Translation retrieval aims to find the most likely translation among a set of target-language strings
for a given source-language string. Previous studies consider the single-best translation as a query
for information retrieval, which may result in translation error propagation. To alleviate this
problem, we propose to use the query lattice, which is a compact representation of exponentially
many queries containing translation alternatives. We verified the effectiveness of query lattice
through experiments, where our method explores a much larger search space (from 1 query to
1.24 ? 10
62
queries), runs much faster (from 0.75 to 0.13 second per sentence), and retrieves
more accurately (from 83.76% to 93.16% in precision) than the standard method based on the
query single-best. In addition, we show that query lattice significantly outperforms the method
of (Munteanu and Marcu, 2005) on the task of parallel sentence mining from comparable corpora.
1 Introduction
Translation retrieval aims to search for the most probable translation candidate from a set of target-
language strings for a given source-language string. Early translation retrieval methods were widely
used in example-based and memory-based translation systems (Sato and Nagao, 1990; Nirenburg et al.,
1993; Baldwin and Tanaka, 2000; Baldwin, 2001). Often, the document set is a list of translation records
that are pairs of source-language and target-language strings. Given an input source string, the retrieval
system returns a translation record of maximum similarity to the input on the source side. Although these
methods prove to be effective in example-based and memory-based translation systems, they heavily rely
on parallel corpora that are limited both in size and domain.
More recently, Liu et al. (2012) have proposed a new translation retrieval architecture that depends
only on monolingual corpora. Given an input source string, their system retrieves translation candidates
from a set of target-language sentences. This can be done by combining machine translation (MT) and
information retrieval (IR): machine translation is used to transform the input source string to a coarse
translation, which serves as a query to retrieve the most probable translation in the monolingual corpus.
Therefore, it is possible for translation retrieval to have access to a huge volume of monolingual corpora
that are readily available on the Web.
However, the MT + IR pipeline suffers from the translation error propagation problem. Liu et al.
(2012) use 1-best translations, which are inevitably erroneous due to the ambiguity and structural di-
vergence of natural languages, as queries to the IR module. As a result, translation mistakes will be
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
Corresponding author: Jia Xu. Tel: +86-10-62781693 Ext 1683. Homepage: iiis.tsinghua.edu.cn/?xu
2031
propagated to the retrieval process. This situation aggravates when high-accuracy MT systems are not
available for resource-scarce languages.
In this work, we propose to use query lattice in translation retrieval to alleviate the translation error
propagation problem. A query lattice is a compact representation of exponentially many queries. We
design a retrieval algorithm that takes the query lattice as input to search for the most probable translation
candidate from a set of target-language sentences. As compared with Liu et al. (2012), our approach
explores a much larger search space (from 1 query to 1.24? 10
62
queries), runs much faster (from 0.75
second per sentence to 0.13), and retrieves more accurately (from 83.76% to 93.16%). We also evaluate
our approach on extracting parallel sentences from comparable corpora. Experiments show that our
translation retrieval system significantly outperforms a state-of-the-art parallel corpus mining system.
2 Related Work
Our work is inspired by three research topics: retrieving translation candidates from parallel corpus,
using lattice to compactly represent exponentially many alternatives, and using lattice as query in infor-
mation retrieval.
1. Translation Retrieval using Parallel Corpus. The idea of retrieving translation candidates from
existing texts originated in example-based and memory-based translation (Sato and Nagao, 1990;
Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use
a parallel corpus (e.g., translation records that are pairs of source-language and target-language
strings), they focus on calculating the similarity between two source-language strings. In contrast,
we evaluate the translational equivalence of a given source string and a target string in a large
monolingual corpus.
2. Lattice in Machine Translation. Lattices have been widely used in machine translation: consider-
ing Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Mat-
soukas et al., 2007), SCFG (Dyer et al., 2008) and so on in the decoding process, minimum bayes
risk decoding (Tromble et al., 2008), minimum error rate training (Macherey et al., 2008), system
combination (Feng et al., 2009), just to name a few. In this work, we are interested in how to use a
lattice that encodes exponentially many translation candidates as a single query to retrieve similar
target sentences via an information retrieval system.
3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to
Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hierarchies
or thesauri in formal concept analysis (Priss, 2000; Cheung and Vogel, 2005). In spoken document
retrieval, however, lattices are used as a compact representation of multiple speech recognition
transcripts to estimate the expected counts of words in each document (Saraclar and Sproat, 2004;
Zhou et al., 2006; Chia et al., 2010). Our work is significantly different from previous work that
uses the bag-of-words model because translation retrieval must take structure and dependencies in
text into account to ensure translational equivalence.
3 Query Lattice for Translation Retrieval
3.1 Translation Retrieval
Let f be a source-language string, E be a set of target-language strings, the problem is how to find the
most probable translation
?
e from E. Note that E is a monolingual corpus rather than a parallel corpus.
Therefore, string matching on the source side (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin
and Tanaka, 2000; Baldwin, 2001) does not apply here.
We use P (e|f) to denote the probability that a target-language sentence e is the translation of a source-
language sentence f . As suggested by Liu et al. (2012), it can be decomposed into two sub-models by
2032
introducing a coarse translation q as a hidden variable:
P (e|f) =
?
q?Q(f)
P (q, e|f) (1)
=
?
q?Q(f)
P (q|f)? P (e|q, f) (2)
where P (q|f) is a translation sub-model, P (e|q, f) is a retrieval sub-model, and Q(f) is the set of all
possible translations of the sentence f . Note that q actually serves as a query to the retrieval sub-model.
To take advantage of various translation and retrieval information sources, we use a log-linear model
(Och and Ney, 2002) to define the conditional probability of a query q and a target sentence e conditioned
on a source sentence f parameterized by a real-valued vector ?:
P (q, e|f ;?) =
exp(? ? h(q, e, f))
?
q
?
?Q(f)
?
e
?
?E
exp(? ? h(q
?
, e
?
, f))
(3)
where h(?) is a vector of feature functions and ? is the corresponding feature weight vector.
Accordingly, the decision rule for the latent variable model is given by
?
e = argmax
e?E
{
?
q?Q(f)
exp(? ? h(q, e, f))
}
(4)
As there are exponentially many queries, it is efficient to approximate the summation over all possible
queries by using maximization instead:
?
e ? argmax
e?E
{
max
q?Q(f)
{
? ? h(q, e, f)
}
}
(5)
Unfortunately, the search space is still prohibitively large since we need to enumerate all possible
queries. Liu et al. (2012) split Eq. (5) into two steps. In the first step, a translation module runs to
produce the 1-best translation
?
q of the input string f as a query:
?
q ? argmax
q?Q(f)
{
?
t
? h
t
(q, e, f)
}
(6)
where h
t
(?) is a vector of translation features and ?
t
is the corresponding feature weight vector. In the
second step, a monolingual retrieval module takes the 1-best translation
?
q as a query to search for the
target string
?
e with the highest score:
?
e ? argmax
e?E
{
?
r
? h
r
(
?
q, e, f)
}
(7)
where h
r
(?) is a vector of retrieval features and ?
r
is the corresponding feature weight vector.
Due to the ambiguity of translation, however, state-of-the-art MT systems are still far from producing
high-quality translations, especially for distantly-related languages. As a result, the 1-best translations
are usually erroneous and potentially introduce retrieval mistakes.
A natural solution is to use n-best lists as queries:
?
e ? argmax
e?E
{
max
q?N(f)
{
? ? h(q, e, f)
}
}
(8)
where N(f) ? T(f) is the n-best translations of the input source sentence f .
2033
Figure 1: Two kinds of query lattices: (a) search graph that is generated after phrase-based decoding and
(b) translation option graph that is generated before decoding. Translation option graph is more compact
and encodes more translation candidates.
Although using n-best lists apparently improves the retrieval accuracy over using 1-best lists, there
are two disadvantages. First, the decision rule in Eq. (8) requires to enumerate all the n translations and
retrieve for n times. In other words, the time complexity increases linearly. Second, an n-best list only
accounts for a tiny fraction of the exponential search space of translation. To make things worse, there
are usually very few variations in n-best translations because of spurious ambiguity - a situation where
multiple derivations give similar or even identical translations.
Therefore, we need to find a more elegant way to enable the retrieval module to explore exponentially
many queries without sacrificing efficiency.
3.2 Query Lattice
We propose to use query lattice to compactly represent exponentially many queries. For example, given
a source sentence ?bushi yu shalong juxing huitan?, we can use the search graph produced by a phrase-
based translation system (Koehn et al., 2007) as a lattice to encode exponentially many derivations.
Figure 1(a) shows a search graph for the example source sentence. Each edge is labeled with an
English phrase as well as the corresponding translation feature value vector. Node 0 denotes the starting
node. Node 7 and node 8 are two ending nodes. Each path from the starting node to an ending node
denotes a query. Paths that reach the same node in the lattice correspond to recombined hypotheses
that have equivalent feature histories (e.g., coverage, last generated target words, the end of last covered
source phrase, etc) in phrase-based decoding.
However, there are two problems with using search graph as query lattice. First, it is computationally
expensive to run a phrase-based system to generate search graphs. The time complexity for phrase-based
decoding with beam search is O(n
2
b) (Koehn et al., 2007), where n is the length of source string and b is
the beam width. Moreover, the memory requirement is usually very high due to language models. As a
result, translation is often two orders of magnitude slower than retrieval. Second, a search graph has too
many ?duplicate? edges due to different reordering, which increase the time complexity of retrieval (see
Section 3.3). For example, in Figure 1(a), the English phrase ?Sharon? occurs two times due to different
reordering.
Alternatively, we propose to use translation option graph as query lattice. In a phrase-based trans-
lation system, translation options that are phrase pairs matching a substring in the input source string
are collected before decoding. These translation options form a query lattice with monotonic reorder-
ing. Figure 1(b) shows an example translation option graph, in which nodes are sorted according to the
positions of source words. Each edge is labeled with an English phrase as well as the corresponding
translation feature value vector.
We believe that translation option graph has three advantages over search graph:
1. Improved efficiency in translation. Translation option graph requires no decoding.
2. Improved efficiency in retrieval. Translation option graph has no duplicate edges.
2034
Algorithm 1 Retrieval with lattice as query.
1: procedure LATTICERETRIEVE(L(f),E, k)
2: Q? GETWORDS(L(f)) . Get distinct words in the lattice to form a coarse query
3: E
k
? RETRIEVE(E, Q, k) . Retrieve top-k target sentences using the coarse query
4: for all e ? E
k
do
5: FINDPATH(L(f), e) . Find a path with the highest score
6: end for
7: SORT(E
k
) . Sort retrieved sentences according the scores
8: return E
k
9: end procedure
Algorithm 2 Find a path with the highest score.
1: procedure FINDPATH(L(f), e)
2: for v ? L(f) in topological order do
3: path(v)? ? . Initialize the Viterbi path at node v
4: score(v)? 0 . Initialize the Viterbi score at node v
5: for u ? IN(v) do . Enumerate all antecedents
6: p? path(u) ? {e
u?v
} . Generate a new path
7: s? score(u) + COMPUTESCORE(e
u?v
) . Compute the path score
8: if s > score(v) then
9: path(v)? p . Update the Viterbi path
10: score(v)? s . Update the Viterbi score
11: end if
12: end for
13: end for
14: end procedure
3. Enlarged search space. Translation option graph represents the entire search space of monotonic
decoding while search graph prunes many translation candidates.
In Figure 1, the search graph has 9 nodes, 10 edges, 4 paths, and 3 distinct translations. In contrast,
the translation option graph has 6 nodes, 9 edges, 10 paths, and 10 distinct translations. Therefore,
translation option graph is more compact and encodes more translation candidates.
Although translation option graph ignores language model and lexcialized reordering models, which
prove to be critical information sources in machine translation, we find that it achieves comparable or
even better retrieval accuracy than search graph (Section 4). This confirms the finding of Liu et al. (2012)
that language model and lexicalized reordering models only have modest effects on translation retrieval.
3.3 Retrieval with Query Lattice
Given a target corpus E and a query lattice L(f) ? Q(f), our goal is to find the target sentence
?
e with
the highest score ? ? h(q, e, f):
?
e ? argmax
e?E
{
max
q?L(f)
{
? ? h(q, e, f)
}
}
(9)
Due to the exponentially large search space, we use a coarse-to-fine algorithm to search for the target
sentence with the highest score, as shown in Algorithm 1. We use an example to illustrate the basic idea.
Given an input source sentence ?bushi yu shalong juxing le huitan?, our system first generates a query
lattice like Figure 1(a). It is non-trivial to directly feed the query lattice to a retrieval system. Instead, we
would like to first collect all distinct words in the lattice: {?Bush?, ?and? , ?Sharon?, ?held?, ?a?, ?talk?,
?talks?, ?with?}. This set serves as a coarse single query and the retrieval system returns a list of target
sentences that contain these words:
2035
Chinese English
Training 1.21M 1.21M
Dev in-domain query 5K
document 2.23M
out-of-domain query 5K
document 2.23M
Test in-domain query 5K
document 2.23M
out-of-domain query 5K
document 2.23M
Table 1: The datasets for the retrieval evaluation. The training set is used to train the phrase-based
translation model and language model for Moses (Koehn et al., 2007). The development set is used
to optimize feature weights using the minimum-error-rate algorithm (Och, 2003). A development set
consists of a query set and a document set. The test set is used to evaluate the retrieval accuracy. To
examine the effect of domains on retrieval performance, we used two development and test sets: in-
domain and out-domain.
President Bush gave a talk at a meeting
Bush held a meeting with Sharon
Sharon and Bush attended a meeting held at London
Note that as a retrieval system usually ignores the structural dependencies in text, the retrieved sentences
(scored by retrieval features) are relevant but not necessarily translations of the input. Therefore, we
can match each retrieved sentence against the query lattice to find a path with the highest score using
additional translation features. For example, the Viterbi path for ?Bush held a meeting with Sharon? in
Figure 1(a) is ?Bush held talks with Sharon?. The translation features of matched arcs in the path are
collected to compute the overall score according to Eq. (9). Finally, the algorithm returns a sorted list:
Bush held a meeting with Sharon
President Bush gave a talk at a meeting
Sharon and Bush attended a meeting held at London
More formally, the input of Algorithm 1 are a query lattice L(f), a target corpus E, and a parameter
k (line 1). The function GETWORDS simply collects all the distinct words appearing in the lattice (line
2), which are used for constructing a coarse boolean query Q. Then, the function RETRIEVE runs to
retrieve the top-k target sentences E
k
in the target corpus E only using standard IR features according
to the query Q (line 3). These first two steps eliminate most unlikely candidates and return a coarse set
of target sentence candidates efficiently.
1
Then, a procedure FINDPATH(L(f), e) runs to search for the
translation with the highest score for each candidate (lines 4-6). Finally, the algorithm returns the sorted
list of target sentences (lines 7-9).
Algorithm 2 shows the procedure FINDPATH(L(f), e), which searches for the path with higher score
using a Viterbi-style algorithm. The function COMPUTESCORE scores an edge according to the Eq. (9)
which linearly combines the translation and retrieval features.
Generally, the lattice-based retrieval algorithm has a time complexity of O(k|E|), where |E| is the
number of edges in the lattice.
4 Experiments
In this section, we try to answer two questions:
1. Does using query lattices improve translation retrieval accuracy over using n-best lists?
2. How does translation retrieval benefit other end-to-end NLP tasks such as machine translation?
1
In our experiments, we set the parameter k to 500 as a larger value of k does not give significant improvements but introduce
more noises.
2036
Accordingly, we evaluated our system in two tasks: translation retrieval (Section 4.1) and parallel
corpus mining (Section 4.2).
4.1 Evaluation on Translation Retrieval
4.1.1 Experimental Setup
In this section, we evaluate the accuracy of translation retrieval: given a query set (i.e., source sentences),
our system returns a sorted list of target sentences. The evaluation metrics include precision@n and
recall.
The datasets for the retrieval evaluation are summarized in Table 1. The training set, which is used to
train the phrase-based translation model and language model for the-state-of-the-art phrase-based system
Moses (Koehn et al., 2007), contains 1.21M Chinese-English sentences with 32.0M Chinese words and
35.2M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on
the English side of the training corpus. The development set, which is used to optimize feature weights
using the minimum-error-rate algorithm (Och, 2003), consists of query set and a document set. We
sampled 5K parallel sentences randomly, in which 5K Chinese sentences are used as queries and half
of their parallelled English sentences(2.5K) mixed with other English sentences(2.3M) as the retrieval
document set. As a result, we can compute precision and recall in a noisy setting. The test set is used
to compute retrieval evaluation metrics. To examine the effect of domains on retrieval performance, we
used two data sets: in-domain and out-domain. The in-domain development and test sets are close to
the training set while the out-domain data sets are not.
We compare three variants of translation retrieval: 1-best list, n-best list, and lattice. For query lattice,
we further distinguish between search graph and translation option graph. They are generated by Moses
with the default setting.
We use both translation and retrieval features in the experiments. The translation features include
phrase translation probabilities, phrase penalty, distance-based and lexicalized reordering models, lan-
guage models, and word penalty. Besides the conventional IR features such as term frequency and
inverse document frequency, we use five additional featured derived from BLEU (Papineni et al., 2002):
the n-gram matching precisions between query and retrieved target sentence (n = 1, 2, 3, 4) and brevity
penalty. These features impose structural constraints on retrieval and ensure translation closeness of re-
trieved target sentences. The minimum-error-rate algorithm supports a variety of loss functions. The loss
function we used in our experiment is 1?P@n. Note that using translation option graph as query lattice
does not include language models and distance-based lexicalized reordering models as features.
4.1.2 Evaluation Results
Table 2 shows the results on the in-domain test set. The ?# candidates? column gives the number of
translation candidates explored by the retrieval module for each source sentence on average. The lattices,
either generated by search graph or by translation options, contain exponentially many candidates. We
find that using lattices dramatically improves the precisions over using 1-best and n-best lists. All the
improvements over 1-best and n-best lists are significant statistically. The 1-best, n-best, and the search
graph lattice share with the same translation time: 5,640 seconds for translating 5,000 queries. Note
that the translation time is zero for the translation option graph because it does not need phrase-based
decoding. For retrieval, the time cost for the n-best list method generally increases linearly. As the search
graph lattice contains many edges, the retrieval time increases by an order of magnitude as compared
with 100-best list. An interesting finding is that using translation options as a lattice contains more
candidates and consumes much less time for retrieval than using search graph as a lattice. One possible
reason is that a search graph generated by Moses usually contains many redundant edges. For example,
Figure 1 is actually a search graph and many phrases occurs multiple times in the lattice (e.g., ?and?
and ?Sharon?). In contrast, a lattice built by translation options hardly has any redundant edges but
still represents exponentially many possible translations. We can also see that the lattice constructed by
search graph considering language model can benefit the precision much, especially when n is little. But
this advantage decreases with n increasing and the time consumed by translation options as lattice is
much less than the search graph as lattice. Besides, the margin between them is not too large so we can
2037
method # candidates
P@n time
n=1 n=5 n=10 n=20 n=100 translation retrieval
1-best 1 87.40 91.40 92.24 92.88 93.64 5,640 82
10-best 10 89.84 93.20 93.96 94.36 95.56 5,640 757
100-best 100 90.76 94.32 95.00 95.76 96.76 5,640 7,421
lattice (graph) 1.20? 10
54
93.60 96.08 96.28 96.52 96.80 5,640 89,795
lattice (options) 4.14? 10
62
93.28 95.84 95.96 96.16 96.84 0 307
Table 2: Results on the in-domain test set. We use the minimum-error-rate training algorithm (Och,
2003) to optimize the feature with the respect to 1?P@n.
method # candidates
P@n time
n=1 n=5 n=10 n=20 n=100 translation retrieval
1-best 1 67.32 76.60 79.40 81.80 83.76 3,660 92
10-best 10 72.68 80.96 83.36 85.84 88.76 3,660 863
100-best 100 78.60 85.76 87.76 89.64 92.16 3,660 8,418
lattice (graph) 1.51? 10
61
84.32 89.40 90.68 91.56 92.44 3,660 67,205
lattice (options) 1.24? 10
65
81.92 88.00 89.80 91.24 93.16 0 645
Table 3: Results on the out-of-domain test set.
0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.3
0.4
0.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 
lattice(search graph)lattice(translation options)100?best?list10?best1?best
Figure 2: In-domain Precision-Recall curves.
0.3 0.4 0.5 0.6 0.7 0.8 0.90.3
0.4
0.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 
lattice(search graph)lattice(translation options)100?best?list10?best1?best
Figure 3: Out-domain Precision-Recall curves.
abandon some little precision for obtain the large time reducing. Therefore, using translation options as
lattices seems to be both effective and efficient.
Table 3 shows the results on the out-of-domain test set. While the precisions for all methods drop, the
margins between lattice-based retrieval and n-best list retrieval increase, suggesting that lattice-based
methods are more robust when dealing with noisy datasets.
Figures 2 and 3 show the Precision-Recall curves on the in-domain and out-of-domain test sets. As
the query set is derived from parallel sentences, recall can be computed in our experiments. The curves
show that using lattices clearly outperforms using 1-best and n-best lists. The margins are larger on the
out-of-domain test set.
4.2 Evaluation on Parallel Corpus Mining
In this section, we evaluate translation retrieval on the parallel corpus mining task: extracting a parallel
corpus from a comparable corpus.
4.2.1 Experimental Setup
The comparable corpus for extracting parallel sentences contains news articles published by Xinhua
News Agency from 1995 to 2010. Table 4 shows the detailed statistics. There are 1.2M Chinese and
1.7M English articles.
We re-implemented the method as described in (Munteanu and Marcu, 2005) as the baseline system.
2038
language articles sentences words vocabulary
Chinese 1.2M 18.5M 441.2M 2.1M
English 1.7M 17.8M 440.2M 3.4M
Table 4: The Xinhua News Comparable Corpus from 1995 to 2010
Munteanu and Marcu (2005) this work
English words Chinese words BLEU English words Chinese Words BLEU
5.00M 4.12M 22.84 5.00M 3.98M 25.44
10.00M 8.20M 25.10 10.00M 8.17M 26.62
15.00M 12.26M 25.41 15.00M 12.49M 26.49
20.00M 16.30M 25.56 20.00M 16.90M 26.87
Table 5: Comparison of BLEU scores using parallel corpora extracted by the baseline and our system.
Given a comparable corpus (see Table 4), both systems extract parallel corpora that are used for training
phrase-base models (Koehn et al., 2007). The baseline system is a re-implementation of the method
described in (Munteanu and Marcu, 2005). Our system uses translation option graph as query lattice.
Our system significantly outperforms the baseline for various sizes.
It assigned a score to each sentence pair using a classifier. Our system used translation option graph as
query lattices due to its simplicity and effectiveness. For each source sentence in the comparable corpus,
our system retrieved the top target sentence together with a score.
To evaluate the quality of extracted parallel corpus, we trained phrase-based models on it and ran
Moses on NIST datasets. The development set is the NIST 2005 test set and the test set is the NIST 2006
test set. The final evaluation metric is case-insensitive BLEU-4.
4.2.2 Evaluation Results
Table 5 shows the comparison of BLEU scores using parallel corpora extracted by the baseline and our
system. We find that our system significantly outperforms the baseline for various parallel corpus sizes.
This finding suggests that using lattice to compactly represent exponentially many alternatives does help
to alleviate the translation error propagation problem and identify parallel sentences of high translational
equivalence.
5 Conclusion
In this work, we propose to use query lattice to address the translation error propagation problem in
translation retrieval. Two kinds of query lattices are used in our experiments: search graph and translation
option graph. We show that translation option graph is more compact and represents a much larger
search space. Our experiments on Chinese-English datasets show that using query lattices significantly
outperforms using n-best lists in the retrieval task. Moreover, we show that translation retrieval is capable
of extracting high-quality parallel corpora from a comparable corpus. In the future, we plan to apply
our approach to retrieving translation candidates directly from the Web, which can be seen as a huge
monolingual corpus.
Acknowledgments
This research is supported by the 973 Program (No. 2014CB340501), the National Natural Science
Foundation of China (No. 61331013 and No. 61033001), the 863 Program (No. 2012AA011102),
Toshiba Corporation Corporate Research & Development Center, and the Singapore National Research
Foundation under its International Research Centre @ Singapore Funding Initiative and administered by
the IDM Programme.
2039
References
T. Baldwin and H. Tanaka. 2000. The effects of word order and segmentation on translation retrieval performance.
In Proceedings of COLING.
Timothy Baldwin. 2001. Low-cost, high-performance translation retrieval: Dumber is better. In Proceedings of
ACL, pages 18?25, Toulouse, France, July. Association for Computational Linguistics.
Karen Cheung and Douglas Vogel. 2005. Complexity reduction in lattice-based information retrieval. Information
Retrieval, pages 285?299.
Tee Kiah Chia, Khe Chai Sim, Haizhou Li, and Hwee Tou Ng. 2010. Statistical lattice-based spoken document
retrieval. ACM Transactions on Information Systems, 28(1).
Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-HLT, pages 1012?1020, Columbus, Ohio, June. Association for Computational Linguistics.
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Yajuan L?u. 2009. Lattice-based system combination for statis-
tical machine translation. In Proceedings of EMNLP, pages 1105?1113, Singapore, August. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL - Demo
and Poster Sessions, pages 177?180, Prague, Czech Republic, June. Association for Computational Linguistics.
Chunyang Liu, Qi Liu, Yang Liu, and Maosong Sun. 2012. THUTR: A translation retrieval system. In Proceed-
ings of COLING - Demo and Poster Sessions, pages 321?328, Mumbai, India, December. The COLING 2012
Organizing Committee.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based minimum error rate
training for statistical machine translation. In Proceedings of EMNLP, pages 725?734, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Spyros Matsoukas, Ivan Bulyko, Bing Xiang, Kham Nguyen, Richard Schwartz, and John Makhoul. 2007. In-
tegrating speech recognition and machine translation. In Proceedings of ICASSP, volume 4, pages IV?1281.
IEEE.
C.N. Moore. 1958. A mathematical theory of the use of language symbols in retrieval. In ICSI 1958.
Dragos Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-
parallel corpora. Computational Linguistics, 31(4):477?1504.
S. Nirenburg, C. Domashnev, and D.J. Grannes. 1993. Two approaches to matching in example-based machine
translation. In TMI 1993.
Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical ma-
chine translation. In Proceedings of ACL, pages 295?302, Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL,
pages 160?167, Sapporo, Japan, July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of ACL, pages 311?318, Philadelphia, Pennsylvania, USA, July.
Association for Computational Linguistics.
Uta Priss. 2000. Lattice-based information retrieval. Knwoledge Organization, 27(3):132?142.
Murat Saraclar and Richard Sproat. 2004. Lattice-based search for spoken utterance retrieval. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-NAACL, pages 129?136, Boston, Massachusetts, USA, May.
Association for Computational Linguistics.
S. Sato and M. Nagao. 1990. Toward memory-based translation. In Proceedings of COLING.
Andreas Stolcke. 2002. Srilm: an extensible language modeling toolkit. In Proceedings of ICSLP.
2040
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk decoding
for statistical machine translation. In Proceedings of EMNLP, pages 620?629, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann Ney. 2005. Integrated chinese word segmentation in
statistical machine translation. In Proceedings of IWSLT 2005, pages 141?147, Pittsburgh, PA, October.
Zheng-Yu Zhou, Peng Yu, Ciprian Chelba, and Frank Seide. 2006. Towards spoken-document retrieval for the
internet: Lattice indexing for large-scale web-search architectures. In Proceedings of HLT-NAACL, pages 415?
422, New York City, USA, June. Association for Computational Linguistics.
2041
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 100?105,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Learning to Detect Hedges and their Scope Using CRF 
Qi Zhao, Chengjie Sun, Bingquan Liu, Yong Cheng 
Harbin Institute of Technology, HIT 
Harbin, PR China 
{qzhao, cjsun, liubq, ycheng}@insun.hit.edu.cn 
 
Abstract 
Detecting speculative assertions is essential 
to distinguish the facts from uncertain 
information for biomedical text. This paper 
describes a system to detect hedge cues and 
their scope using CRF model. HCDic feature 
is presented to improve the system perfor-
mance of detecting hedge cues on BioScope 
corpus. The feature can make use of cross-
domain resources.  
1 Introduction 
George Lakoff (1972) first introduced linguistic 
hedges which indicate that speakers do not back 
up their opinions with facts. Later other linguists 
followed the social functions of hedges closely. 
Interestingly, Robin Lakoff (1975) introduces 
that hedges might be one of the ?women?s 
language features? as they have higher frequency 
in women?s languages than in men?s. 
In the natural language processing domain, 
hedges are very important, too. Along with the 
rapid development of computational and 
biological technology, information extraction 
from huge amount of biomedical resource 
becomes more and more important. While the 
uncertain information can be a noisy factor 
sometimes, affecting the performance of 
information extraction. Biomedical articles are 
rich in speculative, while 17.70% of the 
sentences in the abstracts section of the 
BioScope corpus and 19.44% of the sentences in 
the full papers section contain hedge cues 
(Vincze et al, 2008). In order to distinguish facts 
from uncertain information, detecting speculative 
assertions is essential in biomedical text.  
Hedge detection is paid attention to in the 
biomedical NLP field. Some researchers regard 
the problem as a text classification problem (a 
sentence is speculative or not) using simple 
machine learning techniques. Light et al (2004) 
use substring matching to annotate speculation in 
biomedical text. Medlock and Briscoe (2007) 
create a hedging dataset and use an SVM 
classifier and get to a recall/precision Break-
Even Point (BEP) of 0.76. They report that the 
POS feature performs badly, while lemma 
feature works well. Szarvas (2008) extends the 
work of Medlock and Briscoe with feature 
selection, and further improves the result to a 
BEP of 0.85 by using an external dictionary. 
Szarvas concludes that scientific articles contain 
multiword hedging cues more commonly, and 
the portability of hedge classifiers is limited. 
Halil Kilicoglu and Sabine Bergler (2008) 
propose an algorithm to weight hedge cues, 
which are used to evaluate the speculative 
strength of sentences. Roser Morante and Walter 
Daelemans (2009) introduce a metalearning 
approach to process the scope of negation, and 
they identify the hedge cues and their scope with 
a CRF classifier based on the original work. 
They extract a hedge cues dictionary as well, but 
do not combine it with the CRF model. 
In the CoNLL-2010 shared task (Farkas et al, 
2010), there are two subtasks for worldwide 
participants to choose: 
? Task 1: learning to detect sentences 
contain-ing uncertainty.  
? Task 2: learning to resolve the in-
sentence scope of hedge cues.  
This paper describes a system using CRF 
model for the task, which is partly based on 
Roser Morante and Walter Daelemans? work. 
2 Hedges in the training dataset of 
BioScope and Wikipedia Corpus 
Two training datasets, the BioScope and Wiki-
pedia corpus are provided in the CoNLL-2010 
shared task. BioScope consists of two parts, full 
articles and abstracts collected from biomedical 
papers. The latter is analyzed for having larger 
scale and more information of hedges.  
In Table 1, the percentage of the speculative 
sentences in the abstracts section of BioScope 
corpus is the same as Vincze et al (2008) 
reported. We can estimate 1.28 cue words per 
sentence, meaning that each sentence usually just 
has one hedge cue. The statistics in Table 1 also 
100
indicate that a hedge cue appears 26.7 times on 
average. 
 
Dataset ITEM # 
Sentences 11871 
Certain sentences 9770 
Uncertain 
sentences 
2101 
(17.7%) 
Hedge cues 2694 
cues# per sentence 1.28 
Different hedge 
cues 
143 
Abstracts 
of 
BioScope 
Max length of the 
cues 
4 
Sentences 11111 
Certain sentences 8627 
Uncertain 
sentences 
2484 
(22.4%) 
weasel cues 3133 
Different weasel 
cues 
1984 
Wikipedia 
Max length of the 
cues 
13 words 
 
Table 1: Statistics about the abstracts section of 
the BioScope corpus and Wikipedia corpus. 
 
We extract all the hedge cues from the 
abstracts section of BioScope corpus, getting 143 
different hedge cues and 101 cues with ignoring 
morphological changes. The maximum length of 
the cues is 4, with 1.44 words per hedge cue. 
This suggests that most hedge cues happen to be 
a single word. We assume that hedge cues set is 
a limited one in BioScope corpus. Most hedge 
cues could be identified if the known dataset of 
hedge cues is large enough. The cue words 
collected from the BioScope corpus play an 
important role in the speculative sentences 
detection. 
In contrast to the biomedical abstracts, the 
weasel cues on Wikipedia corpus make a little 
difference. Most weasel cues consist of more 
than one word, and usually appear once. This 
leads to different results in our test. 
A hedge cue word may appear in the non-
speculative sentences. Occurrences of the four 
typical words in speculative and non-speculative 
sentences are counted. 
As shown in Table 2, the cue words can be 
divided into two classes generally. The hedge 
cue words ?feel? and ?suggesting?, which are 
grouped as one class, only act as hedge cues with 
never appearing in the non-speculative sentences. 
While ?may? and ?or? appear both in the 
speculative and non-speculative sentences, which 
are regard as the other one. Moreover, we treat 
the words ?may? and ?or? in the same class 
differently, while ?may? is more likely to be a 
hedge cue than ?or?. The treatment is also 
unequal between ?feel? and ?suggesting?. In the 
training datasets, the non-S#/S# ratio can give a 
weight to distinguish the words in each class. 
After all, we can divide the hedge cues into 4 
groups. 
 
word S# non-S# 
feel 1 0 
suggesting 150 0 
may 516 1 
or 118 6218 
 
Table 2: Statistics of cue words. (S# short for the 
occurrence times in speculative sentences, non-
S# for the count in non-speculative ones) 
3 Methods 
Conditional random fields (CRF) model was 
firstly introduced by Lafferty et al (2001). CRF 
model can avoid the label bias problem of 
HMMs and other learning approaches. It was 
applied to solve sequence-labeling problems, and 
has shown good performance in NER task. We 
consider hedge cues detection as some kind of 
sequence-labeling problem, and the model will 
contribute to a good result.  
We use CRF++ (version 0.51) to implement 
the CRF model. Cheng Yong, one of our team 
members has evaluated the several widespread 
used CRF tool kits, and he points out that 
CRF++ has better precision and recall but longer 
training time. Fortunately, the training time cost 
of BioScope corpus is acceptable. In our system, 
all the data training and testing processing step 
can be completed within 8 minutes (Intel Xeon 
2.0GHz CPU, 6GB RAM). It is likely due to the 
small scale of the training dataset and the limited 
types of the annotation. 
To identify sentences in the biomedical texts 
that contain unreliable or uncertain information 
(CoNLL-2010 shared task1), we start with hedge 
cues detection: 
? If one or more than one hedge cues are 
detected in the sentence, then it will be 
annotated ?uncertain? 
? If not, the sentence will be tagged as 
?certain?. 
101
3.1 Detecting hedge cues 
The BioScope corpus annotation guidelines 1 
show that most typical instances of keywords can 
be grouped into 4 types as Auxiliaries, Verbs of 
hedging or verbs with speculative content, 
Adjectives or adverbs, and Conjunctions. So the 
POS (part-of-speech) is thought to be the feature 
reasonably. Lemma feature of the word and 
chunk features are also considered to improve 
system performance. Chunk features may help to 
the recognition of biomedical entity boundaries. 
GENIA Tagger (Tsuruoka et al, 2005) is em-
ployed to obtain part-of-speech (POS) features, 
chunk features and lemma features. It works well 
for biomedical documents. 
In the biomedical abstracts section of Bio-
Scope corpus, the hedge cues are collected into a 
dictionary (HCDic, short for the Hedge Cues 
Dictionary). As mentioned in section 2, one 
hedge cue appears 26.7 times on average, and we 
assume the set of hedge cues is limited. The 
HCDic consist of 143 different hedge cues 
extracted from the abstracts. The dictionary 
(HCDic) extracted from the corpus is very 
valuable for the system. We can focus on 
whether the word such as ?or? listed in table 2 is 
a hedge cue or not. The cue words in HCDic are 
divided into 4 different levels with the non-S#/S# 
ratio. 
The four types are described as ?L?, ?H?, 
?FL? and ?FH?. ?L? shows low confidence of 
the cue word being a hedge cue, while ?H? 
indicates high confidence about it. The prefix ?F? 
for ?FL?/?FH? shows false negatives may 
happen to the cue word in HCDic. The threshold 
for the non-S#/S# ratio to distinguish ?FL? type 
from ?FH? is set 1.0. As the non-S#/S# ratio of 
?L? and ?H? is always zero, we set the hedge cue 
whose S# is more than 5 as ?H? type as shown in 
table 3. The four types are added into the HCDic 
along with the hedge cues,  
In our experiment, HCDic types of word 
sequence are tagged as follows: 
? If words are found in HCDic using 
maximum matching method, label them 
with their types in HCDic. For hedges of 
multi-word, label them with BI scheme 
which will be described later. 
? If not, tag the words as ?O? type.  
                                                 
1
 http://www.inf.u-szeged.hu/rgai/bioscope 
The processing assigns each token of a 
sentence with an HCDic type. The BIO types for 
each token are involved as features for the CRF. 
The HCDic can be expanded to a larger scale. 
Hedge cues extracted from different corpora can 
be added into HCDic, and regular expression of 
hedge cues can be used, too. This will be helpful 
to the usage of cross-domain resources. 
 
word S# non-S# type  
feel 1 0 L 
suggesting 150 0 H 
may 516 1 FH 
or 118 6218 FL 
 
Table 3: Types of the HCDic words. (S# and 
non-S# have the same meaning as in Table 2) 
 
The features F (F stands for all the Features) 
including unigram, bigram, and trigram types is 
used for CRF as follows: 
 
F(n)(n=-2,-1,0,+1,+2) 
F(n-1)F(n)(n=-1,0,+1,+2) 
F(n-2)F(n-1)F(n) (n=0,+1,+2) 
Where F(0) is the current feature, F(-1) is the 
previous one, F(1) is the following one, etc. 
 
We regard each word in a sentence as a token 
and each token is tagged with a cue-label. The 
BIO scheme is used for tagging multiword hedge 
cues, such as ?whether or not? in our HCDic. 
where B-cue (tag for ?whether?) represents that 
the token is the start of a hedge cue, I-cue (tag 
for ?or?, ?not?) stands for the inside of a hedge 
cue, and O (tag for the other words in the 
sentence) indicates that the token does not 
belong to any hedge cue. 
We also have the method tested on Wikipedia 
corpus with a preprocessing of the HCDic. 
Section 2 reports that most weasel cues in 
Wikipedia corpus are multiword, and usually 
appear once. Different from our assumption in 
BioScope corpus, the set of weasel cues seems 
numerous. The HCDic of Wikipedia would be 
not so valuable if it tags few tokens for a new 
given text. To prevent these from happening, a 
preprocessing of the HCDic is taken. 
Most of the hedge cues in Wikipedia corpus 
accord with the structure of ?adjective + noun? 
e.g. ?many persons?. Although most cue words 
appear just once, the adjective usually happens to 
be the same, and we call them core words. 
Therefore, the hedge cue dictionary (HCDic) can 
be simplified with the core words. It helps to 
102
reduce the scale of the hedges cues from 1984 
cues down to 170. Then, we process the 
Wikipedia text the same way as the BioScope 
corpus. 
3.2 Detecting scope of hedge cues  
This phase (for CoNLL-2010 shared task 2) is 
based on Roser Morante and Walter Daelemans? 
scope detection system. 
CRF model is applied in this part, too. The 
word, POS, lemma, chunk and HCDic tags are 
also applied to be the features as in the step of 
hedge cues detection. In section 3.1, we can 
obtain the hedge cues in a sentence. The scope 
relies on its cue vary much. We make the BIO 
schema of detected hedge cues to be the 
important features of this part. Besides, the 
sentences tagged as ?certain? type are neglected 
in this step. 
Here is an example of golden standard of 
scope label.  
 
<sentence id="S5.149"> We <xcope id="X5.149. 
3"><cue ref="X5.149.3" type= "specula-tion"> 
propose </cue> that IL-10-producing Th1 cells 
<xcope id="X5.149.2"> <cue ref="X5.149.2" 
type= "speculation" >may</cue> be the essential 
regulators of acute infection-induced inflammation 
</xcope> and that such ?self-regulating? Th1 cells 
<xcope id= "X5.149.1"> <cue ref= "X5.149.1" 
type= "speculation" >may</cue> be essential for 
the infection to be cleared without inducing 
immune-mediated pathology </xcope> </xcope>. 
 
As shown, each scope is a block with a 
beginning and an end, and we refer to the 
beginning of scope as scope head (<xcope?>), 
and the end of the scope as scope tail 
(</xcope>). 
The types of the scope are labeled as: 
 
1. Label the token next to scope head as 
?xcope-H? ( e.g. propose, may ) 
2. Tag the token before scope tail as ?xcope-
T?(e.g. pathology for both scopes)  
3. The other words tag ?O? , including the 
words inside the scope and out of it. This 
is very different from the BIO scheme. 
 
The template for each feature is the same as in 
section 3.1. 
Following are our rules to form the scope of a 
hedge: 
 
1. Most hedge cues have only one scope tag, 
meaning there is one-to-one relationship 
between hedge cue and its scope. 
2. The scope labels may be nested. 
3. The scope head of the cue words appears 
nearest before hedge cue. 
4. The scope tail appears far from the cue 
word. 
5. The most frequent head/tail positions of the 
scope are shown in Table 4. 
a) The scope head usually is just before 
the cue words. 
b) The scope tail appears in the end of the 
sentence frequently. 
 
Scopes of hedge cues in BioScope corpus 
should be found for the shared task. The training 
dataset of abstract part is analyzed for its larger 
scale  
 
item Following strings  
with high frequency % 
1 
scope 
head 
<cue...>(cue words) 0.861 
?.?(sentence end) 0.695 
</xcope> 
(another scope tail) 0.144 
2 
scope 
tail 
?,?  ?;?  ?:? 0.078 
 
Table 4: Statistics of the strings nearby the scope 
head and tail. Item 1 shows the word follow 
scope head, and item 2 shows the frequent words 
next to the scope tail. 
 
We analyze the words around the scope head 
and the scope tail. The item 1 in Table 4 shows 
that 86.1% of the following words of the scope 
head are hedge cues. Other following words not 
listed are less than 1%, according to our 
statistics. The item 2 lists the strings with high 
frequency next to the scope tail as well. The first 
2 words in item 2 can be combined sometimes, 
so the percentage of scope tail at the end of the 
sentence can be more than 80%. The strings 
ahead of scope head and tail not listed are also 
counted, but they do not give such valuable 
information as the two items listed in Table 4. 
Therefore, when the CRF model gives low 
confidence, we just set the most probable 
positions of scope head and tail. 
For the one-to-one relationship between hedge 
cues and their scopes, we make rules to insure 
each cue has only one scope, including the scope 
head and scope tail. 
103
Rule 1: if more than one scope heads or tails 
are predicted, we get rid of the farther head or 
nearer tail. 
Rule 2: if none of scope head or tail is pre-
dicted, the head is set to the word just before the 
cue words; the tail is set at the end of the 
sentence. 
Rule 3: if one scope head and one tail are 
predicted, we consider them the result of scope 
detection. 
4 Results 
Our experiments are based on the CoNLL-2010 
shared task?s datasets, including BioScope and 
Wikipedia corpus. All the experiments for 
BioScope use abstracts and full papers for 
training data and the provided evaluation for 
testing. 
We employ CRF model to detect the hedge 
cues in the BioScope. The experiments are 
carried out on different feature sets: words 
sequence with the chunk feature only, lemma 
feature only and POS feature only. The effect of 
the HCDic feature is also evaluated. 
 
Features prec. recall F-score 
Chunk only 0.7236 0.6275 0.6721 
Lemma only 0.7278 0.6103 0.6639 
POS only 0.7320 0.6208 0.6718 
Without 
HCDic 
0.7150 0.6447 0.6781 
ALL 0.7671 0.7393 0.7529 
 
Table 5: Results at hedge cue-level 
 
As described in section 1 of this paper, the 
feature of POS may be not so significant as the 
lemma, but we do not agree with this point of 
view for given POS feature's better performance 
in F-score (in Table 5). The interesting cue-level 
result does not go into for time limitations. The 
F-score of the three features, chunk, lemma and 
POS are approximately equal. When all of the 
three features are used for CRF model, the 
performance is not improved so significantly. 
The recall rate is a bit low in the experiment 
without HCDic features. As shown in Table 5, 
the feature of HCDic is effective to get a better 
score both in precision rate and in recall rate. As 
our assumption, hedges in the evaluation dataset 
are limited, too. Most of them along with some 
non-hedges can be tagged with HCDic. Then the 
tag could contribute to a good recall. It also helps 
the classifier to focus on whether the words with 
?L?, ?FL?, and ?FH? are hedge cues or not, 
which will be good for a better precision. 
With detected hedge cues, we can get senten-
ces containing uncertainty for the shared task 1. 
A sentence is tagged as ?uncertain? type if any 
hedge cue is found in it.  
 
 precision recall F-score 
Without 
HCDic 0.8965 0.7898 0.8398 
ALL  0.8344 0.8481 0.8412 
 
Table 6: Evaluation result of task 1 
 
Statistics in Table 6 show that even poor 
performance in cue-level test can get a 
satisfactory F-score of speculative sentences 
detection as well. It seems that hedges detection 
at cue-level is not proportionate to the sentence-
level. Think about instance of more than one 
cues in a sentence such as the example of golden 
standard in section 3.2, the sentence will be 
tagged even if only one hedge cue has been 
identified (lower recall at cue-level). Moreover, 
in the speculative sentence with one hedge cue, 
false positives (lower precision at cue-level) can 
also lead to the correct result at sentence-level. 
The method is also tested on Wikipedia corpus, 
using provided training dataset and evaluation 
data. The method has a bad performance in our 
close test. The results are listed in Table 7. 
As talked in section 2, hedges in Wikipedia 
corpus are very different from in BioScope 
corpus. Besides, the string matching method for 
simplified HCDic is not so effective. The useful-
ness of HCDic is not so significant for a good 
recall in Wikipedia corpus.  
 
dataset precision recall F-score 
Wikipedia 0.7075 0.2001 0.3120 
BioScope 0.7671 0.7393 0.7529 
 
Table 7: Results of weasel/hedge detection in 
Wikipedia and BioScope corpus. 
 
In CoNLL-2010 shared task 2, the evaluation 
result shows our precision, recall and F-score are 
34.8%, 41% and 37.6%. The performance of 
identifying the scope relies on the cue-level 
detection. Therefore, the false positive and false 
negatives of hedge cues can lead to recognition 
errors. The result shows that our lexical-level 
method for the semantic problem is limited. For 
the time constraints, we do not probe deeply. 
104
5 Conclusions 
This paper presents an approach for extracting 
the hedge cues and their scopes in BioScope 
corpus using two CRF models for CoNLL-2010 
shared task. In the first task, the HCDic feature is 
proposed to improve the system performances, 
getting better performance (84.1% in F-score) 
than the baseline. The HCDic feature is also 
helpful to make use of cross-domain resources. 
The comparison of our methods based on 
between BioScope and Wikipedia corpus is 
given, which shows that ours are good at hedge 
cues detection in BioScope corpus but short at 
the in Wikipedia corpus. To detect the scope of 
hedge cues, we make rules to post process the 
text. For future work, we will look forward to 
constructing regulations for the HCDic to 
improve our system.  
References 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, 
J?nos Csirik, and Gy?rgy Szarvas. 2010. The 
CoNLL-2010 Shared Task: Learning to Detect 
Hedges and their Scope in Natural Language Text. 
In Proceedings of the Fourteenth Conference 
on Computational Natural Language Learning 
(CoNLL-2010): Shared Task, pages 1?12, 
Uppsala, Sweden, July. Association for 
Computational Linguistics. 
Halil Kilicoglu, and Sabine Bergler. 2008. 
Recognizing speculative language in biomedical 
research articles: a linguistically motivated 
perspective. BMC Bioinformatics, 9(Suppl 
11):S10. 
John Lafferty, Andrew K. McCallum, and Fernando 
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling 
sequence data. In ICML, pages 282?289. 
George Lakoff. 1972. Hedges: a study in meaning 
criteria and the logic of fuzzy concepts. Chicago 
Linguistics Society Papers, 8:183?228. 
Marc Light, Xin Y. Qiu, and Padmini Srinivasan. 
2004 The language of bioscience: 
facts,speculations, and statements in between. In 
BioLINK 2004: Linking Biological Literature, 
Ontologies and Databases, pages 17?24. 
Ben Medlock, and Ted Briscoe. 2007. Weakly 
supervised learning for hedge classification in 
scientific literature. In Proceedings of ACL 
2007, pages 992?999. 
Roser Morante, and Walter Daelemans. 2009. 
Learning the scope of hedge cues in biomedical 
texts. In Proceedings of the BioNLP 2009 
Workshop, pages 28-36, Boulder, Colorado, June 
2009. Association for Computational Linguistics. 
Roser Morante, and Walter Daelemans. 2009. A 
metalearning approach to processing the scope of 
negation. In Proceedings of CoNLL-2009. 
Boulder, Colorado. 
Gy?rgy Szarvas. 2008. Hedge classification in 
biomedical texts with a weakly supervised 
selection of keywords. In Proceedings of ACL 
2008, pages 281?289, Columbus, Ohio, USA. 
ACL. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
and Jun?ichi Tsujii. 2005. Developing a robust 
part-of-speech tagger for biomedical text. In: 
Advances in Informatics, PCI 2005, pages 382?
392. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The 
BioScope corpus: biomedical texts annotated for 
uncertainty, negation and their scopes. BMC 
Bioinformatics, 9(Suppl 11):S9. 
105
CRF tagging for head recognition based on Stanford parser 
 
Yong Cheng, Chengjie Sun, Bingquan Liu, Lei Lin 
Harbin Institute of Technology  
{ycheng, cjsun, linl,liubq}@insun.hit.edu.cn 
 
   
Abstract 
Chinese parsing has received more and 
more attention, and in this paper, we use 
toolkit to perform parsing on the data of 
Tsinghua Chinese Treebank (TCT) used in 
CIPS, and we use Conditional Random 
Fields (CRFs) to train specific model for the 
head recognition. At last, we compare 
different results on different POS results. 
1 Introduction 
    In the past decade, Chinese parsing has 
received more and more attention, it is the 
core of Chinese information processing 
technology, and it is also the cornerstone for 
deep understanding of Chinese.  
    Parsing is to identify automatically 
syntactic units in the sentence and give the 
relationship between these units. It is based 
on a given grammar. The results of parsing 
are usually structured syntax tree. For 
example, the parsing result of sentence "?
???????" is as following. 
                           (ROOT 
(dj (nS ??) 
(vp (v ?) 
(np 
                              (np (m ?) (n ??)) 
                           (n ??))))) 
With the development of Chinese 
economy, Chinese information processing 
has become a worldwide hot spot, and 
parsing is an essential task. However, 
parsing is a recognized research problem, 
and it is so difficult to meet the urgent needs 
of industrial applications in accuracy, 
robustness, speed. So the study of Chinese 
grammar and syntax analysis algorithm are 
still the focus of Chinese information 
processing.  
In all the parsing technology research, 
English parsing research is the most in-depth, 
and there are three main aspects of research 
in statistical parsing, they are  parsing model, 
parsing algorithm, and corpus construction. 
As for the parsing model, currently there are 
four commonly used parsing models, PCFG 
model [1], the model based on historical, 
Hierarchical model of progressive, head-
driven model [2]. 
 Since parsing is mostly a data driven 
process, its performance is determined by 
the amount of data in a Treebank on which a 
parser is trained. Much more data for 
English than for any other languages have 
been available so far. Thus most researches 
on parsing are concentrated on English. It is 
unrealistic to directly apply any existing 
parser trained on an English Treebank for 
Chinese sentences. But the methodology is, 
without doubt, highly applicable. Even for 
those corpora with special format and 
information integrated some modification 
and enhancement on a well-performed parser 
to fit the special structure for the data could 
help to obtain a good performance.  
    This paper presents our solution for the 
shared Task 2 of CIPS2010-Chinese Parsing. 
We exploit an existing powerful parser, 
Stanford parser, which has showed its 
effectiveness on English, with necessary 
modifications for parsing Chinese for the 
shared task. Since the corpus used in CIPS is 
from TCT, and the sentence contains the 
head-word information, but for the Stanford 
parser, it can't recognize the head 
constituents. So we apply a sequence tagging 
method to label head constituents based on 
the data extracted from the TCT corpus, In 
section 2 and section 3, we will present the  
Table 1. Training data with different formats 
 
details of our approach, and In section 4, we 
present the details of experiment. 
 
2 Parsing 
    Since English parsing has made many 
achievements, so we investigated some 
statistical parsing models designed for 
English. There are three open source 
constituent parsers, Stanford parser [3], 
Berkeley parser [4] and Bikel's parser [5]. 
Bikel's parser is an implementation of 
Collins' head-driven statistical model [6]. 
The Stanford parser is based on the factored 
model described in [7]. Berkeley parser is 
based on unlexicalized parsing model, as 
described in [8]. 
All the three parsers are claimed to be 
multilingual parsers but only accept training 
data in UPenn Treebank format. To adapt 
these parsers to Tsinghua Chinese Treebank 
(TCT) used in CIP, we firstly transform the 
TCT training data into UPenn format. Then, 
some slight modifications have been made to 
the three parsers. So that they could fulfill 
the needs in our task. 
In our work, we use Stanford parser to 
train our model by change the training data 
to three parts with different formats, one for 
training parsing model, one for training POS 
model, and the last for training head-
recognition model. Table 1 shows the three 
different forms. 
 
3 Head recognition 
    Head recognition is to find the head 
word in a clause, for example, 'np-1' express 
that in the clause, the word with index '1' is 
the key word. 
    To recognize the head constituents, and 
extra step is needed since Stanford parsing 
could not provide a straight forward way for 
this. Consider that head constituents are 
always determined by their syntactic symbol 
and their neighbors, whose order and 
relations strongly affects the head labeling. 
Like chunking [9], it is natural to apply a 
sequence labeling strategy to tackle this 
problem. We adopt the linear-chain CRF 
[10], one of the most successful sequence 
labeling framework so far, for the head 
recognition is this stage.  
    
4 Experiment 
4.1 Data 
    The training data is from Tsinghua 
Chinese Treebank (TCT), and our task is to 
perform full parsing on them. There are 
37218 lines in official released training data, 
As the Table 1 show; we change the data 
into three parts for different models. 
The testing data doesn?t contain POS 
labels, and there are 1000 lines in official 
released testing data. 
 
 
Parsing model 
1.(ROOT (np-0-2 (n ?
???) (cC ??) (np-
0-1 (n ? ? ) (n ?
?) ) ) ) 
2.(ROOT (vp-1 (pp-1 (p 
?) (np-0-2 (np-1 (n ?
?) (n ??) ) (cC ?
? ) (np-2 (a ? ? ) 
(uJDE ?) (np-1 (n ?
?) (np-1 (n ??) (n ?
?) ) ) ) ) ) (vp-1 (d ?
?) (vp-1 (d ??) (v ?
?) ) ) ) ) 
POS model 
1. ??/nS  ??/a  ?
?/n 
2.??/nS  ?/vC  ?/a  
??/n  ??/n  ?/wP  
??/nR  ??/n  ?/vC  
??/m  ?/m  ?/qN  
??/n  ?/uJDE  ??
/n  ?/wE   
Head-recognition 
model 
a O n np 0 
n a O np 1 
 
nS O np np 0 
np nS O np 1 
Table 2. Different POS tagging results 
 original new 
pos accuracy 80.40 94.82 
 
4.2 Models training 
4.2.1 Parsing model training 
    As for training parsing model with 
Stanford parser, since there are little 
parameters need to set, so we directly use the 
Stanford parser to train a model without any 
parameter setting. 
4.2.2 POS model training 
    In this session of the evaluation, POS 
tagging is no longer as a separate task, so we 
have to train our own POS tagging model. In 
the evaluation process, we didn't fully 
consider the POS tagging results' impact on 
the overall results, so we didn't train the POS 
model specially, we directly use the POS 
function in Stanford parser toolkit. This has 
led to relatively poor results in POS tagging, 
and it also affects the overall parsing result. 
After the evaluation, we train a specific 
model to improve the POS tagging results. 
As the table 1 shows, we extract training 
data from the original corpus and adopt the 
linear-chain CRF to train a POS tagging 
model. Table 2 shows the original POS 
tagging results and new results.  
4.2.3 Head recognition model training 
As the table 1 shows, we extract specific 
training data from original corpus.  
Table 3.  Training data formats for Head-
recognition 
original corpus 1.[vp-0 ??/v  [np-1 
??/n  ??/n  ] ]  
temp corpus 1.[np-1 ??/n  ??
/n  ] 
2.[vp-0 ??/v  [np-1 
??/n  ??/n  ] ] 
final corpus n O n np 0 
n n O np 1 
 
v O np vp 1 
np v O vp 0 
Table 4. Statistics the frequency of the words in 
each clause 
number of word statistics number 
< 1 160 
2 50834 
3 12592 
4 56 
5 664 
>5 360 
 
And for head-word recognition, since the 
adjacent clause has little effect on the 
recognition of head-word, so we set the 
clause as the smallest unit. We chose CRF to 
train our model. However, for getting the 
proper format of data for training in CRF, 
We have to do further processing on the data. 
As the table 3 shows, the final data set word 
as the unit. 
For example, the line 'n O np vp 1?, the 
meaning from beginning to end is POS or 
clause mark of current word or clause, POS 
or clause mark of previous word, POS or 
clause mark of latter word, the clause mark 
of current word, and the last mean that if 
current word or clause is headword 1 
represents YES, 0 represents NO. 
4.4 Result and Conclusion 
As we mention before, in evaluation, we 
didn't train specific POS tagging model, So 
we re-train our pos model, and the new 
results is shown in table 6, it can be seen that, 
with the increase of POS result, there is a 
corresponding increase in the overall results. 
Table 5. Performance of head recognition and 
the template for model training 
Boundary + 
Constituent 70.58 
 Boundary + 
Constituent + Head 66.97 
template 
U00:%x[0,0] 
U01:%x[-1,0] 
U02:%x[1,0] 
U04:%x[0,0]/%x[-1,0]
U05:%x[0,0]/%x[1,0]
U06:%x[-1,0]/%x[1,0]
 
 
Table 6. Overall results on different POS results 
 POS Boundary + 
Constituent 
original 80.40 67.00 
new 94.82 74.28 
 
Through our evaluation results, we can 
see that it is not appropriate to directly use 
English parser toolkit to process Chinese. 
And it is urgent to development parsing 
model based on the characteristics of 
Chinese. 
 
 
 
References 
[1] T. L. Booth and R. A. Thompson. Applying 
Probability Measures  to Abstract Languages. 
IEEE Transactions on Computers, 1973, C-
22(5):422-450. 
[2] M. Collins. Three Generative, Lexicalised 
Models for Statistical Parsing. In Proceedings 
of the 35th annual meeting of the association 
for computational linguistics. 
[3] http://nlp.stanford.edu/software/lex-parser.html 
[4] http://code.google.com/p/berkeleyparser 
[5] http://www.cis.upenn.edu/~dbikel/download 
[6] Michael Collins. 1999. Head-Driven Statistical Models for  
     Natural Language Parsing. Ph.D. thesis. 
University of Pennsylvania. 
[7] Dan Klein and Christopher D. Manning 
Accurate unlixicalized parsing. In Proceedings 
of the 41st Annual Meeting on Association for 
Computational Linguistics. 
[8] S Petrov and D Klein. Improved inference for 
unlexicalized parsing. In Proceedings of 
NAACL HLT 2007. 
[9] Fei Sha and Fernando Pereira. 2003. Shallow 
parsing with conditional random fields. In 
Proceedings of HLT-NAACL 2003, pages 
213-220, Edmonton. Canada. 
[10] John Lafferty. Andrew McCallum. And 
Fernando Pereira. 2001. Conditional random 
fields: Probabilistic models for segmenting and 
labeling sequence data. In Proceedings of 
ICML 2001, pages 282-289, Williams College, 
Williamstown, MA, USA. 
