Coling 2010: Poster Volume, pages 427?435,
Beijing, August 2010
Towards Automated Related Work Summarization
Cong Duy Vu Hoang and Min-Yen Kan
Department of Computer Science
School of Computing
National University of Singapore
{hcdvu,kanmy}@comp.nus.edu.sg
Abstract
We introduce the novel problem of auto-
matic related work summarization. Given
multiple articles (e.g., conference/journal
papers) as input, a related work sum-
marization system creates a topic-biased
summary of related work specific to the
target paper. Our prototype Related Work
Summarization system, ReWoS, takes in
set of keywords arranged in a hierarchical
fashion that describes a target paper?s top-
ics, to drive the creation of an extractive
summary using two different strategies for
locating appropriate sentences for general
topics as well as detailed ones. Our initial
results show an improvement over generic
multi-document summarization baselines
in a human evaluation.
1 Introduction
In many fields, a scholar needs to show an under-
standing of the context of his problem and relate
his work to prior community knowledge. A re-
lated work section is often the vehicle for this pur-
pose; it contextualizes the scholar?s contributions
and helps readers understand the critical aspects
of the previous works that current work addresses.
Creating such a summary requires the author to
position his own work within the contextual re-
search to showcase the advantages of his method.
We envision an NLP application that assists in
creating a related work summary. We propose this
related work summarization task as a challenge
to the automatic summarization community. In
its full form, it is a topic-biased, multi-document
summarization problem that takes as input a tar-
get scientific document for which a related work
section needs to be drafted. The output goal is to
create a related work section that finds the relevant
related works and contextually describes them in
relationship to the scientific document at hand.
We dissect the full challenge as bringing to-
gether work of disparate interests; 1) in finding
relevant documents; 2) in identifying the salient
aspects of these documents in relation to the cur-
rent work worth summarizing; and 3) in generat-
ing the final topic-biased summary. While it is
clear that current NLP technology does not let us
build a complete solution for this task, we believe
that tackling the individual components will help
bring us towards an eventual solution.
In fact, existing works in the NLP and rec-
ommendation systems communities have already
begun work that fits towards the completion of
the first two tasks. Citation prediction (Nallapati
et al, 2008) is a growing research area that has
aimed both at predicting citation growth over time
within a community and at individual paper cita-
tion patterns. This year, an automatic keyphrase
extraction task from scientific articles was first
fielded in SemEval-2, partially addressing Task
11. Also, automatic survey generation (Moham-
mad et al, 2009) is becoming a growing field
within the summarization community. However,
to date, we have not yet seen any work that exam-
ines topic-biased summarization of multiple sci-
entific articles. For these reasons, we focus on
Task 3 here ? the creation of a related work sec-
tion, given a structured input of the topics for sum-
mary.. The remaining contributions of our paper
1http://semeval2.fbk.eu/semeval2.php
427
consists of work towards this goal:
? We conduct a study of the argumentative pat-
terns used in related work sections, to describe the
plausible summarization tactics for their creation
in Section 3.
? We describe our approach to generate an extrac-
tive related work summary given an input topic hi-
erarchy tree, using two separate strategies to dif-
ferentiate between summarizing shallow internal
nodes from deep detailed leaf nodes of the topic
tree in Section 4.
2 Related Work
Fully automated related work summarization is
significantly different from traditional summa-
rization. While there are no existing studies on
this specific problem, there are closely related en-
deavors. The iOPENER2 project works towards
automated creation of technical surveys, given a
research topic (Mohammad et al, 2009). Stan-
dard generic multi-document summarization al-
gorithms were applied to generate technical sur-
veys. They showed that citation information was
effective in the generation process. This was also
validated earlier in (Nakov et al, 2004), which
showed that the citing sentences in other papers
can give a useful description of a target work.
Other studies focus mainly on single-document
scientific article summarization. The pioneers of
automated summarization (Luhn, 1958; Baxen-
dale, 1958; Edmundson, 1969) had envisioned
their approaches being used for the automatic cre-
ation of scientific summaries. They examined
various features specific to scientific texts (e.g.,
frequency-based, sentence position, or rhetorical
clues features) which were proven effective for
domain-specific summarization tasks.
Further, Mei and Zhai (2008) and Qazvinian
and Radev (2008) utilized citation information in
creating summaries for a single scientific article in
computational linguistics domain. Also, Schwartz
and Hearst (2006) also utilized the citation sen-
tences to summarize the key concepts and entities
in bioscience texts, and argued that citation sen-
tences may contain informative contributions of a
paper that complement its original abstract.
2http://clair.si.umich.edu/clair/iopener/
These works all center on the role of citations
and their contexts in creating a summary, using ci-
tation information to rank content for extraction.
However, they did not study the rhetorical struc-
ture of the intended summaries, targeting more on
deriving useful content. For working along this
vein, we turn to studies on the rhetorical structure
of scientific articles. Perhaps the most relevant is
work by (Teufel, 1999; Teufel and Moens, 2002)
who defined and studied argumentative zoning of
texts, especially ones in computational linguistics.
While they studied the structure of an entire arti-
cle, it is clear from their studies that a related work
section would contain general background knowl-
edge (BACKGROUND zone) as well as specific in-
formation credited to others (OTHER and BASIS
zones). This vein of work has been followed by
many, including Teufel et al (2009; Angrosh et
al. (2010).
3 Structure of Related Work Section
We first extend the work on rhetorical analysis,
concentrating on related work sections. By study-
ing examples in detail, we gain insight on how to
approach related work summarization. We focus
on a concrete related work example for illustra-
tion, an excerpt of which is shown in Figure 1a.
Focusing on the argumentative progression of the
text, we note the flow through different topics is
hierarchical and can be represented as a topic tree
as in Figure 1b.
This summary provides background knowledge
for a paper on text classification, which is the root
of the topic tree (node 1; lines 1?5). Two top-
ics (?feature selection? and ?machine learning?)
are then presented in parallel (nodes 2 & 3; lines
5?8 & 9?15), where specific details on relevant
works are selected to describe two topics. These
two topics are implicitly understood as subtopics
of a more general topic, namely ?mono-lingual
text classification? (node 4; lines 16?17). The au-
thors use the monolingual topic to contrast it with
the subsequent subtopic ?multi-lingual text classi-
fication? (node 5; lines 18?21). This topic is de-
scribed by elaborating its details through two sub-
topics: ?bilingual text classification? and ?cross-
lingual text classification? (nodes 6 & 7; lines 22?
25 & 25?39) where again, various example works
428
1
2
9
16
21
22
23
32
40
42
35
line line
(a)
1
54
6 732
contrast
parallel parallel
Text classification (lines 1-5)
Feature selection (lines 5-8)
Machine learning (lines 9-15)
Mono-lingual text classification (lines 16-17)
Multi-lingual text classification (lines 18-21)
Bilingual text classification (lines 22-25)
Cross-lingual text classification (lines 25-39)
1
2
3
4
5
6
7
(b)
1
2 3
4 5
6 7
monolingual;language
text;classification
multi-language;multi-lingual;language
features;selection learning;probabilistic bilingual cross-lingual
(c)
Figure 1: a) A related work section extracted from (Wu and Oard, 2008); b) An associated topic hierar-
chy tree of a); c) An associated topic tree, annotated with key words/phrases.
are described and cited. The authors then con-
clude by contrasting their proposed approach with
the introduced relevant approaches (lines 40?42).
This summary illustrates three important
points. First, the topic tree is an essential input
to the summarization process. The topic tree can
be thought of as a high-level rhetorical structure
for which a process then attaches content. While
it is certainly non-trivial to build such a tree, mod-
ifications to hierarchical topic modeling (M. et al,
2004) or keyphrase extraction algorithms (Witten
et al, 1999) we believe can be used to induce a
suitable form. A resulting topic hierarchy from
such a process would provide an associated set
of key words or phrases that would describe the
node, as shown in Figure 1c.
Second, while summaries can be structured in
many ways, they can be viewed as moves along
the topic hierarchy tree. In the example, nodes
2 and 3 are discussed before their parent, as the
parent node (node 4) serves as a useful contrast
to introduce its sibling (node 5). We find variants
of depth-first traversal common, but breadth-first
traversals of nodes with multiple descendants are
more rare. They may be structured this way to
ease the reader?s burden on memory and atten-
tion. This is in line with other summary genres
where information is ordered by high-level logical
considerations that place macro level constraints
(Barzilay et al, 2002).
Third, there is a clear distinction between sen-
tences that describe a general topic and those that
describe work in detail. Generic topics are often
represented by background information, which is
not tied to a particular prior work. These include
definitions or descriptions of a topic?s purpose.
In contrast, detailed information forms the bulk
of the summary and often describes key related
work that is attributable to specific authors. Re-
cently, Jaidka et al (2010) also present the begin-
nings of a corpus study of related work sections,
where they differentiate integrative and descrip-
tive strategies in presenting discourse work. We
see our differentiation between general and de-
tailed topics as a natural parallel to their notion
of integrative and descriptive strategies.
To introspect on these findings further, we cre-
ated a related work data set (called RWSData3),
which includes 20 articles from well-respected
venues in NLP and IR, namely SIGIR, ACL,
NAACL, EMNLP and COLING. We extracted
the related work sections directly from those re-
search articles as well as references the sections
cited. References to books and Ph.D. theses were
removed, as their verbosity would change the
problem drastically (Mihalcea and Ceylan, 2007).
Since we view each related work summary as a
topic-biased summary originating from a topic hi-
erarchy tree, annotation of such topical informa-
tion for our data set is necessary. Each article?s
data consists of the reference related work sum-
mary, the collection of the input research articles
3To be made available at http://wing.comp.nus.
edu.sg/downloads/rwsdata.
429
SbL?RW WbL?RW No?RAs SbL?RA WbL?RA TS TD
average 17.9 522.4 10.9 2386.0 51739.6 3.3 1.8
stdev 7.9 216.5 5.6 1306.7 26682.3 1.7 0.6
min 6 179 2 348 8580 1 1
max 40 922 26 5549 112267 7 3
Table 1: The demographics of RWSData. No, RW, RA, SbL, WbL, TS, and TD are labeled as
(N)umber (o)f, (R)elated (W)orks, (R)eferenced (A)rticles, (S)entence-(b)ased (L)ength of, (W)ord-
(b)ased (L)ength of, (T)ree (S)ize, and (T)ree (D)epth, respectively.
that were referenced and a manually-constructed
topic descriptions in a hierarchical fashion (topic
tree). More details on the demographics of RWS-
Data are shown in Table 1. RWSData summaries
average 17.9 sentences, 522 words in length, cit-
ing an average of 10.9 articles. While hierarchi-
cal, the topic trees are simple, averaging 3.3 topic
nodes in size and average depth of 1.8. Their sim-
plicity furthers our claim that automated methods
would be able to create such trees.
4 ReWoS: Paired General and Specific
Summarization
sentences
Pre-processor
Agent-based rule
Subject-based rule
OR
Verb-based rule Citation-based rule
Topic relevance computation
GCSum
Topic relevance computation
SCSum
Weighting
Context modeling
Ranking
Re-ranking
Post-processor General content sentences
Specific content sentences Generator
Related work 
summary
T F
R
R
T
T T
Specific Content Summarization General Content Summarization
Ranking
Figure 2: The ReWoS architecture. Decision
edges labeled as True, False and Relevant.
Inspired by the above observations, we propose
a novel strategy for related work summarization
with respect to a given topic tree. Note that while
the construction of the topic tree is central to the
process, we consider this outside the scope of the
current work (see ?1); our investigation focuses
on how such input could be utilized to construct a
reasonable topic-biased related work summary.
We posit that sentences within a related work
section come about by means of two separate pro-
cesses ? a process that gives general background
information and another that describes specific au-
thor contributions. A key realization in our work
is that these processes are easily mapped to the
topic tree topologically: general content is de-
scribed in tree-internal nodes, whereas leaf nodes
contribute detailed specifics. In our approach,
these two processes are independent, and com-
bined to construct the final summary.
We have implemented our idea in ReWoS
(Related Work Summarizer), whose general ar-
chitecture is shown in Figure 2. ReWoS is a
largely heuristic system, featuring both a General
Content Summarization (GCSum) and a Specific
Content Summarization (SCSum) modules, pre-
fixed by preprocessing. A natural language tem-
plate generation system fills out the end of the
summary.
ReWoS first applies a set of preprocessing steps
(shown in the top of Figure 2). Input sentences
(i.e., the set of sentences from each related/cited
article) first removes sentences that are too short
(< 7 tokens) or too long (> 80 tokens), ones that
use future tense (possibly future work), and exam-
ple and navigation sentences. This last category
is filtered out by checking for the presence of a
cue phrase among a lexical pattern database: e.g.,
?in the section?, ?figure x shows?, ?for instance?.
Lowercasing and stemming are also performed.
We then direct sentences to either GCSum or
SCSum based on whether it describes the author?s
own work or not, similar in spirit and execution to
(Teufel et al, 2009). If sentence contains indica-
tive pronouns or cue phrases (e.g., ?we?, ?this ap-
430
proach?), the sentence is deemed to describe own
work and is directed to SCSum; otherwise the sen-
tence is directed to the GCSum workflow.
4.1 (G)eneral (C)ontent (Sum)marization
GCSum extracts sentences containing useful
background information on the topics of the inter-
nal node in focus. Since general content sentences
do not specifically describe work done by the au-
thors, we only take sentences that do not have the
author-as-agent as input for GCSum.
We divide such general content sentences into
two groups: indicative and informative. Infor-
mative sentences give detail on a specific aspect
of the problem. They often give definitions, pur-
pose or application of the topic (?Text classifica-
tion is a task that assigns a certain number of pre-
defined labels for a given text.?). In contrast, in-
dicative sentences are simpler, inserted to make
the topic transition explicit and rhetorically sound
(?Many previous studies have studied monolin-
gual text classification.?).
Indicative sentences can be easily generated by
templates, as the primary information that is trans-
mitted is the identity of the topic itself. Informa-
tive sentences, on the other hand, are better ex-
tracted from the source articles themselves, re-
quiring a specific strategy. As informative sen-
tences contain more content, our strategy with
GCSum is to attempt to locate informative sen-
tences to describe the internal nodes, failing which
GCSum falls back to using predefined templates
to generate an indicative placeholder.
To implement GCSum?s informative extractor,
we use a set of heuristics in a decision cascade
to first filter inappropriate sentences (as shown on
the RHS of Figure 2). Remaining candidates (if
any) are then ranked by relevance and the top n
are selected for the summary.
The heuristic cascade?s purpose is to ensure
sentences fit the syntactic structure of commonly-
observed informative sentences. A useful sen-
tence should discuss the topic directly, so GCSum
first checks the subject of each candidate sentence,
filtering sentences whose subject do not contain at
least one topic keyword. We observed that back-
ground sentences often feature specific verbs or
citations. GCSum thus also checks whether stock
verb phrases (i.e., ?based on?, ?make use of? and
23 other patterns) are used as the main verb. Oth-
erwise, GCSum checks for the presence of at least
one citation ? general sentences may list a set of
citations as examples. If both the cue verb and
citation checks fail, the sentence is dropped.
GCSum?s topic relevance computation ranks
remaining sentences based on keyword content.
We state that the topic of an internal node is af-
fected by its surrounding nodes ? ancestor, de-
scendants and siblings. Based on this idea, the
score of a sentence is computed in a discrimina-
tive way using the following linear combination:
scoreS ? scoreQAS + score
Q
S ? score
QR
S (1)
where scoreS is the final relevance score, and
scoreQAS , score
Q
S , and score
QR
S are the compo-
nent scores of the sentence S with respect to the
ancestor, current or other remaining nodes. We
give positive credit to a sentence that contains key-
words from an ancestor node, but penalize sen-
tences with keywords from other topics (as such
sentences would be better descriptors for those
other topics). Component relevance scores are
calculated using Term Frequency ? Inverse Sen-
tence Frequency (TF?ISF) (Otterbacher et al,
2005):
scoreQS =
rel(S,Q)
?
Q? rel(S,Q?)
(2)
=
?
w?Q log(tf
S
w + 1)? log(tfQw + 1)? isfw
Norm
where rel(S,Q) is the relevance of S with respect
to topic Q, Norm is a normalization factor of
rel(S,Q) over all input sentences, tfSw and tfQw
are the term frequencies of token w within S or
sentences that discuss topic Q, respectively. isfw
is the inverse sentence frequency of w.
4.2 (S)pecific (C)ontent (Sum)marization
SCSum aims to extract sentences that contain de-
tailed information about a specific author?s work
that is relevant to the input leaf node?s topic from
the set of sentences that exhibit the author-as-
agent. SCSum starts by computing the topic rel-
evance of each candidate sentence as shown in
Equation (3). This process is identical to the step
in GCSum, except that the term scoreQRS in Equa-
tion (1) is replaced by scoreQSS , which is the rel-
evance of S with respect to its sibling nodes. We
431
hypothesize that given a leaf node, sibling node
topics may have an even more pronounced nega-
tive effect than other remaining nodes in the topic
tree.
scoreS ? scoreQAS + score
Q
S ? score
QS
S (3)
Context Modeling. We note that single sen-
tences occasionally do not contain enough con-
texts to clearly express the idea mentioned in orig-
inal articles. In fact, an agent-based sentence often
introduces a concept but pertinent details are of-
ten described later. Extracting just the agent-based
sentence may incompletely describe a concept and
lead to false inferences. Consider the example
in Figure 3. Here Sentences 0-5 are an contigu-
ous extract of a source article being summarized,
where Sentence 0 is an identified agent-based sen-
tence. Sentence 6 shows a related work section
sentence from a citing article that describes the
original article. It is clear that the citing descrip-
tion is composed of information taken not only
from the agent-based sentence but its context in
the following sentences as well. This observation
Figure 3: A context modeling example.
motivates us to choose nearby sentences within a
contextual window after the agent-based sentence
to represent the topic. We set the contextual win-
dow to 5 and extract a maximum of 2 additional
sentences. These additions are chosen based on
their relevance scores to the topic, using Equa-
tion (3). Sentences with non-zero scores are then
added as contexts of the anchor agent-based sen-
tence, otherwise they are excluded. As a result,
some topics may contain only a single sentence,
but others may be described by additional contex-
tual sentences.
Weighting. The score of a candidate content
sentence is computed from topic relevance com-
putation (SCSum) that includes contributions for
keywords present in the current, ancestor and sib-
ling nodes. We observe that the presence of one or
more of current, ancestor and sibling nodes may
affect the final score from the computation. Thus,
to partially address this, we add a new weighting
coefficient for the score computed from the topic
relevance computation (SCSum) (Equation (3)) as
follows:
score?S = wQA,Q,QSS ? scoreS (4)
where: wQA,Q,QSS is a weighting coefficient that
takes on differing values based on the presence
of keywords in the sentence. Q, QA, and QS de-
note keywords from current, ancestor and sibling
nodes. If the sentence contains keywords from
other sibling nodes, we assign a penalty of 0.1.
Otherwise, we assign a weight of 1.0, 0.5, or 0.25,
based on whether keywords are present from both
the ancestor node and current node, just the cur-
rent node or just the ancestor node.
To build the final summary, ReWoS selects the
top scoring sentence and iteratively adds the next
most highly ranked sentence, until the n sentence
budget is reached. We use SimRank (Li et al,
2008) to remove the next sentence to be added,
if it is too similar to the sentences already in the
summary.
4.3 Generation
ReWoS generates its summaries by using depth-
first traversal to order the topic nodes, as in RWS-
Data we observed this to be the most prevalent
discourse pattern. It calls GCSum and SCSum to
summarize individual nodes, distributing the total
sentence budget equally among nodes.
ReWoS post-processes sentences to improve
fluency where possible. We first replace agentive
forms with a citation to the articles (e.g., ?we? ?
?(Wu and Oard, 2008)?). ReWoS also replaces
found abbreviations with their corresponding long
forms, by connecting abbreviation with their ex-
pansions by utilizing dependency relation output
from the Stanford dependency parser.
432
System ROUGE Recall Scores Human Evaluation Scores
ROUGE-1 ROUGE-2 ROUGE-S4 ROUGE-SU4 Correctness Novelty Fluency Usefulness
LEAD 0.501 0.096 0.116 0.181 3.027 2.764 3.082 2.745
MEAD 0.663 0.178 0.211 0.287 3.009 3.109 2.591 2.700
ReWoS?WCM 0.584 0.127 0.154 0.227 3.618 3.391 3.391 3.609
ReWoS?CM 0.698 0.183 0.218 0.298 3.691 3.618 2.955 3.573
Table 2: Evaluation results for ReWoS variants and baselines.
5 Evaluation
We wish to assess the quality of ReWoS, compar-
ing to state-of-the-art generic summarization sys-
tems. We first detail our baseline systems used for
performance comparison, and defined evaluation
measures specific to related work summary eval-
uation. In our evaluation, we use our manually-
compiled RWSData data set.
We benchmark ReWoS against two baseline
systems: LEAD and MEAD. The LEAD baseline
represents each of the cited article with an equal
number of sentences. The first n sentences are
drawn from the article, meaning that the title and
abstract are usually extracted. The order of the ar-
ticle leads used in the resulting summary was de-
termined by the order of articles to be processed.
MEAD is a well-documented baseline extractive
multi-document summarizer, developed in (Radev
et al, 2004). MEAD offers a set of different fea-
tures that can be parameterized to create result-
ing summaries. We conducted an internal tun-
ing of MEAD to maximize its performance on
the RWSData. The optimal configuation uses just
two tuned features of centroid and cosine similar-
ity. Note that the MEAD baseline does use the
topic tree keywords in computing cosine similar-
ity score. Our ReWoS system is the only sys-
tem that leverages the topic tree structure which
is central to our approach. In our experiments, we
used MEAD toolkit4 to produce the summaries for
LEAD and MEAD baseline systems.
Automatic evaluation was performed with
ROUGE (Lin, 2004), a widely used and rec-
ognized automated summarization evaluation
method. We employed a number of ROUGE vari-
ants, which have been proven to correlate with hu-
man judgments in multi-document summarization
(Lin, 2004). However, given the small size of our
summarization dataset, we can only draw notional
4http://www.summarization.com/mead/
evidence from such an evaluation; it is not possi-
ble to find statistically significant conclusion from
our evaluation.
To partially address this, we also conducted
a human evaluation to assess more fine-grained
qualities of our system. We asked 11 human
judges to follow an evaluation guideline that we
prepared, to evaluate the summary quality, con-
sisting of the following evaluation measures:
? Correctness: Is the summary content actually
relevant to the hierarchical topics given?
? Novelty: Does the summary introduce novel in-
formation that is significant in comparison with
the human created summary?
? Fluency: Does the summary?s exposition flow
well, in terms of syntax as well as discourse?
? Usefulness: Is the summary useful in supporting
the researchers to quickly grasp the related works
given hierarchical topics?
Each judge was asked to grade the four sum-
maries according to the measures on a 5-point
scale of 1 (very poor) to 5 (very good). Sum-
maries 1 and 2 come from LEAD-based and
MEAD systems, respectively. Summaries 3 and
4 come from our proposed ReWoS systems, with-
out (ReWoS?WCM) and with (ReWoS?CM) the
context modeling in SCSum. All summarizers
were set to yield a summary with the same length
(1% of the original relevant articles, measured in
sentences). Due to limited time, only 10 out of 20
evaluation sets were assessed by the evaluators.
Each set was graded at least 3 times by 3 different
evaluators; evaluators did not know the identities
of the systems, which were randomized for each
set examined.
6 Results
ROUGE results are summarized in Table 2. Sur-
prisingly, the MEAD baseline system outperforms
both LEAD baseline and ReWoS?WCM (with-
out context modeling). Only ReWoS?CM (with
433
context modeling) is significantly better than oth-
ers, in terms of all ROUGE variants. Here are
some possible reasons to explain this. First,
ROUGE evaluation seems to work unreasonably
when dealing with verbose summaries, often pro-
duced by MEAD. Second, related work sum-
maries are multi-topic summaries of multi-article
references. This may cause miscalculation from
overlapping n-grams that occur across multiple
topics or references.
Since automatic evaluation with ROUGE does
not allow much introspection, we turn to our hu-
man evaluation. Results are also summarized in
Table 2. They show that both ReWoS?WCM
and ReWoS?CM perform significantly better than
baselines in terms of correctness, novelty, and use-
fulness. This is because our system utilized fea-
tures developed specifically for related work sum-
marization. Also, our proposed systems compare
favorably with LEAD, showing that necessary in-
formation is not only located in titles or abstracts,
but also in relevant portions of the research article
body.
ReWoS?CM (with context modeling) per-
formed equivalent to ReWoS?WCM (without it)
in terms of correctness and usefulness. For nov-
elty, ReWoS?CM is better than ReWoS?WCM.
It proved that the proposed component of con-
text moding is useful in providing new informa-
tion that is necessary for the related work sum-
maries. For fluency, only ReWoS?CM is bet-
ter than baseline systems. This is a negative re-
sult, but is not surprising because the summaries
from the ReWoS?CM which uses context model-
ing seems to be longer than others. It makes the
summaries quite hard to digest; some evaluators
told us that they preferred the shorter summaries.
A future extension is that using information fu-
sion techniques to fuse the contextual sentences
with its anchor agentive sentence.
A detailed error analysis of the results revealed
that there are three main types of errors produced
by our systems. The first issue is in calculat-
ing topic relevance. In the context of related
work summarization, our heuristics-based strate-
gies for sentence extraction cannot capture fully
this issue. Some sentences that have high relevant
scores to topics are not actually semantically rele-
vant to the topics. The second issue of anaphoric
expression is more addressable. Some extracted
sentences still contain anaphoric expression (e.g.,
?they?, ?these?, ?such?, . . . ), making final gen-
erated summaries incoherent. The third issue is
paraphrasing, where substituted paraphrases re-
place the original words and phrases in the source
articles.
7 Conclusion and Future Work
According to the best of our knowledge, auto-
mated related work summarization has not been
studied before. In this paper, we have taken the
initial steps towards solving this problem, by di-
viding the task into general and specific summa-
rization processes. Our initial results show an im-
provement over generic multi-document summa-
rization baselines in human evaluation. However,
our work shows that there is much room for addi-
tional improvement, for which we have outlined a
few challenges.
A shortcoming of our current work is that we
assume that a topic hierarchy tree is given as in-
put. We feel that this is an acceptable limitation
because we feel existing techniques will be able to
create such input, and that the topic trees used in
this study were quite simple. We plan to validate
this by generating these topic trees automatically
in our future work.
Exploring related work summarization comes
at a timely moment, as scholars now have access
to a preponderous amount of scholarly literature.
Automated assistance in interpreting and organiz-
ing scholarly work will help build future applica-
tions for integration with digital libraries and ref-
erence management tools.
References
Angrosh, M. A., Stephen Cranefield, and Nigel
Stanger. 2010. Context identification of sentences
in related work sections using a conditional random
field: towards intelligent digital libraries. In JCDL
?10: Proceedings of the 10th annual joint confer-
ence on Digital libraries, pages 293?302. ACM.
Barzilay, Regina, Noemie Elhadad, and Kathleen R.
McKeown. 2002. Inferring strategies for sen-
tence ordering in multidocument news summariza-
434
tion. In Journal of Artificial Intelligence Research,
volume 17, pages 35?55.
Baxendale, P. B. 1958. Machine-made index for tech-
nical literature - an experiment. IBM Journal of Re-
search Development, 2(4):354?361.
Edmundson, H. P. 1969. New methods in automatic
extracting. Journal of the ACM, 16(2):264?285.
Jaidka, Kokil, Christopher S. G. Khoo, and Jin-Cheon
Na. 2010. Imitating human literature review writ-
ing: An approach to multi-document summariza-
tion. In ICADL, pages 116?119.
Li, Wenjie, Furu Wei, Qin Lu, and Yanxiang He. 2008.
PNR2: Ranking sentences with positive and nega-
tive reinforcement for query-oriented update sum-
marization. In Proceedings of 22nd International
Conference on Computational Linguistics, pages
489?496, Manchester, UK, August.
Lin, Chin-Yew. 2004. Rouge: A package for au-
tomatic evaluation of summaries. In Proceed-
ings of the ACL-04 Workshop Text Summarization
Branches Out, pages 74?81, Spain, July.
Luhn, H. P. 1958. The automatic creation of literature
abstracts. IBM Journal of Research Development,
2(2):159?165.
M., Blei D., Griffiths T. L., Jordan M. I., and Tenen-
baum J. B. 2004. Hierarchical topic models and the
nested chinese restaurant process. In Advances in
Neural Information Processing Systems (NIPS).
Mei, Qiaozhu and ChengXiang Zhai. 2008. Gen-
erating impact-based summaries for scientific lit-
erature. In Proceedings of the 46th Annual Meet-
ing of the Association of Computational Linguistics
(ACL), pages 816?824, Columbus, Ohio, June.
Mihalcea, Rada and Hakan Ceylan. 2007. Explo-
rations in automatic book summarization. In Pro-
ceedings of Empirical Methods in Natural Lan-
guage Processing - Conference on Natural Lan-
guage Learning (EMNLP-CoNLL), pages 380?389,
Prague, Czech Republic, June.
Mohammad, S., B. Dorr, M. Egan, A. Hassan,
P. Muthukrishan, V. Qazvinian, D. Radev, and
D. Zajic. 2009. Using citations to generate surveys
of scientific paradigms. In Proceedings of Human
Language Technologies - North American Associa-
tion for Computational Linguistics (HLT-NAACL),
pages 584?592, Boulder, Colorado, June.
Nakov, Preslav I., Ariel S. Schwartz, and Marti A.
Hearst. 2004. Citances: Citation sentences for se-
mantic analysis of bioscience text. In Workshop on
Search and Discovery in Bioinformatics.
Nallapati, R. M., A. Ahmed, E. P. Xing, and W. W.
Cohen. 2008. Joint latent topic models for text and
citations. In Proceeding of the 14th ACM SIGKDD
International Conference on Knowledge Discovery
in Data and Data Mining, pages 542?550.
Otterbacher, Jahna, Gu?nes? Erkan, and Dragomir R.
Radev. 2005. Using random walks for question-
focused sentence retrieval. In Proceedings of Hu-
man Language Technologies - Empirical Methods in
Natural Language Processing (HLT-EMNLP ?05),
pages 915?922. ACL.
Qazvinian, Vahed and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In Proceedings of International Con-
ference on Computational Linguistics (COLING),
pages 689?696, Manchester, UK, August.
Radev, Dragomir R., Hongyan Jing, Malgorzata Sty,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Information Processing
& Management (IPM), 40(6):919?938.
Schwartz, Ariel S. and Marti Hearst. 2006. Summa-
rizing key concepts using citation sentences. In Pro-
ceedings of Natural language processing of biology
text (BioNLP ?06), pages 134?135. ACL.
Teufel, Simone and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
Teufel, Simone, Advaith Siddharthan, and Colin
Batchelor. 2009. Towards domain-independent ar-
gumentative zoning: Evidence from chemistry and
computational linguistics. In Proceedings of the
2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1493?1502, Singa-
pore, August. Association for Computational Lin-
guistics.
Teufel, Simone. 1999. Argumentative Zoning: Infor-
mation Extraction from Scientific Text. Ph.D. thesis,
University of Edinburgh.
Witten, Ian H., Gordon Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. Kea:
Practical automatic keyphrase extraction. In Pro-
ceedings of Digital Libraries 99 (DL?99), pages
254?255. ACM Press.
Wu, Yejun and Douglas W. Oard. 2008. Bilingual
topic aspect classification with a few training exam-
ples. In SIGIR ?08: Proceedings of the 31st an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 203?210, New York, NY, USA. ACM.
435
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 73?78,
Baltimore, Maryland USA, June 23-24, 2014.
c
?2014 Association for Computational Linguistics
A Rule-Augmented Statistical Phrase-based Translation System
Cong Duy Vu Hoang
?
, AiTi Aw
?
and Nhung T. H. Nguyen
??
?
Human Language Technology Dept.
Institute for Infocomm Research (I
2
R), A*STAR, Singapore
{cdvhoang, aaiti}@i2r.a-star.edu.sg
?
School of Information Science
Japan Advanced Institute of Science and Technology (JAIST), Japan
nthnhung@jaist.ac.jp
Abstract
Interactive or Incremental Statistical Ma-
chine Translation (IMT) aims to provide a
mechanism that allows the statistical mod-
els involved in the translation process to be
incrementally updated and improved. The
source of knowledge normally comes from
users who either post-edit the entire trans-
lation or just provide the translations for
wrongly translated domain-specific termi-
nologies. Most of the existing work on
IMT uses batch learning paradigm which
does not allow translation systems to make
use of the new input instantaneously. We
introduce an adaptive MT framework with
a Rule Definition Language (RDL) for
users to amend MT results through trans-
lation rules or patterns. Experimental re-
sults show that our system acknowledges
user feedback via RDL which improves
the translations of the baseline system on
three test sets for Vietnamese to English
translation.
1 Introduction
In current Statistical Machine Translation (SMT)
framework, users are often seen as passive con-
tributors to MT performance. Even if there is a
collaboration between the users and the system, it
is carried out in a batch learning paradigm (Ortiz-
Martinez et al., 2010), where the training of the
SMT system and the collaborative process are car-
ried out in different stages. To increase the produc-
tivity of the whole translation process, one has to
incorporate human correction activities within the
translation process. Barrachina et al. (2009) pro-
posed an iterative process in which the translator
activity is used by the system to compute its best
?
Work done during an internship at I
2
R, A*STAR.
(or n-best) translation suffix hypotheses to com-
plete the prefix. Ortiz-Martinez et al. (2011) pro-
posed an IMT framework that includes stochas-
tic error-correction models in its statistical formal-
ization to address the prefix coverage problems
in Barrachina et al. (2009). Gonzalez-Rubio et
al. (2013) proposed a similar approach with a spe-
cific error-correction model based on a statistical
interpretation of the Levenshtein distance (Leven-
shtein, 1966). On the other hand, Ortiz-Martinez
et al. (2010) presented an IMT system that is able
to learn from user feedback by incrementally up-
dating the statistical models used by the system.
The key aspect of this proposed system is the use
of HMM-based alignment models trained by an in-
cremental EM algorithm.
Here, we present a system similar to Ortiz-
Martinez et al. (2010). Instead of updating the
translation model given a new sentence pair, we
provide a framework for users to describe trans-
lation rules using a Rule Definition Language
(RDL). Our RDL borrows the concept of the rule-
based method that allows users to control the
translation output by writing rules using their lin-
guistic and domain knowledge. Although statis-
tical methods pre-dominate the machine transla-
tion research currently, rule-based methods are
still promising in improving the translation qual-
ity. This approach is especially useful for low
resource languages where large training corpus
is not always available. The advantage of rule-
based methods is that they can well handle par-
ticular linguistic phenomena which are peculiar to
languages and domains. For example, the TCH
MT system at IWSLT 2008 (Wang et al., 2008)
used dictionary and hand-crafted rules (e.g. regu-
lar expression) to process NEs. Their experiments
showed that handling NE separately (e.g., person
name, location name, date, time, digit) results in
translation quality improvement.
In this paper, we present an adaptive and in-
73
Figure 1: The proposed rule-augmented SMT
framework.
teractive MT system that allows users to correct
the translation and integrate the adaptation into
the next translation cycle. Our experiments show
that the system is specifically effective in han-
dling translation errors related to out of vocabulary
words (OOVs), language expressions, name enti-
ties (NEs), abbreviations, terminologies, idioms,
etc. which cannot be easily addressed in the ab-
sence of in-domain parallel data.
2 System Overview
Figure 1 shows the translation and interactive pro-
cess of our system. The system is trained with a
batch of parallel texts to create a baseline model.
Users improve the translation by adding RDL
rules to change or correct the unsatisfactory trans-
lation. New RDL rules are tested in a working
environment before uploading to the production
environment where they would be used by subse-
quent translation requests.
In our system, RDL Management checks, vali-
dates and indexes the translation rules. The Rule-
Augmented Decoder has two components: (1) the
RDL Matcher to find applicable RDL rules for a
given source text to create dynamic translation hy-
potheses; and (2) the Augmented Decoder to pro-
duce the final consensus translation using both dy-
namic hypotheses and static hypotheses from the
baseline model.
3 Rule Definition Language (RDL)
The Rule Definition Language (RDL) comprises a
RDL grammar, a RDL parser and a RDL matching
algorithm.
3.1 RDL Grammar
Our RDL grammar is represented with a Backus-
Naur Form (BNF)s syntax. The major feature of
Node Type Description
Token Any string of characters in the defined
basic processing unit of the language.
String A constant string of characters.
Identifier A term represents a pre-defined role
(e.g. integer, date, sequence, . . . ).
Meta-node A term executes a specific function
(e.g. casing, selection/option, con-
nection).
Context cue A term describes source context?s ex-
istence.
Function A term executes a pre-defined task.
Table 1: A brief description of RDL nodes.
Figure 2: An Example of RDL Rule.
RDL grammar is the support of pre-defined identi-
fiers and meta-operators which go beyond the nor-
mal framework of regular expression. We also
included a set of pre-defined functions to further
constraint the application and realization of the
rules. This framework allows us to incorporate
semantic information into the rule definition and
derive translation hypotheses using both semantic
and lexical information. A RDL rule is identified
by a unique rule ID and five constituents, includ-
ing Source pattern, rule Condition, Target transla-
tion, Reordering rule and user ConFidence. The
source pattern and target translation can be con-
structed using different combination of node types
as described in Table 1. The rules can be further
conditioned by using some pre-defined functions
and the system allows users to reorder the transla-
tion of the target node. Figure 2 gives an example
of a RDL rule where identifier @Num is used.
3.2 RDL Parsing and Indexing
The RDL Parser checks the syntax of the rules
before indexing and storing them into the rule
database. We utilize the compiler generator (WoB
et al., 2003) to generate a RDL template parser and
then embed all semantic parsing components into
the template to form our RDL Parser.
As rule matching is performed during transla-
tion, searching of the relevant rules have to be very
fast and efficient. We employed the modified ver-
sion of an inverted index scheme (Zobel and Mof-
fat, 2006) for our rule indexing. The algorithm is
74
Figure 3: A linked item chain for a rule source
(@a @b [c] [?d e?] [?f g h?] (?i? | ?j k?)).
represented in Algorithm 1.
Data: ruleID & srcPatn
Result: idxTbl
// To build data structure ? Forward Step
doForward(srcPatn, linkedItmChain);
// To create index table ? Backward Step
doBackward(linkedItmChain, ruleID, idxTbl);
Algorithm 1: Algorithm for RDL rule indexing.
The main idea of the rule indexing algorithm is
to index all string-based nodes in the source pat-
tern of the RDL rule. Each node is represented
using 3-tuple. They are ruleID, number of nodes
in source pattern and all plausible positions of the
node during rule matching. The indexing is car-
ried out via a Forward Step and Backward Step.
The Forward Step builds a linked item chain which
traverses all possible position transitions from one
node to another as illustrated in Figure 3. Note that
S and E are the Start and End Node. The link indi-
cates the order of transition from a node to another.
The numbers refer to the possible positions of an
item in source. The Backward Step starts at the
end of the source pattern; traverses back the link
to index each node using the 3-tuple constructed
in the Forward Step. This data structure allows us
to retrieve, add or update RDL rules efficiently and
incrementally without re-indexing.
3.3 RDL Matching Algorithm
Each word in the source string will be matched
against the index table to retrieve relevant RDL
rules during decoding. The aim is to retrieve all
RDL rules in which the word is used as part of
the context in the source pattern. We sort all the
rules based on the word positions recorded dur-
ing indexing, match their source patterns against
the input string within the given span, check the
conditions and generate the hypotheses if the rules
fulfill all the constraints.
4 Rule-Augmented Decoder
The rule-augmented decoder integrates the dy-
namic hypotheses generated during rule match-
ing with the baseline hypotheses during decoding.
Given a sentence f from a source language F, the
fundamental equation of SMT (Brown et al., 1993)
to translate it into a target sentence e of a target
language E is stated in Equation 1.
e
best
= argmax
e
P
r
(e|f)
= argmax
e
P
r
(f |e)P
r
(e)
= argmax
e
N
?
n=1
?
n
h
n
(e, f)
(1)
Here, P
r
(f |e) is approximated by a translation
model that represents the correlation between the
source and the target sentence and P
r
(e) is ap-
proximated by a language model presenting the
well-formedness of the candidate translation e.
Most of the SMT systems follow a log-linear ap-
proach (Och and Ney, 2002), where direct mod-
elling of the posterior probabilityP
r
(f |e) of Equa-
tion 1 is used. The decoder searches for the best
translation given a set of model h
m
(e, f) by max-
imizing the log-linear feature score (Och and Ney,
2004) as in Equation 1.
For each hypothesis generated by the RDL rule,
an appropriate feature vector score is needed to en-
sure that it will not disturb the probability distribu-
tion of each model and contributes to hypothesis
selection process of SMT decoder.
4.1 Model Score Estimation
The aim of the RDL implementation is to address
the translation of language-specific expressions
(such as date-time, number, title, etc.) and do-
main-specific terminologies. Sometimes, transla-
tion rules and bilingual phrases can be easily ob-
served and obtained from experienced translators
or linguists. However, it is difficult to estimate the
probability of the RDL rules manually to reflect
the correct word or phrase distribution in real data.
Many approaches have been proposed to solve the
OOV problem and estimate word translation prob-
abilities without using parallel data. Koehn et
al. (2000) estimated word translation probabilities
from unrelated monolingual corpora using the EM
algorithm. Habash et al. (2008) presented differ-
ent techniques to extend the phrase table for on-
line handling of OOV. In their approach, the ex-
tended phrases are added to the baseline phrase
75
table with a default weight. Arora et al. (2008)
extended the phrase table by adding new phrase
translations for all source language words that do
not have a single-word entry in the original phrase-
table, but appear in the context of larger phrases.
They adjusted the probabilities of each entry in the
extended phase table.
We performed different experiments to estimate
the lexical translation feature vector for each dy-
namic hypothesis generated by our RDL rules. We
obtain the best performance by estimating the fea-
ture vector score using the baseline phrase table
through context approximation. For each hypoth-
esis generated by the RDL rule, we retrieve en-
tries from the phrase table which have at least one
similar word with the source of the generated hy-
pothesis. We sort the entries based on the sim-
ilarities between the generated and retrieved hy-
potheses using both source and target phrase. The
medium score of the sorted list is assigned to the
generated hypothesis.
5 System Features
The main features of our system are (1) the flexi-
bilities provided to the user to create different lev-
els of translation rules, from simple one-to-one
bilingual phrases to complex generalization rules
for capturing the translation of specific linguis-
tic phenomena; and (2) the ability to validate and
manage translation rules online and incrementally.
5.1 RDL Rule Management
Our system framework is language independent
and has been implemented on a Vietnamese to En-
glish translation project. Figure 4 shows the RDL
Management Screen where a user can add, mod-
ify or delete a translation rule using RDL. A RDL
rule can be created using nodes. Each node can
be defined using string or system predefined meta-
identifiers with or without meta-operators as de-
scribed in Table 1. Based on the node type selected
by the user, the system further restricts the user to
appropriate conditions and translation functions.
The user can define the order of the translation out-
put of each node and at the same time, inform the
system whether to use a specific RDL exclusively
during decoding, in which any phrases from the
baseline phrase table overlapping with that span
will be ignored
1
. The system also provides an edi-
1
Similar to Moses XML markup exclusive feature
http://www.statmt.org/moses/?n=Moses.
Figure 4: RDL Management screen with identi-
fiers & meta-functions supported.
tor for expert users to code the rules using the RDL
controlled language. Each rule is validated by the
RDL parser (discussed in section 3.2), which will
display errors or warning messages when an in-
valid syntax is encountered.
5.2 RDL Rule Validation
Our decoder manages two types of phrase table.
One is the static phrase-table obtained through
the SMT training in parallel texts; the other is
the dynamic table that comprises of the hypothe-
ses generated on-the-fly during RDL rule match-
ing. To ensure only fully tested rules are used in
the production environment, the system supports
two types of dynamic phrase table. The work-
ing phrase-table holds the latest updates made by
the users. The users can test the translation with
these latest modifications using a specific transla-
tion protocol. When users are satisfied with these
modifications, they can perform an operation to
upload the RDL rules to the production phrase-
table, where the RDLs are used for all translation
AdvancedFeatures#ntoc9
76
Named Entity Category Number of Rules
Date-time 120
Measurement 92
Title 13
Designation 12
Number 19
Terminology 178
Location 13
Organization 48
Total 495
Table 2: Statistics of created RDL rules for
Vietnamese-to-English NE Translation.
requests. Uploaded rules can be deleted, modified
and tested again in the working environment be-
fore updated to the production environment. Fig-
ure 5b and Figure 5c show the differences in trans-
lation output before and after applied the RDL rule
in Figure 5a.
6 A Case Study for Vietnamese?English
Translation
We performed an experiment using the proposed
RDL framework for a Vietnamese to English
translation system. As named entity (NE) con-
tributes to most of the OOV occurrences and im-
pacts the system performance for out-of-domain
test data in our system, we studied the NE usage
in a large Vietnamese monolingual corpus com-
prising 50M words to extract RDL rules. We cre-
ated RDL rules for 8 popular NE types including
title, designation, date-time, measurement, loca-
tion, organization, number and terminology. We
made use of a list of anchor words for each NE
category and compiled our RDL rules based on
these anchor words. As a result, we compiled a
total of 495 rules for 8 categories and it took about
3 months for the rule creation. Table 2 shows the
coverage of our compiled rules.
6.1 Experiment & Results
Our experiments were performed on a training set
of about 875K parallel sentences extracted from
web news and revised by native linguists over 2
years. The corpus has 401K and 225K unique En-
glish and Vietnamese tokens. We developed 1008
and 2548 parallel sentences, each with 4 refer-
ences, for development and testing, respectively.
All the reference sentences are created and revised
by different native linguists at different times. We
also trained a very large English language model
using data from Gigaword, Europarl and English
Figure 5: Translation Demo with RDL rules.
Data Set nS nT nMR
TrainFull (VN) 875,579 28,251,775 627,125
TrainFull (EN) 875,579 20,191,526 -
Test1 (VN) 1009 34,717 737
Test1 (4 refs) (EN) 1009 ?25,713 -
Test2 (VN) 1033 29,546 603
Test2 (4 refs) (EN) 1033 ?22,717 -
Test3 (VN) 506 16,817 344
Test3 (4 refs) (EN) 506 ?12,601 -
Dev (VN) 1008 34,803 -
Dev (4 refs) (EN) 1008 ?25,631 -
Table 3: Statistics of Vietnamese-to-English paral-
lel data. nS, nT, and nMR are number of sentence
pairs and tokens, and count of matched rules, re-
spectively.
web texts of Vietnamese authors to validate the
impact of RDL rules on large-scale and domain-
rich corpus. The experimental results show that
created RDL rules improve the translation perfor-
mance on all 3 test sets. Table 3 and Table 4 show
respective data statistics and results of our evalua-
tion. More specifically, the BLEU scores increase
3%, 3.6% and 1.4% on the three sets, respectively.
7 Conclusion
We have presented a system that provides a con-
trol language (Kuhn, 2013) specialized for MT for
users to create translation rules. Our RDL differs
from Moses?s XML mark-up in that it offers fea-
77
Data Set System BLEU NIST METEOR
Set 1 Baseline 39.21 9.2323 37.81
+RDL (all) 39.51 9.2658 37.98
Set 2 Baseline 40.25 9.5174 38.24
+RDL (all) 40.61 9.6092 38.84
Set 3 Baseline 36.77 8.6953 37.65
+RDL (all) 36.91 8.7062 37.69
Table 4: Experimental results with RDL rules.
tures that go beyond the popular regular expres-
sion framework. Without restricting the mark-up
on the source text, we allow multiple translations
to be specified for the same span or overlapping
span.
Our experimental results show that RDL
rules improve the overall performance of the
Vietnamese-to-English translation system. The
framework will be tested for other language pairs
(e.g. Chinese-to-English, Malay-to-English) in
the near future. We also plan to explore advanced
methods to identify and score ?good? dynamic
hypotheses on-the-fly and integrate them into cur-
rent SMT translation system (Simard and Foster,
2013).
Acknowledgments
We would like to thank the reviewers of the paper
for their helpful comments.
References
Paul M. Sumita E. Arora, K. 2008. Translation
of unknown words in phrase-based statistical ma-
chine translation for languages of rich morphol-
ogy. In In Proceedings of the Workshop on Spoken
Language Technologies for Under-Resourced Lan-
guages, SLTU 2008.
Sergio Barrachina, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi,
Antonio Lagarda, Hermann Ney, Jes?us Tom?as, En-
rique Vidal, and Juan-Miguel Vilar. 2009. Sta-
tistical approaches to computer-assisted translation.
Comput. Linguist., 35(1):3?28, March.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263?
311, June.
Jes?us Gonz?alez-Rubio, Daniel Ort??z-Martinez, Jos?e-
Miguel Bened??, and Francisco Casacuberta. 2013.
Interactive machine translation using hierarchical
translation models. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 244?254, Seattle, Washington,
USA, October.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In Proceedings of
ACL: Short Papers, HLT-Short ?08, pages 57?60,
Stroudsburg, PA, USA.
Philipp Koehn and Kevin Knight. 2000. Estimating
word translation probabilities from unrelated mono-
lingual corpora using the em algorithm. In Proceed-
ings of the Seventeenth National Conference on Ar-
tificial Intelligence and Twelfth Conference on Inno-
vative Applications of Artificial Intelligence, pages
711?715. AAAI Press.
Tobias Kuhn. 2013. A survey and classification of con-
trolled natural languages. Computational Linguis-
tics.
VI Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet
Physics Doklady, 10:707.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In In Proceedings of
ACL, pages 295?302, Stroudsburg, PA, USA.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417?449, Decem-
ber.
Daniel Ortiz-Martinez, Ismael Garcia-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for inter-
active statistical machine translation. In In Proceed-
ings of NAACL, HLT ?10, pages 546?554, Strouds-
burg, PA, USA.
Daniel Ortiz-Mart??nez, Luis A. Leiva, Vicent Alabau,
Ismael Garc??a-Varea, and Francisco Casacuberta.
2011. An interactive machine translation system
with online learning. In In Proceedings of ACL:
Systems Demonstrations, HLT ?11, pages 68?73,
Stroudsburg, PA, USA.
Michel Simard and George Foster. 2013. Pepr: Post-
edit propagation using phrase-based statistical ma-
chine translation. Proceedings of the XIV Machine
Translation Summit, pages 191?198.
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi Liu,
Jianfeng Li, Dengjun Ren, and Zhengyu Niu. 2008.
The tch machine translation system for iwslt 2008.
In In Proceedings of IWSLT 2008, Hawaii, USA.
Albrecht WoB, Markus Loberbauer, and Hanspeter
Mossenbock. 2003. Ll(1) conflict resolution in a
recursive descent compiler generator. In Modular
Programming Languages, volume 2789 of Lecture
Notes in Computer Science, pages 192?201.
Justin Zobel and Alistair Moffat. 2006. Inverted files
for text search engines. ACM Comput. Surv., 38,
July.
78
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 36?44,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
An Unsupervised and Data-Driven Approach for Spell Checking in
Vietnamese OCR-scanned Texts
Cong Duy Vu HOANG & Ai Ti AW
Department of Human Language Technology (HLT)
Institute for Infocomm Research (I2R)
A*STAR, Singapore
{cdvhoang,aaiti}@i2r.a-star.edu.sg
Abstract
OCR (Optical Character Recognition) scan-
ners do not always produce 100% accuracy
in recognizing text documents, leading to
spelling errors that make the texts hard to
process further. This paper presents an in-
vestigation for the task of spell checking
for OCR-scanned text documents. First, we
conduct a detailed analysis on characteris-
tics of spelling errors given by an OCR
scanner. Then, we propose a fully auto-
matic approach combining both error detec-
tion and correction phases within a unique
scheme. The scheme is designed in an un-
supervised & data-driven manner, suitable
for resource-poor languages. Based on the
evaluation on real dataset in Vietnamese
language, our approach gives an acceptable
performance (detection accuracy 86%, cor-
rection accuracy 71%). In addition, we also
give a result analysis to show how accurate
our approach can achieve.
1 Introduction and Related Work
Documents that are only available in print re-
quire scanning from OCR devices for retrieval
or e-archiving purposes (Tseng, 2002; Magdy
and Darwish, 2008). However, OCR scanners
do not always produce 100% accuracy in rec-
ognizing text documents, leading to spelling er-
rors that make the texts texts hard to process fur-
ther. Some factors may cause those errors. For
instance, shape or visual similarity forces OCR
scanners to misunderstand some characters; or in-
put text documents do not have good quality, caus-
ing noises in resulting scanned texts. The task of
spell checking for OCR-scanned text documents
proposed aims to solve the above situation.
Researchers in the literature used to approach
this task for various languages such as: English
(Tong and Evans, 1996; Taghva and Stofsky,
2001; Kolak and Resnik, 2002), Chinese (Zhuang
et al, 2004), Japanese (Nagata, 1996; Nagata,
1998), Arabic (Magdy and Darwish, 2006), and
Thai (Meknavin et al, 1998).
The most common approach is to involve users
for their intervention with computer support.
Taghva and Stofsky (2001) designed an interac-
tive system (called OCRSpell) that assists users as
many interactive features as possible during their
correction, such as: choose word boundary, mem-
orize user-corrected words for future correction,
provide specific prior knowledge about typical er-
rors. For certain applications requiring automa-
tion, the interactive scheme may not work.
Unlike (Taghva and Stofsky, 2001), non-
interactive (or fully automatic) approaches have
been investigated. Such approaches need pre-
specified lexicons & confusion resources (Tong
and Evans, 1996), language-specific knowledge
(Meknavin et al, 1998) or manually-created pho-
netic transformation rules (Hodge and Austin,
2003) to assist correction process.
Other approaches used supervised mecha-
nisms for OCR error correction, such as: statis-
tical language models (Nagata, 1996; Zhuang et
al., 2004; Magdy and Darwish, 2006), noisy chan-
nel model (Kolak and Resnik, 2002). These ap-
proaches performed well but are limited due to
requiring large annotated training data specific to
OCR spell checking in languages that are very
hard to obtain.
Further, research in spell checking for
Vietnamese language has been understudied.
36
Hunspell?spellcheck?vn1 & Aspell2 are inter-
active spell checking tools that work based on
pre-defined dictionaries.
According to our best knowledge, there is no
work in the literature reported the task of spell
checking for Vietnamese OCR-scanned text doc-
uments. In this paper, we approach this task in
terms of 1) fully automatic scheme; 2) without us-
ing any annotated corpora; 3) capable of solving
both non-word & real-word spelling errors simul-
taneously. Such an approach will be beneficial for
a poor-resource language like Vietnamese.
2 Error Characteristics
First of all, we would like to observe and analyse
the characteristics of OCR-induced errors in com-
pared with typographical errors in a real dataset.
2.1 Data Overview
We used a total of 24 samples of Vietnamese
OCR-scanned text documents for our analysis.
Each sample contains real & OCR texts, referring
to texts without & with spelling errors, respec-
tively. Our manual sentence segmentation gives
a result of totally 283 sentences for the above 24
samples, with 103 (good, no errors) and 180 (bad,
errors existed) sentences. Also, the number of syl-
lables3 in real &OCR sentences (over all samples)
are 2392 & 2551, respectively.
2.2 Error Classification
We carried out an in-depth analysis on spelling
errors, identified existing errors, and then man-
ually classified them into three pre-defined error
classes. For each class, we also figured out how
an error is formed.
As a result, we classified OCR-induced spelling
errors into three classes:
Typographic or Non-syllable Errors (Class 1):
refer to incorrect syllables (not included
in a standard dictionary). Normally, at
least one character of a syllable is expected
misspelled.
1http://code.google.com/p/
hunspell-spellcheck-vi/
2http://en.wikipedia.org/wiki/GNU_
Aspell/
3In Vietnamese language, we will use the word ?sylla-
ble? instead of ?token? to mention a unit that is separated by
spaces.
Real-syllable or Context-based Errors (Class 2):
refer to syllables that are correct in terms of
their existence in a standard dictionary but
incorrect in terms of their meaning in the
context of given sentence.
Unexpected Errors (Class 3): are accidentally
formed by unknown operators, such as:
insert non-alphabet characters, do incorrect
upper-/lower- case, split/merge/remove
syllable(s), change syllable orders, . . .
Note that errors in Class 1 & 2 can be formed
by applying one of 4 operators4 (Insertion, Dele-
tion, Substitution, Transposition). Class 3 is ex-
clusive, formed by unexpected operators. Table 1
gives some examples of 3 error classes.
An important note is that an erroneous syllable
can contain errors across different classes. Class
3 can appear with Class 1 or Class 2 but Class 1
never appears with Class 2. For example:
? ho?n (correct) || H?an (incorrect) (Class 3 & 1)
? b?t (correct) || b?t? (incorrect) (Class 3 & 2)
Figure 1: Distribution of operators used in Class
1 (left) & Class 2 (right).
2.3 Error Distribution
Our analysis reveals that there are totally 551 rec-
ognized errors over all 283 sentences. Each error
is classified into three wide classes (Class 1, Class
2, Class 3). Specifically, we also tried to identify
operators used in Class 1 & Class 2. As a result,
we have totally 9 more fine-grained error classes
(1A..1D, 2A..2D, 3)5.
We explored the distribution of 3 error classes
in our analysis. Class 1 distributed the most, fol-
lowing by Class 3 (slightly less) and Class 2.
4Their definitions can be found in (Damerau, 1964).
5A, B, C, and D represent for Insertion, Deletion, Sub-
stitution, and Transposition, respectively. For instance, 1A
means Insertion in Class 1.
37
Class Insertion Deletion Substitution Transpositiona
Class 1 ?p (correct) || ?ip (in-
correct) (?i? inserted)
kh?ng (correct) || kh
(incorrect). (???, ?n?,
and ?g? deleted)
y?u (correct) || ??u
(incorrect). (?y? sub-
stituted by ???)
N.A.
Class 2 l?n (correct) || li?n
(contextually incor-
rect). (?i? inserted)
tr?nh (correct) ||
t?nh (contextually
incorrect). (?r?
deleted)
ngay (correct) || ng?y
(contextually incor-
rect). (?a? substituted
by ???)
N.A.
Class 3 x?c nh?n l? (correct) || x||nha0a (incorrect). 3 syllables were misspelled & accidentally merged.
aOur analysis reveals no examples for this operator.
Table 1: Examples of error classes.
Generally, each class contributed a certain quan-
tity of errors (38%, 37%, & 25%), making the
correction process of errors more challenging. In
addition, there are totally 613 counts for 9 fine-
grained classes (over 551 errors of 283 sentences),
yielding an average & standard deviation 3.41 &
2.78, respectively. Also, one erroneous syllable is
able to contain the number of (fine-grained) error
classes as follows: 1(492), 2(56), 3(3), 4(0) ((N)
is count of cases).
We can also observe more about the distribu-
tion of operators that were used within each error
class in Figure 1. The Substitution operator was
used the most in both Class 1 & Class 2, holding
81% & 97%, respectively. Only a few other oper-
ators (Insertion, Deletion) were used. Specially,
the Transposition operator were not used in both
Class 1&Class 2. This justifies the fact that OCR
scanners normally have ambiguity in recognizing
similar characters.
3 Proposed Approach
The architecture of our proposed approach
(namely (VOSE)) is outlined in Figure 2. Our pur-
pose is to develop VOSE as an unsupervised data-
driven approach. It means VOSE will only use
textual data (un-annotated) to induce the detection
& correction strategies. This makes VOSE unique
and generic to adapt to other languages easily.
In VOSE, potential errors will be detected lo-
cally within each error class and will then be cor-
rected globally under a ranking scheme. Specif-
ically, VOSE implements two different detectors
(Non-syllable Detector & Real-syllable Detec-
tor) for two error groups of Class 1/3 & Class
2, respectively. Then, a corrector combines the
outputs from two above detectors based on rank-
ing scheme to produce the final output. Currently,
VOSE implements two different correctors, a
Contextual Corrector and a Weighting-based
Corrector. Contextual Corrector employs lan-
guage modelling to rank a list of potential can-
didates in the scope of whole sentence whereas
Weighting-based Corrector chooses the best
candidate for each syllable that has the highest
weights. The following will give detailed descrip-
tions for all components developed in VOSE.
3.1 Pre-processor
Pre-processor will take in the input text, do
tokenization & normalization steps. Tokeniza-
tion in Vietnamese is similar to one in En-
glish. Normalization step includes: normal-
ize Vietnamese tone & vowel (e.g. h?a ?>
ho?), standardize upper-/lower- cases, find num-
bers/punctuations/abbreviations, remove noise
characters, . . .
This step also extracts unigrams. Each of them
will then be checked whether they exist in a pre-
built list of unigrams (from large raw text data).
Unigrams that do not exist in the list will be re-
garded as Potential Class 1 & 3 errors and then
turned into Non-syllable Detector. Other uni-
grams will be regarded as Potential Class 2 er-
rors passed into Real-syllable Detector.
3.2 Non-syllable Detector
Non-syllable Detector is to detect errors that do
not exist in a pre-built combined dictionary (Class
1 & 3) and then generate a top-k list of poten-
tial candidates for replacement. A pre-built com-
bined dictionary includes all syllables (unigrams)
extracted from large raw text data.
In VOSE, we propose a novel approach that
uses pattern retrieval technique forNon-syllable
38
Figure 2: Proposed architecture of our approach
Detector. This approach aims to retrieve all n-
gram patterns (n can be 2,3) from textual data,
check approximate similarity with original erro-
neous syllables, and then produce a top list of po-
tential candidates for replacement.
We believe that this approach will be able to
not only handle errors with arbitrary changes on
syllables but also utilize contexts (within 2/3 win-
dow size), making possible replacement candi-
dates more reliable, and more semantically to
some extent.
This idea will be implemented in the N-gram
Engine component.
3.3 Real-syllable Detector
Real-syllable Detector is to detect all possible
real-syllable errors (Class 2) and then produce
the top-K list of potential candidates for replace-
ment. The core idea of Real-syllable Detector is
to measure the cohesion of contexts surrounding a
target syllable to check whether it is possibly erro-
neous or not. The cohesion is measured by counts
& probabilities estimated from textual data.
Assume that a K-size contextual window with a
target syllable at central position is chosen.
s1 s2 ? ? ? [sc] ? ? ? sK?1 sK (K syllables, sc to
be checked, K is an experimental odd value (can
be 3, 5, 7, 9).)
The cohesion of a sequence of syllables sK1 bi-
ased to central syllable sc can be measured by one
of three following formulas:
Formula 1:
cohesion1(s
K
1 ) = log(P (s
K
1 ))
= log(P (sc) ?
K?
i 6=c,i=1
P (si|sc))
(1)
Formula 2:
cohesion2(s
K
1 ) = countexist?(sc?2sc?1sc,
sc?1scsc+1, scsc+1sc+2, sc?1sc, scsc+1)
(2)
Formula 3:
cohesion3(s
K
1 ) = countexist?(sc?2 ? sc,
sc?1sc, sc ? sc+2, scsc+1)
(3)
where:
? cohesion(sK1 ) is cohesion measure of sequence s
K
1 .
39
? P (sc) is estimated from large raw text data com-
puted by c(sc)C , whereas c(sc) is unigram count and C
is total count of all unigrams from data.
? P (si|sc) is computed by:
P (si|sc) =
P (si, sc)
P (sc)
=
c(si, sc, |i? c|)
c(sc)
(4)
where:
? c(si, sc, |i? c|) is a distance-sensitive count of two
unigrams si and sc co-occurred and the gap between
them is |i? c| unigrams.
For Formula 1, if cohesion(sK1 ) < Tc with
Tc is a pre-defined threshold, the target syllable is
possibly erroneous.
For Formula 2, instead of probabilities as in
Formula 1, we use counting on existence of n-
grams within a context. It?s maximum value is 5.
Formula 3 is a generalized version of Formula 2
(the wild-card ?*? means any syllable). It?s maxi-
mum value is 4.
N-gram Engine. The N-gram Engine compo-
nent is very important in VOSE. All detectors &
correctors use it.
Data Structure. It is worthy noting that in or-
der to compute probabilities like c(si, sc, |i? c|)
or query the patterns from data, an efficient data
structure needs to be designed carefully. It MUST
satisfy two criteria: 1) space to suit memory re-
quirements 2) speed to suit real-time speed re-
quirement. In this work,N-gram Engine employs
inverted index (Zobel and Moffat, 2006), a well-
known data structure used in text search engines.
Pattern Retrieval. After detecting poten-
tial errors, both Non-syllable Detector and
Real-syllable Detector use N-gram Engine to
find a set of possible replacement syllables by
querying the textual data using 3-gram patterns
(sc?2sc?1[s?c], sc?1[s
?
c]sc+1, and [s
?
c]sc+1sc+2) or
2-gram patterns (sc?1 [s?c], [s
?
c]sc+1), where [s
?
c] is
a potential candidate. To rank a list of top candi-
dates, we compute the weight for each candidate
using the following formula:
weight(si) = ??Sim(si, s
?
c)+(1??)?Freq(si)
(5)
where:
? Sim(si, s?c) is the string similarity between candi-
date syllable si and erroneous syllable s?c .
? Freq(si) is normalized frequency of si over a re-
trieved list of possible candidates.
? ? is a value to control the weight biased to string
similarity or frequency.
In order to compute the string similarity, we
followed a combined weighted string similarity
(CWSS) computation in (Islam and Inkpen, 2009)
as follows:
Sim(si, s
?
c) = ?1 ?NLCS(si, s
?
c)
+?2 ?NCLCS1(si, s
?
c) + ?3 ?NCLCSn(si, s
?
c)
+?4 ?NCLCSz(si, s
?
c)
(6)
where:
? ?1, ?2, ?3, and ?4 are pre-defined weights for each
similarity computation. Initially, all ? are set equal to
1/4.
? NLCS(si, s?c) is normalized length of longest
common subsequence between si and s?c .
? NCLCS1(si, s?c), NCLCSn(si, s
?
c), and
NCLCSz(si, s?c) is normalized length of maximal
consecutive longest common subsequence between
si and s?c starting from the first character, from any
character, and from the last character, respectively.
? Sim(si, s?c) has its value in range of [0, 1].
We believe that the CWSS method will ob-
tain better performance than standard meth-
ods (e.g. Levenshtein-based String Matching
(Navarro, 2001) or n-gram based similarity (Lin,
1998)) because it can exactly capture more infor-
mation (beginning, body, ending) of incomplete
syllables caused by OCR errors. As a result, this
step will produce a ranked top-k list of potential
candidates for possibly erroneous syllables. In ad-
dition, N-gram Engine also stores computation
utilities relating the language models which are
then provided to Contextual Corrector.
3.4 Corrector
In VOSE, we propose two possible correctors:
Weighting-based Corrector
Given a ranked top-K list of potential can-
didates from Non-syllable Detector and Real-
syllable Detector, Weighting-based Corrector
simply chooses the best candidates based on their
weights (Equation 5) to produce the final output.
Contextual Corrector
Given a ranked top-K list of potential can-
didates from Non-syllable Detector and Real-
syllable Detector, Contextual Corrector glob-
ally ranks the best candidate combination using
language modelling scheme.
40
Specifically, Contextual Corrector employs
the language modelling based scheme which
chooses the combination of candidates (sn1 )
? that
makes PP ((sn1 )
?) maximized over all combina-
tions as follows:
(sn1 )
?
best = argmax(sn1 )? PP ((s
n
1 )
?) (7)
where: PP (.) is a language modelling score or per-
plexity (Jurafsky and Martin, 2008; Koehn, 2010).
In our current implementation, we used Depth-
First Traversal (DFS) strategy to examine over all
combinations. The weakness of DFS strategy is
the explosion of combinations if the number of
nodes (syllables in our case) grows more than 10.
In this case, the speed of DFS-based Contextual
Corrector is getting slow. Future work can con-
sider beam search decoding idea in Statistical
Machine Translation (Koehn, 2010) to adapt for
Contextual Corrector.
3.5 Prior Language-specific Knowledge
SinceVOSE is an unsupervised & data-driven ap-
proach, its performance depends on the quality
and quantity of raw textual data. VOSE?s cur-
rent design allows us to integrate prior language-
specific knowledge easily.
Some possible sources of prior knowledge
could be utilized as follows:
? Vietnamese Character Fuzzy Matching - In
Vietnamese language, some characters look very
similar, forcing OCR scanners mis-recognition.
Thus, we created a manual list of highly similar
characters (as shown in Table 2) and then inte-
grate this into VOSE. Note that this integration
takes place in the process of string similarity com-
putation.
? English Words & Vietnamese Abbrevia-
tions Filtering - In some cases, there exist En-
glish words or Vietnamese abbreviations. VOSE
may suggest wrong replacements for those cases.
Thus, a syllable in either English words or Viet-
namese abbreviations will be ignored in VOSE.
4 Experiments
4.1 Baseline Systems
According to our best knowledge, previous sys-
tems that are able to simultaneously handle both
non-syllable and real-syllable errors do not exist,
especially apply for Vietnamese language. We be-
lieve that VOSE is the first one to do that.
No. Character Similar Characters
1 a {? ? ? ? ? ? ? ?}
2 e {? ? ? ?} + {c}
3 i {? ?} + {l}
4 o {? ? ? ? ?}
5 u {? ? ? ? ?}
6 y {? ?}
7 d {?}
Table 2: Vietnamese similar characters.
4.2 N-gram Extraction Data
In VOSE, we extracted ngrams from the raw tex-
tual data. Table 3 shows data statistics used in our
experiments.
4.3 Evaluation Measure
We used the following measure to evaluate the
performance of VOSE:
- For Detection:
DF =
2?DR?DP
DR+DP
(8)
Where:
? DR (Detection Recall) = the fraction of errors
correctly detected.
? DP (Detection Precision) = the fraction of de-
tected errors that are correct.
? DF (Detection F-Measure) = the combination
of detection recall and precision.
- For Correction:
CF =
2? CR? CP
CR+ CP
(9)
Where:
? CR (Correction Recall) = the fraction of errors
correctly amended.
? CP (Correction Precision) = the fraction of
amended errors that are correct.
? CF (Correction F-Measure) = the combination
of correction recall and precision.
4.4 Results
We carried out our evaluation based on the real
dataset as described in Section 2. In our evalua-
tion, we intend:
? To evaluate whether VOSE can benefit from ad-
dition of more data, meaning that VOSE is actu-
ally a data-driven system.
? To evaluate the effectiveness of language mod-
elling based corrector in compared to weighing
41
N-grams
No Dataset NumOfSents Vocabulary 2-gram 3-gram 4-gram 5-gram
1 DS1 1,328,506 102,945 1,567,045 8,515,894 17,767,103 24,700,815
2 DS2a 2,012,066 169,488 2,175,454 12,610,281 27,961,302 40,295,888
3 DS3b 283 1,546 6,956 9,030 9,671 9,946
4 DS4c 344 1,755 6,583 7,877 8,232 8,383
aincludes DS1 and more
bannotated test data (not included in DS1 & DS2) as described in Section 2
cweb contexts data (not included in others) crawled from the Internet
Table 3: Ngram extraction data statistics.
based corrector.
? To evaluate whether prior knowledge specific
to Vietnamese language can help VOSE.
The overall evaluation result (in terms of detec-
tion & correction accuracy) is shown in Table 4.
In our experiments, all VOSE(s) except of VOSE
6 used contextual corrector (Section 3.4). Also,
Real-syllable Detector (Section 3.3) used Equa-
tion 3 which revealed the best result in our pre-
evaluation (we do not show the results because
spaces do not permit).
We noticed the tone & vowel normalization
step in Pre-processormodule. This step is impor-
tant specific to Vietnamese language. VOSE 2a in
Table 4 shows that VOSE using that step gives a
significant improvement (vs. VOSE 1) in both de-
tection & correction.
We also tried to assess the impact of language
modelling order factor in VOSE. VOSE using 3-
gram language modelling gives the best result
(VOSE 2a vs. VOSE 2b & 2c). Because of this,
we chose 3-gram for next VOSE set-ups.
We experiment how data addition affects
VOSE. First, we used bigger data (DS2) for ngram
extraction and found the significant improvement
(VOSE 3a vs. VOSE 2a). Second, we tried an
interesting set-up in which VOSE utilized ngram
extraction data with annotated test data (Dataset
DS3) only in order to observe the recall ability
of VOSE. Resulting VOSE (VOSE 3b) performed
extremely well.
As discussed in Section 3.5, VOSE allows in-
tegrated prior language-specific knowledge that
helps improve the performance (VOSE 4). This
justifies that statistical method in combined with
such prior knowledge is very effective.
Specifically, for each error in test data, we
crawled the web sentences containing contexts in
which that error occurs (called web contexts). We
added such web contexts into ngram extraction
data. With this strategy, we can improve the per-
formance of VOSE significantly (VOSE 5), ob-
taining the best result. Again, we?ve proved that
more data VOSE has, more accurate it performs.
The result of VOSE 6 is to show the superiority
of VOSE using contextual corrector in compared
with using weighting-based corrector (VOSE 6 vs.
VOSE 4). However, weighting-based corrector
has much faster speed in correction than contex-
tual corrector which is limited due to DFS traver-
sal & language modelling ranking.
Based on the above observations, we have two
following important claims:
? First, the addition of more data in ngram ex-
traction process is really useful for VOSE.
? Second, prior knowledge specific to Viet-
namese language helps to improve the perfor-
mance of VOSE.
? Third, contextual corrector with language mod-
elling is superior than weighting-based corrector
in terms of the accuracy.
4.5 Result Analysis
Based on the best results produced by our ap-
proach (VOSE), we recognize & categorize cases
that VOSE is currently unlikely to detect & cor-
rect properly.
Consecutive Cases (Category 1)
When there are 2 or 3 consecutive errors, their
contexts are limited or lost. This issue will af-
fect the algorithm implemented in VOSE utilizing
the contexts to predict the potential replacements.
VOSE can handle such errors to limited extent.
Merging Cases (Category 2)
In this case, two or more erroneous syllables
are accidentally merged. Currently, VOSE cannot
42
Detection Accuracy Correction Accuracy
Set-up Recall Precision F1 Recall Precision F1 Remark
VOSE 1 0.8782 0.5954 0.7097 0.6849 0.4644 0.5535 w/o TVN + 3-LM + DS1
VOSE 2a 0.8782 0.6552 0.7504 0.6807 0.5078 0.5817 w/ TVN + 3-LM + DS1
VOSE 2b 0.8782 0.6552 0.7504 0.6744 0.5031 0.5763 w/ TVN + 4-LM + DS1
VOSE 2c 0.8782 0.6552 0.7504 0.6765 0.5047 0.5781 w/ TVN + 5-LM + DS1
VOSE 3a 0.8584 0.7342 0.7914 0.6829 0.5841 0.6296 w/ TVN + 3-LM + DS2
VOSE 3b 0.9727 0.9830 0.9778 0.9223 0.9321 0.9271 w/ TVN + 3-LM + DS3
VOSE 4 0.8695 0.7988 0.8327 0.7095 0.6518 0.6794 VOSE 3a + PK
VOSE 5 0.8674 0.8460 0.8565 0.7200 0.7023 0.7110 VOSE 4 + DS4
VOSE 6 0.8695 0.7988 0.8327 0.6337 0.5822 0.6069 VOSE 4 but uses WC
Table 4: Evaluation results. Abbreviations: TVN (Tone & Vowel Normalization); N-LM (N-order
Language Modelling); DS (Dataset); PK (Prior Knowledge); WC (Weighting-based Corrector).
handle such cases. We aim to investigate this in
our future work.
Proper Noun/Abbreviation/Number Cases
(both in English, Vietnamese) (Category 3)
Abbreviations or proper nouns or numbers are
unknown (for VOSE) because they do not appear
in ngram extraction data. If VOSE marks them as
errors, it could not correct them properly.
Ambiguous Cases (Category 4)
Ambiguity can happen in:
? cases in which punctuation marks (e.g. comma,
dot, dash, . . . ) are accidentally added between two
different syllable or within one syllable.
? cases never seen in ngram extraction data.
? cases relating to semantics in Vietnamese.
? cases where one Vietnamese syllable that is
changed incorrectly becomes an English word.
Lost Cases (Category 5)
This case happens when a syllable which is ac-
cidentally lost most of its characters or too short
becomes extremely hard to correct.
Additionally, we conducted to observe the dis-
tribution of the above categories (Figure 3). As
can be seen, Category 4 dominates more than 70%
cases that VOSE has troubles for detection & cor-
rection.
5 Conclusion & Future Work
In this paper, we?ve proposed & developed a new
approach for spell checking task (both detection
and correction) for Vietnamese OCR-scanned text
documents. The approach is designed in an un-
supervised & data-driven manner. Also, it allows
Figure 3: Distribution of categories in the result
of VOSE 4 (left) & VOSE 5 (right).
to integrate the prior language-specific knowledge
easily.
Based on the evaluation on a real dataset,
the system currently offers an acceptable perfor-
mance (best result: detection accuracy 86%, cor-
rection accuracy 71%). With just an amount
of small n-gram extraction data, the obtained re-
sult is very promising. Also, the detailed error
analysis in previous section reveals that cases that
current system VOSE cannot solve are extremely
hard, referring to the problem of semantics-
related ambiguity in Vietnamese language.
Further remarkable point of proposed approach
is that it can perform the detection & correction
processes in real-time manner.
Future works include some directions. First, we
should crawl and add more textual data for n-gram
extraction to improve the performance of current
system. More data VOSE has, more accurate it
performs. Second, we should investigate more on
categories (as discussed earlier) that VOSE could
not resolve well. Last, we also adapt this work for
another language (like English) to assess the gen-
eralization and efficiency of proposed approach.
43
References
Fred J. Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Commun.
ACM, 7:171?176, March.
Victoria J. Hodge and Jim Austin. 2003. A com-
parison of standard spell checking algorithms and
a novel binary neural approach. IEEE Trans. on
Knowl. and Data Eng., 15(5):1073?1081, Septem-
ber.
Aminul Islam and Diana Inkpen. 2009. Real-word
spelling correction using google web it 3-grams.
In Proceedings of the 2009 Conference on Empir-
ical Methods in Natural Language Processing: Vol-
ume 3 - Volume 3, EMNLP ?09, pages 1241?1249,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Prentice Hall, second
edition, February.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Okan Kolak and Philip Resnik. 2002. Ocr error
correction using a noisy channel model. In Pro-
ceedings of the second international conference on
Human Language Technology Research, HLT ?02,
pages 257?262, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Dekang Lin. 1998. An information-theoretic def-
inition of similarity. In Proceedings of the Fif-
teenth International Conference on Machine Learn-
ing, ICML ?98, pages 296?304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Walid Magdy and Kareem Darwish. 2006. Arabic ocr
error correction using character segment correction,
language modeling, and shallow morphology. In
Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 408?414, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Walid Magdy and Kareem Darwish. 2008. Effect of
ocr error correction on arabic retrieval. Inf. Retr.,
11:405?425, October.
Surapant Meknavin, Boonserm Kijsirikul, Ananlada
Chotimongkol, and Cholwich Nuttee. 1998. Com-
bining trigram and winnow in thai ocr error cor-
rection. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Compu-
tational Linguistics - Volume 2, ACL ?98, pages
836?842, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Masaaki Nagata. 1996. Context-based spelling cor-
rection for japanese ocr. In Proceedings of the 16th
conference on Computational linguistics - Volume
2, COLING ?96, pages 806?811, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Masaaki Nagata. 1998. Japanese ocr error correction
using character shape similarity and statistical lan-
guage model. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ?98, pages
922?928, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Comput. Surv., 33(1):31?88,
March.
Kazem Taghva and Eric Stofsky. 2001. Ocrspell: an
interactive spelling correction system for ocr errors
in text. International Journal of Document Analysis
and Recognition, 3:2001.
Xian Tong and David A. Evans. 1996. A statistical
approach to automatic ocr error correction in con-
text. In Proceedings of the Fourth Workshop on
Very Large Corpora (WVLC-4, pages 88?100.
Yuen-Hsien Tseng. 2002. Error correction in a chi-
nese ocr test collection. In Proceedings of the 25th
annual international ACM SIGIR conference on Re-
search and development in information retrieval,
SIGIR ?02, pages 429?430, New York, NY, USA.
ACM.
Li Zhuang, Ta Bao, Xioyan Zhu, Chunheng Wang,
and S. Naoi. 2004. A chinese ocr spelling check
approach based on statistical language models. In
Systems, Man and Cybernetics, 2004 IEEE Interna-
tional Conference on, volume 5, pages 4727 ? 4732
vol.5.
Justin Zobel and Alistair Moffat. 2006. Inverted files
for text search engines. ACM Comput. Surv., 38,
July.
44
