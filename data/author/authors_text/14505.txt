A Comparative Study of Language Models for
Book and Author Recognition
O?zlem Uzuner and Boris Katz
MIT,Computer Science and Artificial Intelligence Laboratory,
Cambridge, MA 02139
{ozlem, boris}@csail.mit.edu
Abstract. Linguistic information can help improve evaluation of simi-
larity between documents; however, the kind of linguistic information to
be used depends on the task. In this paper, we show that distributions
of syntactic structures capture the way works are written and accurately
identify individual books more than 76% of the time. In comparison,
baseline features, e.g., tfidf-weighted keywords, function words, etc., give
an accuracy of at most 66%. However, testing the same features on au-
thorship attribution shows that distributions of syntactic structures are
less successful than function words on this task; syntactic structures vary
even among the works of the same author whereas features such as func-
tion words are distributed more similarly among the works of an author
and can more effectively capture authorship.
1 Introduction
Expression is an abstract concept that we define as ?the way people convey
particular content?. Copyrights protect an author?s expression of content where
content refers to the information contained in a work and expression refers to
the linguistic choices of authors in presenting this content. Therefore, capturing
expression is important for copyright infringement detection.
In this paper, we evaluate syntactic elements of expression in two contexts:
book recognition for copyright infringement detection and authorship attribu-
tion. Our first goal is to enable identification of individual books from their
expression of content, even when they share content, and even when they are
written by the same person. For this purpose, we use a corpus that includes
translations of the same original work into English by different people. For the
purposes of this study, we refer to the translations as books and an original work
itself as a title.
Given the syntactic elements of expression, our second goal is to test them on
authorship attribution, where the objective is to identify all works by a particu-
lar author. Our syntactic elements of expression capture differences in the way
people express content and could be useful for authorship attribution. However,
the experiments we present here indicate that syntactic elements of expression
are more successful at identifying expression in individual books while function
words are more successful at identifying authors.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 969?980, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
970 O?. Uzuner and B. Katz
2 Related Work
In text classification literature, similarity of works has been evaluated, for ex-
ample, in terms of genre, e.g., novels vs. poems, in terms of the style of au-
thors, e.g., Austen?s novels vs. Kipling?s novels, and in terms of topic, e.g., sto-
ries about earthquakes vs. stories about volcanoes. In this paper, we compare
several different language models in two different classification tasks: book recog-
nition based on similarity of expression, and authorship attribution. Authorship
attribution has been studied in the literature; however, evaluation of similar-
ity of expression, e.g., Verne?s 20000 Leagues vs. Flaubert?s Madame Bovary,
is a novel task that we endeavor to address as a first step towards copyright
infringement detection.
We define expression as ?the linguistic choices of authors in presenting con-
tent?: content of works and the linguistic choices made while presenting it to-
gether constitute expression. Therefore, capturing expression requires measuring
similarity of works in terms of both of these components.
To classify documents based on their content, most approaches focus on key-
words. Keywords contain information regarding the ideas and facts presented
in documents and, despite being ambiguous in many contexts, have been heav-
ily exploited to represent content. In addition to keywords, subject?verb and
verb?object relationships [12], noun phrases [12,13], synonym sets of words from
WordNet [12], semantic classes of verbs [12] from Levin?s studies [21], and proper
nouns have all been used to capture content.
Linguistic choices of authors have been studied in stylometry for authorship
attribution. Brinegar [7], Glover [9] and Mendenhall [22], among others, used
distribution of word lengths to identify authors, e.g., Glover and Hirst studied
distributions of two- and three-letter words [9]. Thisted et al [33] and Holmes [14]
studied the idea of richness of vocabulary and the rate at which new words are
introduced to the text. Many others experimented with distributions of sentence
lengths [9,18,24,30,31,32,38,40], sequences of letters [17,20], and syntactic classes
(part of speech) of words [9,20,19].
Mosteller and Wallace [25] studied the distributions of function words to
identify the authors of 12 unattributed Federalist papers. Using a subset of the
function words from Mosteller and Wallace?s work, Peng [26] showed that verbs
(used as function words, e.g., be, been, was, had) are important for differentiating
between authors. Koppel et al [19] studied the ?stability? of function words and
showed that the features that are most useful for capturing the style of authors
are ?unstable?, i.e., they can be replaced without changing the meaning of the
text. Koppel et al?s measure of stability identified function words, tensed verbs,
and some part-of-speech tag trigrams as unstable.
Syntactically more-informed studies of the writings of authors came from
diMarco and Wilkinson [39] who treated style as a means for achieving par-
ticular communicative goals and used parsed text to study the syntactic ele-
ments associated with each goal, e.g., clarity vs. obscurity. Adapting elements
from Halliday and Hasan [10,11], diMarco et al studied the use of cohesive el-
ements of text, e.g., anaphora and ellipsis, and disconnective elements of text,
A Comparative Study of Language Models for Book and Author Recognition 971
e.g., parenthetical constructions, as well as the patterns in the use of relative
clauses, noun embeddings, and hypotaxis (marked by subordinating conjunc-
tions) when authors write with different communicative goals.
Expression is related to both content and style. However, it is important to
differentiate expression from style. Style refers to the linguistic elements that,
independently of content, persist over the works of an author and has been widely
studied in authorship attribution. Expression involves the linguistic elements
that relate to how an author phrases particular content and can be used to
identify potential copyright infringement.
3 Syntactic Elements of Expression
We hypothesize that, given particular content, authors choose from a set of
semantically equivalent syntactic constructs to create their own expression of
it. As a result, different authors may choose to express the same content in
different ways. In this paper, we capture the differences in expression of authors
by studying [34,35,36]:
? sentence-initial and -final phrase structures that capture the shift in focus
and emphasis of a sentence due to reordered material,
? semantic classes and argument structures of verbs such as those used in
START for question answering [16] and those presented by Levin [21],
? syntactic classes of embedding verbs, i.e., verbs that take clausal arguments,
such as those studied by Alexander and Kunz [1] and those used in START
for parsing and generation [15], and
? linguistic complexity of sentences, measured both in terms of depths of
phrases and in terms of depths of clauses, examples of which are shown
in Table 1.
Table 1. Sample sentences broken down into their clauses and the depth of the top-
level subject (the number on the left) and predicate (the number on the right)
Sentence Depth of
Clauses
[I]a [would not think that [this]b [was possible]b]a 0, 2
[I]a [have found [it]b [difficult to say that [I]c [like it]c]b] a. 2, 2
[That [she]b [would give such a violent reaction]b]a [was unexpected]a. 1, 1
[For [her]b [to see this note]b]a [is impossible]a. 1, 1
[Wearing the blue shirt]a [was a good idea]a. 1, 1
[It]a [is not known whether [he]b [actually libelled the queen]b]a. 0, 2
[He]a [was shown that [the plan]b [was impractical]b ]a. 0, 2
[They]a [believed [him]b [to be their only hope]b]a. 0, 2
[I]a [suggest [he]b [go alone]b]a. 0, 2
[I]a [waited for [John]b [to come]b]a. 0, 2
972 O?. Uzuner and B. Katz
We extracted all of these features from part-of-speech tagged text [5] and
studied their distributions in different works. We also studied their correlations
with each other, e.g., semantic verb classes and the syntactic structure of the
alternation [21] in which they occur. The details of the relevant computations
are discussed by Uzuner [34].
3.1 Validation
We validated the syntactic elements of expression using the chi-square (and/or
likelihood ratio) test of independence. More specifically, for each of sentence-
initial and -final phrase structures, and semantic and syntactic verb classes, we
tested the null hypothesis that these features are used similarly by all authors
and that the differences observed in different books are due to chance. We per-
formed chi-square tests in three different settings: on different translations of
the same title (similar content but different expression), on different books by
different authors (different content and different expression), and on disjoint sets
of chapters from the same book (similar content and similar expression).
For almost all of the identified features, we were able to reject the null hy-
pothesis when comparing books that contain different expression, indicating that
regardless of content, these features can capture expression. For all of the fea-
tures, we were unable to reject the null hypothesis when we compared chapters
from the same book, indicating a certain consistency in the distributions of these
features throughout a work.
4 Evaluation
We used the syntactic elements of expression, i.e., sentence-initial and sentence-
final phrase structures, semantic and syntactic classes of verbs, and measures of
linguistic complexity [34,35,36], for book recognition and for authorship attribu-
tion.
4.1 Baseline Features
To evaluate the syntactic elements of expression, we compared the performance
of these features to baseline features that capture content and baseline features
that capture the way works are written. Our baseline features that capture con-
tent included tfidf-weighted keywords [27,28] excluding proper nouns, because
for copyright infringement purposes, proper nouns can easily be changed without
changing the content or expression of the documents and a classifier based on
proper nouns would fail to recognize otherwise identical works. Baseline features
that focus on the way people write included function words [25,26], distributions
of word lengths [22,40], distributions of sentence lengths [14], and a basic set
of linguistic features, extracted from tokenized, part-of-speech tagged, and/or
syntactically parsed text. This basic set of linguistic features included the num-
ber of words and the number of sentences in the document; type?token ratio;
A Comparative Study of Language Models for Book and Author Recognition 973
average and standard deviation of the lengths of words (in characters) and of
the lengths of sentences (in words) in the document; frequencies of declarative
sentences, interrogatives, imperatives, and fragmental sentences; frequencies of
active voice sentences, be-passives, and get-passives; frequencies of ?s-genitives,
of-genitives, and of phrases that lack genitives; frequency of overt negations; and
frequency of uncertainty markers [9,34].
4.2 Classification Experiments
We compared the syntactic elements of expression with the baseline features
in two separate experiments: recognizing books even when some of them are
derived from the same title (different translations) and recognizing authors. For
these experiments, we split books into chapters, created balanced sets of relevant
classes, and used boosted [29] decision trees [41] to classify chapters into books
and authors. We tuned parameters on the training set: we determined that the
performance of classifiers stabilized at around 200 rounds of boosting and we
eliminated from each feature set the features with zero information gain [8,37].
Recognizing Books: Copyrights protect original expression of content for a
limited time period. After the copyright period of a work, its derivatives by
different people are eligible for their own copyright and need to be recognized
from their unique expression of content. Our experiment on book recognition
focused on and addressed this scenario.
Data: For this experiment, we used a corpus that included 49 books derived from
45 titles ; for 3 of the titles, the corpus included multiple books (3 books for the
title Madame Bovary, 2 books for 20000 Leagues, and 2 books for The Kreutzer
Sonata). The remaining titles included works from J. Austen, F. Dostoyevski,
C. Dickens, A. Doyle, G. Eliot, G. Flaubert, T. Hardy, I. Turgenev, V. Hugo,
W. Irving, J. London, W. M. Thackeray, L. Tolstoy, M. Twain, and J. Verne.
We obtained 40?50 chapters from each book (including each of the books that
are derived from the same title), and used 60% of the chapters from each book
for training and the remaining 40% for testing.
Results: The results of this evaluation showed that the syntactic elements of
expression accurately recognized books 76% of the time; they recognized each
of the paraphrased books 89% of the time (see right column in Table 2). In
either case, the syntactic elements of expression significantly outperformed all
individual baseline features (see Table 2).
The syntactic elements of expression contain no semantic information; they
recognize books from the way they are written. The fact that these features
can differentiate between translations of the same title implies that translators
add their own expression to works, even when their books are derived from
the same title, and that the expressive elements chosen by each translator help
differentiate between books derived from the same title.
Despite recognizing books more accurately than each of the individual base-
line features, syntactic elements of expression on their own are less effective
974 O?. Uzuner and B. Katz
Table 2. Classification results on the test set for recognizing books from their expres-
sion of content even when some books contain similar content
Feature Set Accuracy on complete Accuracy on
corpus paraphrases only
Syntactic elements of expression 76% 89%
Tfidf-weighted keywords 66% 88%
Function words 61% 81%
Baseline linguistic 42% 53%
Dist. of word length 29% 72%
Dist. of sentence length 13% 14%
than the combined baseline features in recognizing books; the combined baseline
features give an accuracy of 88% on recognizing books (compare this to 76%
accuracy by the syntactic elements of expression alone). But the performance of
the combined baseline features is further improved by the addition of syntactic
elements of expression (see Table 3). This improvement is statistically significant
at ? = 0.05.
Table 3. Classification results of combined feature sets on the test set for book recog-
nition even when some books contain similar content
Feature Set Accuracy on complete Accuracy on
corpus paraphrases only
All baseline features +
syntactic elements of expression 92% 98%
All baseline features 88% 97%
Ranking the combined features based on information gain for recognizingbooks
shows that the syntactic elements of expression indeed play a significant role in rec-
ognizing books accurately; of the top tenmost useful features identifiedby informa-
tion gain, seven are syntactic elements of expression (see rows in italics in Table 4).
In the absence of syntactic elements of expression, the top ten most useful
features identified by information gain from the complete set of baseline features
reveal that the keywords ?captain? and ?sister? are identified as highly discrim-
inative features. Similarly, the function words ?she?, ?her?, and ??ll? are highly
discriminative (see Table 5). Part of the predictive power of these features is due
to the distinct contents of most of the books in this corpus; we expect that as
the corpus grows, these words will lose predictive power.
Recognizing Authors: In Section 2, we described the difference between style
and expression. These concepts, though different, both relate to the way people
write. Then, an interesting question to answer is: Can the same set of features
help recognize both books (from their unique expression) and authors (from their
unique style)?
A Comparative Study of Language Models for Book and Author Recognition 975
Table 4. Top ten features identified by information gain for recognizing books even
when some books share content. Features which are syntactic elements of expression
are in italics; baseline features are in roman.
Features
Std. dev. of the depths of the top-level left branches (measured in phrase depth)
Std. dev. of the depths of the top-level right branches (measured in phrase depth)
Std. dev. of the depths of the deepest prepositional phrases of sentences
(measured in phrase depth)
% of words that are one character long
Average word length
% of sentences that contain unembedded verbs
% of sentences that contain an unembedded verb with noun phrase object (0-V-NP)
Frequency of the word ?the? (normalized by chapter length)
Avg. depth of the subordinating clauses at the beginning of sentences
(measured in phrase depth)
% of sentences that contain equal numbers of clauses in left and right branch
Type-token ratio
Table 5. Top ten baseline features identified by information gain that recognize books
even when some books share content
Features
% words that are one character long
Average word length
Frequency of the word ?the? (normalized by chapter length)
Type-token ratio
Frequency of the word ?captain? (tfidf-weighted)
Probability of Negations
Frequency of the word ?sister? (tfidf-weighted)
Frequency of the word ?she? (normalized by chapter length)
Frequency of the word ?her? (normalized by chapter length)
Frequency of the word ??ll? (normalized by chapter length)
Data: In order to answer this question, we experimented with a corpus of books
that were written by native speakers of English. This corpus included works from
eight authors: three titles by W. Irving, four titles by G. Eliot, five titles by J.
Austen, six titles by each of C. Dickens and T. Hardy, eight titles by M. Twain,
and nine titles by each of J. London and W. M. Thackeray.
Results: To evaluate the different sets of features on recognizing authors from
their style, we trained models on a subset of the titles by each of these authors
and tested on a different subset of titles by the same authors. We repeated this
experiment five times so that several different sets of titles were trained and tested
on. At each iteration, we used 150 chapters from each of the authors for training
and 40 chapters from each of the authors for testing.
976 O?. Uzuner and B. Katz
Table 6. Results for authorship attribution. Classifier is trained on 150 chapters from
each author and tested on 40 chapters from each author. The chapters in the training
and test sets come from different titles.
Feature Set AccuracyAccuracyAccuracyAccuracyAccuracy
Run 1 Run 2 Run 3 Run 4 Run 5
Function words 86% 89% 87% 90% 81%
Syntactic elements of expression 64% 63% 64% 55% 62%
Distribution of word length 33% 37% 44% 53% 35%
Baseline linguistic 39% 39% 41% 48% 28%
Distribution of sentence length 33% 41% 31% 41% 25%
Table 7. Average classification results on authorship attribution
Feature Set Avg. Accuracy
Function words 87%
Syntactic elements of expression 62%
Distribution of word length 40%
Baseline linguistic 39%
Distribution of sentence length 34%
The results in Table 7 show that function words capture the style of authors
better than any of the other features; syntactic elements of expression are not as
effective as function words in capturing the style of authors. This finding is consis-
tent with our intuition: we selected the syntactic elements of expression for their
ability to differentiate between individual works, even when some titles are written
by the same author and even when some books were derived from the same title.
Recognizing the style of an author requires focus on the elements that are similar in
the works written by the same author, instead of focus on elements that differenti-
ate these works. However, the syntactic elements of expression are not completely
devoid of any style information: they recognize authors accurately 62%of the time.
In comparison, the function words recognize authors accurately 87% of the time.
Top ten most predictive function words identified by information gain for author-
ship attribution are: the, not, of, she, very, be, her, ?s, and, and it.
Combining the baseline features together does not improve the performance of
function words on authorship attribution: function words give an accuracy of 87%
by themselves whereas the combined baseline features give an accuracy of 86%.1
Adding the syntactic elements of expression to the combination of baseline features
hurts performance (see Table 8).
We believe that the size of the corpus is an important factor in this conclu-
sion. More specifically, we expect that as more authors are added to the corpus,
the contribution of syntactic elements of expression to authorship attribution will
increase. To test this hypothesis, we repeated our experiments with up to thir-
teen authors. We observed that the syntactic elements of expression improved the
1 This difference is not statistically significant.
A Comparative Study of Language Models for Book and Author Recognition 977
Table8.Average classification results of combined feature sets on authorship attribution
Feature Set Average Accuracy for 8 Authors
All baseline features +
syntactic elements of expression 81%
All baseline features 86%
Function words 87%
Syntactic elements
of expression 62%
Table 9. Average classification results of combined feature sets on authorship attribu-
tion. For these experiments, the original corpus was supplemented with works from W.
Ainsworth, L. M. Alcott, T. Arthur, M. Braddon, and H. James.
Feature Set Average Accuracy for 8-13 Authors
8 9 10 11 12 13
All baseline features +
syntactic elements of expression 81% 88% 88.4% 87.6% 88% 88%
All baseline features 86% 86% 87.8% 86.6% 86% 86.8%
Function words 87% 86.4% 85.4% 85.2% 84.8% 82.6%
Syntactic elements
of expression 62% 65.6% 68.2% 67.4% 66% 64.4%
performance of the baseline features: as we added more authors to the corpus, the
performance of function words degraded, the performance of syntactic elements of
expression improved, and the performance of the combined feature set remained
fairly consistent at around 88% (see Table 9).
4.3 Conclusion
In this paper, we compared several different language models on two classifica-
tion tasks: book recognition and authorship attribution. In particular, we evalu-
ated syntactic elements of expression consisting of sentence-initial and -final phrase
structures, semantic and syntactic categories of verbs, and linguistic complexity
measures, on recognizing books (even when they are derived from the same title)
and on recognizing authors. Through experiments on a corpus of novels, we have
shown that syntactic elements of expression outperform all individual baseline fea-
tures in recognizing books and when combined with the baseline features, they im-
prove recognition of books.
In our authorship attribution experiments, we have shown that the syntactic
elements of expression are not as useful as function words in recognizing the style
978 O?. Uzuner and B. Katz
of authors. This finding highlights the need for a task-dependent approach to en-
gineering feature sets for text classification. In our experiments, feature sets that
have been engineered for studying expression and the language models based on
these feature sets outperform all others in identifying expression. Similarly, feature
sets that have been engineered for studying style and the language models based
on these feature sets outperform syntactic elements of expression in authorship
attribution.
References
1. D. Alexander and W. J. Kunz. SomeClasses of Verbs inEnglish. Linguistics Research
Project. Indiana University, 1964.
2. J. C. Baker. A Test of Authorship Based on the Rate at which New Words Enter an
Author?s Text. Journal of the Association for Literary and Linguistic Computing,
3(1), 36?39, 1988.
3. D. Biber. A Typology of English Texts. Language, 27, 3?43, 1989.
4. D. Biber, S. Conrad, and R. Reppen. Corpus Linguistics: Investigating Language
Structure and Use. Cambridge University Press, 1998.
5. E. Brill. A Simple Rule-Based Part of Speech Tagger. Proceedings of the 3rd Con-
ference on Applied Natural Language Processing, 1992.
6. M. Diab, J. Schuster, and P. Bock. A Preliminary Statistical Investigation into the
Impact of an N-GramAnalysisApproach based onWordSyntacticCategories toward
Text Author Classification. In Proceedings of Sixth International Conference on
Artificial Intelligence Applications, 1998.
7. C. S. Brinegar. Mark Twain and the QuintusCurtius Snodgrass Letters: A Statistical
Test of Authorship. Journal of the American Statistical Association, 58, 85?96, 1963.
8. G. Forman. An Extensive Empirical Study of Feature Selection Metrics for Text
Classification. Journal of Machine Learning Research, 3, 1289?1305, 2003.
9. A. Glover and G. Hirst. Detecting stylistic inconsistencies in collaborative writing. In
Sharples, Mike and van derGeest, Thea (eds.), The new writing environment: Writers
at work in a world of technology. London: Springer-Verlag, 1996.
10. M. Halliday and R. Hasan. Cohesion in English. London: Longman, 1976.
11. M. Halliday. An introduction to functional grammar. London; Baltimore, Md., USA
: Edward Arnold, 1985.
12. V. Hatzivassiloglou, J. Klavans, and E. Eskin. Detecting Similarity by Applying
Learning over Indicators. 37th Annual Meeting of the ACL, 1999.
13. V. Hatzivassiloglou, J. Klavans, M. Holcombe, R. Barzilay, M.Y. Kan, and
K.R. McKeown. SimFinder: A Flexible Clustering Tool for Summarization.
NAACL?01 Automatic Summarization Workshop, 2001.
14. D. I. Holmes. Authorship Attribution. Computers and the Humanities, 28, 87?106.
Kluwer Academic Publishers, Netherlands, 1994.
15. B. Katz. Using English for Indexing and Retrieving. Artificial Intelligence at MIT:
Expanding Frontiers. P. H. Winston and S. A. Shellard, eds. MIT Press. Cambridge,
MA., 1990.
16. B. Katz andB. Levin. ExploitingLexical Regularities in Designing Natural Language
Systems. In Proceedings of the 12th International Conference on Computational Lin-
guistics, COLING ?88, 1988.
17. D. Khmelev and F. Tweedie. Using Markov Chains for Identification of Writers.
Literary and Linguistic Computing, 16(4), 299?307, 2001.
A Comparative Study of Language Models for Book and Author Recognition 979
18. G. Kjetsaa. The Authorship of the Quiet Don. ISBN 0391029487. International Spe-
cialized Book Service Inc., 1984.
19. M. Koppel, N. Akiva, and I. Dagan. A Corpus-Independent Feature Set for Style-
Based Text Categorization. Proceedings of IJCAI?03 Workshop on Computational
Approaches to Style Analysis and Synthesis, 2003.
20. O. V. Kukushkina, A. A. Polikarpov, and D. V. Khemelev. Using Literal and Gram-
matical Statistics for Authorship Attribution. Published in Problemy Peredachi In-
formatsii,37(2), April-June 2000, 96?108. Translated in ?Problems of Information
Transmission?, 172?184.
21. B. Levin. English Verb Classes and Alternations. A Preliminary Investigation. ISBN
0-226-47533-6. University of Chicago Press. Chicago, 1993.
22. T. C. Mendenhall. Characteristic Curves of Composition. Science, 11, 237?249,
1887.
23. G. A. Miller, E. B. Newman, and E. A. Friedman.: Length-Frequency Statistics for
Written English. Information and Control,1(4), 370?389, 1958.
24. A. Q. Morton. The Authorship of Greek Prose. Journal of the Royal Statistical
Society (A), 128, 169?233, 1965.
25. F. Mosteller and D. L. Wallace. Inference in an authorship Problem. Journal of the
American Statistical Association, 58(302), 275?309, 1963.
26. R. D. Peng and H. Hengartner. Quantitative Analysis of Literary Styles. The Amer-
ican Statistician, 56(3), 175?185, 2002.
27. G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval.
Information Processing and Management, 24(5), 513?523, 1998.
28. G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing.
Communications of the ACM, 18(11), 613?620, 1975.
29. R. E. Schapire. The Boosting Approach to Machine Learning. In MSRI Workshop
on Nonlinear Estimation and Classification, 2002.
30. H. S. Sichel. On a Distribution Representing Sentence-Length in Written Prose.
Journal of the Royal Statistical Society (A), 137, 25?34, 1974.
31. M. W. A. Smith. Recent Experience and New Developments of Methods for the
Determination of Authorship. Association for Literary and Linguistic Computing
Bulletin, 11, 73?82, 1983.
32. D. R. Tallentire. An Appraisal of Methods and Models in Computational Stylistics,
with Particular Reference to Author Attribution. PhD Thesis. University of Cam-
bridge, 1972.
33. R. Thisted and B. Efron. Did Shakespeare Write a Newly-discovered Poem?
Biometrika, 74, 445?455, 1987.
34. O?. Uzuner. Identifying Expression Fingerprints using Linguistic Information. Ph.D.
Dissertation. Massachusetts Institute of Technology, 2005.
35. O?. Uzuner and B. Katz. Capturing Expression Using Linguistic Information. In
Proceedings of the 20th National Conference on Artificial Intelligence (AAAI-05),
2005.
36. O?. Uzuner, B. Katz and Thade Nahnsen. Using Syntactic Information to Identify
Plagiarism. In Proceedings of the Association for Computational Linguistics Work-
shop on Educational Applications (ACL 2005), 2005.
37. Y. Yang and J. O. Pedersen. A Comparative Study on Feature Selection in Text Cat-
egorization. In Proceedings of ICML-97, 14th International Conference on Machine
Learning. 412?420, 1997.
980 O?. Uzuner and B. Katz
38. G. U. Yule. On Sentence-Length as a Statistical Characteristic of Style in Prose,
with Application to Two Cases of Disputed Authorship. Biometrika, 30, 363?390,
1938.
39. J. Wilkinson and C. diMarco. Automated Multi-purpose Text Processing. In Pro-
ceedings of IEEE Fifth Annual Dual-Use Technologies and Applications Conference,
1995.
40. C. B. Williams. Mendenhall?s Studies of Word-Length Distribution in the Works of
Shakespeare and Bacon. Biometrika, 62(1), 207?212, 1975.
41. I. H. Witten and E. Frank. Data Mining: Practical machine Learning Tools with
Java Implementations. Morgan Kaufmann, San Francisco, 2000.
Lexical Chains and Sliding Locality Windows in Content-based 
Text Similarity Detection 
Thade Nahnsen, ?zlem Uzuner, Boris Katz 
Computer Science and Artificial Intelligence Laboratory 
Massachusetts Institute of Technology 
Cambridge, MA 02139 
{tnahnsen,ozlem,boris}@csail.mit.edu 
 
 
Abstract 
We present a system to determine 
content similarity of documents. 
Our goal is to identify pairs of book 
chapters that are translations of the 
same original chapter.  Achieving 
this goal requires identification of 
not only the different topics in the 
documents but also of the particular 
flow of these topics.   
Our approach to content 
similarity evaluation employs n-
grams of lexical chains and 
measures similarity using the 
cosine of vectors of n-grams of 
lexical chains, vectors of tf*idf-
weighted keywords, and vectors of 
unweighted lexical chains 
(unigrams of lexical chains).  Our 
results show that n-grams of 
unordered lexical chains of length 
four or more are particularly useful 
for the recognition of content 
similarity. 
1   Introduction 
This paper addresses the problem of determining 
content similarity between chapters of literary 
novels.  We aim to determine content similarity 
even when book chapters contain more than one 
topic by resolving exact content matches rather 
than finding similarities in dominant topics.   
Our solution to this problem relies on lexical 
chains extracted from WordNet [6]. 
2   Related Work 
Lexical Chains (LC) represent lexical items 
which are conceptually related to each other, for 
example, through hyponymy or synonymy 
relations.  Such conceptual relations have 
previously been used in evaluating cohesion, 
e.g., by Halliday and Hasan [2, 3].    Barzilay 
and Elhadad [1] used lexical chains for text 
summarization; they identified important 
sentences in a document by retrieving strong 
chains.  Silber and McCoy [7] extended the 
work of Barzilay and Elhadad; they developed 
an algorithm that is linear in time and space for 
efficient identification of lexical chains in large 
documents.  In this algorithm, Silber and McCoy 
first created a text representation in the form of 
metachains, i.e., chains that capture all possible 
lexical chains in the document.  After creating 
the metachains, they used a scoring algorithm to 
identify the lexical chains that are most relevant 
to the document, eliminated unnecessary 
overhead information from the metachains, and 
selected the lexical chains representing the 
document.  Our method for building lexical 
chains follows this algorithm. 
N-gram based language models, i.e., models 
that divide text into n-word (or n-character) 
strings, are frequently used in natural language 
processing.  In plagiarism detection, the overlap 
of n-grams between two documents has been 
used to determine whether one document 
plagiarizes another [4].   In general, n-grams 
capture local relations.  In our case, they capture 
local relations between lexical chains and 
between concepts represented by these chains.   
Three main streams of research in content 
similarity detection are: 1) shallow, statistical 
analysis of documents, 2) analysis of rhetorical 
relations in texts [5], and 3) deep syntactic 
150
analysis [8]. Shallow methods do not include 
much linguistic information and provide a very 
rough model of content while approaches that 
use syntactic analysis generally require 
significant computation. Our approach strikes a 
compromise between these two extremes: it uses 
the linguistic knowledge provided in WordNet 
as a way of making use of low-cost linguistic 
information for building lexical chains that can 
help detect content similarity.   
3   Lexical Chains in Content Similarity 
Detection 
3.1   Corpus 
The experiments in this paper were performed 
on a corpus consisting of chapters from 
translations of four books (Table 1) that cover a 
variety of topics.  Many of the chapters from 
each book deal with similar topics; therefore, 
fine-grained content analysis is required to 
identify chapters that are derived from the same 
original chapter. 
 
# 
translati
ons 
Title # 
chapters 
2 20,000 Leagues under the Sea 47 
3 Madame Bovary 35 
2 The Kreutzer Sonata 28 
2 War and Peace 365 
Table 1: Corpus 
3.2   Computing Lexical Chains 
Our approach to calculating lexical chains uses 
nouns, verbs, and adjectives present in 
WordNetV2.0.  We first extract such words 
from each chapter in the corpus and represent 
each chapter as a set of these word instances {I1, 
?, In}.  Each instance of each of these words 
has a set of possible interpretations, IN, in 
WordNet.  These interpretations are either the 
synsets or the hypernyms of the instances.  
Given these interpretations, we apply a slightly 
modified version of the algorithm by Silber and 
McCoy [7] to automatically disambiguate 
nouns, verbs, and adjectives, i.e., to select the 
correct interpretation, for each instance.  Silber 
and McCoy?s algorithm computes all of the 
scored metachains for all senses of each word in 
the document and attributes the word to the 
metachain to which it contributes the most.   
During this process, the algorithm computes the 
contribution of a word to a given chain by 
considering 1) the semantic relations between 
the synsets of the words that are members of the 
same metachain, and 2) the distance between 
their respective instances in the discourse.  Our 
approach uses these two parameters, with minor 
modifications.  Silber and McCoy measure 
distance in terms of paragraphs on prose text; 
we measure distance in terms of sentences in 
order to handle both dialogue and prose text.  
 
 
Figure 1: Intermediate representation after 
eliminating words that are not nouns, verbs, or 
adjectives and after identifying lexical chains 
(represented by WordNet synset IDs).  Note that 
{kitchen, bathroom} are represented by the same 
synset ID which corresponds to the synset ID of 
their common hypernym ?room?.  {kitchen, 
bathroom} is a lexical chain.  Ties are broken in 
favor of hypernyms. 
 
Following Silber and McCoy, we allow 
different types of conceptual relations to 
contribute differently to each lexical chain, i.e., 
the contribution of each word to a lexical chain 
is dependent on its semantic relation to the chain 
(see Table 2).  After scoring, concepts that are 
dominant in the text segment are identified and 
each word is represented by only the WordNet 
ID of the synset (or the hypernym/hyponym set) 
that best fits its local context.  Figure 1 gives an 
example of the resulting intermediate 
representation, corresponding to the 
interpretation, S, found for each word instance, 
I, that can be used to represent each chapter, C, 
where C = {S1, ?, Sm}.   
 
Lexical 
semantic 
relation 
Distance <= 
6 sentences 
Distance > 
6 sentences 
Same word 1 0 
Hyponym 0.5 0 
Hypernym 0.5 0 
Sibling 0.2 0 
Table 2: Contribution to lexical chains 
Original document (underlined words are represented 
with lexical chains): 
The furniture in the kitchen seems beautiful, but the bathroom 
seems untidy. 
 
Intermediate representation (lexical chains): 
03281101   03951013   02071636   00218842   03951013  
02071636   02336718    
 
151
3.3 Determining the Locality Window 
After computing the lexical chains, we created a 
representation for text by substituting the correct 
lexical chain for each noun, verb, and adjective 
in each document. We omitted the remaining 
parts of speech from the documents (see Figure 
1 for sample intermediate representation). We 
obtained ordered and unordered n-grams of 
lexical chains from this representation. 
Ordered n-grams consist of n consecutive 
lexical chains extracted from text. These ordered 
n-grams preserve the original order of the lexical 
chains in the text. Corresponding unordered n-
grams disregard this order. The resulting text 
representation is T = {gram1, gram2, ?, gramn}, 
where grami = [lc1, ?,  lcn], where lci ? {I1, ?, 
Ik} (the chains that represent Chapter C). The 
elements in grami may be sorted or unsorted, 
depending on the selected method.  N-grams are 
extracted from text using sliding locality 
windows and provide what we call ?attribute 
vectors?. The attribute vector for ordered n-
grams has the form C = {(e1, ?, en), (e2, ?, 
en+1), ?, (em-n, ?, em)} where (e1, ?, en) is an 
ordered n-gram and em is the last lexical chain in 
the chapter.  For unordered n-grams, the 
attribute vector has the form C = {sort[(e1, ?, 
en)], sort[(e2, ?, en+1)], ?, sort[(em-n, ?, em)]} 
where sort[?] indicates alphabetical sorting of 
chains (rather than the actual order in which the 
chains appear in the text).  
We evaluated similarity between pairs of 
book chapters using the cosine of the attribute 
vectors of n-grams of lexical chains (sliding 
locality windows of width n).  We varied the 
width of the sliding locality windows from two 
to five elements.  
4   Evaluation 
We used cosine similarity as the distance metric, 
computed the cosine of the angle between the 
vectors of pairs of documents in the corpus, and 
ranked the pairs based on this score.  We 
identified the top n most similar pairs (also 
referred to as ?selection level of n?) and 
considered them to be similar in content. 
We calculated similarity between pairs of 
documents in several different ways, evaluated 
these approaches with the standard information 
retrieval measures, i.e., precision, recall, and f-
measure, and compared our results with two 
baselines. The first baseline measured the 
similarity of documents with tf*idf-weighted 
keywords; the second used the cosine of 
unweighted lexical chains (unigrams of lexical 
chains). 
The corpus of parallel translations provides 
data that can be used as ground truth for content 
similarity; corresponding chapters from different 
translations of the same original title are 
considered similar in content, i.e., chapter 1 of 
translation 1 of Madame Bovary is similar in 
content to chapter 1 of translation 2 of Madame 
Bovary. 
Figure 2 shows the f-measure of different 
methods for measuring similarity between pairs 
of chapters using ordered lexical chains, 
unordered lexical chains, and baselines.  These 
graphs present the results when the top 100?
1,600 most similar pairs in the corpus are 
considered similar in content and the rest are 
considered dissimilar (selection level of 100?
1,600).  The total number of chapter pairs is 
approximately 1,000,000. Of these, 1,080 (475 
unique chapters with 2 or 3 translations each) 
are considered similar for evaluation purposes. 
The results indicate that four similarity 
measures gave the best performance.  These 
were tri-grams, quadri-grams, penta-grams, and 
hexa-grams of unordered lexical chains.  The 
peak f-measure at the selection level of 1,100 
chapter pairs was 0.981. Chi squared tests 
performed on the f-measures (when the top 
1,100 pairs were considered similar) were 
significant at p = 0.001. 
Closer analysis of the graphs in Figure 2 
shows that, at the optimal selection level, n-
grams of ordered lexical chains of length greater 
than four significantly outperformed the baseline 
at p = 0.001 while n-grams of ordered lexical 
chains of length less than or equal to four are 
significantly outperformed by the baseline at the 
same p. A similar observation cannot be made 
for the n-grams of unordered lexical chains; for 
these n-grams, the performance degradation 
appears at n = 7, i.e., the corresponding curves 
have a steeper negative incline than the baseline.   
After the cut-off point of 1,100 chapter pairs, 
the performance of all algorithms declines. This 
is due to the evaluation method we have chosen: 
although the cut-off for similarity judgement can 
be increased, the number of chapters that are in 
fact similar does not change and at high cut-off 
values many dissimilar pairs are considered 
similar, leading to degradation in performance. 
152
Figures 2a and 2b show that some of the 
lexical chain representations do not outperform 
the tf*idf-weighted baseline.  A comparison of 
Figures 2a and 2b shows that, for n < 5, n-grams 
of ordered lexical chains perform worse than n-
grams of unordered lexical chains. This 
indicates that between different translations of 
the same book the order of chains changes 
significantly, but that the chains within 
contiguous regions (locality windows) of the 
texts remain similar.   
Interestingly, ordered n-grams of length 3 to 5 
perform significantly better than unordered n-
grams of the same length. This implies that, 
during translation, the order of the content 
words does not change enormously for three to 
five lexical chain elements.  Allowing flexible 
order for the lexical chains (i.e., unordered 
lexical chains) in these n-grams therefore hurts 
performance by allowing many false positives.  
However, for longer n-grams to be successful, 
the order of the lexical chains has to be flexible. 
Figure 2: F-Measure
. 
F-M e as u r e  vs . C h ap te r s  Se le cte d  (Un o r d e r e d  N-Gr am s )
0
0,1
0 ,2
0 ,3
0 ,4
0 ,5
0 ,6
0 ,7
0 ,8
0 ,9
1
100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600
C h ap te r s  Se le cte d
F
-M
ea
su
re
u2gram/LC u3gram/LC u4gram/LC u5gram/LC u6gram/LC
u7gram/LC tf *id f c os ine
 
(a) F-Measure: Unordered n-grams vs. the baselines 
F- M e a s u r e  v s . C h a p t e r s  S e le c t e d  ( O r d e r e d  N-G r a m s )
0
0 ,1
0 ,2
0 ,3
0 ,4
0 ,5
0 ,6
0 ,7
0 ,8
0 ,9
1
100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600
C h a p te r s  S e le c t e d
F
-M
ea
su
re
tf * id f c o s in e 4g ram/LC 5g ram/LC 6g ram/LC
7g ram/LC 2g ram/LC 3g ram/LC
 
(b) F-Measure: Ordered n-grams vs. the baselines 
   
  ngram/LC ? unordered n-grams of lexical chains are used in the attribute vector 
  ungram/LC ? ordered n-grams of lexical chains are used in the attribute vector  
  tf*idf ? tf*idf weighted words are used in the attribute vector 
  cosine ? the standard information retrieval measure; words are used in the attribute vector 
153
5   Future Work 
Currently, our similarity measures do not 
employ any weighting scheme for n-grams, i.e., 
every n-gram is given the same weight.  For 
example, the n-gram ?be it as it has been? in 
lexical chain form corresponds to synsets for the 
words be, have and be.  The trigram of these 
lexical chains does not convey significant 
meaning.  On the other hand, the n-gram ?the 
lawyer signed the heritage? is converted into the 
trigram of lexical chains of lawyer, sign, and 
heritage.  This trigram is more meaningful than 
the trigram be have be, but in our scheme both 
trigrams will get the same weight.  As a result, 
two documents that share the trigram be have be 
will look as similar as two documents that share 
lawyer sign heritage. This problem can be 
addressed in two possible ways: using a ?stop 
word? list to filter such expressions completely 
or giving different weights to n-grams based on 
the number of their occurrences in the corpus.   
6   Conclusion 
We have presented a system that extends 
previous work on lexical chains to content 
similarity detection.   This system employs 
lexical chains and sliding locality windows, and 
evaluates similarity using the cosine of n-grams 
of lexical chains and tf*idf weighted keywords.  
The results indicate that lexical chains are 
effective for detecting content similarity 
between pairs of chapters corresponding to the 
same original in a corpus of parallel translations.  
References 
1. Barzilay, R., Elhadad, M. 1999. Using lexical 
chains for text summarization. In: Inderjeet 
Mani and Mark T. Maybury, eds., Advances 
in AutomaticText Summarization, pp. 111?
121. Cambridge/MA, London/England: MIT 
Press. 
2. Halliday, M. and Hasan, R. 1976. Cohesion in 
English. Longman, London. 
3. Halliday, M. and Hasan, R. 1989. Language, 
context, and text. Oxford University Press, 
Oxford, UK. 
4. Lyon, C., Malcolm, J. and Dickerson, B. 
2001. Detecting Short Passages of Similar 
Text in Large Document Collections, In 
Proceedings of the 2001 Conference on 
Empirical Methods in Natural Language 
Processing, pp.118-125. 
5. Marcu, D. 1997. The Rhetorical Parsing, 
Summarization, and Generation of Natural 
Language Texts (Ph.D. dissertation). Univ. of 
Toronto. 
6. Miller, G., Beckwith, R., Felbaum, C., Gross, 
D., and Miller, K. 1990. Introduction to 
WordNet: An online lexical database. J. 
Lexicography, 3(4), pp. 235-244. 
7. Silber, G. and McCoy, K. 2002. Efficiently 
computed lexical chains as an intermediate 
representation for automatic text 
summarization. Computational Linguistics, 
28(4). 
8. Uzuner, O., Davis, R., Katz, B. 2004. Using 
Empirical Methods for Evaluating Expression 
and Content Similarity. In: Proceedings of the 
37th Hawaiian International Conference on 
System Sciences (HICSS-37).  IEEE 
Computer Society. 
 
154
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 37?44, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Using Syntactic Information to Identify Plagiarism
O?zlem Uzuner, Boris Katz, and Thade Nahnsen
Massachusetts Institute of Technology
Computer Science and Articial Intelligence Laboratory
Cambridge, MA 02139
ozlem,boris,tnahnsen@csail.mit.edu
Abstract
Using keyword overlaps to identify pla-
giarism can result in many false negatives
and positives: substitution of synonyms
for each other reduces the similarity be-
tween works, making it difficult to rec-
ognize plagiarism; overlap in ambiguous
keywords can falsely inflate the similar-
ity of works that are in fact different in
content. Plagiarism detection based on
verbatim similarity of works can be ren-
dered ineffective when works are para-
phrased even in superficial and immate-
rial ways. Considering linguistic informa-
tion related to creative aspects of writing
can improve identification of plagiarism
by adding a crucial dimension to evalu-
ation of similarity: documents that share
linguistic elements in addition to content
are more likely to be copied from each
other. In this paper, we present a set of
low-level syntactic structures that capture
creative aspects of writing and show that
information about linguistic similarities
of works improves recognition of plagia-
rism (over tfidf-weighted keywords alone)
when combined with similarity measure-
ments based on tfidf-weighted keywords.
1 Introduction
To plagiarize is ?to steal and pass off (the ideas
or words of another) as one?s own; [to] use (an-
other?s production) without crediting the source; [or]
to commit literary theft [by] presenting as new and
original an idea or product derived from an exist-
ing source?.1 Plagiarism is frequently encountered
in academic settings. According to turnitin.com, a
2001 survey of 4500 high school students revealed
that ?15% [of students] had submitted a paper ob-
tained in large part from a term paper mill or web-
site?. Increased rate of plagiarism hurts quality of
education received by students; facilitating recog-
nition of plagiarism can help teachers control this
damage.
To facilitate recognition of plagiarism, in the re-
cent years many commercial and academic prod-
ucts have been developed. Most of these approaches
identify verbatim plagiarism2 and can fail when
works are paraphrased. To recognize plagiarism
in paraphrased works, we need to capture similar-
ities that go beyond keywords and verbatim over-
laps. Two works that exhibit similarity both in their
conceptual content (as indicated by keywords) and
in their expression of this content should be consid-
ered more similar than two works that are similar
only in content. In this context, content refers to
the story or the information; expression refers to the
linguistic choices of authors used in presenting the
content, i.e., creative elements of writing, such as
whether authors tend toward passive or active voice,
whether they prefer complex sentences with embed-
ded clauses or simple sentences with independent
clauses, as well as combinations of such choices.
Linguistic information can be a source of power
for measuring similarity between works based on
1www.webster.com
2www.turnitin.com
37
their expression of content. In this paper, we use lin-
guistic information related to the creative aspects of
writing to improve recognition of paraphrased doc-
uments as a first step towards plagiarism detection.
To identify a set of features that relate to the linguis-
tic choices of authors, we rely on different syntactic
expressions of the same content. After identifying
the relevant features (which we call syntactic ele-
ments of expression), we rely on patterns in the use
of these features to recognize paraphrases of works.
In the absence of real-life plagiarism data, in this
paper, we use a corpus of parallel translations of
novels as surrogate for plagiarism data. Transla-
tions of titles, i.e., original works, into English by
different people provide us with books that are para-
phrases of the same content. We use these para-
phrases to automatically identify:
1. Titles even when they are paraphrased, and
2. Pairs of book chapters that are paraphrases of
each other.
Our first experiment shows that syntactic elements
of expression outperform all baselines in recogniz-
ing titles even when they are paraphrased, provid-
ing a way of recognizing copies of works based on
the similarities in their expression of content. Our
second experiment shows that similarity measure-
ments based on the combination of tfidf-weighted
keywords and syntactic elements of expression out-
perform the weighted keywords in recognizing pairs
of book chapters that are paraphrases of each other.
2 Related Work
We define expression as ?the linguistic choices of
authors in presenting a particular content? (Uzuner,
2005; Uzuner and Katz, 2005). Linguistic similarity
between works has been studied in the text classifi-
cation literature for identifying the style of an author.
However, it is important to differentiate expression
from style. Style refers to the linguistic elements
that, independently of content, persist over the works
of an author and has been widely studied in author-
ship attribution. Expression involves the linguistic
elements that relate to how an author phrases par-
ticular content and can be used to identify potential
copyright infringement or plagiarism. Similarities
in the expression of similar content in two differ-
ent works signal potential copying. We hypothesize
that syntax plays a role in capturing expression of
content. Our approach to recognizing paraphrased
works is based on phrase structure of sentences in
general, and structure of verb phrases in particular.
Most approaches to similarity detection use com-
putationally cheap but linguistically less informed
features (Peng and Hengartner, 2002; Sichel, 1974;
Williams, 1975) such as keywords, function words,
word lengths, and sentence lengths; approaches that
include deeper linguistic information, such as syn-
tactic information, usually incur significant compu-
tational costs (Uzuner et al, 2004). Our approach
identifies useful linguistic information without in-
curring the computational cost of full text pars-
ing; it uses context-free grammars to perform high-
level syntactic analysis of part-of-speech tagged
text (Brill, 1992). It turns out that such a level of
analysis is sufficient to capture syntactic informa-
tion related to creative aspects of writing; this in
turn helps improve recognition of paraphrased doc-
uments. The results presented here show that ex-
traction of useful linguistic information for text clas-
sification purposes does not have to be computa-
tionally prohibitively expensive, and that despite the
tradeoff between the accuracy of features and com-
putational efficiency, we can extract linguistically-
informed features without full parsing.
3 Identifying Creative Aspects of Writing
In this paper, we first identify linguistic elements
of expression and then study patterns in the use of
these elements to recognize a work even when it is
paraphrased. Translated literary works provide ex-
amples of linguistic elements that differ in expres-
sion but convey similar content. These works pro-
vide insight into the linguistic elements that capture
expression. For example, consider the following se-
mantically equivalent excerpts from three different
translations of Madame Bovary by Gustave Flaubert.
Excerpt 1: ?Now Emma would often take it into
her head to write him during the day. Through her
window she would signal to Justin, and he would
whip off his apron and fly to la huchette. And when
Rodolphe arrived in response to her summons, it
was to hear that she was miserable, that her husband
was odious, that her life was a torment.? (Trans-
lated by Unknown1.)
38
Excerpt 2: ?Often, even in the middle of the day,
Emma suddenly wrote to him, then from the win-
dow made a sign to Justin, who, taking his apron
off, quickly ran to la huchette. Rodolphe would
come; she had sent for him to tell him that she was
bored, that her husband was odious, her life fright-
ful.? (Translated by Aveling.)
Excerpt 3: ?Often, in the middle of the day, Emma
would take up a pen and write to him. Then she
would beckon across to Justin, who would off with
his apron in an instant and fly away with the letter
to la huchette. And Rodolphe would come. She
wanted to tell him that life was a burden to her, that
she could not endure her husband and that things
were unbearable.? (Translated by Unknown2.)
Inspired by syntactic differences displayed in
such parallel translations, we identified a novel set
of syntactic features that relate to how people con-
vey content.
3.1 Syntactic Elements of Expression
We hypothesize that given particular content, au-
thors choose from a set of semantically equivalent
syntactic constructs to express this content. To para-
phrase a work without changing content, people
try to interchange semantically equivalent syntactic
constructs; patterns in the use of various syntactic
constructs can be sufficient to indicate copying.
Our observations of the particular expressive
choices of authors in a corpus of parallel translations
led us to define syntactic elements of expression in
terms of sentence-initial and -final phrase structures,
semantic classes and argument structures of verb
phrases, and syntactic classes of verb phrases.
3.1.1 Sentence-initial and -final phrase
structures
The order of phrases in a sentence can shift the
emphasis of a sentence, can attract attention to par-
ticular pieces of information and can be used as an
expressive tool.
1 (a) Martha can finally put some money in the bank.
(b) Martha can put some money in the bank, finally.
(c) Finally, Martha can put some money in the bank.
2 (a) Martha put some money in the bank on Friday.
(b) On Friday, Martha put some money in the bank.
(c) Some money is what Martha put in the bank on Fri-
day.
(d) In the bank is where Martha put some money on
Friday.
The result of such expressive changes affect the
distributions of various phrase types in sentence-
initial and -final positions; studying these distribu-
tions can help us capture some elements of expres-
sion. Despite its inability to detect the structural
changes that do not affect the sentence-initial and
-final phrase types, this approach captures some of
the phrase-level expressive differences between se-
mantically equivalent content; it also captures dif-
ferent sentential structures, including question con-
structs, imperatives, and coordinating and subordi-
nating conjuncts.
3.1.2 Semantic Classes of Verbs
Levin (1993) observed that verbs that exhibit sim-
ilar syntactic behavior are also related semantically.
Based on this observation, she sorted 3024 verbs
into 49 high-level semantic classes. Verbs of ?send-
ing and carrying?, such as convey, deliver,
move, roll, bring, carry, shuttle, and
wire, for example, are collected under this seman-
tic class and can be further broken down into five
semantically coherent lower-level classes which in-
clude ?drive verbs?, ?carry verbs?, ?bring and take
verbs?, ?slide verbs?, and ?send verbs?. Each of
these lower-level classes represents a group of verbs
that have similarities both in semantics and in syn-
tactic behavior, i.e., they can grammatically un-
dergo similar syntactic alternations. For example,
?send verbs? can be seen in the following alterna-
tions (Levin, 1993):
1. Base Form
? Nora sent the book to Peter.
? NP + V + NP + PP.
2. Dative Alternation
? Nora sent Peter the book.
? NP + V + NP + NP.
Semantics of verbs in general, and Levin?s verb
classes in particular, have previously been used for
evaluating content and genre similarity (Hatzivas-
siloglou et al, 1999). In addition, similar seman-
tic classes of verbs were used in natural language
processing applications: START was the first nat-
ural language question answering system to use
such verb classes (Katz and Levin, 1988). We use
39
Levin?s semantic verb classes to describe the ex-
pression of an author in a particular work. We as-
sume that semantically similar verbs are often used
in semantically similar syntactic alternations; we
describe part of an author?s expression in a par-
ticular work in terms of the semantic classes of
verbs she uses and the particular argument struc-
tures, e.g., NP + V + NP + PP, she prefers for them.
As many verbs belong to multiple semantic classes,
to capture the dominant semantic verb classes in
each document we credit all semantic classes of all
observed verbs. We extract the argument structures
from part of speech tagged text, using context-free
grammars (Uzuner, 2005).
3.1.3 Syntactic Classes of Verbs
Levin?s verb classes include exclusively ?non-
embedding verbs?, i.e., verbs that do not take
clausal arguments, and need to be supplemented by
classes of ?embedding verbs? that do take such argu-
ments. Alexander and Kunz (1964) identified syn-
tactic classes of embedding verbs, collected a com-
prehensive set of verbs for each class, and described
the identified verb classes with formulae written in
terms of phrasal and clausal elements, such as verb
phrase heads (Vh), participial phrases (Partcp.), in-
finitive phrases (Inf.), indicative clauses (IS), and
subjunctives (Subjunct.). We used 29 of the more
frequent embedding verb classes and identified their
distributions in different works. Examples of these
verb classes are shown in Table 1. Further examples
can be found in (Uzuner, 2005; Uzuner and Katz,
2005).
Syntactic Formula Example
NP + Vh + NP + from The belt kept him from dying.
+ Partcp.
NP + Vh + that + IS He admitted that he was guilty.
NP + Vh + that I request that she go alone.
+ Subjunct.
NP + Vh + to + Inf. My father wanted to travel.
NP + Vh + wh + IS He asked if they were alone.
NP + pass. + Partcp. He was seen stealing.
Table 1: Sample syntactic formulae and examples of
embedding verb classes.
We study the syntax of embedding verbs by iden-
tifying their syntactic class and the structure of
their observed embedded arguments. After identi-
fying syntactic and semantic characteristics of verb
phrases, we combine these features to create fur-
ther elements of expression, e.g., syntactic classes
of embedding verbs and the classes of semantic non-
embedding verbs they co-occur with.
4 Evaluation
We tested sentence-initial and -final phrase struc-
tures, semantic and syntactic classes of verbs, and
structure of verb arguments, i.e., syntactic elements
of expression, in paraphrase recognition and in pla-
giarism detection in two ways:
? Recognizing titles even when they are para-
phrased, and
? Recognizing pairs of book chapters that are
paraphrases of each other.
For our experiments, we split books into chapters,
extracted all relevant features from each chapter, and
normalized them by the length of the chapter.
4.1 Recognizing Titles
Frequently, people paraphrase parts of rather than
complete works. For example, they may paraphrase
chapters or paragraphs from a work rather than the
whole work. We tested the effectiveness of our
features on recognizing paraphrased components of
works by focusing on chapter-level excerpts (smaller
components than chapters have very sparse vectors
given our sentence-level features and will be the
foci of future research) and using boosted decision
trees (Witten and Frank, 2000).
Our goal was to recognize chapters from the ti-
tles in our corpus even when some titles were para-
phrased into multiple books; in this context, titles
are original works and paraphrased books are trans-
lations of these titles. For this, we assumed the ex-
istence of one legitimate book from each title. We
used this book to train a model that captured the syn-
tactic elements of expression used in this title. We
used the remaining paraphrases of the title (i.e., the
remaining books paraphrasing the title) as the test
set?these paraphrases are considered to be plagia-
rized copies and should be identified as such given
the model for the title.
40
4.1.1 Data
Real life plagiarism data is difficult to obtain.
However, English translations of foreign titles ex-
ist and can be obtained relatively easily. Titles that
have been translated on different occasions by dif-
ferent translators and that have multiple translations
provide us with examples of books that paraphrase
the same content and serve as our surrogate for pla-
giarism data.
To evaluate syntactic elements of expression on
recognizing paraphrased chapters from titles, we
compared the performance of these features with
tfidf-weighted keywords on a 45-way classifica-
tion task. The corpus used for this experiment
included 49 books from 45 titles. Of the 45 ti-
tles, 3 were paraphrased into a total of 7 books
(3 books paraphrased the title Madame Bovary, 2
books paraphrased 20000 Leagues, and 2 books
paraphrased The Kreutzer Sonata). The remaining
titles included works from J. Austen (1775-1817),
C. Dickens (1812-1870), F. Dostoyevski (1821-
1881), A. Doyle (1859-1887), G. Eliot (1819-
1880), G. Flaubert (1821-1880), T. Hardy (1840-
1928), V. Hugo (1802-1885), W. Irving (1789-
1859), J. London (1876-1916), W. M. Thack-
eray (1811-1863), L. Tolstoy (1828-1910), I. Tur-
genev (1818-1883), M. Twain (1835-1910), and
J. Verne (1828-1905).
4.1.2 Baseline Features
The task described in this section focuses on rec-
ognizing paraphrases of works based on the way
they are written. Given the focus of authorship attri-
bution literature on ?the way people write?, to eval-
uate the syntactic elements of expression on recog-
nizing paraphrased chapters of a work, we compared
these features against features frequently used in au-
thorship attribution as well as features used in con-
tent recognition.
Tfidf-weighted Keywords: Keywords, i.e., con-
tent words, are frequently used in content-based text
classification and constitute one of our baselines.
Function Words: In studies of authorship at-
tribution, many researchers have taken advantage
of the differences in the way authors use function
words (Mosteller and Wallace, 1963; Peng and Hen-
gartner, 2002). In our studies, we used a set of 506
function words (Uzuner, 2005).
Distributions of Word Lengths and Sentence
Lengths: Distributions of word lengths and sen-
tence lengths have been used in the literature for
authorship attribution (Mendenhall, 1887; Williams,
1975; Holmes, 1994). We include these features
in our sets of baselines along with information
about means and standard deviations of sentence
lengths (Holmes, 1994).
Baseline Linguistic Features: Sets of surface,
syntactic, and semantic features have been found to
be useful for authorship attribution and have been
adopted here as baseline features. These features
included: the number of words and the number of
sentences in the document; type?token ratio; aver-
age and standard deviation of the lengths of words
(in characters) and of the lengths of sentences (in
words) in the document; frequencies of declara-
tive sentences, interrogatives, imperatives, and frag-
mental sentences; frequencies of active voice sen-
tences, be-passives and get-passives; frequencies of
?s-genitives, of-genitives and of phrases that lack
genitives; frequency of overt negations, e.g., ?not?,
?no?, etc.; and frequency of uncertainty markers,
e.g., ?could?, ?possibly?, etc.
4.1.3 Experiment
To recognize chapters from the titles in our corpus
even when some titles were paraphrased into mul-
tiple books, we randomly selected 40?50 chapters
from each title. We used 60% of the selected chap-
ters from each title for training and the remaining
40% for testing. For paraphrased titles, we selected
training chapters from one of the paraphrases and
testing chapters from the remaining paraphrases. We
repeated this experiment three times; at each round,
a different paraphrase was chosen for training and
the rest were used for testing.
Our results show that, on average, syntactic ele-
ments of expression accurately recognized compo-
nents of titles 73% of the time and significantly out-
performed all baselines3 (see middle column in Ta-
ble 2).4
3The tfidf-weighted keywords used in this experiment do not
include proper nouns. These words are unique to each title and
can be easily replaced without changing content or expression
in order to trick a plagiarism detection system that would rely
on proper nouns.
4For the corpora used in this paper, a difference of 4% or
more is statistically significant with ? = 0.05.
41
Feature Set Avg. Avg.
accuracy accuracy
(complete (para-
corpus) phrases)
only
Syntactic elements of expression 73% 95%
Function words 53% 34%
Tfidf-weighted keywords 47% 38%
Baseline linguistic 40% 67%
Dist. of word length 18% 54%
Dist. of sentence length 12% 17%
Table 2: Classification results (on the test set) for
recognizing titles in the corpus even when some ti-
tles are paraphrased (middle column) and classifi-
cation results only on the paraphrased titles (right
column). In either case, random chance would rec-
ognize a paraphrased title 2% of the time.
The right column in Table 2 shows that the syntac-
tic elements of expression accurately recognized on
average 95% of the chapters taken from paraphrased
titles. This finding implies that some of our elements
of expression are common to books that are derived
from the same title. This commonality could be due
to the similarity of their content or due to the under-
lying expression of the original author.
4.2 Recognizing Pairs of Paraphrased
Chapters
Experiments in Section 4.1 show that we can use
syntactic elements of expression to recognize titles
and their components based on the way they are
written even when some works are paraphrased. In
this section, our goal is to identify pairs of chapters
that paraphrase the same content, i.e., chapter 1 of
translation 1 of Madame Bovary and chapter 1 of
translation 2 of Madame Bovary. For this evalua-
tion, we used a similar approach to that presented by
Nahnsen et al (2005).
4.2.1 Data
Our data for this experiment included 47 chap-
ters from each of two translations of 20000 Leagues
under the Sea (Verne), 35 chapters from each of 3
translations of Madame Bovary (Flaubert), 28 chap-
ters from each of two translations of The Kreutzer
Sonata (Tolstoy), and 365 chapters from each of 2
translations of War and Peace (Tolstoy). Pairing
up the chapters from these titles provided us with
more than 1,000,000 chapter pairs, of which approx-
imately 1080 were paraphrases of each other.5
4.2.2 Experiment
For experiments on finding pairwise matches, we
used similarity of vectors of tfidf-weighted key-
words;6 and the multiplicative combination of the
similarity of vectors of tfidf-weighed keywords of
works with the similarity of vectors of syntactic ele-
ments of expression of these works. We used cosine
to evaluate the similarity of the vectors of works. We
omitted the remaining baseline features from this
experiment?they are features that are common to
majority of the chapters from each book, they do
not relate to the task of finding pairs of chapters that
could be paraphrases of each other.
We ranked all chapter pairs in the corpus based
on their similarity. From this ranked list, we iden-
tified the top n most similar pairs and predicted that
they are paraphrases of each other. We evaluated our
methods with precision, recall, and f-measure.7
Figure 1: Precision.
Figures 1, 2, and 3 show that syntactic elements
of expression improve the performance of tfidf-
weighted keywords in recognizing pairs of para-
phrased chapters significantly in terms of precision,
recall, and f-measure for all n; in all of these figures,
the blue line marked syn tdf represents the per-
formance of tfidf-weighted keywords enhanced with
5Note that this number double-counts the paraphrased pairs;
however, this fact is immaterial for our discussion.
6In this experiment, proper nouns are included in the
weighted keywords.
7The ground truth marks only the same chapter from two
different translations of the same title as similar, i.e., chapter x
of translation 1 of Madame Bovary and chapter y of translation
2 of Madame Bovary are similar only when x = y.
42
Figure 2: Recall.
syntactic elements of expression. More specifically,
the peak f-measure for tfidf-weighted keywords is
approximately 0.77 without contribution from syn-
tactic elements of expression. Adding information
about similarity of syntactic features to cosine sim-
ilarity of tfidf-weighted keywords boosts peak f-
measure value to approximately 0.82.8 Although
the f-measure of both representations degrade when
n > 1100, this degradation is an artifact of the eval-
uation metric: the corpus includes only 1080 similar
pairs, at n > 1100, recall is very close to 1, and
therefore increasing n hurts overall performance.
Figure 3: F-measure.
5 Conclusion
Plagiarism is a problem at all levels of education.
Increased availability of digital versions of works
makes it easier to plagiarize others? work and the
large volumes of information available on the web
makes it difficult to identify cases of plagiarism.
8The difference is statistically significant at ? = 0.05.
To identify plagiarism even when works are para-
phrased, we propose studying the use of particular
syntactic constructs as well as keywords in docu-
ments.
This paper shows that syntactic information can
help recognize works based on the way they are
written. Syntactic elements of expression that fo-
cus on the changes in the phrase structure of works
help identify paraphrased components of a title. The
same features help improve identification of pairs
of chapters that are paraphrases of each other, de-
spite the content these chapters share with the rest
of the chapters taken from the same title. The re-
sults presented in this paper are based on experi-
ments that use translated novels as surrogate for pla-
giarism data. Our future work will extend our study
to real life plagiarism data.
6 Acknowledgements
The authors would like to thank Sue Felshin for her
insightful comments. This work is supported in part
by the Advanced Research and Development Activ-
ity as part of the AQUAINT research program.
References
D. Alexander and W. J. Kunz. 1964. Some classes of
verbs in English. In Linguistics Research Project. In-
diana University, June.
E. Brill. 1992. A simple rule-based part of speech tag-
ger. In Proceedings of the 3rd Conference on Applied
Natural Language Processing.
V. Hatzivassiloglou, J. Klavans, and E. Eskin. 1999. De-
tecting similarity by applying learning over indicators.
In Proceedings of the 37th Annual Meeting of the ACL.
D. I. Holmes. 1994. Authorship attribution. Computers
and the Humanities, 28.
B. Katz and B. Levin. 1988. Exploiting lexical reg-
ularities in designing natural language systems. In
Proceedings of the 12th Int?l Conference on Compu-
tational Linguistics (COLING ?88).
B. Levin. 1993. English Verb Classes and Alternations.
A Preliminary Investigation. University of Chicago
Press.
T. C. Mendenhall. 1887. Characteristic curves of com-
position. Science, 11.
43
F. Mosteller and D. L. Wallace. 1963. Inference in an au-
thorship problem. Journal of the American Statistical
Association, 58(302).
T. Nahnsen, O?. Uzuner, and B. Katz. 2005. Lexical
chains and sliding locality windows in content-based
text similarity detection. CSAIL Memo, AIM-2005-
017.
R. D. Peng and H. Hengartner. 2002. Quantitative analy-
sis of literary styles. The American Statistician, 56(3).
H. S. Sichel. 1974. On a distribution representing
sentence-length in written prose. Journal of the Royal
Statistical Society (A), 137.
O?. Uzuner and B. Katz. 2005. Capturing expression us-
ing linguistic information. In Proceedings of the 20th
National Conference on Artificial Intelligence (AAAI-
05).
O?. Uzuner, R. Davis, and B. Katz. 2004. Using em-
pirical methods for evaluating expression and content
similarity. In Proceedings of the 37th Hawaiian Inter-
national Conference on System Sciences (HICSS-37).
IEEE Computer Society.
O?. Uzuner. 2005. Identifying Expression Fingerprints
Using Linguistic Information. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
C. B. Williams. 1975. Mendenhall?s studies of word-
length distribution in the works of Shakespeare and
Bacon. Biometrika, 62(1).
I. H. Witten and E. Frank. 2000. Data Mining: Practical
Machine Learning Tools with Java Implementations.
Morgan Kaufmann, San Francisco.
44
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 65?73,
New York, June 2006. c?2006 Association for Computational Linguistics
Role of Local Context in Automatic Deidentification
of Ungrammatical, Fragmented Text
Tawanda Sibanda
CSAIL
Massachusetts Institute of Technology
Cambridge, MA 02139
tawanda@mit.edu
Ozlem Uzuner
Department of Information Studies
College of Computing and Information
University at Albany, SUNY
Albany, NY 12222
ouzuner@albany.edu
Abstract
Deidentification of clinical records is a
crucial step before these records can be
distributed to non-hospital researchers.
Most approaches to deidentification rely
heavily on dictionaries and heuristic rules;
these approaches fail to remove most per-
sonal health information (PHI) that cannot
be found in dictionaries. They also can fail
to remove PHI that is ambiguous between
PHI and non-PHI.
Named entity recognition (NER) tech-
nologies can be used for deidentification.
Some of these technologies exploit both
local and global context of a word to iden-
tify its entity type. When documents are
grammatically written, global context can
improve NER.
In this paper, we show that we can dei-
dentify medical discharge summaries us-
ing support vector machines that rely on a
statistical representation of local context.
We compare our approach with three dif-
ferent systems. Comparison with a rule-
based approach shows that a statistical
representation of local context contributes
more to deidentification than dictionaries
and hand-tailored heuristics. Compari-
son with two well-known systems, SNoW
and IdentiFinder, shows that when the lan-
guage of documents is fragmented, local
context contributes more to deidentifica-
tion than global context.
1 Introduction
Medical discharge summaries contain information
that is useful to clinical researchers who study the
interactions between, for example, different med-
ications and diseases. However, these summaries
include explicit personal health information (PHI)
whose release would jeopardize privacy. In the
United States, the Health Information Portability
and Accountability Act (HIPAA) provides guide-
lines for protecting the confidentiality of health care
information. HIPAA lists seventeen pieces of textual
PHI of which the following appear in medical dis-
charge summaries: first and last names of patients,
their health proxies, and family members; doctors?
first and last names; identification numbers; tele-
phone, fax, and pager numbers; hospital names; ge-
ographic locations; and dates. Removing PHI from
medical documents is the goal of deidentification.
This paper presents a method based on a statis-
tical representation of local context for automati-
cally removing explicit PHI from medical discharge
summaries, despite the often ungrammatical, frag-
mented, and ad hoc language of these documents,
even when some words in the documents are am-
biguous between PHI and non-PHI (e.g., ?Hunting-
ton? as the name of a person and as the name of
a disease), and even when some of the PHI cannot
be found in dictionaries (e.g., misspelled and/or for-
eign names). This method differs from traditional
approaches to deidentification in its independence
from dictionaries and hand-tailored heuristics. It
applies statistical named entity recognition (NER)
methods to the more challenging task of deidenti-
65
fication but differs from traditional NER approaches
in its heavy reliance on a statistical representation of
local context. Finally, this approach targets all PHI
that appear in medical discharge summaries. Experi-
ments reported in this paper show that context plays
a more important role in deidentification than dic-
tionaries, and that a statistical representation of lo-
cal context contributes more to deidentification than
global context.
2 Related Work
In the literature, named entities such as people,
places, and organizations mentioned in news arti-
cles have been successfully identified by various ap-
proaches (Bikel et al, 1999; McCallum et al, 2000;
Riloff and Jones, 1996; Collins and Singer, 1999;
Hobbs et al, 1996). Most of these approaches are
tailored to a particular domain, e.g., understanding
disaster news; they exploit both the characteristics
of the entities they focus on and the contextual clues
related to these entities.
In the biomedical domain, NER has focused on
identification of biological entities such as genes
and proteins (Collier et al, 2000; Yu et al, 2002).
Various statistical approaches, e.g., a maximum
entropy model (Finkel et al, 2004), HMMs and
SVMs (GuoDong et al, 2005), have been used with
various feature sets including surface and syntac-
tic features, word formation patterns, morphologi-
cal patterns, part-of-speech tags, head noun triggers,
and coreferences.
Deidentification refers to the removal of identi-
fying information from records. Some approaches
to deidentification have focused on particular cat-
egories of PHI, e.g., Taira et al focused on only
patient names (2002), Thomas et al focused on
proper names including doctors? names (2002). For
full deidentification, i.e., removal of all PHI, Gupta
et al used ?a complex set of rules, dictionaries,
pattern-matching algorithms, and Unified Medical
Language System? (2004). Sweeney?s Scrub sys-
tem employed competing algorithms that used pat-
terns and lexicons to find PHI. Each of the algo-
rithms included in her system specialized in one
kind of PHI, each calculated the probability that a
given word belonged to the class of PHI that it spe-
cialized in, and the algorithm with the highest prece-
dence and the highest probability labelled the given
word. This system identified 99-100% of all PHI in
the test corpus of patient records and letters to physi-
cians (1996).
We use a variety of features to train a support
vector machine (SVM) that can automatically ex-
tract local context cues and can recognize PHI (even
when some PHI are ambiguous between PHI and
non-PHI, and even when PHI do not appear in dic-
tionaries). We compare this approach with three
others: a heuristic rule-based approach (Douglass,
2005), the SNoW (Sparse Network of Winnows)
system?s NER component (Roth and Yih, 2002), and
IdentiFinder (Bikel et al, 1999). The heuristic rule-
based system relies heavily on dictionaries. SNoW
and IdentiFinder consider some representation of the
local context of words; they also rely on informa-
tion about global context. Local context helps them
recognize stereotypical names and name structures.
Global context helps these systems update the prob-
ability of observing a particular entity type based on
the other entity types contained in the sentence. We
hypothesize that, given the mostly fragmented and
ungrammatical nature of discharge summaries, local
context will be more important for deidentification
than global context. We further hypothesize that lo-
cal context will be a more reliable indication of PHI
than dictionaries (which can be incomplete). The re-
sults presented in this paper show that SVMs trained
with a statistical representation of local context out-
perform all baselines. In other words, a classifier
that relies heavily on local context (very little on
dictionaries, and not at all on global context) out-
performs classifiers that rely either on global con-
text or dictionaries (but make much less use of lo-
cal context). Global context cannot contribute much
to deidentification when the language of documents
is fragmented; dictionaries cannot contribute to dei-
dentification when PHI are either missing from dic-
tionaries or are ambiguous between PHI and non-
PHI. Local context remains a reliable indication of
PHI under these circumstances.
The features used for our SVM-based system can
be enriched in order to automatically acquire more
and varied local context information. The features
discussed in this paper have been chosen because of
their simplicity and effectiveness on both grammati-
cal and ungrammatical free text.
66
3 Corpora
Discharge summaries are the reports generated by
medical personnel at the end of a patient?s hospi-
tal stay and contain important information about the
patient?s health. Linguistic processing of these doc-
uments is challenging, mainly because these reports
are full of medical jargon, acronyms, shorthand no-
tations, misspellings, ad hoc language, and frag-
ments of sentences. Our goal is to identify the PHI
used in discharge summaries even when text is frag-
mented and ad hoc, even when many words in the
summaries are ambiguous between PHI and non-
PHI, and even when many PHI contain misspelled
or foreign words.
In this study, we worked with various corpora
consisting of discharge summaries. One of these
corpora was obtained already deidentified1; i.e.,
(many) PHI (and some non-PHI) found in this cor-
pus had been replaced with the generic placeholder
[REMOVED]. An excerpt from this corpus is below:
HISTORY OF PRESENT ILLNESS: The patient
is a 77-year-old-woman with long standing hyper-
tension who presented as a Walk-in to me at the
[REMOVED] Health Center on [REMOVED]. Re-
cently had been started q.o.d. on Clonidine since
[REMOVED] to taper off of the drug. Was told to
start Zestril 20 mg. q.d. again. The patient was sent
to the [REMOVED] Unit for direct admission for
cardioversion and anticoagulation, with the Cardi-
ologist, Dr. [REMOVED] to follow.
SOCIAL HISTORY: Lives alone, has one daughter
living in [REMOVED]. Is a non-smoker, and does
not drink alcohol.
HOSPITAL COURSE AND TREATMENT: Dur-
ing admission, the patient was seen by Cardiology,
Dr. [REMOVED], was started on IV Heparin, So-
talol 40 mg PO b.i.d. increased to 80 mg b.i.d.,
and had an echocardiogram. By [REMOVED] the
patient had better rate control and blood pressure
control but remained in atrial fibrillation. On [RE-
MOVED], the patient was felt to be medically sta-
ble.
...
We hand-annotated this corpus and experimented
with it in several ways: we used it to generate
a corpus of discharge summaries in which the
[REMOVED] tokens were replaced with appropri-
ate, fake PHI obtained from dictionaries2 (Douglass,
1Authentic clinical data is very difficult to obtain for privacy
reasons; therefore, the initial implementation of our system was
tested on previously deidentified data that we reidentified.
2e.g., John Smith initiated radiation therapy ...
2005); we used it to generate a second corpus in
which most of the [REMOVED] tokens and some
of the remaining text were appropriately replaced
with lexical items that were ambiguous between PHI
and non-PHI3; we used it to generate another cor-
pus in which all of the [REMOVED] tokens corre-
sponding to names were replaced with appropriately
formatted entries that could not be found in dictio-
naries4. For all of these corpora, we generated real-
istic substitutes for the [REMOVED] tokens using
dictionaries (e.g., a dictionary of names from US
Census Bureau) and patterns (e.g., names of people
could be of the formats, ?Mr. F. Lastname?, ?First-
name Lastname?, ?Lastname?, ?F. M. Lastname?,
etc.; dates could appear as ?dd/mm/yy?, ?dd Mon-
thName, yyyy?, ?ddth of MonthName, yyyy?, etc.).
In addition to these reidentified corpora (i.e., cor-
pora generated from previously deidentified data),
we also experimented with authentic discharge sum-
maries5. The approximate distributions of PHI in the
reidentified corpora and in the authentic corpus are
shown in Table 1.
Class No. in reidentified No. in authentic
summaries summaries
Non-PHI 17872 112720
Patient 1047 287
Doctor 311 730
Location 24 84
Hospital 592 651
Date 735 1933
ID 36 477
Phone 39 32
Table 1: Distribution of different PHI (in terms of number of
words) in the corpora.
4 Baseline Approaches
4.1 Rule-Based Baseline: Heuristic+Dictionary
Traditional deidentification approaches rely heavily
on dictionaries and hand-tailored heuristics.
3e.g., D. Sessions initiated radiation therapy...
4e.g., O. Ymfgkstjj initiated radiation therapy ...
5We obtained authentic discharge summaries with real PHI
in the final stages of this project.
67
We obtained one such system (Douglass, 2005)
that used three kinds of dictionaries:
? PHI lookup tables for female and male first
names, last names, last name prefixes, hospital
names, locations, and states.
? A dictionary of ?common words? that should
never be classified as PHI.
? Lookup tables for context clues such as titles,
e.g., Mr.; name indicators, e.g., proxy, daugh-
ter; location indicators, e.g., lives in.
Given these dictionaries, this system identifies key-
words that appear in the PHI lookup tables but do
not occur in the common words list, finds approx-
imate matches for possibly misspelled words, and
uses patterns and indicators to find PHI.
4.2 SNoW
SNoW is a statistical classifier that includes a NER
component for recognizing entities and their rela-
tions. To create a hypothesis about the entity type of
a word, SNoW first takes advantage of ?words, tags,
conjunctions of words and tags, bigram and trigram
of words and tags?, number of words in the entity,
bigrams of words in the entity, and some attributes
such as the prefix and suffix, as well as informa-
tion about the presence of the word in a dictionary
of people, organization, and location names (Roth
and Yih, 2002). After this initial step, it uses the
possible relations of the entity with other entities in
the sentence to strengthen or weaken its hypothe-
sis about the entity?s type. The constraints imposed
on the entities and their relationships constitute the
global context of inference. Intuitively, information
about global context and constraints imposed on the
relationships of entities should improve recognition
of both entities and relations. Roth and Yih (2002)
present results that support this hypothesis.
SNoW can recognize entities that correspond to
people, locations, and organizations. For deidenti-
fication purposes, all of these entities correspond to
PHI; however, they do not constitute a comprehen-
sive set. We evaluated SNoW only on the PHI it is
built to recognize. We trained and tested its NER
component using ten-fold cross-validation on each
of our corpora.
4.3 IdentiFinder
IdentiFinder uses Hidden Markov Models to learn
the characteristics of names of entities, including
people, locations, geographic jurisdictions, organi-
zations, dates, and contact information (Bikel et al,
1999). For each named entity class, this system
learns a bigram language model which indicates the
likelihood that a sequence of words belongs to that
class. This model takes into consideration features
of words, such as whether the word is capitalized, all
upper case, or all lower case, whether it is the first
word of the sentence, or whether it contains digits
and punctuation. Thus, it captures the local context
of the target word (i.e., the word to be classified; also
referred to as TW). To find the names of all entities,
the system finds the most likely sequence of entity
types in a sentence given a sequence of words; thus,
it captures the global context of the entities in a sen-
tence.
We obtained this system pre-trained on a news
corpus and applied it to our corpora. We mapped
its entity tags to our PHI and non-PHI labels. Ad-
mittedly, testing IdentiFinder on the discharge sum-
maries puts this system at a disadvantage compared
to the other statistical approaches. However, despite
this shortcoming, IdentiFinder helps us evaluate the
contribution of global context to deidentification.
5 SVMs with Local Context
We hypothesize that systems that rely on dictionar-
ies and hand-tailored heuristics face a major chal-
lenge when particular PHI can be used in many dif-
ferent contexts, when PHI are ambiguous, or when
the PHI cannot be found in dictionaries. We further
hypothesize that given the ungrammatical and ad
hoc nature of our data, despite being very powerful
systems, IdentiFinder and SNoW may not provide
perfect deidentification. In addition to being very
fragmented, discharge summaries do not present in-
formation in the form of relations between entities,
and many sentences contain only one entity. There-
fore, the global context utilized by IdentiFinder and
SNoW cannot contribute reliably to deidentification.
When run on discharge summaries, the strength of
these systems comes from their ability to recognize
the structure of the names of different entity types
and the local contexts of these entities.
68
Discharge summaries contain patterns that can
serve as local context. Therefore, we built an SVM-
based system that, given a target word (TW), would
accurately predict whether the TW was part of PHI.
We used a development corpus to find features that
captured as much of the immediate context of the
TW as possible, paying particular attention to cues
human annotators found useful for deidentification.
We added to this some surface characteristics for the
TW itself and obtained the following features: the
TW itself, the word before, and the word after (all
lemmatized); the bigram before and the bigram af-
ter TW (lemmatized); the part of speech of TW, of
the word before, and of the word after; capitalization
of TW; length of TW; MeSH ID of the noun phrase
containing TW (MeSH is a dictionary of Medical
Subject Headings and is a subset of the Unified Med-
ical Language System (UMLS) of the National Li-
brary of Medicine); presence of TW, of the word
before, and of the word after TW in the name, lo-
cation, hospital, and month dictionaries; the heading
of the section in which TW appears, e.g., ?History
of Present Illness?; and, whether TW contains ?-? or
?/? characters. Note that some of these features, e.g.,
capitalization and punctuation within TW, were also
used in IdentiFinder.
We used the SVM implementation provided by
LIBSVM (Chang and Lin, 2001) with a linear ker-
nel to classify each word in the summaries as ei-
ther PHI or non-PHI based on the above-listed fea-
tures. We evaluated this system using ten-fold cross-
validation.
6 Evaluation
Local context contributes differently to each of the
four deidentification systems. Our SVM-based ap-
proach uses only local context. The heuristic, rule-
based system relies heavily on dictionaries. Identi-
Finder uses a simplified representation of local con-
text and adds to this information about the global
context as represented by transition probabilities be-
tween entities in the sentence. SNoW uses local con-
text as well, but it also makes an effort to benefit
from relations between entities. Given the difference
in the strengths of these systems, we compared their
performance on both the reidentified and authentic
corpora (see Section 3). We hypothesized that given
the nature of medical discharge summaries, Iden-
tiFinder would not be able to find enough global
context and SNoW would not be able to make use
of relations (because many sentences in this cor-
pus contain only one entity). We further hypothe-
sized that when the data contain words ambiguous
between PHI and non-PHI, or when the PHI cannot
be found in dictionaries, the heuristic, rule-based ap-
proach would perform poorly. In all of these cases,
SVMs trained with local context information would
be sufficient for proper deidentification.
To compare the SVM approach with Identi-
Finder, we evaluated both on PHI consisting of
names of people (i.e., patient and doctor names),
locations (i.e., geographic locations), and organiza-
tions (i.e., hospitals), as well as PHI consisting of
dates, and contact information (i.e., phone numbers,
pagers). We omitted PHI representing ID numbers
from this experiment in order to be fair to Identi-
Finder which was not trained on this category. To
compare the SVM approach with SNoW, we trained
both systems with only PHI consisting of names of
people, locations, and organizations, i.e., the entities
that SNoW was designed to recognize.
6.1 Deidentifying Reidentified and Authentic
Discharge Summaries
We first deidentified:
? Previously deidentified discharge summaries
into which we inserted invented but realistic
surrogates for PHI without deliberately intro-
ducing ambiguous words or words not found in
dictionaries, and
? Authentic discharge summaries with real PHI.
Our experiments showed that SVMs with local
context outperformed all other approaches. On the
reidentified corpus, SVMs gave an F-measure of
97.2% for PHI. In comparison, IdentiFinder, hav-
ing been trained on the news corpus, gave an F-
measure of 67.4% and was outperformed by the
heuristic+dictionary approach (see Table 2).6
6Note that in deidentification, recall is much more important
than precision. Low recall indicates that many PHI remain in
the documents and that there is high risk to patient privacy. Low
precision means that words that do not correspond to PHI have
also been removed. This hurts the integrity of the data but does
not present a risk to privacy.
69
We evaluated SNoW only on the three kinds
of entities it is designed to recognize. We cross-
validated it on our corpora and found that its per-
formance in recognizing people, locations, and or-
ganizations was 96.2% in terms of F-measure (see
Table 37). In comparison, our SVM-based system,
when retrained to only consider people, locations,
and organizations so as to be directly comparable to
SNoW, had an F-measure of 98%.8
Method Class P R F
SVM PHI 96.8% 97.7% 97.2%
IFinder PHI 60.2% 76.7% 67.4%
H+D PHI 88.9% 67.6% 76.8%
SVM Non-PHI 99.6% 99.5% 99.6%
IFinder Non-PHI 95.8% 91.4% 93.6%
H+D Non-PHI 95.2% 95.2% 95.2%
Table 2: Precision, Recall, and F-measure on reidentified dis-
charge summaries. IFinder refers to IdentiFinder, H+D refers to
heuristic+dictionary approach.
Method Class P R F
SVM PHI 97.7% 98.2% 98.0%
SNoW PHI 96.1% 96.2% 96.2%
SVM Non-PHI 99.8% 99.8% 99.8%
SNoW Non-PHI 99.6% 99.6% 99.6%
Table 3: Evaluation of SNoW and SVM on recognizing peo-
ple, locations, and organizations found in reidentified discharge
summaries.
Similarly, on the authentic discharge summaries,
the SVM approach outperformed all other ap-
proaches in recognizing PHI (see Tables 4 and 5).
6.2 Deidentifying Data with Ambiguous PHI
In discharge summaries, the same words can appear
both as PHI and as non-PHI. For example, in the
same corpus, the word ?Swan? can appear both as
the name of a medical device (i.e., ?Swan Catheter?)
and as the name of a person, etc. Ideally, we would
like to deidentify data even when many words in the
7The best performances are marked in bold in all of the ta-
bles in this paper.
8For all of the corpora presented in this paper, a performance
difference of 1% or more is statistically significant at ? = 0.05.
Method Class P R F
SVM PHI 97.5% 95.0% 96.2%
IFinder PHI 25.2% 45.2% 32.3%
H+D PHI 81.9% 87.6% 84.7%
SVM Non-PHI 99.8% 99.9% 99.9%
IFinder Non-PHI 97.1% 93.3% 95.2%
H+D Non-PHI 99.6% 99.6% 99.6%
Table 4: Evaluation on authentic discharge summaries.
Method Class P R F
SVM PHI 97.4% 93.8% 95.6%
SNoW PHI 93.7% 93.4% 93.6%
SVM Non-PHI 99.9% 100% 100%
SNoW Non-PHI 99.9% 99.9% 99.9%
Table 5: Evaluation of SNoW and SVM on authentic dis-
charge summaries.
corpus are ambiguous between PHI and non-PHI.
We hypothesize that given ambiguities in the data,
context will play an important role in determining
whether the particular instance of the word is PHI
and that given the many fragmented sentences in our
corpus, local context will be particularly useful. To
test these hypotheses, we generated a corpus by rei-
dentifying the previously deidentified corpus with
words that were ambiguous between PHI and non-
PHI, making sure to use each ambiguous word both
as PHI and non-PHI, and also making sure to cover
all acceptable formats of all PHI (see Section 3). The
resulting distribution of PHI is shown in Table 6.
Class Total # Words # Ambiguous Words
Non-PHI 19296 3781
Patient 1047 514
Doctor 311 247
Location 24 24
Hospital 592 82
Date 736 201
ID 36 0
Phone 39 0
Table 6: Distribution of PHI when some words are ambiguous
between PHI and non-PHI.
70
Our results showed that, on this corpus, the SVM-
based system accurately recognized 91.9% of all
PHI; its performance, measured in terms of F-
measure was also significantly better than all other
approaches both on the complete corpus containing
ambiguous entries (see Table 7 and Table 8) and only
on the ambiguous words in this corpus (see Table 9).
Method Class P R F
SVM PHI 92.0% 92.1% 92.0%
IFinder PHI 45.4% 71.4% 55.5%
H+D PHI 70.1% 46.6% 56.0%
SVM Non-PHI 98.9% 98.9% 98.9%
IFinder Non-PHI 95.0% 86.5% 90.1%
H+D Non-PHI 92.7% 92.7% 92.7%
Table 7: Evaluation on the corpus containing ambiguous
data.
Method Class P R F
SVM PHI 92.1% 92.8% 92.5%
SNoW PHI 91.6% 77% 83.7%
SVM Non-PHI 99.3% 99.2% 99.3%
SNoW Non-PHI 97.6% 99.3% 98.4%
Table 8: Evaluation of SNoW and SVM on ambiguous data.
Method Class P R F
SVM PHI 90.2% 87.5% 88.8%
IFinder PHI 55.8% 64.0% 59.6%
H+D PHI 59.8% 24.3% 34.6%
SNoW PHI 91.6% 82.9% 87.1%
SVM Non-PHI 90.5% 92.7% 91.6%
IFinder Non-PHI 69.0% 61.3% 64.9%
H+D Non-PHI 59.9% 87.4% 71.1%
SNoW Non-PHI 90.4% 95.5% 92.9%
Table 9: Evaluation only on ambiguous people, locations,
and organizations found in ambiguous data.
6.3 Deidentifying PHI Not Found in
Dictionaries
Some medical documents contain foreign or mis-
spelled names that need to be effectively removed.
To evaluate the different deidentification approaches
under such circumstances, we generated a corpus in
which the names of people, locations, and hospitals
were all random permutations of letters. The result-
ing words were not found in any dictionaries but fol-
lowed the general format of the entity name category
to which they belonged. The distribution of PHI in
this third corpus is in Table 10.
Class Total PHI PHI Not in Dict.
Non-PHI 17872 0
Patient 1045 1045
Doctor 302 302
Location 24 24
Hospital 376 376
Date 735 0
ID 36 0
Phone 39 0
Table 10: Distribution of PHI in the corpus where all PHI
associated with names are randomly generated so as not to be
found in dictionaries.
On this data set, dictionaries cannot contribute to
deidentification because none of the PHI appear in
dictionaries. Under these conditions, proper deiden-
tification relies completely on context. Our results
showed that SVM approach outperformed all other
approaches on this corpus also (Tables 11 and 12).
Method Class P R F
SVM PHI 94.0% 96.0% 95.0%
IFinder PHI 55.1% 65.5% 59.8%
H+D PHI 76.4% 27.8% 40.8%
SVM Non-PHI 99.4% 99.1% 99.3%
IFinder Non-PHI 94.4% 91.6% 92.9%
H+D Non-PHI 90.7% 90.7% 90.7%
Table 11: Evaluation on the corpus containing PHI not in
dictionaries.
Of only the PHI not found in dictionaries, 95.5%
was accurately identified by the SVM approach. In
comparison, the heuristic+dictionary approach ac-
curately identified those PHI that could not be found
in dictionaries 11.1% of the time, IdentiFinder rec-
ognized these entities 76.7% of the time and SNoW
gave an accuracy of 79% (see Table 13).
71
Method Class P R F
SVM PHI 93.9% 96.0% 95.0%
SNoW PHI 93.7% 79.0% 85.7%
SVM Non-PHI 99.6% 99.4% 99.5%
SNoW Non-PHI 98.0% 99.5% 98.7%
Table 12: Evaluation of SNoW and SVM on the people, loca-
tions, and organizations found in the corpus containing PHI not
found in dictionaries.
Method SVM IFinder SNoW H+D
Precision 95.5% 76.7% 79.0% 11.1%
Table 13: Precision on only the PHI not found in dictionaries.
6.4 Feature Importance
As hypothesized, in all experiments, the SVM-
based approach outperformed all other approaches.
SVM?s feature set included a total of 26 features,
12 of which were dictionary-related features (ex-
cluding MeSH). Information gain showed that the
most informative features for deidentification were
the TW, the bigram before TW, the bigram after TW,
the word before TW, and the word after TW.
Note that the TW itself is important for classifi-
cation; many of the non-PHI correspond to common
words that appear in the corpus frequently and the
SVM learns the fact that some words, e.g., the, ad-
mit, etc., are never PHI. In addition, the context of
TW (captured in the form of unigrams and bigrams
of words and part-of-speech tags surrounding TW)
contributes significantly to deidentification.
There are many ways of automatically capturing
context. In our data, unigrams and bigrams of words
and their part-of-speech tags seem to be sufficient
for a statistical representation of local context. The
global context, as represented within IdentiFinder
and SNoW, could not contribute much to deiden-
tification on this corpus because of the fragmented
nature of the language of these documents, because
most sentences in this corpus contain only one en-
tity, and because many sentences do not include ex-
plicit relations between entities. However, there is
enough structure in this data that can be captured by
local context; lack of relations between entities and
the inability to capture global context do not hold us
back from almost perfect deidentification.
7 Conclusion
We presented a set of experimental results that show
that local context contributes more to deidentifica-
tion than dictionaries and global context when work-
ing with medical discharge summaries. These docu-
ments are characterized by incomplete, fragmented
sentences, and ad hoc language. They use a lot
of jargon, many times omit subjects of sentences,
use entity names that can be misspelled or foreign
words, can include entity names that are ambigu-
ous between PHI and non-PHI, etc. Similar doc-
uments in many domains exist; our experiments
here show that even on such challenging corpora,
local context can be exploited to identify entities.
Even a rudimentary statistical representation of lo-
cal context, as captured by unigrams and bigrams of
lemmatized keywords and part-of-speech tags, gives
good results and outperforms more sophisticated ap-
proaches that rely on global context. The simplicity
of the representation of local context and the results
obtained using this simple representation are partic-
ularly promising for many tasks that require pro-
cessing ungrammatical and fragmented text where
global context cannot be counted on.
8 Acknowledgements
This publication was made possible by grant num-
ber R01-EB001659 from the National Institute
of Biomedical Imaging and Bioengineering; by
grant number N01-LM-3-3513 on National Multi-
Protocol Ensemble for Self-Scaling Systems for
Health from National Library of Medicine; and, by
grant number U54-LM008748 on Informatics for In-
tegrating Biology to the Bedside from National Li-
brary of Medicine.
We are grateful to Professor Peter Szolovits and
Dr. Boris Katz for their insights, and to Professor
Carol Doll, Sue Felshin, Gregory Marton, and Tian
He for their feedback on this paper.
References
J. J. Berman. 2002. Concept-Match Medical Data
Scrubbing: How Pathology Text Can Be Used in
Research. Archives of Pathology and Laboratory
Medicine, 127(6).
D. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999.
72
An Algorithm That Learns What?s in a Name. Ma-
chine Learning Journal Special Issue on Natural Lan-
guage Learning, 34(1/3).
C. Chang and C. Lin. 2001. LIBSVM: a Library for Sup-
port Vector Machines.
N. Collier, C. Nobata, and J. Tsujii. 2000. Extracting
the Names of Genes and Gene Products with a Hidden
Markov Model. Proceedings of COLING.
M. Collins and Y. Singer. 1999. Unsupervised Mod-
els for Named Entity Classification. Proceedings of
EMNLP.
J. Finkel, S. Dingare, H. Nguyen, M. Nissim, C. Man-
ning, and G. Sinclair. 2004. Exploiting Context for
Biomedical Entity Recognition: From Syntax to the
Web. Proceedings of Joint Workshop on Natural Lan-
guage Processing in Biomedicine and its Applications
at COLING.
R. Gaizauskas, G. Demetriou, P. Artymiuk, and P. Willett.
2003. Protein Structures and Information Extraction
from Biological Texts: The PASTA System. Bioinfor-
matics, 19(1).
Z. GuoDong, Z. Jie, S. Jian, S. Dan, T. ChewLim. 2005.
Recognizing Names in Biomedical Texts: a Machine
Learning Approach. Bioinformatics, 20(7).
D. Gupta, M. Saul, J. Gilbertson. 2004. Evalua-
tion of a Deidentification (De-Id) Software Engine to
Share Pathology Reports and Clinical Documents for
Research. American Journal of Clinical Pathology,
121(6).
J. R. Hobbs, D. E. Appelt, J. Bear, D. Israel, M.
Kameyama, M. Stickel, and M. Tyson. 1996. FAS-
TUS: A Cascaded Finite-State Transducer for Extract-
ing Information from Natural-Language Text. In Fi-
nite State Devices for Natural Language Processing.
MIT Press, Cambridge, MA.
M. Douglass, G. D. Clifford, A. Reisner, G. B. Moody,
R. G. Mark. 2005. Computer-Assisted De-
Identification of Free Text in the MIMIC II Database.
Computers in Cardiology. 32:331-334.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum Entropy Markov Models for Information Extrac-
tion and Segmentation. Proceedings of ICML.
E. Riloff and R. Jones. 1996. Automatically Generating
Extraction Patterns from Untagged Text. Proceedings
of AAAI-96.
D. Roth and W. Yih. 2002. Probabilistic Reasoning
for Entity and Relation Recognition. Proceedings of
COLING.
P. Ruch, R. H. Baud, A. Rassinoux, P. Bouillon, G.
Robert. 2000. Medical Document Anonymization
with a Semantic Lexicon. Proceedings of AMIA.
M. Surdeanu, S. M. Harabagiu, J. Williams, and P.
Aarseth. 2003. Using Predicate-Argument Structures
for Information Extraction. Proceedings of ACL 2003.
L. Sweeney. 1996. Replacing personally-identifying in-
formation in medical records, the scrub system. Jour-
nal of the American Medical Informatics Association.
R. K. Taira, A. A. T. Bui, H. Kangarloo. 2002. Identifi-
cation of patient name references within medical doc-
uments using semantic selectional restrictions. Pro-
ceedings of AMIA.
S. M. Thomas, B. Mamlin, G. Schadow, C. McDonald.
2002. A Successful Technique for Removing Names
in Pathology Reports Using an Augmented Search and
Replace Method. Proceedings of AMIA.
H. Yu, V. Hatzivassiloglou, C. Friedman, W. J. Wilbur.
2002. Automatic Extraction of Gene and Protein Syn-
onyms from MEDLINE and Journal Articles. Pro-
ceedings of AMIA.
73
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 61?67,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Extracting Medication Information from Discharge Summaries  Scott Halgrim, Fei Xia, Imre Solti, Eithon Cadag ?zlem Uzuner University of Washington University of Albany, SUNY PO Box 543450 135 Western Ave Seattle, WA 98195, USA Albany, NY 12222, USA   {captnpi,fxia,solti,ecadag}@uw.edu ouzuner@uamail.albany.edu     
 
Abstract Extracting medication information from clinical records has many potential appli-cations and was the focus of the i2b2 challenge in 2009. We present a hybrid system, comprised of machine learning and rule-based modules, for medication information extraction. With only a hand-ful of template-filling rules, the system?s core is a cascade of statistical classifiers for field detection. It achieved good per-formance that was comparable to the top systems in the i2b2 challenge, demon-strating that a heavily statistical ap-proach can perform as well or better than systems with many sophisticated rules. The system can easily incorporate addi-tional resources such as medication name lists to further improve performance.   
1 Introduction   Narrative clinical records store patient medical information, and extracting this information from these narratives supports data management and enables many applications (Levin et al, 2007). Informatics for Integrating the Biology and the Bedside (i2b2) is an NIH-funded National Center for Biomedical Computing based at Partners HealthCare System, and it has organized annual NLP shared tasks and challenges since 2006 (https://www.i2b2.org/). The Third i2b2 Workshop on NLP Challenges for Clinical Records in 2009 studied the extraction of medication information from hospital discharge summaries 
(https://www.i2b2.org/NLP/Medication/), a task we refer to as the i2b2 challenge in this paper.      In the past decade, there has been extensive re-search on information extraction in both the gen-eral and biomedical domains (Wellner et al, 2004; Grenager et al, 2005; Poon and Domingos, 2007; Meystre et al 2008; Rozenfeld and Feldman, 2008). Interestingly, despite the recent prevalence of statistical approaches in most NLP tasks (in-cluding information extraction), most of the sys-tems developed for the i2b2 challenge were rule-based. In this paper we present our hybrid system, whose core is a cascade of statistical classifiers that identify medication fields such as medication names and dosages. The fields are then assembled to form medication entries. While our system did not participate in the i2b2 challenge (as we were part of the organizing team), it achieved good re-sults that matched the top i2b2 systems.  
2 The i2b2 Challenge This section provides a brief introduction to the i2b2 challenge. 2.1 The task The i2b2 challenge studied the automatic extrac-tion of information corresponding to the following fields from hospital discharge summaries (Uzuner, et al, 2010a): names of medications (m) taken by the patient, dosages (do), modes (mo), frequencies (f), durations (du), and reasons (r) for taking these medications. We refer to the medication field as the name field and the other five fields as the non-name fields. All non-name fields correspond to some name field mention; if they were specified within a two-line window of that name mention, 
61
the i2b2 challenge required such fields to be linked to the name field to form an entry. For each entry, a system must determine whether the entry ap-peared in a list of medications or in narrative text. Table 1 shows an excerpt from a discharge sum-mary and the corresponding entries in the gold standard. The first entry appears in narrative text, and the second in a list of medication information.  Excerpt  of  Discharge Summary 55   the patient noted that he had a recurrence of this  56   vague chest discomfort as he was sitting and  57   talking to friends. He took a sublingual  58   Nitroglycerin without relief. ... 65  Flomax ( Tamsulosin  )  0.4 mg,  po, qd,... Gold  standard: m=?Nitroglycerin? 58:0 58:0 ||do=?nm?||       mo=?sublingual? 57:6 57:6 ||f=?nm? ||du=?nm? ||      r=?vague chest discomfort? 56:0 56:2 ||     ln=?narrative? ... m="flomax ( tamsulosin )" 65:0 65:3||do="0.4 mg"    65:4 65:5||mo="po" 65:6 65:6||f="qd" 65:7    65:7||du="nm"||r="nm"||ln="list"  Table 1: A sample discharge summary excerpt and the corresponding entries in the gold standard. The fields inside an entry are separated by ?||?. Each field is represented by the string and its position (i.e., ?line number: token number? offsets). ?nm? means the field value is not mentioned for this me-dication name.  2.2 Data Sets The i2b2 challenge used a total of 1243 discharge summaries: ? 696 of these summaries were released to par-ticipants for system development, and the i2b2 organizing team provided the gold standard annotation for 17 of them.   ? Participating teams could choose to annotate more files themselves. The University of Syd-ney team annotated 145 out of the 696 summa-ries (including re-annotating 14 of the 17 files annotated by the i2b2 organizing team) and generously shared their annotations with i2b2 after the challenge for future research. We ob-tained and used 110 of their annotations as our training set and the remaining 35 summaries as our development set. 
 ? The participating teams produced system out-puts for 547 discharge summaries set aside for testing. After the challenge, 251 of these sum-maries were annotated by the challenge par-ticipants, and these 251 summaries formed the final test set (Uzuner et al, 2010b).     The sizes of the data sets used in our experiments are shown in Table 2. The training and develop-ment sets were created by the University of Syd-ney, and the test data is the i2b2 official challenge test set. The average number of entries and fields vary among the three sets because the summaries in the test set were chosen randomly from the 547 summaries, whereas the University of Sydney team annotated the longest summaries. 
2.3 Additional resources Besides the training data, the participating teams were allowed to use any additional tools and re-sources that they had access to, including resources not available to the public. All challenge partici-pants used additional resources such as UMLS (www.nlm.nih.gov/research/umls/), but the exact resources used varied from team to team. There-fore, the challenge was similar to the so-called open-track challenge in the general NLP field, as opposed to a closed-track challenge that could re-quire that all the participants use only the list of resources specified by the challenge organizers  2.4 Evaluation metrics  The i2b2 challenge used two sets of evaluation metrics:  horizontal and vertical metrics. Horizon-tal metrics measured system performance at the entry level, whereas vertical metrics measured sys-tem performance at the field level. Both sets of metrics compared the system output and the gold standard at the span level for exact match and at the token level for inexact match, using precision, recall, and F-score (Uzuner et al, 2010a). The pri-mary metric for the challenge is exact horizontal F-score, which is the metric we use to evaluate our system.      
62
 Table 2: The data sets used in our experiments. The numbers in parentheses are the average numbers of entries or fields per discharge summary.   2.5 Participating systems Twenty teams participated in the challenge. Fifteen teams used rule-based approaches, and the rest used statistical or hybrid approaches. The perform-ances of the top five systems are shown in Table 3. Among them, only the top system, developed by the University of Sydney, used a hybrid approach, whereas the rest were rule-based.  Rank Precision Recall F-score 1 89.6 82.0 85.7 2 84.0 80.3 82.1 3 86.4 76.6 81.2 4 78.4 82.3 80.3 5 84.1 75.8 79.7 Table 3: The performance (exact horizontal preci-sion/recall/F-score) of the top five i2b2 systems on the test set.  3 System description   We developed a hybrid system with three process-ing steps: (1) a preprocessing step that finds sec-tion boundaries, (2) a field detection step that identifies the six fields, and (3) a field linking step that links fields together to form entries. The sec-ond step is a statistical system, whereas the other two steps are rule-based. The second step was the main focus of this study. 3.1 Preprocessing  In addition to common processing steps such as part-of-speech (POS) tagging, our preprocessing 
step includes a section segmenter that breaks dis-charge summaries into sections. Discharge summa-ries tend to consist of sections such as ?ADMIT DIAGNOSIS?, ?PAST MEDICAL HISTORY?, and ?DISCHARGE MEDICATIONS?. Knowing section boundaries is important for the i2b2 chal-lenge because, according to the annotation guide-lines for creating the gold standard, medications occurring under certain sections (e.g., family his-tory and allergic reaction) should be excluded from the system output. Furthermore, knowing the types of sections could be useful for field detection and field linking; for example, entries in the ?DISCHARGE MEDICATIONS? section are more likely to appear in a list of medications than in nar-rative text. The set of sections and the exact spelling of section headings vary across discharge summaries. The section segmenter uses regular expressions (e.g., ?^\s*([A-Z\s]+):? -- a line starting with a se-quence of capitalized words followed by a colon) to collect potential section headings from the train-ing data, and the headings whose frequencies are higher than a threshold are used to identify section boundaries in the discharge summaries.  3.2 Field detection  This step consists of three modules: the first mod-ule, find_name, finds medication names in a dis-charge summary; the second module, context_type, processes each medication name identified by find_name and determines whether the medication appears in narrative text or in a list of medications; the third module, find_others, detects the five non-name field types.      For find_name and find_others, we follow the common practice of treating named-entity (NE) detection as a sequence labeling task with the BIO 
Data  Sets # of  Summaries # of Entries # of Fields # of Names # of Doses # of Freq # of Modes # of Duration # of Reason Training set 110  5970  (54.3) 14886 (135.3) 5684 (51.7) 2929 (26.6) 2740 (24.9) 2146 (19.5) 302  (2.7) 1085 (9.9) Dev set 35  2401  (68.6) 5988 (171.1) 2302 (65.8) 1163 (33.2) 1096 (31.3) 880 (25.1) 111  (3.2) 436 (12.5) Test set 251  8936 (35.6) 22041 (87.8) 8495 (33.8) 4387 (17.5) 3999 (15.9) 3307 (13.2) 511  (2.0) 1342 (5.3) 
63
tagging scheme; that is, each token in the input is tagged with B-x (beginning an NE of type x), I-x (inside an NE of type x) and O (outside any NE).   3.2.1 The find_name module As this module identifies medication names only, the tagset under the BIO scheme has three tags: B-m for beginning of a name, I-m for inside a name, and O for outside. Various features are used for this module, which we group into four types:   ? (F1) includes word n-gram features (n=1,2,3). For instance, the bigram wi-1 wi looks at the current word and the previous word.   ? (F2) contains features that check properties of the current word and its neighbors (e.g., the POS tag, the affixes and the length of a word, the type of section that a word appears in, whether a word is capitalized, whether a word is a number, etc.)   ? (F3) checks the BIO tags of previous words  ? (F4) contains features that check whether n-grams formed by neighboring words appear as part of medication names in given medication name lists. The name lists can come from la-beled training data or additional resources such as UMLS. 3.2.2 The context_type module This module is a binary classifier which deter-mines whether a medication name occurs in a list or narrative context. Features used by this module include the section name as identified by the pre-processing step, the number of commas and words on the current line, the position of the medication name on the current line, and the current and near-by words.   3.2.3 The find_others module This module complements the find_name module and uses eleven BIO tags to identify five non-name fields. The feature set used in this module is very similar to the one used in find_name except that some features in (F2) and (F4) are modified to suit the needs of the non-name fields. For instance, a feature will check whether a word fits a common pattern for dosage. In addition, some features in 
(F2) look at the output of previous modules: e.g., the location of nearby medication names as this information can be provided by the find_name module at test time.  3.3 Field linking  Once medication names and other fields have been found, the final step is to form entries by associat-ing each medication name with its related fields. Our current implementation uses simple heuristics. First, we go over each non-name field and link it with the closest preceding medication name unless the distance between the non-name field and its closest following medication name is much shorter. Second, we assemble the (name, non-name) pairs to form medication entries with a few rules.       More information about the modules discussed in this section and features used by the modules is available in (Halgrim, 2009). 4 Experimental results  In this section, we report the performance of our system on the development set (Section 4.1-4.3) and the test set (Section 4.4). The data sets are de-scribed in Table 2. For all the experiments in this section, unless specified otherwise, we report exact horizontal precision/recall/F-score, the primary metrics for the i2b2 challenge.     For the three modules in the field detection step, we use the Maximum Entropy (MaxEnt) learner in the Mallet package (McCallum, 2002) because, in general, MaxEnt produces good results without much parameter tuning and the training time for MaxEnt is much faster than more sophisticated algorithms such as CRF (Lafferty et al, 2001).     To determine whether the difference between two systems? performances is statistically signifi-cant, we use approximate randomization tests (No-reen, 1989) as follows. Given two systems that we would like to compare, we first calculate the dif-ference between exact horizontal F-scores. Then two pseudo-system outputs are generated by ran-domly swapping (at 0.5 probability) the two sys-tem outputs for each discharge summary. If the difference between F-scores of these pseudo-outputs is no less than the original F-score differ-ence, a counter, cnt, is increased by one. This process was repeated n=10,000 times, and the p-value of the significance is equal to (cnt+1)/(n+1). 
64
If the p-value is smaller than a predefined thresh-old (e.g., 0.05), we conclude that the difference between the two systems is statistically significant.  4.1 Performance of the whole system  4.1.1 Effect of feature sets  To test the effect of feature sets on system per-formance, we trained find_name and find_others with different feature sets and tested the whole sys-tem on the development set. For (F4), we used two medication name lists. The first list consists of medication names gathered from the training data. The second list includes drug names from the FDA database (www.accessdata.fda.gov/scripts/cder/ndc/). We use the second list to test whether adding features that check the information in an additional re-source could improve the system performance.    The results are in Table 4. For the last two rows, F1-F4a uses the first medication name list, and F1-F4b uses both lists. The F-score difference between all adjacent rows is statistically significant at p?0.01, except for the pair F1-F3 vs. F1-F4a. It is not surprising that using the first medication name list on top of F1-F3 does not improve the perform-ance, as the same kind of information has already been captured by F1 features. The improvement of F1-F4b over F1-F4a shows that the system can easily incorporate additional resources and achieve a statistically significant (at p?0.01) gain.   Features Precision Recall F-score F1 72.5 60.3 65.8 F1-F2 82.5 78.2 80.3 F1-F3 88.4 77.9 82.8 F1-F4a 87.4 77.9 82.4 F1-F4b 88.1 79.4 83.5  Table 4: System performance on the development set with different feature sets 4.1.2 Effect of training data size Figure 1 shows the system performance on the de-velopment set when different portions of the train-
ing set are used for training. The curve with ?+? signs represents the results for F1-F4b, and the curve with circles represents the results for F1-F4a. The figure illustrates that, as the training data size increases, the F-score with both feature sets im-proves. In addition, the additional resource is most helpful when the training data size is small, as in-dicated by the decreasing gap between the two sets of F-scores when the size of training data in-creases.   
 Figure 1: System performance on the development set with different training data sizes (Legend: ? represents F-scores with features in F1-F4a; + rep-resents F-scores with features in F1-F4b)  4.1.3 Pipeline vs. find_all   The current field detection step is a pipeline ap-proach with three modules: find_name, con-text_type, and find_others. Having three separate modules allows each module to choose the features that are most appropriate for it. In addition, later modules can use features that check the output of the previous modules. A potential downside of the pipeline system is that the errors in the early mod-ule would propagate to later modules. An alterna-tive is to use a single module to detect all six field types together.  Figure 2 shows the result of find_all in compari-son to the result for the three-module pipeline. Both use the F1-F4b feature sets, except that find_others uses some features that check the out-put of previous modules which are not available to find_all.  
65
 Figure 2: Pipeline vs. find_all for field detection (Legend: ? represents F-scores with find_all; + represents F-scores with the three-module pipeline)      Interestingly, when 10% of the training set is used for training, find_all has a higher F-score than the pipeline approach, although the difference is not statistically significant at p?0.05. As more data is used for training, the pipeline outperforms find_all, and when at least 50% of the training data is used, the difference between the two is statisti-cally significant at p?0.05. One possible explana-tion for this phenomenon is that as more training data becomes available, the early modules in the pipeline make fewer errors; as a result, the disad-vantage of the pipeline approach caused by error propagation is outweighed by the advantage that the later modules in the pipeline can use features that check the output of the earlier modules. 4.2 Performance of the field detection step Table 5 shows the exact precision/recall/F-score on identifying the six field types, using all the training data, F1-F4b features, and the pipeline approach for field detection. A span in the system output exactly matches a span in the gold standard if the two spans are identical and have the same field type. Among the six fields, the results for duration and reason are the lowest. That is because duration and reason are longer phrases than the other four field types and there are fewer strong, reliable cues to signal their presence.  When making the narrative/list distinction, the accuracy of our context_type module is 95.4%. In contrast, the accuracy of the baseline (which treats each medication name as in a list context) is only 55.6%. 
  Precision Recall F-score Name 91.2 88.5 89.9 Dosage 96.6 90.8 93.6 Frequency 93.9 89.0 91.8 Mode 95.7 90.3 92.9 Duration 73.8 43.2 54.5 Reason 72.2 31.0 43.3 All fields 92.6 84.5 88.4 Table 5: The performance (exact preci-sion/recall/F-score) of field detection on the devel-opment set.   4.3 Performance of the field linking step In order to evaluate the field linking step, we gen-erated a list of (name, non-name) pairs from the gold standard, where the name and non-name fields appear in the same entry in the gold stan-dard. We then compared these pairs with the ones produced by the field linking step and calculated precision/recall/F-score. Table 6 shows the result of two experiments: in the cheating experiment, the input to the field linking step is the fields from the gold standard; in the non-cheating experiment, the input is the output of the field detection step. These experiments show that, while the heuristic rules used in this step work reasonably well when the input is accurate, the performance deteriorates con-siderably when the input is noisy, an issue we plan to address in future work.   Precision Recall F-score Non-cheating 87.4 75.1 80.8 Cheating 96.2 94.5 95.3 Table 6: The performance of the field linking step on the development set (cheating: assuming perfect field input; non-cheating: using the output of the field detection step) 4.4 Results on the test data Table 7 shows the system performance on the i2b2 official test data. The system was trained on the union of the training and development data. Com-pared with the top five i2b2 systems (see Table 3), our system was second only to the best i2b2 sys-tem, which used more resources and more sophis-ticated rules for field linking (Patrick and Li, 2009).    
66
 Precision Recall F-score Horizontal 88.6 80.2 84.1 Name 92.6 87.1 89.8 Dosage 96.3 90.2 93.1 Frequency 95.6 90.8 93.2 Mode 96.7 90.2 93.3 Duration 70.6 40.5 51.5 Reason 73.4 34.7 47.1 All fields 91.6 82.7 86.9 Table 7: System performance on the test set when trained on the union of the training and the devel-opment sets with F1-F4b features. 5 Conclusion  We present a hybrid system for medication extrac-tion. The system is built around a pipeline of cas-cading statistical classifiers for field detection. It achieves good performance that is comparable to the top systems in the i2b2 challenge, and incorpo-rating additional resources as features further im-proves the performance. In the future, we plan to replace the current rule-based field linking module with a statistical module to improve accuracy.  Acknowledgments This work was supported in part by US DOD grant N00244-091-0081 and NIH Grants 1K99LM010227-0110, U54LM008748, and T15LM007442-06. We would also like to thank three anonymous reviewers for helpful comments. 
References  Trond Grenager, Dan Klein, and Christopher Manning. 2005. Unsupervised learning of field segmentation models for information extraction. In Proc. of ACL-2005. Scott Halgrim. 2009. A Pipeline Machine Learning Ap-proach to Biomedical Information Extraction. Master Thesis. University of Washington.  J. Lafferty and A. McCallum and F. Pereira. 2001. Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data. In Proc. of the 18th International Conference on Machine Learning (ICML-2001). Matthew A. Levin, Marina Krol, Ankur M. Doshi, and David L. Reich. 2007. Extraction and mapping of drug names from free text to a standardized nomen-clature. AMIA Symposium Proceedings, pp 438-442. 
 S. Meystre, G. Savova, K. Kipper-Schuler, and J. Hur-dle. 2008. Extracting Information from Textual Documents in the Electronic Health Record: A Re-view of Recent Research. IMIA Yearbook of Medical Informatics Methods Inf Med 2008; 47 Suppl 1:128-44.  Andrew McCallum. 2002. Mallet: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu Eric W. Noreen. 1989. Computer intensive methods for testing hypotheses: an introduction. John Wiley & Sons. Jon Patrick and Min Li, 2009. A Cascade Approach to   Extract Medication Event (i2b2 challenge 2009). Presentation at the Third i2b2 Workshop, November 2009, San Francisco, CA.  Hoifung Poon and Pedro Domingos. 2007. Joint infer-ence in information extraction. In Proc. of the Na-tional Conference on Artificial Intelligence (AAAI), pp 913-918. Benjamin Rozenfeld and Ronen Feldman. 2008. Self-supervised relation extraction from the web. Knowl-edge and Information Systems, 17(1):17-33. ?zlem Uzuner, Imre Solti, and Eithon Cadag, 2010a. Extracting Medication Information from Clinical Text. Submitted to JAMIA.  ?zlem Uzuner, Imre Solti, Fei Xia, and Eithon Cadag, 2010b. Community Annotation Experiment for Ground Truth Generation for the i2b2 Medication Challenge. Submitted to JAMIA.  Ben Wellner, Andrew McCallum, Fuchun Peng, and Michael Hay. 2004. An integrated, conditional model of information extraction and coreference with appli-cation to citation matching. In Proc. of the 20th Con-ference on Uncertainty in AI (UAI-2004).    
67
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 23?27,
Uppsala, July 2010.
Does Negation Really Matter? 
Ira Goldstein 
University at Albany, SUNY 
Albany, NY USA 
ig4895@albany.edu 
?zlem Uzuner 
University at Albany, SUNY 
Albany, NY USA 
ouzuner@albany.edu 
 
Abstract 
We explore the role negation and speculation 
identification plays in the multi-label docu-
ment-level classification of medical reports for 
diseases. We identify the polarity of assertions 
made on noun phrases which reference dis-
eases in the medical reports. We experiment 
with two machine learning classifiers: one 
based upon Lucene and the other based upon 
BoosTexter. We find the performance of these 
systems on document-level classification of 
medical reports for diseases fails to show im-
provement when their input is enhanced by the 
polarity of assertions made on noun phrases. 
We conclude that due to the nature of our ma-
chine learning classifiers, information on the 
polarity of phrase-level assertions does not 
improve performance on our data in a multi-
label document-level classification task. 
1 Introduction 
In the medical domain, a substantial amount of 
patient data is stored as free text in patient medi-
cal report narratives (Spat et al 2008) and needs 
to be processed in order to be converted to more 
widely-useful structured information. These nar-
ratives contain a variety of useful information 
that can support syndromic surveillance (Shapiro 
2004), decision support (Fiszman et al 2000), 
and problem list generation (Sibanda et al 2006). 
Physicians often assert negative or speculative 
diagnoses in medical reports (Rao et al 2003) to 
keep track of all potential diagnoses that have 
been considered and to provide information that 
contrasts with the positive diagnoses (Kim and 
Park 2006). The noun phrases (NP) associated 
with negative and speculative assertions in medi-
cal reports may be confused with positively as-
serted NPs, thereby adversely affecting auto-
mated classification system performance.  In the 
medical domain, verbs often play a reduced role 
or are implied in assertions.  We therefore focus 
our investigation of assertions on NPs. 
In this paper, we describe the polarity of an 
assertion as being positive, speculative, or nega-
tive. Assertion classification is a generally ac-
cepted means for resolving problems caused by 
negation and speculation. Averbuch et al (2004) 
use context to identify negative/positive in-
stances of various symptoms. Mutalik et al 
(2001) show that the Unified Medical Language 
System (UMLS) Metathesaurus can be used to 
reliably detect negated concepts in medical nar-
ratives. Harkema et al (2009) develop ConText 
to determine not only positive and negative as-
sertions, but also assertions referencing someone 
other than the patient. 
The literature is filled with reports of systems 
which employ assertion classification (e.g., 
Google Scholar lists 134 documents citing 
Chapman et al?s (2001) NegEx). However, few 
reports describe how much assertion classifica-
tion contributes to the final system performance. 
Two exceptions are Goldstein et al (2007) and 
Ambert and Cohen (2009). 
Goldstein et al develop a hand-crafted rule 
based system to classify radiological reports 
from the 2007 Computational Medicine Center 
(CMC) Challenge (Pestian et al 2007). They 
show that negation and speculation play key 
roles in classifying their reports. Ambert and Co-
hen apply a machine learning (ML) approach to 
classifying discharge summaries from the 2008 
i2b2 Obesity Challenge (Uzuner 2008). They 
report that due to ?false negations,? simply add-
ing negation detection to their base system does 
not consistently improve performance. Prompted 
by these contradicting results in the literature, we 
explore the role assertion classification plays in 
the multi-label classification of medical reports 
from both the CMC and i2b2 challenges.  
We attempt to improve document-level classi-
fication performance of two multi-label ML clas-
sifiers by identifying the polarity of assertions on 
NPs. We experiment with medical reports from 
two different corpora. We detect NPs which ref-
erence diseases. We then identify the polarity of 
the assertion made for each NP. We show that 
enriching reports with the polarity of the asser-
tions does not improve performance for multi-
label document-level classification of medical 
23
reports into diseases in our corpora. Our findings 
imply that, despite common practice, the contri-
bution of assertion classification may be limited 
when employing ML approaches to predicting 
document-level labels of medical reports. 
2 Data 
The data were provided by the CMC challenge 
(Pestian et al 2007) and the i2b2 Obesity Chal-
lenge (Uzuner 2008). Both data sets had been de-
identified (anonymized) and, where appropriate, 
re-identified with surrogates. Our task is to de-
termine the presence of diseases in the patient 
based upon medical report narratives. The insti-
tutional review boards of the SUNY Albany and 
Partners HealthCare approved this study. 
2.1 CMC Data Set 
The CMC data set consists of a training set of 
978 radiology reports and a test set of 976 radi-
ology reports. Each report is labeled with 
ICD-9-CM (National Center for Health Statistics 
2010) standard diagnostic classification codes.  
The reports have been hand labeled with 45 
ICD-9-CM. Each code represents a distinct dis-
ease present in the patient. The codes reflect only 
the definite diagnoses mentioned in that report. 
At least one code is assigned to each report. Mul-
tiple codes per report are allowed. For each re-
port in the test set, we predict which diseases are 
present in the patient and label the report with 
the ICD-9-CM code for that disease. Any code 
not assigned to a report implies that the corre-
sponding disease is not present in the patient. 
2.2 i2b2 Data Set 
The i2b2 data set consists of a training set of 720 
discharge summaries and a test set of 501 dis-
charge summaries. These medical reports range 
in size from 133 words to more than 3000 words. 
The reports have been labeled for information on 
obesity and 15 of its most frequent co-
morbidities. For each report, each disease is la-
beled as being present, absent, or questionable in 
the patient, or unmentioned in the narrative. Mul-
tiple codes per report are allowed. 
Since we are interested in those diseases pre-
sent in the patient, we retain the present class and 
collapse the absent, questionable, and 
unmentioned categories into a not present class. 
For each report in the test set we predict whether 
each of the 16 diseases is present or not present 
in the patient. We label each report with our pre-
diction for each of the 16 diseases. 
3 Methods 
We preprocess the medical report narratives 
with a Noun Phrase Detection Pre-processor 
(NPDP) to detect noun phrases referencing dis-
eases. We implement our own version of Con-
Text (Harkema et al 2009), enhance it to also 
detect speculation, and employ it to identify the 
polarity of assertions made on the detected NPs. 
We expand the text of the medical reports with 
asserted NPs. We conflate lexical variations of 
words. We train two different types of classifiers 
on each of the training sets. We apply labels to 
both the expanded and non-expanded reports us-
ing two ML classifiers. We evaluate and report 
results only on the test sets. 
3.1 Noun Phrase and Assertion Detection  
We detect noun phrases via an NPDP. We build 
our NPDP based on MetaMap (Aronson 2001). 
The NPDP identifies NPs which reference dis-
eases in medical reports. We select 17 UMLS 
semantic types whose concepts can assist in the 
classification of diseases. First, NPDP maps NPs 
in the text to UMLS semantic types. If the 
mapped semantic type is one of the target seman-
tic types, NPDP then tags the NP. 
NPDP uses the pre-UMLS negation phrases of 
Extended NegEx (Sibanda et al 2006) to identify 
adjectives indicating the absence or uncertainty 
of each tagged NPs. It differentiates these adjec-
tives from all other adjectives modifying tagged 
NPs. For example, possible in possible reflux is 
excluded from the tagged NP, whereas severe in 
severe reflux is retained. We then identify the 
polarity of the assertion made on each NP. In 
order to distinguish the polarity of the assertions 
from one another, we do not modify the positive 
assertions, but transform the negative and specu-
lative assertions in the following manner: Sen-
tences containing negative assertions are re-
peated and modified with the NP pre-pended 
with ?abs? (e.g., ?Patient denies fever.? is re-
peated as ?Patient denies absfever.?). Similarly, 
sentences containing speculative assertions are 
repeated and modified with the NP pre-pended 
with ?poss?. We refer to these transformed terms 
as asserted noun phrases. We assert NPs for the 
unmodified text of both the data sets. Table 1 
provides a breakdown of the assertions for each 
of the detected NPs for each of the data sets. 
We examine the performance of our enhanced 
implementation of ConText by comparing its 
results against CMC test set NPs manually anno-
tated by a nurse librarian and author IG. Table 2 
24
shows the performance for each of the three po-
larities. We find these results to be comparable to 
those reported in the literature: Mutalik et al?s 
(2001) NegFinder finds negated concepts with a 
recall of .957; Chapman et al?s (2001) NegEx 
report a precision of .8449 and a recall of .8241. 
CMC i2b2 Assertion Training Test Training Test
Positive 2,168 2,117 47,860 34,112
Speculative 312 235 3,264 2,166
Negative 351 353 8,202 5,654
Table 1 - Distribution of Asserted Noun Phrases for 
both the CMC and i2b2 data sets. 
Assertion Precision Recall F1-Measure
Positive 0.991 0.967 0.979 
Speculative 0.982 0.946 0.964 
Negative 0.770 0.983 0.864 
Table 2 - Assertion Performance on the CMC test set. 
3.2 Lucene Classifier  
We follow the k-Nearest Neighbor (Cover and 
Hart 1967) process previously described in Gold-
stein et al (2007) to build our Lucene-based 
classifier. Classification is based on the nearest 
training samples, as determined by the feature 
vectors. This approach assumes that similar 
training samples will cluster together in the fea-
ture vector space. The nearest training samples 
are considered to be those that are most similar 
to the data sample. 
We build our Lucene-based classifier using 
Apache Lucene (Gospodneti? and Hatcher 2005). 
We use the Lucene library to determine the simi-
larity of medical report narratives. We determine 
which training reports are similar to the target 
report based upon their text. For each target re-
port we retrieve the three most similar training 
reports and assign to the target report any codes 
that are used by the majority of these reports. In 
cases where the retrieved reports do not provide 
a majority code, the fourth nearest training report 
is used. If a majority code is still not found, a 
NULL code is assigned to the target report. 
We first run the Lucene Classifier on lower 
case, stemmed text of the medical reports. We 
refer to this as the Base Lucene Classifier run. 
We next run the Lucene Classifier on the text 
expanded with asserted noun phrases. We refer 
to this as the Asserted Lucene Classifier run. 
3.3 BoosTexter Classifier 
BoosTexter (Schapire and Singer 2000) builds 
classifiers from textual data by performing mul-
tiple iterations of dividing the text into subsam-
ples upon which weak decision-stub learners are 
trained. Among these weak learners, BoosTexter 
retains those that perform even marginally better 
than chance. After a set number of iterations, the 
retained weak learners are combined into the fi-
nal classifier. BoosTexter classifies text using 
individual words (unigrams), strings of consecu-
tive words (n-grams), or strings of non-
consecutive words, without considering seman-
tics. 
We cross-validate BoosTexter (tenfold) on the 
CMC training set. We establish the optimal pa-
rameters on the CMC training set to be 1100 it-
erations, with n-grams of up to four words. We 
find the optimal parameters of the i2b2 training 
set to be similar to those of the CMC training set. 
For consistency, we apply the parameters of 
1100 iterations and n-grams of up to four words 
to both data sets. In addition, we apply unigrams 
to BoosTexter in order to provide BoosTexter 
classifier results that are comparable to those of 
the Lucene classifiers. 
We create two classifiers with BoosTexter us-
ing the lower case, stemmed text of the medical 
reports: one with unigrams and one with 
n-grams. We refer to these as Base BoosTexter 
Classifier runs. For each of unigrams and 
n-grams, we create runs on the text expanded 
with the asserted noun phrases. We refer to these 
as Asserted BoosTexter Classifier runs. 
4 Evaluation 
We evaluate our classifiers on both the plain text 
of the reports and on text expanded with asserted 
NPs. We present results in terms of micro-
averaged precision, recall, and F1-measure 
(?zg?r et al 2005). We check the significance of 
classifier performance differences at ?=0.10. We 
apply a two-tailed Z test, with Z = ?1.645. 
5 Results and Discussion 
Table 3 and Table 4 show our systems? perform-
ances. We predict ICD-9-CM codes for each of 
the 976 CMC test reports. We predict whether or 
not each of 16 diseases is present in the patient 
for each of the 501 i2b2 test set reports. 
Negative Reports Positive Reports 
Run 
Preci-
sion
Re-
call 
F1-
Meas-
ure 
Preci-
sion 
Re-
call 
F1-
Meas
ure 
CMC Base 0.991 0.993 0.992 0.717 0.664 0.690
CMC Asserted 0.991 0.992 0.992 0.712 0.668 0.690
i2b2 Base 0.905 0.886 0.896 0.612 0.660 0.635
i2b2 Asserted 0.904 0.890 0.897 0.618 0.651 0.634
Table 3 - Lucene Classifier?s Performance. 
25
The Asserted Lucene and BoosTexter Classi-
fier runs show no significant difference in per-
formance from their Base runs on either corpus. 
These results indicate that asserted noun phrases 
do not contribute to the document-level classifi-
cation of our medical reports 
5.1 Contribution of Asserted Noun Phrases 
Through analysis of the Base and Asserted runs, 
we find enough similarities in the text of the 
training and test reports for a given class to allow 
our ML classifiers to correctly predict the labels 
without needing to identify the polarity of the 
assertions made on individual NPs. For example, 
for the CMC target report 97729923: 
5-year-9-month - old female 
with two month history of 
cough. Evaluate for pneumonia. 
No pneumonia. 
the Base Lucene Classifier retrieves report 
97653364: 
Two - year-old female with 
cough off and on for a month 
(report states RSV nasal 
wash). 
No radiographic features of 
pneumonia. 
which allows the system to classify the target 
report with the ICD-9-CM code for cough. While 
identifying the polarity of the assertions for 
pneumonia strengthens the evidence for cough 
and not pneumonia, it cannot further improve the 
already correct document-level classification. 
These unenhanced assertions do not stand in the 
way of correct classification by our systems.  
5.2  Approach, Data, and Task 
Hand-crafted rule-based approaches usually 
encode the most salient information that the ex-
perts would find useful in classification and 
would therefore benefit from explicit assertion 
classification subsystems, e.g., Goldstein et al, 
(2007). On the other hand, ML approaches have 
the ability to identify previously undetected pat-
terns in data (Mitchell et al 1990). This enables 
ML approaches to find patterns that may not be 
obvious to experts, while still performing correct 
classification. Therefore, the contribution of as-
serted NPs appears to be limited when applied to 
ML approaches to document-level classification 
of medical reports. This is not to say that an ML 
approach to document-level classification will 
never benefit from identifying the polarity of 
NPs; only that on our data we find no improve-
ment. 
Negative Reports Positive Reports 
Run 
Preci-
sion 
Re-
call 
F1-
Meas
ure 
Preci-
sion 
Re-
call 
F1-
Meas
ure 
Base 0.993 0.995 0.994 0.812 0.747 0.778CMC 
uni-
gram Asserted 0.993 0.996 0.995 0.837 0.767 0.800
Base 0.995 0.996 0.996 0.865 0.812 0.838CMC
n-
gram Asserted 0.995 0.996 0.996 0.866 0.812 0.839
Base 0.970 0.973 0.917 0.902 0.889 0.895i2b2 
uni-
gram Asserted 0.970 0.975 0.973 0.908 0.891 0.899
Base 0.971 0.976 0.974 0.911 0.895 0.903i2b2 
n-
gram Asserted 0.974 0.977 0.975 0.914 0.903 0.908
Table 4 - BoosTexter Classifier?s Performance. 
The CMC and i2b2 data sets can each be de-
scribed as being homogenous; they come from a 
relatively small communities and limited geo-
graphic areas. In these data, variation in vocabu-
lary that might arise from the use of regional ex-
pressions would be limited. This would be espe-
cially true for the CMC data since it comes from 
a single medical department at a single hospital. 
It would not be surprising for colleagues in a 
given department who work together for a period 
of time to adopt similar writing styles and to em-
ploy consistent terminologies (Suchan 1995). 
Our task is one of multi-label document-level 
classification. Working at the document level, 
each negative and speculative assertion would 
play only a small role in predicting class labels. 
The homogeneity of the text in our data sets, 
and the task of document-level classification may 
have been factors in our results. Future research 
should examine how the characteristics of the 
data and the nature of the task affect the role of 
assertion classification. 
6 Conclusion 
Identifying the polarity of phrase-level assertions 
in document-level classification of medical re-
ports may not always be necessary. The specific 
task and approach applied, along with the charac-
teristics of the corpus under study, should be 
considered when deciding the appropriateness of 
assertion classification. The results of this study 
show that on our data and task, identifying the 
polarity of the assertions made on noun phrases 
does not improve machine learning approaches 
to multi-label document-level classification of 
medical reports. 
26
References  
Kyle H. Ambert and Aaron M. Cohen. 2009. A Sys-
tem for Classifying Disease Comorbidity Status 
from Medical Discharge Summaries Using Auto-
mated Hotspot and Negated Concept Detection. 
Journal of the American Medical Informatics As-
sociation 16(4):590-95. 
Alan R. Aronson. 2001. Effective Mapping of Bio-
medical Text to the UMLS Metathesaurus: The 
Metamap Program. Proceedings of the AMIA sym-
posium. 17-21. 
Mordechai Averbuch, Tom H. Karson, Benjamin 
Ben-Ami, Oded Maimon, and Lior Rokach. 2004. 
Context-Sensitive Medical Information Retrieval. 
Medinfo. MEDINFO 11(Pt 1):282-86. 
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 2001. 
A Simple Algorithm for Identifying Negated Find-
ings and Diseases in Discharge Summaries. Jour-
nal of Biomedical Informatics 34(5):301-10. 
Thomas M. Cover and Peter E. Hart. 1967. Nearest 
Neighbor Pattern Classification. IEEE Transac-
tions on Information Theory 13(1):21-27. 
Marcelo Fiszman, Wendy W. Chapman, Dominik 
Aronsky, and R. Scott Evans. 2000. Automatic De-
tection of Acute Bacterial Pneumonia from Chest 
X-Ray Reports. Journal of the American Medical 
Informatics Association 7:593-604. 
Ira Goldstein, Anna Arzumtsyan, and ?zlem Uzuner. 
2007. Three Approaches to Automatic Assignment 
of ICD-9-CM Codes to Radiology Reports. Pro-
ceedings of the AMIA symposium. 279-83. 
Otis Gospodneti? and Erik Hatcher. 2005. Lucene in 
Action. Greenwich, CT: Manning. 
Henk Harkema, John N. Dowling, Tyler Thornblade, 
and Wendy W. Chapman. 2009. Context: An Algo-
rithm for Determining Negation, Experiencer, and 
Temporal Status from Clinical Reports. Journal Of 
Biomedical Informatics 42(5):839-51. 
Jung-Jae Kim and Jong C. Park. 2006. Extracting 
Contrastive Information from Negation Patterns in 
Biomedical Literature. ACM Transactions on Asian 
Language Information Processing (TALIP) 
5(1):44-60. 
Tom Mitchell, Bruce Buchanan, Gerald DeJong, 
Thomas Dietterich, Paul Rosenbloom, and Alex 
Waibel. 1990. Machine Learning. Annual Review 
of Computer Science. Vol.4. Eds. Joseph  F. Traub, 
Barbara J. Grosz, Butler W. Lampson and Nils J. 
Nilsson. Palo Alto, CA: Annual Reviews.  
Pradeep G. Mutalik, Aniruddha Deshpande, and 
Prakash M. Nadkarni. 2001. Use of General-
Purpose Negation Detection to Augment Concept 
Indexing of Medical Documents: A Quantitative 
Study Using the UMLS. Journal of the American 
Medical Informatics Association 8(6):598-609. 
National Center for Health Statistics. 2010. ICD - 
ICD-9-CM - International Classification of Dis-
eases, Ninth Revision, Clinical Modification. Ac-
cessed: May 1, 2010. 
<www.cdc.gov/nchs/icd/icd9cm.htm>. 
Arzucan ?zg?r, Levent ?zg?r, and Tunga G?ng?r. 
2005. Text Categorization with Class-Based and 
Corpus-Based Keyword Selection. ISCIS 2005. 
Eds. P?nar Yolum, Tunga G?ng?r, Fikret G?rgen 
and Can ?zturan. Istanbul, Turkey: Springer. 606-
15 of Lecture Notes in Computer Science. 
John P. Pestian, Christopher Brew, Pawel 
Matykiewicz, D. J. Hovermale, Neil Johnson, K. 
Bretonnel Cohen, and W?odzis?aw Duch. 2007. A 
Shared Task Involving Multi-Label Classification 
of Clinical Free Text. ACL:BioNLP. Prague: Asso-
ciation for Computational Linguistics. 97-104. 
R. Bharat Rao, Sathyakama Sandilya, Radu Stefan 
Niculescu, Colin Germond, and Harsha Rao. 2003. 
Clinical and Financial Outcomes Analysis with Ex-
isting Hospital Patient Records. Proceedings of the 
Ninth ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining: ACM 
Press New York, NY, USA. 416-25. 
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A Boosting-Based System for Text Categoriza-
tion. Machine Learning 39(2):135-68. 
Alan R. Shapiro. 2004. Taming Variability in Free 
Text: Application to Health Surveillance. MMWR. 
Morbidity And Mortality Weekly Report 53 
Suppl:95-100. 
Tawanda Carleton Sibanda, T. He, Peter Szolovits, 
and ?zlem Uzuner. 2006. Syntactically-Informed 
Semantic Category Recognition in Discharge 
Summaries. Proceedings of the AMIA symposium. 
714-8. 
Stephan Spat, Bruno Cadonna, Ivo Rakovac, Christian 
G?tl, Hubert Leitner, G?nther Stark, and Peter 
Beck. 2008. Enhanced Information Retrieval from 
Narrative German-Language Clinical Text Docu-
ments Using Automated Document Classification. 
Studies In Health Technology And Informatics 
136:473-78. 
Jim Suchan. 1995. The Influence of Organizational 
Metaphors on Writers' Communication Roles and 
Stylistic Choices. Journal of Business Communica-
tion 32(1):7-29. 
?zlem Uzuner. 2008. Second I2b2 Workshop on 
Natural Language Processing Challenges for Clini-
cal Records. Proceedings of the AMIA sympo-
sium:1252-53. 
 
 
27
