ORANGE: a Method for Evaluating Automatic Evaluation Metrics                    
for Machine Translation 
Chin-Yew Lin and Franz Josef Och 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292, USA 
{cyl,och}@isi.edu 
 
Abstract 
Comparisons of automatic evaluation metrics 
for machine translation are usually conducted 
on corpus level using correlation statistics 
such as Pearson?s product moment correlation 
coefficient or Spearman?s rank order 
correlation coefficient between human scores 
and automatic scores. However, such 
comparisons rely on human judgments of 
translation qualities such as adequacy and 
fluency. Unfortunately, these judgments are 
often inconsistent and very expensive to 
acquire. In this paper, we introduce a new 
evaluation method, ORANGE, for evaluating 
automatic machine translation evaluation 
metrics automatically without extra human 
involvement other than using a set of reference 
translations. We also show the results of 
comparing several existing automatic metrics 
and three new automatic metrics using 
ORANGE. 
1 Introduction 
To automatically evaluate machine translations, 
the machine translation community recently 
adopted an n-gram co-occurrence scoring 
procedure BLEU (Papineni et al 2001). A similar 
metric, NIST, used by NIST (NIST 2002) in a 
couple of machine translation evaluations in the 
past two years is based on BLEU. The main idea of 
BLEU is to measure the translation closeness 
between a candidate translation and a set of 
reference translations with a numerical metric. 
Although the idea of using objective functions to 
automatically evaluate machine translation quality 
is not new (Su et al 1992), the success of BLEU 
prompts a lot of interests in developing better 
automatic evaluation metrics. For example, Akiba 
et al (2001) proposed a metric called RED based 
on edit distances over a set of multiple references.  
Nie?en et al (2000) calculated the length-
normalized edit distance, called word error rate 
(WER), between a candidate and multiple 
reference translations. Leusch et al (2003) 
proposed a related measure called position-
independent word error rate (PER) that did not 
consider word position, i.e. using bag-of-words 
instead. Turian et al (2003) introduced General 
Text Matcher (GTM) based on accuracy measures 
such as recall, precision, and F-measure.  
With so many different automatic metrics 
available, it is necessary to have a common and 
objective way to evaluate these metrics. 
Comparison of automatic evaluation metrics are 
usually conducted on corpus level using correlation 
analysis between human scores and automatic 
scores such as BLEU, NIST, WER, and PER. 
However, the performance of automatic metrics in 
terms of human vs. system correlation analysis is 
not stable across different evaluation settings. For 
example, Table 1 shows the Pearson?s linear 
correlation coefficient analysis of 8 machine 
translation systems from 2003 NIST Chinese-
English machine translation evaluation. The 
Pearson? correlation coefficients are computed 
according to different automatic evaluation 
methods vs. human assigned adequacy and 
fluency. BLEU1, 4, and 12 are BLEU with 
maximum n-gram lengths of 1, 4, and 12 
respectively. GTM10, 20, and 30 are GTM with 
exponents of 1.0, 2.0, and 3.0 respectively. 95% 
confidence intervals are estimated using bootstrap 
resampling (Davison and Hinkley 1997). From the 
BLEU group, we found that shorter BLEU has better 
adequacy correlation while longer BLEU has better 
fluency correlation. GTM with smaller exponent 
has better adequacy correlation and GTM with 
larger exponent has better fluency correlation. 
NIST is very good in adequacy correlation but not 
as good as GTM30 in fluency correlation. Based 
on these observations, we are not able to conclude 
which metric is the best because it depends on the 
manual evaluation criteria. This results also 
indicate that high correlation between human and 
automatic scores in both adequacy and fluency 
cannot always been achieved at the same time. 
The best performing metrics in fluency 
according to Table 1 are BLEU12 and GTM30 
(dark/green cells). However, many metrics are 
statistically equivalent (gray cells) to them when 
we factor in the 95% confidence intervals. For 
example, even PER is as good as BLEU12 in 
adequacy. One reason for this might be due to data 
sparseness since only 8 systems are available. 
The other potential problem for correlation 
analysis of human vs. automatic framework is that 
high corpus-level correlation might not translate to 
high sentence-level correlation. However, high 
sentence-level correlation is often an important 
property that machine translation researchers look 
for. For example, candidate translations shorter 
than 12 words would have zero BLEU12 score but 
BLEU12 has the best correlation with human 
judgment in fluency as shown in Table 1. 
In order to evaluate the ever increasing number 
of automatic evaluation metrics for machine 
translation objectively, efficiently, and reliably, we 
introduce a new evaluation method: ORANGE. We 
describe ORANGE in details in Section 2 and 
briefly introduce three new automatic metrics that 
will be used in comparisons in Section 3. The 
results of comparing several existing automatic 
metrics and the three new automatic metrics using 
ORANGE will be presented in Section 4. We 
conclude this paper and discuss future directions in 
Section 5. 
2 ORANGE 
Intuitively a good evaluation metric should give 
higher score to a good translation than a bad one. 
Therefore, a good translation should be ranked 
higher than a bad translation based their scores. 
One basic assumption of all automatic evaluation 
metrics for machine translation is that reference 
translations are good translations and the more a 
machine translation is similar to its reference 
translations the better. We adopt this assumption 
and add one more assumption that automatic 
translations are usually worst than their reference 
translations. Therefore, reference translations 
should be ranked higher than machine translations 
on average if a good automatic evaluation metric is 
used. Based on these assumptions, we propose a 
new automatic evaluation method for evaluation of 
automatic machine translation metrics as follows: 
 
Given a source sentence, its machine 
translations, and its reference translations, we 
compute the average rank of the reference 
translations within the combined machine and 
reference translation list. For example, a 
statistical machine translation system such as 
ISI?s AlTemp SMT system (Och 2003) can 
generate a list of n-best alternative translations 
given a source sentence. We compute the 
automatic scores for the n-best translations 
and their reference translations. We then rank 
these translations, calculate the average rank 
of the references in the n-best list, and 
compute the ratio of the average reference 
rank to the length of the n-best list. We call 
this ratio ?ORANGE? (Oracle1 Ranking for 
Gisting Evaluation) and the smaller the ratio 
is, the better the automatic metric is. 
 
There are several advantages of the proposed 
ORANGE evaluation method: 
? No extra human involvement ? ORANGE 
uses the existing human references but not 
human evaluations. 
? Applicable on sentence-level ? Diagnostic 
error analysis on sentence-level is naturally 
provided. This is a feature that many 
machine translation researchers look for. 
? Many existing data points ? Every sentence 
is a data point instead of every system 
(corpus-level). For example, there are 919 
sentences vs. 8 systems in the 2003 NIST 
Chinese-English machine translation 
evaluation. 
? Only one objective function to optimize ? 
Minimize a single ORANGE score instead of 
maximize Pearson?s correlation coefficients 
between automatic scores and human 
judgments in adequacy, fluency, or other 
quality metrics.  
? A natural fit to the existing statistical 
machine translation framework ? A metric 
that ranks a good translation high in an n-
best list could be easily integrated in a 
minimal error rate statistical machine 
translation training framework (Och 2003). 
The overall system performance in terms of 
                                                     
1 Oracles refer to the reference translations used in 
the evaluation procedure. 
Method Pearson 95%L 95%U Pearson 95%L 95%U
BLEU1 0.86 0.83 0.89 0.81 0.75 0.86
BLEU4 0.77 0.72 0.81 0.86 0.81 0.90
BLEU12 0.66 0.60 0.72 0.87 0.76 0.93
NIST 0.89 0.86 0.92 0.81 0.75 0.87
WER 0.47 0.41 0.53 0.69 0.62 0.75
PER 0.67 0.62 0.72 0.79 0.74 0.85
GTM10 0.82 0.79 0.85 0.73 0.66 0.79
GTM20 0.77 0.73 0.81 0.86 0.81 0.90
GTM30 0.74 0.70 0.78 0.87 0.81 0.91
Adequacy Fluency
Table 1. Pearson's correlation analysis of 8 
machine translation systems in 2003 NIST 
Chinese-English machine translation 
evaluation. 
generating more human like translations 
should also be improved.  
Before we demonstrate how to use ORANGE to 
evaluate automatic metrics, we briefly introduce 
three new metrics in the next section. 
3 Three New Metrics 
ROUGE-L and ROUGE-S are described in details 
in Lin and Och (2004). Since these two metrics are 
relatively new, we provide short summaries of 
them in Section 3.1 and Section 3.3 respectively. 
ROUGE-W, an extension of ROUGE-L, is new and 
is explained in details in Section 3.2. 
3.1 ROUGE-L: Longest Common Sub-
sequence 
Given two sequences X and Y, the longest 
common subsequence (LCS) of X and Y is a 
common subsequence with maximum length 
(Cormen et al 1989). To apply LCS in machine 
translation evaluation, we view a translation as a 
sequence of words. The intuition is that the longer 
the LCS of two translations is, the more similar the 
two translations are. We propose using LCS-based 
F-measure to estimate the similarity between two 
translations X of length m and Y of length n, 
assuming X is a reference translation and Y is a 
candidate translation, as follows: 
 
Rlcs m
YXLCS ),(
=       (1) 
Plcs n
YXLCS ),(
=       (2) 
Flcs
lcslcs
lcslcs
PR
PR
2
2 )1(
?
?
+
+
=   (3) 
 
Where LCS(X,Y) is the length of a longest 
common subsequence of X and Y, and ? = Plcs/Rlcs 
when ?Flcs/?Rlcs_=_?Flcs/?Plcs. We call the LCS-
based F-measure, i.e. Equation 3, ROUGE-L. 
Notice that ROUGE-L is 1 when X = Y since 
LCS(X,Y) = m or n; while ROUGE-L is zero when 
LCS(X,Y) = 0, i.e. there is nothing in common 
between X and Y.  
One advantage of using LCS is that it does not 
require consecutive matches but in-sequence 
matches that reflect sentence level word order as n-
grams. The other advantage is that it automatically 
includes longest in-sequence common n-grams, 
therefore no predefined n-gram length is necessary.  
By only awarding credit to in-sequence unigram 
matches, ROUGE-L also captures sentence level 
structure in a natural way. Consider the following 
example: 
 
S1. police killed the gunman 
S2. police kill the gunman 
S3. the gunman kill police 
 
Using S1 as the reference translation, S2 has a 
ROUGE-L score of 3/4 = 0.75 and S3 has a ROUGE-
L score of 2/4 = 0.5, with ? = 1. Therefore S2 is 
better than S3 according to ROUGE-L. This 
example illustrated that ROUGE-L can work 
reliably at sentence level. However, LCS suffers 
one disadvantage: it only counts the main in-
sequence words; therefore, other alternative LCSes 
and shorter sequences are not reflected in the final 
score. In the next section, we introduce ROUGE-W. 
3.2 ROUGE-W: Weighted Longest Common 
Subsequence 
LCS has many nice properties as we have 
described in the previous sections. Unfortunately, 
the basic LCS also has a problem that it does not 
differentiate LCSes of different spatial relations 
within their embedding sequences. For example, 
given a reference sequence X and two candidate 
sequences Y1 and Y2 as follows: 
 
X:  [A B C D E F G] 
Y1: [A B C D H I K] 
Y2:  [A H B K C I D] 
 
Y1 and Y2 have the same ROUGE-L score. 
However, in this case, Y1 should be the better 
choice than Y2 because Y1 has consecutive matches. 
To improve the basic LCS method, we can simply 
remember the length of consecutive matches 
encountered so far to a regular two dimensional 
dynamic program table computing LCS. We call 
this weighted LCS (WLCS) and use k to indicate 
the length of the current consecutive matches 
ending at words xi and yj. Given two sentences X 
and Y, the recurrent relations can be written as 
follows: 
 
(1) If xi = yj Then 
// the length of consecutive matches at 
// position i-1 and j-1 
k = w(i-1,j-1) 
c(i,j) = c(i-1,j-1) + f(k+1) ? f(k) 
// remember the length of consecutive 
// matches at position i, j 
w(i,j) = k+1 
(2) Otherwise 
If c(i-1,j) > c(i,j-1) Then 
c(i,j) = c(i-1,j) 
w(i,j) = 0           // no match at i, j 
Else c(i,j) = c(i,j-1) 
 w(i,j) = 0           // no match at i, j 
(3) WLCS(X,Y) = c(m,n) 
 
Where c is the dynamic programming table, 0 <= 
i <= m, 0 <= j <= n, w is the table storing the 
length of consecutive matches ended at c table 
position i and j, and f is a function of consecutive 
matches at the table position, c(i,j). Notice that by 
providing different weighting function f, we can 
parameterize the WLCS algorithm to assign 
different credit to consecutive in-sequence 
matches.  
The weighting function f must have the property 
that f(x+y) > f(x) + f(y) for any positive integers x 
and y. In other words, consecutive matches are 
awarded more scores than non-consecutive 
matches. For example, f(k)-=-?k ? ? when k >= 0, 
and ?, ? > 0. This function charges a gap penalty 
of ?? for each non-consecutive n-gram sequences. 
Another possible function family is the polynomial 
family of the form k? where -? > 1. However, in 
order to normalize the final ROUGE-W score, we 
also prefer to have a function that has a close form 
inverse function. For example, f(k)-=-k2 has a close 
form inverse function f -1(k)-=-k1/2. F-measure 
based on WLCS can be computed as follows, 
given two sequences X of length m and Y of length 
n: 
Rwlcs ???
?
???
?
=
?
)(
),(1
mf
YXWLCSf       (4) 
Pwlcs ???
?
???
?
=
?
)(
),(1
nf
YXWLCSf       (5) 
Fwlcs  
wlcswlcs
wlcswlcs
PR
PR
2
2 )1(
?
?
+
+
=           (6) 
   
 f -1 is the inverse function of f. We call the 
WLCS-based F-measure, i.e. Equation 6, ROUGE-
W. Using Equation 6 and f(k)-=-k2 as the 
weighting function, the ROUGE-W scores for 
sequences Y1 and Y2 are 0.571 and 0.286 
respectively. Therefore, Y1 would be ranked 
higher than Y2 using WLCS. We use the 
polynomial function of the form k? in the 
experiments described in Section 4 with the 
weighting factor ? varying from 1.1 to 2.0 with 0.1 
increment. ROUGE-W is the same as ROUGE-L 
when ? is set to 1. 
In the next section, we introduce the skip-bigram 
co-occurrence statistics. 
3.3 ROUGE-S: Skip-Bigram Co-Occurrence 
Statistics 
Skip-bigram is any pair of words in their sentence 
order, allowing for arbitrary gaps. Skip-bigram co-
occurrence statistics measure the overlap of skip-
bigrams between a candidate translation and a set 
of reference translations. Using the example given 
in Section 3.1: 
 
S1. police killed the gunman 
S2. police kill the gunman 
S3. the gunman kill police 
S4. the gunman police killed 
 
each sentence has C(4,2)2 = 6 skip-bigrams. For 
example, S1 has the following skip-bigrams: 
 
(?police killed?, ?police the?, ?police gunman?, 
?killed the?, ?killed gunman?, ?the gunman?) 
 
Given translations X of length m and Y of length 
n, assuming X is a reference translation and Y is a 
candidate translation, we compute skip-bigram-
based F-measure as follows: 
 
Rskip2 )2,(
),(2
mC
YXSKIP
=           (7) 
Pskip2 )2,(
),(2
nC
YXSKIP
=           (8) 
Fskip2 
2
2
2
22
2 )1(
skipskip
skipskip
PR
PR
?
?
+
+
=   (9) 
 
Where SKIP2(X,Y) is the number of skip-bigram 
matches between X and Y, ? = Pskip2/Rskip2 when 
?Fskip2/?Rskip2_=_?Fskip2/?Pskip2, and  C is the 
combination function. We call the skip-bigram-
based F-measure, i.e. Equation 9, ROUGE-S. Using 
Equation 9 with ? = 1 and S1 as the reference, S2?s 
ROUGE-S score is 0.5, S3 is 0.167, and S4 is 0.333. 
Therefore, S2 is better than S3 and S4, and S4 is 
better than S3.  
One advantage of skip-bigram vs. BLEU is that it 
does not require consecutive matches but is still 
sensitive to word order. Comparing skip-bigram 
with LCS, skip-bigram counts all in-order 
matching word pairs while LCS only counts one 
longest common subsequence. We can limit the 
maximum skip distance, between two in-order 
words to control the admission of a skip-bigram. 
We use skip distances of 1 to 9 with increment of 1 
(ROUGE-S1 to 9) and without any skip distance 
constraint (ROUGE-S*).  
In the next section, we present the evaluations of 
BLEU, NIST, PER, WER, ROUGE-L, ROUGE-W, 
and ROUGE-S using the ORANGE evaluation 
method described in Section 2. 
                                                     
2 Combinations: C(4,2) = 4!/(2!*2!) = 6. 
4 Experiments 
Comparing automatic evaluation metrics using 
the ORANGE evaluation method is straightforward. 
To simulate real world scenario, we use n-best lists 
from ISI?s state-of-the-art statistical machine 
translation system, AlTemp (Och 2003), and the 
2002 NIST Chinese-English evaluation corpus as 
the test corpus. There are 878 source sentences in 
Chinese and 4 sets of reference translations 
provided by LDC3. For exploration study, we 
generate 1024-best list using AlTemp for 872 
source sentences. AlTemp generates less than 1024 
alternative translations for 6 out of the 878 source 
                                                     
3 Linguistic Data Consortium prepared these manual 
translations as part of the DARPA?s TIDES project. 
sentences. These 6 source sentences are excluded 
from the 1024-best set. In order to compute BLEU 
at sentence level, we apply the following 
smoothing technique: 
Add one count to the n-gram hit and total n-
gram count for n > 1. Therefore, for candidate 
translations with less than n words, they can 
still get a positive smoothed BLEU score from 
shorter n-gram matches; however if nothing 
matches then they will get zero scores. 
We call the smoothed BLEU: BLEUS. For each 
candidate translation in the 1024-best list and each 
reference, we compute the following scores: 
1. BLEUS1 to 9 
2. NIST, PER, and WER 
3. ROUGE-L 
4. ROUGE-W with weight ranging from 1.1 
to 2.0 with increment of 0.1 
5. ROUGE-S with maximum skip distance 
ranging from 0 to 9 (ROUGE-S0 to S9) 
and without any skip distance limit 
(ROUGE-S*) 
We compute the average score of the references 
and then rank the candidate translations and the 
references according to these automatic scores. 
The ORANGE score for each metric is calculated as 
the average rank of the average reference (oracle) 
score over the whole corpus (872 sentences) 
divided by the length of the n-best list plus 1. 
Assuming the length of the n-best list is N and the 
size of the corpus is S (in number of sentences), we 
compute Orange as follows: 
 
       ORANGE = 
)1(
)(
1
+
???
?
???
??
=
NS
OracleRank
S
i
i
    (10) 
 
Rank(Oraclei) is the average rank of source 
sentence i?s reference translations in n-best list i. 
Table 2 shows the results for BLEUS1 to 9. To 
assess the reliability of the results, 95% confidence 
intervals (95%-CI-L for lower bound and CI-U for 
upper bound) of average rank of the oracles are 
Method ORANGE Avg Rank 95%-CI-L  95%-CI-U
BLEUS1 35.39% 363 337 387
BLEUS2 25.51% 261 239 283
BLEUS3 23.74% 243 221 267
BLEUS4 23.13% 237 215 258
BLEUS5 23.13% 237 215 260
BLEUS6 22.91% 235 211 257
BLEUS7 22.98% 236 213 258
BLEUS8 23.20% 238 214 261
BLEUS9 23.56% 241 218 265
Table 2. ORANGE scores for BLEUS1 to 9. 
Method Pearson 95%L 95%U Pearson 95%L 95%U
BLEUS1 0.87 0.84 0.90 0.83 0.77 0.88
BLEUS2 0.84 0.81 0.87 0.85 0.80 0.90
BLEUS3 0.80 0.76 0.84 0.87 0.82 0.91
BLEUS4 0.76 0.72 0.80 0.88 0.83 0.92
BLEUS5 0.73 0.69 0.78 0.88 0.83 0.91
BLEUS6 0.70 0.65 0.75 0.87 0.82 0.91
BLEUS7 0.65 0.60 0.70 0.85 0.80 0.89
BLEUS8 0.58 0.52 0.64 0.82 0.76 0.86
BLEUS9 0.50 0.44 0.57 0.76 0.70 0.82
Adequacy Fluency
Table 3. Pearson's correlation analysis 
BLEUS1 to 9 vs. adequacy and fluency of 8 
machine translation systems in 2003 NIST 
Chinese-English machine translation 
evaluation. 
Method ORANGE Avg Rank 95%-CI-L 95%-CI-U
ROUGE-L 20.56% 211 190 234
ROUGE-W-1.1 20.45% 210 189 232
ROUGE-W-1.2 20.47% 210 186 230
ROUGE-W-1.3 20.69% 212 188 234
ROUGE-W-1.4 20.91% 214 191 238
ROUGE-W-1.5 21.17% 217 196 241
ROUGE-W-1.6 21.47% 220 199 242
ROUGE-W-1.7 21.72% 223 200 245
ROUGE-W-1.8 21.88% 224 204 246
ROUGE-W-1.9 22.04% 226 203 249
ROUGE-W-2.0 22.25% 228 206 250
Table 4. ORANGE scores for ROUGE-L and 
ROUGE-W-1.1 to 2.0. 
Method ORANGE Avg Rank 95%-CI-L 95%-CI-U
ROUGE-S0 25.15% 258 234 280
ROUGE-S1 22.44% 230 209 253
ROUGE-S2 20.38% 209 186 231
ROUGE-S3 19.81% 203 183 226
ROUGE-S4 19.66% 202 177 224
ROUGE-S5 19.95% 204 184 226
ROUGE-S6 20.32% 208 187 230
ROUGE-S7 20.77% 213 191 236
ROUGE-S8 21.42% 220 198 242
ROUGE-S9 21.92% 225 204 247
ROUGE-S* 27.43% 281 259 304
Table 5. ORANGE scores for ROUGE-S1 to 9 
and ROUGE-S*. 
estimated using bootstrap resampling (Davison and 
Hinkley). According to Table 2, BLEUS6 
(dark/green cell) is the best performer among all 
BLEUSes, but it is statistically equivalent to 
BLEUS3, 4, 5, 7, 8, and 9 with 95% of confidence. 
Table 3 shows Pearson?s correlation coefficient 
for BLEUS1 to 9 over 8 participants in 2003 NIST 
Chinese-English machine translation evaluation. 
According to Table 3, we find that shorter BLEUS 
has better correlation with adequacy. However, 
correlation with fluency increases when longer n-
gram is considered but decreases after BLEUS5. 
There is no consensus winner that achieves best 
correlation with adequacy and fluency at the same 
time. So which version of BLEUS should we use? 
A reasonable answer is that if we would like to 
optimize for adequacy then choose BLEUS1; 
however, if we would like to optimize for fluency 
then choose BLEUS4 or BLEUS5. According to 
Table 2, we know that BLEUS6 on average places 
reference translations at rank 235 in a 1024-best 
list machine translations that is significantly better 
than BLEUS1 and BLEUS2. Therefore, we have 
better chance of finding more human-like 
translations on the top of an n-best list by choosing 
BLEUS6 instead of BLEUS2. To design automatic 
metrics better than BLEUS6, we can carry out error 
analysis over the machine translations that are 
ranked higher than their references. Based on the 
results of error analysis, promising modifications 
can be identified. This indicates that the ORANGE 
evaluation method provides a natural automatic 
evaluation metric development cycle. 
Table 4 shows the ORANGE scores for ROUGE-L 
and ROUGE-W-1.1 to 2.0. ROUGE-W 1.1 does have 
better ORANGE score but it is equivalent to other 
ROUGE-W variants and ROUGE-L. Table 5 lists 
performance of different ROUGE-S variants. 
ROUGE-S4 is the best performer but is only 
significantly better than ROUGE-S0 (bigram), 
ROUGE-S1, ROUGE-S9 and ROUGE-S*. The 
relatively worse performance of ROUGE-S* might 
to due to spurious matches such as ?the the? or 
?the of?. 
Table 6 summarizes the performance of 7 
different metrics. ROUGE-S4 (dark/green cell) is 
the best with an ORANGE score of 19.66% that is 
statistically equivalent to ROUGE-L and ROUGE-
W-1.1 (gray cells) and is significantly better than 
BLEUS6, NIST, PER, and WER. Among them 
PER is the worst. 
To examine the length effect of n-best lists on  
the relative performance of automatic metrics, we 
use the AlTemp SMT system to generate a 16384-
best list and compute ORANGE scores for BLEUS4, 
PER, WER, ROUGE-L, ROUGE-W-1.2, and 
ROUGE-S4. Only 474 source sentences that have 
more than 16384 alternative translations are used 
in this experiment. Table 7 shows the results. It 
confirms that when we extend the length of the n-
best list to 16 times the size of the 1024-best, the 
relative performance of each automatic evaluation 
metric group stays the same. ROUGE-S4 is still the 
best performer. Figure 1 shows the trend of 
ORANGE scores for these metrics over N-best list 
of N from 1 to 16384 with length increment of 64. 
It is clear that relative performance of these metrics 
stay the same over the entire range. 
5 Conclusion 
In this paper we introduce a new automatic 
evaluation method, ORANGE, to evaluate automatic 
evaluation metrics for machine translations. We 
showed that the new method can be easily 
implemented and integrated with existing 
statistical machine translation frameworks. 
ORANGE assumes a good automatic evaluation 
metric should assign high scores to good 
translations and assign low scores to bad 
translations. Using reference translations as 
examples of good translations, we measure the 
quality of an automatic evaluation metric based on 
the average rank of the references within a list of 
alternative machine translations. Comparing with 
traditional approaches that require human 
judgments on adequacy or fluency, ORANGE 
requires no extra human involvement other than 
the availability of reference translations.  It also 
streamlines the process of design and error analysis 
for developing new automatic metrics. Using 
ORANGE, we have only one parameter, i.e. 
ORANGE itself, to optimize vs. two in correlation 
analysis using human assigned adequacy and 
fluency. By examining the rank position of the 
Method ORANGE Avg Rank 95%-CI-L 95%-CI-U
BLEUS6 22.91% 235 211 257
NIST 29.70% 304 280 328
PER 36.84% 378 350 403
WER 23.90% 245 222 268
ROUGE-L 20.56% 211 190 234
ROUGE-W-1.1 20.45% 210 189 232
ROUGE-S4 19.66% 202 177 224
Table 6. Summary of ORANGE scores for 7 
automatic evaluation metrics. 
Method ORANGE Avg Rank 95%-CI-L 95%-CI-U
BLEUS4 18.27% 2993 2607 3474
PER 28.95% 4744 4245 5292
WER 19.36% 3172 2748 3639
ROUGE-L 16.22% 2657 2259 3072
ROUGE-W-1.2 15.87% 2600 2216 2989
ROUGE-S4 14.92% 2444 2028 2860
Table 7. Summary of ORANGE scores for 6 
automatic evaluation metrics (16384-best list). 
references, we can easily identify the confusion set 
of the references and propose new features to 
improve automatic metrics. 
One caveat of the ORANGE method is that what 
if machine translations are as good as reference 
translations? To rule out this scenario, we can 
sample instances where machine translations are 
ranked higher than human translations. We then 
check the portion of the cases where machine 
translations are as good as the human translations.  
If the portion is small then the ORANGE method 
can be confidently applied. We conjecture that this 
is the case for the currently available machine 
translation systems. However, we plan to conduct 
the sampling procedure to verify this is indeed the 
case. 
References  
Akiba, Y., K. Imamura, and E. Sumita. 2001. Using 
Multiple Edit Distances to Automatically Rank 
Machine Translation Output. In Proceedings of the 
MT Summit VIII, Santiago de Compostela, Spain. 
Cormen, T. R., C. E. Leiserson, and R. L. Rivest. 1989. 
Introduction to Algorithms. The MIT Press. 
Davison, A. C. and D. V. Hinkley. 1997. Bootstrap 
Methods and Their Application. Cambridge 
University Press. 
Leusch, G., N. Ueffing, and H. Ney. 2003. A Novel 
String-to-String Distance Measure with Applications 
to Machine Translation Evaluation. In Proceedings of 
MT Summit IX, New Orleans, U.S.A. 
Lin, C-Y. and F.J. Och. 2004. Automatic Evaluation of 
Machine Translation Quality Using Longest 
Common Subsequence and Skip-Bigram Statistics. 
Submitted. 
Nie?en S., F.J. Och, G, Leusch, H. Ney. 2000. An 
Evaluation Tool for Machine Translation: Fast 
Evaluation for MT Research. In Proceedings of the 
2nd International Conference on Language Resources 
and Evaluation, Athens, Greece. 
NIST. 2002. Automatic Evaluation of Machine 
Translation Quality using N-gram Co-Occurrence 
Statistics.   AAAAAAAAAAA 
http://www.nist.gov/speech/tests/mt/doc 
Franz Josef Och. 2003. Minimum Error Rate Training 
for Statistical Machine Translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), Sapporo, 
Japan. 
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2001. 
Bleu: a Method for Automatic Evaluation of Machine 
Translation. IBM Research Report RC22176 
(W0109-022). 
Su, K.-Y., M.-W. Wu, and J.-S. Chang. 1992. A New 
Quantitative Quality Measure for Machine 
Translation System. In Proceedings of COLING-92, 
Nantes, France. 
Turian, J. P., L. Shen, and I. D. Melamed. 2003. 
Evaluation of Machine Translation and its 
Evaluation. In Proceedings of MT Summit IX, New 
Orleans, U.S.A. 
 
2000 4000 6000 8000 10000 12000 14000 16000
0.1
0.15
0.2
0.25
0.3
0.35
0.4
ORANGE at Different NBEST Cutoff Length avg
NBEST Cutoff Length = 1 to 16384
O
RA
NG
E
BLEUS4
ROUGE?L
PER
ROUGE?S4
WER
ROUGE?W?1?2
Figure 1. ORANGE scores for 6 metrics vs. length of n-best list from 1 to 
16384 with increment of 64.
c? 2003 Association for Computational Linguistics
A Systematic Comparison of Various
Statistical Alignment Models
Franz Josef Och? Hermann Ney?
University of Southern California RWTH Aachen
We present and compare various methods for computing word alignments using statistical or
heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della
Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and
refinements. These statistical models are compared with two heuristic models based on the Dice
coefficient. We present different methods for combining word alignments to perform a symmetriza-
tion of directed statistical alignment models. As evaluation criterion, we use the quality of the
resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate
the models on the German-English Verbmobil task and the French-English Hansards task. We
perform a detailed analysis of various design decisions of our statistical alignment system and
evaluate these on training corpora of various sizes. An important result is that refined align-
ment models with a first-order dependence and a fertility model yield significantly better results
than simple heuristic models. In the Appendix, we present an efficient training algorithm for the
alignment models presented.
1. Introduction
We address in this article the problem of finding the word alignment of a bilingual
sentence-aligned corpus by using language-independent statistical methods. There is
a vast literature on this topic, and many different systems have been suggested to
solve this problem. Our work follows and extends the methods introduced by Brown,
Della Pietra, Della Pietra, and Mercer (1993) by using refined statistical models for
the translation process. The basic idea of this approach is to develop a model of the
translation process with the word alignment as a hidden variable of this process, to
apply statistical estimation theory to compute the ?optimal? model parameters, and
to perform alignment search to compute the best word alignment.
So far, refined statistical alignment models have in general been rarely used. One
reason for this is the high complexity of these models, which makes them difficult
to understand, implement, and tune. Instead, heuristic models are usually used. In
heuristic models, the word alignments are computed by analyzing some association
score metric of a link between a source language word and a target language word.
These models are relatively easy to implement.
In this article, we focus on consistent statistical alignment models suggested in the
literature, but we also describe a heuristic association metric. By providing a detailed
description and a systematic evaluation of these alignment models, we give the reader
various criteria for deciding which model to use for a given task.
? Information Science Institute (USC/ISI), 4029 Via Marina, Suite 1001, Marina del Rey, CA 90292.
? Lehrstuhl fu?r Informatik VI, Computer Science Department, RWTH Aachen?University of Technology,
D-52056 Aachen, Germany.
20
Computational Linguistics Volume 29, Number 1
Figure 1
Example of a word alignment (VERBMOBIL task).
We propose to measure the quality of an alignment model by comparing the qual-
ity of the most probable alignment, the Viterbi alignment, with a manually produced
reference alignment. This has the advantage of enabling an automatic evaluation to be
performed. In addition, we shall show that this quality measure is a precise and reli-
able evaluation criterion that is well suited to guide designing and training statistical
alignment models.
The software used to train the statistical alignment models described in this article
is publicly available (Och 2000).
1.1 Problem Definition
We follow Brown, Della Pietra, Della Pietra, and Mercer (1993) to define alignment
as an object for indicating the corresponding words in a parallel text. Figure 1 shows
an example. Very often, it is difficult for a human to judge which words in a given
target string correspond to which words in its source string. Especially problematic
is the alignment of words within idiomatic expressions, free translations, and missing
function words. The problem is that the notion of ?correspondence? between words
is subjective. It is important to keep this in mind in the evaluation of word alignment
quality. We shall deal with this problem in Section 5.
The alignment between two word strings can be quite complicated. Often, an
alignment includes effects such as reorderings, omissions, insertions, and word-to-
phrase alignments. Therefore, we need a very general representation of alignment.
Formally, we use the following definition for alignment in this article. We are given
a source (French) string f J1 = f1, . . . , fj, . . . , fJ and a target language (English) string
eI1 = e1, . . . , ei, . . . , eI that have to be aligned. We define an alignment between the two
word strings as a subset of the Cartesian product of the word positions; that is, an
21
Och and Ney Comparison of Statistical Alignment Models
alignment A is defined as
A ? {(j, i): j = 1, . . . , J; i = 1, . . . , I} (1)
Modeling the alignment as an arbitrary relation between source and target language
positions is quite general. The development of alignment models that are able to deal
with this general representation, however, is hard. Typically, the alignment models pre-
sented in the literature impose additional constraints on the alignment representation.
Typically, the alignment representation is restricted in a way such that each source
word is assigned to exactly one target word. Alignment models restricted in this way
are similar to the concept of hidden Markov models (HMMs) in speech recognition.
The alignment mapping in such models consists of associations j ? i = aj from source
position j to target position i = aj. The alignment a
J
1 = a1, . . . , aj, . . . , aJ may contain
alignments aj = 0 with the ?empty? word e0 to account for source words that are
not aligned with any target word. Constructed in such a way, the alignment is not
a relation between source and target language positions, but only a mapping from
source to target language positions.
In Melamed (2000), a further simplification is performed that enforces a one-to-one
alignment for nonempty words. This means that the alignment mapping aJ1 must be
injective for all word positions aj > 0. Note that many translation phenomena cannot
be handled using restricted alignment representations such as this one. Especially,
methods such as Melamed?s are in principle not able to achieve a 100% recall. The
problem can be reduced through corpus preprocessing steps that perform grouping
and splitting of words.
Some papers report improvements in the alignment quality of statistical methods
when linguistic knowledge is used (Ker and Chang 1997; Huang and Choi 2000). In
these methods, the linguistic knowledge is used mainly to filter out incorrect align-
ments. In this work, we shall avoid making explicit assumptions concerning the lan-
guage used. By avoiding these assumptions, we expect our approach to be applicable
to almost every language pair. The only assumptions we make are that the parallel
text is segmented into aligned sentences and that the sentences are segmented into
words. Obviously, there are additional implicit assumptions in the models that are
needed to obtain a good alignment quality. For example, in languages with a very
rich morphology, such as Finnish, a trivial segmentation produces a high number of
words that occur only once, and every learning method suffers from a significant data
sparseness problem.
1.2 Applications
There are numerous applications for word alignments in natural language processing.
These applications crucially depend on the quality of the word alignment (Och and
Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word align-
ment methods is the automatic extraction of bilingual lexica and terminology from
corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000).
Statistical alignment models are often the basis of single-word-based statistical
machine translation systems (Berger et al 1994; Wu 1996; Wang and Waibel 1998;
Nie?en et al 1998; Garc??a-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney
2001; Germann et al 2001). In addition, these models are the starting point for re-
fined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999)
or example-based translation systems (Brown 1997). In such systems, the quality of
the machine translation output directly depends on the quality of the initial word
alignment (Och and Ney 2000).
22
Computational Linguistics Volume 29, Number 1
Another application of word alignments is in the field of word sense disambigua-
tion (Diab 2000). In Yarowsky, Ngai, and Wicentowski (2001), word alignment is used
to transfer text analysis tools such as morphologic analyzers or part-of-speech taggers
from a language, such as English, for which many tools already exist to languages for
which such resources are scarce.
1.3 Overview
In Section 2, we review various statistical alignment models and heuristic models.
We present a new statistical alignment model, a log-linear combination of the best
models of Vogel, Ney, and Tillmann (1996) and Brown, Della Pietra, Della Pietra, and
Mercer (1993). In Section 3, we describe the training of the alignment models and
present a new training schedule that yields significantly better results. In addition,
we describe how to deal with overfitting, deficient models, and very small or very
large training corpora. In Section 4, we present some heuristic methods for improving
alignment quality by performing a symmetrization of word alignments. In Section 5,
we describe an evaluation methodology for word alignment methods dealing with
the ambiguities associated with the word alignment annotation based on generalized
precision and recall measures. In Section 6, we present a systematic comparison of the
various statistical alignment models with regard to alignment quality and translation
quality. We assess the effect of training corpora of various sizes and the use of a
conventional bilingual dictionary. In the literature, it is often claimed that the refined
alignment models of Brown, Della Pietra, Della Pietra, and Mercer (1993) are not
suitable for small corpora because of data sparseness problems. We show that this is
not the case if these models are parametrized suitably. In the Appendix, we describe
some methods for efficient training of fertility-based alignment models.
2. Review of Alignment Models
2.1 General Approaches
We distinguish between two general approaches to computing word alignments: sta-
tistical alignment models and heuristic models. In the following, we describe both
types of models and compare them from a theoretical viewpoint.
The notational convention we employ is as follows. We use the symbol Pr(?)
to denote general probability distributions with (almost) no specific assumptions. In
contrast, for model-based probability distributions, we use the generic symbol p(?).
2.1.1 Statistical Alignment Models. In statistical machine translation, we try to model
the translation probability Pr(f J1 | eI1), which describes the relationship between a
source language string f J1 and a target language string e
I
1. In (statistical) alignment
models Pr(f J1, a
J
1 | eI1), a ?hidden? alignment a
J
1 is introduced that describes a mapping
from a source position j to a target position aj. The relationship between the translation
model and the alignment model is given by
Pr(f J1 | eI1) =
?
aJ1
Pr(f J1, a
J
1 | eI1) (2)
The alignment aJ1 may contain alignments aj = 0 with the empty word e0 to account
for source words that are not aligned with any target word.
In general, the statistical model depends on a set of unknown parameters ? that is
learned from training data. To express the dependence of the model on the parameter
23
Och and Ney Comparison of Statistical Alignment Models
set, we use the following notation:
Pr(f J1, a
J
1 | eI1) = p?(f
J
1, a
J
1 | eI1) (3)
The art of statistical modeling is to develop specific statistical models that capture
the relevant properties of the considered problem domain. In our case, the statistical
alignment model has to describe the relationship between a source language string
and a target language string adequately.
To train the unknown parameters ?, we are given a parallel training corpus con-
sisting of S sentence pairs {(fs, es) : s = 1, . . . , S}. For each sentence pair (fs, es), the
alignment variable is denoted by a = aJ1. The unknown parameters ? are determined
by maximizing the likelihood on the parallel training corpus:
?? = argmax
?
S
?
s=1
?
a
p?(fs, a | es) (4)
Typically, for the kinds of models we describe here, the expectation maximization (EM)
algorithm (Dempster, Laird, and Rubin 1977) or some approximate EM algorithm is
used to perform this maximization. To avoid a common misunderstanding, however,
note that the use of the EM algorithm is not essential for the statistical approach, but
only a useful tool for solving this parameter estimation problem.
Although for a given sentence pair there is a large number of alignments, we can
always find a best alignment:
a?J1 = argmax
aJ1
p??(f
J
1, a
J
1 | eI1) (5)
The alignment a?J1 is also called the Viterbi alignment of the sentence pair (f
J
1, e
I
1). (For
the sake of simplicity, we shall drop the index ? if it is not explicitly needed.)
Later in the article, we evaluate the quality of this Viterbi alignment by comparing
it to a manually produced reference alignment. The parameters of the statistical align-
ment models are optimized with respect to a maximum-likelihood criterion, which
is not necessarily directly related to alignment quality. Such an approach, however,
requires training with manually defined alignments, which is not done in the research
presented in this article. Experimental evidence shows (Section 6) that the statistical
alignment models using this parameter estimation technique do indeed obtain a good
alignment quality.
In this paper, we use Models 1 through 5 described in Brown, Della Pietra, Della
Pietra, and Mercer (1993), the hidden Markov alignment model described in Vogel,
Ney, and Tillmann (1996) and Och and Ney (2000), and a new alignment model, which
we call Model 6. All these models use a different decomposition of the probability
Pr(f J1, a
J
1 | eI1).
2.1.2 Heuristic Models. Considerably simpler methods for obtaining word alignments
use a function of the similarity between the types of the two languages (Smadja, Mc-
Keown, and Hatzivassiloglou 1996; Ker and Chang 1997; Melamed 2000). Frequently,
variations of the Dice coefficient (Dice 1945) are used as this similarity function. For
each sentence pair, a matrix including the association scores between every word at
every position is then obtained:
dice(i, j) =
2 ? C(ei, fj)
C(ei) ? C(fj)
(6)
24
Computational Linguistics Volume 29, Number 1
C(e, f ) denotes the co-occurrence count of e and f in the parallel training corpus. C(e)
and C(f ) denote the count of e in the target sentences and the count of f in the source
sentences, respectively. From this association score matrix, the word alignment is then
obtained by applying suitable heuristics. One method is to choose as alignment aj = i
for position j the word with the largest association score:
aj = argmax
i
{dice(i, j)} (7)
A refinement of this method is the competitive linking algorithm (Melamed 2000).
In a first step, the highest-ranking word position (i, j) is aligned. Then, the correspond-
ing row and column are removed from the association score matrix. This procedure is
iteratively repeated until every source or target language word is aligned. The advan-
tage of this approach is that indirect associations (i.e., words that co-occur often but
are not translations of each other) occur less often. The resulting alignment contains
only one-to-one alignments and typically has a higher precision than the heuristic
model defined in equation (7).
2.1.3 A Comparison of Statistical Models and Heuristic Models. The main advan-
tage of the heuristic models is their simplicity. They are very easy to implement and
understand. Therefore, variants of the heuristic models described above are widely
used in the word alignment literature.
One problem with heuristic models is that the use of a specific similarity function
seems to be completely arbitrary. The literature contains a large variety of different
scoring functions, some including empirically adjusted parameters. As we show in
Section 6, the Dice coefficient results in a worse alignment quality than the statistical
models.
In our view, the approach of using statistical alignment models is more coherent.
The general principle for coming up with an association score between words results
from statistical estimation theory, and the parameters of the models are adjusted such
that the likelihood of the models on the training corpus is maximized.
2.2 Statistical Alignment Models
2.2.1 Hidden Markov Alignment Model. The alignment model Pr(f J1, a
J
1 | eI1) can be
structured without loss of generality as follows:
Pr(f J1, a
J
1 | eI1) = Pr(J | eI1) ?
J
?
j=1
Pr(fj, aj | f j?11 , a
j?1
1 , e
I
1) (8)
= Pr(J | eI1) ?
J
?
j=1
Pr(aj | f j?11 , a
j?1
1 , e
I
1) ? Pr(fj | f
j?1
1 , a
j
1, e
I
1) (9)
Using this decomposition, we obtain three different probabilities: a length probability
Pr(J | eI1), an alignment probability Pr(aj | f
j?1
1 , a
j?1
1 , e
I
1) and a lexicon probability
Pr(fj | f j?11 , a
j
1, e
I
1). In the hidden Markov alignment model, we assume a first-order
dependence for the alignments aj and that the lexicon probability depends only on the
word at position aj:
Pr(aj | f j?11 , a
j?1
1 , e
I
1) = p(aj | aj?1, I) (10)
Pr(fj | f j?11 , a
j
1, e
I
1) = p(fj | eaj) (11)
25
Och and Ney Comparison of Statistical Alignment Models
Later in the article, we describe a refinement with a dependence on eaj?1 in the
alignment model. Putting everything together and assuming a simple length model
Pr(J | eI1) = p(J | I), we obtain the following basic HMM-based decomposition of
p(f J1 | eI1):
p(f J1 | eI1) = p(J | I) ?
?
aJ1
J
?
j=1
[p(aj | aj?1, I) ? p(fj | eaj)] (12)
with the alignment probability p(i | i?, I) and the translation probability p(f | e).
To make the alignment parameters independent of absolute word positions, we
assume that the alignment probabilities p(i | i?, I) depend only on the jump width
(i? i?). Using a set of non-negative parameters {c(i? i?)}, we can write the alignment
probabilities in the form
p(i | i?, I) = c(i ? i
?)
?I
i??=1 c(i
?? ? i?)
(13)
This form ensures that the alignment probabilities satisfy the normalization constraint
for each conditioning word position i?, i? = 1, . . . , I. This model is also referred to as a
homogeneous HMM (Vogel, Ney, and Tillmann 1996). A similar idea was suggested
by Dagan, Church, and Gale (1993).
In the original formulation of the hidden Markov alignment model, there is no
empty word that generates source words having no directly aligned target word. We
introduce the empty word by extending the HMM network by I empty words e2II+1.
The target word ei has a corresponding empty word ei+I (i.e., the position of the empty
word encodes the previously visited target word). We enforce the following constraints
on the transitions in the HMM network (i ? I, i? ? I) involving the empty word e0:1
p(i + I | i?, I) = p0 ? ?(i, i?) (14)
p(i + I | i? + I, I) = p0 ? ?(i, i?) (15)
p(i | i? + I, I) = p(i | i?, I) (16)
The parameter p0 is the probability of a transition to the empty word, which has to be
optimized on held-out data. In our experiments, we set p0 = 0.2.
Whereas the HMM is based on first-order dependencies p(i = aj | aj?1, I) for the
alignment distribution, Models 1 and 2 use zero-order dependencies p(i = aj | j, I, J):
? Model 1 uses a uniform distribution p(i | j, I, J) = 1/(I + 1):
Pr(f J1, a
J
1 | eI1) =
p(J | I)
(I + 1)J
?
J
?
j=1
p(fj | eaj) (17)
Hence, the word order does not affect the alignment probability.
? In Model 2, we obtain
Pr(f J1, a
J
1 | eI1) = p(J | I) ?
J
?
j=1
[p(aj | j, I, J) ? p(fj | eaj)] (18)
1 ?(i, i?) is the Kronecker function, which is one if i = i? and zero otherwise.
26
Computational Linguistics Volume 29, Number 1
To reduce the number of alignment parameters, we ignore the
dependence on J in the alignment model and use a distribution p(aj | j, I)
instead of p(aj | j, I, J).
2.3 Fertility-Based Alignment Models
In the following, we give a short description of the fertility-based alignment models
of Brown, Della Pietra, Della Pietra, and Mercer (1993). A gentle introduction can be
found in Knight (1999b).
The fertility-based alignment models (Models 3, 4, and 5) (Brown, Della Pietra,
Della Pietra, and Mercer 1993) have a significantly more complicated structure than
the simple Models 1 and 2. The fertility ?i of a word ei in position i is defined as the
number of aligned source words:
?i =
?
j
?(aj, i) (19)
The fertility-based alignment models contain a probability p(? | e) that the target word
e is aligned to ? words. By including this probability, it is possible to explicitly describe
the fact that for instance the German word u?bermorgen produces four English words
(the day after tomorrow). In particular, the fertility ? = 0 is used for prepositions
or articles that have no direct counterpart in the other language.
To describe the fertility-based alignment models in more detail, we introduce,
as an alternative alignment representation, the inverted alignments, which define a
mapping from target to source positions rather than the other way around. We allow
several positions in the source language to be covered; that is, we consider alignments
B of the form
B: i ? Bi ? {1, . . . , j, . . . , J}. (20)
An important constraint for the inverted alignment is that all positions of the source
sentence must be covered exactly once; that is, the Bi have to form a partition of the
set {1, . . . , j, . . . , J}. The number of words ?i = |Bi| is the fertility of the word ei. In the
following, Bik refers to the kth element of Bi in ascending order.
The inverted alignments BI0 are a different way to represent normal alignments
aJ1. The set B0 contains the positions of all source words that are aligned with the
empty word. Fertility-based alignment models use the following decomposition and
assumptions:2
Pr(f J1, a
J
1 | eI1) = Pr(f
J
1, B
I
0 | eI1) (21)
= Pr(B0 | BI1) ?
I
?
i=1
Pr(Bi | Bi?11 , eI1) ? Pr(f
J
1 | BI0, eI1) (22)
= p(B0 | BI1) ?
I
?
i=1
p(Bi | Bi?1, ei) ?
I
?
i=0
?
j?Bi
p(fj | ei) (23)
As might be seen from this equation, we have tacitly assumed that the set B0 of words
aligned with the empty word is generated only after the nonempty positions have
2 The original description of the fertility-based alignment models in Brown, Della Pietra, Della Pietra,
and Mercer (1993) includes a more refined derivation of the fertility-based alignment models.
27
Och and Ney Comparison of Statistical Alignment Models
been covered. The distribution p(Bi | Bi?1, ei) is different for Models 3, 4, and 5:
? In Model 3, the dependence of Bi on its predecessor Bi?1 is ignored:
p(Bi | Bi?1, ei) = p(?i | ei) ?i!
?
j?Bi
p(j | i, J) (24)
We obtain an (inverted) zero-order alignment model p(j | i, J).
? In Model 4, every word is dependent on the previous aligned word and
on the word classes of the surrounding words. First, we describe the
dependence of alignment positions. (The dependence on word classes is
for now ignored and will be introduced later.) We have two (inverted)
first-order alignment models: p=1(?j | ? ? ?) and p>1(?j | ? ? ?). The
difference between this model and the first-order alignment model in the
HMM lies in the fact that here we now have a dependence along the
j-axis instead of a dependence along the i-axis. The model p=1(?j | ? ? ?) is
used to position the first word of a set Bi, and the model p>1(?j | ? ? ?) is
used to position the remaining words from left to right:
p(Bi | Bi?1, ei) = p(?i | ei) ?p=1(Bi1 ?B?(i) | ? ? ?)
?i
?
k=2
p>1(Bik ?Bi,k?1 | ? ? ?) (25)
The function i ? i? = ?(i) gives the largest value i? < i for which |Bi? | > 0.
The symbol B?(i) denotes the average of all elements in B?(i).
? Both Model 3 and Model 4 ignore whether or not a source position has
been chosen. In addition, probability mass is reserved for source
positions outside the sentence boundaries. For both of these reasons, the
probabilities of all valid alignments do not sum to unity in these two
models. Such models are called deficient (Brown, Della Pietra, Della
Pietra, and Mercer 1993). Model 5 is a reformulation of Model 4 with a
suitably refined alignment model to avoid deficiency. (We omit the
specific formula. We note only that the number of alignment parameters
for Model 5 is significantly larger than for Model 4.)
Models 3, 4, and 5 define the probability p(B0 | BI1) as uniformly distributed for the
?0! possibilities given the number of words aligned with the empty word ?0 = |B0|.
Assuming a binomial distribution for the number of words aligned with the empty
word, we obtain the following distribution for B0:
p(B0 | BI1) = p
(
?0 |
I
?
i=1
?i
)
? 1
?0!
(26)
=
(
J ? ?0
?0
)
(1 ? p1)J?2?0 p?01 ?
1
?0!
(27)
The free parameter p1 is associated with the number of words that are aligned with
the empty word. There are ?0! ways to order the ?0 words produced by the empty
word, and hence, the alignment model of the empty word is nondeficient. As we will
28
Computational Linguistics Volume 29, Number 1
see in Section 3.2, this creates problems for Models 3 and 4. Therefore, we modify
Models 3 and 4 slightly by replacing ?0! in equation (27) with J?0 :
p(B0 | BI1) =
(
J ? ?0
?0
)
(1 ? p1)J?2?0 p?01 ?
1
J?0
(28)
As a result of this modification, the alignment models for both nonempty words and
the empty word are deficient.
2.3.1 Model 6. As we shall see, the alignment models with a first-order dependence
(HMM, Models 4 and 5) produce significantly better results than the other alignment
models. The HMM predicts the distance between subsequent source language po-
sitions, whereas Model 4 predicts the distance between subsequent target language
positions. This implies that the HMM makes use of locality in the source language,
whereas Model 4 makes use of locality in the target language. We expect to achieve
better alignment quality by using a model that takes into account both types of de-
pendencies. Therefore, we combine HMM and Model 4 in a log-linear way and call
the resulting model Model 6:
p6(f, a | e) =
p4(f, a | e)? ? pHMM(f, a | e)
?
a?,f? p4(f
?, a? | e)? ? pHMM(f?, a? | e)
(29)
Here, the interpolation parameter ? is employed to weigh Model 4 relative to the
hidden Markov alignment model. In our experiments, we use Model 4 instead of
Model 5, as it is significantly more efficient in training and obtains better results.
In general, we can perform a log-linear combination of several models pk(f, a | e),
k = 1, . . . , K by
p6(f, a | e) =
?K
k=1 pk(f, a | e)?k
?
a?,f?
?K
k=1 pk(f
?, a? | e)?k
(30)
The interpolation parameters ?k are determined in such a way that the alignment
quality on held-out data is optimized.
We use a log-linear combination instead of the simpler linear combination be-
cause the values of Pr(f, a | e) typically differ by orders of magnitude for HMM and
Model 4. In such a case, we expect the log-linear combination to be better than a linear
combination.
2.3.2 Alignment Models Depending on Word Classes. For HMM and Models 4 and
5, it is straightforward to extend the alignment parameters to include a dependence
on the word classes of the surrounding words (Och and Ney 2000). In the hidden
Markov alignment model, we allow for a dependence of the position aj on the class
of the preceding target word C(eaj?1): p(aj | aj?1, I, C(eaj?1)). Similarly, we can include
dependencies on source and target word classes in Models 4 and 5 (Brown, Della
Pietra, Della Pietra, and Mercer 1993). The categorization of the words into classes
(here: 50 classes) is performed automatically by using the statistical learning procedure
described in Kneser and Ney (1993).
2.3.3 Overview of Models. The main differences among the statistical alignment mod-
els lie in the alignment model they employ (zero-order or first-order), the fertility
model they employ, and the presence or absence of deficiency. In addition, the models
differ with regard to the efficiency of the E-step in the EM algorithm (Section 3.1).
Table 1 offers an overview of the properties of the various alignment models.
29
Och and Ney Comparison of Statistical Alignment Models
Table 1
Overview of the alignment models.
Model Alignment model Fertility model E-step Deficient
Model 1 uniform no exact no
Model 2 zero-order no exact no
HMM first-order no exact no
Model 3 zero-order yes approximative yes
Model 4 first-order yes approximative yes
Model 5 first-order yes approximative no
Model 6 first-order yes approximative yes
2.4 Computation of the Viterbi Alignment
We now develop an algorithm to compute the Viterbi alignment for each alignment
model. Although there exist simple polynomial algorithms for the baseline Models 1
and 2, we are unaware of any efficient algorithm for computing the Viterbi alignment
for the fertility-based alignment models.
For Model 2 (also for Model 1 as a special case), we obtain
a?J1 = argmax
aJ1
Pr(f J1, a
J
1 | eI1) (31)
= argmax
aJ1
?
?
?
p(J | I) ?
J
?
j=1
[p(aj | j, I) ? p(fj | eaj)]
?
?
?
(32)
=
[
argmax
aj
{p(aj | j, I) ? p(fj | eaj)}
]J
j=1
(33)
Hence, the maximization over the (I+1)J different alignments decomposes into J max-
imizations of (I + 1) lexicon probabilities. Similarly, the Viterbi alignment for Model 2
can be computed with a complexity of O(I ? J).
Finding the optimal alignment for the HMM is more complicated than for Model 1
or Model 2. Using a dynamic programming approach, it is possible to obtain the Viterbi
alignment for the HMM with a complexity of O(I2 ? J) (Vogel, Ney, and Tillmann 1996).
For the refined alignment models, however, namely, Models 3, 4, 5, and 6, max-
imization over all alignments cannot be efficiently carried out. The corresponding
search problem is NP-complete (Knight 1990a). For short sentences, a possible so-
lution could be an A* search algorithm (Och, Ueffing, and Ney 2001). In the work
presented here, we use a more efficient greedy search algorithm for the best align-
ment, as suggested in Brown, Della Pietra, Della Pietra, and Mercer (1993). The basic
idea is to compute the Viterbi alignment of a simple model (such as Model 2 or HMM).
This alignment is then iteratively improved with respect to the alignment probability
of the refined alignment model. (For further details on the greedy search algorithm,
see Brown, Della Pietra, Della Pietra, and Mercer [1993].) In the Appendix, we present
methods for performing an efficient computation of this pseudo-Viterbi alignment.
3. Training
3.1 EM Algorithm
In this section, we describe our approach to determining the model parameters ?.
Every model has a specific set of free parameters. For example, the parameters ? for
30
Computational Linguistics Volume 29, Number 1
Model 4 consist of lexicon, alignment, and fertility parameters:
? = {{p(f | e)}, {p=1(?j | ? ? ?)}, {p>1(?j | ? ? ?)}, {p(? | e)}, p1} (34)
To train the model parameters ?, we use a maximum-likelihood approach, as described
in equation (4), by applying the EM algorithm (Baum 1972). The different models are
trained in succession on the same data; the final parameter values of a simpler model
serve as the starting point for a more complex model.
In the E-step of Model 1, the lexicon parameter counts for one sentence pair (e, f)
are calculated:
c(f | e; e, f) =
?
e,f
N(e, f)
?
a
Pr(a | e, f)
?
j
?(f , fj)?(e, eaj) (35)
Here, N(e, f) is the training corpus count of the sentence pair (f, e). In the M-step, the
lexicon parameters are computed:
p(f | e) =
?
s c(f | e; fs, es)
?
s,f c(f | e; fs, es)
(36)
Similarly, the alignment and fertility probabilities can be estimated for all other align-
ment models (Brown, Della Pietra, Della Pietra, and Mercer 1993). When bootstrapping
from a simpler model to a more complex model, the simpler model is used to weigh the
alignments, and the counts are accumulated for the parameters of the more complex
model.
In principle, the sum over all (I+1)J alignments has to be calculated in the E-step.
Evaluating this sum by explicitly enumerating all alignments would be infeasible.
Fortunately, Models 1 and 2 and HMM have a particularly simple mathematical form
such that the EM algorithm can be implemented efficiently (i.e., in the E-step, it is
possible to efficiently evaluate all alignments). For the HMM, this is referred to as the
Baum-Welch algorithm (Baum 1972).
Since we know of no efficient way to avoid the explicit summation over all align-
ments in the EM algorithm in the fertility-based alignment models, the counts are
collected only over a subset of promising alignments. For Models 3 to 6, we perform
the count collection only over a small number of good alignments. To keep the training
fast, we consider only a small fraction of all alignments. We compare three different
methods for using subsets of varying sizes:
? The simplest method is to perform Viterbi training using only the best
alignment found. As the Viterbi alignment computation itself is very
time consuming for Models 3 to 6, the Viterbi alignment is computed
only approximately, using the method described in Brown, Della Pietra,
Della Pietra, and Mercer (1993).
? Al-Onaizan et al (1999) suggest using as well the neighboring
alignments of the best alignment found. (For an exact definition of the
neighborhood of an alignment, the reader is referred to the Appendix.)
? Brown, Della Pietra, Della Pietra, and Mercer (1993) use an even larger
set of alignments, including also the pegged alignments, a large set of
alignments with a high probability Pr(f J1, a
J
1 | eI1). The method for
constructing these alignments (Brown, Della Pietra, Della Pietra, and
Mercer 1993) guarantees that for each lexical relationship in every
sentence pair, at least one alignment is considered.
31
Och and Ney Comparison of Statistical Alignment Models
In Section 6, we show that by using the HMM instead of Model 2 in bootstrap-
ping the fertility-based alignment models, the alignment quality can be significantly
improved. In the Appendix, we present an efficient training algorithm of the fertility-
based alignment models.
3.2 Is Deficiency a Problem?
When using the EM algorithm on the standard versions of Models 3 and 4, we observe
that during the EM iterations more and more words are aligned with the empty word.
This results in a poor alignment quality, because too many words are aligned to the
empty word. This progressive increase in the number of words aligned with the empty
word does not occur when the other alignment models are used. We believe that this
is due to the deficiency of Model 3 and Model 4.
The use of the EM algorithm guarantees that the likelihood increases for each
iteration. This holds for both deficient and nondeficient models. For deficient models,
however, as the amount of deficiency in the model is reduced, the likelihood increases.
In Models 3 and 4 as defined in Brown, Della Pietra, Della Pietra, and Mercer (1993),
the alignment model for nonempty words is deficient, but the alignment model for
the empty word is nondeficient. Hence, the EM algorithm can increase likelihood by
simply aligning more and more words with the empty word.3
Therefore, we modify Models 3 and 4 slightly, such that the empty word also has
a deficient alignment model. The alignment probability is set to p(j | i, J) = 1/J for each
source word aligned with the empty word. Another remedy, adopted in Och and Ney
(2000), is to choose a value for the parameter p1 of the empty-word fertility and keep
it fixed.
3.3 Smoothing
To overcome the problem of overfitting on the training data and to enable the models
to cope better with rare words, we smooth the alignment and fertility probabilities. For
the alignment probabilities of the HMM (and similarly for Models 4 and 5), we perform
an interpolation with a uniform distribution p(i | j, I) = 1/I using an interpolation
parameter ?:
p?(aj | aj?1, I) = (1 ? ?) ? p(aj | aj?1, I) + ? ?
1
I
(37)
For the fertility probabilities, we assume that there is a dependence on the number
of letters g(e) of e and estimate a fertility distribution p(? | g) using the EM algorithm.
Typically, longer words have a higher fertility. By making this assumption, the model
can learn that the longer words usually have a higher fertility than shorter words.
Using an interpolation parameter ?, the fertility distribution is then computed as
p?(? | e) =
(
1 ? ?
? + n(e)
)
? p(? | e) + ?
? + n(e)
? p(? | g(e)) (38)
Here, n(e) denotes the frequency of e in the training corpus. This linear interpolation
ensures that for frequent words (i.e., n(e)  ?), the specific distribution p(? | e) dom-
inates, and that for rare words (i.e., n(e)  ?), the general distribution p(? | g(e))
dominates.
The interpolation parameters ? and ? are determined in such a way that the
alignment quality on held-out data is optimized.
3 This effect did not occur in Brown, Della Pietra, Della Pietra, and Mercer (1993), as Models 3 and 4
were not trained directly.
32
Computational Linguistics Volume 29, Number 1
3.4 Bilingual Dictionary
A conventional bilingual dictionary can be considered an additional knowledge source
that can be used in training. We assume that the dictionary is a list of word strings
(e, f). The entries for each language can be a single word or an entire phrase.
To integrate a dictionary into the EM algorithm, we compare two different
methods:
? Brown, Della Pietra, Della Pietra, Goldsmith, et al (1993) developed a
multinomial model for the process of constructing a dictionary (by a
human lexicographer). By applying suitable simplifications, the method
boils down to adding every dictionary entry (e, f) to the training corpus
with an entry-specific count called effective multiplicity, expressed as
?(e, f):
?(e, f) =
?(e) ? p(f | e)
1 ? e?(e)?p(f|e) (39)
In this section, ?(e) is an additional parameter describing the size of the
sample that is used to estimate the model p(f | e). This count is then
used instead of N(e, f) in the EM algorithm as shown in equation (35).
? Och and Ney (2000) suggest that the effective multiplicity of a dictionary
entry be set to a large value ?+  1 if the lexicon entry actually occurs
in one of the sentence pairs of the bilingual corpus and to a low value
otherwise:
?(e, f) =
{
?+ if e and f co-occur
?? otherwise
(40)
As a result, only dictionary entries that indeed occur in the training
corpus have a large effect in training. The motivation behind this is to
avoid a deterioration of the alignment as a result of out-of-domain
dictionary entries. Every entry in the dictionary that does co-occur in the
training corpus can be assumed correct and should therefore obtain a
high count. We set ?? = 0.
4. Symmetrization
In this section, we describe various methods for performing a symmetrization of our
directed statistical alignment models by applying a heuristic postprocessing step that
combines the alignments in both translation directions (source to target, target to
source).
The baseline alignment model does not allow a source word to be aligned with
more than one target word. Therefore, lexical correspondences like that of the German
compound word Zahnarzttermin with the English dentist?s appointment cause problems,
because a single source word must be mapped to two or more target words. Therefore,
the resulting Viterbi alignment of the standard alignment models has a systematic loss
in recall.
To solve this problem, we perform training in both translation directions (source to
target, target to source). As a result, we obtain two alignments aJ1 and b
I
1 for each pair
of sentences in the training corpus. Let A1 = {(aj, j) | aj > 0} and A2 = {(i, bi) | bi > 0}
denote the sets of alignments in the two Viterbi alignments. To increase the quality
of the alignments, we combine A1 and A2 into one alignment matrix A using the
33
Och and Ney Comparison of Statistical Alignment Models
following combination methods:
? Intersection: A = A1 ? A2.
? Union: A = A1 ? A2.
? Refined method: In a first step, the intersection A = A1 ? A2 is
determined. The elements of this intersection result from both Viterbi
alignments and are therefore very reliable. Then, we extend the
alignment A iteratively by adding alignments (i, j) occurring only in the
alignment A1 or in the alignment A2 if neither fj nor ei has an alignment
in A, or if both of the following conditions hold:
? The alignment (i, j) has a horizontal neighbor (i ? 1, j), (i + 1, j)
or a vertical neighbor (i, j ? 1), (i, j + 1) that is already in A.
? The set A ? {(i, j)} does not contain alignments with both
horizontal and vertical neighbors.
Obviously, the intersection of the two alignments yields an alignment consisting of
only one-to-one alignments with a higher precision and a lower recall than either
one separately. The union of the two alignments yields a higher recall and a lower
precision of the combined alignment than either one separately. Whether a higher
precision or a higher recall is preferred depends on the final application for which
the word alignment is intended. In applications such as statistical machine translation
(Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000),
so an alignment union would probably be chosen. In lexicography applications, we
might be interested in alignments with a very high precision obtained by performing
an alignment intersection.
5. Evaluation Methodology
In the following, we present an annotation scheme for single-word-based alignments
and a corresponding evaluation criterion.
It is well known that manually performing a word alignment is a complicated
and ambiguous task (Melamed 1998). Therefore, in performing the alignments for
the research presented here, we use an annotation scheme that explicitly allows for
ambiguous alignments. The persons conducting the annotation are asked to specify
alignments of two different kinds: an S (sure) alignment, for alignments that are un-
ambiguous, and a P (possible) alignment, for ambiguous alignments. The P label is
used especially to align words within idiomatic expressions and free translations and
missing function words (S ? P).
The reference alignment thus obtained may contain many-to-one and one-to-many
relationships. Figure 2 shows an example of a manually aligned sentence with S and
P labels.
The quality of an alignment A = {(j, aj) | aj > 0} is then computed by appropriately
redefined precision and recall measures:
recall =
|A ? S|
|S| , precision =
|A ? P|
|A| (41)
and the following alignment error rate (AER), which is derived from the well-known
F-measure:
AER(S, P; A) = 1 ? |A ? S|+ |A ? P||A|+ |S| (42)
34
Computational Linguistics Volume 29, Number 1
Figure 2
A manual alignment with S (filled squares) and P (unfilled squares) connections.
These definitions of precision, recall and the AER are based on the assumption
that a recall error can occur only if an S alignment is not found and a precision error
can occur only if the found alignment is not even P.
The set of sentence pairs for which the manual alignment is produced is randomly
selected from the training corpus. It should be emphasized that all the training of the
models is performed in a completely unsupervised way (i.e., no manual alignments
are used). From this point of view, there is no need to have a test corpus separate from
the training corpus.
Typically, the annotation is performed by two human annotators, producing sets
S1, P1, S2, P2. To increase the quality of the resulting reference alignment, the anno-
tators are presented with the mutual errors and asked to improve their alignments
where possible. (Mutual errors of the two annotators A and B are the errors in the
alignment of annotator A if we assume the alignment of annotator B as reference and
the errors in the alignment of annotator B if we assume the alignment of annotator A
as reference.) From these alignments, we finally generate a reference alignment that
contains only those S connections on which both annotators agree and all P connec-
tions from both annotators. This can be accomplished by forming the intersection of
the sure alignments (S = S1?S2) and the union of the possible alignments (P = P1?P2),
respectively. By generating the reference alignment in this way, we obtain an alignment
error rate of 0 percent when we compare the S alignments of every single annotator
with the combined reference alignment.
6. Experiments
We present in this section results of experiments involving the Verbmobil and Hansards
tasks. The Verbmobil task (Wahlster 2000) is a (German-English) speech translation task
35
Och and Ney Comparison of Statistical Alignment Models
Table 2
Corpus characteristics of the Verbmobil task.
German English
Training corpus Sentences 34,446 ? 34K
Words 329,625 343,076
Vocabulary 5,936 3,505
Singletons 2,600 1,305
Bilingual dictionary Entries 4,404
Words 4,758 5,543
Test corpus Sentences 354
Words 3,233 3,109
Table 3
Corpus characteristics of the Hansards task.
French English
Training corpus Sentences 1470K
Words 24.33M 22.16M
Vocabulary 100,269 78,332
Singletons 40,199 31,319
Bilingual dictionary Entries 28,701
Words 28,702 30,186
Test corpus Sentences 500
Words 8,749 7,946
in the domain of appointment scheduling, travel planning, and hotel reservation. The
bilingual sentences used in training are correct transcriptions of spoken dialogues.
However, they include spontaneous speech effects such as hesitations, false starts, and
ungrammatical phrases. The French-English Hansards task consists of the debates in
the Canadian parliament. This task has a very large vocabulary of about 100,000 French
words and 80,000 English words.4
Statistics for the two corpora are shown in Tables 2 and 3. The number of running
words and the vocabularies are based on full-form words and the punctuation marks.
We produced smaller training corpora by randomly choosing 500, 2,000 and 8,000
sentences from the Verbmobil task and 500, 8,000, and 128,000 sentences from the
Hansards task.
For both tasks, we manually aligned a randomly chosen subset of the training
corpus. From this subset of the corpus, the first 100 sentences are used as the de-
velopment corpus to optimize the model parameters that are not trained via the EM
4 We do not use the Blinker annotated corpus described in Melamed (1998), since the domain is very
special (the Bible) and a different annotation methodology is used.
36
Computational Linguistics Volume 29, Number 1
Table 4
Comparison of alignment error rate percentages for various training schemes (Verbmobil task;
Dice+C: Dice coefficient with competitive linking).
Size of training corpus
Model Training scheme 0.5K 2K 8K 34K
Dice 28.4 29.2 29.1 29.0
Dice+C 21.5 21.8 20.1 20.4
Model 1 15 19.3 19.0 17.8 17.0
Model 2 1525 27.7 21.0 15.8 13.5
HMM 15H5 19.1 15.4 11.4 9.2
Model 3 152533 25.8 18.4 13.4 10.3
15H533 18.1 14.3 10.5 8.1
Model 4 15253343 23.4 14.6 10.0 7.7
15H543 17.3 11.7 9.1 6.5
15H53343 16.8 11.7 8.4 6.3
Model 5 15H54353 17.3 11.4 8.7 6.2
15H5334353 16.9 11.8 8.5 5.8
Model 6 15H54363 17.2 11.3 8.8 6.1
15H5334363 16.4 11.7 8.0 5.7
algorithm (e.g., the smoothing parameters). The remaining sentences are used as the
test corpus.
The sequence of models used and the number of training iterations used for each
model is referred to in the following as the training scheme. Our standard train-
ing scheme on Verbmobil is 15H5334363. This notation indicates that five iterations
of Model 1, five iterations of HMM, three iterations of Model 3, three iterations
of Model 4, and three iterations of Model 6 are performed. On Hansards, we use
15H10334363. This training scheme typically gives very good results and does not lead
to overfitting. We use the slightly modified versions of Model 3 and Model 4 described
in Section 3.2 and smooth the fertility and the alignment parameters. In the E-step of
the EM algorithm for the fertility-based alignment models, we use the Viterbi align-
ment and its neighborhood. Unless stated otherwise, no bilingual dictionary is used
in training.
6.1 Models and Training Schemes
Tables 4 and 5 compare the alignment quality achieved using various models and
training schemes. In general, we observe that the refined models (Models 4, 5, and 6)
yield significantly better results than the simple Model 1 or Dice coefficient. Typically,
the best results are obtained with Model 6. This holds across a wide range of sizes
for the training corpus, from an extremely small training corpus of only 500 sentences
up to a training corpus of 1.5 million sentences. The improvement that results from
using a larger training corpus is more significant, however, if more refined models are
used. Interestingly, even on a tiny corpus of only 500 sentences, alignment error rates
under 30% are achieved for all models, and the best models have error rates somewhat
under 20%.
We observe that the alignment quality obtained with a specific model heavily
depends on the training scheme that is used to bootstrap the model.
37
Och and Ney Comparison of Statistical Alignment Models
Table 5
Comparison of alignment error rate percentages for various training schemes (Hansards task;
Dice+C: Dice coefficient with competitive linking).
Size of training corpus
Model Training scheme 0.5K 8K 128K 1.47M
Dice 50.9 43.4 39.6 38.9
Dice+C 46.3 37.6 35.0 34.0
Model 1 15 40.6 33.6 28.6 25.9
Model 2 1525 46.7 29.3 22.0 19.5
HMM 15H5 26.3 23.3 15.0 10.8
Model 3 152533 43.6 27.5 20.5 18.0
15H533 27.5 22.5 16.6 13.2
Model 4 15253343 41.7 25.1 17.3 14.1
15H53343 26.1 20.2 13.1 9.4
15H543 26.3 21.8 13.3 9.3
Model 5 15H54353 26.5 21.5 13.7 9.6
15H5334353 26.5 20.4 13.4 9.4
Model 6 15H54363 26.0 21.6 12.8 8.8
15H5334363 25.9 20.3 12.5 8.7
Figure 3
Comparison of alignment error rate (in percent) for Model 1 and Dice coefficient (left: 34K
Verbmobil task, right: 128K Hansards task).
6.2 Heuristic Models versus Model 1
We pointed out in Section 2 that from a theoretical viewpoint, the main advantage
of statistical alignment models in comparison to heuristic models is the well-founded
mathematical theory that underlies their parameter estimation. Tables 4 and 5 show
that the statistical alignment models significantly outperform the heuristic Dice coef-
ficient and the heuristic Dice coefficient with competitive linking (Dice+C). Even the
simple Model 1 achieves better results than the two Dice coefficient models.
It is instructive to analyze the alignment quality obtained in the EM training of
Model 1. Figure 3 shows the alignment quality over the iteration numbers of Model 1.
We see that the first iteration of Model 1 achieves significantly worse results than the
Dice coefficient, but by only the second iteration, Model 1 gives better results than the
Dice coefficient.
38
Computational Linguistics Volume 29, Number 1
Table 6
Effect of using more alignments in training fertility models on alignment error rate (Verbmobil
task). Body of table presents error rate percentages.
Size of training corpus
Training scheme Alignment set 0.5K 2K 8K 34K
Viterbi 17.8 12.6 8.6 6.6
15H5334363 +neighbors 16.4 11.7 8.0 5.7
+pegging 16.4 11.2 8.2 5.7
Viterbi 24.1 16.0 11.6 8.6
1525334353 +neighbors 22.9 14.2 9.8 7.6
+pegging 22.0 13.3 9.7 6.9
Table 7
Effect of using more alignments in training fertility models on alignment error rate (Hansards
task). Body of table presents error rate percentages.
Size of training corpus
Training scheme Alignment set 0.5K 8K 128K
Viterbi 25.8 20.3 12.6
15H10334363 +neighbors 25.9 20.3 12.5
+pegging 25.8 19.9 12.6
Viterbi 41.9 25.1 17.6
1525334353 +neighbors 41.7 24.8 16.1
+pegging 41.2 23.7 15.8
6.3 Model 2 versus HMM
An important result of these experiments is that the hidden Markov alignment model
achieves significantly better results than Model 2. We attribute this to the fact that the
HMM is a homogeneous first-order alignment model, and such models are able to
better represent the locality and monotonicity properties of natural languages. Both
models have the important property of allowing an efficient implementation of the
EM algorithm (Section 3). On the largest Verbmobil task, the HMM achieves an im-
provement of 3.8% over Model 2. On the largest Hansards task, the improvement is
8.7%. Interestingly, this advantage continues to hold after bootstrapping more refined
models. On Model 4, the improvement is 1.4% and 4.8%, respectively.
We conclude that it is important to bootstrap the refined alignment models with
good initial parameters. Obviously, if we use Model 2 for bootstrapping, we eventually
obtain a poor local optimum.
6.4 The Number of Alignments in Training
In Tables 6 and 7, we compare the results obtained by using different numbers of
alignments in the training of the fertility-based alignment models. We compare the
three different approaches described in Section 3: using only the Viterbi alignment,
using in addition the neighborhood of the Viterbi alignment, and using the pegged
alignments. To reduce the training time, we restrict the number of pegged alignments
by using only those in which Pr(f, a | e) is not much smaller than the probability of the
Viterbi alignment. This reduces the training time drastically. For the large Hansards
39
Och and Ney Comparison of Statistical Alignment Models
Table 8
Computing time on the 34K Verbmobil task (on 600 MHz Pentium III machine).
Seconds per iteration
Alignment set Model 3 Model 4 Model 5
Viterbi 48.0 251.0 248.0
+neighbors 101.0 283.0 276.0
+pegging 129.0 3,348.0 3,356.0
Table 9
Effect of smoothing on alignment error rate (Verbmobil task, Model 6). Body of table presents
error rate percentages.
Size of training corpus
Smoothing method 0.5K 2K 8K 34K
None 19.7 14.9 10.9 8.3
Fertility 18.4 14.3 10.3 8.0
Alignment 16.8 13.2 9.1 6.4
Alignment and fertility 16.4 11.7 8.0 5.7
corpus, however, there still is an unacceptably large training time. Therefore, we report
the results for only up to 128,000 training sentences.
The effect of pegging strongly depends on the quality of the starting point used
for training the fertility-based alignment models. If we use Model 2 as the starting
point, we observe a significant improvement when we use the neighborhood align-
ments and the pegged alignments. If we use only the Viterbi alignment, the results are
significantly worse than using additionally the neighborhood of the Viterbi alignment.
If we use HMM as the starting point, we observe a much smaller effect. We conclude
that using more alignments in training is a way to avoid a poor local optimum.
Table 8 shows the computing time for performing one iteration of the EM algo-
rithm. Using a larger set of alignments increases the training time for Model 4 and
Model 5 significantly. Since using the pegging alignments yields only a moderate
improvement in performance, all following results are obtained by using the neigh-
borhood of the Viterbi alignment without pegging.
6.5 Effect of Smoothing
Tables 9 and 10 show the effect on the alignment error rate of smoothing the alignment
and fertility probabilities. We observe a significant improvement when we smooth
the alignment probabilities and a minor improvement when we smooth the fertility
probabilities. An analysis of the alignments shows that smoothing the fertility proba-
bilities significantly reduces the frequently occurring problem of rare words forming
?garbage collectors? in that they tend to align with too many words in the other
language (Brown, Della Pietra, Della Pietra, Goldsmith, et al 1993).
Without smoothing, we observe early overfitting: The alignment error rate in-
creases after the second iteration of HMM, as shown in Figure 4. On the Verbmobil
task, the best alignment error rate is obtained in the second iteration. On the Hansards
task, the best alignment error rate is obtained in the sixth iteration. In iterations sub-
sequent to the second on the Verbmobil task and the sixth on the Hansards task, the
alignment error rate increases significantly. With smoothing of the alignment param-
40
Computational Linguistics Volume 29, Number 1
Figure 4
Overfitting on the training data with the hidden Markov alignment model using various
smoothing parameters (top: 34K Verbmobil task, bottom: 128K Hansards task).
41
Och and Ney Comparison of Statistical Alignment Models
Table 10
Effect of smoothing on alignment error rate (Hansards task, Model 6). Body of table presents
error rate percentages.
Size of training corpus
Smoothing method 0.5K 8K 128K 1470K
None 28.6 23.3 13.3 9.5
Fertility 28.3 22.5 12.7 9.3
Alignment 26.5 21.2 13.0 8.9
Alignment and fertility 25.9 20.3 12.5 8.7
Table 11
Effect of word classes on alignment error rate (Verbmobil task). Body of table presents error
rate percentages.
Size of training corpus
Word classes 0.5K 2K 8K 34K
No 16.5 11.7 8.0 6.3
Yes 16.4 11.7 8.0 5.7
Table 12
Effect of word classes on alignment error rate (Hansards task). Body of table presents error
rate percentages.
Size of training corpus
Word classes 0.5K 8K 128K 1470K
No 25.5 20.7 12.8 8.9
Yes 25.9 20.3 12.5 8.7
eters, we obtain a lower alignment error rate, overfitting occurs later in the process,
and its effect is smaller.
6.6 Alignment Models Depending on Word Classes
Tables 11 and 12 show the effects of including a dependence on word classes in the
alignment model, as described in Section 2.3. The word classes are always trained
on the same subset of the training corpus as is used for the training of the align-
ment models. We observe no significant improvement in performance as a result
of including dependence on word classes when a small training corpus is used. A
possible reason for this lack of improvement is that either the word classes them-
selves or the resulting large number of alignment parameters cannot be estimated
reliably using a small training corpus. When a large training corpus is used, however,
there is a clear improvement in performance on both the Verbmobil and the Hansards
tasks.
6.7 Using a Conventional Bilingual Dictionary
Tables 13 and 14 show the effect of using a conventional bilingual dictionary in training
on the Verbmobil and Hansards tasks, respectively. We compare the two methods for
using the dictionary described in Section 3.4. We observe that the method with a fixed
42
Computational Linguistics Volume 29, Number 1
Table 13
Effect of using a conventional dictionary on alignment error rate (Verbmobil task). Body of
table presents error rate percentages.
Size of training corpus
Bilingual dictionary 0.5K 2K 8K 34K
No 16.4 11.7 8.0 5.7
Yes/? var. 10.9 9.0 6.9 5.1
Yes/?+ = 8 9.7 7.6 6.0 5.1
Yes/?+ = 16 10.0 7.8 6.0 4.6
Yes/?+ = 32 10.4 8.5 6.4 4.7
Table 14
Effect of using a conventional dictionary on alignment error rate (Hansards task). Body of
table presents error rate percentages.
Size of training corpus
Bilingual dictionary 0.5K 8K 128K 1470K
No 25.9 20.3 12.5 8.7
Yes/? var. 23.3 18.3 12.3 8.6
Yes/?+ = 8 22.7 18.5 12.2 8.6
Yes/?+ = 16 23.1 18.7 12.1 8.6
Yes/?+ = 32 24.9 20.2 11.7 8.3
threshold of ?+ = 16 gives the best results. The method with a varying ? gives worse
results, but this method has one fewer parameter to be optimized on held-out data.
On small corpora, there is an improvement of up to 6.7% on the Verbmobil task
and 3.2% on the Hansards task, but when a larger training corpus is used, the im-
provements are reduced to 1.1% and 0.4%, respectively. Interestingly, the amount
of the overall improvement contributed by the use of a conventional dictionary is
small compared to the improvement achieved through the use of better alignment
models.
6.8 Generalized Alignments
In this section, we compare the results obtained using different translation directions
and using the symmetrization methods described in Section 4. Tables 15 and 16 show
precision, recall, and alignment error rate for the last iteration of Model 6 for both
translation directions. In this experiment, we use the conventional dictionary as well.
Particularly for the Verbmobil task, with the language pair German-English, we ob-
serve that for German as the source language the alignment error rate is much higher
than for English as source language. A possible reason for this difference in the align-
ment error rates is that the baseline alignment representation as a vector aJ1 does not
allow German word compounds (which occur frequently) to be aligned with more
than one English word.
The effect of merging alignments by forming the intersection, the union, or the
refined combination of the Viterbi alignments in both translation directions is shown in
Tables 17 and 18. Figure 5 shows the corresponding precision/recall graphs. By using
the refined combination, we can increase precision and recall on the Hansards task. The
lowest alignment error rate on the Hansards task is obtained by using the intersection
43
Och and Ney Comparison of Statistical Alignment Models
Table 15
Effect of training corpus size and translation direction on precision, recall, and alignment error
rate (Verbmobil task + dictionary). All figures are percentages.
English ? German German ? English
Corpus size Precision Recall AER Precision Recall AER
0.5K 87.6 93.1 10.0 77.9 80.3 21.1
2K 90.5 94.4 7.8 88.1 88.1 11.9
8K 92.7 95.7 6.0 90.2 89.1 10.3
34K 94.6 96.3 4.6 92.5 89.5 8.8
Table 16
Effect of training corpus size and translation direction on precision, recall, and alignment error
rate (Hansards task + dictionary). All figures are percentages.
English ? French French ? English
Corpus size Precision Recall AER Precision Recall AER
0.5K 73.0 83.8 23.1 68.5 79.1 27.8
8K 77.0 88.9 18.7 76.0 88.5 19.5
128K 84.5 93.5 12.1 84.6 93.3 12.2
1470K 89.4 94.7 8.6 89.1 95.2 8.6
Table 17
Effect of alignment combination on precision, recall, and alignment error rate (Verbmobil task
+ dictionary). All figures are percentages.
Intersection Union Refined method
Corpus size Precision Recall AER Precision Recall AER Precision Recall AER
0.5K 97.5 76.8 13.6 74.8 96.1 16.9 87.8 92.9 9.9
2K 97.2 85.6 8.6 84.1 96.9 10.6 91.3 94.2 7.4
8K 97.5 86.6 8.0 87.0 97.7 8.5 92.8 96.0 5.8
34K 98.1 87.6 7.2 90.6 98.4 6.0 94.0 96.9 4.7
Table 18
Effect of alignment combination on precision, recall, and alignment error rate (Hansards task +
dictionary). All figures are percentages.
Intersection Union Refined method
Corpus size Precision Recall AER Precision Recall AER Precision Recall AER
0.5K 91.5 71.3 18.7 63.4 91.6 29.0 75.5 84.9 21.1
8K 95.6 82.8 10.6 68.2 94.4 24.2 83.3 90.0 14.2
128K 96.7 90.0 6.3 77.8 96.9 16.1 89.4 94.4 8.7
1470K 96.8 92.3 5.2 84.2 97.6 11.3 91.5 95.5 7.0
44
Computational Linguistics Volume 29, Number 1
Figure 5
Effect of various symmetrization methods on precision and recall for different training corpus
sizes (top: Verbmobil task, bottom: Hansards task).
45
Och and Ney Comparison of Statistical Alignment Models
method. By forming a union or intersection of the alignments, we can obtain very high
recall or precision values on both the Hansards task and the Verbmobil task.
6.9 Effect of Alignment Quality on Translation Quality
Alignment models similar to those studied in this article have been used as a start-
ing point for refined phrase-based statistical machine translation systems (Alshawi,
Bangalore, and Douglas 1998; Och, Tillmann, and Ney 1999; Ney et al 2000). In Och
and Ney (2000), the overall result of the experimental evaluation has been that an
improved alignment quality yields an improved subjective quality of the statistical
machine translation system as well.
7. Conclusion
In this article, we have discussed in detail various statistical and heuristic word align-
ment models and described various modifications and extensions to models known in
the literature. We have developed a new statistical alignment model (Model 6) that has
yielded the best results among all the models we considered in the experiments we
have conducted. We have presented two methods for including a conventional bilin-
gual dictionary in training and described heuristic symmetrization algorithms that
combine alignments in both translation directions possible between two languages,
producing an alignment with a higher precision, a higher recall, or an improved align-
ment error rate.
We have suggested measuring the quality of an alignment model using the quality
of the Viterbi alignment compared to that achieved in a manually produced reference
alignment. This quality measure has the advantage of automatic evaluation. To pro-
duce the reference alignment, we have used a refined annotation scheme that reduces
the problems and ambiguities associated with the manual construction of a word
alignment.
We have performed various experiments to assess the effect of different alignment
models, training schemes, and knowledge sources. The key results of these experi-
ments are as follows:
? Statistical alignment models outperform the simple Dice coefficient.
? The best results are obtained with our Model 6. In general, very
important ingredients of a good model seem to be a first-order
dependence between word positions and a fertility model.
? Smoothing and symmetrization have a significant effect on the alignment
quality achieved by a particular model.
? The following methods have only a minor effect on the quality of
alignment achieved by a particular model:
? adding entries of a conventional bilingual dictionary to the
training data.
? making the alignment models dependent on word classes (as in
Models 4 and 5).
? increasing the number of alignments used in the approximation
of the EM algorithm for the fertility-based alignment models.
Further improvements in alignments are expected to be produced through the
adoption of cognates (Simard, Foster, and Isabelle 1992) and from statistical alignment
46
Computational Linguistics Volume 29, Number 1
models based on word groups rather than single words (Och, Tillmann, and Ney
1999). The use of models that explicitly deal with the hierarchical structures of natural
language is very promising (Wu 1996; Yamada and Knight 2001).
We plan to develop structured models for the lexicon, alignment, and fertility prob-
abilities using maximum-entropy models. This is expected to allow an easy integration
of more dependencies, such as in a second-order alignment model, without running
into the problem of the number of alignment parameters getting unmanageably large.
Furthermore, it will be important to verify the applicability of the statistical align-
ment models examined in this article to less similar language pairs such as Chinese-
English and Japanese-English.
Appendix: Efficient Training of Fertility-Based Alignment Models
In this Appendix, we describe some methods for efficient training of fertility-based
alignment models. The core idea is to enumerate only a small subset of good align-
ments in the E-step of the EM algorithm instead of enumerating all (I + 1)J align-
ments. This small subset of alignments is the set of neighboring alignments of the
best alignment that can be found by a greedy search algorithm. We use two operators
to transform alignments: The move operator m[i,j](a) changes aj := i, and the swap
operator s[j1,j2](a) exchanges aj1 and aj2 . The neighborhood N (a) of an alignment a is
then defined as the set of all alignments that differ by one move or one swap from
alignment a:
N (a) = {a? : ?i,j : a? = m[i,j](a) ? ?j1,j2 : a? = s[j1,j2](a)} (43)
For one step of the greedy search algorithm, we define the following hill-climbing
operator (for Model 3), which yields for an alignment a the most probable alignment
b(a) in the neighborhood N (a):
b(a) = argmax
a??N (a)
p3(a? | e, f) (44)
Similarly, we define a hill-climbing operator for the other alignment models.
Straightforward Implementation
A straightforward count collection procedure for a sentence pair (f,e) following the
description in Brown, Della Pietra, Della Pietra, and Mercer (1993) is as follows:5
1. Calculate the Viterbi alignment of Model 2: a0 := argmaxa p2(f, a | e),
n := 0.
2. While in the neighborhood N (an) an alignment a? exists with
p3(a? | e, f) > p3(an | e, f):
(a) Set an+1 to the best alignment in the neighborhood.
(b) n := n + 1.
3. Calculate
s :=
?
a?N (an)
Pr(f, a | e) (45)
5 To simplify the description, we ignore the process known as pegging, which generates a bigger number
of alignments considered in training.
47
Och and Ney Comparison of Statistical Alignment Models
4. For each alignment a in the neighborhood N (an)
(a) Calculate
p := Pr(a | e, f) (46)
=
Pr(f, a | e)
s
(47)
(b) For each j := 1 to J: Increase alignment counts
c(j | aj, m, l; e, f) := c(j | aj, m, l; e, f) + p (48)
(c) For each i := 1 to I: Increase the fertility counts with p:
c(?i | ei; e, f) := c(?i | ei; e, f) + p (49)
(d) Increase the counts for p1:
c(1; e, f) := c(1; e, f) + p ? ?0 (50)
A major part of the time in this procedure is spent on calculating the probability
Pr(a? | e, f) of an alignment a?. In general, this takes about (I + J) operations. Brown,
Della Pietra, Della Pietra, and Mercer (1993) describe a method for obtaining Pr(a? |
e, f) incrementally from Pr(a | e, f) if alignment a differs only by moves or swaps from
alignment a?. This method results in a constant number of operations that is sufficient
to calculate the score of a move or the score of a swap.
Refined Implementation: Fast Hill Climbing
Analyzing the training program reveals that most of the time is spent on the compu-
tation of the costs of moves and swaps. To reduce the number of operations required
in such computation, these values are cached in two matrices. We use one matrix for
the scores of a move aj := i:
Mij =
Pr(m[i,j](a) | e, f)
Pr(a | e, f) ? (1 ? ?(aj, i)) (51)
and an additional matrix for the scores of a swap of aj and aj? :
Sjj ? =
?
?
?
Pr(s[j,j?](a) | e, f)
Pr(a | e, f) ? (1 ? ?(aj, aj
?)) if j < j?
0 otherwise
(52)
During the hill climbing, it is sufficient, after making a move or a swap, to update
only those rows or columns in the matrix that are affected by the move or swap. For
example, when performing a move aj := i, it is necessary to
? update in matrix M the columns j? with aj? = aj or aj? = i.
? update in matrix M the rows aj and i.
? update in matrix S the rows and the columns j? with aj? = aj or aj? = i.
Similar updates have to be performed after a swap. In the count collection (step 3), it
is possible to use the same matrices as obtained in the last hill-climbing step.
By restricting in this way the number of matrix entries that need to be updated,
it is possible to reduce the number of operations in hill climbing by about one order
of magnitude.
48
Computational Linguistics Volume 29, Number 1
Refined Implementation: Fast Count Collection
The straightforward algorithm given for performing the count collection has the dis-
advantage of requiring that all alignments in the neighborhood of alignment a be
enumerated explicitly. In addition, it is necessary to perform a loop over all targets
and a loop over all source positions to update the lexicon/alignment and the fertil-
ity counts. To perform the count collection in an efficient way, we use the fact that
the alignments in the neighborhood N (a) are very similar. This allows the sharing of
many operations in the count collection process.
To efficiently obtain the alignment and lexicon probability counts, we introduce the
following auxiliary quantities that use the move and swap matrices that are available
after performing the hill climbing described above:
? probability of all alignments in the neighborhood N (a):
Pr(N (a) | e, f) =
?
a??N (a)
Pr(a? | e, f) (53)
= Pr(a | e, f) ?
?
?1 +
?
i,j
Mij +
?
j,j?
Sjj ?
?
? (54)
? probability of all alignments in the neighborhood N (a) that differ in
position j from alignment a:
Pr(Nj(a) | e, f) =
?
a??N (a)
Pr(a? | e, f)(1 ? ?(aj, a?j)) (55)
= Pr(a | e, f)
?
?
?
i
Mij +
?
j?
(Sjj ? + Sj?j)
?
? (56)
For the alignment counts c(j | i; e, f) and the lexicon counts c(f | e; e, f), we have
c(j | i; e, f) =
{
Pr(N (a) | e, f)?Pr(Nj(a) | e, f) if i=aj
Pr(a | e, f)
(
Mij +
?
j? ?(aj? , i)?(Sjj ?+Sj?j)
)
if i =aj
(57)
c(f | e; e, f) =
?
i
?
j
c(j | i; e, f) ? ?(f , fj) ? ?(e, ei) (58)
To obtain the fertility probability counts and the count for p1 efficiently, we intro-
duce the following auxiliary quantities:
? probability of all alignments that have an increased fertility for position i:
Pr(N+1i (a) | e, f) = Pr(a | f, e)
?
?
?
j
(1 ? ?(aj, i)) ? Mij
?
? (59)
? probability of all alignments that have a decreased fertility for position i:
Pr(N?1i (a) | e, f) = Pr(a | e, f)
?
?
?
j
?(aj, i)
?
i?
Mi?j
?
? (60)
49
Och and Ney Comparison of Statistical Alignment Models
? probability of all alignments that have an unchanged fertility for posi-
tion i:
Pr(N+0i (a) | e, f) = Pr(N (a) | e, f)
? Pr(N+1i (a) | e, f)? Pr(N
?1
i (a) | e, f) (61)
These quantities do not depend on swaps, since a swap does not change the fertilities
of an alignment. For the fertility counts, we have:
c(? | e; e, f) =
?
i
?(e, ei)
?
k
Pr(N+ki (a) | e, f)?(?i + k,?) (62)
For p1, we have:
c(1; e, f) =
?
k
Pr(N+k0 (a) | e, f)(?0 + k) (63)
Using the auxiliary quantities, a count collection algorithm can be formulated that
requires about O(max(I, J)2) operations. This is one order of magnitude faster than the
straightforward algorithm described above. In practice, we observe that the resulting
training is 10?20 times faster.
Acknowledgments
This work has been partially supported as
part of the Verbmobil project (contract
number 01 IV 701 T4) by the German
Federal Ministry of Education, Science,
Research and Technology and as part of the
EuTrans project (project number 30268) by
the European Union. In addition, this work
has been partially supported by the
National Science Foundation under grant
no. IIS-9820687 through the 1999 Workshop
on Language Engineering, Center for
Language and Speech Processing, Johns
Hopkins University. All work for this paper
was done at RWTH Aachen.
References
Al-Onaizan, Yaser, Jan Curin, Michael Jahr,
Kevin Knight, John D. Lafferty, I. Dan
Melamed, David Purdy, Franz J. Och,
Noah A. Smith, and David Yarowsky.
1999. Statistical machine translation. Final
Report, JHU Workshop. Available at
http://www.clsp.jhu.edu/ws99/projects
/mt/final report/mt-final-report.ps.
Alshawi, Hiyan, Srinivas Bangalore, and
Shona Douglas. 1998. Automatic
acquisition of hierarchical transduction
models for machine translation. In
COLING?ACL ?98: 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, volume 1,
pages 41?47, Montreal, Canada, August.
Baum, L. E. 1972. An inequality and
associated maximization technique in
statistical estimation for probabilistic
functions of Markov processes.
Inequalities, 3:1?8.
Berger, Adam L., Peter F. Brown, Stephen A.
Della Pietra, Vincent J. Della Pietra,
John R. Gillett, John D. Lafferty, Harry
Printz, and Lubos Ures. 1994. The
Candide system for machine translation.
In Proceedings of the ARPA Workshop on
Human Language Technology,
pages 157?162, Plainsboro, New Jersey,
March.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, M. J. Goldsmith,
J. Hajic, R. L. Mercer, and S. Mohanty.
1993. But dictionaries are data too. In
Proceedings of the ARPA Workshop on Human
Language Technology, pages 202?205,
Plainsboro, New Jersey, March.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical
machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Brown, Ralf D. 1997. Automated dictionary
extraction for ?knowledge-free?
example-based translation. In Seventh
International Conference on Theoretical and
Methodological Issues in Machine Translation
(TMI-97), pages 111?118, Santa Fe, New
Mexico, July.
Dagan, Ido, Kenneth W. Church, and
50
Computational Linguistics Volume 29, Number 1
William A. Gale. 1993. Robust bilingual
word alignment for machine aided
translation. In Proceedings of the Workshop
on Very Large Corpora, pages 1?8,
Columbus, Ohio, June.
Dempster, A. P., N. M. Laird, and D. B.
Rubin. 1977. Maximum likelihood from
incomplete data via the EM algorithm.
Journal of the Royal Statistical Society,
Series B, 39(1):1?22.
Diab, Mona. 2000. An unsupervised method
for multilingual word sense tagging using
parallel corpora: A preliminary
investigation. In ACL-2000 Workshop on
Word Senses and Multilinguality, pages 1?9,
Hong Kong, October.
Dice, Lee R. 1945. Measures of the amount
of ecologic association between species.
Journal of Ecology, 26:297?302.
Garc??a-Varea, Ismael, Francisco Casacuberta,
and Hermann Ney. 1998. An iterative,
DP-based search algorithm for statistical
machine translation. In Proceedings of the
International Conference on Spoken Language
Processing (ICSLP?98), pages 1235?1238,
Sydney, Australia, November.
Germann, Ulrich, Michael Jahr, Kevin
Knight, Daniel Marcu, and Kenji Yamada.
2001. Fast decoding and optimal decoding
for machine translation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 228?235, Toulouse, France, July.
Huang, Jin-Xia and Key-Sun Choi. 2000.
Chinese-Korean word alignment based on
linguistic comparison. In Proceedings of the
38th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 392?399, Hong Kong, October.
Ker, Sue J. and Jason S. Chang. 1997. A
class-based approach to word alignment.
Computational Linguistics, 23(2):313?343.
Kneser, Reinhard and Hermann Ney. 1993.
Improved clustering techniques for
class-based statistical language modelling.
In European Conference on Speech
Communication and Technology,
pages 973?976, Berlin, Germany,
September.
Knight, Kevin. 1999a. Decoding complexity
in word-replacement translation models.
Computational Linguistics, 25(4):607?615.
Knight, Kevin. 1999b. A Statistical MT
Tutorial Workbook. Available at
http://www.isi.edu/natural-language/
mt/wkbk.rtf.
Melamed, I. Dan. 1998. Manual annotation
of translational equivalence: The blinker
project. Technical Report 98-07, Institute
for Research in Cognitive Science,
Philadelphia.
Melamed, I. Dan. 2000. Models of
translational equivalence among words.
Computational Linguistics, 26(2):221?249.
Ney, Hermann, Sonja Nie?en, Franz J. Och,
Hassan Sawaf, Christoph Tillmann, and
Stephan Vogel. 2000. Algorithms for
statistical translation of spoken language.
IEEE Transactions on Speech and Audio
Processing, 8(1):24?36.
Nie?en, Sonja, Stephan Vogel, Hermann
Ney, and Christoph Tillmann. 1998. A
DP-based search algorithm for statistical
machine translation. In COLING-ACL ?98:
36th Annual Meeting of the Association for
Computational Linguistics and 17th
International Conference on Computational
Linguistics, pages 960?967, Montreal,
Canada, August.
Och, Franz J. 2000. Giza++: Training of
statistical translation models. Available at
http://www-i6.informatik.rwth-
aachen.de/?och/software/GIZA++.html.
Och, Franz J. and Hermann Ney. 2000. A
comparison of alignment models for
statistical machine translation. In COLING
?00: The 18th International Conference on
Computational Linguistics, pages 1086?1090,
Saarbru?cken, Germany, August.
Och, Franz J., Christoph Tillmann, and
Hermann Ney. 1999. Improved alignment
models for statistical machine translation.
In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora,
pages 20?28, University of Maryland,
College Park, June.
Och, Franz J., Nicola Ueffing, and Hermann
Ney. 2001. An efficient A* search
algorithm for statistical machine
translation. In Data-Driven Machine
Translation Workshop, pages 55?62,
Toulouse, France, July.
Och, Franz J. and Hans Weber. 1998.
Improving statistical natural language
translation with categories and rules. In
COLING-ACL ?98: 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, pages 985?989,
Montreal, Canada, August.
Simard, M., G. Foster, and P. Isabelle. 1992.
Using cognates to align sentences in
bilingual corpora. In Fourth International
Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-92),
pages 67?81, Montreal, Canada.
Smadja, Frank, Kathleen R. McKeown, and
Vasileios Hatzivassiloglou. 1996.
Translating collocations for bilingual
lexicons: A statistical approach.
Computational Linguistics, 22(1):1?38.
51
Och and Ney Comparison of Statistical Alignment Models
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation.
In COLING ?96: The 16th International
Conference on Computational Linguistics,
pages 836?841, Copenhagen, Denmark,
August.
Wahlster, Wolfgang, editor. 2000. Verbmobil:
Foundations of speech-to-speech translations.
Springer Verlag, Berlin.
Wang, Ye-Yi and Alex Waibel. 1998. Fast
decoding for statistical machine
translation. In Proceedings of the
International Conference on Speech and
Language Processing, pages 1357?1363,
Sydney, Australia, November.
Wu, Dekai. 1996. A polynomial-time
algorithm for statistical machine
translation. In Proceedings of the 34th
Annual Conference of the Association for
Computational Linguistics (ACL ?96),
pages 152?158, Santa Cruz, California,
June.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 523?530, Toulouse, France,
July.
Yarowsky, David, Grace Ngai, and Richard
Wicentowski. 2001. Inducing multilingual
text analysis tools via robust projection
across aligned corpora. In Human
Language Technology Conference, pages
109?116, San Diego, California, March.
Yarowsky, David and Richard Wicentowski.
2000. Minimally supervised
morphological analysis by multimodal
alignment. In Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 207?216, Hong
Kong, October.
Statistical Phrase-Based Translation
Philipp Koehn, Franz Josef Och, Daniel Marcu
Information Sciences Institute
Department of Computer Science
University of Southern California
koehn@isi.edu, och@isi.edu, marcu@isi.edu
Abstract
We propose a new phrase-based translation
model and decoding algorithm that enables
us to evaluate and compare several, previ-
ously proposed phrase-based translation mod-
els. Within our framework, we carry out a
large number of experiments to understand bet-
ter and explain why phrase-based models out-
perform word-based models. Our empirical re-
sults, which hold for all examined language
pairs, suggest that the highest levels of perfor-
mance can be obtained through relatively sim-
ple means: heuristic learning of phrase trans-
lations from word-based alignments and lexi-
cal weighting of phrase translations. Surpris-
ingly, learning phrases longer than three words
and learning phrases from high-accuracy word-
level alignment models does not have a strong
impact on performance. Learning only syntac-
tically motivated phrases degrades the perfor-
mance of our systems.
1 Introduction
Various researchers have improved the quality of statis-
tical machine translation system with the use of phrase
translation. Och et al [1999]?s alignment template model
can be reframed as a phrase translation system; Yamada
and Knight [2001] use phrase translation in a syntax-
based translation system; Marcu and Wong [2002] in-
troduced a joint-probability model for phrase translation;
and the CMU and IBM word-based statistical machine
translation systems1 are augmented with phrase transla-
tion capability.
Phrase translation clearly helps, as we will also show
with the experiments in this paper. But what is the best
1Presentations at DARPA IAO Machine Translation Work-
shop, July 22-23, 2002, Santa Monica, CA
method to extract phrase translation pairs? In order to
investigate this question, we created a uniform evaluation
framework that enables the comparison of different ways
to build a phrase translation table.
Our experiments show that high levels of performance
can be achieved with fairly simple means. In fact,
for most of the steps necessary to build a phrase-based
system, tools and resources are freely available for re-
searchers in the field. More sophisticated approaches that
make use of syntax do not lead to better performance. In
fact, imposing syntactic restrictions on phrases, as used in
recently proposed syntax-based translation models [Ya-
mada and Knight, 2001], proves to be harmful. Our ex-
periments also show, that small phrases of up to three
words are sufficient for obtaining high levels of accuracy.
Performance differs widely depending on the methods
used to build the phrase translation table. We found ex-
traction heuristics based on word alignments to be better
than a more principled phrase-based alignment method.
However, what constitutes the best heuristic differs from
language pair to language pair and varies with the size of
the training corpus.
2 Evaluation Framework
In order to compare different phrase extraction methods,
we designed a uniform framework. We present a phrase
translation model and decoder that works with any phrase
translation table.
2.1 Model
The phrase translation model is based on the noisy chan-
nel model. We use Bayes rule to reformulate the transla-
tion probability for translating a foreign sentence
 
into
English  as
argmax	
 

argmax 
 
 





This allows for a language model 


and a separate
translation model 
 
 


.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 48-54
                                                         Proceedings of HLT-NAACL 2003
During decoding, the foreign input sentence
 
is seg-
mented into a sequence of   phrases 


. We assume a
uniform probability distribution over all possible segmen-
tations.
Each foreign phrase 

in 

 is translated into an En-
glish phrase



. The English phrases may be reordered.
Phrase translation is modeled by a probability distribution
	
 
 



 

. Recall that due to the Bayes rule, the translation
direction is inverted from a modeling standpoint.
Reordering of the English output phrases is modeled
by a relative distortion probability distribution 
 





, where 

denotes the start position of the foreign
phrase that was translated into the  th English phrase, and

 denotes the end position of the foreign phrase trans-
lated into the 
 

th English phrase.
In all our experiments, the distortion probability distri-
bution 
 


is trained using a joint probability model (see
Section 3.3). Alternatively, we could also use a simpler
distortion model 
 
  


 ffA Smorgasbord of Features for Statistical Machine Translation
Franz Josef Och
USC/ISI
Daniel Gildea
U. of Rochester
Sanjeev Khudanpur
Johns Hopkins U.
Anoop Sarkar
Simon Fraser U.
Kenji Yamada
Xerox/XRCE
Alex Fraser
USC/ISI
Shankar Kumar
Johns Hopkins U.
Libin Shen
U. of Pennsylvania
David Smith
Johns Hopkins U.
Katherine Eng
Stanford U.
Viren Jain
U. of Pennsylvania
Zhen Jin
Mt. Holyoke
Dragomir Radev
U. of Michigan
Abstract
We describe a methodology for rapid exper-
imentation in statistical machine translation
which we use to add a large number of features
to a baseline system exploiting features from a
wide range of levels of syntactic representation.
Feature values were combined in a log-linear
model to select the highest scoring candidate
translation from an n-best list. Feature weights
were optimized directly against the BLEU eval-
uation metric on held-out data. We present re-
sults for a small selection of features at each
level of syntactic representation.
1 Introduction
Despite the enormous progress in machine translation
(MT) due to the use of statistical techniques in recent
years, state-of-the-art statistical systems often produce
translations with obvious errors. Grammatical errors in-
clude lack of a main verb, wrong word order, and wrong
choice of function words. Frequent problems of a less
grammatical nature include missing content words and
incorrect punctuation.
In this paper, we attempt to address these problems by
exploring a variety of new features for scoring candidate
translations. A high-quality statistical translation system
is our baseline, and we add new features to the exist-
ing set, which are then combined in a log-linear model.
To allow an easy integration of new features, the base-
line system provides an n-best list of candidate transla-
tions which is then reranked using the new features. This
framework allows us to incorporate different types of fea-
tures, including features based on syntactic analyses of
the source and target sentences, which we hope will ad-
dress the grammaticality of the translations, as well as
lower-level features. As we work on n-best lists, we can
easily use global sentence-level features.
We begin by describing our baseline system and the
n-best rescoring framework within which we conducted
our experiments. We then present a selection of new fea-
tures, progressing from word-level features to those based
to part-of-speech tags and syntactic chunks, and then to
features based on Treebank-based syntactic parses of the
source and target sentences.
2 Log-linear Models for Statistical MT
The goal is the translation of a text given in some source
language into a target language. We are given a source
(?Chinese?) sentence f = fJ1 = f1, . . . , fj , . . . , fJ ,
which is to be translated into a target (?English?) sentence
e = eI1 = e1, . . . , ei, . . . , eI Among all possible target
sentences, we will choose the sentence with the highest
probability:
e?I1 = argmax
eI1
{Pr(eI1|f
J
1 )} (1)
As an alternative to the often used source-channel ap-
proach (Brown et al, 1993), we directly model the pos-
terior probability Pr(eI1|fJ1 ) (Och and Ney, 2002) us-
ing a log-linear combination of feature functions. In
this framework, we have a set of M feature functions
hm(eI1, f
J
1 ),m = 1, . . . ,M . For each feature function,
there exists a model parameter ?m,m = 1, . . . ,M . The
direct translation probability is given by:
Pr(eI1|f
J
1 ) =
exp[
?M
m=1 ?mhm(e
I
1, f
J
1 )]
?
e?I1
exp[
?M
m=1 ?mhm(e
?I
1, f
J
1 )]
(2)
We obtain the following decision rule:
e?I1 = argmax
eI1
{ M?
m=1
?mhm(e
I
1, f
J
1 )
}
(3)
The standard criterion for training such a log-linear
model is to maximize the probability of the parallel train-
ing corpus consisting of S sentence pairs {(fs, es) : s =
1, . . . , S}. However, this does not guarantee optimal per-
formance on the metric of translation quality by which
our system will ultimately be evaluated. For this reason,
we optimize the parameters directly against the BLEU
metric on held-out data. This is a more difficult optimiza-
tion problem, as the search space is no longer convex.
Figure 1: Example segmentation of Chinese sentence and
its English translation into alignment templates.
However, certain properties of the BLEU metric can be
exploited to speed up search, as described in detail by
Och (2003). We use this method of optimizing feature
weights throughout this paper.
2.1 Baseline MT System: Alignment Templates
Our baseline MT system is the alignment template system
described in detail by Och, Tillmann, and Ney (1999)
and Och and Ney (2004). In the following, we give a
short description of this baseline model.
The probability model of the alignment template sys-
tem for translating a sentence can be thought of in distinct
stages. First, the source sentence words fJ1 are grouped to
phrases f?K1 . For each phrase f? an alignment template z is
chosen and the sequence of chosen alignment templates
is reordered (according to piK1 ). Then, every phrase f?
produces its translation e? (using the corresponding align-
ment template z). Finally, the sequence of phrases e?K1
constitutes the sequence of words eI1.
Our baseline system incorporated the following feature
functions:
Alignment Template Selection Each alignment
template is chosen with probability p(z|f?), estimated by
relative frequency. The corresponding feature function in
our log-linear model is the log probability of the product
of p(z|f?) for all used alignment templates used.
Word Selection This feature is based on the lexical
translation probabilities p(e|f), estimated using relative
frequencies according to the highest-probability word-
level alignment for each training sentence. A translation
probability conditioned on the source and target position
within the alignment template p(e|f, i, j) is interpolated
with the position-independent probability p(e|f).
Phrase Alignment This feature favors monotonic
alignment at the phrase level. It measures the ?amount
of non-monotonicity? by summing over the distance (in
the source language) of alignment templates which are
consecutive in the target language.
Language Model Features As a language model
feature, we use a standard backing off word-based tri-
gram language model (Ney, Generet, and Wessel, 1995).
The baseline system actually includes four different lan-
guage model features trained on four different corpora:
the news part of the bilingual training data, a large Xin-
hua news corpus, a large AFP news corpus, and a set of
Chinese news texts downloaded from the web.
Word/Phrase Penalty This word penalty feature
counts the length in words of the target sentence. Without
this feature, the sentences produced tend to be too short.
The phrase penalty feature counts the number of phrases
produced, and can allow the model to prefer either short
or long phrases.
Phrases from Conventional Lexicon The baseline
alignment template system makes use of the Chinese-
English lexicon provided by LDC. Each lexicon entry is
a potential phrase translation pair in the alignment tem-
plate system. To score the use of these lexicon entries
(which have no normal translation probability), this fea-
ture function counts the number of times such a lexicon
entry is used.
Additional Features A major advantage of the log-
linear modeling approach is that it is easy to add new
features. In this paper, we explore a variety of features
based on successively deeper syntactic representations of
the source and target sentences, and their alignment. For
each of the new features discussed below, we added the
feature value to the set of baseline features, re-estimated
feature weights on development data, and obtained re-
sults on test data.
3 Experimental Framework
We worked with the Chinese-English data from the recent
evaluations, as both large amounts of sentence-aligned
training corpora and multiple gold standard reference
translations are available. This is a standard data set,
making it possible to compare results with other systems.
In addition, working on Chinese allows us to use the ex-
isting Chinese syntactic treebank and parsers based on it.
For the baseline MT system, we distinguish the fol-
lowing three different sentence- or chunk-aligned parallel
training corpora:
? training corpus (train): This is the basic training
corpus used to train the alignment template transla-
tion model (word lexicon and phrase lexicon). This
corpus consists of about 170M English words. Large
parts of this corpus are aligned on a sub-sentence
level to avoid the existence of very long sentences
which would be filtered out in the training process
to allow a manageable word alignment training.
? development corpus (dev): This is the training cor-
pus used in discriminative training of the model-
parameters of the log-linear translation model. In
most experiments described in this report this cor-
pus consists of 993 sentences (about 25K words) in
both languages.
? test corpus (test): This is the test corpus used to
assess the quality of the newly developed feature
functions. It consists of 878 sentences (about 25K
words).
For development and test data, we have four English (ref-
erence) translations for each Chinese sentence.
3.1 Reranking, n-best lists, and oracles
For each sentence in the development, test, and the blind
test corpus a set of 16,384 different alternative transla-
tions has been produced using the baseline system. For
extracting the n-best candidate translations, an A* search
is used. These n-best candidate translations are the basis
for discriminative training of the model parameters and
for re-ranking.
We used n-best reranking rather than implementing
new search algorithms. The development of efficient
search algorithms for long-range dependencies is very
complicated and a research topic in itself. The rerank-
ing strategy enabled us to quickly try out a lot of new
dependencies, which would not have been be possible if
the search algorithm had to be changed for each new de-
pendency.
On the other hand, the use of n-best list rescoring lim-
its the possibility of improvements to what is available
in the n-best list. Hence, it is important to analyze the
quality of the n-best lists by determining how much of an
improvement would be possible given a perfect reranking
algorithm. We computed the oracle translations, that is,
the set of translations from our n-best list that yields the
best BLEU score.1
We use the following two methods to compute the
BLEU score of an oracle translation:
1. optimal oracle (opt): We select the oracle sentences
which give the highest BLEU score compared to the
set of 4 reference translations. Then, we compute
BLEU score of oracle sentences using the same set
of reference translations.
2. round-robin oracle (rr): We select four differ-
ent sets of oracle sentences which give the highest
BLEU score compared to each of the 4 references
translations. Then, we compute for each set of or-
acle sentences a BLEU score using always those
three references to score that have not been cho-
sen to select the oracle. Then, these 4 3-reference
BLEU scores are averaged.
1Note that due to the corpus-level holistic nature of the
BLEU score it is not trivial to compute the optimal set of oracle
translations. We use a greedy search algorithm for the oracle
translations that might find only a local optimum. Empirically,
we do not observe a dependence on the starting point, hence we
believe that this does not pose a significant problem.
Table 1: Oracle BLEU scores for different sizes of the
n-best list. The avBLEUr3 scores are computed with
respect to three reference translations averaged over the
four different choices of holding out one reference.
avBLEUr3[%] BLEUr4
n rr opt opt
human 35.8 -
1 28.3 28.3 31.6
4 29.1 30.8 34.5
16 29.9 33.2 37.3
64 30.6 35.6 38.7
256 31.3 37.8 42.8
1024 31.7 40.0 45.3
4096 32.0 41.8 47.3
The first method provides the theoretical upper bound of
what BLEU score can be obtained by rescoring a given n-
best list. Using this method with a 1000-best list, we ob-
tain oracle translations that outperform the BLEU score
of the human translations. The oracle translations achieve
113% against the human BLEU score on the test data
(Table 1), while the first best translations obtain 79.2%
against the human BLEU score. The second method uses
a different references for selection and scoring. Here, us-
ing an 1000-best list, we obtain oracle translations with a
relative human BLEU score of 88.5%.
Based on the results of the oracle experiment, and
in order to make rescoring computationally feasible for
features requiring significant computation for each hy-
pothesis, we used the top 1000 translation candidates for
our experiments. The baseline system?s BLEU score is
31.6% on the test set (equivalent to the 1-best oracle in
Table 1). This is the benchmark against which the contri-
butions of the additional features described in the remain-
der of this paper are to be judged.
3.2 Preprocessing
As a precursor to developing the various syntactic fea-
tures described in this report, the syntactic represen-
tations on which they are based needed to be com-
puted. This involved part-of-speech tagging, chunking,
and parsing both the Chinese and English side of our
training, development, and test sets.
Applying the part-of-speech tagger to the often un-
grammatical MT output from our n-best lists sometimes
led to unexpected results. Often the tagger tries to ?fix
up? ungrammatical sentences, for example by looking for
a verb when none is present:
China NNP 14 CD open JJ border NN
cities NNS achievements VBZ remarkable JJ
Here, although achievements has never been seen as a
verb in the tagger?s training data, the prior for a verb
in this position is high enough to cause a present tense
verb tag to be produced. In addition to the inaccura-
cies of the MT system, the difference in genre from the
tagger?s training text can cause problems. For example,
while our MT data include news article headlines with no
verb, headlines are not included in the Wall Street Journal
text on which the tagger is trained. Similarly, the tagger
is trained on full sentences with normalized punctuation,
leading it to expect punctuation at the end of every sen-
tence, and produce a punctuation tag even when the evi-
dence does not support it:
China NNP ?s POS economic JJ
development NN and CC opening VBG
up RP 14 CD border NN cities NNS
remarkable JJ achievements .
The same issues affect the parser. For example the
parser can create verb phrases where none exist, as in the
following example in which the tagger correctly did not
identify a verb in the sentence:
These effects have serious implications for designing
syntactic feature functions. Features such ?is there a verb
phrase? may not do what you expect. One solution would
be features that involve the probability of a parse subtree
or tag sequence, allowing us to ask ?how good a verb
phrase is it?? Another solution is more detailed features
examining more of the structure, such as ?is there a verb
phrase with a verb??
4 Word-Level Feature Functions
These features, directly based on the source and target
strings of words, are intended to address such problems as
translation choice, missing content words, and incorrect
punctuation.
4.1 Model 1 Score
We used IBM Model 1 (Brown et al, 1993) as one of the
feature functions. Since Model 1 is a bag-of-word trans-
lation model and it gives the sum of all possible alignment
probabilities, a lexical co-occurrence effect, or triggering
effect, is expected. This captures a sort of topic or seman-
tic coherence in translations.
As defined by Brown et al (1993), Model 1 gives a
probability of any given translation pair, which is
p(f |e; M1) =

(l + 1)m
m?
j=1
l?
i=0
t(fj |ei).
We used GIZA++ to train the model. The training data is
a subset (30 million words on the English side) of the en-
tire corpus that was used to train the baseline MT system.
For a missing translation word pair or unknown words,
where t(fj |ei) = 0 according to the model, a constant
t(fj |ei) = 10?40 was used as a smoothing value.
The average %BLEU score (average of the best four
among different 20 search initial points) is 32.5. We also
tried p(e|f ; M1) as feature function, but did not obtain
improvements which might be due to an overlap with the
word selection feature in the baseline system.
The Model 1 score is one of the best performing fea-
tures. It seems to ?fix? the tendency of our baseline sys-
tem to delete content words and it improves word selec-
tion coherence by the triggering effect. It is also possible
that the triggering effect might work on selecting a proper
verb-noun combination, or a verb-preposition combina-
tion.
4.2 Lexical Re-ordering of Alignment Templates
As shown in Figure 1 the alignment templates (ATs)
used in the baseline system can appear in various con-
figurations which we will call left/right-monotone and
left/right-continuous. We built 2 out of these 4 models to
distinguish two types of lexicalized re-ordering of these
ATs:
The left-monotone model computes the total proba-
bility of all ATs being left monotone: where the lower
left corner of the AT touches the upper right corner of the
previous AT. Note that the first word in the current AT
may or may not immediately follow the last word in the
previous AT. The total probability is the product over all
alignment templates i, either P (ATi is left-monotone) or
1 ? P (ATi is left-monotone).
The right-continuous model computes the total prob-
ability of all ATs being right continuous: where the
lower left corner of the AT touches the upper right cor-
ner of the previous AT and the first word in the cur-
rent AT immediately follows the last word in the pre-
vious AT. The total probability is the product over all
alignment templates i, either P (ATi is right-continuous)
or 1 ? P (ATi is right-continuous).
In both models, the probabilities P have been esti-
mated from the full training data (train).
5 Shallow Syntactic Feature Functions
By shallow syntax, we mean the output of the part-of-
speech tagger and chunkers. We hope that such features
can combine the strengths of tag- and chunk-based trans-
lation systems (Schafer and Yarowsky, 2003) with our
baseline system.
5.1 Projected POS Language Model
This feature uses Chinese POS tag sequences as surro-
gates for Chinese words to model movement. Chinese
words are too sparse to model movement, but an attempt
to model movement using Chinese POS may be more
successful. We hope that this feature will compensate for
a weak model of word movement in the baseline system.
Chinese POS sequences are projected to English us-
ing the word alignment. Relative positions are indicated
for each Chinese tag. The feature function was also tried
without the relative positions:
CD +0 M +1 NN +3 NN -1 NN +2 NN +3
14 (measure) open border cities
The table shows an example tagging of an English hy-
pothesis showing how it was generated from the Chinese
sentence. The feature function is the log probability out-
put by a trigram language model over this sequence. This
is similar to the HMM Alignment model (Vogel, Ney, and
Tillmann, 1996) but in this case movement is calculated
on the basis of parts of speech.
The Projected POS feature function was one of the
strongest performing shallow syntactic feature functions,
with a %BLEU score of 31.8. This feature function can
be thought of as a trade-off between purely word-based
models, and full generative models based upon shallow
syntax.
6 Tree-Based Feature Functions
Syntax-based MT has shown promise in the
work of, among others, Wu and Wong (1998) and
Alshawi, Bangalore, and Douglas (2000). We hope that
adding features based on Treebank-based syntactic
analyses of the source and target sentences will address
grammatical errors in the output of the baseline system.
6.1 Parse Tree Probability
The most straightforward way to integrate a statistical
parser in the system would be the use of the (log of the)
parser probability as a feature function. Unfortunately,
this feature function did not help to obtain better results
(it actually seems to significantly hurt performance).
To analyze the reason for this, we performed an ex-
periment to test if the used statistical parser assigns a
higher probability to presumably grammatical sentences.
The following table shows the average log probability as-
signed by the Collins parser to the 1-best (produced), or-
acle and the reference translations:
Hypothesis 1-best Oracle Reference
log(parseProb) -147.2 -148.5 -154.9
We observe that the average parser log-probability of
the 1-best translation is higher than the average parse
log probability of the oracle or the reference translations.
Hence, it turns out that the parser is actually assigning
higher probabilities to the ungrammatical MT output than
to the presumably grammatical human translations. One
reason for that is that the MT output uses fewer unseen
words and typically more frequent words which lead to
a higher language model probability. We also performed
experiments to balance this effect by dividing the parser
probability by the word unigram probability and using
this ?normalized parser probability? as a feature function,
but also this did not yield improvements.
6.2 Tree-to-String Alignment
A tree-to-string model is one of several syntax-
based translation models used. The model is a
conditional probability p(f |T (e)). Here, we used
a model defined by Yamada and Knight (2001) and
Yamada and Knight (2002).
Internally, the model performs three types of opera-
tions on each node of a parse tree. First, it reorders the
child nodes, such as changing VP ? VB NP PP into
VP ? NP PP VB. Second, it inserts an optional word at
each node. Third, it translates the leaf English words into
Chinese words. These operations are stochastic and their
probabilities are assumed to depend only on the node, and
are independent of other operations on the node, or other
nodes. The probability of each operation is automatically
obtained by a training algorithm, using about 780,000 En-
glish parse tree-Chinese sentence pairs. The probability
of these operations ?(eki,j) is assumed to depend on the
edge of the tree being modified, eki,j , but independent of
everything else, giving the following equation,
p(f |T (e)) =
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j) (4)
where ? varies over the possible alignments between the
f and e and ?(eki,j) is the particular operations (in ?) for
the edge eki,j .
The model is further extended to incorporate phrasal
translations performed at each node of the input parse
tree (Yamada and Knight, 2002). An English phrase cov-
ered by a node can be directly translated into a Chinese
phrase without regular reorderings, insertions, and leaf-
word translations.
The model was trained using about 780,000 English
parse tree-Chinese sentence pairs. There are about 3 mil-
lion words on the English side, and they were parsed by
Collins? parser.
Since the model is computationally expensive, we
added some limitations on the model operations. As the
base MT system does not produce a translation with a
big word jump, we restrict the model not to reorder child
nodes when the node covers more than seven words. For
a node that has more than four children, the reordering
probability is set to be uniform. We also introduced prun-
ing, which discards partial (subtree-substring) alignments
if the probability is lower than a threshold.
The model gives a sum of all possible alignment prob-
abilities for a pair of a Chinese sentence and an English
parse tree. We also calculate the probability of the best
alignment according to the model. Thus, we have the fol-
lowing two feature functions:
hTreeToStringSum(e, f) = log(
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
hTreeToStringViterbi(e, f) = log(max
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
As the model is computationally expensive, we sorted the
n-best list by the sentence length, and processed them
from the shorter ones to the longer ones. We used 10
CPUs for about five days, and 273/997 development sen-
tences and 237/878 test sentences were processed.
The average %BLEU score (average of the best four
among different 20 search initial points) was 31.7 for
both hTreeToStringSum and hTreeToStringViterbi. Among the pro-
cessed development sentences, the model preferred the
oracle sentences over the produced sentence in 61% of
the cases.
The biggest problem of this model is that it is compu-
tationally very expensive. It processed less than 30% of
the n-best lists in long CPU hours. In addition, we pro-
cessed short sentences only. For long sentences, it is not
practical to use this model as it is.
6.3 Tree-to-Tree Alignment
A tree-to-tree translation model makes use of syntac-
tic tree for both the source and target language. As in
the tree-to-string model, a set of operations apply, each
with some probability, to transform one tree into another.
However, when training the model, trees for both the
source and target languages are provided, in our case
from the Chinese and English parsers.
We began with the tree-to-tree alignment model pre-
sented by Gildea (2003). The model was extended to han-
dle dependency trees, and to make use of the word-level
alignments produced by the baseline MT system. The
probability assigned by the tree-to-tree alignment model,
given the word-level alignment with which the candidate
translation was generated, was used as a feature in our
rescoring system.
We trained the parameters of the tree transformation
operations on 42,000 sentence pairs of parallel Chinese-
English data from the Foreign Broadcast Information Ser-
vice (FBIS) corpus. The lexical translation probabili-
ties Pt were trained using IBM Model 1 on the 30 mil-
lion word training corpus. This was done to overcome
the sparseness of the lexical translation probabilities es-
timated while training the tree-to-tree model, which was
not able to make use of as much training data.
As a test of the tree-to-tree model?s discrimination, we
performed an oracle experiment, comparing the model
scores on the first sentence in the n-best list with candi-
date giving highest BLEU score. On the 1000-best list for
the 993-sentence development set, restricting ourselves
to sentences with no more than 60 words and a branching
factor of no more than five in either the Chinese or En-
glish tree, we achieved results for 480, or 48% of the 993
sentences. Of these 480, the model preferred the pro-
duced over the oracle 52% of the time, indicating that
it does not in fact seem likely to significantly improve
BLEU scores when used for reranking. Using the prob-
ability of the source Chinese dependency parse aligning
with the n-best hypothesis dependency parse as a feature
function, making use of the word-level alignments, yields
a 31.6 %BLEU score ? identical to our baseline.
6.4 Markov Assumption for Tree Alignments
The tree-based feature functions described so far have the
following limitations: full parse tree models are expen-
sive to compute for long sentences and for trees with flat
constituents and there is limited reordering observed in
the n-best lists that form the basis of our experiments. In
addition to this, higher levels of parse tree are rarely ob-
served to be reordered between source and target parse
trees.
In this section we attack these problems using a simple
Markov model for tree-based alignments. It guarantees
tractability: compared to a coverage of approximately
30% of the n-best list by the unconstrained tree-based
models, using the Markov model approach provides 98%
coverage of the n-best list. In addition, this approach is
robust to inaccurate parse trees.
The algorithm works as follows: we start with word
alignments and two parameters: n for maximum number
of words in tree fragment and k for maximum height of
tree fragment. We proceed from left to right in the Chi-
nese sentence and incrementally grow a pair of subtrees,
one subtree in Chinese and the other in English, such that
each word in the Chinese subtree is aligned to a word in
the English subtree. We grow this pair of subtrees un-
til we can no longer grow either subtree without violat-
ing the two parameter values n and k. Note that these
aligned subtree pairs have properties similar to alignment
templates. They can rearrange in complex ways between
source and target. Figure 2 shows how subtree-pairs for
parameters n = 3 and k = 3 can be drawn for this
sentence pair. In our experiments, we use substantially
bigger tree fragments with parameters set to n = 8 and
k = 9.
Once these subtree-pairs have been obtained, we can
easily assert a Markov assumption for the tree-to-tree and
tree-to-string translation models that exploits these pair-
ings. Let consider a sentence pair in which we have dis-
covered n subtree-pairs which we can call Frag0, . . .,
Fragn. We can then compute a feature function for the
sentence pair using the tree-to-string translation model as
follows:
hMarkovTreeToString =
logPtree-to-string(Frag0) + . . . + logPtree-to-string(Fragn)
Using this Markov assumption on tree alignments with
Figure 2: Markov assumption on tree alignments.
the Tree to String model described in Section 6.2 we ob-
tain a coverage improvement to 98% coverage from the
original 30%. The accuracy of the tree to string model
also improved with a %BLEU score of 32.0 which is the
best performing single syntactic feature.
6.5 Using TAG elementary trees for scoring word
alignments
In this section, we consider another method for carving
up the full parse tree. However, in this method, instead of
subtree-pairs we consider a decomposition of parse trees
that provides each word with a fragment of the original
parse tree as shown in Figure 3. The formalism of Tree-
Adjoining Grammar (TAG) provides the definition what
each tree fragment should be and in addition how to de-
compose the original parse trees to provide the fragments.
Each fragment is a TAG elementary tree and the compo-
sition of these TAG elementary trees in a TAG deriva-
tion tree provides the decomposition of the parse trees.
The decomposition into TAG elementary trees is done by
augmenting the parse tree for source and target sentence
with head-word and argument (or complement) informa-
tion using heuristics that are common to most contempo-
rary statistical parsers and easily available for both En-
glish and Chinese. Note that we do not use the word
alignment information for the decomposition into TAG
elementary trees.
Once we have a TAG elementary tree per word,
we can create several models that score word align-
ments by exploiting the alignments between TAG ele-
mentary trees between source and target. Let tfi and
tei be the TAG elementary trees associated with the
aligned words fi and ei respectively. We experimented
with two models over alignments: unigram model over
alignments:
?
i P (fi, tfi , ei, tei) and conditional model:?
i P (ei, tei | fi, tfi) ? P (fi+1, tfi+1 | fi, tfi)
We trained both of these models using the SRI Lan-
guage Modeling Toolkit using 60K aligned parse trees.
We extracted 1300 TAG elementary trees each for Chi-
Figure 3: Word alignments with TAG elementary trees.
nese and for English. The unigram model gets a %BLEU
score of 31.7 and the conditional model gets a %BLEU
score of 31.9.
%BLEU
Baseline 31.6
IBM Model 1 p(f |e) 32.5
Tree-to-String Markov fragments 32.0
Right-continuous alignment template 32.0
TAG conditional bigrams 31.9
Left-monotone alignment template 31.9
Projected POS LM 31.8
Tree-to-String 31.7
TAG unigram 31.7
Tree-to-Tree 31.6
combination 32.9
Table 2: Results for the baseline features, each new fea-
ture added to the baseline features on its own, and a com-
bination of new features.
7 Conclusions
The use of discriminative reranking of an n-best list pro-
duced with a state-of-the-art statistical MT system al-
lowed us to rapidly evaluate the benefits of off-the-shelf
parsers, chunkers, and POS taggers for improving syntac-
tic well-formedness of the MT output. Results are sum-
marized in Table 2; the best single new feature improved
the %BLEU score from 31.6 to 32.5. The 95% confi-
dence intervals computed with the bootstrap resampling
method are about 0.8%. In addition to experiments with
single features we also integrated multiple features using
a greedy approach where we integrated at each step the
feature that most improves the BLEU score. This feature
integration produced a statistically significant improve-
ment of absolute 1.3% to 32.9 %BLEU score.
Our single best feature, and in fact the only single fea-
ture to produce a truly significant improvement, was the
IBM Model 1 score. We attribute its success that it ad-
dresses the weakness of the baseline system to omit con-
tent words and that it improves word selection by em-
ploying a triggering effect. We hypothesize that this al-
lows for better use of context in, for example, choosing
among senses of the source language word.
A major goal of this work was to find out if we can ex-
ploit annotated data such as treebanks for Chinese and
English and make use of state-of-the-art deep or shal-
low parsers to improve MT quality. Unfortunately, none
of the implemented syntactic features achieved a statisti-
cally significant improvement in the BLEU score. Poten-
tial reasons for this might be:
? As described in Section 3.2, the use of off-the-shelf
taggers and parsers has various problems due to vari-
ous mismatches between the parser training data and
our application domain. This might explain that the
use of the parser probability as feature function was
not successful. A potential improvement might be to
adapt the parser by retraining it on the full training
data that has been used by the baseline system.
? The use of a 1000-best list limits the potential im-
provements. It is possible that more improvements
could be obtained using a larger n-best list or a word
graph representation of the candidates.
? The BLEU score is possibly not sufficiently sensi-
tive to the grammaticality of MT output. This could
not only make it difficult to see an improvement in
the system?s output, but also potentially mislead the
BLEU-based optimization of the feature weights. A
significantly larger corpus for discriminative train-
ing and for evaluation would yield much smaller
confidence intervals.
? Our discriminative training technique, which di-
rectly optimizes the BLEU score on a development
corpus, seems to have overfitting problems with
large number of features. One could use a larger de-
velopment corpus for discriminative training or in-
vestigate alternative discriminative training criteria.
? The amount of annotated data that has been used
to train the taggers and parsers is two orders of
magnitude smaller than the parallel training data
that has been used to train the baseline system (or
the word-based features). Possibly, a comparable
amount of annotated data (e.g. a treebank with 100
million words) is needed to obtain significant im-
provements.
This is the first large scale integration of syntactic analy-
sis operating on many different levels with a state-of-the-
art phrase-based MT system. The methodology of using
a log-linear feature combination approach, discriminative
reranking of n-best lists computed with a state-of-the-art
baseline system allowed members of a large team to si-
multaneously experiment with hundreds of syntactic fea-
ture functions on a common platform.
Acknowledgments
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 0121285.
References
Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas. 2000.
Learning dependency translation models as collections of
finite state head transducers. Computational Linguistics,
26(1):45?60.
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Gildea, Daniel. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. of the 41st Annual Meeting of the
Association for Computational Linguistics (ACL), Sapporo,
Japan.
Ney, Hermann, M. Generet, and Frank Wessel. 1995. Ex-
tensions of absolute discounting for language modeling. In
Proc. of the Fourth European Conf. on Speech Communica-
tion and Technology, Madrid, Spain.
Och, Franz Josef. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL), Sap-
poro, Japan.
Och, Franz Josef and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL), Philadel-
phia, PA.
Och, Franz Josef and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Computa-
tional Linguistics. Accepted for Publication.
Och, Franz Josef, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical machine
translation. In Proc. of the Joint SIGDAT Conf. on Empiri-
cal Methods in Natural Language Processing and Very Large
Corpora, College Park, MD.
Schafer, Charles and David Yarowsky. 2003. Statistical ma-
chine translation using coercive two-level syntactic transduc-
tion. In Proc. of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Philadel-
phia, PA.
Vogel, Stephan, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational Lin-
guistics, Copenhagen, Denmark.
Wu, Dekai and H. Wong. 1998. Machine translation with a
stochastic grammatical channel. In COLING-ACL ?98: 36th
Annual Meeting of the Association for Computational Lin-
guistics and 17th Int. Conf. on Computational Linguistics,
Montreal, Canada.
Yamada, Kenji and Kevin Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of the 39th Annual Meet-
ing of the Association for Computational Linguistics (ACL),
Toulouse, France.
Yamada, Kenji and Kevin Knight. 2002. A decoder for syntax-
based MT. In Proc. of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Philadelphia,
PA.
Discriminative Reranking for Machine Translation
Libin Shen
Dept. of Comp. & Info. Science
Univ. of Pennsylvania
Philadelphia, PA 19104
libin@seas.upenn.edu
Anoop Sarkar
School of Comp. Science
Simon Fraser Univ.
Burnaby, BC V5A 1S6
anoop@cs.sfu.ca
Franz Josef Och
Info. Science Institute
Univ. of Southern California
Marina del Rey, CA 90292
och@isi.edu
Abstract
This paper describes the application of discrim-
inative reranking techniques to the problem of
machine translation. For each sentence in the
source language, we obtain from a baseline sta-
tistical machine translation system, a ranked   -
best list of candidate translations in the target
language. We introduce two novel perceptron-
inspired reranking algorithms that improve on
the quality of machine translation over the
baseline system based on evaluation using the
BLEU metric. We provide experimental results
on the NIST 2003 Chinese-English large data
track evaluation. We also provide theoretical
analysis of our algorithms and experiments that
verify that our algorithms provide state-of-the-
art performance in machine translation.
1 Introduction
The noisy-channel model (Brown et al, 1990) has been
the foundation for statistical machine translation (SMT)
for over ten years. Recently so-called reranking tech-
niques, such as maximum entropy models (Och and Ney,
2002) and gradient methods (Och, 2003), have been ap-
plied to machine translation (MT), and have provided
significant improvements. In this paper, we introduce
two novel machine learning algorithms specialized for
the MT task.
Discriminative reranking algorithms have also con-
tributed to improvements in natural language parsing
and tagging performance. Discriminative reranking al-
gorithms used for these applications include Perceptron,
Boosting and Support Vector Machines (SVMs). In the
machine learning community, some novel discriminative
ranking (also called ordinal regression) algorithms have
been proposed in recent years. Based on this work, in
this paper, we will present some novel discriminative
reranking techniques applied to machine translation. The
reranking problem for natural language is neither a clas-
sification problem nor a regression problem, and under
certain conditions MT reranking turns out to be quite dif-
ferent from parse reranking.
In this paper, we consider the special issues of apply-
ing reranking techniques to the MT task and introduce
two perceptron-like reranking algorithms for MT rerank-
ing. We provide experimental results that show that the
proposed algorithms achieve start-of-the-art results on the
NIST 2003 Chinese-English large data track evaluation.
1.1 Generative Models for MT
The seminal IBM models (Brown et al, 1990) were
the first to introduce generative models to the MT task.
The IBM models applied the sequence learning paradigm
well-known from Hidden Markov Models in speech
recognition to the problem of MT. The source and tar-
get sentences were treated as the observations, but the
alignments were treated as hidden information learned
from parallel texts using the EM algorithm. This source-
channel model treated the task of finding the probability
	

, where  is the translation in the target (English)
language for a given source (foreign) sentence  , as two
generative probability models: the language model 

which is a generative probability over candidate transla-
tions and the translation model 
 which is a gener-
ative conditional probability of the source sentence given
a candidate translation  .
The lexicon of the single-word based IBM models does
not take word context into account. This means unlikely
alignments are being considered while training the model
and this also results in additional decoding complexity.
Several MT models were proposed as extensions of the
IBM models which used this intuition to add additional
linguistic constraints to decrease the decoding perplexity
and increase the translation quality.
Wang and Waibel (1998) proposed an SMT model
based on phrase-based alignments. Since their transla-
tion model reordered phrases directly, it achieved higher
accuracy for translation between languages with differ-
ent word orders. In (Och and Weber, 1998; Och et al,
1999), a two-level alignment model was employed to uti-
lize shallow phrase structures: alignment between tem-
plates was used to handle phrase reordering, and word
alignments within a template were used to handle phrase
to phrase translation.
However, phrase level alignment cannot handle long
distance reordering effectively. Parse trees have also
been used in alignment models. Wu (1997) introduced
constraints on alignments using a probabilistic syn-
chronous context-free grammar restricted to Chomsky-
normal form. (Wu, 1997) was an implicit or self-
organizing syntax model as it did not use a Treebank. Ya-
mada and Knight (2001) used a statistical parser trained
using a Treebank in the source language to produce parse
trees and proposed a tree to string model for alignment.
Gildea (2003) proposed a tree to tree alignment model us-
ing output from a statistical parser in both source and tar-
get languages. The translation model involved tree align-
ments in which subtree cloning was used to handle cases
of reordering that were not possible in earlier tree-based
alignment models.
1.2 Discriminative Models for MT
Och and Ney (2002) proposed a framework for MT based
on direct translation, using the conditional model 

estimated using a maximum entropy model. A small
number of feature functions defined on the source and
target sentence were used to rerank the translations gen-
erated by a baseline MT system. While the total num-
ber of feature functions was small, each feature function
was a complex statistical model by itself, as for exam-
ple, the alignment template feature functions used in this
approach.
Och (2003) described the use of minimum error train-
ing directly optimizing the error rate on automatic MT
evaluation metrics such as BLEU. The experiments
showed that this approach obtains significantly better re-
sults than using the maximum mutual information cri-
terion on parameter estimation. This approach used the
same set of features as the alignment template approach
in (Och and Ney, 2002).
SMT Team (2003) also used minimum error training
as in Och (2003), but used a large number of feature func-
tions. More than 450 different feature functions were
used in order to improve the syntactic well-formedness
of MT output. By reranking a 1000-best list generated by
the baseline MT system from Och (2003), the BLEU (Pa-
pineni et al, 2001) score on the test dataset was improved
from 31.6% to 32.9%.
2 Ranking and Reranking
2.1 Reranking for NLP tasks
Like machine translation, parsing is another field of natu-
ral language processing in which generative models have
been widely used. In recent years, reranking techniques,
especially discriminative reranking, have resulted in sig-
nificant improvements in parsing. Various machine learn-
ing algorithms have been employed in parse reranking,
such as Boosting (Collins, 2000), Perceptron (Collins and
Duffy, 2002) and Support Vector Machines (Shen and
Joshi, 2003). The reranking techniques have resulted in a
13.5% error reduction in labeled recall/precision over the
previous best generative parsing models. Discriminative
reranking methods for parsing typically use the notion of
a margin as the distance between the best candidate parse
and the rest of the parses. The reranking problem is re-
duced to a classification problem by using pairwise sam-
ples.
In (Shen and Joshi, 2004), we have introduced a new
perceptron-like ordinal regression algorithm for parse
reranking. In that algorithm, pairwise samples are used
for training and margins are defined as the distance be-
tween parses of different ranks. In addition, the uneven
margin technique has been used for the purpose of adapt-
ing ordinal regression to reranking tasks. In this paper,
we apply this algorithm to MT reranking, and we also
introduce a new perceptron-like reranking algorithm for
MT.
2.2 Ranking and Ordinal Regression
In the field of machine learning, a class of tasks (called
ranking or ordinal regression) are similar to the rerank-
ing tasks in NLP. One of the motivations of this paper
is to apply ranking or ordinal regression algorithms to
MT reranking. In the previous works on ranking or or-
dinal regression, the margin is defined as the distance
between two consecutive ranks. Two large margin ap-
proaches have been used. One is the PRank algorithm,
a variant of the perceptron algorithm, that uses multi-
ple biases to represent the boundaries between every two
consecutive ranks (Crammer and Singer, 2001; Harring-
ton, 2003). However, as we will show in section 3.7, the
PRank algorithm does not work on the reranking tasks
due to the introduction of global ranks. The other ap-
proach is to reduce the ranking problem to a classification
problem by using the method of pairwise samples (Her-
brich et al, 2000). The underlying assumption is that the
samples of consecutive ranks are separable. This may
become a problem in the case that ranks are unreliable
when ranking does not strongly distinguish between can-
didates. This is just what happens in reranking for ma-
chine translation.
3 Discriminative Reranking for MT
The reranking approach for MT is defined as follows:
First, a baseline system generates   -best candidates. Fea-
tures that can potentially discriminate between good vs.
bad translations are extracted from these   -best candi-
dates. These features are then used to determine a new
ranking for the   -best list. The new top ranked candidate
in this   -best list is our new best candidate translation.
3.1 Advantages of Discriminative Reranking
Discriminative reranking allows us to use global features
which are unavailable for the baseline system. Second,
we can use features of various kinds and need not worry
about fine-grained smoothing issues. Finally, the statis-
tical machine learning approach has been shown to be
effective in many NLP tasks. Reranking enables rapid
experimentation with complex feature functions, because
the complex decoding steps in SMT are done once to gen-
erate the N-best list of translations.
3.2 Problems applying reranking to MT
First, we consider how to apply discriminative reranking
to machine translation. We may directly use those algo-
rithms that have been successfully used in parse rerank-
ing. However, we immediately find that those algorithms
are not as appropriate for machine translation. Let 
be the candidate ranked at the  th position for the source
sentence, where ranking is defined on the quality of the
candidates. In parse reranking, we look for parallel hy-
perplanes successfully separating  and  Minimum Error Rate Training in Statistical Machine Translation
Franz Josef Och
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
och@isi.edu
Abstract
Often, the training procedure for statisti-
cal machine translation models is based on
maximum likelihood or related criteria. A
general problem of this approach is that
there is only a loose relation to the final
translation quality on unseen text. In this
paper, we analyze various training criteria
which directly optimize translation qual-
ity. These training criteria make use of re-
cently proposed automatic evaluation met-
rics. We describe a new algorithm for effi-
cient training an unsmoothed error count.
We show that significantly better results
can often be obtained if the final evalua-
tion criterion is taken directly into account
as part of the training procedure.
1 Introduction
Many tasks in natural language processing have
evaluation criteria that go beyond simply count-
ing the number of wrong decisions the system
makes. Some often used criteria are, for example,
F-Measure for parsing, mean average precision for
ranked retrieval, and BLEU or multi-reference word
error rate for statistical machine translation. The use
of statistical techniques in natural language process-
ing often starts out with the simplifying (often im-
plicit) assumption that the final scoring is based on
simply counting the number of wrong decisions, for
instance, the number of sentences incorrectly trans-
lated in machine translation. Hence, there is a mis-
match between the basic assumptions of the used
statistical approach and the final evaluation criterion
used to measure success in a task.
Ideally, we would like to train our model param-
eters such that the end-to-end performance in some
application is optimal. In this paper, we investigate
methods to efficiently optimize model parameters
with respect to machine translation quality as mea-
sured by automatic evaluation criteria such as word
error rate and BLEU.
2 Statistical Machine Translation with
Log-linear Models
Let us assume that we are given a source (?French?)
sentence
 



	


	


	


	

 , which is
to be translated into a target (?English?) sentence





	


	


	


	


 Among all possible
target sentences, we will choose the sentence with
the highest probability:1


 Automatic Evaluation of Machine Translation Quality Using Longest Com-
mon Subsequence and Skip-Bigram Statistics  
Chin-Yew Lin and Franz Josef Och 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292, USA 
{cyl,och}@isi.edu 
 
Abstract 
In this paper we describe two new objective 
automatic evaluation methods for machine 
translation. The first method is based on long-
est common subsequence between a candidate 
translation and a set of reference translations. 
Longest common subsequence takes into ac-
count sentence level structure similarity natu-
rally and identifies longest co-occurring in-
sequence n-grams automatically.  The second 
method relaxes strict n-gram matching to skip-
bigram matching. Skip-bigram is any pair of 
words in their sentence order. Skip-bigram co-
occurrence statistics measure the overlap of 
skip-bigrams between a candidate translation 
and a set of reference translations. The empiri-
cal results show that both methods correlate 
with human judgments very well in both ade-
quacy and fluency. 
1 Introduction 
Using objective functions to automatically evalu-
ate machine translation quality is not new. Su et al 
(1992) proposed a method based on measuring edit 
distance (Levenshtein 1966) between candidate 
and reference translations. Akiba et al (2001) ex-
tended the idea to accommodate multiple refer-
ences.  Nie?en et al (2000) calculated the length-
normalized edit distance, called word error rate 
(WER), between a candidate and multiple refer-
ence translations. Leusch et al (2003) proposed a 
related measure called position-independent word 
error rate (PER) that did not consider word posi-
tion, i.e. using bag-of-words instead. Instead of 
error measures, we can also use accuracy measures 
that compute similarity between candidate and ref-
erence translations in proportion to the number of 
common words between them as suggested by 
Melamed (1995). An n-gram co-occurrence meas-
ure, BLEU, proposed by Papineni et al (2001) that 
calculates co-occurrence statistics based on n-gram 
overlaps have shown great potential. A variant of 
BLEU developed by NIST (2002) has been used in 
two recent large-scale machine translation evalua-
tions. 
Recently, Turian et al (2003) indicated that 
standard accuracy measures such as recall, preci-
sion, and the F-measure can also be used in evalua-
tion of machine translation. However, results based 
on their method, General Text Matcher (GTM), 
showed that unigram F-measure correlated best 
with human judgments while assigning more 
weight to higher n-gram (n > 1) matches achieved 
similar performance as Bleu. Since unigram 
matches do not distinguish words in consecutive 
positions from words in the wrong order, measures 
based on position-independent unigram matches 
are not sensitive to word order and sentence level 
structure. Therefore, systems optimized for these 
unigram-based measures might generate adequate 
but not fluent target language. 
Since BLEU has been used to report the perform-
ance of many machine translation systems and it 
has been shown to correlate well with human 
judgments, we will explain BLEU in more detail 
and point out its limitations in the next section. We 
then introduce a new evaluation method called 
ROUGE-L that measures sentence-to-sentence 
similarity based on the longest common subse-
quence statistics between a candidate translation 
and a set of reference translations in Section 3. 
Section 4 describes another automatic evaluation 
method called ROUGE-S that computes skip-
bigram co-occurrence statistics. Section 5 presents 
the evaluation results of ROUGE-L, and ROUGE-
S and compare them with BLEU, GTM, NIST, 
PER, and WER in correlation with human judg-
ments in terms of adequacy and fluency. We con-
clude this paper and discuss extensions of the 
current work in Section 6. 
2 BLEU and N-gram Co-Occurrence 
To automatically evaluate machine translations 
the machine translation community recently 
adopted an n-gram co-occurrence scoring proce-
dure BLEU (Papineni et al 2001). In two recent 
large-scale machine translation evaluations spon-
sored by NIST, a closely related automatic evalua-
tion method, simply called NIST score, was used. 
The NIST (NIST 2002) scoring method is based on 
BLEU. 
The main idea of BLEU is to measure the simi-
larity between a candidate translation and a set of 
reference translations with a numerical metric. 
They used a weighted average of variable length n-
gram matches between system translations and a 
set of human reference translations and showed 
that the weighted average metric correlating highly 
with human assessments.  
BLEU measures how well a machine translation 
overlaps with multiple human translations using n-
gram co-occurrence statistics. N-gram precision in 
BLEU is computed as follows: 
 
? ?
? ?
? ??
? ??
?
?
=
}{
}{
)(
)(
CandidatesC Cgramn
CandidatesC Cgramn
clip
n gramnCount
gramnCount
p  (1) 
 
Where Countclip(n-gram) is the maximum num-
ber of n-grams co-occurring in a candidate transla-
tion and a reference translation, and Count(n-
gram) is the number of n-grams in the candidate 
translation. To prevent very short translations that 
try to maximize their precision scores, BLEU adds a 
brevity penalty, BP, to the formula: 
 
)2(
1
|)|/||1( ??
?
??
?
?
>
=
? rcife
rcif
BP cr  
 
Where |c| is the length of the candidate transla-
tion and |r| is the length of the reference transla-
tion. The BLEU formula is then written as follows: 
 
)3(logexp
1
??
???
?
?= ?
=
N
n
nn pwBPBLEU  
 
The weighting factor, wn, is set at 1/N. 
Although BLEU has been shown to correlate well 
with human assessments, it has a few things that 
can be improved. First the subjective application of 
the brevity penalty can be replaced with a recall 
related parameter that is sensitive to reference 
length. Although brevity penalty will penalize can-
didate translations with low recall by a factor of e(1-
|r|/|c|), it would be nice if we can use the traditional 
recall measure that has been a well known measure 
in NLP as suggested by Melamed (2003). Of 
course we have to make sure the resulting compos-
ite function of precision and recall is still correlates 
highly with human judgments. 
Second, although BLEU uses high order n-gram 
(n>1) matches to favor candidate sentences with 
consecutive word matches and to estimate their 
fluency, it does not consider sentence level struc-
ture. For example, given the following sentences: 
 
S1. police killed the gunman 
S2. police kill the gunman1 
S3. the gunman kill police 
 
We only consider BLEU with unigram and bi-
gram, i.e. N=2, for the purpose of explanation and 
call this BLEU-2. Using S1 as the reference and S2 
and S3 as the candidate translations, S2 and S3 
would have the same BLEU-2 score, since they 
both have one bigram and three unigram matches2. 
However, S2 and S3 have very different meanings. 
Third, BLEU is a geometric mean of unigram to 
N-gram precisions. Any candidate translation 
without a N-gram match has a per-sentence BLEU 
score of zero. Although BLEU is usually calculated 
over the whole test corpus, it is still desirable to 
have a measure that works reliably at sentence 
level for diagnostic and introspection purpose. 
To address these issues, we propose three new 
automatic evaluation measures based on longest 
common subsequence statistics and skip bigram 
co-occurrence statistics in the following sections. 
3 Longest Common Subsequence 
3.1 ROUGE-L 
A sequence Z = [z1, z2, ..., zn] is a subsequence of 
another sequence X = [x1, x2, ..., xm], if there exists 
a strict increasing sequence [i1, i2, ..., ik] of indices 
of X such that for all j = 1, 2, ..., k, we have xij = zj  
(Cormen et al 1989). Given two sequences X and 
Y, the longest common subsequence (LCS) of X 
and Y is a common subsequence with maximum 
length. We can find the LCS of two sequences of 
length m and n using standard dynamic program-
ming technique in O(mn) time. 
LCS has been used to identify cognate candi-
dates during construction of N-best translation 
lexicons from parallel text. Melamed (1995) used 
the ratio (LCSR) between the length of the LCS of 
two words and the length of the longer word of the 
two words to measure the cognateness between 
them. He used as an approximate string matching 
algorithm. Saggion et al (2002) used normalized 
pairwise LCS (NP-LCS) to compare similarity be-
tween two texts in automatic summarization 
evaluation. NP-LCS can be shown as a special case 
of Equation (6) with ? = 1. However, they did not 
provide the correlation analysis of NP-LCS with 
                                                     
1 This is a real machine translation output. 
2 The ?kill? in S2 or S3 does not match with ?killed? in 
S1 in strict word-to-word comparison.  
human judgments and its effectiveness as an auto-
matic evaluation measure. 
To apply LCS in machine translation evaluation, 
we view a translation as a sequence of words. The 
intuition is that the longer the LCS of two transla-
tions is, the more similar the two translations are. 
We propose using LCS-based F-measure to esti-
mate the similarity between two translations X of 
length m and Y of length n, assuming X is a refer-
ence translation and Y is a candidate translation, as 
follows: 
Rlcs 
m
YXLCS ),(
=       (4) 
Plcs 
n
YXLCS ),(
=       (5) 
Flcs
lcslcs
lcslcs
PR
PR
2
2 )1(
?
?
+
+
=   (6) 
 
Where LCS(X,Y) is the length of a longest common 
subsequence of X and Y, and ? = Plcs/Rlcs when 
?Flcs/?Rlcs_=_?Flcs/?Plcs. We call the LCS-based F-
measure, i.e. Equation 6, ROUGE-L. Notice that 
ROUGE-L is 1 when X = Y since LCS(X,Y) = m or 
n; while ROUGE-L is zero when LCS(X,Y) = 0, i.e. 
there is nothing in common between X and Y. F-
measure or its equivalents has been shown to have 
met several theoretical criteria in measuring accu-
racy involving more than one factor (Van Rijsber-
gen 1979). The composite factors are LCS-based 
recall and precision in this case. Melamed et al 
(2003) used unigram F-measure to estimate ma-
chine translation quality and showed that unigram 
F-measure was as good as BLEU.  
One advantage of using LCS is that it does not 
require consecutive matches but in-sequence 
matches that reflect sentence level word order as n-
grams. The other advantage is that it automatically 
includes longest in-sequence common n-grams, 
therefore no predefined n-gram length is necessary.  
ROUGE-L as defined in Equation 6 has the prop-
erty that its value is less than or equal to the mini-
mum of unigram F-measure of X and Y. Unigram 
recall reflects the proportion of words in X (refer-
ence translation) that are also present in Y (candi-
date translation); while unigram precision is the 
proportion of words in Y that are also in X. Uni-
gram recall and precision count all co-occurring 
words regardless their orders; while ROUGE-L 
counts only in-sequence co-occurrences.  
By only awarding credit to in-sequence unigram 
matches, ROUGE-L also captures sentence level 
structure in a natural way. Consider again the ex-
ample given in Section 2 that is copied here for 
convenience: 
 
S1. police killed the gunman 
S2. police kill the gunman 
S3. the gunman kill police 
 
As we have shown earlier, BLEU-2 cannot differ-
entiate S2 from S3. However, S2 has a ROUGE-L 
score of 3/4 = 0.75 and S3 has a ROUGE-L score 
of 2/4 = 0.5, with ? = 1. Therefore S2 is better than 
S3 according to ROUGE-L. This example also il-
lustrated that ROUGE-L can work reliably at sen-
tence level. 
However, LCS only counts the main in-sequence 
words; therefore, other longest common subse-
quences and shorter sequences are not reflected in 
the final score. For example, consider the follow-
ing candidate sentence: 
 
S4. the gunman police killed 
 
Using S1 as its reference, LCS counts either ?the 
gunman? or ?police killed?, but not both; therefore, 
S4 has the same ROUGE-L score as S3. BLEU-2 
would prefer S4 over S3. In Section 4, we will in-
troduce skip-bigram co-occurrence statistics that 
do not have this problem while still keeping the 
advantage of in-sequence (not necessary consecu-
tive) matching that reflects sentence level word 
order. 
3.2 Multiple References 
So far, we only demonstrated how to compute 
ROUGE-L using a single reference. When multiple 
references are used, we take the maximum LCS 
matches between a candidate translation, c, of n 
words and a set of u reference translations of  mj 
words. The LCS-based F-measure can be 
computed as follows: 
Rlcs-multi ???
?
???
?
=
=
j
ju
j m
crLCS ),(
max 1       (7) 
Plcs-multi ???
?
???
?
=
= n
crLCS ju
j
),(
max 1       (8) 
Flcs-multi  
multilcsmultilcs
multilcsmultilcs
PR
PR
??
??
+
+
= 2
2 )1(
?
?
 (9) 
 
where ? = Plcs-multi/Rlcs-multi when ?Flcs-multi/?Rlcs-
multi_=_?Flcs-multi/?Plcs-multi. 
 
This procedure is also applied to computation of 
ROUGE-S when multiple references are used. In 
the next section, we introduce the skip-bigram co-
occurrence statistics. In the next section, we de-
scribe how to extend ROUGE-L to assign more 
credits to longest common subsequences with con-
secutive words. 
3.3 ROUGE-W: Weighted Longest Common 
Subsequence 
LCS has many nice properties as we have de-
scribed in the previous sections. Unfortunately, the 
basic LCS also has a problem that it does not dif-
ferentiate LCSes of different spatial relations 
within their embedding sequences. For example, 
given a reference sequence X and two candidate 
sequences Y1 and Y2 as follows: 
 
X:  [A B C D E F G] 
Y1: [A B C D H I K] 
Y2:  [A H B K C I D] 
 
Y1 and Y2 have the same ROUGE-L score. How-
ever, in this case, Y1 should be the better choice 
than Y2 because Y1 has consecutive matches. To 
improve the basic LCS method, we can simply re-
member the length of consecutive matches encoun-
tered so far to a regular two dimensional dynamic 
program table computing LCS. We call this 
weighted LCS (WLCS) and use k to indicate the 
length of the current consecutive matches ending at 
words xi and yj. Given two sentences X and Y, the 
WLCS score of X and Y can be computed using the 
following dynamic programming procedure: 
 
(1) For (i = 0; i <=m; i++) 
        c(i,j) = 0  // initialize c-table 
        w(i,j) = 0 // initialize w-table 
(2) For (i = 1; i <= m; i++) 
        For (j = 1; j <= n; j++) 
          If xi = yj Then 
     // the length of consecutive matches at 
     // position i-1 and j-1 
     k = w(i-1,j-1) 
     c(i,j) = c(i-1,j-1) + f(k+1) ? f(k) 
     // remember the length of consecutive 
     // matches at position i, j 
     w(i,j) = k+1 
          Otherwise 
     If c(i-1,j) > c(i,j-1) Then 
    c(i,j) = c(i-1,j) 
    w(i,j) = 0           // no match at i, j 
     Else c(i,j) = c(i,j-1) 
     w(i,j) = 0           // no match at i, j 
(3) WLCS(X,Y) = c(m,n) 
 
Where c is the dynamic programming table, c(i,j) 
stores the WLCS score ending at word xi of X and 
yj of Y, w is the table storing the length of consecu-
tive matches ended at c table position i and j, and f 
is a function of consecutive matches at the table 
position, c(i,j). Notice that by providing different 
weighting function f, we can parameterize the 
WLCS algorithm to assign different credit to con-
secutive in-sequence matches.  
The weighting function f must have the property 
that f(x+y) > f(x) + f(y) for any positive integers x 
and y. In other words, consecutive matches are 
awarded more scores than non-consecutive 
matches. For example, f(k)-=-?k ? ? when k >= 0, 
and ?, ? > 0. This function charges a gap penalty 
of ?? for each non-consecutive n-gram sequences. 
Another possible function family is the polynomial 
family of the form k? where -? > 1. However, in 
order to normalize the final ROUGE-W score, we 
also prefer to have a function that has a close form 
inverse function. For example, f(k)-=-k2 has a close 
form inverse function f -1(k)-=-k1/2. F-measure 
based on WLCS can be computed as follows, 
given two sequences X of length m and Y of length 
n: 
Rwlcs ???
?
???
?
=
?
)(
),(1
mf
YXWLCSf       (10) 
Pwlcs ???
?
???
?
=
?
)(
),(1
nf
YXWLCSf       (11) 
Fwlcs  
wlcswlcs
wlcswlcs
PR
PR
2
2 )1(
?
?
+
+
=           (12) 
 
Where f -1 is the inverse function of f. We call the 
WLCS-based F-measure, i.e. Equation 12, 
ROUGE-W. Using Equation 12 and f(k)-=-k2 as the 
weighting function, the ROUGE-W scores for se-
quences Y1 and Y2 are 0.571 and 0.286 respec-
tively. Therefore, Y1 would be ranked higher than 
Y2 using WLCS. We use the polynomial function 
of the form k? in the ROUGE evaluation package. In 
the next section, we introduce the skip-bigram co-
occurrence statistics. 
4 ROUGE-S: Skip-Bigram Co-Occurrence 
Statistics 
Skip-bigram is any pair of words in their sen-
tence order, allowing for arbitrary gaps. Skip-
bigram co-occurrence statistics measure the over-
lap of skip-bigrams between a candidate translation 
and a set of reference translations. Using the ex-
ample given in Section 3.1: 
 
S1. police killed the gunman 
S2. police kill the gunman 
S3. the gunman kill police 
S4. the gunman police killed 
 
Each sentence has C(4,2)3 = 6 skip-bigrams. For 
example, S1 has the following skip-bigrams: 
 
                                                     
3 Combination: C(4,2) = 4!/(2!*2!) = 6. 
(?police killed?, ?police the?, ?police gunman?, 
?killed the?, ?killed gunman?, ?the gunman?)  
 
S2 has three skip-bigram matches with S1 (?po-
lice the?, ?police gunman?, ?the gunman?), S3 has 
one skip-bigram match with S1 (?the gunman?), 
and S4 has two skip-bigram matches with S1 (?po-
lice killed?, ?the gunman?).  Given translations X 
of length m and Y of length n, assuming X is a ref-
erence translation and Y is a candidate translation, 
we compute skip-bigram-based F-measure as fol-
lows: 
 
Rskip2 )2,(
),(2
mC
YXSKIP
=           (13) 
Pskip2 )2,(
),(2
nC
YXSKIP
=           (14) 
Fskip2 
2
2
2
22
2 )1(
skipskip
skipskip
PR
PR
?
?
+
+
=   (15) 
 
Where SKIP2(X,Y) is the number of skip-bigram 
matches between X and Y, ? = Pskip2/Rskip2 when 
?Fskip2/?Rskip2_=_?Fskip2/?Pskip2, and  C is the combi-
nation function. We call the skip-bigram-based F-
measure, i.e. Equation 15, ROUGE-S. 
Using Equation 15 with ? = 1 and S1 as the ref-
erence, S2?s ROUGE-S score is 0.5, S3 is 0.167, 
and S4 is 0.333. Therefore, S2 is better than S3 and 
S4, and S4 is better than S3. This result is more 
intuitive than using BLEU-2 and ROUGE-L. One 
advantage of skip-bigram vs. BLEU is that it does 
not require consecutive matches but is still sensi-
tive to word order. Comparing skip-bigram with 
LCS, skip-bigram counts all in-order matching 
word pairs while LCS only counts one longest 
common subsequence. 
We can limit the maximum skip distance, dskip, 
between two in-order words that is allowed to form 
a skip-bigram. Applying such constraint, we limit 
skip-bigram formation to a fix window size. There-
fore, computation time can be reduced and hope-
fully performance can be as good as the version 
without such constraint. For example, if we set dskip 
to 0 then ROUGE-S is equivalent to bigram over-
lap. If we set dskip to 4 then only word pairs of at 
most 4 words apart can form skip-bigrams. 
Adjusting Equations 13, 14, and 15 to use maxi-
mum skip distance limit is straightforward: we 
only count the skip-bigram matches, SKIP2(X,Y), 
within the maximum skip distance and replace de-
nominators of Equations 13, C(m,2), and 14, 
C(n,2), with the actual numbers of within distance 
skip-bigrams from the reference and the candidate 
respectively.  
In the next section, we present the evaluations of 
ROUGE-L, ROUGE-S, and compare their per-
formance with other automatic evaluation meas-
ures. 
5 Evaluations 
One of the goals of developing automatic evalua-
tion measures is to replace labor-intensive human 
evaluations. Therefore the first criterion to assess 
the usefulness of an automatic evaluation measure 
is to show that it correlates highly with human 
judgments in different evaluation settings. How-
ever, high quality large-scale human judgments are 
hard to come by. Fortunately, we have access to 
eight MT systems? outputs, their human assess-
ment data, and the reference translations from 2003 
NIST Chinese MT evaluation (NIST 2002a). There 
were 919 sentence segments in the corpus. We first 
computed averages of the adequacy and fluency 
scores of each system assigned by human evalua-
tors. For the input of automatic evaluation meth-
ods, we created three evaluation sets from the MT 
outputs: 
1. Case set: The original system outputs with 
case information. 
2. NoCase set: All words were converted 
into lower case, i.e. no case information 
was used. This set was used to examine 
whether human assessments were affected 
by case information since not all MT sys-
tems generate properly cased output. 
3. Stem set: All words were converted into 
lower case and stemmed using the Porter 
stemmer (Porter 1980). Since ROUGE 
computed similarity on surface word 
level, stemmed version allowed ROUGE 
to perform more lenient matches. 
To accommodate multiple references, we use a 
Jackknifing procedure. Given N references, we 
compute the best score over N sets of N-1 refer-
ences. The final score is the average of the N best 
scores using N different sets of N-1 references.  
The Jackknifing procedure is adopted since we 
often need to compare system and human perform-
ance and the reference translations are usually the 
only human translations available. Using this pro-
cedure, we are able to estimate average human per-
formance by averaging N best scores of one 
reference vs. the rest N-1 references.  
We then computed average BLEU1-12 4 , GTM 
with exponents of 1.0, 2.0, and 3.0, NIST, WER, 
and PER scores over these three sets. Finally we 
applied ROUGE-L, ROUGE-W with weighting 
function k1.2, and ROUGE-S without skip distance 
                                                     
4 BLEUN computes BLEU over n-grams up to length N. 
Only BLEU1, BLEU4, and BLEU12 are shown in Table 1.  
limit and with skip distant limits of 0, 4, and 9. 
Correlation analysis based on two different correla-
tion statistics, Pearson?s ? and Spearman?s ?, with 
respect to adequacy and fluency are shown in Ta-
ble 1.  
The Pearson?s correlation coefficient5 measures the 
strength and direction of a linear relationship be-
tween any two variables, i.e. automatic metric 
score and human assigned mean coverage score in 
our case. It ranges from +1 to -1. A correlation of 1 
means that there is a perfect positive linear rela-
tionship between the two variables, a correlation of 
-1 means that there is a perfect negative linear rela-
tionship between them, and  a correlation of 0 
means that there is no linear relationship between 
them. Since we would like to use automatic 
evaluation metric not only in comparing systems 
                                                     
5 For a quick overview of the Pearson?s coefficient, see: 
http://davidmlane.com/hyperstat/A34739.html. 
but also in in-house system development, a good 
linear correlation with human judgment would en-
able us to use automatic scores to predict corre-
sponding human judgment scores. Therefore, 
Pearson?s correlation coefficient is a good measure 
to look at. 
Spearman?s correlation coefficient 6  is also a 
measure of correlation between two variables. It is 
a non-parametric measure and is a special case of 
the Pearson?s correlation coefficient when the val-
ues of data are converted into ranks before comput-
ing the coefficient. Spearman?s correlation 
coefficient does not assume the correlation be-
tween the variables is linear. Therefore it is a use-
ful correlation indicator even when good linear 
correlation, for example, according to Pearson?s 
correlation coefficient between two variables could 
                                                     
6 For a quick overview of the Spearman?s coefficient, see: 
http://davidmlane.com/hyperstat/A62436.html. 
Adequacy
Method P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U
BLEU1 0.86 0.83 0.89 0.80 0.71 0.90 0.87 0.84 0.90 0.76 0.67 0.89 0.91 0.89 0.93 0.85 0.76 0.95
BLEU4 0.77 0.72 0.81 0.77 0.71 0.89 0.79 0.75 0.82 0.67 0.55 0.83 0.82 0.78 0.85 0.76 0.67 0.89
BLEU12 0.66 0.60 0.72 0.53 0.44 0.65 0.72 0.57 0.81 0.65 0.25 0.88 0.72 0.58 0.81 0.66 0.28 0.88
NIST 0.89 0.86 0.92 0.78 0.71 0.89 0.87 0.85 0.90 0.80 0.74 0.92 0.90 0.88 0.93 0.88 0.83 0.97
WER 0.47 0.41 0.53 0.56 0.45 0.74 0.43 0.37 0.49 0.66 0.60 0.82 0.48 0.42 0.54 0.66 0.60 0.81
PER 0.67 0.62 0.72 0.56 0.48 0.75 0.63 0.58 0.68 0.67 0.60 0.83 0.72 0.68 0.76 0.69 0.62 0.86
ROUGE-L 0.87 0.84 0.90 0.84 0.79 0.93 0.89 0.86 0.92 0.84 0.71 0.94 0.92 0.90 0.94 0.87 0.76 0.95
ROUGE-W 0.84 0.81 0.87 0.83 0.74 0.90 0.85 0.82 0.88 0.77 0.67 0.90 0.89 0.86 0.91 0.86 0.76 0.95
ROUGE-S* 0.85 0.81 0.88 0.83 0.76 0.90 0.90 0.88 0.93 0.82 0.70 0.92 0.95 0.93 0.97 0.85 0.76 0.94
ROUGE-S0 0.82 0.78 0.85 0.82 0.71 0.90 0.84 0.81 0.87 0.76 0.67 0.90 0.87 0.84 0.90 0.82 0.68 0.90
ROUGE-S4 0.82 0.78 0.85 0.84 0.79 0.93 0.87 0.85 0.90 0.83 0.71 0.90 0.92 0.90 0.94 0.84 0.74 0.93
ROUGE-S9 0.84 0.80 0.87 0.84 0.79 0.92 0.89 0.86 0.92 0.84 0.76 0.93 0.94 0.92 0.96 0.84 0.76 0.94
GTM10 0.82 0.79 0.85 0.79 0.74 0.83 0.91 0.89 0.94 0.84 0.79 0.93 0.94 0.92 0.96 0.84 0.79 0.92
GTM20 0.77 0.73 0.81 0.76 0.69 0.88 0.79 0.76 0.83 0.70 0.55 0.83 0.83 0.79 0.86 0.80 0.67 0.90
GTM30 0.74 0.70 0.78 0.73 0.60 0.86 0.74 0.70 0.78 0.63 0.52 0.79 0.77 0.73 0.81 0.64 0.52 0.80
Fluency
Method P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U
BLEU1 0.81 0.75 0.86 0.76 0.62 0.90 0.73 0.67 0.79 0.70 0.62 0.81 0.70 0.63 0.77 0.79 0.67 0.90
BLEU4 0.86 0.81 0.90 0.74 0.62 0.86 0.83 0.78 0.88 0.68 0.60 0.81 0.83 0.78 0.88 0.70 0.62 0.81
BLEU12 0.87 0.76 0.93 0.66 0.33 0.79 0.93 0.81 0.97 0.78 0.44 0.94 0.93 0.84 0.97 0.80 0.49 0.94
NIST 0.81 0.75 0.87 0.74 0.62 0.86 0.70 0.64 0.77 0.68 0.60 0.79 0.68 0.61 0.75 0.77 0.67 0.88
WER 0.69 0.62 0.75 0.68 0.57 0.85 0.59 0.51 0.66 0.70 0.57 0.82 0.60 0.52 0.68 0.69 0.57 0.81
PER 0.79 0.74 0.85 0.67 0.57 0.82 0.68 0.60 0.73 0.69 0.60 0.81 0.70 0.63 0.76 0.65 0.57 0.79
ROUGE-L 0.83 0.77 0.88 0.80 0.67 0.90 0.76 0.69 0.82 0.79 0.64 0.90 0.73 0.66 0.80 0.78 0.67 0.90
ROUGE-W 0.85 0.80 0.90 0.79 0.63 0.90 0.78 0.73 0.84 0.72 0.62 0.83 0.77 0.71 0.83 0.78 0.67 0.90
ROUGE-S* 0.84 0.78 0.89 0.79 0.62 0.90 0.80 0.74 0.86 0.77 0.64 0.90 0.78 0.71 0.84 0.79 0.69 0.90
ROUGE-S0 0.87 0.81 0.91 0.78 0.62 0.90 0.83 0.78 0.88 0.71 0.62 0.82 0.82 0.77 0.88 0.76 0.62 0.90
ROUGE-S4 0.84 0.79 0.89 0.80 0.67 0.90 0.82 0.77 0.87 0.78 0.64 0.90 0.81 0.75 0.86 0.79 0.67 0.90
ROUGE-S9 0.84 0.79 0.89 0.80 0.67 0.90 0.81 0.76 0.87 0.79 0.69 0.90 0.79 0.73 0.85 0.79 0.69 0.90
GTM10 0.73 0.66 0.79 0.76 0.60 0.87 0.71 0.64 0.78 0.80 0.67 0.90 0.66 0.58 0.74 0.80 0.64 0.90
GTM20 0.86 0.81 0.90 0.80 0.67 0.90 0.83 0.77 0.88 0.69 0.62 0.81 0.83 0.77 0.87 0.74 0.62 0.89
GTM30 0.87 0.81 0.91 0.79 0.67 0.90 0.83 0.77 0.87 0.73 0.62 0.83 0.83 0.77 0.88 0.71 0.60 0.83
With Case Information (Case) Lower Case (NoCase) Lower Case & Stemmed (Stem)
With Case Information (Case) Lower Case (NoCase) Lower Case & Stemmed (Stem)
Table 1. Pearson?s ? and Spearman?s ? correlations of automatic evaluation measures vs. adequacy
and fluency: BLEU1, 4, and 12 are BLEU with maximum of 1, 4, and 12 grams, NIST is the NIST 
score, ROUGE-L is LCS-based F-measure (? = 1), ROUGE-W is weighted LCS-based  F-measure (? 
= 1). ROUGE-S* is skip-bigram-based co-occurrence statistics with any skip distance limit, ROUGE-
SN is skip-bigram-based F-measure (? = 1) with maximum skip distance of N, PER is position inde-
pendent word error rate, and WER is word error rate. GTM 10, 20, and 30 are general text matcher 
with exponents of 1.0, 2.0, and 3.0. (Note, only BLEU1, 4, and 12 are shown here to preserve space.) 
 
not be found. It also suits the NIST MT evaluation 
scenario where multiple systems are ranked ac-
cording to some performance metrics. 
To estimate the significance of these correlation 
statistics, we applied bootstrap resampling, gener-
ating random samples of the 919 different sentence 
segments. The lower and upper values of 95% con-
fidence interval are also shown in the table. Dark 
(green) cells are the best correlation numbers in 
their categories and light gray cells are statistically 
equivalent to the best numbers in their categories. 
Analyzing all runs according to the adequacy and 
fluency table, we make the following observations: 
Applying the stemmer achieves higher correla-
tion with adequacy but keeping case information 
achieves higher correlation with fluency except for 
BLEU7-12 (only BLEU12 is shown). For example, 
the Pearson?s ? (P) correlation of ROUGE-S* with 
adequacy increases from 0.85 (Case) to 0.95 
(Stem) while its Pearson?s ? correlation with flu-
ency drops from 0.84 (Case) to 0.78 (Stem). We 
will focus our discussions on the Stem set in ade-
quacy and Case set in fluency. 
The Pearson's ? correlation values in the Stem 
set of the Adequacy Table, indicates that ROUGE-
L and ROUGE-S with a skip distance longer than 0 
correlate highly and linearly with adequacy and 
outperform BLEU and NIST. ROUGE-S* achieves 
that best correlation with a Pearson?s ? of 0.95. 
Measures favoring consecutive matches, i.e. 
BLEU4 and 12, ROUGE-W, GTM20 and 30, 
ROUGE-S0 (bigram), and WER have lower Pear-
son?s ?. Among them WER (0.48) that tends to 
penalize small word movement is the worst per-
former. One interesting observation is that longer 
BLEU has lower correlation with adequacy. 
Spearman?s ? values generally agree with Pear-
son's ? but have more equivalents. 
The Pearson's ? correlation values in the Stem 
set of the Fluency Table, indicates that BLEU12 has 
the highest correlation (0.93) with fluency. How-
ever, it is statistically indistinguishable with 95% 
confidence from all other metrics shown in the 
Case set of the Fluency Table except for WER and 
GTM10.  
GTM10 has good correlation with human judg-
ments in adequacy but not fluency; while GTM20 
and GTM30, i.e. GTM with exponent larger than 
1.0, has good correlation with human judgment in 
fluency but not adequacy. 
ROUGE-L and ROUGE-S*, 4, and 9 are good 
automatic evaluation metric candidates since they 
perform as well as BLEU in fluency correlation 
analysis and outperform BLEU4 and 12 signifi-
cantly in adequacy. Among them, ROUGE-L is the 
best metric in both adequacy and fluency correla-
tion with human judgment according to Spear-
man?s correlation coefficient and is statistically 
indistinguishable from the best metrics in both 
adequacy and fluency correlation with human 
judgment according to Pearson?s correlation coef-
ficient. 
6 Conclusion 
In this paper we presented two new objective 
automatic evaluation methods for machine transla-
tion, ROUGE-L based on longest common subse-
quence (LCS) statistics between a candidate 
translation and a set of reference translations. 
Longest common subsequence takes into account 
sentence level structure similarity naturally and 
identifies longest co-occurring in-sequence n-
grams automatically while this is a free parameter 
in BLEU.   
To give proper credit to shorter common se-
quences that are ignored by LCS but still retain the 
flexibility of non-consecutive matches, we pro-
posed counting skip bigram co-occurrence. The 
skip-bigram-based ROUGE-S* (without skip dis-
tance restriction) had the best Pearson's ? correla-
tion of 0.95 in adequacy when all words were 
lower case and stemmed. ROUGE-L, ROUGE-W, 
ROUGE-S*, ROUGE-S4, and ROUGE-S9 were 
equal performers to BLEU in measuring fluency. 
However, they have the advantage that we can ap-
ply them on sentence level while longer BLEU such 
as BLEU12 would not differentiate any sentences 
with length shorter than 12 words (i.e. no 12-gram 
matches). We plan to explore their correlation with 
human judgments on sentence-level in the future. 
We also confirmed empirically that adequacy and 
fluency focused on different aspects of machine 
translations. Adequacy placed more emphasis on 
terms co-occurred in candidate and reference trans-
lations as shown in the higher correlations in Stem 
set than Case set in Table 1; while the reverse was 
true in the terms of fluency. 
The evaluation results of ROUGE-L, ROUGE-
W, and ROUGE-S in machine translation evalua-
tion are very encouraging. However, these meas-
ures in their current forms are still only applying 
string-to-string matching. We have shown that bet-
ter correlation with adequacy can be reached by 
applying stemmer. In the next step, we plan to ex-
tend them to accommodate synonyms and para-
phrases. For example, we can use an existing 
thesaurus such as WordNet (Miller 1990) or creat-
ing a customized one by applying automated syno-
nym set discovery methods (Pantel and Lin 2002) 
to identify potential synonyms. Paraphrases can 
also be automatically acquired using statistical 
methods as shown by Barzilay and Lee (2003). 
Once we have acquired synonym and paraphrase 
data, we then need to design a soft matching func-
tion that assigns partial credits to these approxi-
mate matches. In this scenario, statistically 
generated data has the advantage of being able to 
provide scores reflecting the strength of similarity 
between synonyms and paraphrased.  
ROUGE-L, ROUGE-W, and ROUGE-S have 
also been applied in automatic evaluation of sum-
marization and achieved very promising results 
(Lin 2004). In Lin and Och (2004), we proposed a 
framework that automatically evaluated automatic 
MT evaluation metrics using only manual transla-
tions without further human involvement. Accord-
ing to the results reported in that paper, ROUGE-L, 
ROUGE-W, and ROUGE-S also outperformed 
BLEU and NIST. 
References 
Akiba, Y., K. Imamura, and E. Sumita. 2001. Us-
ing Multiple Edit Distances to Automatically 
Rank Machine Translation Output. In Proceed-
ings of the MT Summit VIII, Santiago de Com-
postela, Spain. 
Barzilay, R. and L. Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Mul-
tiple-Sequence Alignmen. In Proceeding of 
NAACL-HLT 2003, Edmonton, Canada. 
Leusch, G., N. Ueffing, and H. Ney. 2003. A 
Novel String-to-String Distance Measure with 
Applications to Machine Translation Evaluation. 
In Proceedings of MT Summit IX, New Orleans, 
U.S.A. 
Levenshtein, V. I. 1966. Binary codes capable of 
correcting deletions, insertions and reversals. 
Soviet Physics Doklady. 
Lin, C.Y. 2004. ROUGE: A Package for Automatic 
Evaluation of Summaries. In Proceedings of the 
Workshop on Text Summarization Branches 
Out, post-conference workshop of ACL 2004, 
Barcelona, Spain. 
Lin, C.-Y. and F. J. Och. 2004. ORANGE: a Method 
for Evaluating Automatic Evaluation Metrics for 
Machine Translation. In Proceedings of 20th In-
ternational Conference on Computational Lin-
guistic (COLING 2004), Geneva, Switzerland. 
Miller, G. 1990. WordNet: An Online Lexical Da-
tabase. International Journal of Lexicography, 
3(4). 
Melamed, I.D. 1995. Automatic Evaluation and 
Uniform Filter Cascades for Inducing N-best 
Translation Lexicons. In Proceedings of the 3rd 
Workshop on Very Large Corpora (WVLC3). 
Boston, U.S.A. 
Melamed, I.D., R. Green and J. P. Turian. 2003. 
Precision and Recall of Machine Translation. In 
Proceedings of NAACL/HLT 2003, Edmonton, 
Canada. 
Nie?en S., F.J. Och, G, Leusch, H. Ney. 2000. An 
Evaluation Tool for Machine Translation: Fast 
Evaluation for MT Research. In Proceedings of 
the 2nd International Conference on Language 
Resources and Evaluation, Athens, Greece. 
NIST. 2002. Automatic Evaluation of Machine 
Translation Quality using N-gram Co-
Occurrence Statistics.   AAAAAAAAAAA 
http://www.nist.gov/speech/tests/mt/doc/ngram-
study.pdf 
Pantel, P. and Lin, D. 2002. Discovering Word 
Senses from Text. In Proceedings of SIGKDD-
02. Edmonton, Canada. 
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 
2001. BLEU: a Method for Automatic Evaluation 
of Machine Translation. IBM Research Report 
RC22176 (W0109-022). 
Porter, M.F. 1980. An Algorithm for Suffix Strip-
ping. Program, 14, pp. 130-137. 
Saggion H., D. Radev, S. Teufel, and W. Lam. 
2002. Meta-Evaluation of Summaries in a 
Cross-Lingual Environment Using Content-
Based Metrics. In Proceedings of COLING-
2002, Taipei, Taiwan. 
Su, K.-Y., M.-W. Wu, and J.-S. Chang. 1992. A 
New Quantitative Quality Measure for Machine 
Translation System. In Proceedings of 
COLING-92, Nantes, France. 
Thompson, H. S. 1991. Automatic Evaluation of 
Translation Quality: Outline of Methodology 
and Report on Pilot Experiment. In Proceedings 
of the Evaluator?s Forum, ISSCO, Geneva, 
Switzerland. 
Turian, J. P., L. Shen, and I. D. Melamed. 2003. 
Evaluation of Machine Translation and its 
Evaluation. In Proceedings of MT Summit IX, 
New Orleans, U.S.A. 
Van Rijsbergen, C.J. 1979. Information Retrieval. 
Butterworths. London. 
Maximum Entropy Models for Named Entity Recognition
Oliver Bender
 
and Franz Josef Och

and Hermann Ney
 
 
Lehrstuhl fu?r Informatik VI

Information Sciences Institute
Computer Science Department University of Southern California
RWTH Aachen - University of Technology Marina del Rey, CA 90292
D-52056 Aachen, Germany och@isi.edu

bender,ney  @cs.rwth-aachen.de
Abstract
In this paper, we describe a system that applies
maximum entropy (ME) models to the task of
named entity recognition (NER). Starting with
an annotated corpus and a set of features which
are easily obtainable for almost any language,
we first build a baseline NE recognizer which
is then used to extract the named entities and
their context information from additional non-
annotated data. In turn, these lists are incor-
porated into the final recognizer to further im-
prove the recognition accuracy.
1 Introduction
In this paper, we present an approach for extracting the
named entities (NE) of natural language inputs which
uses the maximum entropy (ME) framework (Berger et
al., 1996). The objective can be described as follows.
Given a natural input sequence 	
    

we
choose the NE tag sequence 
 






with the
highest probability among all possible tag sequences:



  ffStatistical QA - Classifier vs. Re-ranker: What?s the difference? 
Deepak Ravichandran, Eduard Hovy, and Franz Josef Och 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
{ravichan,hovy,och}@isi.edu 
Abstract 
In this paper, we show that we can ob-
tain a good baseline performance for 
Question Answering (QA) by using 
only 4 simple features. Using these fea-
tures, we contrast two approaches used 
for a Maximum Entropy based QA sys-
tem. We view the QA problem as a 
classification problem and as a re-
ranking problem. Our results indicate 
that the QA system viewed as a re-
ranker clearly outperforms the QA sys-
tem used as a classifier. Both systems 
are trained using the same data.  
1 Introduction 
Open-Domain factoid Question Answering (QA) 
is defined as the task of answering fact-based 
questions phrased in Natural Language. Exam-
ples of some question and answers that fall in 
the fact-based category are: 
 
1. What is the capital of Japan? - Tokyo 
2. What is acetaminophen? - Non-aspirin pain killer 
3. Where is the Eiffel Tower? - Paris 
 
The architecture of most of QA systems consists 
of two basic modules: the information retrieval 
(IR) module and the answer pinpointing module. 
These two modules are used in a typical pipeline 
architecture. 
For a given question, the IR module finds a 
set of relevant segments. Each segment typically 
consists of at most R sentences1 . The answer 
pinpointing module processes each of these seg-
ments and finds the appropriate answer phrase. 
                                                          
1
 In our experiments we use R=1  
phrase. Evaluation of a QA system is judged on 
the basis on the final output answer and the cor-
responding evidence provided by the segment. 
This paper focuses on the answer pinpointing 
module. Typical QA systems perform re-ranking 
of candidate answers as an important step in 
pinpointing. The goal is to rank the most likely 
answer first by using either symbolic or statisti-
cal methods. Some QA systems make use of 
statistical answer pinpointing (Xu et. al, 2002; 
Ittycheriah, 2001; Ittycheriah and Salim, 2002) 
by treating it as a classification problem. In this 
paper, we cast the pinpointing problem in a sta-
tistical framework and compare two approaches, 
classification and re-ranking. 
2 Statistical Answer Pinpointing 
2.1 Answer Modeling 
The answer-pinpointing module gets as input a 
question q and a set of possible answer candi-
dates }...{ 21 Aaaa . It outputs one of the answer 
}...{ 21 Aaaaa ?  from the candidate answer set. 
We consider two ways of modeling this prob-
lem.  
One approach is the traditional classification 
view (Ittycheriah, 2001) where we present each 
Question-Answer pair to the classifier which 
classifies it as either correct answer (true) or in-
correct answer (false), based on some evidence 
(features).  
In this case, we model )},...{,|( 21 qaaaacP A . 
Here, 
 false}{true,c = signifies the correctness 
of the answer a  with respect to the question q . 
The probability )},...{,|( 21 qaaaacP A  for each QA 
pair is modeled independently of other such 
pairs. Thus, for the same question, many QA 
pairs are presented to the classifier as independ-
ent events (histories). If the training corpus con-
tains Q questions with A answers for each ques-
tion, the total number of events (histories) would 
be equal to Q?A with two classes (futures) (cor-
rect or incorrect answer) for each event. Once 
the probabilities )},...{,|( 21 qaaaacP A  have been 
computed, the system has to return the best an-
swer. The following decision rule is used: 
)]},...{,|([maxarg 21 qaaaatruePa A
a
=
?
 
Another way of viewing the problem is as a 
re-ranking task. This is possible because the QA 
task requires the identification of only one cor-
rect answer, instead of identifying all the correct 
answer in the collection. In this case, we model 
)},...{|( 21 qaaaaP A . If the training corpus contains 
Q questions with A answers for each question, 
the total number of events (histories) would be 
equal to Q, with A classes (futures). This view 
requires the following decision-rule to identify 
the answer that seems most promising: 
)]},...{|([maxarg 21 qaaaaPa A
a
=
?
 
In summary, 
 Classifier Re-ranker 
#Events (Histo-
ries) 
Q?A Q 
#Classes (Futures) 
per event 
2 A 
 
where, 
Q = total number of questions. 
A = total number of answer chunks considered 
for each question. 
2.2 Maximum Entropy formulation 
We use Maximum Entropy to model the given 
problem both as a classifier and a re-ranker. We 
define M feature functions, 
Mmqaaaaf Am ,....,1),},...{,( 21 = , that may be useful 
in characterizing the task. Della Pietra et. al 
(1995) contains good description of Maximum 
Entropy models. 
We model the classifier as follows: 
? ?
?
=
=
=
?
1
21?,
1
21,
21
)]},...{,(exp[
)]},...{,(exp[
)},...{,|(
c
M
m
Amcm
M
m
Amcm
A
qaaaaf
qaaaaf
qaaaacP
?
?
 
where, 
},{;,....,1;
,
falsetruecMmcm ==? are the model 
parameters. 
The decision rule for choosing the best an-
swer is: 
])},...{,([maxarg
)]},...{,|([maxarg
1
21,
21
?
=
?
=
=
M
m
Amtruem
a
A
a
qaaaaf
qaaaatruePa
?
 
The above decision rule requires comparison of 
different probabilities of the 
form )},...{,|( 21 qaaaatrueP A . However, these 
probabilities are modeled as independent events 
(histories) in the classifier and hence the training 
criterion does not make them directly compara-
ble. 
For the re-ranker, we model the probability 
as: 
( )
? ?
?
=
=
=
?
1
21
1
21
21
)]},...{,?(exp[
)]},...{,(exp[
},...{|
a
M
m
Amm
M
m
Amm
A
qaaaaf
qaaaaf
qaaaaP
?
?
 
where, 
Mmm ,....,1; =?  are the model parameters. 
Note that for the classifier the model parameters 
are cm,?  , whereas for the re-ranker they are m? . 
This is because for the classifier, each feature 
function has different weights associated with 
each class (future). Hence, the classifier has 
twice the model parameters as compared to the 
re-ranker. 
The decision rule for the re-ranker is given by: 
.
?
=
?
=
=
M
m
Amm
a
A
a
qaaaaf
qaaaaPa
1
21
21
)]},...{,([maxarg
)]},...{|([maxarg
?
  
The re-ranker makes the probabilities 
)},...{|( 21 qaaaaP A , considered for the decision 
rule, directly comparable against each other, by 
incorporating them into the training criterion 
itself. Table 1 summarizes the differences of the 
two models. 
  
2.3 Feature Functions 
Using above formulation to model the probabil-
ity distribution we need to come up with features 
fj. We use only four basic feature functions for 
our system. 
1. Frequency: It has been observed that the 
correct answer has a higher frequency 
(Magnini et al; 2002) in the collection of 
answer chunks (C). Hence we count the 
number of time a potential answer occurs in 
the IR output and use its logarithm as a fea-
ture. This is a positive continuous valued 
feature. 
2. Expected Answer Class: Most of the current 
QA systems employ some type of Answer 
Class Identification module. Thus questions 
like ?When did Bill Clinton go to college??, 
would be identified as a question asking 
about a time (or a time period), ?Where is 
the sea of tranquility?? would be identified 
as a question asking for a location. If the an-
swer class matches the expected answer 
class (derived from the question by the an-
swer identification module) this feature fires 
(i.e., it has a value of 1). Details of this mod-
ule are explained in Hovy et al (2002). This 
is a binary-valued feature. 
3. Question Word Absent: Usually a correct 
answer sentence contains a few of the ques-
tion words. This feature fires if the candidate 
answer does not contain any of the question 
words. This is also a binary valued feature. 
4. Word Match: It is the sum of ITF2 values for 
the words in the question that matches iden-
tically with the words in the answer sen-
tence. This is a positive continuous valued 
feature. 
2.4 Training 
We train our Maximum Entropy model using 
Generalized Iterative scaling (Darroch and 
Ratcliff, 1972) approach by using YASMET3 . 
3 Evaluation Metric 
The performance of the QA system is highly 
dependent on the performance of the two indi-
vidual modules IR and answer-pinpointing. The 
system would have excellent performance if 
both have good accuracy. Hence, we need a 
good evaluation metric to evaluate each of these 
components individually. One standard metric 
for IR is recall and precision. We can modify 
this metric for QA as follows: 
                                                          
2
 ITF = Inverse Term Frequency. We take a large inde-
pendent corpus & estimate ITF(W) =1/(count(W)), where 
W = Word. 
3
 YASMET. (Yet Another Small Maximum Entropy 
Toolkit) http://www-i6.informatik.rwth-aachen.de/ 
Colleagues/och/software/YASMET.html 
 
 Classifier Re-Ranker 
Mode
ling 
Equa-
tion ? ?
?
=
=
=
?
1
21?,
1
21,
21
)]},...{,(exp[
)]},...{,(exp[
)},...{,|(
c
M
m
Amcm
M
m
Amcm
A
qaaaaf
qaaaaf
qaaaacP
?
?
 
? ?
?
=
=
=
?
1
21
1
21
21
)]},...{,?(exp[
)]},...{,(exp[
)},...{|(
a
M
m
Amm
M
m
Amm
A
qaaaaf
qaaaaf
qaaaaP
?
?
 
 
Deci-
sion 
Rule  
])},...{,([maxarg
)}},...{,|({maxarg
1
21,
21
?
=
?
=
=
M
m
Atruemm
a
A
a
qaaaaf
qaaaatruePa
?
 
 
.
?
=
?
=
=
M
m
Amm
a
A
a
qaaaaf
qaaaaPa
1
21
21
)]},...{,([maxarg
)]},...{|([maxarg
?
 
 
Table 1 : Model comparison between a Classifier and Re-ranker 
 segmentsanswer relevant  Total #
returnedsegment  answer relevant  #
  Recall =  
returned segments Total #
returned segmentsanswer relevant  #
 Precision =  
It is almost impossible to measure recall be-
cause the IR collection is typically large and in-
volves several hundreds of thousands of 
documents. Hence, we evaluate our IR by only 
the precision measure at top N segments. This 
method is actually a rather sloppy approximation 
to the original recall and precision measure. 
Questions with fewer correct answers in the col-
lection would have a lower precision score as 
compared to questions with many answers. 
Similarly, it is unclear how one would evaluate 
answer questions with No Answer (NIL) in the 
collection using this metric. All these questions 
would have zero precision from the IR collec-
tion. 
The answer-pinpointing module is evaluated 
by checking if the answer returned by the system 
as the top ranked (#1) answer is correct/incorrect 
with respect to the IR collection and the true 
answer. Hence, if the IR system fails to return 
even a single sentence that contains the correct 
answer for the given question, we do not penal-
ize the answer-pinpointing module. It is again 
unclear how to evaluate questions with No an-
swer (NIL). (Here, for our experiments we at-
tribute the error to the IR module.) 
Finally, the combined system is evaluated by 
using the standard technique, wherein the An-
swer (ranked #1) returned by the system is 
judged to be either correct or incorrect and then 
the average is taken. 
Question: 
1395 Who is Tom Cruise married to ? 
 
IR Output: 
1 Tom Cruise is married to actress Nicole Kidman and they have two adopted children . 
2 Tom Cruise is married to Nicole Kidman . 
. 
. 
 
Output of Chunker: (The number to the left of each chunk records the IR sentence from 
which that particular chunk came) 
1  Tom Cruise 
1  Tom 
1  Cruise 
1  is married 
1  married 
1  actress Nicole Kidman and they 
1  actress Nicole Kidman 
1  actress 
1  Nicole Kidman 
1  Nicole 
1  Kidman 
1  they 
1  two adopted children 
1  two 
1  adopted 
1  children 
2  Tom Cruise 
2  Tom 
2  Cruise 
2  is married 
2  married 
2  Nicole Kidman 
2  Nicole 
2  Kidman 
. 
. 
 
Figure 1 : Candidate answer extraction for a question. 
4 Experiments 
4.1 Framework 
Information Retrieval 
For our experiments, we use the Web search 
engine AltaVista. For every question, we re-
move stop-words and present all other question 
words as query to the Web search engine. The 
top relevant documents are downloaded. We 
apply a sentence segmentor, and identify those 
sentences that have high ITF overlapping words 
with the given question. The sentences are then 
re-ranked accordingly and only the top K sen-
tences (segments) are presented as output of the 
IR system. 
Candidate Answer Extraction 
For a given question, the IR returns top K 
segments. For our experiments a segment con-
sists of one sentence. We parse each of the sen-
tences and obtain a set of chunks, where each 
chunk is a node of the parse tree. Each chunk is 
viewed as a potential answer. For our experi-
ments we restrict the number of potential an-
swers to be at most 5000. We illustrate this 
process in Figure 1. 
Training/Test Data 
Table 2 : Training size and sources. 
 
 Training + 
Validation 
Test 
Question collec-
tion 
TREC 9 + 
TREC 10 
TREC11 
Total questions 1192 500 
 
We use the TREC 9 and TREC 10 data sets 
for training and the TREC 11 data set for testing. 
We initially apply the IR step as described above 
and obtain a set of at most 5000 answers. For 
each such answer we use the pattern file sup-
plied by NIST to tag answer chunks as either 
correct (1) or incorrect (0). This is a very noisy 
way of tagging data. In some cases, even though 
the answer chunk may be tagged as correct it 
may not be supported by the accompanying sen-
tence, while in other cases, a correct chunk may 
be graded as incorrect, since the pattern file list 
did not represent a exhaustive list of answers. 
We set aside 20% of the training data for valida-
tion. 
4.2 Classifier vs. Re-Ranker 
We evaluate the performance of the QA system 
viewed as a classifier (with a post-processing 
step) and as a re-ranker. In order to do a fair 
evaluation of the system we test the performance 
of the QA system under varying conditions of 
the output of the IR system. The results are 
shown in Table 3. 
The results should be read in the following 
way: We use the same IR system. However, dur-
ing each run of the experiment we consider only 
the top K sentences returned by the IR system 
K={1,10,50,100,150,200}. The column ? cor-
rect?  represents the number of questions the en-
tire QA (IR + re-ranker) system answered 
correctly. ? IR Loss?  represents the average 
number of questions for which the IR failed 
completely (i.e., the IR did not return even a sin-
gle sentence that contains the correct answer). 
The IR precision is the precision of the IR sys-
tem for the number of sentences considered. An-
swer-pinpointing performance is based on the 
metric described above. Finally, the overall 
score is the score of the entire QA system. (i.e., 
precision at rank#1). 
The ? Overall Precision" column indicates 
that the re-ranker clearly outperforms the classi-
fier. However, it is also very interesting to com-
pare the performance of the re-ranker ? Overall 
Precision?  with the ? Answer-Pinpointing preci-
sion? . For example, in the last row, for the re-
ranker the ? Answer-Pinpointing Precision?  is 
0.5182 whereas the ? Overall Precision?  is only 
0.34. The difference is due to the performance of 
the poor performance of the IR system (? IR 
Loss?  = 0.344). 
4.3 Oracle IR system 
In order to determine the performance of the 
answer pinpointing module alone, we perform 
the so-called oracle IR experiment. Here, we 
present to the answer pinpointing module only 
those sentences from IR that contain an answer4. 
The task of the answer pinpointing module is to 
pick out of the correct answer from the given 
collection. We report results in Table 4. In these 
results too the re-ranker has better performance 
as compared to the classifier. However, as we 
see from the results, there is a lot of room for 
improvement for the re-ranker system, even with 
a perfect IR system. 
5 Discussion 
Our experiments clearly indicate that the QA 
system viewed as a re-ranker outperforms the 
QA system viewed as a classifier. The difference 
stem from the following reasons: 
1. The classification training criteria work on a 
more difficult objective function of trying to 
find whether each candidate answer answers 
the given question, as opposed to trying to 
find the best answer for the given question. 
Hence, the same feature set that works for 
the re-ranker need not work for the classi-
fier. The feature set used in this problem is 
not good enough to help the classifier dis-
tinguish between correct and incorrect an-
                                                          
4
 This was performed by extracting all the sentences that 
were judged to have the correct answer by human evalua-
tors during the TREC 2002 evaluations. 
swers for the given question (even though it 
is good for the re-ranker to come up with the 
best answer). 
2. The comparison of probabilities across dif-
ferent events (histories) for the classifier, 
during the decision rule process, is problem-
atic. This is because the probabilities, which 
we obtain after the classification approach, 
are only a poor estimate of the true probabil-
ity. The re-ranker, however, directly allows 
these probabilities to be comparable by in-
corporating them into the model itself. 
3. The QA system viewed as a classifier suf-
fers from the problem of a highly unbal-
anced data set. We have less than 1% 
positive examples and more than 99% nega-
tive examples (we had almost 4 million 
training data events) in the problem. Ittyche-
riah (2001), and Ittycheriah and Roukos 
(2002), use a more controlled environment 
for training their system. They have 23% 
positive examples and 77% negative exam-
ples. They prune out most of the incorrect 
answer initially, using a pre-processing step 
by using either a rule-based system (Ittyche-
riah, 2001) or a statistical system (Ittyche-
riah et al, 2002); and hence obtain a much 
more manageable distribution in the training 
phase of the Maximum Entropy model. 
Answer-Pinpointing 
Precision Number Correct Overall Precision IR Sen-
tences 
Total 
ques-
tions IR Precision IR Loss Classifier Re-ranker Classifier Re-ranker Classifier Re-ranker 
1 500 0.266 0.742 0.0027 0.3565 29 46 0.058 0.092 
10 500 0.2018 0.48 0.0016 0.4269 7 111 0.014 0.222 
50 500 0.1155 0.386 0.0015 0.4885 6 150 0.012 0.3 
100 500 0.0878 0.362 0.0015 0.5015 5 160 0.01 0.32 
150 500 0.0763 0.35 0.0015 0.5138 5 167 0.01 0.334 
200 500 0.0703 0.344 0.0015 0.5182 3 170 0.01 0.34 
Table 3 : Results for Classifier and Re-ranker under varying conditions of IR. 
IR Sentences = Total IR sentences considered for every question 
IR Precision = Precision @ (IR Sentences) 
IR Loss = (Number of Questions for which the IR did not produce a single answer)/(Total Questions) 
Overall Precision = (Number Correct)/(Total Questions) 
  
 
 
 
 
6 Conclusion 
The re-ranker system is very robust in handling 
large amounts of data and still produces reason-
able results. There is no need for a major pre-
processing step (for eliminating undesirable in-
correct answers from the training) or the post-
processing step (for selecting the most promis-
ing answer.) 
We also consider it significant that a QA sys-
tem with just 4 features (viz. Frequency, 
Expected Answer Type, Question word absent, 
and ITF word match) is a good baseline system 
and performs better than the median perform-
ance of all the QA systems in the TREC 2002 
evaluations5.  
Ittycheriah (2001), and Ittycheriah and Rou-
kos (2002) have shown good results by using a 
range of features for Maximum Entropy QA sys-
tems. Also, the results indicate that there is 
scope for research in IR for QA systems. The 
QA system has an upper ceiling on performance 
due to the quality of the IR system. The QA 
community has yet to address these problems in 
a principled way, and the IR details of most of 
the system are hidden behind the complicated 
system architecture. 
The re-ranking model basically changes the 
objective function for training and the system is 
directly optimized on the evaluation function 
criteria (though still using Maximum Likelihood 
training). Also this approach seems to be very 
robust to noisy training data and is highly scal-
able. 
Acknowledgements. 
This work was supported by the Advance Re-
search and Development Activity (ARDA)?s 
Advanced Question Answering for Intelligence 
(AQUAINT) Program under contract number 
                                                          
5
 However, since the IR system used here was from the 
Web, our results are not directly comparable with the 
TREC systems. 
MDA908-02-C-007. The authors wish to ex-
press particular gratitude to Dr. Abraham It-
tycheriah, both for his supervision and education 
of the first author during his summer visit to 
IBM TJ Watson Research Center in 2002 and 
for his thoughtful comments on this paper, 
which was inspired by his work. 
References 
Darroch, J. N., and D. Ratcliff. 1972. Generalized 
iterative scaling for log-linear models. Annals of 
Mathematical Statistics, 43:1470?1480. 
Hermjakob, U. 1997. Learning Parse and Translation 
Decisions from Examples with Rich Context. 
Ph.D. Dissertation, University of Texas at Austin, 
Austin, TX.  
Hovy, E.H., U. Hermjakob, D. Ravichandran. 2002. 
A Question/Answer Typology with Surface Text 
Patterns. Proceedings of the DARPA Human Lan-
guage Technology Conferenc,. San Diego, CA, 
247?250. 
Ittycheriah, A. 2001. Trainable Question Answering 
System. Ph.D. Dissertation, Rutgers, The State 
University of New Jersey, New Brunswick, NJ. 
Ittycheriah., A., and S. Roukos. 2002. IBM?S Ques-
tion Answering System-TREC-11. Proceedings of 
TREC 2002, NIST, MD, 394?401. 
Magnini, B, M. Negri, R. Prevete, and H. Tanev. 
2002. Is it the Right Answer? Exploiting Web Re-
dundancy for Answer Validation. Proceedings of 
the 40th Meeting of the Association of Computa-
tional Linguistics, Philadelphia, PA, 425?432. 
Della Pietra, S., V. Della Pietra, and J. Lafferty. 
1995. Inducing Features of Random Fields, Tech-
nical Report Department of Computer Science, 
Carnegie-Mellon University, CMU?CS-95?144. 
Xu, J., A. J. Licuanan, S. May, R. Miller, and R. 
Weischedel. 2002. TREC2002QA at BBN: An-
swer Selection and Confidence Estimation. Pro-
ceedings of TREC 2002. NIST MD. 290?295 
Answer-Pinpointing 
Precision Total ques-
tions IR precision Classifier Re-ranker 
429 1.0 0.156 0.578 
Table 4 : Performance with a perfect IR system 
 
A Compar i son  of  A l ignment  Mode ls  for S ta t i s t i ca l  Mach ine  
Trans la t ion  
Franz Josef Och and Hermann Ney 
Lehrstuhl fiir Informatik VI, Comlmter Science Department 
RWTH Aachen - University of Technology 
D-52056 Aachen, Germany 
{och, ney}~inf ormat ik. ruth-aachen, de 
Abst ract  
In this paper, we t)resent and compare various align- 
nmnt models for statistical machine translation. We 
propose to measure tile quality of an aligmnent 
model using the quality of the Viterbi alignment 
comt)ared to a manually-produced alignment and de- 
scribe a refined mmotation scheme to produce suit- 
able reference alignments. We also con,pare the im- 
pact of different; alignment models on tile translation 
quality of a statistical machine translation system. 
1 I n t roduct ion  
In statistical machine translation (SMT) it is neces- 
sm'y to model the translation probability P r ( f l  a Ic~). 
Here .fi' = f denotes tile (15'ench) source and e{ = e 
denotes the (English) target string. Most SMT 
models (Brown et al, 1993; Vogel et al, 1996) 
try to model word-to-word corresl)ondences between 
source and target words using an alignment nmpl)ing 
from source l)osition j to target position i = aj. 
We can rewrite tim t)robal)ility Pr(fille~) t) 3, in- 
troducing the 'hidden' alignments ai 1 := al ...aj...a.l 
(aj C {0 , . . . , /} ) :  
Pr(f~lel) = ~Pr(f i ' ,a~le{) 
.1 
? j -1  I~ = E H Pr(fj 'ajlf i '-"al 'e l )  
q, j=l 
To allow fbr French words wlfich do not directly cor- 
respond to any English word an artificial 'empty' 
word c0 is added to the target sentence at position 
i=0.  
The different alignment models we present pro- 
vide different decoInt)ositions of Pr(f~,a~le(). An 
alignnlent 5~ for which holds 
a~ = argmax Pr(fi' , a'l'\[eI) 
at 
for a specific model is called V i terb i  al ignment of" 
this model. 
In this paper we will describe extensions to tile 
Hidden-Markov alignment model froln (Vogel et al, 
1.996) and compare tlmse to Models 1 - 4 of (Brown 
et al, 1993). We t)roI)ose to measure the quality of 
an alignment nlodel using the quality of tlle Viterbi 
alignment compared to a manually-produced align- 
ment. This has the advantage that once having pro- 
duced a reference alignlnent, the evaluation itself can 
be performed automatically. In addition, it results in 
a very precise and relia.ble valuation criterion which 
is well suited to assess various design decisions in 
modeling and training of statistical alignment mod- 
els. 
It, is well known that manually pertbrming a word 
aligmnent is a COlnplicated and ambiguous task 
(Melamed, 1998). Therefore, to produce tlle refer- 
ence alignment we use a relined annotation scheme 
which reduces the complications and mnbiguities oc- 
curring in the immual construction of a word align- 
ment. As we use tile alignment models for machine 
translation purposes, we also evahlate the resulting 
translation quality of different nlodels. 
2 Al ignment  w i th  HMM 
In the Hidden-Markov alignment model we assume 
a first-order dependence for tim aligmnents aj and 
that the translation probability depends Olfly on aj 
and not  Oil (tj_l: 
- ~-' el) =p(ajl. j-,,Z)p(J~l%) Pr(fj,(glf~ ',% , 
Later, we will describe a refinement with a depen- 
dence on e,,j_, iu the alignment model. Putting 
everything together, we have the following basic 
HMM-based modeh 
.1 
*'(flJl~I) = ~ I I  \[~,(-jla~.-,, z). p(fj l%)\] (1) 
at j= l  
with the alignment I)robability p(ili',I ) and the 
translation probability p(fle). To find a Viterbi 
aligninent for the HMM-based model we resort to 
dynamic progralnming (Vogel et al, 1996). 
The training of tlm HMM is done by the EM- 
algorithm. In the E-step the lexical and alignment 
1086 
counts for one sentenee-i)air (f, e) are calculated: 
c(flc; f, e) = E P"(a l f '  e) ~ 5(f, f~)5(e, c~) 
a i,j 
,.:(ill', z; f, e) = E / ' , ' (a i r ,  e) aj) 
a j 
In the M-step the lexicon and translation probabili- 
ties are: 
p(f le) o< ~-~c(fle;f('~),e (~)) 
8 
P( i l i ' , I )  o (Ec ( i l i ' , I ; fO) ,e (~) )  
8 
To avoid the smlunation ov(;r all possible aligmnents 
a, (Vogel et el., 1996) use the maximum apllroxima- 
tion where only the Viterbi alignlnent )ath is used to 
collect counts. We used the Baron-Welch-algorithm 
(Baum, 1972) to train the model parameters in out' 
ext)eriments. Theret/y it is possible to t)erti)rm an 
efl-iciellt training using; all aligmnents. 
To make the alignlnenl; t)arameters indo,1)en(lent 
t'ronl absolute word i)ositions we assmne that the 
alignment i)robabilities p(i\[i', I )  (lel)end only Oil the 
jmnp width (i - i'). Using a set of non-negative 
t)arameters {c(i - i ' )} ,  we can write the alignment 
probabilities ill the fl)rm: 
~'(i - i') (2) p(i l i ' ,  I)  = 
c(,,:" - i ' )  
This form ensures that for eadl word posilion it, 
i' = 1, ..., I , the aligmnent probat)ilities atis(y th(, 
normalization constraint. 
Extension:  refined a l igmnent mode l  
The count table e(i - i') has only 2.1  ......... - 1 en- 
tries. This might be suitable for small corpora, but 
fi)r large corpora it is possil)le to make a more re- 
fine(1 model of Pr (a j  ~i-I  i - I  Ji ,% ,c'~). Est)ecially, we 
analyzed the effect of a det)endence on c,b_ ~ or .fj. 
As a dependence on all English words wouht result 
ill a huge mmflmr of aligmnent 1)arameters we use as 
(Brown et el., 1993) equivalence classes G over tlle 
English and the French words. Here G is a mallping 
of words to (:lasses. This real)ping is trained au- 
tonmtically using a modification of the method de- 
scrilled ill (Kneser and Ney, 1991.). We use 50 classes 
in our exlmriments. The most general form of align- 
ment distribution that we consider in the ItMM is 
p(aj - a.+_, la(%), G(f~), h -  
Extension:  empty  word 
In the original formulation of the HMM alignment 
model there ix no 'empty' word which generates 
Fren(:h words having no directly aligned English 
word. A direct inchlsion of an eml/ty wor(t ill the 
HMM model by adding all c o as in (Brown et al, 
1.993) is not 1)ossit)le if we want to model the j un lp  
distances i - i', as the I)osition i = 0 of tim emt)ty 
word is chosen arbitrarily. Therefore, to introduce 
the eml)ty word we extend the HMM network by I 
empty words ci+ 1.'2I The English word ci has a co l  
rest)onding eml)ty word el+ I. The I)osition of the 
eml)ty word encodes the previously visited English 
word. 
We enforce the following constraints for the tran- 
sitions in the HMM network (i _< I, i' _< I): 
p(i  + I l i ' , I )  = pff . 5( i , i ' )  
V(i + I l l '  + I, I )  = J J .  5( i , i ' )  
p(i l i '  + I, 1) = p(iIi ' ,1) 
The parameter pff is the 1)robability of a transition 
to the emt)ty word. In our extleriments we set pIl = 
0.2. 
Smooth ing  
For a t)etter estimation of infrequent events we in- 
troduce the following smoothing of alignment )rob- 
abilities: 
1 
F(a j I~ j - , ,~)  = ~" ~- + (1 - , , ) .p (a j la j _ l  , I )  
in our exlleriments we use (t = 0.4. 
3 Mode l  1 and  Mode l  2 
l~cl)lacing the (l(~,t)endence on aj - l  in the HMM 
alignment mo(M I)y a del)endence on j, we olltain 
a model wlfich (:an lie seen as a zero-order Hid(l(m- 
Markov Model which is similar to Model 2 1)rot)ose(t 
t/y (Brown et al, 1993). Assmning a mfiform align- 
ment prol)ability p(i l j ,  I )  = 1/1, we obtain Model 
1. 
Assuming that the dominating factor in the align- 
ment model of Model 2 is the distance relative to the 
diagonal line of the (j, i) plane the too(tel p(i l j  , I)  can 
1)e structured as tbllows (Vogel et al, 1996): 
,'(i -, 
- (3) v(ilj, 5 = Ei,=t r ( ' i '  l 
This model will be referred to as diagonal-oriented 
Model 2. 
4 Mode l  3 and  Mode l  4 
Model:  The fertility models of (Brown et el., 1993) 
explicitly model the probability l,(?lc) that the En- 
glish word c~ is aligned to 
4,, = E 
J 
\]~rench words. 
1087 
Model 3 of (Brown et al, 1993) is a zero-order 
alignment model like Model 2 including in addi- 
tion fertility paranmters. Model 4 of (Brown et al, 
1993) is also a first-order alignment model (along 
the source positions) like the HMM, trot includes 
also fertilities. In Model 4 the alignment position 
j of an English word depends on the alignment po- 
sition of tile previous English word (with non-zero 
fertility) j ' . It models a jump distance j - j '  (for con- 
secutive English words) while in the HMM a jump 
distance i - i '  (for consecutive French words) is mod- 
eled. Tile full description of Model 4 (Brown et al, 
1993) is rather complica.ted as there have to be con- 
sidered tile cases that English words have fertility 
larger than one and that English words have fertil- 
ity zero. 
For training of Model 3 and Model 4, we use an 
extension of the program GlZA (A1-Onaizan et al, 
1999). Since there is no efficient way in these mod- 
els to avoid tile explicit summation over all align- 
ments in the EM-algorithin, the counts are collected 
only over a subset of promising alignments. It is not 
known an efficient algorithm to compute the Viterbi 
alignment for the Models 3 and 4. Therefore, the 
Viterbi alignment is comlmted only approximately 
using the method described in (Brown et al, 1993). 
The models 1-4 are trained in succession with the 
tinal parameter values of one model serving as the 
starting point tbr the next. 
A special problein in Model 3 and Model 4 con- 
cerns the deficiency of tile model. This results in 
problems in re-estimation of the parameter which 
describes the fertility of the empty word. In nor- 
real EM-training, this parameter is steadily decreas- 
ing, producing too many aligmnents with tile empty 
word. Therefore we set tile prot)ability for aligning 
a source word with tile emt)ty word at a suitably 
chosen constant value. 
As in tile HMM we easily can extend the depen- 
dencies in the alignment model of Model 4 easily 
using the word class of the previous English word 
E = G(ci,), or the word class of the French word 
F = G(I j)  (Brown et al, 1993). 
5 Inc lud ing  a Manual Dictionary 
We propose here a simple method to make use of 
a bilingual dictionary as an additional knowledge 
source in the training process by extending the train- 
ing corpus with the dictionary entries. Thereby, the 
dictionary is used already in EM-training and can 
improve not only the alignment fox" words which are 
in the dictionary but indirectly also for other words. 
The additional sentences in the training cortms are 
weighted with a factor Fl~x during the EM-training 
of the lexicon probabilities. 
We assign tile dictionary entries which really co- 
occur in the training corpus a high weight Fle.~. and 
the remaining entries a vex'y low weight. In our ex- 
periments we use Flex = 10 for the co-occurring dic- 
tionary entries which is equivalent to adding every 
dictionary entry ten times to the training cortms. 
6 The Al ignment Template  System 
The statistical machine-translation method descri- 
bed in (Och et al, 1999) is based on a word aligned 
traiifing corIms and thereby makes use of single- 
word based alignment models. Tile key element of 
tiffs apt/roach are the alignment emplates which are 
pairs of phrases together with an alignment between 
the words within tile phrases. The advantage of 
the alignment emplate approach over word based 
statistical translation models is that word context 
and local re-orderings are explicitly taken into ac- 
count. We typically observe that this approach pro- 
duces better translations than the single-word based 
models. The alignment templates are automatically 
trailmd using a parallel trailxing corlms. For more 
information about the alignment template approach 
see (Och et at., 1999). 
7 Resu l ts  
We present results on the Verbmobil Task which is 
a speech translation task ill the donmin of appoint- 
nxent scheduling, travel planning, and hotel reserva- 
tion (Wahlster, 1993). 
We measure the quality of tile al)ove inentioned 
aligmnent models with x'espect to alignment quality 
and translation quality. 
To obtain a refereuce aligmnent for evaluating 
alignlnent quality, we manually aligned about 1.4 
percent of onr training corpus. We allowed the hu- 
mans who pertbrmed the alignment o specify two 
different kinds of alignments: an S (sure) a, lignment 
which is used for alignmelxts which are unambigu- 
ously and a P (possible) alignment which is used 
for alignments which might or might not exist. The 
P relation is used especially to align words within 
idiomatic expressions, free translations, and missing 
function words. It is guaranteed that S C P. Figure 
1 shows all example of a manually aligned sentence 
with S and P relations. The hunxan-annotated align- 
ment does not prefer rely translation direction and 
lnay therefore contain many-to-one and one-to-many 
relationships. The mmotation has been performed 
by two annotators, producing sets $1, 1~, S2, P2. 
Tile reference aliglunent is produced by forming the 
intersection of the sure aligmnents (S = $1 rqS2) and 
the ration of the possible atignumnts (P = P1 U P'2). 
Tim quality of an alignment A = { (j, aj) } is mea- 
sured using the following alignment error rate: 
AER(S, P; A) = 1 - IA o Sl + IA o Pl 
IAI + ISl 
1088 
that  . . . . . . . . .  \ [ \ ]  
at  . . . . . . . . .  \ [ \ ]  
. . . . . . .  V1V1.  
l eave  . . . . . . .  \[---'l \ [ - "~ " 
. . . . . . .  l i E \ ] .  
l e t  . . . . . . .  C l l -1  " 
e . . . . . .  ? . . . .  
say  . . . . .  ? . . . . .  
would " ? . . . . . . .  
T . . . .  ? . . . . . .  
then"  " ? . . . . . . . .  
? \ [ \ ]  . . . . . . . .  o 
yes  ? . . . . . . . . . .  
-rn I:I '13 O ? ? -~t ~1 
J~ 
o 
Figure i: Exmnple of a manually annotated align- 
ment with sure (filled dots) and possible commotions. 
Obviously, if we colnpare the sure alignnlents of ev- 
ery sitigle annotator with the reference a.ligmnent we 
obtain an AEI{ of zero percent. 
~\[ifl)le l.: Cort)us characteristics for alignment quality 
experiments. 
Train Sente iH : ( i s  
Words 
Vocalmlary 
Dictionary Entries 
Words 
Test Sentences 
Words 
German I English 
34 446 
329 625 / 343 076 
5 936 \] 3 505 
4 183 
4 533 I 5 324 
354 
3 109 I 3 233 
Tal)le 1 shows the characteristics of training and 
test corlms used in the alignment quality ext)eri- 
inents. The test cortms for these ext)eriments (not 
for the translation exl)eriments) is 1)art of the train- 
ing corpus. 
Table 2 shows the aligmnent quality of different 
alignment models. Here the alignment models of 
IIMM and Model 4 do not include a dependence 
on word classes. We conclude that more sophisti- 
cated alignment lnodels are crtlcial tbr good align- 
ment quality. Consistently, the use of a first-order 
aligmnent model, modeling an elnpty word and fer- 
tilities result in better alignments. Interestingly, the 
siinl)ler HMM aligninent model outt)erforms Model 
3 which shows the importance of first-order align- 
ment models. The best t)erformanee is achieved 
with Model 4. The improvement by using a dictio- 
nary is small eomI)ared to the effect of using 1)etter 
a.lignmellt models. We see a significant dill'erence 
in alignment quality if we exchange source and tar- 
get languages. This is due to the restriction in all 
alignment models that a source language word can 
1)e aligned to at most one target language word. If 
German is source language the t'requelltly occurring 
German word coml)ounds, camlot be aligned cor- 
rectly, as they typically correspond to two or more 
English words. 
WaNe 3 shows the effect of including a det)endence 
on word classes in the aligmnent model of ItMM or 
Model 4. By using word classes the results can be 
Table 3: Eft'cot of including a det)endence on word 
classes in the aligmnent model. 
AER \[%\] 
Det)endencies -IIMM I Model 4 
no 8.0 6.5 
source 7.5 6.0 
target 7.1 6.1 
source ? target 7.6 6.1 
improved by 0.9% when using the ItMM and by 0.5% 
when using Model 4. 
For the translation experiments we used a differ- 
ent training and an illdetmndent test corpus (Table 
4). 
Table 4: Corlms characteristics for translation (tual- 
it;.), exlmriments. 
Train 
S ~e,t 
Sentences  
Words 
Vocabulary 
Se l l te l lees  
Words 
PP (trigram LM) 
I German English 
58332 
519523 549921 
7 940 4 673 
147 
1968 2173 
(40.3) 28.8 
For tile evMuation of the translation quality we 
used the automatically comlmtable Word Error Rate 
(WEll.) and the Subjective Sentence Error Rate 
(SSEll,) (Niefien et al, 2000). The WEll, corre- 
spomls to the edit distance t)etween the produced 
translation and one t)redefined reference translation. 
To obtain the SSER the translations are classified by 
human experts into a small number of quality classes 
ranging from "l)ertbet" to "at)solutely wrong". In 
comparison to the WEll,, this criterion is more mean- 
ingflfl, but it is also very exl)ensive to measure. The 
translations are produced by the aligmnent template 
system mentioned in the previous ection. 
1089 
Table 2: Alignment error rate (AER \[%\]) of ditl~rent alignment models tbr the translations directions English 
into German (German words have fertilities) and German into English. 
English -+ German German -~ English 
Dictionary no yes no yes 
Empty Word no lYes yes no l yes yes 
Model 1 17.8 16.9 16.0 22.9 21.7 20.3 
Model 2 12.8 12.5 11.7 17.5 17.1 15.7 
Model 2(diag) 11.8 10.5 9.8 16.4 15.1 13.3 
Mode l  3 10.5 9.3 8.5 15.7 14.5 12.1 
HMM 10.5 9.2 8.0 14.1 12.9 11.5 
Model 4 9.0 7.8 6.5 14.0 12.5 10.8 
Table 5: Effect of different alignment models on 
translation quality. 
Alignlnent Model 
in Training WER\[%\] SSER\[%\] 
Model 1 49.8 22.2 
HMM 47.7 19.3 
Model 4 48.6 16.8 
The results are shown in Table 5. We see a clear 
improvement in translation quality as measured by 
SSER whereas WER is inore or less the same for all 
models. The imwovement is due to better lexicons 
and better alignment templates extracted from the 
resulting aliglunents. 
8 Conclusion 
We have evaluated vm'ious statistical alignment 
models by conlparing the Viterbi alignment of the 
model with a human-made alignment. We have 
shown that by using inore sophisticated models the 
quality of the alignments improves ignificantly. Fur- 
ther improvements in producing better alignments 
are expected from using the HMM alignment model 
to bootstrap the fertility models, fronl making use of 
cognates, and from statistical lignment models that 
are based on word groups rather than single words. 
Acknowledgment 
This article has been partially supported as 
part of the Verbmobil project (contract nmnber 
01 IV 701 T4) by the German Federal Ministry of 
Education, Science, Research and Technology. 
References 
Y. A1-Onaizan, J. Cur\]n, M. Jahr, K. Knight, J. Laf- 
ferty, I. D. Melamed, F. a. Och, D. Purdy, N. A. 
Smith, and D. Yarowsky. 1999. Statistical ina- 
chine translation, final report, JHU workshop. 
http ://www. clsp. j hu. edu/ws99/proj ects/mt/ 
f inal_report/mr- f inal-report, ps. 
L.E. Baum. 1972. An Inequality and Associated 
Maximization Technique in Statistical Estimation 
for Probabilistie Functions of Markov Processes. 
Inequalities, 3:1 8. 
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, 
and R. L. Mercer. 1993. The mathenlatics ofsta- 
tistical machine trmlslation: Parameter estima- 
tion. Computational Linguistics, 19(2):263-311. 
R. Kneser and H. Ney. 1991. Forming Word Classes 
by Statistical Clustering for Statistical Langm~ge 
Modelling. In 1. Quantitative Linguistics Conf. 
I. D. Melamed. 1998. Manual mmotation of transla- 
tional equivalence: The Blinker project. Technical 
Report 98-07, IRCS. 
S. Niegen, F. J. ()ch, G. Leusch, and H. Ney. 
2000. An evaluation tool \]'or machine translation: 
Fast evaluation for mt research. In Proceedings of 
the Second International Conference on Language 
Resources and Evaluation, pages 39-45, Athens, 
Greece, May June. 
F. J. Och, C. Tilhnalm, mid H. Ney. 1999. Improved 
alignment models for statistical machine transla- 
tion. In In Prec. of the Joint SIGDAT Co~? on 
Empirical Methods in Natural Language Process- 
ing and Very LaTye Corpora, pages 20-28, Univer- 
sity of Marylmld, College Park, MD, USA, June. 
S. Vogel, H. Ney, and C. Tilhnann. 1996. HMM- 
based word alignment in statistical translation. 
In COLING '96: The 16th Int. Conf. on Compu- 
tational Linguistics, pages 836-841, Copenhagen, 
August. 
W. Wahlster. 1993. Verbmobil: Translation of face- 
to-face dialogs. In P~vc. of the MT Summit IV, 
pages 127-135, Kobe, Jat)an. 
1090 
 
	
ff387
388
389
390
391
392
393
394
11
12
13
14
15
16
17
18
Robust Knowledge Discovery from Parallel Speech and
Text Sources
F. Jelinek, W. Byrne, S. Khudanpur, B. Hladka?. CLSP, Johns Hopkins University, Baltimore, MD.
H. Ney, F. J. Och. RWTH Aachen University, Aachen, Germany
J. Cur???n. Charles University, Prague, Czech Rep.
J. Psutka. University of West Bohemia, Pilsen, Czech Rep.
1. INTRODUCTION
As a by-product of the recent information explosion, the same
basic facts are often available from multiple sources such as the In-
ternet, television, radio and newspapers. We present here a project
currently in its early stages that aims to take advantage of the re-
dundancies in parallel sources to achieve robustness in automatic
knowledge extraction.
Consider, for instance, the following sampling of actual news
from various sources on a particular day:
CNN: James McDougal, President Bill Clinton?s former business
partner in Arkansas and a cooperating witness in the White-
water investigation, died Sunday while serving a federal prison
term. He was 57.
MSNBC: Fort Worth, Texas, March 8. Whitewater figure James
McDougal died of an apparent heart attack in a private com-
munity hospital in Fort Worth, Texas, Sunday. He was 57.
ABC News: Washington, March 8. James McDougal, a key figure
in Independent Counsel Kenneth Starr?s Whitewater investi-
gation, is dead.
The Detroit News: Fort Worth. James McDougal, a key witness
in Kenneth Starr?s Whitewater investigation of President Clin-
ton and First Lady Hillary Rodham Clinton, died of a heart
attack in a prison hospital Sunday. He was 57.
San Jose Mercury News: James McDougal, the wily Arkansas
banking rogue who drew Bill Clinton and Hillary Rodham
Clinton into real estate deals that have come to haunt them,
died Sunday of cardiac arrest just months before he hoped to
be released from prison. He was 57.
The Miami Herald: Washington. James McDougal, the wily
Arkansas financier and land speculator at the center of the
original Whitewater probe against President Clinton, died
Sunday.
.
Story
Alignment
Speech
RecognitionSpeech Sources
Basic Models:
acoustic
lexical
language
Topic specific
acoustic and language
models
stories
Aligned Sentence
retrieval
Ranked
Answers
Query
Text sources
Figure 1: Information Flow in Alignment and Extraction
We propose to align collections of stories, much like the exam-
ple above, from multiple text and speech sources and then develop
methods that exploit the resulting parallelism both as a tool to im-
prove recognition accuracy and to enable the development of sys-
tems that can reliably extract information from parallel sources.
Our goal is to develop systems that align text sources and rec-
ognize parallel speech streams simultaneously in several languages
by making use of all related text and speech. The initial systems
we intend to develop will process each language independently.
However, our ultimate and most ambitious objective is to align text
sources and recognize speech using a single, integrated multilin-
gual ASR system. Of course, if sufficiently accurate automatic ma-
chine translation (MT) techniques ([1]) were available, we could
address multilingual processing and single language systems in the
same way. However MT techniques are not yet reliable enough
that we expect all words and phrases recognized within languages
to contribute to recognition across languages. We intend to develop
methods that identify the particular words and phrases that both can
be translated reliably and also used to improve story recognition.
As MT technology improves it can be incorporated more exten-
sively within the processing paradigm we propose. We consider
this proposal a framework within which successful MT techniques
can eventually be used for multilingual acoustic processing.
2. PROJECT OBJECTIVES
The first objective is to enhance multi-lingual information sys-
tems by exploiting the processing capabilities for resource-rich lan-
guages to enhance the capabilities for resource-impoverished lan-
guage. The second objective is to advance information retrieval and
knowledge information systems by providing them with consider-
ably improved multi-lingual speech recognition capabilities. Our
research plan proceeds in several steps to (i) collect and (ii) align
multi-lingual parallel speech and text sources, (iii) exploit paral-
lelism for improving ASR within a language, and to (iv) exploit
parallelism for improving ASR across languages. The main infor-
mation flows involved in aligning and exploiting parallel sources
are illustrated in Figure 1. We will initially focus on German, En-
glish and Czech language sources. This section summarizes the
major components of our project.
2.1 Parallel Speech and Text Sources
The monolingual speech and text collections that we will use
to develop techniques to exploit parallelism for improving ASR
within a language are readily available. For instance, the North
American News Text corpus of parallel news streams from 16 US
newspapers and newswire is available from LDC. A 3-year period
yields over 350 million words of multi-source news text.
In addition to data developed within the TIDES and other HLT
programs, we are in the process of identifying and creating our own
multilingual parallel speech and text sources.
FBIS TIDES Multilingual Newstext Collection
For the purposes of developing multilingual alignment techniques,
we intend to use the 240 day, contemporaneous, multilingual news
text collection made available for use to TIDES projects by FBIS.
This corpus contains news in our initial target languages of English,
German, and Czech. The collections are highly parallel, in that
much of the stories are direct translations.
Radio Prague Multilingual Speech and Text Corpus
Speech and news text from Radio Prague was collected under the
direction of J. Psutka with the consent of Radio Prague. The col-
lection contains speech and text in 5 languages: Czech, English,
German, French, and Spanish. The collection began June 1, 2000
and continued for approximately 3 months. The text collection con-
tains the news scripts used for the broadcast; the broadcasts more
or less follow the scripts. The speech is about 3 minutes per day
in each language, which should yield a total of about 5 hours of
speech per language.
Our initial analysis of the Radio Prague corpus suggest that only
approximately 5% of the stories coincide in topic, and that there
is little, if any, direct translation of stories. We anticipate that this
sparseness will make this corpus significantly hard to analyze than
another, highly-parallel corpus. However, we expect this is the
sort of difficulty that will likely be encountered in processing ?real-
world? multilingual news sources.
2.2 Story-level Alignment
Once we have the multiple streams of information we must be
able to align them according to story. A story is the description of
one or more events that happened in a single day and that are re-
ported in a single article by a daily news source the next day. We
expect that we will use the same techniques used in the Topic De-
tection (TDT) field ([5]). Independently of the specific details of
the alignment procedure, there is now substantial evidence that re-
lated stories from parallel streams can be identified using standard
statistical Information Retrieval (IR) techniques.
Sentence Alignment As part of the infrastructure needed to in-
corporate cross-lingual information into language models, we are
employing statistical MT systems to generate English/German and
English/Czech alignments of sentences in the FBIS Newstext Col-
lection. For the English/German sentence and single-word based
alignments, we plan to use statistical models ([4]) [3] which gen-
erate both sentence and word alignments. For English/Czech sen-
tence alignment, we will employ the statistical models trained as
part of the Czech-English MT system developed during the 1999
Johns Hopkins Summer Workshop ([2]).
2.3 Multi-Source Automatic Speech
Recognition
The scenario we propose is extraction of information from paral-
lel text followed by repeated recognition of parallel broadcasts, re-
sulting in a gradual lowering the WER. The first pass is performed
in order to find the likely topics discussed in the story and to iden-
tify the topics relevant to the query. In this process, the acoustic
model will be improved by deriving pronunciation specifications
for out-of-vocabulary words and fixed phrases extracted from the
parallel stories. The language model will be improved by extending
the coverage of the underlying word and phrase vocabulary, and by
specializing the model?s statistics to the narrow topic at hand. As
long as a round of recognition yields new information, the corre-
sponding improvement is incorporated into the recognizer modules
and bootstrapping of the system continues.
Story-specific Language Models from Parallel Speech and Text
Our goal is to create language models combining specific but sparse
statistics, derived from relevant parallel material, with reliable but
unspecific statistics obtainable from large general corpora. We will
create special n-gram language models from the available text, re-
lated or parallel to the spoken stories. We can then interpolate
this special model with a larger pre-existing model, possibly de-
rived from training text associated to the topic of the story. Our
recent STIMULATE work demonstrated success in construction of
topic-specific language models on the basis of hierarchically topic-
organized corpora [8].
Unlike building models from parallel texts, the training of story
specific language models from recognized speech is also affected
by recognition errors in the data which will be used for language
modeling. Confidence measures can be used to estimate the cor-
rectness of individual words or phrases on the recognizer output.
Using this information, n-gram statistics can be extracted from the
recognizer output by selecting those events which are likely to be
correct and which can therefore be used to adjust the original lan-
guage model without introducing new errors to the recognition sys-
tem.
Language Models with Cross-Lingual Lexical Triggers
A trigger language model ([6], [7]) will be constructed for the tar-
get language from the text corpus, where the lexical triggers are not
from the word-history in the target language, but from the aligned
recognized stories in the source language. The trigger informa-
tion becomes most important in those cases in which the baseline
n-gram model in the target language does not supply sufficient in-
formation to predict a word. We expect that content words in the
source language are good predictors for content words in the target
language and that these words are difficult to predict using the tar-
get language alone, and the mutual information techniques used to
identify trigger pairs will be useful here.
Once a spoken source-language story has been recognized, the
words found here there will be used as triggers in the language
model for the recognition of the target-language news broadcasts.
3. SUMMARY
Our goal is to align collections of stories from multiple text and
speech sources in more than one language and then develop meth-
ods that exploit the resulting parallelism both as a tool to improve
recognition accuracy and to enable the development of systems that
can reliably extract information from parallel sources. Much like
a teacher rephrases a concept in a variety of ways to help a class
understand it, the multiple sources, we expect, will increase the po-
tential of success in knowledge extraction. We envision techniques
that will operate repeatedly on multilingual sources by incorporat-
ing newly discovered information in one language into the models
used for all the other languages. Applications of these methods ex-
tend beyond news sources to other multiple-source domains such
as office email and voice-mail, or classroom materials such as lec-
tures, notes and texts.
4. REFERENCES
[1] P. F. Brown, S. A. DellaPietra, V. J. D. Pietra, and R. L.
Mercer. The mathematics of statistical translation.
Computational Linguistics, 19(2), 1993.
[2] K. K. et al Statistical machine translation, WS?99 Final
Report, Johns Hopkins University, 1999.
http://www.clsp.jhu.edu/ws99/projects/mt.
[3] F. J. Och and H. Ney. Improved statistical alignment models.
In ACL?00, pages 440?447, 2000.
[4] F. J. Och, C. Tillmann, and H. Ney. Improved alignment
models for statistical machine translation. In EMNLP/VLC?99,
pages 20?28, 1999.
[5] Proceedings of the Topic Detection and Tracking workshop.
University of Maryland, College Park, MD, October 1997.
[6] C. Tillmann and H. Ney. Selection criteria for word trigger
pairs in language modelling. In ICGI?96, pages 95?106, 1996.
[7] C. Tillmann and H. Ney. Statistical language modeling and
word triggers. In SPECOM?96, pages 22?27, 1996.
[8] D. Yarowsky. Exploiting nonlocal and syntactic word
relationships in language models for conversational speech
recognition, a NSF STIMULATE Project IRI9618874, 1997.
Johns Hopkins University.
The RWTH System for Statistical Translation of Spoken
Dialogues
H. Ney, F. J. Och, S. Vogel
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen, University of Technology
D-52056 Aachen, Germany
ABSTRACT
This paper gives an overview of our work on statistical ma-
chine translation of spoken dialogues, in particular in the
framework of the Verbmobil project. The goal of the
Verbmobil project is the translation of spoken dialogues
in the domains of appointment scheduling and travel plan-
ning. Starting with the Bayes decision rule as in speech
recognition, we show how the required probability distri-
butions can be structured into three parts: the language
model, the alignment model and the lexicon model. We
describe the components of the system and report results
on the Verbmobil task. The experience obtained in the
Verbmobil project, in particular a large-scale end-to-end
evaluation, showed that the statistical approach resulted in
significantly lower error rates than three competing transla-
tion approaches: the sentence error rate was 29% in compar-
ison with 52% to 62% for the other translation approaches.
1. INTRODUCTION
In comparison with written language, speech and espe-
cially spontaneous speech poses additional difficulties for
the task of automatic translation. Typically, these difficul-
ties are caused by errors of the recognition process, which is
carried out before the translation process. As a result, the
sentence to be translated is not necessarily well-formed from
a syntactic point-of-view. Even without recognition errors,
speech translation has to cope with a lack of conventional
syntactic structures because the structures of spontaneous
speech differ from that of written language.
The statistical approach shows the potential to tackle
these problems for the following reasons. First, the statisti-
cal approach is able to avoid hard decisions at any level of
the translation process. Second, for any source sentence, a
translated sentence in the target language is guaranteed to
be generated. In most cases, this will be hopefully a syn-
tactically perfect sentence in the target language; but even
if this is not the case, in most cases, the translated sentence
will convey the meaning of the spoken sentence.
.
Whereas statistical modelling is widely used in speech
recognition, there are so far only a few research groups that
apply statistical modelling to language translation. The pre-
sentation here is based on work carried out in the framework
of the EuTrans project [8] and the Verbmobil project [25].
2. STATISTICAL DECISION THEORY
AND LINGUISTICS
2.1 The Statistical Approach
The use of statistics in computational linguistics has been
extremely controversial for more than three decades. The
controversy is very well summarized by the statement of
Chomsky in 1969 [6]:
?It must be recognized that the notion of a ?probability
of a sentence? is an entirely useless one, under any
interpretation of this term?.
This statement was considered to be true by the major-
ity of experts from artificial intelligence and computational
linguistics, and the concept of statistics was banned from
computational linguistics for many years.
What is overlooked in this statement is the fact that, in an
automatic system for speech recognition or text translation,
we are faced with the problem of taking decisions. It is
exactly here where statistical decision theory comes in. In
speech recognition, the success of the statistical approach is
based on the equation:
Speech Recognition = Acoustic?Linguistic Modelling
+ Statistical Decision Theory
Similarly, for machine translation, the statistical approach
is expressed by the equation:
Machine Translation = Linguistic Modelling
+ Statistical Decision Theory
For the ?low-level? description of speech and image signals,
it is widely accepted that the statistical framework allows
an efficient coupling between the observations and the mod-
els, which is often described by the buzz word ?subsymbolic
processing?. But there is another advantage in using prob-
ability distributions in that they offer an explicit formalism
for expressing and combining hypothesis scores:
? The probabilities are directly used as scores: These
scores are normalized, which is a desirable property:
when increasing the score for a certain element in the
set of all hypotheses, there must be one or several other
elements whose scores are reduced at the same time.
? It is straightforward to combine scores: depending
on the task, the probabilities are either multiplied or
added.
? Weak and vague dependencies can be modelled eas-
ily. Especially in spoken and written natural language,
there are nuances and shades that require ?grey levels?
between 0 and 1.
2.2 Bayes Decision Rule and
System Architecture
In machine translation, the goal is the translation of a
text given in a source language into a target language. We
are given a source string fJ1 = f1...fj ...fJ , which is to be
translated into a target string eI1 = e1...ei...eI . In this arti-
cle, the term word always refers to a full-form word. Among
all possible target strings, we will choose the string with the
highest probability which is given by Bayes decision rule [5]:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )}
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)} .
Here, Pr(eI1) is the language model of the target language,
and Pr(fJ1 |eI1) is the string translation model which will be
decomposed into lexicon and alignment models. The argmax
operation denotes the search problem, i.e. the generation
of the output sentence in the target language. The overall
architecture of the statistical translation approach is sum-
marized in Figure 1.
In general, as shown in this figure, there may be additional
transformations to make the translation task simpler for the
algorithm. The transformations may range from the cate-
gorization of single words and word groups to more complex
preprocessing steps that require some parsing of the source
string. We have to keep in mind that in the search procedure
both the language and the translation model are applied af-
ter the text transformation steps. However, to keep the
notation simple, we will not make this explicit distinction in
the subsequent exposition.
3. ALIGNMENT MODELLING
3.1 Concept
A key issue in modelling the string translation probabil-
ity Pr(fJ1 |eI1) is the question of how we define the corre-
spondence between the words of the target sentence and the
words of the source sentence. In typical cases, we can as-
sume a sort of pairwise dependence by considering all word
pairs (fj , ei) for a given sentence pair (fJ1 ; eI1). Here, we will
further constrain this model by assigning each source word
to exactly one target word. Later, this requirement will be
relaxed. Models describing these types of dependencies are
referred to as alignment models [5, 24].
When aligning the words in parallel texts, we typically
observe a strong localization effect. Figure 2 illustrates this
effect for the language pair German?English. In many cases,
although not always, there is an additional property: over
large portions of the source string, the alignment is mono-
tone.
Source Language Text
Transformation
 Lexicon Model
Language Model
Global Search:
 
 
Target Language Text
 
over
 
 Pr(f1  J  |e1I )
 
 
 Pr(   e1I )
 
 
 Pr(f1  J  |e1I )   Pr(   e1I )
  
e1I
f1 J
maximize  Alignment Model
Transformation
Figure 1: Architecture of the translation approach
based on Bayes decision rule.
3.2 Basic Models
To arrive at a quantitative specification, we define the
alignment mapping: j ? i = aj , which assigns a word fj
in position j to a word ei in position i = aj . We rewrite
the probability for the translation model by introducing the
?hidden? alignments aJ1 := a1...aj ...aJ for each sentence pair
(fJ1 ; eI1). To structure this probability distribution, we fac-
torize it over the positions in the source sentence and limit
the alignment dependencies to a first-order dependence:
Pr(fJ1 |eI1) = p(J |I) ?
X
aJ1
J
Y
j=1
[p(aj |aj?1, I, J) ? p(fj |eaj )] .
Here, we have the following probability distributions:
? the sentence length probability: p(J |I), which is in-
cluded here for completeness, but can be omitted with-
out loss of performance;
? the lexicon probability: p(f |e);
? the alignment probability: p(aj |aj?1, I, J).
By making the alignment probability p(aj |aj?1, I, J) depen-
dent on the jump width aj ? aj?1 instead of the absolute
positions aj , we obtain the so-called homogeneous hidden
Markov model, for short HMM [24].
We can also use a zero-order model p(aj |j, I, J), where
there is only a dependence on the absolute position index j
of the source string. This is the so-called model IBM-2 [5].
Assuming a uniform alignment probability p(aj |j, I, J) =
1/I, we arrive at the so-called model IBM-1.
These models can be extended to allow for source words
having no counterpart in the translation. Formally, this
is incorporated into the alignment models by adding a so-
called ?empty word? at position i = 0 to the target sentence
and aligning all source words without a direct translation to
this empty word.
well
I
think
if
we
can
make
it
at
eight
on
both
days
ja ich den
ke
we
nn wir das
hin
kri
ege
n an
bei
den
Ta
ge
n
ac
ht Uh
r
Figure 2: Word-to-word alignment.
In [5], more refined alignment models are introduced by
using the concept of fertility. The idea is that often a word
in the target language may be aligned to several words in
the source language. This is the so-called model IBM-3. Us-
ing, in addition, first-order alignment probabilities along the
positions of the source string leads us to model IBM-4. Al-
though these models take one-to-many alignments explicitly
into account, the lexicon probabilities p(f |e) are still based
on single words in each of the two languages.
In systematic experiments, it was found that the qual-
ity of the alignments determined from the bilingual training
corpus has a direct effect on the translation quality [14].
3.3 Alignment Template Approach
A general shortcoming of the baseline alignment models
is that they are mainly designed to model the lexicon de-
pendences between single words. Therefore, we extend the
approach to handle word groups or phrases rather than sin-
gle words as the basis for the alignment models [15]. In
other words, a whole group of adjacent words in the source
sentence may be aligned with a whole group of adjacent
words in the target language. As a result, the context of
words tends to be explicitly taken into account, and the
differences in local word orders between source and target
languages can be learned explicitly. Figure 3 shows some of
the extracted alignment templates for a sentence pair from
the Verbmobil training corpus. The training algorithm for
the alignment templates extracts all phrase pairs which are
aligned in the training corpus up to a maximum length of 7
words. To improve the generalization capability of the align-
ment templates, the templates are determined for bilingual
word classes rather than words directly. These word classes
are determined by an automatic clustering procedure [13].
4. SEARCH
The task of the search algorithm is to generate the most
likely target sentence eI1 of unknown length I for an observed
source sentence fJ1 . The search must make use of all three
knowledge sources as illustrated by Figure 4: the alignment
model, the lexicon model and the language model. All three
okay
,
how
about
the
nineteenth
at
maybe
,
two
o?clock
in
the
afternoon
?
oka
y ,
wie
sie
ht es am
ne
un
ze
hnt
en aus ,
vie
lle
ich
t um
zw
ei Uhr
na
chm
itt
ags ?
Figure 3: Example of a word alignment and of ex-
tracted alignment templates.
of them must contribute in the final decision about the words
in the target language.
To illustrate the specific details of the search problem, we
slightly change the definitions of the alignments:
? we use inverted alignments as in the model IBM-4 [5]
which define a mapping from target to source positions
rather the other way round.
? we allow several positions in the source language to be
covered, i.e. we consider mappings B of the form:
B : i ? Bi ? {1, ...j, ...J}
We replace the sum over all alignments by the best
alignment, which is referred to as maximum approxima-
tion in speech recognition. Using a trigram language model
p(ei|, ei?2, ei?1), we obtain the following search criterion:
max
BI1 ,eI1
I
Y
i=1
2
4[p(ei|ei?1i?2) ? p(Bi|Bi?1, I, J) ?
Y
j?Bi
p(fj |ei)]
3
5
Considering this criterion, we can see that we can build
up hypotheses of partial target sentences in a bottom-to-
top strategy over the positions i of the target sentence ei1
as illustrated in Figure 5. An important constraint for the
alignment is that all positions of the source sentence should
be covered exactly once. This constraint is similar to that
of the travelling salesman problem where each city has to
be visited exactly once. Details on various search strategies
can be found in [4, 9, 12, 21].
In order to take long context dependences into account,
we use a class-based five-gram language model with backing-
off. Beam-search is used to handle the huge search space. To
normalize the costs of partial hypotheses covering different
parts of the input sentence, an (optimistic) estimation of the
remaining cost is added to the current accumulated cost as
follows. For each word in the source sentence, a lower bound
on its translation cost is determined beforehand. Using this
SENTENCE INSOURCE LANGUAGE
TRANSFORMATION
SENTENCE GENERATEDIN TARGET LANGUAGE
SENTENCE 
 
KNOWLEDGE SOURCESSEARCH: INTERACTION OF 
                            KNOWLEDGE SOURCES
WORD + POSITION
ALIGNMENT 
     
LANGUAGE MODEL
BILINGUAL LEXICON 
ALIGNMENTMODELWORD RE-ORDERING
SYNTACTIC ANDSEMANTIC ANALYSIS
LEXICAL CHOICE
HYPOTHESES
HYPOTHESES
HYPOTHESES
TRANSFORMATION
Figure 4: Illustration of search in statistical trans-
lation.
lower bound, it is possible to achieve an efficient estimation
of the remaining cost.
5. EXPERIMENTAL RESULTS
5.1 The Task and the Corpus
Within the Verbmobil project, spoken dialogues were
recorded. These dialogues were manually transcribed and
later manually translated by Verbmobil partners (Hildes-
heim for Phase I and Tu?bingen for Phase II). Since different
human translators were involved, there is great variability
in the translations.
Each of these so-called dialogues turns may consist of sev-
eral sentences spoken by the same speaker and is sometimes
SOURCE POSITION
TA
RG
ET
 P
O
SI
TI
O
N
i
i-1
j
Figure 5: Illustration of bottom-to-top search.
rather long. As a result, there is no one-to-one correspon-
dence between source and target sentences. To achieve a
one-to-one correspondence, the dialogue turns are split into
shorter segments using punctuation marks as potential split
points. Since the punctuation marks in source and target
sentences are not necessarily identical, a dynamic program-
ming approach is used to find the optimal segmentation
points. The number of segments in the source sentence and
in the test sentence can be different. The segmentation is
scored using a word-based alignment model, and the seg-
mentation with the best score is selected. This segmented
corpus is the starting point for the training of translation
and language models. Alignment models of increasing com-
plexity are trained on this bilingual corpus [14].
A standard vocabulary had been defined for the various
speech recognizers used in Verbmobil. However, not all
words of this vocabulary were observed in the training cor-
pus. Therefore, the translation vocabulary was extended
semi-automatically by adding about 13 000 German?English
word pairs from an online bilingual lexicon available on the
web. The resulting lexicon contained not only word-word
entries, but also multi-word translations, especially for the
large number of German compound words. To counteract
the sparseness of the training data, a couple of straightfor-
ward rule-based preprocessing steps were applied before any
other type of processing:
? categorization of proper names for persons and cities,
? normalization of:
? numbers,
? time and date phrases,
? spelling: don?t ? do not,...
? splitting of
German compound words.
Table 1 gives the characteristics of the training corpus
and the lexicon. The 58 000 sentence pairs comprise about
half a million running words for each language of the bilin-
gual training corpus. The vocabulary size is the number of
distinct full-form words seen in the training corpus. Punctu-
ation marks are treated as regular words in the translation
approach. Notice the large number of word singletons, i. e.
words seen only once. The extended vocabulary is the vo-
cabulary after adding the manual bilingual lexicon.
5.2 Offline Results
During the progress of the Verbmobil project, different
variants of statistical translation were implemented, and ex-
Table 1: Bilingual training corpus, recognition lex-
icon and translation lexicon (PM = punctuation
mark).
German English
Training Text Sentences 58 332
Words (+PMs) 519 523 549 921
Vocabulary 7 940 4 673
Singletons 44.8% 37.6%
Recognition Vocabulary 10 157 6 871
Translation Manual Pairs 12 779
Ext. Vocab. 11 501 6 867
perimental tests were performed for both text and speech
input. To summarize these experimental tests, we briefly
report experimental offline results for the following transla-
tion approaches:
? single-word based approach [20];
? alignment template approach [15];
? cascaded transducer approach [23]:
unlike the other two-approaches, this approach re-
quires a semi-automatic training procedure, in which
the structure of the finite state transducers is designed
manually. For more details, see [23].
The offline tests were performed on text input for the trans-
lation direction from German to English. The test set con-
sisted of 251 sentences, which comprised 2197 words and 430
punctuation marks. The results are shown in Table 2. To
judge and compare the quality of different translation ap-
proaches in offline tests, we typically use the following error
measures [11]:
? mWER (multi-reference word error rate):
For each test sentence sk in the source language, there
are several reference translationsRk = {rk1, . . . , rknk}
in the target language. For each translation of the test
sentence sk, the edit distances (number of substitu-
tions, deletions and insertions as in speech recognition)
to all sentences in Rk are calculated, and the smallest
distance is selected and used as error measure.
? SSER (subjective sentence error rate):
Each translated sentence is judged by a human exam-
iner according to an error scale from 0.0 (semantically
and syntactically correct) to 1.0 (completely wrong).
Both error measures are reported in Table 2. Although
the experiments with the cascaded transducers [23] were not
fully optimized yet, the preliminary results indicated that
this semi-automatic approach does not generalize as well
as the other two fully automatic approaches. Among these
two, the alignment template approach was found to work
consistently better across different test sets (and also tasks
different from Verbmobil). Therefore, the alignment tem-
plate approach was used in the final Verbmobil prototype
system.
5.3 Disambiguation Examples
In the statistical translation approach as we have pre-
sented it, no explicit word sense disambiguation is per-
formed. However, a kind of implicit disambiguation is pos-
sible due to the context information of the alignment tem-
plates and the language model as shown by the examples
in Table 3. The first two groups of sentences contain the
Table 2: Comparison of three statistical translation
approaches (test on text input: 251 sentences =
2197 words + 430 punctuation marks).
Translation mWER SSER
Approach [%] [%]
Single-Word Based 38.2 35.7
Alignment Template 36.0 29.0
Cascaded Transducers >40.0 >40.0
verbs ?gehen? and ?annehmen? which have different transla-
tions, some of which are rather collocational. The correct
translation is only possible by taking the whole sentence
into account. Some improvement can be achieved by ap-
plying morpho-syntactic analysis, e.g handling of the sepa-
rated verb prefixes in German [10]. The last two sentences
show the implicit disambiguation of the temporal and spa-
tial sense for the German preposition ?vor?. Although the
system has not been tailored to handle such types of disam-
biguation, the translated sentences are all acceptable, apart
from the sentence: The meeting is to five.
5.4 Integration into the Verbmobil Prototype
System
The statistical approach to machine translation is em-
bodied in the stattrans module which is integrated into the
Verbmobil prototype system. We briefly review those as-
pects of it that are relevant for the statistical translation ap-
proach. The implementation supports the translation direc-
tions from German to English and from English to German.
In regular processing mode, the stattrans module receives
its input from the repair module [18]. At that time, the
word lattices and best hypotheses from the speech recogni-
tion systems have already been prosodically annotated, i.e.
information about prosodic segment boundaries, sentence
mode and accentuated syllables are added to each edge in
the word lattice [2]. The translation is performed on the
single best sentence hypothesis of the recognizer.
The prosodic boundaries and the sentence mode informa-
tion are utilized by the stattrans module as follows. If there
is a major phrase boundary, a full stop or question mark is
inserted into the word sequence, depending on the sentence
mode as indicated by the prosody module. Additional com-
mas are inserted for other types of segment boundaries. The
prosody module calculates probabilities for segment bound-
aries, and thresholds are used to decide if the sentence marks
are to be inserted. These thresholds have been selected in
such a way that, on the average, for each dialogue turn, a
good segmentation is obtained. The segment boundaries re-
strict possible word reordering between source and target
language. This not only improves translation quality, but
also restricts the search space and thereby speeds up the
translation process.
5.5 Large-Scale End-to-End Evaluation
Whereas the offline tests reported above were important
for the optimization and tuning of the system, the most
important evaluation was the final evaluation of the Verb-
mobil prototype in spring 2000. This end-to-end evaluation
of the Verbmobil system was performed at the University
of Hamburg [19]. In each session of this evaluation, two
native speakers conducted a dialogue. They did not have
any direct contact and could only interact by speaking and
listening to the Verbmobil system.
Three other translation approaches had been integrated
into the Verbmobil prototype system:
? a classical transfer approach [3, 7, 22],
which is based on a manually designed analysis gram-
mar, a set of transfer rules, and a generation grammar,
? a dialogue act based approach [16],
which amounts to a sort of slot filling by classifying
Table 3: Disambiguation examples (?: using morpho-syntactic analysis).
Ambiguous Word Text Input Translation
gehen Wir gehen ins Theater. We will go to the theater.
Mir geht es gut. I am fine.
Es geht um Geld. It is about money.
Geht es bei Ihnen am Montag? Is it possible for you on Monday?
Das Treffen geht bis 5 Uhr. The meeting is to five.
annehmen Wir sollten das Angebot annehmen. We should accept that offer.
Ich nehme das Schlimmste an. I will assume the worst.?
vor Wir treffen uns vor dem Fru?hstu?ck. We meet before the breakfast.
Wir treffen uns vor dem Hotel. We will meet in front of the hotel.
each sentence into one out of a small number of possi-
ble sentence patterns and filling in the slot values,
? an example-based approach [1],
where a sort of nearest neighbour concept is applied to
the set of bilingual training sentence pairs after suit-
able preprocessing.
In the final end-to-end evaluation, human evaluators
judged the translation quality for each of the four trans-
lation results using the following criterion:
Is the sentence approximatively correct: yes/no?
The evaluators were asked to pay particular attention to
the semantic information (e.g. date and place of meeting,
participants etc) contained in the translation. A missing
translation as it may happen for the transfer approach or
other approaches was counted as wrong translation. The
evaluation was based on 5069 dialogue turns for the trans-
lation from German to English and on 4136 dialogue turns
for the translation from English to German. The speech
recognizers used had a word error rate of about 25%. The
overall sentence error rates, i.e. resulting from recognition
and translation, are summarized in Table 4. As we can see,
the error rates for the statistical approach are smaller by a
factor of about 2 in comparison with the other approaches.
In agreement with other evaluation experiments, these ex-
periments show that the statistical modelling approach may
be comparable to or better than the conventional rule-based
approach. In particular, the statistical approach seems to
have the advantage if robustness is important, e.g. when
the input string is not grammatically correct or when it is
corrupted by recognition errors.
Although both text and speech input are translated with
good quality on the average by the statistical approach,
Table 4: Sentence error rates of end-to-end evalua-
tion (speech recognizer with WER=25%; corpus of
5069 and 4136 dialogue turns for translation Ger-
man to English and English to German, respec-
tively).
Translation Method Error [%]
Semantic Transfer 62
Dialogue Act Based 60
Example Based 52
Statistical 29
there are examples where the syntactic structure of the pro-
duced sentence is not correct. Some of these syntactic errors
are related to long range dependencies and syntactic struc-
tures that are not captured by the m-gram language model
used. To cope with these problems, morpho-syntactic anal-
ysis [10] and grammar-based language models [17] are cur-
rently being studied.
6. SUMMARY
In this paper, we have given an overview of the statistical
approach to machine translation and especially its imple-
mentation in the Verbmobil prototype system. The sta-
tistical system has been trained on about 500 000 running
words from a bilingual German?English corpus. Transla-
tions are performed for both directions, i.e. from German
to English and from English to German. Comparative eval-
uations with other translation approaches of the Verbmo-
bil prototype system show that the statistical translation
is superior, especially in the presence of speech input and
ungrammatical input.
Acknowledgment
The work reported here was supported partly by the Verb-
mobil project (contract number 01 IV 701 T4) by the Ger-
man Federal Ministry of Education, Science, Research and
Technology and as part of the EuTrans project (ESPRIT
project number 30268) by the European Community.
Training Toolkit
In a follow-up project of the statistical machine translation
project during the 1999 Johns Hopkins University workshop,
we have developped a publically available toolkit for the
training of different alignment models, including the models
IBM-1 to IBM-5 [5] and an HMM alignment model [14, 24].
The software can be downloaded at
http://www-i6.Informatik.RWTH-Aachen.DE/
~och/software/GIZA++.html.
7. REFERENCES
[1] M. Auerswald: Example-based machine translation
with templates. In [25], pp. 418?427.
[2] A. Batliner, J. Buckow, H. Niemann, E. No?th,
V. Warnke: The prosody module. In [25], pp. 106?
121.
[3] T. Becker, A. Kilger, P. Lopez, P. Poller: The
Verbmobil generation component VM-GECO. In [25],
pp. 481?496.
[4] A. L. Berger, P. F. Brown, J. Cocke, S. A. Della Pietra,
V. J. Della Pietra, J. R. Gillett, J. D. Lafferty,
R. L. Mercer, H. Printz,L. Ures: The Candide Sys-
tem for Machine Translation. ARPA Human Lan-
guage Technology Workshop, Plainsboro, NJ, Morgan
Kaufmann Publishers, pp. 152-157, San Mateo, CA,
March 1994.
[5] P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
R. L. Mercer: The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, Vol. 19, No. 2, pp. 263?311, 1993.
[6] N. Chomsky: ?Quine?s Empirical Assumptions?, in
D. Davidson, J. Hintikka (eds.): Words and objections.
Essays on the work of W. V. Quine, Reidel, Dordrecht,
The Netherlands, 1969.
[7] M. C. Emele, M. Dorna, A. Lu?deling, H. Zinsmeister,
C. Rohrer: Semantic-based transfer. In [25], pp. 359?
376.
[8] EuTrans Project; Instituto Tecnolo?gico de Informa?tica
(ITI, Spain), Fondazione Ugo Bordoni (FUB, Italy),
RWTH Aachen, Lehrstuhl f. Informatik VI (Ger-
many), Zeres GmbH Bochum (Germany): Example-
Based Language Translation Systems. Final report of
the EuTrans project (EU project number 30268), July
2000.
[9] H. Ney, S. Nie?en, F. J. Och, H. Sawaf, C. Tillmann,
S. Vogel: Algorithms for statistical translation of spo-
ken language. IEEE Trans. on Speech and Audio Pro-
cessing Vol. 8, No. 1, pp. 24?36, Jan. 2000.
[10] S. Nie?en, H. Ney: Improving SMT quality with
morpho-syntactic analysis. 18th Int. Conf. on Compu-
tational Linguistics, pp. 1081-1085, Saarbru?cken, Ger-
many, July 2000.
[11] S. Nie?en, F.-J. Och, G. Leusch, H. Ney: An evalua-
tion tool for machine translation: Fast evaluation for
MT research. 2nd Int. Conf. on Language Resources
and Evaluation, pp.39?45, Athens, Greece, May 2000.
[12] S. Nie?en, S. Vogel, H. Ney, C. Tillmann: A DP
based search algorithm for statistical machine transla-
tion. COLING?ACL ?98: 36th Annual Meeting of the
Association for Computational Linguistics and 17th
Int. Conf. on Computational Linguistics, pp. 960?967,
Montreal, Canada, Aug. 1998.
[13] F. J. Och: An efficient method to determine bilingual
word classes. 9th Conf. of the European Chapter of the
Association for Computational Linguistics, pp. 71?76,
Bergen, Norway, June 1999.
[14] F. J. Och, H. Ney: A comparison of alignment
models for statistical machine translation. 18th Int.
Conf. on Computational Linguistics, pp. 1086-1090,
Saarbru?cken, Germany, July 2000.
[15] F. J. Och, C. Tillmann, H. Ney: Improved alignment
models for statistical machine translation. Joint SIG-
DAT Conf. on Empirical Methods in Natural Language
Processing and Very Large Corpora, 20?28, University
of Maryland, College Park, MD, June 1999.
[16] N. Reithinger, R. Engel: Robust content extraction for
translation and dialog processing. In [25], pp. 428?437.
[17] H. Sawaf, K. Schu?tz, H. Ney: On the use of grammar
based language models for statistical machine trans-
lation. 6th Int. Workshop on Parsing Technologies,
pp. 231?241, Trento, Italy, Feb. 2000.
[18] J. Spilker, M. Klarner, G. Go?rz: Processing self-
corrections in a speech-to-speech system. In [25],
pp. 131?140.
[19] L. Tessiore, W. v. Hahn: Functional validation of
a machine translation system: Verbmobil. In [25],
pp. 611?631.
[20] C. Tillmann, H. Ney: Word re-ordering in a DP-based
approach to statistical MT. 18th Int. Conf. on Com-
putational Linguistics 2000, Saarbru?cken, Germany,
pp. 850-856, Aug. 2000.
[21] C. Tillmann, S. Vogel, H. Ney, A. Zubiaga: A DP-
based search using monotone alignments in statisti-
cal translation. 35th Annual Conf. of the Association
for Computational Linguistics, pp. 289?296, Madrid,
Spain, July 1997.
[22] H. Uszkoreit, D. Flickinger, W. Kasper, I. A. Sag:
Deep linguistic analysis with HPSG. In [25], pp. 216?
263.
[23] S. Vogel, H. Ney: Translation with Cascaded Finite-
State Transducers. ACL Conf. (Assoc. for Comput.
Linguistics), Hongkong, pp. 23-30, Oct. 2000.
[24] S. Vogel, H. Ney, C. Tillmann: HMM-based word
alignment in statistical translation. 16th Int. Conf. on
Computational Linguistics, pp. 836?841, Copenhagen,
Denmark, August 1996.
[25] W. Wahlster (Ed.): Verbmobil: Foundations of speech-
to-speech translations. Springer-Verlag, Berlin, Ger-
many, 2000.
 
	Refined Lexicon Models for Statistical Machine Translation using a
Maximum Entropy Approach
Ismael Garc??a Varea
Dpto. de Informa?tica
Univ. de Castilla-La Mancha
Campus Universitario s/n
02071 Albacete, Spain
ivarea@info-ab.uclm.es
Franz J. Och and
Hermann Ney
Lehrstuhl fu?r Inf. VI
RWTH Aachen
Ahornstr., 55
D-52056 Aachen, Germany
 
och|ney  @cs.rwth-aachen.de
Francisco Casacuberta
Dpto. de Sist. Inf. y Comp.
Inst. Tecn. de Inf. (UPV)
Avda. de Los Naranjos, s/n
46071 Valencia, Spain
fcn@iti.upv.es
Abstract
Typically, the lexicon models used in
statistical machine translation systems
do not include any kind of linguistic
or contextual information, which often
leads to problems in performing a cor-
rect word sense disambiguation. One
way to deal with this problem within
the statistical framework is to use max-
imum entropy methods. In this paper,
we present how to use this type of in-
formation within a statistical machine
translation system. We show that it is
possible to significantly decrease train-
ing and test corpus perplexity of the
translation models. In addition, we per-
form a rescoring of  -Best lists us-
ing our maximum entropy model and
thereby yield an improvement in trans-
lation quality. Experimental results are
presented on the so-called ?Verbmobil
Task?.
1 Introduction
Typically, the lexicon models used in statistical
machine translation systems are only single-word
based, that is one word in the source language cor-
responds to only one word in the target language.
Those lexicon models lack from context infor-
mation that can be extracted from the same paral-
lel corpus. This additional information could be:
 Simple context information: information of
the words surrounding the word pair;
 Syntactic information: part-of-speech in-
formation, syntactic constituent, sentence
mood;
 Semantic information: disambiguation in-
formation (e.g. from WordNet), cur-
rent/previous speech or dialog act.
To include this additional information within the
statistical framework we use the maximum en-
tropy approach. This approach has been applied
in natural language processing to a variety of
tasks. (Berger et al, 1996) applies this approach
to the so-called IBM Candide system to build con-
text dependent models, compute automatic sen-
tence splitting and to improve word reordering in
translation. Similar techniques are used in (Pap-
ineni et al, 1996; Papineni et al, 1998) for so-
called direct translation models instead of those
proposed in (Brown et al, 1993). (Foster, 2000)
describes two methods for incorporating informa-
tion about the relative position of bilingual word
pairs into a maximum entropy translation model.
Other authors have applied this approach to lan-
guage modeling (Rosenfeld, 1996; Martin et al,
1999; Peters and Klakow, 1999). A short review
of the maximum entropy approach is outlined in
Section 3.
2 Statistical Machine Translation
The goal of the translation process in statisti-
cal machine translation can be formulated as fol-
lows: A source language string 	  


 

is to be translated into a target language string










. In the experiments reported in
this paper, the source language is German and the
target language is English. Every target string is
considered as a possible translation for the input.
If we assign a probability      to each pair
of strings       , then according to Bayes? de-
cision rule, we have to choose the target string
that maximizes the product of the target language
model     and the string translation model
Discriminative Training and Maximum Entropy Models for Statistical
Machine Translation
Franz Josef Och and Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen - University of Technology
D-52056 Aachen, Germany
{och,ney}@informatik.rwth-aachen.de
Abstract
We present a framework for statistical
machine translation of natural languages
based on direct maximum entropy mod-
els, which contains the widely used sour-
ce-channel approach as a special case. All
knowledge sources are treated as feature
functions, which depend on the source
language sentence, the target language
sentence and possible hidden variables.
This approach allows a baseline machine
translation system to be extended easily by
adding new feature functions. We show
that a baseline statistical machine transla-
tion system is significantly improved us-
ing this approach.
1 Introduction
We are given a source (?French?) sentence fJ1 =
f1, . . . , fj , . . . , fJ , which is to be translated into a
target (?English?) sentence eI1 = e1, . . . , ei, . . . , eI .
Among all possible target sentences, we will choose
the sentence with the highest probability:1
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )} (1)
The argmax operation denotes the search problem,
i.e. the generation of the output sentence in the target
language.
1The notational convention will be as follows. We use the
symbol Pr(?) to denote general probability distributions with
(nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol p(?).
1.1 Source-Channel Model
According to Bayes? decision rule, we can equiva-
lently to Eq. 1 perform the following maximization:
e?I1 = argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)} (2)
This approach is referred to as source-channel ap-
proach to statistical MT. Sometimes, it is also re-
ferred to as the ?fundamental equation of statisti-
cal MT? (Brown et al, 1993). Here, Pr(eI1) is
the language model of the target language, whereas
Pr(fJ1 |eI1) is the translation model. Typically, Eq. 2
is favored over the direct translation model of Eq. 1
with the argument that it yields a modular approach.
Instead of modeling one probability distribution,
we obtain two different knowledge sources that are
trained independently.
The overall architecture of the source-channel ap-
proach is summarized in Figure 1. In general, as
shown in this figure, there may be additional trans-
formations to make the translation task simpler for
the algorithm. Typically, training is performed by
applying a maximum likelihood approach. If the
language model Pr(eI1) = p?(eI1) depends on pa-
rameters ? and the translation model Pr(fJ1 |eI1) =
p?(fJ1 |eI1) depends on parameters ?, then the opti-
mal parameter values are obtained by maximizing
the likelihood on a parallel training corpus fS1 , eS1
(Brown et al, 1993):
?? = argmax
?
S?
s=1
p?(fs|es) (3)
?? = argmax
?
S?
s=1
p?(es) (4)
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 295-302.
                         Proceedings of the 40th Annual Meeting of the Association for
Source
Language Text
??
Preprocessing
Pr(eI1): Language Modeloo
Global Search
e?I1 = argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)}
??
??
Pr(fJ1 |eI1): Translation Modeloo
Postprocessing
??
Target
Language Text
Figure 1: Architecture of the translation approach based on source-channel models.
We obtain the following decision rule:
e?I1 = argmax
eI1
{p??(eI1) ? p??(fJ1 |eI1)} (5)
State-of-the-art statistical MT systems are based on
this approach. Yet, the use of this decision rule has
various problems:
1. The combination of the language model p??(eI1)
and the translation model p??(fJ1 |eI1) as shown
in Eq. 5 can only be shown to be optimal if the
true probability distributions p??(eI1) = Pr(eI1)
and p??(fJ1 |eI1) = Pr(fJ1 |eI1) are used. Yet,
we know that the used models and training
methods provide only poor approximations of
the true probability distributions. Therefore, a
different combination of language model and
translation model might yield better results.
2. There is no straightforward way to extend a
baseline statistical MT model by including ad-
ditional dependencies.
3. Often, we observe that comparable results are
obtained by using the following decision rule
instead of Eq. 5 (Och et al, 1999):
e?I1 = argmax
eI1
{p??(eI1) ? p??(eI1|fJ1 )} (6)
Here, we replaced p??(fJ1 |eI1) by p??(eI1|fJ1 ).
From a theoretical framework of the source-
channel approach, this approach is hard to jus-
tify. Yet, if both decision rules yield the same
translation quality, we can use that decision
rule which is better suited for efficient search.
1.2 Direct Maximum Entropy Translation
Model
As alternative to the source-channel approach, we
directly model the posterior probability Pr(eI1|fJ1 ).
An especially well-founded framework for doing
this is maximum entropy (Berger et al, 1996). In
this framework, we have a set of M feature func-
tions hm(eI1, fJ1 ),m = 1, . . . ,M . For each feature
function, there exists a model parameter ?m,m =
1, . . . ,M . The direct translation probability is given
Source
Language Text
??
Preprocessing
?? ?1 ? h1(eI1, fJ1 )oo
Global Search
argmax
eI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
??
?2 ? h2(eI1, fJ1 )oo
. . .oo
Postprocessing
??
Target
Language Text
Figure 2: Architecture of the translation approach based on direct maximum entropy models.
by:
Pr(eI1|fJ1 ) = p?M1 (e
I
1|fJ1 ) (7)
= exp[
?M
m=1 ?mhm(eI1, fJ1 )]?
e?I1 exp[
?M
m=1 ?mhm(e?I1, fJ1 )]
(8)
This approach has been suggested by (Papineni et
al., 1997; Papineni et al, 1998) for a natural lan-
guage understanding task.
We obtain the following decision rule:
e?I1 = argmax
eI1
{
Pr(eI1|fJ1 )
}
= argmax
eI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
Hence, the time-consuming renormalization in Eq. 8
is not needed in search. The overall architecture of
the direct maximum entropy models is summarized
in Figure 2.
Interestingly, this framework contains as special
case the source channel approach (Eq. 5) if we use
the following two feature functions:
h1(eI1, fJ1 ) = log p??(eI1) (9)
h2(eI1, fJ1 ) = log p??(fJ1 |eI1) (10)
and set ?1 = ?2 = 1. Optimizing the corresponding
parameters ?1 and ?2 of the model in Eq. 8 is equiv-
alent to the optimization of model scaling factors,
which is a standard approach in other areas such as
speech recognition or pattern recognition.
The use of an ?inverted? translation model in the
unconventional decision rule of Eq. 6 results if we
use the feature function logPr(eI1|fJ1 ) instead of
logPr(fJ1 |eI1). In this framework, this feature can
be as good as logPr(fJ1 |eI1). It has to be empirically
verified, which of the two features yields better re-
sults. We even can use both features logPr(eI1|fJ1 )
and logPr(fJ1 |eI1), obtaining a more symmetric
translation model.
As training criterion, we use the maximum class
posterior probability criterion:
??M1 = argmax
?M1
{ S?
s=1
log p?M1 (es|fs)
}
(11)
This corresponds to maximizing the equivocation
or maximizing the likelihood of the direct transla-
tion model. This direct optimization of the poste-
rior probability in Bayes decision rule is referred to
as discriminative training (Ney, 1995) because we
directly take into account the overlap in the proba-
bility distributions. The optimization problem has
one global optimum and the optimization criterion
is convex.
1.3 Alignment Models and Maximum
Approximation
Typically, the probability Pr(fJ1 |eI1) is decomposed
via additional hidden variables. In statistical align-
ment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is in-
troduced as a hidden variable:
Pr(fJ1 |eI1) =
?
aJ1
Pr(fJ1 , aJ1 |eI1)
The alignment mapping is j ? i = aj from source
position j to target position i = aj .
Search is performed using the so-called maximum
approximation:
e?I1 = argmax
eI1
?
?
?Pr(e
I
1) ?
?
aJ1
Pr(fJ1 , aJ1 |eI1)
?
?
?
? argmax
eI1
{
Pr(eI1) ?maxaJ1
Pr(fJ1 , aJ1 |eI1)
}
Hence, the search space consists of the set of all pos-
sible target language sentences eI1 and all possible
alignments aJ1 .
Generalizing this approach to direct translation
models, we extend the feature functions to in-
clude the dependence on the additional hidden vari-
able. Using M feature functions of the form
hm(eI1, fJ1 , aJ1 ),m = 1, . . . ,M , we obtain the fol-
lowing model:
Pr(eI1, aJ1 |fJ1 ) =
=
exp
(?M
m=1 ?mhm(eI1, fJ1 , aJ1 )
)
?
e?I1,a?J1 exp
(?M
m=1 ?mhm(e?I1, fJ1 , a?J1 )
)
Obviously, we can perform the same step for transla-
tion models with an even richer structure of hidden
variables than only the alignment aJ1 . To simplify
the notation, we shall omit in the following the de-
pendence on the hidden variables of the model.
2 Alignment Templates
As specific MT method, we use the alignment tem-
plate approach (Och et al, 1999). The key elements
of this approach are the alignment templates, which
are pairs of source and target language phrases to-
gether with an alignment between the words within
the phrases. The advantage of the alignment tem-
plate approach compared to single word-based sta-
tistical translation models is that word context and
local changes in word order are explicitly consid-
ered.
The alignment template model refines the transla-
tion probability Pr(fJ1 |eI1) by introducing two hid-
den variables zK1 and aK1 for the K alignment tem-
plates and the alignment of the alignment templates:
Pr(fJ1 |eI1) =
?
zK1 ,aK1
Pr(aK1 |eI1) ?
Pr(zK1 |aK1 , eI1) ? Pr(fJ1 |zK1 , aK1 , eI1)
Hence, we obtain three different probability
distributions: Pr(aK1 |eI1), Pr(zK1 |aK1 , eI1) and
Pr(fJ1 |zK1 , aK1 , eI1). Here, we omit a detailed de-
scription of modeling, training and search, as this is
not relevant for the subsequent exposition. For fur-
ther details, see (Och et al, 1999).
To use these three component models in a direct
maximum entropy approach, we define three dif-
ferent feature functions for each component of the
translation model instead of one feature function for
the whole translation model p(fJ1 |eI1). The feature
functions have then not only a dependence on fJ1
and eI1 but also on zK1 , aK1 .
3 Feature functions
So far, we use the logarithm of the components of
a translation model as feature functions. This is a
very convenient approach to improve the quality of
a baseline system. Yet, we are not limited to train
only model scaling factors, but we have many possi-
bilities:
? We could add a sentence length feature:
h(fJ1 , eI1) = I
This corresponds to a word penalty for each
produced target word.
? We could use additional language models by
using features of the following form:
h(fJ1 , eI1) = h(eI1)
? We could use a feature that counts how many
entries of a conventional lexicon co-occur in
the given sentence pair. Therefore, the weight
for the provided conventional dictionary can be
learned. The intuition is that the conventional
dictionary is expected to be more reliable than
the automatically trained lexicon and therefore
should get a larger weight.
? We could use lexical features, which fire if a
certain lexical relationship (f, e) occurs:
h(fJ1 , eI1) =
?
?
J?
j=1
?(f, fj)
?
? ?
( I?
i=1
?(e, ei)
)
? We could use grammatical features that relate
certain grammatical dependencies of source
and target language. For example, using a func-
tion k(?) that counts how many verb groups ex-
ist in the source or the target sentence, we can
define the following feature, which is 1 if each
of the two sentences contains the same number
of verb groups:
h(fJ1 , eI1) = ?(k(fJ1 ), k(eI1)) (12)
In the same way, we can introduce semantic
features or pragmatic features such as the di-
alogue act classification.
We can use numerous additional features that deal
with specific problems of the baseline statistical MT
system. In this paper, we shall use the first three of
these features. As additional language model, we
use a class-based five-gram language model. This
feature and the word penalty feature allow a straight-
forward integration into the used dynamic program-
ming search algorithm (Och et al, 1999). As this is
not possible for the conventional dictionary feature,
we use n-best rescoring for this feature.
4 Training
To train the model parameters ?M1 of the direct trans-
lation model according to Eq. 11, we use the GIS
(Generalized Iterative Scaling) algorithm (Darroch
and Ratcliff, 1972). It should be noted that, as
was already shown by (Darroch and Ratcliff, 1972),
by applying suitable transformations, the GIS algo-
rithm is able to handle any type of real-valued fea-
tures. To apply this algorithm, we have to solve var-
ious practical problems.
The renormalization needed in Eq. 8 requires a
sum over a large number of possible sentences,
for which we do not know an efficient algorithm.
Hence, we approximate this sum by sampling the
space of all possible sentences by a large set of
highly probable sentences. The set of considered
sentences is computed by an appropriately extended
version of the used search algorithm (Och et al,
1999) computing an approximate n-best list of trans-
lations.
Unlike automatic speech recognition, we do not
have one reference sentence, but there exists a num-
ber of reference sentences. Yet, the criterion as it
is described in Eq. 11 allows for only one reference
translation. Hence, we change the criterion to al-
low Rs reference translations es,1, . . . , es,Rs for the
sentence es:
??M1 = argmax
?M1
{ S?
s=1
1
Rs
Rs?
r=1
log p?M1 (es,r|fs)
}
We use this optimization criterion instead of the op-
timization criterion shown in Eq. 11.
In addition, we might have the problem that no
single of the reference translations is part of the n-
best list because the search algorithm performs prun-
ing, which in principle limits the possible transla-
tions that can be produced given a certain input sen-
tence. To solve this problem, we define for max-
imum entropy training each sentence as reference
translation that has the minimal number of word er-
rors with respect to any of the reference translations.
5 Results
We present results on the VERBMOBIL task, which
is a speech translation task in the domain of appoint-
ment scheduling, travel planning, and hotel reser-
vation (Wahlster, 1993). Table 1 shows the cor-
pus statistics of this task. We use a training cor-
pus, which is used to train the alignment template
model and the language models, a development cor-
pus, which is used to estimate the model scaling fac-
tors, and a test corpus.
Table 1: Characteristics of training corpus (Train),
manual lexicon (Lex), development corpus (Dev),
test corpus (Test).
German English
Train Sentences 58 073
Words 519 523 549 921
Singletons 3 453 1 698
Vocabulary 7 939 4 672
Lex Entries 12 779
Ext. Vocab. 11 501 6 867
Dev Sentences 276
Words 3 159 3 438
PP (trigr. LM) - 28.1
Test Sentences 251
Words 2 628 2 871
PP (trigr. LM) - 30.5
So far, in machine translation research does not
exist one generally accepted criterion for the evalu-
ation of the experimental results. Therefore, we use
a large variety of different criteria and show that the
obtained results improve on most or all of these cri-
teria. In all experiments, we use the following six
error criteria:
? SER (sentence error rate): The SER is com-
puted as the number of times that the generated
sentence corresponds exactly to one of the ref-
erence translations used for the maximum en-
tropy training.
? WER (word error rate): The WER is computed
as the minimum number of substitution, inser-
tion and deletion operations that have to be per-
formed to convert the generated sentence into
the target sentence.
? PER (position-independent WER): A short-
coming of the WER is the fact that it requires
a perfect word order. The word order of an
acceptable sentence can be different from that
of the target sentence, so that the WER mea-
sure alone could be misleading. To overcome
this problem, we introduce as additional mea-
sure the position-independent word error rate
(PER). This measure compares the words in the
two sentences ignoring the word order.
? mWER (multi-reference word error rate): For
each test sentence, there is not only used a sin-
gle reference translation, as for the WER, but
a whole set of reference translations. For each
translation hypothesis, the edit distance to the
most similar sentence is calculated (Nie?en et
al., 2000).
? BLEU score: This score measures the precision
of unigrams, bigrams, trigrams and fourgrams
with respect to a whole set of reference trans-
lations with a penalty for too short sentences
(Papineni et al, 2001). Unlike all other eval-
uation criteria used here, BLEU measures ac-
curacy, i.e. the opposite of error rate. Hence,
large BLEU scores are better.
? SSER (subjective sentence error rate): For a
more detailed analysis, subjective judgments
by test persons are necessary. Each trans-
lated sentence was judged by a human exam-
iner according to an error scale from 0.0 to 1.0
(Nie?en et al, 2000).
? IER (information item error rate): The test sen-
tences are segmented into information items.
For each of them, if the intended information
is conveyed and there are no syntactic errors,
the sentence is counted as correct (Nie?en et
al., 2000).
In the following, we present the results of this ap-
proach. Table 2 shows the results if we use a direct
translation model (Eq. 6).
As baseline features, we use a normal word tri-
gram language model and the three component mod-
els of the alignment templates. The first row shows
the results using only the four baseline features with
?1 = ? ? ? = ?4 = 1. The second row shows the
result if we train the model scaling factors. We see a
systematic improvement on all error rates. The fol-
lowing three rows show the results if we add the
word penalty, an additional class-based five-gram
Table 2: Effect of maximum entropy training for alignment template approach (WP: word penalty feature,
CLM: class-based language model (five-gram), MX: conventional dictionary).
objective criteria [%] subjective criteria [%]
SER WER PER mWER BLEU SSER IER
Baseline(?m = 1) 86.9 42.8 33.0 37.7 43.9 35.9 39.0
ME 81.7 40.2 28.7 34.6 49.7 32.5 34.8
ME+WP 80.5 38.6 26.9 32.4 54.1 29.9 32.2
ME+WP+CLM 78.1 38.3 26.9 32.1 55.0 29.1 30.9
ME+WP+CLM+MX 77.8 38.4 26.8 31.9 55.2 28.8 30.9
0.74
0.76
0.78
0.8
0.82
0.84
0.86
0.88
0.9
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
se
nte
nc
e e
rro
r ra
te 
(SE
R)
 
number of iterations
MEME+WPME+WP+CLMME+WP+CLM+MX
Figure 3: Test error rate over the iterations of the
GIS algorithm for maximum entropy training of
alignment templates.
language model and the conventional dictionary fea-
tures. We observe improved error rates for using the
word penalty and the class-based language model as
additional features.
Figure 3 show how the sentence error rate (SER)
on the test corpus improves during the iterations of
the GIS algorithm. We see that the sentence error
rates converges after about 4000 iterations. We do
not observe significant overfitting.
Table 3 shows the resulting normalized model
scaling factors. Multiplying each model scaling fac-
tor by a constant positive value does not affect the
decision rule. We see that adding new features also
has an effect on the other model scaling factors.
6 Related Work
The use of direct maximum entropy translation mod-
els for statistical machine translation has been sug-
Table 3: Resulting model scaling factors of maxi-
mum entropy training for alignment templates; ?1:
trigram language model; ?2: alignment template
model, ?3: lexicon model, ?4: alignment model
(normalized such that ?4m=1 ?m = 4).
ME +WP +CLM +MX
?1 0.86 0.98 0.75 0.77
?2 2.33 2.05 2.24 2.24
?3 0.58 0.72 0.79 0.75
?4 0.22 0.25 0.23 0.24
WP ? 2.6 3.03 2.78
CLM ? ? 0.33 0.34
MX ? ? ? 2.92
gested by (Papineni et al, 1997; Papineni et al,
1998). They train models for natural language un-
derstanding rather than natural language translation.
In contrast to their approach, we include a depen-
dence on the hidden variable of the translation model
in the direct translation model. Therefore, we are
able to use statistical alignment models, which have
been shown to be a very powerful component for
statistical machine translation systems.
In speech recognition, training the parameters of
the acoustic model by optimizing the (average) mu-
tual information and conditional entropy as they are
defined in information theory is a standard approach
(Bahl et al, 1986; Ney, 1995). Combining various
probabilistic models for speech and language mod-
eling has been suggested in (Beyerlein, 1997; Peters
and Klakow, 1999).
7 Conclusions
We have presented a framework for statistical MT
for natural languages, which is more general than the
widely used source-channel approach. It allows a
baseline MT system to be extended easily by adding
new feature functions. We have shown that a base-
line statistical MT system can be significantly im-
proved using this framework.
There are two possible interpretations for a statis-
tical MT system structured according to the source-
channel approach, hence including a model for
Pr(eI1) and a model for Pr(fJ1 |eI1). We can inter-
pret it as an approximation to the Bayes decision rule
in Eq. 2 or as an instance of a direct maximum en-
tropy model with feature functions logPr(eI1) and
logPr(fJ1 |eI1). As soon as we want to use model
scaling factors, we can only do this in a theoretically
justified way using the second interpretation. Yet,
the main advantage comes from the large number of
additional possibilities that we obtain by using the
second interpretation.
An important open problem of this approach is
the handling of complex features in search. An in-
teresting question is to come up with features that
allow an efficient handling using conventional dy-
namic programming search algorithms.
In addition, it might be promising to optimize the
parameters directly with respect to the error rate of
the MT system as is suggested in the field of pattern
and speech recognition (Juang et al, 1995; Schlu?ter
and Ney, 2001).
References
L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mer-
cer. 1986. Maximum mutual information estimation
of hidden markov model parameters. In Proc. Int.
Conf. on Acoustics, Speech, and Signal Processing,
pages 49?52, Tokyo, Japan, April.
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?72, March.
P. Beyerlein. 1997. Discriminative model combina-
tion. In Proc. of the IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 238?
245, Santa Barbara, CA, December.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear models. Annals of Mathe-
matical Statistics, 43:1470?1480.
B. H. Juang, W. Chou, and C. H. Lee. 1995. Statisti-
cal and discriminative methods for speech recognition.
In A. J. R. Ayuso and J. M. L. Soler, editors, Speech
Recognition and Coding - New Advances and Trends.
Springer Verlag, Berlin, Germany.
H. Ney. 1995. On the probabilistic-interpretation of
neural-network classifiers and discriminative training
criteria. IEEE Trans. on Pattern Analysis and Machine
Intelligence, 17(2):107?119, February.
S. Nie?en, F. J. Och, G. Leusch, and H. Ney. 2000.
An evaluation tool for machine translation: Fast eval-
uation for MT research. In Proc. of the Second Int.
Conf. on Language Resources and Evaluation (LREC),
pages 39?45, Athens, Greece, May.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint SIGDAT Conf. on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 20?28, University of Maryland, Col-
lege Park, MD, June.
K. A. Papineni, S. Roukos, and R. T. Ward. 1997.
Feature-based language understanding. In European
Conf. on Speech Communication and Technology,
pages 1435?1438, Rhodes, Greece, September.
K. A. Papineni, S. Roukos, and R. T. Ward. 1998. Max-
imum likelihood and discriminative training of direct
translation models. In Proc. Int. Conf. on Acoustics,
Speech, and Signal Processing, pages 189?192, Seat-
tle, WA, May.
K. A. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Division, Thomas J. Watson Research
Center, Yorktown Heights, NY, September.
J. Peters and D. Klakow. 1999. Compact maximum en-
tropy language models. In Proc. of the IEEE Workshop
on Automatic Speech Recognition and Understanding,
Keystone, CO, December.
R. Schlu?ter and H. Ney. 2001. Model-based MCE bound
to the true Bayes? error. IEEE Signal Processing Let-
ters, 8(5):131?133, May.
W. Wahlster. 1993. Verbmobil: Translation of face-to-
face dialogs. In Proc. of MT Summit IV, pages 127?
135, Kobe, Japan, July.
An Efficient A* Search Algorithm for Statistical Machine Translation
Franz Josef Och, Nicola Ueffing, Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen - University of Technology
D-52056 Aachen, Germany
{och,ueffing,ney}@informatik.rwth-aachen.de
Abstract
In this paper, we describe an efficient
A* search algorithm for statistical ma-
chine translation. In contrary to beam-
search or greedy approaches it is possi-
ble to guarantee the avoidance of search
errors with A*. We develop various so-
phisticated admissible and almost ad-
missible heuristic functions. Especially
our newly developped method to per-
form a multi-pass A* search with an
iteratively improved heuristic function
allows us to translate even long sen-
tences. We compare the A* search al-
gorithm with a beam-search approach
on the Hansards task.
1 Introduction
The goal of machine translation is the transla-
tion of a text given in some source language into
a target language. We are given a source string
fJ1 = f1...fj ...fJ , which is to be translated into a
target string eI1 = e1...ei...eI . Among all possible
target strings, we will choose the string with the
highest probability:
e?I1 = argmaxeI1
{
Pr(eJ1 |f I1 )
}
= argmax
eI1
{
Pr(eI1) ? Pr(fJ1 |eI1)
}
The argmax operation denotes the search prob-
lem, i.e. the generation of the output sentence
in the target language. Pr(eI1) is the language
model of the target language, whereas Pr(fJ1 |eI1)
denotes the translation model.
Many statistical translation models (Brown et
al., 1993; Vogel et al, 1996; Och and Ney, 2000b)
try to model word-to-word correspondences be-
tween source and target words. These correspon-
dences are called an alignment. The model is
often further restricted in a way such that each
source word is assigned exactly one target word.
The alignment mapping is j ? i = aj from
source position j to target position i = aj . The
alignment aJ1 may contain alignments aj = 0
with the ?empty? word e0 to account for source
words that are not aligned to any target word. In
(statistical) alignment models Pr(fJ1 , aJ1 |eI1), the
alignment aJ1 is introduced as a hidden variable.
Typically, the search is performed using the so-
called maximum approximation:
e?I1 = argmaxeI1
?
??
??
Pr(eI1) ?
?
aJ1
Pr(fJ1 , aJ1 |eI1)
?
??
??
= argmax
eI1
{
Pr(eI1) ?maxaJ1
Pr(fJ1 , aJ1 |eI1)
}
The search space consists of the set of all possible
target language strings eI1 and all possible align-
ments aJ1 .
2 IBM Model 4
Various statistical alignment models of the form
Pr(fJ1 , aJ1 |eI1) have been introduced in (Brown
et al, 1993; Vogel et al, 1996; Och and Ney,
2000a). In this paper we use the so-called Model
4 from (Brown et al, 1993).
In Model 4 the statistical alignment model is
decomposed into five sub-models:
? the lexicon model p(f |e) for the probability
that the source word f is a translation of the
target word e,
? the distortion model p=1(j?j?|C(fj), E) for
the probability that the translations of two
consecutive target words have the position
difference j ? j? where C(fj) is the word
class of fj and E is the word class of the
first of the two consecutive target words,
? the distortion model p>1(j ? j?|C(fj)) for
the probability that the words aligned to one
target words have the position difference j?
j?,
? the fertility model p(?|e) for the probability
that a target language word e is aligned to ?
source language words,
? the empty word fertility model p(?0|e0) for
the probability that exactly ?0 words remain
unaligned to.
The final probability p(fJ1 , aJ1 |eI1) for Model 4 is
obtained by multiplying the probabilities of the
sub-models for all words. For a detailed descrip-
tion for Model 4 the reader is referred to (Brown
et al, 1993).
We use Model 4 in this paper for two reasons.
First, it has been shown that Model 4 produces
a very good alignment quality in comparison to
various other alignment models (Och and Ney,
2000b). Second, the dependences in the distortion
model along the target language words make it
quite easy to integrate standard n-gram language
models in the search process. This would be more
difficult in the HMM alignment model (Vogel et
al., 1996). Yet, many of the results presented in
the following are also applicable to other align-
ment models.
3 Search problem
The following tasks have to be performed both us-
ing A* and beam search (BS):
? The search space has to be structured into
a search graph. This search graph typically
includes an initial node, intermediary nodes
(partial hypotheses), and goal nodes (com-
pleted hypotheses). A node contains the fol-
lowing information:
? the predecessor words u, v in the target
language,
? the score of the hypothesis,
? a backpointer to the preceding partial
hypothesis,
? the model specific information de-
scribed at the end of this subsection.
? A scoring function Q(n) + h(n) has to be
defined which assigns a score to every node
n. For beam search, this is the score Q(n) of
a best path to this node. In the A* algorithm,
an estimation h(n) of the score of a best path
from node n to a goal node is added.
(Berger et al, 1996) presented a method to struc-
ture the search space. Our search algorithm for
Model 4 uses a similar structuring of the search
space. We will shortly review the basic concepts
of this search space structure: Every partial hy-
pothesis consists of a prefix of the target sentence
and a corresponding alignment. A partial hypoth-
esis is extended by accounting for exactly one ad-
ditional word of the source sentence. Every exten-
sion yields an extension score which is computed
by taking into account the lexicon, distortion, and
fertility probabilities involved with this extension.
A partial hypothesis is called open if more source
words are to be aligned to the current target word
in the following extensions. A hypothesis that is
not open is said to be closed. Every extension of
an open hypothesis will extend the fertility of the
previously produced target word and an extension
of a closed hypothesis will produce a new word.
Therefore, the language model score is added as
well if a closed hypothesis is extended.
It is prohibitive to consider all possible transla-
tions of all words. Instead, we restrict the search
to the most promising candidates by calculating
?inverse translations? (Al-Onaizan et al, 1999).
The inverse translation probability p(e | f) of a
source word f is calculated as
p(e | f) = p (f | e) p (e)?
e?
p (f | e?) p (e?) ,
where we use a unigram model p (e) to esti-
mate the prior probability of a target word be-
ing used. Like (Al-Onaizan et al, 1999), we use
only the top 12 translations of a given source lan-
guage word. In addition, we remove from this list
all words whose inverse translation probability is
lower than 0.01 times the best inverse translation
probability. This observation pruning is the only
pruning involved in our A* search algorithm. Ex-
periments showed this does not impair translation
quality, but the search becomes much more effi-
cient.
In order to keep the search space as small as
possible it is crucial to perform a recombina-
tion of search hypotheses. Every two hypothe-
ses which can be distinguished by neither the lan-
guage model state nor the translation model state
can be recombined, only the hypothesis with a
better score of the two needs to be considered in
the subsequent search process. We use a standard
trigram language model, so the relevant language
model state of node n consists of the current word
w(n) and the previous word v(n) (later on we will
describe an improvement to this). The translation
model state depends on the specific model depen-
dencies of Model 4:
? a coverage set C(n) containing the already
translated source language positions,
? the position j(n) of the previously translated
source word,
? a flag indicating whether the hypothesis is
open or closed,
? the number of source language words which
are aligned to the empty word,
? a flag showing whether the hypothesis is a
complete hypothesis or not.
Efficient language model recombination
The recombination procedure which is described
above can be improved by taking into account the
backing-off structure of the language model. The
trigram language model we use has the property
that if the count of the bigram N(u, v) = 0, then
the probability P (w|u, v) depends only on v. In
this case the recombination can be significantly
improved by recombining all nodes whose lan-
guage model state has the property N(u, v) = 0
only with respect to v. Obviously, this could be
generalized to other types of language models as
well.
Experiments have shown that by using this ef-
ficient recombination, the number of needed hy-
potheses can be reduced by about a factor of 4.
Search algorithms
We evaluate the following two search algorithms:
? beam search algorithm (BS): (Tillmann,
2001; Tillmann and Ney, 2000)
In this algorithm the search space is explored
in a breadth-first manner. The search algo-
rithm is based on a dynamic programming
approach and applies various pruning tech-
niques in order to restrict the number of con-
sidered hypotheses. For more details see
(Tillmann, 2001).
? A* search algorithm:
In A*, all search hypotheses are managed in
a priority queue. The basic A* search (Nils-
son, 1971) can be described as follows:
1. initialize priority queue with an empty
hypothesis
2. remove the hypothesis with the highest
score from the priority queue
3. if this hypothesis is a goal hypothesis:
output this hypothesis and terminate
4. produce all extensions of this hypothe-
sis and put the extensions to the queue
5. goto 2
The so-called heuristic function estimates
the probability of a completion of a partial
hypothesis. This function is called admissi-
ble if it never underestimates this probabil-
ity. Thus, admissible heuristic functions are
always optimistic. The A* search algorithm
corresponds to the Dijkstra algorithm if the
heuristic function is equal to zero.
4 Admissible heuristic function
In order to perform an efficient search with the
A* search algorithm it is crucial to use a good
heuristic function. We only know of the work by
(Wang and Waibel, 1997) dealing with heuristic
functions for search in statistical machine trans-
lation. They developed a simple heuristic func-
tion for Model 2 from (Brown et al, 1993) which
was non admissible. In the following we de-
velop a guaranteed admissible heuristic function
for Model 4 taking into account distortion proba-
bilities and the coupling of lexicon, fertility, and
language model probabilities.
The basic idea for developing a heuristic func-
tion for the alignment models is the fact that all
source sentence positions which have not been
covered so far still have to be translated in order
to complete the sentence. Therefore, the value of
the heuristic function HX(n) for a node n can
be deduced if we have an estimation hX(j) of the
optimal score of translating position j (here X de-
notes different possibilities to choose the heuristic
function):
HX(n) =
?
j 6?C(n)
hX(j) ,
where C(n) is the coverage set.
The simplest realization of a heuristic func-
tion, denoted as hT (j), takes into account only
the translation probability p(f |e):
hT (j) = maxe p(fj |e)
This heuristic function can be refined by intro-
ducing also the fertility probabilities (symbol F)
of a target word e:
hTF (j) =
= max
{
max
e 6=e0,?
p(fj |e) ?
?
p(?|e), p(f |e0)
}
Thereby, a coupling between the translation and
fertility probabilities is achieved. We have to
take the ?-th root in order to avoid that the fer-
tility probability of a target word whose fertility
is higher than one is taken into account for every
source word aligned to it. For words which are
translated by the empty word e0, no fertility prob-
ability is used.
The language model can be incorporated by
considering that for every target word there exists
an optimal language model probability:
pL(e) = maxu,v p(e|u, v)
Here, we assume a trigram language model.
Thus, a heuristic function including a coupling
between translation, fertility, and language model
probabilities (TFL) is given by:
hTFL(j) =
= max
{
max
e,?
p(fj |e) ?
?
p(?|e)pL(e), p(f |e0)
}
This value can be precomputed efficiently before
the search process itself starts.
The heuristic function for the distortion proba-
bilities depends on the used model. For Model 4,
we obtain:
hD(j) = max
j?,E
p(j ? j?|E,C(fj))
Here, E refers to the class of the previously
aligned target word.
The heuristic functions hD(j) involve maxi-
mizations over the source positions j?. The do-
main of this variable shrinks during search as
more and more words get translated. Therefore, it
is possible to improve this heuristic function dur-
ing search to perform a maximization only over
the free source language positions j?. For Model 4
we compute the following heuristic function with
two arguments:
hD(j?, j) = max
E
p(j ? j?|E,C(fj))
Thus, we obtain as an estimation of the distortion
probability
hD(j) = max
j? 6?C(n)
hD(j?, j) .
This yields the following heuristic functions tak-
ing into account translation, fertility, language,
and distortion model probabilities:
HTFLD(n) =
?
j 6?C(n)
hTFL(j) ? hD(j) (1)
Using these heuristic functions we have the over-
head of performing this rest cost estimation for
every coverage set in search. The experiments
will show that these additional costs are over-
compensated by the gain in reducing the search
space that has to be expanded during the A*
search.
To assess the predictive power of the vari-
ous components in the heuristic, we compare the
value of the heuristic function of the empty hy-
pothesis with the score of the optimal transla-
tion. A heuristic function is better if the dif-
ference between these two values is small. Ta-
ble 1 contains a comparison of various heuristic
functions. We compare the average costs (nega-
tive logarithm of the probabilities) of the optimal
translation and the average of the estimated costs
of the empty hypothesis. Typically, the estimated
costs of TFLD and the real costs differ by factor
3.
Table 1: Predictive power of admissible and almost admissible heuristic functions.
sentence HF for initial node empirical goal node
length T TF TFL TFLD score score
6 5.1 7.2 12.7 13.0 25.9 35.5
8 5.7 8.2 16.0 16.3 29.8 43.7
10 8.1 11.6 19.4 19.7 36.5 55.8
12 9.5 13.7 20.7 21.1 43.9 63.4
We will see later in Section 6 that the guar-
anteed admissible heuristic functions described
above result in dramatically more efficient search.
5 Empirical heuristic functions
In this section we describe a new method to ob-
tain an almost admissible heuristic function by a
multi pass search. This yields a significantly more
efficient search than using the admissible heuris-
tic functions. Thus, we lose the strict guarantee to
avoid search errors, but obtain a significant time
gain.
The idea of an empirical heuristic function
is to perform a multi-pass search. In the first
pass a good admissible heuristic function (here:
HTFLD) is used. If this search does not need too
much memory the search process is finished. If
the search failed, it is restarted using an improved
heuristic function which had been obtained during
the initial search process. This heuristic function
is computed such that it has the property that it
is admissible with respect to the explored search
space. That means, the heuristic function is op-
timistic with respect to every node in the search
space explored in the first pass.
Specifically, during the first pass, we maintain
a two-dimensional matrix hE(j, j?) with (J +2) ?
(J + 2) entries which are all initialized with ?.
The entry hE(j, j?) is the best score that was com-
puted for translating the source language word in
position j? if the previously covered source sen-
tence position is j. The matrix entry is updated
for every extension of a node n ? n?:
hE(j(n), j(n?)) :=
= max
{
hE(j(n), j(n?)), p(n, n?)
}
Here, p(n, n?) is the probability of the extension
n ? n?. hE(0, j) is the empirical score of start-
ing a sentence by covering the j-th source sen-
tence position first. Likewise, hE(j, J + 1) is the
empirical score of finishing a sentence with j as
the last source sentence position that was covered.
This yields
hE(j) = max
j? 6?C(n)?j?=J+1
hE(j, j?) .
In this calculation of hE(j), we maximize over
the columns of a matrix. The translation of the
source sentence can be viewed as a Traveling
Salesman Problem where the source sentence po-
sitions are the cities that have to be visited. Thus,
the maximization over the columns is equivalent
to assuring that the position j will be left after
the visit. We design an improved heuristic func-
tion using the following principle (Aigner, 1993):
Each city has to be both reached and left. There-
fore, in order to take an upper bound of reaching
a city into account, we divide each column of the
matrix by its maximum and maximize over the
rows of the matrix (Aigner, 1993):
hE+(j) = max
j? 6?C(n)?j?=j(n)
hE(j?, j)/hE(j?) .
We obtain the following empirical heuristic func-
tions:
HE(n) =
?
j 6?C(n)?j=j(n)
hE(j)
HE+(n) =
=
?
j 6?C(n)?j=j(n)
hE(j) ?
?
j? 6?C(n)?j?=J+1
hE+(j?)
If the search fails in the first pass due to the re-
striction of the number of hypotheses ? which was
1 million in all experiments ? the search can be
started again using HE+(n) as a heuristic. To
avoid an overestimation of the actual costs, we
multiply the empirical costs by a factor lower than
Table 3: Training corpus statistics (* without
punctuation marks).
French English
sentences 49000 49000
words 743903 816964
words* 664058 730880
average sentence length 16.9 14.6
vocabulary size 19831 24892
Table 4: Test corpora statistics.
Corpus # Sentences # Words
F E
T6 50 300 329
T8 50 400 403
T10 50 500 509
T12 50 600 601
T14 50 700 644
1. We found in our experiments that a factor of
0.7 is sufficient. The search was restarted up to 4
times if it failed. Using this method, it is possi-
ble to translate sentences that are longer than 10
words with a restriction to 1 million hypotheses.
Table 1 shows the value of the empirical heuris-
tic function of the empty node compared to the
score of the optimal goal node. The estimated
costs and the real costs now differ only by a fac-
tor of 1.5 instead of a factor of 3 for the TFLD
heuristic function before.
6 Results
We present results on the HANSARDS task which
consists of proceedings of the Canadian parlia-
ment that are kept both in French and in English.
Table 3 shows the details of our training corpus.
We used different the test corpora with sentences
of length 6-14 words (Table 4).
In all experiments, we use the following two
error criteria:
? WER (word error rate):
The WER is computed as the minimum
number of substitution, insertion and dele-
tion operations that have to be performed to
convert the generated string into the target
string.
? PER (position independent word error rate):
The word order of a French/English sentence
pair can be quite different. As a result, the
word order of the automatically generated
target sentence can be different from that of
the given target sentence, but nevertheless
acceptable so that the WER measure alone
could be misleading. In order to overcome
this problem, we introduce the position inde-
pendent word error rate (PER) as additional
measure. This measure compares the words
in the two sentences without taking the word
order into account.
In the following experiments we restricted the
maximum number of active search hypotheses in
A* search to 1 million. Every hypothesis has an
effective memory requirement of about 100 Byte.
Therefore, we obtain a dynamic memory require-
ment of about 100 MByte.
In order to speed up the search, we restricted
the reordering of words in IBM-style (Berger et
al., 1996; Tillmann, 2001). According to this re-
striction, up to 3 source sentence positions may be
skipped and translated later, i. e. during the search
process there may be up to 3 uncovered positions
left of the rightmost covered position in the source
sentence. The word error rate does not increase
compared to a non-restricted reordering, but the
search becomes much more efficient.
Table 5 shows how many sentences with differ-
ent sentence lengths can be translated using beam
search and A* with various heuristic functions.
Obviously, the BS approach is able to translate
any sentence length, therefore the search success
rate is 100%. Without any heuristic function A*
is only able to translate all 8-word sentences (with
the restriction of a maximum number of 1 million
hypotheses). Using more sophisticated heuristic
functions we are also able to translate all 10-word
sentences with A*.
Table 6 compares the search errors of A* and
BS. During the BS search, translation pruning
is carried out. The different hypotheses are dis-
tinguished according to the set of covered posi-
tions of the source sentence. For every set, the
best score of all hypotheses is computed. Only
those hypotheses are kept whose score is greater
than this best score multiplied with a threshold.
We chose the threshold to be 2.5, 5.0, 7.5 and 10.0
(see Table 6).
Table 2: Effect of observation pruning on the translation quality (average over all test sets).
# inverse 10 12 14 16 18 20
translations
WER 73.81 73.33 75.50 76.23 76.19 76.59
PER 68.02 66.93 70.07 71.16 71.24 71.16
Table 5: Search Success Rate (1 million hypothe-
ses) [%].
sentence length 6 8 10 12
BS 100 100 100 100
A*: no 100 100 86 12
T 100 100 88 20
TF 100 100 88 22
TFL 100 100 92 36
TFLD 100 100 92 36
E 100 100 100 74
E+ 100 100 100 84
Table 6: Search errors [%].
sentence length 6 8 10 12 14
BS 2.5 26 28 38 50 38
5.0 2 0 2 6 4
7.5 0 0 0 4 2
10.0 0 0 0 4 2
A* 0 0 0 0 0
For A* we never observe any search errors. In
the case of the admissible heuristic functions, this
is guaranteed by the approach. As can be seen
from Table 6, the BS algorithm with a large beam
rarely produces search errors.
Table 7 compares the translation efficiency of
the various search algorithms. We see that beam
search even with a very large beam producing
only very few search errors is much more efficient
than the used A* search algorithm.
Table 8 contains an assessment of translation
quality comparison of A* and BS using the T6,
T8, T10, T12-test corpus. For A*, we use the E+
rest cost estimation as this gives optimal results.
From the 200 sentences of these test corpora we
can translate 192 sentences using the 1 million hy-
potheses constraint. For the remaining sentences
we performed a search with 4 million hypotheses
Table 7: Average search time [s] per sentence.
sentence
length 6 8 10 12
BS: 2.5 0.06 0.18 0.60 1.16
5.0 0.24 0.84 2.90 6.48
7.5 0.50 2.14 7.06 16.26
10.0 0.78 3.30 11.86 26.42
A*: E+ 1.58 13.04 100 394
(cf. below) which lead to a success for all the 12-
word sentences.
The number of hypotheses in A* search
We restricted the maximal number of hypotheses
to 1 million. This was sufficient for translating
10-word sentences, as the search algorithm suc-
cess rate in Table 5 shows. For longer sentences
it is necessary to allow for a larger number of hy-
potheses. For the sentences of lengths 12 and 14,
we performed an A* search (E+) with 2, 4 and
8 million possible hypotheses. The search algo-
rithm success rate for those searches is contained
in Table 9. We see a significant effect on the num-
ber of successful searches.
7 Conclusion
We have developed sophisticated admissible and
almost admissible heuristic functions for statis-
tical machine translation. We have focussed on
Model 4, but most of the computations could
be easily extended to other statistical alignment
models (like HMM or Model 5). We especially
have observed the following effects:
? The heuristic function has a strong effect on
the efficiency of the A* search. Without any
heuristic function only 75 % of the test cor-
pus sentences can be translated (using the
1 million hypotheses constraint). Using the
Table 8: Translation quality.
BS (2.5) BS (5.0) BS (7.5) BS (10.0) A* (E+)
WER 69.65 68.78 68.68 68.68 68.68
PER 62.65 61.62 61.51 61.51 61.45
Table 9: A* (E+) Success Rate for 12- and 14-word sentences [%].
# hypotheses 1 million 2 million 4 million 8 million
12 42 80 100 100
14 2 20 70 100
best admissible heuristic function TFLD
we can translate 82 %.
? Using the empirical heuristic function we
can translate 96 % of the sentences with
A* search. This heuristic function does not
guarantee to avoid search errors, but this
case never occurred in our experiments.
From these results we conclude that it is often
possible to faster compute acceptable results us-
ing a beam search approach. Therefore, this is
the method of choice in practice. From a theo-
retical viewpoint it is interesting that using A* it
is possible to translate guaranteed without search
errors. In addition, without having a chance to
perform search without search errors it is almost
impossible to assess if errors in translation should
be assigned to the model/training or to the search
heuristics. Therefore, the A* algorithm is espe-
cially useful during the development of a statisti-
cal machine translation system.
Acknowledgment
This paper is based on work supported partly
by the VERBMOBIL project (contract number
01 IV 701 T4) by the German Federal Min-
istry of Education, Science, Research and Tech-
nology. In addition, this work was supported
by the National Science Foundation under Grant
No. IIS-9820687 through the 1999 Workshop on
Language Engineering, Center for Language and
Speech Processing, Johns Hopkins University.
References
M. Aigner. 1993. Diskrete Mathematik. Verlag Vieweg,
Braunschweig/Wiesbaden, Germany.
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Laf-
ferty, I. D. Melamed, F. J. Och, D. Purdy, N. A.
Smith, and D. Yarowsky. 1999. Statistical ma-
chine translation, final report, JHU workshop.
http://www.clsp.jhu.edu/ws99/projects/
mt/final report/mt-final-report.ps.
A. L. Berger, S. A. Della Pietra P. F. Brown, V. J. Della
Pietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer.
1996. Language translation apparatus and method of us-
ing context-based translation models. In United States
Patent, number 5510981. April.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
N. Nilsson. 1971. Problem-Solving Methods in Artificial
Intelligence. McGraw-Hill, McGraw-Hill, New York.
F. J. Och and H. Ney. 2000a. A comparison of alignment
models for statistical machine translation. In COLING
?00: The 18th Int. Conf. on Computational Linguistics,
pages 1086?1090, Saarbru?cken, Germany, August.
F. J. Och and H. Ney. 2000b. Improved statistical alignment
models. In Proc. of the 38th Annual Meeting of the As-
sociation for Computational Linguistics, pages 440?447,
Hongkong, China, October.
C. Tillmann and H. Ney. 2000. Word re-ordering and DP-
based search in statistical machine translation. In COL-
ING ?00: The 18th Int. Conf. on Computational Linguis-
tics, pages 850?856, Saarbru?cken, Germany, August.
C. Tillmann. 2001. Word Re-Ordering and Dynamic Pro-
gramming based Search Algorithms for Statistical Ma-
chine Translation. Ph.D. thesis, RWTH Aachen, Ger-
many, May.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word
alignment in statistical translation. In COLING ?96: The
16th Int. Conf. on Computational Linguistics, pages 836?
841, Copenhagen, August.
Ye-Yi Wang and Alex Waibel. 1997. Decoding algorithm in
statistical translation. In Proc. 35th Annual Conf. of the
Association for Computational Linguistics, pages 366?
372, Madrid, Spain, July.
 	
	 	 	ffProceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1145?1152
Manchester, August 2008
A Systematic Comparison of Phrase-Based, Hierarchical and
Syntax-Augmented Statistical MT
Andreas Zollmann
?
and Ashish Venugopal
?
and Franz Och and Jay Ponte
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94303, USA
{zollmann,ashishv}@cs.cmu.edu {och,ponte}@google.com
Abstract
Probabilistic synchronous context-free
grammar (PSCFG) translation models
define weighted transduction rules that
represent translation and reordering oper-
ations via nonterminal symbols. In this
work, we investigate the source of the im-
provements in translation quality reported
when using two PSCFG translation mod-
els (hierarchical and syntax-augmented),
when extending a state-of-the-art phrase-
based baseline that serves as the lexical
support for both PSCFG models. We
isolate the impact on translation quality
for several important design decisions in
each model. We perform this comparison
on three NIST language translation tasks;
Chinese-to-English, Arabic-to-English
and Urdu-to-English, each representing
unique challenges.
1 Introduction
Probabilistic synchronous context-free grammar
(PSCFG) models define weighted transduction
rules that are automatically learned from parallel
training data. As in monolingual parsing, such
rules make use of nonterminal categories to gener-
alize beyond the lexical level. In the example be-
low, the French (source language) words ?ne? and
?pas? are translated into the English (target lan-
guage) word ?not?, performing reordering in the
context of a nonterminal of type ?VB? (verb).
VP ? ne VB pas, do not VB : w
1
?
Work done during internships at Google Inc.
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
VB ? veux,want : w
2
.
As with probabilistic context-free grammars, each
rule has a left-hand-side nonterminal (VP and VB
in the two rules above), which constrains the rule?s
usage in further composition, and is assigned a
weight w, estimating the quality of the rule based
on some underlying statistical model. Transla-
tion with a PSCFG is thus a process of compos-
ing such rules to parse the source language while
synchronously generating target language output.
PSCFG approaches such as Chiang (2005) and
Zollmann and Venugopal (2006) typically begin
with a phrase-based model as the foundation for
the PSCFG rules described above. Starting with
bilingual phrase pairs extracted from automatically
aligned parallel text (Och and Ney, 2004; Koehn et
al., 2003), these PSCFG approaches augment each
contiguous (in source and target words) phrase
pair with a left-hand-side symbol (like the VP in
the example above), and perform a generalization
procedure to form rules that include nonterminal
symbols. We can thus view PSCFG methods as
an attempt to generalize beyond the purely lexi-
cal knowledge represented in phrase based mod-
els, allowing reordering decisions to be explicitly
encoded in each rule. It is important to note that
while phrase-based models cannot explicitly repre-
sent context sensitive reordering effects like those
in the example above, in practice, phrase based
models often have the potential to generate the
same target translation output by translating source
phrases out of order, and allowing empty trans-
lations for some source words. Apart from one
or more language models scoring these reorder-
ing alternatives, state-of-the-art phrase-based sys-
tems are also equipped with a lexicalized distortion
model accounting for reordering behavior more di-
rectly. While previous work demonstrates impres-
1145
sive improvements of PSCFG over phrase-based
approaches for large Chinese-to-English data sce-
narios (Chiang, 2005; Chiang, 2007; Marcu et al,
2006; DeNeefe et al, 2007), these phrase-based
baseline systems were constrained to distortion
limits of four (Chiang, 2005) and seven (Chiang,
2007; Marcu et al, 2006; DeNeefe et al, 2007),
respectively, while the PSCFG systems were able
to operate within an implicit reordering window of
10 and higher.
In this work, we evaluate the impact of the ex-
tensions suggested by the PSCFG methods above,
looking to answer the following questions. Do the
relative improvements of PSCFG methods persist
when the phrase- based approach is allowed com-
parable long-distance reordering, and when the n-
gram language model is strong enough to effec-
tively select from these reordered alternatives? Do
these improvements persist across language pairs
that exhibit significantly different reodering effects
and how does resource availablility effect relative
performance? In order to answer these questions
we extend our PSCFG decoder to efficiently han-
dle the high order LMs typically applied in state-
of-the-art phrase based translation systems. We
evaluate the phrase-based system for a range of re-
ordering limits, up to those matching the PSCFG
approaches, isolating the impact of the nontermi-
nal based approach to reordering. Results are pre-
sented in multiple language pairs and data size
scenarios, highlighting the limited impact of the
PSCFG model in certain conditions.
2 Summary of approaches
Given a source language sentence f , statistical ma-
chine translation defines the translation task as se-
lecting the most likely target translation e under a
model P (e|f), i.e.:
?
e(f) = argmax
e
P (e|f) = argmax
e
m
?
i=1
h
i
(e, f)?
i
where the argmax operation denotes a search
through a structured space of translation ouputs
in the target language, h
i
(e, f) are bilingual fea-
tures of e and f and monolingual features of e,
and weights ?
i
are trained discriminitively to max-
imize translation quality (based on automatic met-
rics) on held out data (Och, 2003).
Both phrase-based and PSCFG approaches
make independence assumptions to structure this
search space and thus most features h
i
(e, f) are
designed to be local to each phrase pair or rule.
A notable exception is the n-gram language model
(LM), which evaluates the likelihood of the se-
quential target words output. Phrase-based sys-
tems also typically allow source segments to be
translated out of order, and include distortion mod-
els to evaluate such operations. These features
suggest the efficient dynamic programming al-
gorithms for phrase-based systems described in
Koehn et al (2004).
We now discuss the translation models com-
pared in this work.
2.1 Phrase Based MT
Phrase-based methods identify contiguous bilin-
gual phrase pairs based on automatically gener-
ated word alignments (Och et al, 1999). Phrase
pairs are extracted up to a fixed maximum length,
since very long phrases rarely have a tangible im-
pact during translation (Koehn et al, 2003). Dur-
ing decoding, extracted phrase pairs are reordered
to generate fluent target output. Reordered trans-
lation output is evaluated under a distortion model
and corroborated by one or more n-gram language
models. These models do not have an explicit rep-
resentation of how to reorder phrases. To avoid
search space explosion, most systems place a limit
on the distance that source segments can be moved
within the source sentence. This limit, along with
the phrase length limit (where local reorderings
are implicit in the phrase), determine the scope of
reordering represented in a phrase-based system.
All experiments in this work limit phrase pairs to
have source and target length of at most 12, and
either source length or target length of at most 6
(higher limits did not result in additional improve-
ments). In our experiments phrases are extracted
by the method described in Och and Ney (2004)
and reordering during decoding with the lexical-
ized distortion model from Zens and Ney (2006).
The reordering limit for the phrase based system
(for each language pair) is increased until no addi-
tional improvements result.
2.2 Hierarchical MT
Building upon the success of phrase-based meth-
ods, Chiang (2005) presents a PSCFG model of
translation that uses the bilingual phrase pairs of
phrase-based MT as starting point to learn hierar-
chical rules. For each training sentence pair?s set of
extracted phrase pairs, the set of induced PSCFG
rules can be generated as follows: First, each
1146
phrase pair is assigned a generic X-nonterminal as
left-hand-side, making it an initial rule. We can
now recursively generalize each already obtained
rule (initial or including nonterminals)
N ? f
1
. . . f
m
/e
1
. . . e
n
for which there is an initial rule
M ? f
i
. . . f
u
/e
j
. . . e
v
where 1 ? i < u ? m and 1 ? j < v ? n, to
obtain a new rule
N ? f
i?1
1
X
k
f
m
u+1
/e
j?1
1
X
k
e
n
v+1
where e.g. f
i?1
1
is short-hand for f
1
. . . f
i?1
, and
where k is an index for the nonterminal X that
indicates the one-to-one correspondence between
the new X tokens on the two sides (it is not in
the space of word indices like i, j, u, v,m, n). The
recursive form of this generalization operation al-
lows the generation of rules with multiple nonter-
minal symbols.
Performing translation with PSCFG grammars
amounts to straight-forward generalizations of
chart parsing algorithms for PCFG grammars.
Adaptations to the algorithms in the presence of n-
gram LMs are discussed in (Chiang, 2007; Venu-
gopal et al, 2007; Huang and Chiang, 2007).
Extracting hierarchical rules in this fashion can
generate a large number of rules and could in-
troduce significant challenges for search. Chiang
(2005) places restrictions on the extracted rules
which we adhere to as well. We disallow rules
with more than two nonterminal pairs, rules with
adjacent source-side nonterminals, and limit each
rule?s source side length (i.e., number of source
terminals and nonterminals) to 6. We extract rules
from initial phrases of maximal length 12 (exactly
matching the phrase based system).
1
Higher length
limits or allowing more than two nonterminals per
rule do not yield further improvements for systems
presented here.
During decoding, we allow application of all
rules of the grammar for chart items spanning up
to 15 source words (for sentences up to length 20),
or 12 source words (for longer sentences), respec-
tively. When that limit is reached, only a special
glue rule allowing monotonic concatenation of hy-
potheses is allowed. (The same holds for the Syn-
tax Augmented system.)
1
Chiang (2005) uses source length limit 5 and initial
phrase length limit 10.
2.3 Syntax Augmented MT
Syntax Augmented MT (SAMT) (Zollmann and
Venugopal, 2006) extends Chiang (2005) to in-
clude nonterminal symbols from target language
phrase structure parse trees. Each target sentence
in the training corpus is parsed with a stochas-
tic parser?we use Charniak (2000))?to produce
constituent labels for target spans. Phrases (ex-
tracted from a particular sentence pair) are as-
signed left-hand-side nonterminal symbols based
on the target side parse tree constituent spans.
Phrases whose target side corresponds to a con-
stituent span are assigned that constituent?s label as
their left-hand-side nonterminal. If the target span
of the phrase does not match a constituent in the
parse tree, heuristics are used to assign categories
that correspond to partial rewriting of the tree.
These heuristics first consider concatenation oper-
ations, forming categories such as ?NP+V?, and
then resort to CCG (Steedman, 1999) style ?slash?
categories such as ?NP/NN.? or ?DT\NP?. In the
spirit of isolating the additional benefit of syntactic
categories, the SAMT system used here also gen-
erates a purely hierarchical (single generic nonter-
minal symbol) variant for each syntax-augmented
rule. This allows the decoder to choose between
translation derivations that use syntactic labels and
those that do not. Additional features introduced
in SAMT rules are: a relative frequency estimated
probability of the rule given its left-hand-side non-
terminal, and a binary feature for the the purely
hierachial variants.
3 Large N-Gram LMs for PSCFG
decoding
Brants et al (2007) demonstrate the value of large
high-order LMs within a phrase-based system. Re-
cent results with PSCFG based methods have typ-
ically relied on significantly smaller LMs, as a
result of runtime complexity within the decoder.
In this work, we started with the publicly avail-
able PSCFG decoder described in Venugopal et al
(2007) and extended it to efficiently use distributed
higher-order LMs under the Cube-Pruning decod-
ing method from Chiang (2007). These extensions
allow us to verify that the benefits of PSCFG mod-
els persist in the presence of large, powerful n-
gram LMs.
3.1 Asynchronous N-Gram LMs
As described in Brants et al (2007), using large
distributed LMs requires the decoder to perform
1147
asynchronous LM requests. Scoring n-grams un-
der this distributed LM involves queuing a set
of n-gram probability requests, then distributing
these requests in batches to dedicated LM servers,
and waiting for the resulting probabilities, before
accessing them to score chart items. In order
to reduce the number of such roundtrip requests
in the chart parsing decoding algorithm used for
PSCFGs, we batch all n-gram requests for each
cell.
This single batched request per cell paradigm
requires some adaptation of the Cube-Pruning al-
gorithm. Cube-Pruning is an early pruning tech-
nique used to limit the generation of low quality
chart items during decoding. The algorithm calls
for the generation of N-Best chart items at each
cell (across all rules spanning that cell). The n-
gram LM is used to score each generated item,
driving the N-Best search algorithm of Huang and
Chiang (2005) toward items that score well from
a translation model and language model perspec-
tive. In order to accomodate batched asynchronous
LM requests, we queue n-gram requests for the top
N*K chart items without the n-gram LM where
K=100. We then generate the top N chart items
with the n-gram LM once these probabilties are
available. Chart items attempted to be generated
during Cube-Pruning that would require LM prob-
abilities of n-grams not in the queued set are dis-
carded. While discarding these items could lead
to search errors, in practice they tend to be poorly
performing items that do not affect final translation
quality.
3.2 PSCFG Minimal-State Recombination
To effectively compare PSCFG approaches to
state-of-the-art phrase-based systems, we must be
able to use high order n-gram LMs during PSCFG
decoding, but as shown in Chiang (2007), the
number of chart items generated during decoding
grows exponentially in the the order of the n-gram
LM. Maintaining full n?1 word left and right his-
tories for each chart item (required to correctly se-
lect the argmax derivation when considering a n-
gram LM features) is prohibitive for n > 3.
We note however, that the full n ? 1 left and
right word histories are unneccesary to safely com-
pare two competing chart items. Rather, given
the sparsity of high order n-gram LMs, we only
need to consider those histories that can actually
be found in the n-gram LM. This allows signifi-
cantly more chart items to be recombined during
decoding, without additional search error. The n-
gram LM implementation described in Brants et
al. (2007) indicates when a particular n-gram is
not found in the model, and returns a shortened
n-gram or (?state?) that represents this shortened
condition. We use this state to identify the left and
right chart item histories, thus reducing the number
of equivalence classes per cell.
Following Venugopal et al (2007), we also cal-
culate an estimate for the quality of each chart
item?s left state based on the words represented
within the state (since we cannot know the tar-
get words that might precede this item in the fi-
nal translation). This estimate is only used during
Cube-Pruning to limit the number of chart items
generated.
The extensions above allows us to experiment
with the same order of n-gram LMs used in state-
of-the-art phrase based systems. While experi-
ments in this work include up to 5-gram mod-
els, we have succesfully run these PSCFG systems
with higher order n-gram LM models as well.
4 Experiments
4.1 Chinese-English and Arabic-English
We report experiments on three data configura-
tions. The first configuration (Full) uses all the
data (both bilingual and monolingual) data avail-
able for the NIST 2008 large track translation
task. The parallel training data comprises of 9.1M
sentence pairs (223M Arabic words, 236M En-
glish words) for Arabic-English and 15.4M sen-
tence pairs (295M Chinese Words, 336M English
words) for Chinese-English. This configuration
(for both Chinese-English and Arabic-English) in-
cludes three 5-gram LMs trained on the target side
of the parallel data (549M tokens, 448M 1..5-
grams), the LDC Gigaword corpus (3.7B tokens,
2.9B 1..5-grams) and the Web 1T 5-Gram Cor-
pus (1T tokens, 3.8B 1..5-grams). The second
configuration (TargetLM) uses a single language
model trained only on the target side of the paral-
lel training text to compare approaches with a rela-
tively weaker n-gram LM. The third configuration
is a simulation of a low data scenario (10%TM),
where only 10% of the bilingual training data is
used, with the language model from the TargetLM
configuration. Translation quality is automatically
evaluated by the IBM-BLEU metric (Papineni et
al., 2002) (case-sensitive, using length of the clos-
est reference translation) on the following publicly
1148
Ch.-En. System \%BLEU Dev (MT04) MT02 MT03 MT05 MT06 MT08 TstAvg
FULL
Phraseb. reo=4 37.5 38.0 38.9 36.5 32.2 26.2 34.4
Phraseb. reo=7 40.2 40.3 41.1 38.5 34.6 27.7 36.5
Phraseb. reo=12 41.3* 41.0 41.8 39.4 35.2 27.9 37.0
Hier. 41.6* 40.9 42.5 40.3 36.5 28.7 37.8
SAMT 41.9* 41.0 43.0 40.6 36.5 29.2 38.1
TARGET-LM
Phraseb. reo=4 35.9* 36.0 36.0 33.5 30.2 24.6 32.1
Phraseb. reo=7 38.3* 38.3 38.6 35.8 31.8 25.8 34.1
Phraseb. reo=12 39.0* 38.7 38.9 36.4 33.1 25.9 34.6
Hier. 38.1* 37.8 38.3 36.0 33.5 26.5 34.4
SAMT 39.9* 39.8 40.1 36.6 34.0 26.9 35.5
TARGET-LM, 10%TM
Phraseb. reo=12 36.4* 35.8 35.3 33.5 29.9 22.9 31.5
Hier. 36.4* 36.5 36.3 33.8 31.5 23.9 32.4
SAMT 36.5* 36.1 35.8 33.7 31.2 23.8 32.1
Ar.-En. System \%BLEU Dev (MT04) MT02 MT03 MT05 MT06 MT08 TstAvg
FULL
Phraseb. reo=4 51.7 64.3 54.5 57.8 45.9 44.2 53.3
Phraseb. reo=7 51.7* 64.5 54.3 58.2 45.9 44.0 53.4
Phraseb. reo=9 51.7 64.3 54.4 58.3 45.9 44.0 53.4
Hier. 52.0* 64.4 53.5 57.5 45.5 44.1 53.0
SAMT 52.5* 63.9 54.2 57.5 45.5 44.9 53.2
TARGET-LM
Phraseb. reo=4 49.3 61.3 51.4 53.0 42.6 40.2 49.7
Phraseb. reo=7 49.6* 61.5 51.9 53.2 42.8 40.1 49.9
Phraseb. reo=9 49.6 61.5 52.0 53.4 42.8 40.1 50.0
Hier. 49.1* 60.5 51.0 53.5 42.0 40.0 49.4
SAMT 48.3* 59.5 50.0 51.9 41.0 39.1 48.3
TARGET-LM, 10%TM
Phraseb. reo=7 47.7* 59.4 50.1 51.5 40.5 37.6 47.8
Hier. 46.7* 58.2 48.8 50.6 39.5 37.4 46.9
SAMT 45.9* 57.6 48.7 50.7 40.0 37.3 46.9
Table 1: Results (% case-sensitive IBM-BLEU) for Ch-En and Ar-En NIST-large. Dev. scores with * indicate that the param-
eters of the decoder were MER-tuned for this configuration and also used in the corresponding non-marked configurations.
available NIST test corpora: MT02, MT03, MT05,
MT06, MT08. We used the NIST MT04 corpus
as development set to train the model parameters
?. All of the systems were evaluated based on the
argmax decision rule. For the purposes of stable
comparison across multiple test sets, we addition-
ally report a TstAvg score which is the average of
all test set scores.
2
Table 1 shows results comparing phrase-based,
hierarchical and SAMT systems on the Chinese-
English and Arabic-English large-track NIST 2008
tasks. Our primary goal in Table 1 is to evaluate
the relative impact of the PSCFG methods above
the phrase-based approach, and to verify that these
improvements persist with the use of of large n-
gram LMs. We also show the impact of larger
reordering capability under the phrase-based ap-
proach, providing a fair comparison to the PSCFG
approaches.
2
We prefer this over taking the average over the aggregate
test data to avoid artificially generous BLEU scores due to
length penalty effects resulting from e.g. being too brief in a
hard test set but compensating this by over-generating in an
easy test set.
Chinese-to-English configurations: We see
consistent improvements moving from phrase-
based models to PSCFG models. This trend
holds in both LM configurations (Full and Tar-
getLM) as well as the 10%TM case, with the ex-
ception of the hierarchical system for TargetLM,
which performs slightly worse than the maximum-
reordering phrase-based system.
We vary the reordering limit ?reo? for the
phrase-based Full and TargetLM configurations
and see that Chinese-to-English translation re-
quires significant reordering to generate fluent
translations, as shown by the TstAvg difference be-
tween phrase-based reordering limited to 4 words
(34.4) and 12 words (37.0). Increasing the reorder-
ing limit beyond 12 did not yield further improve-
ment. Relative improvements over the most capa-
ble phrase-based model demonstrate that PSCFG
models are able to model reordering effects more
effectively than our phrase-based approach, even
in the presence of strong n-gram LMs (to aid the
distortion models) and comparable reordering con-
straints.
1149
Our results with hierarchical rules are consis-
tent with those reported in Chiang (2007), where
the hierarchical system uses a reordering limit of
10 (implicit in the maximum length of the initial
phrase pairs used for the construction of the rules,
and the decoder?s maximum source span length,
above which only the glue rule is applied) and is
compared to a phrase-based system with a reorder-
ing limit of 7.
Arabic-to-English configurations: Neither the
hierarchical nor the SAMT system show consis-
tent improvements over the phrase-based baseline,
outperforming the baseline on some test sets, but
underperforming on others. We believe this is due
to the lack of sufficient reordering phenomena be-
tween the two languages, as evident by the mini-
mal TstAvg improvement the phrase-based system
can achieve when increasing the reordering limit
from 4 words (53.3) to 9 words (53.4).
N-Gram LMs: The impact of using addi-
tional language models in configuration Full in-
stead of only a target-side LM (configuration Tar-
getLM) is clear; the phrase-based system improves
the TstAvg score from 34.6 to 37.0 for Chinese-
English and from 50.0 to 53.4 for Arabic-English.
Interestingly, the hierarchical system and SAMT
benefit from the additional LMs to the same extent,
and retain their relative improvement compared to
the phrase-based system for Chinese-English.
Expressiveness: In order to evaluate how much
of the improvement is due to the relatively weaker
expressiveness of the phrase-based model, we tried
to regenerate translations produced by the hierar-
chical system with the phrase-based decoder by
limiting the phrases applied during decoding to
those matching the desired translation (?forced
translation?). By forcing the phrase-based system
to follow decoding hypotheses consistent with a
specific target output, we can determine whether
the phrase-based system could possibly generate
this output. We used the Chinese-to-English NIST
MT06 test (1664 sentences) set for this experi-
ment. Out of the hierarchical system?s translations,
1466 (88%) were generable by the phrase-based
system. The relevant part of a sentence for which
the hierarchical translation was not phrase-based
generable is shown in Figure 1. The reason for the
failure to generate the translation is rather unspec-
tacular: While the hierarchical system is able to
delete the Chinese word meaning ?already? using
the rule spanning [27-28], which it learned by gen-
eralizing a training phrase pair in which ?already?
was not explicitly represented in the target side, the
phrase-based system has to account for this Chi-
nese word either directly or in a phrase combining
the previous word (Chinese for ?epidemic?) or fol-
lowing word (Chinese for ?outbreak?).
Out of the generable forced translations, 1221
(83%) had a higher cost than the phrase-based sys-
tem?s preferred output; in other words, the fact
that the phrase-based system does not prefer these
forced translations is mainly inherent in the model
rather than due to search errors.
These results indicate that a phrase-based sys-
tem with sufficiently powerful reordering features
and LM might be able to narrow the gap to a hier-
archical system.
System \ %BLEU Dev MT08
Phr.b. reo=4 12.8 18.1
Phr.b. reo=7 14.2 19.9
Phr.b. reo=10 14.8* 20.2
Phr.b. reo=12 15.0 20.1
Hier. 16.0* 22.1
SAMT 16.1* 22.6
Table 2: Translation quality (% case-sensitive IBM-BLEU)
for Urdu-English NIST-large. We mark dev. scores with *
to indicate that the parameters of the corresponding decoder
were MER-tuned for this configuration.
4.2 Urdu-English
Table 2 shows results comparing phrase-based,
hierarchical and SAMT system on the Urdu-
English large-track NIST 2008 task. Systems were
trained on the bilingual data provided by the NIST
competition (207K sentence pairs; 2.2M Urdu
words / 2.1M English words) and used a n-gram
LM estimated from the English side of the parallel
data (4M 1..5-grams). We see clear improvements
moving from phrase-based to hierarchy, and addi-
tional improvements from hierarchy to syntax. As
with Chinese-to-English, longer-distance reorder-
ing plays an important role when translating from
Urdu to English (the phrase-based system is able
to improve the test score from 18.1 to 20.2), and
PSCFGs seem to be able to take this reordering
better into account than the phrasal distance-based
and lexical-reordering models.
4.3 Are all rules important?
One might assume that only a few hierarchical
rules, expressing reordering phenomena based on
common words such as prepositions, are sufficient
to obtain the desired gain in translation quality
1150
Figure 1: Example from NIST MT06 for which the hierarchical system?s first best hypothesis was not generable by the phrase-
based system. The hierarchical system?s decoding parse tree contains the translation in its leaves in infix order (shaded). Each
non-leaf node denotes an applied PSCFG rule of the form: [Spanned-source-positions:Left-hand-side->source/target]
Ch.-En. System \%BLEU Dev (MT04) MT02 MT03 MT05 MT06 MT08 TstAvg
Phraseb. 41.3* 41.0 41.8 39.4 35.2 27.9 37.0
Hier. default (mincount=3) 41.6* 40.9 42.5 40.3 36.5 28.7 37.8
Hier. mincount=4 41.4 41.0 42.5 40.4 36.1 28.4 37.7
Hier. mincount=8 41.0 41.0 42.0 40.5 35.7 27.8 37.4
Hier. mincount=16 40.7 40.3 41.5 40.0 35.2 27.8 37.0
Hier. mincount=32 40.4 40.0 41.5 39.5 34.8 27.5 36.6
Hier. mincount=64 39.8 40.0 40.9 39.1 34.6 27.3 36.4
Hier. mincount=128 39.4 39.8 40.3 38.7 34.0 26.6 35.9
Hier. 1NT 40.1* 39.8 41.1 39.1 35.1 28.1 36.6
Urdu-En. System \%BLEU Dev MT08
Phraseb. 15.0* 20.1
Hier. default (mincount=2) 16.0* 22.1
Hier. mincount=4 15.7 22.0
Hier. mincount=8 15.4 21.5
Hier. mincount=16 15.1 21.3
Hier. mincount=32 14.9 20.7
Hier. mincount=64 14.6 20.1
Hier. mincount=128 14.4 19.6
Hier. 1NT 15.3* 20.8
Table 3: Translation quality (% case-sensitive IBM-BLEU) for Chinese-English and Urdu-English NIST-large when restricting
the hierarchical rules. We mark dev. scores with * to indicate that the parameters of the corresponding decoder were MER-tuned
for this configuration.
over a phrase-based system. Limiting the number
of rules used could reduce search errors caused by
spurious ambiguity during decoding. Potentially,
hierarchical rules based on rare phrases may not
be needed, as these phrase pairs can be substituted
into the nonterminal spots of more general and
more frequently encountered hierarchical rules.
As Table 3 shows, this is not the case. In
these experiments for Hier., we retained all non-
hierarchical rules (i.e., phrase pairs) but removed
hierarchical rules below a threshold ?mincount?.
Increasing mincount to 16 (Chinese-English) or 64
(Urdu-English), respectively, already deteriorates
performance to the level of the phrase-based sys-
tem, demonstrating that the highly parameterized
reordering model implicit in using more rules does
result in benefits. This immediate reduction in
translation quality when removing rare rules can
be explained by the following effect. Unlike in
a phrase-based system, where any phrase can po-
tentially be reordered, rules in the PSCFG must
compose to generate sub-translations that can be
reordered. Removing rare rules, even those that
are highly lexicalized and do not perform any re-
ordering (but still include nonterminal symbols),
increases the likelihood that the glue rule is applied
simply concatenating span translations without re-
ordering.
Removing hierarchical rules occurring at most
twice (Chinese-English) or once (Urdu-English),
respectively, did not impact performance, and led
to a significant decrease in rule table size and de-
coding speed.
We also investigate the relative impact of the
1151
rules with two nonterminals, over using rules with
a single nonterminal. Using two nonterminals al-
lows more lexically specific reordering patterns at
the cost of decoding runtime. Configuration ?Hier.
1NT? represents a hierarchical system in which
only rules with at most one nonterminal pair are
extracted instead of two as in Configuration ?Hier.
default?. The resulting test set score drop is more
than one BLEU point for both Chinese-to-English
and Urdu-to-English.
5 Conclusion
In this work we investigated the value of PSCFG
approaches built upon state-of-the-art phrase-
based systems. Our experiments show that PSCFG
approaches can yield substantial benefits for lan-
guage pairs that are sufficiently non-monotonic.
Suprisingly, the gap (or non-gap) between phrase-
based and PSCFG performance for a given lan-
guage pair seems to be consistent across small and
large data scenarios, and for weak and strong lan-
guage models alike. In sufficiently non-monotonic
languages, the relative improvements of phrase-
based systems persist when compared against a
state-of-the art phrase-based system that is capable
of equally long reordering operations modeled by
a lexicalized distortion model and a strong n-gram
language model. We hope that this work addresses
several of the important questions that the research
community has regarding the impact and value of
these PSCFG approaches.
Acknowledgments
We thank Richard Zens and the anonymous re-
viewers for their useful comments and sugges-
tions.
References
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proc. of EMNLP-
CoNLL.
Charniak, Eugene. 2000. A maximum entropy-
inspired parser. In Proc. of HLT/NAACL.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of ACL.
Chiang, David. 2007. Hierarchical phrase based trans-
lation. Computational Linguistics, 33(2):201?228.
DeNeefe, Steve, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proc. of EMNLP-
CoNLL.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT.
Huang, Liang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. of ACL.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT/NAACL.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2004. Pharaoh: A beam search decoder for phrase-
base statistical machine translation models. In Proc.
of AMTA.
Marcu, Daniel, Wei Wang, Abdessamad Echihabi,
and Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Language
Phrases. In Proc. of EMNLP.
Och, Franz and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Och, Franz Josef, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for statis-
tical machine translation. In Proc. of EMNLP.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In Proc. of ACL.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL.
Steedman, Mark. 1999. Alternative quantifier scope in
CCG. In Proc. of ACL.
Venugopal, Ashish, Andreas Zollmann, and Vogel
Stephan. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Proc. of
HLT/NAACL.
Zens, Richard and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Proc. of the Workshop on Statistical Ma-
chine Translation, HLT/NAACL.
Zollmann, Andreas and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proc. of the Workshop on Statistical Machine
Translation, HLT/NAACL.
1152
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 42?50, Prague, June 2007. c?2007 Association for Computational Linguistics
Improving Word Alignment with Bridge Languages
Shankar Kumar and Franz Och and Wolfgang Macherey
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043, U.S.A.
{shankarkumar,och,wmach}@google.com
Abstract
We describe an approach to improve
Statistical Machine Translation (SMT)
performance using multi-lingual, parallel,
sentence-aligned corpora in several bridge
languages. Our approach consists of a sim-
ple method for utilizing a bridge language to
create a word alignment system and a proce-
dure for combining word alignment systems
from multiple bridge languages. The final
translation is obtained by consensus de-
coding that combines hypotheses obtained
using all bridge language word alignments.
We present experiments showing that mul-
tilingual, parallel text in Spanish, French,
Russian, and Chinese can be utilized in
this framework to improve translation
performance on an Arabic-to-English task.
1 Introduction
Word Alignment of parallel texts forms a cru-
cial component of phrase-based statistical machine
translation systems. High quality word alignments
can yield more accurate phrase-pairs which improve
quality of a phrase-based SMT system (Och and
Ney, 2003; Fraser and Marcu, 2006b).
Much of the recent work in word alignment has
focussed on improving the word alignment quality
through better modeling (Och and Ney, 2003; Deng
and Byrne, 2005; Martin et al, 2005) or alternative
approaches to training (Fraser and Marcu, 2006b;
Moore, 2005; Ittycheriah and Roukos, 2005). In
this paper we explore a complementary approach to
improve word alignments using multi-lingual, par-
allel (or multi-parallel) corpora. Two works in the
literature are very relevant to our approach. Borin
(2000) describes a non-statistical approach where a
pivot alignment is used to combine direct translation
and indirect translation via a third language. Filali
and Bilmes (2005) present a multi-lingual extension
to the IBM/HMMmodels. Our current approach dif-
fers from this latter work in that we propose a sim-
ple framework to combine word alignments from
any underlying statistical alignment model without
the need for changing the structure of the model.
While both of the above papers focus on improv-
ing word alignment quality, we demonstrate that
our approach can yield improvements in transla-
tion performance. In particular, we aim to improve
an Arabic-to-English (Ar-En) system using multi-
parallel data from Spanish (Es), French (Fr), Rus-
sian (Ru) and Chinese (Zh). The parallel data in
these languages X ? {Es, Fr,Ru, Zh} is used to
generate word alignments between Arabic-X and
X-English. These alignments are then combined to
obtain multiple word alignments for Arabic-English
and the final translation systems.
The motivation for this approach is two-fold.
First, we believe that parallel corpora available
in several languages provide a better training ma-
terial for SMT systems relative to bilingual cor-
pora. Such multi-lingual parallel corpora are be-
coming widely available; examples include proceed-
ings of the United Nations in six languages (UN,
2006), European Parliament (EU, 2005; Koehn,
2003), JRC Acquis corpus (EU, 2007) and religious
texts (Resnik et al, 1997). Word alignment systems
42
trained on different language-pairs (e.g. French-
English versus Russian-English) make errors which
are somewhat orthogonal. In such cases, incorrect
alignment links between a sentence-pair can be cor-
rected when a translation in a third language is avail-
able. Thus it can help resolve errors in word align-
ment. We combine word alignments using several
bridge languages with the aim of correcting some
of the alignment errors. The second advantage of
this approach is that the word alignment from each
bridge language can be utilized to build a phrase-
based SMT system. This provides a diverse collec-
tion of translation hypotheses for MT system com-
bination (Bangalore et al, 2002; Sim et al, 2007;
Matusov et al, 2006; Macherey and Och, 2007). Fi-
nally, a side benefit of this paper is that it provides a
study that compares alignment qualities and BLEU
scores for models in different languages trained on
parallel text which is held identical across all lan-
guages.
We show that parallel corpora in multiple lan-
guages can be exploited to improve the translation
performance of a phrase-based translation system.
This paper gives specific recipes for using a bridge
language to construct a word alignment and for com-
bining word alignments produced by multiple statis-
tical alignment models.
The rest of this paper is organized as follows: Sec-
tion 2 gives an overview of our framework for gen-
erating word alignments in a single language-pair.
In Section 3, we describe how a bridge language
may be used for producing word alignments. In Sec-
tion 4, we describe a scheme to combine word align-
ments from several bridge languages. Section 5 de-
scribes our experimental setup and reports the align-
ment and translation performance. A final discus-
sion is presented in Section 6.
2 Word Alignment Framework
A statistical translation model (Brown et al, 1993;
Och and Ney, 2003) describes the relationship be-
tween a pair of sentences in the source and target
languages (f = fJ1 , e = e
I
1) using a translation
probability P (f |e). Alignment models introduce a
hidden alignment variable a = aJ1 to specify a map-
ping between source and target words; aj = i in-
dicates that the jth source word is linked to the ith
target word. Alignment models assign a probabil-
ity P (f ,a|e) to the source sentence and alignment
conditioned on the target sentence. The transla-
tion probability is related to the alignment model as:
P (f |e) =
?
a P?(f ,a|e), where ? is a set of param-
eters.
Given a sentence-pair (f , e), the most likely
(Viterbi) word alignment is found as (Brown et al,
1993): a? = argmaxa P (f ,a|e). An alternate cri-
terion is the Maximum A-Posteriori (MAP) frame-
work (Ge, 2004; Matusov et al, 2004). We use a
refinement of this technique.
Given any word alignment model, posterior prob-
abilities can be computed as (Brown et al, 1993)
P (aj = i|e, f) =
?
a
P (a|f , e)?(i, aj), (1)
where i ? {0, 1, ..., I}. The assignment aj = 0
corresponds to the NULL (empty) alignment. These
posterior probabilities form a matrix of size (I+1)?
J , where entries along each column sum to one.
The MAP alignment for each source position j ?
{1, 2, ..., J} is then computed as
aMAP (j) = argmax
i
P (aj = i|e, f). (2)
We note that these posterior probabilities can be
computed efficiently for some alignment models
such as the HMM (Vogel et al, 1996; Och and Ney,
2003), Models 1 and 2 (Brown et al, 1993).
In the next two sections, we describe how poste-
rior probabilities can be used to a) construct align-
ment systems from a bridge language, and b) merge
several alignment systems.
3 Constructing Word Alignment Using a
Bridge Language
We assume here that we have triples of sentences
that are translations of each other in languages F, E,
and the bridge language G: f = fJ1 , e = e
I
1,g =
gK1 . Our goal is to obtain posterior probability es-
timates for the sentence-pair in FE: (f , e) using the
posterior probability estimates for the sentence pairs
in FG: (f ,g) and GE: (g, e). The word alignments
between the above sentence-pairs are referred to as
aFE , aFG, and aGE respectively; the notation aFE
indicates that the alignment maps a position in F to
a position in E.
43
We first express the posterior probability as a sum
over all possible translations g in G and hidden
alignments aFG.
P (aFEj = i|e, f)
=
?
g
P (aFEj = i,g|e, f)
=
?
g,k
P (aFEj = i,g, a
FG
j = k|e, f)
=
?
g,k
{
P (g|e, f)P (aFGj = k|g, e, f)
?P (aFEj = i|a
FG
j = k,g, e, f)
}
(3)
We now make some assumptions to simplify the
above expression. First, there is exactly one trans-
lation g in bridge language G corresponding to the
sentence-pair f , e. Since aGE
aFGj
= i = aFEj , we can
express
P (aFEj = i|a
FG
j = k,g, f , e) = P (a
GE
k = i|g, e).
Finally, alignments in FG do not depend on E.
Under these assumptions, we arrive at the final ex-
pression for the posterior probability FE in terms of
posterior probabilities for GF and EG
P (aFEj = i|e, f) = (4)
K?
k=0
P (aFGj = k|g, f)P (a
GE
k = i|g, e)
The above expression states that the posterior prob-
ability matrix for FE can be obtained using a simple
matrix multiplication of posterior probability ma-
trices for GE and FG. In this multiplication, we
prepend a column to the GE matrix corresponding
to k = 0. This probability P (aGEk = i) when k = 0
is not assigned by the alignment model; we set it as
follows
P (aGEk = i|k = 0) =
{
 i = 0
1?
I i ? {1, 2, ..., I}
The parameter  controls the number of empty align-
ments; a higher value favors more empty alignments
and vice versa. In our experiments, we set  = 0.5.
4 Word Alignment Combination Using
Posterior Probabilities
We next show how Word Alignment Posterior Prob-
abilities can be used for combining multiple word
alignment systems. In our context, we use this pro-
cedure to combine word alignments produced using
multiple bridge languages.
Suppose we have translations in bridge languages
G1, G2, ..., GN , we can generate a posterior prob-
ability matrix for FE using each of the bridge lan-
guages. In addition, we can always generate a poste-
rior probability matrix for FE with the FE alignment
model directly without using any bridge language.
These N + 1 posterior matrices can be combined as
follows. Here, the variable B indicates the bridge
language. B ? {G0, G1, ..., GN}; G0 indicates the
case when no bridge language is used.
P (aFEj = i|e, f) (5)
=
N?
l=0
P (B = Gl, a
FE
j = i|e, f)
=
N?
l=0
P (B = Gl)P (a
FE
j = i|Gl, e, f),
where P (aFEj = i|Gl, j, e, f) is the posterior proba-
bility when bridge language B = Gl. The probabili-
ties P (B = Gl) sum to one over l ? {0, 1, 2, ..., N}
and represent the prior probability of bridge lan-
guage l. In our experiments, we use a uniform prior
P (B = Gl) = 1N+1 . Equation 5 provides us a way
to combine word alignment posterior probabilites
from multiple bridge languages. In our alignment
framework (Section 2), we first interpolate the pos-
terior probability matrices (Equation 5) and then ex-
tract the MAP word alignment (Equation 2) from the
resulting matrix.
5 Experiments
We now present experiments to demonstrate the ad-
vantages of using bridge languages. Our experi-
ments are performed in the open data track of the
NIST Arabic-to-English (A-E) machine translation
task 1.
5.1 Training and Test Data
Our approach to word alignment (Section 3) requires
aligned sentences in multiple languages. For train-
ing alignment models, we use the ODS United Na-
1http://www.nist.gov/speech/tests/mt/
44
Set # of Ar words (K) # of sentences
dev1 48.6 2007
dev2 11.4 498
test 37.8 1610
blind 36.5 1797
Table 1: Statistics for the test data.
tions parallel data (UN, 2006) which contains par-
liamentary documents from 1993 onwards in all six
official languages of the UN: Arabic (Ar), Chinese
(Zh), English (En), French (Fr), Russian (Ru), and
Spanish (Es).
We merge the NIST 2001-2005 Arabic-English
evaluation sets into a pool and randomly sam-
ple this collection to create two development sets
(dev1,dev2) and a test set (test) with 2007, 498, and
1610 sentences respectively. Our blind test (blind)
set is the NIST part of the NIST 06 evaluation set
consisting of 1797 sentences. The GALE portion of
the 06 evaluation set is not used in this paper. We re-
port results on the test and blind sets. Some statistics
computed on the test data are shown in Table 1.
5.2 Alignment Model Training
For training Arabic-English alignment models, we
use Chinese, French, Russian and Spanish as bridge
languages. We train a model for Ar-En and 4 mod-
els each for Ar-X and X-En, where X is the bridge
language. To obtain aligned sentences in these lan-
guage pairs, we train 9 sentence aligners. We then
train alignment models for all 9 language-pairs us-
ing a recipe consisting of 6 Model-1 iterations and
6 HMM iterations. Finally, Word Alignment Poste-
rior Probabilities are generated over the bitext. In
Table 2, we report the perplexities of the alignment
models for the translation directions where either
Arabic or English is predicted. There are 55M Ara-
bic tokens and 58M English tokens. We observe
that the alignment model using Spanish achieves the
lowest perplexity; this value is even lower than the
perplexity of the direct Arabic-English model. Per-
plexity is related to the hardness of the word align-
ment; the results suggest that bridge languages such
as Spanish make alignment task easier while others
do not. We stress that perplexity is not related to the
alignment or the translation performance.
Bridge Perplexity
Lang ? Ar ?En
None 113.8 26.1
Es 99.0 22.9
Fr 138.6 30.2
Ru 128.3 27.5
Zh 126.1 34.6
Table 2: Perplexities of the alignment models.
5.3 Bridge Language Word Alignments
Each of the 4 bridge languages is utilized for con-
structing a word alignment for Arabic-English. Us-
ing each bridge language X, we obtain Arabic-
English word alignments in both translation direc-
tions (AE and EA). The posterior matrix for AE is
obtained using AX and XE matrices while the EA
matrix is obtained from EX and XA matrices (Equa-
tion 4). The AE (EA) matrices from the bridge
languages are then interpolated with the AE (EA)
matrix obtained from the alignment model trained
directly on Arabic-English (Section 4). The MAP
word alignment for AE (EA) direction is computed
from the AE (EA) matrix. We next outline how these
word alignments are utilized in building a phrase-
based SMT system.
5.4 Phrase-based SMT system
Our phrase-based SMT system is similar to the
alignment template system described in Och and
Ney (2004). We first extract an inventory of phrase-
pairs up to length 7 from the union of AE and EA
word alignments. Various feature functions (Och
and Ney, 2004) are then computed over the entries
in the phrase table. 5-gram word language models
in English are trained on a variety of monolingual
corpora (Brants et al, 2007). Minimum Error Rate
Training (MERT) (Och, 2003) under BLEU crite-
rion is used to estimate 20 feature function weights
over the larger development set (dev1).
Translation is performed using a standard dy-
namic programming beam-search decoder (Och and
Ney, 2004). Decoding is done in two passes. An ini-
tial list of 1000-best hypotheses is generated by the
decoder. This list is then rescored using Minimum
Bayes-Risk (MBR) decoding (Kumar and Byrne,
2004). The MBR scaling parameter is tuned on the
smaller development set (dev2).
45
Bridge Metrics(%)
Language AE EA
Prec Rec AER Prec Rec AER
None 74.1 73.9 26.0 67.3 57.7 37.9
Es 61.7 56.3 41.1 50.0 40.2 55.4
Fr 52.9 48.0 49.7 42.3 33.6 62.5
Ru 57.4 50.8 46.1 40.2 31.6 64.6
Zh 44.3 39.3 58.3 39.7 29.9 65.9
AC1 70.0 65.0 32.6 56.8 46.4 48.9
Table 3: Alignment Performance with Bridge Lan-
guages
5.5 Alignment Results
We first report alignment performance (Table 3) of
the alignment models obtained using the bridge lan-
guages. Alignment results are reported in terms
of Precision (Prec), Recall (Rec) and Alignment
Error Rate (AER). We report these numbers on
a 94-sentence test set with translations in all six
languages and human word alignments in Arabic-
English. Our human word alignments do not dis-
tinguish between Sure and Probable links (Och and
Ney, 2003).
In these experiments, we first identify the com-
mon subset of sentences which have translations in
all six languages. Each of the 9 alignment models
is then trained on this subset. We report Alignment
performance in both translation directions: Arabic-
to-English (AE) and English-to-Arabic (EA). The
first row (None) gives the results when no bridge
language is used.
Among the bridge languages, Spanish gives the
best alignment for Arabic-English while Chinese re-
sults in the worst. This might be related to how dif-
ferent the bridge language is relative to either En-
glish or Arabic. The last row (AC1) shows the per-
formance of the alignment obtained by combining
None/Es/Fr/Ru/Zh alignments. This alignment out-
performs all bridge alignments but is weaker than
the alignment without any bridge language. Our
hypothesis is that a good choice of interpolation
weights (Equation 5) would reduce AER of the AC1
combination. However, we did not investigate these
choices in this paper. We report alignment error rates
here to give the readers an idea of the vastly differ-
ent alignment performance using each of the bridge
languages.
5.6 Translation Results
We now report translation performance of our tech-
niques. We measure performance using the NIST
implementation of case sensitive BLEU-4 on true-
cased translations. We observed in experiments
not reported here that results are almost identical
with/without Minimum Error Rate Training ; we
therefore report the results without the training. We
note that the blind set is the NIST subset of the 2006
NIST evaluation set. The systems reported here are
for the Unlimited Data Track in Arabic-to-English
and obtain competitive performance relative to the
results reported on the NIST official results page 2
We present three sets of experiments. In Table 4,
we describe the first set where all 9 alignment mod-
els are trained on nearly the same set of sentences
(1.9M sentences, 57.5M words in English). This
makes the alignment models in all bridge languages
comparable. In the first rowmarked None, we do not
use a bridge language. Instead, an Ar-En alignment
model is trained directly on the set of sentence pairs.
The next four rows give the performance of align-
ment models trained using the bridge languages Es,
Fr, Ru and Zh respectively. For each language, we
use the procedure (Section 3) to obtain the posterior
probability matrix for Arabic-English from Arabic-
X and X-English matrices. The row AC1 refers to
alignment combination using interpolation of poste-
rior probabilities described in Section 4. We com-
bine posterior probability matrices from the systems
in the first four rows: None, Es, Ru and Zh. We
exclude the Zh system from the AC1 combination
because it is found to degrade the translation perfor-
mance by 0.2 points on the test set.
In the final six rows of Table 4, we show the per-
formance of a consensus decoding technique that
produces a single output hypothesis by combin-
ing translation hypotheses from multiple systems;
this is an MBR-like candidate selection procedure
based on BLEU correlation matrices and is de-
scribed in Macherey and Och (2007). We first report
performance of the consensus output by combining
None systems with/without MERT. Each of the fol-
lowing rows provides the results from consensus de-
coding for adding an extra system both with/without
MERT. Thus, the final row (TC1) combines transla-
2
http://www.nist.gov/speech/tests/mt/mt06eval official results.html
46
tions from 12 systems: None, Es, Fr, Ru, Zh, AC1
with/without MERT. All entries marked with an as-
terisk are better than the None baseline with 95%
statistical significance computed using paired boot-
strap resampling (Koehn, 2004).
35 40 45 50 55 60 65 7037
37.5
38
38.5
39
39.5
40
40.5
None
Es
Fr
Ru
Zh
AC1
100?AER(%)
BLE
U(%
)
Figure 1: 100-AER (%) vs. BLEU(%) on the blind
set for 6 systems from Table 3.
Figure 1 shows the plot between 100-AER% (av-
erage of EA/AE directions) and BLEU for the six
systems in Table 3. We observe that AER is loosely
correlated to BLEU (? = 0.81) though the re-
lation is weak, as observed earlier by Fraser and
Marcu (2006a). Among the bridge languages, Span-
ish gives the lowest AER/highest BLEU while Chi-
nese results in highest AER/lowest BLEU. We can
conclude that Spanish is closest to Arabic/English
while Chinese is the farthest. All the bridge lan-
guages yield lower BLEU/higher AER relative to the
No-Bridge baseline. Therefore, our estimate of the
posterior probability (Equation 4) is always worse
than the posterior probability obtained using a di-
rect model. The alignment combination (AC1) be-
haves differently from other bridge systems in that it
gives a higher AER and a higher BLEU relative to
None baseline. We hypothesize that AC1 is differ-
ent from the bridge language systems since it arises
from a different process: interpolation with the di-
rect model (None).
Both system combination techniques give im-
provements relative to None baseline: alignment
combination AC1 gives a small gain (0.2 points)
while the consensus translation TC1 results in a
larger improvement (0.8 points). The last 4 rows
of the table show that the performance of the hy-
pothesis consensus steadily increases as systems get
added to the None baseline. This shows that while
bridge language systems are weaker than the di-
rect model, they can provide complementary sources
of evidence. To further validate this hypothesis,
we compute inter-system BLEU scores between
None/es and all the systems in Table 5. We observe
that the baseline (None) is very dissimilar from the
rest of the systems. We hypothesize that the baseline
system has an alignment derived from a real align-
ment model while the rest of the bridge systems are
derived using matrix multiplication. The low inter-
system BLEU scores show that the bridge systems
provide diverse hypotheses relative to the baseline
and therefore contribute to gains in consensus de-
coding.
Bridge Lang # Msents BLEU (%)
test blind
None 1.9 52.1 40.1
Es 1.9 51.7 39.8
Fr 1.9 51.2 39.5
Ru 1.9 50.4 38.7
Zh 1.9 48.4 37.1
AC1 1.9 52.1 40.3
Hypothesis Consensus
None 1.9 51.9 39.8
+Es 1.9 52.2 40.0
+Fr 1.9 52.4? 40.5?
+Ru 1.9 52.8? 40.7?
+Zh 1.9 52.6? 40.6?
+AC1 = TC1 1.9 53.0? 40.9?
Table 4: Translation Experiments for Set 1; Results
are reported on the test and blind set: (NIST portion
of 2006 NIST eval set).
Ref None es fr ru zh AC1
None 100.0 60.0 59.8 59.7 59.5 58.7
es 59.6 100.0 79.9 69.3 67.4 70.5
Table 5: Inter-system BLEU scores (%) between
None/es and all systems in Table 3.
To gain some insight about how the bridge sys-
tems help in Table 4, we present an example in Ta-
ble 6. The example shows the consensus Transla-
tions and the 12 input translations for the consensus
decoding. The example suggests that the inputs to
the consensus decoding exhibit diversity.
Table 7 reports the second and third sets of ex-
periments. For both sets, we first train each bridge
language system X using all aligned sentences avail-
47
System MERT Hypothesis
None N The President of the National Conference Visit Iraqi Kurdistan Iraqi
None Y President of the Iraqi National Conference of Iraqi Kurdistan Visit
Es N President of the Iraqi National Congress to Visit Iraqi Kurdistan
Es Y President of the Iraqi National Congress to Visit Iraqi Kurdistan
Fr N President of the Iraqi National Conference Visits Iraqi Kurdistan
Fr Y Chairman of the Iraqi National Conference Visits Iraqi Kurdistan
Ru N The Chairman of the Iraqi National Conference Visits Iraqi Kurdistan
Ru Y Chairman of the Iraqi National Conference Visit the Iraqi Kurdistan
Zh N The Chairman of the Iraqi National Conference Visits Iraqi Kurdistan
Zh Y The Chairman of the Iraqi National Conference Visit Iraqi Kurdistan
AC1 N President of the Iraqi National Congress to Visit Iraqi Kurdistan
AC1 Y Chairman of the Iraqi National Congress to Visit Iraqi Kurdistan
TC1 - The Chairman of the Iraqi National Conference Visits Iraqi Kurdistan
Ref - Head of Iraqi National Congress Visits Iraqi Kurdistan
Table 6: An example showing the Consensus Translation (TC1) and the 12 inputs for consensus decoding.
The final row shows the reference translation.
able in Ar, En and X. In Set 2, the first row (Union)
is an alignment model trained on all sentence-pairs
in Ar-En which are available in at least one bridge
language X. AC2 refers to alignment combination
using bridge languages Es/Fr/Ru and Union. TC2
refers to the translation combination from 12 sys-
tems: Es/Fr/Ru/Zh/Union/AC2 with/without Mini-
mum Error Rate training. Finally, the goal in Set 3
(last 3 rows) is to improve the best Arabic-English
system that can be built using all available sen-
tence pairs from the UN corpus. The first row
(Direct) gives the performance of this Ar-En sys-
tem; AC3 refers to alignment combination using
Es/Fr/Ru and Direct. TC3 merges translations from
Es/Fr/Ru/Zh/Direct/AC3. All entries marked with
an asterisk (plus) are better than the Union (Direct)
baseline with 95% statistical significance computed
using paired bootstrap resampling (Koehn, 2004).
The motivation behind Sets 2 and 3 is to train all
bridge language systems on as much bitext as possi-
ble. As a consequence, these systems give better re-
sults than the corresponding systems in Table 4. The
Union system outperforms None by 1.7/1.4 BLEU
points and provides a better baseline. We show un-
der this scenario that system combination techniques
AC2 and TC2 can still give smaller improvements
(0.3/0.5 and 1.0/0.7 points) relative to this baseline.
As mentioned earlier, our approach requires
sentence-aligned corpora. In our experiments, we
use a single sentence aligner for each language pair
(total of 9 aligners). Since these aligners make inde-
pendent decisions on sentence boundaries, we end
up with a smaller pool of sentences (1.9M) that is
common across all language pairs. In contrast, a
sentence aligner that makes simultaneous decisions
in multiple languages would result in a larger set of
common sentence pairs (close to 7M sentence pairs).
Simard (1999) describes a sentence aligner of this
type that improves alignment on a trilingual paral-
lel text. Since we do not currently have access to
such an aligner, we simulate that situation with Sets
2 and 3: AC2/AC3 do not insist that a sentence-pair
be present in all input word alignments. We note that
Set 2 is a data scenario that falls between Sets 1 and
3.
Set 3 provides the best baseline for Arabic-
English based on the UN data by training on
all parallel sentence-pairs. In this situation, sys-
tem combination with bridge languages (AC3/TC3)
gives reasonable improvements in BLEU on the test
set (0.4/1.0 points) but only modest improvements
(0.1/0.4 points) on the blind set. However, this does
show that the bridge systems continue to provide or-
thogonal evidence at different operating points.
6 Discussion
We have described a simple approach to improve
word alignments using bridge languages. This in-
cludes two components: a matrix multiplication to
assemble a posterior probability matrix for the de-
sired language-pair FE using a pair of posterior
probability matrices FG and GE relative to a bridge
language G. The second component is a recipe for
combining word alignment systems by linearly in-
48
Bridge Lang # Msents BLEU (%)
test blind
Es 4.7 53.7 40.9
Fr 4.7 53.2 40.7
Ru 4.5 52.4 39.9
Zh 3.4 49.7 37.9
Set 2
Union 7.2 53.8 41.5
AC2 7.2 54.1 42.0?
TC2 - 54.8? 42.2?
Set 3
Direct 7.0 53.9 42.2
AC3 9.0 54.3+ 42.3
TC3 - 54.9+ 42.6+
Table 7: Translation performance for Sets 2 and 3 on
test and blind:NIST portion of 2006 NIST eval set.
terpolating posterior probability matrices from dif-
ferent sources. In our case, these sources are multi-
ple bridge languages. However, this method is more
generally applicable for combining posterior matri-
ces from different alignment models such as HMM
and Model-4. Such an approach contrasts with the
log-linear HMM/Model-4 combination proposed by
Och and Ney (2003).
There has been recent work by Ayan and Dorr
(2006) on combining word alignments from differ-
ent alignment systems; this paper describes a maxi-
mum entropy framework for this combination. Their
approach operates at the level of the alignment links
and uses maximum entropy to decide whether or
not to include an alignment link in the final out-
put. In contrast, we use posterior probabilities as the
interface between different alignment models. An-
other difference is that this maxent framework re-
quires human word aligned data for training feature
weights. We do not require any human word aligned
data to train our combiner.
Another advantage of our approach is that it is
based on word alignment posterior probability ma-
trices that can be generated by any underlying align-
ment model. Therefore, this method can be used to
combine word alignments generated by fairly dis-
similar word alignment systems as long as the sys-
tems can produce posterior probabilities.
Bridge languages have been used by NLP re-
searchers as a means to induce translation lexicons
between distant languages without the need for par-
allel corpora (Schafer and Yarowsky, 2002; Mann
and Yarowsky, 2001). Our current approach differs
from these efforts in that we use bridge languages to
improve word alignment quality between sentence
pairs. Furthermore, we do not use linguistic insight
to identify bridge languages. In our framework, a
good bridge language is one that provides the best
translation performance using the posterior matrix
multiplication. Our experiments show that Spanish
is a better bridge language relative to Chinese for
Arabic-to-English translation. We speculate that if
our approach was carried out on a data set with hun-
dreds of languages, we might be able to automati-
cally identify language families.
A downside of our approach is the requirement
for exact sentence-aligned parallel data. Except for
a few corpora such as UN, European Parliament etc,
such a resource is hard to find. One solution is to cre-
ate such parallel data by automatic translation and
then retaining reliable translations by using confi-
dence metrics (Ueffing and Ney, 2005).
Our approach to using bridge languages is ex-
tremely simple. Despite its simplicity, the system
combination gives improvements in alignment and
translation performance. In future work, we will
consider several extensions to this framework that
lead to more powerful system combination strategies
using multiple bridge languages. We recall that the
present approach trains bridge systems (e.g. Arabic-
to-French, French-to-English) until the alignment
stage and then uses these for constructing Arabic-
to-English word alignment. An alternate scenario
would be to build phrase-based SMT systems for
Arabic-to-Spanish and Spanish-to-English, and then
obtain Arabic-to-English translation by first trans-
lating from Arabic into Spanish and then Spanish
into English. Such end-to-end bridge systems may
lead to an even more diverse pool of hypotheses that
could further improve system combination.
References
N. Ayan and B. Dorr. 2006. A maximum entropy
approach to combining word alignments. In HLT-
NAACL, New York, New York.
S. Bangalore, V. Murdock, and G. Riccardi. 2002. Boot-
strapping bilingual data using consensus translation
for a multilingual instant messaging system. In COL-
ING, Taipei, Taiwan.
L. Borin. 2000. You?ll take the high road and I?ll take the
49
low road: Using a third language to improve bilingual
word alignment. In COLING, pages 97?103, Saar-
brucken, Germany.
T. Brants, A. Popat, P. Xu, F. Och, and J. Dean. 2007.
Large language models in machine translation. In
EMNLP, Prague, Czech Republic.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Y. Deng and W. Byrne. 2005. HMM word and
phrase alignment for statistical machine translation. In
EMNLP, Vancouver, Canada.
EU, 2005. European Parliament Proceedings.
http://www.europarl.europa.eu.
EU, 2007. JRC Acquis Corpus. http://langtech.jrc.it/JRC-
Acquis.html.
K. Filali and J. Bilmes. 2005. Leveraging multiple lan-
guages to improve statistical mt word alignments. In
IEEE Workshop on Automatic Speech Recognition and
Understanding, San Juan, Puerto Rico.
A. Fraser and D. Marcu. 2006a. Measuring word align-
ment quality for statistical machine translation. Tech-
nical Report ISI-TR-616, ISI/University of Southern
California.
A. Fraser and D. Marcu. 2006b. Semi-supervised train-
ing for statistical word alignment. In ACL, pages 769?
776, Sydney, Australia.
N. Ge. 2004. Improvements in word alignments. In
Presentation given at DARPA/TIDES workshop.
A. Ittycheriah and S. Roukos. 2005. A maximum en-
tropy word aligner for arabic-english machine transla-
tion. In EMNLP, Vancouver, Canada.
P. Koehn, 2003. European Parlia-
ment Proceedings, Sentence Aligned.
http://people.csail.mit.edu/koehn/publications/europarl/.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In EMNLP, Barcelona, Spain.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In HLT-
NAACL, pages 169?176, Boston, MA, USA.
W. Macherey and F. Och. 2007. An empirical study on
computing consensus translations from multiple ma-
chine translation systems. In EMNLP, Prague, Czech
Republic.
G. Mann and D. Yarowsky. 2001. Multipath translation
lexicon induction via bridge languages. In NAACL,
Pittsburgh, PA, USA.
J. Martin, R. Mihalcea, and T. Pedersen. 2005. Word
alignment for languages with scarce resources. In ACL
Workshop on Building and Using Parallel Texts, pages
65?74, Ann Arbor, MI, USA.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric word
alignments for statistical machine translation. InCOL-
ING, Geneva, Switzerland.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine transla-
tion systems using enhanced hypotheses alignment. In
EACL, Trento, Italy.
R. C. Moore. 2005. A discriminative framework for
bilingual word alignment. In EMNLP, Vancouver,
Canada.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19 ? 51.
F. Och and H. Ney. 2004. The alignment template ap-
proach to statistical machine translation. Computa-
tional Linguistics, 30(4):417 ? 449.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL, Sapporo, Japan.
P. Resnik, M. Olsen, and M. Diab. 1997. Creating a
parallel corpus from the book of 2000 tongues. In
Text Encoding Initiative 10th Anniversary User Con-
ference, Providence, RI, USA.
C. Schafer and D. Yarowsky. 2002. Inducing translation
lexicons via diverse similarity measures and bridge
languages. In CoNLL, Taipei, Taiwan.
K. C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, and P. C.
Woodland. 2007. Consensus network decoding for
statistical machine translation system combination. In
IEEE International Conference on Acoustics, Speech,
and Signal Processing, Honolulu, HI, USA.
M. Simard. 1999. Text translation alignment: Three lan-
guages are better than two. In EMNLP-VLC, College
Park, MD, USA.
N. Ueffing and H. Ney. 2005. Word-level confidence
estimation for machine translation using phrase-based
translation models. In EMNLP, pages 763 ? 770, Van-
couver, Canada.
UN, 2006. ODS UN Parallel Corpus. http://ods.un.org/.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based
word alignment in statistical translation. In COLING,
pages 836?841, Copenhagen, Denmark.
50
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 858?867, Prague, June 2007. c?2007 Association for Computational Linguistics
Large Language Models in Machine Translation
Thorsten Brants Ashok C. Popat Peng Xu Franz J. Och Jeffrey Dean
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94303, USA
{brants,popat,xp,och,jeff}@google.com
Abstract
This paper reports on the benefits of large-
scale statistical language modeling in ma-
chine translation. A distributed infrastruc-
ture is proposed which we use to train on
up to 2 trillion tokens, resulting in language
models having up to 300 billion n-grams. It
is capable of providing smoothed probabil-
ities for fast, single-pass decoding. We in-
troduce a new smoothing method, dubbed
Stupid Backoff, that is inexpensive to train
on large data sets and approaches the quality
of Kneser-Ney Smoothing as the amount of
training data increases.
1 Introduction
Given a source-language (e.g., French) sentence f ,
the problem of machine translation is to automati-
cally produce a target-language (e.g., English) trans-
lation e?. The mathematics of the problem were for-
malized by (Brown et al, 1993), and re-formulated
by (Och and Ney, 2004) in terms of the optimization
e? = arg max
e
M
?
m=1
?mhm(e, f) (1)
where {hm(e, f)} is a set of M feature functions and
{?m} a set of weights. One or more feature func-
tions may be of the form h(e, f) = h(e), in which
case it is referred to as a language model.
We focus on n-gram language models, which are
trained on unlabeled monolingual text. As a general
rule, more data tends to yield better language mod-
els. Questions that arise in this context include: (1)
How might one build a language model that allows
scaling to very large amounts of training data? (2)
How much does translation performance improve as
the size of the language model increases? (3) Is there
a point of diminishing returns in performance as a
function of language model size?
This paper proposes one possible answer to the
first question, explores the second by providing
learning curves in the context of a particular statis-
tical machine translation system, and hints that the
third may yet be some time in answering. In particu-
lar, it proposes a distributed language model training
and deployment infrastructure, which allows direct
and efficient integration into the hypothesis-search
algorithm rather than a follow-on re-scoring phase.
While it is generally recognized that two-pass de-
coding can be very effective in practice, single-pass
decoding remains conceptually attractive because it
eliminates a source of potential information loss.
2 N -gram Language Models
Traditionally, statistical language models have been
designed to assign probabilities to strings of words
(or tokens, which may include punctuation, etc.).
Let wL1 = (w1, . . . , wL) denote a string of L tokens
over a fixed vocabulary. An n-gram language model
assigns a probability to wL1 according to
P (wL1 ) =
L
?
i=1
P (wi|wi?11 ) ?
L
?
i=1
P? (wi|wi?1i?n+1)
(2)
where the approximation reflects a Markov assump-
tion that only the most recent n ? 1 tokens are rele-
vant when predicting the next word.
858
For any substring wji of wL1 , let f(w
j
i ) denote the
frequency of occurrence of that substring in another
given, fixed, usually very long target-language string
called the training data. The maximum-likelihood
(ML) probability estimates for the n-grams are given
by their relative frequencies
r(wi|wi?1i?n+1) =
f(wii?n+1)
f(wi?1i?n+1)
. (3)
While intuitively appealing, Eq. (3) is problematic
because the denominator and / or numerator might
be zero, leading to inaccurate or undefined probabil-
ity estimates. This is termed the sparse data prob-
lem. For this reason, the ML estimate must be mod-
ified for use in practice; see (Goodman, 2001) for a
discussion of n-gram models and smoothing.
In principle, the predictive accuracy of the lan-
guage model can be improved by increasing the or-
der of the n-gram. However, doing so further exac-
erbates the sparse data problem. The present work
addresses the challenges of processing an amount
of training data sufficient for higher-order n-gram
models and of storing and managing the resulting
values for efficient use by the decoder.
3 Related Work on Distributed Language
Models
The topic of large, distributed language models is
relatively new. Recently a two-pass approach has
been proposed (Zhang et al, 2006), wherein a lower-
order n-gram is used in a hypothesis-generation
phase, then later the K-best of these hypotheses are
re-scored using a large-scale distributed language
model. The resulting translation performance was
shown to improve appreciably over the hypothesis
deemed best by the first-stage system. The amount
of data used was 3 billion words.
More recently, a large-scale distributed language
model has been proposed in the contexts of speech
recognition and machine translation (Emami et al,
2007). The underlying architecture is similar to
(Zhang et al, 2006). The difference is that they in-
tegrate the distributed language model into their ma-
chine translation decoder. However, they don?t re-
port details of the integration or the efficiency of the
approach. The largest amount of data used in the
experiments is 4 billion words.
Both approaches differ from ours in that they store
corpora in suffix arrays, one sub-corpus per worker,
and serve raw counts. This implies that all work-
ers need to be contacted for each n-gram request.
In our approach, smoothed probabilities are stored
and served, resulting in exactly one worker being
contacted per n-gram for simple smoothing tech-
niques, and in exactly two workers for smoothing
techniques that require context-dependent backoff.
Furthermore, suffix arrays require on the order of 8
bytes per token. Directly storing 5-grams is more
efficient (see Section 7.2) and allows applying count
cutoffs, further reducing the size of the model.
4 Stupid Backoff
State-of-the-art smoothing uses variations of con-
text-dependent backoff with the following scheme:
P (wi|wi?1i?k+1) =
{
?(wii?k+1) if (wii?k+1) is found
?(wi?1i?k+1)P (wii?k+2) otherwise
(4)
where ?(?) are pre-computed and stored probabili-
ties, and ?(?) are back-off weights. As examples,
Kneser-Ney Smoothing (Kneser and Ney, 1995),
Katz Backoff (Katz, 1987) and linear interpola-
tion (Jelinek and Mercer, 1980) can be expressed in
this scheme (Chen and Goodman, 1998). The recur-
sion ends at either unigrams or at the uniform distri-
bution for zero-grams.
We introduce a similar but simpler scheme,
named Stupid Backoff 1 , that does not generate nor-
malized probabilities. The main difference is that
we don?t apply any discounting and instead directly
use the relative frequencies (S is used instead of
P to emphasize that these are not probabilities but
scores):
S(wi|wi?1i?k+1) =
?
?
?
?
?
f(wii?k+1)
f(wi?1i?k+1)
if f(wii?k+1) > 0
?S(wi|wi?1i?k+2) otherwise
(5)
1The name originated at a time when we thought that such
a simple scheme cannot possibly be good. Our view of the
scheme changed, but the name stuck.
859
In general, the backoff factor ? may be made to de-
pend on k. Here, a single value is used and heuris-
tically set to ? = 0.4 in all our experiments2 . The
recursion ends at unigrams:
S(wi) =
f(wi)
N (6)
with N being the size of the training corpus.
Stupid Backoff is inexpensive to calculate in a dis-
tributed environment while approaching the quality
of Kneser-Ney smoothing for large amounts of data.
The lack of normalization in Eq. (5) does not affect
the functioning of the language model in the present
setting, as Eq. (1) depends on relative rather than ab-
solute feature-function values.
5 Distributed Training
We use the MapReduce programming model (Dean
and Ghemawat, 2004) to train on terabytes of data
and to generate terabytes of language models. In this
programming model, a user-specified map function
processes an input key/value pair to generate a set of
intermediate key/value pairs, and a reduce function
aggregates all intermediate values associated with
the same key. Typically, multiple map tasks oper-
ate independently on different machines and on dif-
ferent parts of the input data. Similarly, multiple re-
duce tasks operate independently on a fraction of the
intermediate data, which is partitioned according to
the intermediate keys to ensure that the same reducer
sees all values for a given key. For additional details,
such as communication among machines, data struc-
tures and application examples, the reader is referred
to (Dean and Ghemawat, 2004).
Our system generates language models in three
main steps, as described in the following sections.
5.1 Vocabulary Generation
Vocabulary generation determines a mapping of
terms to integer IDs, so n-grams can be stored us-
ing IDs. This allows better compression than the
original terms. We assign IDs according to term fre-
quency, with frequent terms receiving small IDs for
efficient variable-length encoding. All words that
2The value of 0.4 was chosen empirically based on good
results in earlier experiments. Using multiple values depending
on the n-gram order slightly improves results.
occur less often than a pre-determined threshold are
mapped to a special id marking the unknown word.
The vocabulary generation map function reads
training text as input. Keys are irrelevant; values are
text. It emits intermediate data where keys are terms
and values are their counts in the current section
of the text. A sharding function determines which
shard (chunk of data in the MapReduce framework)
the pair is sent to. This ensures that all pairs with
the same key are sent to the same shard. The re-
duce function receives all pairs that share the same
key and sums up the counts. Simplified, the map,
sharding and reduce functions do the following:
Map(string key, string value) {
// key=docid, ignored; value=document
array words = Tokenize(value);
hash_map<string, int> histo;
for i = 1 .. #words
histo[words[i]]++;
for iter in histo
Emit(iter.first, iter.second);
}
int ShardForKey(string key, int nshards) {
return Hash(key) % nshards;
}
Reduce(string key, iterator values) {
// key=term; values=counts
int sum = 0;
for each v in values
sum += ParseInt(v);
Emit(AsString(sum));
}
Note that the Reduce function emits only the aggre-
gated value. The output key is the same as the inter-
mediate key and automatically written by MapRe-
duce. The computation of counts in the map func-
tion is a minor optimization over the alternative of
simply emitting a count of one for each tokenized
word in the array. Figure 1 shows an example for
3 input documents and 2 reduce shards. Which re-
ducer a particular term is sent to is determined by a
hash function, indicated by text color. The exact par-
titioning of the keys is irrelevant; important is that all
pairs with the same key are sent to the same reducer.
5.2 Generation of n-Grams
The process of n-gram generation is similar to vo-
cabulary generation. The main differences are that
now words are converted to IDs, and we emit n-
grams up to some maximum order instead of single
860
Figure 1: Distributed vocabulary generation.
words. A simplified map function does the follow-
ing:
Map(string key, string value) {
// key=docid, ignored; value=document
array ids = ToIds(Tokenize(value));
for i = 1 .. #ids
for j = 0 .. maxorder-1
Emit(ids[i-j .. i], "1");
}
Again, one may optimize the Map function by first
aggregating counts over some section of the data and
then emit the aggregated counts instead of emitting
?1? each time an n-gram is encountered.
The reduce function is the same as for vocabu-
lary generation. The subsequent step of language
model generation will calculate relative frequencies
r(wi|wi?1i?k+1) (see Eq. 3). In order to make that step
efficient we use a sharding function that places the
values needed for the numerator and denominator
into the same shard.
Computing a hash function on just the first words
of n-grams achieves this goal. The required n-
grams wii?n+1 and wi?1i?n+1 always share the same
first word wi?n+1, except for unigrams. For that we
need to communicate the total count N to all shards.
Unfortunately, sharding based on the first word
only may make the shards very imbalanced. Some
terms can be found at the beginning of a huge num-
ber of n-grams, e.g. stopwords, some punctuation
marks, or the beginning-of-sentence marker. As an
example, the shard receiving n-grams starting with
the beginning-of-sentence marker tends to be several
times the average size. Making the shards evenly
sized is desirable because the total runtime of the
process is determined by the largest shard.
The shards are made more balanced by hashing
based on the first two words:
int ShardForKey(string key, int nshards) {
string prefix = FirstTwoWords(key);
return Hash(prefix) % nshards;
}
This requires redundantly storing unigram counts in
all shards in order to be able to calculate relative fre-
quencies within shards. That is a relatively small
amount of information (a few million entries, com-
pared to up to hundreds of billions of n-grams).
5.3 Language Model Generation
The input to the language model generation step is
the output of the n-gram generation step: n-grams
and their counts. All information necessary to calcu-
late relative frequencies is available within individ-
ual shards because of the sharding function. That is
everything we need to generate models with Stupid
Backoff. More complex smoothing methods require
additional steps (see below).
Backoff operations are needed when the full n-
gram is not found. If r(wi|wi?1i?n+1) is not found,
then we will successively look for r(wi|wi?1i?n+2),
r(wi|wi?1i?n+3), etc. The language model generation
step shards n-grams on their last two words (with
unigrams duplicated), so all backoff operations can
be done within the same shard (note that the required
n-grams all share the same last word wi).
5.4 Other Smoothing Methods
State-of-the-art techniques like Kneser-Ney
Smoothing or Katz Backoff require additional,
more expensive steps. At runtime, the client needs
to additionally request up to 4 backoff factors for
each 5-gram requested from the servers, thereby
multiplying network traffic. We are not aware of
a method that always stores the history backoff
factors on the same shard as the longer n-gram
without duplicating a large fraction of the entries.
This means one needs to contact two shards per
n-gram instead of just one for Stupid Backoff.
Training requires additional iterations over the data.
861
Step 0 Step 1 Step 2
context counting unsmoothed probs and interpol. weights interpolated probabilities
Input key wii?n+1 (same as Step 0 output) (same as Step 1 output)
Input value f(wii?n+1) (same as Step 0 output) (same as Step 1 output)
Intermediate key wii?n+1 wi?1i?n+1 wi?n+1i
Sharding wii?n+1 wi?1i?n+1 w
i?n+2
i?n+1 , unigrams duplicated
Intermediate value fKN (wii?n+1) wi,fKN (wii?n+1)
fKN (wii?n+1)?D
fKN (wi?1i?n+1)
,?(wi?1i?n+1)
Output value fKN (wii?n+1) wi,
fKN (wii?n+1)?D
fKN (wi?1i?n+1)
,?(wi?1i?n+1) PKN (wi|wi?1i?n+1), ?(wi?1i?n+1)
Table 1: Extra steps needed for training Interpolated Kneser-Ney Smoothing
Kneser-Ney Smoothing counts lower-order n-
grams differently. Instead of the frequency of the
(n? 1)-gram, it uses the number of unique single
word contexts the (n?1)-gram appears in. We use
fKN(?) to jointly denote original frequencies for the
highest order and context counts for lower orders.
After the n-gram counting step, we process the n-
grams again to produce these quantities. This can
be done similarly to the n-gram counting using a
MapReduce (Step 0 in Table 1).
The most commonly used variant of Kneser-Ney
smoothing is interpolated Kneser-Ney smoothing,
defined recursively as (Chen and Goodman, 1998):
PKN (wi|wi?1i?n+1) =
max(fKN(wii?n+1) ? D, 0)
fKN(wi?1i?n+1)
+ ?(wi?1i?n+1)PKN (wi|wi?1i?n+2),
where D is a discount constant and {?(wi?1i?n+1)} are
interpolation weights that ensure probabilities sum
to one. Two additional major MapReduces are re-
quired to compute these values efficiently. Table 1
describes their input, intermediate and output keys
and values. Note that output keys are always the
same as intermediate keys.
The map function of MapReduce 1 emits n-gram
histories as intermediate keys, so the reduce func-
tion gets all n-grams with the same history at the
same time, generating unsmoothed probabilities and
interpolation weights. MapReduce 2 computes the
interpolation. Its map function emits reversed n-
grams as intermediate keys (hence we use wi?n+1i
in the table). All unigrams are duplicated in ev-
ery reduce shard. Because the reducer function re-
ceives intermediate keys in sorted order it can com-
pute smoothed probabilities for all n-gram orders
with simple book-keeping.
Katz Backoff requires similar additional steps.
The largest models reported here with Kneser-Ney
Smoothing were trained on 31 billion tokens. For
Stupid Backoff, we were able to use more than 60
times of that amount.
6 Distributed Application
Our goal is to use distributed language models in-
tegrated into the first pass of a decoder. This may
yield better results than n-best list or lattice rescor-
ing (Ney and Ortmanns, 1999). Doing that for lan-
guage models that reside in the same machine as the
decoder is straight-forward. The decoder accesses
n-grams whenever necessary. This is inefficient in a
distributed system because network latency causes a
constant overhead on the order of milliseconds. On-
board memory is around 10,000 times faster.
We therefore implemented a new decoder archi-
tecture. The decoder first queues some number of
requests, e.g. 1,000 or 10,000 n-grams, and then
sends them together to the servers, thereby exploit-
ing the fact that network requests with large numbers
of n-grams take roughly the same time to complete
as requests with single n-grams.
The n-best search of our machine translation de-
coder proceeds as follows. It maintains a graph of
the search space up to some point. It then extends
each hypothesis by advancing one word position in
the source language, resulting in a candidate exten-
sion of the hypothesis of zero, one, or more addi-
tional target-language words (accounting for the fact
that variable-length source-language fragments can
correspond to variable-length target-language frag-
ments). In a traditional setting with a local language
model, the decoder immediately obtains the nec-
essary probabilities and then (together with scores
862
Figure 2: Illustration of decoder graph and batch-
querying of the language model.
from other features) decides which hypotheses to
keep in the search graph. When using a distributed
language model, the decoder first tentatively extends
all current hypotheses, taking note of which n-grams
are required to score them. These are queued up for
transmission as a batch request. When the scores are
returned, the decoder re-visits all of these tentative
hypotheses, assigns scores, and re-prunes the search
graph. It is then ready for the next round of exten-
sions, again involving queuing the n-grams, waiting
for the servers, and pruning.
The process is illustrated in Figure 2 assuming a
trigram model and a decoder policy of pruning to
the four most promising hypotheses. The four ac-
tive hypotheses (indicated by black disks) at time t
are: There is, There may, There are, and There were.
The decoder extends these to form eight new nodes
at time t + 1. Note that one of the arcs is labeled ,
indicating that no target-language word was gener-
ated when the source-language word was consumed.
The n-grams necessary to score these eight hypothe-
ses are There is lots, There is many, There may be,
There are lots, are lots of, etc. These are queued up
and their language-model scores requested in a batch
manner. After scoring, the decoder prunes this set as
indicated by the four black disks at time t + 1, then
extends these to form five new nodes (one is shared)
at time t + 2. The n-grams necessary to score these
hypotheses are lots of people, lots of reasons, There
are onlookers, etc. Again, these are sent to the server
together, and again after scoring the graph is pruned
to four active (most promising) hypotheses.
The alternating processes of queuing, waiting and
scoring/pruning are done once per word position in
a source sentence. The average sentence length in
our test data is 22 words (see section 7.1), thus we
have 23 rounds3 per sentence on average. The num-
ber of n-grams requested per sentence depends on
the decoder settings for beam size, re-ordering win-
dow, etc. As an example for larger runs reported in
the experiments section, we typically request around
150,000 n-grams per sentence. The average net-
work latency per batch is 35 milliseconds, yield-
ing a total latency of 0.8 seconds caused by the dis-
tributed language model for an average sentence of
22 words. If a slight reduction in translation qual-
ity is allowed, then the average network latency per
batch can be brought down to 7 milliseconds by re-
ducing the number of n-grams requested per sen-
tence to around 10,000. As a result, our system can
efficiently use the large distributed language model
at decoding time. There is no need for a second pass
nor for n-best list rescoring.
We focused on machine translation when describ-
ing the queued language model access. However,
it is general enough that it may also be applicable
to speech decoders and optical character recognition
systems.
7 Experiments
We trained 5-gram language models on amounts of
text varying from 13 million to 2 trillion tokens.
The data is divided into four sets; language mod-
els are trained for each set separately4 . For each
training data size, we report the size of the result-
ing language model, the fraction of 5-grams from
the test data that is present in the language model,
and the BLEU score (Papineni et al, 2002) obtained
by the machine translation system. For smaller train-
ing sizes, we have also computed test-set perplexity
using Kneser-Ney Smoothing, and report it for com-
parison.
7.1 Data Sets
We compiled four language model training data sets,
listed in order of increasing size:
3One additional round for the sentence end marker.
4Experience has shown that using multiple, separately
trained language models as feature functions in Eq (1) yields
better results than using a single model trained on all data.
863
 1e+07
 1e+08
 1e+09
 1e+10
 1e+11
 1e+12
 10  100  1000  10000  100000  1e+06
 0.1
 1
 10
 100
 1000
N
um
be
r o
f n
-g
ra
m
s
Ap
pr
ox
. L
M
 s
ize
 in
 G
B
LM training data size in million tokens
x1.8/x2
x1.8/x2
x1.8/x2
x1.6/x2
target
+ldcnews
+webnews
+web
Figure 3: Number of n-grams (sum of unigrams to
5-grams) for varying amounts of training data.
target: The English side of Arabic-English parallel
data provided by LDC5 (237 million tokens).
ldcnews: This is a concatenation of several English
news data sets provided by LDC6 (5 billion tokens).
webnews: Data collected over several years, up to
December 2005, from web pages containing pre-
dominantly English news articles (31 billion to-
kens).
web: General web data, which was collected in Jan-
uary 2006 (2 trillion tokens).
For testing we use the ?NIST? part of the 2006
Arabic-English NIST MT evaluation set, which is
not included in the training data listed above7. It
consists of 1797 sentences of newswire, broadcast
news and newsgroup texts with 4 reference transla-
tions each. The test set is used to calculate transla-
tion BLEU scores. The English side of the set is also
used to calculate perplexities and n-gram coverage.
7.2 Size of the Language Models
We measure the size of language models in total
number of n-grams, summed over all orders from
1 to 5. There is no frequency cutoff on the n-grams.
5http://www.nist.gov/speech/tests/mt/doc/
LDCLicense-mt06.pdf contains a list of parallel resources
provided by LDC.
6The bigger sets included are LDC2005T12 (Gigaword,
2.5B tokens), LDC93T3A (Tipster, 500M tokens) and
LDC2002T31 (Acquaint, 400M tokens), plus many smaller
sets.
7The test data was generated after 1-Feb-2006; all training
data was generated before that date.
target webnews web
# tokens 237M 31G 1.8T
vocab size 200k 5M 16M
# n-grams 257M 21G 300G
LM size (SB) 2G 89G 1.8T
time (SB) 20 min 8 hours 1 day
time (KN) 2.5 hours 2 days ?
# machines 100 400 1500
Table 2: Sizes and approximate training times for
3 language models with Stupid Backoff (SB) and
Kneser-Ney Smoothing (KN).
There is, however, a frequency cutoff on the vocab-
ulary. The minimum frequency for a term to be in-
cluded in the vocabulary is 2 for the target, ldcnews
and webnews data sets, and 200 for the web data set.
All terms below the threshold are mapped to a spe-
cial term UNK, representing the unknown word.
Figure 3 shows the number of n-grams for lan-
guage models trained on 13 million to 2 trillion to-
kens. Both axes are on a logarithmic scale. The
right scale shows the approximate size of the served
language models in gigabytes. The numbers above
the lines indicate the relative increase in language
model size: x1.8/x2 means that the number of n-
grams grows by a factor of 1.8 each time we double
the amount of training data. The values are simi-
lar across all data sets and data sizes, ranging from
1.6 to 1.8. The plots are very close to straight lines
in the log/log space; linear least-squares regression
finds r2 > 0.99 for all four data sets.
The web data set has the smallest relative increase.
This can be at least partially explained by the higher
vocabulary cutoff. The largest language model gen-
erated contains approx. 300 billion n-grams.
Table 2 shows sizes and approximate training
times when training on the full target, webnews, and
web data sets. The processes run on standard current
hardware with the Linux operating system. Gen-
erating models with Kneser-Ney Smoothing takes
6 ? 7 times longer than generating models with
Stupid Backoff. We deemed generation of Kneser-
Ney models on the web data as too expensive and
therefore excluded it from our experiments. The es-
timated runtime for that is approximately one week
on 1500 machines.
864
 50
 100
 150
 200
 250
 300
 350
 10  100  1000  10000  100000  1e+06
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
Pe
rp
le
xit
y
Fr
ac
tio
n 
of
 c
ov
er
ed
 5
-g
ra
m
s
LM training data size in million tokens
+.022/x2
+.035/x2
+.038/x2
+.026/x2
target KN PP
ldcnews KN PP
webnews KN PP
target C5
+ldcnews C5
+webnews C5
+web C5
Figure 4: Perplexities with Kneser-Ney Smoothing
(KN PP) and fraction of covered 5-grams (C5).
7.3 Perplexity and n-Gram Coverage
A standard measure for language model quality is
perplexity. It is measured on test data T = w|T |1 :
PP (T ) = e
? 1|T |
|T |
 
i=1
log p(wi|wi?1i?n+1) (7)
This is the inverse of the average conditional prob-
ability of a next word; lower perplexities are bet-
ter. Figure 4 shows perplexities for models with
Kneser-Ney smoothing. Values range from 280.96
for 13 million to 222.98 for 237 million tokens tar-
get data and drop nearly linearly with data size (r2 =
0.998). Perplexities for ldcnews range from 351.97
to 210.93 and are also close to linear (r2 = 0.987),
while those for webnews data range from 221.85 to
164.15 and flatten out near the end. Perplexities are
generally high and may be explained by the mix-
ture of genres in the test data (newswire, broadcast
news, newsgroups) while our training data is pre-
dominantly written news articles. Other held-out
sets consisting predominantly of newswire texts re-
ceive lower perplexities by the same language mod-
els, e.g., using the full ldcnews model we find per-
plexities of 143.91 for the NIST MT 2005 evaluation
set, and 149.95 for the NIST MT 2004 set.
Note that the perplexities of the different language
models are not directly comparable because they use
different vocabularies. We used a fixed frequency
cutoff, which leads to larger vocabularies as the
training data grows. Perplexities tend to be higher
with larger vocabularies.
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 10  100  1000  10000  100000  1e+06
Te
st
 d
at
a 
BL
EU
LM training data size in million tokens
+0.62BP/x2
+0.56BP/x2
+0.51BP/x2
+0.66BP/x2
+0.70BP/x2
+0.39BP/x2
+0.15BP/x2
target KN
+ldcnews KN
+webnews KN
target SB
+ldcnews SB
+webnews SB
+web SB
Figure 5: BLEU scores for varying amounts of data
using Kneser-Ney (KN) and Stupid Backoff (SB).
Perplexities cannot be calculated for language
models with Stupid Backoff because their scores are
not normalized probabilities. In order to neverthe-
less get an indication of potential quality improve-
ments with increased training sizes we looked at the
5-gram coverage instead. This is the fraction of 5-
grams in the test data set that can be found in the
language model training data. A higher coverage
will result in a better language model if (as we hy-
pothesize) estimates for seen events tend to be bet-
ter than estimates for unseen events. This fraction
grows from 0.06 for 13 million tokens to 0.56 for 2
trillion tokens, meaning 56% of all 5-grams in the
test data are known to the language model.
Increase in coverage depends on the training data
set. Within each set, we observe an almost constant
growth (correlation r2 ? 0.989 for all sets) with
each doubling of the training data as indicated by
numbers next to the lines. The fastest growth oc-
curs for webnews data (+0.038 for each doubling),
the slowest growth for target data (+0.022/x2).
7.4 Machine Translation Results
We use a state-of-the-art machine translation system
for translating from Arabic to English that achieved
a competitive BLEU score of 0.4535 on the Arabic-
English NIST subset in the 2006 NIST machine
translation evaluation8 . Beam size and re-ordering
window were reduced in order to facilitate a large
8See http://www.nist.gov/speech/tests/mt/
mt06eval official results.html for more results.
865
number of experiments. Additionally, our NIST
evaluation system used a mixture of 5, 6, and 7-gram
models with optimized stupid backoff factors for
each order, while the learning curve presented here
uses a fixed order of 5 and a single fixed backoff fac-
tor. Together, these modifications reduce the BLEU
score by 1.49 BLEU points (BP)9 at the largest train-
ing size. We then varied the amount of language
model training data from 13 million to 2 trillion to-
kens. All other parts of the system are kept the same.
Results are shown in Figure 5. The first part
of the curve uses target data for training the lan-
guage model. With Kneser-Ney smoothing (KN),
the BLEU score improves from 0.3559 for 13 mil-
lion tokens to 0.3832 for 237 million tokens. At
such data sizes, Stupid Backoff (SB) with a constant
backoff parameter ? = 0.4 is around 1 BP worse
than KN. On average, one gains 0.62 BP for each
doubling of the training data with KN, and 0.66 BP
per doubling with SB. Differences of more than 0.51
BP are statistically significant at the 0.05 level using
bootstrap resampling (Noreen, 1989; Koehn, 2004).
We then add a second language model using ldc-
news data. The first point for ldcnews shows a large
improvement of around 1.4 BP over the last point
for target for both KN and SB, which is approxi-
mately twice the improvement expected from dou-
bling the amount of data. This seems to be caused
by adding a new domain and combining two models.
After that, we find an improvement of 0.56?0.70 BP
for each doubling of the ldcnews data. The gap be-
tween Kneser-Ney Smoothing and Stupid Backoff
narrows, starting with a difference of 0.85 BP and
ending with a not significant difference of 0.24 BP.
Adding a third language models based on web-
news data does not show a jump at the start of the
curve. We see, however, steady increases of 0.39?
0.51 BP per doubling. The gap between Kneser-Ney
and Stupid Backoff is gone, all results with Stupid
Backoff are actually better than Kneser-Ney, but the
differences are not significant.
We then add a fourth language model based on
web data and Stupid Backoff. Generating Kneser-
Ney models for these data sizes is extremely ex-
pensive and is therefore omitted. The fourth model
91 BP = 0.01 BLEU. We show system scores as BLEU, dif-
ferences as BP.
shows a small but steady increase of 0.15 BP per
doubling, surpassing the best Kneser-Ney model
(trained on less data) by 0.82 BP at the largest
size. Goodman (2001) observed that Kneser-Ney
Smoothing dominates other schemes over a broad
range of conditions. Our experiments confirm this
advantage at smaller language model sizes, but show
the advantage disappears at larger data sizes.
The amount of benefit from doubling the training
size is partly determined by the domains of the data
sets10. The improvements are almost linear on the
log scale within the sets. Linear least-squares regres-
sion shows correlations r2 > 0.96 for all sets and
both smoothing methods, thus we expect to see sim-
ilar improvements when further increasing the sizes.
8 Conclusion
A distributed infrastructure has been described to
train and apply large-scale language models to ma-
chine translation. Experimental results were pre-
sented showing the effect of increasing the amount
of training data to up to 2 trillion tokens, resulting
in a 5-gram language model size of up to 300 billion
n-grams. This represents a gain of about two orders
of magnitude in the amount of training data that can
be handled over that reported previously in the liter-
ature (or three-to-four orders of magnitude, if one
considers only single-pass decoding). The infra-
structure is capable of scaling to larger amounts of
training data and higher n-gram orders.
The technique is made efficient by judicious
batching of score requests by the decoder in a server-
client architecture. A new, simple smoothing tech-
nique well-suited to distributed computation was
proposed, and shown to perform as well as more
sophisticated methods as the size of the language
model increases.
Significantly, we found that translation quality as
indicated by BLEU score continues to improve with
increasing language model size, at even the largest
sizes considered. This finding underscores the value
of being able to train and apply very large language
models, and suggests that further performance gains
may be had by pursuing this direction further.
10There is also an effect of the order in which we add the
models. As an example, web data yields +0.43 BP/x2 when
added as the second model. A discussion of this effect is omit-
ted due to space limitations.
866
References
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Harvard, Cambridge,
MA, USA.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. In Sixth
Symposium on Operating System Design and Imple-
mentation (OSDI-04), San Francisco, CA, USA.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of ICASSP-2007, Honolulu, HI, USA.
Joshua Goodman. 2001. A bit of progress in language
modeling. Technical Report MSR-TR-2001-72, Mi-
crosoft Research, Redmond, WA, USA.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters from
sparse data. In Pattern Recognition in Practice, pages
381?397. North Holland.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3).
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 181?
184.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP-04, Barcelona, Spain.
Hermann Ney and Stefan Ortmanns. 1999. Dynamic
programming search for continuous speech recogni-
tion. IEEE Signal Processing Magazine, 16(5):64?83.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL-
02, pages 311?318, Philadelphia, PA, USA.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list
re-ranking. In Proceedings of EMNLP-2006, pages
216?223, Sydney, Australia.
867
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 986?995, Prague, June 2007. c?2007 Association for Computational Linguistics
An Empirical Study on Computing Consensus Translations
from Multiple Machine Translation Systems
Wolfgang Macherey
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
wmach@google.com
Franz Josef Och
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
och@google.com
Abstract
This paper presents an empirical study on
how different selections of input translation
systems affect translation quality in system
combination. We give empirical evidence
that the systems to be combined should be
of similar quality and need to be almost
uncorrelated in order to be beneficial for sys-
tem combination. Experimental results are
presented for composite translations com-
puted from large numbers of different re-
search systems as well as a set of transla-
tion systems derived from one of the best-
ranked machine translation engines in the
2006 NIST machine translation evaluation.
1 Introduction
Computing consensus translations from the outputs
of multiple machine translation engines has become
a powerful means to improve translation quality in
many machine translation tasks. Analogous to the
ROVER approach in automatic speech recognition
(Fiscus, 1997), a composite translation is computed
by voting on the translation outputs of multiple
machine translation systems. Depending on how
the translations are combined and how the voting
scheme is implemented, the composite translation
may differ from any of the original hypotheses.
While elementary approaches simply select for each
sentence one of the original translations, more so-
phisticated methods allow for combining transla-
tions on a word or a phrase level.
Although system combination could be shown
to result in substantial improvements in terms of
translation quality (Matusov et al, 2006; Sim et al,
2007), not every possible ensemble of translation
outputs has the potential to outperform the primary
translation system. In fact, an adverse combina-
tion of translation systems may even deteriorate
translation quality. This holds to a greater extent,
when the ensemble of translation outputs contains a
significant number of translations produced by low
performing but highly correlated systems.
In this paper we present an empirical study on
how different ensembles of translation outputs affect
performance in system combination. In particular,
we will address the following questions:
? To what extent can translation quality benefit
from combining systems developed by multiple
research labs?
Despite an increasing number of translation
engines, most state-of-the-art systems in statis-
tical machine translation are nowadays based
on implementations of the same techniques.
For instance, word alignment models are often
trained using the GIZA++ toolkit (Och and
Ney, 2003); error minimizing training criteria
such as the Minimum Error Rate Training
(Och, 2003) are employed in order to learn
feature function weights for log-linear models;
and translation candidates are produced using
phrase-based decoders (Koehn et al, 2003)
in combination with n-gram language models
(Brants et al, 2007).
All these methods are established as de facto
standards and form an integral part of most
statistical machine translation systems. This,
however, raises the question as to what ex-
tent translation quality can be expected to
improve when similarly designed systems are
combined.
? How can a set of diverse translation systems be
built from a single translation engine?
Without having access to different translation
986
engines, it is desirable to build a large number
of diverse translation systems from a single
translation engine that are useful in system
combination. The mere use of N -best lists
and word lattices is often not effective, because
N -best candidates may be highly correlated,
thus resulting in small diversity compared to
the first best hypothesis. Therefore, we need a
canonical way to build a large pool of diverse
translation systems from a single translation
engine.
? How can an ensemble of translation outputs
be selected from a large pool of translation
systems?
Once a large pool of translation systems is
available, we need an effective means to select
a small ensemble of translation outputs for
which the combined system outperforms the
best individual system.
These questions will be investigated on the basis
of three approaches to system combination: (i) an
MBR-like candidate selection method based on
BLEU correlation matrices, (ii) confusion networks
built from word sausages, and (iii) a novel two-
pass search algorithm that aims at finding consensus
translations by reordering bags of words constituting
the consensus hypothesis.
Experiments were performed on two Chinese-
English text translation corpora under the conditions
of the large data track as defined for the 2006 NIST
machine translation evaluation (MT06). Results
are reported for consensus translations built from
system outputs provided by MT06 participants as
well as systems derived from one of the best-ranked
translation engines.
The remainder of this paper is organized as fol-
lows: in Section 2, we describe three combina-
tion methods for computing consensus translations.
In Sections 3.1 and 3.2, we present experimental
results on combining system outputs provided by
MT06 participants. Section 3.3 shows how correla-
tion among translation systems affects performance
in system combination. In Section 3.4, we discuss
how a single translation engine can be modified
in order to produce a large number of diverse
translation systems. First experimental results us-
ing a greedy search algorithm to select a small
ensemble of translation outputs from a large pool
of canonically built translation systems are reported.
A summary presented in Section 4 concludes the
paper.
2 Methods for System Combination
System combination in machine translation aims to
build a composite translation from system outputs
of multiple machine translation engines. Depending
on how the systems are combined and which voting
scheme is implemented, the consensus translation
may differ from any of the original candidate trans-
lations. In this section, we discuss three approaches
to system combination.
2.1 System Combination via Candidate
Selection
The easiest and most straightforward approach to
system combination simply returns one of the orig-
inal candidate translations. Typically, this selection
is made based on translation scores, confidence esti-
mations, language and other models (Nomoto, 2004;
Paul et al, 2005). For many machine translation
systems, however, the scores are often not normal-
ized or may even not be available, which makes
it difficult to apply this technique. We therefore
propose an alternative method based on ?correlation
matrices? computed from the BLEU performance
measure (Papineni et al, 2001).
Let e1, ..., eM denote the outputs of M translation
systems, each given as a sequence of words in
the target language. An element of the BLEU
correlation matrix B  pbijq is defined as the
sentence-based BLEU score between a candidate
translation ei and a pseudo-reference translation ej
pi, j  1, ...,Mq:
bij  BPpei, ejq  exp
$
'
'
%
1
4
4?
n1
log ?npei, ejq
,
/
/
-
.
(1)
Here, BP denotes the brevity penalty factor with ?n
designating the n-gram precisions.
Because the BLEU score is computed on a sen-
tence rather than a corpus-level, n-gram precisions
are capped by the maximum over 12|ei| and ?n in
order to avoid singularities, where |ei| is the length
of the candidate translation 1.
Due to the following properties, B can be inter-
preted as a correlation matrix, although the term
does not hold in a strict mathematical sense: (i)
bij P r0, 1s; (ii) bij  1.0 ?? ei  ej ; (iii) bij 
0.0 ?? eiXej  H, i.e., bij is zero if and only if
none of the words which constitute ei can be found
1 Note that for non-zero n-gram precisions, ?n is always
larger than 12|e| .
987
in ej and vice versa. The BLEU correlation matrix
is in general, however, not symmetric, although in
practice, ||bij  bji|| is typically negligible.
Each translation system m is assigned to a system
prior weight ?m P r0, 1s, which reflects the perfor-
mance of system m relatively to all other translation
systems. If no prior knowledge is available, ?m is
set to 1{M .
Now, let ?  p?1, ..., ?M qJ denote a vector of
system prior weights and let b1, ...,bM denote the
row vectors of the matrix B. Then the translation
system with the highest consensus is given by:
e  em with
m  argmax
em
!
?J  bm
) (2)
The candidate selection rule in Eq. (2) has two useful
properties:
? The selection does not depend on scored trans-
lation outputs; the mere target word sequence
is sufficient. Hence, this technique is also
applicable to rule-based translation systems 2.
? Using the components of the row-vector bm
as feature function values for the candidate
translation em (m  1, ...,M ), the system
prior weights ? can easily be trained using
the Minimum Error Rate Training described in
(Och, 2003).
Note that the candidate selection rule in Eq. (2)
is equivalent to re-ranking candidate translations
according to the Minimum Bayes Risk (MBR) deci-
sion rule (Kumar and Byrne, 2004), provided that
the system prior weights are used as estimations
of the posterior probabilities ppe|fq for a source
sentence f . Due to the proximity of this method
to the MBR selection rule, we call this combination
scheme MBR-like system combination.
2.2 ROVER-Like Combination Schemes
ROVER-like combination schemes aim at comput-
ing a composite translation by voting on confusion
networks that are built from translation outputs
of multiple machine translation engines via an it-
erative application of alignments (Fiscus, 1997).
To accomplish this, one of the original candidate
translations, e.g. em, is chosen as the primary
translation hypothesis, while all other candidates en
pn  mq are aligned with the word sequence of
2 This property is not exclusive to this combination scheme
but also holds for the methods discussed in Sections 2.2 and 2.3.
the primary translation. To limit the costs when
aligning a permutation of the primary translation,
the alignment metric should allow for small shifts
of contiguous word sequences in addition to the
standard edit operations deletions, insertions, and
substitutions. These requirements are met by the
Translation Edit Rate (TER) (Snover et al, 2006):
TERpei, ejq
Del  Ins  Sub  Shift
|ej |
(3)
The outcome of the iterated alignments is a word
transition network which is also known as word
sausage because of the linear sequence of corre-
spondence sets that constitute the network. Since
both the order and the elements of a correspondence
set depend on the choice of the primary transla-
tion, each candidate translation is chosen in turn
as the primary system. This results in a total of
M word sausages that are combined into a single
super network. The word sequence along the cost-
minimizing path defines the composite translation.
To further optimize the word sausages, we replace
each system prior weight ?m with the lp-norm over
the normalized scalar product between the weight
vector ? and the row vector bm:
?1m 
p?J  bmq`
?
m?
p?J  bm?q`
, ` P r0, 8q (4)
As ` approaches  8, ?1m  1 if and only if
system m has the highest consensus among all input
systems; otherwise, ?1m  0. Thus, the word
sausages are able to emulate the candidate selection
rule described in Section 2.1. Setting `  0 yields
uniform system prior weights, and setting B to
the unity matrix provides the original prior weights
vector. Word sausages which take advantage of the
refined system prior weights are denoted by word
sausages+.
2.3 A Two-Pass Search Algorithm
The basic idea of the two-pass search algorithm is
to compute a consensus translation by reordering
words that are considered to be constituents of the
final consensus translation.
Initially, the two-pass search is given a repository
of candidate translations which serve as pseudo
references together with a vector of system prior
weights. In the first pass, the algorithm uses
a greedy strategy to determine a bag of words
which minimizes the position-independent word er-
ror rate (PER). These words are considered to be
988
constituents of the final consensus translation. The
greedy strategy implicitly ranks the constituents,
i.e., words selected at the beginning of the first
phase reduce the PER the most and are considered
to be more important than constituents selected in
the end. The first pass finishes when putting further
constituents into the bag of words does not improve
the PER.
The list of constituents is then passed to a sec-
ond search algorithm, which starts with the empty
string and then expands all active hypotheses by
systematically inserting the next unused word from
the list of constituents at different positions in the
current hypothesis. For instance, a partial consensus
hypothesis of length l expands into l   1 new
hypotheses of length l 1. The resulting hypotheses
are scored with respect to the TER measure based on
the repository of weighted pseudo references. Low-
scoring hypotheses are pruned to keep the space of
active hypotheses small. The algorithm will finish
if either no constituents are left or if expanding the
set of active hypotheses does not further decrease
the TER score. Optionally, the best consensus hy-
pothesis found by the two-pass search is combined
with all input translation systems via the MBR-like
combination scheme described in Section 2.1. This
refinement is called two-pass+.
2.4 Related Work
Research on multi-engine machine translation goes
back to the early nineties. In (Robert and Nirenburg,
1994), a semi-automatic approach is described that
combines outputs from three translation systems to
build a consensus translation. (Nomoto, 2004) and
(Paul et al, 2005) used translation scores, language
and other models to select one of the original
translations as consensus translation. (Bangalore et
al., 2001) used a multiple string alignment algorithm
in order to compute a single confusion network,
on which a consensus hypothesis was computed
through majority voting. Because the alignment
procedure was based on the Levenshtein distance,
it was unable to align translations with significantly
different word orders. (Jayaraman and Lavie, 2005)
tried to overcome this problem by using confi-
dence scores and language models in order to rank
a collection of synthetic combinations of words
extracted from the original translation hypotheses.
Experimental results were only reported for the
METEOR metric (Banerjee and Lavie, 2005). In
(Matusov et al, 2006), pairwise word alignments
of the original translation hypotheses were estimated
for an enhanced statistical alignment model in order
Table 1: Corpus statistics for two Chinese-English
text translation sets: ZHEN-05 is a random
selection of test data used in NIST evaluations prior
to 2006; ZHEN-06 comprises the NIST portion of
the Chinese-English evaluation data used in the
2006 NIST machine translation evaluation.
corpus Chinese English
ZHEN-05 sentences 2390
chars / words 110647 67737
ZHEN-06 sentences 1664
chars / words 64292 41845
to explicitly capture word re-ordering. Although
the proposed method was not compared with other
approaches to system combination, it resulted in
substantial gains and provided new insights into
system combination.
3 Experimental Results
Experiments were conducted on two corpora for
Chinese-English text translations, the first of which
is compiled from a random selected subset of eval-
uation data used in the NIST MT evaluations up to
the year 2005. The second data set consists of the
NIST portion of the Chinese-English data used in
the MT06 evaluation and comprises 1664 Chinese
sentences collected from broadcast news articles
(565 sentences), newswire texts (616 sentences), and
news group texts (483 sentences). Both corpora
provide 4 reference translations per source sentence.
Table 1 summarizes some corpus statistics.
For all experiments, system performance was
measured in terms of the IBM-BLEU score (Pap-
ineni et al, 2001). Compared to the NIST imple-
mentation of the BLEU score, IBM-BLEU follows
the original definition of the brevity penalty (BP)
factor: while in the NIST implementation the BP is
always based on the length of the shortest reference
translation, the BP in the IBM-BLEU score is based
on the length of the reference translation which is
closest to the candidate translation length. Typically,
IBM-BLEU scores tend to be smaller than NIST-
BLEU scores. In the following, BLEU always refers
to the IBM-BLEU score.
Except for the results reported in Section 3.2, we
used uniform system prior weights throughout all
experiments. This turned out to be more stable when
combining different sets of translation systems and
helped to improve generalization.
989
Table 2: BLEU scores and brevity penalty (BP) factors determined on the ZHEN-06 test set for primary
systems together with consensus systems for the MBR-like candidate selection method obtained by
combining each three adjacent systems with uniform system prior weights. Primary systems are sorted in
descending order with respect to their BLEU score. The 95% confidence intervals are computed using the
bootstrap re-sampling normal approximation method (Noreen, 1989).
combination primary system consensus oracle
BLEU CI 95% BP BLEU ? BP pair-CI 95% BLEU BP
01, 02, 03 32.10 (0.88) 0.93 32.97 (+0.87) 0.92 [+0.29, +1.46] 38.54 0.94
01, 15, 16 32.10 (0.88) 0.93 23.55 ( -8.54) 0.92 [ -9.29, -7.80] 33.55 0.95
02, 03, 04 31.71 (0.90) 0.96 31.55 ( -0.16) 0.92 [ -0.65, +0.29] 37.23 0.95
03, 04, 05 29.59 (0.88) 0.87 29.55 ( -0.04) 0.88 [ -0.53, +0.41] 35.55 0.92
03, 04, 06 29.59 (0.88) 0.87 29.83 (+0.24) 0.90 [ -0.29, +0.71] 35.69 0.93
04, 05, 06 27.70 (0.87) 0.94 28.52 (+0.82) 0.91 [+0.15, +1.49] 34.67 0.94
05, 06, 07 27.05 (0.81) 0.88 28.21 (+1.16) 0.92 [+0.63, +1.66] 33.89 0.94
05, 06, 08 27.05 (0.81) 0.88 28.47 (+1.42) 0.91 [+0.95, +1.95] 34.18 0.93
06, 07, 08 27.02 (0.76) 0.92 28.12 (+1.10) 0.94 [+0.59, +1.59] 33.87 0.95
07, 08, 09 26.75 (0.79) 0.97 27.79 (+1.04) 0.94 [+0.52, +1.51] 33.54 0.95
08, 09, 10 26.41 (0.81) 0.92 26.78 (+0.37) 0.94 [ -0.07, +0.86] 32.47 0.96
09, 10, 11 25.05 (0.84) 0.90 24.96 ( -0.09) 0.94 [ -0.59, +0.46] 30.92 0.97
10, 11, 12 23.48 (0.68) 1.00 24.24 (+0.76) 0.94 [+0.27, +1.30] 30.08 0.96
11, 12, 13 23.26 (0.74) 0.95 24.05 (+0.79) 0.92 [+0.40, +1.23] 29.56 0.93
12, 13, 14 22.38 (0.78) 0.87 22.68 (+0.30) 0.89 [ -0.28, +0.95] 28.58 0.91
13, 14, 15 22.13 (0.72) 0.89 21.29 ( -0.84) 0.90 [ -1.33, -0.33] 26.61 0.92
14, 15, 16 17.42 (0.66) 0.93 18.45 (+1.03) 0.92 [+0.45, +1.56] 23.30 0.95
15 17.20 (0.64) 0.91 ? ? ? ? ? ?
16 15.21 (0.63) 0.96 ? ? ? ? ? ?
3.1 Combining Multiple Research Systems
In a first experiment, we investigated the effect
of combining translation outputs provided from
different research labs. Each translation system
corresponds to a primary system submitted to the
NIST MT06 evaluation 3. Table 2 shows the BLEU
scores together with their corresponding BP factors
for the primary systems of 16 research labs (site
names were anonymized). Primary systems are
sorted in descending order with respect to their
BLEU score. Table 2 also shows the consensus
translation results for the MBR-like candidate selec-
tion method. Except where marked with an asterisk,
all consensus systems are built from the outputs
of three adjacent systems. While only few com-
bined systems show a degradation, the majority of
all consensus translations achieve substantial gains
between 0.2% and 1.4% absolute in terms of BLEU
score on top of the best individual (primary) system.
The column CI provides 95% confidence intervals
for BLEU scores with respect to the primary system
baseline using the bootstrap re-sampling normal
3 For more information see http://www.nist.gov/
speech/tests/mt/mt06eval_official_results.
html
approximation method (Noreen, 1989). The column
?pair-CI? shows 95% confidence intervals relative
to the primary system using the paired bootstrap
re-sampling method (Koehn, 2004). The princi-
ple of the paired bootstrap method is to create a
large number of corresponding virtual test sets by
consistently selecting candidate translations with re-
placement from both the consensus and the primary
system. The confidence interval is then estimated
over the differences between the BLEU scores of
corresponding virtual test sets. Improvements are
considered to be significant if the left boundary of
the confidence interval is larger than zero.
Oracle BLEU scores shown in Table 2 are com-
puted by selecting the best translation among the
three candidates. The oracle scores might indicate a
larger potential of the MBR-like selection rule, and
further gains could be expected if the candidate se-
lection rule is combined with confidence measures.
Table 2 shows that it is important that all trans-
lation systems achieve nearly equal quality; com-
bining high-performing systems with low-quality
translations typically results in clear performance
losses compared to the primary system, which is the
case when combining, e.g., systems 01, 15, and 16.
990
Table 3: BLEU scores and brevity penalty (BP) factors determined on the ZHEN-06 test set for the
combination of multiple research systems using the MBR-like selection method with uniform and trained
system prior weights. Prior weights are trained using 5-fold cross validation. The 95% confidence intervals
realtive to uniform weights are computed using the paired bootstrap re-sampling method (Koehn, 2004).
# systems combination uniform ? opt. on dev. ? opt. on test
BLEU BP BLEU BP pair-CI 95% BLEU BP
3 01 ? 03 32.98 0.92 33.03 0.93 [ -0.23, +0.34] 33.60 0.93
4 01 ? 04 33.44 0.93 33.46 0.93 [ -0.26, +0.29] 34.97 0.94
5 01 ? 05 33.07 0.92 33.14 0.93 [ -0.29, +0.43] 34.33 0.93
6 01 ? 06 32.86 0.92 33.53 0.93 [+0.26, +1.08] 34.43 0.93
7 01 ? 07 33.08 0.93 33.51 0.93 [+0.04, +0.82] 34.49 0.93
8 01 ? 08 33.12 0.93 33.47 0.93 [ -0.06, +0.75] 34.50 0.94
9 01 ? 09 33.15 0.93 33.22 0.93 [ -0.35, +0.51] 34.68 0.93
10 01 ? 10 33.01 0.93 33.59 0.94 [+0.18, +0.96] 34.79 0.94
11 01 ? 11 32.84 0.94 33.40 0.94 [+0.13, +0.98] 34.76 0.94
12 01 ? 12 32.73 0.93 33.49 0.94 [+0.34, +1.18] 34.83 0.94
13 01 ? 13 32.71 0.93 33.54 0.94 [+0.39, +1.26] 34.91 0.94
14 01 ? 14 32.66 0.93 33.69 0.94 [+0.58, +1.47] 34.97 0.94
15 01 ? 15 32.47 0.93 33.57 0.94 [+0.63, +1.57] 34.99 0.94
16 01 ? 16 32.51 0.93 33.62 0.94 [+0.62, +1.59] 35.00 0.94
3.2 Non-Uniform System Prior Weights
As pointed out in Section 2.1, a useful property
of the MBR-like system selection method is that
system prior weights can easily be trained using
the Minimum Error Rate Training (Och, 2003).
In this section, we investigate the effect of using
non-uniform system weights for the combination of
multiple research systems. Since for each research
system, only the first best translation candidate
was provided, we used a five-fold cross validation
scheme in order to train and evaluate the system
prior weights. For this purpose, all research systems
were consistently split into five random partitions of
almost equal size. The partitioning procedure was
document preserving, i.e., sentences belonging to
the same document were guaranteed to be assigned
to the same partition. Each of the five partitions
played once the role of the evaluation set while
the other four partitions were used as development
data to train the system prior weights. Consensus
systems were computed for each held out set using
the system prior weights estimated on the respec-
tive development sets. The combination results
determined on all held out sets were then concate-
nated and evaluated with respect to the ZHEN-06
reference translations. Table 3 shows the results
for the combinations of up to 16 research systems
using either uniform or trained system prior weights.
System 01 achieved the highest BLEU score on all
five constellations of development partitions and is
therefore the primary system to which all results in
Table 3 compare. In comparison to uniform weights,
consensus translations using trained weights are
more robust toward the integration of low perform-
ing systems into the combination scheme. The
best combined system obtained with trained system
prior weights (01-14) is, however, not significantly
better than the best combined system using uniform
weights (01-04), for which the 95% confidence
interval yields r0.17, 0.66s according to the paired
bootstrap re-sampling method.
Table 3 also shows the theoretically achievable
BLEU scores when optimizing the system prior
weights on the held out data. This provides an upper
bound to what extent system combination might
benefit if an ideal set of system prior weights were
used.
3.3 Effect of Correlation on System
Combination
The degree of correlation among input translation
systems is a key factor which decides whether
translation outputs can be combined such a way that
the overall system performance improves. Correla-
tion can be considered as a reciprocal measure of
diversity: if the correlation is too large (? 90%),
there will be insufficient diversity among the input
systems and the consensus system will at most be
able to only marginally outperform the best indi-
991
Table 4: BLEU scores obtained on ZHEN-05 with uniform prior weights and a 10-way system combination
using the MBR-like candidate selection rule, word sausages, and the two-pass search algorithm together
with their improved versions ?sausages+? and ?two-pass+?, respectively for different sample sizes of the
FBIS training corpus.
sampling primary mbr-like sausages sausages+ two-pass two-pass+
r%s BLEU CI 95% BP BLEU BP BLEU BP BLEU BP BLEU BP BLEU BP
5 27.82 (0.65) 1.00 29.51 1.00 29.00 0.97 30.25 0.99 29.58 0.94 29.93 0.96
10 29.70 (0.69) 1.00 31.42 1.00 30.74 0.98 31.99 0.99 31.30 0.95 31.75 0.97
20 31.37 (0.69) 1.00 32.56 1.00 32.64 1.00 33.17 0.99 32.60 0.96 32.76 0.98
40 32.66 (0.66) 1.00 33.52 1.00 33.23 0.99 33.98 1.00 33.65 0.97 33.88 0.99
80 33.67 (0.66) 1.00 34.17 1.00 33.93 0.99 34.38 1.00 34.20 0.99 34.35 1.00
100 33.90 (0.67) 1.00 34.03 1.00 33.98 1.00 34.02 1.00 33.90 1.00 34.08 1.00
vidual translation system. If the correlation is too
low (? 5%), there might be no consensus among the
input systems and the quality of the consensus trans-
lations will hardly differ from a random selection of
the candidates.
To study how correlation affects performance in
system combination, we built a large number of
systems trained on randomly sampled portions of the
FBIS 4 training data collection. Sample sizes ranged
between 5% and 100% with each larger data set dou-
bling the size of the next smaller collection. For each
sample size, we created 10 data sets, thus resulting in
a total of 610 training corpora. On each data set, a
new translation system was trained from scratch and
4 LDC catalog number: LDC2003E14
 27
 28
 29
 30
 31
 32
 33
 34
 35
 0  10  20  30  40  50  60  70  80  90  100
BL
EU
 [%
]
sampling [%]
consensus system: 10
9
8
7
6
5
4
3
primary system:  1
Figure 1: Incremental system combination on
ZHEN-05 using the MBR-like candidate selection
rule and uniform prior weights. Systems were
trained with different sample sizes of the FBIS data.
used for decoding the ZHEN-05 test sentences. All
60 systems applied the MBR decision rule (Kumar
and Byrne, 2004), which gave an additional 0.5%
gain on average on top of using the maximum a-
posteriori (MAP) decision rule. Systems trained on
equally amounts of training data were incrementally
combined. Figure 1 shows the evolution of the
BLEU scores as a function of the number of sys-
tems as the sample size is increased from 5?100%.
Table 4 shows the BLEU scores obtained with a 10-
way system combination using the MBR-like can-
didate selection rule, word sausages, and the two-
pass search algorithm together with their improved
versions ?sausages+? and ?two-pass+?, respectively.
In order to measure the correlation between the in-
dividual translation systems, we computed the inter-
system BLEU score matrix as shown exemplary
 0  10  20  30  40  50  60  70  80  90  100
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 85
co
rr
ela
tio
n [%
]
sampling [%]
consensus
Figure 2: Evolution of the correlation on ZHEN-05
averaged over 10 systems in the course of the sample
size.
992
Table 5: Minimum, maximum, and average inter-system BLEU score correlations for (i) the primary
systems of the 2006 NIST machine translation evaluation on the ZHEN-06 test data, (ii) different training
corpus sizes (FBIS), and (iii) a greedy strategy which chooses 15 systems out of a pool of 200 translation
systems.
ZHEN-6 ZHEN-5 ZHEN-5 ZHEN-6
16 primary FBIS sampling, 10 systems 15 systems 15 systems
systems 5% 10% 20% 40% 80% 100% greedy selection ZHEN-5 selection
min 0.08 0.38 0.44 0.47 0.53 0.60 0.72 0.55 0.50
mean 0.18 0.40 0.45 0.50 0.56 0.66 0.79 0.65 0.61
median 0.19 0.40 0.45 0.49 0.56 0.64 0.78 0.63 0.58
max 0.28 0.42 0.47 0.53 0.58 0.70 0.88 0.85 0.83
in Table 6 for the 16 MT06 primary submissions.
Figure 2 shows the evolution of the correlation
averaged over 10 systems as the sample size is
increased from 5?100%. Note that all systems were
optimized using a non-deterministic implementation
of the Minimum Error Rate Training described in
(Och, 2003). Hence, using all of the FBIS corpus
data does not necessarily result in fully correlated
systems, since the training procedure may pick a
different solution for same training data in order
to increase diversity. Both Table 4 and Figure 1
clearly indicate that increasing the correlation (and
thus reducing the diversity) substantially reduces the
potential of a consensus system to outperform the
primary translation system. Ideally, the correlation
should not be larger than 30%.
Especially for low inter-system correlations and
reduced translation quality, both the enhanced ver-
sions of the word sausage combination method
and the two-pass search outperform the MBR-like
candidate selection scheme. This advantage, how-
ever, diminishes as soon as the correlation increases
and translations produced by the individual systems
become more similar.
3.4 Toward Automatic System Generation and
Selection
Sampling the training data is an effective means
to investigate the effect of system correlation on
consensus performance. However, this is done at the
expense of the overall system quality. What we need
instead is a method to reduce correlation without
sacrificing system performance.
A simple, though computationally very expensive
way to build an ensemble of low-correlated sta-
tistical machine translation systems from a single
translation engine is to train a large pool of sys-
tems, in which each of the systems is trained with
a slightly different set of parameters. Changing
only few parameters at a time typically results in
only small changes in system performance but may
have a strong impact on system correlation. In
our experiments we observed that changing pa-
rameters which affect the training procedure at a
very early stage, are most effective and introduce
larger diversity. For instance, changing the training
procedure for word alignment models turned out to
be most beneficial; for details see (Och and Ney,
2003). Other parameters that were changed include
the maximum jump width in word re-ordering, the
choice of feature function weights for the log-linear
translation models, and the set of language models
used in decoding.
Once a large pool of translation systems has
been generated, we need a method to select a
small ensemble of diverse translation outputs that
are beneficial for computing consensus translations.
Here, we used a greedy strategy to rank the systems
with respect to their ability to improve system
Table 6: Inter-system BLEU score matrix for
primary systems of the NIST 2006 TIDES machine
translation evaluation on the ZHEN-06 test data.
Id 01 02 03 04 05    14 15 16
01 1.00 0.27 0.26 0.23 0.26    0.15 0.15 0.12
02 0.27 1.00 0.27 0.22 0.25    0.15 0.15 0.12
03 0.26 0.27 1.00 0.21 0.28    0.15 0.15 0.10
04 0.23 0.22 0.21 1.00 0.19    0.14 0.12 0.12
05 0.26 0.25 0.28 0.19 1.00    0.16 0.17 0.11
06 0.27 0.24 0.25 0.21 0.26    0.16 0.18 0.13
.
.
.
.
.
.
.
.
.
14 0.15 0.15 0.15 0.14 0.16    1.00 0.12 0.08
15 0.15 0.15 0.15 0.12 0.17    0.12 1.00 0.09
16 0.12 0.12 0.10 0.12 0.11    0.08 0.09 1.00
993
 37.7
 37.8
 37.9
 38
 38.1
 38.2
 38.3
 38.4
 5  10  15  20  25  30  35  40
BL
EU
[%
]
number of systems
ZHEN-5: consensus system
primary system
 5  10  15  20  25  30  35  40
 31.2
 31.4
 31.6
 31.8
 32
 32.2
 32.4
 32.6
BL
EU
[%
]
number of systems
ZHEN-6: oracle selection
consensus system
primary system
Figure 3: BLEU score of the consensus translation as a function of the number of systems on the ZHEN-05
sentences (left) and ZHEN-06 sentences (right). The middle curve (right) shows the variation of the BLEU
score on the ZHEN-06 data when the greedy selection of the ZHEN-05 is used.
combination. Initially, the greedy strategy selected
the best individual system and then continued by
adding those systems to the ensemble, which gave
the highest gain in terms of BLEU score according
to the MBR-like system combination method. Note
that the greedy strategy is not guaranteed to increase
the BLEU score of the combined system when a
new system is added to the ensemble of translation
systems.
In a first experiment, we trained approximately
200 systems using different parameter settings in
training. Each system was then used to decode both
the ZHEN-05 and the ZHEN-06 test sentences using
the MBR decision rule. The upper curve in Figure 3
(left) shows the evolution of the BLEU score on
the ZHEN-05 sentences in the course of the number
of selected systems. The upper curve in Figure 3
(right) shows the BLEU score of the consensus
translation as a function of the number of systems
when the selection is done on the ZHEN-06 set. This
serves as an oracle. The middle curve (right) shows
the function of the BLEU score when the system
selection made on the ZHEN-05 set is used in order
to combine the translation outputs for the ZHEN-06
data. Although system combination gave moderate
improvements on top of the primary system, the
greedy strategy still needs further refinements in or-
der to improve generalization. While the correlation
statistics shown in Table 5 indicate that changing the
training parameters helps to substantially decrease
system correlation, there is still need for additional
methods in order to reduce the level of inter-system
BLEU scores such that they fall within the range of
r0.2, 0.3s.
4 Conclusions
In this paper, we presented an empirical study
on how different selections of translation outputs
affect translation quality in system combination.
Composite translations were computed using (i) a
candidate selection method based on inter-system
BLEU score matrices, (ii) an enhanced version of
word sausage networks, and (iii) a novel two-pass
search algorithm which determines and re-orders
bags of words that build the constituents of the final
consensus hypothesis. All methods gave statistically
significant improvements.
We showed that both a high diversity among the
original translation systems and a similar translation
quality among the translation systems are essential
in order to gain substantial improvements on top of
the best individual translation systems.
Experiments were conducted on the NIST portion
of the Chinese English text translation corpus used
for the 2006 NIST machine translation evaluation.
Combined systems were built from primary systems
of up to 16 different research labs as well as systems
derived from one of the best-ranked translation
engines.
We trained a large pool of translation systems
from a single translation engine and presented first
experimental results for a greedy search to select an
ensemble of translation systems for system combi-
nation.
994
References
S. Banerjee and A. Lavie. 2005. METEOR: An
Automatic Metric for MT Evaluation with Improved
Correlation with Human Judgments. In Proceedings
of Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, 43th Annual
Meeting of the Association of Computational Linguis-
tics (ACL-2005), Ann Arbor, MI, USA, June.
S. Bangalore, G. Bodel, and G. Riccardi. 2001. Com-
puting Consensus Translation from Multiple Machine
Translation Systems. In 2001 Automatic Speech
Recognition and Understanding (ASRU) Workshop,
Madonna di Campiglio, Trento, Italy, December.
T. Brants, A. Popat, P. Xu, F. Och, and J. Dean. 2007.
Large Language Models in Machine Tranlation. In
Proceedings of the 2007 Conference on Empirical
Methods in Natural Language Processing, Prague,
Czech Republic. Association for Computational Lin-
guistics.
J. G. Fiscus. 1997. A Post-Processing System to Yield
Reduced Word Error Rates: Recognizer Output Voting
Error Reduction (ROVER). In Proceedings 1997
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 347?352, Santa Barbara, CA,
USA, December.
S. Jayaraman and A. Lavie. 2005. Multi-Engine Ma-
chine Translation Guided by Explicit Word Matching.
In 10th Conference of the European Association for
Machine Translation (EAMT), pages 143?152, Bu-
dapest, Hungary.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. In NAACL ?03: Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology, pages 48?54,
Edmonton, Canada. Association for Computational
Linguistics.
P. Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pages 388?395, Barcelona,
Spain, August. Association for Computational Lin-
guistics.
S. Kumar and W. Byrne. 2004. Minimum Bayes-
Risk Decoding for Statistical Machine Translation.
In Proc. HLT-NAACL, pages 196?176, Boston, MA,
USA, May.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 33?40, Trento, Italy, April.
T. Nomoto. 2004. Multi-Engine Machine Translation
with Voted Language Model. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 494?501,
Barcelona, Spain, July.
E. W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons, Canada.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In 41st Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Machine
Translation. Technical Report RC22176 (W0109-
022), IBM Research Division, Thomas J. Watson
Research Center, Yorktown Heights, NY, USA.
M. Paul, T. Doi, Y. Hwang, K. Imamura, H. Okuma, and
E. Sumita. 2005. Nobody is Perfect: ATR?s Hybrid
Approach to Spoken Language Translation. In Inter-
national Workshop on Spoken Language Translation,
pages 55?62, Pittsburgh, PA, USA, October.
F. Robert and S. Nirenburg. 1994. Three Heads are
Better than One. In Proceedings of the Fourth ACL
Conference on Applied Natural Language Processing,
Stuttgart, Germany, October.
K. C. Sim, W. Byrne, M. Gales, H. Sahbi, and P.C.
Woodland. 2007. Consensus network decoding for
statistical machine translation system combination. In
IEEE Int. Conf. on Acoustics, Speech, and Signal
Processing, Honolulu, HI, USA, April.
M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micci-
ulla, and R. Weischedel. 2006. A Study of Translation
Edit Rate with Targeted Human Annotation. In
Proceedings of Association for Machine Translation in
the Americas.
995
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 620?629,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation
Roy W. Tromble1 and Shankar Kumar2 and Franz Och2 and Wolfgang Macherey2
1Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
royt@jhu.edu
2 Google Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043, USA
{shankarkumar,och,wmach}@google.com
Abstract
We present Minimum Bayes-Risk (MBR) de-
coding over translation lattices that compactly
encode a huge number of translation hypothe-
ses. We describe conditions on the loss func-
tion that will enable efficient implementation
of MBR decoders on lattices. We introduce
an approximation to the BLEU score (Pap-
ineni et al, 2001) that satisfies these condi-
tions. The MBR decoding under this approx-
imate BLEU is realized using Weighted Fi-
nite State Automata. Our experiments show
that the Lattice MBR decoder yields mod-
erate, consistent gains in translation perfor-
mance over N-best MBR decoding on Arabic-
to-English, Chinese-to-English and English-
to-Chinese translation tasks. We conduct a
range of experiments to understand why Lat-
tice MBR improves upon N-best MBR and
study the impact of various parameters on
MBR performance.
1 Introduction
Statistical language processing systems for speech
recognition, machine translation or parsing typically
employ the Maximum A Posteriori (MAP) deci-
sion rule which optimizes the 0-1 loss function. In
contrast, these systems are evaluated using metrics
based on string-edit distance (Word Error Rate), n-
gram overlap (BLEU score (Papineni et al, 2001)),
or precision/recall relative to human annotations.
Minimum Bayes-Risk (MBR) decoding (Bickel and
Doksum, 1977) aims to address this mismatch by se-
lecting the hypothesis that minimizes the expected
error in classification. Thus it directly incorporates
the loss function into the decision criterion. The ap-
proach has been shown to give improvements over
the MAP classifier in many areas of natural lan-
guage processing including automatic speech recog-
nition (Goel and Byrne, 2000), machine transla-
tion (Kumar and Byrne, 2004; Zhang and Gildea,
2008), bilingual word alignment (Kumar and Byrne,
2002), and parsing (Goodman, 1996; Titov and Hen-
derson, 2006; Smith and Smith, 2007).
In statistical machine translation, MBR decoding
is generally implemented by re-ranking an N -best
list of translations produced by a first-pass decoder;
this list typically contains between 100 and 10, 000
hypotheses. Kumar and Byrne (2004) show that
MBR decoding gives optimal performance when the
loss function is matched to the evaluation criterion;
in particular, MBR under the sentence-level BLEU
loss function (Papineni et al, 2001) gives gains on
BLEU. This is despite the fact that the sentence-level
BLEU loss function is an approximation to the exact
corpus-level BLEU.
A different MBR inspired decoding approach is
pursued in Zhang and Gildea (2008) for machine
translation using Synchronous Context Free Gram-
mars. A forest generated by an initial decoding pass
is rescored using dynamic programming to maxi-
mize the expected count of synchronous constituents
in the tree that corresponds to the translation. Since
each constituent adds a new 4-gram to the existing
translation, this approach approximately maximizes
the expected BLEU.
In this paper we explore a different strategy
to perform MBR decoding over Translation Lat-
tices (Ueffing et al, 2002) that compactly encode a
huge number of translation alternatives relative to an
N -best list. This is a model-independent approach
620
in that the lattices could be produced by any statis-
tical MT system ? both phrase-based and syntax-
based systems would work in this framework. We
will introduce conditions on the loss functions that
can be incorporated in Lattice MBR decoding. We
describe an approximation to the BLEU score (Pa-
pineni et al, 2001) that will satisfy these condi-
tions. Our Lattice MBR decoding is realized using
Weighted Finite State Automata.
We expect Lattice MBR decoding to improve
upon N -best MBR primarily because lattices con-
tain many more candidate translations than the N -
best list. This has been demonstrated in speech
recognition (Goel and Byrne, 2000). We conduct
a range of translation experiments to analyze lattice
MBR and compare it with N -best MBR. An impor-
tant aspect of our lattice MBR is the linear approxi-
mation to the BLEU score. We will show that MBR
decoding under this score achieves a performance
that is at least as good as the performance obtained
under sentence-level BLEU score.
The rest of the paper is organized as follows. We
review MBR decoding in Section 2 and give the for-
mulation in terms of a gain function. In Section 3,
we describe the conditions on the gain function for
efficient decoding over a lattice. The implementa-
tion of lattice MBR with Weighted Finite State Au-
tomata is presented in Section 4. In Section 5, we in-
troduce the corpus BLEU approximation that makes
it possible to perform efficient lattice MBR decod-
ing. An example of lattice MBR with a toy lattice
is presented in Section 6. We present lattice MBR
experiments in Section 7. A final discussion is pre-
sented in Section 8.
2 Minimum Bayes Risk Decoding
Minimum Bayes-Risk (MBR) decoding aims to find
the candidate hypothesis that has the least expected
loss under the probability model (Bickel and Dok-
sum, 1977). We begin with a review of MBR decod-
ing for Statistical Machine Translation (SMT).
Statistical MT (Brown et al, 1990; Och and Ney,
2004) can be described as a mapping of a word se-
quence F in the source language to a word sequence
E in the target language; this mapping is produced
by the MT decoder ?(F ). If the reference transla-
tion E is known, the decoder performance can be
measured by the loss function L(E, ?(F )). Given
such a loss function L(E,E?) between an automatic
translation E? and the reference E, and an under-
lying probability model P (E|F ), the MBR decoder
has the following form (Goel and Byrne, 2000; Ku-
mar and Byrne, 2004):
E? = argmin
E??E
R(E?)
= argmin
E??E
?
E?E
L(E,E?)P (E|F ),
where R(E?) denotes the Bayes risk of candidate
translation E? under the loss function L.
If the loss function between any two hypotheses
can be bounded: L(E,E?) ? Lmax, the MBR de-
coder can be rewritten in terms of a gain function
G(E,E?) = Lmax ? L(E,E?):
E? = argmax
E??E
?
E?E
G(E,E?)P (E|F ). (1)
We are interested in performing MBR decoding
under a sentence-level BLEU score (Papineni et al,
2001) which behaves like a gain function: it varies
between 0 and 1, and a larger value reflects a higher
similarity. We will therefore use Equation 1 as the
MBR decoder.
We note that E represents the space of transla-
tions. For N -best MBR, this space E is the N -best
list produced by a baseline decoder. We will investi-
gate the use of a translation lattice for MBR decod-
ing; in this case, E will represent the set of candi-
dates encoded in the lattice.
In general, MBR decoding can use different
spaces for hypothesis selection and risk computa-
tion: argmax and the sum in Equation 1 (Goel,
2001). As an example, the hypothesis could be se-
lected from the N -best list while the risk is com-
puted based on the entire lattice. Therefore, the
MBR decoder can be more generally written as fol-
lows:
E? = argmax
E??Eh
?
E?Ee
G(E,E?)P (E|F ), (2)
where Eh refers to the Hypothesis space from where
the translations are chosen, and Ee refers to the Evi-
dence space that is used for computing the Bayes-
risk. We will present experiments (Section 7) to
show the relative importance of these two spaces.
621
3 Lattice MBR Decoding
We now present MBR decoding on translation lat-
tices. A translation word lattice is a compact rep-
resentation for very large N -best lists of transla-
tion hypotheses and their likelihoods. Formally,
it is an acyclic Weighted Finite State Acceptor
(WFSA) (Mohri, 2002) consisting of states and arcs
representing transitions between states. Each arc is
labeled with a word and a weight. Each path in the
lattice, consisting of consecutive transitions begin-
ning at the distinguished initial state and ending at a
final state, expresses a candidate translation. Aggre-
gation of the weights along the path1 produces the
weight of the path?s candidate H(E,F ) according
to the model. In our setting, this weight will imply
the posterior probability of the translation E given
the source sentence F :
P (E|F ) =
exp (?H(E,F ))
?
E??E exp (?H(E
?, F ))
. (3)
The scaling factor ? ? [0,?) flattens the distribu-
tion when ? < 1, and sharpens it when ? > 1.
Because a lattice may represent a number of can-
didates exponential in the size of its state set, it is of-
ten impractical to compute the MBR decoder (Equa-
tion 1) directly. However, if we can express the gain
function G as a sum of local gain functions gi, then
we now show that Equation 1 can be refactored and
the MBR decoder can be computed efficiently. We
loosely call a gain function local if it can be ap-
plied to all paths in the lattice via WFSA intersec-
tion (Mohri, 2002) without significantly multiplying
the number of states.
In this paper, we are primarily concerned with lo-
cal gain functions that weight n-grams. Let N =
{w1, . . . , w|N |} be the set of n-grams and let a local
gain function gw : E ? E ? R, for w ? N , be as
follows:
gw(E,E
?) = ?w#w(E
?)?w(E), (4)
where ?w is a constant, #w(E?) is the number of
times that w occurs in E?, and ?w(E) is 1 if w ? E
and 0 otherwise. That is, gw is ?w times the number
of occurrences of w in E?, or zero if w does not oc-
cur in E. We first assume that the overall gain func-
tion G(E,E?) can then be written as a sum of local
1using the log semiring?s extend operator
gain functions and a constant ?0 times the length of
the hypothesis E?.
G(E,E?) = ?0|E
?|+
?
w?N
gw(E,E
?) (5)
= ?0|E
?|+
?
w?N
?w#w(E
?)?w(E)
Given a gain function of this form, we can rewrite
the risk (sum in Equation 1) as follows
?
E?E
G(E,E?)P (E|F )
=
?
E?E
(
?0|E
?|+
?
w?N
?w#w(E
?)?w(E)
)
P (E|F )
= ?0|E
?|+
?
w?N
?w#w(E
?)
?
E?Ew
P (E|F ),
where Ew = {E ? E|?w(E) > 0} represents the
paths of the lattice containing the n-gram w at least
once. TheMBR decoder on lattices (Equation 1) can
therefore be written as
E? = argmax
E??E
{
?0|E
?|+
?
w?N
?w#w(E
?)p(w|E)
}
. (6)
Here p(w|E) =
?
E?Ew P (E|F ) is the posterior
probability of the n-gram w in the lattice. We have
thus replaced a summation over a possibly exponen-
tial number of items (E ? E) with a summation over
the number of n-grams that occur in E , which is at
worst polynomial in the number of edges in the lat-
tice that defines E . We compute the posterior proba-
bility of each n-gram w as:
p(w|E) =
?
E?Ew
P (E|F ) =
Z(Ew)
Z(E)
, (7)
where Z(E) =
?
E??E exp(?H(E
?, F )) (denomi-
nator in Equation 3) and
Z(Ew) =
?
E??Ew exp(?H(E
?, F )). Z(E) and
Z(Ew) represent the sums2 of weights of all paths
in the lattices Ew and E respectively.
4 WFSA MBR Computations
We now show how the Lattice MBR Decision Rule
(Equation 6) can be implemented using Weighted
Finite State Automata (Mohri, 1997). There are four
steps involved in decoding starting from weighted
finite-state automata representing the candidate out-
puts of a translation system. We will describe these
2in the log semiring, where log+(x, y) = log(ex + ey) is
the collect operator (Mohri, 2002)
622
steps in the setting where the evidence lattice Ee may
be different from the hypothesis lattice Eh (Equa-
tion 2).
1. Extract the set of n-grams that occur in the ev-
idence lattice Ee. For the usual BLEU score, n
ranges from one to four.
2. Compute the posterior probability p(w|E) of
each of these n-grams.
3. Intersect each n-gram w, with an appropriate
weight (from Equation 6), to an initially un-
weighted copy of the hypothesis lattice Eh.
4. Find the best path in the resulting automaton.
Computing the set of n-grams N that occur in a
finite automaton requires a traversal, in topological
order, of all the arcs in the automaton. Because the
lattice is acyclic, this is possible. Each state q in the
automaton has a corresponding set of n-grams Nq
ending there.
1. For each state q,Nq is initialized to {}, the set
containing the empty n-gram.
2. Each arc in the automaton extends each of its
source state?s n-grams by its word label, and
adds the resulting n-grams to the set of its tar-
get state. ( arcs do not extend n-grams, but
transfer them unchanged.) n-grams longer than
the desired order are discarded.
3. N is the union over all states q of Nq.
Given an n-gram, w, we construct an automaton
matching any path containing the n-gram, and in-
tersect that automaton with the lattice to find the set
of paths containing the n-gram (Ew in Equation 7).
Suppose E represent the weighted lattice, we com-
pute3: Ew = E ? (w w ??), where w = (?? w ??)
is the language that contains all strings that do not
contain the n-gram w. The posterior probability
p(w|E) of n-gram w can be computed as a ratio of
the total weights of paths in Ew to the total weights
of paths in the original lattice (Equation 7).
For each n-gram w ? N , we then construct
an automaton that accepts an input E with weight
3in the log semiring (Mohri, 2002)
equal to the product of the number of times the n-
gram occurs in the input (#w(E)), the n-gram fac-
tor ?w from Equation 6, and the posterior proba-
bility p(w|E). The automaton corresponds to the
weighted regular expression (Karttunen et al, 1996):
w?(w/(?wp(w|E)) w?)?.
We successively intersect each of these automata
with an automaton that begins as an unweighted
copy of the lattice Eh. This automaton must also
incorporate the factor ?0 of each word. This can
be accomplished by intersecting the unweighted lat-
tice with the automaton accepting (?/?0)?. The
resulting MBR automaton computes the total ex-
pected gain of each path. A path in this automa-
ton that corresponds to the word sequence E? has
cost: ?0|E?|+
?
w?N ?w#w(E)p(w|E) (expression
within the curly brackets in Equation 6).
Finally, we extract the best path from the resulting
automaton4, giving the lattice MBR candidate trans-
lation according to the gain function (Equation 6).
5 Linear Corpus BLEU
Our Lattice MBR formulation relies on the decom-
position of the overall gain function as a sum of lo-
cal gain functions (Equation 5). We here describe a
linear approximation to the log(BLEU score) (Pap-
ineni et al, 2001) which allows such a decomposi-
tion. This will enable us to rewrite the log(BLEU)
as a linear function of n-gram matches and the hy-
pothesis length. Our strategy will be to use a first
order Taylor-series approximation to what we call
the corpus log(BLEU) gain: the change in corpus
log(BLEU) contributed by the sentence relative to
not including that sentence in the corpus.
Let r be the reference length of the corpus, c0 the
candidate length, and {cn|1 ? n ? 4} the number
of n-gram matches. Then, the corpus BLEU score
B(r, c0, cn) can be defined as follows (Papineni et
al., 2001):
logB = min
(
0, 1?
r
c0
)
+
1
4
4?
n=1
log
cn
c0 ??n
,
? min
(
0, 1?
r
c0
)
+
1
4
4?
n=1
log
cn
c0
,
where we have ignored ?n, the difference between
the number of words in the candidate and the num-
4in the (max,+) semiring (Mohri, 2002)
623
ber of n-grams. If L is the average sentence length
in the corpus, ?n ? (n? 1)
c0
L .
The corpus log(BLEU) gain is defined as the
change in log(BLEU) when a new sentence?s (E?)
statistics are added to the corpus statistics:
G = logB? ? logB,
where the counts in B? are those of B plus those for
the current sentence. We will assume that the brevity
penalty (first term in the above approximation) does
not change when adding the new sentence. In exper-
iments not reported here, we found that taking into
account the brevity penalty at the sentence level can
cause large fluctuations in lattice MBR performance
on different test sets. We therefore treat only cns as
variables.
The corpus log BLEU gain is approximated by a
first-order vector Taylor series expansion about the
initial values of cn.
G ?
N?
n=0
(c?n ? cn)
? logB?
?c?n
?
?
?
?
c?n=cn
, (8)
where the partial derivatives are given by
? logB
?c0
=
?1
c0
, (9)
? logB
?cn
=
1
4cn
.
Substituting the derivatives in Equation 8 gives
G = ? logB ? ?
?c0
c0
+
1
4
4?
n=1
?cn
cn
, (10)
where each ?cn = c?n ? cn counts the statistic in
the sentence of interest, rather than the corpus as a
whole. This score is therefore a linear function in
counts of words ?c0 and n-gram matches ?cn. Our
approach ignores the count clipping present in the
exact BLEU score where a correct n-gram present
once in the reference but several times in the hypoth-
esis will be counted only once as correct. Such an
approach is also followed in Dreyer et al (2007).
Using the above first-order approximation to gain
in log corpus BLEU, Equation 9 implies that ?0, ?w
from Section 3 would have the following values:
?0 =
?1
c0
(11)
?w =
1
4c|w|
.
5.1 N-gram Factors
We now describe how the n-gram factors (Equa-
tion 11) are computed. The factors depend on
a set of n-gram matches and counts (cn; n ?
{0, 1, 2, 3, 4}). These factors could be obtained from
a decoding run on a development set. However, do-
ing so could make the performance of lattice MBR
very sensitive to the actual BLEU scores on a partic-
ular run. We would like to avoid such a dependence
and instead, obtain a set of parameters which can
be estimated from multiple decoding runs without
MBR. To achieve this, we make use of the properties
of n-gram matches. It is known that the average n-
gram precisions decay approximately exponentially
with n (Papineni et al, 2001). We now assume that
the number of matches of each n-gram is a constant
ratio r times the matches of the corresponding n? 1
gram.
If the unigram precision is p, we can obtain the
n-gram factors (n ? {1, 2, 3, 4}) (Equation 11) as a
function of the parameters p and r, and the number
of unigram tokens T :
?0 =
?1
T
(12)
?n =
1
4Tp? rn?1
We set p and r to the average values of unigram pre-
cision and precision ratio across multiple develop-
ment sets. Substituting the above factors in Equa-
tion 6, we find that the MBR decision does not de-
pend on T ; therefore any value of T can be used.
6 An Example
Figure 1 shows a toy lattice and the final MBR au-
tomaton (Section 4) for BLEU with a maximum n-
gram order of 2. We note that the MBR hypothesis
(bcde) has a higher decoder cost relative to the MAP
hypothesis (abde). However, bcde gets a higher ex-
pected gain (Equation 6) than abde since it shares
more n-grams with the Rank-3 hypothesis (bcda).
This illustrates how a lattice can help select MBR
translations that can differ from the MAP transla-
tion.
7 Experiments
We now present experiments to evaluate MBR de-
coding on lattices under the linear corpus BLEU
624
01
2
3
4
5
6
7
8
9
10
0
1
2
3
4
7
5
8
6
c/0.013
d/0.013
d/?0.008
d/?0.008
e/0.004
a/0.038
a/0.5
b/0.6
b/0.6
b/0.6
c/0.6
c/0.6
d/0.3
d/0.4
e/0.5
a/0.5
a/0.063
b/0.043
b/0.043
b/0.013
c/0.013
Figure 1: An example translation lattice with decoder
costs (top) and its MBR Automaton for BLEU-2 (bot-
tom). The bold path in the top is the MAP hypothesis
and the bold path in the bottom is the MBR hypothe-
sis. The precision parameters in Equation 12 are set to:
T = 10, p = 0.85, r = 0.72.
Dataset # of sentences
aren zhen enzh
dev1 1353 1788 1664
dev2 663 919 919
blind 1360 1357 1859
Table 1: Statistics over the development and test sets.
gain. We start with a description of the data sets
and the SMT system.
7.1 Development and Blind Test Sets
We present our experiments on the constrained data
track of the NIST 2008 Arabic-to-English (aren),
Chinese-to-English (zhen), and English-to-Chinese
(enzh) machine translation tasks.5 In all language
pairs, the parallel and monolingual data consists of
all the allowed training sets in the constrained track.
For each language pair, we use two development
sets: one for Minimum Error Rate Training (Och,
2003; Macherey et al, 2008), and the other for tun-
ing the scale factor for MBR decoding. Our devel-
opment sets consists of the NIST 2004/2003 evalu-
ation sets for both aren and zhen, and NIST 2006
(NIST portion)/2003 evaluation sets for enzh. We
report results on NIST 2008 which is our blind test
set. Statistics computed over these data sets are re-
ported in Table 1.
5http://www.nist.gov/speech/tests/mt/
7.2 MT System Description
Our phrase-based statistical MT system is similar to
the alignment template system described in Och and
Ney (2004). The system is trained on parallel cor-
pora allowed in the constrained track. We first per-
form sentence and sub-sentence chunk alignment on
the parallel documents. We then train word align-
ment models (Och and Ney, 2003) using 6 Model-1
iterations and 6 HMM iterations. An additional 2 it-
erations of Model-4 are performed for zhen and enzh
pairs. Word Alignments in both source-to-target
and target-to-source directions are obtained using
the Maximum A-Posteriori (MAP) framework (Ma-
tusov et al, 2004). An inventory of phrase-pairs
up to length 5 is then extracted from the union of
source-target and target-source alignments. Several
feature functions are then computed over the phrase-
pairs. 5-gram word language models are trained on
the allowed monolingual corpora. Minimum Error
Rate Training under BLEU is used for estimating
approximately 20 feature function weights over the
dev1 development set.
Translation is performed using a standard dy-
namic programming beam-search decoder (Och and
Ney, 2004) using two decoding passes. The first de-
coder pass generates either a lattice or anN -best list.
MBR decoding is performed in the second pass. The
MBR scaling parameter (? in Equation 3) is tuned
on the dev2 development set.
7.3 Translation Results
We next report translation results from lattice MBR
decoding. All results will be presented on the NIST
2008 evaluation sets. We report results using the
NIST implementation of the BLEU score which
computes the brevity penalty using the shortest ref-
erence translation for each segment (NIST, 2002
2008). The BLEU scores are reported at the word-
level for aren and zhen but at the character level for
enzh. We measure statistical significance using 95%
confidence intervals computed with paired bootstrap
resampling (Koehn, 2004). In all tables, systems in a
column show statistically significant differences un-
less marked with an asterisk.
We first compare lattice MBR toN -best MBR de-
coding and MAP decoding (Table 2). In these ex-
periments, we hold the likelihood scaling factor ? a
625
BLEU(%)
aren zhen enzh
MAP 43.7 27.9 41.4
N -best MBR 43.9 28.3? 42.0
Lattice MBR 44.9 28.5? 42.6
Table 2: Lattice MBR, N -best MBR & MAP decoding.
On zhen, Lattice MBR and N -best MBR do not show
statistically significant differences.
constant; it is set to 0.2 for aren and enzh, and 0.1
for zhen. The translation lattices are pruned using
Forward-Backward pruning (Sixtus and Ortmanns,
1999) so that the average numbers of arcs per word
(lattice density) is 30. For N -best MBR, we use
N -best lists of size 1000. To match the loss func-
tion, Lattice MBR is performed at the word level for
aren/zhen and at the character level for enzh. Our
lattice MBR is implemented using the Google Open-
Fst library.6 In our experiments, p, r (Equation 12)
have values of 0.85/0.72, 0.80/0.62, and 0.63/0.48
for aren, zhen, and enzh respectively.
We note that Lattice MBR provides gains of 0.2-
1.0 BLEU points over N -best MBR, which in turn
gives 0.2-0.6 BLEU points over MAP. These gains
are obtained on top of a baseline system that has
competitive performance relative to the results re-
ported in the NIST 2008 Evaluation.7 This demon-
strates the effectiveness of lattice MBR decoding as
a realization of MBR decoding which yields sub-
stantial gains over the N -best implementation.
The gains from lattice MBR over N -best MBR
could be due to a combination of factors. These in-
clude: 1) better approximation of the corpus BLEU
score, 2) larger hypothesis space, and 3) larger evi-
dence space. We now present experiments to tease
apart these factors.
Our first experiment restricts both the hypothesis
and evidence spaces in lattice MBR to the 1000-best
list (Table 3). We compare this toN -best MBRwith:
a) sentence-level BLEU, and b) sentence-level log
BLEU.
The results show that when restricted to the 1000-
best list, Lattice MBR performs slightly better than
N -best MBR (with sentence BLEU) on aren/enzh
while N -best MBR is better on zhen. We hypothe-
6http://www.openfst.org/
7
http://www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
BLEU(%)
aren zhen enzh
Lattice MBR, Lin. Corpus BLEU 44.2 28.1 42.2
N -best MBR, Sent. BLEU 43.9? 28.3? 42.0?
N -best MBR, Sent. Log BLEU 44.0? 28.3? 41.9?
Table 3: Lattice and N-best MBR (with Sentence
BLEU/Sentence log BLEU) on a 1000-best list. In each
column, entries with an asterisk do not show statistically
significant differences.
BLEU(%)
Hyp Space Evid Space aren zhen enzh
Lattice Lattice 44.9 28.5 42.6
1000-best Lattice 44.6 28.5 42.6
Lattice 1000-best 44.1? 28.0? 42.1
1000-best 1000-best 44.2? 28.1? 42.2
Table 4: Lattice MBR with restrictions on hypothesis and
evidence spaces. In each column, entries with an asterisk
do not show statistically significant differences.
size that on aren/enzh, the linear corpus BLEU gain
(Equation 10) is better correlated to the actual cor-
pus BLEU than sentence-level BLEU while the op-
posite is true on zhen. N -best MBR gives similar
results with either sentence BLEU or sentence log
BLEU. This confirms that using a log BLEU score
does not change the outcome of MBR decoding and
further justifies our Taylor-series approximation of
the log BLEU score.
We next attempt to understand factors 2 and 3. To
do that, we carry out lattice MBR when either the
hypothesis or the evidence space in Equation 2 is re-
stricted to 1000-best hypotheses (Table 4). For com-
parison, we also include results from lattice MBR
when both hypothesis and evidence spaces are iden-
tical: either the full lattice or the 1000-best list (from
Tables 2 and 3).
These results show that lattice MBR results are
almost unchanged when the hypothesis space is re-
stricted to a 1000-best list. However, when the ev-
idence space is shrunk to a 1000-best list, there is
a significant degradation in performance; these lat-
ter results are almost identical to the scenario when
both evidence and hypothesis spaces are restricted
to the 1000-best list. This experiment throws light
on what makes lattice MBR effective over N -best
MBR. Relative to the N -best list, the translation lat-
tice provides a better estimate of the expected BLEU
score. On the other hand, there are few hypotheses
626
outside the 1000-best list which are selected by lat-
tice MBR.
Finally, we show how the performance of lattice
MBR changes as a function of the lattice density.
The lattice density is the average number of arcs per
word and can be varied using Forward-Backward
pruning (Sixtus and Ortmanns, 1999). Figure 2 re-
ports the average number of lattice paths and BLEU
scores as a function of lattice density. The results
show that Lattice MBR performance generally im-
proves when the size of the lattice is increased.
However, on zhen, there is a small drop beyond a
density of 10. This could be due to low quality (low
posterior probability) hypotheses that get included at
the larger densities and result in a poorer estimate of
the expected BLEU score. On aren and enzh, there
are some gains beyond a lattice density of 30. These
gains are relatively small and come at the expense
of higher memory usage; we therefore work with a
lattice density of 30 in all our experiments. We note
that Lattice MBR is operating over lattices which are
gigantic in comparison to the number of paths in an
N -best list. At a lattice density of 30, the lattices in
aren contain on an average about 1081 hypotheses!
7.4 Lattice MBR Scale Factor
We next examine the role of the scale factor ? in
lattice MBR decoding. The MBR scale factor de-
termines the flatness of the posterior distribution
(Equation 3). It is chosen using a grid search on the
dev2 set (Table 1). Figure 3 shows the variation in
BLEU scores on eval08 as this parameter is varied.
The results show that it is important to tune this fac-
tor. The optimal scale factor is identical for all three
language pairs. In experiments not reported in this
paper, we have found that the optimal scaling factor
on a moderately sized development set carries over
to unseen test sets.
7.5 Maximum n-gram Order
Lattice MBR Decoding (Equation 6) involves com-
puting a posterior probability for each n-gram in the
lattice. We would like to speed up the Lattice MBR
computation (Section 4) by restricting the maximum
order of the n-grams in the procedure. The results
(Table 5) show that on aren, there is no degradation
if we limit the maximum order of the n-grams to
3. However, on zhen/enzh, there is improvement by
BLEU(%)
Max n-gram order aren zhen enzh
1 38.7 26.8 40.0
2 44.1 27.4 42.2
3 44.9 28.0 42.4
4 44.9 28.5 42.6
Table 5: Lattice MBR as a function of max n-gram order.
considering 4-grams. We can therefore reduce Lat-
tice MBR computations in aren.
8 Discussion
We have presented a procedure for performing Min-
imum Bayes-Risk Decoding on translation lattices.
This is a significant development in that the MBR
decoder operates over a very large number of trans-
lations. In contrast, the current N -best implementa-
tion of MBR can be scaled to, at most, a few thou-
sands of hypotheses. If the number of hypotheses
is greater than, say 20,000, the N -best MBR be-
comes computationally expensive. The lattice MBR
technique is efficient when performed over enor-
mous number of hypotheses (up to 1080) since it
takes advantage of the compact structure of the lat-
tice. Lattice MBR gives consistent improvements in
translation performance over N -best MBR decod-
ing, which is used in many state-of-the-art research
translation systems. Moreover, we see gains on three
different language pairs.
There are two potential reasons why Lattice MBR
decoding could outperform N -best MBR: a larger
hypothesis space from which translations could be
selected or a larger evidence space for computing the
expected loss. Our experiments show that the main
improvement comes from the larger evidence space:
a larger set of translations in the lattice provides a
better estimate of the expected BLEU score. In other
words, the lattice provides a better posterior distri-
bution over translation hypotheses relative to an N -
best list. This is a novel insight into the workings
of MBR decoding. We believe this could be possi-
bly employed when designing discriminative train-
ing approaches for machine translation. More gener-
ally, we have found a component in machine transla-
tion where the posterior distribution over hypotheses
plays a crucial role.
We have shown the effect of the MBR scaling fac-
627
10 20 30 40
44
44.2
44.4
44.6
44.8
45 aren
         Lattice Density
33
85
121
161
187
208
B
L
E
U
(
%
)
10 20 30 4028.2
28.3
28.4
28.5
28.6
28.7 zhen
Lattice Density
6
22
37
49
59
65
10 20 30 4041.8
42
42.2
42.4
42.6 enzh
      Lattice Density
3
10
17
25
30
34
Figure 2: Lattice MBR vs. lattice density: aren/zhen/enzh. Each point also shows the loge(Avg. # of paths).
0 0.2 0.4 0.6 0.8 1
44
44.2
44.4
44.6
44.8 aren
Scale Factor 
B
L
E
U
(
%
)
0 0.2 0.4 0.6 0.8 127.9
28
28.1
28.2
28.3
28.4
28.5 zhen
Scale Factor
0 0.2 0.4 0.6 0.8 1
41.8
42
42.2
42.4
42.6
42.8 enzh
Scale Factor
Figure 3: Lattice MBR with various scale factors ?: aren/zhen/enzh.
tor on the performance of lattice MBR. The scale
factor determines the flatness of the posterior distri-
bution over translation hypotheses. A scale of 0.0
means a uniform distribution while 1.0 implies that
there is no scaling. This is an important parameter
that needs to be tuned on a development set. There
has been prior work in MBR speech recognition and
machine translation (Goel and Byrne, 2000; Ehling
et al, 2007) which has shown the need for tuning
this factor. Our MT system parameters are trained
with Minimum Error Rate Training which assigns a
very high posterior probability to the MAP transla-
tion. As a result, it is necessary to flatten the prob-
ability distribution so that MBR decoding can select
hypotheses other than the MAP hypothesis.
Our Lattice MBR implementation is made pos-
sible due to the linear approximation of the BLEU
score. This linearization technique has been applied
elsewhere when working with BLEU: Smith and
Eisner (2006) approximate the expectation of log
BLEU score. In both cases, a linear metric makes
it easier to compute the expectation. While we have
applied lattice MBR decoding to the approximate
BLEU score, we note that our procedure (Section 3)
is applicable to other gain functions which can be
decomposed as a sum of local gain functions. In par-
ticular, our framework might be useful with transla-
tion metrics such as TER (Snover et al, 2006) or
METEOR (Lavie and Agarwal, 2007).
In contrast to a phrase-based SMT system, a syn-
tax based SMT system (e.g. Zollmann and Venu-
gopal (2006)) can generate a hypergraph that rep-
resents a generalized translation lattice with words
and hidden tree structures. We believe that our lat-
tice MBR framework can be extended to such hy-
pergraphs with loss functions that take into account
both BLEU scores as well as parse tree structures.
Lattice and Forest based search and training pro-
cedures are not yet common in statistical machine
translation. However, they are promising because
the search space of translations is much larger than
the typical N -best list (Mi et al, 2008). We hope
that our approach will provide some insight into the
design of lattice-based search procedures along with
the use of non-linear, global loss functions such as
BLEU.
References
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected topics. Holden-
Day Inc., Oakland, CA, USA.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J . Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S.
628
Roossin. 1990. A Statistical Approach to Machine
Translation. Computational Linguistics, 16(2):79?85.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing Reordering Constraints for SMT Using Efficient
BLEU Oracle Computation. In SSST, NAACL-HLT
2007, pages 103?110, Rochester, NY, USA, April.
N. Ehling, R. Zens, and H. Ney. 2007. Minimum Bayes
Risk Decoding for BLEU. In ACL 2007, pages 101?
104, Prague, Czech Republic, June.
V. Goel and W. Byrne. 2000. Minimum Bayes-Risk Au-
tomatic Speech Recognition. Computer Speech and
Language, 14(2):115?135.
V. Goel. 2001. Minimum Bayes-Risk Automatic Speech
Recognition. Ph.D. thesis, Johns Hopkins University,
Baltimore, MD, USA.
J. Goodman. 1996. Parsing Algorithms and Metrics. In
ACL, pages 177?183, Santa Cruz, CA, USA.
L. Karttunen, J-p. Chanod, G. Grefenstette, and
A. Schiller. 1996. Regular Expressions for Language
Engineering. Natural Language Engineering, 2:305?
328.
P. Koehn. 2004. Statistical Significance Tests for Ma-
chine Translation Evaluation. In EMNLP, Barcelona,
Spain.
S. Kumar and W. Byrne. 2002. Minimum Bayes-Risk
word alignments of bilingual texts. In EMNLP, pages
140?147, Philadelphia, PA, USA.
S. Kumar and W. Byrne. 2004. Minimum Bayes-Risk
Decoding for Statistical Machine Translation. In HLT-
NAACL, pages 169?176, Boston, MA, USA.
A. Lavie and A. Agarwal. 2007. METEOR: An Auto-
matic Metric for MT Evaluation with High Levels of
Correlation with Human Judgments. In SMT Work-
shop, ACL, pages 228?231, Prague, Czech Republic.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based Minimum Error Rate Training for Sta-
tistical Machine Translation. In EMNLP, Honolulu,
Hawaii, USA.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
Word Alignments for Statistical Machine Translation.
In COLING, Geneva, Switzerland.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-Based Trans-
lation. In ACL, Columbus, OH, USA.
M.Mohri. 1997. Finite-state transducers in language and
speech processing. Computational Linguistics, 23(3).
M. Mohri. 2002. Semiring frameworks and algorithms
for shortest-distance problems. Journal of Automata,
Languages and Combinatorics, 7(3):321?350.
NIST. 2002-2008. The NIST Machine Translation Eval-
uations. http://www.nist.gov/speech/tests/mt/.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19 ? 51.
F. Och and H. Ney. 2004. The Alignment Template Ap-
proach to Statistical Machine Translation. Computa-
tional Linguistics, 30(4):417 ? 449.
F. Och. 2003. Minimum Error Rate Training in Statisti-
cal Machine Translation. In ACL, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Machine
Translation. Technical Report RC22176 (W0109-
022), IBM Research Division.
A. Sixtus and S. Ortmanns. 1999. High Quality
Word Graphs Using Forward-Backward Pruning. In
ICASSP, Phoenix, AZ, USA.
D. Smith and J. Eisner. 2006. Minimum Risk Anneal-
ing for Training Log-Linear Models. In ACL, Sydney,
Australia.
D. Smith and N. Smith. 2007. Probabilistic models of
nonprojective dependency trees. In EMNLP-CoNLL,
Prague, Czech Republic.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In AMTA, Boston,
MA, USA.
I. Titov and J. Henderson. 2006. Loss Minimization in
Parse Reranking. In EMNLP, Sydney, Australia.
N. Ueffing, F. Och, and H. Ney. 2002. Generation of
Word Graphs in Statistical Machine Translation. In
EMNLP, Philadelphia, PA, USA.
H. Zhang and D. Gildea. 2008. Efficient Multi-pass De-
coding for Synchronous Context Free Grammars. In
ACL, Columbus, OH, USA.
A. Zollmann and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In
HLT-NAACL, New York, NY, USA.
629
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 725?734,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Lattice-based Minimum Error Rate Training
for Statistical Machine Translation
Wolfgang Macherey Franz Josef Och Ignacio Thayer Jakob Uszkoreit
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
{wmach,och,thayer,uszkoreit}@google.com
Abstract
Minimum Error Rate Training (MERT) is an
effective means to estimate the feature func-
tion weights of a linear model such that an
automated evaluation criterion for measuring
system performance can directly be optimized
in training. To accomplish this, the training
procedure determines for each feature func-
tion its exact error surface on a given set of
candidate translations. The feature function
weights are then adjusted by traversing the
error surface combined over all sentences and
picking those values for which the resulting
error count reaches a minimum. Typically,
candidates in MERT are represented as N -
best lists which contain the N most probable
translation hypotheses produced by a decoder.
In this paper, we present a novel algorithm that
allows for efficiently constructing and repre-
senting the exact error surface of all trans-
lations that are encoded in a phrase lattice.
Compared to N -best MERT, the number of
candidate translations thus taken into account
increases by several orders of magnitudes.
The proposed method is used to train the
feature function weights of a phrase-based
statistical machine translation system. Experi-
ments conducted on the NIST 2008 translation
tasks show significant runtime improvements
and moderate BLEU score gains over N -best
MERT.
1 Introduction
Many statistical methods in natural language pro-
cessing aim at minimizing the probability of sen-
tence errors. In practice, however, system quality
is often measured based on error metrics that assign
non-uniform costs to classification errors and thus
go far beyond counting the number of wrong de-
cisions. Examples are the mean average precision
for ranked retrieval, the F-measure for parsing, and
the BLEU score for statistical machine transla-
tion (SMT). A class of training criteria that provides
a tighter connection between the decision rule and
the final error metric is known as Minimum Error
Rate Training (MERT) and has been suggested for
SMT in (Och, 2003).
MERT aims at estimating the model parameters
such that the decision under the zero-one loss func-
tion maximizes some end-to-end performance mea-
sure on a development corpus. In combination with
log-linear models, the training procedure allows for
a direct optimization of the unsmoothed error count.
The criterion can be derived from Bayes? decision
rule as follows: Let f  f1, ..., fJ denote a source
sentence (?French?) which is to be translated into a
target sentence (?English?) e  e1, ..., eI . Under
the zero-one loss function, the translation which
maximizes the a posteriori probability is chosen:
e?  argmax
e
 
Prpe|fq
( (1)
Since the true posterior distribution is unknown,
Prpe|fq is modeled via a log-linear translation model
which combines some feature functions hmpe, fq
with feature function weights ?m, m  1, ...,M :
Prpe|fq  p?M1 pe|fq

exp

?M
m1 ?mhmpe, fq

?
e1 exp

?M
m1 ?mhmpe1, fq

(2)
The feature function weights are the parameters of
the model, and the objective of the MERT criterion
is to find a parameter set ?M1 that minimizes the error
count on a representative set of training sentences.
More precisely, let fS1 denote the source sentences
of a training corpus with given reference translations
725
rS1 , and let Cs  tes,1, ..., es,Ku denote a set of K
candidate translations. Assuming that the corpus-
based error count for some translations eS1 is addi-
tively decomposable into the error counts of the indi-
vidual sentences, i.e., EprS1 , eS1 q 
?S
s1 Eprs, esq,
the MERT criterion is given as:
??M1  argmin
?M1
#
S?
s1
E
 
rs, e?pfs;?M1 q

+
(3)
 argmin
?M1
#
S?
s1
K?
k1
Eprs, es,kq?
 
e?pfs;?M1 q, es,k

+
with
e?pfs;?M1 q  argmaxe
#
M?
m1
?mhmpe, fsq
+
(4)
In (Och, 2003), it was shown that linear models can
effectively be trained under the MERT criterion us-
ing a special line optimization algorithm. This line
optimization determines for each feature function
hm and sentence fs the exact error surface on a set
of candidate translations Cs. The feature function
weights are then adjusted by traversing the error
surface combined over all sentences in the training
corpus and moving the weights to a point where the
resulting error reaches a minimum.
Candidate translations in MERT are typically rep-
resented as N -best lists which contain the N most
probable translation hypotheses. A downside of this
approach is, however, that N -best lists can only
capture a very small fraction of the search space.
As a consequence, the line optimization algorithm
needs to repeatedly translate the development corpus
and enlarge the candidate repositories with newly
found hypotheses in order to avoid overfitting on Cs
and preventing the optimization procedure from
stopping in a poor local optimum.
In this paper, we present a novel algorithm that
allows for efficiently constructing and representing
the unsmoothed error surface for all translations
that are encoded in a phrase lattice. The number
of candidate translations thus taken into account
increases by several orders of magnitudes compared
to N -best MERT. Lattice MERT is shown to yield
significantly faster convergence rates while it ex-
plores a much larger space of candidate translations
which is exponential in the lattice size. Despite
this vast search space, we show that the suggested
algorithm is always efficient in both running time
and memory.
The remainder of this paper is organized as fol-
lows. Section 2 briefly reviews N -best MERT and
introduces some basic concepts that are used in
order to develop the line optimization algorithm for
phrase lattices in Section 3. Section 4 presents an
upper bound on the complexity of the unsmoothed
error surface for the translation hypotheses repre-
sented in a phrase lattice. This upper bound is
used to prove the space and runtime efficiency of
the suggested algorithm. Section 5 lists some best
practices for MERT. Section 6 discusses related
work. Section 7 reports on experiments conducted
on the NIST 2008 translation tasks. The paper
concludes with a summary in Section 8.
2 Minimum Error Rate Training on
N -best Lists
The goal of MERT is to find a weights set that
minimizes the unsmoothed error count on a rep-
resentative training corpus (cf. Eq. (3)). This
can be accomplished through a sequence of line
minimizations along some vector directions tdM1 u.
Starting from an initial point ?M1 , computing the
most probable sentence hypothesis out of a set of K
candidate translations Cs  te1, ..., eKu along the
line ?M1   ?  dM1 results in the following optimiza-
tion problem (Och, 2003):
e?pfs; ?q  argmax
ePCs
!
p?M1   ?  dM1 qJ  hM1 pe, fsq
)
 argmax
ePCs
"
?
m
?mhmpe, fsq
loooooooomoooooooon
ape,fsq
  ? 
?
m
dmhmpe, fsq
loooooooomoooooooon
bpe,fsq
*
 argmax
ePCs
 
ape, fsq   ?  bpe, fsq
looooooooooomooooooooooon
pq
( (5)
Hence, the total score pq for any candidate trans-
lation corresponds to a line in the plane with ? as
the independent variable. For any particular choice
of ?, the decoder seeks that translation which yields
the largest score and therefore corresponds to the
topmost line segment.
Overall, the candidate repository Cs defines K
lines where each line may be divided into at most
K line segments due to possible intersections with
the other K  1 lines. The sequence of the topmost
line segments constitute the upper envelope which
is the pointwise maximum over all lines induced by
Cs. The upper envelope is a convex hull and can
be inscribed with a convex polygon whose edges
are the segments of a piecewise linear function in ?
(Papineni, 1999; Och, 2003):
Envpfq  max
ePC
 
ape, fq   ?  bpe, fq : ? P R
( (6)
726
Score
?
Error
count
?
0
0
e1
e2
e5
e6
e8
e1e2
e3
e4
e5
e6
e7
e8
Figure 1: The upper envelope (bold, red curve) for a set
of lines is the convex hull which consists of the topmost
line segments. Each line corresponds to a candidate
translation and is thus related to a certain error count.
Envelopes can efficiently be computed with Algorithm 1.
The importance of the upper envelope is that it pro-
vides a compact encoding of all possible outcomes
that a rescoring of Cs may yield if the parameter
set ?M1 is moved along the chosen direction. Once
the upper envelope has been determined, we can
project its constituent line segments onto the error
counts of the corresponding candidate translations
(cf. Figure 1). This projection is independent of
how the envelope is generated and can therefore be
applied to any set of line segments1.
An effective means to compute the upper enve-
lope is a sweep line algorithm which is often used in
computational geometry to determine the intersec-
tion points of a sequence of lines or line segments
(Bentley and Ottmann, 1979). The idea is to shift
(?sweep?) a vertical ray from 8 to  8 over the
plane while keeping track of those points where two
or more lines intersect. Since the upper envelope
is fully specified by the topmost line segments, it
suffices to store the following components for each
line object ?: the x-intercept ?.x with the left-
adjacent line, the slope ?.m, and the y-intercept ?.y;
a fourth component, ?.t, is used to store the candi-
date translation. Algorithm 1 shows the pseudo code
for a sweep line algorithm which reduces an input
array a[0..K-1] consisting of the K line objects
of the candidate repository Cs to its upper envelope.
By construction, the upper envelope consists of at
most K line segments. The endpoints of each line
1 For lattice MERT, it will therefore suffice to find an
efficient way to compute the upper envelope over all translations
that are encoded in a phrase graph.
Algorithm 1 SweepLine
input: array a[0..K-1] containing lines
output: upper envelope of a
sort(a:m);
j = 0; K = size(a);
for (i = 0; i < K; ++i) {
? = a[i];
?.x = -8;
if (0 < j) {
if (a[j-1].m == ?.m) {
if (?.y <= a[j-1].y) continue;
--j;
}
while (0 < j) {
?.x = (?.y - a[j-1].y)/
(a[j-1].m - ?.m);
if (a[j-1].x < ?.x) break;
--j;
}
if (0 == j) ?.x = -8;
a[j++] = ?;
} else a[j++] = ?;
}
a.resize(j);
return a;
segment define the interval boundaries at which the
decision made by the decoder will change. Hence,
as ? increases from 8 to  8, we will see that
the most probable translation hypothesis will change
whenever ? passes an intersection point.
Let ?fs1 ? ?
fs
2 ? ...? ?
fs
Ns denote the sequence of
interval boundaries and let ?Efs1 ,?Efs2 , ...,?EfsNs
denote the corresponding sequence of changes in the
error count where ?Efsn is the amount by which the
error count will change if ? is moved from a point in
r?fsn1, ?fsn q to a point in r?fsn , ?
fs
n 1q. Both sequences
together provide an exhaustive representation of the
unsmoothed error surface for the sentence fs along
the line ?M1   ?  dM1 . The error surface for the
whole training corpus is obtained by merging the
interval boundaries (and their corresponding error
counts) over all sentences in the training corpus.
The optimal ? can then be found by traversing the
merged error surface and choosing a point from the
interval where the total error reaches its minimum.
After the parameter update, ??M1  ?M1  ?opt dM1 ,
the decoder may find new translation hypotheses
which are merged into the candidate repositories if
they are ranked among the top N candidates. The
relation K  N holds therefore only in the first
iteration. From the second iteration on, K is usually
larger than N . The sequence of line optimizations
and decodings is repeated until (1) the candidate
repositories remain unchanged and (2) ?opt  0.
727
3 Minimum Error Rate Training on
Lattices
In this section, the algorithm for computing the
upper envelope on N -best lists is extended to phrase
lattices. For a description on how to generate
lattices, see (Ueffing et al, 2002).
Formally, a phrase lattice for a source sentence f
is defined as a connected, directed acyclic graph
Gf  pVf , Ef q with vertice set Vf , unique source and
sink nodes s, t P Vf , and a set of arcs Ef ? Vf  Vf .
Each arc is labeled with a phrase ?ij  ei1 , ..., eij
and the (local) feature function values hM1 p?ij , fq.
A path ?  pv0, ?0, v1, ?1, ..., ?n1, vnq in Gf (with
?i P Ef and vi, vi 1 P Vf as the tail and head of
?i, 0 ? i ? n) defines a partial translation epi of f
which is the concatenation of all phrases along this
path. The corresponding feature function values are
obtained by summing over the arc-specific feature
function values:
? : 
v0
?0,1
???????
hM1 p?0,1, fq

v1
?1,2
???????
hM1 p?1,2, fq
  
?n1,n
?????????
hM1 p?n1,n, fq

vn
epi  ?
i,j :vi?vjPpi
?ij  ?0,1  ...  ?n1,n
hM1 pepi, fq 
?
i,j :vi?vjPpi
hM1 p?ij, fq
In the following, we use the notation inpvq and outpvq
to refer to the set of incoming and outgoing arcs for
a node v P Vf . Similarly, headp?q and tailp?q denote
the head and tail of ? P Ef .
To develop the algorithm for computing the up-
per envelope of all translation hypotheses that are
encoded in a phrase lattice, we first consider a node
v P Vf with some incoming and outgoing arcs:
v
v1?
Each path that starts at the source node s and ends in
v defines a partial translation hypothesis which can
be represented as a line (cf. Eq. (5)). We now assume
that the upper envelope for these partial translation
hypotheses is known. The lines that constitute this
envelope shall be denoted by f1, ..., fN . Next we
consider continuations of these partial translation
candidates by following one of the outgoing arcs
Algorithm 2 Lattice Envelope
input: a phrase lattice Gf  pVf , Ef q
output: upper envelope of Gf
a = H;
L = H;
TopSort(Gf);
for v = s to t do {
a = SweepLine(
?
?Pinpvq
L[?]);
foreach (? P inpvq)
L.delete(?);
foreach (? P outpvq) {
L[?] = a;
for (i = 0; i < a.size(); ++i) {
L[?][i].m = a[i].m +
?
m dmhmp?, fq;
L[?][i].y = a[i].y +
?
m ?mhmp?, fq;
L[?][i].p = a[i].p ?v,headp?q;
}
}
}
return a;
? P outpvq. Each such arc defines another line
denoted by gp?q. If we add the slope and y-intercept
of gp?q to each line in the set tf1, ..., fN u, then the
upper envelope will be constituted by segments of
f1   gp?q, ..., fN   gp?q. This operation neither
changes the number of line segments nor their rela-
tive order in the envelope, and therefore it preserves
the structure of the convex hull. As a consequence,
we can propagate the resulting envelope over an
outgoing arc ? to a successor node v1  headp?q.
Other incoming arcs for v1 may be associated with
different upper envelopes, and all that remains is
to merge these envelopes into a single combined
envelope. This is, however, easy to accomplish
since the combined envelope is simply the convex
hull of the union over the line sets which constitute
the individual envelopes. Thus, by merging the
arrays that store the line segments for the incoming
arcs and applying Algorithm 1 to the resulting array
we obtain the combined upper envelope for all
partial translation candidates that are associated with
paths starting at the source node s and ending in
v1. The correctness of this procedure is based on
the following two observations:
(1) A single translation hypothesis cannot consti-
tute multiple line segments of the same envelope.
This is because translations associated with different
line segments are path-disjoint.
(2) Once a partial translation has been discarded
from an envelope because its associated line f? is
completely covered by the topmost line segments
of the convex hull, there is no path continuation
that could bring back f? into the upper envelope
728
again. Proof: Suppose that such a continuation
exists, then this continuation can be represented as
a line g, and since f? has been discarded from the
envelope, the path associated with g must also be a
valid continuation for the line segments f1, ..., fN
that constitute the envelope. Thus it follows that
maxpf1   g, ..., fN   gq  maxpf1, ..., fN q   g ?
f?   g for some ? P R. This, however, is in contra-
diction with the premise that f? ? maxpf1, ..., fN q
for all ? P R.
To keep track of the phrase expansions when
propagating an envelope over an outgoing arc ? P
tailpvq, the phrase label ?v,headp?q has to be appended
from the right to all partial translation hypotheses in
the envelope. The complete algorithm then works
as follows: First, all nodes in the phrase lattice
are sorted in topological order. Starting with the
source node, we combine for each node v the upper
envelopes that are associated with v?s incoming arcs
by merging their respective line arrays and reducing
the merged array into a combined upper envelope
using Algorithm 1. The combined envelope is then
propagated over the outgoing arcs by associating
each ? P outpvq with a copy of the combined
envelope. This copy is modified by adding the
parameters (slope and y-intercept) of the line gp?q
to the envelope?s constituent line segments. The
envelopes of the incoming arcs are no longer needed
and can be deleted in order to release memory. The
envelope computed at the sink node is by construc-
tion the convex hull over all translation hypotheses
represented in the lattice, and it compactly encodes
those candidates which maximize the decision rule
Eq. (1) for any point along the line ?M1   ?  dM1 .
Algorithm 2 shows the pseudo code. Note that
the component ?.x does not change and therefore
requires no update.
It remains to verify that the suggested algorithm
is efficient in both running time and memory. For
this purpose, we first analyze the complexity of
Algorithm 1 and derive from it the running time of
Algorithm 2.
After sorting, each line object in Algorithm 1 is
visited at most three times. The first time is when
it is picked by the outer loop. The second time is
when it either gets discarded or when it terminates
the inner loop. Whenever a line object is visited
for the third time, it is irrevocably removed from
the envelope. The runtime complexity is therefore
dominated by the initial sorting and amounts to
OpK logKq
Topological sort on a phrase lattice G  pV, Eq
can be performed in time ?p|V|   |E |q. As will be
shown in Section 4, the size of the upper envelope
for G can never exceed the size of the arc set E . The
same holds for any subgraph G
rs,vs of G which is
induced by the paths that connect the source node
s with v P V . Since the envelopes propagated from
the source to the sink node can only increase linearly
in the number of previously processed arcs, the total
running time amounts to a worst case complexity of
Op|V|  |E | log |E |q.
4 Upper Bound for Size of Envelopes
The memory efficiency of the suggested algorithm
results from the following theorem which provides
a novel upper bound for the number of cost mini-
mizing paths in a directed acyclic graph with arc-
specific affine cost functions. The bound is not only
meaningful for proving the space efficiency of lattice
MERT, but it also provides deeper insight into the
structure and complexity of the unsmoothed error
surface induced by log-linear models. Since we are
examining a special class of shortest paths problems,
we will invert the sign of each local feature function
value in order to turn the feature scores into cor-
responding costs. Hence, the objective of finding
the best translation hypotheses in a phrase lattice
becomes the problem of finding all cost-minimizing
paths in a graph with affine cost functions.
Theorem: Let G  pV, Eq be a connected directed
acyclic graph with vertex set V , unique source and
sink nodes s, t P V , and an arc set E ? V  V in
which each arc ? P E is associated with an affine
cost function c?p?q  a?  ?   b?, a?, b? P R.
Counting ties only once, the cardinality of the union
over the sets of all cost-minimizing paths for all
? P R is then upper-bounded by |E |:



?
?PR
 
? : ?  ?pG; ?q is a cost-minimizing
path in G given ?
(



? |E | (7)
Proof: The proposition holds for the empty graph
as well as for the case that V  ts, tu with all
arcs ? P E joining the source and sink node. Let
G therefore be a larger graph. Then we perform
an s-t cut and split G into two subgraphs G1 (left
subgraph) and G2 (right subgraph). Arcs spanning
the section boundary are duplicated (with the costs
of the copied arcs in G2 being set to zero) and
connected with a newly added head or tail node:
G: G G1 2c1
c3
c2
c4
c1
c3
c4
c2
0: :
729
The zero-cost arcs in G2 that emerged from the
duplication process are contracted, which can be
done without loss of generality because zero-cost
arcs do not affect the total costs of paths in the
lattice. The contraction essentially amounts to a
removal of arcs and is required in order to ensure
that the sum of edges in both subgraphs does not
exceed the number of edges in G. All nodes in
G1 with out-degree zero are then combined into a
single sink node t1. Similarly, nodes in G2 whose
in-degree is zero are combined into a single source
node s2. Let N1 and N2 denote the number of
arcs in G1 and G2, respectively. By construction,
N1   N2  |E |. Both subgraphs are smaller
than G and thus, due to the induction hypothesis,
their lower envelopes consist of at most N1 and N2
line segments, respectively. We further notice that
either envelope is a convex hull whose constituent
line segments inscribe a convex polygon, in the
following denoted by P1 and P2. Now, we combine
both subgraphs into a single graph G1 by merging
the sink node t1 in G1 with the source node s2
in G2. The merged node is an articulation point
whose removal would disconnect both subgraphs,
and hence, all paths in G1 that start at the source
node s and stop in the sink node t lead through this
articulation point. The graph G1 has at least as many
cost minimizing paths as G, although these paths
as well as their associated costs might be different
from those in G. The additivity of the cost function
and the articulation point allow us to split the costs
for any path from s to t into two portions: the first
portion can be attributed to G1 and must be a line
inside P1; the remainder can be attributed to G2
and must therefore be a line inside P2. Hence, the
total costs for any path in G1 can be bounded by
the convex hull of the superposition of P1 and P2.
This convex hull is again a convex polygon which
consists of at most N1   N2 edges, and therefore,
the number of cost minimizing paths in G1 (and thus
also in G) is upper bounded by N1   N2. l
Corollary: The upper envelope for a phrase lattice
Gf  pVf , Ef q consists of at most |Ef | line segments.
This bound can even be refined and one obtains
(proof omitted) |E | |V| 2. Both bounds are tight.
This result may seem somewhat surprising as it
states that, independent of the choice of the direction
along which the line optimization is performed, the
structure of the error surface is far less complex
than one might expect based on the huge number
of alternative translation candidates that are rep-
resented in the lattice and thus contribute to the
error surface. In fact, this result is a consequence
of using a log-linear model which constrains how
costs (or scores, respectively) can evolve due to
hypothesis expansion. If instead quadratic cost
functions were used, the size of the envelopes could
not be limited in the same way. The above theorem
does not, however, provide any additional guidance
that would help to choose more promising directions
in the line optimization algorithm to find better local
optima. To alleviate this problem, the following
section lists some best practices that we found to be
useful in the context of MERT.
5 Practical Aspects
This section addresses some techniques that we
found to be beneficial in order to improve the
performance of MERT.
(1) Random Starting Points: To prevent the line
optimization algorithm from stopping in a poor local
optimum, MERT explores additional starting points
that are randomly chosen by sampling the parameter
space.
(2) Constrained Optimization: This technique
allows for limiting the range of some or all feature
function weights by defining weights restrictions.
The weight restriction for a feature function hm is
specified as an interval Rm  rlm, rms, lm, rm P
RYt8, 8u which defines the admissible region
from which the feature function weight ?m can be
chosen. If the line optimization is performed under
the presence of weights restrictions, ? needs to be
chosen such that the following constraint holds:
lM1 ? ?M1   ?  dM1 ? rM1 (8)
(3) Weight Priors: Weight priors give a small (pos-
itive or negative) boost ? on the objective function
if the new weight is chosen such that it matches a
certain target value ?m:
?opt  argmin?
!
?
s
E
 
rs, e?pfs; ?q

 
?
m
?p?m   ?  dm, ?mq  ?
)
(9)
A zero-weights prior (?m  0) provides a means of
doing feature selection since the weight of a feature
function which is not discriminative will be set to
zero. An initial-weights prior (?m  ?m) can
be used to confine changes in the parameter update
with the consequence that the new parameter may
be closer to the initial weights set. Initial weights
priors are useful in cases where the starting weights
already yield a decent baseline.
730
(4) Interval Merging: The interval r?fsi , ?fsi 1q of
a translation hypothesis can be merged with the
interval r?fsi1, ?
fs
i q of its left-adjacent translation
hypothesis if the corresponding change in the error
count ?Efsi  0. The resulting interval r?fsi1, ?fsi 1q
has a larger range, and the choice of ?opt may be
more reliable.
(5) Random Directions: If the directions chosen in
the line optimization algorithm are the coordinate
axes of the M -dimensional parameter space, each
iteration will result in the update of a single feature
function only. While this update scheme provides
a ranking of the feature functions according to their
discriminative power (each iteration picks the fea-
ture function for which changing the corresponding
weight yields the highest gain), it does not take
possible correlations between the feature functions
into account. As a consequence, the optimization
procedure may stop in a poor local optimum. On
the other hand, it is difficult to compute a direction
that decorrelates two or more correlated feature
functions. This problem can be alleviated by ex-
ploring a large number of random directions which
update many feature weights simultaneously. The
random directions are chosen as the lines which
connect some randomly distributed points on the
surface of an M -dimensional hypersphere with the
hypersphere?s center. The center of the hypersphere
is defined as the initial parameter set.
6 Related Work
As suggested in (Och, 2003), an alternative method
for the optimization of the unsmoothed error count is
Powell?s algorithm combined with a grid-based line
optimization (Press et al, 2007, p. 509). In (Zens
et al, 2007), the MERT criterion is optimized on
N -best lists using the Downhill Simplex algorithm
(Press et al, 2007, p. 503). The optimization proce-
dure allows for optimizing other objective function
as, e.g., the expected BLEU score. A weakness
of the Downhill Simplex algorithm is, however, its
decreasing robustness for optimization problems in
more than 10 dimensions. A different approach
to minimize the expected BLEU score is suggested
in (Smith and Eisner, 2006) who use deterministic
annealing to gradually turn the objective function
from a convex entropy surface into the more com-
plex risk surface. A large variety of different search
strategies for MERT are investigated in (Cer et al,
2008), which provides many fruitful insights into
the optimization process. In (Duh and Kirchhoff,
2008), MERT is used to boost the BLEU score on
Table 1: Corpus statistics for three text translation sets:
Arabic-to-English (aren), Chinese-to-English (zhen),
and English-to-Chinese (enzh). Development and test
data are compiled from evaluation data used in past
NIST Machine Translation Evaluations.
data set collection # of sentences
aren zhen enzh
dev1 nist02 1043 878 ?
dev2 nist04 1353 1788 ?
blind nist08 1360 1357 1859
N -best re-ranking tasks. The incorporation of a
large number of sparse feature functions is described
in (Watanabe et al, 2007). The paper investigates a
perceptron-like online large-margin training for sta-
tistical machine translation. The described approach
is reported to yield significant improvements on top
of a baseline system which employs a small number
of feature functions whose weights are optimized
under the MERT criterion. A study which is comple-
mentary to the upper bound on the size of envelopes
derived in Section 4 is provided in (Elizalde and
Woods, 2006) which shows that the number of
inference functions of any graphical model as, for
instance, Bayesian networks and Markov random
fields is polynomial in the size of the model if the
number of parameters is fixed.
7 Experiments
Experiments were conducted on the NIST 2008
translation tasks under the conditions of the con-
strained data track for the language pairs Arabic-
to-English (aren), English-to-Chinese (enzh), and
Chinese-to-English (zhen). The development cor-
pora were compiled from test data used in the
2002 and 2004 NIST evaluations. Each corpus set
provides 4 reference translations per source sen-
tence. Table 1 summarizes some corpus statistics.
Table 2: BLEU score results on the NIST-08 test set
obtained after 25 iterations using N -best MERT or 5
iterations using lattice MERT, respectively.
dev1+dev2 blind
task loss N -best lattice N -best lattice
aren MBR 56.6 57.4 42.9 43.9
0-1 56.7 57.4 42.8 43.7
enzh MBR 39.7 39.6 36.5 38.8
0-1 40.4 40.5 35.1 37.6
zhen MBR 39.5 39.7 27.5 28.2
0-1 39.6 39.6 27.0 27.6
731
 35.5
 36
 36.5
 37
 37.5
 38
 38.5
 39
 39.5
 40
 0  5  10  15  20  25
B
LE
U
[%
]
iteration
lattice MERT
N-best MERT
 5
 15
 25
 35
 45
 0  5  10  15  20  25
Figure 2: BLEU scores for N -best MERT and lattice
MERT after each decoding step on the zhen-dev1 corpus.
The grey shaded subfigure shows the complete graph
including the bottom part for N -best MERT.
Translation results were evaluated using the mixed-
case BLEU score metric in the implementation as
suggested by (Papineni et al, 2001).
Translation results were produced with a state-of-
the-art phrase-based SMT system which uses EM-
trained word alignment models (IBM1, HMM) and
a 5-gram language model built from the Web-1T
collection2 . Translation hypotheses produced on the
blind test data were reranked using the Minimum-
Bayes Risk (MBR) decision rule (Kumar and Byrne,
2004; Tromble et al, 2008). Each system uses a log-
linear combination of 20 to 30 feature functions.
In a first experiment, we investigated the conver-
gence speed of lattice MERT and N -best MERT.
2http://www.ldc.upenn.edu, catalog entry: LDC2006T13
 35.6
 35.8
 36
 36.2
 36.4
 36.6
 36.8
 37
-15 -10 -5  0  5  10  15
B
LE
U
[%
]
?
lattice
50-best
 36.28
 36.3
 36.32
 36.34
-0.2 -0.1  0  0.1  0.2
 35
 35.5
 36
 36.5
 37
-40 -20  0  20  40
Figure 3: Error surface of the phrase penalty feature after
the first iteration on the zhen-dev1 corpus.
 0  1  2  3  4  5
 35.5
 36
 36.5
 37
 37.5
 38
 38.5
 39
 39.5
 40
B
LE
U
[%
]
iteration
lattice MERT: 1000 directions
1 direction
Figure 4: BLEU scores on the zhen-dev1 corpus for
lattice MERT with additional directions.
Figure 2 shows the evolution of the BLEU score
in the course of the iteration index on the zhen-
dev1 corpus for either method. In each iteration,
the training procedure translates the development
corpus using the most recent weights set and merges
the top ranked candidate translations (either repre-
sented as phrase lattices or N -best lists) into the
candidate repositories before the line optimization
is performed. For N -best MERT, we used N  50
which yielded the best results. In contrast to lattice
MERT, N -best MERT optimizes all dimensions in
each iteration and, in addition, it also explores a
large number of random starting points before it
re-decodes and expands the hypothesis set. As is
typical for N -best MERT, the first iteration causes
a dramatic performance loss caused by overadapting
the candidate repositories, which amounts to more
than 27.3 BLEU points. Although this performance
loss is recouped after the 5th iteration, the initial
decline makes the line optimization under N -best
MERT more fragile since the optimum found at the
end of the training procedure is affected by the initial
performance drop rather than by the choice of the
initial start weights. Lattice MERT on the other hand
results in a significantly faster convergence speed
and reaches its optimum already in the 5th iteration.
For lattice MERT, we used a graph density of 40
arcs per phrase which corresponds to an N -best size
of more than two octillion p2  1027q entries. This
huge number of alternative candidate translations
makes updating the weights under lattice MERT
more reliable and robust and, compared to N -best
MERT, it becomes less likely that the same feature
weight needs to be picked again and adjusted in
subsequent iterations. Figure 4 shows the evolution
of the BLEU score on the zhen-dev1 corpus using
732
Table 3: BLEU score results on the NIST-08 tests set
obtained after 5 iterations using lattice MERT with
different numbers of random directions in addition to the
optimization along the coordinate axes.
# random dev1+dev2 blind
task directions 0-1 MBR 0-1 MBR
aren ? 57.4 57.4 43.7 43.9
1000 57.6 57.7 43.9 44.5
zhen ? 39.6 39.7 27.6 28.2
500 39.5 39.9 27.9 28.3
lattice MERT with 5 weights updates per iteration.
The performance drop in iteration 1 is also attributed
to overfitting the candidate repository. The decline
of less than 0.5% in terms of BLEU is, however,
almost negligible compared to the performance drop
of more than 27% in case of N -best MERT. The
vast number of alternative translation hypotheses
represented in a lattice also increases the number
of phase transitions in the error surface, and thus
prevents MERT from selecting a low performing
feature weights set at early stages in the optimization
procedure. This is illustrated in Figure 3, where
lattice MERT and N -best MERT find different op-
tima for the weight of the phrase penalty feature
function after the first iteration. Table 2 shows the
BLEU score results on the NIST 2008 blind test
using the combined dev1+dev2 corpus as training
data. While only the aren task shows improvements
on the development data, lattice MERT provides
consistent gains over N -best MERT on all three
blind test sets. The reduced performance for N -best
MERT is a consequence of the performance drop in
the first iteration which causes the final weights to
be far off from the initial parameter set. This can
impair the ability of N -best MERT to generalize to
unseen data if the initial weights are already capable
of producing a decent baseline. Lattice MERT on
the other hand can produce weights sets which are
closer to the initial weights and thus more likely to
retain the ability to generalize to unseen data. It
could therefore be worthwhile to investigate whether
a more elaborated version of an initial-weights prior
allows for alleviating this effect in case of N -
best MERT. Table 3 shows the effect of optimizing
the feature function weights along some randomly
chosen directions in addition to the coordinate axes.
The different local optima found on the development
set by using random directions result in additional
gains on the blind test sets and range from 0.1% to
0.6% absolute in terms of BLEU.
8 Summary
We presented a novel algorithm that allows for
efficiently constructing and representing the un-
smoothed error surface over all sentence hypotheses
that are represented in a phrase lattice. The proposed
algorithm was used to train the feature function
weights of a log-linear model for a statistical ma-
chine translation system under the Minimum Error
Rate Training (MERT) criterion. Lattice MERT was
shown analytically and experimentally to be supe-
rior over N -best MERT, resulting in significantly
faster convergence speed and a reduced number of
decoding steps. While the approach was used to
optimize the model parameters of a single machine
translation system, there are many other applications
in which this framework can be useful, too. One
possible usecase is the computation of consensus
translations from the outputs of multiple machine
translation systems where this framework allows us
to estimate the system prior weights directly on con-
fusion networks (Rosti et al, 2007; Macherey and
Och, 2007). It is also straightforward to extend the
suggested method to hypergraphs and forests as they
are used, e.g., in hierarchical and syntax-augmented
systems (Chiang, 2005; Zollmann and Venugopal,
2006). Our future work will therefore focus on how
much system combination and syntax-augmented
machine translation can benefit from lattice MERT
and to what extent feature function weights can
robustly be estimated using the suggested method.
References
J. L. Bentley and T. A. Ottmann. 1979. Algorithms for
reporting and counting geometric intersections. IEEE
Trans. on Computers, C-28(9):643?647.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regular-
ization and Search for Minimum Error Rate Training.
In Proceedings of the Third Workshop on Statistical
Machine Translation, 46th Annual Meeting of the
Association of Computational Linguistics: Human
Language Technologies (ACL-2008 HLT), pages 26?
34, Columbus, OH, USA, June.
D. Chiang. 2005. A Hierarchical Phrase-based Model
for Statistical Machine Translation. In ACL-2005,
pages 263?270, Ann Arbor, MI, USA, June.
K. Duh and K. Kirchhoff. 2008. Beyond Log-Linear
Models: Boosted Minimum Error Rate Training for
N-best Re-ranking. In Proceedings of the Third
Workshop on Statistical Machine Translation, 46th
Annual Meeting of the Association of Computational
Linguistics: Human Language Technologies (ACL-
2008 HLT), pages 37?40, Columbus, OH, USA, June.
733
S. Elizalde and K. Woods. 2006. Bounds on the Number
of Inference Functions of a Graphical Model, October.
arXiv:math/0610233v1.
S. Kumar and W. Byrne. 2004. Minimum Bayes-
Risk Decoding for Statistical Machine Translation.
In Proc. HLT-NAACL, pages 196?176, Boston, MA,
USA, May.
W. Macherey and F. J. Och. 2007. An Empirical Study
on Computing Consensus Translations from Multiple
Machine Translation Systems. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 986?995, Prague, Czech Republic,
June.
F. J. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In 41st Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2001. BLEU: a Method for Automatic Evaluation
of Machine Translation. Technical Report RC22176
(W0109-022), IBM Research Division, Thomas J.
Watson Research Center, Yorktown Heights, NY,
USA.
K. A. Papineni. 1999. Discriminative training via
linear programming. In IEEE Int. Conf. on Acoustics,
Speech, and Signal Processing, volume 2, pages 561?
564, Phoenix, AZ, March.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.
Flannery. 2007. Numerical Recipes: The Art of
Scientific Computing. Cambridge University Press,
Cambridge, UK, third edition.
A. V. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas,
R. Schwartz, and B. Dorr. 2007. Combining outputs
from multiple machine translation systems. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 228?235, Rochester, New York,
April. Association for Computational Linguistics.
D. A. Smith and J. Eisner. 2006. Minimum Risk
Annealing for Training Log-linear Models. In 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics (Coling/ACL-2006), pages
787?794, Sydney, Australia, July.
R. Tromble, S. Kumar, F. J. Och, and W. Macherey. 2008.
Lattice minimum bayes-risk decoding for statistical
machine translation. In Conference on Empirical
Methods in Natural Language Processing (EMNLP),
page 10, Waikiki, Honolulu, Hawaii, USA, October.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation
of word graphs in statistical machine translation. In
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, pages 156?
163, Philadelphia, PE, July.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 764?773,
Prague, Czech Republic.
R. Zens, S. Hasan, and H. Ney. 2007. A Systematic
Comparison of Training Criteria for Statistical Ma-
chine Translation. In Proceedings of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In
NAACL ?06: Proceedings of the 2006 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 138?141, New York, NY, June.
Association for Computational Linguistics.
734
c? 2004 Association for Computational Linguistics
The Alignment Template Approach to
Statistical Machine Translation
Franz Josef Och? Hermann Ney?
Google RWTH Aachen
A phrase-based statistical machine translation approach ? the alignment template approach ? is
described. This translation approach allows for general many-to-many relations between words.
Thereby, the context of words is taken into account in the translation model, and local changes
in word order from source to target language can be learned explicitly. The model is described
using a log-linear modeling approach, which is a generalization of the often used source?channel
approach. Thereby, the model is easier to extend than classical statistical machine translation
systems. We describe in detail the process for learning phrasal translations, the feature functions
used, and the search algorithm. The evaluation of this approach is performed on three different
tasks. For the German?English speech Verbmobil task, we analyze the effect of various sys-
tem components. On the French?English Canadian Hansards task, the alignment template
system obtains significantly better results than a single-word-based translation model. In the
Chinese?English 2002 National Institute of Standards and Technology (NIST) machine transla-
tion evaluation it yields statistically significantly better NIST scores than all competing research
and commercial translation systems.
1. Introduction
Machine translation (MT) is a hard problem, because natural languages are highly
complex, many words have various meanings and different possible translations, sen-
tences might have various readings, and the relationships between linguistic entities
are often vague. In addition, it is sometimes necessary to take world knowledge into
account. The number of relevant dependencies is much too large and those depen-
dencies are too complex to take them all into account in a machine translation system.
Given these boundary conditions, a machine translation system has to make decisions
(produce translations) given incomplete knowledge. In such a case, a principled ap-
proach to solving that problem is to use the concepts of statistical decision theory to try
to make optimal decisions given incomplete knowledge. This is the goal of statistical
machine translation.
The use of statistical techniques in machine translation has led to dramatic im-
provements in the quality of research systems in recent years. For example, the statis-
tical approaches of the Verbmobil evaluations (Wahlster 2000) or the U.S. National
? 1600 Amphitheatre Parkway, Mountain View, CA 94043. E-mail: och@google.com.
? Lehrstuhl fu?r Informatik VI, Computer Science Department, RWTH Aachen?University of Technology,
Ahornstr. 55, 52056 Aachen, Germany. E-mail: ney@cs.rwth-aachen.de.
Submission received: 19 November 2002; Revised submission received: 7 October 2003; Accepted for
publication: 1 June 2004
418
Computational Linguistics Volume 30, Number 4
Institute of Standards and Technology (NIST)/TIDES MT evaluations 2001 through
20031 obtain the best results. In addition, the field of statistical machine translation is
rapidly progressing, and the quality of systems is getting better and better. An im-
portant factor in these improvements is definitely the availability of large amounts of
data for training statistical models. Yet the modeling, training, and search methods
have also improved since the field of statistical machine translation was pioneered by
IBM in the late 1980s and early 1990s (Brown et al 1990; Brown et al 1993; Berger et
al. 1994). This article focuses on an important improvement, namely, the use of (gen-
eralized) phrases instead of just single words as the core elements of the statistical
translation model.
We describe in Section 2 the basics of our statistical translation model. We suggest
the use of a log-linear model to incorporate the various knowledge sources into an
overall translation system and to perform discriminative training of the free model
parameters. This approach can be seen as a generalization of the originally suggested
source?channel modeling framework for statistical machine translation.
In Section 3, we describe the statistical alignment models used to obtain a word
alignment and techniques for learning phrase translations from word alignments. Here,
the term phrase just refers to a consecutive sequence of words occurring in text and
has to be distinguished from the use of the term in a linguistic sense. The learned
bilingual phrases are not constrained by linguistic phrase boundaries. Compared to
the word-based statistical translation models in Brown et al (1993), this model is based
on a (statistical) phrase lexicon instead of a single-word-based lexicon. Looking at the
results of the recent machine translation evaluations, this approach seems currently to
give the best results, and an increasing number of researchers are working on different
methods for learning phrase translation lexica for machine translation purposes (Marcu
and Wong 2002; Venugopal, Vogel, and Waibel 2003; Tillmann 2003; Koehn, Och, and
Marcu 2003). Our approach to learning a phrase translation lexicon works in two
stages: In the first stage, we compute an alignment between words, and in the second
stage, we extract the aligned phrase pairs. In our machine translation system, we then
use generalized versions of these phrases, called alignment templates, that also include
the word alignment and use word classes instead of the words themselves.
In Section 4, we describe the various components of the statistical translation
model. The backbone of the translation model is the alignment template feature func-
tion, which requires that a translation of a new sentence be composed of a set of align-
ment templates that covers the source sentence and the produced translation. Other
feature functions score the well-formedness of the produced target language sentence
(i.e., language model feature functions), the number of produced words, or the or-
der of the alignment templates. Note that all components of our statistical machine
translation model are purely data-driven and that there is no need for linguistically
annotated corpora. This is an important advantage compared to syntax-based trans-
lation models (Yamada and Knight 2001; Gildea 2003; Charniak, Knight, and Yamada
2003) that require a parser for source or target language.
In Section 5, we describe in detail our search algorithm and discuss an efficient
implementation. We use a dynamic-programming-based beam search algorithm that
allows a trade-off between efficiency and quality. We also discuss the use of heuristic
functions to reduce the number of search errors for a fixed beam size.
In Section 6, we describe various results obtained on different tasks. For the
German?English Verbmobil task, we analyze the effect of various system compo-
1 http://www.nist.gov/speech/tests/mt/.
419
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Figure 1
Architecture of the translation approach based on a log-linear modeling approach.
nents. On the French?English Canadian Hansards task, the alignment template sys-
tem obtains significantly better results than a single-word-based translation model. In
the Chinese?English 2002 NIST machine translation evaluation it yields results that are
significantly better statistically than all competing research and commercial translation
systems.
2. Log-Linear Models for Statistical Machine Translation
We are given a source (French) sentence f = f J1 = f1, . . . , fj, . . . , fJ, which is to be trans-
lated into a target (English) sentence e = eI1 = e1, . . . , ei, . . . , eI. Among all possible
target sentences, we will choose the sentence with the highest probability:2
e?I1 = argmax
eI1
{Pr(eI1 | f
J
1)} (1)
The argmax operation denotes the search problem, that is, the generation of the output
sentence in the target language.
As an alternative to the often used source?channel approach (Brown et al 1993),
we directly model the posterior probability Pr(eI1 | f
J
1) (Och and Ney 2002). An es-
pecially well-founded framework for doing this is the maximum-entropy framework
(Berger, Della Pietra, and Della Pietra 1996). In this framework, we have a set of M fea-
ture functions hm(eI1, f
J
1), m = 1, . . . , M. For each feature function, there exists a model
2 The notational convention employed in this article is as follows. We use the symbol Pr(?) to denote
general probability distributions with (nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol p(?).
420
Computational Linguistics Volume 30, Number 4
parameter ?m, m = 1, . . . , M. The direct translation probability is given by
Pr(eI1 | f
J
1) = p?M1 (e
I
1 | f
J
1) (2)
=
exp[
?M
m=1 ?mhm(e
I
1, f
J
1)]
?
e? I1
exp[
?M
m=1 ?mhm(e
?I
1, f
J
1)]
(3)
This approach has been suggested by Papineni, Roukos, and Ward (1997, 1998) for a
natural language understanding task.
We obtain the following decision rule:
e?I1 = argmax
eI1
{
Pr(eI1 | f
J
1)
}
= argmax
eI1
{
M
?
m=1
?mhm(eI1, f
J
1)
}
Hence, the time-consuming renormalization in equation (3) is not needed in search.
The overall architecture of the log-linear modeling approach is summarized in Figure 1.
A standard criterion on a parallel training corpus consisting of S sentence pairs
{(fs, es): s = 1, . . . , S} for log-linear models is the maximum class posterior probability
criterion, which can be derived from the maximum-entropy principle:
??M1 = argmax
?M1
{
S
?
s=1
log p?M1 (es | fs)
}
(4)
This corresponds to maximizing the equivocation or maximizing the likelihood of the
direct-translation model. This direct optimization of the posterior probability in Bayes?
decision rule is referred to as discriminative training (Ney 1995) because we directly
take into account the overlap in the probability distributions. The optimization prob-
lem under this criterion has very nice properties: There is one unique global optimum,
and there are algorithms (e.g. gradient descent) that are guaranteed to converge to the
global optimum. Yet the ultimate goal is to obtain good translation quality on un-
seen test data. An alternative training criterion therefore directly optimizes translation
quality as measured by an automatic evaluation criterion (Och 2003).
Typically, the translation probability Pr(eI1 | f
J
1) is decomposed via additional hid-
den variables. To include these dependencies in our log-linear model, we extend the
feature functions to include the dependence on the additional hidden variable. Using
for example the alignment aJ1 as hidden variable, we obtain M feature functions of the
form hm(eI1, f
J
1, a
J
1), m = 1, . . . , M and the following model:
Pr(eI1, a
J
1 | f
J
1) =
exp
(
?M
m=1 ?mhm(e
I
1, f
J
1, a
J
1)
)
?
e? I1,a
?J
1
exp
(
?M
m=1 ?mhm(e
?I
1, f
J
1, a
?J
1)
)
Obviously, we can perform the same step for translation models with an even richer
set of hidden variables than only the alignment aJ1.
3. Learning Translation Lexica
In this section, we describe methods for learning the single-word and phrase-based
translation lexica that are the basis of the machine translation system described in
421
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Section 4. First, we introduce the basic concepts of statistical alignment models, which
are used to learn word alignment. Then, we describe how these alignments can be
used to learn bilingual phrasal translations.
3.1 Statistical Alignment Models
In (statistical) alignment models Pr(f J1, a
J
1 | eI1), a ?hidden? alignment a = a
J
1 is intro-
duced that describes a mapping from a source position j to a target position aj. The
relationship between the translation model and the alignment model is given by
Pr(f J1 | eI1) =
?
aJ1
Pr(f J1, a
J
1 | eI1) (5)
The alignment aJ1 may contain alignments aj = 0 with the ?empty? word e0 to account
for source words that are not aligned with any target word.
In general, the statistical model depends on a set of unknown parameters ? that is
learned from training data. To express the dependence of the model on the parameter
set, we use the following notation:
Pr(f J1, a
J
1 | eI1) = p?(f
J
1, a
J
1 | eI1) (6)
A detailed description of different specific statistical alignment models can be found in
Brown et al (1993) and Och and Ney (2003). Here, we use the hidden Markov model
(HMM) alignment model (Vogel, Ney, and Tillmann 1996) and Model 4 of Brown et
al. (1993) to compute the word alignment for the parallel training corpus.
To train the unknown parameters ?, we are given a parallel training corpus con-
sisting of S sentence pairs {(fs, es): s = 1, . . . , S}. For each sentence pair (fs, es), the
alignment variable is denoted by a = aJ1. The unknown parameters ? are determined
by maximizing the likelihood on the parallel training corpus:
?? = argmax
?
{
S
?
s=1
[
?
a
p?(fs, a | es)
]}
(7)
This optimization can be performed using the expectation maximization (EM) algo-
rithm (Dempster, Laird, and Rubin 1977). For a given sentence pair there are a large
number of alignments. The alignment a?J1 that has the highest probability (under a
certain model) is also called the Viterbi alignment (of that model):
a?J1 = argmax
aJ1
p??(f
J
1, a
J
1 | eI1) (8)
A detailed comparison of the quality of these Viterbi alignments for various statistical
alignment models compared to human-made word alignments can be found in Och
and Ney (2003).
3.2 Symmetrization
The baseline alignment model does not allow a source word to be aligned with two
or more target words. Therefore, lexical correspondences like the German compound
word Zahnarzttermin for dentist?s appointment cause problems because a single source
word must be mapped onto two or more target words. Therefore, the resulting Viterbi
alignment of the standard alignment models has a systematic loss in recall. Here, we
422
Computational Linguistics Volume 30, Number 4
Figure 2
Example of a (symmetrized) word alignment (Verbmobil task).
describe various methods for performing a symmetrization of our directed statistical
alignment models by applying a heuristic postprocessing step that combines the align-
ments in both translation directions (source to target, target to source). Figure 2 shows
an example of a symmetrized alignment.
To solve this problem, we train in both translation directions. For each sentence
pair, we compute two Viterbi alignments aJ1 and b
I
1. Let A1 = {(aj, j) | aj > 0} and
A2 = {(i, bi) | bi > 0} denote the sets of alignments in the two Viterbi alignments. To
increase the quality of the alignments, we can combine (symmetrize) A1 and A2 into
one alignment matrix A using one of the following combination methods:
? Intersection: A = A1 ? A2.
? Union: A = A1 ? A2.
? Refined method: In a first step, the intersection A = A1 ? A2 is
determined. The elements of this intersection result from both Viterbi
alignments and are therefore very reliable. Then, we extend the
alignment A iteratively by adding alignments (i, j) occurring only in the
423
Och and Ney The Alignment Template Approach to Statistical Machine Translation
alignment A1 or in the alignment A2 if neither fj nor ei have an alignment
in A, or if the following conditions both hold:
? The alignment (i, j) has a horizontal neighbor (i ? 1, j), (i + 1, j)
or a vertical neighbor (i, j ? 1), (i, j + 1) that is already in A.
? The set A ? {(i, j)} does not contain alignments with both
horizontal and vertical neighbors.
Obviously, the intersection yields an alignment consisting of only one-to-one align-
ments with a higher precision and a lower recall. The union yields a higher recall
and a lower precision of the combined alignment. The refined alignment method is
often able to improve precision and recall compared to the nonsymmetrized align-
ments. Whether a higher precision or a higher recall is preferred depends on the final
application of the word alignment. For the purpose of statistical MT, it seems that a
higher recall is more important. Therefore, we use the union or the refined combination
method to obtain a symmetrized alignment matrix.
The resulting symmetrized alignments are then used to train single-word-based
translation lexica p(e | f ) by computing relative frequencies using the count N(e, f ) of
how many times e and f are aligned divided by the count N(f ) of how many times
the word f occurs:
p(e | f ) = N(e, f )
N(f )
3.3 Bilingual Contiguous Phrases
In this section, we present a method for learning relationships between whole phrases
of m source language words and n target language words. This algorithm, which
will be called phrase-extract, takes as input a general word alignment matrix (Sec-
tion 3.2). The output is a set of bilingual phrases.
In the following, we describe the criterion that defines the set of phrases that is
consistent with the word alignment matrix:
BP(f J1, eI1, A) =
{(
f j+mj , e
i+n
i
)
: ?(i?, j?) ? A : j ? j? ? j + m ? i ? i? ? i + n (9)
??(i?, j?) ? A : j ? j? ? j + m ? i ? i? ? i + n
}
Hence, the set of all bilingual phrases that are consistent with the alignment is con-
stituted by all bilingual phrase pairs in which all words within the source language
phrase are aligned only with the words of the target language phrase and the words
of the target language phrase are aligned only with the words of the source language
phrase. Note that we require that at least one word in the source language phrase be
aligned with at least one word of the target language phrase. As a result there are no
empty source or target language phrases that would correspond to the ?empty word?
of the word-based statistical alignment models.
These phrases can be computed straightforwardly by enumerating all possible
phrases in one language and checking whether the aligned words in the other lan-
guage are consecutive, with the possible exception of words that are not aligned at
all. Figure 3 gives the algorithm phrase-extract that computes the phrases. The algo-
rithm takes into account possibly unaligned words at the boundaries of the source or
target language phrases. Table 1 shows the bilingual phrases containing between two
and seven words that result from the application of this algorithm to the alignment
of Figure 2.
424
Computational Linguistics Volume 30, Number 4
Table 1
Examples of two- to seven-word bilingual phrases obtained by applying the algorithm
phrase-extract to the alignment of Figure 2.
ja , yes ,
ja , ich yes , I
ja , ich denke mal yes , I think
ja , ich denke mal , yes , I think ,
ja , ich denke mal , also yes , I think , well
, ich , I
, ich denke mal , I think
, ich denke mal , , I think ,
, ich denke mal , also , I think , well
, ich denke mal , also wir , I think , well we
ich denke mal I think
ich denke mal , I think ,
ich denke mal , also I think , well
ich denke mal , also wir I think , well we
ich denke mal , also wir wollten I think , well we plan to
denke mal , think ,
denke mal , also think , well
denke mal , also wir think , well we
denke mal , also wir wollten think , well we plan to
, also , well
, also wir , well we
, also wir wollten , well we plan to
also wir well we
also wir wollten well we plan to
wir wollten we plan to
in unserer in our
in unserer Abteilung in our department
in unserer Abteilung ein neues Netzwerk a new network in our department
in unserer Abteilung ein neues Netzwerk set up a new network in our department
aufbauen
unserer Abteilung our department
ein neues a new
ein neues Netzwerk a new network
ein neues Netzwerk aufbauen set up a new network
neues Netzwerk new network
It should be emphasized that this constraint to consecutive phrases limits the ex-
pressive power. If a consecutive phrase in one language is translated into two or three
nonconsecutive phrases in the other language, there is no corresponding bilingual
phrase pair learned by this approach. In principle, this approach to learning phrases
from a word-aligned corpus could be extended straightforwardly to handle noncon-
secutive phrases in source and target language as well. Informal experiments have
shown that allowing for nonconsecutive phrases significantly increases the number of
extracted phrases and especially increases the percentage of wrong phrases. Therefore,
we consider only consecutive phrases.
3.4 Alignment Templates
In the following, we add generalization capability to the bilingual phrase lexicon by
replacing words with word classes and also by storing the alignment information
for each phrase pair. These generalized and alignment-annotated phrase pairs are
called alignment templates. Formally, an alignment template z is a triple (FJ
?
1 , E
I?
1 , A?)
425
Och and Ney The Alignment Template Approach to Statistical Machine Translation
INPUT: eI1, f
J
1, A
i1 := 1
WHILE i1 ? I
i2 := i1
WHILE i2 ? I
TP := {j|?i : i1 ? i ? i2 ? A(i, j)}
IF quasi-consecutive(TP)
THEN j1 := min(TP)
j2 := max(TP)
SP := {i|?j : j1 ? j ? j2 ? A(i, j)}
IF SP ? {i1, i1 + 1, . . . , i2}
THEN BP := BP ? {(ei2i1 , f
j2
j1
)}
WHILE j1 > 0 ? ?i : A(i, j1) = 0
j?? := j2
WHILE j?? ? J ? ?i : A(i, j??) = 0
BP := BP ? {(ei2i1 , f
j??
j1
)}
j?? := j?? + 1
j1 := j1 ? 1
OUTPUT: BP
Figure 3
Algorithm phrase-extract for extracting phrases from a word-aligned sentence pair. Here
quasi-consecutive(TP) is a predicate that tests whether the set of words TP is consecutive,
with the possible exception of words that are not aligned.
that describes the alignment A? between a source class sequence FJ
?
1 and a target class
sequence EI
?
1 . If each word corresponds to one class, an alignment template corresponds
to a bilingual phrase together with an alignment within this phrase. Figure 4 shows
examples of alignment templates.
The alignment A? is represented as a matrix with J? ? (I? + 1) binary elements. A
matrix element with value 1 means that the words at the corresponding positions are
aligned, and the value 0 means that the words are not aligned. If a source word is not
aligned with a target word, then it is aligned with the empty word e0, which is at the
imaginary position i = 0.
The classes used in FJ
?
1 and E
I?
1 are automatically trained bilingual classes using
the method described in Och (1999) and constitute a partition of the vocabulary of
source and target language. In general, we are not limited to disjoint classes as long
as each specific instance of a word is disambiguated, that is, uniquely belongs to a
specific class. In the following, we use the class function C to map words to their
classes. Hence, it would be possible to employ parts-of-speech or semantic categories
instead of the automatically trained word classes used here.
The use of classes instead of the words themselves has the advantage of better
generalization. For example, if there exist classes in source and target language that
contain town names, it is possible that an alignment template learned using a specific
town name can be generalized to other town names.
In the following, e? and f? denote target and source phrases, respectively. To train
the probability of applying an alignment template p(z = (FJ
?
1 , E
I?
1 , A?) | f? ), we use an
extended version of the algorithm phrase-extract from Section 3.3. All bilingual
phrases that are consistent with the alignment are extracted together with the align-
426
Computational Linguistics Volume 30, Number 4
Figure 4
Examples of alignment templates obtained in training.
ment within this bilingual phrase. Thus, we obtain a count N(z) of how often an
alignment template occurred in the aligned training corpus. The probability of using
an alignment template to translate a specific source language phrase f? is estimated by
means of relative frequency:
p(z = (FJ
?
1 , E
I?
1 , A?) | f? ) =
N(z) ? ?(FJ
?
1 , C(f? ))
N(C(f? ))
(10)
To reduce the memory requirement of the alignment templates, we compute these
probabilities only for phrases up to a certain maximal length in the source language.
427
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Depending on the size of the corpus, the maximal length in the experiments is be-
tween four and seven words. In addition, we remove alignment templates that have
a probability lower than a certain threshold. In the experiments, we use a threshold
of 0.01.
It should be emphasized that this algorithm for computing aligned phrase pairs
and their associated probabilities is very easy to implement. The joint translation model
suggested by Marcu and Wong (2002) tries to learn phrases as part of a full EM
algorithm, which leads to very large memory requirements and a rather complicated
training algorithm. A comparison of the two approaches can be found in Koehn, Och,
and Marcu (2003).
4. Translation Model
To describe our translation model based on the alignment templates described in the
previous section in a formal way, we first decompose both the source sentence f J1 and
the target sentence eI1 into a sequence of phrases (k = 1, . . . , K):
f J1 = f?
K
1 , f?k = fjk?1+1, . . . , fjk (11)
eI1 = e?
K
1 , e?k = eik?1+1, . . . , eik (12)
Note that there are a large number of possible segmentations of a sentence pair into K
phrase pairs. In the following, we will describe the model for a specific segmentation.
Eventually, however, a model can be described in which the specific segmentation is
not known when new text is translated. Hence, as part of the overall search process
(Section 5), we also search for the optimal segmentation.
To allow possible reordering of phrases, we introduce an alignment on the phrase
level ?K1 between the source phrases f?
K
1 and the target phrases e?
K
1 . Hence, ?
K
1 is a
permutation of the phrase positions 1, . . . , K and indicates that the phrases e?k and
f??k are translations of one another. We assume that for the translation between these
phrases a specific alignment template zk is used:
e?k
zk?? f??k
Hence, our model has the following hidden variables:
?K1 , z
K
1
Figure 5 gives an example of the word alignment and phrase alignment of a
German?English sentence pair.
We describe our model using a log-linear modeling approach. Hence, all knowl-
edge sources are described as feature functions that include the given source language
string f J1, the target language string e
I
1, and the above-stated hidden variables. Hence,
we have the following functional form of all feature functions:
h(eI1, f
J
1, ?
K
1 , z
K
1 )
Figure 6 gives an overview of the decisions made in the alignment template model.
First, the source sentence words f J1 are grouped into phrases f?
K
1 . For each phrase f? an
alignment template z is chosen and the sequence of chosen alignment templates is
reordered (according to ?K1 ). Then, every phrase f? produces its translation e? (using the
corresponding alignment template z). Finally, the sequence of phrases e?K1 constitutes
the sequence of words eI1.
428
Computational Linguistics Volume 30, Number 4
Figure 5
Example of segmentation of German sentence and its English translation into alignment
templates.
Figure 6
Dependencies in the alignment template model.
429
Och and Ney The Alignment Template Approach to Statistical Machine Translation
4.1 Feature Functions
4.1.1 Alignment Template Selection. To score the use of an alignment template, we
use the probability p(z | f? ) defined in Section 3. We establish a corresponding feature
function by multiplying the probability of all used alignment templates and taking the
logarithm:
hAT(e
I
1, f
J
1, ?
K
1 , z
K
1 ) = log
K
?
k=1
p(zk | f
j?k
j?k?1+1
) (13)
Here, j?k?1 + 1 is the position of the first word of alignment template zk in the source
language sentence and j?k is the position of the last word of that alignment template.
Note that this feature function requires that a translation of a new sentence be
composed of a set of alignment templates that covers both the source sentence and
the produced translation. There is no notion of ?empty phrase? that corresponds to
the ?empty word? in word-based statistical alignment models. The alignment on the
phrase level is actually a permutation, and no insertions or deletions are allowed.
4.1.2 Word Selection. For scoring the use of target language words, we use a lexicon
probability p(e | f ), which is estimated using relative frequencies as described in Sec-
tion 3.2. The target word e depends on the aligned source words. If we denote the
resulting word alignment matrix by A := A?K1 ,zK1 and the predicted word class for word
ei by Ei, then the feature function hWRD is defined as follows:
hWRD(e
I
1, f
J
1, ?
K
1 , z
K
1 ) = log
I
?
i=1
p(ei | {fj | (i, j) ? A}, Ei) (14)
For p(ei | {fj | (i, j) ? A}) we use a uniform mixture of a single-word model p(e | f ),
which is constrained to predict only words that are in the predicted word class Ei:
p(ei | {fj | (i, j) ? A}, Ei) =
?
{j|(i,j)?A} p(ei | fj)
|{j | (i, j) ? A}| ? ?(C(ei), Ei)
A disadvantage of this model is that the word order is ignored in the translation
model. The translations the day after tomorrow or after the day tomorrow for the German
word u?bermorgen receive an identical probability. Yet the first one should obtain a
significantly higher probability. Hence, we also include a dependence on the word
positions in the lexicon model p(e | f , i, j):
p(ei | fj,
i?1
?
i?=1
[(i?, j) ? A],
j?1
?
j?=1
[(i, j?) ? A]) (15)
Here, [(i?, j) ? A] is 1 if (i?, j) ? A and 0 otherwise. As a result, the word ei depends
not only on the aligned French word fj, but also on the number of preceding French
words aligned with ei and on the number of the preceding English words aligned with
fj. This model distinguishes the positions within a phrasal translation. The number of
parameters of p(e | f , i, j) is significantly higher than that of p(e | f ) alone. Hence,
there is a data estimation problem especially for words that rarely occur. Therefore,
we linearly interpolate the models p(e | f ) and p(e | f , i, j).
4.1.3 Phrase Alignment. The phrase alignment feature simply takes into account that
very often a monotone alignment is a correct alignment. Hence, the feature function
hAL measures the ?amount of nonmonotonicity? by summing over the distance (in the
430
Computational Linguistics Volume 30, Number 4
source language) of alignment templates that are consecutive in the target language:
hAL(e
I
1, f
J
1, ?
K
1 , z
K
1 ) =
K+1
?
k=1
|j?k?1 ? j?k?1 | (16)
Here, j?0 is defined to equal 0 and j?K+1?1 is defined to equal J. The above-stated sum
includes k = K + 1 to include the distance from the end position of the last phrase to
the end of sentence.
The sequence of K = 6 alignment templates in Figure 5 corresponds to the follow-
ing sum of seven jump distances: 0 + 0 + 1 + 3 + 2 + 0 + 0 = 6.
4.1.4 Language Model Features. As a default language model feature, we use a stan-
dard backing-off word-based trigram language model (Ney, Generet, and Wessel 1995):
hLM(e
I
1, f
J
1, ?
K
1 , z
K
1 ) = log
I+1
?
i=1
p(ei | ei?2, ei?1) (17)
In addition, we use a 5-gram class-based language model:
hCLM(e
I
1, f
J
1, ?
K
1 , z
K
1 ) = log
I+1
?
i=1
p(C(ei) | C(ei?4), . . . , C(ei?1)) (18)
The use of the language model feature in equation (18) helps take long-range depen-
dencies better into account.
4.1.5 Word Penalty. To improve the scoring for different target sentence lengths, we
also use as a feature the number of produced target language words (i.e., the length
of the produced target language sentence):
hWP(e
I
1, f
J
1, ?
K
1 , z
K
1 ) = I (19)
Without this feature, we typically observe that the produced sentences tend to be too
short.
4.1.6 Conventional Lexicon. We also use a feature that counts how many entries of a
conventional lexicon co-occur in the given sentence pair. Therefore, the weight for the
provided conventional dictionary can be learned:
hLEX(e
I
1, f
J
1, ?
K
1 , z
K
1 ) = #CO-OCCURRENCES(LEX, e
I
1, f
J
1) (20)
The intuition is that the conventional dictionary LEX is more reliable than the auto-
matically trained lexicon and therefore should get a larger weight.
4.1.7 Additional Features. A major advantage of the log-linear modeling approach
used is that we can add numerous features that deal with specific problems of the
baseline statistical MT system. Here, we will restrict ourselves to the described set
of features. Yet we could use grammatical features that relate certain grammatical
dependencies of source and target language. For example, using a function k(?) that
counts how many arguments the main verb of a sentence has in the source or target
sentence, we can define the following feature, which has a nonzero value if the verb
in each of the two sentences has the same number of arguments:
h(f J1, e
I
1, ?
K
1 , z
K
1 ) = ?(k(f
J
1), k(e
I
1)) (21)
In the same way, we can introduce semantic features or pragmatic features such as
the dialogue act classification.
431
Och and Ney The Alignment Template Approach to Statistical Machine Translation
4.2 Training
For the three different tasks on which we report results, we use two different training
approaches. For the Verbmobil task, we train the model parameters ?M1 according
to the maximum class posterior probability criterion (equation (4)). For the French?
English Hansards task and the Chinese?English NIST task, we simply tune the model
parameters by coordinate descent on held-out data with respect to the automatic eval-
uation metric employed, using as a starting point the model parameters obtained on
the Verbmobil task. Note that this tuning depends on the starting point of the model
parameters and is not guaranteed to converge to the global optimum on the training
data. As a result, this approach is limited to a very small number of model parame-
ters. An efficient algorithm for performing this tuning for a larger number of model
parameters can be found in Och (2003).
A standard approach to training the log-linear model parameters of the maximum
class posterior probability criterion is the GIS (Generalized Iterative Scaling) algorithm
(Darroch and Ratcliff 1972). To apply this algorithm, we have to solve various practi-
cal problems. The renormalization needed in equation (3) requires a sum over many
possible sentences, for which we do not know of an efficient algorithm. Hence, we ap-
proximate this sum by extracting a large set of highly probable sentences as a sample
from the space of all possible sentences (n-best approximation). The set of considered
sentences is computed by means of an appropriately extended version of the search
algorithm described in Section 5.
Using an n-best approximation, we might face the problem that the parameters
trained with the GIS algorithm yield worse translation results even on the training
corpus. This can happen because with the modified model scaling factors, the n-best
list can change significantly and can include sentences that have not been taken into
account in training. Using these sentences, the new model parameters might perform
worse than the old model parameters. To avoid this problem, we proceed as follows.
In a first step, we perform a search, compute an n-best list, and use this n-best list to
train the model parameters. Second, we use the new model parameters in a new search
and compute a new n-best list, which is combined with the existing n-best list. Third,
using this extended n-best list, new model parameters are computed. This process is
iterated until the resulting n-best list does not change. In this algorithm, convergence
is guaranteed, as in the limit the n-best list will contain all possible translations. In
practice, the algorithm converges after five to seven iterations. In our experiments this
final n-best list contains about 500?1000 alternative translations.
We might have the problem that none of the given reference translations is part
of the n-best list because the n-best list is too small or because the search algorithm
performs pruning which in principle limits the possible translations that can be pro-
duced given a certain input sentence. To solve this problem, we define as reference
translation for maximum-entropy training each sentence that has the minimal number
of word errors with respect to any of the reference translations in the n-best list. More
details of the training procedure can be found in Och and Ney (2002).
5. Search
In this section, we describe an efficient search architecture for the alignment template
model.
5.1 General Concept
In general, the search problem for statistical MT even using only Model 1 of Brown
et al (1993) is NP-complete (Knight 1999). Therefore, we cannot expect to develop
432
Computational Linguistics Volume 30, Number 4
efficient search algorithms that are guaranteed to solve the problem without search
errors. Yet for practical applications it is acceptable to commit some search errors
(Section 6.1.2). Hence, the art of developing a search algorithm lies in finding suitable
approximations and heuristics that allow an efficient search without committing too
many search errors.
In the development of the search algorithm described in this section, our main
aim is that the search algorithm should be efficient. It should be possible to translate
a sentence of reasonable length within a few seconds of computing time. We accept
that the search algorithm sometimes results in search errors, as long as the impact
on translation quality is minor. Yet it should be possible to reduce the number of
search errors by increasing computing time. In the limit, it should be possible to
search without search errors. The search algorithm should not impose any principal
limitations. We also expect that the search algorithm be able to scale up to very long
sentences with an acceptable computing time.
To meet these aims, it is necessary to have a mechanism that restricts the search
effort. We accomplish such a restriction by searching in a breadth-first manner with
pruning: beam search. In pruning, we constrain the set of considered translation can-
didates (the ?beam?) only to the promising ones. We compare in beam search those
hypotheses that cover different parts of the input sentence. This makes the comparison
of the probabilities problematic. Therefore, we integrate an admissible estimation of
the remaining probabilities to arrive at a complete translation (Section 5.6)
Many of the other search approaches suggested in the literature do not meet the
described aims:
? Neither optimal A* search (Och, Ueffing, and Ney 2001) nor optimal
integer programming (Germann et al 2001) for statistical MT allows
efficient search for long sentences.
? Greedy search algorithms (Wang 1998; Germann et al 2001) typically
commit severe search errors (Germann et al 2001).
? Other approaches to solving the search problem obtain polynomial time
algorithms by assuming monotone alignments (Tillmann et al 1997) or
imposing a simplified recombination structure (Nie?en et al 1998).
Others make simplifying assumptions about the search space
(Garc??a-Varea, Casacuberta, and Ney 1998; Garc??a-Varea et al 2001), as
does the original IBM stack search decoder (Berger et al 1994). All these
simplifications ultimately make the search problem simpler but
introduce fundamental search errors.
In the following, we describe our search algorithm based on the concept of beam
search, which allows a trade-off between efficiency and quality by adjusting the size of
the beam. The search algorithm can be easily adapted to other phrase-based translation
models. For single-word-based search in MT, a similar algorithm has been described
in Tillmann and Ney (2003).
5.2 Search Problem
Putting everything together and performing search in maximum approximation, we
obtain the following decision rule:
e?I1 = argmax
eI1,?
K
1 ,z
K
1
{
M
?
m=1
?m ? hm(eI1, f
J
1, ?
K
1 , z
K
1 )
}
(22)
433
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Using the four feature functions AT, AL, WRD, and LM, we obtain the following
decision rule:3
e?I1 = argmax
eI1,?
K
1 ,z
K
1
{
(23)
I
?
i=1
(
?LM log p(ei | ei?2, ei?1) + ?WRD log p(ei | {fj | (i, j) ? A}, Ei)
)
(24)
+
K
?
k=1
(
?AT log p(zk | f
j?k
j?k?1+1
) + ?AL ? |j?k ? j?k?1+1|
)
(25)
+?AL ? (J ? j?K) + ?LM log p(EOS | eI?1, eI)
}
(26)
Here, we have grouped the contributions of the various feature functions into those for
each word (from LM and WRD, expression (24)), those for every alignment template
(from AT and AL, expression (25)), and those for the end of sentence (expression (26)),
which includes a term log p(EOS | eI?1, eI) for the end-of-sentence language model
probability.
To extend this decision rule for the word penalty (WP) feature function, we sim-
ply obtain an additional term ?WP for each word. The class-based 5-gram language
model (CLM) can be included like the trigram language model. Note that all these fea-
ture functions decompose nicely into contributions for each produced target language
word or for each covered source language word. This makes it possible to develop
an efficient dynamic programming search algorithm. Not all feature functions have
this nice property: For the conventional lexicon feature function (LEX), we obtain an
additional term in our decision rule which depends on the full sentence. Therefore,
this feature function will not be integrated in the dynamic programming search but
instead will be used to rerank the set of candidate translations produced by the search.
5.3 Structure of Search Space
We have to structure the search space in a suitable way to search efficiently. In our
search algorithm, we generate search hypotheses that correspond to prefixes of target
language sentences. Each hypothesis is the translation of a part of the source language
sentence. A hypothesis is extended by appending one target word. The set of all hy-
potheses can be structured as a graph with a source node representing the sentence
start, goal nodes representing complete translations, and intermediate nodes repre-
senting partial translations. There is a directed edge between hypotheses n1 and n2 if
the hypothesis n2 is obtained by appending one word to hypothesis n1. Each edge has
associated costs resulting from the contributions of all feature functions. Finally, our
search problem can be reformulated as finding the optimal path through this graph.
In the first step, we determine the set of all source phrases in f? for which an appli-
cable alignment template exists. Every possible application of an alignment template
z = (FJ
?
1 , E
I?
1 , A?) to a subsequence f
j+J??1
j of the source sentence is called an alignment
template instantiation Z = (z, j). Hence, the set of all alignment template instantiations
for the source sentence f J1 is
{
Z = (z, j) | z = (FJ
?
1 , E
I?
1 , A?) ? ?j : p(z | f
j+J??1
j ) > 0
}
(27)
3 Note that here some of the simplifying notation of Section 4 has been used.
434
Computational Linguistics Volume 30, Number 4
If the source sentence contains words that have not been seen in the training data, we
introduce a new alignment template that performs a one-to-one translation of each of
these words by itself.
In the second step, we determine a set of probable target language words for
each target word position in the alignment template instantiation. Only these words
are then hypothesized in the search. We call this selection of highly probable words
observation pruning (Tillmann and Ney 2000). As a criterion for a word e at position
i in the alignment template instantiation, we use
?(Ei, C(e)) ?
J?
?
j=0
A?(i, j)
?
i? A?(i
?, j)
? p(e | fj) (28)
In our experiments, we hypothesize only the five best-scoring words.
A decision is a triple d = (Z, e, l) consisting of an alignment template instantiation
Z, the generated word e, and the index l of the generated word in Z. A hypothesis n
corresponds to a valid sequence of decisions di1. The possible decisions are as follows:
1. Start a new alignment template: di = (Zi, ei, 1). In this case, the index
l = 1. This decision can be made only if the previous decision di?1
finished an alignment template and if the newly chosen alignment
template instantiation does not overlap with any previously chosen
alignment template instantiation. The resulting decision score
corresponds to the contribution of the LM and the WRD features
(expression (24)) for the produced word and the contribution of AL and
AT features (expression (25)) for the started alignment template.
2. Extend an alignment template: di = (Zi, ei, l). This decision can be made
only if the previous decision uses the same alignment template
instantiation and has as index l ? 1: di?1 = (Zi, ei?1, l ? 1). The resulting
decision score corresponds to the contribution of the LM and the WRD
features (expression (24)).
3. Finish the translation of a sentence: di = (EOS, EOS, 0). In this case, the
hypothesis is marked as a goal hypothesis. This decision is possible only
if the previous decision di?1 finished an alignment template and if the
alignment template instantiations completely cover the input sentence.
The resulting decision score corresponds to the contribution of
expression (26).
Any valid and complete sequence of decisions dI+11 uniquely corresponds to a certain
translation eI1, a segmentation into K phrases, a phrase alignment ?
K
1 , and a sequence
of alignment template instantiations zK1 . The sum of the decision scores is equal to the
corresponding score described in expressions (24)?(26).
A straightforward representation of all hypotheses would be the prefix tree of all
possible sequences of decisions. Obviously, there would be a large redundancy in this
search space representation, because there are many search nodes that are indistin-
guishable in the sense that the subtrees following these search nodes are identical. We
can recombine these identical search nodes; that is, we have to maintain only the most
probable hypothesis (Bellman 1957).
In general, the criterion for recombining a set of nodes is that the hypotheses can be
distinguished by neither language nor translation model. In performing recombination,
435
Och and Ney The Alignment Template Approach to Statistical Machine Translation
INPUT: implicitly defined search space (functions Recombine, Extend)
H = {initial-hypothesis}
WHILE H = ?
Hext := ?
FOR n ? H
IF hypothesis n is final
THEN Hfin := Hfin ? {n}
ELSE Hext := Hext ? Extend(n)
H := Recombine(Hext)
Q? = maxn?H Q(n)
H := {n ? H : Q(n) > log(tp) + Q?}
H := HistogramPruning(H, Np)
n? = argmax
n?Hfin
Q(n)
OUTPUT: n?
Figure 7
Algorithm for breadth-first search with pruning.
we obtain a search graph instead of a search tree. The exact criterion for performing
recombination for the alignment templates is described in Section 5.5.
5.4 Search Algorithm
Theoretically, we could use any graph search algorithm to search the optimal path in
the search space. We use a breadth-first search algorithm with pruning. This approach
offers very good possibilities for adjusting the trade-off between quality and efficiency.
In pruning, we always compare hypotheses that have produced the same number of
target words.
Figure 7 shows a structogram of the algorithm. As the search space increases expo-
nentially, it is not possible to explicitly represent it. Therefore, we represent the search
space implicitly, using the functions Extend and Recombine. The function Extend pro-
duces new hypotheses extending the current hypothesis by one word. Some hypothe-
ses might be identical or indistinguishable by the language and translation models.
These are recombined by the function Recombine. We expand the search space such
that only hypotheses with the same number of target language words are recombined.
In the pruning step, we use two different types of pruning. First, we perform prun-
ing relative to the score Q? of the current best hypothesis. We ignore all hypotheses that
have a probability lower than log(tp)+Q?, where tp is an adjustable pruning parameter.
This type of pruning can be performed when the hypothesis extensions are computed.
Second, in histogram pruning (Steinbiss, Tran, and Ney 1994), we maintain only the
best Np hypotheses. The two pruning parameters tp and Np have to be optimized with
respect to the trade-off between efficiency and quality.
5.5 Implementation
In this section, we describe various issues involved in performing an efficient imple-
mentation of a search algorithm for the alignment template approach.
A very important design decision in the implementation is the representation of
a hypothesis. Theoretically, it would be possible to represent search hypotheses only
by the associated decision and a back-pointer to the previous hypothesis. Yet this
would be a very inefficient representation for the implementation of the operations
436
Computational Linguistics Volume 30, Number 4
that have to be performed in the search. The hypothesis representation should contain
all information required to perform efficiently the computations needed in the search
but should contain no more information than that, to keep the memory consumption
small.
In search, we produce hypotheses n, each of which contains the following infor-
mation:
1. e: the final target word produced
2. h: the state of the language model (to predict the following word)
3. c = cJ1: the coverage vector representing the already covered positions of
the source sentence (cj = 1 means the position j is covered, cj = 0 means
the position j is not covered)
4. Z: a reference to the alignment template instantiation that produced the
final target word
5. l: the position of the final target word in the alignment template
instantiation
6. Q(n): the accumulated score of all previous decisions
7. n?: a reference to the previous hypothesis
Using this representation, we can perform the following operations very efficiently:
? Determining whether a specific alignment template instantiation can be
used to extend a hypothesis. To do this, we check whether the positions
of the alignment template instantiation are still free in the hypothesis
coverage vector.
? Checking whether a hypothesis is final. To do this, we determine
whether the coverage vector contains no uncovered position. Using a bit
vector as representation, the operation to check whether a hypothesis is
final can be implemented very efficiently.
? Checking whether two hypotheses can be recombined. The criterion for
recombining two hypotheses n1 = (e1, h1, c1, Z1, l1) and
n2 = (e2, h2, c2, Z2, l2) is
h1 = h2? identical language model state
c1 = c2? identical coverage vector
( (Z1 = Z2 ? l1 = l2)? alignment template instantiation is identical
(J(Z1) = l1 ? J(Z2) = l2) ) alignment template instantiation finished
We compare in beam search those hypotheses that cover different parts of the input
sentence. This makes the comparison of the probabilities problematic. Therefore, we
integrate an admissible estimation of the remaining probabilities to arrive at a complete
translation. Details of the heuristic function for the alignment templates are provided
in the next section.
5.6 Heuristic Function
To improve the comparability of search hypotheses, we introduce heuristic functions.
A heuristic function estimates the probabilities of reaching the goal node from a certain
437
Och and Ney The Alignment Template Approach to Statistical Machine Translation
search node. An admissible heuristic function is always an optimistic estimate; that
is, for each search node, the product of edge probabilities of reaching a goal node
is always equal to or smaller than the estimated probability. For an A*-based search
algorithm, a good heuristic function is crucial to being able to translate long sentences.
For a beam search algorithm, the heuristic function has a different motivation. It is used
to improve the scoring of search hypotheses. The goal is to make the probabilities of
all hypotheses more comparable, in order to minimize the chance that the hypothesis
leading to the optimal translation is pruned away.
Heuristic functions for search in statistical MT have been used in Wang and Waibel
(1997) and Och, Ueffing, and Ney (2001). Wang and Waibel (1997) have described a
simple heuristic function for Model 2 of Brown et al (1993) that was not admissible.
Och, Ueffing, and Ney (2001) have described an admissible heuristic function for
Model 4 of Brown et al (1993) and an almost-admissible heuristic function that is
empirically obtained.
We have to keep in mind that a heuristic function is helpful only if the overhead
introduced in computing the heuristic function is more than compensated for by the
gain obtained through a better pruning of search hypotheses. The heuristic functions
described in the following are designed such that their computation can be performed
efficiently.
The basic idea for developing a heuristic function for an alignment model is that all
source sentence positions that have not been covered so far still have to be translated
to complete the sentence. If we have an estimation rX(j) of the optimal score for
translating position j, then the value of the heuristic function RX(n) for a node n
can be inferred by summing over the contribution for every position j that is not in
the coverage vector c(n) (here X denotes different possibilities to choose the heuristic
function):
RX(n) =
?
j?c(n)
rX(j) (29)
The situation in the case of the alignment template approach is more complicated, as
not every word is translated alone, but typically the words are translated in context.
Therefore, the basic quantity for the heuristic function in the case of the alignment
template approach is a function r(Z) that assigns to every alignment template in-
stantiation Z a maximal probability. Using r(Z), we can induce a position-dependent
heuristic function r(j):
r(j) := max
Z:j(Z)?j?j(Z)+J(Z)?1
r(Z)/J(Z) (30)
Here, J(Z) denotes the number of source language words produced by the alignment
template instantiation Z and j(Z) denotes the position of the first source language
word. It can be easily shown that if r(Z) is admissible, then r(j) is also admissible. We
have to show that for all nonoverlapping sequences ZK1 the following holds:
K
?
k=1
r(Zk) ?
?
j?c(ZK1 )
r(j) (31)
Here, c(ZK1 ) denotes the set of all positions covered by the sequence of alignment
templates ZK1 . This can be shown easily:
K
?
k=1
r(Zk) =
K
?
k=1
J(Zk)
?
j=1
r(Zk)/J(Zk) (32)
438
Computational Linguistics Volume 30, Number 4
INPUT: coverage vector cJ1, previously covered position j
ff = min({j? | cj? = 0})
mj = |j ? ff|
WHILE ff = (J + 1)
fo := min({j? | j? > ff ? cj? = 1})
ff := min({j? | j? > fo ? cj? = 0 ? j? = J + 1})
mj := mj + |ff ? fo|
OUTPUT: mj
Figure 8
Algorithm min-jumps to compute the minimum number of needed jumps D(cJ1, j) to complete
the translation.
=
?
j?c(ZK1 )
r(Zk(j))/J(Zk(j)) (33)
?
?
j?c(ZK1 )
max
Z:j(Z)?j?j(Z)+J(Z)?1
r(Z)/J(Z) (34)
Here, k(j) denotes the phrase index k that includes the target language word position j.
In the following, we develop various heuristic functions r(Z) of increasing complexity.
The simplest realization of a heuristic function r(Z) takes into account only the prior
probability of an alignment template instantiation:
RAT(Z = (z, j)) = ?AT ? log p(z | fj,j+J(z)?1) (35)
The lexicon model can be integrated as follows:
RWRD(Z) = ?WRD ?
j(Z)+J(Z)?1
?
j?=j(Z)
max
e
log p(e | fj?) (36)
The language model can be incorporated by considering that for each target word
there exists an optimal language model probability:
pL(e) = max
e?,e??
p(e | e?, e??) (37)
Here, we assume a trigram language model. In general, it is necessary to maximize
over all possible different language model histories. We can also combine the language
model and the lexicon model into one heuristic function:
RWRD+LM(Z) =
j(Z)+J(Z)?1
?
j?=j(Z)
max
e
?WRD log(p(e | fj?)) + ?LM log(p
L(e)) (38)
To include the phrase alignment probability in the heuristic function, we compute
the minimum sum of all jump widths that is needed to complete the translation. This
sum can be computed efficiently using the algorithm shown in Figure 8. Then, an
admissible heuristic function for the jump width is obtained by
RAL(c, j) = ?AL ? D(c, j) (39)
439
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Table 2
Statistics for Verbmobil task: training corpus (Train), conventional dictionary (Lex),
development corpus (Dev), test corpus (Test) (Words*: words without punctuation marks).
No Preprocessing With Preprocessing
German English German English
Train Sentences 58,073
Words 519,523 549,921 522,933 548,874
Words* 418,974 453,612 420,919 450,297
Singletons 3,453 1,698 3,570 1,763
Vocabulary 7,940 4,673 8,102 4,780
Lex Entries 12,779
Extended vocabulary 11,501 6,867 11,904 7,089
Dev Sentences 276
Words 3,159 3,438 3,172 3,445
Trigram perplexity ? 28.1 ? 26.3
Test Sentences 251
Words 2,628 2,871 2,640 2,862
Trigram perplexity ? 30.5 ? 29.9
Combining all the heuristic functions for the various models, we obtain as final heuris-
tic function for a search hypothesis n
R(n) = RAL(c(n), j(n)) +
?
j?c(n)
(
RAT(j) + RWRD+LM(j)
)
(40)
6. Results
6.1 Results on the Verbmobil Task
We present results on the Verbmobil task, which is a speech translation task in the
domain of appointment scheduling, travel planning, and hotel reservation (Wahlster
2000). Table 2 shows the corpus statistics for this task. We use a training corpus,
which is used to train the alignment template model and the language models, a
development corpus, which is used to estimate the model scaling factors, and a test
corpus. On average, 3.32 reference translations for the development corpus and 5.14
reference translations for the test corpus are used.
A standard vocabulary had been defined for the various speech recognizers used
in Verbmobil. However, not all words of this vocabulary were observed in the train-
ing corpus. Therefore, the translation vocabulary was extended semiautomatically by
adding about 13,000 German?English entries from an online bilingual lexicon avail-
able on the Web. The resulting lexicon contained not only word-word entries, but
also multi-word translations, especially for the large number of German compound
words. To counteract the sparseness of the training data, a couple of straightforward
rule-based preprocessing steps were applied before any other type of processing:
? normalization of
? numbers
? time and date phrases
? spelling (e.g., don?t ? do not)
? splitting of German compound words.
440
Computational Linguistics Volume 30, Number 4
So far, in machine translation research there is no generally accepted criterion
for the evaluation of experimental results. Therefore, we use various criteria. In the
following experiments, we use:
? WER (word error rate)/mWER (multireference word error rate): The
WER is computed as the minimum number of substitution, insertion,
and deletion operations that have to be performed to convert the
generated sentence into the target sentence. In the case of the
multireference word error rate for each test sentence, not just a single
reference translation is used, as for the WER, but a whole set of reference
translations. For each translation hypothesis, the edit distance to the
most similar sentence is calculated (Nie?en et al 2000).
? PER (position-independent WER): A shortcoming of the WER is the fact
that it requires a perfect word order. An acceptable sentence can have a
word order that is different from that of the target sentence, so the WER
measure alone could be misleading. To overcome this problem, we
introduce as an additional measure the position-independent word error
rate. This measure compares the words in the two sentences, ignoring
the word order.
? BLEU (bilingual evalutation understudy) score: This score measures the
precision of unigrams, bigrams, trigrams, and 4-grams with respect to a
whole set of reference translations, with a penalty for too-short sentences
(Papineni et al 2001). Unlike all other evaluation criteria used here,
BLEU measures accuracy, that is, the opposite of error rate. Hence, the
larger BLEU scores, the better.
In the following, we analyze the effect of various system components: alignment tem-
plate length, search pruning, and language model n-gram size. A systematic evaluation
of the alignment template system comparing it with other translation approaches (e.g.,
rule-based) has been performed in the Verbmobil project and is described in Tessiore
and von Hahn (2000). There, the alignment-template-based system achieved a sig-
nificantly larger number of ?approximately correct? translations than the competing
translation systems (Ney, Och, and Vogel 2001).
6.1.1 Effect of Alignment Template Length. Table 3 shows the effect of constraining
the maximum length of the alignment templates in the source language. Typically, it
is necessary to restrict the alignment template length to keep memory requirements
low. We see that using alignment templates with only one or two words in the source
languages results in very bad translation quality. Yet using alignment templates with
lengths as small as three words yields optimal results.
6.1.2 Effect of Pruning and Heuristic Function. In the following, we analyze the effect
of beam search pruning and of the heuristic function. We use the following criteria:
? Number of search errors: A search error occurs when the search
algorithm misses the most probable translation and produces a
translation which is less probable. As we typically cannot efficiently
compute the probability of the optimal translation, we cannot efficiently
compute the number of search errors. Yet we can compute a lower
bound on the number of search errors by comparing the translation
441
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Table 3
Effect of alignment template length on translation quality.
AT length PER [%] mWER [%] BLEU [%]
1 29.8 39.9 44.6
2 27.0 33.0 53.6
3 26.5 30.7 56.1
4 26.9 31.4 55.7
5 26.8 31.4 55.7
6 26.5 30.9 56.0
7 26.5 30.9 56.1
Table 4
Effect of pruning parameter tp and heuristic function on search efficiency for direct-translation
model (Np = 50,000).
no heuristic function AT+WRD +LM +AL
time search time search time search time search
tp [s] errors [s] errors [s] errors [s] errors
10?2 0.0 194 0.0 174 0.0 150 0.0 91
10?4 0.2 97 0.2 57 0.3 40 0.2 13
10?6 2.0 61 2.8 21 4.1 11 1.8 3
10?8 11.9 41 15.0 7 19.9 5 9.5 1
10?10 45.6 38 50.9 6 65.2 3 32.0 1
10?12 114.6 34 119.2 5 146.2 2 75.2 0
found under specific pruning thresholds with the best translation that
we have found using very conservative pruning thresholds.
? Average translation time per sentence: Pruning is used to adjust the
trade-off between efficiency and quality. Hence, we present the average
time needed to translate one sentence of the test corpus.
? Translation quality (mWER, BLEU): Typically, a sentence can have many
different correct translations. Therefore, a search error does not
necessarily result in poorer translation quality. It is even possible that a
search error can improve translation quality. Hence, we analyze the effect
of search on translation quality, using the automatic evaluation criteria
mWER and BLEU.
Tables 4 and 5 show the effect of the pruning parameter tp with the histogram
pruning parameter Np = 50,000. Tables 6 and 7 show the effect of the pruning pa-
rameter Np with the pruning parameter tp = 10?12. In all four tables, we provide the
results for using no heuristic functions and three variants of an increasingly infor-
mative heuristic function. The first is an estimate of the alignment template and the
lexicon probability (AT+WRD), the second adds an estimate of the language model
(+LM) probability, and the third also adds the alignment probability (+AL). These
heuristic functions are described in Section 5.6.
Without a heuristic function, even more than a hundred seconds per sentence
cannot guarantee search-error-free translation. We draw the conclusion that a good
heuristic function is very important to obtaining an efficient search algorithm.
442
Computational Linguistics Volume 30, Number 4
Table 5
Effect of pruning parameter tp and heuristic function on error rate for direct-translation model
(Np = 50,000).
error rates [%]
no heuristic function AT+WRD +LM +AL
tp mWER BLEU mWER BLEU mWER BLEU mWER BLEU
10?2 48.9 46.8 44.3 49.4 40.9 51.3 33.6 53.4
10?4 39.8 50.9 35.0 53.8 32.3 55.0 30.7 55.9
10?6 37.1 51.3 31.8 55.0 30.9 55.6 30.8 56.0
10?8 35.7 53.0 31.4 55.7 31.2 55.7 30.9 56.0
10?10 36.1 52.9 31.3 55.8 31.0 55.9 30.8 56.0
10?12 35.7 52.9 31.2 55.9 31.0 55.9 30.8 56.0
Table 6
Effect of pruning parameter Np and heuristic function on search efficiency for
direct-translation model (tp = 10?12).
no heuristic function AT+WRD +LM +AL
time search time search time search time search
Np [s] errors [s] errors [s] errors [s] errors
1 0.0 237 0.0 238 0.0 238 0.0 232
10 0.0 169 0.0 154 0.0 148 0.0 98
100 0.3 101 0.3 69 0.3 60 0.2 21
1,000 2.2 65 2.3 33 2.4 27 2.0 5
10,000 18.3 40 18.3 10 21.1 5 14.3 1
50,000 114.6 34 119.2 5 146.2 2 75.2 0
Table 7
Effect of pruning parameter Np and heuristic function on error rate for direct-translation
model (tp = 10?12).
error rates [%]
no heuristic function AT+WRD +LM +AL
Np mWER BLEU mWER BLEU mWER BLEU mWER BLEU
1 64.4 29.7 61.9 31.7 59.8 32.4 49.4 38.2
10 46.6 46.9 43.0 49.2 42.0 49.1 34.6 52.3
100 41.0 49.8 36.7 52.6 34.8 53.8 31.3 55.6
1,000 37.8 51.5 33.0 54.5 32.3 55.3 30.6 56.0
10,000 35.5 53.1 31.4 55.6 30.9 55.6 30.8 56.0
50,000 35.7 52.9 31.2 55.9 31.0 55.9 30.8 56.0
443
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Table 8
Effect of the length of the language model history (Unigram/Bigram/Trigram: word-based;
CLM: class-based 5-gram).
Language model type PP PER [%] mWER [%] BLEU [%]
Zerogram 4781.0 38.1 45.9 29.0
Unigram 203.1 30.2 40.9 37.7
Bigram 38.3 26.9 32.9 53.0
Trigram 29.9 26.8 31.8 55.2
Trigram + CLM ? 26.5 30.9 56.1
In addition, the search errors have a more severe effect on the error rates if we
do not use a heuristic function. If we compare the error rates in Table 7, which corre-
spond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 search
errors) using no heuristic function and an mWER of 32.6% (57 search errors) using
the combined heuristic function. The reason is that without a heuristic function, often
the ?easy? part of the input sentence is translated first. This yields severe reordering
errors.
6.1.3 Effect of the Length of the Language Model History. In this work, we use only
n-gram-based language models. Ideally, we would like to take into account long-range
dependencies. Yet long n-grams are seen rarely and are therefore rarely used on unseen
data. Therefore, we expect that extending the history length will at some point not
improve further translation quality.
Table 8 shows the effect of the length of the language model history on translation
quality. We see that the language model perplexity improves from 4,781 for a unigram
model to 29.9 for a trigram model. The corresponding translation quality improves
from an mWER of 45.9% to an mWER of 31.8%. The largest effect seems to come from
taking into account the bigram dependence, which achieves an mWER of 32.9%. If we
perform log-linear interpolation of a trigram model with a class-based 5-gram model,
we observe an additional small improvement in translation quality to an mWER of
30.9%.
6.2 Results on the Hansards task
The Hansards task involves the proceedings of the Canadian parliament, which are
kept by law in both French and English. About three million parallel sentences of this
bilingual data have been made available by the Linguistic Data Consortium (LDC).
Here, we use a subset of the data containing only sentences of up to 30 words. Table 9
shows the training and test corpus statistics.
The results for French to English and for English to French are shown in Table 10.
Because of memory limitations, the maximum alignment template length has been
restricted to four words. We compare here against the single-word-based search for
Model 4 described in Tillmann (2001). We see that the alignment template approach
obtains significantly better results than the single-word-based search.
6.3 Results on Chinese?English
Various statistical, example-based, and rule-based MT systems for a Chinese?English
news domain were evaluated in the NIST 2002 MT evaluation.4 Using the alignment
4 Evaluation home page: http://www.nist.gov/speech/tests/mt/mt2001/index.htm.
444
Computational Linguistics Volume 30, Number 4
Table 9
Corpus statistics for Hansards task (Words*: words without punctuation marks).
French English
Training Sentences 1,470,473
Words 24,338,195 22,163,092
Words* 22,175,069 20,063,378
Vocabulary 100,269 78,332
Singletons 40,199 31,319
Test Sentences 5,432
Words 97,646 88,773
Trigram perplexity ? 179.8
Table 10
Translation results on the Hansards task.
French?English English?French
Translation approach WER [%] PER [%] WER [%] PER [%]
Alignment templates 61.5 49.2 60.9 47.9
Single-word-based: monotone search 65.5 53.0 66.6 56.3
Single-word-based: reordering search 64.9 51.4 66.0 54.4
Table 11
Corpus statistics for Chinese?English corpora?large data track (Words*: words without
punctuation marks).
No preprocessing With preprocessing
Chinese English Chinese English
Train Sentences 1,645,631
Unique sentences 1,289,890
Words 31,175,023 33,044,374 30,849,149 32,511,418
Words* 27,091,283 29,212,384 26,828,721 28,806,735
Singletons 15,324 24,933 5,336 26,344
Vocabulary 67,103 92,488 45,111 85,116
Lex Entries 80,977
Extended vocabulary 76,182 100,704 54,190 93,350
Dev Sentences 993
Words 26,361 32,267 25,852 31,607
Trigram perplexity ? 237,154 ? 171,922
Test Sentences 878
Words 24,540 ? 24,144 ?
template approach described in this article, we participated in these evaluations. The
problem domain is the translation of Chinese news text into English. Table 11 gives
an overview on the training and test data. The English vocabulary consists of full-
form words that have been converted to lowercase letters. The number of sentences
has been artificially increased by adding certain parts of the original training material
more than once to the training corpus, in order to give larger weight to those parts
of the training corpus that consist of high-quality aligned Chinese news text and are
therefore expected to be especially helpful for the translation of the test data.
445
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Table 12
Results of Chinese?English NIST MT evaluation, June 2002, large data track (NIST-09 score:
larger values are better).
System NIST-09 score
Alignment template approach 7.65
Competing research systems 5.03?7.34
Best of six commercial off-the-shelf systems 6.08
The Chinese language poses special problems because the boundaries of Chinese
words are not marked. Chinese text is provided as a sequence of characters, and it is
unclear which characters have to be grouped together to obtain entities that can be
interpreted as words. For statistical MT, it would be possible to ignore this fact and
treat the Chinese characters as elementary units and translate them into English. Yet
preliminary experiments showed that the existing alignment models produce better
results if the Chinese characters are segmented in a preprocessing step into single
words. We use the LDC segmentation tool.5
For the English corpus, the following preprocessing steps are applied. First, the
corpus is tokenized; it is then segmented into sentences, and all uppercase characters
are converted to lowercase. As the final evaluation criterion does not distinguish case,
it is not necessary to deal with the case information.
Then, the preprocessed Chinese and English corpora are sentence aligned in which
the lengths of the source and target sentences are significantly different. From the
resulting corpus, we automatically replace translations. In addition, only sentences
with less than 60 words in English and Chinese are used.
To improve the translation of Chinese numbers, we use a categorization of Chi-
nese number and date expressions. For the statistical learning, all number and date
expressions are replaced with one of two generic symbols, $number or $date. The num-
ber and date expressions are subjected to a rule-based translation by simple lexicon
lookup. The translation of the number and date expressions is inserted into the out-
put using the alignment information. For Chinese and English, this categorization is
implemented independently of the other language.
To evaluate MT quality on this task, NIST made available the NIST-09 evaluation
tool. This tool provides a modified BLEU score by computing a weighted precision of
n-grams modified by a length penalty for very short translations. Table 12 shows the
results of the official evaluation performed by NIST in June 2002. With a score of 7.65,
the results obtained were statistically significantly better than any other competing
approach. Differences in the NIST score larger than 0.12 are statistically significant at
the 95% level. We conclude that the developed alignment template approach is also
applicable to unrelated language pairs such as Chinese?English and that the developed
statistical models indeed seem to be largely language-independent. Table 13 shows
various example translations.
7. Conclusions
We have presented a framework for statistical MT for natural languages which is
more general than the widely used source?channel approach. It allows a baseline MT
5 The LDC segmentation tool is available at
http://morph.ldc.upenn.edu/Projects/Chinese/LDC ch.htm#cseg.
446
Computational Linguistics Volume 30, Number 4
Table 13
Example translations for Chinese?English MT.
Reference Significant Accomplishment Achieved in the Economic Construction of
the Fourteen Open Border Cities in China
Translation The opening up of the economy of China?s fourteen City made
significant achievements in construction
Reference Xinhua News Agency, Beijing, Feb. 12?Exciting accomplishment has
been achieved in 1995 in the economic construction of China?s fourteen
border cities open to foreigners.
Translation Xinhua News Agency, Beijing, February 12?China?s opening up to the
outside world of the 1995 in the fourteen border pleased
to obtain the construction of the economy.
Reference Foreign Investment in Jiangsu?s Agriculture on the Increase
Translation To increase the operation of foreign investment in Jiangsu agriculture
Reference According to the data provided today by the Ministry of Foreign Trade
and Economic Cooperation, as of November this year, China has
actually utilized 46.959 billion US dollars of foreign capital, including
40.007 billion US dollars of direct investment from foreign businessmen.
Translation The external economic and trade cooperation Department today
provided that this year, the foreign capital actually utilized by China on
November to US $46.959 billion, including of foreign company direct
investment was US $40.007 billion.
Reference According to officials from the Provincial Department of Agriculture
and Forestry of Jiangsu, the ?Three-Capital? ventures approved by
agencies within the agricultural system of Jiangsu Province since 1994
have numbered more than 500 and have utilized over 700 million US
dollars worth of foreign capital, respectively three times and seven
times more than in 1993.
Translation Jiangsu Province for the Secretaries said that, from the 1994 years,
Jiangsu Province system the approval of the ?three-funded?
enterprises, there are more than 500, foreign investment utilization
rate of more than US $700 million, 1993 years before three and seven.
Reference The actual amount of foreign capital has also increased more than 30%
as compared with the same period last year.
Translation The actual amount of foreign investment has increased by more
than 30% compared with the same period last year.
Reference Import and Export in Pudong New District Exceeding 9 billion US
dollars This Year
Translation Foreign trade imports and exports of this year to the Pudong
new Region exceeds US $9 billion
system to be extended easily by adding new feature functions. We have described
the alignment template approach for statistical machine translation, which uses two
different alignment levels: a phrase-level alignment between phrases and a word-
level alignment between single words. As a result the context of words has a greater
influence, and the changes in word order from source to target language can be learned
explicitly. An advantage of this method is that machine translation is learned fully
automatically through the use of a bilingual training corpus. We have shown that the
presented approach is capable of achieving better translation results on various tasks
compared to other statistical, example-based, or rule-based translation systems. This
is especially interesting, as our system is structured simpler than many competing
systems.
447
Och and Ney The Alignment Template Approach to Statistical Machine Translation
We expect that better translation can be achieved by using models that go beyond
the flat phrase segmentation that we perform in our model. A promising avenue is to
gradually extend the model to take into account to some extent the recursive structure
of natural languages using ideas from Wu and Wong (1998) or Alshawi, Bangalore, and
Douglas (2000). We expect other improvements as well from learning nonconsecutive
phrases in source or target language and from better generalization methods for the
learned-phrase pairs.
Acknowledgments
The work reported here was carried out
while the first author was with the
Lehrstuhl fu?r Informatik VI, Computer
Science Department, RWTH
Aachen?University of Technology.
References
Alshawi, Hiyan, Srinivas Bangalore, and
Shona Douglas. 2000. Learning
dependency translation models as
collections of finite state head transducers.
Computational Linguistics, 26(1):45?60.
Bellman, Richard. 1957. Dynamic
Programming. Princeton University Press,
Princeton.
Berger, Adam L., Peter F. Brown, Stephen A.
Della Pietra, Vincent J. Della Pietra,
John R. Gillett, John D. Lafferty, Harry
Printz, and Lubos Ures?. 1994. The
Candide system for machine translation.
In Proceedings of the ARPA Workshop on
Human Language Technology, pages
157?162, Plainsboro, NJ, March.
Berger, Adam L., Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural
language processing. Computational
Linguistics, 22(1):39?72.
Brown, Peter F., J. Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Frederick
Jelinek, John D. Lafferty, Robert L. Mercer,
and Paul S. Roossin. 1990. A statistical
approach to machine translation.
Computational Linguistics, 16(2):79?85.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical
machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Charniak, Eugene, Kevin Knight, and Kenji
Yamada. 2003. Syntax-based language
models for machine translation. In MT
Summit IX, pages 40?46, New Orleans,
September.
Darroch, J. N. and D. Ratcliff. 1972.
Generalized iterative scaling for log-linear
models. Annals of Mathematical Statistics,
43:1470?1480.
Dempster, A. P., N. M. Laird, and D. B.
Rubin. 1977. Maximum likelihood from
incomplete data via the EM algorithm.
Journal of the Royal Statistical Society,
Series B, 39(1):1?22.
Garc??a-Varea, Ismael, Francisco Casacuberta,
and Hermann Ney. 1998. An iterative,
DP-based search algorithm for statistical
machine translation. In Proceedings of the
International Conference on Spoken Language
Processing (ICSLP?98), pages 1235?1238,
Sydney, November.
Garc??a-Varea, Ismael, Franz Josef Och,
Hermann Ney, and Francisco
Casacuberta. 2001. Refined lexicon models
for statistical machine translation using a
maximum entropy approach. In
Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 204?211, Toulouse, France,
July.
Germann, Ulrich, Michael Jahr, Kevin
Knight, Daniel Marcu, and Kenji Yamada.
2001. Fast decoding and optimal decoding
for machine translation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics (ACL), pages
228?235, Toulouse, France, July.
Gildea, Daniel. 2003. Loosely tree-based
alignment for machine translation. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 80?87, Sapporo, Japan, July.
Knight, Kevin. 1999. Decoding complexity
in word-replacement translation models.
Computational Linguistics, 25(4):607?615.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the Human
Language Technology and North American
Association for Computational Linguistics
Conference (HLT/NAACL), pages 127?133,
Edmonton, Alberta.
Marcu, Daniel and William Wong. 2002. A
phrase-based, joint probability model for
statistical machine translation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP-2002), pages 133?139,
Philadelphia, July.
448
Computational Linguistics Volume 30, Number 4
Ney, Hermann. 1995. On the
probabilistic-interpretation of
neural-network classifiers and
discriminative training criteria. IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 17(2):107?119.
Ney, Hermann, Margit Generet, and Frank
Wessel. 1995. Extensions of absolute
discounting for language modeling. In
Proceedings of the Fourth European Conference
on Speech Communication and Technology,
pages 1245?1248, Madrid, September.
Ney, Hermann, Franz Josef Och, and
Stephan Vogel. 2001. The RWTH system
for statistical translation of spoken
dialogues. In Proceedings of the ARPA
Workshop on Human Language Technology,
San Diego, March.
Nie?en, Sonja, Franz Josef Och, Gregor
Leusch, and Hermann Ney. 2000. An
evaluation tool for machine translation:
Fast evaluation for machine translation
research. In Proceedings of the Second
International Conference on Language
Resources and Evaluation (LREC), pages
39?45, Athens, May.
Nie?en, Sonja, Stephan Vogel, Hermann
Ney, and Christoph Tillmann. 1998. A
DP-based search algorithm for statistical
machine translation. In COLING-ACL ?98:
36th Annual Meeting of the Association for
Computational Linguistics and 17th
International Conference on Computational
Linguistics, pages 960?967, Montreal,
August.
Och, Franz Josef. 1999. An efficient method
for determining bilingual word classes. In
EACL ?99: Ninth Conference of the European
Chapter of the Association for Computational
Linguistics, pages 71?76, Bergen, Norway,
June.
Och, Franz Josef. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo, Japan,
July.
Och, Franz Josef and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics (ACL), pages
295?302, Philadelphia, July.
Och, Franz Josef and Hermann Ney. 2003. A
systematic comparison of various
statistical alignment models.
Computational Linguistics, 29(1):19?51.
Och, Franz Josef, Nicola Ueffing, and
Hermann Ney. 2001. An efficient A*
search algorithm for statistical machine
translation. In Data-Driven Machine
Translation Workshop, pages 55?62,
Toulouse, France, July.
Papineni, Kishore A., Salim Roukos, and
R. Todd Ward. 1997. Feature-based
language understanding. In European
Conference on Speech Communication and
Technology, pages 1435?1438, Rhodes,
Greece, September.
Papineni, Kishore A., Salim Roukos, and
R. Todd Ward. 1998. Maximum likelihood
and discriminative training of direct
translation models. In Proceedings of the
International Conference on Acoustics, Speech,
and Signal Processing, pages 189?192,
Seattle, May.
Papineni, Kishore A., Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2001. Bleu:
A method for automatic evaluation of
machine translation. Technical Report
RC22176 (W0109-022), IBM Research
Division, Thomas J. Watson Research
Center, Yorktown Heights, NY.
Steinbiss, Volker, Bach-Hiep Tran, and
Hermann Ney. 1994. Improvements in
beam search. In Proceedings of the
International Conference on Spoken Language
Processing (ICSLP?94), pages 2143?2146,
Yokohama, Japan, September.
Tessiore, Lorenzo and Walther von Hahn.
2000. Functional validation of a machine
interpretation system: Verbmobil. In
Wolfgang Wahlster, editor, Verbmobil:
Foundations of Speech-to-Speech Translations,
pages 611?631. Springer, Berlin.
Tillmann, Christoph. 2001. Word Re-ordering
and Dynamic Programming Based Search
Algorithms for Statistical Machine
Translation. Ph.D. thesis, Computer
Science Department, RWTH Aachen,
Germany.
Tillmann, Christoph. 2003. A projection
extension algorithm for statistical
machine translation. In Michael Collins
and Mark Steedman, editors, Proceedings
of the 2003 Conference on Empirical Methods
in Natural Language Processing, pages 1?8,
Sapporo, Japan.
Tillmann, Christoph and Hermann Ney.
2000. Word re-ordering and DP-based
search in statistical machine translation.
In COLING ?00: The 18th International
Conference on Computational Linguistics,
pages 850?856, Saarbru?cken, Germany,
July.
Tillmann, Christoph and Hermann Ney.
2003. Word reordering and a dynamic
programming beam search algorithm for
statistical machine translation.
Computational Linguistics, 29(1):97?133.
449
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Tillmann, Christoph, Stephan Vogel,
Hermann Ney, and Alex Zubiaga. 1997. A
DP-based search using monotone
alignments in statistical translation. In
Proceedings of the 35th Annual Conference of
the Association for Computational Linguistics,
pages 289?296, Madrid, July.
Venugopal, Ashish, Stephan Vogel, and
Alex Waibel. 2003. Effective phrase
translation extraction from alignment
models. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 319?326, Sapporo,
Japan, July.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation.
In COLING ?96: The 16th International
Conference on Computational Linguistics,
pages 836?841, Copenhagen, August.
Wahlster, Wolfgang, editor. 2000. Verbmobil:
Foundations of Speech-to-Speech Translations.
Springer, Berlin.
Wang, Ye-Yi. 1998. Grammar Inference and
Statistical Machine Translation. Ph.D. thesis,
School of Computer Science, Language
Technologies Institute, Carnegie Mellon
University, Pittsburgh.
Wang, Ye-Yi and Alex Waibel. 1997.
Decoding algorithm in statistical
translation. In Proceedings of the 35th
Annual Conference of the Association for
Computational Linguistics, pages 366?372,
Madrid, July.
Wu, Dekai and William Wong. 1998.
Machine translation with a stochastic
grammatical channel. In COLING-ACL
?98: 36th Annual Meeting of the Association
for Computational Linguistics and 17th
International Conference on Computational
Linguistics, pages 1408?1414, Montreal,
August.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 523?530, Toulouse, France,
July.
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 245?253,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using a Dependency Parser to Improve SMT for Subject-Object-Verb
Languages
Peng Xu, Jaeho Kang, Michael Ringgaard and Franz Och
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
{xp,jhkang,ringgaard,och}@google.com
Abstract
We introduce a novel precedence reordering
approach based on a dependency parser to sta-
tistical machine translation systems. Similar
to other preprocessing reordering approaches,
our method can efficiently incorporate linguis-
tic knowledge into SMT systems without in-
creasing the complexity of decoding. For a set
of five subject-object-verb (SOV) order lan-
guages, we show significant improvements in
BLEU scores when translating from English,
compared to other reordering approaches, in
state-of-the-art phrase-based SMT systems.
1 Introduction
Over the past ten years, statistical machine transla-
tion has seen many exciting developments. Phrase-
based systems (Och, 2002; Koehn et.al., 2003;
Och and Ney, 2004) advanced the machine transla-
tion field by allowing translations of word sequences
(a.k.a., phrases) instead of single words. This ap-
proach has since been the state-of-the-art because of
its robustness in modeling local word reordering and
the existence of an efficient dynamic programming
decoding algorithm.
However, when phrase-based systems are used
between languages with very different word or-
ders, such as between subject-verb-object (SVO)
and subject-object-verb (SOV) languages, long dis-
tance reordering becomes one of the key weak-
nesses. Many reordering methods have been pro-
posed in recent years to address this problem in dif-
ferent aspects.
The first class of approaches tries to explicitly
model phrase reordering distances. Distance based
distortion model (Och, 2002; Koehn et.al., 2003) is
a simple way of modeling phrase level reordering.
It penalizes non-monotonicity by applying a weight
to the number of words between two source phrases
corresponding to two consecutive target phrases.
Later on, this model was extended to lexicalized
phrase reordering (Tillmann, 2004; Koehn, et.al.,
2005; Al-Onaizan and Papineni, 2006) by applying
different weights to different phrases. Most recently,
a hierarchical phrase reordering model (Galley and
Manning, 2008) was proposed to dynamically deter-
mine phrase boundaries using efficient shift-reduce
parsing. Along this line of research, discrimina-
tive reordering models based on a maximum entropy
classifier (Zens and Ney, 2006; Xiong, et.al., 2006)
also showed improvements over the distance based
distortion model. None of these reordering models
changes the word alignment step in SMT systems,
therefore, they can not recover from the word align-
ment errors. These models are also limited by a
maximum allowed reordering distance often used in
decoding.
The second class of approaches puts syntactic
analysis of the target language into both modeling
and decoding. It has been shown that direct model-
ing of target language constituents movement in ei-
ther constituency trees (Yamada and Knight, 2001;
Galley et.al., 2006; Zollmann et.al., 2008) or depen-
dency trees (Quirk, et.al., 2005) can result in signifi-
cant improvements in translation quality for translat-
ing languages like Chinese and Arabic into English.
A simpler alternative, the hierarchical phrase-based
245
approach (Chiang, 2005; Wu, 1997) also showed
promising results for translating Chinese to English.
Similar to the distance based reordering models, the
syntactical or hierarchical approaches also rely on
other models to get word alignments. These mod-
els typically combine machine translation decoding
with chart parsing, therefore significantly increase
the decoding complexity. Even though some re-
cent work has shown great improvements in decod-
ing efficiency for syntactical and hierarchical ap-
proaches (Huang and Chiang, 2007), they are still
not as efficient as phrase-based systems, especially
when higher order language models are used.
Finally, researchers have also tried to put source
language syntax into reordering in machine trans-
lation. Syntactical analysis of source language
can be used to deterministically reorder input sen-
tences (Xia and McCord, 2004; Collins et.al., 2005;
Wang et.al., 2007; Habash, 2007), or to provide mul-
tiple orderings as weighted options (Zhang et.al.,
2007; Li et.al., 2007; Elming, 2008). In these
approaches, input source sentences are reordered
based on syntactic analysis and some reordering
rules at preprocessing step. The reordering rules
can be either manually written or automatically ex-
tracted from data. Deterministic reordering based on
syntactic analysis for the input sentences provides
a good way of resolving long distance reordering,
without introducing complexity to the decoding pro-
cess. Therefore, it can be efficiently incorporated
into phrase-based systems. Furthermore, when the
same preprocessing reordering is performed for the
training data, we can still apply other reordering ap-
proaches, such as distance based reordering and hi-
erarchical phrase reordering, to capture additional
local reordering phenomena that are not captured by
the preprocessing reordering. The work presented in
this paper is largely motivated by the preprocessing
reordering approaches.
In the rest of the paper, we first introduce our de-
pendency parser based reordering approach based on
the analysis of the key issues when translating SVO
languages to SOV languages. Then, we show exper-
imental results of applying this approach to phrase-
based SMT systems for translating from English to
five SOV languages (Korean, Japanese, Hindi, Urdu
and Turkish). After showing that this approach can
also be beneficial for hierarchical phrase-based sys-
John can hit ballthe
?? ? ??
? ????? 
.
.
Figure 1: Example Alignment Between an English and a
Korean Sentence
tems, we will conclude the paper with future re-
search directions.
2 Translation between SVO and SOV
Languages
In linguistics, it is possible to define a basic word
order in terms of the verb (V) and its arguments,
subject (S) and object (O). Among all six possible
permutations, SVO and SOV are the most common.
Therefore, translating between SVO and SOV lan-
guages is a very important area to study. We use
English as a representative of SVO languages and
Korean as a representative for SOV languages in our
discussion about the word orders.
Figure 1 gives an example sentence in English and
its corresponding translation in Korean, along with
the alignments between the words. Assume that we
split the sentences into four phrases: (John , t@),
(can hit , `  ????), (the ball , ? ?D)
and (. , .). Since a phrase-based decoder generates
the translation from left to right, the following steps
need to happen when we translate from English to
Korean:
? Starts from the beginning of the sentence,
translates ?John? to ?t@?;
? Jumps to the right by two words, translates ?the
ball? to ???D?;
? Jumps to the left by four words, translates ?can
hit? to ?`?????;
? Finally, jumps to the right by two words, trans-
lates ?.? to ?.?.
It is clear that in order for the phrase-based decoder
to successfully carry out all of the reordering steps, a
very strong reordering model is required. When the
sentence gets longer with more complex structure,
the number of words to move over during decod-
ing can be quite high. Imagine when we translate
246
Figure 2: Dependency Parse Tree of an Example English
Sentence
the sentence ?English is used as the first or second
language in many countries around the world .?.
The decoder needs to make a jump of 13 words in
order to put the translation of ?is used? at the end
of the translation. Normally in a phrase-based de-
coder, very long distance reordering is not allowed
because of efficiency considerations. Therefore, it
is very difficult in general to translate English into
Korean with proper word order.
However, knowing the dependency parse trees of
the English sentences may simplify the reordering
problem significantly. In the simple example in Fig-
ure 1, if we analyze the English sentence and know
that ?John? is the subject, ?can hit? is the verb and
?the ball? is the object, we can reorder the English
into SOV order. The resulting sentence ?John the
ball can hit .? will only need monotonic translation.
This motivates us to use a dependency parser for En-
glish to perform the reordering.
3 Precedence Reordering Based on a
Dependency Parser
Figure 2 shows the dependency tree for the example
sentence in the previous section. In this parse, the
verb ?hit? has four children: a subject noun ?John?,
an auxiliary verb ?can?, an object noun ?ball? and a
punctuation ?.?. When transforming the sentence to
SOV order, we need to move the object noun and the
subtree rooted at it to the front of the head verb, but
after the subject noun. We can have a simple rule to
achieve this.
However, in reality, there are many possible chil-
dren for a verb. These children have some relative
ordering that is typically fixed for SOV languages.
In order to describe this kind of ordering, we pro-
pose precedence reordering rules based on a depen-
dency parse tree. All rules here are based English
and Korean examples, but they also apply to other
SOV languages, as we will show later empirically.
A precedence reordering rule is a mapping from
T to a set of tuples {(L,W,O)}, where T is the
part-of-speech (POS) tag of the head in a depen-
dency parse tree node, L is a dependency label for
a child node, W is a weight indicating the order of
that child node and O is the type of order (either
NORMAL or REVERSE). The type of order is only
used when we have multiple children with the same
weight, while the weight is used to determine the
relative order of the children, going from largest to
smallest. The weight can be any real valued num-
ber. The order type NORMAL means we preserve
the original order of the children, while REVERSE
means we flip the order. We reserve a special label
self to refer to the head node itself so that we can
apply a weight to the head, too. We will call this
tuple a precedence tuple in later discussions. In this
study, we use manually created rules only.
Suppose we have a precedence rule: VB ?
(nsubj, 2, NORMAL), (dobj, 1, NORMAL), (self,
0, NORMAL). For the example shown in Figure 2,
we would apply it to the ROOT node and result in
?John the ball can hit .?.
Given a set of rules, we apply them in a depen-
dency tree recursively starting from the root node. If
the POS tag of a node matches the left-hand-side of
a rule, the rule is applied and the order of the sen-
tence is changed. We go through all children of the
node and get the precedence weights for them from
the set of precedence tuples. If we encounter a child
node that has a dependency label not listed in the set
of tuples, we give it a default weight of 0 and de-
fault order type of NORMAL. The children nodes
are sorted according to their weights from highest to
lowest, and nodes with the same weights are ordered
according to the type of order defined in the rule.
3.1 Verb Precedence Rules
Verb movement is the most important movement
when translating from English (SVO) to Korean
(SOV). In a dependency parse tree, a verb node can
potentially have many children. For example, aux-
iliary and passive auxiliary verbs are often grouped
together with the main verb and moved together with
it. The order, however, is reversed after the move-
ment. In the example of Figure 2, the correct Korean
247
     
??
? ??
? ????? 
   
??? ?
.
Figure 3: Dependency Parse Tree with Alignment for a
Sentence with Preposition Modifier
word order is ?` (hit)  ????(can) . Other
categories that are in the same group are phrasal verb
particle and negation.
If the verb in an English sentence has a preposi-
tional phrase as a child, the prepositional phrase is
often placed before the direct object in the Korean
counterpart. As shown in Figure 3, ?)?t \?
(?with a bat?) is actually between ?t@? (?John?)
and ???D? (?the ball?).
Another common reordering phenomenon is
when a verb has an adverbial clause modifier. In that
case, the whole adverbial clause is moved together to
be in front of the subject of the main sentence. Inside
the adverbial clause, the ordering follows the same
verb reordering rules, so we recursively reorder the
clause.
Our verb precedence rule, as in Table 1, can cover
all of the above reordering phenomena. One way
to interpret this rule set is as follows: for any node
whose POS tag is matches VB* (VB, VBZ, VBD,
VBP, VBN, VBG), we group the children node that
are phrasal verb particle (prt), auxiliary verb (aux),
passive auxiliary verb (auxpass), negation (neg) and
the verb itself (self) together and reverse them. This
verb group is moved to the end of the sentence. We
move adverbial clause modifier to the beginning of
the sentence, followed by a group of noun subject
(nsubj), preposition modifier and anything else not
listed in the table, in their original order. Right be-
fore the verb group, we put the direct object (dobj).
Note that all of the children are optional.
3.2 Adjective Precedence Rules
Similar to the verbs, adjectives can also take an aux-
iliary verb, a passive auxiliary verb and a negation
T (L, W, O)
VB*
(advcl, 1, NORMAL)
(nsubj, 0, NORMAL)
(prep, 0, NORMAL)
(dobj, -1, NORMAL)
(prt, -2, REVERSE)
(aux, -2, REVERSE)
(auxpass, -2, REVERSE)
(neg, -2, REVERSE)
(self, -2, REVERSE)
JJ or JJS or JJR
(advcl, 1, NORMAL)
(self, -1, NORMAL)
(aux, -2, REVERSE)
(auxpass, -2, REVERSE)
(neg, -2, REVERSE)
(cop, -2, REVERSE)
NN or NNS
(prep, 2, NORMAL)
(rcmod, 1, NORMAL)
(self, 0, NORMAL)
IN or TO (pobj, 1, NORMAL)(self, -1, NORMAL)
Table 1: Precedence Rules to Reorder English to SOV
Language Order (These rules were extracted manually by
a bilingual speaker after looking at some text book exam-
ples in English and Korean, and the dependency parse
trees of the English examples.)
as modifiers. In such cases, the change in order from
English to Korean is similar to the verb rule, except
that the head adjective itself should be in front of the
verbs. Therefore, in our adjective precedence rule in
the second panel of Table 1, we group the auxiliary
verb, the passive auxiliary verb and the negation and
move them together after reversing their order. They
are moved to right after the head adjective, which is
put after any other modifiers.
For both verb and adjective precedence rules,
we also apply some heuristics to prevent exces-
sive movements. In order to do this, we disallow
any movement across punctuation and conjunctions.
Therefore, for sentences like ?John hit the ball but
Sam threw the ball?, the reordering result would be
?John the ball hit but Sam the ball threw?, instead
of ?John the ball but Sam the ball threw hit?.
3.3 Noun and Preposition Precedence Rules
In Korean, when a noun is modified by a preposi-
tional phrase, such as in ?the way to happiness?,
the prepositional phrase is usually moved in front of
the noun, resulting in ??? (happiness)<\ ?
8 (to the way)? . Similarly for relative clause mod-
ifier, it is also reordered to the front of the head noun.
For preposition head node with an object modifier,
248
the order is the object first and the preposition last.
One example is ?with a bat? in Figure 3. It corre-
sponds to ?)?t (a bat) \(with)?. We handle
these types of reordering by the noun and preposi-
tion precedence rules in the third and fourth panel of
Table 1.
With the rules defined in Table 1, we now show a
more complex example in Figure 4. First, the ROOT
node matches an adjective rule, with four children
nodes labeled as (csubj, cop, advcl, p), and with
precedence weights of (0, -2, 1, 0). The ROOT node
itself has a weight of -1. After reordering, the sen-
tence becomes: ?because we do n?t know what the
future has Living exciting is .?. Note that the whole
adverbial phrase rooted at ?know? is moved to the
beginning of the sentence. After that, we see that
the child node rooted at ?know? matches a verb rule,
with five children nodes labeled as (mark, nsubj,
aux, neg, ccomp), with weights (0, 0, -2, -2, 0). In
this case, the verb itself also has weight -2. Now
we have two groups of nodes, with weight 0 and -2,
respectively. The first group has a NORMAL order
and the second group has a REVERSE order. Af-
ter reordering, the sentence becomes: ?because we
what the future has know n?t do Living exciting
is .?. Finally, we have another node rooted at ?has?
that matches the verb rule again. After the final re-
ordering, we end up with the sentence: ?because we
the future what has know n?t do Living exciting
is .?. We can see in Figure 4 that this sentence has an
almost monotonic alignment with a reasonable Ko-
rean translation shown in the figure1.
4 Related Work
As we mentioned in our introduction, there have
been several studies in applying source sentence re-
ordering using syntactical analysis for statistical ma-
chine translation. Our precedence reordering ap-
proach based on a dependency parser is motivated by
those previous works, but we also distinguish from
their studies in various ways.
Several approaches use syntactical analysis to
provide multiple source sentence reordering options
through word lattices (Zhang et.al., 2007; Li et.al.,
2007; Elming, 2008). A key difference between
1We could have improved the rules by using a weight of -3
for the label ?mark?, but it was not in our original set of rules.
their approaches and ours is that they do not perform
reordering during training. Therefore, they would
need to rely on reorder units that are likely not vio-
lating ?phrase? boundaries. However, since we re-
order both training and test data, our system oper-
ates in a matched condition. They also focus on ei-
ther Chinese to English (Zhang et.al., 2007; Li et.al.,
2007) or English to Danish (Elming, 2008), which
arguably have less long distance reordering than be-
tween English and SOV languages.
Studies most similar to ours are those preprocess-
ing reordering approaches (Xia and McCord, 2004;
Collins et.al., 2005; Wang et.al., 2007; Habash,
2007). They all perform reordering during prepro-
cessing based on either automatically extracted syn-
tactic rules (Xia and McCord, 2004; Habash, 2007)
or manually written rules (Collins et.al., 2005; Wang
et.al., 2007). Compared to these approaches, our
work has a few differences. First of all, we study
a wide range of SOV languages using manually ex-
tracted precedence rules, not just for one language
like in these studies. Second, as we will show in
the next section, we compare our approach to a
very strong baseline with more advanced distance
based reordering model, not just the simplest distor-
tion model. Third, our precedence reordering rules,
like those in Habash, 2007, are more flexible than
those other rules. Using just one verb rule, we can
perform the reordering of subject, object, preposi-
tion modifier, auxiliary verb, negation and the head
verb. Although we use manually written rules in
this study, it is possible to learn our rules automat-
ically from alignments, similarly to Habash, 2007.
However, unlike Habash, 2007, our manually writ-
ten rules handle unseen children and their order nat-
urally because we have a default precedence weight
and order type, and we do not need to match an often
too specific condition, but rather just treat all chil-
dren independently. Therefore, we do not need to
use any backoff scheme in order to have a broad cov-
erage. Fourth, we use dependency parse trees rather
than constituency trees.
There has been some work on syntactic word or-
der model for English to Japanese machine transla-
tion (Chang and Toutanova, 2007). In this work, a
global word order model is proposed based on fea-
tures including word bigram of the target sentence,
displacements and POS tags on both source and tar-
249
                  
??? ???
  ?????????? ? ?
?
? ??
??? ???
.
we the Livingwhatfuture knowhas n't do excitingbecause is .
csubj cop detmarkROOT auxnsubj neg advcl nsubjdobj ccomp p
Living is thebecauseexciting dowe n't know futurewhat has
.
VBG VBZ DTINJJ VBPPRP RB VB NNWP VBZ
.
Label
Token
POS
Figure 4: A Complex Reordering Example (Reordered English sentence and alignments are at the bottom.)
get sides. They build a log-linear model using these
features and apply the model to re-rank N -best lists
from a baseline decoder. Although we also study the
reordering problem in English to Japanese transla-
tion, our approach is to incorporate the linguistically
motivated reordering directly into modeling and de-
coding.
5 Experiments
We carried out all our experiments based on a state-
of-the-art phrase-based statistical machine transla-
tion system. When training a system for English
to any of the 5 SOV languages, the word alignment
step includes 3 iterations of IBM Model-1 training
and 2 iterations of HMM training. We do not use
Model-4 because it is slow and it does not add much
value to our systems in a pilot study. We use the
standard phrase extraction algorithm (Koehn et.al.,
2003) to get al phrases up to length 5. In addition
to the regular distance distortion model, we incor-
porate a maximum entropy based lexicalized phrase
reordering model (Zens and Ney, 2006) as a fea-
ture used in decoding. In this model, we use 4 re-
ordering classes (+1, > 1, ?1, < ?1) and words
from both source and target as features. For source
words, we use the current aligned word, the word
before the current aligned word and the next aligned
word; for target words, we use the previous two
words in the immediate history. Using this type of
features makes it possible to directly use the maxi-
mum entropy model in the decoding process (Zens
and Ney, 2006). The maximum entropy models are
trained on all events extracted from training data
word alignments using the LBFGS algorithm (Mal-
ouf, 2002). Overall for decoding, we use between 20
System Source Target
English?Korean 303M 267M
English?Japanese 316M 350M
English?Hindi 16M 17M
English?Urdu 17M 19M
English?Turkish 83M 76M
Table 2: Training Corpus Statistics (#words) of Systems
for 5 SOV Languages
to 30 features, whose weights are optimized using
MERT (Och, 2003), with an implementation based
on the lattice MERT (Macherey et.al., 2008).
For parallel training data, we use an in-house col-
lection of parallel documents. They come from var-
ious sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. Therefore, for some doc-
uments in the training data, we do not necessarily
have the exact clean translations. Table 2 shows the
actual statistics about the training data for all five
languages we study. For all 5 SOV languages, we
use the target side of the parallel data and some more
monolingual text from crawling the web to build 4-
gram language models.
We also collected about 10K English sentences
from the web randomly. Among them, 9.5K are used
as evaluation data. Those sentences were translated
by humans to all 5 SOV languages studied in this
paper. Each sentence has only one reference trans-
lation. We split them into 3 subsets: dev contains
3,500 sentences, test contains 1,000 sentences and
the rest of 5,000 sentences are used in a blindtest
set. The dev set is used to perform MERT training,
while the test set is used to select trained weights
due to some nondeterminism of MERT training. We
use IBM BLEU (Papineni et al, 2002) to evaluate
250
our translations and use character level BLEU for
Korean and Japanese.
5.1 Preprocessing Reordering and Reordering
Models
We first compare our precedence rules based prepro-
cessing reordering with the maximum entropy based
lexicalized reordering model. In Table 3, Baseline
is our system with both a distance distortion model
and the maximum entropy based lexicalized reorder-
ing model. For all results reported in this section,
we used a maximum allowed reordering distance of
10. In order to see how the lexicalized reordering
model performs, we also included systems with and
without it (-LR means without it). PR is our pro-
posed approach in this paper. Note that since we ap-
ply precedence reordering rules during preprocess-
ing, we can combine this approach with any other
reordering models used during decoding. The only
difference is that with the precedence reordering, we
would have a different phrase table and in the case
of LR, different maximum entropy models.
In order to implement the precedence rules, we
need a dependency parser. We choose to use a
deterministic inductive dependency parser (Nivre
and Scholz, 2004) for its efficiency and good ac-
curacy. Our implementation of the deterministic
dependency parser using maximum entropy models
as the underlying classifiers achieves 87.8% labeled
attachment score and 88.8% unlabeled attachment
score on standard Penn Treebank evaluation.
As our results in Table 3 show, for all 5 lan-
guages, by using the precedence reordering rules as
described in Table 1, we achieve significantly bet-
ter BLEU scores compared to the baseline system.
In the table, We use two stars (??) to mean that
the statistical significance test using the bootstrap
method (Koehn, 2004) gives an above 95% signif-
icance level when compared to the baselie. We mea-
sured the statistical significance level only for the
blindtest data.
Note that for Korean and Japanese, our prece-
dence reordering rules achieve better absolute
BLEU score improvements than for Hindi, Urdu and
Turkish. Since we only analyzed English and Ko-
rean sentences, it is possible that our rules are more
geared toward Korean. Japanese has almost exactly
the same word order as Korean, so we could assume
Language System dev test blind
Korean
BL 25.8 27.0 26.2
-LR 24.7 25.6 25.1
-LR+PR 27.3 28.3 27.5**
+PR 27.8 28.7 27.9**
Japanese
BL 29.5 29.3 29.3
-LR 29.2 29.0 29.0
-LR+PR 30.3 31.0 30.6**
+PR 30.7 31.2 31.1**
Hindi
BL 19.1 18.9 18.3
-LR 17.4 17.1 16.4
-LR+PR 19.6 18.8 18.7**
+PR 19.9 18.9 18.8**
Urdu
BL 9.7 9.5 8.9
-LR 9.1 8.6 8.2
-LR+PR 10.0 9.6 9.6**
+PR 10.0 9.8 9.6**
Turkish
BL 10.0 10.5 9.8
-LR 9.1 10.0 9.0
-LR+PR 10.5 11.0 10.3**
+PR 10.5 10.9 10.4**
Table 3: BLEU Scores on Dev, Test and Blindtest for En-
glish to 5 SOV Languages with Various Reordering Op-
tions (BL means baseline, LR means maximum entropy
based lexialized phrase reordering model, PR means
precedence rules based preprocessing reordering.)
the benefits can carry over to Japanese.
5.2 Reordering Constraints
One of our motivations of using the precedence re-
ordering rules is that English will look like SOV lan-
guages in word order after reordering. Therefore,
even monotone decoding should be able to produce
better translations. To see this, we carried out a con-
trolled experiment, using Korean as an example.
Clearly, after applying the precedence reordering
rules, our English to Korean system is not sensitive
to the maximum allowed reordering distance any-
more. As shown in Figure 5, without the rules, the
blindtest BLEU scores improve monotonically as
the allowed reordering distance increases. This indi-
cates that the order difference between English and
Korean is very significant. Since smaller allowed
reordering distance directly corresponds to decod-
ing time, we can see that with the same decoding
speed, our proposed approach can achieve almost
5% BLEU score improvements on blindtest set.
5.3 Preprocessing Reordering and
Hierarchical Model
The hierarchical phrase-based approach has been
successfully applied to several systems (Chiang,
251
1 2 4 6 8 10Maximum Allowed Reordering Distance
0.23
0.24
0.25
0.26
0.27
0.28
Blindt
est BL
EU Sc
ore No LexReorderBaselineNo LexReorder, with ParserReorderWith ParserReorder
Figure 5: Blindtest BLEU Score for Different Maximum
Allowed Reordering Distance for English to Korean Sys-
tems with Different Reordering Options
2005; Zollmann et.al., 2008). Since hierarchical
phrase-based systems can capture long distance re-
ordering by using a PSCFG model, we expect it to
perform well in English to SOV language systems.
We use the same training data as described in the
previous sections for building hierarchical systems.
The same 4-gram language models are also used for
the 5 SOV languages. We adopt the SAMT pack-
age (Zollmann and Venugopal, 2006) and follow
similar settings as Zollmann et.al., 2008. We allow
each rule to have at most 6 items on the source side,
including nonterminals and extract rules from initial
phrases of maximum length 12. During decoding,
we allow application of all rules of the grammar for
chart items spanning up to 12 source words.
Since our precedence reordering applies at pre-
processing step, we can train a hierarchical system
after applying the reordering rules. When doing so,
we use exactly the same settings as a regular hier-
archical system. The results for both hierarchical
systems and those combined with the precedence re-
ordering are shown in Table 4, together with the best
normal phrase-based systems we copy from Table 3.
Here again, we mark any blindtest BLEU score that
is better than the corresponding hierarchical system
with confidence level above 95%. Note that the hier-
archical systems can not use the maximum entropy
based lexicalized phrase reordering models.
Except for Hindi, applying the precedence re-
ordering rules in a hierarchical system can achieve
statistically significant improvements over a normal
hierarchical system. We conjecture that this may be
because of the simplicity of our reordering rules.
Language System dev test blind
Korean
PR 27.8 28.7 27.9
Hier 27.4 27.7 27.9
PR+Hier 28.5 29.1 28.8**
Japanese
PR 30.7 31.2 31.1**
Hier 30.5 30.6 30.5
PR+Hier 31.0 31.3 31.1**
Hindi
PR 19.9 18.9 18.8
Hier 20.3 20.3 19.3
PR+Hier 20.0 19.7 19.3
Urdu
PR 10.0 9.8 9.6
Hier 10.4 10.3 10.0
PR+Hier 11.2 10.7 10.7**
Turkish
PR 10.5 10.9 10.4
Hier 11.0 11.8 10.5
PR+Hier 11.1 11.6 10.9**
Table 4: BLEU Scores on Dev, Test and Blindtest for En-
glish to 5 SOV Languages in Hierarchical Phrase-based
Systems (PR is precedence rules based preprocessing re-
ordering, same as in Table 3, while Hier is the hierarchi-
cal system.)
Other than the reordering phenomena covered by
our rules in Table 1, there could be still some local or
long distance reordering. Therefore, using a hierar-
chical phrase-based system can improve those cases.
Another possible reason is that after the reordering
rules apply in preprocessing, English sentences in
the training data are very close to the SOV order. As
a result, EM training becomes much easier and word
alignment quality becomes better. Therefore, a hier-
archical phrase-based system can extract better rules
and hence achievesbetter translation quality.
We also point out that hierarchical phrase-based
systems require a chart parsing algorithm during de-
coding. Compared to the efficient dynamic pro-
gramming in phrase-based systems, it is much
slower. This makes our approach more appealing
in a realtime statistical machine translation system.
6 Conclusion
In this paper, we present a novel precedence re-
ordering approach based on a dependency parser.
We successfully applied this approach to systems
translating English to 5 SOV languages: Korean,
Japanese, Hindi, Urdu and Turkish. For all 5 lan-
guages, we achieve statistically significant improve-
ments in BLEU scores over a state-of-the-art phrase-
based baseline system. The amount of training data
for the 5 languages varies from around 17M to more
than 350M words, including some noisy data from
252
the web. Our proposed approach has shown to be
robust and versatile. For 4 out of the 5 languages,
our approach can even significantly improve over a
hierarchical phrase-based baseline system. As far as
we know, we are the first to show that such reorder-
ing rules benefit several SOV languages.
We believe our rules are flexible and can cover
many linguistic reordering phenomena. The format
of our rules also makes it possible to automatically
extract rules from word aligned corpora. In the fu-
ture, we plan to investigate along this direction and
extend the rules to languages other than SOV.
The preprocessing reordering like ours is known
to be sensitive to parser errors. Some preliminary
error analysis already show that indeed some sen-
tences suffer from parser errors. In the recent years,
several studies have tried to address this issue by us-
ing a word lattice instead of one reordering as in-
put (Zhang et.al., 2007; Li et.al., 2007; Elming,
2008). Although there is clearly room for improve-
ments, we also feel that using one reordering during
training may not be good enough either. It would be
very interesting to investigate ways to have efficient
procedure for training EM models and getting word
alignments using word lattices on the source side of
the parallel data. Along this line of research, we
think some kind of tree-to-string model (Liu et.al.,
2006) could be interesting directions to pursue.
References
Yaser Al-Onaizan and Kishore Papineni 2006. Distortion Models for
Statistical Machine Translation In Proceedings of ACL
Pi-Chuan Chang and Kristina Toutanova 2007. A Discriminative Syn-
tactic Word Order Model for Machine Translation In Proceedings
of ACL
David Chiang 2005. A Hierarchical Phrase-based Model for Statistical
Machine Translation In Proceedings of ACL
Michael Collins, Philipp Koehn and Ivona Kucerova 2005. Clause
Restructuring for Statistical Machine Translation In Proceedings of
ACL
Jakob Elming 2008. Syntactic Reordering Integrated with Phrase-
based SMT In Proceedings of COLING
Michel Galley and Christopher D. Manning 2008. A Simple and Ef-
fective Hierarchical Phrase Reordering Model In Proceedings of
EMNLP
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve
DeNeefe, Wei Wang and Ignacio Thayer 2006. Scalable Inference
and Training of Context-Rich Syntactic Translation Models In Pro-
ceedings of COLING-ACL
Nizar Habash 2007. Syntactic Preprocessing for Statistical Machine
Translation In Proceedings of 11th MT Summit
Liang Huang and David Chiang 2007. Forest Rescoring: Faster De-
coding with Integrated Language Models, In Proceedings of ACL
Philipp Koehn 2004. Statistical Significance Tests for Machine Trans-
lation Evaluation In Proceedings of EMNLP
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris
Callison-Burch, Miles Osborne and David Talbot 2005. Edinborgh
System Description for the 2005 IWSLT Speech Translation Evalu-
ation In International Workshop on Spoken Language Translation
Philipp Koehn, Franz J. Och and Daniel Marcu 2003. Statistical
Phrase-based Translation, In Proceedings of HLT-NAACL
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou, Minghui Li and Yi
Guan 2007. A Probabilistic Approach to Syntax-based Reordering
for Statistical Machine Translation, In Proceedings of ACL
Yang Liu, Qun Liu and Shouxun Lin 2006. Tree-to-string Alignment
Template for Statistical Machine Translation, In Proceedings of
COLING-ACL
Wolfgang Macherey, Franz J. Och, Ignacio Thayer and Jakob Uszkoreit
2008. Lattice-based Minimum Error Rate Training for Statistical
Machine Translation In Proceedings of EMNLP
Robert Malouf 2002. A comparison of algorithms for maximum en-
tropy parameter estimation In Proceedings of the Sixth Workshop
on Computational Language Learning (CoNLL-2002)
Joakim Nivre and Mario Scholz 2004. Deterministic Dependency Pars-
ing for English Text. In Proceedings of COLING
Franz J. Och 2002. Statistical Machine Translation: From Single Word
Models to Alignment Template Ph.D. Thesis, RWTH Aachen, Ger-
many
Franz J. Och. 2003. Minimum Error Rate Training in Statistical Ma-
chine Translation. In Proceedings of ACL
Franz J. Och and Hermann Ney 2004. The Alignment Template Ap-
proach to Statistical Machine Translation. Computational Linguis-
tics, 30:417-449
Kishore Papineni, Roukos, Salim et al 2002. BLEU: A Method for
Automatic Evaluation of Machine Translation. In Proceedings of
ACL
Chris Quirk, Arul Menezes and Colin Cherry 2005. Dependency Tree
Translation: Syntactically Informed Phrasal SMT In Proceedings of
ACL
Christoph Tillmann 2004. A Block Orientation Model for Statistical
Machine Translation In Proceedings of HLT-NAACL
Chao Wang, Michael Collins and Philipp Koehn 2007. Chinese Syntac-
tic Reordering for Statistical Machine Translation In Proceedings of
EMNLP-CoNLL
Dekai Wu 1997. Stochastic Inversion Transduction Grammars and
Bilingual Parsing of Parallel Corpus In Computational Linguistics
23(3):377-403
Fei Xia and Michael McCord 2004. Improving a Statistical MT Sys-
tem with Automatically Learned Rewrite Patterns In Proceedings of
COLING
Deyi Xiong, Qun Liu and Shouxun Lin 2006. Maximum Entropy
Based Phrase Reordering Model for Statistical Machine Translation
In Proceedings of COLING-ACL
Kenji Yamada and Kevin Knight 2001. A Syntax-based Statistical
Translation Model In Proceedings of ACL
Yuqi Zhang, Richard Zens and Hermann Ney 2007. Improve Chunk-
level Reordering for Statistical Machine Translation In Proceedings
of IWSLT
Richard Zens and Hermann Ney 2006. Discriminative Reordering
Models for Statistical Machine Translation In Proceedings of the
Workshop on Statistical Machine Translation, HLT-NAACL pages
55-63
Andreas Zollmann and Ashish Venugopal 2006. Syntax Augmented
Machine Translation via Chart Parsing In Proceedings of NAACL
2006 - Workshop on Statistical Machine Translation
Andreas Zollmann, Ashish Venugopal, Franz Och and Jay Ponte
2008. A Systematic Comparison of Phrase-Based, Hierarchical and
Syntax-Augmented Statistical MT In Proceedings of COLING
253
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163?171,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Efficient Minimum Error Rate Training and
Minimum Bayes-Risk Decoding for
Translation Hypergraphs and Lattices
Shankar Kumar1 and Wolfgang Macherey1 and Chris Dyer2 and Franz Och1
1Google Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043, USA
{shankarkumar,wmach,och}@google.com
2Department of Linguistics
University of Maryland
College Park, MD 20742, USA
redpony@umd.edu
Abstract
Minimum Error Rate Training (MERT)
and Minimum Bayes-Risk (MBR) decod-
ing are used in most current state-of-the-
art Statistical Machine Translation (SMT)
systems. The algorithms were originally
developed to work with N -best lists of
translations, and recently extended to lat-
tices that encode many more hypotheses
than typical N -best lists. We here extend
lattice-based MERT and MBR algorithms
to work with hypergraphs that encode a
vast number of translations produced by
MT systems based on Synchronous Con-
text Free Grammars. These algorithms
are more efficient than the lattice-based
versions presented earlier. We show how
MERT can be employed to optimize pa-
rameters for MBR decoding. Our exper-
iments show speedups from MERT and
MBR as well as performance improve-
ments from MBR decoding on several lan-
guage pairs.
1 Introduction
Statistical Machine Translation (SMT) systems
have improved considerably by directly using the
error criterion in both training and decoding. By
doing so, the system can be optimized for the
translation task instead of a criterion such as like-
lihood that is unrelated to the evaluation met-
ric. Two popular techniques that incorporate the
error criterion are Minimum Error Rate Train-
ing (MERT) (Och, 2003) and Minimum Bayes-
Risk (MBR) decoding (Kumar and Byrne, 2004).
These two techniques were originally developed
for N -best lists of translation hypotheses and re-
cently extended to translation lattices (Macherey
et al, 2008; Tromble et al, 2008) generated by a
phrase-based SMT system (Och and Ney, 2004).
Translation lattices contain a significantly higher
number of translation alternatives relative to N -
best lists. The extension to lattices reduces the
runtimes for both MERT and MBR, and gives per-
formance improvements from MBR decoding.
SMT systems based on synchronous context
free grammars (SCFG) (Chiang, 2007; Zollmann
and Venugopal, 2006; Galley et al, 2006) have
recently been shown to give competitive perfor-
mance relative to phrase-based SMT. For these
systems, a hypergraph or packed forest provides a
compact representation for encoding a huge num-
ber of translation hypotheses (Huang, 2008).
In this paper, we extend MERT and MBR
decoding to work on hypergraphs produced by
SCFG-based MT systems. We present algorithms
that are more efficient relative to the lattice al-
gorithms presented in Macherey et al (2008;
Tromble et al (2008). Lattice MBR decoding uses
a linear approximation to the BLEU score (Pap-
ineni et al, 2001); the weights in this linear loss
are set heuristically by assuming that n-gram pre-
cisions decay exponentially with n. However, this
may not be optimal in practice. We employ MERT
to select these weights by optimizing BLEU score
on a development set.
A related MBR-inspired approach for hyper-
graphs was developed by Zhang and Gildea
(2008). In this work, hypergraphs were rescored to
maximize the expected count of synchronous con-
stituents in the translation. In contrast, our MBR
algorithm directly selects the hypothesis in the
hypergraph with the maximum expected approx-
imate corpus BLEU score (Tromble et al, 2008).
will soon announce 
X1  X2
X1  X2
X1  X2
X1  X2
X1  X2
X1 its future in the 
X1 its future in the 
Suzuki
soon
its future in
X1 announces
Rally World Championship
Figure 1: An example hypergraph.
163
2 Translation Hypergraphs
A translation lattice compactly encodes a large
number of hypotheses produced by a phrase-based
SMT system. The corresponding representation
for an SMT system based on SCFGs (e.g. Chi-
ang (2007), Zollmann and Venugopal (2006), Mi
et al (2008)) is a directed hypergraph or a packed
forest (Huang, 2008).
Formally, a hypergraph is a pair H = ?V, E?
consisting of a vertex set V and a set of hyperedges
E ? V? ? V . Each hyperedge e ? E connects a
head vertex h(e) with a sequence of tail vertices
T (e) = {v1, ..., vn}. The number of tail vertices
is called the arity (|e|) of the hyperedge. If the ar-
ity of a hyperedge is zero, h(e) is called a source
vertex. The arity of a hypergraph is the maximum
arity of its hyperedges. A hyperedge of arity 1 is a
regular edge, and a hypergraph of arity 1 is a regu-
lar graph (lattice). Each hyperedge is labeled with
a rule re from the SCFG. The number of nontermi-
nals on the right-hand side of re corresponds with
the arity of e. An example without scores is shown
in Figure 1. A path in a translation hypergraph in-
duces a translation hypothesis E along with its se-
quence of SCFG rules D = r1, r2, ..., rK which,
if applied to the start symbol, derives E. The se-
quence of SCFG rules induced by a path is also
called a derivation tree for E.
3 Minimum Error Rate Training
Given a set of source sentences FS1 with corre-
sponding reference translations RS1 , the objective
of MERT is to find a parameter set ??M1 which min-
imizes an automated evaluation criterion under a
linear model:
??M1 = argmin
?M1
? SX
s=1
Err
`
Rs, E?(Fs; ?
M
1 )
?
ff
E?(Fs; ?
M
1 ) = argmax
E
? SX
s=1
?mhm(E, Fs)
ff
.
In the context of statistical machine translation,
the optimization procedure was first described in
Och (2003) for N -best lists and later extended to
phrase-lattices in Macherey et al (2008). The al-
gorithm is based on the insight that, under a log-
linear model, the cost function of any candidate
translation can be represented as a line in the plane
if the initial parameter set ?M1 is shifted along a
direction dM1 . Let C = {E1, ..., EK} denote a set
of candidate translations, then computing the best
scoring translation hypothesis E? out of C results in
the following optimization problem:
E?(F ; ?) = argmax
E?C
n
(?M1 + ? ? d
M
1 )
> ? hM1 (E,F )
o
= argmax
E?C
?
X
m
?mhm(E,F )
| {z }
=a(E,F )
+ ? ?
X
m
dmhm(E,F )
| {z }
=b(E,F )
ff
= argmax
E?C
?
a(E,F ) + ? ? b(E,F )
| {z }
(?)
?
Hence, the total score (?) for each candidate trans-
lation E ? C can be described as a line with
? as the independent variable. For any particu-
lar choice of ?, the decoder seeks that translation
which yields the largest score and therefore corre-
sponds to the topmost line segment. If ? is shifted
from ?? to +?, other translation hypotheses
may at some point constitute the topmost line seg-
ments and thus change the decision made by the
decoder. The entire sequence of topmost line seg-
ments is called upper envelope and provides an ex-
haustive representation of all possible outcomes
that the decoder may yield if ? is shifted along
the chosen direction. Both the translations and
their corresponding line segments can efficiently
be computed without incorporating any error crite-
rion. Once the envelope has been determined, the
translation candidates of its constituent line seg-
ments are projected onto their corresponding error
counts, thus yielding the exact and unsmoothed er-
ror surface for all candidate translations encoded
in C. The error surface can now easily be traversed
in order to find that ?? under which the new param-
eter set ?M1 + ?? ? d
M
1 minimizes the global error.
In this section, we present an extension of the
algorithm described in Macherey et al (2008)
that allows us to efficiently compute and repre-
sent upper envelopes over all candidate transla-
tions encoded in hypergraphs. Conceptually, the
algorithm works by propagating (initially empty)
envelopes from the hypergraph?s source nodes
bottom-up to its unique root node, thereby ex-
panding the envelopes by applying SCFG rules to
the partial candidate translations that are associ-
ated with the envelope?s constituent line segments.
To recombine envelopes, we need two operators:
the sum and the maximum over convex polygons.
To illustrate which operator is applied when, we
transform H = ?V, E? into a regular graph with
typed nodes by (1) marking all vertices v ? V with
the symbol ? and (2) replacing each hyperedge
e ? E , |e| > 1, with a small subgraph consisting
of a new vertex v?(e) whose incoming and out-
going edges connect the same head and tail nodes
164
Algorithm 1 ?-operation (Sum)
input: associative map a: V ? Env(V), hyperarc e
output: Minkowski sum of envelopes over T (e)
for (i = 0; i < |T (e)|; ++i) {
v = Ti(e);
pq.enqueue(? v, i, 0?);
}
L = ?;
D = ? e, ?1 ? ? ? ?|e|?
while (!pq.empty()) {
? v, i, j? = pq.dequeue();
` = A[v][j];
D[i+1] = `.D;
if (L.empty() ? L.back().x < `.x) {
if (0 < j) {
`.y += L.back().y - A[v][j-1].y;
`.m += L.back().m - A[v][j-1].m;
}
L.push_back(`);
L.back().D = D;
} else {
L.back().y += `.y;
L.back().m += `.m;
L.back().D[i+1] = `.D;
if (0 < j) {
L.back().y -= A[v][j-1].y;
L.back().m -= A[v][j-1].m;
}
}
if (++j < A[v].size())
pq.enqueue(? v, i, j?);
}
return L;
in the transformed graph as were connected by e
in the original graph. The unique outgoing edge
of v?(e) is associated with the rule re; incoming
edges are not linked to any rule. Figure 2 illus-
trates the transformation for a hyperedge with ar-
ity 3. The graph transformation is isomorphic.
The rules associated with every hyperedge spec-
ify how line segments in the envelopes of a hyper-
edge?s tail nodes can be combined. Suppose we
have a hyperedge e with rule re : X ? aX1bX2c
and T (e) = {v1, v2}. Then we substitute X1 and
X2 in the rule with candidate translations associ-
ated with line segments in envelopes Env(v1) and
Env(v2) respectively.
To derive the algorithm, we consider the gen-
eral case of a hyperedge e with rule re : X ?
w1X1w2...wnXnwn+1. Because the right-hand
side of re has n nonterminals, the arity of e is
|e| = n. Let T (e) = {v1, ..., vn} denote the
tail nodes of e. We now assume that each tail
node vi ? T (e) is associated with the upper en-
velope over all candidate translations that are in-
duced by derivations of the corresponding nonter-
minal symbol Xi. These envelopes shall be de-
Algorithm 2 ?-operation (Max)
input: array L[0..K-1] containing line objects
output: upper envelope of L
Sort(L:m);
j = 0; K = size(L);
for (i = 0; i < K; ++i) {
` = L[i];
`.x = -?;
if (0 < j) {
if (L[j-1].m == `.m) {
if (`.y <= L[j-1].y) continue;
--j;
}
while (0 < j) {
`.x = (`.y - L[j-1].y)/
(L[j-1].m - `.m);
if (L[j-1].x < `.x) break;
--j;
}
if (0 == j) `.x = -?;
L[j++] = `;
} else L[j++] = `;
}
L.resize(j);
return L;
noted by Env(vi). To decompose the problem of
computing and propagating the tail envelopes over
the hyperedge e to its head node, we now define
two operations, one for either node type, to spec-
ify how envelopes associated with the tail vertices
are propagated to the head vertex.
Nodes of Type ???: For a type ? node, the
resulting envelope is the Minkowski sum over
the envelopes of the incoming edges (Berg et
al., 2008). Since the envelopes of the incoming
edges are convex hulls, the Minkowski sum pro-
vides an upper bound to the number of line seg-
ments that constitute the resulting envelope: the
bound is the sum over the number of line seg-
ments in the envelopes of the incoming edges, i.e.:?
?Env(v?(e))
?
? ?
?
v??T (e)
?
?Env(v?)
?
?.
Algorithm 1 shows the pseudo code for comput-
ing the Minkowski sum over multiple envelopes.
The line objects ` used in this algorithm are
encoded as 4-tuples, each consisting of the x-
intercept with `?s left-adjacent line stored as `.x,
the slope `.m, the y-intercept `.y, and the (partial)
derivation tree `.D. At the beginning, the leftmost
line segment of each envelope is inserted into a
priority queue pq. The priority is defined in terms
of a line?s x-intercept such that lower values imply
higher priority. Hence, the priority queue enumer-
ates all line segments from left to right in ascend-
ing order of their x-intercepts, which is the order
needed to compute the Minkowski sum.
Nodes of Type ???: The operation performed
165
=?
= max
Figure 2: Transformation of a hypergraph into
a factor graph and bottom-up propagation of en-
velopes.
at nodes of type ??? computes the convex hull
over the union of the envelopes propagated over
the incoming edges. This operation is a ?max?
operation and it is identical to the algorithm de-
scribed in (Macherey et al, 2008) for phrase lat-
tices. Algorithm 2 contains the pseudo code.
The complete algorithm then works as follows:
Traversing all nodes in H bottom-up in topolog-
ical order, we proceed for each node v ? V over
its incoming hyperedges and combine in each such
hyperedge e the envelopes associated with the tail
nodes T (e) by computing their sum according to
Algorithm 1 (?-operation). For each incoming
hyperedge e, the resulting envelope is then ex-
panded by applying the rule re to its constituent
line segments. The envelopes associated with dif-
ferent incoming hyperedges of node v are then
combined and reduced according to Algorithm 2
(?-operation). By construction, the envelope at
the root node is the convex hull over the line seg-
ments of all candidate translations that can be de-
rived from the hypergraph.
The suggested algorithm has similar properties
as the algorithm presented in (Macherey et al,
2008). In particular, it has the same upper bound
on the number of line segments that constitute the
envelope at the root node, i.e, the size of this enve-
lope is guaranteed to be no larger than the number
of edges in the transformed hypergraph.
4 Minimum Bayes-Risk Decoding
We first review Minimum Bayes-Risk (MBR) de-
coding for statistical MT. An MBR decoder seeks
the hypothesis with the least expected loss under a
probability model (Bickel and Doksum, 1977). If
we think of statistical MT as a classifier that maps
a source sentence F to a target sentence E, the
MBR decoder can be expressed as follows:
E? = argmin
E??G
?
E?G
L(E,E?)P (E|F ), (1)
where L(E,E?) is the loss between any two hy-
potheses E and E?, P (E|F ) is the probability
model, and G is the space of translations (N -best
list, lattice, or a hypergraph).
MBR decoding for translation can be performed
by reranking an N -best list of hypotheses gener-
ated by an MT system (Kumar and Byrne, 2004).
This reranking can be done for any sentence-
level loss function such as BLEU (Papineni et al,
2001), Word Error Rate, or Position-independent
Error Rate.
Recently, Tromble et al (2008) extended
MBR decoding to translation lattices under an
approximate BLEU score. They approximated
log(BLEU) score by a linear function of n-gram
matches and candidate length. If E and E? are the
reference and the candidate translations respec-
tively, this linear function is given by:
G(E,E?) = ?0|E
?|+
?
w
?|w|#w(E
?)?w(E), (2)
where w is an n-gram present in either E or E?,
and ?0, ?1, ..., ?N are weights which are deter-
mined empirically, where N is the maximum n-
gram order.
Under such a linear decomposition, the MBR
decoder (Equation 1) can be written as
E? = argmax
E??G
?0|E
?|+
?
w
?|w|#w(E
?)p(w|G), (3)
where the posterior probability of an n-gram in the
lattice is given by
p(w|G) =
?
E?G
1w(E)P (E|F ). (4)
Tromble et al (2008) implement the MBR
decoder using Weighted Finite State Automata
(WFSA) operations. First, the set of n-grams
is extracted from the lattice. Next, the posterior
probability of each n-gram is computed. A new
automaton is then created by intersecting each n-
gram with weight (from Equation 2) to an un-
weighted lattice. Finally, the MBR hypothesis is
extracted as the best path in the automaton. We
will refer to this procedure as FSAMBR.
The above steps are carried out one n-gram at
a time. For a moderately large lattice, there can
be several thousands of n-grams and the proce-
dure becomes expensive. We now present an alter-
nate approximate procedure which can avoid this
166
enumeration making the resulting algorithm much
faster than FSAMBR.
4.1 Efficient MBR for lattices
The key idea behind this new algorithm is to
rewrite the n-gram posterior probability (Equa-
tion 4) as follows:
p(w|G) =
?
E?G
?
e?E
f(e, w,E)P (E|F ) (5)
where f(e, w,E) is a score assigned to edge e on
path E containing n-gram w:
f(e, w,E) =
?
?
?
1 w ? e, p(e|G) > p(e?|G),
e? precedes e on E
0 otherwise
(6)
In other words, for each pathE, we count the edge
that contributes n-gramw and has the highest edge
posterior probability relative to its predecessors on
the path E; there is exactly one such edge on each
lattice path E.
We note that f(e, w,E) relies on the full path
E which means that it cannot be computed based
on local statistics. We therefore approximate the
quantity f(e, w,E) with f?(e, w,G) that counts
the edge e with n-gram w that has the highest arc
posterior probability relative to predecessors in the
entire lattice G. f?(e, w,G) can be computed lo-
cally, and the n-gram posterior probability based
on f? can be determined as follows:
p(w|G) =
X
E?G
X
e?E
f?(e, w,G)P (E|F ) (7)
=
X
e?E
1w?ef
?(e, w,G)
X
E?G
1E(e)P (E|F )
=
X
e?E
1w?ef
?(e, w,G)P (e|G),
where P (e|G) is the posterior probability of a lat-
tice edge. The algorithm to perform Lattice MBR
is given in Algorithm 3. For each node t in the lat-
tice, we maintain a quantity Score(w, t) for each
n-gram w that lies on a path from the source node
to t. Score(w, t) is the highest posterior probabil-
ity among all edges on the paths that terminate on t
and contain n-gram w. The forward pass requires
computing the n-grams introduced by each edge;
to do this, we propagate n-grams (up to maximum
order ?1) terminating on each node.
4.2 Extension to Hypergraphs
We next extend the Lattice MBR decoding algo-
rithm (Algorithm 3) to rescore hypergraphs pro-
duced by a SCFG based MT system. Algorithm 4
is an extension to the MBR decoder on lattices
Algorithm 3 MBR Decoding on Lattices
1: Sort the lattice nodes topologically.
2: Compute backward probabilities of each node.
3: Compute posterior prob. of each n-gram:
4: for each edge e do
5: Compute edge posterior probability P (e|G).
6: Compute n-gram posterior probs. P (w|G):
7: for each n-gram w introduced by e do
8: Propagate n? 1 gram suffix to he.
9: if p(e|G) > Score(w, T (e)) then
10: Update posterior probs. and scores:
p(w|G) += p(e|G) ? Score(w, T (e)).
Score(w, he) = p(e|G).
11: else
12: Score(w, he) = Score(w, T (e)).
13: end if
14: end for
15: end for
16: Assign scores to edges (given by Equation 3).
17: Find best path in the lattice (Equation 3).
(Algorithm 3). However, there are important dif-
ferences when computing the n-gram posterior
probabilities (Step 3). In this inside pass, we now
maintain both n-gram prefixes and suffixes (up to
the maximum order?1) on each hypergraph node.
This is necessary because unlike a lattice, new n-
grams may be created at subsequent nodes by con-
catenating words both to the left and the right side
of the n-gram. When the arity of the edge is 2,
a rule has the general form aX1bX2c, where X1
and X2 are sequences from tail nodes. As a result,
we need to consider all new sequences which can
be created by the cross-product of the n-grams on
the two tail nodes. E.g. if X1 = {c, cd, d} and
X2 = {f, g}, then a total of six sequences will
result. In practice, such a cross-product is not pro-
Algorithm 4 MBR Decoding on Hypergraphs
1: Sort the hypergraph nodes topologically.
2: Compute inside probabilities of each node.
3: Compute posterior prob. of each hyperedge P (e|G).
4: Compute posterior prob. of each n-gram:
5: for each hyperedge e do
6: Merge the n-grams on the tail nodes T (e). If the
same n-gram is present on multiple tail nodes, keep
the highest score.
7: Apply the rule on e to the n-grams on T (e).
8: Propagate n? 1 gram prefixes/suffixes to he.
9: for each n-gram w introduced by this hyperedge do
10: if p(e|G) > Score(w, T (e)) then
11: p(w|G) += p(e|G) ? Score(w, T (e))
Score(w, he) = p(e|G)
12: else
13: Score(w, he) = Score(w, T (e))
14: end if
15: end for
16: end for
17: Assign scores to hyperedges (Equation 3).
18: Find best path in the hypergraph (Equation 3).
167
hibitive when the maximum n-gram order in MBR
does not exceed the order of the n-gram language
model used in creating the hypergraph. In the lat-
ter case, we will have a small set of unique prefixes
and suffixes on the tail nodes.
5 MERT for MBR Parameter
Optimization
Lattice MBR Decoding (Equation 3) assumes a
linear form for the gain function (Equation 2).
This linear function contains n + 1 parameters
?0, ?1, ..., ?N , where N is the maximum order of
the n-grams involved. Tromble et al (2008) ob-
tained these factors as a function of n-gram preci-
sions derived from multiple training runs. How-
ever, this does not guarantee that the resulting
linear score (Equation 2) is close to the corpus
BLEU. We now describe how MERT can be used
to estimate these factors to achieve a better ap-
proximation to the corpus BLEU.
We recall that MERT selects weights in a lin-
ear model to optimize an error criterion (e.g. cor-
pus BLEU) on a training set. The lattice MBR
decoder (Equation 3) can be written as a lin-
ear model: E? = argmaxE??G
?N
i=0 ?igi(E
?, F ),
where g0(E?, F ) = |E?| and gi(E?, F ) =?
w:|w|=i #w(E
?)p(w|G).
The linear approximation to BLEU may not
hold in practice for unseen test sets or language-
pairs. Therefore, we would like to allow the de-
coder to backoff to the MAP translation in such
cases. To do that, we introduce an additional fea-
ture function gN+1(E,F ) equal to the original de-
coder cost for this sentence. A weight assignment
of 1.0 for this feature function and zeros for the
other feature functions would imply that the MAP
translation is chosen. We now have a total ofN+2
feature functions which we optimize using MERT
to obtain highest BLEU score on a training set.
6 Experiments
We now describe our experiments to evaluate
MERT and MBR on lattices and hypergraphs, and
show how MERT can be used to tune MBR pa-
rameters.
6.1 Translation Tasks
We report results on two tasks. The first one is
the constrained data track of the NIST Arabic-
to-English (aren) and Chinese-to-English (zhen)
translation task1. On this task, the parallel and the
1http://www.nist.gov/speech/tests/mt
Dataset # of sentences
aren zhen
dev 1797 1664
nist02 1043 878
nist03 663 919
Table 1: Statistics over the NIST dev/test sets.
monolingual data included all the allowed train-
ing sets for the constrained track. Table 1 reports
statistics computed over these data sets. Our de-
velopment set (dev) consists of the NIST 2005 eval
set; we use this set for optimizing MBR parame-
ters. We report results on NIST 2002 and NIST
2003 evaluation sets.
The second task consists of systems for 39
language-pairs with English as the target language
and trained on at most 300M word tokens mined
from the web and other published sources. The de-
velopment and test sets for this task are randomly
selected sentences from the web, and contain 5000
and 1000 sentences respectively.
6.2 MT System Description
Our phrase-based statistical MT system is simi-
lar to the alignment template system described in
(Och and Ney, 2004; Tromble et al, 2008). Trans-
lation is performed using a standard dynamic pro-
gramming beam-search decoder (Och and Ney,
2004) using two decoding passes. The first de-
coder pass generates either a lattice or an N -best
list. MBR decoding is performed in the second
pass.
We also train two SCFG-based MT systems:
a hierarchical phrase-based SMT (Chiang, 2007)
system and a syntax augmented machine transla-
tion (SAMT) system using the approach described
in Zollmann and Venugopal (2006). Both systems
are built on top of our phrase-based systems. In
these systems, the decoder generates an initial hy-
pergraph or anN -best list, which are then rescored
using MBR decoding.
6.3 MERT Results
Table 2 shows runtime experiments for the hyper-
graph MERT implementation in comparison with
the phrase-lattice implementation on both the aren
and the zhen system. The first two columns show
the average amount of time in msecs that either
algorithm requires to compute the upper envelope
when applied to phrase lattices. Compared to the
algorithm described in (Macherey et al, 2008)
which is optimized for phrase lattices, the hyper-
graph implementation causes a small increase in
168
Avg. Runtime/sent [msec]
(Macherey 2008) Suggested Alg.
aren zhen aren zhen
phrase lattice 8.57 7.91 10.30 8.65
hypergraph ? ? 8.19 8.11
Table 2: Average time for computing envelopes.
running time. This increase is mainly due to the
representation of line segments; while the phrase-
lattice implementation stores a single backpointer,
the hypergraph version stores a vector of back-
pointers.
The last two columns show the average amount
of time that is required to compute the upper en-
velope on hypergraphs. For comparison, we prune
hypergraphs to the same density (# of edges per
edge on the best path) and achieve identical run-
ning times for computing the error surface.
6.4 MBR Results
We first compare the new lattice MBR (Algo-
rithm 3) with MBR decoding on 1000-best lists
and FSAMBR (Tromble et al, 2008) on lattices
generated by the phrase-based systems; evaluation
is done using both BLEU and average run-time per
sentence (Table 3). Note that N -best MBR uses
a sentence BLEU loss function. The new lattice
MBR algorithm gives about the same performance
as FSAMBR while yielding a 20X speedup.
We next report the performance of MBR on hy-
pergraphs generated by Hiero/SAMT systems. Ta-
ble 4 compares Hypergraph MBR (HGMBR) with
MAP and MBR decoding on 1000 best lists. On
some systems such as the Arabic-English SAMT,
the gains from Hypergraph MBR over 1000-best
MBR are significant. In other cases, Hypergraph
MBR performs at least as well as N -best MBR.
In all cases, we observe a 7X speedup in run-
time. This shows the usefulness of Hypergraph
MBR decoding as an efficient alternative to N -
best MBR.
6.5 MBR Parameter Tuning with MERT
We now describe the results by tuning MBR n-
gram parameters (Equation 2) using MERT. We
first compute N + 1 MBR feature functions on
each edge of the lattice/hypergraph. We also in-
clude the total decoder cost on the edge as as addi-
tional feature function. MERT is then performed
to optimize the BLEU score on a development set;
For MERT, we use 40 random initial parameters as
well as parameters computed using corpus based
statistics (Tromble et al, 2008).
BLEU (%) Avg.
aren zhen time
nist03 nist02 nist03 nist02 (ms.)
MAP 54.2 64.2 40.1 39.0 -
N -best MBR 54.3 64.5 40.2 39.2 3.7
Lattice MBR
FSAMBR 54.9 65.2 40.6 39.5 3.7
LatMBR 54.8 65.2 40.7 39.4 0.2
Table 3: Lattice MBR for a phrase-based system.
BLEU (%) Avg.
aren zhen time
nist03 nist02 nist03 nist02 (ms.)
Hiero
MAP 52.8 62.9 41.0 39.8 -
N -best MBR 53.2 63.0 41.0 40.1 3.7
HGMBR 53.3 63.1 41.0 40.2 0.5
SAMT
MAP 53.4 63.9 41.3 40.3 -
N -best MBR 53.8 64.3 41.7 41.1 3.7
HGMBR 54.0 64.6 41.8 41.1 0.5
Table 4: Hypergraph MBR for Hiero/SAMT systems.
Table 5 shows results for NIST systems. We
report results on nist03 set and present three sys-
tems for each language pair: phrase-based (pb),
hierarchical (hier), and SAMT; Lattice MBR is
done for the phrase-based system while HGMBR
is used for the other two. We select the MBR
scaling factor (Tromble et al, 2008) based on the
development set; it is set to 0.1, 0.01, 0.5, 0.2, 0.5
and 1.0 for the aren-phrase, aren-hier, aren-samt,
zhen-phrase zhen-hier and zhen-samt systems re-
spectively. For the multi-language case, we train
phrase-based systems and perform lattice MBR
for all language pairs. We use a scaling factor of
0.7 for all pairs. Additional gains can be obtained
by tuning this factor; however, we do not explore
that dimension in this paper. In all cases, we prune
the lattices/hypergraphs to a density of 30 using
forward-backward pruning (Sixtus and Ortmanns,
1999).
We consider a BLEU score difference to be a)
gain if is at least 0.2 points, b) drop if it is at most
-0.2 points, and c) no change otherwise. The re-
sults are shown in Table 6. In both tables, the fol-
lowing results are reported: Lattice/HGMBR with
default parameters (?5, 1.5, 2, 3, 4) computed us-
ing corpus statistics (Tromble et al, 2008),
Lattice/HGMBR with parameters derived from
MERT both without/with the baseline model cost
feature (mert?b, mert+b). For multi-language
systems, we only show the # of language-pairs
with gains/no-changes/drops for each MBR vari-
ant with respect to the MAP translation.
169
We observed in the NIST systems that MERT
resulted in short translations relative to MAP on
the unseen test set. To prevent this behavior,
we modify the MERT error criterion to include
a sentence-level brevity scorer with parameter ?:
BLEU+brevity(?). This brevity scorer penalizes
each candidate translation that is shorter than the
average length over its reference translations, us-
ing a penalty term which is linear in the difference
between either length. We tune ? on the develop-
ment set so that the brevity score of MBR transla-
tion is close to that of the MAP translation.
In the NIST systems, MERT yields small im-
provements on top of MBR with default param-
eters. This is the case for Arabic-English Hi-
ero/SAMT. In all other cases, we see no change
or even a slight degradation due to MERT.
We hypothesize that the default MBR parame-
ters (Tromble et al, 2008) are well tuned. There-
fore there is little gain by additional tuning using
MERT.
In the multi-language systems, the results show
a different trend. We observe that MBR with de-
fault parameters results in gains on 18 pairs, no
differences on 9 pairs, and losses on 12 pairs.
When we optimize MBR features with MERT, the
number of language pairs with gains/no changes/-
drops is 22/5/12. Thus, MERT has a bigger impact
here than in the NIST systems. We hypothesize
that the default MBR parameters are sub-optimal
for some language pairs and that MERT helps to
find better parameter settings. In particular, MERT
avoids the need for manually tuning these param-
eters by language pair.
Finally, when baseline model costs are added
as an extra feature (mert+b), the number of pairs
with gains/no changes/drops is 26/8/5. This shows
that this feature can allow MBR decoding to back-
off to the MAP translation. When MBR does not
produce a higher BLEU score relative to MAP
on the development set, MERT assigns a higher
weight to this feature function. We see such an
effect for 4 systems.
7 Discussion
We have presented efficient algorithms
which extend previous work on lattice-based
MERT (Macherey et al, 2008) and MBR de-
coding (Tromble et al, 2008) to work with
hypergraphs. Our new MERT algorithm can work
with both lattices and hypergraphs. On lattices, it
achieves similar run-times as the implementation
System BLEU (%)
MAP MBR
default mert-b mert+b
aren.pb 54.2 54.8 54.8 54.9
aren.hier 52.8 53.3 53.5 53.7
aren.samt 53.4 54.0 54.4 54.0
zhen.pb 40.1 40.7 40.7 40.9
zhen.hier 41.0 41.0 41.0 41.0
zhen.samt 41.3 41.8 41.6 41.7
Table 5: MBR Parameter Tuning on NIST systems
MBR wrt. MAP default mert-b mert+b
# of gains 18 22 26
# of no-changes 9 5 8
# of drops 12 12 5
Table 6: MBR on Multi-language systems.
described in Macherey et al (2008). The new
Lattice MBR decoder achieves a 20X speedup
relative to either FSAMBR implementation
described in Tromble et al (2008) or MBR on
1000-best lists. The algorithm gives comparable
results relative to FSAMBR. On hypergraphs
produced by Hierarchical and Syntax Augmented
MT systems, our MBR algorithm gives a 7X
speedup relative to 1000-best MBR while giving
comparable or even better performance.
Lattice MBR decoding is obtained under a lin-
ear approximation to BLEU, where the weights
are obtained using n-gram precisions derived from
development data. This may not be optimal in
practice for unseen test sets and language pairs,
and the resulting linear loss may be quite differ-
ent from the corpus level BLEU. In this paper, we
have described how MERT can be employed to
estimate the weights for the linear loss function
to maximize BLEU on a development set. On an
experiment with 40 language pairs, we obtain im-
provements on 26 pairs, no difference on 8 pairs
and drops on 5 pairs. This was achieved with-
out any need for manual tuning for each language
pair. The baseline model cost feature helps the al-
gorithm effectively back off to the MAP transla-
tion in language pairs where MBR features alone
would not have helped.
MERT and MBR decoding are popular tech-
niques for incorporating the final evaluation met-
ric into the development of SMT systems. We be-
lieve that our efficient algorithms will make them
more widely applicable in both SCFG-based and
phrase-based MT systems.
170
References
M. Berg, O. Cheong, M. Krefeld, and M. Overmars,
2008. Computational Geometry: Algorithms and
Applications, chapter 13, pages 290?296. Springer-
Verlag, 3rd edition.
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected topics. Holden-
Day Inc., Oakland, CA, USA.
D. Chiang. 2007. Hierarchical phrase based transla-
tion . Computational Linguistics, 33(2):201 ? 228.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable Inference
and Training of Context-Rich Syntactic Translation
Models. . In COLING/ACL, Sydney, Australia.
L. Huang. 2008. Advanced Dynamic Programming
in Semiring and Hypergraph Frameworks. In COL-
ING, Manchester, UK.
S. Kumar and W. Byrne. 2004. Minimum Bayes-
Risk Decoding for Statistical Machine Translation.
In HLT-NAACL, Boston, MA, USA.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008. Lattice-based Minimum Error Rate Train-
ing for Statistical Machine Translation. In EMNLP,
Honolulu, Hawaii, USA.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-Based
Translation. In ACL, Columbus, OH, USA.
F. Och and H. Ney. 2004. The Alignment Template
Approach to Statistical Machine Translation. Com-
putational Linguistics, 30(4):417 ? 449.
F. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In ACL, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Ma-
chine Translation. Technical Report RC22176
(W0109-022), IBM Research Division.
A. Sixtus and S. Ortmanns. 1999. High Quality
Word Graphs Using Forward-Backward Pruning. In
ICASSP, Phoenix, AZ, USA.
R. Tromble, S. Kumar, F. Och, andW.Macherey. 2008.
Lattice Minimum Bayes-Risk Decoding for Statis-
tical Machine Translation. In EMNLP, Honolulu,
Hawaii.
H. Zhang and D. Gildea. 2008. Efficient Multi-pass
Decoding for Synchronous Context Free Grammars.
In ACL, Columbus, OH, USA.
A. Zollmann and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In
HLT-NAACL, New York, NY, USA.
171
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 158?166,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
?Poetic? Statistical Machine Translation: Rhyme and Meter
Dmitriy Genzel Jakob Uszkoreit Franz Och
Google, Inc.
1600 Amphitheatre Pkwy
Mountain View, CA 94043, USA
{dmitriy,uszkoreit,och}@google.com
Abstract
As a prerequisite to translation of poetry, we
implement the ability to produce translations
with meter and rhyme for phrase-based MT,
examine whether the hypothesis space of such
a system is flexible enough to accomodate
such constraints, and investigate the impact of
such constraints on translation quality.
1 Introduction
Machine translation of poetry is probably one of the
hardest possible tasks that can be considered in com-
putational linguistics, MT, or even AI in general. It
is a task that most humans are not truly capable of.
Robert Frost is reported to have said that poetry is
that which gets lost in translation. Not surprisingly,
given the task?s difficulty, we are not aware of any
work in the field that attempts to solve this problem,
or even discuss it, except to mention its difficulty,
and professional translators like to cite it as an exam-
ple of an area where MT will never replace a human
translator. This may well be true in the near or even
long term. However, there are aspects of the prob-
lem that we can already tackle, namely the problem
of the poetic form.
Vladimir Nabokov, in his famous translation of
Eugene Onegin (Nabokov, 1965), a poem with a
very strict meter and rhyming scheme, heavily dis-
parages those translators that attempt to preserve the
form, claiming that since it is impossible to perfectly
preserve both the form and the meaning, the form
must be entirely sacrificed. On the other hand, Dou-
glas Hofstadter, who spends 600 pages describing
how to translate a 60 word poem in 80 different ways
in Le Ton beau de Marot (1998), makes a strong case
that a poem?s form must be preserved in translation,
if at all possible. Leaving the controversy to the pro-
fessional translators, we investigate whether or not
it is possible to produce translations that conform to
certain metrical constraints common in poetry.
Statistical machine translation techniques, unlike
their traditional rule-based counterparts, are in fact
well-suited to the task. Because the number of po-
tential translation hypotheses is very large, it is not
unreasonable to expect that some of them should
conform to an externally imposed standard. The
goal of this paper is to investigate how these hy-
potheses can be efficiently identified, how often they
are present, and what the quality penalty for impos-
ing them is.
2 Related Work
There has been very little work related to the transla-
tion of poetry. There has been some work where MT
techniques were used to produce poetry (Jiang and
Zhou, 2008). In other computational poetry work,
Ramakrishnan et al(2009) generate song lyrics from
melody and various algorithms for poetry gener-
ation (Manurung et al, 2000; D??az-Agudo et al,
2002). There are books (Hartman, 1996) and arti-
cles (Bootz, 1996) on the subject of computer poetry
from a literary point of view. Finally, we must men-
tion Donald Knuth?s seminal work on complexity of
songs (Knuth, 1984).
158
3 Statistical MT and Poetry
We can treat any poetic form as a constraint on the
potential outputs. A naive approach to ensure that an
output of the MT system is, say, a haiku, is to create
a haiku detector and to examine a (very large) n-best
list of translations. This approach would not suc-
ceed very often, since the haikus that may be among
the possible translations are a very small fraction of
all translations, and the MT decoder is not actively
looking for them, since it is not part of the cost it
attempts to minimize. Instead, we would want to re-
cast ?Haikuness? as a feature function, such that a
real haiku has 0 cost, and those outputs that are not,
have large cost. This feature function must be local,
rather than global, so as to guide the decoder search.
The concept of feature functions as used in sta-
tistical MT is described by Och and Ney (Och and
Ney, 2002). For a phrase based system, a feature
function is a function whose inputs are a partial hy-
pothesis state sin, and a phrase pair p, and whose
outputs are the hypothesis state after p is appended
to the hypothesis: sout, and the cost incurred, c. For
hierarchical, tree-to-string and some other types of
MT systems which combine two partial hypotheses
and are not generating translations left-to-right, one
instead has two partial hypotheses states sleft and
sright as inputs, and the outputs are the same. Our
first goal is to describe how these functions can be
efficiently implemented.
The feature function costs are multiplied by fixed
weights and added together to obtain the total hy-
pothesis cost. Normally feature functions include
the logarithm of probability of target phrase given
source, source given target and other phrase-local
features which require no state to be kept, as well
as features like language model, which require non-
trivial state. The weights are usually learned auto-
matically, however we will set them manually for
our feature functions to be effectively infinite, since
we want them to override all other sources of infor-
mation.
We will now examine some different kinds of po-
etry and consider the properties of such feature func-
tions, especially with regard to keeping necessary
state. We are concerned with minimizing the amount
of information to be kept, both due to memory re-
quirements, and especially to ensure that compati-
ble hypotheses can be efficiently recombined by the
decoder.
3.1 Line-length constrained poetry
Some poetic genres, like the above-mentioned
haiku, require that a poem contain a certain num-
ber of lines (3 for haiku), each containing a certain
number of syllables (5,7,5 for haiku). These gen-
res include lanternes, fibs, tankas, and many others.
These genres impose two constraints. The first con-
straint is on total length. This requires that each hy-
pothesis state contain the current translation length
(in syllables). In addition, whenever a hypothesis is
expanded, we must keep track of whether or not it
would be possible to achieve the desired final length
with such an expansion. For example, if in the ini-
tial state, we have a choice between two phrases, and
picking the longer of the two would make it impos-
sible to have a 17-syllable translation later on, we
must impose a high cost on it, so as to avoid going
down a garden path.
The second constraint is on placing line breaks:
they must come at word boundaries. Therefore the
5th and 12th (and obviously 17th) syllable must end
words. This also requires knowing the current hy-
pothesis? syllable length, but unlike the first con-
straint, it can be scored entirely locally, without con-
sidering possible future expansions. For either con-
straint, however, the sentence has to be assembled
strictly left-to-right, which makes it impossible to
build partial hypotheses that do not start the sen-
tence, which hierarchical and tree-to-string decoders
require.
3.2 Rhythmic poetry
Some famous Western poetry, notably Shakespeare,
is written in rhythmic poetry, also known as blank
verse. This poetry imposes a constraint on the pat-
tern of stressed and unstressed syllables. For exam-
ple, if we use 0 to indicate no stress, and 1 to indicate
stress, blank verse with iambic foot obeys the regu-
lar expression (01)?, while one with a dactylic foot
looks like (100)?. This genre is the easiest to han-
dle, because it does not require current position, but
only its value modulo foot length (e.g. for an iamb,
whether the offset is even or odd). It is even possi-
ble, as described in Section 4, to track this form in a
decoder that is not left-to-right.
159
3.3 Rhythmic and rhyming poetry
The majority of English poetry that was written un-
til recently has both rhythm and rhyme. Generally
speaking, a poetic genre of this form can be de-
scribed by two properties. The first is a rhyming
scheme. A rhyming scheme is a string of letters,
each corresponding to a line of a poem, such that
the same letter is used for the lines that rhyme.
E.g. aa is a scheme for a couplet, a 2-line poem
whose lines rhyme. A sonnet might have a com-
plicated scheme like abbaabbacdecde. The second
property concerns meter. Usually lines that rhyme
have the same meter (i.e. the exact sequence of
stressed and unstressed syllables). For example, an
iambic pentameter is an iamb repeated 5 times, i.e.
0101010101. We can describe a genre completely
by its rhyming scheme and a meter for each letter
used in the rhyming scheme. We will refer to this ob-
ject as genre description. E.g. {abab, a : 010101, b :
10101010} is a quatrain with trimeter iambic and
tetrameter trochaic lines. Note that the other two
kinds of poetry can also be fit by this structure, if
one permits another symbol (we use *) to stand for
the syllables whose stress is not important, e.g. a
haiku: {abc, a : ?????, b : ???????, c : ?????}.
For this type of genre, we need to obey the same two
constraints as in the line-based poetry, but also to en-
sure that rhyming constraints hold. This requires us
to include in a state, for any outstanding rhyme let-
ter, the word that occurred at the end of that line. It
is not sufficient to include just the syllable that must
rhyme, because we wish to avoid self-rhymes (word
rhyming with an identical word).
4 Stress pattern feature function
We will first discuss an easier special case, namely
a feature function for blank verse, which we will re-
fer to as stress pattern feature function. This feature
function can be used for both phrase-based and hier-
archical systems.
In addition to a statistical MT system (Och and
Ney, 2004; Koehn et al, 2007), it is necessary to
have the means to count the syllables in a word and
to find out which ones are stressed. This can be done
with a pronunciation module of a text-to-speech
system, or a freely available pronunciation dictio-
nary, such as CMUDict (Rudnicky, 2010). Out-of-
vocabulary words can be treated as always imposing
a high cost.
4.1 Stress pattern for a phrase-based system
In a phrase based system, the feature function state
consists of the current hypothesis length modulo
foot length. For a 2-syllable foot, it is either 0 or
1, for a 3-syllable foot, 0, 1, or 2. The proposed
target phrase is converted into a stress pattern using
the pronunciation module, and the desired stress pat-
tern is left shifted by the current offset. The cost is
the number of mismatches of the target phrase vs.
the pattern. For example, if the desired pattern is
010, current offset is 1, and the proposed new phrase
has pattern 10011, we shift the desired pattern by 1,
obtaining 100 and extend it to length 5, obtaining
10010, matching it against the proposal. There is
one mismatch, at the fifth position, and we report a
cost of 1. The new state is simply the old state plus
phrase length, modulo foot length, 0 in this example.
4.2 Stress pattern for a hierarchical system
In a hierarchical system, we in general do not know
how a partial hypothesis might be combined on the
left. A hypothesis that is a perfect fit for pattern 010
would be horrible if it is placed at an offset that is
not divisible by 3, and vice versa, an apparently bad
hypothesis might be perfectly good if placed at such
an offset. To solve this problem, we create states
that track how well a partial hypothesis fits not only
the desired pattern, but all patterns obtained by plac-
ing this pattern at any offset, and also the hypothesis
length (modulo foot length, as usual). For instance,
if we observe a pattern 10101, we record the fol-
lowing state: {length: 1, 01 cost: 5, 10 cost: 0}.
If we now combine this state with another, such as
{length: 0, 01 cost: 1, 10 cost: 0}, we simply add
the lengths, and combine the costs either of the same
kind (if left state?s length is even), or the opposite (if
it is odd). In this instance we get {length: 1, 01
cost: 5, 10 cost: 1}. If both costs are greater than 0,
we can subtract the minimum cost and immediately
output it as cost: this is the unavoidable cost of this
combination. For this example we get cost of 1, and
a new state: {length: 1, 01 cost: 4, 10 cost: 0}. For
the final state, we output the remaining cost for the
pattern we desire. The approach is very similar for
feet of length 3.
160
4.3 Stress pattern: Whatever fits
With a trivial modification we can output transla-
tions that can fit any one of the patterns, as long
as we do not care which. The approach is identical
for both hierarchical and phrase-based systems. We
simply track all foot patterns (length 2 and length
3 are the only ones used in poetry) as in the above
algorithm, taking care to combine the right pattern
scores based on length offset. The length offset now
has to be tracked modulo 2*3.
This feature function can now be used to trans-
late arbitrary text into blank verse, picking whatever
meter fits best. If no meters can fit completely, it
will produce translations with the fewest violations
(assuming the weight for this feature function is set
high).
5 General poetic form feature function
In this section we discuss a framework for track-
ing any poetic genre, specified as a genre descrip-
tion object (Section 3.3 above). As in the case of
the stress pattern function, we use a statistical MT
system, which is now required to be phrase-based
only. We also use a pronunciation dictionary, but
in addition to tracking the number and stress of syl-
lables, we must now be able to provide a function
that classifies a pair of words as rhyming or non-
rhyming. This is in itself a non-trivial task (Byrd
and Chodorow, 1985), due to lack of a clear defini-
tion of what constitutes a rhyme. In fact rhyming is
a continuum, from very strong rhymes to weak ones.
We use a very weak definition which is limited to a
single syllable: if the final syllables of both words
have the same nucleus and coda1, we say that the
words rhyme. We accept this weak definition be-
cause we prefer to err on the side of over-generation
and accept even really bad poetry.
5.1 Tracking the target length
The hardest constraint to track efficiently is the
range of lengths of the resulting sentence. Phrase-
based decoders use a limited-width beam as they
build up possible translations. Once a hypothesis
drops out of the beam, it cannot be recovered, since
no backtracking is done. Therefore we cannot afford
1In phonology, nucleus and coda together are in fact called
rhyme or rime
to explore a part of the hypothesis space which has
no possible solutions for our constraints, we must be
able to prune a hypothesis as soon as it leads us to
such a subspace, otherwise we will end up on an un-
recoverable garden path. To avoid this problem, we
need to have a set of possible sentence lengths avail-
able at any point in the search, and to impose a high
cost if the desired length is not in that set.
Computing this set exactly involves a standard dy-
namic programming sweep over the phrase lattice,
including only uncovered source spans. If the maxi-
mum source phrase size is k, source sentence length
is n and maximum target/source length ratio for a
phrase is l (and therefore target sentence is limited
to at most ln words), this sweep requires going over
O(n2) source ranges, each of which can be produced
in k ways, and tracking ln potential lengths in each,
resulting in O(n3kl) algorithm. This is unaccept-
ably slow to be done for each hypothesis (even not-
ing that hypotheses with the same set of already cov-
ered source position can share this computation).
We will therefore solve this task approximately.
First, we can note that in most cases the set of possi-
ble target lengths is a range. This is due to phrase
extraction constraints, which normally ensure that
the lengths of target phrases form a complete range.
This means that it is sufficient to track only a mini-
mum and maximum value for each range, reducing
time to O(n2k). Second, we can note that whenever
a source range is interrupted by a covered phrase and
split into two ranges, the minimal and maximal sen-
tence length is simply the sum of the correspond-
ing lengths over the two uncovered subranges, plus
the current hypothesis length. Therefore, if we pre-
compute the minimum and maximum lengths over
all ranges, using the same dynamic programming al-
gorithm in advance, it is only necessary to iterate
over the uncovered ranges (at most O(n), and O(1)
in practice, due to reordering constraints) at runtime
and sum their minimum and maximum values. As a
result, we only need to do O(n2k) work upfront, and
on average O(1) extra work for each hypothesis.
5.2 State space
A state for the feature function must contain the fol-
lowing elements:
? Current sentence length (in syllables)
161
? Set of uncovered ranges (as needed for the
computation above)
? Zero or more letters from the rhyming scheme
with the associated word that has an outstand-
ing rhyme
5.3 The combination algorithm
To combine the hypothesis state sin with a phrase
pair p, do the following
1. Initialize cost as 0, sout as sin
2. Update sout: increment sentence length by tar-
get phrase length (in syllables), update cover-
age range
3. Compute minimum and maximum achievable
sentence length; if desired length not in range,
increment cost by a penalty
4. For each word in the target phrase
(a) If the word?s syllable pattern does not
match against desired pattern, add number
of mismatches to cost
(b) If at the end of a line:
i. If the line would end mid-word, incre-
ment cost by a penalty
ii. Let x be this line?s rhyme scheme let-
ter
iii. If x is present in the state sout, check
if the word associated with x rhymes
with the current word, if not, incre-
ment cost by a penalty
iv. Remove x with associated word from
the state sout
v. If letter x occurs further in the
rhyming scheme, add x with the cur-
rent word to the state sout
5.4 Tracking multiple patterns
The above algorithm will allow to efficiently search
the hypothesis space for a single genre description
object. In practice, however, there may be several
desirable patterns, any one of which would be ac-
ceptable. A naive approach, to use multiple fea-
ture functions, one with each pattern, does not work,
since the decoder is using a (log-)linear model, in
which costs are additive. As a result, a pattern that
matches one pattern, but not another, will still have
high cost, perhaps as high as a pattern that partially
matches both. We need to combine feature functions
not linearly, but with a min operator. This is easily
achieved by creating a combined state that encodes
the union of each individual function?s states (which
can share most of the information), and in addition
each feature function?s current total cost. As long
as at least one function has zero cost (i.e. can po-
tentially match), no cost is reported to the decoder.
As soon as all costs become positive, the minimum
over all costs is reported to the decoder as unavoid-
able cost, and should be subtracted from each fea-
ture function cost, bringing the minimum stored in
the output state back to 0.
It is also possible to prune the set of functions that
are still viable, based on their cost, to avoid keeping
track of patterns that cannot possibly match. Using
this approach we can translate arbitrary text, provide
a large number of poetic patterns and expect to get
some sort of poem at the end. Given a wide variety
of poetic genres, it is not unreasonable to expect that
for most inputs, some pattern will apply. Of course,
for translating actual poetry, we would likely have a
specific form in mind, and a positive outcome would
be less likely.
6 Results
We train a baseline phrase-based French-English
system using WMT-09 corpora (Callison-Burch et
al., 2009) for training and evaluation. We use a pro-
prietary pronunciation module to provide phonetic
representation of English words.
6.1 Stress Pattern Feature Function
We have no objective means of ?poetic? quality eval-
uation. We are instead interested in two metrics:
percentage of sentences that can be translated while
obeying a stress pattern constraint, and the impact
of this constraint on BLEU score (Papineni et al,
2002). Obviously, WMT test set is not itself in any
way poetic, so we use it merely to see if arbitrary
text can be forced into this constraint.
The BLEU score impact on WMT has been fairly
consistent during our experimentation: the BLEU
score is roughly halved. In particular, for the
above system the baseline score is 35.33, and stress
162
Table 1: Stress pattern distribution
Name Pattern % of matches
Iamb 01 9.6%
Trochee 10 7.2%
Anapest 001 27.1%
Amphibrach 010 32.1%
Dactyl 100 23.8%
pattern-constrained system only obtains 18.93.
The proportion of sentences successfully matched
is 85%, and if we permit a single stress error, it is
93%, which suggests that the constraint can be sat-
isfied in the great majority of cases. The distribution
of stress patterns among the perfect matches is given
in Table 1.
Some of the more interesting example translations
with stress pattern enforcement enabled are given in
table 2.
6.2 Poetic Form Feature Function
For poetic form feature function, we perform the
same evaluation as above, to estimate the impact of
forcing prose into an arbitrary poetic form, but to get
more relevant results we also translate a poetic work
with a specific genre requirement.
Our poetic form feature function is given a list
of some 210 genre descriptions which vary from
Haikus to Shakespearean sonnets. Matching any one
of them satisfies the constraint. We translate WMT
blind set and obtain a BLEU score of 17.28 with the
baseline of 35.33 as above. The proportion of sen-
tences that satisfied one of the poetic constraints is
87%. The distribution of matched genres is given
in Table 3. Some of the more interesting example
translations are given in table 2.
For a proper poetic evaluation, we use a French
translation of Oscar Wilde?s Ballad of Reading Gaol
by Jean Guiloineau as input, and the original Wilde?s
text as reference. The poem consists of 109 stanzas
of 6 lines each, with a genre description of {abcbdb,
a/c/d: 01010101, b: 010101}. The French version
obeys the same constraint. We treat each stanza as a
sentence to be translated. The baseline BLEU score
is 10.27. This baseline score is quite low, as can
be expected for matching a literal MT translation
against a professional poetical translation. We eval-
uate our system with a poetic constraint given above.
Table 3: Genre distribution for WMT corpus.
(Descriptions of these genres can be found in Wikipedia,
http://en.wikipedia.org)
Genre Number Percentage
No poem 809 13.1%
Blank verse 5107 82.7%
Couplet 81 1.3%
Haiku 42 0.7%
Cinquain 33 0.5%
Dodoitsu 24 0.4%
Quinzaine 23 0.4%
Choka 18 0.3%
Fib 15 0.2%
Tanka 14 0.2%
Lanterne 4 0.1%
Triplet 1 0.02%
Quatrain 1 0.02%
Total 6172 100%
The resulting score is 7.28. Out of 109 stanzas, we
found 12 translations that satisfy the genre constraint
(If we allow any poetic form, 108 out of 109 stanzas
match some form). Two sample stanzas that satisfied
the constraints are given in Table 4.
7 Discussion and Future Work
In this work we demonstrate how modern-day sta-
tistical MT system can be constrained to search for
translations obeying particular length, meter, and
rhyming constraints, whether a single constraint, or
any one of a set. We further demonstrate that the hy-
pothesis space is often rich enough that these con-
straints can be satisfied. The impact on translation
quality, however, is quite profound, as is to be ex-
pected. It seems that at the present state of machine
translation, one does indeed have to choose between
getting either the form or the meaning right. In the
present form, however, we can already find good
translations, as a sort of found poetry (Drury, 2006),
by translating a large quantity of text, whether poetic
or not.
This is the first attempt to deal with poetry trans-
lation, and the great majority of work to achieve rea-
sonable quality in form and meaning still remains to
be done. One major problem with the current fea-
ture function is that while it can avoid the areas of
the search space where length constraints cannot be
163
Table 2: Example translations. Stressed syllables are italicized
Reference A police spokesman said three people had been arrested and the
material was being examined.
Baseline A policeman said that three people were arrested and that the ma-
terial is currently being analyzed.
Stress Pattern (001) A police said that three were arrested and that the equipment is
currently being examined.
Poetic: Couplet in amphi-
brachic tetrameter
An of ficer stated that three were arrested
and that the equipment is currently tested.
Reference A trio of retired generals launched a mutiny in the Lords, protest-
ing against cuts in military spending: being armed-forces minister
is, they claimed, a part-time job.
Baseline A trio of retired generals have launched a revolt among the Lords,
protesting against cuts in military spending: they have proclaimed
only Minister of Defence is for them, a part-time employment.
Stress Pattern (010) A trio of general retirement launched a rebellion among Lords,
protesting the spending cuts troops: they claimed Minister only
defense is for them, a part-time job.
Poetic: Blank Verse in amphi-
brachic trimeter
A trio of generals retired
have launched an uprising among Lords,
protesting the spending cuts members:
they minister only proclaimed the
defense is for them, a part-time job.
Reference We must continue to condemn human rights abuses in Burma.
Baseline We must continue to denounce the violations of human rights
abuses in Burma.
Stress Pattern (100) We must continue to speak out against rights abuses committed in
Burma.
Poetic: Haiku: 5-7-5 syllables We must continue
denounce violations of
human rights Burma.
164
Table 4: Sample translations from Oscar Wilde?s Ballad of Reading Gaol.
Wilde?s original Our translation
He did not wring his hands, as do Without hands twisted like these men,
Those witless men who dare Poor men without hope, dare
To try to rear the changeling Hope To nourish hope in our vault
In the cave of black Despair: Of desperation there
He only looked upon the sun, And looked toward the sun, drink cool
And drank the morning air. Until the evening air.
With slouch and swing around the ring We are in our circle we
We trod the Fool?s Parade! Dragged like the Fools? Parade!
We did not care: we knew we were It mattered little, since we were
The Devil?s Own Brigade: The Devil?s sad Brigade:
And shaven head and feet of lead A shaved head and the feet of lead
Make a merry masquerade. Regardless gay charade!
satisfied, it cannot avoid the areas where rhyming
constraints are impossible to satisfy. As a result, we
need to allow a very wide hypothesis beam (5000 per
each source phrase coverage), to ensure that enough
hypotheses are considered, so that there are some
that lead to correct solutions later. We do not cur-
rently have a way to ensure that this happens, al-
though we can attempt to constrain the words that
end lines to have possible rhymes, or employ other
heuristics. A more radical solution is to create an
entirely different decoding algorithm which places
words not left-to-right, or in a hierarchical fashion,
but first placing words that must rhyme, and build-
ing hypotheses around them, like human translators
of poetry do.
As a result, the system at present is too slow, and
we cannot make it available online as a demo, al-
though we may be able to do so in the future.
The current approach relies on having enough lex-
ical variety in the phrase table to satisfy constraints.
Since our goal is not to be literal, but to obtain a
satisfactory compromise between form and mean-
ing, it would clearly be beneficial to augment target
phrases with synonyms and paraphrases, or to allow
for words to be dropped or added.
8 Acknowledgements
We would like to thank all the members of the MT
team at Google, especially Richard Zens and Moshe
Dubiner, for their help. We are thankful to the
anonymous reviewers for their comments, especially
to the one that to our amazement did the entire re-
view in verse2.
References
P. Bootz. 1996. Poetic machinations. Visible Language,
30(2):118?37.
Roy J. Byrd and Martin S. Chodorow. 1985. Using an
on-line dictionary to find rhyming words and pronun-
ciations for unknown words. In Proceedings of the
23rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 277?283, Chicago, Illinois,
USA, July. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
B. D??az-Agudo, P. Gerva?s, and P. Gonza?lez-Calero.
2002. Poetry generation in colibriza. In Advances in
Case-Based Reasoning, pages 157?159. Springer.
John Drury. 2006. The poetry dictionary. Writer?s Di-
gest Books.
C.O. Hartman. 1996. Virtual muse: experiments in com-
puter poetry. Wesleyan University Press.
Douglas R. Hofstadter. 1998. Le Ton Beau De Marot:
In Praise of the Music of Language. Perseus Books
Group.
2With the reviewer?s permission, we feel that the ex-
tra work done by the reviewer deserves to be seen by
more than a few people, and make it available for you to
view at: http://research.google.com/archive/
papers/review_in_verse.html
165
Long Jiang and Ming Zhou. 2008. Generating Chi-
nese couplets using a statistical MT approach. In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 377?
384, Manchester, UK, August. Coling 2008 Organiz-
ing Committee.
D.E. Knuth. 1984. The complexity of songs. Communi-
cations of the ACM, 27(4):344?346.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
H.M. Manurung, G. Ritchie, and H. Thompson. 2000.
Towards a computational model of poetry generation.
In Proceedings of AISB Symposium on Creative and
Cultural Aspects and Applications of AI and Cognitive
Science, pages 79?86. Citeseer.
Vladimir Nabokov. 1965. Eugene Onegin: A Novel in
Verse by Alexandr Pushkin, Translated from the Rus-
sian. Bollingen Foundation.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Ananth Ramakrishnan A., Sankar Kuppan, and Sobha
Lalitha Devi. 2009. Automatic generation of Tamil
lyrics for melodies. In Proceedings of the Workshop
on Computational Approaches to Linguistic Creativity,
pages 40?46, Boulder, Colorado, June. Association for
Computational Linguistics.
Alex Rudnicky. 2010. The Carnegie Mellon pronounc-
ing dictionary, version 0.7a. Online.
166
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183?192,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Training a Parser for Machine Translation Reordering
Jason Katz-Brown Slav Petrov Ryan McDonald Franz Och
David Talbot Hiroshi Ichikawa Masakazu Seno Hideto Kazawa
Google
{jasonkb|slav|ryanmcd|och|talbot|ichikawa|seno|kazawa}@google.com
Abstract
We propose a simple training regime that can
improve the extrinsic performance of a parser,
given only a corpus of sentences and a way
to automatically evaluate the extrinsic quality
of a candidate parse. We apply our method
to train parsers that excel when used as part
of a reordering component in a statistical ma-
chine translation system. We use a corpus of
weakly-labeled reference reorderings to guide
parser training. Our best parsers contribute
significant improvements in subjective trans-
lation quality while their intrinsic attachment
scores typically regress.
1 Introduction
The field of syntactic parsing has received a great
deal of attention and progress since the creation of
the Penn Treebank (Marcus et al, 1993; Collins,
1997; Charniak, 2000; McDonald et al, 2005;
Petrov et al, 2006; Nivre, 2008). A common?
and valid?criticism, however, is that parsers typi-
cally get evaluated only on Section 23 of the Wall
Street Journal portion of the Penn Treebank. This
is problematic for many reasons. As previously ob-
served, this test set comes from a very narrow do-
main that does not necessarily reflect parser perfor-
mance on text coming from more varied domains
(Gildea, 2001), especially web text (Foster, 2010).
There is also evidence that after so much repeated
testing, parsers are indirectly over-fitting to this set
(Petrov and Klein, 2007). Furthermore, parsing was
never meant as a stand-alone task, but is rather a
means to an end, towards the goal of building sys-
tems that can process natural language input.
This is not to say that parsers are not used in larger
systems. All to the contrary, as parsing technology
has become more mature, parsers have become ef-
ficient and accurate enough to be useful in many
natural language processing systems, most notably
in machine translation (Yamada and Knight, 2001;
Galley et al, 2004; Xu et al, 2009). While it has
been repeatedly shown that using a parser can bring
net gains on downstream application quality, it is of-
ten unclear how much intrinsic parsing accuracy ac-
tually matters.
In this paper we try to shed some light on this is-
sue by comparing different parsers in the context of
machine translation (MT). We present experiments
on translation from English to three Subject-Object-
Verb (SOV) languages,1 because those require ex-
tensive syntactic reordering to produce grammatical
translations. We evaluate parse quality on a num-
ber of extrinsic metrics, including word reordering
accuracy, BLEU score and a human evaluation of fi-
nal translation quality. We show that while there is
a good correlation between those extrinsic metrics,
parsing quality as measured on the Penn Treebank
is not a good indicator of the final downstream ap-
plication quality. Since the word reordering metric
can be computed efficiently offline (i.e. without the
use of the final MT system), we then propose to tune
parsers specifically for that metric, with the goal of
improving the performance of the overall system.
To this end we propose a simple training regime
1We experiment with Japanese, Korean and Turkish, but
there is nothing language specific in our approach.
183
which we refer to as targeted self-training (Sec-
tion 2). Similar to self-training, a baseline model
is used to produce predictions on an unlabeled data
set. However, rather than directly training on the
output of the baseline model, we generate a list of
hypotheses and use an external signal to select the
best candidate. The selected parse trees are added
to the training data and the model is then retrained.
The experiments in Section 5 show that this simple
procedure noticeably improves our parsers for the
task at hand, resulting in significant improvements
in downstream translation quality, as measured in a
human evaluation on web text.
This idea is similar in vein to McClosky. et al
(2006) and Petrov et al (2010), except that we use an
extrinsic quality metric instead of a second parsing
model for making the selection. It is also similar to
Burkett and Klein (2008) and Burkett et al (2010),
but again avoiding the added complexity introduced
by the use of additional (bilingual) models for can-
didate selection.
It should be noted that our extrinsic metric is com-
puted from data that has been manually annotated
with reference word reorderings. Details of the re-
ordering metric and the annotated data we used are
given in Sections 3 and 4. While this annotation re-
quires some effort, such annotations are much easier
to obtain than full parse trees. In our experiments
in Section 6 we show that we can obtain similar
improvements on downstream translation quality by
targeted self-training with weakly labeled data (in
form of word reorderings), as with training on the
fully labeled data (with full syntactic parse trees).
2 Targeted Self-Training
Our technique for retraining a baseline parser is an
extension of self-training. In standard parser self-
training, one uses the baseline parsing model to
parse a corpus of sentences, and then adds the 1-best
output of the baseline parser to the training data. To
target the self-training, we introduce an additional
step, given as Algorithm 1. Instead of taking the 1-
best parse, we produce a ranked n-best list of predic-
tions and select the parser which gives the best score
according to an external evaluation function. That
is, instead of relying on the intrinsic model score,
we use an extrinsic score to select the parse towards
Algorithm 1 Select parse that maximizes an extrin-
sic metric.
Input: baseline parser B
Input: sentence S
Input: function COMPUTEEXTRINSIC(parse P )
Output: a parse for the input sentence
Pn = {P1, . . . , Pn} ? n-best parses of S by B
maxScore = 0
bestParse = ?
for k = 1 to n do
extrinsicScore = COMPUTEEXTRINSIC(Pk)
if extrinsicScore > maxScore then
maxScore = extrinsicScore
bestParse = Pk
end if
end for
return bestParse
which to update. In the case of a tie, we prefer the
parse ranked most highly in the n-best list.
The motivation of this selection step is that good
performance on the downstream external task, mea-
sured by the extrinsic metric, should be predictive
of an intrinsically good parse. At the very least,
even if the selected parse is not syntactically cor-
rect, or even if it goes against the original treebank-
ing guidelines, it results in a higher extrinsic score
and should therefore be preferred.
One could imagine extending this framework by
repeatedly running self-training on successively im-
proving parsers in an EM-style algorithm. A recent
work by Hall et al (2011) on training a parser with
multiple objective functions investigates a similar
idea in the context of online learning.
In this paper we focus our attention on machine
translation as the final application, but one could en-
vision applying our techniques to other applications
such as information extraction or question answer-
ing. In particular, we explore one application of
targeted self-training, where computing the extrin-
sic metric involves plugging the parse into an MT
system?s reordering component and computing the
accuracy of the reordering compared to a reference
word order. We now direct our attention to the de-
tails of this application.
184
3 The MT Reordering Task
Determining appropriate target language word or-
der for a translation is a fundamental problem in
MT. When translating between languages with sig-
nificantly different word order such as English and
Japanese, it has been shown that metrics which ex-
plicitly account for word-order are much better cor-
related with human judgments of translation qual-
ity than those that give more weight to word choice,
like BLEU (Lavie and Denkowski, 2009; Isozaki et
al., 2010a; Birch and Osborne, 2010). This demon-
strates the importance of getting reordering right.
3.1 Reordering as a separately evaluable
component
One way to break down the problem of translat-
ing between languages with different word order
is to handle reordering and translation separately:
first reorder source-language sentences into target-
language word order in a preprocessing step, and
then translate the reordered sentences. It has been
shown that good results can be achieved by reorder-
ing each input sentence using a series of tree trans-
formations on its parse tree. The rules for tree
transformation can be manually written (Collins et
al., 2005; Wang, 2007; Xu et al, 2009) or auto-
matically learned (Xia and McCord, 2004; Habash,
2007; Genzel, 2010).
Doing reordering as a preprocessing step, sepa-
rately from translation, makes it easy to evaluate re-
ordering performance independently from the MT
system. Accordingly, Talbot et al (2011) present a
framework for evaluating the quality of reordering
separately from the lexical choice involved in trans-
lation. They propose a simple reordering metric
based on METEOR?s reordering penalty (Lavie and
Denkowski, 2009). This metric is computed solely
on the source language side. To compute it, one
takes the candidate reordering of the input sentence
and partitions it into a set C of contiguous spans
whose content appears contiguously in the same or-
der in the reference. The reordering score is then
computed as
?(esys, eref) = 1?
|C| ? 1
|e| ? 1 .
This metric assigns a score between 0 and 1 where 1
indicates that the candidate reordering is identical to
the reference and 0 indicates that no two words that
are contiguous in the candidate reordering are con-
tiguous in the reference. For example, if a reference
reordering is A B C D E, candidate reordering A
B E C D would get score 1?(3?1)/(5?1) = 0.5.
Talbot et al (2011) show that this reordering score
is strongly correlated with human judgment of trans-
lation quality. Furthermore, they propose to evalu-
ate the reordering quality of an MT system by com-
puting its reordering score on a test set consisting
of source language sentences and their reference re-
orderings. In this paper, we take the same approach
for evaluation, and in addition, we use corpora of
source language sentences and their reference re-
orderings for training the system, not just testing
it. We describe in more detail how the reference re-
ordering data was prepared in Section 4.1.
3.2 Reordering quality as predictor of parse
quality
Figure 1 gives concrete examples of good and bad
reorderings of an English sentence into Japanese
word order. It shows that a bad parse leads to a bad
reordering (lacking inversion of verb ?wear? and ob-
ject ?sunscreen?) and a low reordering score. Could
we flip this causality around, and perhaps try to iden-
tify a good parse tree based on its reordering score?
With the experiments in this paper, we show that in-
deed a high reordering score is predictive of the un-
derlying parse tree that was used to generate the re-
ordering being a good parse (or, at least, being good
enough for our purpose).
In the case of translating English to Japanese or
another SOV language, there is a large amount of
reordering required, but with a relatively small num-
ber of reordering rules one can cover a large pro-
portion of reordering phenomena. Isozaki et al
(2010b), for instance, were able to get impressive
English?Japanese results with only a single re-
ordering rule, given a suitable definition of a head.
Hence, the reordering task depends crucially on a
correct syntactic analysis and is extremely sensitive
to parser errors.
185
4 Experimental Setup
4.1 Treebank data
In our experiments the baseline training corpus is
the Wall Street Journal (WSJ) section of the Penn
Treebank (Marcus et al, 1993) using standard train-
ing/development/testing splits. We converted the
treebank to match the tokenization expected by our
MT system. In particular, we split tokens containing
hyphens into multiple tokens and, somewhat sim-
plistically, gave the original token?s part-of-speech
tag to all newly created tokens. In Section 6 we
make also use of the Question Treebank (QTB)
(Judge et al, 2006), as a source of syntactically an-
notated out-of-domain data. Though we experiment
with both dependency parsers and phrase structure
parsers, our MT system assumes dependency parses
as input. We use the Stanford converter (de Marneffe
et al, 2006) to convert phrase structure parse trees to
dependency parse trees (for both treebank trees and
predicted trees).
4.2 Reference reordering data
We aim to build an MT system that can accurately
translate typical English text that one finds on the
Internet to SOV langauges. To this end, we ran-
domly sampled 13595 English sentences from the
web and created Japanese-word-order reference re-
orderings for them. We split the sentences arbitrarily
into a 6268-sentence Web-Train corpus and a 7327-
sentence Web-Test corpus.
To make the reference alignments we used the
technique suggested by Talbot et al (2011): ask
annotators to translate each English sentence to
Japanese extremely literally and annotate which En-
glish words align to which Japanese words. Golden
reference reorderings can be made programmati-
cally from these annotations. Creating a large set
of reference reorderings is straightforward because
annotators need little special background or train-
ing, as long as they can speak both the source and
target languages. We chose Japanese as the target
language through which to create the English refer-
ence reorderings because we had access to bilingual
annotators fluent in English and Japanese.
Good parse
Reordered:
15 or greater of an SPF has that sunscreen Wear
Reordering score: 1.0 (matches reference)
Bad parse
Reordered:
15 or greater of an SPF has that Wear sunscreen
Reordering score: 0.78 (?Wear? is out of place)
Figure 1: Examples of good and bad parses and cor-
responding reorderings for translation from English to
Japanese. The good parse correctly identifies ?Wear? as
the main verb and moves it to the end of the sentence; the
bad parse analyses ?Wear sunscreen? as a noun phrase
and does not reorder it. This example was one of the
wins in the human evaluation of Section 5.2.
4.3 Parsers
The core dependency parser we use is an implemen-
tation of a transition-based dependency parser using
an arc-eager transition strategy (Nivre, 2008). The
parser is trained using the averaged perceptron algo-
rithm with an early update strategy as described in
Zhang and Clark (2008). The parser uses the fol-
lowing features: word identity of the first two words
on the buffer, the top word on the stack and the head
of the top word on the stack (if available); part-of-
speech identities of the first four words on the buffer
and top two words on the stack; dependency arc la-
bel identities for the top word on the stack, the left
and rightmost modifier of the top word on the stack,
and the leftmost modifier of the first word in the
buffer. We also include conjunctions over all non-
lexical features.
We also give results for the latent variable parser
(a.k.a. BerkeleyParser) of Petrov et al (2006). We
convert the constituency trees output by the Berke-
leyParser to labeled dependency trees using the same
procedure that is applied to the treebanks.
While the BerkeleyParser views part-of-speech
(POS) tagging as an integral part of parsing, our
dependency parser requires the input to be tagged
186
with a separate POS tagger. We use the TnT tag-
ger (Brants, 2000) in our experiments, because of
its efficiency and ease of use. Tagger and parser are
always trained on the same data.
For all parsers, we lowercase the input at train and
test time. We found that this improves performance
in parsing web text. In addition to general upper-
case/lowercase noisiness of the web text negatively
impacting scores, we found that the baseline case-
sensitive parsers are especially bad at parsing imper-
ative sentences, as discussed in Section 5.3.2.
4.4 Reordering rules
In this paper we focus on English to Japanese, Ko-
rean, and Turkish translation. We use a superset of
the reordering rules proposed by Xu et al (2009),
which flatten a dependency tree into SOV word or-
der that is suitable for all three languages. The rules
define a precedence order for the dependents of each
part of speech. For example, a slightly simplified
version of the precedence order of child labels for
a verbal head HEADVERB is: advcl, nsubj, prep,
[other children], dobj, prt, aux, neg, HEADVERB,
mark, ref, compl.
Alternatively, we could have used an automatic
reordering-rule learning framework like that of Gen-
zel (2010). Because the reordering accuracy met-
ric can be computed for any source/target language
pair, this would have made our approach language
completely independent and applicable to any lan-
guage pair. We chose to use manually written rules
to eliminate the variance induced by the automatic
reordering-rule learning framework.
4.5 MT system
We carried out all our translation experiments on a
state-of-the-art phrase-based statistical MT system.
During both training and testing, the system reorders
source-language sentences in a preprocessing step
using the above-mentioned rules. During decoding,
we used an allowed jump width of 4 words. In ad-
dition to the regular distance distortion model, we
incorporate a maximum entropy based lexicalized
phrase reordering model (Zens and Ney, 2006) as
a feature used in decoding.
Overall for decoding, we use between 20 to
30 features, whose weights are optimized using
MERT (Och, 2003). All experiments for a given lan-
guage pair use the same set of MERT weights tuned
on a system using a separate parser (that is neither
the baseline nor the experiment parser). This po-
tentially underestimates the improvements that can
be obtained, but also eliminates MERT as a pos-
sible source of improvement, allowing us to trace
back improvements in translation quality directly to
parser changes.2
For parallel training data, we use a custom collec-
tion of parallel documents. They come from vari-
ous sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. For all language pairs, we
trained on approximately 300 million source words
each.
5 Experiments Reordering Web Text
We experimented with parsers trained in three dif-
ferent ways:
1. Baseline: trained only on WSJ-Train.
2. Standard self-training: trained on WSJ-Train
and 1-best parse of the Web-Train set by base-
line parser.
3. Targeted self-training: trained on WSJ-Train
and, for each sentence in Web-Train, the parse
from the baseline parser?s 512-best list that
when reordered gives the highest reordering
score.3
5.1 Standard self-training vs targeted
self-training
Table 1 shows that targeted self-training on Web-
Train significantly improves Web-Test reordering
score more than standard self-training for both the
shift-reduce parser and for the BerkeleyParser. The
reordering score is generally divorced from the at-
tachment scores measured on the WSJ-Test tree-
bank: for the shift-reduce parser, Web-Test reorder-
ing score and WSJ-Test labeled attachment score
2We also ran MERT on all systems and the pattern of im-
provement is consistent, but sometimes the improvement is big-
ger or smaller after MERT. For instance, the BLEU delta for
Japanese is +0.0030 with MERT on both sides as opposed to
+0.0025 with no MERT.
3We saw consistent but diminishing improvements as we in-
creased the size of the n-best list.
187
Parser Web-Test reordering WSJ-Test LAS
Shift-reduce WSJ baseline 0.757 85.31%
+ self-training 1x 0.760 85.26%
+ self-training 10x 0.756 84.14%
+ targeted self-training 1x 0.770 85.19%
+ targeted self-training 10x 0.777 84.48%
Berkeley WSJ baseline 0.780 88.66%
+ self-training 1x 0.785 89.21%
+ targeted self-training 1x 0.790 89.32%
Table 1: English?Japanese reordering scores on Web-Test for standard self-training and targeted self-training on
Web-Train. Label ?10x? indicates that the self-training data was weighted 10x relative to the WSJ training data.
Bolded reordering scores are different from WSJ-only baseline with 95% confidence but are not significantly different
from each other within the same group.
English to BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.1777 0.1802 2.56 2.69 yes (at 95% level)
Korean 0.3229 0.3259 2.61 2.70 yes (at 90% level)
Turkish 0.1344 0.1370 2.10 2.20 yes (at 95% level)
Table 2: BLEU scores and human evaluation results for translation between three language pairs, varying only the
parser between systems. ?WSJ-only? corresponds to the baseline WSJ-only shift-reduce parser; ?Targeted? corre-
sponds to the Web-Train targeted self-training 10x shift-reduce parser.
(LAS) are anti-correlated, but for BerkeleyParser
they are correlated. Interestingly, weighting the self-
training data more seems to have a negative effect on
both metrics.4
One explanation for the drops in LAS is that some
parts of the parse tree are important for downstream
reordering quality while others are not (or only to
a lesser extent). Some distinctions between labels
become less important; for example, arcs labeled
?amod? and ?advmod? are transformed identically
by the reordering rules. Some semantic distinctions
also become less important; for example, any sane
interpretation of ?red hot car? would be reordered
the same, that is, not at all.
5.2 Translation quality improvement
To put the improvement of the MT system in terms
of BLEU score (Papineni et al, 2002), a widely used
metric for automatic MT evaluation, we took 5000
sentences from Web-Test and had humans gener-
ate reference translations into Japanese, Korean, and
4We did not attempt this experiment for the BerkeleyParser
since training was too slow.
Turkish. We then trained MT systems varying only
the parser used for reordering in training and decod-
ing. Table 2 shows that targeted self-training data
increases BLEU score for translation into all three
languages.
In addition to BLEU increase, a side-by-side hu-
man evaluation on 500 sentences (sampled from
the 5000 used to compute BLEU scores) showed
a statistically significant improvement for all three
languages (see again Table 2). For each sen-
tence, we asked annotators to simultaneously score
both translations from 0 to 6, with guidelines
that 6=?Perfect?, 4=?Most Meaning/Grammar?,
2=?Some Meaning/Grammar?, 0=?Nonsense?. We
computed confidence intervals for the average score
difference using bootstrap resampling; a difference
is significant if the two-sided confidence interval
does not include 0.
5.3 Analysis
As the divergence between the labeled attachment
score on the WSJ-Test data and the reordering score
on the WSJ-Test data indicates, parsing web text
188
Parser Click as N Click as V Imperative rate
case-sensitive shift-reduce WSJ-only 74 0 6.3%
case-sensitive shift-reduce + Web-Train targeted self-training 75 0 10.5%
case-insensitive shift-reduce WSJ-only 75 0 10.3%
case-insensitive shift-reduce + Web-Train targeted self-training 75 0 11.6%
Berkeley WSJ-only 35 35 11.9%
Berkeley + Web-Train targeted self-training 13 58 12.5%
(WSJ-Train) 1 0 0.7%
Table 3: Counts on Web-Test of ?click? tagged as a noun and verb and percentage of sentences parsed imperatively.
poses very different challenges compared to parsing
newswire. We show how our method improves pars-
ing performance and reordering performance on two
examples: the trendy word ?click? and imperative
sentences.
5.3.1 Click
The word ?click? appears only once in the train-
ing portion of the WSJ (as a noun), but appears many
times in our Web test data. Table 3 shows the distri-
bution of part-of-speech tags that different parsers
assign to ?click?. The WSJ-only parsers tag ?click?
as a noun far too frequently. The WSJ-only shift-
reduce parser refuses to tag ?click? as a verb even
with targeted self-training, but BerkeleyParser does
learn to tag ?click? more often as a verb.
It turns out that the shift-reduce parser?s stub-
bornness is not due to a fundamental problem of
the parser, but due to an artifact in TnT. To in-
crease speed, TnT restricts the choices of tags for
known words to previously-seen tags. This causes
the parser?s n-best lists to never hypothesize ?click?
as a verb, and self-training doesn?t click no matter
how targeted it is. This shows that the targeted self-
training approach heavily relies on the diversity of
the baseline parser?s n-best lists.
It should be noted here that it would be easy to
combine our approach with the uptraining approach
of Petrov et al (2010). The idea would be to use the
BerkeleyParser to generate the n-best lists; perhaps
we could call this targeted uptraining. This way, the
shift-reduce parser could benefit both from the gen-
erally higher quality of the parse trees produced by
the BerkeleyParser, as well as from the information
provided by the extrinsic scoring function.
5.3.2 Imperatives
As Table 3 shows, the WSJ training set contains
only 0.7% imperative sentences.5 In contrast, our
test sentences from the web contain approximately
10% imperatives. As a result, parsers trained exclu-
sively on the WSJ underproduce imperative parses,
especially a case-sensitive version of the baseline.
Targeted self-training helps the parsers to predict im-
perative parses more often.
Targeted self-training works well for generating
training data with correctly-annotated imperative
constructions because the reordering of main sub-
jects and verbs in an SOV language like Japanese
is very distinct: main subjects stay at the begin-
ning of the sentence, and main verbs are reordered
to the end of the sentence. It is thus especially easy
to know whether an imperative parse is correct or
not by looking at the reference reordering. Figure 1
gives an example: the bad (WSJ-only) parse doesn?t
catch on to the imperativeness and gets a low re-
ordering score.
6 Targeted Self-Training vs Training on
Treebanks for Domain Adaptation
If task-specific annotation is cheap, then it is rea-
sonable to consider whether we could use targeted
self-training to adapt a parser to a new domain as
a cheaper alternative to making new treebanks. For
example, if we want to build a parser that can reorder
question sentences better than our baseline WSJ-
only parser, we have these two options:
1. Manually construct PTB-style trees for 2000
5As an approximation, we count every parse that begins with
a root verb as an imperative.
189
questions and train on the resulting treebank.
2. Create reference reorderings for 2000 questions
and then do targeted self-training.
To compare these approaches, we created reference
reordering data for our train (2000 sentences) and
test (1000 sentences) splits of the Question Tree-
bank (Judge et al, 2006). Table 4 shows that both
ways of training on QTB-Train sentences give sim-
ilarly large improvements in reordering score on
QTB-Test. Table 5 confirms that this corresponds
to very large increases in English?Japanese BLEU
score and subjective translation quality. In the hu-
man side-by-side comparison, the baseline transla-
tions achieved an average score of 2.12, while the
targeted self-training translations received a score of
2.94, where a score of 2 corresponds to ?some mean-
ing/grammar? and ?4? corresponds to ?most mean-
ing/grammar?.
But which of the two approaches is better? In
the shift-reduce parser, targeted self-training gives
higher reordering scores than training on the tree-
bank, and in BerkeleyParser, the opposite is true.
Thus both approaches produce similarly good re-
sults. From a practical perspective, the advantage of
targeted self-training depends on whether the extrin-
sic metric is cheaper to calculate than treebanking.
For MT reordering, making reference reorderings is
cheap, so targeted self-training is relatively advanta-
geous.
As before, we can examine whether labeled at-
tachment score measured on the test set of the
QTB is predictive of reordering quality. Table 4
shows that targeted self-training raises LAS from
64.78?69.17%. But adding the treebank leads
to much larger increases, resulting in an LAS of
84.75%, without giving higher reordering score. We
can conclude that high LAS is not necessary to
achieve top reordering scores.
Perhaps our reordering rules are somehow defi-
cient when it comes to reordering correctly-parsed
questions, and as a result the targeted self-training
process steers the parser towards producing patho-
logical trees with little intrinsic meaning. To explore
this possibility, we computed reordering scores after
reordering the QTB-Test treebank trees directly. Ta-
ble 4 shows that this gives reordering scores similar
to those of our best parsers. Therefore it is at least
possible that the targeted self-training process could
have resulted in a parser that achieves high reorder-
ing score by producing parses that look like those in
the QuestionBank.
7 Related Work
Our approach to training parsers for reordering is
closely related to self/up-training (McClosky. et al,
2006; Petrov et al, 2010). However, unlike uptrain-
ing, our method does not use only the 1-best output
of the first-stage parser, but has access to the n-best
list. This makes it similar to the work of McClosky.
et al (2006), except that we use an extrinsic metric
(MT reordering score) to select a high quality parse
tree, rather than a second, reranking model that has
access to additional features.
Targeted self-training is also similar to the re-
training of Burkett et al (2010) in which they
jointly parse unannotated bilingual text using a mul-
tiview learning objective, then retrain the monolin-
gual parser models to include each side of the jointly
parsed bitext as monolingual training data. Our ap-
proach is different in that it doesn?t use a second
parser and bitext to guide the creation of new train-
ing data, and instead relies on n-best lists and an
extrinsic metric.
Our method can be considered an instance of
weakly or distantly supervised structured prediction
(Chang et al, 2007; Chang et al, 2010; Clarke et al,
2010; Ganchev et al, 2010). Those methods attempt
to learn structure models from related external sig-
nals or aggregate data statistics. This work differs
in two respects. First, we use the external signals
not as explicit constraints, but to compute an ora-
cle score used to re-rank a set of parses. As such,
there are no requirements that it factor by the struc-
ture of the parse tree and can in fact be any arbitrary
metric. Second, our final objective is different. In
weakly/distantly supervised learning, the objective
is to use external knowledge to build better struc-
tured predictors. In our case this would mean using
the reordering metric as a means to train better de-
pendency parsers. Our objective, on the other hand,
is to use the extrinsic metric to train parsers that are
specifically better at the reordering task, and, as a re-
sult, better suited for MT. This makes our work more
in the spirit of Liang et al (2006), who train a per-
190
Parser QTB-Test reordering QTB-Test LAS
Shift-reduce WSJ baseline 0.663 64.78%
+ treebank 1x 0.704 77.12%
+ treebank 10x 0.768 84.75%
+ targeted self-training 1x 0.746 67.84%
+ targeted self-training 10x 0.779 69.17%
Berkeley WSJ baseline 0.733 76.50%
+ treebank 1x 0.800 87.79%
+ targeted self-training 1x 0.775 80.64%
(using treebank trees directly) 0.788 100%
Table 4: Reordering and labeled attachment scores on QTB-Test for treebank training and targeted self-training on
QTB-Train.
English to QTB-Test BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.2379 0.2615 2.12 2.94 yes (at 95% level)
Table 5: BLEU scores and human evaluation results for English?Japanese translation of the QTB-Test corpus, varying
only the parser between systems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training
10x shift-reduce parser.
ceptron model for an end-to-end MT system where
the alignment parameters are updated based on se-
lecting an alignment from a n-best list that leads to
highest BLEU score. As mentioned earlier, this also
makes our work similar to Hall et al (2011) who
train a perceptron algorithm on multiple objective
functions with the goal of producing parsers that are
optimized for extrinsic metrics.
It has previously been observed that parsers of-
ten perform differently for downstream applications.
Miyao et al (2008) compared parser quality in the
biomedical domain using a protein-protein interac-
tion (PPI) identification accuracy metric. This al-
lowed them to compare the utility of extant depen-
dency parsers, phrase structure parsers, and deep
structure parsers for the PPI identification task. One
could apply the targeted self-training technique we
describe to optimize any of these parsers for the PPI
task, similar to how we have optimized our parser
for the MT reordering task.
8 Conclusion
We introduced a variant of self-training that targets
parser training towards an extrinsic evaluation met-
ric. We use this targeted self-training approach to
train parsers that improve the accuracy of the word
reordering component of a machine translation sys-
tem. This significantly improves the subjective qual-
ity of the system?s translations from English into
three SOV languages. While the new parsers give
improvements in these external evaluations, their in-
trinsic attachment scores go down overall compared
to baseline parsers trained only on treebanks. We
conclude that when using a parser as a component
of a larger external system, it can be advantageous
to incorporate an extrinsic metric into parser train-
ing and evaluation, and that targeted self-training is
an effective technique for incorporating an extrinsic
metric into parser training.
References
A. Birch and M. Osborne. 2010. LRscore for evaluating
lexical and reordering quality in MT. In ACL-2010
WMT.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP ?00.
D. Burkett and D. Klein. 2008. Two languages are better
than one (for syntactic parsing). In EMNLP ?08.
D. Burkett, S. Petrov, J. Blitzer, and D. Klein. 2010.
Learning better monolingual models with unannotated
bilingual text. In CoNLL ?10.
191
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In ACL
?07.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In ICML ?10.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world?s response.
In CoNLL ?10.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In ACL
?05.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL ?97.
M.-C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC ?06.
J. Foster. 2010. ?cba to check the spelling?: Investigat-
ing parser performance on discussion forum posts. In
NAACL ?10.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In HLT-NAACL ?04.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Genzel. 2010. Automatically learning source-side re-
ordering rules for large scale machine translation. In
COLING ?10.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP ?01.
N. Habash. 2007. Syntactic preprocessing for statistical
machine translation. In MTS ?07.
K. Hall, R. McDonald, J. Katz-Brown, and M. Ringgaard.
2011. Training dependency parsers by jointly optimiz-
ing multiple objectives. In EMNLP ?11.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.
2010a. Automatic evaluation of translation quality for
distant language pairs. In EMNLP ?10.
H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh. 2010b.
Head finalization: A simple reordering rule for SOV
languages. In ACL-2010 WMT.
J. Judge, A. Cahill, and J. v. Genabith. 2006. Question-
Bank: creating a corpus of parse-annotated questions.
In ACL ?06.
A. Lavie and M. Denkowski. 2009. The Meteor metric
for automatic evaluation of machine translation. Ma-
chine Translation, 23(2-3).
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL ?06.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
D. McClosky., E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In NAACL ?06.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL
?05.
Y. Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and J. Tsu-
jii. 2008. Task-oriented evaluation of syntactic parsers
and their representations. In ACL ?08.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ?03.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL ?02.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL ?07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
S. Petrov, P. Chang, and M. Ringgaard H. Alshawi. 2010.
Uptraining for accurate deterministic question parsing.
In EMNLP ?10.
D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,
M. Seno, and F. Och. 2011. A lightweight evalua-
tion framework for machine translation reordering. In
EMNLP-2011 WMT.
C. Wang. 2007. Chinese syntactic reordering for statisti-
cal machine translation. In EMNLP ?07.
F. Xia and M. McCord. 2004. Improving a statistical MT
system with automatically learned rewrite patterns. In
Coling ?04.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a
dependency parser to improve SMT for subject-object-
verb languages. In NAACL-HLT ?09.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL ?01.
R. Zens and H. Ney. 2006. Discriminative reordering
models for statistical machine translation. In NAACL-
06 WMT.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing. In EMNLP ?08.
192
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1363?1372,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Watermarking the Outputs of Structured Prediction with an
application in Statistical Machine Translation.
Ashish Venugopal1 Jakob Uszkoreit1 David Talbot1 Franz J. Och1 Juri Ganitkevitch2
1Google, Inc. 2Center for Language and Speech Processing
1600 Amphitheatre Parkway Johns Hopkins University
Mountain View, 94303, CA Baltimore, MD 21218, USA
{avenugopal,uszkoreit,talbot,och}@google.com juri@cs.jhu.edu
Abstract
We propose a general method to water-
mark and probabilistically identify the
structured outputs of machine learning al-
gorithms. Our method is robust to lo-
cal editing operations and provides well
defined trade-offs between the ability to
identify algorithm outputs and the qual-
ity of the watermarked output. Unlike
previous work in the field, our approach
does not rely on controlling the inputs to
the algorithm and provides probabilistic
guarantees on the ability to identify col-
lections of results from one?s own algo-
rithm. We present an application in statis-
tical machine translation, where machine
translated output is watermarked at mini-
mal loss in translation quality and detected
with high recall.
1 Motivation
Machine learning algorithms provide structured
results to input queries by simulating human be-
havior. Examples include automatic machine
translation (Brown et al, 1993) or automatic
text and rich media summarization (Goldstein
et al, 1999). These algorithms often estimate
some portion of their models from publicly avail-
able human generated data. As new services
that output structured results are made avail-
able to the public and the results disseminated
on the web, we face a daunting new challenge:
Machine generated structured results contam-
inate the pool of naturally generated human
data. For example, machine translated output
and human generated translations are currently
both found extensively on the web, with no auto-
matic way of distinguishing between them. Al-
gorithms that mine data from the web (Uszko-
reit et al, 2010), with the goal of learning to
simulate human behavior, will now learn mod-
els from this contaminated and potentially self-
generated data, reinforcing the errors commit-
ted by earlier versions of the algorithm.
It is beneficial to be able to identify a set of
encountered structured results as having been
generated by one?s own algorithm, with the pur-
pose of filtering such results when building new
models.
Problem Statement: We define a struc-
tured result of a query q as r = {z1 ? ? ? zL} where
the order and identity of elements zi are impor-
tant to the quality of the result r. The structural
aspect of the result implies the existence of alter-
native results (across both the order of elements
and the elements themselves) that might vary in
their quality.
Given a collection of N results, CN =
r1 ? ? ? rN , where each result ri has k ranked alter-
natives Dk(qi) of relatively similar quality and
queries q1 ? ? ? qN are arbitrary and not controlled
by the watermarking algorithm, we define the
watermarking task as:
Task. Replace ri with r?i ? Dk(qi) for some sub-
set of results in CN to produce a watermarked
collection C?N
such that:
? C?N is probabilistically identifiable as having
been generated by one?s own algorithm.
1363
? the degradation in quality from CN to the
watermarked C?N should be analytically con-
trollable, trading quality for detection per-
formance.
? C?N should not be detectable as water-
marked content without access to the gen-
erating algorithms.
? the detection of C?N should be robust to sim-
ple edit operations performed on individual
results r ? C?N .
2 Impact on Statistical Machine
Translation
Recent work(Resnik and Smith, 2003; Munteanu
and Marcu, 2005; Uszkoreit et al, 2010) has
shown that multilingual parallel documents can
be efficiently identified on the web and used as
training data to improve the quality of statisti-
cal machine translation.
The availability of free translation services
(Google Translate, Bing Translate) and tools
(Moses, Joshua), increase the risk that the con-
tent found by parallel data mining is in fact gen-
erated by a machine, rather than by humans. In
this work, we focus on statistical machine trans-
lation as an application for watermarking, with
the goal of discarding documents from training
if they have been generated by one?s own algo-
rithms.
To estimate the magnitude of the problem,
we used parallel document mining (Uszkoreit et
al., 2010) to generate a collection of bilingual
document pairs across several languages. For
each document, we inspected the page content
for source code that indicates the use of trans-
lation modules/plug-ins that translate and pub-
lish the translated content.
We computed the proportion of the content
within our corpus that uses these modules. We
find that a significant proportion of the mined
parallel data for some language pairs is gener-
ated via one of these translation modules. The
top 3 languages pairs, each with parallel trans-
lations into English, are Tagalog (50.6%), Hindi
(44.5%) and Galician (41.9%). While these
proportions do not reflect impact on each lan-
guage?s monolingual web, they are certainly high
enough to affect machine translations systems
that train on mined parallel data. In this work,
we develop a general approach to watermark
structured outputs and apply it to the outputs
of a statistical machine translation system with
the goal of identifying these same outputs on the
web. In the context of the watermarking task
defined above, we output selecting alternative
translations for input source sentences. These
translations often undergo simple edit and for-
matting operations such as case changes, sen-
tence and word deletion or post editing, prior to
publishing on the web. We want to ensure that
we can still detect watermarked translations de-
spite these edit operations. Given the rapid pace
of development within machine translation, it
is also important that the watermark be robust
to improvements in underlying translation qual-
ity. Results from several iterations of the system
within a single collection of documents should be
identifiable under probabilistic bounds.
While we present evaluation results for sta-
tistical machine translation, our proposed ap-
proach and associated requirements are applica-
ble to any algorithm that produces structured
results with several plausible alternatives. The
alternative results can arise as a result of inher-
ent task ambiguity (for example, there are mul-
tiple correct translations for a given input source
sentence) or modeling uncertainty (for example,
a model assigning equal probability to two com-
peting results).
3 Watermark Structured Results
Selecting an alternative r? from the space of al-
ternatives Dk(q) can be stated as:
r? = arg max
r?Dk(q)
w(r,Dk(q), h) (1)
where w ranks r ? Dk(q) based on r?s presen-
tation of a watermarking signal computed by a
hashing operation h. In this approach, w and
its component operation h are the only secrets
held by the watermarker. This selection crite-
rion is applied to all system outputs, ensuring
that watermarked and non-watermarked version
of a collection will never be available for compar-
ison.
1364
A specific implementation of w within our wa-
termarking approach can be evaluated by the
following metrics:
? False Positive Rate: how often non-
watermarked collections are falsely identi-
fied as watermarked.
? Recall Rate: how often watermarked col-
lections are correctly identified as water-
marked.
? Quality Degradation: how significantly
does C?N differ from CN when evaluated by
task specific quality metrics.
While identification is performed at the col-
lection level, we can scale these metrics based
on the size of each collection to provide more
task sensitive metrics. For example, in machine
translation, we count the number of words in
the collection towards the false positive and re-
call rates.
In Section 3.1, we define a random hashing
operation h and a task independent implemen-
tation of the selector function w. Section 3.2
describes how to classify a collection of water-
marked results. Section 3.3 and 3.4 describes re-
finements to the selection and classification cri-
teria that mitigate quality degradation. Follow-
ing a comparison to related work in Section 4,
we present experimental results for several lan-
guages in Section 5.
3.1 Watermarking: CN ? C?N
We define a random hashing operation h that is
applied to result r. It consists of two compo-
nents:
? A hash function applied to a structured re-
sult r to generate a bit sequence of a fixed
length.
? An optional mapping that maps a single
candidate result r to a set of sub-results.
Each sub-result is then hashed to generate
a concatenated bit sequence for r.
A good hash function produces outputs whose
bits are independent. This implies that we can
treat the bits for any input structured results
as having been generated by a binomial distri-
bution with equal probability of generating 1s
vs 0s. This condition also holds when accu-
mulating the bit sequences over a collection of
results as long as its elements are selected uni-
formly from the space of possible results. There-
fore, the bits generated from a collection of un-
watermarked results will follow a binomial dis-
tribution with parameter p = 0.5. This result
provides a null hypothesis for a statistical test
on a given bit sequence, testing whether it is
likely to have been generated from a binomial
distribution binomial(n, p) where p = 0.5 and n
is the length of the bit sequence.
For a collection CN = r1 ? ? ? rN , we can define
a watermark ranking function w to systemati-
cally select alternatives r?i ? Dk(q), such that
the resulting C?N is unlikely to produce bit se-
quences that follow the p = 0.5 binomial distri-
bution. A straightforward biasing criteria would
be to select the candidate whose bit sequence ex-
hibits the highest ratio of 1s. w can be defined
as:
w(r,Dk(q), h) =
#(1, h(r))
|h(r)| (2)
where h(r) returns the randomized bit sequence
for result r, and #(x, ~y) counts the number of
occurrences of x in sequence ~y. Selecting alter-
natives results to exhibit this bias will result in
watermarked collections that exhibit this same
bias.
3.2 Detecting the Watermark
To classify a collection CN as watermarked or
non-watermarked, we apply the hashing opera-
tion h on each element in CN and concatenate
the sequences. This sequence is tested against
the null hypothesis that it was generated by a
binomial distribution with parameter p = 0.5.
We can apply a Fisherian test of statistical sig-
nificance to determine whether the observed dis-
tribution of bits is unlikely to have occurred by
chance under the null hypothesis (binomial with
p = 0.5).
We consider a collection of results that rejects
the null hypothesis to be watermarked results
generated by our own algorithms. The p-value
under the null hypothesis is efficiently computed
1365
by:
p? value = Pn(X ? x) (3)
=
n?
i=x
(n
i
)
pi(1? p)n?i (4)
where x is the number of 1s observed in the col-
lection, and n is the total number of bits in the
sequence. Comparing this p-value against a de-
sired significance level ?, we reject the null hy-
pothesis for collections that have Pn(X ? x) <
?, thus deciding that such collections were gen-
erated by our own system.
This classification criteria has a fixed false
positive rate. Setting ? = 0.05, we know that
5% of non-watermarked bit sequences will be
falsely labeled as watermarked. This parameter
? can be controlled on an application specific ba-
sis. By biasing the selection of candidate results
to produce more 1s than 0s, we have defined
a watermarking approach that exhibits a fixed
false positive rate, a probabilistically bounded
detection rate and a task independent hashing
and selection criteria. In the next sections, we
will deal with the question of robustness to edit
operations and quality degradation.
3.3 Robustness and Inherent Bias
We would like the ability to identify water-
marked collections to be robust to simple edit
operations. Even slight modifications to the ele-
ments within an item r would yield (by construc-
tion of the hash function), completely different
bit sequences that no longer preserve the biases
introduced by the watermark selection function.
To ensure that the distributional biases intro-
duced by the watermark selector are preserved,
we can optionally map individual results into a
set of sub-results, each one representing some lo-
cal structure of r. h is then applied to each sub-
result and the results concatenated to represent
r. This mapping is defined as a component of
the h operation.
While a particular edit operation might af-
fect a small number of sub-results, the majority
of the bits in the concatenated bit sequence for
r would remain untouched, thereby limiting the
damage to the biases selected during watermark-
ing. This is of course no defense to edit opera-
tions that are applied globally across the result;
our expectation is that such edits would either
significantly degrade the quality of the result or
be straightforward to identify directly.
For example, a sequence of words r = z1 ? ? ? zL
can be mapped into a set of consecutive n-gram
sequences. Operations to edit a word zi in r will
only affect events that consider the word zi. To
account for the fact that alternatives in Dk(q)
might now result in bit sequences of different
lengths, we can generalize the biasing criteria to
directly reflect the expected contribution to the
watermark by defining:
w(r,Dk(q), h) = Pn(X ? #(1, h(r))) (5)
where Pn gives probabilities from binomial(n =
|h(r)|, p = 0.5).
Inherent collection level biases: Our null
hypothesis is based on the assumption that col-
lections of results draw uniformly from the space
of possible results. This assumption might not
always hold and depends on the type of the re-
sults and collection. For example, considering
a text document as a collection of sentences,
we can expect that some sentences might repeat
more frequently than others.
This scenario is even more likely when ap-
plying a mapping into sub-results. n-gram se-
quences follow long-tailed or Zipfian distribu-
tions, with a small number of n-grams contribut-
ing heavily toward the total number of n-grams
in a document.
A random hash function guarantees that in-
puts are distributed uniformly at random over
the output range. However, the same input will
be assigned the same output deterministically.
Therefore, if the distribution of inputs is heav-
ily skewed to certain elements of the input space,
the output distribution will not be uniformly
distributed. The bit sequences resulting from
the high frequency sub-results have the potential
to generate inherently biased distributions when
accumulated at the collection level. We want to
choose a mapping that tends towards generating
uniformly from the space of sub-results. We can
empirically measure the quality of a sub-result
mapping for a specific task by computing the
1366
false positive rate on non-watermarked collec-
tions. For a given significance level ?, an ideal
mapping would result in false positive rates close
to ? as well.
Figure 1 shows false positive rates from 4 al-
ternative mappings, computed on a large corpus
of French documents (see Table 1 for statistics).
Classification decisions are made at the collec-
tion level (documents) but the contribution to
the false positive rate is based on the number
of words in the classified document. We con-
sider mappings from a result (sentence) into its
1-grams, 1 ? 5-grams and 3 ? 5 grams as well
as the non-mapping case, where the full result
is hashed.
Figure 1 shows that the 1-grams and 1 ? 5-
gram generate sub-results that result in heav-
ily biased false positive rates. The 3 ? 5 gram
mapping yields false positive rates close to their
theoretically expected values. 1 Small devia-
tions are expected since documents make differ-
ent contributions to the false positive rate as a
function of the number of words that they repre-
sent. For the remainder of this work, we use the
3-5 gram mapping and the full sentence map-
ping, since the alternatives generate inherently
distributions with very high false positive rates.
3.4 Considering Quality
The watermarking described in Equation 3
chooses alternative results on a per result basis,
with the goal of influencing collection level bit
sequences. The selection criteria as described
will choose the most biased candidates available
in Dk(q). The parameter k determines the ex-
tent to which lesser quality alternatives can be
chosen. If all the alternatives in each Dk(q) are
of relatively similar quality, we expect minimal
degradation due to watermarking.
Specific tasks however can be particularly sen-
sitive to choosing alternative results. Discrimi-
native approaches that optimize for arg max se-
lection like (Och, 2003; Liang et al, 2006; Chi-
ang et al, 2009) train model parameters such
1In the final version of this paper we will perform sam-
pling to create a more reliable estimate of the false posi-
tive rate that is not overly influenced by document length
distributions.
that the top-ranked result is well separated from
its competing alternatives. Different queries also
differ in the inherent ambiguity expected from
their results; sometimes there really is just one
correct result for a query, while for other queries,
several alternatives might be equally good.
By generalizing the definition of the w func-
tion to interpolate the estimated loss in quality
and the gain in the watermarking signal, we can
trade-off the ability to identify the watermarked
collections against quality degradation:
w(r,Dk(q), fw) = ? ? gain(r,Dk(q), fw)
?(1? ?) ? loss(r,Dk(q))
(6)
Loss: The loss(r,Dk(q)) function reflects the
quality degradation that results from selecting
alternative r as opposed to the best ranked can-
didate in Dk(q)). We will experiment with two
variants:
lossrank(r,Dk(q)) = (rank(r)? k)/k
losscost(r,Dk(q)) = (cost(r)?cost(r1))/ cost(r1)
where:
? rank(r): returns the rank of r within Dk(q).
? cost(r): a weighted sum of features (not
normalized over the search space) in a log-
linear model such as those mentioned in
(Och, 2003).
? r1: the highest ranked alternative in Dk(q).
lossrank provides a generally applicable criteria
to select alternatives, penalizing selection from
deep within Dk(q). This estimate of the qual-
ity degradation does not reflect the generating
model?s opinion on relative quality. losscost con-
siders the relative increase in the generating
model?s cost assigned to the alternative trans-
lation.
Gain: The gain(r,Dk(q), fw) function reflects
the gain in the watermarking signal by selecting
candidate r. We simply define the gain as the
Pn(X ? #(1, h(r))) from Equation 5.
1367
 0 0.2 0.4 0.6 0.8 1 0.05 0.10 0.25 0.5portion of documents p-value thresholdexpectedobserved
(a) 1-grams mapping
 0 0.2 0.4 0.6 0.8 1 0.05 0.10 0.25 0.5portion of documents p-value thresholdexpectedobserved
(b) 1? 5-grams mapping
 0 0.2 0.4 0.6 0.8 1 0.05 0.10 0.25 0.5portion of documents p-value thresholdexpectedobserved
(c) 3? 5-grams mapping
 0 0.2 0.4 0.6 0.8 1 0.05 0.10 0.25 0.5portion of documents p-value thresholdexpectedobserved
(d) Full result hashing
Figure 1: Comparison of expected false positive rates against observed false positive rates for different
sub-result mappings.
4 Related Work
Using watermarks with the goal of transmitting
a hidden message within images, video, audio
and monolingual text media is common. For
structured text content, linguistic approaches
like (Chapman et al, 2001; Gupta et al, 2006)
use language specific linguistic and semantic
expansions to introduce hidden watermarks.
These expansions provide alternative candidates
within which messages can be encoded. Re-
cent publications have extended this idea to ma-
chine translation, using multiple systems and
expansions to generate alternative translations.
(Stutsman et al, 2006) uses a hashing function
to select alternatives that encode the hidden
message in the lower order bits of the transla-
tion. In each of these approaches, the water-
marker has control over the collection of results
into which the watermark is to be embedded.
These approaches seek to embed a hidden
message into a collection of results that is se-
lected by the watermarker. In contrast, we ad-
dress the condition where the input queries are
not in the watermarker?s control.
The goal is therefore to introduce the water-
mark into all generated results, with the goal of
probabilistically identifying such outputs. Our
approach is also task independent, avoiding the
need for templates to generate additional al-
ternatives. By addressing the problem directly
within the search space of a dynamic program-
ming algorithm, we have access to high quality
alternatives with well defined models of qual-
ity loss. Finally, our approach is robust to local
word editing. By using a sub-result mapping, we
increase the level of editing required to obscure
the watermark signal; at high levels of editing,
the quality of the results themselves would be
significantly degraded.
5 Experiments
We evaluate our watermarking approach applied
to the outputs of statistical machine translation
under the following experimental setup.
A repository of parallel (aligned source and
target language) web documents is sampled to
produce a large corpus on which to evaluate the
watermarking classification performance. The
1368
corpora represent translations into 4 diverse tar-
get languages, using English as the source lan-
guage. Each document in this corpus can be
considered a collection of un-watermarked struc-
tured results, where source sentences are queries
and each target sentence represents a structured
result.
Using a state-of-the-art phrase-based statisti-
cal machine translation system (Och and Ney,
2004) trained on parallel documents identified
by (Uszkoreit et al, 2010), we generate a set
of 100 alternative translations for each source
sentence. We apply the proposed watermarking
approach, along with the proposed refinements
that address task specific loss (Section 3.4) and
robustness to edit operations (Section 3.3) to
generate watermarked corpora.
Each method is controlled via a single param-
eter (like k or ?) which is varied to generate
alternative watermarked collections. For each
parameter value, we evaluate the Recall Rate
and Quality Degradation with the goal of find-
ing a setting that yields a high recall rate, min-
imal quality degradation. False positive rates
are evaluated based on a fixed classification sig-
nificance level of ? = 0.05. The false posi-
tive and recall rates are evaluated on the word
level; a document that is misclassified or cor-
rectly identified contributes its length in words
towards the error calculation. In this work, we
use ? = 0.05 during classification corresponding
to an expected 5% false positive rate. The false
positive rate is a function of h and the signifi-
cance level ? and therefore constant across the
parameter values k and ?.
We evaluate quality degradation on human
translated test corpora that are more typical for
machine translation evaluation. Each test cor-
pus consists of 5000 source sentences randomly
selected from the web and translated into each
respective language.
We chose to evaluate quality on test corpora
to ensure that degradations are not hidden by
imperfectly matched web corpora and are con-
sistent with the kind of results often reported for
machine translation systems. As with the clas-
sification corpora, we create watermarked ver-
sions at each parameter value. For a given pa-
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.5  0.6  0.7  0.8  0.9  1
B
L
E
U
 
l
o
s
s
recall
FRENCH max K-BestFRENCH model cost gainFRENCH rank gain
Figure 2: BLEU loss against recall of watermarked
content for the baseline approach (max K-best),
rank and cost interpolation.
rameter value, we measure false positive and re-
call rates on the classification corpora and qual-
ity degradation on the evaluation corpora.
Table 1 shows corpus statistics for the classi-
fication and test corpora and non-watermarked
BLEU scores for each target language. All
source texts are in English.
5.1 Loss Interpolated Experiments
Our first set of experiments demonstrates base-
line performance using the watermarking crite-
ria in Equation 5 versus the refinements sug-
gested in Section 3.4 to mitigate quality degra-
dation. The h function is computed on the full
sentence result r with no sub-event mapping.
The following methods are evaluated in Figure 2.
? Baseline method (labeled ?max K-best?):
selects r? purely based on gain in water-
marking signal (Equation 5) and is param-
eterized by k: the number of alternatives
considered for each result.
? Rank interpolation: incorporates rank into
w, varying the interpolation parameter ?.
? Cost interpolation: incorporates cost into
w, varying the interpolation parameter ?.
The observed false positive rate on the French
classification corpora is 1.9%.
1369
Classification Quality
Target # words # sentences # documents # words # sentences BLEU %
Arabic 200107 15820 896 73592 5503 12.29
French 209540 18024 600 73592 5503 26.45
Hindi 183676 13244 1300 73409 5489 20.57
Turkish 171671 17155 1697 73347 5486 13.67
Table 1: Content statistics for classification and quality degradation corpora. Non-watermarked BLEU
scores are reported for the quality corpora.
We consider 0.2% BLEU loss as a thresh-
old for acceptable quality degradation. Each
method is judged by its ability to achieve high
recall below this quality degradation threshold.
Applying cost interpolation yields the best
results in Figure 2, achieving a recall of 85%
at 0.2% BLEU loss, while rank interpolation
achieves a recall of 76%. The baseline approach
of selecting the highest gain candidate within a
depth of k candidates does not provide sufficient
parameterization to yield low quality degrada-
tion. At k = 2, this method yields almost 90%
recall, but with approximately 0.4% BLEU loss.
5.2 Robustness Experiments
In Section 5.2, we proposed mapping results into
sub-events or features. We considered alterna-
tive feature mappings in Figure 1, finding that
mapping sentence results into a collection of 3-
5 grams yields acceptable false positive rates at
varied levels of ?.
Figure 3 presents results that compare mov-
ing from the result level hashing to the 3-5 gram
sub-result mapping. We show the impact of the
mapping on the baseline max K-best method as
well as for cost interpolation. There are sub-
stantial reductions in recall rate at the 0.2%
BLEU loss level when applying sub-result map-
pings in cases. The cost interpolation method
recall drops from 85% to 77% when using the
3-5 grams event mapping. The observed false
positive rate of the 3-5 gram mapping is 4.7%.
By using the 3-5 gram mapping, we expect
to increase robustness against local word edit
operations, but we have sacrificed recall rate due
to the inherent distributional bias discussed in
Section 3.3.
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.5  0.6  0.7  0.8  0.9  1
B
L
E
U
 
l
o
s
s
recall
FRENCH nbest max K-best full sentenceFRENCH max K-best 3-5 gramsFRENCH cost interp. full sentenceFRENCH cost interp. 3-5 grams
Figure 3: BLEU loss against recall of watermarked
content for the baseline and cost interpolation meth-
ods using both result level and 3-5 gram mapped
events.
5.3 Multilingual Experiments
The watermarking approach proposed here in-
troduces no language specific watermarking op-
erations and it is thus broadly applicable to
translating into all languages. In Figure 4, we
report results for the baseline and cost interpola-
tion methods, considering both the result level
and 3-5 gram mapping. We set ? = 0.05 and
measure recall at 0.2% BLEU degradation for
translation from English into Arabic, French,
Hindi and Turkish. The observed false posi-
tive rates for full sentence hashing are: Arabic:
2.4%, French: 1.8%, Hindi: 5.6% and Turkish:
5.5%, while for the 3-5 gram mapping, they are:
Arabic: 5.8%, French: 7.5%, Hindi:3.5% and
Turkish: 6.2%. Underlying translation qual-
ity plays an important role in translation qual-
ity degradation when watermarking. Without
a sub-result mapping, French (BLEU: 26.45%)
1370
 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 Arabic French Hindi Turkishrecall sentence-level3-to-5 grams
Figure 4: Loss of recall when using 3-5 gram mapping
vs sentence level mapping for Arabic, French, Hindi
and Turkish translations.
achieves recall of 85% at 0.2% BLEU loss, while
the other languages achieve over 90% recall at
the same BLEU loss threshold. Using a sub-
result mapping degrades quality for each lan-
guage pair, but changes the relative perfor-
mance. Turkish experiences the highest rela-
tive drop in recall, unlike French and Arabic,
where results are relatively more robust to using
sub-sentence mappings. This is likely a result of
differences in n-gram distributions across these
languages. The languages considered here all
use space separated words. For languages that
do not, like Chinese or Thai, our approach can
be applied at the character level.
6 Conclusions
In this work we proposed a general method
to watermark and probabilistically identify the
structured outputs of machine learning algo-
rithms. Our method provides probabilistic
bounds on detection ability, analytic control on
quality degradation and is robust to local edit-
ing operations. Our method is applicable to
any task where structured outputs are generated
with ambiguities or ties in the results. We ap-
plied this method to the outputs of statistical
machine translation, evaluating each refinement
to our approach with false positive and recall
rates against BLEU score quality degradation.
Our results show that it is possible, across sev-
eral language pairs, to achieve high recall rates
(over 80%) with low false positive rates (between
5 and 8%) at minimal quality degradation (0.2%
BLEU), while still allowing for local edit opera-
tions on the translated output. In future work
we will continue to investigate methods to mit-
igate quality loss.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.
Peter F. Brown, Vincent J.Della Pietra, Stephen
A. Della Pietra, and Robert. L. Mercer. 1993.
The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Lin-
guistics, 19:263?311.
Mark Chapman, George Davida, and Marc
Rennhardway. 2001. A practical and effec-
tive approach to large-scale automated linguistic
steganography. In Proceedings of the Information
Security Conference.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine trans-
lation. In North American Chapter of the Associa-
tion for Computational Linguistics - Human Lan-
guage Technologies (NAACL-HLT).
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text docu-
ments: Sentence selection and evaluation metrics.
In Research and Development in Information Re-
trieval, pages 121?128.
Gaurav Gupta, Josef Pieprzyk, and Hua Xiong
Wang. 2006. An attack-localizing watermarking
scheme for natural language documents. In Pro-
ceedings of the 2006 ACM Symposium on Informa-
tion, computer and communications security, ASI-
ACCS ?06, pages 157?165, New York, NY, USA.
ACM.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein,
and Ben Taskar. 2006. An end-to-end dis-
criminative approach to machine translation. In
Proceedings of the Joint International Conference
on Computational Linguistics and Association of
Computational Linguistics (COLING/ACL, pages
761?768.
Dragos Stefan Munteanu and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2004. The
1371
alignment template approach to statistical ma-
chine translation. Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings
of the 2003 Meeting of the Asssociation of Com-
putational Linguistics.
Philip Resnik and Noah A. Smith. 2003. The web as
a parallel corpus. computational linguistics. Com-
putational Linguistics.
Ryan Stutsman, Mikhail Atallah, Christian
Grothoff, and Krista Grothoff. 2006. Lost
in just the translation. In Proceedings of the 2006
ACM Symposium on Applied Computing.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and
Moshe Dubiner. 2010. Large scale parallel doc-
ument mining for machine translation. In Pro-
ceedings of the 2010 COLING.
1372
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 975?983,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Model Combination for Machine Translation
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz Och
UC Berkeley Google, Inc.
denero@berkeley.edu {shankarkumar,ciprianchelba,och}@google.com
Abstract
Machine translation benefits from two types
of decoding techniques: consensus decoding
over multiple hypotheses under a single model
and system combination over hypotheses from
different models. We present model combina-
tion, a method that integrates consensus de-
coding and system combination into a uni-
fied, forest-based technique. Our approach
makes few assumptions about the underly-
ing component models, enabling us to com-
bine systems with heterogenous structure. Un-
like most system combination techniques, we
reuse the search space of component models,
which entirely avoids the need to align trans-
lation hypotheses. Despite its relative sim-
plicity, model combination improves trans-
lation quality over a pipelined approach of
first applying consensus decoding to individ-
ual systems, and then applying system combi-
nation to their output. We demonstrate BLEU
improvements across data sets and language
pairs in large-scale experiments.
1 Introduction
Once statistical translation models are trained, a de-
coding approach determines what translations are fi-
nally selected. Two parallel lines of research have
shown consistent improvements over the standard
max-derivation decoding objective, which selects
the highest probability derivation. Consensus de-
coding procedures select translations for a single
system by optimizing for model predictions about
n-grams, motivated either as minimizing Bayes risk
(Kumar and Byrne, 2004), maximizing sentence
similarity (DeNero et al, 2009), or approximating a
max-translation objective (Li et al, 2009b). System
combination procedures, on the other hand, generate
translations from the output of multiple component
systems (Frederking and Nirenburg, 1994). In this
paper, we present model combination, a technique
that unifies these two approaches by learning a con-
sensus model over the n-gram features of multiple
underlying component models.
Model combination operates over the compo-
nent models? posterior distributions over translation
derivations, encoded as a forest of derivations.1 We
combine these components by constructing a linear
consensus model that includes features from each
component. We then optimize this consensus model
over the space of all translation derivations in the
support of all component models? posterior distribu-
tions. By reusing the components? search spaces,
we entirely avoid the hypothesis alignment problem
that is central to standard system combination ap-
proaches (Rosti et al, 2007).
Forest-based consensus decoding techniques dif-
fer in whether they capture model predictions
through n-gram posteriors (Tromble et al, 2008;
Kumar et al, 2009) or expected n-gram counts
(DeNero et al, 2009; Li et al, 2009b). We evaluate
both in controlled experiments, demonstrating their
empirical similarity. We also describe algorithms for
expanding translation forests to ensure that n-grams
are local to a forest?s hyperedges, and for exactly
computing n-gram posteriors efficiently.
Model combination assumes only that each trans-
lation model can produce expectations of n-gram
features; the latent derivation structures of compo-
nent systems can differ arbitrarily. This flexibility
allows us to combine phrase-based, hierarchical, and
syntax-augmented translation models. We evaluate
by combining three large-scale systems on Chinese-
English and Arabic-English NIST data sets, demon-
strating improvements of up to 1.4 BLEU over the
1In this paper, we use the terms translation forest and hyper-
graph interchangeably.
975
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
Step 1: Compute Single-Model N-gram Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based system Hierarchical system
...
...
R
R
pb
R
h
R
R
pb
R
h
?saw the?:
[
v2pb = 0.7, v2h = 1.0
]
[?pb = 1] [?h = 1]
v2pb(?saw the?) = 0.7
green witch
was here
blue witch
green witch was here
blue witch was here
was here
green witch was here
blue witch was here
green witch blue witch
v2h(?saw the?) = 1.0
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.6
?saw the?
1.0
?man with?
I ... man
0.4
?telescope the?
0.3
?saw with?
I ... telescope
?I saw the man with the telescope?
?I saw with the telescope the man? 
Step 1: Compute Combination Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based model Hierarchical model
...
...
R
R
pb
R
h
[?pb = 1] [?h = 1]
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
v2h(?saw the?) = 0.7v2pb(?saw the?) = 0.9
?saw the?:
[
v2pb = 0.9, v2h = 0.7
]
applied rule
rule root
rule leaves
n !? P (n)
Figure 1: An example translation forest encoding two
synchronous derivations for a Spanish sentence: one solid
and one dotted. Nodes are annotated with their left and
right unigram contexts, and hyperedges are annotated
with scores ? ? ?(r) and the bigrams they introduce.
best single systemmax-derivation baseline, and con-
sistent improvements over a more complex multi-
system pipeline that includes independent consensus
decoding and system combination.
2 Model Combination
Model combination is a model-based approach to se-
lecting translations using information from multiple
component systems. Each system provides its poste-
rior distributions over derivations Pi(d|f), encoded
as a weighted translation forest (i.e., translation hy-
pergraph) in which hyperedges correspond to trans-
lation rule applications r.2 The conditional distribu-
tion over derivations takes the form:
Pi(d|f) =
exp
[?
r?d ?i ? ?i(r)
]
?
d??D(f) exp
[?
r?d? ?i ? ?i(r)
]
whereD(f) is the set of synchronous derivations en-
coded in the forest, r iterates over rule applications
in d, and ?i is the parameter vector for system i. The
feature vector ?i is system specific and includes both
translation model and language model features. Fig-
ure 1 depicts an example forest.
Model combination includes four steps, described
below. The entire sequence is illustrated in Figure 2.
2Phrase-based systems produce phrase lattices, which are in-
stances of forests with arity 1.
2.1 Computing Combination Features
The first step in model combination is to com-
pute n-gram expectations from component system
posteriors?the same quantities found in MBR, con-
sensus, and variational decoding techniques. For an
n-gram g and system i, the expectation
vni (g) = EPi(d|f) [h(d, g)]
can be either an n-gram expected count, if h(d, g)
is the count of g in d, or the posterior probability
that d contains g, if h(d, g) is an indicator function.
Section 3 describes how to compute these features
efficiently.
2.2 Constructing a Search Space
The second step in model combination constructs a
hypothesis space of translation derivations, which
includes all derivations present in the forests con-
tributed by each component system. This search
space D is also a translation forest, and consists of
the conjoined union of the component forests. Let
Ri be the root node of component hypergraph Di.
For all i, we include all of Di in D, along with an
edge from Ri to R, the root of D. D may contain
derivations from different types of translation sys-
tems. However, D only contains derivations (and
therefore translations) that appeared in the hypothe-
sis space of some component system. We do not in-
termingle the component search spaces in any way.
2.3 Features for the Combination Model
The third step defines a new combination model over
all of the derivations in the search space D, and then
annotates D with features that allow for efficient
model inference. We use a linear model over four
types of feature functions of a derivation:
1. Combination feature functions on n-grams
vni (d) =
?
g?Ngrams(d) v
n
i (g) score a deriva-
tion according to the n-grams it contains.
2. Model score feature function b gives the model
score ?i ? ?i(d) of a derivation d under the sys-
tem i that d is from.
3. A length feature ` computes the word length of
the target-side yield of a derivation.
4. A system indicator feature ?i is 1 if the deriva-
tion came from system i, and 0 otherwise.
976
All of these features are local to rule applications
(hyperedges) in D. The combination features pro-
vide information sharing across the derivations of
different systems, but are functions of n-grams, and
so can be scored on any translation forest. Model
score features are already local to rule applications.
The length feature is scored in the standard way.
System indicator features are scored only on the hy-
peredges fromRi toR that link each component for-
est to the common root.
Scoring the joint search space D with these fea-
tures involves annotating each rule application r (i.e.
hyperedge) with the value of each feature.
2.4 Model Training and Inference
We have defined the following combination model
sw(d) with weights w over derivations d from I dif-
ferent component models:
I?
i=1
[
4?
n=1
wni v
n
i (d) + w
?
i ?i(d)
]
+wb?b(d)+w`?`(d)
Because we have assessed all of these features on
local rule applications, we can find the highest scor-
ing derivation d? = arg max
d?D
sw(d) using standard
max-sum (Viterbi) inference over D.
We learn the weights of this consensus model us-
ing hypergraph-based minimum-error-rate training
(Kumar et al, 2009). This procedure maximizes the
translation quality of d? on a held-out set, according
to a corpus-level evaluation metric B(?; e) that com-
pares to a reference set e. We used BLEU, choosing
w to maximize the BLEU score of the set of transla-
tions predicted by the combination model.
3 Computing Combination Features
The combination features vni (d) score derivations
from each model with the n-gram predictions of the
others. These predictions sum over all derivations
under a single component model to compute a pos-
terior belief about each n-gram. In this paper, we
compare two kinds of combination features, poste-
rior probabilities and expected counts.3
3The model combination framework could incorporate ar-
bitrary features on the common output space of the models, but
we focus on features that have previously proven useful for con-
sensus decoding.
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
Step 1: Compute Singl -Model N-gram Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based system Hierarchical system
...
...
R
R
pb
R
h
R
R
pb
R
h
?saw the?:
[
v2pb = 0.7, v2h = 1.0
]
[?pb = 1] [?h = 1]
v2pb(?saw the?) = 0.7
green witch
was here
blue witch
green witch was here
blue witch was here
was here
green witch was here
blue witch was here
green witch blue witch
v2h(?saw the?) = 1.0
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.6
?saw the?
1.0
?man with?
I ... man
0.4
?telescope the?
0.3
?saw with?
I ... telescope
?I saw the man with the telescope?
?I saw with the telescope the man? 
Step 1: Compute Combination Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based model Hierarchical model
...
...
R
R
pb
R
h
[?pb = 1] [?h = 1]
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
v2h(?saw the?) = 0.7v2pb(?saw the?) = 0.9
?saw the?:
[
v2pb = 0.9, v2h = 0.7
]
applied rule
rule root
rule leaves
n !? P (n)
Figure 2: Model combination applied to a phrase-based
(pb) and a hierarchical model (h) includes four steps. (1)
shows an excerpt of the bigram feature function for each
component, (2) depicts the result of conjoining a phrase
lattice with a hierarchical forest, (3) shows example hy-
peredge features of the combination model, including bi-
gram features vni and system indicators ?i, and (4) gives
training and decoding objectives.
Posterior probabilities represent a model?s be-
lief that the translation will contain a particular n-
gram at least once. They can be expressed as
EP (d|f) [?(d, g)] for an indicator function ?(d, g)
that is 1 if n-gram g appears in derivation d. These
quantities arise in approximating BLEU for lattice-
based and hypergraph-based minimum Bayes risk
decoding (Tromble et al, 2008; Kumar et al, 2009).
Expected n-gram counts EP (d|f) [c(d, g)] represent
the model?s belief of how many times an n-gram g
will appear in the translation. These quantities ap-
pear in forest-based consensus decoding (DeNero et
al., 2009) and variational decoding (Li et al, 2009b).
977
Methods for computing both of these quantities ap-
pear in the literature. However, we address two out-
standing issues below. In Section 5, we also com-
pare the two quantities experimentally.
3.1 Computing N -gram Posteriors Exactly
Kumar et al (2009) describes an efficient approx-
imate algorithm for computing n-gram posterior
probabilities. Algorithm 1 is an exact algorithm that
computes all n-gram posteriors from a forest in a
single inside pass. The algorithm tracks two quanti-
ties at each node n: regular inside scores ?(n) and
n-gram inside scores ??(n, g) that sum the scores of
all derivations rooted at n that contain n-gram g.
For each hyperedge, we compute b?(g), the sum of
scores for derivations that do not contain g (Lines 8-
11). We then use that quantity to compute the score
of derivations that do contain g (Line 17).
Algorithm 1 Computing n-gram posteriors
1: for n ? N in topological order do
2: ?(n)? 0
3: ??(n, g)? 0, ?g ? Ngrams(n)
4: for r ? Rules(n) do
5: w ? exp [? ? ?(r)]
6: b? w
7: b?(g)? w, ?g ? Ngrams(n)
8: for ` ? Leaves(r) do
9: b? b? ?(`)
10: for g ? Ngrams(n) do
11: b?(g)? b?(g)?
(
?(`)? ??(`, g)
)
12: ?(n)? ?(n) + b
13: for g ? Ngrams(n) do
14: if g ? Ngrams(r) then
15: ??(n, g)? ??(n, g)+b
16: else
17: ??(n, g)? ??(n, g)+b? b?(g)
18: for g ? Ngrams(root) (all g in the HG) do
19: P (g|f)? ??(root,g)?(root)
This algorithm can in principle compute the pos-
terior probability of any indicator function on local
features of a derivation. More generally, this algo-
rithm demonstrates how vector-backed inside passes
can compute quantities beyond expectations of local
features (Li and Eisner, 2009).4 Chelba and Maha-
jan (2009) developed a similar algorithm for lattices.
4Indicator functions on derivations are not locally additive
3.2 Ensuring N -gram Locality
DeNero et al (2009) describes an efficient algorithm
for computing n-gram expected counts from a trans-
lation forest. This method assumes n-gram local-
ity of the forest, the property that any n-gram intro-
duced by a hyperedge appears in all derivations that
include the hyperedge. However, decoders may re-
combine forest nodes whenever the language model
does not distinguish between n-grams due to back-
off (Li and Khudanpur, 2008). In this case, a forest
encoding of a posterior distribution may not exhibit
n-gram locality in all regions of the search space.
Figure 3 shows a hypergraph which contains non-
local trigrams, along with its local expansion.
Algorithm 2 expands a forest to ensure n-gram lo-
cality while preserving the encoded distribution over
derivations. Let a forest (N,R) consist of nodes N
and hyperedges R, which correspond to rule appli-
cations. Let Rules(n) be the subset of R rooted by
n, and Leaves(r) be the leaf nodes of rule applica-
tion r. The expanded forest (Ne, Re) is constructed
by a function Reapply(r, L) that applies the rule of r
to a new set of leavesL ? Ne, forming a pair (r?, n?)
consisting of a new rule application r? rooted by n?.
P is a map from nodes in N to subsets of Ne which
tracks how N projects to Ne. Two nodes in Ne are
identical if they have the same (n?1)-gram left and
right contexts and are projections of the same node
in N . The symbol
?
denotes a set cross-product.
Algorithm 2 Expanding for n-gram locality
1: Ne ? {}; Re ? {}
2: for n ? N in topological order do
3: P (n)? {}
4: for r ? Rules(n) do
5: for L ?
?
`?Leaves(r) [P (`)] do
6: r?, n? ? Reapply(r, L)
7: P (n)? P (n) ? {n?}
8: Ne ? Ne ? {n?}
9: Re ? Re ? {r?}
This transformation preserves the original distri-
bution over derivations by splitting states, but main-
taining continuations from those split states by du-
plicating rule applications. The process is analogous
over the rules of a derivation, even if the features they indicate
are local. Therefore, Algorithm 1 is not an instance of an ex-
pectation semiring computation.
978
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
Step 1: Compute Single-Model N-gram Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based system Hierarchical system
...
...
R
R
pb
R
h
R
R
pb
R
h
?saw the?: [v2pb = 0.7, v2h = 1.0
]
[?pb = 1] [?h = 1]
v2pb(?saw the?) = 0.7
green witch
was here
blue witch
green witch was here
blue witch was here
was here
green witch was here
blue witch was here
green witch blue witch
v2h(?saw the?) = 1.0
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.6
?saw the?
1.0
?man with?
I ... man
0.4
?telescope the?
0.3
?saw with?
I ... telescope
?I saw the man with the telescope?
?I saw with the telescope the man? 
Step 1: Compute Combination Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based model Hierarchical model
...
...
R
R
pb
R
h
[?pb = 1] [?h = 1]
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
v2h(?saw the?) = 0.7v2pb(?saw the?) = 0.9
?saw the?: [v2pb = 0.9, v2h = 0.7
]
applied rule
rule root
rule leaves
n !? P (n)
Figure 3: Hypergraph expansion ensures n-gram locality
without affecting the distribution over derivations. In the
left example, trigrams ?green witch was? and ?blue witch
was? are non-local due to language model back-off. On
the right, states are split to enforce trigram locality.
to expanding bigram lattices to e code a trigram his-
tory at each lattice node (Weng et al, 1998).
4 Relationship to Prior Work
Model combination is a multi-system generaliza-
tion of consensus or minimum Bayes risk decod-
ing. When only one component system is included,
model combination is identical to minimum Bayes
risk decoding over hypergraphs, as described in Ku-
mar et al (2009).5
4.1 System Combination
System combination techniques in machine trans-
lat on take as input the outputs {e1, ? ? ? , ek} of k
translation systems, where ei is a structured transla-
tion object (or k-best lists thereof), typically viewed
as a sequence of words. The dominant approach in
the field chooses a primary translation ep as a back-
bone, then finds an alignment ai to the backbone for
each ei. A new search space is constructed from
these backbone-aligned outputs, and then a voting
procedure or feature-based model predicts a final
consensus translation (Rosti et al, 2007). Model
combination entirely avoids this alignment problem
by viewing hypotheses as n-gram occurrence vec-
tors rather than word sequences.
Model combination also requires less total com-
putation than applying system combination to
5We do not refer to model combination as a minimum Bayes
risk decoding procedure despite this similarity because risk im-
plies a belief distribution over outputs, and we now have mul-
tiple output distributions that are not necessarily calibrated.
Moreover, our generalized, multi-model objective (Section 2.4)
is motivated by BLEU, but not a direct approximation to it.
consensus-decoded outputs. The best consensus de-
coding methods for individual systems already re-
quire the computation-intensive steps of model com-
bination: producing lattices or forests, computing n-
gram feature expectations, and re-decoding to max-
imize a secondary consensus objective. Hence, to
maximize the performance of system combination,
these steps must be performed for each system,
whereas model combination requires only one for-
est rescoring pass over all systems.
Model combination also leverages aggregate
statistics from the components? posteriors, whereas
system combiners typically do not. Zhao and He
(2009) showed that n-gram posterior features are
useful in the context of a system combination model,
even when computed from k-best lists.
Despite these advantages, system combination
may be more appropriate in some settings. In par-
ticular, model combination is designed primarily for
st tistical systems that generate hypergraph outputs.
Model combination can in principle integrate a non-
statisti al system that generates either a single hy-
pothesis or an unweighted forest.6 Likewise, the pro-
cedure c uld be applied to statistical systems that
only generate k-best lists. However, we would not
expect the same strong performance from model
combination in these constrained settings.
4.2 Joint Decoding and Collaborative Decoding
Liu et al (2009) describes two techniques for com-
bining multiple synchronous grammars, which the
authors characterize as joint decoding. Joint de-
coding does not involve a consensus or minimum-
Bayes-risk decoding objective; indeed, their best
results come from standard max-derivation decod-
ing (with a multi-system grammar). More impor-
tantly, their computations rely on a correspondence
between nodes in the hypergraph outputs of differ-
ent systems, and so they can only joint decode over
models with similar search strategies. We combine a
phrase-based model that uses left-to-right decoding
with two hierarchical systems that use bottom-up de-
coding ? a scenario to which joint decoding is not
applicable. Though Liu et al (2009) rightly point
out that most models can be decoded either left-to-
6A single hypothesis can be represented as a forest, while an
unweighted forest could be assigned a uniform distribution.
979
right or bottom-up, such changes can have substan-
tial implications for search efficiency and search er-
ror. We prefer to maintain the flexibility of using dif-
ferent search strategies in each component system.
Li et al (2009a) is another related technique for
combining translation systems by leveraging model
predictions of n-gram features. K-best lists of par-
tial translations are iteratively reranked using n-
gram features from the predictions of other mod-
els (which are also iteratively updated). Our tech-
nique differs in that we use no k-best approxima-
tions, have fewer parameters to learn (one consensus
weight vector rather than one for each collaborating
decoder) and produce only one output, avoiding an
additional system combination step at the end.
5 Experiments
We report results on the constrained data track of the
NIST 2008 Arabic-to-English (ar-en) and Chinese-
to-English (zh-en) translation tasks.7 We train on all
parallel and monolingual data allowed in the track.
We use the NIST 2004 eval set (dev) for optimiz-
ing parameters in model combination and test on
the NIST 2008 evaluation set. We report results
using the IBM implementation of the BLEU score
which computes the brevity penalty using the clos-
est reference translation for each segment (Papineni
et al, 2002). We measure statistical significance us-
ing 95% confidence intervals computed using paired
bootstrap resampling. In all table cells (except for
Table 3) systems without statistically significant dif-
ferences are marked with the same superscript.
5.1 Base Systems
We combine outputs from three systems. Our
phrase-based system is similar to the alignment tem-
plate system described by Och and Ney (2004).
Translation is performed using a standard left-
to-right beam-search decoder. Our hierarchical
systems consist of a syntax-augmented system
(SAMT) that includes target-language syntactic cat-
egories (Zollmann and Venugopal, 2006) and a
Hiero-style system with a single non-terminal (Chi-
ang, 2007). Each base system yields state-of-the-art
translation performance, summarized in Table 1.
7http://www.nist.gov/speech/tests/mt
BLEU (%)
ar-en zh-en
Sys Base dev nist08 dev nist08
PB MAX 51.6 43.9 37.7 25.4
PB MBR 52.4? 44.6? 38.6? 27.3?
PB CON 52.4? 44.6? 38.7? 27.2?
Hiero MAX 50.9 43.3 40.0 27.2
Hiero MBR 51.4? 43.8? 40.6? 27.8
Hiero CON 51.5? 43.8? 40.5? 28.2
SAMT MAX 51.7 43.8 40.8? 28.4
SAMT MBR 52.7? 44.5? 41.1? 28.8?
SAMT CON 52.6? 44.4? 41.1? 28.7?
Table 1: Performance of baseline systems.
BLEU (%)
ar-en zh-en
Approach dev nist08 dev nist08
Best MAX system 51.7 43.9 40.8 28.4
Best MBR system 52.7 44.5 41.1 28.8?
MC Conjoin/SI 53.5 45.3 41.6 29.0?
Table 2: Performance from the best single system for
each language pair without consensus decoding (Best
MAX system), the best system with minimum Bayes risk
decoding (Best MBR system), and model combination
across three systems.
For each system, we report the performance of
max-derivation decoding (MAX), hypergraph-based
MBR (Kumar et al, 2009), and a linear version of
forest-based consensus decoding (CON) (DeNero et
al., 2009). MBR and CON differ only in that the first
uses n-gram posteriors, while the second uses ex-
pected n-gram counts. The two consensus decoding
approaches yield comparable performance. Hence,
we report performance for hypergraph-based MBR
in our comparison to model combination below.
5.2 Experimental Results
Table 2 compares model combination (MC) to the
best MAX and MBR systems. Model combination
uses a conjoined search space wherein each hyper-
edge is annotated with 21 features: 12 n-gram poste-
rior features vni computed from the PB/Hiero/SAMT
forests for n ? 4; 4 n-gram posterior features vn
computed from the conjoined forest; 1 length fea-
ture `; 1 feature b for the score assigned by the base
model; and 3 system indicator (SI) features ?i that
select which base system a derivation came from.
We refer to this model combination approach as MC
980
BLEU (%)
ar-en zh-en
Strategy dev nist08 dev nist08
Best MBR system 52.7 44.5 41.1 28.8
MBR Conjoin 52.3 44.5 40.5 28.3
MBR Conjoin/feats-best 52.7 44.9 41.2 28.8
MBR Conjoin/SI 53.1 44.9 41.2 28.9
MC 1-best HG 52.7 44.6 41.1 28.7
MC Conjoin 52.9 44.6 40.3 28.1
MC Conjoin/base/SI 53.5 45.1 41.2 28.9
MC Conjoin/SI 53.5 45.3 41.6 29.0
Table 3: Model Combination experiments.
Conjoin/SI. Model combination improves over the
single best MAX system by 1.4 BLEU in ar-en and
0.6 BLEU in zh-en, and always improves over MBR.
This improvement could arise due to multiple rea-
sons: a bigger search space, the consensus features
from constituent systems, or the system indicator
features. Table 3 teases apart these contributions.
We first perform MBR on the conjoined hyper-
graph (MBR-Conjoin). In this case, each edge is
tagged with 4 conjoined n-gram features vn, along
with length and base model features. MBR-Conjoin
is worse than MBR on the hypergraph from the
single best system. This could imply that either
the larger search space introduces poor hypotheses
or that the n-gram posteriors obtained are weaker.
When we now restrict the n-gram features to those
from the best system (MBR Conjoin/feats-best),
BLEU scores increase relative to MBR-Conjoin.
This implies that the n-gram features computed over
the conjoined hypergraph are weaker than the corre-
sponding features from the best system.
Adding system indicator features (MBR Con-
join+SI) helps the MBR-Conjoin system consider-
ably; the resulting system is better than the best
MBR system. This could mean that the SI features
guide search towards stronger parts of the larger
search space. In addition, these features provide a
normalization of scores across systems.
We next do several model-combination experi-
ments. We perform model combination using the
search space of only the best MBR system (MC
1best HG). Here, the hypergraph is annotated with
n-gram features from the 3 base systems, as well as
length and base model features. A total of 3 ? 4 +
1 + 1 = 14 features are added to each edge. Sur-
BLEU (%)
ar-en zh-en
Approach Base dev nist08 dev nist08
Sent-level MAX 51.8? 44.4? 40.8? 28.2?
Word-level MAX 52.0? 44.4? 40.8? 28.1?
Sent-level MBR 52.7+ 44.6? 41.2 28.8+
Word-level MBR 52.5+ 44.7? 40.9 28.8+
MC-conjoin-SI 53.5 45.3 41.6 29.0+
Table 4: BLEU performance for different system and
model combination approaches. Sentence-level and
word-level system combination operate over the sentence
output of the base systems, which are either decoded to
maximize derivation score (MAX) or to minimize Bayes
risk (MBR).
prisingly, n-gram features from the additional sys-
tems did not help select a better hypothesis within
the search space of a single system.
When we expand the search space to the con-
joined hypergraph (MC Conjoin), it performs worse
relative to MC 1-best. Since these two systems are
identical in their feature set, we hypothesize that
the larger search space has introduced erroneous hy-
potheses. This is similar to the scenario where MBR
Conjoin is worse than MBR 1-best. As in the MBR
case, adding system indicator features helps (MC
Conjoin/base/SI). The result is comparable to MBR
on the conjoined hypergraph with SI features.
We finally add extra n-gram features which are
computed from the conjoined hypergraph (MC Con-
join + SI). This gives the best performance although
the gains over MC Conjoin/base/SI are quite small.
Note that these added features are the same n-gram
features used in MBR Conjoin. Although they are
not strong by themselves, they provide additional
discriminative power by providing a consensus score
across all 3 base systems.
5.3 Comparison to System Combination
Table 4 compares model combination to two sys-
tem combination algorithms. The first, which we
call sentence-level combination, chooses among the
base systems? three translations the sentence that
has the highest consensus score. The second, word-
level combination, builds a ?word sausage? from
the outputs of the three systems and chooses a path
through the sausage with the highest score under
a similar model (Macherey and Och, 2007). Nei-
981
BLEU (%)
ar-en zh-en
Approach dev nist08 dev nist08
HG-expand 52.7? 44.5? 41.1? 28.8?
HG-noexpand 52.7? 44.5? 41.1? 28.8?
Table 5: MBR decoding on the syntax augmented system,
with and without hypergraph expansion.
ther system combination technique provides much
benefit, presumably because the underlying systems
all share the same data, pre-processing, language
model, alignments, and code base.
Comparing system combination when no consen-
sus (i.e., minimum Bayes risk) decoding is utilized
at all, we find that model combination improves
upon the result by up to 1.1 BLEU points. Model
combination also performs slightly better relative to
system combination over MBR-decoded systems. In
the latter case, system combination actually requires
more computation compared to model combination;
consensus decoding is performed for each system
rather than only once for model combination. This
experiment validates our approach. Model combina-
tion outperforms system combination while avoid-
ing the challenge of aligning translation hypotheses.
5.4 Algorithmic Improvements
Section 3 describes two improvements to comput-
ing n-gram posteriors: hypergraph expansion for n-
gram locality and exact posterior computation. Ta-
ble 5 shows MBR decoding with and without expan-
sion (Algorithm 2) in a decoder that collapses nodes
due to language model back-off. These results show
that while expansion is necessary for correctness, it
does not affect performance.
Table 6 compares exact n-gram posterior compu-
tation (Algorithm 1) to the approximation described
by Kumar et al (2009). Both methods yield identical
results. Again, while the exact method guarantees
correctness of the computation, the approximation
suffices in practice.
6 Conclusion
Model combination is a consensus decoding strat-
egy over a collection of forests produced by multi-
ple machine translation systems. These systems can
BLEU (%)
ar-en zh-en
Posteriors dev nist08 dev nist08
Exact 52.4? 44.6? 38.6? 27.3?
Approximate 52.5? 44.6? 38.6? 27.2?
Table 6: MBR decoding on the phrase-based system with
either exact or approximate posteriors.
have varied decoding strategies; we only require that
each system produce a forest (or a lattice) of trans-
lations. This flexibility allows the technique to be
applied quite broadly. For instance, de Gispert et al
(2009) describe combining systems based on mul-
tiple source representations using minimum Bayes
risk decoding?likewise, they could be combined
via model combination.
Model combination has two significant advan-
tages over current approaches to system combina-
tion. First, it does not rely on hypothesis alignment
between outputs of individual systems. Aligning
translation hypotheses accurately can be challeng-
ing, and has a substantial effect on combination per-
formance (He et al, 2008). Instead of aligning hy-
potheses, we compute expectations of local features
of n-grams. This is analogous to how BLEU score is
computed, which also views sentences as vectors of
n-gram counts (Papineni et al, 2002) . Second, we
do not need to pick a backbone system for combina-
tion. Choosing a backbone system can also be chal-
lenging, and also affects system combination perfor-
mance (He and Toutanova, 2009). Model combina-
tion sidesteps this issue by working with the con-
joined forest produced by the union of the compo-
nent forests, and allows the consensus model to ex-
press system preferences via weights on system in-
dicator features.
Despite its simplicity, model combination pro-
vides strong performance by leveraging existing
consensus, search, and training techniques. The
technique outperforms MBR and consensus decod-
ing on each of the component systems. In addition,
it performs better than standard sentence-based or
word-based system combination techniques applied
to either max-derivation or MBR outputs of the indi-
vidual systems. In sum, it is a natural and effective
model-based approach to multi-system decoding.
982
References
Ciprian Chelba and M. Mahajan. 2009. A dynamic
programming algorithm for computing the posterior
probability of n-gram occurrences in automatic speech
recognition lattices. Personal communication.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.
2009. Minimum bayes risk combination of translation
hypotheses from alternative morphological decompo-
sitions. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of the Association for Computational Lin-
guistics and IJCNLP.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proceedings of the Con-
ference on Applied Natural Language Processing.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Asso-
ciation for Computational Linguistics and IJCNLP.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
Workshop on Syntax and Structure in Statistical Trans-
lation.
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and
Ming Zhou. 2009a. Collaborative decoding: Partial
hypothesis re-ranking using translation consensus be-
tween decoders. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b.
Variational decoding for statistical machine transla-
tion. In Proceedings of the Association for Compu-
tational Linguistics and IJCNLP.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of the Association for Computational Lin-
guistics and IJCNLP.
Wolfgang Macherey and Franz Och. 2007. An empirical
study on computing consensus translations from mul-
tiple machine translation systems. In EMNLP, Prague,
Czech Republic.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Translation.
Computational Linguistics, 30(4):417 ? 449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
Association for Computational Linguistics.
Antti-Veikko I. Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie J.
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice minimum Bayes-risk
decoding for statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Fuliang Weng, Andreas Stolcke, and Ananth Sankar.
1998. Efficient lattice representation and generation.
In Intl. Conf. on Spoken Language Processing.
Yong Zhao and Xiaodong He. 2009. Using n-gram based
features for machine translation system combination.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the NAACL 2006 Workshop on statisti-
cal machine translation.
983
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1395?1404,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Language-independent Compound Splitting with Morphological Operations
Klaus Macherey1 Andrew M. Dai2 David Talbot1 Ashok C. Popat1 Franz Och1
1Google Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043, USA
{kmach,talbot,popat,och}@google.com
2University of Edinburgh
10 Crichton Street
Edinburgh, UK EH8 9AB
a.dai@ed.ac.uk
Abstract
Translating compounds is an important prob-
lem in machine translation. Since many com-
pounds have not been observed during train-
ing, they pose a challenge for translation sys-
tems. Previous decompounding methods have
often been restricted to a small set of lan-
guages as they cannot deal with more complex
compound forming processes. We present a
novel and unsupervised method to learn the
compound parts and morphological operations
needed to split compounds into their com-
pound parts. The method uses a bilingual
corpus to learn the morphological operations
required to split a compound into its parts.
Furthermore, monolingual corpora are used to
learn and filter the set of compound part can-
didates. We evaluate our method within a ma-
chine translation task and show significant im-
provements for various languages to show the
versatility of the approach.
1 Introduction
A compound is a lexeme that consists of more than
one stem. Informally, a compound is a combina-
tion of two or more words that function as a single
unit of meaning. Some compounds are written as
space-separated words, which are called open com-
pounds (e.g. hard drive), while others are written
as single words, which are called closed compounds
(e.g. wallpaper). In this paper, we shall focus only
on closed compounds because open compounds do
not require further splitting.
The objective of compound splitting is to split a
compound into its corresponding sequence of con-
stituents. If we look at how compounds are created
from lexemes in the first place, we find that for some
languages, compounds are formed by concatenating
existing words, while in other languages compound-
ing additionally involves certain morphological op-
erations. These morphological operations can be-
come very complex as we illustrate in the following
case studies.
1.1 Case Studies
Below, we look at splitting compounds from 3 differ-
ent languages. The examples introduce in part the
notation used for the decision rule outlined in Sec-
tion 3.1.
1.1.1 English Compound Splitting
The word flowerpot can appear as a closed or open
compound in English texts. To automatically split
the closed form we have to try out every split point
and choose the split with minimal costs according to
a cost function. Let's assume that we already know
that flowerpot must be split into two parts. Then we
have to position two split points that mark the end of
each part (one is always reserved for the last charac-
ter position). The number of split points is denoted
by K (i.e. K = 2), while the position of split points
is denoted by n1 and n2. Since flowerpot consists of
9 characters, we have 8 possibilities to position split
point n1 within the characters c1, . . . , c8. The final
split point corresponds with the last character, that is,
n2 = 9. Trying out all possible single splits results
in the following candidates:
flowerpot ? f+ lowerpot
flowerpot ? fl+ owerpot
...
flowerpot ? flower+ pot
...
flowerpot ? flowerpo+ t
1395
If we associate each compound part candidate with
a cost that reflects how frequent this part occurs in a
large collection of English texts, we expect that the
correct split flower + pot will have the lowest cost.
1.1.2 German Compound Splitting
The previous example covered a casewhere the com-
pound is constructed by directly concatenating the
compound parts. While this works well for En-
glish, other languages require additional morpholog-
ical operations. To demonstrate, we look at the Ger-
man compound Verkehrszeichen (traffic sign) which
consists of the two nouns Verkehr (traffic) and Zei-
chen (sign). Let's assume that we want to split this
word into 3 parts, that is, K = 3. Then, we get the
following candidates.
Verkehrszeichen ? V+ e+ rkehrszeichen
Verkehrszeichen ? V+ er+ kehrszeichen
...
Verkehrszeichen ? Verkehr+ s+ zeichen
...
Verkehrszeichen ? Verkehrszeich+ e+ n
Using the same procedure as described before, we
can lookup the compound parts in a dictionary or de-
termine their frequency from large text collections.
This yields the optimal split points n1 = 7, n2 =
8, n3 = 15. The interesting part here is the addi-
tional s morpheme, which is called a linking mor-
pheme, because it combines the two compound parts
to form the compound Verkehrszeichen. If we have
a list of all possible linking morphemes, we can
hypothesize them between two ordinary compound
parts.
1.1.3 Greek Compound Splitting
The previous example required the insertion of a
linking morpheme between two compound parts.
We shall now look at a more complicated mor-
phological operation. The Greek compound
?????????? (cardboard box) consists of the two
parts ????? (paper) and ????? (box). Here, the
problem is that the parts ????? and ????? are not
valid words in Greek. To lookup the correct words,
we must substitute the suffix of the compound part
candidates with some other morphemes. If we allow
the compound part candidates to be transformed by
some morphological operation, we can lookup the
transformed compound parts in a dictionary or de-
termine their frequencies in some large collection of
Greek texts. Let's assume that we need only one split
point. Then this yields the following compound part
candidates:
?????????? ? ? + ?????????
?????????? ? ? + ????????? g2 : ? / ?
?????????? ? ? + ????????? g2 : ? / ?
...
?????????? ? ????? + ????? g1 : ? / ? ,
g2 : ? / ?...
?????????? ? ????????? + ? g1 : ? / ?
?????????? ? ????????? + ? g2 : ? / ?
Here, gk : s/t denotes the kth compound part which
is obtained by replacing string s with string t in the
original string, resulting in the transformed part gk.
1.2 Problems and Objectives
Our goal is to design a language-independent com-
pound splitter that is useful for machine translation.
The previous examples addressed the importance of
a cost function that favors valid compound parts ver-
sus invalid ones. In addition, the examples have
shown that, depending on the language, the morpho-
logical operations can become very complex. For
most Germanic languages like Danish, German, or
Swedish, the list of possible linking morphemes is
rather small and can be provided manually. How-
ever, in general, these lists can become very large,
and language experts who could provide such lists
might not be at our disposal. Because it seems in-
feasible to list the morphological operations explic-
itly, we want to find and extract those operations
automatically in an unsupervised way and provide
them as an additional knowledge source to the de-
compounding algorithm.
Another problem is how to evaluate the quality
of the compound splitter. One way is to compile
for every language a large collection of compounds
together with their valid splits and to measure the
proportion of correctly split compounds. Unfortu-
nately, such lists do not exist for many languages.
1396
While the training algorithm for our compound split-
ter shall be unsupervised, the evaluation data needs
to be verified by human experts. Since we are in-
terested in improving machine translation and to cir-
cumvent the problem of explicitly annotating com-
pounds, we evaluate the compound splitter within a
machine translation task. By decompounding train-
ing and test data of a machine translation system, we
expect an increase in the number of matching phrase
table entries, resulting in better translation quality
measured in BLEU score (Papineni et al, 2002).
If BLEU score is sensitive enough to measure the
quality improvements obtained from decompound-
ing, there is no need to generate a separate gold stan-
dard for compounds.
Finally, we do not want to split non-compounds
and named entities because we expect them to be
translated non-compositionally. For example, the
German wordDeutschland (Germany) could be split
into two parts Deutsch (German) + Land (coun-
try). Although this is a valid split, named entities
should be kept as single units. An example for a
non-compound is the German participle vereinbart
(agreed) which could be wrongly split into the parts
Verein (club) + Bart (beard). To avoid overly eager
splitting, we will compile a list of non-compounds in
an unsupervised way that serves as an exception list
for the compound splitter. To summarize, we aim to
solve the following problems:
? Define a cost function that favors valid com-
pound parts and rejects invalid ones.
? Learn morphological operations, which is im-
portant for languages that have complex com-
pound forming processes.
? Apply compound splitting to machine transla-
tion to aid in translation of compounds that have
not been seen in the bilingual training data.
? Avoid splitting non-compounds and named en-
tities as this may result in wrong translations.
2 Related work
Previous work concerning decompounding can be
divided into two categories: monolingual and bilin-
gual approaches.
Brown (2002) describes a corpus-driven approach
for splitting compounds in a German-English trans-
lation task derived from a medical domain. A large
proportion of the tokens in both texts are cognates
with a Latin or Greek etymological origin. While the
English text keeps the cognates as separate tokens,
they are combined into compounds in the German
text. To split these compounds, the author compares
both the German and the English cognates on a char-
acter level to find reasonable split points. The algo-
rithm described by the author consists of a sequence
of if-then-else conditions that are applied on the two
cognates to find the split points. Furthermore, since
the method relies on finding similar character se-
quences between both the source and the target to-
kens, the approach is restricted to cognates and can-
not be applied to split more complex compounds.
Koehn and Knight (2003) present a frequency-
based approach to compound splitting for German.
The compound parts and their frequencies are es-
timated from a monolingual corpus. As an exten-
sion to the frequency approach, the authors describe
a bilingual approach where they use a dictionary ex-
tracted from parallel data to find better split options.
The authors allow only two linking morphemes be-
tween compound parts and a few letters that can be
dropped. In contrast to our approach, those opera-
tions are not learned automatically, but must be pro-
vided explicitly.
Garera and Yarowsky (2008) propose an approach
to translate compounds without the need for bilin-
gual training texts. The compound splitting pro-
cedure mainly follows the approach from (Brown,
2002) and (Koehn and Knight, 2003), so the em-
phasis is put on finding correct translations for com-
pounds. To accomplish this, the authors use cross-
language compound evidence obtained from bilin-
gual dictionaries. In addition, the authors describe a
simple way to learn glue characters by allowing the
deletion of up to two middle and two end charac-
ters.1 More complex morphological operations are
not taken into account.
Alfonseca et al (2008b) describe a state-of-the-
art German compound splitter that is particularly ro-
bust with respect to noise and spelling errors. The
compound splitter is trained on monolingual data.
Besides applying frequency and probability-based
methods, the authors also take the mutual informa-
tion of compound parts into account. In addition, the
1However, the glue characters found by this procedure seem
to be biased for at least German and Albanian. A very frequent
glue morpheme like -es- is not listed, while glue morphemes
like -k- and -h- rank very high, although they are invalid glue
morphemes for German. Albanian shows similar problems.
1397
authors look for compound parts that occur in dif-
ferent anchor texts pointing to the same document.
All these signals are combined and the weights are
trained using a support vector machine classifier. Al-
fonseca et al (2008a) apply this compound splitter
on various other Germanic languages.
Dyer (2009) applies a maximum entropy model
of compound splitting to generate segmentation lat-
tices that serve as input to a translation system.
To train the model, reference segmentations are re-
quired. Here, we produce only single best segmen-
tations, but otherwise do not rely on reference seg-
mentations.
3 Compound Splitting Algorithm
In this section, we describe the underlying optimiza-
tion problem and the algorithm used to split a token
into its compound parts. Starting from Bayes' de-
cision rule, we develop the Bellman equation and
formulate a dynamic programming-based algorithm
that takes a word as input and outputs the constituent
compound parts. We discuss the procedure used to
extract compound parts from monolingual texts and
to learn themorphological operations using bilingual
corpora.
3.1 Decision Rule for Compound Splitting
Given a token w = c1, . . . , cN = cN1 consisting of a
sequence of N characters ci, the objective function
is to find the optimal number K? and sequence of split
points n?K?0 such that the subwords are the constituents
of the token, where2 n0 := 0 and nK := N :
w = cN1 ? (K?, n?K?0 ) =
= argmax
K,nK0
{
Pr(cN1 ,K, nK0 )
}
(1)
= argmax
K,nK0
{
Pr(K) ? Pr(cN1 , nK0 |K)
}
u argmax
K,nK0
{
p(K) ?
K
?
k=1
p(cnknk?1+1, nk?1|K)?
?p(nk|nk?1,K)} (2)
with p(n0) = p(nK |?) ? 1. Equation 2 requires that
token w can be fully decomposed into a sequence
2For algorithmic reasons, we use the start position 0 to rep-
resent a fictitious start symbol before the first character of the
word.
of lexemes, the compound parts. Thus, determin-
ing the optimal segmentation is sufficient for finding
the constituents. While this may work for some lan-
guages, the subwords are not valid words in general
as discussed in Section 1.1.3. Therefore, we allow
the lexemes to be the result of a transformation pro-
cess, where the transformed lexemes are denoted by
gK1 . This leads to the following refined decision rule:
w = cN1 ? (K?, n?K?0 , g?K?1 ) =
= argmax
K,nK0 ,gK1
{
Pr(cN1 ,K, nK0 , gK1 )
}
(3)
= argmax
K,nK0 ,gK1
{
Pr(K) ? Pr(cN1 , nK0 , gK1 |K)
}
(4)
u argmax
K,nK0 ,gK1
{
p(K) ?
K
?
k=1
p(cnknk?1+1, nk?1, gk|K)
? ?? ?
compound part probability
?
? p(nk|nk?1,K)
}
(5)
The compound part probability is a zero-order
model. If we penalize each split with a constant split
penalty ?, and make the probability independent of
the number of splits K, we arrive at the following
decision rule:
w = cN1 ? (K?, n?K?1 , g?K?1 )
= argmax
K,nK0 ,gK1
{
?K ?
K
?
k=1
p(cnknk?1+1, nk?1, gk)
}
(6)
3.2 Dynamic Programming
We use dynamic programming to find the optimal
split sequence. Each split infers certain costs that
are determined by a cost function. The total costs of
a decomposed word can be computed from the in-
dividual costs of the component parts. For the dy-
namic programming approach, we define the follow-
ing auxiliary function Q with nk = j:
Q(cj1) = max
nk0 ,gk1
{
?k ?
k
?
?=1
p(cn?n??1+1, n??1, g?)
}
that is, Q(cj1) is equal to the minimal costs (maxi-
mum probability) that we assign to the prefix string
cj1 where we have used k split points at positions nk1 .
This yields the following recursive equation:
Q(cj1) = maxnk,gk
{
? ? Q(cnk?11 )?
? p(cnknk?1+1, nk?1, gk)
}
(7)
1398
Algorithm 1 Compound splitting
Input: input word w = cN1
Output: compound parts
Q(0) = 0
Q(1) = ? ? ? = Q(N) = ?
for i = 0, . . . , N ? 1 do
for j = i + 1, . . . , N do
split-costs = Q(i) + cost(cji+1, i, gj) +
split-penalty
if split-costs < Q(j) then
Q(j) = split-costs
B(j) = (i, gj)
end if
end for
end for
with backpointer
B(j) = argmax
nk,gk
{
? ? Q(cnk?11 )?
? p(cnknk?1+1, nk?1, gk)
}
(8)
Using logarithms in Equations 7 and 8, we can inter-
pret the quantities as additive costs rather than proba-
bilities. This yields Algorithm 1, which is quadratic
in the length of the input string. By enforcing that
each compound part does not exceed a predefined
constant length `, we can change the second for loop
as follows:
for j = i + 1, . . . ,min(i + `,N) do
With this change, Algorithm 1 becomes linear in the
length of the input word, O(|w|).
4 Cost Function and Knowledge Sources
The performance of Algorithm 1 depends on
the cost function cost(?), that is, the probability
p(cnknk?1+1, nk?1, gk). This cost function incorpo-
rates knowledge about morpheme transformations,
morpheme positionswithin a compound part, and the
compound parts themselves.
4.1 Learning Morphological Operations using
Phrase Tables
Let s and t be strings of the (source) language al-
phabet A. A morphological operation s/t is a pair
of strings s, t ? A?, where s is replaced by t. With
the usual definition of the Kleene operator ?, s and
t can be empty, denoted by ?. An example for such
a pair is ?/es, which models the linking morpheme
es in the German compound Bundesagentur (federal
agency):
Bundesagentur ? Bund+ es+ Agentur .
Note that by replacing either s or t with ?, we can
model insertions or deletions of morphemes. The
explicit dependence on position nk?1 in Equation 6
allows us to determine if we are at the beginning,
in the middle, or at the end of a token. Thus, we
can distinguish between start, middle, or end mor-
phemes and hypothesize them during search.3 Al-
though not explicitly listed in Algorithm 1, we dis-
allow sequences of linking morphemes. This can
be achieved by setting the costs to infinity for those
morpheme hypotheses, which directly succeed an-
other morpheme hypothesis.
To learn the morphological operations involved
in compounding, we determine the differences be-
tween a compound and its compound parts. This can
be done by computing the Levenshtein distance be-
tween the compound and its compound parts, with
the allowable edit operations being insertion, dele-
tion, or substitution of one or more characters. If we
store the current and previous characters, edit opera-
tion and the location (prefix, infix or suffix) at each
position during calculation of the Levenshtein dis-
tance then we can obtain the morphological opera-
tions required for compounding. Applying the in-
verse operations, that is, replacing twith s yields the
operation required for decompounding.
4.1.1 Finding Compounds and their Parts
To learn the morphological operations, we need
compounds together with their compound parts. The
basic idea of finding compound candidates and their
compound parts in a bilingual setting are related to
the ideas presented in (Garera and Yarowsky, 2008).
Here, we use phrase tables rather than dictionaries.
Although phrase tablesmight containmore noise, we
believe that overall phrase tables cover more phe-
nomena of translations thanwhat can be found in dic-
tionaries. The procedure is as follows. We are given
a phrase table that provides translations for phrases
from a source language l into English and from En-
glish into l. Under the assumption that English does
not contain many closed compounds, we can search
3We jointly optimize over K and the split points nk, so we
know that cnKnK?1 is a suffix of w.
1399
the phrase table for those single-token source words
f in language l, which translate into multi-token En-
glish phrases e1, . . . , en for n > 1. This results
in a list of (f ; e1, . . . , en) pairs, which are poten-
tial compound candidates together with their English
translations. If for each pair, we take each token ei
from the English (multi-token) phrase and lookup
the corresponding translation for language l to get
gi, we should find entries that have at least some
partial match with the original source word f , if f
is a true compound. Because the translation phrase
table was generated automatically during the train-
ing of a multi-language translation system, there is
no guarantee that the original translations are cor-
rect. Thus, the bilingual extraction procedure is
subject to introduce a certain amount of noise. To
mitigate this, thresholds such as minimum edit dis-
tance between the potential compound and its parts,
minimum co-occurrence frequencies for the selected
bilingual phrase pairs and minimum source and tar-
get word lengths are used to reduce the noise at the
expense of finding fewer compounds. Those entries
that obey these constraints are output as triples of
form:
(f ; e1, . . . , en; g1, . . . , gn) (9)
where
? f is likely to be a compound,
? e1, . . . , en is the English translation, and
? g1, . . . , gn are the compound parts of f .
The following example for German illustrates the
process. Suppose that the most probable translation
for?berweisungsbetrag is transfer amount using the
phrase table. We then look up the translation back to
German for each translated token: transfer translates
to?berweisung and amount translates toBetrag. We
then calculate the distance between all permutations
of the parts and the original compound and choose
the one with the lowest distance and highest transla-
tion probability: ?berweisung Betrag.
4.2 Monolingual Extraction of Compound
Parts
The most important knowledge source required for
Algorithm 1 is a word-frequency list of compound
parts that is used to compute the split costs. The
procedure described in Section 4.1.1 is useful for
learning morphological operations, but it is not suffi-
cient to extract an exhaustive list of compound parts.
Such lists can be extracted frommonolingual data for
which we use language model (LM) word frequency
lists in combination with some filter steps. The ex-
traction process is subdivided into 2 passes, one over
a high-quality news LM to extract the parts and the
other over a web LM to filter the parts.
4.2.1 Phase 1: Bootstrapping pass
In the first pass, we generate word frequency lists de-
rived from news articles for multiple languages. The
motivation for using news articles rather than arbi-
trary web texts is that news articles are in general
less noisy and contain fewer spelling mistakes. The
language-dependent word frequency lists are filtered
according to a sequence of filter steps. These filter
steps include discarding all words that contain digits
or punctuations other than hyphen, minimum occur-
rence frequency, and a minimum length which we
set to 4. The output is a table that contains prelim-
inary compound parts together with their respective
counts for each language.
4.2.2 Phase 2: Filtering pass
In the second pass, the compound part vocabulary
is further reduced and filtered. We generate a LM
vocabulary based on arbitrary web texts for each lan-
guage and build a compound splitter based on the vo-
cabulary list that was generated in phase 1. We now
try to split every word of the web LM vocabulary
based on the compound splitter model from phase
1. For the compound parts that occur in the com-
pound splitter output, we determine how often each
compound part was used and output only those com-
pound parts whose frequency exceed a predefined
threshold n.
4.3 Example
Suppose we have the following word frequencies
output from pass 1:
floor 10k poll 4k
flow 9k pot 5k
flower 15k potter 20k
In pass 2, we observe the word flowerpot. With the
above list, the only compound parts used are flower
and pot. If we did not split any other words and
threshold at n = 1, our final list would consist of
flower and pot. This filtering pass has the advantage
of outputting only those compound part candidates
1400
which were actually used to split words from web
texts. The thresholding also further reduces the risk
of introducing noise. Another advantage is that since
the set of parts output in the first pass may contain a
high number of compounds, the filter is able to re-
move a large number of these compounds by exam-
ining relative frequencies. In our experiments, we
have assumed that compound part frequencies are
higher than the compound frequency and so remove
words from the part list that can themselves be split
and have a relatively high frequency. Finally, after
removing the low frequency compound parts, we ob-
tain the final compound splitter vocabulary.
4.4 Generating Exception Lists
To avoid eager splitting of non-compounds and
named entities, we use a variant of the procedure de-
scribed in Section 4.1.1. By emitting all those source
words that translate with high probability into single-
token English words, we obtain a list of words that
should not be split.4
4.5 Final Cost Function
The final cost function is defined by the following
components which are combined log-linearly.
? The split penalty ? penalizes each compound
part to avoid eager splitting.
? The cost for each compound part gk is com-
puted as ? logC(gk), where C(gk) is the un-
igram count for gk obtained from the news LM
word frequency list. Since we use a zero-order
model, we can ignore the normalization and
work with unigram counts rather than unigram
probabilities.
? Because Algorithm 1 iterates over the charac-
ters of the input token w, we can infer from the
boundaries (i, j) if we are at the start, in the
middle, or at the end of the token. Applying
a morphological operation adds costs 1 to the
overall costs.
Although the cost function is language dependent,
we use the same split penalty weight ? = 20 for all
languages except for German, where the split penalty
weight is set to 13.5.
5 Results
To show the language independence of the approach
within a machine translation task, we translate from
languages belonging to different language families
into English. The publicly available Europarl corpus
is not suitable for demonstrating the utility of com-
pound splitting because there are few unseen com-
pounds in the test section of the Europarl corpus.
The WMT shared translation task has a broader do-
main compared to Europarl but covers only a few
languages. Hence, we present results for German-
English using the WMT-07 data and cover other lan-
guages using non-public corporawhich contain news
as well as open-domain web texts. Table 1 lists the
various corpus statistics. The source languages are
grouped according to their language family.
For learning the morphological operations, we al-
lowed the substitution of at most 2 consecutive char-
acters. Furthermore, we only allowed at most one
morphological substitution to avoid introducing too
much noise. The found morphological operations
were sorted according to their frequencies. Those
which occurred less than 100 times were discarded.
Examples of extracted morphological operations are
given in Table 2. Because the extraction procedure
described in Section 4.1 is not purely restricted to the
case of decompounding, we found that many mor-
phological operations emitted by this procedure re-
flect morphological variations that are not directly
linked to compounding, but caused by inflections.
To generate the language-dependent lists of com-
pound parts, we used language model vocabulary
lists5 generated from news texts for different lan-
guages as seeds for the first pass. These lists were
filtered by discarding all entries that either con-
tained digits, punctuations other than hyphens, or se-
quences of the same characters. In addition, the in-
frequent entries were discarded as well to further re-
duce noise. For the second pass, we used the lists
generated in the first pass together with the learned
morphological operations to construct a preliminary
compound splitter. We then generated vocabulary
lists for monolingual web texts and applied the pre-
liminary compound splitter onto this list. The used
4Because we will translate only into English, this is not an
issue for the introductory example flowerpot.
5The vocabulary lists also contain the word frequencies. We
use the term vocabulary list synonymously for a word frequency
list.
1401
Family Src Language #Tokens Train src/trg #Tokens Dev src/trg #Tokens Tst src/trg
Germanic Danish 196M 201M 43, 475 44, 479 72, 275 74, 504
German 43M 45M 23, 151 22, 646 45, 077 43, 777
Norwegian 251M 255M 42, 096 43, 824 70, 257 73, 556
Swedish 201M 213M 42, 365 44, 559 70, 666 74, 547
Hellenic Greek 153M 148M 47, 576 44, 658 79, 501 74, 776
Uralic Estonian 199M 244M 34, 987 44, 658 57, 916 74, 765
Finnish 205M 246M 32, 119 44, 658 53, 365 74, 771
Table 1: Corpus statistics for various language pairs. The target language is always English. The source languages are
grouped according to their language family.
Language morpholog. operations
Danish -/?, s/?
German -/?, s/?, es/?, n/?, e/?, en/?
Norwegian -/?, s/?, e/?
Swedish -/?, s/?
Greek ?/?, ?/?, ?/?, ?/?, ?/?, ?/?
Estonian -/?, e/?, se/?
Finnish ?/n, n/?, ?/en
Table 2: Examples of morphological operations that were
extracted from bilingual corpora.
compound parts were collected and sorted according
to their frequencies. Those which were used at least
2 times were kept in the final compound parts lists.
Table 3 reports the number of compound parts kept
after each pass. For example, the Finnish news vo-
cabulary list initially contained 1.7M entries. After
removing non-alpha and infrequent words in the first
filter step, we obtained 190K entries. Using the pre-
liminary compound splitter in the second filter step
resulted in 73K compound part entries.
The finally obtained compound splitter was in-
tegrated into the preprocessing pipeline of a state-
of-the-art statistical phrase-based machine transla-
tion system that works similar to the Moses de-
coder (Koehn et al, 2007). By applying the com-
pound splitter during both training and decoding we
ensured that source language tokens were split in
the same way. Table 4 presents results for vari-
ous language-pairs with and without decompound-
ing. Both the Germanic and the Uralic languages
show significant BLEU score improvements of 1.3
BLEU points on average. The confidence inter-
vals were computed using the bootstrap resampling
normal approximation method described in (Noreen,
1989). While the compounding process for Ger-
manic languages is rather simple and requires only a
few linking morphemes, compounds used in Uralic
languages have a richer morphology. In contrast to
the Germanic and Uralic languages, we did not ob-
serve improvements for Greek. To investigate this
lack of performance, we turned off transliteration
and kept unknown source words in their original
script. We analyzed the number of remaining source
characters in the baseline system and the system us-
ing compound splitting by counting the number of
Greek characters in the translation output. The num-
ber of remaining Greek characters in the translation
output was reduced from 6, 715 in the baseline sys-
tem to 3, 624 in the systemwhich used decompound-
ing. In addition, a few other metrics like the number
of source words that consisted of more than 15 char-
acters decreased as well. Because we do not know
how many compounds are actually contained in the
Greek source sentences6 and because the frequency
of using compounds might vary across languages,
we cannot expect the same performance gains across
languages belonging to different language families.
An interesting observation is, however, that if one
language from a language family shows performance
gains, then there are performance gains for all the
languages in that family. We also investigated the ef-
fect of not using any morphological operations. Dis-
allowing all morphological operations accounts for
a loss of 0.1 - 0.2 BLEU points across translation
systems and increases the compound parts vocabu-
lary lists by up to 20%, which means that most of the
gains can be achieved with simple concatenation.
The exception lists were generated according to
the procedure described in Section 4.4. Since we
aimed for precision rather than recall when con-
structing these lists, we inserted only those source
6Quite a few of the remaining Greek characters belong to
rare named entities.
1402
Language initial vocab size #parts after 1st pass #parts after 2nd pass
Danish 918, 708 132, 247 49, 592
German 7, 908, 927 247, 606 45, 059
Norwegian 1, 417, 129 237, 099 62, 107
Swedish 1, 907, 632 284, 660 82, 120
Greek 877, 313 136, 436 33, 130
Estonian 742, 185 81, 132 36, 629
Finnish 1, 704, 415 190, 507 73, 568
Table 3: Number of remaining compound parts for various languages after the first and second filter step.
System BLEU[%] w/o splitting BLEU[%] w splitting ? CI 95%
Danish 42.55 44.39 1.84 (? 0.65)
German WMT-07 25.76 26.60 0.84 (? 0.70)
Norwegian 42.77 44.58 1.81 (? 0.64)
Swedish 36.28 38.04 1.76 (? 0.62)
Greek 31.85 31.91 0.06 (? 0.61)
Estonian 20.52 21.20 0.68 (? 0.50)
Finnish 25.24 26.64 1.40 (? 0.57)
Table 4: BLEU score results for various languages translated into English with and without compound splitting.
Language Split source translation
German no Die EU ist nicht einfach ein Freundschaftsclub. The EU is not just a Freundschaftsclub.
yes Die EU ist nicht einfach ein Freundschaft Club The EU is not simply a friendship club.
Greek no ?? ????? ??????????? ??????????; What ??????????? configuration?
yes ?? ????? ????? ?????? ??????????; What is pulse code modulation?
Finnish no Lis?vuodevaatteet ja pyyheliinat ovat kaapissa. Lis?vuodevaatteet and towels are in the closet.
yes Lis? Vuode Vaatteet ja pyyheliinat ovat kaapissa. Extra bed linen and towels are in the closet.
Table 5: Examples of translations into English with and without compound splitting.
words whose co-occurrence count with a unigram
translation was at least 1, 000 and whose translation
probability was larger than 0.1. Furthermore, we re-
quired that at least 70%of all target phrase entries for
a given source word had to be unigrams. All decom-
pounding results reported in Table 4 were generated
using these exception lists, which prevented wrong
splits caused by otherwise overly eager splitting.
6 Conclusion and Outlook
We have presented a language-independent method
for decompounding that improves translations for
compounds that otherwise rarely occur in the bilin-
gual training data. We learned a set of morpholog-
ical operations from a translation phrase table and
determined suitable compound part candidates from
monolingual data in a two pass process. This al-
lowed us to learn morphemes and operations for lan-
guages where these lists are not available. In addi-
tion, we have used the bilingual information stored
in the phrase table to avoid splitting non-compounds
as well as frequent named entities. All knowledge
sources were combined in a cost function that was
applied in a compound splitter based on dynamic
programming. Finally, we have shown this improves
translation performance on languages from different
language families.
The weights were not optimized in a systematic
way but set manually to their respective values. In
the future, the weights of the cost function should be
learned automatically by optimizing an appropriate
error function. Instead of using gold data, the devel-
opment data for optimizing the error function could
be collected without supervision using the methods
proposed in this paper.
1403
References
Enrique Alfonseca, Slaven Bilac, and Stefan Paries.
2008a. Decompounding query keywords from com-
pounding languages. In Proc. of the 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL): Human Language Technologies (HLT), pages
253--256, Columbus, Ohio, USA, June.
Enrique Alfonseca, Slaven Bilac, and Stefan Paries.
2008b. German decompounding in a difficult corpus.
In A. Gelbukh, editor, Lecture Notes in Computer Sci-
ence (LNCS): Proc. of the 9th Int. Conf. on Intelligent
Text Processing and Computational Linguistics (CI-
CLING), volume 4919, pages 128--139. Springer Ver-
lag, February.
Ralf D. Brown. 2002. Corpus-Driven Splitting of Com-
poundWords. In Proc. of the 9th Int. Conf. on Theoret-
ical andMethodological Issues inMachine Translation
(TMI), pages 12--21, Keihanna, Japan, March.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proc. of
the Human Language Technologies (HLT): The An-
nual Conf. of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL), pages
406--414, Boulder, Colorado, June.
Nikesh Garera and David Yarowsky. 2008. Translating
Compounds by Learning Component Gloss Transla-
tion Models via Multiple Languages. In Proc. of the
3rd Internation Conference on Natural Language Pro-
cessing (IJCNLP), pages 403--410, Hyderabad, India,
January.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proc. of the 10th
Conf. of the European Chapter of the Association for
Computational Linguistics (EACL), volume 1, pages
187--193, Budapest, Hungary, April.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of the 44th
Annual Meeting of the Association for Computational
Linguistics (ACL), volume 1, pages 177--180, Prague,
Czech Republic, June.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proc. of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 311--318, Philadel-
phia, Pennsylvania, July.
1404
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 12?21,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
A Lightweight Evaluation Framework for Machine Translation Reordering
David Talbot1 and Hideto Kazawa2 and Hiroshi Ichikawa2
Jason Katz-Brown2 and Masakazu Seno2 and Franz J. Och1
1 Google Inc. 2 Google Japan
1600 Amphitheatre Parkway Roppongi Hills Mori Tower
Mountain View, CA 94043 6-10-1 Roppongi, Tokyo 106-6126
{talbot, och}@google.com {kazawa, ichikawa}@google.com
{jasonkb, seno}@google.com
Abstract
Reordering is a major challenge for machine
translation between distant languages. Recent
work has shown that evaluation metrics that
explicitly account for target language word or-
der correlate better with human judgments of
translation quality. Here we present a simple
framework for evaluating word order indepen-
dently of lexical choice by comparing the sys-
tem?s reordering of a source sentence to ref-
erence reordering data generated from manu-
ally word-aligned translations. When used to
evaluate a system that performs reordering as
a preprocessing step our framework allows the
parser and reordering rules to be evaluated ex-
tremely quickly without time-consuming end-
to-end machine translation experiments. A
novelty of our approach is that the translations
used to generate the reordering reference data
are generated in an alignment-oriented fash-
ion. We show that how the alignments are
generated can significantly effect the robust-
ness of the evaluation. We also outline some
ways in which this framework has allowed our
group to analyze reordering errors for English
to Japanese machine translation.
1 Introduction
Statistical machine translation systems can perform
poorly on distant language pairs such as English
and Japanese. Reordering errors are a major source
of poor or misleading translations in such systems
(Isozaki et al, 2010). Unfortunately the stan-
dard evaluation metrics used by the statistical ma-
chine translation community are relatively insensi-
tive to the long-distance reordering phenomena en-
countered when translating between such languages
(Birch et al, 2010).
The ability to rapidly evaluate the impact of
changes on a system can significantly accelerate the
experimental cycle. In a large statistical machine
translation system, we should ideally be able to ex-
periment with separate components without retrain-
ing the complete system. Measures such as per-
plexity have been successfully used to evaluate lan-
guage models independently in speech recognition
eliminating some of the need for end-to-end speech
recognition experiments. In machine translation,
alignment error rate has been used with some mixed
success to evaluate word-alignment algorithms but
no standard evaluation frameworks exist for other
components of a machine translation system (Fraser
and Marcu, 2007).
Unfortunately, BLEU (Papineni et al, 2001) and
other metrics that work with the final output of a ma-
chine translation system are both insensitive to re-
ordering phenomena and relatively time-consuming
to compute: changes to the system may require the
realignment of the parallel training data, extraction
of phrasal statistics and translation of a test set. As
training sets grow in size, the cost of end-to-end ex-
perimentation can become significant. However, it is
not clear that measurements made on any single part
of the system will correlate well with human judg-
ments of the translation quality of the whole system.
Following Collins et al (2005a) and Wang (2007),
Xu et al (2009) showed that when translating from
English to Japanese (and to other SOV languages
such as Korean and Turkish) applying reordering as
12
a preprocessing step that manipulates a source sen-
tence parse tree can significantly outperform state-
of-the-art phrase-based and hierarchical machine
translation systems. This result is corroborated by
Birch et al (2009) whose results suggest that both
phrase-based and hierarchical translation systems
fail to capture long-distance reordering phenomena.
In this paper we describe a lightweight framework
for measuring the quality of the reordering compo-
nents in a machine translation system. While our
framework can be applied to any translation sys-
tem in which it is possible to derive a token-level
alignment from the input source tokens to the out-
put target tokens, it is of particular practical interest
when applied to a system that performs reordering
as a preprocessing step (Xia and McCord, 2004). In
this case, as we show, it allows for extremely rapid
and sensitive analysis of changes to parser, reorder-
ing rules and other reordering components.
In our framework we evaluate the reordering pro-
posed by a system separately from its choice of tar-
get words by comparing it to a reference reordering
of the sentence generated from a manually word-
aligned translation. Unlike previous work (Isozaki
et al, 2010), our approach does not rely on the sys-
tem?s output matching the reference translation lexi-
cally. This makes the evaluation more robust as there
may be many ways to render a source phrase in the
target language and we would not wish to penalize
one that simply happens not to match the reference.
In the next section we review related work on
reordering for translation between distant language
pairs and automatic approaches to evaluating re-
ordering in machine translation. We then describe
our evaluation framework including certain impor-
tant details of how our reference reorderings were
created. We evaluate the framework by analyz-
ing how robustly it is able to predict improvements
in subjective translation quality for an English to
Japanese machine translation system. Finally, we
describe ways in which the framework has facili-
tated development of the reordering components in
our system.
2 Related Work
2.1 Evaluating Reordering
The ability to automatically evaluate machine trans-
lation output has driven progress in statistical ma-
chine translation; however, shortcomings of the
dominant metric, BLEU (Papineni et al, 2001) , par-
ticularly with respect to reordering, have long been
recognized (Callison-burch and Osborne, 2006).
Reordering has also been identified as a major fac-
tor in determining the difficulty of statistical ma-
chine translation between two languages (Birch et
al., 2008) hence BLEU scores may be most unreli-
able precisely for those language pairs for which sta-
tistical machine translation is most difficult (Isozaki
et al, 2010).
There have been many results showing that met-
rics that account for reordering are better correlated
with human judgements of translation quality (Lavie
and Denkowski, 2009; Birch and Osborne, 2010;
Isozaki et al, 2010). Examples given in Isozaki et
al. (2010) where object and subject arguments are
reversed in a Japanese to English statistical machine
translation system demonstrate how damaging re-
ordering errors can be and it should therefore not
come as a surprise that word order is a strong pre-
dictor of translation quality; however, there are other
advantages to be gained by focusing on this specific
aspect of the translation process in isolation.
One problem for all automatic evaluation metrics
is that multiple equally good translations can be con-
structed for most input sentences and typically our
reference data will contain only a small fraction of
these. Equally good translations for a sentence may
differ both in terms of lexical choice and word or-
der. One of the potential advantages of designing a
metric that looks only at word order, is that it may,
to some extent, factor out variability along the di-
mension of the lexical choice. Previous work on au-
tomatic evaluation metrics that focus on reordering,
however, has not fully exploited this.
The evaluation metrics proposed in Isozaki et al
(2010) compute a reordering score by comparing
the ordering of unigrams and bigrams that appear
in both the system?s translation and the reference.
These scores are therefore liable to overestimate
the reordering quality of sentences that were poorly
translated. While Isozaki et al (2010) does propose
13
a work-around to this problem which combines the
reordering score with a lexical precision term, this
clearly introduces a bias in the metric whereby poor
translations are evaluated primarily on their lexical
choice and good translations are evaluated more on
the basis of their word order. In our experience
word order is particularly poor in those sentences
that have the lowest lexical overlap with reference
translations; hence we would like to be able to com-
pute the quality of reordering in all sentences inde-
pendently of the quality of their lexical choice.
Birch and Osborne (2010) are closer to our ap-
proach in that they use word alignments to induce a
permutation over the source sentence. They com-
pare a source-side permutation generated from a
word alignment of the reference translation with one
generated from the system?s using various permuta-
tion distances. However, Birch and Osborne (2010)
only demonstrate that these metrics are correlated
with human judgements of translation quality when
combined with BLEU score and hence take lexical
choice into account.
Birch et al (2010) present the only results we
are aware of that compute the correlation be-
tween human judgments of translation quality and
a reordering-only metric independently of lexical
choice. Unfortunately, the experimental set-up there
is somewhat flawed. The authors ?undo? reorderings
in their reference translations by permuting the ref-
erence tokens and presenting the permuted transla-
tions to human raters. While many machine trans-
lation systems (including our own) assume that re-
ordering and translation can be factored into sepa-
rate models, e.g. (Xia and McCord, 2004), and per-
form these two operations in separate steps, the lat-
ter conditioned on the former, Birch et al (2010) are
making a much stronger assumption when they per-
form these simulations: they are assuming that lexi-
cal choice and word order are entirely independent.
It is easy to find cases where this assumption does
not hold and we would in general be very surprised
if a similar change in the reordering component in
our system did not also result in a change in the lex-
ical choice of the system; an effect which their ex-
periments are unable to model.
Another minor difference between our evaluation
framework and (Birch et al, 2010) is that we use
a reordering score that is based on the minimum
number of chunks into which the candidate and ref-
erence permutations can be concatenated similar to
the reordering component of METEOR (Lavie and
Denkowski, 2009). As we show, this is better cor-
related with human judgments of translation quality
than Kendall?s ? . This may be due to the fact that
it counts the number of ?jumps? a human reader has
to make in order to parse the system?s order if they
wish to read the tokens in the reference word order.
Kendall?s ? on the other hand penalizes every pair
of words that are in the wrong order and hence has
a quadratic (all-pairs) flavor which in turn might ex-
plain why Birch et al (2010) found that the square-
root of this quantity was a better predictor of trans-
lation quality.
2.2 Evaluation Reference Data
To create the word-aligned translations from which
we generate our reference reordering data, we used
a novel alignment-oriented translation method. The
method (described in more detail below) seeks
to generate reference reorderings that a machine
translation system might reasonably be expected to
achieve. Fox (2002) has analyzed the extent to
which translations seen in a parallel corpus can be
broken down into clean phrasal units: they found
that most sentence pairs contain examples of re-
ordering that violate phrasal cohesion, i.e. the cor-
responding words in the target language are not
completely contiguous or solely aligned to the cor-
responding source phrase. These reordering phe-
nomena are difficult for current statistical transla-
tion models to learn directly. We therefore deliber-
ately chose to create reference data that avoids these
phenomena as much as possible by having a single
annotator generate both the translation and its word
alignment. Our word-aligned translations are cre-
ated with a bias towards simple phrasal reordering.
Our analysis of the correlation between reorder-
ing scores computed on reference data created from
such alignment-oriented translations with scores
computed on references generated from standard
professional translations of the same sentences sug-
gests that the alignment-oriented translations are
more useful for evaluating a current state-of-the-art
system. We note also that while prior work has con-
jectured that automatically generated alignments are
a suitable replacement for manual alignments in the
14
context of reordering evaluation (Birch et al, 2008),
our results suggest that this is not the case at least for
the language pair we consider, English-Japanese.
3 A Lightweight Reordering Evaluation
We now present our lightweight reordering evalu-
ation framework; this consists of (1) a method for
generating reference reordering data from manual
word-alignments; and (2) a reordering metric for
scoring a sytem?s proposed reordering against this
reference data; and (3) a stand-alone evaluation tool.
3.1 Generating Reference Reordering Data
We follow Birch and Osborne (2010) in using ref-
erence reordering data that consists of permuations
of source sentences in a test set. We generate these
from word alignments of the source sentences to
reference translations. Unlike previous work, how-
ever, we have the same annotator generate both the
reference translation and the word alignment. We
also explicitly encourage the translators to generate
translations that are easy to align even if this does
result in occasionally unnatural translations. For in-
stance in English to Japanese translation we require
that all personal pronouns are translated; these are
often omitted in natural Japanese. We insist that
all but an extremely small set of words (articles and
punctuation for English to Japanese) be aligned. We
also disprefer non-contiguous alignments of a sin-
gle source word and require that all target words be
aligned to at least one source token. In Japanese
this requires deciding how to align particles that
mark syntactic roles; we choose to align these to-
gether with the content word (jiritsu-go) of the cor-
responding constituent (bunsetsu). Asking annota-
tors to translate and perform word alignment on the
same sentence in a single session does not necessar-
ily increase the annotation burden over stand-alone
word alignment since it encourages the creation of
alignment-friendly translations which can be aligned
more rapidly. Annotators need little special back-
ground or training for this task, as long as they can
speak both the source and target languages.
To generate a permutation from word alignments
we rank the source tokens by the position of the first
target token to which they are aligned. If multiple
source tokens are aligned to a single target word
or span we ignore the ordering within these source
spans; this is indicated by braces in Table 2. We
place unaligned source words immediately before
the next aligned source word or at the end of the
sentence if there is none. Table 2 shows the ref-
erence reordering derived from various translations
and word alignments.
3.2 Fuzzy Reordering Score
To evaluate the quality of a system?s reordering
against this reference data we use a simple reorder-
ing metric related to METEOR?s reordering compo-
nent (Lavie and Denkowski, 2009) . Given the refer-
ence permutation of the source sentence ?ref and the
system?s reordering of the source sentence ?sys ei-
ther generated directly by a reordering component or
inferred from the alignment between source and tar-
get phrases used in the decoder, we align each word
in ?sys to an instance of itself in ?ref taking the first
unmatched instance of the word if there is more than
one. We then define C to be the number chunks of
contiguously aligned words. If M is the number of
words in the source sentence then the fuzzy reorder-
ing score is computed as,
FRS(?sys, ?ref) = 1?
C ? 1
M ? 1
. (1)
This metric assigns a score between 0 and 1 where
1 indicates that the system?s reordering is identical
to the reference. C has an intuitive interpretation as
the number of times a reader would need to jump in
order to read the system?s reordering of the sentence
in the order proposed by the reference.
3.3 Evaluation Tool
While the framework we propose can be applied to
any machine translation system in which a reorder-
ing of the source sentence can be inferred from the
translation process, it has proven particularly use-
ful applied to a system that performs reordering as
a separate preprocessing step. Such pre-ordering
approaches (Xia and McCord, 2004; Collins et al,
2005b) can be criticized for greedily committing to
a single reordering early in the pipeline but in prac-
tice they have been shown to perform extremely well
on language pairs that require long distance reorder-
ing and have been successfully combined with other
more integrated reordering models (Xu et al, 2009).
15
The performance of a parser-based pre-ordering
component is a function of the reordering rules and
parser; it is therefore desirable that these can be eval-
uated efficiently. Both parser and reordering rules
may be evaluated using end-to-end automatic met-
rics such as BLEU score or in human evaluations.
Parsers may also be evaluated using intrinsic tree-
bank metrics such as labeled accuracy. Unfortu-
nately these metrics are either expensive to compute
or, as we show, unpredictive of improvements in hu-
man perceptions of translation quality.
Having found that the fuzzy reordering score pro-
posed here is well-correlated with changes in human
judgements of translation quality, we established a
stand-alone evaluation tool that takes a set of re-
ordering rules and a parser and computes the re-
ordering scores on a set of reference reorderings.
This has become the most frequently used method
for evaluating changes to the reordering component
in our system and has allowed teams working on
parsing, for instance, to contribute significant im-
provements quite independently.
4 Experimental Set-up
We wish to determine whether our evaluation frame-
work can predict which changes to reordering com-
ponents will result in statistically significant im-
provements in subjective translation quality of the
end-to-end system. To that end we created a num-
ber of systems that differ only in terms of reorder-
ing components (parser and/or reordering rules). We
then analyzed the corpus- and sentence-level corre-
lation of our evaluation metric with judgements of
human translation quality.
Previous work has compared either quite separate
systems, e.g. (Isozaki et al, 2010), or systems that
are artificially different from each other (Birch et al,
2010). There has also been a tendency to measure
corpus-level correlation. We are more interested in
comparing systems that differ in a realistic manner
from one another as would typically be required in
development. We also believe sentence-level cor-
relation is more important than corpus-level corre-
lation since good sentence-level correlation implies
that a metric can be used for detailed analysis of a
system and potentially to optimize it.
4.1 Systems
We carried out all our experiments using a state-of-
the-art phrase-based statistical English-to-Japanese
machine translation system (Och, 2003). Dur-
ing both training and testing, the system reorders
source-language sentences in a preprocessing step
using a set of rules written in the framework pro-
posed by (Xu et al, 2009) that reorder an English
dependency tree into target word order. During de-
coding, we set the reordering window to 4 words.
In addition to the regular distance distortion model,
we incorporate a maximum entropy based lexical-
ized phrase reordering model (Zens and Ney, 2006).
For parallel training data, we use an in-house collec-
tion of parallel documents. These come from vari-
ous sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. We trained our system on
about 300 million source words.
The reordering rules applied to the English de-
pendency tree define a precedence order for the chil-
dren of each head category (a coarse-grained part of
speech). For example, a simplified version of the
precedence order for child labels of a verbal head
HEADVERB is: advcl, nsubj, prep, [other children],
dobj, prt, aux, neg, HEADVERB, mark, ref, compl.
The dependency parser we use is an implementa-
tion of a transition-based dependency parser (Nivre,
2008). The parser is trained using the averaged per-
ceptron algorithm with an early update strategy as
described in Zhang and Clark (2008).
We created five systems using different parsers;
here targeted self-training refers to a training pro-
cedure proposed by Katz-Brown et al (2011) that
uses our reordering metric and separate reference re-
ordering data to pick parses for self-training: an n-
best list of parses is generated for each English sen-
tence for which we have reference reordering data
and the parse tree that results in the highest fuzzy
reordering score is added to our parser?s training set.
Parsers P3, P4 and P5 differ in how that framework
is applied and how much data is used.
? P1 Penn Treebank, perceptron, greedy search
? P2 Penn Treebank, perceptron, beam search
? P3 Penn Treebank, perceptron, beam search,
targeted self-training on web data
16
? P4 Penn Treebank, perceptron, beam search,
targeted self-training on web data
? P5 Penn Treebank, perceptron, beam search,
targeted self-training on web data, case insen-
sitive
We also created five systems using the fifth parser
(P5) but with different sets of reordering rules:
? R1 No reordering
? R2 Reverse reordering
? R3 Head final reordering with reverse reorder-
ing for words before the head
? R4 Head final reordering with reverse reorder-
ing for words after the head
? R5 Superset of rules from (Xu et al, 2009)
Reverse reordering places words in the reverse of the
English order. Head final reordering moves the head
of each dependency after all its children. Rules in R3
and R4 overlap significantly with the rules for noun
and verb subtrees respectively in R5. Otherwise all
systems were identical. The rules in R5 have been
extensively hand-tuned while R1 and R2 are rather
naive. System P5R5 was our best performing system
at the time these experiments were conducted.
We refer to systems by a combination of parser
and reordering rules set identifiers, for instance, sys-
tem P2R5, uses parser P2 with reordering rules R5.
We conducted two subjective evaluations in which
bilingual human raters were asked to judge trans-
lations on a scale from 0 to 6 where 0 indicates
nonsense and 6 is perfect. The first experiment
(Parsers) contrasted systems with different parsers
and the second (Rules) varied the reordering rules.
In each case three bilingual evaluators were shown
the source sentence and the translations produced by
all five systems.
4.2 Meta-analysis
We perform a meta-analysis of the following metrics
and the framework by computing correlations with
the results of these subjective evaluations of transla-
tion quality:
1. Evaluation metrics: BLEU score on final trans-
lations, Kendall?s ? and fuzzy reordering score
on reference reordering data
2. Evaluation data: both manually-generated and
automatically-generated word alignments on
both standard professional and alignment-
oriented translations of the test sentences
The automatic word alignments were generated us-
ing IBM Model 1 in order to avoid directional biases
that higher-order models such as HMMs have.
Results presented in square parentheses are 95
percent confidence intervals estimated by bootstrap
resampling on the test corpus (Koehn, 2004).
Our test set contains 500 sentences randomly
sampled from the web. We have both professional
and alignment-friendly translations for these sen-
tences. We created reference reorderings for this
data using the method described in Section 3.1.
The lack of a broad domain and publically avail-
able Japanese test corpus makes the use of this non-
standard test set unfortunately unavoidable.
The human raters were presented with the source
sentence, the human reference translation and the
translations of the various systems simultaneously,
blind and in a random order. Each rater was allowed
to rate no more than 3 percent of the sentences and
three ratings were elicited for each sentence. Rat-
ings were a single number between 0 and 6 where 0
indicates nonsense and 6 indicates a perfectly gram-
matical translation of the source sentence.
5 Results
Table 2 shows four reference reorderings generated
from various translations and word alignments. The
automatic alignments are significantly sparser than
the manual ones but in these examples the refer-
ence reorderings still seem reasonable. Note how the
alignment-oriented translation includes a pronoun
(translation for ?I?) that is dropped in the slightly
more natural standard translation to Japanese.
Table 1 shows the human judgements of transla-
tion quality for the 10 systems (note that P5R5 ap-
pears in both experiments but was scored differently
as human judgments are affected by which other
translations are present in an experiment). There is a
clear ordering of the systems in each experiment and
17
1. Parsers Subjective Score (0-6) 2. Rules Subjective Score (0-6)
P1R5 2.173 [2.086, 2.260] P5R1 1.258 [1.191, 1.325]
P2R5 2.320 [2.233, 2.407] P5R2 1.825 [1.746, 1.905]
P3R5 2.410 [2.321, 2.499] P5R3 1.849 [1.767, 1.931]
P4R5 2.453 [2.366, 2.541] P5R4 2.205 [2.118, 2.293]
P5R5 2.501 [2.413, 2.587] P5R5 2.529 [2.441, 2.619]
Table 1: Human judgements of translation quality for 1. Parsers and 2. Rules.
Metric Sentence-level correlation
r ?
Fuzzy reordering 0.435 0.448
Kendall?s ? 0.371 0.450
BLEU 0.279 0.302
Table 6: Pearson?s correlation (r) and Spearman?s rank
correlation (?) with subjective translation quality at
sentence-level.
we see that both the choice of parser and reordering
rules clearly effects subjective translation quality.
We performed pairwise significance tests using
bootstrap resampling for each pair of ?improved?
systems in each experiment. Tables 3, 4 and 5
shows which pairs were judged to be statistically
significant improvements at either 95 or 90 percent
level under the different metrics. These tests were
computed on the same 500 sentences. All pairs
but one are judged to be statistically significant im-
provements in subjective translation quality. Sig-
nificance tests performed using the fuzzy reorder-
ing metric are identical to the subjective scores for
the Parsers experiment but differ on one pairwise
comparison for the Rules experiment. According to
BLEU score, however, none of the parser changes
are significant at the 95 percent level and only one
pairwise comparison (between the two most differ-
ent systems) was significant at the 90 percent level.
BLEU score appears more sensitive to the larger
changes in the Rules experiment but is still in dis-
agreement with the results of the human evaluation
on four pairwise comparisons.
Table 6 shows the sentence-level correlation of
different metrics with human judgments of transla-
tion quality. Here both the fuzzy reordering score
and Kendall?s ? are computed on the reference
reordering data generated as described in Section
3.1. Both metrics are computed by running our
Translation Alignment Sentence-level
r ?
Alignment-oriented Manual 0.435 0.448
Alignment-oriented Automatic 0.234 0.252
Standard Manual 0.271 0.257
Standard Automatic 0.177 0.159
Table 7: Pearson?s correlation (r) and Spearman?s rank
correlation (?) with subjective translation quality at the
sentence-level for different types of reordering reference
data: (i) alignment-oriented translation vs. standard, (ii)
manual vs. automatic alignment.
lightweight evaluation tool and involve no transla-
tion whatsoever. These lightweight metrics are also
more correlated with subjective quality than BLEU
score at the sentence level.
Table 7 shows how the correlation between fuzzy
reordering score and subjective translation quality
degrades as we move from manual to automatic
alignments and from alignment-oriented translations
to standard ones. The automatically aligned refer-
ences, in particular, are less correlated with subjec-
tive translation scores then BLEU; we believe this
may be due to the poor quality of word alignments
for languages such as English and Japanese due to
the long-distance reordering between them.
Finally we present some intrinsic evaluation met-
rics for the parsers used in the first of our experi-
ments. Table 8 demonstrates that certain changes
may not be best captured by standard parser bench-
marks. While the first four parser models improve
on the WSJ benchmarks as they improve subjective
translation quality the best parser according to sub-
jective translation qualtiy (P5) is actually the worst
under both metrics on the treebank data. We con-
jecture that this is due to the fact that P5 (unlike the
other parsers) is case insensitive. While this helps us
significantly on our test set drawn from the web, it
18
Standard / ManualSource              How Can I Qualify For A Mortgage Tax Deduction ?Reordering        A Mortgage {{ Tax Deduction }} For I Qualify How Can ?Translation                        ?? ??? ?? ? ?? ? ?? ? ?? ? ? ?? ?? ? ?? ?? ? ?Alignment          6,6,7_8,4,3,3,3,3,3,0,0,0,0,0,1,1,9,9
Alignment-oriented / ManualSource              How Can I Qualify For A Mortgage Tax Deduction ?Reordering        I How A Mortgage {{ Tax Deduction }} For Qualify Can ?Translation                         ? ? ?? ? ?? ?? ??? ? ?? ? ?? ? ??? ?? ? ?? ?? ? ?Alignment         2,2,0,0,0,6,6,6,7_8,4,3,3,3,1,1,1,1,1,9
Standard / AutomaticSource              We do not claim to cure , prevent or treat any disease .Reordering        any disease cure , prevent or treat claim to We do not .Translation           ???? ?? ? ?? ,  ?? ,            ??? ?? ? ?? ?? ?? ? ? ?? ?? ? .Alignment         10,11,,5,6,7,,8,9,,,4,,,,2,2,2,12
Alignment-oriented / AutomaticSource            We do not claim to cure , prevent or treat any disease .Reordering        We any disease cure , prevent or treat claim to do not .Translation              ? ? ? ???? ?? ? ?? ,           ?? ???? ?? ? ?? ? ?? ? ?? ? .Alignment          0,0,,10,11,,5,6,7,8,9,,,,3,4,2,2,12
Table 2: Reference reordering data generated via various methods: (i) alignment-oriented vs. standard translation, (ii)
manual vs. automatic word alignment
Exp. 1 Parsers Exp. 2 Reordering Rules
P2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5
P1R5 +** +** +** +** P5R1 +** +** +** +**
P2R5 +** +** +** P5R2 0 +** +**
P3R5 +** P5R3 +** +**
P4R5 0 P5R4 +**
Table 3: Pairwise significance in subjective evaluation (0 = not significant, * = 90 percent, ** = 95 percent).
Exp. 1 Parsers Exp. 2 Reordering Rules
P2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5
P1R5 +** +** +** +** P5R1 0 +** +** +**
P2R5 +** +** +** P5R2 +** +** +**
P3R5 +** +** P5R3 +** +**
P4R5 0 P5R4 +**
Table 4: Pairwise significance in fuzzy reordering score (0 = not significant, * = 90 percent, ** = 95 percent).
Exp. 1 Parsers Exp. 2 Reordering Rules
P2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5
P1R5 0 0 +* +* P5R1 +** +** +** +**
P2R5 0 0 0 P5R2 0 +** +**
P3R5 0 0 P5R3 0 +*
P4R5 0 P5R4 0
Table 5: Pairwise significance in BLEU score (0 = not significant, * = 90 percent, ** = 95 percent).
19
Parser Labeled attachment POS accuracy
P1 0.807 0.954
P2 0.822 0.954
P3 0.827 0.955
P4 0.830 0.955
P5 0.822 0.944
Table 8: Intrinsic parser metrics on WSJ dev set.
Figure 1: P1 and P5?s parse trees and automatic reorder-
ing (using R5 ruleset) and fuzzy score.
hurts parsing performance on cleaner newswire.
6 Discussion
We have found that in practice this evaluation frame-
work is sufficiently correlated with human judg-
ments of translation quality to be rather useful for
performing detailed error analysis of our English-to-
Japanese system. We have used it in the following
ways in simple error analysis sessions:
? To identify which words are most frequently re-
ordered incorrectly
? To identify systematic parser and/or POS errors
? To identify the worst reordered sentences
? To evaluate individual reordering rules
Figures 1 and 2 show pairs of parse trees together
with their resulting reorderings and scores against
Figure 2: P1 and P5?s parse trees and automatic reorder-
ing (using R5 ruleset) and fuzzy score.
the reference. These are typical of the parser er-
rors that impact reordering and which are correctly
identified by our framework. In related joint work
(Katz-Brown et al, 2011) and (Hall et al, 2011), it
is shown that the framework can be used to optimize
reordering components automatically.
7 Conclusions
We have presented a lightweight framework for eval-
uating reordering in machine translation and demon-
strated that this is able to accurately distinguish sig-
nificant changes in translation quality due to changes
in preprocessing components such as the parser or
reordering rules used by the system. The sentence-
level correlation of our metric with judgements of
human translation quality was shown to be higher
than other standard evaluation metrics while our
evaluation has the significant practical advantage of
not requiring an end-to-end machine translation ex-
periment when used to evaluate a separate reorder-
ing component. Our analysis has also highlighted
the benefits of creating focused evaluation data that
attempts to factor out some of the phenomena found
in real human translation. While previous work has
provided meta-analysis of reordering metrics across
quite independent systems, ours is we believe the
first to provide a detailed comparison of systems
20
that differ only in small but realistic aspects such as
parser quality. In future work we plan to use the
framework to provide a more comprehensive analy-
sis of the reordering capabilities of a broad range of
machine translation systems.
References
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 327?332,
Uppsala, Sweden, July.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting success in machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 745?
754, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2009. A quantitative analysis of reordering phenom-
ena. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation, pages 197?205, Athens,
Greece, March.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for mt evaluation: evaluating reorder-
ing. Machine Translation, 24:15?26, March.
Chris Callison-burch and Miles Osborne. 2006. Re-
evaluating the role of bleu in machine translation re-
search. In In EACL, pages 249?256.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005a. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 531?540, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005b. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 531?540, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 304?3111, July.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Comput. Linguist., 33:293?303, September.
Keith Hall, Ryan McDonald, and Jason Katz-Brown.
2011. Training dependency parsers by jointly optimiz-
ing multiple objective functions. In Proc. of EMNLP
2011.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic evalu-
ation of translation quality for distant language pairs.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 944?
952, Cambridge, MA, October. Association for Com-
putational Linguistics.
Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz
Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno,
and Hideto Kazawa. 2011. Training a Parser for Ma-
chine Translation Reordering. In Proc. of EMNLP
2011.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395.
Alon Lavie and Michael J. Denkowski. 2009. The me-
teor metric for automatic evaluation of machine trans-
lation. Machine Translation, 23(2-3):105?115.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513?553.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ?03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Chao Wang. 2007. Chinese syntactic reordering for
statistical machine translation. In In Proceedings of
EMNLP, pages 737?745.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the 20th international con-
ference on Computational Linguistics, COLING ?04,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
245?253, Boulder, Colorado, June.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings of the Workshop on Statistical Machine
Translation, pages 55?63.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
21
