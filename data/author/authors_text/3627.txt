Building a New Internet Chat System for Sharing Timing Information  
Kanayo Ogura 
 
School of 
knowledge Science, 
Japan Advanced Institute of 
Science and Technology 
 
1-1 Asahidai,Tatsunokuchi, 
Nomi, Ishikawa, 
923-1292, Japan 
 
k-ogura@jaist.ac.jp 
Takeshi Masuda 
 
NEC Informatec Systems, 
Ltd. 
 
2561,Shinohara, 
Kohoku, Yokohama, 
 Kanagawa, 
222-0026, Japan 
 
t-masuda@ 
qg.jp.nec.com 
Masato Ishizaki 
 
Institute Socio-Information 
and Communication Studies, 
The Univ. of Tokyo 
 
 
7-3-1 Hongo, Bunkyo-ku, 
Tokyo,  
113-0033, Japan 
 
ishizaki@isics.u-
tokyo.ac.jp 
Abstract 
Chat system has gained popularity as a 
tool for real-time conversation. How-
ever, standard chat systems have prob-
lems due to lack of timing information. 
To tackle this problem, we have built a 
system which has the following func-
tions: 1) function of making typing state 
visible; 2) floor holding function at the 
start of typing. The evaluation results 
show that the sys-tem with each new 
function significantly increases the 
number of turns, which indicates the ef-
fectiveness of the new functions for 
smooth communication. The survey re-
sults showed that the system with the 
function of making typing state visible 
significantly different from that without 
them concerning 1) easiness of adjust-
ing the timing of utterances and 
smoothness of conversations, and 2) 
easiness of using the system. 
1 Introduction 
2 
2.1 
Use of computer communication tools is now 
indispensable to our everyday life because of the 
proliferation of computer network. These com-
munication tools include E-mail, BBS (Bulletin 
Board System) and chat systems. Among all, 
chat system users have been increasing dramati-
cally for its real  time nature. Despite of its popu-
larity, standard chat systems do not allow users 
to share timing information, which is thought to 
be necessary for smooth communication. This 
often makes chat conversations confusing such 
as ones with a lot of repetitions and corrections. 
To tackle the problem of lack of timing in-
formation, we have implemented a system which 
has the following functions: 1) function of mak-
ing typing state visible; 2) floor holding function 
at the start of typing.  
To evaluate the effectiveness of the system, 
the length of utterance and the number of utter-
ances are used as quantitative index for smooth 
communication. We also conducted question-
naire surveys of users' evaluation of the system 
from effective-ness of the new functions to easi-
ness of using the system. 
The rest of the paper is organized as follows. 
Section two explains the problems of standard 
chat systems and related studies to tackle them. 
Section three describes our implemented new 
system with explanatory examples. Section four 
shows the effectiveness of our new system by 
quantitative evaluation results. Section five con-
cludes with some final remarks and our further 
attempt to improve the system. 
 
Previous Work 
Problems of standard chat systems 
In chat conversations, even if no message ap-
pears on the screen, this does not mean other us-
ers are typing a message. Other users might be 
reading or waiting for the others' message or be 
leaving the computer. This is due to the mecha-
nism of standard chat systems. In standard chat 
systems, a user sends a message by pressing the 
return key. This means that what users know is 
only complete utterances, without the informa-
tion on how the utterances are made: In face-to-
face conversation, the participants sometimes 
signal the difficulty of making utterances by in-
serting fillers and pauses, but in chat conversa-
tion, the participants cannot send such kind of 
information. The lack of this process information 
has been known to cause phenomena similar to 
overlap in face-to-face conversation and interrup-
tion [1,2]. Figure 1 shows an example of this 
overlap-like phenomenon. 
 
----------------------------------------------------------- 
1:A>I?m going to visit a company tomorrow. 
2:B>You are going to Osaka, aren?t you? 
3:A>So, how is the Job interview of T Com-
pany going? 
4:A>Yes, I?m going to Osaka. 
5:B>I have to submit data. So ? 
----------------------------------------------------------- 
Figure 1: Example of overlap-like phenomena 
 
In Figure 1, Speaker A talked about the visit to 
some company (utterance 1) and speaker B 
checked where A would go in response to utter-
ance 1. At almost the same time of B's response, 
A sent his message about the job interview (ut-
terance 3), which made adjacent turns semanti-
cally irrelevant. This overlap-like phenomenon 
can be escaped if at least A knows B is typing a 
response to A. 
 
2.2 
2.3 
Communicating the information on how 
the utterances are made 
MSN messenger [3] shows whether the partici-
pants are typing at the bottom of the system win-
dow. Tangible Chat [5] communicates the state 
of the other user's typing using vibration of the 
cushions. When a user starts to type, the other 
user's chair cushion vibrates, which enables users 
to share typing information without distracting 
their attention to the messages. In UNIX talk pro-
gram (a little old chat system), users can send a 
message character by character, which allows 
them to know what the others are doing. 
Alternative Interfaces for Chat realized two 
proto-type systems; Status Client for sharing 
status information; Flow Client for sharing time 
sequence information [4]. Status client imple-
mented the following functions for sharing user?s 
status information. 
 
? When a user presses a key, his or her name in 
the participants list is highlighted. 
? A user?s last utterance appears next to his or 
her name in the participants list. 
? When a user starts to write a message, it ap-
pears in gray color next to his or her name in 
the participants list. 
 
Flow Client improved the following floor hold-
ing function so that slow typists can easily join 
conversation. 
 
? A user has own track on screen 
?  Visualization of participants' character and 
timing information 
? Auto scrolling of the message history 
 
The design concept of our new system 
 
- User interface 
Many people have already used the current chat 
systems and got accustomed to the interface of 
the current system. This observation was con-
firmed in the evaluation of Status Clients and 
Flow clients [4]. Thus, we decided to make the 
interface of our new system similar to that of the 
current system. 
 
- The information the proposed system communi-
cates 
Our preliminary experiments confirmed that the 
information about whether the other participants 
are writing does not improve easiness of using 
the system. Based on this result, we decided to 
examine two approaches: one is to increase the 
information to be communicated. That is, as in 
the UNIX talk program, the system communi-
cates what the participants write in real time; The 
other is to focus on the floor holding function. In 
face-to-face conversation, the information on 
how the utterances are made is used for taking or 
holding the floor. Thus if the chat system users 
can take or hold the floor easily, this might con-
tribute to improving easiness of using the system. 
     With respect to the function of the floor hold-
ing, the former function might subsume the latter. 
However, this does not mean both functions are 
the same. Some users do not want to show the 
process of utterance making and even think it 
distracts their attention. If this is true, and the 
system can support the floor holding function 
effectively, then system does not have to com-
municate what the participants writes character 
by character, which will be examined in Section 
four. 
3 
3.1 
3.2 
The Implemented New Chat System 
We implemented a new system in which users 
can share the process information. An example 
of the system display is shown in Figure 2 and 3. 
A user sends messages and read conversation 
history in the main window (Figure 2), and rec-
ognizes the typing state of other users in the sub 
window (Figure 3). 
 
Function of making the typing state visi-
ble 
The new system can show the typing state of 
other users for all the time. When a user connects 
to the system, his or her nicknames appear in the 
sub window. Figure 3 shows the display where 
'miho' and 'yo' connected to the system and 
'miho' sent a message  "?? (Ishikawa)". Each 
time a user starts to write a message, a new text 
appears next to his or her name in the sub win-
dow. 
 
Floor holding function at the start of 
typing 
In face-to-face conversation, people monitor 
each other's behavior, which helps them take 
turns smoothly. But in the standard chat systems, 
a user cannot know each other's states, since s/he 
see only complete utterances without the infor-
mation on how the utterances are made. A user 
sometimes misses his or her turn because of this. 
The first user is writing a message in response to 
the second's, while the second user sends another 
message which is accepted by the system before 
the first user's response. To deal with this prob-
lem, the new system implemented a floor holding 
function at the start of writing a message. 
When a user starts to write a message, the system 
holds the place or turn in advance for the user, 
displaying [--- start to write a message ---] in the 
main window. Thus, utterances are displayed in 
the order of the time when a user starts to type. 
Users can send their messages without consider-
ing their typing speed. 
The system also allows users to stop sending 
a message in the middle. In this case, the system 
holds a line with the message [stop writing a 
message without sending] in gray color in the 
main window. This function is for showing the 
status or the activity of a user even when s/he 
does not send a message. 
 
 
 
 
 
 
 
 
Figure2: Main window of the system 
 
 
 
 
 
 
 
 
Figure3: Sub window of the system 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
----------------------------------------------------------- 
1:???????? 
take           hello 
 
2:???????? 
miho         hello 
 
3:?????????ALL 
yo               hello         ALL 
 
4:???????????? 
take     it is cold today, isn?t it? 
  
5:?????????????????
miho    [stop writing a message] 
 
6:????????? 
 yo            Yeah 
 
7:???????????????? 
miho   Where are you all come from ? 
 
8:???????????????? 
take   [--- start writing a message ---] 
 
9:??????? 
yo       I'm from Ishikawa 
 
10: ???????????????? 
miho  [--- start writing a message ---] 
 
11: ????????????????  
yo      [--- start writing a message ---] 
----------------------------------------------------------- 
Figure 4: English translation of the chat dialogue 
in the main window (Figure 2)  
 
 
4 
4.1 
Evaluation of the System 
Experimental Design 
Four experimental systems were built to evaluate 
the effectiveness of the system (Table 1) 
 
 
 
 
 
 
 
 
Function  System 
 Typing state  Floor holding 
system1 invisible at the end of typing 
system2 visible at the end of typing 
system3 invisible at the start of typing 
system4 visible at the start of typing 
Table1: Experimental systems 
 
 
In the experiments, eight groups, each of which 
consists of three subjects, engage in chat conver-
sation. All subjects are computer users at the in-
ter-mediate level or higher: they have 
experiences of using chat systems and no prob-
lems of typing. The task of the experiments is 
just to chat with each other about any topics for 
twenty minutes. 
 
4.2 Chat logs 
The average and the standard deviation of the 
number and the length of turns are shown in Ta-
ble 3 and 4, respectively. 
 
 
Ave.(SD)  B1 B2 
A1 16.83 (6.32)  25.33 (10.01)  
A2 24.42 (10.4)  22.00 (8.61)  
Table 2: Average number of the turns and its SD 
 
 
 
Ave. (SD)  B1 B2 
A1 17.82 (5.97) 16.48 (4.71) 
A2 17.70 (6.04) 17.98 (4.09) 
Table 3: Average length of the turns and its SD  
 
 
ble 4. 
 
We examined the effects of the functions by 
applying two-way ANOVA to the number and 
the length of turns. Factors and levels are 
summarized in Ta
 
 
 
 
 
 
 
 Level 1 Level 2 
Factor A? 
(typing state ) 
visible 
(A1) 
invisible 
(A2) 
Factor B? 
(floor holding) 
at the end 
of typing 
(B1) 
at the start 
of typing 
(B2) 
Table 4: Factor and Level for ANOVA 
 
 
Table 5 and 6 shows that main effects concerning 
both the number and the length of turns are not 
significant. But interaction effects concerning the 
number of turns are significant. The detailed 
analysis of the interaction effects showed that the 
system with each new function significantly in-
creases the number of turns. 
The previous work suggested two hypothe-
ses about the number and the length of turns 
[4][5]. 
 
 
(1) If users share the process information, this 
will prompt smoothness of communication 
which results in the increase of the number of   
turns. 
 
(2) If users use the function of floor holding at 
the start of writing a message, they will be able 
to write what they really want without consider-
ing time pressure. This makes the length of turns 
longer than that without using the function. 
 
Adopting these assumptions, the results here 
might indicate the effectiveness of the new func-
tions for smooth communication. The number of 
turns didn't increase when participants used the 
system with both functions. These functions are 
displayed on different windows and might dis-
tract user's attention. 
However, these assumptions can be said to be 
rather na?ve, since the number and the length of 
turns can be affected by many other factors like 
the familiarity between the participants and kinds 
of topics the participants happen to choose. Still 
we do not have good evaluation criteria for the 
functions of chat systems in general and thus the 
interpretation of our data against these assump-
tions should be used as corroboration to show the 
effectiveness of our system, but apparently not 
enough to prove it. Therefore, we conducted 
questionnaire survey to examine the usability of 
the system. 
 
 
 SS  d.f.  MS  F  
Factor A 54.19 1 54.19 0.62 
  Level B1 345.04 1 345.04. 3.92* 
  Level B2 66.67 1 66.67 0.76 
Factor B 111.02 1 111.02 1.26 
  Level A1 433.05 1 433.05 4.93** 
  Level A2 35.04 1 35.04 0.40 
Interaction 357.52 1 357.52 4.07** 
Error 87.95 44      1.99  
Total  47   
(*: level of significance 10% **: 5%) 
Table 5: ANOVA for the number of turns 
 
 
  SS d.f. MS F   
Factor A   5.70 1   5.70 0.19   
Factor B   3.38   1   3.38 0.11   
Interaction   7.48   1   7.84 0.26   
    Error 1332.47   44  30.28  1.00 
 Total     47   
Table 6: ANOVA for the length of turns 
 
 
4.3 Questionnaire survey 
 
 Item 1      Item  2    Item 3 
System 1 
System 2 
System 3 
System 4 
2.58           2.67        3.50 
1.92           3.67        3.75 
2.17           3.00        3.00 
1.58           3.92         4.00 
Item 1: smoothness of conversation 
Item 2: Easiness of adjusting the timing of 
making utterances 
Item 3: Easiness of using the system 
 
Table 7: Evaluation results of questionnaire sur-
vey 
 
Experimental subjects were asked to answer the 
questions such as effectiveness of the new func-
tions and the easiness of using the system on 
five-point scale. Table 7 shows the part of the 
results. 
The results showed that the system with the 
function of making typing state visible (factor A) 
gains significantly higher score than that without 
them concerning the smoothness of conversa-
tions, the easiness of adjusting the timing of ut-
terances and the easiness of using the system 
(Table 8,9 and 10). This suggests that the func-
tion of making typing state visible is effective in 
chat systems. 
 
 
  SS d.f. MS F 
Factor A 4.69 1 4.69 3.90* 
Factor B 1.69 1 1.69 1.41 
Interaction 0.02 1 0.02 0.02 
Error 1.20 44 0.03  
Total  47    
There are no great differences for all the sys-
tems and the ratio of system 4 is the lowest. The 
combination of functions might raise awareness 
for others' behavior, but the combination effects 
should be examined more thoroughly as future 
work 
 
 
(*: level of significance 10% ) 
Table 8: " The smoothness of conversations? 
 
 
  SS d.f. MS F 
Factor A 11.02 1 11.02 8.38** 
Factor B 1.02 1 1.02 0.78 
Interaction 0.02 1 0.02 0.02 
Error 1.32 44 0.03  
Total  47    
4.3%
6.4%
8.3%
2.7%
System1 4.3%
System2 6.4%
System3 8.3%
System4 2.7%
 Figure5: The ratio of overlap-like phenomena 
 
4.5 Semantically irrelevant turns in adjacent 
positions (**: level of significance 5%) 
Table 9: " Easiness of adjusting the timing of 
utterances " Table 11 shows examples of sequence change of 
turns. In the normal sequence of turns example, 
speaker A was able to respond to speaker B?s 
utterance 40, while in the sequence change of the 
turns example, speaker B?s utterance 42 wrongly 
preceded A?s response 41 to B?s utterance 40, 
which makes adjacent utterances 40 and 41 dis-
rupted. 
 
  SS d.f. MS F 
Factor A 4.69 1 4.69 5.83** 
Factor B 0.19 1 0.19 0.23 
Interaction 1.69 1 1.69 2.09 
Error 0.81 44 0.02  
Total  47    
 
[Sequence change of turns] 
(**:level of significance 5%) 
Table 10: " The easiness of using the system " 
4.4 The number of overlap-like phenomena 
The new system enables users to share the proc-
ess information. Theoretically this should de-
crease overlap-like phenomena observed in 
conversations using the standard chat systems.  
Uttr.No.   
40 B Smother the sliced meat and the 
flour separately 
41 B So it's difficult to cook it by my-
self 
42 A Ah I'm getting hungry somehow 
 
 
[Normal sequence of turns] Figure 5 shows the ratio of the overlap-like 
phenomena. It is difficult to find overlap-like 
phenomena from chat logs. Here we counted the 
number of the places which satisfies the follow-
ing  conditions: one is that the interval of utter-
ances should be short (three seconds here); The 
other is that the topic threads should be parallel. 
We used an algorithm proposed in [6] to extract 
topic threads.  
Uttr.No.   
40 B Smother the sliced meat and the 
flour separately 
42 A Ah I'm getting hungry some-
how 
41 B So it's difficult to cook it by 
myself 
 
Table 11: Examples of sequence change of turns 
5 
sign. 
Conclusion and Further Study  
 
We built a new system for sharing the process in-
formation. The system has the following func-
tions: 1) function of making typing state visible; 
2) floor holding function at the start of typing. 
The evaluation results showed that the system 
with each function significantly increases the 
number of turns, which might indicate the effec-
tiveness of the new functions for smooth com-
munication. The survey results showed that the 
system with the function of making typing state 
visible gained significantly higher score than the 
system without it concerning easiness of adjust-
ing the timing of making utterances, smoothness 
of conversations, and easiness of using the sys-
tem.  
S ystem3
17.3%
82.7%
changing
other
 
S ystem4
13.%
86.7%
changing
other
 
This system was confirmed to be effective to 
the problems caused by the lack of time informa-
tion, but did not solve it completely. We tried 
another approach to solve one of the problems, 
semantically irrelevant turns in adjacent position. 
We implemented a function by which users ex-
plicitly specify a semantically relevant turn with 
its number and those relevant turns are displayed 
in the same color (Figure 8). We conducted 
questionnaire survey to verify the effectiveness 
of this function, but the results confirmed the 
usefulness of this function, but also the need for 
the improvement of the interface de
 Figure 7: The ratio of changing sequence of turns 
for system 3 and 4    
 
 
The number of sequence change of turns is ex-
pected to decrease when the system has the floor 
holding function. Based on the method proposed 
in [6], these turns were examined and their ratio 
was calculated shown in Figure 7. The results 
indicated that there are no great differences for 
both systems (changing sequence of turns existed 
at least 10% in both systems). The ratio of sys-
tem 4 is a little lower than that of system 3. This 
might be able to be interpreted that visibility of 
typing information can be one of the factors to 
decrease sequence of change of turns, but this 
need s to be examined in future work. 
 
 
 
Please type the number of a semantically
relevant turn to the current. 
 
Figure 8: Example of the Dialog Box for a new 
function 
 
 
 
  
  
  
  
  
  
 
References 
[1] Hosoma, H. (2000) What do people presup-
pose in chat conversations -Timing Structure 
of chat and speech conversations?in Okada, 
M., Mishima, H.and Sasaki, M. (eds.) Em-
bodiment and Computers, bit magazine, Kyo-
ritsu Publisher, Japan, pp.339-349. 
[2] Mizukami, E. and Migita, M. (2002) Order of 
Chat Conversations ? Study of Conversation 
Structure by Interval Analysis, Cognitive 
Studies: Bulletin of the Japanese Cognitive 
Science Society, Vol.9?No.1, pp.77-88? 
[3] MSN messenger ?
http://messenger.microsoft.com/? 
[4] Vronay, D., Smith, M., and Drucker, S. 
(1999) Alternative Interfaces for Chat, Proc. 
of the 12th Annual ACM Symposium on User 
Interface Software and Technology
?UIST99?? 
[5] Yamada, Y. Hirano, T. and Nishimoto, K. 
(2002) Tangible Chat: Communication of con-
versation situation awareness using a sense of 
touch in a key-board chat system, Tech. 
Report SIG-GW-43-18?Information Process-
ing Society of Japan, pp.103-108? 
[6] Ogura, K. and Ishizaki, M. (2002) The char-
acteristics analysis about the topic change in 
Chat Conversations ? Tech. Report SIG-
SLUD-A202-3, Japan 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 100?103,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Implicit Proposal Filtering
in Multi-Party Consensus-Building Conversations
Yasuhiro Katagiri
Future University ? Hakodate
katagiri@fun.ac.jp
Yosuke Matsusaka
National Institute of Advanced
Industrial Science and Technology
yosuke.matsusaka@aist.go.jp
Yasuharu Den
Chiba University
den@cogsci.l.chiba-u.ac.jp
Mika Enomoto
Tokyo University of Technology
menomoto@media.teu.ac.jp
Masato Ishizaki
The University of Tokyo
ishizaki@iii.u-tokyo.ac.jp
Katsuya Takanashi
Kyoto University
takanasi@ar.media.kyoto-u.ac.jp
Abstract
An attempt was made to statistically estimate
proposals which survived the discussion to
be incorporated in the final agreement in an
instance of a Japanese design conversation.
Low level speech and vision features of hearer
behaviors corresponding to aiduti, noddings
and gaze were found to be a positive pre-
dictor of survival. The result suggests that
non-linguistic hearer responses work as im-
plicit proposal filters in consensus building,
and could provide promising candidate fea-
tures for the purpose of recognition and sum-
marization of meeting events.
1 Introduction
Non-verbal signals, such as gaze, head nods, fa-
cial expressions and bodily gestures, play signif-
icant roles in the conversation organization func-
tions. Several projects have been collecting multi-
modal conversation data (Carletta et al, 2006) for
multi-party dialogues in order to develop techniques
for meeting event recognitions from non-verbal as
well as verbal signals. We investigate, in this paper,
hearer response functions in multi-party consensus-
building conversations. We focus particularly on the
evaluative aspect of verbal and non-verbal hearer re-
sponses. During the course of a consensus-building
discussion meeting, a series of proposals are put
on the table, examined, evaluated and accepted or
rejected. The examinations of proposals can take
the form of explicit verbal exchanges, but they can
also be implicit through accumulations of hearer
responses. Hearers would express, mostly uncon-
sciously for non-verbal signals, their interest and
positive appraisals toward a proposal when it is
introduced and is being discussed, and that these
hearer responses would collectively contribute to the
determination of final consensus making. The ques-
tion we address is whether and in what degree it is
possible and effective to filter proposals and estimate
agreement by using verbal and non-verbal hearer re-
sponses in consensus-building discussion meetings.
2 Multi-Party Design Conversation Data
2.1 Data collection
We chose multi-party design conversations for the
domain of our investigation. Different from a fixed
problem solving task with a ?correct? solution, par-
ticipants are given partially specified design goals
and engage in a discussion to come up with an agree-
ment on the final design plan. The condition of our
data collection was as follows:
Number of participants: six for each session
Arrangement: face-to-face conversation
Task: Proposal for a new mobile phone business
Role: No pre-determined role was imposed
A compact meeting archiver equipment, AIST-
MARC (Asano and Ogata, 2006), which can cap-
ture panoramic video and speaker-separated speech
streams, was used to record conversations (Fig. 1).
The data we examined consist of one 30 minutes
conversation conducted by 5 males and 1 female.
Even though we did not assign any roles, a chairper-
son and a clerk were spontaneously elected by the
participants at the beginning of the session.
100
Figure 1: AIST-MARC and a recording scene
2.2 Data Annotation
2.2.1 Clause units
In order to provide a clause level segmentation
of a multi-channel speech stream, we extended the
notion of ?clause units (CUs)?, originally developed
for analyzing spoken monologues in the Corpus of
Spontaneous Japanese (Takanashi et al, 2003), to
include reactive tokens (Clancy et al, 1996) and
other responses in spoken conversations. Two of the
authors who worked on the Corpus of Spontaneous
Japanese independently worked on the data and re-
solved the differences, which created 1403 CUs con-
sisting of 469 complete utterances, 857 reactive to-
kens, and 77 incomplete or fragmental utterances.
2.2.2 Proposal units
We developed a simple classification scheme of
discourse segments for multi-party consensus build-
ing conversations based on the idea of ?interaction
process analysis? (Bales, 1950).
Proposal: Presentation of new ideas and their eval-
uation. Substructure are often realized through
elaboration and clarification.
Summary: Sum up multiple proposals possibly
with their assessment
Orientation: Lay out a topic to be discussed and
signal a transition of conversation phases, initi-
ated mostly by the facilitator of the discussion
Miscellaneous: Other categories including opening
and closing segments
The connectivity between clause units, the content
of the discussion, interactional roles, relationship
with adjacent segments and discourse markers were
considered in the identification of proposal units.
Two of the authors, one worked on the Corpus of
Spontaneous Japanese and the other worked for the
Figure 2: Image processing algorithm
project of standardization of discourse tagging, in-
dependently worked on the data and resolved the
differences, which resulted in 19 proposals, 8 sum-
maries, 19 orientations and 2 miscellaneouses.
2.3 Core clause units and survived proposal
units
Core clause units (CUs) were selected, out of all the
clause units, based on whether the CUs have sub-
stantial content as a proposal. A CU was judged
as a core CU, when the annotator would find it ap-
propriate to express, upon hearing the CU, either an
approval or a disapproval to its content if she were
in the position of a participant of the conversation.
Three of the authors worked on the text data exclud-
ing the reactive tokens, and the final selection was
settled by majority decision. 35 core CUs were se-
lected from 235 CUs in the total of 19 proposal PUs.
Cohen?s kappa agreement rate was 0.894.
Survived proposal units (PUs) were similarly se-
lected, out of all the proposal units, based on
whether the PUs were incorporated in the final
agreement among all the participants. 9 survived
PUs were selected from 19 proposal PUs.
3 Feature Extraction of Hearer?s Behavior
For each clause unit (CU), verbal and non-verbal
features concerning hearer?s behavior were ex-
tracted from the audio and the video data.
3.1 Non-Verbal Features
We focused on nodding and gaze, which were ap-
proximated by vertical and horizontal head move-
ments of participants.
An image processing algorithm (Figure 2) was ap-
plied to estimate head directions and motions (Mat-
susaka, 2005). Figure 3 shows a sample scene and
the results of applying head direction estimation al-
gorithm.
101
Figure 3: Sample scene with image processing results.
The circles represent detected face areas, and the lines in
the circles represent head directions.
For each CU, the vertical and horizontal compo-
nents of head movements of 5 hearers were calcu-
lated for two regions, the region inside the CU and
the 1-sec region immediately after the CU. For each
of the two regions, the mean and the peak values and
the relative location, in the region, of the peak were
computed. These 12 non-verbal features were used
for the statistical modeling.
3.2 Verbal Features
Verbal features were extracted from the audio data.
For each CU, power values of 5 hearers were ex-
tracted for two regions, ?within? and ?after? CU, and
for each of the two regions, the mean and the peak
values and the relative location, in the region, of
the peak were computed. In addition to these ver-
bal features, we also used aiduti features of reactive
tokens (RTs). The percentage of the total duration
of RTs, the total number of RTs, and the number of
participants who produced an RT were computed in
?within? and ?after? regions for each of the CUs. A
total of 12 CU verbal features were used for the sta-
tistical modeling.
4 Experiments
4.1 Overview of the Algorithm
Statistical modeling was employed to see if it is pos-
sible to identify the proposal units (PUs) that are sur-
vived in the participants? final consensus. To this
end, we, first, find the dominant clause unit (CU) in
each PU, and, then, based on the verbal and non-
verbal features of these CUs, we classify PUs into
?survived? and ?non-survived.?
Table 1: The optimal model for finding core-CUs
Estimate
(Intercept) ?1.72
within/speech power/mean ?11.54
after/vertical motion/peak loc. ?4.25
after/speech power/mean 3.91
after/aiduti/percent 3.02
Table 2: Confusion matrix of core-CU prediction experi-
ment (precision = 0.50, recall = 0.086)
Predicted
Observed Non-core Core
Non-core 431 3
Core 32 3
4.2 Finding Dominant CUs
A logistic regression model was used to model the
coreness of CUs. A total of 24 verbal and non-verbal
features were used as explanatory variables. Since
the number of non-core CUs was much larger than
that of core CUs, down-sampling of negative in-
stances was performed. To obtain a reliable estima-
tion, a sort of Monte Carlo simulation was adopted.
A model selection by using AIC was applied for
the 35 core CUs and another 35 non-core CUs that
were re-sampled from among the set of 434 com-
plete and non-core CUs. This process was repeated
100 times, and the features frequently selected in
this simulation were used to construct the optimal
model. Table 1 shows the estimated coefficient for
the optimal model, and Table 2 shows the accu-
racy based on a leave-1-out cross validation. The
dominant CU in each PU was identified as the CU
600 700 800 900 1000 1100 1200
0.0
0.2
0.4
0.6
0.8
1.0
time[sec]
core
?CU 
likelih
ood
O P S S O O P S S S P P O S P OO0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0(0) (0) (1) (0) (0)(0)(0) (0) (0) (0) (0) (0) (1) (0) (0) (0) (0)(0)
Figure 4: The predicted coreness of CUs. Dominant CUs
were defined to be CUs with the highest coreness in each
of the PUs. Black and white dots are CUs labeled as core
and non-core.
102
Table 3: The optimal model for finding survived-PUs
Estimate
within/vertical motion/peak val. 3.96
within/speech power/mean ?27.76
after/speech power/peak val. 1.49
Table 4: Result of the survived-PU prediction (precision
= 0.83, recall = 0.44)
Predicted
Observed Non-survived Survived
Non-survived 37 1
Survived 4 5
with the highest predicted value in that PU. Figure 4
shows the predicted values for coreness.
4.3 Finding Survived PUs
The verbal and non-verbal features of the dominant
CUs of each of the PUs were used for the modeling
of the survived-PU prediction. Discriminant analy-
sis was utilized and a model selection was applied
for the 47 PUs. Table 3 shows the estimated coeffi-
cient for the optimal model, and Table 4 shows the
accuracy based on a leave-1-out cross validation.
5 Discussions
The results of our estimation experiments indicate
that the final agreement outcome of the discus-
sion can be approximately estimated at the proposal
level. Though it may not be easy to identify actual
utterances contributing to the agreement (core-CUs),
the dominant CUs in PUs were found to be effective
in the identification of survived-PUs. The prediction
accuracy of survived-PUs was about 89%, with the
chance level of 69%, whereas that of core-CUs was
about 92%, with the chance level of 86%.
In terms of hearer response features, intensity
of verbal responses (within/speech power/mean, af-
ter/speech power/mean), and immediate nodding re-
sponses (after/vertical motion/peak loc.) were the
most common contributing features in core-CU es-
timation. In contrast, occurrence of a strong aiduti
immediately after, rather than within, the core-
CU (after/speech power/peak val.), and a strong
nodding within the core-CU (within/vertical mo-
tion/peak val.) appear to be signaling support from
hearers to the proposal. It should be noted that iden-
tification of target hearer behaviors must be vali-
dated against manual annotations before these gen-
eralizations are established. Nevertheless, the re-
sults are mostly coherent with our intuitions on the
workings of hearer responses in conversations.
6 Conclusions
We have shown that approximate identification of
the proposal units incorporated into the final agree-
ment can be obtained through the use of statistical
pattern recognition techniques on low level speech
and vision features of hearer behaviors. The result
provides a support for the idea that hearer responses
convey information on hearers? affective and evalu-
ative attitudes toward conversation topics, which ef-
fectively functions as implicit filters for the propos-
als in the consensus building process.
Acknowledgments
The work reported in this paper was supported by Japan
Society for the Promotion of Science Grants-in-aid for
Scientific Research (B) 18300052.
References
F. Asano and J. Ogata. 2006. Detection and separation
of speech events in meeting recordings. In Proc. Inter-
speech, pages 2586?2589.
R. F. Bales. 1950. A set of categories for the analysis
of small group interaction. American Sociological Re-
view, 15:257?263.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M.
Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I.
McCowan, W. Post, D. Reidsma, and P.Wellner. 2006.
The AMI meeting corpus: A pre-announcement. In
Machine Learning for Multimodal Interaction, pages
28?39.
P. M. Clancy, S. A. Thompson, R. Suzuki, and H. Tao.
1996. The conversational use of reactive tokens in En-
glish, Japanese and Mandarin. Journal of Pragmatics,
26:355?387.
Y. Matsusaka. 2005. Recognition of 3 party conversation
using prosody and gaze. In Proc. Interspeech, pages
1205?1208.
K. Takanashi, T. Maruyama, K. Uchimoto, and H.
Isahara. 2003. Identification of ?sentence? in.
spontaneous Japanese: detection and modification of
clause boundaries. In Proc. ISCA & IEEE Workshop
on Spontaneous Speech Processing and Recognition,
pages 183?186.
103
