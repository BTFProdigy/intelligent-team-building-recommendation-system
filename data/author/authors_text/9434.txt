Identifying Real or Fake Articles: Towards better Language Modeling
Sameer Badaskar
School of Computer Science
Carnegie Mellon University
Pittsburgh PA, United States
sbadaska@cs.cmu.edu
Sachin Agarwal
School of Computer Science
Carnegie Mellon University
Pittsburgh PA, United States
sachina@cs.cmu.edu
Shilpa Arora
School of Computer Science
Carnegie Mellon University
Pittsburgh PA, United States
shilpaa@cs.cmu.edu
Abstract
The problem of identifying good features
for improving conventional language mod-
els like trigrams is presented as a classifica-
tion task in this paper. The idea is to use
various syntactic and semantic features ex-
tracted from a language for classifying be-
tween real-world articles and articles gener-
ated by sampling a trigram language model.
In doing so, a good accuracy obtained on the
classification task implies that the extracted
features capture those aspects of the lan-
guage that a trigram model may not. Such
features can be used to improve the exist-
ing trigram language models. We describe
the results of our experiments on the classi-
fication task performed on a Broadcast News
Corpus and discuss their effects on language
modeling in general.
1 Introduction
Statistical Language Modeling techniques attempt
to model language as a probability distribution
of its components like words, phrases and topics.
Language models find applications in classification
tasks like Speech Recognition, Handwriting Recog-
nition and Text Categorization among others. Con-
ventional language models based on n-grams ap-
proximate the probability distribution of a language
by computing probabilities of words conditioned on
previous n words as follows
P (s) ?
m?
i=1
p(wi|wi?n+1, . . . , wi?1) (1)
In most applications, lower order n-grams (such as
bigram or trigram) are used but they are an unre-
alistic approximation of the underlying language.
Higher order n-grams are desirable but they present
problems concerning data sparsity. On the other
hand, low order n-grams are incapable of represent-
ing other aspects of the language like the underlying
topics, topical redundancy etc. In order to build a
better language model, additional features have to
be augmented to the existing language model (e.g.
a trigram model) which capture those aspects of the
language that the trigram model does not. Now, one
way to test the goodness of a feature under consider-
ation is to use it in a framework like an exponential
model (Rosenfeld, 1997; Cai et al, 2000) and note
the improvement in perplexity. An alternative way
(Eneva et al, 2001) is as follows: Let L be the lan-
guage and L? be an approximation of the language
obtained by sampling the trigram language model.
Also, let X be a piece of text obtained from either L
or L?. Let y = h(f(X)) such that y = 1 if X ? L
and y = 0 if X ? L? where f(.) is the computed fea-
ture and h(.) is the hypothesis function (a classifier
like AdaBoost, SVM etc). If Pr[y = h(f(x))] is
found to be sufficiently high, it means that the fea-
ture f(x) is able to distinguish effectively between
the actual language L and the approximate language
L?. In other words, f(x) captures those features of
the language that are complementary to the ones
captured by the trigram model and therefore f(x)
is a good feature to augment the trigram language
model with.
The formalism explained previously can be inter-
preted as a classification task in-order to distinguish
817
between Real articles and Fake articles. Articles of
different lengths drawn at random from the Broad-
cast News Corpus (BNC)1 are termed as Real arti-
cles (from language L). Articles generated by sam-
pling the trigram model trained on the same corpus
are termed as Fake articles (language L?). These arti-
cles together form the training data for the classifier
to associate the features with the classification labels
(real or fake) where the features are computed from
the text. The features that give high classification ac-
curacy on the test set of articles are considered good
candidates for adding to the trigram model. Further-
more, the confidence that the classifier attaches to
a classification decision can be used to compute the
perplexity.
In this paper, a classification-task based formal-
ism is used to investigate the goodness of some new
features for language modeling. At the same time
features proposed in the previous literature on lan-
guage modeling are also revisited (Cai et al, 2000)
Section 2 discusses various syntactic and semantic
features used for the classification task, Section 3
gives details about the experiments conducted and
the classification results obtained and finally, Sec-
tion 4 concludes the paper by discussing the implica-
tions of the classification results on language model-
ing with pointers to improvements and future work.
2 Feature Engineering
To differentiate a real article from a fake one, the
empirical, syntactic and semantic characteristics of
a given article are used to compute the features for
the classification task. The various types of features
that were experimented are as follows:
2.1 Empirical Features
Empirical features are based on the statistical anal-
ysis of both the real and fake articles. They include
the count of uncommon pairs of words within an ar-
ticle, the ratio of perplexity of trigram and quadgram
models for a given article and the nature of the POS
tags that occur at the start and end of sentences in an
article.
1http://www.cs.cmu.edu/ roni/11761-s07/project/LM-train-
100MW.txt.gz
Ratio of Perplexity of trigram and quad-gram
models
Given an article, the ratio of its perplexity for a tri-
gram model to a quad-gram model is computed. The
trigram and quad-gram models are both trained on
the same BNC corpus. Both real and fake articles
would give a low perplexity score for the tri-gram
model but for the quad-gram model, real articles
would have significantly lower perplexity than the
fake articles. This implies that the ratio of trigram
to quad-gram perplexities would be lower for a fake
article than for a real article. In other words, this ra-
tio is similar to computing the likelihood ratio of an
article w.r.t the trigram and quad-gram models. The
histogram in Figure 1 shows a good separation in
the distribution of values of this feature for the real
and fake articles which indicates the effectiveness of
this feature. A quadgram language model is a better
approximation of real text than a trigram model and
by using this as a feature, we are able to demonstrate
the usefulness of the classification task as a method
for identifying good features for language modeling.
In the subsequent sections, we investigate other fea-
tures using this classification framework.
Figure 1: Histogram for the ratio of perplexities with
respect to Trigram and Quadgram Language models
over the training set
Count of uncommon pairs of words
Content words are the frequently occurring words in
the corpus excluding the stop-words. All the words
in corpus are ranked according to frequency of their
occurrence and content words are defined to be the
words with rank between 150 and 6500. A list of
common content word pairs (pairs of content words
818
atleast 5 words apart) is prepared from the real cor-
pus by sorting the list of content word pairs by their
frequency of occurrence and retaining those above a
certain threshold. For a given article, a list of content
word pairs is compared against this list and word
pairs not in this list form the set of uncommon word
pairs.
A real article is expected to have lesser num-
ber of uncommon content-word pairs than fake arti-
cles. When normalized by the total number of word
pairs, we get the probability of finding an uncom-
mon content-word pair in an article. This probabil-
ity is greater for fake articles than the real articles
and we use this probability as a feature for the clas-
sification task.
Start and End POS Tags
Certain POS tags are more probable than others to
appear at the beginning or end of a real sentence.
This characteristic of real text could be used as a
feature to distinguish real articles from fake. The
distribution of POS tags of the first and last words of
the sentences in an article is used as a feature. Our
experiments show that this feature had very little ef-
fect in the overall contribution to the classification
accuracy over the development set.
2.2 Syntactic Features
These features are derived from the parse struc-
ture of the sentence. It is hypothesized that real
sentences tend to be grammatical while the same
may not be the case for fake sentences. An objec-
tive measure of the grammaticality of a sentence
can be obtained by running it through a statisti-
cal parser. The log-likelihood score returned by
the parser can be used to judge the grammatical-
ity of a sentence and thus determine whether it
is fake or real. The Charniak Parser (Charniak,
2001; Charniak, 2005) was used for assessing the
grammaticality of the articles under test. Given
an article containing sentences S1, S2, . . . , SN with
lengths L1, L2, . . . , LN , we compute the parser log-
likelihood scores P (S1), P (S2), . . . , P (SN ). The
overall grammaticality score for an article is given
by
PGram =
?N
i=1 LiP (Si)?N
i=1 Li
(2)
The grammaticality score was normalized using the
average and standard deviation over the entire train-
ing set. This feature gave small improvement in
terms of classification accuracy. There may be sev-
eral reasons for this: (1) Our training data consisted
of spoken transcripts from a broadcast news corpus
whereas the Charniak Parser was trained on a differ-
ent domain (Wall Street Journal) and (2) The parser
was trained on mixed case text where as the data we
used was all upper case.
2.3 Semantic Features
Real articles contain sentences with correlated pairs
of content-words and sentences that are correlated
with each other. An article with such sentence/word
correlations is said to be semantically coherent. Ow-
ing to the use of only the short term word history for
computing the probability distribution of a language,
a trigram model fails to model semantic coherence
and we exploit this fact for the classification task.
Specifically, we intend to model both intra-sentence
and inter-sentence semantic coherence and use them
as features for classification.
Intra-sentence Coherence
To model the intra-sentence word correlations, we
use Yule?s Q-statistic (Eneva et al, 2001). The word
correlations are learned from the BNC corpus as
well as the fake corpus. The coherence score for
an article is defined as the sum of the correlations
between pairs of content words present in the arti-
cle. The coherence score for an article is normalized
by the total number of content-word pairs found in
the article. Since the trigram and quad-gram lan-
guage model can capture short distance coherences
well, coherences between distant words can be used
to differentiate between real and fake articles. The
Yule Q-statistic is calculated for every pair of con-
tent words, which are atleast 5 words apart within a
sentence, both in the real and fake corpus.
The articles are scored according to content word-
pair correlations learned from the real as well as
fake corpus. Each article is given two scores, one
for the word-pair correlations from real articles and
other for the word-pair correlations from fake arti-
cles. For a real article, the real word-pair correla-
tion score would be relatively higher compared to
the fake word-pair correlation score (and vice-versa
819
for a fake article).
Modeling Topical Redundancy (Inter-sentence
Coherence)
A characteristic of real articles is that they tend to
be cohesive in terms of the topic under discussion.
For example, a news-article about a particular event
(topic) would have several direct or indirect refer-
ences to the event. We interpret this as some sort
of a redundancy in terms of the information con-
tent which we term as Topical Redundancy. The
fake articles would not exhibit such a redundancy.
If a real article is transformed to another represen-
tation space where some form of truncation is ap-
plied, on transformation back to the original space,
the amount of information-loss may not be signif-
icant due to information redundancy. However, if
the same process is applied on a fake article, the
information-loss would be significant when trans-
formed back to the original space. We intend to ex-
ploit this fact for our classification task.
Let DW?N be an article represented in the form
of a matrix, where W is the article vocabulary and N
is the number of sentences in that article. Every term
of this matrix represents the frequency of occurrence
of a vocabulary word in a particular sentence. We
construct a sentence-sentence matrix as follows:
A = DTD (3)
We now transform A into the Eigen-space using Sin-
gular Value Decomposition (SVD) which gives
A = USUT (4)
Here, UN?N is the eigen-vector matrix and SN?N
is the diagonal eigen-value matrix. If we retain only
the top K eigen-values from S , we get the truncated
(lossy) form S?K?K . Thus the truncated form of A
i.e. A? is
A? = US?UT (5)
We believe that the information loss ? A?A? ?2
will not be significant in the case of real articles
since the topical redundancy is captured in a very
compact manner by the eigen-representation. How-
ever, in the case of a fake article, the loss is con-
siderable. For a real article, the matrix would be
less sparse than a fake article and so is the case for
the reconstructed matrix. Therefore, the statistics -
mean, median, minimum and maximum computed
from the reconstructed matrix have higher values for
real articles than a fake articles. We use these statis-
tics as features for classifying the article. Figure 2
show the histograms of the statistics computed from
the reconstructed matrix for the training set. As can
be seen, there is a good separation between the two
classes fake and real in all the cases. Using these
features increased the classification accuracy by a
significant amount as shown later. From another per-
spective, these features model the inter-sentence se-
mantic coherence (Deerwester et al, 1990) within an
article and this is consistent with our notion of topi-
cal redundancy as explained previously. The matrix
package developed by NIST (Hicklin et al, 2005)
was used for SVD.
3 Experimental Results
3.1 Data Distribution
The training data consisted of 1000 articles (500 real
and 500 fake) obtained from Broadcast News Cor-
pus (BNC) and the test set consisted of 200 articles
(100 real and 100 fake). Additionally, a develop-
ment dataset consisting of 200 articles and having
the same distribution as that of the test dataset was
used for tuning the parameters of the classifiers. To
ensure that the training and test data come from the
same article length distribution, the training data was
resampled to have the same percentage of articles of
a given length as in the test set. The article length
distribution for both the training(resampled) and test
datasets is shown in Tables 1 and 2.
3.2 Classifier
Classifiers like AdaBoost (Freund et al, 1999) and
Max-Entropy (Rosenfeld, 1997) models were used
for the classification task.
The number of iterations for AdaBoost was esti-
mated using 5-fold cross-validation. Given a sub-
set of features, Maxent classified 74.5% of the doc-
uments correctly compared to 82% for AdaBoost.
Therefore, Adaboost was chosen as the classifier for
further experiments.
820
(a) Mean (b) Median
(c) Minimum (d) Maximum
Figure 2: Histograms of topical redundancy features computed over the training set. In (b) , the median
values for the fake articles are close to zero and hence cannot be seen clearly.
3.3 Results and Discussion
We used two performance measures to evaluate our
model. First is the accuracy which measures the
number of articles correctly classified as real or fake
and the second measure is the log-probability that
the model assigns to the classification decision i.e. it
measures the confidence the model has in its classi-
fication. Table 3 shows our experimental results on
the syntactic, semantic and empirical features.
The combination of syntactic, semantic and em-
pirical features gave an accuracy of 91.5% with an
average log-likelihood of -0.22 on development data
set. The accuracy on the test dataset was 87% with
an average log-likelihood of -0.328.
4 Conclusions and Future Work
In this work, we have used a classification-task
based formalism for evaluating various syntactic,
semantic and empirical features with the objective
of improving conventional language models. Fea-
tures that perform well in the task of classifying
real and trigram-generated fake articles are useful
for augmenting the trigram model. Semantic fea-
tures, such as topical redundancy, model long-range
dependencies which are not captured by a trigram
language model. Therefore, the semantic features
contribute significantly to the classification task ac-
curacy. Additionally, linguistic resources such as
WordNet (WordNet, 1998) can be used to model
821
# Sentences
per article
# Real
Art.
# Fake
Art.
% Total
(Real &
Fake)
1 938 940 19.76
2 440 471 9.58
3 502 474 10.26
4 507 533 10.94
5 497 525 10.75
7 431 524 10.05
10 475 479 10.04
15 482 421 9.50
20 421 446 9.12
Table 1: Distribution of article lengths for training
dataset.
# Sentences
per article
# Real
Art.
# Fake
Art.
% Total
(Real &
Fake)
1 20 20 20
2 10 10 10
3 10 10 10
4 10 10 10
5 10 10 10
7 10 10 10
10 10 10 10
15 10 10 10
20 10 10 10
Table 2: Distribution of article lengths for test
dataset.
topical redundancy using synonyms and other inter-
word dependencies. The semantic features we ex-
plored assume a single underlying topic for an arti-
cle which may not be always true. An article can
be a representation of different topics and we aim to
explore this direction in future.
References
Can Cai, Larry Wasserman and Roni Rosenfeld.
2000. Exponential language models, logistic regres-
sion, and semantic coherence. Proceedings of the
NIST/DARPA Speech Transcription Workshop.
Eugene Charniak. 2001. Immediate-Head Parsing for
Language Models. Proceedings of 39th Annual Meet-
ing of the ACL, 124-131.
Feature Combination Classification
Accuracy
Avg. Log
Likelihood
Syntactic 60.5% -0.663
Semantic 79.5% -0.510
Empirical 83.0% -0.446
Semantic + Syntactic 80.0% -0.553
Semantic + Empirical 86.0% -0.410
Semantic + Syntactic +
Empirical
91.5% -0.220
Table 3: Performance of different features on the
development-set.
Eugene Charniak. 2005. ftp://ftp.cs.brown.edu/pub/ nl-
parser/parser05Aug16.tar.gz
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of Information Theory. John Wiley & Sons, New York.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of
Japanese Society for Artificial Intelligence, 41(6).
Elena Eneva, Rose Hoberman and Lucian Lita. 2001.
Learning within-sentence semantic coherence. Pro-
ceedings of the EMNLP 2001.
Yoav Freund and Robert E. Schapire. 1999. A short in-
troduction to boosting Journal of Japanese Society for
Artificial Intelligence, 14(5):771-780.
Joe Hicklin, Cleve Moler, Peter Webb. 2005.
http://math.nist.gov/javanumerics/jama/
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Roni Rosenfeld. 1997. A whole sentence maximum en-
tropy language model. In Proc. of the IEEE Workshop
on Automatic Speech Recognition and Understanding,
1997.
WordNet: An Electronic Lexical Database, ISBN-13:
978-0-262-06197-1.
822
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67?71,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Non-textual Event Summarization by Applying Machine Learning to
Template-based Language Generation
Mohit Kumar and Dipanjan Das and Sachin Agarwal and Alexander I. Rudnicky
Language Technologies Institute
Carnegie Mellon University, Pittsburgh, USA
mohitkum,dipanjan,sachina,air@cs.cmu.edu
Abstract
We describe a learning-based system that
creates draft reports based on observation
of people preparing such reports in a tar-
get domain (conference replanning). The
reports (or briefings) are based on a mix
of text and event data. The latter consist
of task creation and completion actions,
collected from a wide variety of sources
within the target environment. The report
drafting system is part of a larger learning-
based cognitive assistant system that im-
proves the quality of its assistance based
on an opportunity to learn from observa-
tion. The system can learn to accurately
predict the briefing assembly behavior and
shows significant performance improve-
ments relative to a non-learning system,
demonstrating that it?s possible to create
meaningful verbal descriptions of activity
from event streams.
1 Introduction
We describe a system for recommending items for
a briefing created after a session with a crisis man-
agement system in a conference replanning do-
main. The briefing system is learning-based, in
that it initially observes how one set of users cre-
ates such briefings then generates draft reports for
another set of users. This system, the Briefing
Assistant(BA), is part of a set of learning-based
cognitive assistants each of which observes users
and learns to assist users in performing their tasks
faster and more accurately.
The difference between this work from
most previous efforts, primarily based on text-
extraction approaches is the emphasis on learning
to summarize event patterns. This work also
differs in its emphasis on learning from user
behavior in the context of a task.
Report generation from non-textual sources has
been previously explored in the Natural Language
Generation (NLG) community in a variety of do-
mains, based on, for example, a database of events.
However, a purely generative approach is not suit-
able in our circumstances, as we want to summa-
rize a variety of tasks that the user is performing
and present a summary tailored to a target audi-
ence, a desirable characteristic of good briefings
(Radev and McKeown, 1998). Thus we approach
the problem by applying learning techniques com-
bined with a template-based generation system to
instantiate the briefing-worthy report items. The
task of instantiating the briefing-worthy items is
similar to the task of Content Selection (Duboue,
2004) in the Generation pipeline however our ap-
proach minimizes linguistic involvement. Our
choice of a template-based generative system was
motivated by recent discussions in the NLG com-
munity (van Deemter et al, 2005) about the prac-
ticality and effectiveness of this approach.
The plan of the paper is as follows. We describe
relevant work from existing literature in the next
section. Then, we provide brief system description
followed by experiments and results. We conclude
with a summary of the work.
2 Related Work
Event based summarization has been studied in the
summarization community. (Daniel et al, 2003)
described identification of sub-events in multiple
documents. (Filatova and Hatzivassiloglou, 2004)
mentioned the use of event-based features in ex-
tractive summarization and (Wu, 2006; Li et al,
2006) describe similar work based on events oc-
curring in text. However, unlike the case at hand,
all the work on event-based summarization used
text as source material.
Non-textual summarization has also been ex-
plored in the Natural Language Generation (NLG)
community within the broad task of generating
67
reports based on database of events in specific
domains such as medical (Portet et al, 2009),
weather (Belz, 2007), sports (Oh and Shrobe,
2008) etc. However, in our case we want to sum-
marize a variety of tasks that the user is perform-
ing and present a summary to an intended audi-
ence (as defined by a report request).
Recent advances in NLG research use statis-
tical approaches at various stages of processing
in the generation pipeline like content selection
(Duboue and McKeown, 2003; Barzilay and Lee,
2004), probabilistic generation rules (Belz, 2007).
Our proposed approach differs from these in that
we apply machine learning after generation of all
the templates, as a post-processing step, to rank
them for inclusion in the final briefing. We could
have used a general purpose template-based gen-
eration framework like TG/2 (Busemann, 2005),
but since the number of templates and their corre-
sponding aggregators is limited, we chose an ap-
proach based on string manipulation.
We found in our work that an approach based
on modeling individual users and then combining
the outputs of such models using a voting scheme
gives the best results, although our approach is
distinguishable from collaborative filtering tech-
niques used for driving recommendation systems
(Hofmann, 2004). We believe this is due to the
fact that the individual sessions from which rank-
ing models are learned, although they range over
the same collection of component tasks, can lead
to very different (human-generated) reports. That
is, the particular history of a session will affect
what is considered to be briefing-worthy.
3 System Overview
Figure 1: Briefing Assistant Data Flow.
The Briefing Assistant Model: We treat the
task of briefing generation in the current domain1
as non-textual event-based summarization. The
1More details about the domain and the interaction of BA
with the larger system are mentioned in a longer version of
the paper (Kumar et al, 2009)
Figure 2: The category tree showing the informa-
tion types that we expect in a briefing.
events are the task creation and task completion
actions logged by various cognitive assistants in
the system (so-called specialists). As part of the
design phase for the template-based generation
component, we identified a set of templates, based
on the actual briefings written by users in a sepa-
rate experiment. Ideally, we would like to adopt
a corpus-based approach to automatically extract
the templates in the domain, like (Kumar et al,
2008), but since the sample briefings available to
us were very few, the application of such corpus-
based techniques was not necessary. Based on
this set of templates we identified the patterns that
needed to be extracted from the event logs in order
to populate the templates. A ranking model was
also designed for ordering instantiations of this set
of templates and to recommend the top 4 most rel-
evant ones for a given session.
The overall data flow for BA during a session
(runtime) is shown in Figure 1. The various spe-
cialist modules generate task related events that
are logged in a database. The aggregators operate
over this database and emails to extract relevant
patterns. These patterns in turn are used to popu-
late templates which constitute candidate briefing
items. The candidate briefing items are then or-
dered by the ranking module and presented to the
user.
Template Design and Aggregators: The set
of templates used in the current instantiation of
the BA was derived from a corpus of human-
generated briefings collected in a previous exper-
iment using the same crisis management system.
The set of templates was designed to cover the
range of items that users in that experiment chose
to include in their reports corresponding to nine
categories shown in Figure 2. We found that in-
formation can be conveyed at different levels of
granularity (for example, qualitatively or quantita-
tively). The appropriate choice of granularity for
68
a particular session is a factor that the system can
learn2.
Ranking Model, Classifiers and Features: The
ranking module orders candidate templates so
that the four most relevant ones appear in the
briefing draft. The ranking system consists of
a consensus-based classifier, based on individual
classifier models for each user in the training set.
The prediction from each classifier are combined
(averaged) to produce a final rank of each tem-
plate.
We used the Minorthird package (Cohen, 2004)
for modeling. Specifically we allowed the sys-
tem to experiment with eleven different learning
schemes and select the best one based on cross-
validation within the training corpus. The schemes
were Naive Bayes, Voted Perceptron, Support
Vector Machines, Ranking Perceptron, K Nearest
Neighbor, Decision Tree, AdaBoost, Passive Ag-
gressive learner, Maximum Entropy learner, Bal-
anced Winnow and Boosted Ranking learner.
The features3 used in the system are static or
dynamic. Static features reflect the properties of
the templates irrespective of the user?s activity
whereas the dynamic features are based on the
actual events that took place. We used the In-
formation Gain (IG) metric for feature selection,
experimenting with seven different cut-off values
All, 20, 15, 10, 7, 5, 4 for the total number of se-
lected features.
4 Experiments and Results
Experimental Setup: Two experimental condi-
tions were used to differentiate performance based
on knowledge engineering, designated MinusL
and performance based on learning, designated
PlusL.4
Email Trigger: In the simulated conference
replanning crisis, the briefing was triggered by
an email containing explicit information requests,
not known beforehand. To customize the brief-
ing according to the request, a natural language
processing module identified the categories of in-
formation requested. The details of the module
are beyond the scope of the current paper as it
2The details of template design process including sample
templates, categories of templates and details of aggregators
are presented in (Kumar et al, 2009)
3Detailed description of the features are mentioned in
(Kumar et al, 2009)
4The details of the experimental setup as part of the larger
cognitive assistant system are presented in (Kumar et al,
2009).
is external to our system; it took into account
the template categories we earlier identified. Fig-
ure 4 shows a sample briefing email stimulus.
The mapping from the sample email in the figure
to the categories is as follows: ?expected atten-
dance? - Property-Session; ?how many sessions
have been rescheduled?, ?how many still need to
be rescheduled?, ?any problems you see as you
try to reschedule? - Session-Reschedule; ?status
of food service (I am worried about the keynote
lunch)? - Catering Vendors.
Training: Eleven expert users5 were asked to
provide training by using the system then generat-
ing the end of session briefing using the BA GUI.
For this training phase, no item ranking was per-
formed by the system, i.e. all the templates were
populated by the aggregators and recommenda-
tions were random. The expert user was asked
to select the best possible four items and was fur-
ther asked to judge the usefulness of the remaining
items. The resulting training data consists of the
activity log, extracted features and the user-labeled
items. The trigger message for the training users
did not contain any specific information request.
Test: Subjects were recruited to use the crisis
management system in MinusL and PlusL condi-
tion, although they were not aware of the condition
of the system and they were not involved with the
project. There were 54 test runs in the MinusL
condition and 47 in the PlusL condition. Out of
these runs, 29 subjects in MinusL and 43 subjects
in PlusL wrote a briefing using the BA. We report
the evaluation scores for this latter set.
Evaluation: The base performance metric is
Recall, defined in terms of the briefing templates
recommended by the system compared to the tem-
plates ultimately selected by the user. We justify
this by noting that Recall can be directly linked to
the expected time savings for the users. We cal-
culate two variants of Recall: Category-based?
calculated by matching the categories of the BA
recommended templates and user selected ones
ignoring the granularity and Template-based?
calculated by matching the exact templates. The
first metric indicates whether the right category of
information was selected and the latter indicates
whether the information was presented at the ap-
propriate level of detail.
We also performed subjective human evaluation
5Members of the project from other groups who were
aware of the scenario and various system functionalities but
not the ML methods
69
using a panel of three judges. The judges assigned
scores (0-4) to each of the bullets based on the
coverage of the crisis, clarity and conciseness, ac-
curacy and the correct level of granularity. They
were advised about certain briefing-specific char-
acteristics (e.g. negative bullet items are useful
and hence should be rated favorably). They were
also asked to provide a global assessment of report
quality, and evaluate the coverage of the requests
in the briefing stimulus email message. This pro-
cedure was very similar to the one used as the basis
for template selection.
Experiment: The automatic evaluation met-
ric used for the trained system configuration is
the Template-based recall measure. To obtain
the final system configuration, we automatically
evaluate the system under the various combina-
tions of parameter settings with eleven different
learning schemes and seven different feature se-
lection threshold (as mentioned in previous sec-
tions). Thus a total of 77 different configurations
are tested. For each configuration, we do a eleven-
fold cross-validation between the 11 training users
i.e. we leave one user as the test user and consider
the remaining ten users as training users. We av-
erage the performance across the 11 test cases and
obtain the final score for the configuration. We
choose the configuration with the highest score as
the final trained system configuration. The learned
system configuration in the current test includes
Balanced Winnow (Littlestone, 1988) and top 7
features.
Results: We noticed that four users in PlusL
condition took more than 8 minutes to complete
the briefing when the median time taken by the
users in PlusL condition was 55 seconds, so we
did not include these users in our analysis in order
to maintain the homogeneity of the dataset. These
four data points were identified as extreme outliers
using a procedure suggested by (NIST, 2008)6.
There were no extreme outliers in MinusL condi-
tion.
Figure 3a shows the Recall values for the Mi-
nusL and PlusL conditions. The learning delta
i.e. the difference between the recall values of
PlusL and MinusL is 33% for Template-based re-
call and 21% for Category-based recall. These
differences are significant at the p < 0.001 level.
6Extreme outliers are defined as data points that are out-
side the range [Q1?3?IQ,Q3+3?IQ] in a box plot. Q1 is
lower quartile, Q3 is upper quartile and IQ is the difference
(Q3?Q1) is the interquartile range.
The statistical significance for the Template-based
metric, which was the metric used for select-
ing system parameters during the training phase,
shows that learning is effective in this case. Since
the email stimulus processing module extracts the
briefing categories from the email the Category-
based and Template-based recall is expected to be
high for the baseline MinusL case. In our test, the
email stimuli had 3 category requests and so the
Category-based recall of 0.77 and Template-based
recall of 0.67 in MinusL is not unexpected.
Figure 3b shows the Judges? panel scores for
the briefings in MinusL and PlusL condition. The
learning delta in this case is 3.6% which is also
statistically significant, at p < 0.05. The statistical
significance of the learning delta validates that the
briefings generated during PlusL conditions are
better than MinusL condition. The absolute differ-
ence in the qualitative briefing scores between the
two conditions is small because MinusL users can
select from all candidates, while the recommenda-
tions they receive are random. Consequently they
need to spend more time in finding the right items.
The average time taken for a briefing in MinusL
condition is about 83 seconds and 62 seconds in
PlusL (see Figure 3c). While the time difference
is high (34%) it is not statistically significant due
to high variance.
Four of the top 10 most frequently selected fea-
tures across users for this system are dynamic fea-
tures. This indicates that the learning model is
capturing the user?s world state and the recom-
mendations are related to the underlying events.
We believe this validates the process we used to
generate briefing reports from non-textual events.
5 Summary
The Briefing Assistant is not designed to learn
the generic attributes of good reports; rather it?s
meant to rapidly learn the attributes of good re-
ports within a particular domain and to accom-
modate specific information needs on a report-by-
report basis. We found that learned customiza-
tion produces reports that are judged to be of bet-
ter quality. We also found that a consensus-based
modeling approach, which incorporates informa-
tion from multiple users, yields the best perfor-
mance. We believe that our approach can be used
to create flexible summarization systems for a va-
riety of applications.
70
(a) (b) (c)
Figure 3: (a) Recall values for MinusL and PlusL conditions (b) Briefing scores from the judges? panel
for MinusL and PlusL conditions (c) Briefing time taken for MinusL and PlusL conditions.
Figure 4: Template categories corresponding to
the Briefing request email.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL.
Anja Belz. 2007. Probabilistic generation of weather
forecast texts. In Proceedings of HLT-NAACL.
Stephan Busemann. 2005. Ten years after: An update
on TG/2 (and friends). In Proceedings of European
Natural Language Generation Workshop.
William W. Cohen. 2004. Minorthird: Methods for
identifying names and ontological relations in text
using heuristics for inducing regularities from data.
http://minorthird.sourceforge.net, 10th Jun 2009.
Naomi Daniel, Dragomir Radev, and Timothy Allison.
2003. Sub-event based multi-document summariza-
tion. In Proceedings of HLT-NAACL.
Pablo A. Duboue and Kathleen R. McKeown. 2003.
Statistical acquisition of content selection rules for
natural language generation. In Proceedings of
EMNLP.
Pablo A. Duboue. 2004. Indirect supervised learning
of content selection logic. In Proceedings of INLG.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
Event-based extractive summarization. In Text Sum-
marization Branches Out: Proceedings of the ACL-
04 Workshop.
Thomas Hofmann. 2004. Latent semantic models for
collaborative filtering. ACM Transactions on Infor-
mation Systems, 22(1):89?115.
Mohit Kumar, Dipanjan Das, and Alexander I. Rud-
nicky. 2008. Automatic extraction of briefing tem-
plates. In Proceedings of IJCNLP.
Mohit Kumar, Dipanjan Das, Sachin Agarwal, and
Alexander I. Rudnicky. 2009. Non-textual event
summarization by applying machine learning to
template-based language generation. Technical Re-
port CMU-LTI-09-012, Language Technologies In-
stitute, Carnegie Mellon University.
Wenjie Li, Mingli Wu, Qin Lu, Wei Xu, and Chunfa
Yuan. 2006. Extractive summarization using inter-
and intra- event relevance. In Proceedings of ACL.
Nick Littlestone. 1988. Learning quickly when irrele-
vant attributes abound: A new linear-threshold algo-
rithm. Machine Learning, 2(4):285?318.
NIST. 2008. NIST/SEMATECH e-
handbook of statistical methods.
http://www.itl.nist.gov/div898/handbook/, 10th
Jun 2009.
Alice Oh and Howard Shrobe. 2008. Generating base-
ball summaries from multiple perspectives by re-
ordering content. In Proceedings of INLG.
Franc?ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy
Sykes. 2009. Automatic generation of textual sum-
maries from neonatal intensive care data. Artificial
Intelligence, 173(7-8):789?816.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):470?500.
Kees van Deemter, Emiel Krahmer, and Mariet The-
une. 2005. Real versus template-based natural lan-
guage generation: A false opposition? Computa-
tional Linguistics, 31(1):15?24.
Mingli Wu. 2006. Investigations on event-based sum-
marization. In Proceedings of ACL.
71
