Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 277?285,
Beijing, August 2010
Entity Disambiguation for Knowledge Base Population
?Mark Dredze and ?Paul McNamee and ?Delip Rao and ?Adam Gerber and ?Tim Finin
?Human Language Technology Center of Excellence, Center for Language and Speech Processing
Johns Hopkins University
?University of Maryland ? Baltimore County
mdredze,mcnamee,delip,adam.gerber@jhu.edu, finin@umbc.edu
Abstract
The integration of facts derived from information extraction
systems into existing knowledge bases requires a system to
disambiguate entity mentions in the text. This is challeng-
ing due to issues such as non-uniform variations in entity
names, mention ambiguity, and entities absent from a knowl-
edge base. We present a state of the art system for entity dis-
ambiguation that not only addresses these challenges but also
scales to knowledge bases with several million entries using
very little resources. Further, our approach achieves perfor-
mance of up to 95% on entities mentioned from newswire
and 80% on a public test set that was designed to include
challenging queries.
1 Introduction
The ability to identify entities like people, orga-
nizations and geographic locations (Tjong Kim
Sang and De Meulder, 2003), extract their at-
tributes (Pasca, 2008), and identify entity rela-
tions (Banko and Etzioni, 2008) is useful for sev-
eral applications in natural language processing
and knowledge acquisition tasks like populating
structured knowledge bases (KB).
However, inserting extracted knowledge into a
KB is fraught with challenges arising from nat-
ural language ambiguity, textual inconsistencies,
and lack of world knowledge. To the discern-
ing human eye, the ?Bush? in ?Mr. Bush left
for the Zurich environment summit in Air Force
One.? is clearly the US president. Further con-
text may reveal it to be the 43rd president, George
W. Bush, and not the 41st president, George H.
W. Bush. The ability to disambiguate a polyse-
mous entity mention or infer that two orthograph-
ically different mentions are the same entity is
crucial in updating an entity?s KB record. This
task has been variously called entity disambigua-
tion, record linkage, or entity linking. When per-
formed without a KB, entity disambiguation is
called coreference resolution: entity mentions ei-
ther within the same document or across multi-
ple documents are clustered together, where each
cluster corresponds to a single real world entity.
The emergence of large scale publicly avail-
able KBs like Wikipedia and DBPedia has spurred
an interest in linking textual entity references to
their entries in these public KBs. Bunescu and
Pasca (2006) and Cucerzan (2007) presented im-
portant pioneering work in this area, but suffer
from several limitations including Wikipedia spe-
cific dependencies, scale, and the assumption of
a KB entry for each entity. In this work we in-
troduce an entity disambiguation system for link-
ing entities to corresponding Wikipedia pages de-
signed for open domains, where a large percent-
age of entities will not be linkable. Further, our
method and some of our features readily general-
ize to other curated KB. We adopt a supervised
approach, where each of the possible entities con-
tained within Wikipedia are scored for a match to
the query entity. We also describe techniques to
deal with large knowledge bases, like Wikipedia,
which contain millions of entries. Furthermore,
our system learns when to withhold a link when
an entity has no matching KB entry, a task that
has largely been neglected in prior research in
cross-document entity coreference. Our system
produces high quality predictions compared with
recent work on this task.
2 Related Work
The information extraction oeuvre has a gamut of
relation extraction methods for entities like per-
sons, organizations, and locations, which can be
classified as open- or closed-domain depending
on the restrictions on extractable relations (Banko
and Etzioni, 2008). Closed domain systems ex-
tract a fixed set of relations while in open-domain
systems, the number and type of relations are un-
bounded. Extracted relations still require process-
ing before they can populate a KB with facts:
namely, entity linking and disambiguation.
277
Motivated by ambiguity in personal name
search, Mann and Yarowsky (2003) disambiguate
person names using biographic facts, like birth
year, occupation and affiliation. When present
in text, biographic facts extracted using regular
expressions help disambiguation. More recently,
the Web People Search Task (Artiles et al, 2008)
clustered web pages for entity disambiguation.
The related task of cross document corefer-
ence resolution has been addressed by several
researchers starting from Bagga and Baldwin
(1998). Poesio et al (2008) built a cross document
coreference system using features from encyclo-
pedic sources like Wikipedia. However, success-
ful coreference resolution is insufficient for cor-
rect entity linking, as the coreference chain must
still be correctly mapped to the proper KB entry.
Previous work by Bunescu and Pasca (2006)
and Cucerzan (2007) aims to link entity men-
tions to their corresponding topic pages in
Wikipedia but the authors differ in their ap-
proaches. Cucerzan uses heuristic rules and
Wikipedia disambiguation markup to derive map-
pings from surface forms of entities to their
Wikipedia entries. For each entity in Wikipedia,
a context vector is derived as a prototype for the
entity and these vectors are compared (via dot-
product) with the context vectors of unknown en-
tity mentions. His work assumes that all entities
have a corresponding Wikipedia entry, but this as-
sumption fails for a significant number of entities
in news articles and even more for other genres,
like blogs. Bunescu and Pasca on the other hand
suggest a simple method to handle entities not in
Wikipedia by learning a threshold to decide if the
entity is not in Wikipedia. Both works mentioned
rely on Wikipedia-specific annotations, such as
category hierarchies and disambiguation links.
We just recently became aware of a system
fielded by Li et al at the TAC-KBP 2009 eval-
uation (2009). Their approach bears a number
of similarities to ours; both systems create candi-
date sets and then rank possibilities using differing
learning methods, but the principal difference is in
our approach to NIL prediction. Where we simply
consider absence (i.e., the NIL candidate) as an-
other entry to rank, and select the top-ranked op-
tion, they use a separate binary classifier to decide
whether their top prediction is correct, or whether
NIL should be output. We believe relying on fea-
tures that are designed to inform whether absence
is correct is the better alternative.
3 Entity Linking
We define entity linking as matching a textual en-
tity mention, possibly identified by a named en-
tity recognizer, to a KB entry, such as a Wikipedia
page that is a canonical entry for that entity. An
entity linking query is a request to link a textual
entity mention in a given document to an entry in
a KB. The system can either return a matching en-
try or NIL to indicate there is no matching entry.
In this work we focus on linking organizations,
geo-political entities and persons to a Wikipedia
derived KB.
3.1 Key Issues
There are 3 challenges to entity linking:
Name Variations. An entity often has multiple
mention forms, including abbreviations (Boston
Symphony Orchestra vs. BSO), shortened forms
(Osama Bin Laden vs. Bin Laden), alternate
spellings (Osama vs. Ussamah vs. Oussama),
and aliases (Osama Bin Laden vs. Sheikh Al-
Mujahid). Entity linking must find an entry de-
spite changes in the mention string.
Entity Ambiguity. A single mention, like
Springfield, can match multiple KB entries, as
many entity names, like people and organizations,
tend to be polysemous.
Absence. Processing large text collections vir-
tually guarantees that many entities will not ap-
pear in the KB (NIL), even for large KBs.
The combination of these challenges makes
entity linking especially challenging. Consider
an example of ?William Clinton.? Most read-
ers will immediately think of the 42nd US pres-
ident. However, the only two William Clintons in
Wikipedia are ?William de Clinton? the 1st Earl
of Huntingdon, and ?William Henry Clinton? the
British general. The page for the 42nd US pres-
ident is actually ?Bill Clinton?. An entity link-
ing system must decide if either of the William
Clintons are correct, even though neither are ex-
act matches. If the system determines neither
278
matches, should it return NIL or the variant ?Bill
Clinton?? If variants are acceptable, then perhaps
?Clinton, Iowa? or ?DeWitt Clinton? should be
acceptable answers?
3.2 Contributions
We address these entity linking challenges.
Robust Candidate Selection. Our system is
flexible enough to find name variants but suffi-
ciently restrictive to produce a manageable can-
didate list despite a large-scale KB.
Features for Entity Disambiguation. We de-
veloped a rich and extensible set of features based
on the entity mention, the source document, and
the KB entry. We use a machine learning ranker
to score each candidate.
Learning NILs. We modify the ranker to learn
NIL predictions, which obviates hand tuning and
importantly, admits use of additional features that
are indicative of NIL.
Our contributions differ from previous efforts
(Bunescu and Pasca, 2006; Cucerzan, 2007) in
several important ways. First, previous efforts de-
pend on Wikipedia markup for significant perfor-
mance gains. We make no such assumptions, al-
though we show that optional Wikipedia features
lead to a slight improvement. Second, Cucerzan
does not handle NILs while Bunescu and Pasca
address them by learning a threshold. Our ap-
proach learns to predict NIL in a more general
and direct way. Third, we develop a rich fea-
ture set for entity linking that can work with any
KB. Finally, we apply a novel finite state machine
method for learning name variations. 1
The remaining sections describe the candidate
selection system, features and ranking, and our
novel approach learning NILs, followed by an
empirical evaluation.
4 Candidate Selection for Name Variants
The first system component addresses the chal-
lenge of name variants. As the KB contains a large
number of entries (818,000 entities, of which 35%
are PER, ORG or GPE), we require an efficient se-
lection of the relevant candidates for a query.
Previous approaches used Wikipedia markup
for filtering ? only using the top-k page categories
1http://www.clsp.jhu.edu/ markus/fstrain
(Bunescu and Pasca, 2006) ? which is limited to
Wikipedia and does not work for general KBs.
We consider a KB independent approach to selec-
tion that also allows for tuning candidate set size.
This involves a linear pass over KB entry names
(Wikipedia page titles): a naive implementation
took two minutes per query. The following sec-
tion reduces this to under two seconds per query.
For a given query, the system selects KB entries
using the following approach:
? Titles that are exact matches for the mention.
? Titles that are wholly contained in or contain
the mention (e.g., Nationwide and Nationwide In-
surance).
? The first letters of the entity mention match the
KB entry title (e.g., OA and Olympic Airlines).
? The title matches a known alias for the entity
(aliases described in Section 5.2).
? The title has a strong string similarity score
with the entity mention. We include several mea-
sures of string similarity, including: character
Dice score > 0.9, skip bigram Dice score > 0.6,
and Hamming distance <= 2.
We did not optimize the thresholds for string
similarity, but these could obviously be tuned to
minimize the candidate sets and maximize recall.
All of the above features are general for any
KB. However, since our evaluation used a KB
derived from Wikipedia, we included a few
Wikipedia specific features. We added an entry if
its Wikipedia page appeared in the top 20 Google
results for a query.
On the training dataset (Section 7) the selection
system attained a recall of 98.8% and produced
candidate lists that were three to four orders of
magnitude smaller than the KB. Some recall er-
rors were due to inexact acronyms: ABC (Arab
Banking; ?Corporation? is missing), ASG (Abu
Sayyaf; ?Group? is missing), and PCF (French
Communist Party; French reverses the order of the
pre-nominal adjectives). We also missed Interna-
tional Police (Interpol) and Becks (David Beck-
ham; Mr. Beckham and his wife are collectively
referred to as ?Posh and Becks?).
279
4.1 Scaling Candidate Selection
Our previously described candidate selection re-
lied on a linear pass over the KB, but we seek
more efficient methods. We observed that the
above non-string similarity filters can be pre-
computed and stored in an index, and that the skip
bigram Dice score can be computed by indexing
the skip bigrams for each KB title. We omitted
the other string similarity scores, and collectively
these changes enable us to avoid a linear pass over
the KB. Finally we obtained speedups by serving
the KB concurrently2. Recall was nearly identical
to the full system described above: only two more
queries failed. Additionally, more than 95% of
the processing time was consumed by Dice score
computation, which was only required to cor-
rectly retrieve less than 4% of the training queries.
Omitting the Dice computation yielded results in
a few milliseconds. A related approach is that of
canopies for scaling clustering for large amounts
of bibliographic citations (McCallum et al, 2000).
In contrast, our setting focuses on alignment vs.
clustering mentions, for which overlapping parti-
tioning approaches like canopies are applicable.
5 Entity Linking as Ranking
We select a single correct candidate for a query
using a supervised machine learning ranker. We
represent each query by a D dimensional vector
x, where x ? RD, and we aim to select a sin-
gle KB entry y, where y ? Y , a set of possible
KB entries for this query produced by the selec-
tion system above, which ensures that Y is small.
The ith query is given by the pair {xi, yi}, where
we assume at most one correct KB entry.
To evaluate each candidate KB entry in Y we
create feature functions of the form f(x, y), de-
pendent on both the example x (document and en-
tity mention) and the KB entry y. The features
address name variants and entity disambiguation.
We take a maximum margin approach to learn-
ing: the correct KB entry y should receive a
higher score than all other possible KB entries
y? ? Y, y? 6= y plus some margin ?. This learning
2Our Python implementation with indexing features and
four threads achieved up to 80? speedup compared to naive
implementation.
constraint is equivalent to the ranking SVM algo-
rithm of Joachims (2002), where we define an or-
dered pair constraint for each of the incorrect KB
entries y? and the correct entry y. Training sets pa-
rameters such that score(y) ? score(y?) + ?. We
used the library SVMrank to solve this optimiza-
tion problem.3 We used a linear kernel, set the
slack parameter C as 0.01 times the number of
training examples, and take the loss function as
the total number of swapped pairs summed over
all training examples. While previous work used
a custom kernel, we found a linear kernel just as
effective with our features. This has the advan-
tage of efficiency in both training and prediction 4
? important considerations in a system meant to
scale to millions of KB entries.
5.1 Features for Entity Disambiguation
200 atomic features represent x based on each
candidate query/KB pair. Since we used a lin-
ear kernel, we explicitly combined certain fea-
tures (e.g., acroynym-match AND known-alias) to
model correlations. This included combining each
feature with the predicted type of the entity, al-
lowing the algorithm to learn prediction functions
specific to each entity type. With feature combina-
tions, the total number of features grew to 26,569.
The next sections provide an overview; for a de-
tailed list see McNamee et al (2009).
5.2 Features for Name Variants
Variation in entity name has long been recog-
nized as a bane for information extraction sys-
tems. Poor handling of entity name variants re-
sults in low recall. We describe several features
ranging from simple string match to finite state
transducer matching.
String Equality. If the query name and KB en-
try name are identical, this is a strong indication of
a match, and in our KB entry names are distinct.
However, similar or identical entry names that
refer to distinct entities are often qualified with
parenthetical expressions or short clauses. As
an example, ?London, Kentucky? is distinguished
3www.cs.cornell.edu/people/tj/svm_light/svm_rank.html
4Bunescu and Pasca (2006) report learning tens of thou-
sands of support vectors with their ?taxonomy? kernel while
a linear kernel represents all support vectors with a single
weight vector, enabling faster training and prediction.
280
from ?London, Ontario?, ?London, Arkansas?,
?London (novel)?, and ?London?. Therefore,
other string equality features were used, such as
whether names are equivalent after some transfor-
mation. For example, ?Baltimore? and ?Baltimore
City? are exact matches after removing a common
GPE word like city; ?University of Vermont? and
?University of VT? match if VT is expanded.
Approximate String Matching. Many entity
mentions will not match full names exactly. We
added features for character Dice, skip bigram
Dice, and left and right Hamming distance scores.
Features were set based on quantized scores.
These were useful for detecting minor spelling
variations or mistakes. Features were also added if
the query was wholly contained in the entry name,
or vice-versa, which was useful for handling ellip-
sis (e.g., ?United States Department of Agricul-
ture? vs. ?Department of Agriculture?). We also
included the ratio of the recursive longest com-
mon subsequence (Christen, 2006) to the shorter
of the mention or entry name, which is effective at
handling some deletions or word reorderings (e.g.,
?Li Gong? and ?Gong Li?). Finally, we checked
whether all of the letters of the query are found in
the same order in the entry name (e.g., ?Univ Wis-
consin? would match ?University of Wisconsin?).
Acronyms. Features for acronyms, using dic-
tionaries and partial character matches, enable
matches between ?MIT? and ?Madras Institute of
Technology? or ?Ministry of Industry and Trade.?
Aliases. Many aliases or nicknames are non-
trivial to guess. For example JAVA is the
stock symbol for Sun Microsystems, and ?Gin-
ger Spice? is a stage name of Geri Halliwell. A
reasonable way to do this is to employ a dictio-
nary and alias lists that are commonly available
for many domains5.
FST Name Matching. Another measure of sur-
face similarity between a query and a candidate
was computed by training finite-state transducers
similar to those described in Dreyer et al (2008).
These transducers assign a score to any string pair
by summing over all alignments and scoring all
5We used multiple lists, including class-specific lists (i.e.,
for PER, ORG, and GPE) lists extracted from Freebase (Bol-
lacker et al, 2008) and Wikipedia redirects. PER, ORG, and
GPE are the commonly used terms for entity types for peo-
ple, organizations and geo-political regions respectively.
contained character n-grams; we used n-grams of
length 3 and less. The scores are combined using a
global log-linear model. Since different spellings
of a name may vary considerably in length (e.g.,
J Miller vs. Jennifer Miller) we eliminated the
limit on consecutive insertions used in previous
applications.6
5.3 Wikipedia Features
Most of our features do not depend on Wikipedia
markup, but it is reasonable to include features
from KB properties. Our feature ablation study
shows that dropping these features causes a small
but statistically significant performance drop.
WikiGraph statistics. We added features de-
rived from the Wikipedia graph structure for an
entry, like indegree of a node, outdegree of a node,
and Wikipedia page length in bytes. These statis-
tics favor common entity mentions over rare ones.
Wikitology. KB entries can be indexed with hu-
man or machine generated metadata consisting of
keywords or categories in a domain-appropriate
taxonomy. Using a system called Wikitology,
Syed et al (2008) investigated use of ontology
terms obtained from the explicit category system
in Wikipedia as well as relationships induced from
the hyperlink graph between related Wikipedia
pages. Following this approach we computed top-
ranked categories for the query documents and
used this information as features. If none of the
candidate KB entries had corresponding highly-
ranked Wikitology pages, we used this as a NIL
feature (Section 6.1).
5.4 Popularity
Although it may be an unsafe bias to give prefer-
ence to common entities, we find it helpful to pro-
vide estimates of entity popularity to our ranker
as others have done (Fader et al, 2009). Apart
from the graph-theoretic features derived from the
Wikipedia graph, we used Google?s PageRank to
by adding features indicating the rank of the KB
entry?s corresponding Wikipedia page in a Google
query for the target entity mention.
6Without such a limit, the objective function may diverge
for certain parameters of the model; we detect such cases and
learn to avoid them during training.
281
5.5 Document Features
The mention document and text associated with a
KB entry contain context for resolving ambiguity.
Entity Mentions. Some features were based on
presence of names in the text: whether the query
appeared in the KB text and the entry name in the
document. Additionally, we used a named-entity
tagger and relation finder, SERIF (Boschee et al,
2005), identified name and nominal mentions that
were deemed co-referent with the entity mention
in the document, and tested whether these nouns
were present in the KB text. Without the NE anal-
ysis, accuracy on non-NIL entities dropped 4.5%.
KB Facts. KB nodes contain infobox attributes
(or facts); we tested whether the fact text was
present in the query document, both locally to a
mention, or anywhere in the text. Although these
facts were derived from Wikipedia infoboxes,
they could be obtained from other sources as well.
Document Similarity We measured similarity
between the query document and the KB text in
two ways: cosine similarity with TF/IDF weight-
ing (Salton and McGill, 1983); and using the Dice
coefficient over bags of words. IDF values were
approximated using counts from the Google 5-
gram dataset as by Klein and Nelson (2008).
Entity Types. Since the KB contained types
for entries, we used these as features as well as
the predicted NE type for the entity mention in
the document text. Additionally, since only a
small number of KB entries had PER, ORG, or
GPE types, we also inferred types from Infobox
class information to attain 87% coverage in the
KB. This was helpful for discouraging selection
of eponymous entries named after famous enti-
ties (e.g., the former U.S. president vs. ?John F.
Kennedy International Airport?).
5.6 Feature Combinations
To take into account feature dependencies we cre-
ated combination features by taking the cross-
product of a small set of diverse features. The
attributes used as combination features included
entity type; a popularity based on Google?s rank-
ings; document comparison using TF/IDF; cov-
erage of co-referential nouns in the KB node
text; and name similarity. The combinations were
cascaded to allow arbitrary feature conjunctions.
Thus it is possible to end up with a feature kbtype-
is-ORG AND high-TFIDF-score AND low-name-
similarity. The combined features increased the
number of features from roughly 200 to 26,000.
6 Predicting NIL Mentions
So far we have assumed that each example has a
correct KB entry; however, when run over a large
corpus, such as news articles, we expect a signifi-
cant number of entities will not appear in the KB.
Hence it will be useful to predict NILs.
We learn when to predict NIL using the SVM
ranker by augmenting Y to include NIL, which
then has a single feature unique to NIL answers.
It can be shown that (modulo slack variables) this
is equivalent to learning a single threshold ? for
NIL predictions as in Bunescu and Pasca (2006).
Incorporating NIL into the ranker has several
advantages. First, the ranker can set the thresh-
old optimally without hand tuning. Second, since
the SVM scores are relative within a single exam-
ple and cannot be compared across examples, set-
ting a single threshold is difficult. Third, a thresh-
old sets a uniform standard across all examples,
whereas in practice we may have reasons to favor
a NIL prediction in a given example. We design
features for NIL prediction that cannot be cap-
tured in a single parameter.
6.1 NIL Features
Integrating NIL prediction into learning means
we can define arbitrary features indicative of NIL
predictions in the feature vector corresponding to
NIL. For example, if many candidates have good
name matches, it is likely that one of them is cor-
rect. Conversely, if no candidate has high entry-
text/article similarity, or overlap between facts
and the article text, it is likely that the entity is
absent from the KB. We included several features,
such as a) the max, mean, and difference between
max and mean for 7 atomic features for all KB
candidates considered, b) whether any of the can-
didate entries have matching names (exact and
fuzzy string matching), c) whether any KB en-
try was a top Wikitology match, and d) if the top
Google match was not a candidate.
282
Micro-Averaged Macro-Averaged
Best Median All Features Best Features Best Median All Features Best Features
All 0.8217 0.7108 0.7984 0.7941 0.7704 0.6861 0.7695 0.7704
non-NIL 0.7725 0.6352 0.7063 0.6639 0.6696 0.5335 0.6097 0.5593
NIL 0.8919 0.7891 0.8677 0.8919 0.8789 0.7446 0.8464 0.8721
Table 1: Micro and macro-averaged accuracy for TAC-KBP data compared to best and median reported performance.
Results are shown for all features as well as removing a small number of features using feature selection on development data.
7 Evaluation
We evaluated our system on two datasets: the
Text Analysis Conference (TAC) track on Knowl-
edge Base Population (TAC-KBP) (McNamee and
Dang, 2009) and the newswire data used by
Cucerzan (2007) (Microsoft News Data).
Since our approach relies on supervised learn-
ing, we begin by constructing our own training
corpus.7 We highlighted 1496 named entity men-
tions in news documents (from the TAC-KBP doc-
ument collection) and linked these to entries in
a KB derived from Wikipedia infoboxes. 8 We
added to this collection 119 sample queries from
the TAC-KBP data. The total of 1615 training ex-
amples included 539 (33.4%) PER, 618 (38.3%)
ORG, and 458 (28.4%) GPE entity mentions. Of
the training examples, 80.5% were found in the
KB, matching 300 unique entities. This set has a
higher number of NIL entities than did Bunescu
and Pasca (2006) (10%) but lower than the TAC-
KBP test set (43%).
All system development was done using a train
(908 examples) and development (707 examples)
split. The TAC-KBP and Microsoft News data
sets were held out for final tests. A model trained
on all 1615 examples was used for experiments.
7.1 TAC-KBP 2009 Experiments
The KB is derived from English Wikipedia pages
that contained an infobox. Entries contain basic
descriptions (article text) and attributes. The TAC-
KBP query set contains 3904 entity mentions for
560 distinct entities; entity type was only provided
for evaluation. The majority of queries were for
organizations (69%). Most queries were missing
from the KB (57%). 77% of the distinct GPEs
in the queries were present in the KB, but for
7Data available from www.dredze.com
8http://en.wikipedia.org/wiki/Help:Infobox
PERs and ORGs these percentages were signifi-
cantly lower, 19% and 30% respectively.
Table 1 shows results on TAC-KBP data us-
ing all of our features as well a subset of features
based on feature selection experiments on devel-
opment data. We include scores for both micro-
averaged accuracy ? averaged over all queries
? and macro-averaged accuracy ? averaged over
each unique entity ? as well as the best and me-
dian reported results for these data (McNamee
and Dang, 2009). We obtained the best reported
results for macro-averaged accuracy, as well as
the best results for NIL detection with micro-
averaged accuracy, which shows the advantage of
our approach to learning NIL. See McNamee et
al. (2009) for additional experiments.
The candidate selection phase obtained a re-
call of 98.6%, similar to that of development data.
Missed candidates included Iron Lady, which
refers metaphorically to Yulia Tymoshenko, PCC,
the Spanish-origin acronym for the Cuban Com-
munist Party, and Queen City, a former nickname
for the city of Seattle, Washington. The system re-
turned a mean of 76 candidates per query, but the
median was 15 and the maximum 2772 (Texas). In
about 10% of cases there were four or fewer can-
didates and in 10% of cases there were more than
100 candidate KB nodes. We observed that ORGs
were more difficult, due to the greater variation
and complexity in their naming, and that they can
be named after persons or locations.
7.2 Feature Effectiveness
We performed two feature analyses on the TAC-
KBP data: an additive study ? starting from a
small baseline feature set used in candidate selec-
tion we add feature groups and measure perfor-
mance changes (omitting feature combinations),
and an ablative study ? starting from all features,
remove a feature group and measure performance.
283
Class All non-NIL NIL
Baseline 0.7264 0.4621 0.9251
Acronyms 0.7316 0.4860 0.9161
NE Analysis 0.7661 0.7181 0.8022
Google 0.7597 0.7421 0.7730
Doc/KB Text Similarity 0.7313 0.6699 0.7775
Wikitology 0.7318 0.4549 0.9399
All 0.7984 0.7063 0.8677
Table 2: Additive analysis: micro-averaged accuracy.
Table 2 shows the most significant features in
the feature addition experiments. The baseline
includes only features based on string similarity
or aliases and is not effective at finding correct
entries and strongly favors NIL predictions. In-
clusion of features based on analysis of named-
entities, popularity measures (e.g., Google rank-
ings), and text comparisons provided the largest
gains. The overall changes are fairly small,
roughly ?1%; however changes in non-NIL pre-
cision are larger.
The ablation study showed considerable redun-
dancy across feature groupings. In several cases,
performance could have been slightly improved
by removing features. Removing all feature com-
binations would have improved overall perfor-
mance to 81.05% by gaining on non-NIL for a
small decline on NIL detection.
7.3 Experiments on Microsoft News Data
We downloaded the evaluation data used in
Cucerzan (2007)9: 20 news stories from MSNBC
with 642 entity mentions manually linked to
Wikipedia and another 113 mentions not having
any corresponding link to Wikipedia.10 A sig-
nificant percentage of queries were not of type
PER, ORG, or GPE (e.g., ?Christmas?). SERIF
assigned entity types and we removed 297 queries
not recognized as entities (counts in Table 3).
We learned a new model on the training data
above using a reduced feature set to increase
speed.11 Using our fast candidate selection sys-
tem, we resolved each query in 1.98 seconds (me-
dian). Query processing time was proportional to
9http://research.microsoft.com/en-us/um/people/silviu/WebAssistant/TestData/
10One of the MSNBC news articles is no longer available
so we used 759 total entities.
11We removed Google, FST and conjunction features
which reduced system accuracy but increased performance.
Num. Queries Accuracy
Total Nil All non-NIL NIL
NIL 452 187 0.4137 0.0 1.0
GPE 132 20 0.9696 1.00 0.8000
ORG 115 45 0.8348 0.7286 1.00
PER 205 122 0.9951 0.9880 1.00
All 452 187 0.9469 0.9245 0.9786
Cucerzan (2007) 0.914 - -
Table 3: Micro-average results for Microsoft data.
the number of candidates considered. We selected
a median of 13 candidates for PER, 12 for ORG
and 102 for GPE. Accuracy results are in Table
3. The high results reported for this dataset over
TAC-KBP is primarily because we perform very
well in predicting popular and rare entries ? both
of which are common in newswire text.
One issue with our KB was that it was derived
from infoboxes in Wikipedia?s Oct 2008 version
which has both new entities, 12 and is missing en-
tities.13 Therefore, we manually confirmed NIL
answers and new answers for queries marked as
NIL in the data. While an exact comparison is not
possible (as described above), our results (94.7%)
appear to be at least on par with Cucerzan?s sys-
tem (91.4% overall accuracy).With the strong re-
sults on TAC-KBP, we believe that this is strong
confirmation of the effectiveness of our approach.
8 Conclusion
We presented a state of the art system to disam-
biguate entity mentions in text and link them to
a knowledge base. Unlike previous approaches,
our approach readily ports to KBs other than
Wikipedia. We described several important chal-
lenges in the entity linking task including han-
dling variations in entity names, ambiguity in en-
tity mentions, and missing entities in the KB, and
we showed how to each of these can be addressed.
We described a comprehensive feature set to ac-
complish this task in a supervised setting. Impor-
tantly, our method discriminately learns when not
to link with high accuracy. To spur further re-
search in these areas we are releasing our entity
linking system.
122008 vs. 2006 version used in Cucerzan (2007) We
could not get the 2006 version from the author or the Internet.
13Since our KB was derived from infoboxes, entities not
having an infobox were left out.
284
References
Javier Artiles, Satoshi Sekine, and Julio Gonzalo.
2008. Web people search: results of the first evalu-
ation and the plan for the second. In WWW.
Amit Bagga and Breck Baldwin. 1998. Entity-
based cross-document coreferencing using the vec-
tor space model. In Conference on Computational
Linguistics (COLING).
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Association for Computational Linguistics.
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In SIGMOD Management of Data.
E. Boschee, R. Weischedel, and A. Zamanian. 2005.
Automatic information extraction. In Conference
on Intelligence Analysis.
Razvan C. Bunescu and Marius Pasca. 2006. Using
encyclopedic knowledge for named entity disam-
biguation. In European Chapter of the Assocation
for Computational Linguistics (EACL).
Peter Christen. 2006. A comparison of personal name
matching: Techniques and practical issues. Techni-
cal Report TR-CS-06-02, Australian National Uni-
versity.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on wikipedia data. In Em-
pirical Methods in Natural Language Processing
(EMNLP).
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In Empirical Methods in
Natural Language Processing (EMNLP).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling Wikipedia-based named entity dis-
ambiguation to arbitrary web text. In WikiAI09
Workshop at IJCAI 2009.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Knowledge Discovery
and Data Mining (KDD).
Martin Klein and Michael L. Nelson. 2008. A com-
parison of techniques for estimating IDF values to
generate lexical signatures for the web. In Work-
shop on Web Information and Data Management
(WIDM).
Fangtao Li, Zhicheng Zhang, Fan Bu, Yang Tang,
Xiaoyan Zhu, and Minlie Huang. 2009. THU
QUANTA at TAC 2009 KBP and RTE track. In Text
Analysis Conference (TAC).
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Conference
on Natural Language Learning (CONLL).
Andrew McCallum, Kamal Nigam, and Lyle Ungar.
2000. Efficient clustering of high-dimensional data
sets with application to reference matching. In
Knowledge Discovery and Data Mining (KDD).
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track.
In Text Analysis Conference (TAC).
Paul McNamee, Mark Dredze, Adam Gerber, Nikesh
Garera, Tim Finin, James Mayfield, Christine Pi-
atko, Delip Rao, David Yarowsky, and Markus
Dreyer. 2009. HLTCOE approaches to knowledge
base population at TAC 2009. In Text Analysis Con-
ference (TAC).
Marius Pasca. 2008. Turning web text and search
queries into factual knowledge: hierarchical class
attribute extraction. In National Conference on Ar-
tificial Intelligence (AAAI).
Massimo Poesio, David Day, Ron Artstein, Jason Dun-
can, Vladimir Eidelman, Claudio Giuliano, Rob
Hall, Janet Hitzeman, Alan Jern, Mijail Kabadjov,
Stanley Yong, Wai Keong, Gideon Mann, Alessan-
dro Moschitti, Simone Ponzetto, Jason Smith, Josef
Steinberger, Michael Strube, Jian Su, Yannick Ver-
sley, Xiaofeng Yang, and Michael Wick. 2008. Ex-
ploiting lexical and encyclopedic resources for en-
tity disambiguation: Final report. Technical report,
JHU CLSP 2007 Summer Workshop.
Gerard Salton and Michael McGill. 1983. Introduc-
tion to Modern Information Retrieval. McGraw-
Hill Book Company.
Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Confer-
ence on Natural Language Learning (CONLL).
Zareen Syed, Tim Finin, and Anupam Joshi. 2008.
Wikipedia as an ontology for describing documents.
In Proceedings of the Second International Confer-
ence on Weblogs and Social Media. AAAI Press.
285
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 32?35,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
KELVIN: a tool for automated knowledge base construction
Paul McNamee, James Mayfield
Johns Hopkins University
Human Language Technology Center of Excellence
Tim Finin, Tim Oates
University of Maryland
Baltimore County
Dawn Lawrie
Loyola University Maryland
Tan Xu, Douglas W. Oard
University of Maryland
College Park
Abstract
We present KELVIN, an automated system for
processing a large text corpus and distilling a
knowledge base about persons, organizations,
and locations. We have tested the KELVIN
system on several corpora, including: (a) the
TAC KBP 2012 Cold Start corpus which con-
sists of public Web pages from the University
of Pennsylvania, and (b) a subset of 26k news
articles taken from English Gigaword 5th edi-
tion.
Our NAACL HLT 2013 demonstration per-
mits a user to interact with a set of search-
able HTML pages, which are automatically
generated from the knowledge base. Each
page contains information analogous to the
semi-structured details about an entity that are
present in Wikipedia Infoboxes, along with
hyperlink citations to supporting text.
1 Introduction
The Text Analysis Conference (TAC) Knowledge
Base Population (KBP) Cold Start task1 requires
systems to take set of documents and produce a
comprehensive set of <Subject, Predicate, Object>
triples that encode relationships between and at-
tributes of the named-entities that are mentioned in
the corpus. Systems are evaluated based on the fi-
delity of the constructed knowledge base. For the
2012 evaluation, a fixed schema of 42 relations (or
slots), and their logical inverses was provided, for
example:
? X:Organization employs Y:Person
1See details at http://www.nist.gov/tac/2012/
KBP/task_guidelines/index.html
? X:Person has-job-title title
? X:Organization headquartered-in Y:Location
Multiple layers of NLP software are required for
this undertaking, including at the least: detection of
named-entities, intra-document co-reference resolu-
tion, relation extraction, and entity disambiguation.
To help prevent a bias towards learning about
prominent entities at the expense of generality,
KELVIN refrains from mining facts from sources
such as documents obtained through Web search,
Wikipedia2, or DBpedia.3 Only facts that are as-
serted in and gleaned from the source documents are
posited.
Other systems that create large-scale knowledge
bases from general text include the Never-Ending
Language Learning (NELL) system at Carnegie
Mellon University (Carlson et al, 2010), and the
TextRunner system developed at the University of
Washington (Etzioni et al, 2008).
2 Washington Post KB
No gold-standard KBs were available to us to assist
during the development of KELVIN, so we relied on
qualitative assessment to gauge the effectiveness of
our extracted relations ? by manually examining ten
random samples for each relations, we ascertained
that most relations were between 30-80% accurate.
Although the TAC KBP 2012 Cold Start task was a
pilot evaluation of a new task using a novel evalua-
tion methodology, the KELVIN system did attain the
highest reported F1 scores.4
2http://en.wikipedia.org/
3http://www.dbpedia.org/
40.497 0-hop & 0.363 all-hops, as reported in the prelimi-
nary TAC 2012 Evaluation Results.
32
During our initial development we worked with
a 26,143 document collection of 2010 Washington
Post articles and the system discovered 194,059 re-
lations about 57,847 named entities. KELVIN learns
some interesting, but rather dubious relations from
the Washington Post articles5
? Sen. Harry Reid is an employee of the ?Repub-
lican Party.? Sen. Reid is also an employee of
the ?Democratic Party.?
? Big Foot is an employee of Starbucks.
? MacBook Air is a subsidiary of Apple Inc.
? Jill Biden is married to Jill Biden.
However, KELVIN also learns quite a number of
correct facts, including:
? Warren Buffett owns shares of Berkshire Hath-
away, Burlington Northern Santa Fe, the Wash-
ington Post Co., and four other stocks.
? Jared Fogle is an employee of Subway.
? Freeman Hrabowski works for UMBC,
founded the Meyerhoff Scholars Program, and
graduated from Hampton University and the
University of Illinois.
? Supreme Court Justice Elena Kagan attended
Oxford, Harvard, and Princeton.
? Southwest Airlines is headquartered in Texas.
? Ian Soboroff is a computer scientist6 employed
by NIST.7
3 Pipeline Components
3.1 SERIF
BBN?s SERIF tool8 (Boschee et al, 2005) provides
a considerable suite of document annotations that
are an excellent basis for building a knowledge base.
The functions SERIF can provide are based largely
5All 2010 Washington Post articles from English Gigaword
5th ed. (LDC2011T07).
6Ian is the sole computer scientist discovered in processing
a year of news. In contrast, KELVIN found 52 lobbyists.
7From Washington Post article (WPB ENG 20100506.0012
in LDC2011T07).
8Statistical Entity & Relation Information Finding.
Slotname Count
per:employee of 60,690
org:employees 44,663
gpe:employees 16,027
per:member of 14,613
org:membership 14,613
org:city of headquarters 12,598
gpe:headquarters in city 12,598
org:parents 6,526
org:country of headquarters 4,503
gpe:headquarters in country 4,503
Table 1: Most prevalent slots extracted by SERIF from
the Washington Post texts.
Slotname Count
per:title 44,896
per:employee of 39,101
per:member of 20,735
per:countries of residence 8,192
per:origin 4,187
per:statesorprovinces of residence 3,376
per:cities of residence 3,376
per:country of birth 1,577
per:age 1,233
per:spouse 1,057
Table 2: Most prevalent slots extracted by FACETS from
the Washington Post texts.
on the NIST ACE specification,9 and include: (a)
identifying named-entities and classifying them by
type and subtype; (b) performing intra-document
co-reference analysis, including named mentions,
as well as co-referential nominal and pronominal
mentions; (c) parsing sentences and extracting intra-
sentential relations between entities; and, (d) detect-
ing certain types of events.
In Table 1 we list the most common slots SERIF
extracts from the Washington Post articles.
3.2 FACETS
FACETS, another BBN tool, is an add-on pack-
age that takes SERIF output and produces role and
argument annotations about person noun phrases.
FACETS is implemented using a conditional-
9The principal types of ACE named-entities are per-
sons, organizations, and geo-political entities (GPEs).
GPEs are inhabited locations with a government. See
http://www.itl.nist.gov/iad/mig/tests/ace/
2008/doc/ace08-evalplan.v1.2d.pdf.
33
Figure 1: Simple rendering of KB page about former Florida congressman Joe Scarborough. Many facts are correct
? he lived in and was employed by the State of Florida; he has a brother George; he was a member of the Republican
House of Representatives; and, he is employed by MSNBC.
exponential learner trained on broadcast news. The
attributes FACETS can recognize include general at-
tributes like religion and age (which anyone might
have), as well as role-specific attributes, such as
medical specialty for physicians, or academic insti-
tution for someone associated with an university.
In Table 2 we report the most prevalent slots
FACETS extracts from the Washington Post.10
3.3 CUNY toolkit
To increase our coverage of relations we also in-
tegrated the KBP Slot Filling Toolkit (Chen et al,
2011) developed at the CUNY BLENDER Lab.
Given that the KBP toolkit was designed for the tra-
ditional slot filling task at TAC, this primarily in-
volved creating the queries that the tool expected as
input and parallelizing the toolkit to handle the vast
number of queries issued in the cold start scenarios.
To informally gauge the accuracy of slots
extracted from the CUNY tool, some coarse as-
sessment was done over a small collection of 807
New York Times articles that include the string
?University of Kansas.? From this collection, 4264
slots were identified. Nine different types of slots
were filled in order of frequency: per:title (37%),
per:employee of (23%), per:cities of residence
(17%), per:stateorprovinces of residence (6%),
10Note FACETS can independently extract some slots that
SERIF is capable of discovering (e.g., employment relations).
org:top members/employees (6%), org:member of
(6%), per:countries of residence (2%), per:spouse
(2%), and per:member of (1%). We randomly sam-
pled 10 slot-fills of each type, and found accuracy
to vary from 20-70%.
3.4 Coreference
We used two methods for entity coreference. Un-
der the theory that name ambiguity may not be a
huge problem, we adopted a baseline approach of
merging entities across different documents if their
canonical mentions were an exact string match af-
ter some basic normalizations, such as removing
punctuation and conversion to lower-case charac-
ters. However we also used the JHU HLTCOE
CALE system (Stoyanov et al, 2012), which maps
named-entity mentions to the TAC-KBP reference
KB, which was derived from a 2008 snapshot of En-
glish Wikipedia. For entities that are not found in the
KB, we reverted to exact string match. CALE entity
linking proved to be the more effective approach for
the Cold Start task.
3.5 Timex2 Normalization
SERIF recognizes, but does not normalize, temporal
expressions, so we used the Stanford SUTime pack-
age, to normalize date values.
34
Figure 2: Supporting text for some assertions about Mr. Scarborough. Source documents are also viewable by
following hyperlinks.
3.6 Lightweight Inference
We performed a small amount of light inference to
fill some slots. For example, if we identified that
a person P worked for organization O, and we also
extracted a job title T for P, and if T matched a set
of titles such as president or minister we asserted
that the tuple <O, org:top members employees, P>
relation also held.
4 Ongoing Work
There are a number of improvements that we are un-
dertaking, including: scaling to much larger corpora,
detecting contradictions, expanding the use of infer-
ence, exploiting the confidence of extracted infor-
mation, and applying KELVIN to various genres of
text.
5 Script Outline
The KB generated by KELVIN is best explored us-
ing a Wikipedia metaphor. Thus our demonstration
consists of a web browser that starts with a list of
moderately prominent named-entities that the user
can choose to examine (e.g., investor Warren Buf-
fett, Supreme Court Justice Elena Kagan, Southwest
Airlines Co., the state of Florida). Selecting any
entity takes one to a page displaying its known at-
tributes and relations, with links to documents that
serve as provenance for each assertion. On every
page, each entity is hyperlinked to its own canon-
ical page; therefore the user is able to browse the
KB much as one browses Wikipedia by simply fol-
lowing links. A sample generated page is shown in
Figure 1 and text that supports some of the learned
assertions in the figure is shown in Figure 2. We
also provide a search interface to support jump-
ing to a desired entity and can demonstrate access-
ing the data encoded in the semantic web language
RDF (World Wide Web Consortium, 2013), which
supports ontology browsing and executing complex
SPARQL queries (Prud?Hommeaux and Seaborne,
2008) such as ?List the employers of people living
in Nebraska or Kansas who are older than 40.?
References
E. Boschee, R. Weischedel, and A. Zamanian. 2005. Au-
tomatic information extraction. In Proceedings of the
2005 International Conference on Intelligence Analy-
sis, McLean, VA, pages 2?4.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
Z. Chen, S. Tamang, A. Lee, X. Li, and H. Ji. 2011.
Knowledge Base Population (KBP) Toolkit @ CUNY
BLENDER LAB Manual.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68?74, Decem-
ber.
E Prud?Hommeaux and A. Seaborne. 2008. SPARQL
query language for RDF. Technical report, World
Wide Web Consortium, January.
Veselin Stoyanov, James Mayfield, Tan Xu, Douglas W.
Oard, Dawn Lawrie, Tim Oates, and Tim Finin. 2012.
A context-aware approach to entity linking. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge Ex-
traction, AKBC-WEKEX ?12, pages 62?67, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
World Wide Web Consortium. 2013. Resource Descrip-
tion Framework Specification. ?http://http://
www.w3.org/RDF/. ?[Online; accessed 8 April,
2013]?.
35
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 44?52, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems
Lushan Han, Abhay Kashyap, Tim Finin
Computer Science and
Electrical Engineering
University of Maryland, Baltimore County
Baltimore MD 21250
{lushan1,abhay1,finin}@umbc.edu
James Mayfield and Jonathan Weese
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore MD 21211
mayfield@jhu.edu, jonny@cs.jhu.edu
Abstract
We describe three semantic text similarity
systems developed for the *SEM 2013 STS
shared task and the results of the correspond-
ing three runs. All of them shared a word sim-
ilarity feature that combined LSA word sim-
ilarity and WordNet knowledge. The first,
which achieved the best mean score of the 89
submitted runs, used a simple term alignment
algorithm augmented with penalty terms. The
other two runs, ranked second and fourth, used
support vector regression models to combine
larger sets of features.
1 Introduction
Measuring semantic text similarity has been a re-
search subject in natural language processing, infor-
mation retrieval and artificial intelligence for many
years. Previous efforts have focused on compar-
ing two long texts (e.g., for document classification)
or a short text with a long text (e.g., Web search),
but there are a growing number of tasks requiring
computing the semantic similarity between two sen-
tences or other short text sequences. They include
paraphrase recognition (Dolan et al, 2004), Twitter
tweets search (Sriram et al, 2010), image retrieval
by captions (Coelho et al, 2004), query reformula-
tion (Metzler et al, 2007), automatic machine trans-
lation evaluation (Kauchak and Barzilay, 2006) and
schema matching (Han et al, 2012).
There are three predominant approaches to com-
puting short text similarity. The first uses informa-
tion retrieval?s vector space model (Meadow, 1992)
in which each text is modeled as a ?bag of words?
and represented using a vector. The similarity be-
tween two texts is then computed as the cosine
similarity of the vectors. A variation on this ap-
proach leverages web search results (e.g., snip-
pets) to provide context for the short texts and en-
rich their vectors using the words in the snippets
(Sahami and Heilman, 2006). The second approach
is based on the assumption that if two sentences or
other short text sequences are semantically equiva-
lent, we should be able to align their words or ex-
pressions. The alignment quality can serve as a
similarity measure. This technique typically pairs
words from the two texts by maximizing the sum-
mation of the word similarity of the resulting pairs
(Mihalcea et al, 2006). The third approach com-
bines different measures and features using machine
learning models. Lexical, semantic and syntactic
features are computed for the texts using a variety
of resources and supplied to a classifier, which then
assigns weights to the features by fitting the model
to training data (Saric et al, 2012).
For evaluating different approaches, the 2013 Se-
mantic Textual Similarity (STS) task asked auto-
matic systems to compute sentence similarity ac-
cording to a scale definition ranging from 0 to 5,
with 0 meaning unrelated and 5 semantically equiv-
alent (Agirre et al, 2012; Agirre et al, 2013). The
example sentence pair ?The woman is playing the
violin? and ?The young lady enjoys listening to the
guitar? is scored as only 1 and the pair ?The bird is
bathing in the sink? and ?Birdie is washing itself in
the water basin? is given a score of 5.
The vector-space approach tends to be too shallow
for the task, since solving it well requires discrimi-
nating word-level semantic differences and goes be-
44
yond simply comparing sentence topics or contexts.
Our first run uses an align-and-penalize algorithm,
which extends the second approach by giving penal-
ties to the words that are poorly aligned. Our other
two runs use a support vector regression model to
combine a large number of general and domain spe-
cific features. An important and fundamental feature
used by all three runs is a powerful semantic word
similarity model based on a combination of Latent
Semantic Analysis (LSA) (Deerwester et al, 1990;
Landauer and Dumais, 1997) and knowledge from
WordNet (Miller, 1995).
The remainder of the paper proceeds as follows.
Section 2 presents the hybrid word similarity model.
Section 3 describes the align-and-penalize approach
used for the PairingWords run. In Section 4 we de-
scribe the SVM approach used for the Galactus and
Saiyan runs. Section 5 discusses the results and is
followed by a short conclusion.
2 Semantic Word Similarity Model
Our word similarity model was originally developed
for the Graph of Relations project (UMBC, 2013a)
which maps informal queries with English words
and phrases for an RDF linked data collection into
a SPARQL query. For this, we wanted a metric
in which only the semantics of a word is consid-
ered and not its lexical category. For example, the
verb ?marry? should be semantically similar to the
noun ?wife?. Another desiderata was that the met-
ric should give highest scores and lowest scores in
its range to similar and non-similar words, respec-
tively. In this section, we describe how we con-
structed the model by combining LSA word simi-
larity and WordNet knowledge.
2.1 LSA Word Similarity
LSA Word Similarity relies on the distributional hy-
pothesis that words occurring in the same contexts
tend to have similar meanings (Harris, 1968).
2.1.1 Corpus Selection and Processing
In order to produce a reliable word co-occurrence
statistics, a very large and balanced text corpus is
required. After experimenting with several cor-
pus choices including Wikipedia, Project Gutenberg
e-Books (Hart, 1997), ukWaC (Baroni et al, 2009),
Reuters News stories (Rose et al, 2002) and LDC
gigawords, we selected the Web corpus from the
Stanford WebBase project (Stanford, 2001). We
used the February 2007 crawl, which is one of the
largest collections and contains 100 million web
pages from more than 50,000 websites. The Web-
Base project did an excellent job in extracting tex-
tual content from HTML tags but still has abun-
dant text duplications, truncated text, non-English
text and strange characters. We processed the collec-
tion to remove undesired sections and produce high
quality English paragraphs. We detected paragraphs
using heuristic rules and only retrained those whose
length was at least two hundred characters. We elim-
inated non-English text by checking the first twenty
words of a paragraph to see if they were valid En-
glish words. We used the percentage of punctuation
characters in a paragraph as a simple check for typi-
cal text. We removed duplicated paragraphs using a
hash table. Finally, we obtained a three billion words
corpus of good quality English, which is available at
(Han and Finin, 2013).
2.1.2 Word Co-Occurrence Generation
We performed POS tagging and lemmatization on
the WebBase corpus using the Stanford POS tagger
(Toutanova et al, 2000). Word/term co-occurrences
are counted in a moving window of a fixed size
that scans the entire corpus1. We generated two co-
occurrence models using window sizes ?1 and ?4
because we observed different natures of the models.
?1 window produces a context similar to the depen-
dency context used in (Lin, 1998a). It provides a
more precise context but only works for comparing
words within the same POS. In contrast, a context
window of ?4 words allows us to compute semantic
similarity between words with different POS.
Our word co-occurrence models were based on
a predefined vocabulary of more than 22,000 com-
mon English words and noun phrases. We also
added to it more than 2,000 verb phrases extracted
from WordNet. The final dimensions of our word
co-occurrence matrices are 29,000 ? 29,000 when
words are POS tagged. Our vocabulary includes
only open-class words (i.e. nouns, verbs, adjectives
and adverbs). There are no proper nouns in the vo-
cabulary with the only exception of country names.
1We used a stop-word list consisting of only the three arti-
cles ?a?, ?an? and ?the?.
45
Word Pair ?4 model ?1 model
1. doctor NN, physician NN 0.775 0.726
2. car NN, vehicle NN 0.748 0.802
3. person NN, car NN 0.038 0.024
4. car NN, country NN 0.000 0.016
5. person NN, country NN 0.031 0.069
6. child NN, marry VB 0.098 0.000
7. wife NN, marry VB 0.548 0.274
8. author NN, write VB 0.364 0.128
9. doctor NN, hospital NN 0.473 0.347
10. car NN, driver NN 0.497 0.281
Table 1: Ten examples from the LSA similarity model
2.1.3 SVD Transformation
Singular Value Decomposition (SVD) has been
found to be effective in improving word similar-
ity measures (Landauer and Dumais, 1997). SVD
is typically applied to a word by document ma-
trix, yielding the familiar LSA technique. In
our case we apply it to our word by word ma-
trix. In literature, this variation of LSA is some-
times called HAL (Hyperspace Analog to Lan-
guage) (Burgess et al, 1998).
Before performing SVD, we transform the raw
word co-occurrence count fij to its log frequency
log(fij + 1). We select the 300 largest singular val-
ues and reduce the 29K word vectors to 300 dimen-
sions. The LSA similarity between two words is de-
fined as the cosine similarity of their corresponding
word vectors after the SVD transformation.
2.1.4 LSA Similarity Examples
Ten examples obtained using LSA similarity are
given in Table 1. Examples 1 to 6 illustrate that the
metric has a good property of differentiating simi-
lar words from non-similar words. Examples 7 and
8 show that the ?4 model can detect semantically
similar words even with different POS while the ?1
model yields much worse performance. Example 9
and 10 show that highly related but not substitutable
words can also have a strong similarity but the ?1
model has a better performance in discriminating
them. We call the ?1 model and the ?4 model
as concept similarity and relation similarity respec-
tively since the ?1 model has a good performance
on nouns and the ?4 model is good at computing
similarity between relations regardless of POS of
words, such as ?marry to? and ?is the wife of?.
2.2 Combining with WordNet Knowledge
Statistical word similarity measures have limita-
tions. Related words can have similarity scores as
high as what similar words get, as illustrated by
?doctor? and ?hospital? in Table 1. Word similar-
ity is typically low for synonyms having many word
senses since information about different senses are
mashed together (Han et al, 2013). By using Word-
Net, we can reduce the above issues.
2.2.1 Boosting LSA similarity using WordNet
We increase the similarity between two words if any
of the following relations hold.
? They are in the same WordNet synset.
? One word is the direct hypernym of the other.
? One word is the two-link indirect hypernym of
the other.
? One adjective has a direct similar to relation
with the other.
? One adjective has a two-link indirect similar to
relation with the other.
? One word is a derivationally related form of the
other.
? One word is the head of the gloss of the other
or its direct hypernym or one of its direct hy-
ponyms.
? One word appears frequently in the glosses of
the other and its direct hypernym and its direct
hyponyms.
We use the algorithm described in (Collins, 1999)
to find a word gloss header. We require a minimum
LSA similarity of 0.1 between the two words to filter
out noisy data when extracting WordNet relations.
We define a word?s ?significant senses? to deal
with the problem of WordNet trivial senses. The
word ?year?, for example, has a sense ?a body of
students who graduate together? which makes it a
synonym of the word ?class?. This causes problems
because ?year? and ?class? are not similar, in gen-
eral. A sense is significant, if any of the following
conditions are met: (i) it is the first sense; (ii) its
WordNet frequency count is not less than five; or
(iii) its word form appears first in its synset?s word
46
form list and it has a WordNet sense number less
than eight.
We assign path distance of zero to the category
1, path distance of one to the category 2, 4 and 6,
and path distance of two to the other categories. The
new similarity between word x and y by combining
LSA similarity and WordNet relations is shown in
the following equation
sim?(x, y) = simLSA(x, y) + 0.5e??D(x,y) (1)
where D(x, y) is the minimal path distance between
x and y. Using the e??D(x,y) to transform simple
shortest path length has been demonstrated to be
very effective according to (Li et al, 2003). The pa-
rameter ? is set to be 0.25, following their experi-
mental results. The ceiling of sim?(x, y) remains
1.0 and we simply cut the excess.
2.2.2 Dealing with words of many senses
For a word w with many WordNet senses (currently
ten or more), we use its synonyms with fewer senses
(at most one third of that of w) as its substitutions in
computing similarity with another word. Let Sx and
Sy be the sets of all such substitutions of the words
x and y respectively. The new similarity is obtained
using Equation 2.
sim(x, y) = max( max
sx?Sx?{x}
sim?(sx, y),
max
sy?Sy?{y}
sim?(x, sy)) (2)
An online demonstration of a similar model
developed for the GOR project is available
(UMBC, 2013b), but it lacks some of this version?s
features.
3 Align-and-Penalize Approach
First we hypothesize that STS similarity between
two sentences can be computed using
STS = T ? P ? ? P ?? (3)
where T is the term alignments score, P ? is the
penalty for bad term alignments and P ?? is the
penalty for syntactic contradictions led by the align-
ments. However P ?? had not been fully implemented
and was not used in our STS submissions. We show
it here just for completeness.
3.1 Aligning terms in two sentences
We start by applying the Stanford POS tagger to tag
and lemmatize the input sentences. We use our pre-
defined vocabulary, POS tagging data and simple
regular expressions to recognize multi-word terms
including noun and verb phrases, proper nouns,
numbers and time. We ignore adverbs with fre-
quency count larger than 500, 000 in our corpus and
stop words with general meaning.
Equation 4 shows our aligning function g which
finds the counterpart of term t ? S in sentence S?.
g(t) = argmax
t??S?
sim?(t, t?) (4)
sim?(t, t?) is a wrapper function over sim(x, y) in
Equation 2 that uses the relation similarity model.
It compares numerical and time terms by their val-
ues. If they are equal, 1 is returned; otherwise 0.
sim?(t, t?) provides limited comparison over pro-
nouns. It returns 1 between subject pronouns I, we,
they, he, she and their corresponding object pro-
nouns. sim?(t, t?) also outputs 1 if one term is the
acronym of the other term, or if one term is the head
of the other term, or if two consecutive terms in a
sentence match a single term in the other sentence
(e.g. ?long term? and ?long-term?). sim?(t, t?) fur-
ther adds support for matching words2 not presented
in our vocabulary using a simple string similarity al-
gorithm. It computes character bigram sets for each
of the two words without using padding characters.
Dice coefficient is then applied to get the degree of
overlap between the two sets. If it is larger than two
thirds, sim?(t, t?) returns a score of 1; otherwise 0.
g(t) is direction-dependent and does not achieve
one-to-one mapping. This property is useful in mea-
suring STS similarity because two sentences are of-
ten not exact paraphrases of one another. Moreover,
it is often necessary to align multiple terms in one
sentence to a single term in the other sentence, such
as when dealing with repetitions and anaphora or,
e.g., mapping ?people writing books? to ?writers?.
Let S1 and S2 be the sets of terms in two input
sentences. We define term alignments score T as the
following equation shows.
?
t?S1 sim
?(t, g(t))
2 ? |S1|
+
?
t?S2 sim
?(t, g(t))
2 ? |S2|
(5)
2We use the regular expression ?[A-Za-z][A-Za-z]*? to
identify them.
47
3.2 Penalizing bad term alignments
We currently treat two kinds of alignments as ?bad?,
as described in Equation 6. For the set Bi, we have
an additional restriction that neither of the sentences
has the form of a negation. In defining Bi, we used
a collection of antonyms extracted from WordNet
(Mohammad et al, 2008). Antonym pairs are a spe-
cial case of disjoint sets. The terms ?piano? and ?vi-
olin? are also disjoint but they are not antonyms. In
order to broaden the set Bi we will need to develop
a model that can determine when two terms belong
to disjoint sets.
Ai =
{
?t, g(t)? |t ? Si ? sim?(t, g(t)) < 0.05
}
Bi = {?t, g(t)? |t ? Si ? t is an antonymof g(t)}
i ? {1, 2} (6)
We show how we compute P ? in Equation 7.
PAi =
?
?t,g(t)??Ai (sim
?(t, g(t)) +wf (t) ? wp(t))
2 ? |Si|
PBi =
?
?t,g(t)??Bi (sim
?(t, g(t)) + 0.5)
2 ? |Si|
P ? = PA1 + PB1 + PA2 + PB2 (7)
The wf (t) and wp(t) terms are two weighting func-
tions on the term t. wf (t) inversely weights the log
frequency of term t and wp(t) weights t by its part of
speech tag, assigning 1.0 to verbs, nouns, pronouns
and numbers, and 0.5 to terms with other POS tags.
4 SVM approach
We used the scores from the align-and-penalize ap-
proach along with several other features to learn a
support vector regression model. We started by ap-
plying the following preprocessing steps.
? The sentences were tokenized and POS-tagged
using NLTK?s (Bird, 2006) default Penn Tree-
bank based tagger.
? Punctuation characters were removed from the
tokens except for the decimal point in numbers.
? All numbers written as words were converted
into numerals, e.g., ?2.2 million? was replaced
by ?2200000? and ?fifty six? by ?56?.
? All mentions of time were converted into mil-
itary time, e.g., ?5:40pm? was replaced by
?1740? and ?1h30am? by ?0130?.
? Abbreviations were expanded using a compiled
list of commonly used abbreviations.
? About 80 stopwords were removed.
4.1 Ngram Matching
The sentence similarities are derived as a function of
the similarity scores of their corresponding paired
word ngrams. These features closely resemble the
ones used in (Saric et al, 2012). For our system, we
used unigrams, bigrams, trigrams and skip-bigrams,
a special form of bigrams which allow for arbitrary
distance between two tokens.
An ngram from the first sentence is exclusively
paired with an ngram from the second which has the
highest similarity score. Several similarity metrics
are used to generate different features. For bigrams,
trigrams and skip-bigrams, the similarity score for
two ngrams is computed as the arithmetic mean of
the similarity scores of the individual words they
contain. For example, for the bigrams ?he ate? and
?she spoke?, the similarity score is the average of the
similarity scores between the words ?he? and ?she?
and the words ?ate? and ?spoke?.
The ngram overlap of two sentences is defined
as ?the harmonic mean of the degree to which
the second sentence covers the first and the de-
gree to which the first sentence covers the second?
(Saric et al, 2012). Given sets S1 and S2 containing
ngrams from sentences 1 and 2, and sets P1 and P2
containing their paired ngrams along with their sim-
ilarity scores, the ngram overlap score for a given
ngram type is computed using the following equa-
tion.
HM
(
?
n?P1 w(n).sim(n)
?
n?S1 w(n)
,
?
n?P2 w(n).sim(n)
?
n?S2 w(n)
)
(8)
In this formula, HM is the harmonic mean, w(n) is
the weight assigned for the given ngram and sim(n)
is the similarity score of the paired word.
By default, all the ngrams are assigned a uniform
weight of 1. But since different words carry differ-
ent amount of information, e.g. ?acclimatize? vs.
?take?, ?cardiologist? vs. ?person?, we also use in-
formation content as weights. The information con-
tent of a word is as defined in (Saric et al, 2012).
ic(w) = ln
(
?
w??C freq(w
?
)
freq(w)
)
(9)
48
Here C is the set of words in the corpus and freq(w)
is the frequency of a word in the corpus. The
weight of an ngram is the sum of its constituent word
weights. We use refined versions of Google ngram
frequencies (Michel et al, 2011) from (Mem, 2008)
and (Saric et al, 2012) to get the information con-
tent of the words. Words not in this list are assigned
the average weight.
We used several word similarity metrics for
ngram matching apart from the similarity metric de-
scribed in section 2. Our baseline similarity metric
was an exact string match which assigned a score
of 1 if two tokens contained the same sequence of
characters and 0 otherwise. We also used NLTK?s
library to compute WordNet based similarity mea-
sures such as Path Distance Similarity, Wu-Palmer
Similarity (Wu and Palmer, 1994) and Lin Similar-
ity (Lin, 1998b). For Lin Similarity, the Semcor cor-
pus was used for the information content of words.
4.2 Contrast Scores
We computed contrast scores between two sen-
tences using three different lists of antonym pairs
(Mohammad et al, 2008). We used a large list con-
taining 3.5 million antonym pairs, a list of about
22,000 antonym pairs from Wordnet and a list of
50,000 pairs of words with their degree of contrast.
Contrast scores between two sentences were derived
as a function of the number of antonym pairs be-
tween them similar to equation 8 but with negative
values to indicate contrast scores.
4.3 Features
We constructed 52 features from different combina-
tions of similarity metrics, their parameters, ngram
types (unigram, bigram, trigram and skip-bigram)
and ngram weights (equal weight vs. information
content) for all sentence pairs in the training data.
? We used scores from the align-and-penalize ap-
proach directly as a feature.
? Using exact string match over different ngram
types and ngram weights, we extracted eight
features (4 ? 4). We also developed four addi-
tional features (2 ? 2) by includin stopwords in
bigrams and trigrams, motivated by the nature
of MSRvid dataset.
? We used the LSA boosted similarity metric in
three modes: concept similarity, relation simi-
larity and mixed mode, which used the concept
model for nouns and relation model for verbs,
adverbs and adjectives. A total of 24 features
were extracted (4 ? 2 ? 3).
? For Wordnet-based similarity measures, we
used uniform weights for Path and Wu-Palmer
similarity and used the information content of
words (derived from the Semcor corpus) for
Lin similarity. Skip bigrams were ignored and
a total of nine features were produced (3 ? 3).
? Contrast scores used three different lists of
antonym pairs. A total of six features were ex-
tracted using different weight values (3 ? 2).
4.4 Support Vector Regression
The features described in 4.3 were used in dif-
ferent combinations to train several support vec-
tor regression (SVR) models. We used LIBSVM
(Chang and Lin, 2011) to learn the SVR models and
ran a grid search provided by (Saric et al, 2012) to
find the optimal values for the parameters C , g and
p. These models were then used to predict the scores
for the test sets.
The Galactus system was trained on all of STS
2012 data and used the full set of 52 features. The
FnWN dataset was handled slightly differently from
the others. We observed that terms like ?frame? and
?entity? were used frequently in the five sample sen-
tence pairs and treated them as stopwords. To ac-
commodate the vast difference in sentence lengths,
equation 8 was modified to compute the arithmetic
mean instead of the harmonic mean.
The Saiyan system employed data-specific train-
ing and features. The training sets were subsets of
the supplied STS 2012 dataset. More specifically,
the model for headlines was trained on 3000 sen-
tence pairs from MSRvid and MSRpar, SMT used
1500 sentence pairs from SMT europarl and SMT
news, while OnWN used only the 750 OnWN sen-
tence pairs from last year. The FnWN scores were
directly used from the Align-and-Penalize approach.
None of the models for Saiyan used contrast fea-
tures and the model for SMT also ignored similarity
scores from exact string match metric.
49
5 Results and discussion
Table 2 presents the official results of our three runs
in the 2013 STS task. Each entry gives a run?s Pear-
son correlation on a dataset as well as the rank of the
run among all 89 runs submitted by the 35 teams.
The last row shows the mean of the correlations and
the overall ranks of our three runs.
We tested performance of the align-and-penalize
approach on all of the 2012 STS datasets. It ob-
tained correlation values of 0.819 on MSRvid, 0.669
on MSRpar, 0.553 on SMTeuroparl, 0.567 on SMT-
news and 0.722 on OnWN for the test datasets, and
correlation values of 0.814 on MSRvid, 0.707 on
MSRpar and 0.646 on SMTeuroparl for the training
datasets. The performance of the approach without
using the antonym penalty is also tested, producing
correlation scores of 0.795 on MSRvid, 0.667 on
MSRpar, 0.554 on SMTeuroparl, 0.566 on SMTnew
and 0.727 on OnWN, for the test datasets, and 0.794
on MSRvid, 0.707 on MSRpar and 0.651 on SM-
Teuroparl for the training datasets. The average of
the correlation scores on all eight datasets with and
without the antonym penalty is 0.6871 and 0.6826,
respectively. Since the approach?s performance was
only slightly improved when the antonym penalty
was used, we decided to not include this penalty in
our PairingWords run in the hope that its simplicity
would make it more robust.
During development, our SVM approach
achieved correlations of 0.875 for MSRvid, 0.699
for MSRpar, 0.559 for SMTeuroparl, 0.625 for
SMTnews and 0.729 for OnWN on the 2012 STS
test data. Models were trained on their respective
training sets while SMTnews used SMTeuroparl and
OnWN used all the training sets. We experimented
with different features and training data to study
their influence on the performance of the models.
We found that the unigram overlap feature, based on
boosted LSA similarity and weighted by informa-
tion content, could independently achieve very high
correlations. Including more features improved the
accuracy slightly and in some cases added noise.
The difficulty in selecting data specific features and
training for novel datasets is indicated by Saiyan?s
contrasting performance on headlines and OnWN
datasets. The model used for Headlines was trained
on data from seemingly different domains (MSRvid,
Dataset Pairing Galactus Saiyan
Headlines (750 pairs) 0.7642 (3) 0.7428 (7) 0.7838 (1)
OnWN (561 pairs) 0.7529 (5) 0.7053 (12) 0.5593 (36)
FNWN (189 pairs) 0.5818 (1) 0.5444 (3) 0.5815 (2)
SMT (750 pairs) 0.3804 (8) 0.3705 (11) 0.3563 (16)
Weighted mean 0.6181 (1) 0.5927 (2) 0.5683 (4)
Table 2: Performance of our three systems on the four
test sets.
MSRpar) while OnWN was trained only on OnWN
from STS 2012. When the model for headlines
dataset was used to predict the scores for OnWN,
the correlation jumped from 0.55 to 0.71 indicating
that the earlier model suffered from overfitting.
Overfitting is not evident in the performance of
PairingWords and Galactus, which have more con-
sistent performance over all datasets. The relatively
simple PairingWords system has two advantages: it
is faster, since the current Galactus requires comput-
ing a large number of features; and its performance
is more predictable, since training is not needed thus
eliminating noise induced from diverse training sets.
6 Conclusion
We described three semantic text similarity systems
developed for the *SEM 2013 STS shared task and
the results of the corresponding three runs we sub-
mitted. All of the systems used a lexical similarity
feature that combined POS tagging, LSA word sim-
ilarity and WordNet knowledge.
The first run, which achieved the best mean score
out of all 89 submissions, used a simple term align-
ment algorithm augmented with two penalty met-
rics. The other two runs, ranked second and fourth
out of all submissions, used support vector regres-
sion models based on a set of more than 50 addi-
tional features. The runs differed in their feature
sets, training data and procedures, and parameter
settings.
Acknowledgments
This research was supported by AFOSR award
FA9550-08-1-0265 and a gift from Microsoft.
50
References
[Agirre et al2012] Eneko Agirre, Mona Diab, Daniel Cer,
and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task
6: a pilot on semantic textual similarity. In Proceed-
ings of the First Joint Conference on Lexical and Com-
putational Semantics, pages 385?393. Association for
Computational Linguistics.
[Agirre et al2013] Eneko Agirre, Daniel Cer, Mona Diab,
Aitor Gonzalez-Agirre, and Weiwei Guo. 2013. *sem
2013 shared task: Semantic textual similarity, includ-
ing a pilot on typed-similarity. In *SEM 2013: The
Second Joint Conference on Lexical and Computa-
tional Semantics. Association for Computational Lin-
guistics.
[Baroni et al2009] M. Baroni, S. Bernardini, A. Fer-
raresi, and E. Zanchetta. 2009. The wacky wide
web: A collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
[Bird2006] Steven Bird. 2006. Nltk: the natural lan-
guage toolkit. In Proceedings of the COLING/ACL on
Interactive presentation sessions, COLING-ACL ?06,
pages 69?72, Stroudsburg, PA, USA. Association for
Computational Linguistics.
[Burgess et al1998] C. Burgess, K. Livesay, and K. Lund.
1998. Explorations in context space: Words, sen-
tences, discourse. Discourse Processes, 25:211?257.
[Chang and Lin2011] Chih-Chung Chang and Chih-Jen
Lin. 2011. LIBSVM: A library for support vector ma-
chines. ACM Transactions on Intelligent Systems and
Technology, 2:27:1?27:27.
[Coelho et al2004] T.A.S. Coelho, Pa?vel Pereira Calado,
Lamarque Vieira Souza, Berthier Ribeiro-Neto, and
Richard Muntz. 2004. Image retrieval using multiple
evidence ranking. IEEE Trans. on Knowl. and Data
Eng., 16(4):408?417.
[Collins1999] Michael John Collins. 1999. Head-driven
statistical models for natural language parsing. Ph.D.
thesis, University of Pennsylvania.
[Deerwester et al1990] Scott Deerwester, Susan T. Du-
mais, George W. Furnas, Thomas K. Landauer, and
Richard Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(6):391?407.
[Dolan et al2004] Bill Dolan, Chris Quirk, and Chris
Brockett. 2004. Unsupervised construction of
large paraphrase corpora: exploiting massively paral-
lel news sources. In Proceedings of the 20th interna-
tional conference on Computational Linguistics, COL-
ING ?04. Association for Computational Linguistics.
[Ganitkevitch et al2013] Juri Ganitkevitch, Ben-
jamin Van Durme, and Chris Callison-Burch. 2013.
PPDB: The paraphrase database. In HLT-NAACL
2013.
[Han and Finin2013] Lushan Han and Tim Finin. 2013.
UMBC webbase corpus. http://ebiq.org/r/351.
[Han et al2012] Lushan Han, Tim Finin, and Anupam
Joshi. 2012. Schema-free structured querying of db-
pedia data. In Proceedings of the 21st ACM interna-
tional conference on Information and knowledge man-
agement, pages 2090?2093. ACM.
[Han et al2013] Lushan Han, Tim Finin, Paul McNamee,
Anupam Joshi, and Yelena Yesha. 2013. Improving
Word Similarity by Augmenting PMI with Estimates
of Word Polysemy. IEEE Transactions on Knowledge
and Data Engineering, 25(6):1307?1322.
[Harris1968] Zellig Harris. 1968. Mathematical Struc-
tures of Language. Wiley, New York, USA.
[Hart1997] M. Hart. 1997. Project gutenberg electronic
books. http://www.gutenberg.org/wiki/Main Page.
[Kauchak and Barzilay2006] David Kauchak and Regina
Barzilay. 2006. Paraphrasing for automatic evalua-
tion. In HLT-NAACL ?06, pages 455?462.
[Landauer and Dumais1997] T. Landauer and S. Dumais.
1997. A solution to plato?s problem: The latent se-
mantic analysis theory of the acquisition, induction,
and representation of knowledge. In Psychological
Review, 104, pages 211?240.
[Li et al2003] Y. Li, Z.A. Bandar, and D. McLean.
2003. An approach for measuring semantic similar-
ity between words using multiple information sources.
IEEE Transactions on Knowledge and Data Engineer-
ing, 15(4):871?882.
[Lin1998a] Dekang Lin. 1998a. Automatic retrieval and
clustering of similar words. In Proc. 17th Int. Conf. on
Computational Linguistics, pages 768?774, Montreal,
CN.
[Lin1998b] Dekang Lin. 1998b. An information-
theoretic definition of similarity. In Proceedings of the
Fifteenth International Conference on Machine Learn-
ing, ICML ?98, pages 296?304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
[Meadow1992] Charles T. Meadow. 1992. Text Informa-
tion Retrieval Systems. Academic Press, Inc.
[Mem2008] 2008. Google word frequency counts.
http://bit.ly/10JdTRz.
[Metzler et al2007] Donald Metzler, Susan Dumais, and
Christopher Meek. 2007. Similarity measures for
short segments of text. In Proceedings of the 29th
European conference on IR research, pages 16?27.
Springer-Verlag.
[Michel et al2011] Jean-Baptiste Michel, Yuan K. Shen,
Aviva P. Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale Hoiberg,
Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker,
51
Martin A. Nowak, and Erez L. Aiden. 2011. Quan-
titative analysis of culture using millions of digitized
books. Science, 331(6014):176?182, January 14.
[Mihalcea et al2006] Rada Mihalcea, Courtney Corley,
and Carlo Strapparava. 2006. Corpus-based and
knowledge-based measures of text semantic similarity.
In Proceedings of the 21st national conference on Ar-
tificial intelligence, pages 775?780. AAAI Press.
[Miller1995] G.A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):41.
[Mohammad et al2008] Saif Mohammad, Bonnie Dorr,
and Graeme Hirst. 2008. Computing word-pair
antonymy. In Proc. Conf. on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-2008), October.
[Rose et al2002] Tony Rose, Mark Stevenson, and Miles
Whitehead. 2002. The reuters corpus volume 1 - from
yesterdays news to tomorrows language resources. In
In Proceedings of the Third International Conference
on Language Resources and Evaluation, pages 29?31.
[Sahami and Heilman2006] Mehran Sahami and Timo-
thy D. Heilman. 2006. A web-based kernel function
for measuring the similarity of short text snippets. In
Proceedings of the 15th international conference on
World Wide Web, WWW ?06, pages 377?386. ACM.
[Saric et al2012] Frane Saric, Goran Glavas, Mladen
Karan, Jan Snajder, and Bojana Dalbelo Basic. 2012.
Takelab: systems for measuring semantic text simi-
larity. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics, pages 441?
448. Association for Computational Linguistics.
[Sriram et al2010] Bharath Sriram, Dave Fuhry, Engin
Demir, Hakan Ferhatosmanoglu, and Murat Demirbas.
2010. Short text classification in twitter to improve
information filtering. In Proceedings of the 33rd in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 841?842.
ACM.
[Stanford2001] Stanford. 2001. Stanford WebBase
project. http://bit.ly/WebBase.
[Toutanova et al2000] Kristina Toutanova, Dan
Klein, Christopher Manning, William Mor-
gan, Anna Rafferty, and Michel Galley. 2000.
Stanford log-linear part-of-speech tagger.
http://nlp.stanford.edu/software/tagger.shtml.
[UMBC2013a] UMBC. 2013a. Graph of relations
project. http://ebiq.org/j/95.
[UMBC2013b] UMBC. 2013b. Semantic similarity
demonstration. http://swoogle.umbc.edu/SimService/.
[Wu and Palmer1994] Z. Wu and M. Palmer. 1994. Verb
semantic and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL-1994), pages 133?138, Las
Cruces (Mexico).
52
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 416?423,
Dublin, Ireland, August 23-24, 2014.
Meerkat Mafia: Multilingual and Cross-Level
Semantic Textual Similarity Systems
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer Sleeman,
Taneeya Satyapanich, Sunil Gandhi and Tim Finin
University of Maryland, Baltimore County
Baltimore, MD 21250 USA
{abhay1,lushan1,ryus,jsleem1,taneeya1,sunilga1,finin}@umbc.edu
Abstract
We describe UMBC?s systems developed
for the SemEval 2014 tasks on Multi-
lingual Semantic Textual Similarity (Task
10) and Cross-Level Semantic Similarity
(Task 3). Our best submission in the
Multilingual task ranked second in both
English and Spanish subtasks using an
unsupervised approach. Our best sys-
tems for Cross-Level task ranked second
in Paragraph-Sentence and first in both
Sentence-Phrase and Word-Sense subtask.
The system ranked first for the Phrase-
Word subtask but was not included in the
official results due to a late submission.
1 Introduction
We describe the semantic text similarity systems
we developed for two of the SemEval tasks for the
2014 International Workshop on Semantic Evalu-
ation. We developed systems for task 3, Cross-
Level Semantic Similarity (Jurgens et al., 2014),
and task 10, Multilingual Semantic Textual Simi-
larity (Agirre et al., 2014). A key component in
all the systems was an enhanced version of the
word similarity system used in our entry (Han et
al., 2013b) in the 2013 SemEval Semantic Textual
Similarity task.
Our best system in the Multilingual Semantic
Textual Similarity task used an unsupervised ap-
proach and ranked second in both the English and
Spanish subtasks. In the Cross-Level Semantic
Similarity task we developed a number of new al-
gorithms and used new linguistic data resources.
In this task, our best systems ranked second in
the Paragraph-Sentence task, first in the Sentence-
Phrase task and first in the Word-Sense task. The
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails:http://creativecommons.org/licenses/by/4.0/
system ranked first for the Phrase-Word task but
was not included in the official results due to a late
submission.
The remainder of the paper proceeds as follows.
Section 2 describes our word similarity model and
it?s wrapper to deal with named entities and out
of vocabulary words. Sections 3 and 4 describe
how we extended the word similarity model for
the specific tasks. Section 5 presents the results
we achieved on these tasks along with instances
where the system failed. Section 6 highlights our
future plans for improving the system.
2 Semantic Word Similarity Model
2.1 LSA Word Similarity Model
Our word similarity model is a revised version of
the one we used in the 2013 *SEM semantic text
similarity task. This was in turn derived from
a system developed for the Graph of Relations
project (UMBC, 2013b). For SemEval, we wanted
a measure that considered a word?s semantics but
not its lexical category, e.g., the verb ?marry?
should be semantically similar to the noun ?wife?.
An online demonstration of a similar model de-
veloped for the GOR project is available (UMBC,
2013a), but it lacks some of this version?s features.
LSA-based word similarity. LSA Word Simi-
larity relies on the distributional hypothesis that
words occurring in the same context tend to have
similar meanings (Harris, 1968). LSA relies on the
fact that words that are semantically similar (e.g.,
cat and feline or nurse and doctor) are more likely
to occur near one another in text. Thus evidence
for word similarity can be computed from a statis-
tical analysis of a large text corpus.
We extracted raw word co-occurrence statis-
tics from a portion of the 2007 crawl of the Web
corpus from the Stanford WebBase project (Stan-
ford, 2001). We processed the collection to re-
move some undesirable elements (text duplica-
416
Word pair ?4 model ?1 model
1 doctor NN, physician NN 0.775 0.726
2 car NN, vehicle NN 0.748 0.802
3 person NN, car NN 0.038 0.024
4 car NN, country NN 0.000 0.016
5 person NN, country NN 0.031 0.069
6 child NN, marry VB 0.098 0.000
7 wife NN, marry VB 0.548 0.274
8 author NN, write VB 0.364 0.128
9 doctor NN, hospital NN 0.473 0.347
10 car NN, driver NN 0.497 0.281
Table 1: Examples from the LSA similarity model.
tions, truncated text, non-English text and strange
characters) and produced a three billion word cor-
pus of high quality English, which is available on-
line (Han and Finin, 2013).
We performed POS tagging and lemmatiza-
tion on the corpus using the Stanford POS tag-
ger (Toutanova et al., 2000). Word/term co-
occurrences were counted in a moving window
of a fixed size that scans the entire corpus. We
generated two co-occurrence models using win-
dow sizes ?1 and ?4 because we observed differ-
ent natures of the models. ?1 window produces
a context similar to the dependency context used
in (Lin, 1998). It provides a more precise con-
text but is only good for comparing words within
the same POS. This is because words of different
POS are typically surrounded by words in differ-
ent syntactic forms. In contrast, a context window
of ?4 words allows us to compute semantic simi-
larity between words with different POS.
Examples from our LSA similarity model are
given in Table 1. Pairs 1 to 6 illustrate that the
measure has a good property of differentiating
similar words from non-similar words. Examples
7 and 8 show that the ?4 model can detect se-
mantically similar words even with different POS
while the ?1 model yields poor results. The pairs
in 9 and 10 show that highly related, but not sub-
stitutable, words may have a strong similarity and
that the ?1 model is better at detecting them.
Our word co-occurrence models were based on
a predefined vocabulary of more than 22,000 com-
mon English words and noun phrases. We also
added to it more than 2,000 verb phrases extracted
from WordNet. The final dimensions of our word
co-occurrence matrices are 29,000? 29,000 when
words are POS tagged. Our vocabulary includes
only open-class words, i.e., nouns, verbs, adjec-
tives and adverbs. There are no proper nouns in
the vocabulary with the only exception of country
names.
Singular Value Decomposition (SVD) has been
found to be effective in improving word similar-
ity measures (Landauer and Dumais, 1997). SVD
is typically applied to a word by document matrix,
yielding the familiar LSA technique. In our case,
we apply it to our word by word matrix (Burgess et
al., 1998). Before performing SVD, we transform
the raw word co-occurrence count f
ij
to its log fre-
quency log(f
ij
+1). We select the 300 largest sin-
gular values and reduce the 29K word vectors to
300 dimensions. The LSA similarity between two
words is defined as the cosine similarity of their
corresponding word vectors after the SVD trans-
formation. See (Han et al., 2013b; Lushan Han,
2014) for examples and more information on the
LSA model.
Statistical word similarity measures have limi-
tations. Related words can have similarity scores
as high as what similar words get, e.g., ?doctor?
and ?hospital?. Word similarity is typically low
for synonyms that have many word senses since
information about different senses are mashed to-
gether (Han et al., 2013a). To address these issues,
we augment the similarity between two words us-
ing knowledge from WordNet, for example, in-
creasing the score if they are in the same WordNet
synset or if one is a direct or two link hypernym
of the other. See (Han et al., 2013b) for further
details.
2.2 Word Similarity Wrapper
Our word similarity model is restricted to the vo-
cabulary size which only comprises open class
words. For words outside of the vocabulary, we
can only rely on their lexical features and deter-
mine equivalence (which we score as 0 or 1, since
a continuous scale makes little sense in this sce-
nario). An analysis of the previous STS datasets
show that out-of-vocabulary words account for
about 25 ? 45% of the total words. Datasets like
MSRpar and headlines lie on the higher end of this
spectrum due to the high volume of proper nouns.
In the previous version, we computed a charac-
ter bigram overlap score given by
characterBigramScore =
|A ?B|
|A ?B|
where A and B are the set of bigrams from the first
and second word respectively. We compare this
417
against a preset threshold (0.8) to determine equiv-
alence. While this is reasonable for named enti-
ties, it is not the best approach for other classes.
Named Entities. The wrapper is extended to
handle all classes of named entities that are in-
cluded in Stanford CoreNLP (Finkel et al., 2005).
We use heuristic rules to compute the similarity
between two numbers or two dates. To handle
named entity mentions of people, locations and or-
ganizations, we supplement our character bigram
overlap method with the DBpedia Lookup service
(Mendes et al., 2011). For each entity mention, we
select the DBpedia entity with the most inlinks,
which serves as a good estimate of popularity or
significance (Syed et al., 2010). If the two named
entity mentions map to identical DBpedia entities,
we lower our character bigram overlap threshold
to 0.6.
OOV words. As mentioned earlier, when deal-
ing with out-of-vocabulary words, we only have
its lexical features. A straightforward approach is
to simply get more context for the word. Since
our vocabulary is limited, we need to use external
dictionaries to find the word. For our system, we
use Wordnik (Davidson, 2013), which is a compi-
lation of several dictionaries including The Amer-
ican Heritage Dictionary, Wikitionary and Word-
Net. Wordnik provides a REST API to access sev-
eral attributes for a given word such as it?s defini-
tions, examples, related words etc. For out of vo-
cabulary words, we simply retrieve the word pair?s
top definitions and supply it to our existing STS
system (UMBC, 2013a) to compute its similarity.
As a fallback, in case the word is absent even in
Wordnik, we resort to our character bigram over-
lap measure.
3 Multilingual Semantic Text Similarity
3.1 English STS
For the 2014 STS-English subtask we submitted
three runs. They all used a simple term alignment
strategy to compute sentence similarities. The first
run was an unsupervised approach that used the
basic word-similarity model for term alignment.
The next two used a supervised approach to com-
bine the scores from the first run with alignment
scores using the enhanced word-similarity wrap-
per. The two runs differed in their training.
Align and Penalize Approach. The pairingWord
run was produced by the same Align-and-Penalize
system (Han et al., 2013b) that we used in the
2013 STS task with only minor changes. The
biggest change is that we included a small list
of disjoint concepts (Han et al., 2013b) that are
used in the penalization phase, such as {piano, vi-
olin} and {dog, cat}. The disjoint concepts were
manually collected from the MSRvid dataset pro-
vided by the 2012 STS task because we still lack a
reliable general method to automatically produce
them. The list only contains 23 pairs, which can
be downloaded at (dis, 2014).
We also slightly adjusted our stopword list.
We removed a few words that appear in the trial
datasets of 2013 STS task (e.g., frame) but we did
not add any new stopwords for this year?s task. All
the changes are small and we made them only in
the hope that they can slightly improve our system.
Unlike machine learning methods that require
manually selecting an appropriate trained model
for a particular test dataset, our unsupervised
Align-and-Penalize system is applied uniformly
to all six test datasets in 2014 STS task, namely,
deft-forum, deft-news, headlines, images, OnWN
and tweet-news. It achieves the second best rank
among all submitted runs.
Supervised Machine Learning. Our second and
third runs used machine learning approaches sim-
ilar to those we developed for the 2013 STS task
but with significant changes in both pre-processing
and the features extracted.
The most significant pre-processing change was
the use of Stanford coreNLP (Finkel et al., 2005)
tool for tokenization, part-of-speech tagging and
identifying named entity mentions. For the tweet-
news dataset we also removed the hashtag symbol
(?#?) prior to applying the Stanford tools. We use
only open class words and named entity mentions
and remove all other tokens.
We align tokens between two sentences based
on the updated word similarity wrapper that was
described in Section 2.2. We use information
content from Google word frequencies for word
weights similar to our approach last year. The
alignment process is a many-to-one mapping sim-
ilar to the Align and Penalize approach and two
tokens are only aligned if their similarity is greater
than 0.1. The sentence similarity score is then
computed as the average of the scores of their
aligned tokens. This score, along with the Align
and Penalize approach score, are used as features
to train support vector regression (SVR) models.
418
We use an epsilon SVR with a radial basis kernel
function and use a grid search to get the optimal
parameter values for cost, gamma and epsilon. We
use datasets from the previous STS tasks as train-
ing data and the two submitted runs differ in the
choice of their training data.
The first approach, named Hulk, is an attempt
to use a generic model trained on a large data set.
The SVR model uses a total of 3750 sentence pairs
(1500 from MSRvid, 1500 from MSRpar and 750
from headlines) for training. Datasets like SMT
were excluded due to poor quality.
The second approach, named Super Saiyan,
is an attempt at domain specific training. For
OnWN, we used 1361 sentence pairs from previ-
ous OnWN dataset. For Images, we used 1500
sentence pairs from MSRvid dataset. The others
lacked any domain specific training data so we
used a generic training dataset comprising 5111
sentence pairs from MSRvid, MSRpar, headlines
and OnWN datasets.
3.2 Spanish STS
As a base-line for this task we first considered
translating the Spanish sentences to English and
running the same systems explained for the En-
glish Subtask (i.e., pairingWord and Hulk). The
results obtained applying this approach to the pro-
vided training data gave a correlation of 0.777 so,
we selected this approach (with some improve-
ments) for the competition.
Translating the sentences. For the automatic
translation of the sentences from Spanish to En-
glish we used the Google Translate API
1
, a
free, multilingual machine-translation product by
Google. Google Translate presents very accurate
translations for European languages by using sta-
tistical machine translation (Brown et al., 1990)
where the translations are generated on the basis of
statistical models derived from bilingual text cor-
pora. In fact, Google used as part of this corpora
200 billion words from United Nations documents
that are typically published in all six official UN
languages, including English and Spanish.
In the experiments performed with the trial data
we manually evaluated the quality of the trans-
lations (one of the authors is a native Spanish
speaker). The overall translation was very accu-
rate but some statistical anomalies, incorrect trans-
lations due to the abundance of a specific sense of
1
http://translate.google.com
I1: Las costas o costa de un mar, lago o extenso r??o es la
tierra a lo largo del borde de estos.
T11: Costs or the cost of a sea, lake or wide river is the
land along the edge of these.
T12: Coasts or the coast of a sea, lake or wide river is the
land along the edge of these.
T13: Coasts or the coast of a sea, lake or wide river is the
land along the border of these.
...
Figure 1: Three of the English translations for the
Spanish sentence I1.
a word in the training set, appeared.
On one hand, some homonym words are
wrongly translated. For example, the Spanish sen-
tence ?Las costas o costa de un mar [...]? was
translated to ?Costs or the cost of a sea [...]?.
The Spanish word costa has two different senses:
?coast? (the shore of a sea or ocean) and ??cost?
(the property of having material worth). On the
other hand, some words are translated preserving
their semantics but with a slightly different mean-
ing. For example, the Spanish sentence ?Un coj??n
es una funda de tela [...]? was correctly translated
to ?A cushion is a fabric cover [...]?. However,
the Spanish sentence ?Una almohada es un coj??n
en forma rectangular [...]? was translated to ?A
pillow is a rectangular pad [...]?
2
.
Dealing with statistical anomalies. The afore-
mentioned problem of statistical machine transla-
tion caused a slightly adverse effect when comput-
ing the similarity of two English (translated from
Spanish) sentences with the systems explained in
Section 3.1. Therefore, we improved the direct
translation approach by taking into account the
different possible translations for each word in a
Spanish sentence. For that, our system used the in-
formation provided by the Google Translate API,
that is, all the possible translations for every word
of the sentence along with a popularity value. For
each Spanish sentence the system generates all its
possible translations by combining the different
possible translations of each word. For example,
Figure 1 shows three of the English sentences gen-
erated for a given Spanish sentence from the trial
data.
As a way of controlling the combinatorial ex-
plosion of this step, especially for long sentences,
we limited the maximum number of generated
2
Notice that both Spanish sentences used the term coj??n
that should be translated as cushion (the Spanish word for
pad is almohadilla).
419
sentences for each Spanish sentence to 20 and
we only selected words with a popularity greater
than 65. We arrived at the popularity threshold
through experimentation on every sentence in the
trial data set. After this filtering, our input for
the ?news? and ?wikipedia? tests went from 480
and 324 pairs of sentences to 5756 and 1776 pairs,
respectively.
Given a pair of Spanish sentences, I1
and I2, and the set of possible translations
generated by our system for each sentence,
T
I1
= {T
11
, T
12
, T
13
, . . . , T
1n
} and T
I2
=
{T
21
, T
22
, . . . , T
2m
}, we compute the similarity
between them by using the following formula:
SimSPA(I1, I2) =
n
?
i=1
m
?
j=1
SimENG(T
1i
, T
2j
)
n ?m
where SimENG(x, y) computes the similarity of
two English sentences using our existing STS sys-
tem (Han et al., 2013b).
For the final competition we submitted three
runs. The first (Pairing in Table 3) used the
pairingWord system with the direct translation of
the Spanish sentences to English. The second
run (PairingAvg in Table 3) used the formula for
SimSPA(x, y) based on SimENG(x, y) with
the pairingWord system. Finally, the third one
(Hulk in Table 3) used the Hulk system with the
direct translation.
4 Cross Level Similarity
4.1 Sentence to Paragraph/Phrase
We used the three systems developed for the En-
glish sentence similarity subtask and described in
Section 3.1 for both the sentence to paragraph and
sentence to phrase subtasks, producing three runs.
The model for Hulk remained the same (trained
on 3750 sentence pairs from MSRvid, MSRpar
and headlines dataset) but the SuperSaiyan sys-
tem, which is the domain specific approach, used
the given train and trial text pairs (about 530) for
the respective subtasks as training to generate task
specific models.
4.2 Phrase to Word
In our initial experiments, we directly computed
the phrase-word pair similarity using our English
STS. This yielded a very low correlation of 0.239
for the training set, primarily due to the absence of
these phrases and words in our vocabulary. To ad-
dress this issue, we used external sources to obtain
more contextual information and extracted several
features.
Dictionary features. We used Wordnik as a dic-
tionary resource and retrieved definitions and us-
age examples for the word. We then used our
English STS system to measure the similarity be-
tween these and the given phrase to extract two
features.
Web search features. These features were based
on the hypothesis that if a word and phrase have
similar meanings, then a web search that combines
the word and phrase should return similar docu-
ments when compared to a web search for each
individually.
We implemented this idea by comparing results
of three search queries: the word alone, the phrase
alone, and the word and phrase together.
Using the Bing Search API (BIN, 2014), we re-
trieved the top five results for each search, indexed
them with Lucene (Hatcher et al., 2004), and ex-
tracted term frequency vectors for each of the three
search result document sets. For the phrase ?spill
the beans? and word ?confess?, for example, we
built a Lucene index for the set of documents re-
trieved by a Bing search for ?spill the beans?, ?con-
fess?, and ?spill the beans confess?. We calculated
the similarity of pairs of search result sets using
the cosine similarity (1) of their term frequency
vectors.
CosineSimilarity =
n
?
i=1
V 1
i
? V 2
i
?
n
?
i=1
(V 1
i
)
2
?
?
n
?
i=1
(V 2
i
)
2
(1)
We calculated the mean and minimum sim-
ilarity of pairs of results for the phrase and
phrase+word searches. These features were ex-
tracted from the provided training set and used in
conjunction with the dictionary features to train
an SVM regression model to predict similarity
scores.
We observed this method can be problematic
when a word or phrase has multiple meanings.
For example, ?spill the beans? relates to ?confess-
ing? but it is also the name of a coffee shop and
a soup shop. A mix of these pages do get re-
turned by Bing and reduces the accuracy of our re-
sults. However, we found that this technique often
strengthens evidence of similarity enough that it
improves our overall accuracy when used in com-
bination with our dictionary features.
420
Dante#n#1: an Italian poet famous for writing the
Divine Comedy that describes a journey through Hell and
purgatory and paradise guided by Virgil and his idealized
Beatrice
writer#n#1: writes books or stories or articles or the like
professionally for pay
generator#n#3: someone who originates or causes or
initiates something, ?he was the generator of several
complaints?
author#v#1: be the author of, ?She authored this play?
Figure 2: The WordNet sense for Dante#n#1 and
the three author#n senses.
4.3 Word to Sense
For this subtask, we used external resources to re-
trieve more contextual information. For a given
word, we retrieved its synonym set from WordNet
along with their corresponding definitions. We re-
trieved the WordNet definition for the word sense
as well. For example, given a word-sense pair
(author#n, Dante#n#1), we retrieved the synset of
author#n (writer.n.01, generator.n.03, author.v.01)
along with their WordNet definitions and the sense
definition of Dante#n#1. Figure 2 shows the
WordNet data for this example.
By pairing every combination of the word?s
synset and their corresponding definitions with the
sense?s surface form and definition, we created
four features. For each feature, we used our En-
glish STS system to compare their semantic sim-
ilarity and kept the maximum score as feature?s
value.
We found that about 10% of the training
dataset?s words fell outside of WordNet?s vocab-
ulary. Examples of missing words included many
informal or ?slang? words like kegger, crackberry
and post-season. To address this, we used Word-
nik to retrieve the word?s top definition and com-
puted its similarity with the sense. This reduced
the out-of-vocabulary words to about 2% for the
training data. Wordnik thus gave us two addi-
tional features: the maximum semantic similarity
score of word-sense using Wordnik?s additional
definitions for all words and for just the out-of-
vocabulary words. We used these features to train
an SVM regression model with the provided train-
ing set to predict similarity scores.
Dataset Pairing Hulk SuperSaiyan
deft-forum 0.4711 (9) 0.4495 (15) 0.4918 (4)
deft-news 0.7628 (8) 0.7850 (1) 0.7712 (3)
headlines 0.7597 (8) 0.7571 (9) 0.7666 (2)
images 0.8013 (7) 0.7896 (10) 0.7676 (18)
OnWN 0.8745 (1) 0.7872 (18) 0.8022 (12)
tweet-news 0.7793 (2) 0.7571 (7) 0.7651 (4)
Weighted Mean 0.7605 (2) 0.7349 (6) 0.7410 (5)
Table 2: Performance of our three systems on the
six English test sets.
Dataset Pairing PairingAvg Hulk
Wikipedia 0.6682 (12) 0.7431 (6) 0.7382 (8)
News 0.7852 (12) 0.8454 (1) 0.8225 (6)
Weighted Mean 0.7380 (13) 0.8042 (2) 0.7885 (5)
Table 3: Performance of our three systems on the
two Spanish test sets.
5 Results
Multilingual Semantic Text Similarity. Table
2 shows the system performance for the English
STS task. Our best performing system ranked
second
3
, behind first place by only 0.0005.
It employs an unsupervised approach with no
training data required. The supervised systems
that handled named entity recognition and out-
of-vocabulary words performed slightly better on
datasets in the news domain but still suffered from
noise due to diverse training datasets.
Table 3 shows the performance for the Spanish
subtask. The best run achieved a weighted correla-
tion of 0.804, behind first place by only 0.003. The
Hulk system was similar to the Pairing run and
used only one translation per sentence. The per-
formance boost could be attributed to large num-
ber of named entities in the News and Wikipedia
datasets.
Cross Level Similarity. Table 4 shows our per-
formance in the Cross Level Similarity tasks. The
Paragraph-Sentence and Sentence-Phrase yielded
good results (ranked second and first respectively)
with our English STS system because of sufficient
amount of textual information. The correlation
scores dropped as the granularity level of the text
got finer.
The Phrase-Word run achieved a correlation of
0.457, the highest for the subtask. However, an
incorrect file was submitted prior to the deadline
3
An incorrect file for ?deft-forum? dataset was submitted.
The correct version had a correlation of 0.4896 instead of
0.4710. This would have placed it at rank 1 overall.
421
Wordnik BingSim Score
ID S1 S2 Baseline Definitions Example Sim Avg Min SVM GS Error
Idiomatic-212 spill the beans confess 0 0 0 0.0282 0.1516 0.1266 0.5998 4.0 3.4002
Idiomatic-292 screw the pooch mess up 0 0.04553 0.0176 0.0873 0.4238 0.0687 0.7185 4.0 3.2815
Idiomatic-273 on a shoogly peg insecure 0 0.0793 0 0.0846 0.3115 0.1412 0.8830 4.0 3.1170
Slang-115 wacky tabaccy cannabis 0 0 0 0.0639 0.4960 0.1201 0.5490 4.0 3.4510
Slang-26 pray to the porcelain god vomiting 0 0 0 0.0934 0.5275 0.0999 0.6452 4.0 3.3548
Slang-79 rock and roll commence 0 0.2068 0.0720 0.0467 0.5106 0.0560 0.8820 4.0 3.1180
Newswire-160 exercising rights under canon law lawyer 0.0044 0.6864 0.0046 0.3642 0.4990 0.2402 3.5562 0.5 3.0562
Table 5: Examples where our algorithm performed poorly and the scores for individual features.
Dataset Pairing Hulk SuperSaiyan WordExpand
Para.-Sent. 0.794 (10) 0.826 (4) 0.834 (2)
Sent.-Phrase 0.704 (14) 0.705 (13) 0.777 (1)
Phrase-Word 0.457 (1)
Word-Sense 0.389 (1)
Table 4: Performance of our systems on the four
Cross-Level Subtasks.
Figure 3: Average error with respect to category.
which meant that this was not included in the of-
ficial results. Figure 3 shows the average error
(measured as the average deviation from the gold
standard) across different categories for phrase to
word subtask. Our performance is slightly worse
for slang and idiomatic categories when compared
to others which is due to two reasons: (i) the se-
mantics of idioms is not compositional, reducing
the effectiveness of a distributional similarity mea-
sure and (ii) dictionary-based features often failed
to find definitions and/or examples of idioms. Ta-
ble 5 shows some of the words where our algo-
rithm performed poorly and their scores for indi-
vidual features.
The Word-Sense run ranked first in the sub-
task with a correlation score of 0.389. Table 6
shows some of the word-sense pairs where the
system performed poorly. Our system only used
Wordnik?s top definition which was not always the
right one to use to detect the similarity. For ex-
ample, the first definition of cheese#n is ?a solid
food prepared from the pressed curd of milk? but
there is a latter, less prominent one, which is
ID word sense key sense number predicted gold
80 cheese#n moolah%1:21:00:: moolah#n#1 0.78 4
377 bone#n chalk%1:07:00:: chalk#n#2 1.52 4
441 wasteoid#n drug user%1:18:00:: drug user#n#1 0.78 3
Table 6: Examples where our system performed
poorly.
?money?. A second problem is that some words,
like wasteoid#n, were absent even in Wordnik.
Using additional online lexical resources to in-
clude more slangs and idioms, like the Urban Dic-
tionary (Urb, 2014), could address these issues.
However, care must be taken since the quality of
some content is questionable. For example, the
Urban Dictionary?s first definition of ?program-
mer? is ?An organism capable of converting caf-
feine into code?.
6 Conclusion
We described our submissions to the Multilingual
Semantic Textual Similarity (Task 10) and Cross-
Level Semantic Similarity (Task 3) tasks for the
2014 International Workshop on Semantic Eval-
uation. Our best runs ranked second in both En-
glish and Spanish subtasks for Task 10 while rank-
ing first in Sentence-Phrase, Phrase-Word, Word-
Sense tasks and second in Paragraph-Sentence
subtasks for Task 3. Our success is attributed to
a powerful word similarity model based on LSA
word similarity and WordNet knowledge. We
used new linguistic resources like Wordnik to im-
prove our existing system for the Phrase-Word and
Word-Sense tasks and plan to include other re-
sources like ?Urban dictionary? in the future.
Acknowledgements
This research was supported by awards 1228198,
1250627 and 0910838 from the U.S. National Sci-
ence Foundation.
422
References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
2014. BING search API. http://bing.com/developers-
/s/APIBasics.html.
Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Fredrick Jelinek, John D Laf-
ferty, Robert L Mercer, and Paul S Roossin. 1990.
A statistical approach to machine translation. Com-
putational linguistics, 16(2):79?85.
Curt Burgess, Kay Livesay, and Kevin Lund. 1998.
Explorations in context space: Words, sentences,
discourse. Discourse Processes, 25(2-3):211?257.
Sara Davidson. 2013. Wordnik. The Charleston Advi-
sor, 15(2):54?58.
2014. Disjoint concept pairs. http://semanticweb-
archive.cs.umbc.edu/disjointConcepts.txt.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2005. Incorporating non-local
information into information extraction systems by
gibbs sampling. In 43rd Annual Meeting of the ACL,
pages 363?370.
Lushan Han and Tim Finin. 2013. UMBC webbase
corpus. http://ebiq.org/r/351.
Lushan Han, Tim Finin, Paul McNamee, Anupam
Joshi, and Yelena Yesha. 2013a. Improving Word
Similarity by Augmenting PMI with Estimates of
Word Polysemy. IEEE Trans. on Knowledge and
Data Engineering, 25(6):1307?1322.
Lushan Han, Abhay L. Kashyap, Tim Finin,
James Mayfield, and Johnathan Weese. 2013b.
UMBC EBIQUITY-CORE: Semantic Textual
Similarity Systems. In 2nd Joint Conf. on Lexical
and Computational Semantics. ACL, June.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York, USA.
Erik Hatcher, Otis Gospodnetic, and Michael McCand-
less. 2004. Lucene in action. Manning Publications
Greenwich, CT.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
104(2):211.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. 17th Int. Conf. on Compu-
tational Linguistics, pages 768?774, Montreal, CN.
Lushan Han. 2014. Schema Free Querying of Seman-
tic Data. Ph.D. thesis, University of Maryland, Bal-
timore County.
Pablo N Mendes, Max Jakob, Andr?es Garc??a-Silva, and
Christian Bizer. 2011. Dbpedia spotlight: shedding
light on the web of documents. In 7th Int. Conf. on
Semantic Systems, pages 1?8. ACM.
Stanford. 2001. Stanford WebBase project.
http://bit.ly/WebBase.
Zareen Syed, Tim Finin, Varish Mulwad, and Anupam
Joshi. 2010. Exploiting a Web of Semantic Data for
Interpreting Tables. In Proceedings of the Second
Web Science Conference, April.
Kristina Toutanova, Dan Klein, Christopher Manning,
William Morgan, Anna Rafferty, and Michel Gal-
ley. 2000. Stanford log-linear part-of-speech tagger.
http://nlp.stanford.edu/software/tagger.shtml.
UMBC. 2013a. Semantic similarity demonstration.
http://swoogle.umbc.edu/SimService/.
UMBC. 2013b. Umbc graph of relations project.
http://ebiq.org/j/95.
2014. Urban dictionary. http://urbandictionary.com/.
423
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 80?88,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Annotating Named Entities in Twitter Data with Crowdsourcing
Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller and Justin Martineau
Computer Science and Electrical Engineering
University of Maryland, Baltimore County
Baltimore MD 21250
(finin,willm1,anandk1,nick6,jm1)@umbc.edu
Mark Dredze
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore MD 21211
mdredze@cs.jhu.edu
Abstract
We describe our experience using both Ama-
zon Mechanical Turk (MTurk) and Crowd-
Flower to collect simple named entity anno-
tations for Twitter status updates. Unlike most
genres that have traditionally been the focus of
named entity experiments, Twitter is far more
informal and abbreviated. The collected anno-
tations and annotation techniques will provide
a first step towards the full study of named en-
tity recognition in domains like Facebook and
Twitter. We also briefly describe how to use
MTurk to collect judgements on the quality of
?word clouds.?
1 Introduction and Dataset Description
Information extraction researchers commonly work
on popular formal domains, such as news arti-
cles. More diverse studies have included broadcast
news transcripts, blogs and emails (Strassel et al,
2008). However, extremely informal domains, such
as Facebook, Twitter, YouTube or Flickr are start-
ing to receive more attention. Any effort aimed at
studying these informal genres will require at least a
minimal amount of labeled data for evaluation pur-
poses.
This work details how to efficiently annotate large
volumes of data, for information extraction tasks, at
low cost using MTurk (Snow et al, 2008; Callison-
Burch, 2009). This paper describes a case study for
information extraction tasks involving short, infor-
mal messages from Twitter. Twitter is a large multi-
user site for broadcasting short informal messages.
Twitter is an extreme example of an informal genre
(Java et al, 2007) as users frequently abbreviate
their posts to fit within the specified limit. Twitter
is a good choice because it is very popular: Twitter
users generate a tremendous number of status up-
dates (tweets) every day1. This is a good genre to
work on named entity extraction since many tweets
refer to and contain updates about named entities.
Our Twitter data set has over 150 million tweets
from 1.5 million users collected over a period of
three years. Tweets are unlike formal text. They are
limited to a maximum of 140 characters, a limit orig-
inally set to allow them to fit into an SMS message.
Consequently, the use of acronyms and both stan-
dard and non-standard abbreviations (e.g., b4 for be-
fore and ur for your) are very common. Tweets tend
to be telegraphic and often consist of sentence frag-
ments or other ungrammatical sequences. Normal
capitalization rules (e.g., for proper names, book ti-
tles, etc.) are commonly ignored.
Furthermore, users have adopted numerous con-
ventions including hashtags, user mentions, and
retweet markers. A hashtag (e.g., #earthquake) is
a token beginning with a ?#? character that denotes
one of the topic of a status. Hashtags can be used as
pure metadata or serve both as a word and as meta-
data, as the following two examples show.
? EvanEcullen: #chile #earthquake #tsunami They
heard nothing of a tsunami until it slammed into
their house with an unearthly http://tl.gd/d798d
? LarsVonD: Know how to help #Chile after the
#Earthquake
1Pingdom estimated that there were nearly 40 million tweets
a day in January 2010 (pingdom.com, 2010).
80
(1) report from the economist: #chile counts the cost
of a devastating earthquake and makes plans for re-
covery. http://bit.ly/dwoQMD
Note: ?the economist? was not recognized as an
ORG.
(2) how come when george bush wanted to take out
millions for the war congress had no problem...but
whe obama wants money for healthcare the ...
Note: Both ?george bush? and ?obama? were missed
as PERs.
(3) RT @woodmuffin: jay leno interviewing sarah
palin: the seventh seal starts to show a few cracks
Note: RT (code for a re-tweet) was mistaken as a po-
sition and sarah palin missed as a person.
Table 1: Standard named entity systems trained on text from
newswire articles and other well formed documents lose accu-
racy when applied to short status updates.
The Twitter community also has a convention where
user names preceded by an @ character (known as
?mentions?) at the beginning of a status indicate that
it is a message directed at that user. A user mention
in the middle of a message is interpreted as a general
reference to that user. Both uses are shown in this
status:
? paulasword: @obama quit calling @johnboener a
liar, you liar
The token RT is used as a marker that a person is for-
warding a tweet originally sent by another user. Nor-
mally the re-tweet symbol begins the message and
is immediately followed by the user mention of the
original author or sometimes a chain of re-tweeters
ending with the original author, as in
? politicsiswar: RT @KatyinIndy @SamiShamieh:
Ghost towns on rise under Obama
http://j.mp/cwJSUg #tcot #gop (Deindustrial-
ization of U.S.- Generation Zero)
Finally, ?smileys? are common in Twitter statuses to
signal the users? sentiment, as in the following.
? sallytherose: Just wrote a 4-page paper in an hour
and a half. BOiiiiii I?m getting good at this. :) Left-
over Noodles for dinner as a reward. :D
The Twitter search service also uses these to retrieve
tweets matching a query with positive or negative
sentiment.
Typical named entity recognition systems have
been trained on formal documents, such as news
Figure 1: Our Twitter collection is stored in a relational
database and also in the Lucene information retrieval system.
wire articles. Their performance on text from very
different sources, especially informal genres such as
Twitter tweets or Facebook status updates, is poor.
In fact, ?Systems analyzing correctly about 90% of
the sequences from a journalistic corpus can have a
decrease of performance of up to 50% on more in-
formal texts.? (Poibeau and Kosseim, 2001) How-
ever, many large scale information extraction sys-
tems require extracting and integrating useful in-
formation from online social networking sources
that are informal such as Twitter, Facebook, Blogs,
YouTube and Flickr.
To illustrate the problem we applied both the
NLTK (Bird et al, 2009) and the Stanford named
entity recognizers (Finkel et al, 2005) without re-
training to a sample Twitter dataset with mixed re-
sults. We have observed many failures, both false
positives and false negatives. Table 1 shows some
examples of these.
2 Task design
We developed separate tasks on CrowdFlower and
MTurk using a common collection of Twitter sta-
tuses and asked workers to perform the same anno-
tation task in order to fully understand the features
that each provides, and to determine the total amount
of work necessary to produce a result on each ser-
vice. MTurk has the advantage of using standard
HTML and Javascript instead of CrowdFlower?s
CML. However MTurk has inferior data verifica-
tion, in that the service only provides a threshold
on worker agreement as a form of quality control.
81
This is quite poor when tasks are more complicated
than a single boolean judgment, as with the case at
hand. CrowdFlower works across multiple services
and does verification against gold standard data, and
can get more judgements to improve quality in cases
where it?s necessary.
3 Annotation guidelines
The task asked workers to look at Twitter individ-
ual status messages (tweets) and use a toggle but-
ton to tag each word with person (PER), organiza-
tion (ORG), location (LOC), or ?none of the above?
(NONE). Each word also had a check box (labeled
???) to indicate that uncertainty. We provided the
workers with annotation guidelines adapted from the
those developed by the Linguistic Data Consortium
(Linguistic Data Consortium ? LCTL Team, 2006)
which were in turn based on guidelines used for
MUC-7 (Chinchor and Robinson, 1997).
We deliberately kept our annotation goals simple:
We only asked workers to identify three basic types
of named entities.
Our guidelines read:
An entity is a object in the world like a place
or person and a named entity is a phrase that
uniquely refers to an object by its proper name
(Hillary Clinton), acronym (IBM), nickname
(Opra) or abbreviation (Minn.).
Person (PER) entities are limited to humans
(living, deceased, fictional, deities, ...) iden-
tified by name, nickname or alias. Don?t in-
clude titles or roles (Ms., President, coach).
Include suffix that are part of a name (e.g., Jr.,
Sr. or III).
Organization (ORG) entities are limited to
corporations, institutions, government agen-
cies and other groups of people defined by
an established organizational structure. Some
examples are businesses (Bridgestone Sports
Co.), stock ticker symbols (NASDAQ), multi-
national organizations (European Union), po-
litical parties (GOP) non-generic government
entities (the State Department), sports teams
(the Yankees), and military groups (the Tamil
Tigers). Do not tag ?generic? entities like ?the
government? since these are not unique proper
names referring to a specific ORG.
Location (LOC) entities include names of
politically or geographically defined places
(cities, provinces, countries, international re-
gions, bodies of water, mountains, etc.). Lo-
cations also include man-made structures like
airports, highways, streets, factories and mon-
uments.
We instructed annotators to ignore other types of
named entities, e.g., events (World War II), products
(iPhone), animals (Cheetah), inanimate objects and
monetary units (the Euro) and gave them four prin-
ciples to follow when tagging:
? Tag words according to their meaning in the
context of the tweet.
? Only tag names, i.e., words that directly and
uniquely refer to entities.
? Only tag names of the types PER,ORG, and
LOC.
? Use the ??? checkbox to indicate uncertainty
in your tag.
3.1 Data selection
We created a ?gold standard? data set of about 400
tweets to train and screen workers on MTurk, to salt
the MTurk data with worker evaluation data, for use
on CrowdFlower, and to evaluate the performance
of the final NER system after training on the crowd-
sourced annotations. We preselected tweets to an-
notate using the NLTK named entity recognizer to
select statuses that were thought to contain named
entities of the desired types (PER, ORG, LOC).
Initial experiments suggested that a worker can
annotate about 400 tweets an hour. Based on this, we
loaded each MTurk Human Intelligence Tasks (HIT)
with five tweets, and paid workers five cents per HIT.
Thus, if we require that each tweet be annotated by
two workers, we would be able to produce about
4,400 raw annotated tweets with the $100 grant from
Amazon, accounting for their 10% overhead price.
3.2 CrowdFlower
We also experimented with CrowdFlower, a crowd-
sourcing service that uses various worker channels
like MTurk and SamaSource2 and provides an en-
hanced set of management and analytic tools. We
were interested in understanding the advantages and
disadvantages compared to using MTurk directly.
2http://www.samasource.org/
82
Figure 2: CrowdFlower is an enhanced service that feeds into
MTurk and other crowdsourcing systems. It provides conve-
nient management tools that show the performance of workers
for a task.
We prepared a basic front-end for our job using the
CrowdFlower Markup Language (CML) and custom
JavaScript. We used the CrowdFlower interface to
calibrate our job and to decide the pay rate. It con-
siders various parameters like amount of time re-
quired to complete a sample task and the desired ac-
curacy level to come up with a pay rate.
One attractive feature lets one provide a set of
?gold standard? tasks that pair data items with cor-
rect responses. These are automatically mixed into
the stream of regular tasks that workers process. If
a worker makes errors in one of these gold stan-
dard tasks, she gets immediate feedback about her
error and the correct answer is shown. CrowdFlower
claims that error rates are reduced by a factor of
two when gold standards are used(crowdflower.com,
2010). The interface shown in Figure 2 shows the
number of gold tasks the user has seen, and how
many they have gotten correct.
CrowdFlower?s management tools provides a de-
tailed analysis of the workers for a job, including
the trust level, accuracy and past accuracy history
associated with each worker. In addition, the output
records include the geographical region associated
with each worker, information that may be useful
for some tasks.
3.3 MTurk
The current iteration of our MTurk interface is
shown in Figure 3. Each tweet is shown at the top
of the HIT interface so that it can easily be read for
context. Then a table is displayed with each word
of the tweet down the side, and radio buttons to pick
Figure 3: In the MTurk interface a tweet is shown in its entirety
at the top, then a set of radio buttons and a checkbox is shown
for each word of the tweet. These allow the user to pick the
annotation for each word, and indicate uncertainty in labeling.
what kind of entity each word is. Every ten rows,
the header is repeated, to allow the worker to scroll
down the page and still see the column labels. The
interface also provides a checkbox allows the worker
to indicate uncertainty in labeling a word.
We expect that our data will include some tricky
cases where an annotator, even an experienced one,
may be unsure whether a word is part of a named
entity and/or what type it is. For example, is ?Bal-
timore Visionary Art Museum? a LOC followed by
a three word ORG, or a four-word ORG? We con-
sidered and rejected using hierarchical named enti-
ties in order to keep the annotation task simple. An-
other example that might give an annotator pause is
a phrase like ?White House? can be used as a LOC
or ORG, depending on the context.
This measure can act as a measure of a worker?s
quality: if they label many things as ?uncertain?,
we might guess that they are not producing good
results in general. Also, the uncertainty allows for
a finer-grained measure of how closely the results
from two workers for the same tweet match: if the
workers disagree on the tagging of a particular word,
but agree that it is not certain, we could decide that
this word is a bad example and not use it as training
data.
Finally, a help screen is available. When the user
mouses over the word ?Help? in the upper right, the
guidelines discussed in Section 3 are displayed. The
screenshot in Figure 3 shows the help dialog ex-
panded.
The MTurk interface uses hand-written Javascript
to produce the table of words, radio buttons, and
83
Figure 4: Only about one-third of the workers did more than
three HITs and a a few prolific workers accounted for most of
our data.
checkboxes. The form elements have automatically
generated names, which MTurk handles neatly. Ad-
ditional Javascript code collects location informa-
tion from the workers, based on their IP address. A
service provided by Geobytes3 provides the location
data.
4 Results from MTurk
Our dataset was broken into HITs of four previ-
ously unlabeled tweets, and one previously labeled
tweet (analogous to the ?gold? data used by Crowd-
Flower). We submitted 251 HITs, each of which was
to be completed twice, and the job took about 15
hours. Total cost for this job was $27.61, for a total
cost per tweet of about 2.75 cents each (although we
also paid to have the gold tweets annotated again).
42 workers participated, mostly from the US and
India, with Australia in a distant third place. Most
workers did only a single HIT, but most HITs were
done by a single worker. Figure 4 shows more detail.
After collecting results from MTurk, we had to
come up with a strategy for determining which
of the results (if any) were filled randomly. To
do this, we implemented an algorithm much like
Google?s PageRank (Brin and Page, 1998) to judge
the amount of inter-worker agreement. Pseudocode
for our algorithm is presented in Figure 5.
This algorithm doesn?t strictly measure worker
quality, but rather worker agreement, so it?s impor-
3http://www.geobytes.com/
WORKER-AGREE : results ? scores
1 worker ids ? ENUMERATE(KEYS(results))
 Initialize A
2 for worker1 ? worker ids
3 do for worker2 ? worker ids
4 do A[worker1 ,worker2 ]
? SIMILARITY(results[worker1 ],
results[worker2 ])
 Normalize columns of A so that they sum to 1 (elided)
 Initialize x to be normal: each worker
is initially trusted equally.
5 x?
?
1?
n
, . . . , 1?
n
?
 Find the largest eigenvector of A, which
corresponds to the agreement-with-group
value for each worker.
6 i? 0
7 while i < max iter
8 do xnew ? NORMALIZE(A? x)
9 diff ? xnew ? x
10 x = xnew
11 if diff < tolerance
12 then break
13 i? i + 1
14 for workerID ,workerNum ? worker ids
15 do scores[workerID ]? x[workerNum]
16 return scores
Figure 5: Intra-worker agreement algorithm. MTurk results are
stored in an associative array, with worker IDs as keys and lists
of HIT results as values, and worker scores are floating point
values. Worker IDs are mapped to integers to allow standard
matrix notation. The Similarity function in line four just returns
the fraction of HITs done by two workers where their annota-
tions agreed.
tant to ensure that the workers it judges as having
high agreement values are actually making high-
quality judgements. Figure 6 shows the worker
agreement values plotted against the number of re-
sults a particular worker completed. The slope of
this plot (more results returned tends to give higher
scores) is interpreted to be because practice makes
perfect: the more HITs a worker completes, the
more experience they have with the task, and the
more accurate their results will be.
So, with this agreement metric established, we set
out to find out how well it agreed with our expecta-
tion that it would also function as a quality metric.
Consider those workers that completed only a sin-
gle HIT (there are 18 of them): how well did they
do their jobs, and where did they end up ranked as a
result? Since each HIT is composed of five tweets,
84
Figure 6: This log-log plot of worker agreement scores versus
the number of results clearly shows that workers who have done
more HITs have better inter-annotator agreement scores.
even such a small sample can contain a lot of data.
Figure 7 shows a sample annotation for three
tweets, each from a worker who did only one HIT,
and the ranking that the worker received for doing
that annotation. The worst scoring one is apparently
a random fill: there?s no correlation at all between
the answers and the correct ones. The middle tweet
is improved: ?Newbie? isn?t a person in this con-
text, but it?s a mistake a non-native speaker might
make, and everything else is right, and the score is
higher. The last tweet is correctly labeled within our
parameters, and scores the highest. This experiment
shows that our agreement metric functions well as a
correctness metric.
Also of interest is the raw effectiveness of MTurk
workers; did they manage to tag tweets as well as
our experts? After investigating the data, our verdict
is that the answer is not quite?but by carefully com-
bining the tags that two people give the same tweet
it is possible to get good answers nevertheless, at
much lower cost than employing a single expert.
5 Results from CrowdFlower
Our CrowdFlower task involved 30 tweets. Each
tweet was further split into tokens resulting in 506
units as interpreted by CrowdFlower?s system. We
required a total 986 judgments. In addition, we were
Score 0.0243 Score 0.0364 Score 0.0760
Trying org Newbie person Trying none
to org here none out none
decide org nice none TwittEarth org
if org to none - none
it?s org meet none Good none
worth place you none graphics. none
hanging org all none Fun none
around org but none
until org useless. none
the none (URL) none
final org
implosion org
Figure 7: These sample annotations represent the range of
worker quality for three workers who did only one HIT. The
first is an apparently random annotation, the second a plausible
but incorrect one, and the third a correct annotation. Our algo-
rithm assigned these workers scores aligned with their product
quality.
Figure 8: CrowdFlower provides good interfaces to manage
crowdsourcing tasks. This view lets us to monitor the number
of judgements in each category.
required to generate thirteen ?gold? data, which is
the minimum required by the service. Every gold
answer has an optional text with it to inform work-
ers why we believe our answer is the correct one and
theirs is incorrect. This facilitates gradually train-
ing workers up to the point where they can provide
reliably correct results. Figure 8 shows the inter-
face CrowdFlower provides to monitor the number
of judgements in each category.
We used the calibration interface that Crowd-
Flower provides to fix the price for our task (Fig-
ure 9). It considers various parameters like the time
required per unit and desired accuracy level, and also
adds a flat 33%markup on the actual labor costs. We
divided the task into a set of assignments where each
assignment had three tweets and was paid five cents.
We set the time per unit as 30 seconds, so, based on
the desired accuracy level and markup overhead, our
job?s cost was $2.19. This comes to $2 hourly pay
per worker, assuming they take the whole 30 sec-
85
Figure 9: CrowdFlower has an interface that makes it easy to
select an appropriate price for a task.
onds to complete the task.
6 Cloud Comparison
MTurk can also be used to efficiently evaluate re-
sults requiring human judgments. We implemented
an additional HIT to evaluate a new technique we
developed to generate ?word clouds.? In this task
workers choose which of two word clouds generated
from query results by two different algorithms pro-
vides a more useful high level description that can
highlight important features and opinions about the
query topic.
Evaluating how well a set of words describes
and highlights the important features and opinions
pertaining to the subject of the query is subjec-
tive, which necessitates human evaluations. MTurk
workers were given two word clouds, one from our
technique and the other from a baseline relevance
feedback technique (Rocchio (Rocchio, 1971)), for
each query. Queries were shown with a short de-
scriptive blurb to disambiguate it from possible al-
ternatives, reveal the intent of the user who created
the query, and provide a short description of it for
workers who were unfamiliar with the query subject.
Wikipedia links were provided, when applicable, for
anyone needing further information about the query
subject. Workers were asked to use a slider to de-
termine which cloud better represented the key con-
cepts related to the query. The slider would snap
into one of eleven positions, which were labeled
with value judgments they represented. The cen-
ter value indicates that the two clouds were equally
good. Figure 10 shows the final query interface.
Figure 10: MTurk workers were asked which word cloud they
thought best represented returned the results of a query, in this
case ?Buffy the Vampire Slayer?.
6.1 Results
Since MTurk workers are paid per task they com-
plete, there is an incentive to do low quality work
and even to randomly guess to get tasks done as
fast as possible. To ensure a high quality evaluation
we included in every batch of five queries a qual-
ity control question. Quality control questions were
designed to look exactly like the regular cloud com-
parisons, but only one of the two clouds displayed
was actually from the query in the description. The
other word cloud was generated from a different
query with no relation to the real query, and hand
checked to make sure that anyone who was doing a
respectable job would agree that the off-topic word
cloud was a poor result for the query. If a worker?s
response indicated that the off topic cloud was as
good as or better than the real cloud then they failed
that control question, otherwise they passed.
86
We asked that twelve workers label each set of
questions. We only used results from workers that
answered at least seven control questions with an
average accuracy rating of at least 75%. This left
us with a pool of eight reliable workers with an av-
erage accuracy on control questions of about 91%.
Every question was labeled by at least five different
workers with a mode of seven.
Workers were not told which technique produced
which cloud. Techniques were randomly assigned to
either cloud A or B to prevent people from entering
into a ?cloud A is always better? mentality. The po-
sition of the quality control questions were randomly
assigned in each set of five cloud comparisons. The
links to the cloud images were anonymized to ran-
dom numbers followed by the letter A or B for their
position to prevent workers from guessing anything
about either the query or the technique that gener-
ated the cloud.
We applied a filter to remove the query words
from all word clouds. First of all, it would be a
dead giveaway on the control questions. Second,
the query words are already known and thus pro-
vide no extra information about the query to the user
while simultaneously taking up the space that could
be used to represent other more interesting words.
Third, their presence and relative size compared to
the baseline could cause users to ignore other fea-
tures especially when doing a quick scan.
The slider scores were converted into numerical
scores ranging from -5 to +5, with zero represent-
ing that the two clouds were equal. We averaged
the score for each cloud comparison, and determined
that for 44 out of 55 clouds workers found our tech-
nique to be better than the baseline approach.
6.2 Issues
We faced some issues with the CrowdFlower sys-
tem. These included incorrect calibration for jobs,
errors downloading results from completed jobs,
price displayed on MTurk being different that what
was set through CrowdFlower and gold standard
data not getting stored on CrowdFlower system. An-
other problem was with the system?s 10-token limit
on gold standards, which is not yet resolved at the
time of this writing. On the whole, the CrowdFlower
team has been very quick to respond to our problems
and able to correct the problems we encountered.
Figure 11: Statistics for worker #181799. The interface has an
option to ?forgive? the worker for missing gold and an option
to ?flag? the worker so that the answers are excluded while re-
turning the final set of judgments. It also displays workers ID,
past accuracy and source, e.g. MTurk.
6.3 Live Analytics
CrowdFlower?a analytics panel facilitates viewing
the live responses. The trust associated with each
worker can be seen under the workers panel. Work-
ers who do a large amount of work with low trust are
likely scammers or automated bots. Good gold data
ensures that their work is rejected. The system auto-
matically pauses a job when the ratio of untrusted to
trusted judgments exceeds a certain mark. This was
particularly helpful for us to rectify some of our gold
data. Currently, the job is being completed with 61%
accuracy for gold data. This could be due to the cur-
rent issue we are facing as described above. It?s also
possible to view statistics for individual workers, as
shown in Figure 11.
7 Conclusion
Crowdsourcing is an effective way to collect annota-
tions for natural language and information retrieval
research. We found both MTurk and CrowdFlower
to be flexible, relatively easy to use, capable of pro-
ducing usable data, and very cost effective.
Some of the extra features and interface options
that CrowdFlower provided were very useful, but
did their were problems with their ?gold standard?
agreement evaluation tools. Their support staff was
very responsive and helpful, mitigating some of
these problems. We were able to duplicate some of
the ?gold standard? functionality on MTurk directly
by generating our own mix of regular and quality
control queries. We did not attempt to provide im-
87
mediate feedback to workers who enter a wrong an-
swer for the ?gold standard? queries, however.
With these labeled tweets, we plan to train an en-
tity recognizer using the Stanford named entity rec-
ognizer4, and run it on our dataset. After using this
trained entity recognizer to find the entities in our
data, we will compare its accuracy to the existing
recognized entities, which were recognized by an
ER trained on newswire articles. We will also at-
tempt to do named entity linking and entity resolu-
tion on the entire corpus.
We look forward to making use of the data we
collected in our research and expect that we will use
these services in the future when we need human
judgements.
Acknowledgments
This work was done with partial support from the
Office of Naval Research and the Johns Hopkins
University Human Language Technology Center of
Excellence. We thank both Amazon and Dolores
Labs for grants that allowed us to use their systems
for the experiments.
References
S. Bird, E. Klein, and E. Loper. 2009. Natural language
processing with Python. Oreilly & Associates Inc.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. In Seventh Interna-
tional World-Wide Web Conference (WWW 1998).
C. Callison-Burch. 2009. Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazons Me-
chanical Turk. In Proceedings of EMNLP 2009.
N. Chinchor and P. Robinson. 1997. MUC-7 named en-
tity task definition. In Proceedings of the 7th Message
Understanding Conference. NIST.
crowdflower.com. 2010. The error rates with-
out the gold standard is more than twice as
high as when we do use a gold standard.
http://crowdflower.com/general/examples. Accessed
on April 11, 2010.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
the 43nd Annual Meeting of the Association for Com-
putational Linguistics (ACL 2005), volume 100, pages
363?370.
4http://nlp.stanford.edu/software/CRF-NER.shtml
A. Java, X. Song, T. Finin, and B. Tseng. 2007. Why we
twitter: understanding microblogging usage and com-
munities. In Proceedings of the 9th WebKDD and 1st
SNA-KDD 2007 workshop on Web mining and social
network analysis, pages 56?65. ACM.
Linguistic Data Consortium ? LCTL Team. 2006. Sim-
ple named entity guidelines for less commonly taught
languages, March. Version 6.5.
pingdom.com. 2010. Twitter: Now more than 1
billion tweets per month. http://royal.pingdom.com-
/2010/02/10/twitter-now-more-than-1-billion-tweets-
per-month/, February. Accessed on February 15,
2010.
T. Poibeau and L. Kosseim. 2001. Proper Name Extrac-
tion from Non-Journalistic Texts. In Computational
linguistics in the Netherlands 2000: selected papers
from the eleventh CLIN Meeting, page 144. Rodopi.
J. Rocchio. 1971. Relevance feedback in information re-
trieval. In G. Salton, editor, The SMART Retrieval Sys-
tem: Experiments in Automatic Document Processing.
Prentice-Hall.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263. Association for
Computational Linguistics.
Stephanie Strassel, Mark Przybocki, Kay Peterson, Zhiyi
Song, and Kazuaki Maeda. 2008. Linguistic re-
sources and evaluation techniques for evaluation of
cross-document automatic content extraction. In Pro-
ceedings of the 6th International Conference on Lan-
guage Resources and Evaluation.
88
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 78?86,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised techniques for discovering ontology 
elements from Wikipedia article links 
Zareen Syed Tim Finin 
University of Maryland, Baltimore County University of Maryland, Baltimore County 
1000 Hilltop Circle 1000 Hilltop Circle 
Baltimore, MD 21250, USA Baltimore, MD 21250, USA 
zarsyed1@umbc.edu finin@umbc.edu 
 
 
 
 
Abstract 
We present an unsupervised and unrestricted 
approach to discovering an infobox like on-
tology by exploiting the inter-article links 
within Wikipedia. It discovers new slots and 
fillers that may not be available in the 
Wikipedia infoboxes. Our results demonstrate 
that there are certain types of properties that 
are evident in the link structure of resources 
like Wikipedia that can be predicted with high 
accuracy using little or no linguistic analysis. 
The discovered properties can be further used 
to discover a class hierarchy. Our experiments 
have focused on analyzing people in Wikipe-
dia, but the techniques can be directly applied 
to other types of entities in text resources that 
are rich with hyperlinks.  
1 Introduction  
One of the biggest challenges faced by the Seman-
tic Web vision is the availability of structured data 
that can be published as RDF. One approach is to 
develop techniques to translate information in 
spreadsheets, databases, XML documents and 
other traditional data formats into RDF (Syed et al 
2010). Another is to refine the technology needed 
to extract structured information from unstructured 
free text (McNamee and Dang, 2009). 
For both approaches, there is a second problem 
that must be addressed: do we start with an ontol-
ogy or small catalog of ontologies that will be used 
to encode the data or is extracting the right ontol-
ogy part of the problem. We describe exploratory 
work on a system that can discover ontological 
elements as well as data from a free text with em-
bedded hyperlinks. 
Wikipedia is a remarkable and rich online en-
cyclopedia with a wealth of general knowledge 
about varied concepts, entities, events and facts in 
the world. Its size and coverage make it a valuable 
resource for extracting information about different 
entities and concepts. Wikipedia contains both free 
text and structured information related to concepts 
in the form of infoboxes, category hierarchy and 
inter-article links. Infoboxes are the most struc-
tured form and are composed of a set of subject-
attribute-value triples that summarize or highlight 
the key features of the concept or subject of the 
article. Resources like DBpedia (Auer et al, 2007) 
and Freebase (Bollacker et al, 2007) have har-
vested this structured data and have made it avail-
able as triples for semantic querying.  
While infoboxes are a readily available source 
of structured data, the free text of the article con-
tains much more information about the entity. 
Barker et al (2007) unified the state of the art ap-
proaches in natural language processing and 
knowledge representation in their prototype system 
for understanding free text. Text resources which 
are rich in hyperlinks especially to knowledge 
based resources (such as encyclopedias or diction-
aries) have additional information encoded in the 
form of links, which can be used to complement 
the existing systems for text understanding and 
knowledge discovery. Furthermore, systems such 
as Wikify (Mihalcea and Csomai, 2007) can be 
employed to link words in free text to knowledge 
resources like Wikipedia and thus enrich the free 
text with hyperlinks. 
We describe an approach for unsupervised on-
tology discovery from links in the free text of the 
Wikipedia articles, without specifying a relation or 
set of relations in advance. We first identify candi-
date slots and fillers for an entity, then classify en-
78
tities and finally derive a class hierarchy. We have 
evaluated our approach for the Person class, but it 
can be easily generalized to other entity types such 
as organizations, places, and products.   
The techniques we describe are not suggested 
as alternatives to natural language understanding or 
information extraction, but as a source for addi-
tional evidence that can be used to extract onto-
logical elements and relations from the kind of text 
found in Wikipedia and other heavily-linked text 
collections. This approach might be particularly 
useful in ?slot fillings? tasks like the one in the 
Knowledge Base Population track (McNamee and 
Dang, 2010) at the 2009 Text Analysis Confer-
ence.  We see several contributions that this work 
has to offer: 
? Unsupervised and unrestricted ontology discov-
ery. We describe an automatic approach that 
does not require a predefined list of relations or 
training data. The analysis uses inter-article 
links in the text and does not depend on existing 
infoboxes, enabling it to suggest slots and fillers 
that do not exist in any extant infoboxes. 
? Meaningful slot labels. We use WordNet (Mil-
ler et al, 1990) nodes to represent and label 
slots enabling us to exploit WordNet?s hy-
pernym and hyponym relations as a property hi-
erarchy. 
? Entity classification and class labeling. We in-
troduce a new feature set for entity classifica-
tion, i.e. the discovered ranked slots, which per-
forms better than other feature sets extracted 
from Wikipedia. We also present an approach 
for assigning meaningful class label vectors us-
ing WordNet nodes. 
? Deriving a class hierarchy. We have developed 
an approach for deriving a class hierarchy based 
on the ranked slot similarity between classes 
and the label vectors.  
In the remainder of the paper we describe the de-
tails of the approach, mention closely related work, 
present and discuss preliminary results and provide 
some conclusions and possible next steps. 
2 Approach 
Figure 1 shows our ontology discovery framework 
and its major steps. We describe each step in the 
rest of this section.  
2.1 Discovering Candidate Slots and Fillers 
Most Wikipedia articles represent a concept, i.e., a 
generic class of objects (e.g., Musician), an indi-
vidual object (e.g., Michael_Jackson), or a generic 
relation or property (e.g., age). Inter-article links 
within Wikipedia represent relations between con-
cepts. In our approach we consider the linked con-
cepts as candidate fillers for slots related to the 
primary article/concept. There are several cases 
where the filler is subsumed by the slot label for 
example, the infobox present in the article on ?Mi-
chael_Jackson? (Figure 2) mentions pop, rock and 
soul as fillers for the slot Genre and all three of 
these are a type of Genre. The Labels slot contains 
fillers such as Motown, Epic and Legacy which are 
all Record Label Companies. Based on this obser-
vation, we discover and exploit ?isa? relations be-
tween fillers (linked concepts) and WordNet nodes 
to serve as candidate slot labels.  
In order to find an ?isa? relation between a con-
cept and a WordNet synset we use manually cre-
ated mappings by DBpedia, which links about 
467,000 articles to synsets. However, Wikipedia 
has more than two million articles1, therefore, to 
map any remaining concepts we use the automati-
cally generated mappings available between 
WordNet synsets and Wikipedia categories 
(Ponzetto and Navigli, 2009). A single Wikipedia 
article might have multiple categories associated 
with it and therefore multiple WordNet synsets. 
Wikipedia?s category system serves more as a way 
to tag articles and facilitate navigation rather than 
                                                 
1 This estimate is for the English version and does not 
include redirects and administrative pages such as dis-
ambiguation pages. 
 
Figure 1: The ontology discovery framework com-
prises a number of steps, including candidate slot and 
filler discovery followed by slot ranking, slot selec-
tion, entity classification, slot re-ranking, class label-
ing, and class hierarchy discovery. 
79
to categorize them. The article on Michael Jordan, 
for example, has 36 categories associated with it. 
In order to select an individual WordNet synset as 
a label for the concept?s type, we use two heuris-
tics: 
? Category label extraction. Since the first sen-
tence in Wikipedia articles usually defines the 
concept, we extract a category label from the 
first sentence using patterns based on POS tags 
similar to Kazama and Torisawa (2007). 
? Assign matching WordNet synset. We con-
sider all the WordNet synsets associated with 
the categories of the article using the category 
to WordNet mapping (Ponzetto and Navigli, 
2009) and assign the WordNet synset if any of 
the words in the synset matches with the ex-
tracted category label. We repeat the process 
with hypernyms and hyponyms of the synset 
up to three levels.  
2.2 Slot Ranking 
All slots discovered using outgoing links might not 
be meaningful, therefore we have developed tech-
niques for ranking and selecting slots. Our ap-
proach is based on the observation that entities of 
the same type have common slots. For example, 
there is a set of slots common for musical artists 
whereas, a different set is common for basketball 
players. The Wikipedia infobox templates based 
on classes also provide a set of properties or slots 
to use for particular types of entities or concepts.  
In case of people, it is common to note that 
there is a set of slots that are generalized, i.e., they 
are common across all types of persons.  Examples 
are name, born, and spouse.  There are also sets of 
specialized slots, which are generally associated 
with a given profession.  For example, the slots for 
basketball players have information for basketball 
related activities and musical artists have slots with 
music related activities. The slots for ?Mi-
chael_Jordan? include Professional Team(s), NBA 
Draft, Position(s) and slots for ?Michael_Jackson? 
include Genres, Instruments and Labels. 
Another observation is that people engaged in a 
particular profession tend to be linked to others 
within the same profession.  Hence the maxim ?A 
man is known by the company he keeps.? For ex-
ample, basketball players are linked to other bas-
ketball players and politicians are linked to other 
politicians. We rank the slots based on the number 
of linked persons having the same slots. We gener-
ated a list of person articles in Wikipedia by get-
ting all Wikipedia articles under the Person type in 
Freebase2. We randomly select up to 25 linked per-
sons (which also link back) and extract their candi-
date slots and vote for a slot based on the number 
of times it appears as a slot in a linked person nor-
malized by the number of linked persons to assign 
a slot score.  
2.3 Entity Classification and Slot Re-Ranking 
The ranked candidate slots are used to classify en-
tities and then further ranked based on number of 
times they appear among the entities in the cluster. 
We use complete link clustering using a simple slot 
similarity function: 
 
 
 
This similarity metric for slots is computed as the 
cosine similarity between tf.idf weighted slot vec-
tors, where the slot score represents the term fre-
                                                 
2 We found that the Freebase classification for Person 
was more extensive that DBpedia?s in the datasets avail-
able to us in early 2009. 
 
 
 
 
Figure 2.  The Wikipedia infobox 
for the Michael_Jackson article has 
a number of slots from appropriate 
infobox templates. 
 
80
quency component and the inverse document fre-
quency is based on the number of times the slot 
appears in different individuals. 
We also collapsed location expressing slots 
(country, county, state, district, island etc.) into the 
slot labeled location by generating a list of location 
words from WordNet as these slots were causing 
the persons related to same type of geographical 
location to cluster together.  
After clustering, we re-score the slots based on 
number of times they appear among the individuals 
in the cluster normalized by the cluster size. The 
output of clustering is a vector of scored slots as-
sociated with each cluster. 
2.4 Slot Selection 
The slot selection process identifies and filters out 
slots judged to be irrelevant. Our intuition is that 
specialized slots or attributes for a particular entity 
type should be somehow related to each other. For 
example, we would expect attributes like league, 
season and team for basketball players and genre, 
label, song and album for musical artists. If an at-
tribute like album appears for basketball players it 
should be discarded as it is not related to other at-
tributes. 
We adopted a clustering approach for finding 
attributes that are related to each other. For each 
pair of attributes in the slot vector, we compute a 
similarity score based on how many times the two 
attribute labels appear together in Wikipedia per-
son articles within a distance of 100 words as 
compared to the number of times they appear in 
total and weigh it using weights of the individual 
attributes in the slot vector. This metric is captured 
in the following equation, where Df is the docu-
ment frequency and wt is the attribute weight. 
 
 
Our initial experiments using single and com-
plete link clustering revealed that single link was 
more appropriate for slot selection. We got clusters 
at a partition distance of 0.9 and selected the larg-
est cluster from the set of clusters. In addition, we 
also added any attributes exceeding a 0.4 score into 
the set of selected attributes. Selected ranked slots 
for Michael Jackson are given in Table 1.   
2.5 Class Labeling 
Assigning class labels to clusters gives additional 
information about the type of entities in a cluster. 
We generate a cluster label vector for each cluster 
which represents the type of entities in the cluster. 
We compute a list of person types by taking all 
hyponyms under the corresponding person sense in 
WordNet. That list mostly contained the profes-
sions list for persons such as basketball player, 
president, bishop etc. To assign a WordNet type to 
a person in Wikipedia we matched the entries in 
the list to the words in the first sentence of the per-
son article and assigned it the set of types that 
matched. For example, for Michael Jordan the 
matching types found were basketball_player, 
businessman and player. 
We assigned the most frequent sense to the 
matching word as followed by Suchanek et al 
(2008) and Wu and Weld (2008), which works for 
majority of the cases. We then also add all the hy-
pernyms of the matching types under the Person 
node. The vector for Michael Jordan has entries 
basketball_player, athlete, businessperson, person, 
contestant, businessman and player. After getting 
matching types and their hypernyms for all the 
members of the cluster, we score each type based 
on the number of times it occurs in its members 
normalized by the cluster size. For example for one 
of the clusters with 146 basketball players we got 
the following label vector: {player:0.97, contest-
ant:0.97, athlete:0.96, basketball_player:0.96}. To 
select an individual label for a class we can pick 
the label with the highest score (the most general-
Slot Score Fillers Example 
Musician 1.00 ray_charles, sam_cooke ...  
Album 0.99 bad_(album), ... 
Location 0.97 gary,_indiana,  chicago,  ? 
Music_genre 0.90 pop_music, soul_music, ... 
Label 0.79 a&m_records, epic_records, ... 
Phonograph_ 
record 
0.67 
give_in_to_me, 
this_place_hotel ? 
Act 0.59 singing 
Movie 0.46 moonwalker ? 
Company 0.43 war_child_(charity), ? 
Actor 0.41 stan_winston, eddie_murphy,  
Singer 0.40 britney_spears, ? 
Magazine 0.29 entertainment_weekly,? 
Writing_style 0.27 hip_hop_music 
Group 0.21 'n_sync, RIAA 
Song 0.20 d.s._(song) ? 
 
  Table 1: Fifteen slots were discovered for musician 
Michael Jackson along with scores and example fillers. 
81
ized label) or the most specialized label having a 
score above a given threshold. 
2.6 Discovering Class Hierarchy 
We employ two different feature sets to discover 
the class hierarchy, i.e., the selected slot vectors 
and the class label vectors and combine both func-
tions using their weighted sum. The similarity 
functions are described below. 
The common slot similarity function is the co-
sine similarity between the common slot tf.idf vec-
tors, where the slot score represents the tf and the 
idf is based on the number of times a particular slot 
appears in different clusters at that iteration. We 
re-compute the idf term in each iteration. We de-
fine the common slot tf.idf vector for a cluster as 
one where we assign a non-zero weight to only the 
slots that have non-zero weight for all cluster 
members. The label similarity function is the co-
sine similarity between the label vectors for clus-
ters.  The hybrid similarity function is a weighted 
sum of the common slot and label similarity func-
tions. Using these similarity functions we apply 
complete link hierarchical clustering algorithm to 
discover the class hierarchy. 
 
 
3 Experiments and Evaluation 
For our experiments and evaluation we used the 
Wikipedia dump from March 2008 and the DBpe-
dia infobox ontology created from Wikipedia 
infoboxes using hand-generated mappings (Auer et 
al., 2007). The Person class is a direct subclass of 
the owl:Thing class and has 21 immediate sub-
classes and 36 subclasses at the second level. We 
used the persons in different classes in DBpedia 
ontology at level two to generate data sets for ex-
periments.  
There are several articles in Wikipedia that are 
very small and have very few out-links and in-
links. Our approach is based on the out-links and 
availability of information about different related 
things on the article, therefore, in order to avoid 
data sparseness, we randomly select articles with 
greater than 100 in-links and out-links, at least 
5KB page length and having at least five links to 
entities of the same type that link back (in our case 
persons).  
We first compare our slot vector features with 
other features extracted from Wikipedia for entity 
classification task and then evaluate their accuracy. 
We then discover the class hierarchy and compare 
the different similarity functions.  
3.1 Entity Classification 
We did some initial experiments to compare our 
ranked slot features with other feature sets ex-
tracted from Wikipedia. We created a dataset com-
posed of 25 different classes of Persons present at 
level 2 in the DBpedia ontology by randomly se-
lecting 200 person articles from each class. For 
several classes we got less than 200 articles which 
fulfilled our selection criteria defined earlier. We 
generated twelve types of feature sets and evalu-
ated them using ground truth from DBpedia ontol-
ogy. 
We compare tf.idf vectors constructed using 
twelve different feature sets: (1) Ranked slot fea-
tures, where tf is the slot score; (2) Words in first 
sentence of an article; (3) Associated categories; 
(4) Assigned WordNet nodes (see section 2.2); (5) 
Associated categories tokenized into words; (6) 
Combined Feature Sets 1 to 5 (All); (7-11) Feature 
sets 7 to 11 are combinations excluding one feature 
set at a time; (12) Unranked slots where tf is 1 for 
all slots. We applied complete link clustering and 
evaluated the precision, recall and F-measure at 
different numbers of clusters ranging from one to 
100.  Table 2 gives the precision, recall and num-
ber of clusters where we got the maximum F-
measure using different feature sets. 
82
 Feature set 10 (all features except feature 2) gave 
the best F-measure i.e. 0.74, whereas, feature set 1 
(ranked slots only) gave the second best F-measure 
i.e. 0.73 which is very close to the best result. Fea-
ture set 12 (unranked slots) gave a lower F-
measure i.e. 0.61 which shows that ranking or 
weighing slots based on linked entities of the same 
type performs better for classification. 
3.2 Slot and Filler Evaluation 
To evaluate our approach to finding slot fillers, we 
focused on DBpedia classes two levels below Per-
son (e.g., Governor and FigureSkater). We ran-
domly selected 200 articles from each of these 
classes using the criteria defined earlier to avoid 
data sparseness. Classes for which fewer than 20 
articles were found were discarded. The resulting 
dataset comprised 28 classes and 3810 articles3. 
We used our ranked slots tf.idf feature set and 
ran a complete link clustering algorithm producing 
clusters at partition distance of 0.8. The slots were 
re-scored based on the number of times they ap-
peared in the cluster members normalized by the 
cluster size. We applied slot selection over the re-
scored slots for each cluster. In order to evaluate 
our slots and fillers we mapped each cluster to a 
DBpedia class based on the maximum number of 
members of a particular DBpedia class in our clus-
ter. This process predicted 124 unique properties 
for the classes.  Of these, we were able to manually 
align 46 to properties in either DBpedia or Free-
                                                 
3 For some of the classes, fewer than the full comple-
ment of 200 articles were found. 
base for the corresponding class. We initially tried 
to evaluate the discovered slots by comparing them 
with those found in the ontologies underlying 
DBpedia and Freebase, but were able to find an 
overlap in the subject and object pairs for very few 
properties. 
We randomly selected 20 subject object pairs 
for each of the 46 properties from the correspond-
ing classes and manually judged whether or not the 
relation was correct by consulting the correspond-
No. Property Accuracy 
1 automobile_race 1.00 
2 championship 1.00 
3 expressive_style 1.00 
4 fictional_character 1.00 
5 label 1.00 
6 racetrack 1.00 
7 team_sport 1.00 
8 writing_style 1.00 
9 academic_degree 0.95 
10 album 0.95 
11 book 0.95 
12 contest 0.95 
13 election 0.95 
14 league 0.95 
15 phonograph_record 0.95 
16 race 0.95 
17 tournament 0.94 
18 award 0.90 
19 movie 0.90 
20 novel 0.90 
21 school 0.90 
22 season 0.90 
23 serial 0.90 
24 song 0.90 
25 car 0.85 
26 church 0.85 
27 game 0.85 
28 musical_instrument 0.85 
29 show 0.85 
30 sport 0.85 
31 stadium 0.85 
32 broadcast 0.80 
33 telecast 0.80 
34 hockey_league 0.75 
35 music_genre 0.70 
36 trophy 0.70 
37 university 0.65 
38 character 0.60 
39 disease 0.60 
40 magazine 0.55 
41 team 0.50 
42 baseball_club 0.45 
43 club 0.45 
44 party 0.45 
45 captain 0.30 
46 coach 0.25 
  Avg. Accuracy: 0.81 
 
Table 3: Manual evaluation of discovered properties 
 
No. Feature Set k P R F 
1 Ranked Slots  40 0.74 0.72 0.73 
2 First Sentence 89 0.07 0.53 0.12 
3 Categories 1 0.05 1.00 0.10 
4 WordNet Nodes 87 0.40 0.22 0.29 
5 (3 tokenized) 93 0.85 0.47 0.60 
6 All (1 to 5) 68 0.87 0.62 0.72 
7 (All ? 5) 82 0.79 0.46 0.58 
8 (All ? 4) 58 0.78 0.63 0.70 
9 (All ? 3) 53 0.76 0.65 0.70 
10 (All ? 2) 58 0.88 0.63 0.74 
11 (All ? 1) 57 0.77 0.60 0.68 
12 (1 unranked) 34 0.57 0.65 0.61 
 
Table 2: Comparison of the precision, recall and F-
measure for different feature sets for entity classifi-
cation.  The k column shows the number of clusters 
that maximized the F score.  
 
83
ing Wikipedia articles (Table 3).  The average ac-
curacy for the 46 relations was 81%. 
3.3 Discovering Class Hierarchy 
In order to discover the class hierarchy, we took all 
of the clusters obtained earlier at partition distance 
of 0.8 and their corresponding slot vectors after 
slot selection. We experimented with different 
similarity functions and evaluated their accuracy 
by comparing the results with the DBpedia ontol-
ogy. A complete link clustering algorithm was ap-
plied using different settings of the similarity func-
tions and the resulting hierarchy compared to 
DBpedia?s Person class hierarchy. Table 4 shows 
the highest F measure obtained for Person?s imme-
diate sub-classes (L1), ?sub-sub-classes? (L2) and 
the number of clusters (k) for which we got the 
highest F-measure using a particular similarity 
function. 
The highest F-measure both at level 2 (0.63) and 
level 1 (0.79) was obtained by simhyb with wc=0.2, 
wl=0.8 and also at lowest number of clusters at L1 
(k=8). The simhyb (wc=wl=0.5) and simlabel functions 
gave almost the same F-measure at both levels. 
The simcom_slot function gave better performance at 
L1 (F=0.65) than the base line simslot (F=0.55) 
which was originally used for entity clustering. 
However, both these functions gave the same F-
measure at L2 (F=0.61). 
4 Discussion  
In case of property evaluation, properties for which 
the accuracy was 60% or below include coach, 
captain, baseball_club, club, party, team and 
magazine. For the magazine property (correspond-
ing to Writer and ComicsCreator class) we ob-
served that many times a magazine name was men-
tioned in an article because it published some news 
about a person rather than that person contributing 
any article in that magazine. For all the remaining 
properties we observed that these were related to 
some sort of competition. For example, a person 
played against a team, club, coach or captain. The 
political party relation is a similar case, where arti-
cles frequently mention a politician?s party affilia-
tion as well as significant opposition parties. For 
such properties, we need to exploit additional con-
textual information to judge whether the person 
competed ?for? or ?against? a particular team, 
club, coach or party. Even if the accuracy for fill-
ers for such slots is low, it can still be useful to 
discover the kind of slots associated with an entity.  
We also observed that there were some cases 
where the property was related to a family member 
of the primary person such as for disease, school 
and university. Certain other properties such as 
spouse, predecessor, successor, etc. require more 
contextual information and are not directly evident 
in the link structure. However, our experiments 
show that there are certain properties that can be 
predicted with high accuracy using the article links 
only and can be used to enrich the existing infobox 
ontology or for other purposes.  
While our work has mostly experimented with 
person entities, the approach can be applied to oth-
er types as well. For example, we were able to dis-
cover software as a candidate slot for companies 
like Microsoft, Google and Yahoo!, which ap-
peared among the top three ranked slots using our 
slot ranking scheme and corresponds to the prod-
ucts slot in the infoboxes of these companies.  
For class hierarchy discovery, we have ex-
ploited the specialized slots after slot selection. 
One way to incorporate generalized slots in the 
hierarchy is to consider all slots for class members 
(without slot selection) and recursively propagate 
the common slots present at any level to the level 
above it. For example, if we find the slot team to 
be common for different types of Athletes such as 
basketball players, soccer players etc. we can 
propagate it to the Athlete class, which is one level 
higher in the hierarchy.  
5 Related Work 
Unsupervised relation discovery was initially in-
troduced by Hasegawa et al (2004). They devel-
oped an approach to discover relations by cluster-
ing pairs of entities based on intervening words 
represented as context vectors. Shinyama and Se-
kine (2006) generated basic patterns using parts of 
text syntactically connected to the entity and then 
Similarity Function k (L=2) 
F 
(L=2) 
k 
(L=1) 
F 
(L=1) 
simslot  56 0.61 13 0.55 
simcom_slot  74 0.61 15 0.65 
simlabel  50 0.63 10 0.76 
simhyb wc=wl=0.5 59 0.63 10 0.76 
simhyb wc=0.2, wl=0.8 61 0.63 8 0.79 
 
Table 4: Evaluation results for class hierarchy predic-
tion using different similarity functions. 
84
generated a basic cluster composed of a set of 
events having the same relation. 
Several approaches have used linguistic analysis 
to generate features for supervised or un-
supervised relation extraction (Nguyen et al, 2007; 
Etzioni et al, 2008; Yan et al, 2009). Our ap-
proach mainly exploits the heavily linked structure 
of Wikipedia and demonstrates that there are sev-
eral relations that can be discovered with high ac-
curacy without the need of features generated from 
a linguistic analysis of the Wikipedia article text.  
Suchanek et al (2008) used Wikipedia catego-
ries and infoboxes to extract 92 relations by apply-
ing specialized heuristics for each relation and in-
corporated the relations in their YAGO ontology, 
whereas our techniques do not use specialized heu-
ristics based on the type of relation.  Kylin (Weld 
et al, 2008) generated infoboxes for articles by 
learning from existing infoboxes, whereas we can 
discover new fillers for several existing slots and 
also discover new slots for infoboxes. KOG (Wu 
and Weld, 2008) automatically refined the Wiki-
pedia infobox ontology and integrated Wikipedia?s 
infobox-class schemata with WordNet. Since we 
already use the WordNet nodes for representing 
slots, it eliminates the need for several of KOG?s 
infobox refinement steps. 
While YAGO, Kylin and KOG all rely on rela-
tions present in the infoboxes, our approach can 
complement these by discovering new relations 
evident in inter-article links in Wikipedia. For ex-
ample, we could add slots like songs and albums to 
the infobox schema for Musical Artists, movies for 
the Actors infobox schema, and party for the Poli-
ticians schema. 
6 Conclusions and Future Work 
People have been learning by reading for thou-
sands of years.  The past decade, however, has 
seen a significant change in the way people read.  
The developed world now does much of its reading 
online and this change will soon be nearly univer-
sal.  Most online content is read as hypertext via a 
Web browser or custom reading device. Unlike 
text, hypertext is semi-structured information, es-
pecially when links are drawn from global name-
space, making it easy for many documents to link 
unambiguously to a common referent. 
The structured component of hypertext aug-
ments the information in its plain text and provides 
an additional source of information from which 
both people and machines can learn.  The work 
described in this paper is aimed at learning useful 
information, both about the implicit ontology and 
facts, from the links embedded in collection of hy-
pertext documents. 
Our approach is fully unsupervised and does 
not require having a pre-defined catalogue of rela-
tions. We have discovered several new slots and 
fillers that are not present in existing Wikipedia 
infoboxes and also a scheme to rank the slots based 
on linked entities of the same type. We compared 
our results with ground truth from the DBpedia 
infobox ontology and Freebase for the set of prop-
erties that were common and manually evaluated 
the accuracy of the common properties. Our results 
show that there are several properties that can be 
discovered with high accuracy from the link struc-
ture in Wikipedia and can also be used to discover 
a class hierarchy.  
We plan to explore the discovery of slots from 
non-Wikipedia articles by linking them to Wikipe-
dia concepts using existing systems like Wikify 
(Mihalcea and Csomai, 2007). Wikipedia articles 
are encyclopedic in nature with the whole article 
revolving around a single topic or concept.  Con-
sequently, linked articles are a good source of 
properties and relations. This might not be the case 
in other genres, such as news articles, that discuss 
a number of different entities and events.  One way 
to extend this work to other genres is by first de-
tecting the entities in the article and then only 
processing links in sentences that mention an entity 
to discover its properties. 
Acknowledgements 
The research described in this paper was supported 
in part by a Fulbright fellowship, a gift from Mi-
crosoft Research, NSF award IIS-0326460 and the 
Johns Hopkins University Human Language Tech-
nology Center of Excellence. 
85
 References 
S?ren Auer, Christian Bizer, Georgi Kobilarov, Jens 
Lehmann and Zachary Ives. 2007. DBpedia: A nu-
cleus for a web of open data. In Proceedings of the 
6th International Semantic Web Conference: 11?15. 
Ken Barker et al 2007. Learning by reading: A proto-
type system, performance baseline and lessons 
learned, Proceedings of the 22nd National Confer-
ence on Artificial Intelligence, AAAI Press. 
K. Bollacker, R. Cook, and P. Tufts. 2007. Freebase: A 
Shared Database of Structured General Human 
Knowledge. Proceedings of the National Conference 
on Artificial Intelligence (Volume 2): 1962-1963.  
Oren Etzioni, Michele Banko, Stephen Soderland, and 
Daniel S. Weld. 2008. Open information extraction 
from the web. Communications of the ACM 51, 12 
(December): 68-74. 
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering relations among named enti-
ties from large corpora. In Proceedings of the 42nd 
Annual Meeting of the Association for Computa-
tional Linguistics: 415-422.  
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as external knowledge for named en-
tity recognition. In Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning: 698?707. 
Paul McNamee and Hoa Trang Dang. 2009. Overview 
of the TAC 2009 knowledge base population track. 
In Proceedings of the 2009 Text Analysis Confer-
ence. National Institute of Standards and Technol-
ogy, November. 
Rada Mihalcea and Andras Csomai. 2007. Wikify!: 
linking documents to encyclopedic knowledge. In 
Proceedings of the 16th ACM Conference on 
Information and Knowledge Management: 233?242.  
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990. 
WordNet: An on-line lexical database. International 
Journal of Lexicography, 3:235?244.  
Dat P. T. Nguyen, Yutaka Matsuo, and Mitsuru Ishizu-
ka. 2007. Subtree mining for relation extraction from 
Wikipedia. In Proceedings of Human Language 
Technologies: The Annual Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics:125?128. 
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard 
Weikum. 2008. Yago: A large ontology from 
Wikipedia and WordNet. Web Semantics, 6(3):203?
217. 
Zareen Syed, Tim Finin, Varish Mulwad and Anupam 
Joshi. 2010. Exploiting a Web of Semantic Data for 
Interpreting Tables, Proceedings of the Second Web 
Science Conference. 
Simone P. Ponzetto and Roberto Navigli. 2009. Large-
scale taxonomy mapping for restructuring and inte-
grating Wikipedia. In Proceedings of the Twenty-
First International Joint Conference on Artificial In-
telligence: 2083?2088.  
Yusuke Shinyama and Satoshi Sekine. 2006. Pre-emp-
tive information extraction using unrestricted relation 
discovery. In Proceedings of Human Language Tech-
nologies: The Annual Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics:.  
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2008. 
Using Wikipedia to bootstrap open information ex-
trac-tion. SIGMOD Record, 37(4): 62?68. 
Fei Wu and Daniel S. Weld. 2008. Automatically refin-
ing the Wikipedia infobox ontology. In Proceedings 
of the 17th International World Wide Web Confer-
ence, pages 635?644. 
Wikipedia. 2008. Wikipedia, the free encyclopedia. 
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu 
Yang, and Mitsuru Ishizuka. 2009. Unsupervised re-
lation extraction by mining Wikipedia texts using in-
formation from the web. In Proceedings of the 47th 
Annual Meeting of the Association for Computa-
tional Linguistics: Volume 2: 1021?1029.  
 
86
