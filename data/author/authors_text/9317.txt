Chinese Base-Phrases Chunking  
Yuqi Zhang and Qiang Zhou 
State Key Laboratory of Intelligent Technology and Systems 
Department of Computer Science and Technology 
Tsinghua University, Beijing, 100084, P.R.China  
{zyq, zhouq}@s1000e.cs.Tsinghua.edu.cn 
   Abstract 
This paper introduces new definitions of Chinese 
base phrases and presents a hybrid model to 
combine Memory-Based Learning method and 
disambiguation proposal based on lexical 
information and grammar rules populated from a 
large corpus for 9 types of Chinese base phrases 
chunking. Our experiment achieves an accuracy 
(F-measure) of 93.4%. The significance of the 
research lies in the fact that it provides a solid 
foundation for the Chinese parser. 
1 Introduction 
Recognizing simple and non-recursive base phrases 
is an important subtask for many natural language 
processing applications, such as information 
retrieval. Gee and Grosjean (Gee and Grosjean, 
1983) showed psychological evidence that chunks 
like base phrases play an important role in human 
language understanding. CoNLL-2000?s shared 
task identified many kinds of English base phrases, 
which are syntactically related non-overlapping 
groups of words (Tjong and Buchholz, 2000). The 
shared task has significantly heightened the 
progress in the techniques of English partial 
parsing. For Chinese processing, Zhao (1998) put 
forward a definition of Chinese baseNP that is a 
combination of determinative modifier and head 
noun (Zhao, 1998). Based on that research, Zhao et 
al. (2000) extended the concept of baseNP to seven 
types of Chinese base phrases. These base phrases 
may consist of words or other base phrases, but its 
constituents, in turn, should not contain any base 
phrases.  
   In this paper, we put forward the new definition 
of Chinese base phrases, which are simple and 
non-recursive, similar to the CoNLL-2000?s shared 
task. The definition enables us to resolve most local 
ambiguities and is very useful for NLP tasks such as 
name entity recognition and information extraction. 
We construct a hybrid model to recognize nine 
types of Chinese base phrases. Many researches in 
Chinese partial parsing (Zhou, 1996; Zhao, 1998; 
Sun, 2001) have shown that statistical learning is of 
great use for Chinese chunking, especially for large 
corpus. However, the lack of morphological hints in 
Chinese makes it necessary to use semantic and 
syntactic information such as context free grammar 
rules in Chinese processing. In our approach, 
viewing chunking as a tagging problem by 
encoding the chunk structure in new tags attached 
to each word, we use Memory-Based Learning 
(MBL) method to set a tag indicating type and 
position in a base phrase on each word. After which 
grammar rules are used to disambiguate the tags.  
Our test with a corpus of about 2 MB showed that 
the experiment achieves 94.4% in precision and 
92.5% in recall. 
2 Definitions of Chinese Base 
Phrases 
The idea of parsing by chunks goes back to Abney 
(1991). In his definition of chunks in English, he 
assumed that a chunk has syntactic structure and he 
defined chunks in terms of major heads, which are 
all content words except those that appear between 
a function word and the content word which  
selects. A major head is the ?semantic? head (s-head) 
for the root of the chunk headed by it. However, 
s-heads can be defined in terms of syntactic heads. 
If the syntactic head h  of a phrase P is a content 
word,  is also the s-head of P. If h  is a function 
word, the s-head of P is the s-head of the phrase 
selected by . 
f f
h
h
The research enlightens us about the definition of 
Chinese base phrases. In this paper, a Chinese base 
phrase consists of a single content word surrounded 
by a cluster of function words. The single content 
word is the semantic head of the base phrase. The 
forms of base phrases can be expressed as follows. 
 
 
{Modifier} * + head + {complement}* or 
Coordinate structure 
The components of ?modifier? and ?complement? 
are optional. A head could be a simple word as well 
as the structure of ?modifier + head? or ?head + 
complement?, but not ?modifier + head + 
complement?. Coordinate structure could not 
consist of coordinate symbols such as comma and 
co-ordinating conjunction. The type of base phrases 
is congruent with its head?s semantic information. 
In most cases, the type accords with the head?s 
syntactical information, for example, when the head 
is a noun, the phrase is a noun phrase. However, 
when a head is a noun that denotes a place, the base 
phrase including that head is not a noun phrase, but 
a location phrase. 
   We consider 9 types of Chinese base phrases in 
our research: namely adjective phrase (ap), 
distinguisher phrase (bp), adverbial phrase (dp), 
noun phrase (np), temporal phrase (tp), location 
phrase (sp), verb phrase (vp), quantity phrase (mp), 
quasi quantity phrase (mbar). The inner grammar 
structures of every base phrase are very important 
too, but we will discuss that in another paper. 
3 Overview 
The frame of Chinese base phrase parsing is 
composed of two parts: one is the ?Type and 
bracket tagging model?, the other is the ?Base 
phrases acquisition model? which consists of two 
modules which are ?brackets matching ?and 
?correct the types of base phrases?. (See figure 1.) 
The input to the system is a sequence of POS. In the 
?Predict the phrase boundary? module, we predict 
the type, which each word belongs to, and the 
position of each word in a base phrase with 
Memory-Based Learning (MBL)(Using the 
software package provided by Tilburg University.). 
And the result is expressed as a pair formed by base 
phrase type and position information. Because our 
Chinese base phrases are non-recursive and 
non-overlapping, the left and right boundaries of 
base phrases must match with each other which 
means they should be a pair and alternative. 
However, the errors involving in the first part will 
lead to incorrect base phrases because the 
boundaries do not match, for example ?[?[?]?.  In 
the second part, grammar rules that indicate the 
inner structures of base phrases are used to resolve 
the boundary ambiguities. Furthermore, it also 
takes lexical information into account to correct the 
type mistakes. 
The corpus used in the experiment includes 7606 
sentences. It comes from the Chinese Balance 
Corpus including about 2000 thousand words with 
four types: literature (44%), news (30%), academic 
article (20%) and spoken Chinese (6%). These 7606 
sentences are split into 6846 training sentences and 
760 held out for testing.  
  
   
 
                                
 
 
 
 
 
  
Input Type and bracket tagging 
model 
 Obtain feature vectors 
 Predict the phrase boundary 
 Brackets matching Grammar rules
                       
  
Correct the types of base 
phrases  Lexical 
information
Output Base phrases acquisition model
Figure 1: system overview 
4 Predicting the phrase boundaries 
with MBL 
Memory-Based Learning (MBL) is a classification 
based, supervised learning approach: a 
memory-based learning algorithm constructs a 
classifier for a task by storing a set of examples. 
Each example associates a finite number of classes. 
Given a new feature vector, the classifier 
extrapolates its class from those of the most similar 
feature vectors in memory (Daelemans et, al., 1999). 
The input to the ?Predict the phrase boundary? 
module is some feature vectors, which compose of a 
sequence of POS. The solution of the module is to 
find >< ii cr ,
i
ic
,,,{
 (Wojciech and Thorsten, 1998), a 
duple formed by a type tag and a boundary tag for 
each word t . Here r  indicates the boundary tag, 
while  denotes the type tag. 
i
,, },,,, ?? mbarmpbpspdpc j
, LRri
tp
}
apvpnp
,,,{ OIRL
(?-? denotes 
the word is not in any type of base phrases.) 
? The  indicates the position 
of the word in a base phrase as shown below: 
ir
?L?: the left boundary,     ?R?: the right boundary,  
 
?I?: the middle position, ?O?: outside any base 
phrases, ?LR?: the left and right boundary. 
What information is used to represent data in 
feature vectors is an important aspect in MBL 
algorithms. We tried many feature vectors with 
various lengths. And it is interesting to note that the 
feature window is not the bigger the better. When 
the feature window is (-2, +2) in the context, the 
result is the best. So the feature vector in the 
experiment is: (POS-2, POS-1, POS0, POS+1, 
POS+2). The pattern describes the combination of 
feature vector and result duple >< mncr  
: 90,40 ???? mn
(POS-2, POS-1, POS0, POS+1, POS+2, ). >< mncr
For the experiment in the first step, we use 
1TiMBL , an MBL software package developed in 
the ILK-group (Daelemans et, al., 2001). The 
results of phrase boundary prediction with MBL 
shows in table 1.  
Table1?The result of word boundary prediction 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 1 shows that there is much difference 
between the results of various types of base phrases. 
The precisions and recalls of np, vp, mp, ap and dp 
are all almost over 90%. Comparatively, the results 
of sp, tp, bp and mbar are much lower, especially 
their recalls. This is due to some resemblances 
between sp, tp and np in Chinese syntactical 
grammars. Sp and tp may be considered as belong 
to NP, however, in the definition of Chinese base 
phrases, sp, tp and np are defined separately for the 
semantic difference. And the separation can also 
help in other tasks such as proper noun 
identification, information retrieval etc.  
                                                     
TiMBL1  is a software bag about many MBL 
algorithms. It can be download free from 
http://ilk.kub.nl/  
5 Obtaining Chinese base phrases  
5.1 The errors in phrase boundary 
prediction 
There are three types of errors in the results of first 
processing model.  
(1) Boundary ambiguity: the r  ?s mistakes 
will cause the multiple choices regarding the 
boundaries. For example:  ?
i
{np ?/rN  } ?/m  ??/n  } ?
/?  ?/p  {np ??/t  {np ??/n  } ?/u  {np ??/vN  ??
/vN  } {ap ?/dD  ?/a  } ?/??. (Please pay attention to the 
?__? part.) There are altogether three modalities: 
?{ ?{ ?}?, ?{ ?}?}? and 
?{ ?{ ?}?}?. These are caused by the 
redundancy and absence of boundaries. 
mc
mc
mc mc
mc
(2) The type mistake of base phrases: For 
example: in the sentence of ?{np ??/n  } {dp ???
/d  } {vp ?/vC  } {np ????/nS  } ?/f  {tp ??/nR  ??
/n  } ?/p??, the parser mistakes the type of ?{ ??
/nR  ??/n  }? ,which is np, for tp. This error type 
commonly appears between sp, tp and np, as well as 
mbar and mp. 
 
 
Precision  
for<  >mncr
Recall  
For >< mncr
np 
vp 
sp 
tp 
ap 
bp 
dp 
mp 
mbar 
92.27% 
90.40% 
75.15% 
82.87% 
93.52% 
92.60% 
97.56% 
93.90% 
74.15% 
93.61% 
89.65% 
48.41% 
71.62% 
91.89% 
76.38% 
97.63% 
92.38% 
72.26% 
Total1 91.90% 91.65% 
- 97.85% 98.41% 
Total2 93.83% 93.83% 
(3)   Boundaries absence: For example, in the 
sentence of ?{vp ??/v  } {np ??/n  } ?/?  {np ??/n  ?
?/n  } ??/c  {vp ??/v  }?,  ?{np ??/n  ??/n  }? 
should be ?{np ??/n  } {np ??/n  }?. It is very 
difficult to correct this type of errors because the 
boundary distribution accords with the definition of 
Chinese base phrases. The left and right boundaries 
alternate with each other. Therefore, it is very 
difficult to find the errors in the sequence from the 
modalities.  
5.2 Obtaining the whole base phrases 
with Grammar rules 
With the bracket (boundary) representation, 
incorrect bracket will be generated but these will be 
eliminated in the bracket combination process. In 
the experiment, we attempt to apply grammar rules 
that represent the inner structures of Chinese base 
phrases to get rid of the boundary ambiguities. 
These grammar rules are derived from the corpus. 
On the other hand, boundary predictions can find 
many base phrases that do not accord with the 
limited grammar rules.  
   Figure 2 shows the main strategy of how to use 
the grammar rules. When if ()>1, there are more 
 
than one pair of combined brackets in which the 
sequences accord with the grammar rules. We are 
apt to choose the longest possible because the 
shorter sequences appear more in the corpus. The 
longer the sequence, the more weight it should carry. 
When there is only the shorter sequence according 
with grammar rules, it is more possible to be the 
correct one. In this case, one or more boundaries 
will be left. They often need some other boundaries 
to match, so we try to retrieve some missing 
boundaries through the partitions in the sentences 
that should not belong to any base phrases. These 
partitions are the marks of base phrase boundaries. 
If we find these partitions between two ambiguous 
boundaries, we will know where to place the new 
boundary. 
5.3 Correct the type mistake with 
lexical information 
In the Chinese language, some POS sequences may 
belong to different types. For example, ?{vN n}? 
could be np, sp or tp. These sequences often appear 
in np, sp, tp, mp and mbar. It is difficult to know its 
right type even with the grammar rules, as we have 
done in section 5.2. In order to resolve this problem, 
we attempt to use lexical information because it 
implies semantic information to some extent. 
   The lexical information is distinctive between mp 
and mbar. mbar is often composed of numbers such 
as ?1200? and numbers in Chinese such as ???. 
The lexical information between tp and np is also 
obvious, such as ????, ???? and ???? etc. For 
sp and np, the words are ????, ???? etc. 
5.4 Experimental results  
Step 1:  Finding the sequence where the errors appear. The sequences are three types: 
?{?{?}?, ?{?}?}?, ?{?{?}?}?. 
Step 2:  if  (the number of sequences of POS in a pair of matched boundaries according with the grammar
rules) >1 
then {Select the boundaries that make the sequence longest} 
Step 3:  if (the number of sequences of POS in a pair of combined boundaries according with the grammar 
rules) =1 
if (Only the sequence with the shortest length accords with the grammar rules). 
then { Find partitions such as conjunctions, localizers, punctuations and some 
prepositions between the ambiguous boundaries in sequences; 
if (The partitions exist) 
then {Add boundaries to generate whole base phrases according to the
partitions} 
} 
 
Figure 2:  The Algorithm of Matching Boundaries     
The simplest bracket combination algorithm is very 
strict: it only uses adjacent brackets if they appear 
next to each other in the correct order (first open 
and then close) without any intervening brackets. 
The result of the algorithm is shown in table 2, as 
the baseline of the boundary combination 
experiment.  
Table 2: The base-line result 
   Precision Recall F_M 
Np 93.9% 86.1% 89.8% 
Vp 90.6% 86.2% 88.4% 
Sp 75.5% 47.7% 58.4% 
Tp 85.4% 70.2% 77.0% 
Ap 93.4% 83.4% 88.1% 
Bp 93.4% 71.3% 80.9% 
dp 97.7% 94.0% 95.8% 
mp 92.0% 85.3% 88.5% 
mbar ------- 0 ------- 
Total 92.9% 85.7% 89.2% 
 
Table 3: The result of disambiguation with 
grammar rules 
 Precision Recall F_M 
np 94.3% 91.9% 93.1% 
vp 95.0% 94.2% 94.6% 
sp 73.6% 50.9% 60.2% 
tp 84.9% 73.8% 79.0% 
ap 93.5% 89.7% 91.5% 
bp 91.6% 79.4% 85.0% 
dp 97.6% 98.1% 97.8% 
mp 86.7% 90.9% 88.7% 
mbar 63.6% 12.6% 21.1% 
Total 93.9% 92.0% 92.9% 
 
From the table 2, we could see the recalls are 
commonly low. We change another strategy to 
obtain the whole base phrases as described in 
section 5.2. The result of using the grammar rules is 
shown in table 3. 
With the help of grammar rules, all kinds of base 
phrases improved their f-measures though the 
precisions or recalls of some types decrease slightly. 
Comparing with the baseline results in table 2, all 
the recalls increase significantly. However, the 
recalls of sp, tp and mp still do not satisfy us. There 
are more than twenty structures of np which also 
belong to tp or sp. Except in the case where mp and 
mbar have the same structure {m}, they are easily 
distinguished in other structures. (Mbar is always 
composed of numerals and mp always ends with a 
quantifier.) In order to distinguish tp from np, sp 
from np and mbar from mp, we use lexical 
information for the type disambiguation. The 
results are shown in table 4. 
Table 4: The result after using lexical 
information 
 Precision Recall F_M 
np 95.0% 91.9% 93.5% 
vp 95.0% 94.3% 94.6% 
sp 69.2% 71.3% 70.2% 
tp 79.8% 84.1% 81.9% 
ap 93.1% 90.0% 91.5% 
bp 91.6% 79.4% 85.0% 
dp 97.6% 98.1% 97.8% 
mp 93.4% 90.9% 92.1% 
mbar 67.6% 54.1% 60.1% 
Total 94.4% 92.5% 93.4% 
  From the table 4, we could see improvement in 
all the results (precisions and recalls) of mp and 
mbar. It shows that the lexical information is 
effective for distinguishing between them. On the 
contrary, although the f-measures of np and sp 
increase, their precisions decline. Thus, those words 
marking tp and sp are not appropriate for 
disambiguation. We could see the effect of lexical 
information is limited because it is difficult to find 
the words that could distinguish different types of 
base phrases.  
6 Conclusions 
The experiment on identifying Chinese base 
phrases shows that the definition of Chinese base 
phrases is suitable for parsing. It shows good results 
and the efficiency of the proposed approach in 
simplifying sentence structures. Many tasks such as 
chunking on high level could benefit from this. 
With the system described here, we get 9 types of 
Chinese base phrases, and acquire high precisions 
and recalls on most types of base phrases. The 
results of the experiment also show that the use of 
grammar rules is necessary. Grammar rules have 
effects on boundary disambiguation particularly. 
The lexical information is effective in 
distinguishing between mbar and mp.  
Acknowledgements 
This work was supported by the National Science 
Foundation of China (Grant No. 69903007), 
National 973 Foundation (Grant No. 1998030507) 
and National 863 Plan (Grant No. 2001AA114040). 
References 
Abney, Steven. (1991) Parsing by chunks. In 
Berwick, Abney, and Tenny, editors, 
Principle-Based Parsing. Kluwer Academic 
Publishers. 
Erik F. Tjong Kim Sang and Sabine Buchholz. 
(2000). ?Introduction to CoNLL-200 Shared 
Task: Chunking?. Proceedings of CoNLL-2000 
and LLL-2000. Lisbon, Portugal. 127-132. 
J. P. Gee and F. Grosjean (1983) Performance 
structures: A psycholinguistic and linguistic 
appraisal. Cognitive Psychology, 15:411-458 
Sun Honglin (2001) A Content Chunk Parser for 
Unrestricted Chinese Text, Dissertation for the 
degree of Doctor of Science, Peking University.  
Walter Daelemans, Jakub Zavrel, Ko van der Sloot 
(2001) TiMBL:Tilburg Memory-Based Learner 
version 4.0 Reference Guide. 
http://ilk.kub.nl/downloads/pub/papers/ilk0104.p
s.pz. 
Wojciech Skut and Thorsten Brants (1998) Chunk 
Tagger, Statistical Recognition of Noun Phrase, 
In ESSLLI-98 Workshop on Automated 
Acquisition of Syntax and Parsing, Saarbrvcken. 
Zhao Jun (1998) The research on Chinese BaseNP 
Recognition and Structure Analysis, Dissertation 
for the degree of Doctor of Engineering, 
Tsinghua University. 
Zhao et al, (2000) Tie-jun ZHAO, et al ?Statistics 
Based Hybrid Approach to Chinese Base Phrase 
Identification?, In Proceedings of the Second 
Chinese Language Processing Workshop, ACL 
2000, 73-77. 
Zhou, Qiang (1996). Phrase Bracketing and 
Annotating on Chinese Language Corpus, Ph.D. 
dissertation, Peking University. 
 
 
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 1?8,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Chunk-Level Reordering of Source Language Sentences with
Automatically Learned Rules for Statistical Machine Translation
Yuqi Zhang and Richard Zens and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{yzhang,zens,ney}@cs.rwth-aachen.de
Abstract
In this paper, we describe a source-
side reordering method based on syntac-
tic chunks for phrase-based statistical ma-
chine translation. First, we shallow parse
the source language sentences. Then, re-
ordering rules are automatically learned
from source-side chunks and word align-
ments. During translation, the rules are
used to generate a reordering lattice for
each sentence. Experimental results are
reported for a Chinese-to-English task,
showing an improvement of 0.5%?1.8%
BLEU score absolute on various test sets
and better computational efficiency than
reordering during decoding. The exper-
iments also show that the reordering at
the chunk-level performs better than at the
POS-level.
1 Introduction
In machine translation, reordering is one of the ma-
jor problems, since different languages have differ-
ent word order requirements. Many reordering con-
straints have been used for word reorderings, such
as ITG constraints (Wu, 1996), IBM constraints
(Berger et al, 1996) and local constraints (Kanthak
et al, 2005). These approaches do not make use of
any linguistic knowledge.
Several methods have been proposed to use syn-
tactic information to handle the reordering problem,
e.g. (Wu, 1997; Yamada and Knight, 2001; Gildea,
2003; Melamed, 2004; Graehl and Knight, 2004;
Galley et al, 2006). One approach makes use of
bitext grammars to parse both the source and tar-
get languages. Another approach makes use of syn-
tactic information only in the target language. Note
that these models have radically different structures
and parameterizations than phrase-based models for
SMT.
Another kind of approaches is to use syntactic in-
formation in rescoring methods. (Koehn and Knight,
2003) apply a reranking approach to the sub-task
of noun-phrase translation. (Och et al, 2004) and
(Shen et al, 2004) describe the use of syntactic fea-
tures in reranking the output of a full translation sys-
tem, but the syntactic features give very small gains.
In this paper, we present a strategy to reorder
a source sentence using rules based on syntactic
chunks. It is possible to integrate reordering rules di-
rectly into the search process, but here, we consider
a more modular approach: easy to exchange reorder-
ing strategy. To avoid hard decisions before SMT,
we generate a source-reordering lattice instead of a
single reordered source sentence as input to the SMT
system. Then, the decoder uses the reordered source
language model as an additional feature function. A
language model trained on the reordered source-side
chunks gives a score for each path in the lattice. The
novel ideas in this paper are:
? reordering of the source sentence at the chunk
level,
? representing linguistic chunks-reorderings in a
lattice.
1
The rest of this paper is organized as follows. Sec-
tion 2 presents a review of related work. In Sec-
tions 3, we review the phrase-based translation sys-
tem used in this work and propose the framework
of the new reordering method. In Section 4, we in-
troduce the details of the reordering rules, how they
are defined and how to extract them. In Section 5,
we explain how to apply the rules and how to gen-
erate reordering lattice. In Section 6, we present
some results that show that the chunk-level source
reordering is helpful for phrase-based statistical ma-
chine translation. Finally, we conclude this paper
and discuss future work in Section 7.
2 Related Work
Beside the reordering methods during decoding, an
alternative approach is to reorder the input source
sentence to match the word order of the target sen-
tence.
Some reordering methods are carried out on syn-
tactic source trees. (Collins et al, 2005) describe
a method for reordering German for German-to-
English translation, where six transformations are
applied to the surface string of the parsed source
sentence. (Xia and McCord, 2004) propose an ap-
proach for translation from French-to-English. This
approach automatically extracts rewrite patterns by
parsing the source and target sides of the training
corpus. These rewrite patterns can be applied to any
input source sentence so that the rewritten source
and target sentences have similar word order. Both
methods need a parser to generate trees of source
sentences and are applied only as a preprocessing
step.
Another kind of source reordering methods be-
sides full parsing is based on Part-Of-Speech (POS)
tags or word classes. (Costa-jussa` and Fonollosa,
2006) view the source reordering as a translation
task that translate the source language into a re-
ordered source language. Then, the reordered source
sentence is taken as the single input to the standard
SMT system.
(Chen et al, 2006) automatically extract rules
from word alignments. These rules are defined at
the POS level and the scores of matching rules are
used as additional feature functions during rescor-
ing. (Crego and Marin?o, 2006) integrate source-side
reordering into SMT decoding. They automatically
learn rewrite patterns from word alignment and rep-
resent the patterns with POS tags. To our knowledge
no work is reported on the reordering with shallow
parsing.
Decoding lattices were already used in (Zens et
al., 2002; Kanthak et al, 2005). Those approaches
used linguistically uninformed word-level reorder-
ings.
3 System Overview
In this section, we will describe the phrase-based
SMT system which we use for the experiments.
Then, we will give an outline of the extentions with
the chunk-level source reordering model.
3.1 The Baseline Phrase-based SMT System
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Among all possible tar-
get language sentences, we will choose the sentence
with the highest probability:
e?I?1 = argmax
I,eI1
{
Pr(eI1|fJ1 )
} (1)
= argmax
I,eI1
{
Pr(eI1) ? Pr(fJ1 |eI1)
} (2)
This decomposition into two knowledge sources
is known as the source-channel approach to sta-
tistical machine translation (Brown et al, 1990).
It allows an independent modeling of the target
language model Pr(eI1) and the translation model
Pr(fJ1 |eI1). The target language model describes
the well-formedness of the target language sentence.
The translation model links the source language sen-
tence to the target language sentence. The argmax
operation denotes the search problem, i.e., the gen-
eration of the output sentence in the target language.
A generalization of the classical source-channel
approach is the direct modeling of the posterior
probability Pr(eI1|fJ1 ). Using a log-linear model
2
(Och and Ney, 2002), we obtain:
Pr(eI1|fJ1 ) =
exp
(
?M
m=1 ?mhm(eI1, fJ1 )
)
?
I?,e?I?1
exp
(
?M
m=1 ?mhm(e?I
?
1 , fJ1 )
)
(3)
The denominator represents a normalization factor
that depends only on the source sentence fJ1 . There-
fore, we can omit it during the search process. As a
decision rule, we obtain:
e?I?1 = argmax
I,eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
(4)
The log-linear model has the advantage that addi-
tional models h(?) can be easily integrated into the
overall system. The model scaling factors ?M1 are
trained according to the maximum entropy principle,
e.g., using the GIS algorithm. Alternatively, one can
train them with respect to the final translation quality
measured by an error criterion (Och, 2003).
The log-linear model is a natural framework to in-
tegrate many models. The baseline system uses the
following models:
? phrase translation model
? phrase count features
? word-based translation model
? word and phrase penalty
? target language model (6-gram)
? distortion model (assigning costs based on the
jump width)
All the experiments in the paper are evaluated with-
out rescoring. More details about the baseline sys-
tem can be found in (Mauser et al, 2006)
3.2 Source Sentence Reordering Framework
Encouraged by the work of (Xia and McCord, 2004)
and (Crego and Marin?o, 2006), we also reorder the
source language side. Compared to reordering on
the target language side, one advantage is the effi-
ciency since the reordering lattice can be translated
monotonically as in (Zens et al, 2002). Another ad-
vantage is that there is correct sentence information
POS tagging
shallow chunking
Translation Process
Standard Translation Proces
with Source Reordering
source text sentences
reordering rules
SMT system
translation output translation output
source text sentences
SMT system
source reordering lattice
Figure 1: Illustration of the translation process with
and without source reordering.
for the reordering methods, because the source sen-
tences are always given. Syntactic reordering on tar-
get language is difficult, since the methods will de-
grade much because of the errors in hypothesis.
We apply reordering at the syntactic chunk level
which can been seen as an intermediate level be-
tween full parsing and POS tagging. Figure 1 shows
the differences between the new translation frame-
work and the standard translation process. A re-
ordering lattice replaces the original source sentence
as the input to the translation system. The use of a
lattice avoids hard decisions before translation. To
generate the reordering lattice, the source sentence is
first POS tagged and chunk parsed. Then, reorder-
ing rules are applied to the chunks to generate the
reordering lattice.
Reordering rules are the key information for
source reordering. They are automatically learned
from the training data. The details of these two mod-
ules will be introduced in Section 5.
4 Reordering Rules
There has been much work on learning and apply-
ing reordering rules on source language, such as
(Nie?en and Ney, 2001; Xia and McCord, 2004;
Collins et al, 2005; Chen et al, 2006; Crego and
Marin?o, 2006; Popovic? and Ney, 2006). The re-
ordering rules could be composed of words, POS
tags or syntactic tags of phrases. In our work, a rule
is composed of chunk tags and POS tags. There is
3
Table 1: Examples of reordering rules. (lhs: chunk
and POS tag sequence, rhs: permutation )
no. lhs rhs
1. NP0 PP1 u2 n3 0 1 2 3
2. NP0 PP1 u2 n3 3 0 1 2
3. DNP0 NP1 V P2 0 1 2
4. DNP0 NP1 V P2 1 0 2
5. DNP0 NP1 m2 0 1 2
6. DNP0 NP1 m2 ad3 3 0 1 2
7. DNP0 NP1 m2 ad3 v4 4 3 0 1 2
no hierarchical structure in a rule.
4.1 Definition of Reordering Rules
First, we show some rule examples in Table 1. A re-
ordering rule consists of a left-hand-side (lhs) and a
right-hand-side (rhs). The left-hand-side is a syn-
tactic rule (chunk or POS tags), while the right-
hand-side is the reordering positions of the rule. Dif-
ferent rules can share the same left-hand-side, such
as rules no. 1, 2 and no. 3, 4. The rules record
not only the real reordered chunk sequence, but also
the monotone chunk sequences, like no. 1, 3 and
5. Note that the same tag sequence can appear mul-
tiple times according to different contexts, such as
DNP0 NP1 m2 # 0 1 2 in rules no. 5, 6, 7.
4.2 Extraction of Reordering Rules
The extraction of reordering rules is based on the
word alignment and the source sentence chunks.
Here, we train word alignments in both directions
with GIZA++ (Och and Ney, 2003). To get algn-
ment with high accuracy, we use the intersection
alignment here.
For a given word-aligned sentence pair
(fJ1 , eI1, aJ1 ), the source word sequence fJ1 is
first parsed into a chunk sequence FK1 . Accord-
ingly, the word-to-word alignment aJ1 is changed
to a chunk-to-word alignment a?K1 which is the
combination of the target words aligned to the
source words in a chunk. It is defined as:
a?k = {i|i = aj ? j ? [jk, jk+1 ? 1]}
Figure 2: Illustration of three kinds of phrases:
(a)monotone phrase, (b)reordering phrase, (c)cross
phrase. The black box is a word-to-word alignment.
The gray box is a chunk-to-word alignment.
Here, jk denotes the position of the first source word
in kth chunk. The new alignment is 1 : m from
source chunks to target words. It also means a?k is a
set of positions of target words.
We apply the standard phrase extraction algorithm
(Zens et al, 2002) to (FK1 , eI1, a?K1 ). Discarding the
cross phrases, we keep the other phrases as rules. In
a cross phrase, at least two chunk-word alignments
overlap on the target language side. An example
of a cross phrase is illustrated in Figure 2(c). Fig-
ure 2(a) and (b) illustrate the phrases for reordering
rules, which could be monotone phrases or reorder-
ing phrases.
5 Reordering Lattice Generation
5.1 Parsing the Source Sentence
The first step of chunk parsing is word segmentation.
Then, a POS tagger is usually needed for further
syntactic analysis. In our experiments, we use the
tool of ?Inst. of Computing Tech., Chinese Lexical
Analysis System (ICTCLAS)? (Zhang et al, 2003),
which does the two tasks in one pass.
Referring to the description of the chunking task
in CoNLL-20001, instead of English, a Chinese
chunker is processed and evaluated. Each word is
assigned a chunk tag, which contains the name of the
chunk type and ?B? for the first word of the chunk
and ?I? for each other word in the chunk. The ?O?
chunk tag is used for tokens which are not part of
any chunk. We use the maximum entropy tool YAS-
1http://www.cnts.ua.ac.be/conll2000/chunking/
4
Figure 3: Example of applying rules. The left part is the used rules. The right part is the generated new
orders of source words.
MET2 to learn the chunking model. The model is
based on a combination of word and POS tags. Since
specific training and test data are not available for
Chinese chunking, we convert subtrees of the Chi-
nese treebank (LDC2005T01) into chunks. As there
are many ways to choose a subtree, we uses the min-
imum subtree with the following constraints:
? a subtree has more than one child,
? the children of a subtree are all leaves.
Compared to chunking of English as in CoNLL-
2000, there are more chunk types (24 instead of 6)
and no single-word chunks. These two aspects make
chunking for Chinese harder.
5.2 Applying Reordering Rules
First, we search the reordering rules, in which the
chunk sequence matches any tag sequence in the in-
put sentence. A source sentence has many paths
generated by the rules . For a word uncovered by any
rules, its POS tag is used. Each path corresponds to
one sentence permutation.
The left part of the Figure 3 shows seven possible
coverages, the right part is the reordering for each
coverage. Some of the reorderings are identical, like
the permutations in line 1, 3 and 5. That is because
one word sequence is memorized by several rules in
different contexts.
5.3 Lattice Weighting
All reorderings of an input sentence S are com-
pressed and stored in a lattice. Each path is a possi-
2http://www-i6.informatik.rwth-aachen.de/web/Software
/index.html
ble reordering S? and is given a weight W . In this
paper, the weight is computed using a source lan-
guage model p(S?). The weight is used directly in
the decoder, integrated into Equation (4). There is
also a scaling factor for this weight, which is op-
timized together with other scaling factors on the
development data. The probability of the reordered
source sentence is calculated as follows: for a re-
ordered source sentence w1w2...wn, the trigram lan-
guage model is:
p(S?) =
N
?
n=1
p(wn|wn?2, wn?1) (5)
Beside a word N-gram language model, a POS tag
N-gram model or a chunk tag N-gram model could
be used as well.
In this paper, we use a word trigram model. The
model is trained on reordered training source sen-
tences. A training source sentence is parsed into
chunks. In the same way as described in Section
4.2, word-to-word alignments is converted to chunk-
to-word alignments. We reorder the source chunks
to monotonize the chunk-to-word alignments. The
chunk boundaries are kept when this reordering is
done.
6 Experiments
6.1 Chunking Result
In this section, we report results for chunk parsing.
The annotation of the data is derived from the Chi-
nese treebank (LDC2005T01). The corpus is split
into two parts: 1000 sentences are randomly se-
5
Table 2: Statistics of training and test corpus for
chunk parsing.
train test
sentences 17 785 1 000
words 486 468 21 851
chunks 105 773 4 680
words out of chunks 244 416 10 282
Table 3: Chunk parsing result on 1000 sentences.
accuracy precision recall F-measure
74.51% 65.2% 61.5% 63.3
lected as test data. The remaining part is used for
training. The corpus is from the newswire domain.
Table 2 shows the corpus statistics. For the 4 680
chunks in the test set, the chunker has found 4 414
chunks, of which 2 879 are correct. Following the
criteria of CoNLL-2000, the chunker is evaluated
using the F-score, which is a combination of pre-
cision and recall. The result is shown in Table 3.
The accuracy is evaluated at the word level, the
other three metrics are evaluated at the chunk level.
The results at the chunk level are worse than at the
word level, because a chunk is counted as correct
only if the chunk tag and the chunk boundaries are
both correct.
6.2 Translation Results
For the translation experiments, we report the two
accuracy measures BLEU (Papineni et al, 2002)
and NIST (Doddington, 2002) as well as the two
error rates word error rate (WER) and position-
independent word error rate (PER).
We perform translation experiments on the Ba-
sic Traveling Expression Corpus (BTEC) for the
Chinese-English task. It is a speech translation task
in the domain of tourism-related information. We
report results on the IWSLT 2004, 2005 and 2006
evaluation test sets. There are 16 reference trans-
lations for the IWSLT 2004 and 2005 tasks and 7
reference translations for the IWSLT 2006 task.
Table 4 shows the corpus statistics of the task. A
training corpus is used to train the translation model,
the language model and to obtain the reordering
Table 4: Statistics of training and test corpora for the
IWSLT tasks.
Chinese English
Train Sentences 40k
Words 308k 377k
Dev Sentences 489
Words 5 478 6 008
Test Sentences 500
IWSLT04 Words 3 866 3 581
Test Sentences 506
IWSLT05 Words 3 652 3 579
Test Sentences 500
IWSLT06 Words 5 846 ?
rules. A development corpus is used to optimize the
scaling factors for the BLEU score. The English text
is processed using a tokenizer. The Chinese text pro-
cessing uses word segmentation with the ICTCLAS
segmenter (Zhang et al, 2003). The translation is
evaluated case-insensitive and without punctuation
marks.
The translation results are presented in Table 5.
The baseline system is a non-monotone translation
system, in which the decoder does reordering on
the target language side. Compared to the base-
line system, the source reordering method improves
the BLEU score by 0.5% ? 1.8% absolute. It also
achieves a better WER. Note that the used chun-
ker here is out-of-domain 3. An improvement is
achieved even with a low F-measure for chunking.
So, we could hope that larger improvement is possi-
ble using a high-accuracy chunker.
Though the input is a lattice, the source reordering
is still faster than the reordering during decoding,
e.g. for the IWSLT 2006 test set, the baseline system
took 17.5 minutes and the source reordering system
took 12.3 minutes. The result also indicates that the
non-monotone decoding hurts the performance in a
source reordering framework. A similar conclusion
is also presented in (Xia and McCord, 2004).
Additional experiments we carried out to compare
POS-level and chunk-level reorderings. We delete
the chunk information and keep the POS tags. Then,
3The chunker is trained on newswire data, but the test data
is from the tourism domain.
6
Table 5: Translation performance for the Chinese-English IWSLT task
WER[%] PER[%] NIST BLEU[%]
IWSLT04 baseline 47.3 38.2 7.78 39.1
source reordering 46.3 37.2 7.70 40.9
IWSLT05 baseline 45.0 37.3 7.40 41.8
source reordering 44.6 36.8 7.51 42.3
IWSLT06 baseline 67.4 50.0 6.65 22.4
source reordering 65.6 50.4 6.46 23.3
source reordering+non-monotone decoder 66.5 50.3 6.52 22.4
Table 6: Translation performance of reordering
methods on IWSLT 2004 test set
WER PER NIST BLEU
[%] [%] [%]
Baseline 47.3 38.2 7.78 39.1
POS 46.9 37.5 7.38 39.7
Chunk 46.3 37.2 7.70 40.9
Table 7: Lattice information for the Chinese-English
IWSLT 2004 test data
avg. density used translation
pro sent rules time [min/sec]
POS 15.7 6 868 7:08
Chunk 8.2 3 685 3:47
we rerun the source reordering system on the IWSLT
2004 test set. The translation results are shown in
Table 6. Though the accuracy of chunking is low,
the chunk-level method gets better results than POS-
level method. With POS tags, we get more reorder-
ing rules and more paths in the lattice, since the sen-
tence length is longer than with chunks. The statis-
tics are shown in Table 7.
7 Conclusions and Future Work
This paper presents a source-side reordering method
which is based on syntactic chunks. The reordering
rules are automatically learned from bilingual data.
To avoid hard decision before decoding, a reorder-
ing lattice representing all possible reorderings is
used instead of single source sentence for decoding.
The experiments demonstrate that even with a very
poor chunker, the chunk-level source reordering is
still helpful for a state-of-the-art statistical transla-
tion system and it has better performance than the
POS-level source reordering and target-side reorder-
ing.
There are some directions for future work. First,
we would like to try this method on larger data sets
and other language pairs. Second, we are going to
improve the chunking accuracy. Third, we would
reduce the number of rules and prune the lattice.
Acknowledgments
This material is partly based upon work sup-
ported by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-06-
C-0023, and was partially funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (Ne572/5)
References
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?72, March.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra,
F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.
1990. A statistical approach to machine translation. Com-
putational Linguistics, 16(2):79?85, June.
B. Chen, M. Cettolo, and M. Federico. 2006. Reordering
rules for phrase-based statistical machine translation. In
Int. Workshop on Spoken Language Translation Evaluation
Campaign on Spoken Language Translation, pages 1?15,
Kyoto, Japan, November.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause restructur-
ing for statistical machine translation. In Proc. of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL), pages 531?540, Ann Arbor, Michigan, June.
7
M. R. Costa-jussa` and J. A. R. Fonollosa. 2006. Statistical ma-
chine reordering. In Proc. of the Conf. on Empirical Meth-
ods in Natural Language Processing, pages 70?76, Sydney,
Australia, July.
J. M. Crego and J. B. Marin?o. 2006. Integration of postag-
based source reordering into SMT decoding by an extended
search graph. In Proc. of AMTA06, pages 29?36, Mas-
sachusetts, USA, August.
G. Doddington. 2002. Automatic evaluation of machine trans-
lation quality using n-gram co-occurrence statistics. In Proc.
ARPA Workshop on Human Language Technology.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and train-
ing of context-rich syntactic translation models. In Proc. of
the 21st Int. Conf. on Computational Linguistics and 44th
Annual Meeting of the Association for Computational Lin-
guistics, pages 961?968, Sydney, Australia, July.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. In Proc. of the 41th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages 80?87,
Sapporo, Japan, July.
J. Graehl and K. Knight. 2004. Training tree transducers.
In HLT-NAACL 2004: Main Proc., pages 105?112, Boston,
Massachusetts, USA, May 2 - May 7.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005.
Novel reordering approaches in phrase-based statistical ma-
chine translation. In 43rd Annual Meeting of the Assoc. for
Computational Linguistics: Proc. Workshop on Building and
Using Parallel Texts: Data-Driven Machine Translation and
Beyond, pages 167?174, Ann Arbor, Michigan, June.
P. Koehn and K. Knight. 2003. Empirical methods for com-
pound splitting. In Proc. 10th Conf. of the Europ. Chapter
of the Assoc. for Computational Linguistics (EACL), pages
347?354, Budapest, Hungary, April.
A. Mauser, R. Zens, E. Matusov, S. Hasan, and H. Ney. 2006.
The RWTH Statistical Machine Translation System for the
IWSLT 2006 Evaluation. In Proc. of the Int. Workshop
on Spoken Language Translation, pages 103?110, Kyoto,
Japan.
I. Melamed. 2004. Statistical machine translation by parsing.
In The Companion Volume to the Proc. of 42nd Annual Meet-
ing of the Association for Computational Linguistics, pages
653?660.
S. Nie?en and H. Ney. 2001. Morpho-syntactic analysis for
reordering in statistical machine translation. In Proc. of MT
Summit VIII, pages 247?252.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proc. of the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 295?302, Philadelphia,
PA, July.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19?51, March.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,
Z. Jin, and D. Radev. 2004. A smorgasbord of features for
statistical machine translation. In Proc. 2004 Human Lan-
guage Technology Conf. / North American Chapter of the
Association for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 161?168, Boston,MA.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of the 41th Annual Meeting of
the Association for Computational Linguistics (ACL), pages
160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. Bleu: a
method for automatic evaluation of machine translation. In
Proc. of the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 311?318, Philadelphia,
PA, July.
M. Popovic? and H. Ney. 2006. POS-based word reorderings
for statistical machine translation. In Proc. of the Fifth Int.
Conf. on Language Resources and Evaluation (LREC).
L. Shen, A. Sarkar, and F. J. Och. 2004. Discriminative rerank-
ing for machine translation. In HLT-NAACL 2004: Main
Proc., pages 177?184, Boston, Massachusetts, USA, May 2
- May 7.
C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga. 1997. A DP-
based search using monotone alignments in statistical trans-
lation. In Proc. 35th Annual Conf. of the Association for
Computational Linguistics, pages 289?296, Madrid, Spain,
July.
D. Wu. 1996. A polynomial-time algorithm for statistical ma-
chine translation. In Proc. 34th Annual Meeting of the As-
soc. for Computational Linguistics, pages 152?158, Santa
Cruz, CA, June.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377?403, September.
F. Xia and M. McCord. 2004. Improving a statistical MT sys-
tem with automatically learned rewrite patterns. In Proc. of
COLING04, pages 508?514, Geneva, Switzerland, Aug 23?
Aug 27.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In Proc. of the 39th Annual Meeting of
the Association for Computational Linguistics (ACL), pages
523?530, Toulouse, France, July.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based statistical
machine translation. In M. Jarke, J. Koehler, and G. Lake-
meyer, editors, 25th German Conf. on Artificial Intelligence
(KI2002), volume 2479 of Lecture Notes in Artificial Intel-
ligence (LNAI), pages 18?32, Aachen, Germany, September.
Springer Verlag.
H. P. Zhang, Q. Liu, X. Q. Cheng, H. Zhang, and H. K. Yu.
2003. Chinese lexical analysis using hierarchical hidden
markov model. In Proc. of the second SIGHAN workshop
on Chinese language processing, pages 63?70, Morristown,
NJ, USA.
8
Proceedings of NAACL-HLT 2013, pages 783?788,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Measuring the Structural Importance through Rhetorical Structure Index
Narine Kokhlikyan?, Alex Waibel? ?, Yuqi Zhang?, Joy Ying Zhang?
?Karlsruhe Institute of Technology
Adenauerring 2
76131 Karlsruhe, Germany
? Carnegie Mellon University
NASA Research Park, Bldg. 23
Moffett Field, CA 94035
narine.kokhlikyan@student.kit.edu, waibel@cs.cmu.edu,
yuqi.zhang@kit.edu, joy.zhang@sv.cmu.edu
Abstract
In this paper, we propose a novel Rhetorical
Structure Index (RSI) to measure the struc-
tural importance of a word or a phrase. Un-
like TF-IDF and other content-driven mea-
surements, RSI identifies words or phrases
that are structural cues in an unstructured doc-
ument. We show structurally motivated fea-
tures with high RSI values are more useful
than content-driven features for applications
such as segmenting unstructured lecture tran-
scripts into meaningful segments. Experi-
ments show that using RSI significantly im-
proves the segmentation accuracy compared
to TF-IDF, a traditional content-based feature
weighting scheme.
1 Introduction
Online learning, a new trend in distance learning,
provides numerous lectures to students all over the
world. More than 19,000 colleges offer thousands
of free online lectures1. Starting from video record-
ings of lectures which sometimes also come with the
presentation material, a set of processes can be ap-
plied to extract information from the unstructured
data to assist students in browsing, searching and
understanding the content of the lecture. These pro-
cesses include automatic speech recognition (ASR)
which converts the audio to text, lecture segmen-
tation which inserts paragraph boundaries and adds
section titles to the lecture transcriptions, automatic
summarization that generates a short summary from
1http://www.thebestcolleges.org/
free-online-classes-and-course-lectures/
the full lecture, and lecture translation that translates
the lecture from the original language to the native
language of the student.
The transcription of a lecture generated by the
ASR system is a sequence of words which does
not contain any structural information such as para-
graph, section boundaries and section titles. Zhang
et al (2007; 2008; 2010) used acoustic and lin-
guistic features for rhetorical structure detection and
summarization. They showed that linguistic features
such as TF-IDF are the most influential in segmenta-
tion and summarization and that knowing the struc-
ture of a lecture can significantly improve the perfor-
mance of lecture summarization. Our experiments
with a real-time lecture translation system also show
that displaying the rolling translation results of a live
lecture with proper paragraphing and inserted sec-
tion titles makes it easier for students to grasp the
key points during a lecture.
In this paper, we apply existing algorithms,
namely the Hidden Markov Model (HMM) (Gales
and Young, 2007) to unstructured lecture transcrip-
tion to infer the underlying structure for better lec-
ture segmentation and summarization. HMM has
been successfully applied in early works (van Mul-
bregt et al, 1998; Sherman and Liu, 2008) for text
segmentation, event tracking and boundary detec-
tion. The focus of this work is to identify cue
words and phrases that are good indicators of lec-
ture structure. Intuitively, words and phrases such
as ?last week we talked about?, ?this is an out-
line of my talk?, ?now I am going to talk about?,
?in conclusion?, and ?any questions? should be
important features to recognize lecture structure.
783
These words/phrases, however, may not be so im-
portant content-wise. Thus, content-driven met-
rics such as the TF-IDF score usually do not as-
sign higher weights to these structurally impor-
tant words/phrases. We propose a novel metric
called Rhetorical Structural Index (RSI) to weigh
words/phrases based on their structural importance.
2 Rhetorical Structural Index
RSI incorporates both frequency of occurrences and,
more importantly, the position distribution of occur-
rences of a word/phrase. The intuition is that if a
term is a structural marker, it usually occurs at a cer-
tain position in a lecture. Because the term is mainly
about the structure rather than the content of a lec-
ture, it can appear with high frequency in lectures
that are of different topics. For example, ?today we?
occurs at the beginning of a lecture and ?thank you?
usually appears towards the end (Figure 1) no mat-
ter whether the lecture is about history or computer
science.
We define the RSI of a word w as:
RSI(w) =
1
?Var(Lw) + (1? ?)idf(w,D)
(1)
where Lw is the random variable of ?normalized po-
sitions? of a word w in a lecture. For each occur-
rence of w in a particular lecture d, we divide its
position by the length of the lecture |d| to estimate
its ?normalized position?. Lw takes a value between
[0, 1]. A value close to 0 indicates this word occurs
at the beginning and close to 1 means w is close to
the end of the lecture. Var(Lw) is the variance of the
normalized position of a word w. A small Var(Lw)
indicates that w always occurs at certain positions
of a typical lecture (e.g., ?bye?) while a large value
means w can occur at any position (e.g., function
words ?of? and ?the?).
The second part of RSI is the inverse document
frequency (idf), or effectively the document fre-
quency since RSI is proportional to the 1/idf term.
Lectures, such as different research talks, can vary
in content but usually have a very similar structure
and share some common structural cues. A good
structural cue word should be common to many lec-
tures. idf has been widely used in information re-
trieval research to assign higher weights to words
that occur in just a few documents as compared to
Table 1: Examples of n-grams with high RSI values
which are likely to be structural cues.
n-gram Var(Lw) idf RSI
now 0.0004 0.60 1.04
here 0.0004 0.62 1.03
class 0.0001 2.12 0.90
week 0.0001 2.23 0.89
goodbye 0.0001 3.62 0.80
thank you 0.0003 1.53 0.95
talk about 0.0003 1.90 0.92
dealing with 0.0002 2.00 0.91
today we 0.0003 2.51 0.87
see how 0.0009 2.69 0.85
ladies and gentlemen 0.0008 1.35 0.96
last time we 0.0004 2.22 0.89
here we have 0.0005 2.35 0.88
next time we 0.0002 2.51 0.86
common words that occur in all documents. Define
the idf of a word w given a collection of lectures D
as:
idf(w,D) = log
|D|
|{d ? D|w ? d}|
(2)
|D| is the number of all lectures in the collection and
|{d ? D|w ? d}| is the number of lectures where w
appears. A low idf (w,D) value indicates that word
w occurs in many documents and thus is more likely
to be a common structural cue.
Combining the variance of normalized position
and idf by scaling factor ?, we define RSI as in equa-
tion 1. We found 0.9 as an optimal value of ? accord-
ing our experiments over all data sets. A word w
with high RSI value is more likely to be structurally
important. Similarly, we can calculate the RSI val-
ues for phrases (n-grams) such as ?I would like to
talk about?, ?I will switch gear to? and ?thank you
for your attention?.
Table 1 shows examples of n-grams and the cal-
culated variance, idf-scores and RSI values from a
collection of lectures.
3 Incorporating RSI in Lecture
Segmentation
Several algorithms have been developed for text seg-
mentation including the Naive Bayes classifier for
keyword extraction (Balagopalan et al, 2012), the
784
Figure 1: Fitted Poisson-distribution of normalized positions in lectures for the bigrams ?today we? and ?thank you?.
?today we? appears more frequently at the beginning of a lecture, whereas ?thank you? more in the concluding part
of a lecture. The x-axis is the normalized word position in a lecture and y-axis is the probability of seeing the word at
a position.
Hidden Markov Model (Gales and Young, 2007),
the Maximum entropy Markov model (McCallum et
al., 2000), the Conditional Random Field (Lafferty
et al, 2001) and the Latent Content Analysis (Ponte
and Croft, 1997). In this paper, we evaluate the ef-
fectiveness of the proposed RSI feature on lecture
segmentation using an HMM.
We represent each segment in a lecture as a state
in the Markov model and use the EM algorithm
to learn HMM parameters from unlabeled lecture
data. We use a fully connected HMM with five
states. Typical state labels for lecture are: ?Introduc-
tion?, ?Background?, ?Main Topic?, ?Questions?
and ?Conclusion? as shown in Figure 2. HMM states
emit word tokens. Instead of considering the full
vocabulary as the possible emission alphabet, which
usually leads to model over-fitting, we only consider
terms with high RSI values and high TF-IDF* scores
for comparison. For a word w, define its TF-IDF*
score as:
TF-IDF*(w) = max
d
TF-IDF(w, d), (3)
which is the highest TF-IDF score of a word in any
document in the collection. Our experiments try to
answer the question that ?if HMM is meant to cap-
ture the underlying structure of lectures no matter
which topic the lecture is about, what kind of fea-
tures should be emitted from each state to reflect
such structural patterns among lectures??
The learned HMM model is then applied to un-
seen lecture data to label each sentence to be ?In-
troduction?, ?Background?, ?Main Topic?, ?Ques-
tions? or ?Conclusion? and, based on the label, we
segment the lecture to different sections for evalu-
ation. Segment boundaries are defined in the posi-
tions where sentence labels change.
3.0.1 Bootstrap HMM from K-Means
Clustering Segmentation
Initial HMM parameters are bootstrapped using
results from K-means clustering where we cluster a
sequence of sentences to form a ?segment?. K cor-
responds to the number of desired segments of a lec-
ture. Similarities are computed based on the content
similarity (using n-gram matches) and the relative
785
sentence position defined as:
Sim(Si, Cj) = ?M(Si, Cj) + (1? ?)P (Si, Cj),
(4)
where Si is the i-th sentence, Cj is the centroid of
the j-th cluster. M(Si, Cj) is the content similarity
between sentence Si and centroid Cj and P(Si, Cj)
is the position similarity (distance). ? is a scaling
factor (set to optimal value 0.2 based on all data sets
in our experiments).
Content similarity is based on the number of com-
mon words between two sentences, or between a
sentence and the centroid vector of a cluster. Denote
the binary word frequency vector (bag of words) in
sentence Si as ~Si and similarly ~Cj for cluster cen-
troid Cj , define:
M(Si, Cj) =
~Si? ~Cj
? ~Si ?? ~Cj ?
. (5)
P(Si, Cj) measures the position similarity of two
sentences. Position similarity is based on the rela-
tive position distance between the sentence and the
cluster:Define
P (Si, Cj) =
L
|Pos(Si)? Pos(Cj)|+ 
, (6)
where Si is the i-th sentence, Cj is the j-th cluster.
Pos(Si) is the position of sentence Si. Pos(Cj) is
the average sentence position of all members belong
to cluster Cj and L is the total number of sentences
in a lecture.  is a small constant to avoid division
by zeros.
4 Experiments and Evaluation
We evaluated segmentation on three different data
sets: college lectures recorded by Karlsruhe In-
stitute of Technology (KIT), Microsoft research
(MSR) lectures2 and scientific papers3. Both col-
lege and Microsoft research lectures are manually
transcribed. The reason why we do not include ex-
periments on ASR output is that current ASR quality
of lecture data is still quite poor. Word-Error-Rates
(WER) of ASR output range from 24.37 to 30.80 for
KIT lectures. Roughly speaking, every one out of 3
or 4 words is mis-recognized.
2http://research.microsoft.com/apps/catalog/
3http://aclweb.org/anthology-new/
For evaluation, human annotators annotated a few
lectures to create test/reference sets. The test data
from KIT is annotated by one human annotator and
MSR lectures are annotated by four annotators. The
segmentation gold standard is created based on the
agreed annotations. Since the number of annotated
lectures is small and human annotation is subjective,
we also used ACL papers as an additional data set.
ACL papers are in a way ?lectures in written form?
and have titles for section and subsections which can
be used to identify the segments and annotate the
data set automatically. The statistics of each data set
are listed in Table 2.
Table 2: Statistics of three data sets used in the exper-
iments: our own lecture data (KIT), Microsoft research
talks (MSR) and conference proceedings from ACL an-
thology archive. We removed equations, short titles such
as ?Abstract? and ?Conclusion?, when extracting text
from PDF files from the ACL anthology, which results
in a relatively small number of words per paper. Words
are simply tokenized without case normalization or stem-
ming, which results in relatively large vocabulary sizes.
Properties KIT MSR ACL
Num. 74 1,182 3,583
Avg. Num. of Sent. 484 655 212
Avg. Num. of Words 10,078 10,225 3,896
Avg. Duration (Min.) 43.57 39.15 -
Vocabulary Size 1.3K 22K 24K
First, we calculate the RSI and TF-IDF* scores
for each word in the dataset and choose the top N
words as the HMM emission vocabulary. To avoid
over-fitting, we choose N that is much smaller than
the full vocabulary size of the data set. In our ex-
periments, we set N=300 for KIT, N=5000 for MSR
and N=5400 for ACL. The top 5 words with the
highest TF-IDF* scores from the MSR data set are:
?RFID?, ?Cherokee?, ?tree-to-string?, ?GPU?, and
?data-triggered?, whereas the top 5 words selected
by RSI are ?today?, ?work?, ?question?, ?now?, and
?thank?, which are more structurally informative.
To estimate the accuracy of the segmentation
module, we used Recall, Precision, F-Measure and
Pk (Beeferman et al, 1999) as evaluation metric.
We used an error window of length 6 to calculate
Precision, Recall and F-Measure and a sliding win-
dow with a length equal to half of the average seg-
786
Introduction Main Topic Conclusion Background Questions 
Introduction 
word tokens 
Background 
word tokens 
Content 
word tokens 
Questions 
word tokens 
Conclusion 
word tokens 
Figure 2: Fully connected 5-state HMM representing Introduction, Background, Main Topic, Questions, Conclusion
in a typical lecture.
ment length to estimate the Pk score. With error
window we mean that hypothesis boundaries do not
have to be exactly the same as the reference segment
boundaries. Hypothesis boundaries are acceptable
if they are close enough to reference boundaries in
that window. The Pk score indicates the probability
of segmentation inconsistency. Therefore, the lower
the Pk score the better the segmentation is.
Table 3: Segmentation results measured by Pk (the
smaller the better), Precision, Recall and F-Measure
scores (the higher the better) for three data sets compar-
ing HMM using TF-IDF*-filtered word tokens as emis-
sion and RSI-filtered words as emissions.
Evaluation Score KIT MSR ACL
Pk
HMM + TF-IDF* 0.06 0.06 0.05
HMM + RSI 0.01 0.02 0.01
Precision
HMM + TF-IDF* 32.01 30.47 32.85
HMM + RSI 41.10 41.01 42.70
Recall
HMM + TF-IDF* 39.32 36.09 38.08
HMM + RSI 47.38 46.39 48.95
F-Measure
HMM + TF-IDF* 35.29 33.04 35.27
HMM + RSI 44.01 43.53 45.61
The evaluation results on all data sets listed in
Table 3 show that according F-Measure and Pk
scores, considering words with high RSI values as
HMM emission significantly improve over the base-
line method of choosing word tokens with high TF-
IDF* scores.
5 Conclusions
In this work we propose the Rhetorical Structure In-
dex (RSI), a method to identify structurally impor-
tant terms in lectures. Experiments show that terms
with high RSI values are better candidates than those
with high TF-IDF values when used by an HMM-
based segmenter as state emissions. In other words,
terms with high RSI values are more likely to be
structural cues in lectures independent of the lecture
topic. In the future we will run experiments on ASR
output and incorporate other prosodic features such
as pitch, intensity, duration into the RSI to improve
this metric for structural analysis of lectures and ap-
ply the RSI to other structure discovery applications
such as dialogue segmentation.
Acknowledgments
The authors gratefully acknowledge the support by
an interACT student exchange scholarship. The re-
search leading to these results has received funding
from the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement no
287658.
We would like to thank Jan Niehues and Teresa
Herrmann for their suggestions and help.
787
References
A. Balagopalan, L.L. Balasubramanian, V. Balasubrama-
nian, N. Chandrasekharan, and A. Damodar. 2012.
Automatic keyphrase extraction and segmentation of
video lectures. In Technology Enhanced Education
(ICTEE), 2012 IEEE International Conference on,
pages 1?10.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Mach.
Learn., 34(1-3):177?210, February.
Mark J. F. Gales and Steve J. Young. 2007. The ap-
plication of hidden markov models in speech recog-
nition. Foundations and Trends in Signal Processing,
1(3):195?304.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282?289.
Andrew McCallum, Dayne Freitag, and Fernando C. N.
Pereira. 2000. Maximum entropy markov models for
information extraction and segmentation. In Proceed-
ings of the Seventeenth International Conference on
Machine Learning, ICML ?00, pages 591?598, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Jay M. Ponte and W. Bruce Croft. 1997. Text segmenta-
tion by topic. In ECDL, pages 113?125.
Melissa Sherman and Yang Liu. 2008. Using hid-
den markov models for topic segmentation of meeting
transcripts. In SLT, pages 185?188.
Paul van Mulbregt, Ira Carp, Lawrence Gillick, Steve
Lowe, and Jon Yamron. 1998. Text segmentation and
topic tracking on broadcast news via a hidden markov
model approach. In ICSLP.
Justin Jian Zhang, Ricky Ho Yin Chan, and Pascale Fung.
2007. Improving lecture speech summarization using
rhetorical information. In ASRU, pages 195?200.
Justin Jian Zhang, Shilei Huang, and Pascale Fung. 2008.
Rshmm++ for extractive lecture speech summariza-
tion. In SLT, pages 161?164.
Justin Jian Zhang, Ricky Ho Yin Chan, and Pascale Fung.
2010. Extractive speech summarization using shallow
rhetorical structure modeling. IEEE Transactions on
Audio, Speech & Language Processing, 18(6):1147?
1157.
788
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 349?355,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2012
Jan Niehues, Yuqi Zhang, Mohammed Mediani, Teresa Herrmann, Eunah Cho and Alex Waibel
Karlsruhe Institute of Technology
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes the phrase-based SMT
systems developed for our participation
in the WMT12 Shared Translation Task.
Translations for English?German and
English?French were generated using a
phrase-based translation system which is
extended by additional models such as
bilingual, fine-grained part-of-speech (POS)
and automatic cluster language models and
discriminative word lexica. In addition, we
explicitly handle out-of-vocabulary (OOV)
words in German, if we have translations for
other morphological forms of the same stem.
Furthermore, we extended the POS-based
reordering approach to also use information
from syntactic trees.
1 Introduction
In this paper, we describe our systems for the
NAACL 2012 Seventh Workshop on Statistical Ma-
chine Translation. We participated in the Shared
Translation Task and submitted translations for
English?German and English?French. We use a
phrase-based decoder that can use lattices as input
and developed several models that extend the stan-
dard log-linear model combination of phrase-based
MT. In addition to the POS-based reordering model
used in past years, for German-English we extended
it to also use rules learned using syntax trees.
The translation model was extended by the bilin-
gual language model and a discriminative word lex-
icon using a maximum entropy classifier. For the
French-English and English-French translation sys-
tems, we also used phrase table adaptation to avoid
overestimation of the probabilities of the huge, but
noisy Giga corpus. In the German-English system,
we tried to learn translations for OOV words by ex-
ploring different morphological forms of the OOVs
with the same lemma.
Furthermore, we combined different language
models in the log-linear model. We used word-
based language models trained on different parts of
the training corpus as well as POS-based language
models using fine-grained POS information and lan-
guage models trained on automatic word clusters.
The paper is organized as follows: The next sec-
tion gives a detailed description of our systems in-
cluding all the models. The translation results for
all directions are presented afterwards and we close
with a conclusion.
2 System Description
For the French?English systems the phrase table
is based on a GIZA++ word alignment, while the
systems for German?English use a discriminative
word alignment as described in Niehues and Vogel
(2008). The language models are 4-gram SRI lan-
guage models using Kneser-Ney smoothing trained
by the SRILM Toolkit (Stolcke, 2002).
The problem of word reordering is addressed with
POS-based and tree-based reordering models as de-
scribed in Section 2.3. The POS tags used in the
reordering model are obtained using the TreeTagger
(Schmid, 1994). The syntactic parse trees are gen-
erated using the Stanford Parser (Rafferty and Man-
ning, 2008).
An in-house phrase-based decoder (Vogel, 2003)
is used to perform translation. Optimization with
349
regard to the BLEU score is done using Minimum
Error Rate Training as described in Venugopal et al
(2005). During decoding only the top 10 translation
options for every source phrase are considered.
2.1 Data
Our translation models were trained on the EPPS
and News Commentary (NC) corpora. Furthermore,
the additional available data for French and English
(i.e. UN and Giga corpora) were exploited in the
corresponding systems.
The systems were tuned with the news-test2011
data, while news-test2011 was used for testing in all
our systems. We trained language models for each
language on the monolingual part of the training cor-
pora as well as the News Shuffle and the Gigaword
(version 4) corpora. The discriminative word align-
ment model was trained on 500 hand-aligned sen-
tences selected from the EPPS corpus.
2.2 Preprocessing
The training data is preprocessed prior to training
the system. This includes normalizing special sym-
bols, smart-casing the first word of each sentence
and removing long sentences and sentences with
length mismatch.
For the German parts of the training corpus, in
order to obtain a homogenous spelling, we use the
hunspell1 lexicon to map words written according to
old German spelling rules to new German spelling
rules.
In order to reduce the OOV problem of German
compound words, Compound splitting as described
in Koehn and Knight (2003) is applied to the Ger-
man part of the corpus for the German-to-English
system.
The Giga corpus received a special preprocessing
by removing noisy pairs using an SVM classifier as
described in Mediani et al (2011). The SVM clas-
sifier training and test sets consist of randomly se-
lected sentence pairs from the corpora of EPPS, NC,
tuning, and test sets. Giving at the end around 16
million sentence pairs.
2.3 Word Reordering
In contrast to modeling the reordering by a distance-
based reordering model and/or a lexicalized distor-
1http://hunspell.sourceforge.net/
tion model, we use a different approach that relies on
POS sequences. By abstracting from surface words
to POS, we expect to model the reordering more ac-
curately. For German-to-English, we additionally
apply reordering rules learned from syntactic parse
trees.
2.3.1 POS-based Reordering Model
In order to build the POS-based reordering model,
we first learn probabilistic rules from the POS tags
of the training corpus and the alignment. Contin-
uous reordering rules are extracted as described in
Rottmann and Vogel (2007) to model short-range re-
orderings. When translating between German and
English, we apply a modified reordering model with
non-continuous rules to cover also long-range re-
orderings (Niehues and Kolss, 2009).
2.3.2 Tree-based Reordering Model
Word order is quite different between German and
English. And during translation especially verbs or
verb particles need to be shifted over a long dis-
tance in a sentence. Using discontinuous POS rules
already improves the translation tremendously. In
addition, we apply a tree-based reordering model
for the German-English translation. Syntactic parse
trees provide information about the words in a sen-
tence that form constituents and should therefore be
treated as inseparable units by the reordering model.
For the tree-based reordering model, syntactic parse
trees are generated for the whole training corpus.
Then the word alignment between the source and
target language part of the corpus is used to learn
rules on how to reorder the constituents in a Ger-
man source sentence to make it matches the English
target sentence word order better. In order to apply
the rules to the source text, POS tags and a parse
tree are generated for each sentence. Then the POS-
based and tree-based reordering rules are applied.
The original order of words as well as the reordered
sentence variants generated by the rules are encoded
in a word lattice. The lattice is then used as input to
the decoder.
For the test sentences, the reordering based on
POS and trees allows us to change the word order
in the source sentence so that the sentence can be
translated more easily. In addition, we build reorder-
ing lattices for all training sentences and then extract
350
phrase pairs from the monotone source path as well
as from the reordered paths.
2.4 Translation Models
In addition to the models used in the baseline system
described above, we conducted experiments includ-
ing additional models that enhance translation qual-
ity by introducing alternative or additional informa-
tion into the translation modeling process.
2.4.1 Phrase table adaptation
Since the Giga corpus is huge, but noisy, it is
advantageous to also use the translation probabil-
ities of the phrase pair extracted only from the
more reliable EPPS and News commentary cor-
pus. Therefore, we build two phrase tables for the
French?English system. One trained on all data
and the other only trained on the EPPS and News
commentary corpus. The two models are then com-
bined using a log-linear combination to achieve the
adaptation towards the cleaner corpora as described
in (Niehues et al, 2010). The newly created trans-
lation model uses the four scores from the general
model as well as the two smoothed relative frequen-
cies of both directions from the smaller, but cleaner
model. If a phrase pair does not occur in the in-
domain part, a default score is used instead of a rela-
tive frequency. In our case, we used the lowest prob-
ability.
2.4.2 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder according to the best com-
bination of phrases that maximize the translation
and language model scores. This segmentation into
phrases leads to the loss of context information at
the phrase boundaries. Although more target side
context is available to the language model, source
side context would also be valuable for the decoder
when searching for the best translation hypothesis.
To make also source language context available we
use a bilingual language model, in which each token
consists of a target word and all source words it is
aligned to. The bilingual tokens enter the translation
process as an additional target factor and the bilin-
gual language model is applied to the additional fac-
tor like a normal language model. For more details
see Niehues et al (2011).
2.4.3 Discriminative Word Lexica
Mauser et al (2009) have shown that the use
of discriminative word lexica (DWL) can improve
the translation quality. For every target word, they
trained a maximum entropy model to determine
whether this target word should be in the translated
sentence or not using one feature per one source
word.
When applying DWL in our experiments, we
would like to have the same conditions for the train-
ing and test case. For this we would need to change
the score of the feature only if a new word is added
to the hypothesis. If a word is added the second time,
we do not want to change the feature value. In order
to keep track of this, additional bookkeeping would
be required. Also the other models in our translation
system will prevent us from using a word too often.
Therefore, we ignore this problem and can calcu-
late the score for every phrase pair before starting
with the translation. This leads to the following def-
inition of the model:
p(e|f) =
J?
j=1
p(ej |f) (1)
In this definition, p(ej |f) is calculated using a max-
imum likelihood classifier.
Each classifier is trained independently on the
parallel training data. All sentences pairs where the
target word e occurs in the target sentence are used
as positive examples. We could now use all other
sentences as negative examples. But in many of
these sentences, we would anyway not generate the
target word, since there is no phrase pair that trans-
lates any of the source words into the target word.
Therefore, we build a target vocabulary for every
training sentence. This vocabulary consists of all
target side words of phrase pairs matching a source
phrase in the source part of the training sentence.
Then we use all sentence pairs where e is in the tar-
get vocabulary but not in the target sentences as neg-
ative examples. This has shown to have a postive
influence on the translation quality (Mediani et al,
2011) and also reduces training time.
2.4.4 Quasi-Morphological Operations for
OOV words
Since German is a highly inflected language, there
will be always some word forms of a given Ger-
351
Figure 1: Quasi-morphological operations
man lemma that did not occur in the training data.
In order to be able to also translate unseen word
forms, we try to learn quasi-morphological opera-
tions that change the lexical entry of a known word
form to the unknown word form. These have shown
to be beneficial in Niehues and Waibel (2011) using
Wikipedia2 titles. The idea is illustrated in Figure 1.
If we look at the data, our system is able to trans-
late a German word Kamin (engl. chimney), but not
the dative plural form Kaminen. To address this
problem, we try to automatically learn rules how
words can be modified. If we look at the example,
we would like the system to learn the following rule.
If an ?en? is appended to a German word, as it is
done when creating the dative plural form of Kami-
nen, we need to add an ?s? to the end of the English
word in order to perform the same morphological
word transformation. We use only rules where the
ending of the word has at most 3 letters.
Depending on the POS, number, gender or case of
the involved words, the same operation on the source
side does not necessarily correspond to the same op-
eration on the target side.
To account for this ambiguity, we rank the differ-
ent target operation using the following four features
and use the best ranked one. Firstly, we should not
generate target words that do not exist. Here, we
have an advantage that we can use monolingual data
to determine whether the word exists. In addition,
a target operation that often coincides with a given
source operation should be better than one that is
rarely used together with the source operation. We
therefore look at pairs of entries in the lexicon and
count in how many of them the source operation can
be applied to the source side and the target operation
can be applied to the target side. We then use only
operations that occur at least ten times. Furthermore,
2http://www.wikipedia.org/
we use the ending of the source and target word to
determine which pair of operations should be used.
Integration We only use the proposed method for
OOVs and do not try to improve translations of
words that the baseline system already covers. We
look for phrase pairs, for which a source operation
ops exists that changes one of the source words f1
into the OOV word f2. Since we need to apply a
target operation to one word on the target side of the
phrase pair, we only consider phrase pairs where f1
is aligned to one of the target words of the phrase
containing e1. If a target operation exists given f1
and ops, we select the one with the highest rank.
Then we generate a new phrase pair by applying
ops to f1 and opt to e1 keeping the original scores
from the phrase pairs, since the original and syn-
thesized phrase pair are not directly competing any-
way. We do not add several phrase pairs generated
by different operations, since we would then need to
add the features used for ranking the operations into
the MERT. This is problematic, since the operations
were only used for very few words and therefore a
good estimation of the weights is not possible.
2.5 Language Models
The 4-gram language models generated by the
SRILM toolkit are used as the main language mod-
els for all of our systems. For English-French and
French-English systems, we use a good quality cor-
pus as in-domain data to train in-domain language
models. Additionally, we apply the POS and clus-
ter language models in different systems. All lan-
guage models are integrated into the translation sys-
tem by a log-linear combination and received opti-
mal weights during tuning by the MERT.
2.5.1 POS Language Models
The POS language model is trained on the POS
sequences of the target language. In this evalua-
tion, the POS language model is applied for the
English-German system. We expect that having ad-
ditional information in form of probabilities of POS
sequences should help especially in case of the rich
morphology of German. The POS tags are gener-
ated with the RFTagger (Schmid and Laws, 2008)
for German, which produces fine-grained tags that
include person, gender and case information. We
352
use a 9-gram language model on the News Shuf-
fle corpus and the German side of all parallel cor-
pora. More details and discussions about the POS
language model can be found in Herrmann et al
(2011).
2.5.2 Cluster Language Models
The cluster language model follows a similar idea
as the POS language model. Since there is a data
sparsity problem when we substitute words with the
word classes, it is possible to make use of larger
context information. In the POS language model,
POS tags are the word classes. Here, we generated
word classes in a different way. First, we cluster
the words in the corpus using the MKCLS algorithm
(Och, 1999) given a number of classes. Second, we
replace the words in the corpus by their cluster IDs.
Finally, we train an n-gram language model on this
corpus consisting of cluster IDs. Generally, all clus-
ter language models used in our systems are 5-gram.
3 Results
Using the models described above we performed
several experiments leading finally to the systems
used for generating the translations submitted to the
workshop. The following sections describe the ex-
periments for the individual language pairs and show
the translation results. The results are reported as
case-sensitive BLEU scores (Papineni et al, 2002)
on one reference translation.
3.1 German-English
The experiments for the German-English translation
system are summarized in Table 1. The Baseline
system uses POS-based reordering, discriminative
word alignment and a language model trained on the
News Shuffle corpus. By adding lattice phrase ex-
traction small improvements of the translation qual-
ity could be gained.
Further improvements could be gained by adding
a language model trained on the Gigaword corpus
and adding a bilingual and cluster-based language
model. We used 50 word classes and trained a 5-
gram language model. Afterwards, the translation
quality was improved by also using a discriminative
word lexicon. Finally, the best system was achieved
by using Tree-based reordering and using special
treatment for the OOVs. This system generates a
BLEU score of 22.31 on the test data. For the last
two systems, we did not perform new optimization
runs.
System Dev Test
Baseline 23.64 21.32
+ Lattice Phrase Extraction 23.76 21.36
+ Gigaward Language Model 24.01 21.73
+ Bilingual LM 24.19 21.91
+ Cluster LM 24.16 22.09
+ DWL 24.19 22.19
+ Tree-based Reordering - 22.26
+ OOV - 22.31
Table 1: Translation results for German-English
3.2 English-German
The English-German baseline system uses also
POS-based reordering, discriminative word align-
ment and a language model based on EPPS, NC and
News Shuffle. A small gain could be achieved by the
POS-based language model and the bilingual lan-
guage model. Further gain was achieved by using
also a cluster-based language model. For this lan-
guage model, we use 100 word classes and trained
a 5-gram language model. Finally, the best system
uses the discriminative word lexicon.
System Dev Test
Baseline 17.06 15.57
+ POSLM 17.27 15.63
+ Bilingual LM 17.40 15.78
+ Cluster LM 17.77 16.06
+ DWL 17.75 16.28
Table 2: Translation results for English-German
3.3 English-French
Table 3 summarizes how our English-French sys-
tem evolved. The baseline system here was trained
on the EPPS, NC, and UN corpora, while the lan-
guage model was trained on all the French part of
the parallel corpora (including the Giga corpus). It
also uses short-range reordering trained on EPPS
and NC. This system had a BLEU score of around
26.7. The Giga parallel data turned out to be quite
353
beneficial for this task. It improves the scores by
more than 1 BLEU point. More importantly, addi-
tional language models boosted the system quality:
around 1.8 points. In fact, three language models
were log-linearly combined: In addition to the afore-
mentioned, two additional language models were
trained on the monolingual sets (one for News and
one for Gigaword). We could get an improvement
of around 0.2 by retraining the reordering rules on
EPPS and NC only, but using Giza alignment from
the whole data. Adapting the translation model by
using EPPS and NC as in-domain data improves the
BLEU score by only 0.1. This small improvement
might be due to the fact that the news domain is
very broad and that the Giga corpus has already been
carefully cleaned and filtered. Furthermore, using a
bilingual language model enhances the BLEU score
by almost 0.3. Finally, incorporating a cluster lan-
guage model adds an additional 0.1 to the score.
This leads to a system with 30.58.
System Dev Test
Baseline 24.96 26.67
+ GigParData 26.12 28.16
+ Big LMs 29.22 29.92
+ All Reo 29.14 30.10
+ PT Adaptation 29.15 30.22
+ Bilingual LM 29.17 30.49
+ Cluster LM 29.08 30.58
Table 3: Translation results for English-French
3.4 French-English
The development of our system for the French-
English direction is summarized in Table 4. The
baseline system for this direction was trained on the
EPPS, NC, UN and Giga parallel corpora, while the
language model was trained on the French part of the
parallel training corpora. The baseline system in-
cludes the POS-based reordering model with short-
range rules. The largest improvement of 1.7 BLEU
score was achieved by the integration of the bigger
language models which are trained on the English
version of News Shuffle and the Gigaword corpus
(v4). We did not add the language models from the
monolingual English version of EPPS and NC data,
since the experiments have shown that they did not
provide improvement in our system. The second
largest improvement came from the domain adap-
tation that includes an in-domain language model
and adaptations to the phrase extraction. The BLEU
score has improved about 1 BLEU in total. The in-
domain data we used here are parallel EPPS and NC
corpus. Further gains were obtained by augmenting
the system with a bilingual language model adding
around 0.2 BLEU to the previous score. The sub-
mitted system was obtained by adding the cluster
5-gram language model trained on the News Shuf-
fle corpus with 100 clusters and thus giving 30.25 as
the final score.
System Dev Test
Baseline 25.81 27.15
+ Indomain LM 26.17 27.91
+ PT Adaptation 26.33 28.11
+ Big LMs 28.90 29.82
+ Bilingual LM 29.14 30.09
+ Cluster LM 29.31 30.25
Table 4: Translation results for French-English
4 Conclusions
We have presented the systems for our participation
in the WMT 2012 Evaluation for English?German
and English?French. In all systems we could im-
prove by using a class-based language model. Fur-
thermore, the translation quality could be improved
by using a discriminative word lexicon. Therefore,
we trained a maximum entropy classifier for ev-
ery target word. For English?French, adapting the
phrase table helps to avoid using wrong parts of the
noisy Giga corpus. For the German-to-English sys-
tem, we could improve the translation quality addi-
tionally by using a tree-based reordering model and
by special handling of OOV words. For the inverse
direction we could improve the translation quality
by using a 9-gram language model trained on the
fine-grained POS tags.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
354
References
Teresa Herrmann, Mohammed Mediani, Jan Niehues,
and Alex Waibel. 2011. The karlsruhe institute of
technology translation systems for the wmt 2011. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 379?385, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical Meth-
ods for Compound Splitting. In EACL, Budapest,
Hungary.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrim-
inative and Trigger-based Lexicon Models. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 - Vol-
ume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The kit english-
french translation systems for iwslt 2011. In Proceed-
ings of the eight International Workshop on Spoken
Language Translation (IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling. In
Proc. of Third ACL Workshop on Statistical Machine
Translation, Columbus, USA.
Jan Niehues and Alex Waibel. 2011. Using wikipedia to
translate domain-specific terms in smt. In Proceedings
of the eight International Workshop on Spoken Lan-
guage Translation (IWSLT).
Jan Niehues, Mohammed Mediani, Teresa Herrmann,
Michael Heck, Christian Herff, and Alex Waibel.
2010. The KIT Translation system for IWSLT 2010.
In Marcello Federico, Ian Lane, Michael Paul, and
Franc?ois Yvon, editors, Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 93?98.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Associa-
tion for Computational Linguistics, EACL ?99, pages
71?76, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division, T. J.
Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three german treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Workshop
on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
Based Distortion Model. In TMI, Sko?vde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation of
Conditional Probabilities with Decision Trees and an
Application to Fine-Grained POS Tagging. In COL-
ING 2008, Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of ICSLP, Denver,
Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Beyond
(WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Pro-
cessing and Knowledge Engineering, Beijing, China.
355
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130?135,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2014
Teresa Herrmann, Mohammed Mediani, Eunah Cho, Thanh-Le Ha,
Jan Niehues, Isabel Slawik, Yuqi Zhang and Alex Waibel
Institute for Anthropomatics and Robotics
KIT - Karlsruhe Institute of Technology
firstname.lastname@kit.edu
Abstract
In this paper, we present the KIT
systems participating in the Shared
Translation Task translating between
English?German and English?French.
All translations are generated using
phrase-based translation systems, using
different kinds of word-based, part-of-
speech-based and cluster-based language
models trained on the provided data.
Additional models include bilingual lan-
guage models, reordering models based
on part-of-speech tags and syntactic parse
trees, as well as a lexicalized reordering
model. In order to make use of noisy
web-crawled data, we apply filtering
and data selection methods for language
modeling. A discriminative word lexicon
using source context information proved
beneficial for all translation directions.
1 Introduction
We describe the KIT systems for the Shared Trans-
lation Task of the ACL 2014 Ninth Workshop on
Statistical Machine Translation. We participated
in the English?German and English?French
translation directions, using a phrase-based de-
coder with lattice input.
The paper is organized as follows: the next sec-
tion describes the data used for each translation
direction. Section 3 gives a detailed description of
our systems including all the models. The trans-
lation results for all directions are presented after-
wards and we close with a conclusion.
2 Data
We utilize the provided EPPS, NC and Common
Crawl parallel corpora for English?German and
German?English, plus Giga for English?French
and French?English. The monolingual part
of those parallel corpora, the News Shuffle
corpus for all four directions and additionally
the Gigaword corpus for English?French and
German?English are used as monolingual train-
ing data for the different language models. For
optimizing the system parameters, newstest2012
and newstest2013 are used as development and
test data respectively.
3 System Description
Before training we perform a common preprocess-
ing of the raw data, which includes removing long
sentences and sentences with a length mismatch
exceeding a certain threshold. Afterwards, we nor-
malize special symbols, dates, and numbers. Then
we perform smart-casing of the first letter of every
sentence. Compound splitting (Koehn and Knight,
2003) is performed on the source side of the cor-
pus for German?English translation. In order to
improve the quality of the web-crawled Common
Crawl corpus, we filter out noisy sentence pairs us-
ing an SVM classifier for all four translation tasks
as described in Mediani et al. (2011).
Unless stated otherwise, we use 4-gram lan-
guage models (LM) with modified Kneser-Ney
smoothing, trained with the SRILM toolkit (Stol-
cke, 2002). All translations are generated by
an in-house phrase-based translation system (Vo-
gel, 2003), and we use Minimum Error Rate
Training (MERT) as described in Venugopal et
al. (2005) for optimization. The word align-
ment of the parallel corpora is generated using
the GIZA++ Toolkit (Och and Ney, 2003) for
both directions. Afterwards, the alignments are
combined using the grow-diag-final-and heuris-
tic. For English?German, we use discrimi-
native word alignment trained on hand-aligned
data as described in Niehues and Vogel (2008).
The phrase table (PT) is built using the Moses
toolkit (Koehn et al., 2007). The phrase scoring
for the small data sets (German?English) is also
130
done by the Moses toolkit, whereas the bigger sets
(French?English) are scored by our in-house par-
allel phrase scorer (Mediani et al., 2012a). The
phrase pair probabilities are computed using mod-
ified Kneser-Ney smoothing as described in Foster
et al. (2006).
Since German is a highly inflected language,
we try to alleviate the out-of-vocabulary prob-
lem through quasi-morphological operations that
change the lexical entry of a known word form to
an unknown word form as described in Niehues
and Waibel (2011).
3.1 Word Reordering Models
We apply automatically learned reordering rules
based on part-of-speech (POS) sequences and syn-
tactic parse tree constituents to perform source
sentence reordering according to the target lan-
guage word order. The rules are learned
from a parallel corpus with POS tags (Schmid,
1994) for the source side and a word align-
ment to learn reordering rules that cover short
range (Rottmann and Vogel, 2007) and long
range reorderings (Niehues and Kolss, 2009).
In addition, we apply a tree-based reordering
model (Herrmann et al., 2013) to better address
the differences in word order between German and
English. Here, a word alignment and syntactic
parse trees (Rafferty and Manning, 2008; Klein
and Manning, 2003) for the source side of the
training corpus are required to learn rules on how
to reorder the constituents in the source sentence.
The POS-based and tree-based reordering rules
are applied to each input sentence before transla-
tion. The resulting reordered sentence variants as
well as the original sentence are encoded in a re-
ordering lattice. The lattice, which also includes
the original position of each word, is used as input
to the decoder.
In order to acquire phrase pairs matching the
reordered sentence variants, we perform lattice
phrase extraction (LPE) on the training corpus
where phrase are extracted from the reordered
word lattices instead of the original sentences.
In addition, we use a lexicalized reordering
model (Koehn et al., 2005) which stores reorder-
ing probabilities for each phrase pair. During
decoding the lexicalized reordering model deter-
mines the reordering orientation of each phrase
pair at the phrase boundaries. The probability for
the respective orientation with respect to the orig-
inal position of the words is included as an addi-
tional score in the log-linear model of the transla-
tion system.
3.2 Adaptation
In the French?English and English?French sys-
tems, we perform adaptation for translation mod-
els as well as for language models. The EPPS and
NC corpora are used as in-domain data for the di-
rection English?French, while NC corpus is the
in-domain data for French?English.
Two phrase tables are built: one is the out-
of-domain phrase table, which is trained on all
corpora; the other is the in-domain phrase table,
which is trained on in-domain data. We adapt the
translation model by using the scores from the two
phrase tables with the backoff approach described
in Niehues and Waibel (2012). This results in a
phrase table with six scores, the four scores from
the general phrase table as well as the two condi-
tional probabilities from the in-domain phrase ta-
ble. In addition, we take the union of the candidate
phrase pairs collected from both phrase tables A
detailed description of the union method can be
found in Mediani et al. (2012b).
The language model is adapted by log-linearly
combining the general language model and an in-
domain language model. We train a separate lan-
guage model using only the in-domain data. Then
it is used as an additional language model during
decoding. Optimal weights are set during tuning
by MERT.
3.3 Special Language Models
In addition to word-based language models, we
use different types of non-word language models
for each of the systems. With the help of a bilin-
gual language model (Niehues et al., 2011) we
are able to increase the bilingual context between
source and target words beyond phrase bound-
aries. This language model is trained on bilin-
gual tokens created from a target word and all its
aligned source words. The tokens are ordered ac-
cording to the target language word order.
Furthermore, we use language models based
on fine-grained part-of-speech tags (Schmid and
Laws, 2008) as well as word classes to allevi-
ate the sparsity problem for surface words. The
word classes are automatically learned by clus-
tering the words of the corpus using the MKCLS
algorithm (Och, 1999). These n-gram language
models are trained on the target language corpus,
131
where the words have been replaced either by their
corresponding POS tag or cluster ID. During de-
coding, these language models are used as addi-
tional models in the log-linear combination.
The data selection language model is trained
on data automatically selected using cross-entropy
differences between development sets from pre-
vious WMT workshops and the noisy crawled
data (Moore and Lewis, 2010). We selected the
top 10M sentences to train this language model.
3.4 Discriminative Word Lexicon
A discriminative word lexicon (DWL) models the
probability of a target word appearing in the trans-
lation given the words of the source sentence.
DWLs were first introduced by Mauser et al.
(2009). For every target word, they train a maxi-
mum entropy model to determine whether this tar-
get word should be in the translated sentence or
not using one feature per source word.
We use two simplifications of this model that
have shown beneficial to translation quality and
training time in the past (Mediani et al., 2011).
Firstly, we calculate the score for every phrase pair
before translating. Secondly, we restrict the nega-
tive training examples to words that occur within
matching phrase pairs.
In this evaluation, we extended the DWL
with n-gram source context features proposed
by Niehues and Waibel (2013). Instead of rep-
resenting the source sentence as a bag-of-words,
we model it as a bag-of-n-grams. This allows us
to include information about source word order in
the model. We used one feature per n-gram up to
the order of three and applied count filtering for
bigrams and trigrams.
4 Results
This section presents the participating systems
used for the submissions in the four translation
directions of the evaluation. We describe the in-
dividual components that form part of each of
the systems and report the translation qualities
achieved during system development. The scores
are reported in case-sensitive BLEU (Papineni et
al., 2002).
4.1 English-French
The development of our English?French system
is shown in Table 1.
It is noteworthy that, for this direction, we chose
to tune on a subset of 1,000 pairs from news-
test2012, due to the long time the whole set takes
to be decoded. In a preliminary set of experiments
(not reported here), we found no significant differ-
ences between tuning on the small or the big devel-
opment sets. The translation model of the baseline
system is trained on the whole parallel data after
filtering (EPPS, NC, Common Crawl, Giga). The
same data was also used for language modeling.
We also use POS-based reordering.
The biggest improvement was due to using two
additional language models. One consists of a log-
linear interpolation of individual language models
trained on the target side of the parallel data, the
News shuffle, Gigaword and NC corpora. In ad-
dition, an in-domain language model trained only
on NC data is used. This improves the score by
more than 1.4 points. Adaptation of the translation
model towards a smaller model trained on EPPS
and NC brings an additional 0.3 points.
Another 0.3 BLEU points could be gained by
using other special language models: a bilingual
language model together with a 4-gram cluster
language model (trained on all monolingual data
using the MKCLS tool and 500 clusters). Incor-
porating a lexicalized reordering model into the
system had a very noticeable effect on test namely
more than half a BLEU point.
Finally, using a discriminative word lexicon
with source context has a very small positive ef-
fect on the test score, however more than 0.3 on
dev. This final configuration was the basis of our
submitted official translation.
System Dev Test
Baseline 15.63 27.61
+ Big LMs 16.56 29.02
+ PT Adaptation 16.77 29.32
+ Bilingual + Cluster LM 16.87 29.64
+ Lexicalized Reordering 16.92 30.17
+ Source DWL 17.28 30.19
Table 1: Experiments for English?French
4.2 French-English
Several experiments were conducted for the
French?English translation system. They are
summarized in Table 2.
The baseline system is essentially a phrase-
based translation system with some preprocess-
132
ing steps on the source side and utilizing the
short-range POS-based reordering on all parallel
data and fine-grained monolingual corpora such as
EPPS and NC.
Adapting the translation model using a small in-
domain phrase table trained on NC data only helps
us gain more than 0.4 BLEU points.
Using non-word language models including a
bilingual language model and a 4-gram 50-cluster
language model trained on the whole parallel data
attains 0.24 BLEU points on the test set.
Lexicalized reordering improves our system on
the development set by 0.3 BLEU points but has
less effect on the test set with a minor improve-
ment of around 0.1 BLEU points.
We achieve our best system, which is used for
the evaluation, by adding a DWL with source con-
text yielding 31.54 BLEU points on the test set.
System Dev Test
Baseline 30.16 30.70
+ LM Adaptation 30.58 30.94
+ PT Adaptation 30.69 31.14
+ Bilingual + Cluster LM 30.85 31.38
+ Lexicalized Reordering 31.14 31.46
+ Source DWL 31.19 31.54
Table 2: Experiments for French?English
4.3 English-German
Table 3 presents how the English-German transla-
tion system is improved step by step.
In the baseline system, we used parallel data
which consists of the EPPS and NC corpora. The
phrase table is built using discriminative word
alignment. For word reordering, we use word lat-
tices with long range reordering rules. Five lan-
guage models are used in the baseline system; two
word-based language models, a bilingual language
model, and two 9-gram POS-based language mod-
els. The two word-based language models use 4-
gram context and are trained on the parallel data
and the filtered Common Crawl data separately,
while the bilingual language model is built only
on the Common Crawl corpus. The two POS-
based language models are also based on the paral-
lel data and the filtered crawled data, respectively.
When using a 9-gram cluster language model,
we get a slight improvement. The cluster is trained
with 1,000 classes using EPPS, NC, and Common
Crawl data.
We use the filtered crawled data in addition to
the parallel data in order to build the phrase table;
this gave us 1 BLEU point of improvement.
The system is improved by 0.1 BLEU points
when we use lattice phrase extraction along with
lexicalized reordering rules.
Tree-based reordering rules improved the sys-
tem performance further by another 0.1 BLEU
points.
By reducing the context of the two POS-based
language models from 9-grams to 5-grams and
shortening the context of the language model
trained on word classes to 4-grams, the score on
the development set hardly changes but we can see
a slightly improvement for the test case.
Finally, we use the DWL with source context
and build a big bilingual language model using
both the crawled and parallel data. By doing so,
we improved the translation performance by an-
other 0.3 BLEU points. This system was used for
the translation of the official test set.
System Dev Test
Baseline 16.64 18.60
+ Cluster LM 16.76 18.66
+ Common Crawl Data 17.27 19.66
+ LPE + Lexicalized Reordering 17.45 19.75
+ Tree Rules 17.53 19.85
+ Shorter n-grams 17.55 19.92
+ Source DWL + Big BiLM 17.82 20.21
Table 3: Experiments for English?German
4.4 German-English
Table 4 shows the development steps of the
German-English translation system.
For the baseline system, the training data of the
translation model consists of EPPS, NC and the
filtered parallel crawled data. The phrase table
is built using GIZA++ word alignment and lattice
phrase extraction. All language models are trained
with SRILM and scored in the decoding process
with KenLM (Heafield, 2011). We use word lat-
tices generated by short and long range reordering
rules as input to the decoder. In addition, a bilin-
gual language model and a target language model
trained on word clusters with 1,000 classes are in-
cluded in the system.
Enhancing the word reordering with tree-based
reordering rules and a lexicalized reordering
133
model improved the system performance by 0.6
BLEU points.
Adding a language model trained on selected
data from the monolingual corpora gave another
small improvement.
The DWL with source context increased the
score on the test set by another 0.5 BLEU points
and applying morphological operations to un-
known words reduced the out-of-vocabulary rate,
even though no improvement in BLEU can be ob-
served. This system was used to generate the
translation submitted to the evaluation.
System Dev Test
Baseline 24.40 26.34
+ Tree Rules 24.71 26.86
+ Lexicalized Reordering 24.89 26.93
+ LM Data Selection 24.96 27.03
+ Source DWL 25.32 27.53
+ Morphological Operations - 27.53
Table 4: Experiments for German?English
5 Conclusion
In this paper, we have described the systems
developed for our participation in the Shared
Translation Task of the WMT 2014 evaluation
for English?German and English?French. All
translations were generated using a phrase-based
translation system which was extended by addi-
tional models such as bilingual and fine-grained
part-of-speech language models. Discriminative
word lexica with source context proved beneficial
in all four language directions.
For English-French translation using a smaller
development set performed reasonably well and
reduced development time. The most noticeable
gain comes from log-linear interpolation of multi-
ple language models.
Due to the large amounts and diversity of
the data available for French-English, adapta-
tion methods and non-word language models con-
tribute the major improvements to the system.
For English-German translation, the crawled
data and a DWL using source context to guide
word choice brought most of the improvements.
Enhanced word reordering models, namely
tree-based reordering rules and a lexicalized re-
ordering model as well as the source-side fea-
tures for the discriminative word lexicon helped
improve the system performance for German-
English translation.
In average we achieved an improvement of over
1.5 BLEU over the respective baselines for all our
systems.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
?
287658.
References
George F. Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Proceedings of the 2006 Con-
ference on Empirical Methods on Natural Language
Processing (EMNLP), Sydney, Australia.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
Edinburgh, Scotland, United Kingdom.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL 2003), Sapporo, Japan.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of the Eleventh Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2003), Budapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the Second International Workshop
on Spoken Language Translation (IWSLT 2005),
Pittsburgh, PA, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL
2007), Demonstration Session, Prague, Czech Re-
public.
134
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models.
In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Suntec, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight International
Workshop on Spoken Language Translation (IWSLT
2011), San Francisco, CA, USA.
Mohammed Mediani, Jan Niehues, and Alex Waibel.
2012a. Parallel Phrase Scoring for Extra-large Cor-
pora. In The Prague Bulletin of Mathematical Lin-
guistics, number 98.
Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, Jan
Niehues, Eunach Cho, Teresa Herrmann, Rainer
K?argel, and Alexander Waibel. 2012b. The KIT
Translation Systems for IWSLT 2012. In Proceed-
ings of the Ninth International Workshop on Spoken
Language Translation (IWSLT 2012), Hong Kong,
HK.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In Proceedings
of the ACL 2010 Conference Short Papers, Uppsala,
Sweden.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation (WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation (WMT 2008), Columbus, OH,
USA.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia
to Translate Domain-specific Terms in SMT. In
Proceedings of the Eight International Workshop on
Spoken Language Translation (IWSLT 2008), San
Francisco, CA, USA.
J. Niehues and A. Waibel. 2012. Detailed Analysis of
Different Strategies for Phrase Table Adaptation in
SMT. In Proceedings of the Tenth Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA 2012), San Diego, CA, USA.
J. Niehues and A. Waibel. 2013. An MT Error-Driven
Discriminative Word Lexicon using Sentence Struc-
ture Features. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, Scotland, United King-
dom.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for Deter-
mining Bilingual Word Classes. In Proceedings of
the Ninth Conference of the European Chapter of the
Association for Computational Linguistics (EACL
1999), Bergen, Norway.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, T. J. Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German, Columbus, OH,
USA.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of the
11th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI
2007), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In In-
ternational Conference on Computational Linguis-
tics (COLING 2008), Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, United Kingdom.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts, Ann Arbor, Michigan, USA.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
135
